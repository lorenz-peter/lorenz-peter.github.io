[
    {
        "date": "2025-05",
        "title": "F-ANcGAN: An Attention-Enhanced Cycle Consistent Generative Adversarial Architecture for Synthetic Image Generation of Nanoparticles",
        "author": "Varun Ajith, Anindya Pal, Saumik Bhattacharya, and Sayantari Ghosh",
        "link": "http://arxiv.org/abs/2505.18106v1",
        "abstract": "Nanomaterial research is becoming a vital area for energy, medicine, and\nmaterials science, and accurate analysis of the nanoparticle topology is\nessential to determine their properties. Unfortunately, the lack of\nhigh-quality annotated datasets drastically hinders the creation of strong\nsegmentation models for nanoscale imaging. To alleviate this problem, we\nintroduce F-ANcGAN, an attention-enhanced cycle consistent generative\nadversarial system that can be trained using a limited number of data samples\nand generates realistic scanning electron microscopy (SEM) images directly from\nsegmentation maps. Our model uses a Style U-Net generator and a U-Net\nsegmentation network equipped with self-attention to capture structural\nrelationships and applies augmentation methods to increase the variety of the\ndataset. The architecture reached a raw FID score of 17.65 for TiO$_2$ dataset\ngeneration, with a further reduction in FID score to nearly 10.39 by using\nefficient post-processing techniques. By facilitating scalable high-fidelity\nsynthetic dataset generation, our approach can improve the effectiveness of\ndownstream segmentation task training, overcoming severe data shortage issues\nin nanoparticle analysis, thus extending its applications to resource-limited\nfields."
    },
    {
        "date": "2025-05",
        "title": "Towards more transferable adversarial attack in black-box manner",
        "author": "Chun Tong Lei, Zhongliang Guo, Hon Chung Lee, Minh Quoc Duong, and Chun Pong Lau",
        "link": "http://arxiv.org/abs/2505.18097v1",
        "abstract": "Adversarial attacks have become a well-explored domain, frequently serving as\nevaluation baselines for model robustness. Among these, black-box attacks based\non transferability have received significant attention due to their practical\napplicability in real-world scenarios. Traditional black-box methods have\ngenerally focused on improving the optimization framework (e.g., utilizing\nmomentum in MI-FGSM) to enhance transferability, rather than examining the\ndependency on surrogate white-box model architectures. Recent state-of-the-art\napproach DiffPGD has demonstrated enhanced transferability by employing\ndiffusion-based adversarial purification models for adaptive attacks. The\ninductive bias of diffusion-based adversarial purification aligns naturally\nwith the adversarial attack process, where both involving noise addition,\nreducing dependency on surrogate white-box model selection. However, the\ndenoising process of diffusion models incurs substantial computational costs\nthrough chain rule derivation, manifested in excessive VRAM consumption and\nextended runtime. This progression prompts us to question whether introducing\ndiffusion models is necessary. We hypothesize that a model sharing similar\ninductive bias to diffusion-based adversarial purification, combined with an\nappropriate loss function, could achieve comparable or superior transferability\nwhile dramatically reducing computational overhead. In this paper, we propose a\nnovel loss function coupled with a unique surrogate model to validate our\nhypothesis. Our approach leverages the score of the time-dependent classifier\nfrom classifier-guided diffusion models, effectively incorporating natural data\ndistribution knowledge into the adversarial optimization process. Experimental\nresults demonstrate significantly improved transferability across diverse model\narchitectures while maintaining robustness against diffusion-based defenses."
    },
    {
        "date": "2025-05",
        "title": "Linear Mixture Distributionally Robust Markov Decision Processes",
        "author": "Zhishuai Liu, and Pan Xu",
        "link": "http://arxiv.org/abs/2505.18044v1",
        "abstract": "Many real-world decision-making problems face the off-dynamics challenge: the\nagent learns a policy in a source domain and deploys it in a target domain with\ndifferent state transitions. The distributionally robust Markov decision\nprocess (DRMDP) addresses this challenge by finding a robust policy that\nperforms well under the worst-case environment within a pre-specified\nuncertainty set of transition dynamics. Its effectiveness heavily hinges on the\nproper design of these uncertainty sets, based on prior knowledge of the\ndynamics. In this work, we propose a novel linear mixture DRMDP framework,\nwhere the nominal dynamics is assumed to be a linear mixture model. In contrast\nwith existing uncertainty sets directly defined as a ball centered around the\nnominal kernel, linear mixture DRMDPs define the uncertainty sets based on a\nball around the mixture weighting parameter. We show that this new framework\nprovides a more refined representation of uncertainties compared to\nconventional models based on $(s,a)$-rectangularity and $d$-rectangularity,\nwhen prior knowledge about the mixture model is present. We propose a meta\nalgorithm for robust policy learning in linear mixture DRMDPs with general\n$f$-divergence defined uncertainty sets, and analyze its sample complexities\nunder three divergence metrics instantiations: total variation,\nKullback-Leibler, and $\\chi^2$ divergences. These results establish the\nstatistical learnability of linear mixture DRMDPs, laying the theoretical\nfoundation for future research on this new setting."
    },
    {
        "date": "2025-05",
        "title": "Improved Algorithms for Overlapping and Robust Clustering of Edge-Colored Hypergraphs: An LP-Based Combinatorial Approach",
        "author": "Changyeol Lee, Yongho Shin, and Hyung-Chan An",
        "link": "http://arxiv.org/abs/2505.18043v1",
        "abstract": "Clustering is a fundamental task in both machine learning and data mining.\nAmong various methods, edge-colored clustering (ECC) has emerged as a useful\napproach for handling categorical data. Given a hypergraph with (hyper)edges\nlabeled by colors, ECC aims to assign vertex colors to minimize the number of\nedges where the vertex color differs from the edge's color. However,\ntraditional ECC has inherent limitations, as it enforces a nonoverlapping and\nexhaustive clustering. To tackle these limitations, three versions of ECC have\nbeen studied: Local ECC and Global ECC, which allow overlapping clusters, and\nRobust ECC, which accounts for vertex outliers. For these problems, both linear\nprogramming (LP) rounding algorithms and greedy combinatorial algorithms have\nbeen proposed. While these LP-rounding algorithms provide high-quality\nsolutions, they demand substantial computation time; the greedy algorithms, on\nthe other hand, run very fast but often compromise solution quality. In this\npaper, we present an algorithmic framework that combines the strengths of LP\nwith the computational efficiency of combinatorial algorithms. Both\nexperimental and theoretical analyses show that our algorithms efficiently\nproduce high-quality solutions for all three problems: Local, Global, and\nRobust ECC. We complement our algorithmic contributions with\ncomplexity-theoretic inapproximability results and integrality gap bounds,\nwhich suggest that significant theoretical improvements are unlikely. Our\nresults also answer two open questions previously raised in the literature."
    },
    {
        "date": "2025-05",
        "title": "Superplatforms Have to Attack AI Agents",
        "author": "Jianghao Lin, Jiachen Zhu, Zheli Zhou, Yunjia Xi, Weiwen Liu, Yong Yu, and Weinan Zhang",
        "link": "http://arxiv.org/abs/2505.17861v1",
        "abstract": "Over the past decades, superplatforms, digital companies that integrate a\nvast range of third-party services and applications into a single, unified\necosystem, have built their fortunes on monopolizing user attention through\ntargeted advertising and algorithmic content curation. Yet the emergence of AI\nagents driven by large language models (LLMs) threatens to upend this business\nmodel. Agents can not only free user attention with autonomy across diverse\nplatforms and therefore bypass the user-attention-based monetization, but might\nalso become the new entrance for digital traffic. Hence, we argue that\nsuperplatforms have to attack AI agents to defend their centralized control of\ndigital traffic entrance. Specifically, we analyze the fundamental conflict\nbetween user-attention-based monetization and agent-driven autonomy through the\nlens of our gatekeeping theory. We show how AI agents can disintermediate\nsuperplatforms and potentially become the next dominant gatekeepers, thereby\nforming the urgent necessity for superplatforms to proactively constrain and\nattack AI agents. Moreover, we go through the potential technologies for\nsuperplatform-initiated attacks, covering a brand-new, unexplored technical\narea with unique challenges. We have to emphasize that, despite our position,\nthis paper does not advocate for adversarial attacks by superplatforms on AI\nagents, but rather offers an envisioned trend to highlight the emerging\ntensions between superplatforms and AI agents. Our aim is to raise awareness\nand encourage critical discussion for collaborative solutions, prioritizing\nuser interests and perserving the openness of digital ecosystems in the age of\nAI agents."
    },
    {
        "date": "2025-05",
        "title": "Scalable Valuation of Human Feedback through Provably Robust Model Alignment",
        "author": "Masahiro Fujisawa, Masaki Adachi, and Michael A. Osborne",
        "link": "http://arxiv.org/abs/2505.17859v1",
        "abstract": "Despite the importance of aligning language models with human preferences,\ncrowd-sourced human feedback is often noisy -- for example, preferring less\ndesirable responses -- posing a fundamental challenge to alignment. A truly\nrobust alignment objective should yield identical model parameters even under\nsevere label noise, a property known as redescending. We prove that no existing\nalignment methods satisfy this property. To address this, we propose\nH\\\"older-DPO, the first principled alignment loss with a provable redescending\nproperty, enabling estimation of the clean data distribution from noisy\nfeedback. The aligned model estimates the likelihood of clean data, providing a\ntheoretically grounded metric for dataset valuation that identifies the\nlocation and fraction of mislabels. This metric is gradient-free, enabling\nscalable and automated human feedback valuation without costly manual\nverification or clean validation dataset. H\\\"older-DPO achieves\nstate-of-the-art robust alignment performance while accurately detecting\nmislabels in controlled datasets. Finally, we apply H\\\"older-DPO to widely used\nalignment datasets, revealing substantial noise levels and demonstrating that\nremoving these mislabels significantly improves alignment performance across\nmethods."
    },
    {
        "date": "2025-05",
        "title": "Robust Distributed Estimation: Extending Gossip Algorithms to Ranking and Trimmed Means",
        "author": "Anna Van Elst, Igor Colin, and Stephan Cl\u00e9men\u00e7on",
        "link": "http://arxiv.org/abs/2505.17836v1",
        "abstract": "This paper addresses the problem of robust estimation in gossip algorithms\nover arbitrary communication graphs. Gossip algorithms are fully decentralized,\nrelying only on local neighbor-to-neighbor communication, making them\nwell-suited for situations where communication is constrained. A fundamental\nchallenge in existing mean-based gossip algorithms is their vulnerability to\nmalicious or corrupted nodes. In this paper, we show that an outlier-robust\nmean can be computed by globally estimating a robust statistic. More\nspecifically, we propose a novel gossip algorithm for rank estimation, referred\nto as \\textsc{GoRank}, and leverage it to design a gossip procedure dedicated\nto trimmed mean estimation, coined \\textsc{GoTrim}. In addition to a detailed\ndescription of the proposed methods, a key contribution of our work is a\nprecise convergence analysis: we establish an $\\mathcal{O}(1/t)$ rate for rank\nestimation and an $\\mathcal{O}(\\log(t)/t)$ rate for trimmed mean estimation,\nwhere by $t$ is meant the number of iterations. Moreover, we provide a\nbreakdown point analysis of \\textsc{GoTrim}. We empirically validate our\ntheoretical results through experiments on diverse network topologies, data\ndistributions and contamination schemes."
    },
    {
        "date": "2025-05",
        "title": "Imagine Beyond! Distributionally Robust Auto-Encoding for State Space Coverage in Online Reinforcement Learning",
        "author": "Nicolas Castanet, Olivier Sigaud, and Sylvain Lamprier",
        "link": "http://arxiv.org/abs/2505.17830v1",
        "abstract": "Goal-Conditioned Reinforcement Learning (GCRL) enables agents to autonomously\nacquire diverse behaviors, but faces major challenges in visual environments\ndue to high-dimensional, semantically sparse observations. In the online\nsetting, where agents learn representations while exploring, the latent space\nevolves with the agent's policy, to capture newly discovered areas of the\nenvironment. However, without incentivization to maximize state coverage in the\nrepresentation, classical approaches based on auto-encoders may converge to\nlatent spaces that over-represent a restricted set of states frequently visited\nby the agent. This is exacerbated in an intrinsic motivation setting, where the\nagent uses the distribution encoded in the latent space to sample the goals it\nlearns to master. To address this issue, we propose to progressively enforce\ndistributional shifts towards a uniform distribution over the full state space,\nto ensure a full coverage of skills that can be learned in the environment. We\nintroduce DRAG (Distributionally Robust Auto-Encoding for GCRL), a method that\ncombines the $\\beta$-VAE framework with Distributionally Robust Optimization.\nDRAG leverages an adversarial neural weighter of training states of the VAE, to\naccount for the mismatch between the current data distribution and unseen parts\nof the environment. This allows the agent to construct semantically meaningful\nlatent spaces beyond its immediate experience. Our approach improves state\nspace coverage and downstream control performance on hard exploration\nenvironments such as mazes and robotic control involving walls to bypass,\nwithout pre-training nor prior environment knowledge."
    },
    {
        "date": "2025-05",
        "title": "Temporal Consistency Constrained Transferable Adversarial Attacks with Background Mixup for Action Recognition",
        "author": "Ping Li, Jianan Ni, and Bo Pang",
        "link": "http://arxiv.org/abs/2505.17807v1",
        "abstract": "Action recognition models using deep learning are vulnerable to adversarial\nexamples, which are transferable across other models trained on the same data\nmodality. Existing transferable attack methods face two major challenges: 1)\nthey heavily rely on the assumption that the decision boundaries of the\nsurrogate (a.k.a., source) model and the target model are similar, which limits\nthe adversarial transferability; and 2) their decision boundary difference\nmakes the attack direction uncertain, which may result in the gradient\noscillation, weakening the adversarial attack. This motivates us to propose a\nBackground Mixup-induced Temporal Consistency (BMTC) attack method for action\nrecognition. From the input transformation perspective, we design a\nmodel-agnostic background adversarial mixup module to reduce the\nsurrogate-target model dependency. In particular, we randomly sample one video\nfrom each category and make its background frame, while selecting the\nbackground frame with the top attack ability for mixup with the clean frame by\nreinforcement learning. Moreover, to ensure an explicit attack direction, we\nleverage the background category as guidance for updating the gradient of\nadversarial example, and design a temporal gradient consistency loss, which\nstrengthens the stability of the attack direction on subsequent frames.\nEmpirical studies on two video datasets, i.e., UCF101 and Kinetics-400, and one\nimage dataset, i.e., ImageNet, demonstrate that our method significantly boosts\nthe transferability of adversarial examples across several action/image\nrecognition models. Our code is available at\nhttps://github.com/mlvccn/BMTC_TransferAttackVid."
    },
    {
        "date": "2025-05",
        "title": "Sec5GLoc: Securing 5G Indoor Localization via Adversary-Resilient Deep Learning Architecture",
        "author": "Ildi Alla, and Valeria Loscri",
        "link": "http://arxiv.org/abs/2505.17776v1",
        "abstract": "Emerging 5G millimeter-wave and sub-6 GHz networks enable high-accuracy\nindoor localization, but security and privacy vulnerabilities pose serious\nchallenges. In this paper, we identify and address threats including location\nspoofing and adversarial signal manipulation against 5G-based indoor\nlocalization. We formalize a threat model encompassing attackers who inject\nforged radio signals or perturb channel measurements to mislead the\nlocalization system. To defend against these threats, we propose an\nadversary-resilient localization architecture that combines deep learning\nfingerprinting with physical domain knowledge. Our approach integrates\nmulti-anchor Channel Impulse Response (CIR) fingerprints with Time Difference\nof Arrival (TDoA) features and known anchor positions in a hybrid Convolutional\nNeural Network (CNN) and multi-head attention network. This design inherently\nchecks geometric consistency and dynamically down-weights anomalous signals,\nmaking localization robust to tampering. We formulate the secure localization\nproblem and demonstrate, through extensive experiments on a public 5G indoor\ndataset, that the proposed system achieves a mean error approximately 0.58 m\nunder mixed Line-of-Sight (LOS) and Non-Line-of-Sight (NLOS) trajectories in\nbenign conditions and gracefully degrades to around 0.81 m under attack\nscenarios. We also show via ablation studies that each architecture component\n(attention mechanism, TDoA, etc.) is critical for both accuracy and resilience,\nreducing errors by 4-5 times compared to baselines. In addition, our system\nruns in real-time, localizing the user in just 1 ms on a simple CPU. The code\nhas been released to ensure reproducibility\n(https://github.com/sec5gloc/Sec5GLoc)."
    },
    {
        "date": "2025-05",
        "title": "A Distributionally-Robust Framework for Nuisance in Causal Effect Estimation",
        "author": "Akira Tanimoto",
        "link": "http://arxiv.org/abs/2505.17717v1",
        "abstract": "Causal inference requires evaluating models on balanced distributions between\ntreatment and control groups, while training data often exhibits imbalance due\nto historical decision-making policies. Most conventional statistical methods\naddress this distribution shift through inverse probability weighting (IPW),\nwhich requires estimating propensity scores as an intermediate step. These\nmethods face two key challenges: inaccurate propensity estimation and\ninstability from extreme weights. We decompose the generalization error to\nisolate these issues--propensity ambiguity and statistical instability--and\naddress them through an adversarial loss function. Our approach combines\ndistributionally robust optimization for handling propensity uncertainty with\nweight regularization based on weighted Rademacher complexity. Experiments on\nsynthetic and real-world datasets demonstrate consistent improvements over\nexisting methods."
    },
    {
        "date": "2025-05",
        "title": "Tuning Language Models for Robust Prediction of Diverse User Behaviors",
        "author": "Fanjin Meng, Jingtao Ding, Jiahui Gong, Chen Yang, Hong Chen, Zuojian Wang, Haisheng Lu, and Yong Li",
        "link": "http://arxiv.org/abs/2505.17682v1",
        "abstract": "Predicting user behavior is essential for intelligent assistant services, yet\ndeep learning models often struggle to capture long-tailed behaviors. Large\nlanguage models (LLMs), with their pretraining on vast corpora containing rich\nbehavioral knowledge, offer promise. However, existing fine-tuning approaches\ntend to overfit to frequent ``anchor'' behaviors, reducing their ability to\npredict less common ``tail'' behaviors. In this paper, we introduce BehaviorLM,\na progressive fine-tuning approach that addresses this issue. In the first\nstage, LLMs are fine-tuned on anchor behaviors while preserving general\nbehavioral knowledge. In the second stage, fine-tuning uses a balanced subset\nof all behaviors based on sample difficulty to improve tail behavior\npredictions without sacrificing anchor performance. Experimental results on two\nreal-world datasets demonstrate that BehaviorLM robustly predicts both anchor\nand tail behaviors and effectively leverages LLM behavioral knowledge to master\ntail behavior prediction with few-shot examples."
    },
    {
        "date": "2025-05",
        "title": "Enhancing Large Vision-Language Models with Layout Modality for Table Question Answering on Japanese Annual Securities Reports",
        "author": "Hayato Aida, Kosuke Takahashi, and Takahiro Omi",
        "link": "http://arxiv.org/abs/2505.17625v1",
        "abstract": "With recent advancements in Large Language Models (LLMs) and growing interest\nin retrieval-augmented generation (RAG), the ability to understand table\nstructures has become increasingly important. This is especially critical in\nfinancial domains such as securities reports, where highly accurate question\nanswering (QA) over tables is required. However, tables exist in various\nformats-including HTML, images, and plain text-making it difficult to preserve\nand extract structural information. Therefore, multimodal LLMs are essential\nfor robust and general-purpose table understanding. Despite their promise,\ncurrent Large Vision-Language Models (LVLMs), which are major representatives\nof multimodal LLMs, still face challenges in accurately understanding\ncharacters and their spatial relationships within documents. In this study, we\npropose a method to enhance LVLM-based table understanding by incorporating\nin-table textual content and layout features. Experimental results demonstrate\nthat these auxiliary modalities significantly improve performance, enabling\nrobust interpretation of complex document layouts without relying on explicitly\nstructured input formats."
    },
    {
        "date": "2025-05",
        "title": "One Model Transfer to All: On Robust Jailbreak Prompts Generation against LLMs",
        "author": "Linbao Li, Yannan Liu, Daojing He, and Yu Li",
        "link": "http://arxiv.org/abs/2505.17598v1",
        "abstract": "Safety alignment in large language models (LLMs) is increasingly compromised\nby jailbreak attacks, which can manipulate these models to generate harmful or\nunintended content. Investigating these attacks is crucial for uncovering model\nvulnerabilities. However, many existing jailbreak strategies fail to keep pace\nwith the rapid development of defense mechanisms, such as defensive suffixes,\nrendering them ineffective against defended models. To tackle this issue, we\nintroduce a novel attack method called ArrAttack, specifically designed to\ntarget defended LLMs. ArrAttack automatically generates robust jailbreak\nprompts capable of bypassing various defense measures. This capability is\nsupported by a universal robustness judgment model that, once trained, can\nperform robustness evaluation for any target model with a wide variety of\ndefenses. By leveraging this model, we can rapidly develop a robust jailbreak\nprompt generator that efficiently converts malicious input prompts into\neffective attacks. Extensive evaluations reveal that ArrAttack significantly\noutperforms existing attack strategies, demonstrating strong transferability\nacross both white-box and black-box models, including GPT-4 and Claude-3. Our\nwork bridges the gap between jailbreak attacks and defenses, providing a fresh\nperspective on generating robust jailbreak prompts. We make the codebase\navailable at https://github.com/LLBao/ArrAttack."
    },
    {
        "date": "2025-05",
        "title": "Large Language Models in the IoT Ecosystem -- A Survey on Security Challenges and Applications",
        "author": "Kushal Khatiwada, Jayden Hopper, Joseph Cheatham, Ayan Joshi, and Sabur Baidya",
        "link": "http://arxiv.org/abs/2505.17586v1",
        "abstract": "The Internet of Things (IoT) and Large Language Models (LLMs) have been two\nmajor emerging players in the information technology era. Although there has\nbeen significant coverage of their individual capabilities, our literature\nsurvey sheds some light on the integration and interaction of LLMs and IoT\ndevices - a mutualistic relationship in which both parties leverage the\ncapabilities of the other. LLMs like OpenAI's ChatGPT, Anthropic's Claude,\nGoogle's Gemini/BERT, any many more, all demonstrate powerful capabilities in\nnatural language understanding and generation, enabling more intuitive and\ncontext-aware interactions across diverse IoT applications such as smart\ncities, healthcare systems, industrial automation, and smart home environments.\nDespite these opportunities, integrating these resource-intensive LLMs into IoT\ndevices that lack the state-of-the-art computational power is a challenging\ntask. The security of these edge devices is another major concern as they can\neasily act as a backdoor to private networks if the LLM integration is sloppy\nand unsecured. This literature survey systematically explores the current\nstate-of-the-art in applying LLMs within IoT, emphasizing their applications in\nvarious domains/sectors of society, the significant role they play in enhancing\nIoT security through anomaly detection and threat mitigation, and strategies\nfor effective deployment using edge computing frameworks. Finally, this survey\nhighlights existing challenges, identifies future research directions, and\nunderscores the need for cross-disciplinary collaboration to fully realize the\ntransformative potential of integrating LLMs and IoT."
    },
    {
        "date": "2025-05",
        "title": "Ownership Verification of DNN Models Using White-Box Adversarial Attacks with Specified Probability Manipulation",
        "author": "Teruki Sano, Minoru Kuribayashi, Masao Sakai, Shuji Ishobe, and Eisuke Koizumi",
        "link": "http://arxiv.org/abs/2505.17579v1",
        "abstract": "In this paper, we propose a novel framework for ownership verification of\ndeep neural network (DNN) models for image classification tasks. It allows\nverification of model identity by both the rightful owner and third party\nwithout presenting the original model. We assume a gray-box scenario where an\nunauthorized user owns a model that is illegally copied from the original\nmodel, provides services in a cloud environment, and the user throws images and\nreceives the classification results as a probability distribution of output\nclasses. The framework applies a white-box adversarial attack to align the\noutput probability of a specific class to a designated value. Due to the\nknowledge of original model, it enables the owner to generate such adversarial\nexamples. We propose a simple but effective adversarial attack method based on\nthe iterative Fast Gradient Sign Method (FGSM) by introducing control\nparameters. Experimental results confirm the effectiveness of the\nidentification of DNN models using adversarial attack."
    },
    {
        "date": "2025-05",
        "title": "Adaptively Secure Distributed Broadcast Encryption with Linear-Size Public Parameters",
        "author": "Kwangsu Lee",
        "link": "http://arxiv.org/abs/2505.17527v1",
        "abstract": "Distributed broadcast encryption (DBE) is a variant of broadcast encryption\n(BE) that can efficiently transmit a message to a subset of users, in which\nusers independently generate user private keys and user public keys instead of\na central trusted authority generating user keys. In this paper, we propose a\nDBE scheme with constant size ciphertexts, constant size private keys, and\nlinear size public parameters, and prove the adaptive security of our DBE\nscheme under static assumptions in composite-order bilinear groups. The\nprevious efficient DBE schemes with constant size ciphertexts and constant size\nprivate keys are proven secure under the $q$-Type assumption or have a drawback\nof having quadratic size public parameters. In contrast, our DBE scheme is the\nfirst DBE scheme with linear size public parameters proven adaptively secure\nunder static assumptions in composite-order bilinear groups."
    },
    {
        "date": "2025-05",
        "title": "Enhancing Adversarial Robustness of Vision Language Models via Adversarial Mixture Prompt Tuning",
        "author": "Shiji Zhao, Qihui Zhu, Shukun Xiong, Shouwei Ruan, Yize Fan, Ranjie Duan, Qing Guo, and Xingxing Wei",
        "link": "http://arxiv.org/abs/2505.17509v1",
        "abstract": "Large pre-trained Vision Language Models (VLMs) have excellent generalization\ncapabilities but are highly susceptible to adversarial examples, presenting\npotential security risks. To improve the robustness of VLMs against adversarial\nexamples, adversarial prompt tuning methods are proposed to align the text\nfeature with the adversarial image feature without changing model parameters.\nHowever, when facing various adversarial attacks, a single learnable text\nprompt has insufficient generalization to align well with all adversarial image\nfeatures, which finally leads to the overfitting phenomenon. To address the\nabove challenge, in this paper, we empirically find that increasing the number\nof learned prompts can bring more robustness improvement than a longer prompt.\nThen we propose an adversarial tuning method named Adversarial Mixture Prompt\nTuning (AMPT) to enhance the generalization towards various adversarial attacks\nfor VLMs. AMPT aims to learn mixture text prompts to obtain more robust text\nfeatures. To further enhance the adaptability, we propose a conditional weight\nrouter based on the input adversarial image to predict the mixture weights of\nmultiple learned prompts, which helps obtain sample-specific aggregated text\nfeatures aligning with different adversarial image features. A series of\nexperiments show that our method can achieve better adversarial robustness than\nstate-of-the-art methods on 11 datasets under different experimental settings."
    },
    {
        "date": "2025-05",
        "title": "Demonstration of Quantum-Secure Communications in a Nuclear Reactor",
        "author": "Konstantinos Gkouliaras, Vasileios Theos, True Miller, Brian Jowers, George Kennedy, Andy Grant, Terry Cronin, Philip G. Evans, and Stylianos Chatzidakis",
        "link": "http://arxiv.org/abs/2505.17502v1",
        "abstract": "Quantum key distribution (QKD), one of the latest cryptographic techniques,\nfounded on the laws of quantum mechanics rather than mathematical complexity,\npromises for the first time unconditional secure remote communications.\nIntegrating this technology into the next generation nuclear systems - designed\nfor universal data collection and real-time sharing as well as cutting-edge\ninstrumentation and increased dependency on digital technologies - could\nprovide significant benefits enabling secure, unattended, and autonomous\noperation in remote areas, e.g., microreactors and fission batteries. However,\nany practical implementation on a critical reactor system must meet strict\nrequirements on latency, control system compatibility, stability, and\nperformance under operational transients. Here, we report the complete\nend-to-end demonstration of a phase-encoding decoy-state BB84 protocol QKD\nsystem under prototypic conditions on Purdue's fully digital nuclear reactor,\nPUR-1. The system was installed in PUR-1 successfully executing real-time\nencryption and decryption of 2,000 signals over optic fiber distances up to 82\nkm using OTP-based encryption and up to 140 km with AES-based encryption. For a\ncore of 68 signals, OTP-secure communication was achieved for up to 130 km. The\nQKD system maintained a stable secret key rate of 320 kbps and a quantum bit\nerror of 3.8% at 54 km. Our results demonstrate that OTP-based encryption\nintroduces minimal latency while the more key-efficient AES and ASCON\nencryption schemes can significantly increase the number of signals encrypted\nwithout latency penalties. Additionally, implementation of a dynamic key pool\nensures several hours of secure key availability during potential system\ndowntimes. This work shows the potential of quantum-based secure remote\ncommunications for future digitally driven nuclear reactor technologies."
    },
    {
        "date": "2025-05",
        "title": "RoHyDR: Robust Hybrid Diffusion Recovery for Incomplete Multimodal Emotion Recognition",
        "author": "Yuehan Jin, Xiaoqing Liu, Yiyuan Yang, Zhiwen Yu, Tong Zhang, and Kaixiang Yang",
        "link": "http://arxiv.org/abs/2505.17501v1",
        "abstract": "Multimodal emotion recognition analyzes emotions by combining data from\nmultiple sources. However, real-world noise or sensor failures often cause\nmissing or corrupted data, creating the Incomplete Multimodal Emotion\nRecognition (IMER) challenge. In this paper, we propose Robust Hybrid Diffusion\nRecovery (RoHyDR), a novel framework that performs missing-modality recovery at\nunimodal, multimodal, feature, and semantic levels. For unimodal representation\nrecovery of missing modalities, RoHyDR exploits a diffusion-based generator to\ngenerate distribution-consistent and semantically aligned representations from\nGaussian noise, using available modalities as conditioning. For multimodal\nfusion recovery, we introduce adversarial learning to produce a realistic fused\nmultimodal representation and recover missing semantic content. We further\npropose a multi-stage optimization strategy that enhances training stability\nand efficiency. In contrast to previous work, the hybrid diffusion and\nadversarial learning-based recovery mechanism in RoHyDR allows recovery of\nmissing information in both unimodal representation and multimodal fusion, at\nboth feature and semantic levels, effectively mitigating performance\ndegradation caused by suboptimal optimization. Comprehensive experiments\nconducted on two widely used multimodal emotion recognition benchmarks\ndemonstrate that our proposed method outperforms state-of-the-art IMER methods,\nachieving robust recognition performance under various missing-modality\nscenarios. Our code will be made publicly available upon acceptance."
    },
    {
        "date": "2025-05",
        "title": "SecurePay: Enabling Secure and Fast Payment Processing for Platform Economy",
        "author": "Junru Lin, Mingzhe Liu, Songze Li, and Xuechao Wang",
        "link": "http://arxiv.org/abs/2505.17466v1",
        "abstract": "Recent years have witnessed a rapid development of platform economy, as it\neffectively addresses the trust dilemma between untrusted online buyers and\nmerchants. However, malicious platforms can misuse users' funds and\ninformation, causing severe security concerns. Previous research efforts aimed\nat enhancing security in platform payment systems often sacrificed processing\nperformance, while those focusing on processing efficiency struggled to\ncompletely prevent fund and information misuse. In this paper, we introduce\nSecurePay, a secure, yet performant payment processing system for platform\neconomy. SecurePay is the first payment system that combines permissioned\nblockchain with central bank digital currency (CBDC) to ensure fund security,\ninformation security, and resistance to collusion by intermediaries; it also\nfacilitates counter-party auditing, closed-loop regulation, and enhances\noperational efficiency for transaction settlement. We develop a full\nimplementation of the proposed SecurePay system, and our experiments conducted\non personal devices demonstrate a throughput of 256.4 transactions per second\nand an average latency of 4.29 seconds, demonstrating a comparable processing\nefficiency with a centralized system, with a significantly improved security\nlevel."
    },
    {
        "date": "2025-05",
        "title": "Reflectance Prediction-based Knowledge Distillation for Robust 3D Object Detection in Compressed Point Clouds",
        "author": "Hao Jing, Anhong Wang, Yifan Zhang, Donghan Bu, and Junhui Hou",
        "link": "http://arxiv.org/abs/2505.17442v1",
        "abstract": "Regarding intelligent transportation systems for vehicle networking,\nlow-bitrate transmission via lossy point cloud compression is vital for\nfacilitating real-time collaborative perception among vehicles with restricted\nbandwidth. In existing compression transmission systems, the sender lossily\ncompresses point coordinates and reflectance to generate a transmission code\nstream, which faces transmission burdens from reflectance encoding and limited\ndetection robustness due to information loss. To address these issues, this\npaper proposes a 3D object detection framework with reflectance\nprediction-based knowledge distillation (RPKD). We compress point coordinates\nwhile discarding reflectance during low-bitrate transmission, and feed the\ndecoded non-reflectance compressed point clouds into a student detector. The\ndiscarded reflectance is then reconstructed by a geometry-based reflectance\nprediction (RP) module within the student detector for precise detection. A\nteacher detector with the same structure as student detector is designed for\nperforming reflectance knowledge distillation (RKD) and detection knowledge\ndistillation (DKD) from raw to compressed point clouds. Our RPKD framework\njointly trains detectors on both raw and compressed point clouds to improve the\nstudent detector's robustness. Experimental results on the KITTI dataset and\nWaymo Open Dataset demonstrate that our method can boost detection accuracy for\ncompressed point clouds across multiple code rates. Notably, at a low code rate\nof 2.146 Bpp on the KITTI dataset, our RPKD-PV achieves the highest mAP of\n73.6, outperforming existing detection methods with the PV-RCNN baseline."
    },
    {
        "date": "2025-05",
        "title": "VEAttack: Downstream-agnostic Vision Encoder Attack against Large Vision Language Models",
        "author": "Hefei Mei, Zirui Wang, Shen You, Minjing Dong, and Chang Xu",
        "link": "http://arxiv.org/abs/2505.17440v1",
        "abstract": "Large Vision-Language Models (LVLMs) have demonstrated remarkable\ncapabilities in multimodal understanding and generation, yet their\nvulnerability to adversarial attacks raises significant robustness concerns.\nWhile existing effective attacks always focus on task-specific white-box\nsettings, these approaches are limited in the context of LVLMs, which are\ndesigned for diverse downstream tasks and require expensive full-model gradient\ncomputations. Motivated by the pivotal role and wide adoption of the vision\nencoder in LVLMs, we propose a simple yet effective Vision Encoder Attack\n(VEAttack), which targets the vision encoder of LVLMs only. Specifically, we\npropose to generate adversarial examples by minimizing the cosine similarity\nbetween the clean and perturbed visual features, without accessing the\nfollowing large language models, task information, and labels. It significantly\nreduces the computational overhead while eliminating the task and label\ndependence of traditional white-box attacks in LVLMs. To make this simple\nattack effective, we propose to perturb images by optimizing image tokens\ninstead of the classification token. We provide both empirical and theoretical\nevidence that VEAttack can easily generalize to various tasks. VEAttack has\nachieved a performance degradation of 94.5% on image caption task and 75.7% on\nvisual question answering task. We also reveal some key observations to provide\ninsights into LVLM attack/defense: 1) hidden layer variations of LLM, 2) token\nattention differential, 3) M\\\"obius band in transfer attack, 4) low sensitivity\nto attack steps. The code is available at\nhttps://github.com/hfmei/VEAttack-LVLM"
    },
    {
        "date": "2025-05",
        "title": "Misaligning Reasoning with Answers -- A Framework for Assessing LLM CoT Robustness",
        "author": "Enyi Jiang, Changming Xu, Nischay Singh, and Gagandeep Singh",
        "link": "http://arxiv.org/abs/2505.17406v1",
        "abstract": "LLMs' decision-making process is opaque, prompting the need for explanation\ntechniques like Chain-of-Thought. To investigate the relationship between\nanswer and reasoning, we design a novel evaluation framework, MATCHA. In\ndomains like education and healthcare, reasoning is key for model\ntrustworthiness. MATCHA reveals that LLMs under input perturbations can give\ninconsistent or nonsensical reasoning. Additionally, we use LLM judges to\nassess reasoning robustness across models. Our results show that LLMs exhibit\ngreater vulnerability to input perturbations for multi-step and commonsense\ntasks than compared to logical tasks. Also, we show non-trivial transfer rates\nof our successful examples to black-box models. Our evaluation framework helps\nto better understand LLM reasoning mechanisms and guides future models toward\nmore robust and reasoning-driven architectures, enforcing answer-reasoning\nconsistency."
    },
    {
        "date": "2025-05",
        "title": "Adversarial Robustness of Nonparametric Regression",
        "author": "Parsa Moradi, Hanzaleh Akabrinodehi, and Mohammad Ali Maddah-Ali",
        "link": "http://arxiv.org/abs/2505.17356v1",
        "abstract": "In this paper, we investigate the adversarial robustness of regression, a\nfundamental problem in machine learning, under the setting where an adversary\ncan arbitrarily corrupt a subset of the input data. While the robustness of\nparametric regression has been extensively studied, its nonparametric\ncounterpart remains largely unexplored. We characterize the adversarial\nrobustness in nonparametric regression, assuming the regression function\nbelongs to the second-order Sobolev space (i.e., it is square integrable up to\nits second derivative).\n  The contribution of this paper is two-fold: (i) we establish a minimax lower\nbound on the estimation error, revealing a fundamental limit that no estimator\ncan overcome, and (ii) we show that, perhaps surprisingly, the classical\nsmoothing spline estimator, when properly regularized, exhibits robustness\nagainst adversarial corruption. These results imply that if $o(n)$ out of $n$\nsamples are corrupted, the estimation error of the smoothing spline vanishes as\n$n \\to \\infty$. On the other hand, when a constant fraction of the data is\ncorrupted, no estimator can guarantee vanishing estimation error, implying the\noptimality of the smoothing spline in terms of maximum tolerable number of\ncorrupted samples."
    },
    {
        "date": "2025-05",
        "title": "Secure Parsing and Serializing with Separation Logic Applied to CBOR, CDDL, and COSE",
        "author": "Tahina Ramananandro, Gabriel Ebner, Guido Mart\u00ednez, and Nikhil Swamy",
        "link": "http://arxiv.org/abs/2505.17335v1",
        "abstract": "Incorrect handling of security-critical data formats, particularly in\nlow-level languages, are the root cause of many security vulnerabilities.\nProvably correct parsing and serialization tools that target languages like C\ncan help. Towards this end, we present PulseParse, a library of verified parser\nand serializer combinators for non-malleable binary formats. Specifications and\nproofs in PulseParse are in separation logic, offering a more abstract and\ncompositional interface, with full support for data validation, parsing, and\nserialization. PulseParse also supports a class of recursive formats -- with a\nfocus on security and handling adversarial inputs, we show how to parse such\nformats with only a constant amount of stack space.\n  We use PulseParse at scale by providing the first formalization of CBOR, a\nrecursive, binary data format standard, with growing adoption in various\nindustrial standards. We prove that the deterministic fragment of CBOR is\nnon-malleable and provide EverCBOR, a verified library in both C and Rust to\nvalidate, parse, and serialize CBOR objects implemented using PulseParse. Next,\nwe provide the first formalization of CDDL, a schema definition language for\nCBOR. We identify well-formedness conditions on CDDL definitions that ensure\nthat they yield unambiguous, non-malleable formats, and implement EverCDDL, a\ntool that checks that a CDDL definition is well-formed, and then produces\nverified parsers and serializers for it.\n  To evaluate our work, we use EverCDDL to generate verified parsers and\nserializers for various security-critical applications. Notably, we build a\nformally verified implementation of COSE signing, a standard for\ncryptographically signed objects. We also use our toolchain to generate\nverified code for other standards specified in CDDL, including DICE Protection\nEnvironment, a secure boot protocol standard."
    },
    {
        "date": "2025-05",
        "title": "Game-invariant Features Through Contrastive and Domain-adversarial Learning",
        "author": "Dylan Kline",
        "link": "http://arxiv.org/abs/2505.17328v1",
        "abstract": "Foundational game-image encoders often overfit to game-specific visual\nstyles, undermining performance on downstream tasks when applied to new games.\nWe present a method that combines contrastive learning and domain-adversarial\ntraining to learn game-invariant visual features. By simultaneously encouraging\nsimilar content to cluster and discouraging game-specific cues via an\nadversarial domain classifier, our approach produces embeddings that generalize\nacross diverse games. Experiments on the Bingsu game-image dataset (10,000\nscreenshots from 10 games) demonstrate that after only a few training epochs,\nour model's features no longer cluster by game, indicating successful\ninvariance and potential for improved cross-game transfer (e.g., glitch\ndetection) with minimal fine-tuning. This capability paves the way for more\ngeneralizable game vision models that require little to no retraining on new\ngames."
    },
    {
        "date": "2025-05",
        "title": "Advancing Security with Digital Twins: A Comprehensive Survey",
        "author": "Blessing Airehenbuwa, Touseef Hasan, Souvika Sarkar, and Ujjwal Guin",
        "link": "http://arxiv.org/abs/2505.17310v1",
        "abstract": "The proliferation of electronic devices has greatly transformed every aspect\nof human life, such as communication, healthcare, transportation, and energy.\nUnfortunately, the global electronics supply chain is vulnerable to various\nattacks, including piracy of intellectual properties, tampering,\ncounterfeiting, information leakage, side-channel, and fault injection attacks,\ndue to the complex nature of electronic products and vulnerabilities present in\nthem. Although numerous solutions have been proposed to address these threats,\nsignificant gaps remain, particularly in providing scalable and comprehensive\nprotection against emerging attacks. Digital twin, a dynamic virtual replica of\na physical system, has emerged as a promising solution to address these issues\nby providing backward traceability, end-to-end visibility, and continuous\nverification of component integrity and behavior. In this paper, we present a\ncomprehensive survey of the application of digital twins based on their\nfunctional role and application domains. We comprehensively present recent\ndigital twin-based security implementations, including their role in\ncyber-physical systems, Internet of Things, and cryptographic systems,\ndetection of counterfeit electronics, intrusion detection, fault injection, and\nside-channel leakage. To the best of our knowledge, it is the first study to\nconsolidate these security use cases into a unified reference. The paper also\nexplores the integration of large language models with digital twins for\nenhanced security and discusses current challenges, solutions, and future\nresearch directions."
    },
    {
        "date": "2025-05",
        "title": "Approach to Finding a Robust Deep Learning Model",
        "author": "Alexey Boldyrev, Fedor Ratnikov, and Andrey Shevelev",
        "link": "http://arxiv.org/abs/2505.17254v1",
        "abstract": "The rapid development of machine learning (ML) and artificial intelligence\n(AI) applications requires the training of large numbers of models. This\ngrowing demand highlights the importance of training models without human\nsupervision, while ensuring that their predictions are reliable. In response to\nthis need, we propose a novel approach for determining model robustness. This\napproach, supplemented with a proposed model selection algorithm designed as a\nmeta-algorithm, is versatile and applicable to any machine learning model,\nprovided that it is appropriate for the task at hand. This study demonstrates\nthe application of our approach to evaluate the robustness of deep learning\nmodels. To this end, we study small models composed of a few convolutional and\nfully connected layers, using common optimizers due to their ease of\ninterpretation and computational efficiency. Within this framework, we address\nthe influence of training sample size, model weight initialization, and\ninductive bias on the robustness of deep learning models."
    },
    {
        "date": "2025-05",
        "title": "Understanding the Security Landscape of Embedded Non-Volatile Memories: A Comprehensive Survey",
        "author": "Zakia Tamanna Tisha, and Ujjwal Guin",
        "link": "http://arxiv.org/abs/2505.17253v1",
        "abstract": "The modern semiconductor industry requires memory solutions that can keep\npace with the high-speed demands of high-performance computing. Embedded\nnon-volatile memories (eNVMs) address these requirements by offering faster\naccess to stored data at an improved computational throughput and efficiency.\nFurthermore, these technologies offer numerous appealing features, including\nlimited area-energy-runtime budget and data retention capabilities. Among\nthese, the data retention feature of eNVMs has garnered particular interest\nwithin the semiconductor community. Although this property allows eNVMs to\nretain data even in the absence of a continuous power supply, it also\nintroduces some vulnerabilities, prompting security concerns. These concerns\nhave sparked increased interest in examining the broader security implications\nassociated with eNVM technologies. This paper examines the security aspects of\neNVMs by discussing the reasons for vulnerabilities in specific memories from\nan architectural point of view. Additionally, this paper extensively reviews\neNVM-based security primitives, such as physically unclonable functions and\ntrue random number generators, as well as techniques like logic obfuscation.\nThe paper also explores a broad spectrum of security threats to eNVMs,\nincluding physical attacks such as side-channel attacks, fault injection, and\nprobing, as well as logical threats like information leakage,\ndenial-of-service, and thermal attacks. Finally, the paper presents a study of\npublication trends in the eNVM domain since the early 2000s, reflecting the\nrising momentum and research activity in this field."
    },
    {
        "date": "2025-05",
        "title": "Secure and Private Federated Learning: Achieving Adversarial Resilience through Robust Aggregation",
        "author": "Kun Yang, and Neena Imam",
        "link": "http://arxiv.org/abs/2505.17226v1",
        "abstract": "Federated Learning (FL) enables collaborative machine learning across\ndecentralized data sources without sharing raw data. It offers a promising\napproach to privacy-preserving AI. However, FL remains vulnerable to\nadversarial threats from malicious participants, referred to as Byzantine\nclients, who can send misleading updates to corrupt the global model.\nTraditional aggregation methods, such as simple averaging, are not robust to\nsuch attacks. More resilient approaches, like the Krum algorithm, require prior\nknowledge of the number of malicious clients, which is often unavailable in\nreal-world scenarios. To address these limitations, we propose Average-rKrum\n(ArKrum), a novel aggregation strategy designed to enhance both the resilience\nand privacy guarantees of FL systems. Building on our previous work (rKrum),\nArKrum introduces two key innovations. First, it includes a median-based\nfiltering mechanism that removes extreme outliers before estimating the number\nof adversarial clients. Second, it applies a multi-update averaging scheme to\nimprove stability and performance, particularly when client data distributions\nare not identical. We evaluate ArKrum on benchmark image and text datasets\nunder three widely studied Byzantine attack types. Results show that ArKrum\nconsistently achieves high accuracy and stability. It performs as well as or\nbetter than other robust aggregation methods. These findings demonstrate that\nArKrum is an effective and practical solution for secure FL systems in\nadversarial environments."
    },
    {
        "date": "2025-05",
        "title": "Dynamic Encryption-Based Cloud Security Model using Facial Image and Password-based Key Generation for Multimedia Data",
        "author": "Naima Sultana Ayesha, Mehrin Anannya, Md Biplob Hosen, and Rashed Mazumder",
        "link": "http://arxiv.org/abs/2505.17224v1",
        "abstract": "In this cloud-dependent era, various security techniques, such as encryption,\nsteganography, and hybrid approaches, have been utilized in cloud computing to\nenhance security, maintain enormous storage capacity, and provide ease of\naccess. However, the absence of data type-specific encryption and decryption\nprocedures renders multimedia data vulnerable. To address this issue, this\nstudy presents a dynamic encryption-based security architecture that adapts\nencryption methods to any file type, using keys generated from facial images\nand passwords. Four diverse datasets are created, each with a consistent size\nof 2GB, containing varying combinations of image, audio (MP3 and MPEG), video,\ntext, CSV, PPT, and PDF files, to implement the proposed methodology. AES is\nused to encrypt image data, AES-CTR is employed for audio or video data to meet\nreal-time streaming needs, and Blowfish is used for other types of data.\nPerformance analysis on all four datasets is conducted using AWS servers, where\nDATASET-1 demonstrates the best performance compared to the others."
    },
    {
        "date": "2025-05",
        "title": "Fixing Data That Hurts Performance: Cascading LLMs to Relabel Hard Negatives for Robust Information Retrieval",
        "author": "Nandan Thakur, Crystina Zhang, Xueguang Ma, and Jimmy Lin",
        "link": "http://arxiv.org/abs/2505.16967v1",
        "abstract": "Training robust retrieval and reranker models typically relies on large-scale\nretrieval datasets; for example, the BGE collection contains 1.6 million\nquery-passage pairs sourced from various data sources. However, we find that\ncertain datasets can negatively impact model effectiveness -- pruning 8 out of\n15 datasets from the BGE collection reduces the training set size by\n2.35$\\times$ and increases nDCG@10 on BEIR by 1.0 point. This motivates a\ndeeper examination of training data quality, with a particular focus on \"false\nnegatives\", where relevant passages are incorrectly labeled as irrelevant. We\npropose a simple, cost-effective approach using cascading LLM prompts to\nidentify and relabel hard negatives. Experimental results show that relabeling\nfalse negatives with true positives improves both E5 (base) and Qwen2.5-7B\nretrieval models by 0.7-1.4 nDCG@10 on BEIR and by 1.7-1.8 nDCG@10 on zero-shot\nAIR-Bench evaluation. Similar gains are observed for rerankers fine-tuned on\nthe relabeled data, such as Qwen2.5-3B on BEIR. The reliability of the\ncascading design is further supported by human annotation results, where we\nfind judgment by GPT-4o shows much higher agreement with humans than\nGPT-4o-mini."
    },
    {
        "date": "2025-05",
        "title": "MixAT: Combining Continuous and Discrete Adversarial Training for LLMs",
        "author": "Csaba D\u00e9k\u00e1ny, Stefan Balauca, Robin Staab, Dimitar I. Dimitrov, and Martin Vechev",
        "link": "http://arxiv.org/abs/2505.16947v1",
        "abstract": "Despite recent efforts in Large Language Models (LLMs) safety and alignment,\ncurrent adversarial attacks on frontier LLMs are still able to force harmful\ngenerations consistently. Although adversarial training has been widely studied\nand shown to significantly improve the robustness of traditional machine\nlearning models, its strengths and weaknesses in the context of LLMs are less\nunderstood. Specifically, while existing discrete adversarial attacks are\neffective at producing harmful content, training LLMs with concrete adversarial\nprompts is often computationally expensive, leading to reliance on continuous\nrelaxations. As these relaxations do not correspond to discrete input tokens,\nsuch latent training methods often leave models vulnerable to a diverse set of\ndiscrete attacks. In this work, we aim to bridge this gap by introducing MixAT,\na novel method that combines stronger discrete and faster continuous attacks\nduring training. We rigorously evaluate MixAT across a wide spectrum of\nstate-of-the-art attacks, proposing the At Least One Attack Success Rate\n(ALO-ASR) metric to capture the worst-case vulnerability of models. We show\nMixAT achieves substantially better robustness (ALO-ASR < 20%) compared to\nprior defenses (ALO-ASR > 50%), while maintaining a runtime comparable to\nmethods based on continuous relaxations. We further analyze MixAT in realistic\ndeployment settings, exploring how chat templates, quantization, low-rank\nadapters, and temperature affect both adversarial training and evaluation,\nrevealing additional blind spots in current methodologies. Our results\ndemonstrate that MixAT's discrete-continuous defense offers a principled and\nsuperior robustness-accuracy tradeoff with minimal computational overhead,\nhighlighting its promise for building safer LLMs. We provide our code and\nmodels at https://github.com/insait-institute/MixAT."
    },
    {
        "date": "2025-05",
        "title": "REOBench: Benchmarking Robustness of Earth Observation Foundation Models",
        "author": "Xiang Li, Yong Tao, Siyuan Zhang, Siwei Liu, Zhitong Xiong, Chunbo Luo, Lu Liu, Mykola Pechenizkiy, Xiao Xiang Zhu, and Tianjin Huang",
        "link": "http://arxiv.org/abs/2505.16793v1",
        "abstract": "Earth observation foundation models have shown strong generalization across\nmultiple Earth observation tasks, but their robustness under real-world\nperturbations remains underexplored. To bridge this gap, we introduce REOBench,\nthe first comprehensive benchmark for evaluating the robustness of Earth\nobservation foundation models across six tasks and twelve types of image\ncorruptions, including both appearance-based and geometric perturbations. To\nensure realistic and fine-grained evaluation, our benchmark focuses on\nhigh-resolution optical remote sensing images, which are widely used in\ncritical applications such as urban planning and disaster response. We conduct\na systematic evaluation of a broad range of models trained using masked image\nmodeling, contrastive learning, and vision-language pre-training paradigms. Our\nresults reveal that (1) existing Earth observation foundation models experience\nsignificant performance degradation when exposed to input corruptions. (2) The\nseverity of degradation varies across tasks, model architectures, backbone\nsizes, and types of corruption, with performance drop varying from less than 1%\nto over 20%. (3) Vision-language models show enhanced robustness, particularly\nin multimodal tasks. REOBench underscores the vulnerability of current Earth\nobservation foundation models to real-world corruptions and provides actionable\ninsights for developing more robust and reliable models."
    },
    {
        "date": "2025-05",
        "title": "CoTSRF: Utilize Chain of Thought as Stealthy and Robust Fingerprint of Large Language Models",
        "author": "Zhenzhen Ren, GuoBiao Li, Sheng Li, Zhenxing Qian, and Xinpeng Zhang",
        "link": "http://arxiv.org/abs/2505.16785v1",
        "abstract": "Despite providing superior performance, open-source large language models\n(LLMs) are vulnerable to abusive usage. To address this issue, recent works\npropose LLM fingerprinting methods to identify the specific source LLMs behind\nsuspect applications. However, these methods fail to provide stealthy and\nrobust fingerprint verification. In this paper, we propose a novel LLM\nfingerprinting scheme, namely CoTSRF, which utilizes the Chain of Thought (CoT)\nas the fingerprint of an LLM. CoTSRF first collects the responses from the\nsource LLM by querying it with crafted CoT queries. Then, it applies\ncontrastive learning to train a CoT extractor that extracts the CoT feature\n(i.e., fingerprint) from the responses. Finally, CoTSRF conducts fingerprint\nverification by comparing the Kullback-Leibler divergence between the CoT\nfeatures of the source and suspect LLMs against an empirical threshold. Various\nexperiments have been conducted to demonstrate the advantage of our proposed\nCoTSRF for fingerprinting LLMs, particularly in stealthy and robust fingerprint\nverification."
    },
    {
        "date": "2025-05",
        "title": "When Safety Detectors Aren't Enough: A Stealthy and Effective Jailbreak Attack on LLMs via Steganographic Techniques",
        "author": "Jianing Geng, Biao Yi, Zekun Fei, Tongxi Wu, Lihai Nie, and Zheli Liu",
        "link": "http://arxiv.org/abs/2505.16765v1",
        "abstract": "Jailbreak attacks pose a serious threat to large language models (LLMs) by\nbypassing built-in safety mechanisms and leading to harmful outputs. Studying\nthese attacks is crucial for identifying vulnerabilities and improving model\nsecurity. This paper presents a systematic survey of jailbreak methods from the\nnovel perspective of stealth. We find that existing attacks struggle to\nsimultaneously achieve toxic stealth (concealing toxic content) and linguistic\nstealth (maintaining linguistic naturalness). Motivated by this, we propose\nStegoAttack, a fully stealthy jailbreak attack that uses steganography to hide\nthe harmful query within benign, semantically coherent text. The attack then\nprompts the LLM to extract the hidden query and respond in an encrypted manner.\nThis approach effectively hides malicious intent while preserving naturalness,\nallowing it to evade both built-in and external safety mechanisms. We evaluate\nStegoAttack on four safety-aligned LLMs from major providers, benchmarking\nagainst eight state-of-the-art methods. StegoAttack achieves an average attack\nsuccess rate (ASR) of 92.00%, outperforming the strongest baseline by 11.0%.\nIts ASR drops by less than 1% even under external detection (e.g., Llama\nGuard). Moreover, it attains the optimal comprehensive scores on stealth\ndetection metrics, demonstrating both high efficacy and exceptional stealth\ncapabilities. The code is available at\nhttps://anonymous.4open.science/r/StegoAttack-Jail66"
    },
    {
        "date": "2025-05",
        "title": "Robust Vision-Based Runway Detection through Conformal Prediction and Conformal mAP",
        "author": "Alya Zouzou, L\u00e9o and\u00e9ol, M\u00e9lanie Ducoffe, and Ryma Boumazouza",
        "link": "http://arxiv.org/abs/2505.16740v1",
        "abstract": "We explore the use of conformal prediction to provide statistical uncertainty\nguarantees for runway detection in vision-based landing systems (VLS). Using\nfine-tuned YOLOv5 and YOLOv6 models on aerial imagery, we apply conformal\nprediction to quantify localization reliability under user-defined risk levels.\nWe also introduce Conformal mean Average Precision (C-mAP), a novel metric\naligning object detection performance with conformal guarantees. Our results\nshow that conformal prediction can improve the reliability of runway detection\nby quantifying uncertainty in a statistically sound way, increasing safety\non-board and paving the way for certification of ML system in the aerospace\ndomain."
    },
    {
        "date": "2025-05",
        "title": "Harry Potter is Still Here! Probing Knowledge Leakage in Targeted Unlearned Large Language Models via Automated Adversarial Prompting",
        "author": "Bang Trinh Tran To, and Thai Le",
        "link": "http://arxiv.org/abs/2505.17160v1",
        "abstract": "This work presents LURK (Latent UnleaRned Knowledge), a novel framework that\nprobes for hidden retained knowledge in unlearned LLMs through adversarial\nsuffix prompting. LURK automatically generates adversarial prompt suffixes\ndesigned to elicit residual knowledge about the Harry Potter domain, a commonly\nused benchmark for unlearning. Our experiments reveal that even models deemed\nsuccessfully unlearned can leak idiosyncratic information under targeted\nadversarial conditions, highlighting critical limitations of current unlearning\nevaluation standards. By uncovering latent knowledge through indirect probing,\nLURK offers a more rigorous and diagnostic tool for assessing the robustness of\nunlearning algorithms. All code will be publicly available."
    },
    {
        "date": "2025-05",
        "title": "Adversarial Deep Metric Learning for Cross-Modal Audio-Text Alignment in Open-Vocabulary Keyword Spotting",
        "author": "Youngmoon Jung, Yong-Hyeok Lee, Myunghun Jung, Jaeyoung Roh, Chang Woo Han, and Hoon-Young Cho",
        "link": "http://arxiv.org/abs/2505.16735v2",
        "abstract": "For text enrollment-based open-vocabulary keyword spotting (KWS), acoustic\nand text embeddings are typically compared at either the phoneme or utterance\nlevel. To facilitate this, we optimize acoustic and text encoders using deep\nmetric learning (DML), enabling direct comparison of multi-modal embeddings in\na shared embedding space. However, the inherent heterogeneity between audio and\ntext modalities presents a significant challenge. To address this, we propose\nModality Adversarial Learning (MAL), which reduces the domain gap in\nheterogeneous modality representations. Specifically, we train a modality\nclassifier adversarially to encourage both encoders to generate\nmodality-invariant embeddings. Additionally, we apply DML to achieve\nphoneme-level alignment between audio and text, and conduct extensive\ncomparisons across various DML objectives. Experiments on the Wall Street\nJournal (WSJ) and LibriPhrase datasets demonstrate the effectiveness of the\nproposed approach."
    },
    {
        "date": "2025-05",
        "title": "Robust LLM Fingerprinting via Domain-Specific Watermarks",
        "author": "Thibaud Gloaguen, Robin Staab, Nikola Jovanovi\u0107, and Martin Vechev",
        "link": "http://arxiv.org/abs/2505.16723v1",
        "abstract": "As open-source language models (OSMs) grow more capable and are widely shared\nand finetuned, ensuring model provenance, i.e., identifying the origin of a\ngiven model instance, has become an increasingly important issue. At the same\ntime, existing backdoor-based model fingerprinting techniques often fall short\nof achieving key requirements of real-world model ownership detection. In this\nwork, we build on the observation that while current open-source model\nwatermarks fail to achieve reliable content traceability, they can be\neffectively adapted to address the challenge of model provenance. To this end,\nwe introduce the concept of domain-specific watermarking for model\nfingerprinting. Rather than watermarking all generated content, we train the\nmodel to embed watermarks only within specified subdomains (e.g., particular\nlanguages or topics). This targeted approach ensures detection reliability,\nwhile improving watermark durability and quality under a range of real-world\ndeployment settings. Our evaluations show that domain-specific watermarking\nenables model fingerprinting with strong statistical guarantees, controllable\nfalse positive rates, high detection power, and preserved generation quality.\nMoreover, we find that our fingerprints are inherently stealthy and naturally\nrobust to real-world variability across deployment scenarios."
    },
    {
        "date": "2025-05",
        "title": "Experimental robustness benchmark of quantum neural network on a superconducting quantum processor",
        "author": "Hai-Feng Zhang, Zhao-Yun Chen, Peng Wang, Liang-Liang Guo, Tian-Le Wang, Xiao-Yan Yang, Ren-Ze Zhao, Ze-An Zhao, Sheng Zhang, Lei Du, Hao-Ran Tao, Zhi-Long Jia, Wei-Cheng Kong, Huan-Yu Liu, Athanasios V. Vasilakos, Yang Yang, Yu-Chun Wu, Ji Guan, Peng Duan, and Guo-Ping Guo",
        "link": "http://arxiv.org/abs/2505.16714v1",
        "abstract": "Quantum machine learning (QML) models, like their classical counterparts, are\nvulnerable to adversarial attacks, hindering their secure deployment. Here, we\nreport the first systematic experimental robustness benchmark for 20-qubit\nquantum neural network (QNN) classifiers executed on a superconducting\nprocessor. Our benchmarking framework features an efficient adversarial attack\nalgorithm designed for QNNs, enabling quantitative characterization of\nadversarial robustness and robustness bounds. From our analysis, we verify that\nadversarial training reduces sensitivity to targeted perturbations by\nregularizing input gradients, significantly enhancing QNN's robustness.\nAdditionally, our analysis reveals that QNNs exhibit superior adversarial\nrobustness compared to classical neural networks, an advantage attributed to\ninherent quantum noise. Furthermore, the empirical upper bound extracted from\nour attack experiments shows a minimal deviation ($3 \\times 10^{-3}$) from the\ntheoretical lower bound, providing strong experimental confirmation of the\nattack's effectiveness and the tightness of fidelity-based robustness bounds.\nThis work establishes a critical experimental framework for assessing and\nimproving quantum adversarial robustness, paving the way for secure and\nreliable QML applications."
    },
    {
        "date": "2025-05",
        "title": "BitHydra: Towards Bit-flip Inference Cost Attack against Large Language Models",
        "author": "Xiaobei Yan, Yiming Li, Zhaoxin Fan, Han Qiu, and Tianwei Zhang",
        "link": "http://arxiv.org/abs/2505.16670v1",
        "abstract": "Large language models (LLMs) have shown impressive capabilities across a wide\nrange of applications, but their ever-increasing size and resource demands make\nthem vulnerable to inference cost attacks, where attackers induce victim LLMs\nto generate the longest possible output content. In this paper, we revisit\nexisting inference cost attacks and reveal that these methods can hardly\nproduce large-scale malicious effects since they are self-targeting, where\nattackers are also the users and therefore have to execute attacks solely\nthrough the inputs, whose generated content will be charged by LLMs and can\nonly directly influence themselves. Motivated by these findings, this paper\nintroduces a new type of inference cost attacks (dubbed 'bit-flip inference\ncost attack') that target the victim model itself rather than its inputs.\nSpecifically, we design a simple yet effective method (dubbed 'BitHydra') to\neffectively flip critical bits of model parameters. This process is guided by a\nloss function designed to suppress <EOS> token's probability with an efficient\ncritical bit search algorithm, thus explicitly defining the attack objective\nand enabling effective optimization. We evaluate our method on 11 LLMs ranging\nfrom 1.5B to 14B parameters under both int8 and float16 settings. Experimental\nresults demonstrate that with just 4 search samples and as few as 3 bit flips,\nBitHydra can force 100% of test prompts to reach the maximum generation length\n(e.g., 2048 tokens) on representative LLMs such as LLaMA3, highlighting its\nefficiency, scalability, and strong transferability across unseen inputs."
    },
    {
        "date": "2025-05",
        "title": "From Evaluation to Defense: Advancing Safety in Video Large Language Models",
        "author": "Yiwei Sun, Peiqi Jiang, Chuanbin Liu, Luohao Lin, Zhiying Lu, and Hongtao Xie",
        "link": "http://arxiv.org/abs/2505.16643v1",
        "abstract": "While the safety risks of image-based large language models have been\nextensively studied, their video-based counterparts (Video LLMs) remain\ncritically under-examined. To systematically study this problem, we introduce\n\\textbf{VideoSafetyBench (VSB-77k) - the first large-scale, culturally diverse\nbenchmark for Video LLM safety}, which compromises 77,646 video-query pairs and\nspans 19 principal risk categories across 10 language communities. \\textit{We\nreveal that integrating video modality degrades safety performance by an\naverage of 42.3\\%, exposing systemic risks in multimodal attack exploitation.}\nTo address this vulnerability, we propose \\textbf{VideoSafety-R1}, a dual-stage\nframework achieving unprecedented safety gains through two innovations: (1)\nAlarm Token-Guided Safety Fine-Tuning (AT-SFT) injects learnable alarm tokens\ninto visual and textual sequences, enabling explicit harm perception across\nmodalities via multitask objectives. (2) Then, Safety-Guided GRPO enhances\ndefensive reasoning through dynamic policy optimization with rule-based rewards\nderived from dual-modality verification. These components synergize to shift\nsafety alignment from passive harm recognition to active reasoning. The\nresulting framework achieves a 65.1\\% improvement on VSB-Eval-HH, and improves\nby 59.1\\%, 44.3\\%, and 15.0\\% on the image safety datasets MMBench, VLGuard,\nand FigStep, respectively. \\textit{Our codes are available in the supplementary\nmaterials.} \\textcolor{red}{Warning: This paper contains examples of harmful\nlanguage and videos, and reader discretion is recommended.}"
    },
    {
        "date": "2025-05",
        "title": "BadVLA: Towards Backdoor Attacks on Vision-Language-Action Models via Objective-Decoupled Optimization",
        "author": "Xueyang Zhou, Guiyao Tie, Guowen Zhang, Hechang Wang, Pan Zhou, and Lichao Sun",
        "link": "http://arxiv.org/abs/2505.16640v1",
        "abstract": "Vision-Language-Action (VLA) models have advanced robotic control by enabling\nend-to-end decision-making directly from multimodal inputs. However, their\ntightly coupled architectures expose novel security vulnerabilities. Unlike\ntraditional adversarial perturbations, backdoor attacks represent a stealthier,\npersistent, and practically significant threat-particularly under the emerging\nTraining-as-a-Service paradigm-but remain largely unexplored in the context of\nVLA models. To address this gap, we propose BadVLA, a backdoor attack method\nbased on Objective-Decoupled Optimization, which for the first time exposes the\nbackdoor vulnerabilities of VLA models. Specifically, it consists of a\ntwo-stage process: (1) explicit feature-space separation to isolate trigger\nrepresentations from benign inputs, and (2) conditional control deviations that\nactivate only in the presence of the trigger, while preserving clean-task\nperformance. Empirical results on multiple VLA benchmarks demonstrate that\nBadVLA consistently achieves near-100% attack success rates with minimal impact\non clean task accuracy. Further analyses confirm its robustness against common\ninput perturbations, task transfers, and model fine-tuning, underscoring\ncritical security vulnerabilities in current VLA deployments. Our work offers\nthe first systematic investigation of backdoor vulnerabilities in VLA models,\nhighlighting an urgent need for secure and trustworthy embodied model design\npractices. We have released the project page at\nhttps://badvla-project.github.io/."
    },
    {
        "date": "2025-05",
        "title": "CodeMerge: Codebook-Guided Model Merging for Robust Test-Time Adaptation in Autonomous Driving",
        "author": "Huitong Yang, Zhuoxiao Chen, Fengyi Zhang, Zi Huang, and Yadan Luo",
        "link": "http://arxiv.org/abs/2505.16524v1",
        "abstract": "Maintaining robust 3D perception under dynamic and unpredictable test-time\nconditions remains a critical challenge for autonomous driving systems.\nExisting test-time adaptation (TTA) methods often fail in high-variance tasks\nlike 3D object detection due to unstable optimization and sharp minima. While\nrecent model merging strategies based on linear mode connectivity (LMC) offer\nimproved stability by interpolating between fine-tuned checkpoints, they are\ncomputationally expensive, requiring repeated checkpoint access and multiple\nforward passes. In this paper, we introduce CodeMerge, a lightweight and\nscalable model merging framework that bypasses these limitations by operating\nin a compact latent space. Instead of loading full models, CodeMerge represents\neach checkpoint with a low-dimensional fingerprint derived from the source\nmodel's penultimate features and constructs a key-value codebook. We compute\nmerging coefficients using ridge leverage scores on these fingerprints,\nenabling efficient model composition without compromising adaptation quality.\nOur method achieves strong performance across challenging benchmarks, improving\nend-to-end 3D detection 14.9% NDS on nuScenes-C and LiDAR-based detection by\nover 7.6% mAP on nuScenes-to-KITTI, while benefiting downstream tasks such as\nonline mapping, motion prediction and planning even without training. Code and\npretrained models are released in the supplementary material."
    },
    {
        "date": "2025-05",
        "title": "Language-based Security and Time-inserting Supervisor",
        "author": "Damas P. Gruska",
        "link": "http://arxiv.org/abs/2505.16503v1",
        "abstract": "Algebraic methods are employed in order to define language-based security\nproperties of processes. A supervisor is introduced that can disable unwanted\nbehavior of an insecure process by controlling some of its actions or by\ninserting timed actions to make an insecure process secure. We assume a\nsituation where neither the supervisor nor the attacker has complete\ninformation about the ongoing systems behavior. We study the conditions under\nwhich such a supervisor exists, as well as its properties and limitations."
    },
    {
        "date": "2025-05",
        "title": "Implicit Jailbreak Attacks via Cross-Modal Information Concealment on Vision-Language Models",
        "author": "Zhaoxin Wang, Handing Wang, Cong Tian, and Yaochu Jin",
        "link": "http://arxiv.org/abs/2505.16446v1",
        "abstract": "Multimodal large language models (MLLMs) enable powerful cross-modal\nreasoning capabilities. However, the expanded input space introduces new attack\nsurfaces. Previous jailbreak attacks often inject malicious instructions from\ntext into less aligned modalities, such as vision. As MLLMs increasingly\nincorporate cross-modal consistency and alignment mechanisms, such explicit\nattacks become easier to detect and block. In this work, we propose a novel\nimplicit jailbreak framework termed IJA that stealthily embeds malicious\ninstructions into images via least significant bit steganography and couples\nthem with seemingly benign, image-related textual prompts. To further enhance\nattack effectiveness across diverse MLLMs, we incorporate adversarial suffixes\ngenerated by a surrogate model and introduce a template optimization module\nthat iteratively refines both the prompt and embedding based on model feedback.\nOn commercial models like GPT-4o and Gemini-1.5 Pro, our method achieves attack\nsuccess rates of over 90% using an average of only 3 queries."
    },
    {
        "date": "2025-05",
        "title": "Performance Guaranteed Poisoning Attacks in Federated Learning: A Sliding Mode Approach",
        "author": "Huazi Pan, Yanjun Zhang, Leo Yu Zhang, Scott Adams, Abbas Kouzani, and Suiyang Khoo",
        "link": "http://arxiv.org/abs/2505.16403v1",
        "abstract": "Manipulation of local training data and local updates, i.e., the poisoning\nattack, is the main threat arising from the collaborative nature of the\nfederated learning (FL) paradigm. Most existing poisoning attacks aim to\nmanipulate local data/models in a way that causes denial-of-service (DoS)\nissues. In this paper, we introduce a novel attack method, named Federated\nLearning Sliding Attack (FedSA) scheme, aiming at precisely introducing the\nextent of poisoning in a subtle controlled manner. It operates with a\npredefined objective, such as reducing global model's prediction accuracy by\n10\\%. FedSA integrates robust nonlinear control-Sliding Mode Control (SMC)\ntheory with model poisoning attacks. It can manipulate the updates from\nmalicious clients to drive the global model towards a compromised state,\nachieving this at a controlled and inconspicuous rate. Additionally, leveraging\nthe robust control properties of FedSA allows precise control over the\nconvergence bounds, enabling the attacker to set the global accuracy of the\npoisoned model to any desired level. Experimental results demonstrate that\nFedSA can accurately achieve a predefined global accuracy with fewer malicious\nclients while maintaining a high level of stealth and adjustable learning\nrates."
    },
    {
        "date": "2025-05",
        "title": "AdvReal: Adversarial Patch Generation Framework with Application to Adversarial Safety Evaluation of Object Detection Systems",
        "author": "Yuanhao Huang, Yilong Ren, Jinlei Wang, Lujia Huo, Xuesong Bai, Jinchuan Zhang, and Haiyan Yu",
        "link": "http://arxiv.org/abs/2505.16402v1",
        "abstract": "Autonomous vehicles are typical complex intelligent systems with artificial\nintelligence at their core. However, perception methods based on deep learning\nare extremely vulnerable to adversarial samples, resulting in safety accidents.\nHow to generate effective adversarial examples in the physical world and\nevaluate object detection systems is a huge challenge. In this study, we\npropose a unified joint adversarial training framework for both 2D and 3D\nsamples to address the challenges of intra-class diversity and environmental\nvariations in real-world scenarios. Building upon this framework, we introduce\nan adversarial sample reality enhancement approach that incorporates non-rigid\nsurface modeling and a realistic 3D matching mechanism. We compare with 5\nadvanced adversarial patches and evaluate their attack performance on 8 object\ndetecotrs, including single-stage, two-stage, and transformer-based models.\nExtensive experiment results in digital and physical environments demonstrate\nthat the adversarial textures generated by our method can effectively mislead\nthe target detection model. Moreover, proposed method demonstrates excellent\nrobustness and transferability under multi-angle attacks, varying lighting\nconditions, and different distance in the physical world. The demo video and\ncode can be obtained at https://github.com/Huangyh98/AdvReal.git."
    },
    {
        "date": "2025-05",
        "title": "Consistent and Compatible Modelling of Cyber Intrusions and Incident Response Demonstrated in the Context of Malware Attacks on Critical Infrastructure",
        "author": "Peter Maynard, Yulia Cherdantseva, Avi Shaked, Pete Burnap, and Arif Mehmood",
        "link": "http://arxiv.org/abs/2505.16398v1",
        "abstract": "Cyber Security Incident Response (IR) Playbooks are used to capture the steps\nrequired to recover from a cyber intrusion. Individual IR playbooks should\nfocus on a specific type of incident and be aligned with the architecture of a\nsystem under attack. Intrusion modelling focuses on a specific potential cyber\nintrusion and is used to identify where and what countermeasures are needed,\nand the resulting intrusion models are expected to be used in effective IR,\nideally by feeding IR Playbooks designs. IR playbooks and intrusion models,\nhowever, are created in isolation and at varying stages of the system's\nlifecycle. We take nine critical national infrastructure intrusion models -\nexpressed using Sequential AND Attack Trees - and transform them into models of\nthe same format as IR playbooks. We use Security Modelling Framework for\nmodelling attacks and playbooks, and for demonstrating the feasibility of the\nbetter integration between risk assessment and IR at the modelling level. This\nresults in improved intrusion models and tighter coupling between IR playbooks\nand threat modelling which - as we demonstrate - yields novel insights into the\nanalysis of attacks and response actions. The main contributions of this paper\nare (a) a novel way of representing attack trees using the Security Modelling\nFramework,(b) a new tool for converting Sequential AND attack trees into models\ncompatible with playbooks, and (c) the examples of nine intrusion models\nrepresented using the Security Modelling Framework."
    },
    {
        "date": "2025-05",
        "title": "Efficient Motion Prompt Learning for Robust Visual Tracking",
        "author": "Jie Zhao, Xin Chen, Yongsheng Yuan, Michael Felsberg, Dong Wang, and Huchuan Lu",
        "link": "http://arxiv.org/abs/2505.16321v1",
        "abstract": "Due to the challenges of processing temporal information, most trackers\ndepend solely on visual discriminability and overlook the unique temporal\ncoherence of video data. In this paper, we propose a lightweight and\nplug-and-play motion prompt tracking method. It can be easily integrated into\nexisting vision-based trackers to build a joint tracking framework leveraging\nboth motion and vision cues, thereby achieving robust tracking through\nefficient prompt learning. A motion encoder with three different positional\nencodings is proposed to encode the long-term motion trajectory into the visual\nembedding space, while a fusion decoder and an adaptive weight mechanism are\ndesigned to dynamically fuse visual and motion features. We integrate our\nmotion module into three different trackers with five models in total.\nExperiments on seven challenging tracking benchmarks demonstrate that the\nproposed motion module significantly improves the robustness of vision-based\ntrackers, with minimal training costs and negligible speed sacrifice. Code is\navailable at https://github.com/zj5559/Motion-Prompt-Tracking."
    },
    {
        "date": "2025-05",
        "title": "SuperPure: Efficient Purification of Localized and Distributed Adversarial Patches via Super-Resolution GAN Models",
        "author": "Hossein Khalili, Seongbin Park, Venkat Bollapragada, and Nader Sehatbakhsh",
        "link": "http://arxiv.org/abs/2505.16318v1",
        "abstract": "As vision-based machine learning models are increasingly integrated into\nautonomous and cyber-physical systems, concerns about (physical) adversarial\npatch attacks are growing. While state-of-the-art defenses can achieve\ncertified robustness with minimal impact on utility against highly-concentrated\nlocalized patch attacks, they fall short in two important areas: (i)\nState-of-the-art methods are vulnerable to low-noise distributed patches where\nperturbations are subtly dispersed to evade detection or masking, as shown\nrecently by the DorPatch attack; (ii) Achieving high robustness with\nstate-of-the-art methods is extremely time and resource-consuming, rendering\nthem impractical for latency-sensitive applications in many cyber-physical\nsystems.\n  To address both robustness and latency issues, this paper proposes a new\ndefense strategy for adversarial patch attacks called SuperPure. The key\nnovelty is developing a pixel-wise masking scheme that is robust against both\ndistributed and localized patches. The masking involves leveraging a GAN-based\nsuper-resolution scheme to gradually purify the image from adversarial patches.\nOur extensive evaluations using ImageNet and two standard classifiers, ResNet\nand EfficientNet, show that SuperPure advances the state-of-the-art in three\nmajor directions: (i) it improves the robustness against conventional localized\npatches by more than 20%, on average, while also improving top-1 clean accuracy\nby almost 10%; (ii) It achieves 58% robustness against distributed patch\nattacks (as opposed to 0% in state-of-the-art method, PatchCleanser); (iii) It\ndecreases the defense end-to-end latency by over 98% compared to PatchCleanser.\nOur further analysis shows that SuperPure is robust against white-box attacks\nand different patch sizes. Our code is open-source."
    },
    {
        "date": "2025-05",
        "title": "Accelerating Targeted Hard-Label Adversarial Attacks in Low-Query Black-Box Settings",
        "author": "Arjhun Swaminathan, and Mete Akg\u00fcn",
        "link": "http://arxiv.org/abs/2505.16313v1",
        "abstract": "Deep neural networks for image classification remain vulnerable to\nadversarial examples -- small, imperceptible perturbations that induce\nmisclassifications. In black-box settings, where only the final prediction is\naccessible, crafting targeted attacks that aim to misclassify into a specific\ntarget class is particularly challenging due to narrow decision regions.\nCurrent state-of-the-art methods often exploit the geometric properties of the\ndecision boundary separating a source image and a target image rather than\nincorporating information from the images themselves. In contrast, we propose\nTargeted Edge-informed Attack (TEA), a novel attack that utilizes edge\ninformation from the target image to carefully perturb it, thereby producing an\nadversarial image that is closer to the source image while still achieving the\ndesired target classification. Our approach consistently outperforms current\nstate-of-the-art methods across different models in low query settings (nearly\n70\\% fewer queries are used), a scenario especially relevant in real-world\napplications with limited queries and black-box access. Furthermore, by\nefficiently generating a suitable adversarial example, TEA provides an improved\ntarget initialization for established geometry-based attacks."
    },
    {
        "date": "2025-05",
        "title": "Paired and Unpaired Image to Image Translation using Generative Adversarial Networks",
        "author": "Gaurav Kumar, Soham Satyadharma, and Harpreet Singh",
        "link": "http://arxiv.org/abs/2505.16310v1",
        "abstract": "Image to image translation is an active area of research in the field of\ncomputer vision, enabling the generation of new images with different styles,\ntextures, or resolutions while preserving their characteristic properties.\nRecent architectures leverage Generative Adversarial Networks (GANs) to\ntransform input images from one domain to another. In this work, we focus on\nthe study of both paired and unpaired image translation across multiple image\ndomains. For the paired task, we used a conditional GAN model, and for the\nunpaired task, we trained it using cycle consistency loss. We experimented with\ndifferent types of loss functions, multiple Patch-GAN sizes, and model\narchitectures. New quantitative metrics - precision, recall, and FID score -\nwere used for analysis. In addition, a qualitative study of the results of\ndifferent experiments was conducted."
    },
    {
        "date": "2025-05",
        "title": "Poster: Towards an Automated Security Testing Framework for Industrial UEs",
        "author": "Sotiris Michaelides, Daniel Eguiguren Chavez, and Martin Henze",
        "link": "http://arxiv.org/abs/2505.16300v1",
        "abstract": "With the ongoing adoption of 5G for communication in industrial systems and\ncritical infrastructure, the security of industrial UEs such as 5G-enabled\nindustrial robots becomes an increasingly important topic. Most notably, to\nmeet the stringent security requirements of industrial deployments, industrial\nUEs not only have to fully comply with the 5G specifications but also implement\nand use correctly secure communication protocols such as TLS. To ensure the\nsecurity of industrial UEs, operators of industrial 5G networks rely on\nsecurity testing before deploying new devices to their production networks.\nHowever, currently only isolated tests for individual security aspects of\nindustrial UEs exist, severely hindering comprehensive testing. In this paper,\nwe report on our ongoing efforts to alleviate this situation by creating an\nautomated security testing framework for industrial UEs to comprehensively\nevaluate their security posture before deployment. With this framework, we aim\nto provide stakeholders with a fully automated-method to verify that\nhigher-layer security protocols are correctly implemented, while simultaneously\nensuring that the UE's protocol stack adheres to 3GPP specifications."
    },
    {
        "date": "2025-05",
        "title": "Swin Transformer for Robust CGI Images Detection: Intra- and Inter-Dataset Analysis across Multiple Color Spaces",
        "author": "Preeti Mehta, Aman Sagar, and Suchi Kumari",
        "link": "http://arxiv.org/abs/2505.16253v1",
        "abstract": "This study aims to address the growing challenge of distinguishing\ncomputer-generated imagery (CGI) from authentic digital images across three\ndifferent color spaces; RGB, YCbCr, and HSV. Given the limitations of existing\nclassification methods in handling the complexity and variability of CGI, this\nresearch proposes a Swin Transformer based model for accurate differentiation\nbetween natural and synthetic images. The proposed model leverages the Swin\nTransformer's hierarchical architecture to capture local and global features\nfor distinguishing CGI from natural images. Its performance was assessed\nthrough intra- and inter-dataset testing across three datasets: CiFAKE, JSSSTU,\nand Columbia. The model was evaluated individually on each dataset (D1, D2, D3)\nand on the combined datasets (D1+D2+D3) to test its robustness and domain\ngeneralization. To address dataset imbalance, data augmentation techniques were\napplied. Additionally, t-SNE visualization was used to demonstrate the feature\nseparability achieved by the Swin Transformer across the selected color spaces.\nThe model's performance was tested across all color schemes, with the RGB color\nscheme yielding the highest accuracy for each dataset. As a result, RGB was\nselected for domain generalization analysis and compared with other CNN-based\nmodels, VGG-19 and ResNet-50. The comparative results demonstrate the proposed\nmodel's effectiveness in detecting CGI, highlighting its robustness and\nreliability in both intra-dataset and inter-dataset evaluations. The findings\nof this study highlight the Swin Transformer model's potential as an advanced\ntool for digital image forensics, particularly in distinguishing CGI from\nnatural images. The model's strong performance indicates its capability for\ndomain generalization, making it a valuable asset in scenarios requiring\nprecise and reliable image classification."
    },
    {
        "date": "2025-05",
        "title": "VIVID: A Novel Approach to Remediation Prioritization in Static Application Security Testing (SAST)",
        "author": "Naeem Budhwani, Mohammad Faghani, and Hayden Richard",
        "link": "http://arxiv.org/abs/2505.16205v1",
        "abstract": "Static Application Security Testing (SAST) enables organizations to detect\nvulnerabilities in code early; however, major SAST platforms do not include\nvisual aids and present little insight on correlations between tainted data\nchains. We propose VIVID - Vulnerability Information Via Data flow - a novel\nmethod to extract and consume SAST insights, which is to graph the\napplication's vulnerability data flows (VDFs) and carry out graph theory\nanalysis on the resulting VDF directed graph. Nine metrics were assessed to\nevaluate their effectiveness in analyzing the VDF graphs of deliberately\ninsecure web applications. These metrics include 3 centrality metrics, 2\nstructural metrics, PageRank, in-degree, out-degree, and cross-clique\nconnectivity. We present simulations that find that out-degree, betweenness\ncentrality, in-eigenvector centrality, and cross-clique connectivity were found\nto be associated with files exhibiting high vulnerability traffic, making them\nrefactoring candidates where input sanitization may have been missed.\nMeanwhile, out-eigenvector centrality, PageRank, and in-degree were found to be\nassociated with nodes enabling vulnerability flow and sinks, but not\nnecessarily where input validation should be placed. This is a novel method to\nautomatically provide development teams an evidence-based prioritized list of\nfiles to embed security controls into, informed by vulnerability propagation\npatterns in the application architecture."
    },
    {
        "date": "2025-05",
        "title": "SEM: Enhancing Spatial Understanding for Robust Robot Manipulation",
        "author": "Xuewu Lin, Tianwei Lin, Lichao Huang, Hongyu Xie, Yiwei Jin, Keyu Li, and Zhizhong Su",
        "link": "http://arxiv.org/abs/2505.16196v1",
        "abstract": "A key challenge in robot manipulation lies in developing policy models with\nstrong spatial understanding, the ability to reason about 3D geometry, object\nrelations, and robot embodiment. Existing methods often fall short: 3D point\ncloud models lack semantic abstraction, while 2D image encoders struggle with\nspatial reasoning. To address this, we propose SEM (Spatial Enhanced\nManipulation model), a novel diffusion-based policy framework that explicitly\nenhances spatial understanding from two complementary perspectives. A spatial\nenhancer augments visual representations with 3D geometric context, while a\nrobot state encoder captures embodiment-aware structure through graphbased\nmodeling of joint dependencies. By integrating these modules, SEM significantly\nimproves spatial understanding, leading to robust and generalizable\nmanipulation across diverse tasks that outperform existing baselines."
    },
    {
        "date": "2025-05",
        "title": "TRAIL: Transferable Robust Adversarial Images via Latent diffusion",
        "author": "Yuhao Xue, Zhifei Zhang, Xinyang Jiang, Yifei Shen, Junyao Gao, Wentao Gu, Jiale Zhao, Miaojing Shi, and Cairong Zhao",
        "link": "http://arxiv.org/abs/2505.16166v1",
        "abstract": "Adversarial attacks exploiting unrestricted natural perturbations present\nsevere security risks to deep learning systems, yet their transferability\nacross models remains limited due to distribution mismatches between generated\nadversarial features and real-world data. While recent works utilize\npre-trained diffusion models as adversarial priors, they still encounter\nchallenges due to the distribution shift between the distribution of ideal\nadversarial samples and the natural image distribution learned by the diffusion\nmodel. To address the challenge, we propose Transferable Robust Adversarial\nImages via Latent Diffusion (TRAIL), a test-time adaptation framework that\nenables the model to generate images from a distribution of images with\nadversarial features and closely resembles the target images. To mitigate the\ndistribution shift, during attacks, TRAIL updates the diffusion U-Net's weights\nby combining adversarial objectives (to mislead victim models) and perceptual\nconstraints (to preserve image realism). The adapted model then generates\nadversarial samples through iterative noise injection and denoising guided by\nthese objectives. Experiments demonstrate that TRAIL significantly outperforms\nstate-of-the-art methods in cross-model attack transferability, validating that\ndistribution-aligned adversarial feature synthesis is critical for practical\nblack-box attacks."
    },
    {
        "date": "2025-05",
        "title": "BadDepth: Backdoor Attacks Against Monocular Depth Estimation in the Physical World",
        "author": "Ji Guo, Long Zhou, Zhijin Wang, Jiaming He, Qiyang Song, Aiguo Chen, and Wenbo Jiang",
        "link": "http://arxiv.org/abs/2505.16154v1",
        "abstract": "In recent years, deep learning-based Monocular Depth Estimation (MDE) models\nhave been widely applied in fields such as autonomous driving and robotics.\nHowever, their vulnerability to backdoor attacks remains unexplored. To fill\nthe gap in this area, we conduct a comprehensive investigation of backdoor\nattacks against MDE models. Typically, existing backdoor attack methods can not\nbe applied to MDE models. This is because the label used in MDE is in the form\nof a depth map. To address this, we propose BadDepth, the first backdoor attack\ntargeting MDE models. BadDepth overcomes this limitation by selectively\nmanipulating the target object's depth using an image segmentation model and\nrestoring the surrounding areas via depth completion, thereby generating\npoisoned datasets for object-level backdoor attacks. To improve robustness in\nphysical world scenarios, we further introduce digital-to-physical augmentation\nto adapt to the domain gap between the physical world and the digital domain.\nExtensive experiments on multiple models validate the effectiveness of BadDepth\nin both the digital domain and the physical world, without being affected by\nenvironmental factors."
    },
    {
        "date": "2025-05",
        "title": "Outsourcing SAT-based Verification Computations in Network Security",
        "author": "Qi Duan, and Ehab Al-Shaer",
        "link": "http://arxiv.org/abs/2505.16137v1",
        "abstract": "The emergence of cloud computing gives huge impact on large computations.\nCloud computing platforms offer servers with large computation power to be\navailable for customers. These servers can be used efficiently to solve\nproblems that are complex by nature, for example, satisfiability (SAT)\nproblems. Many practical problems can be converted to SAT, for example, circuit\nverification and network configuration analysis. However, outsourcing SAT\ninstances to the servers may cause data leakage that can jeopardize system's\nsecurity. Before\n  outsourcing the SAT instance, one needs to hide the input information. One\nway to preserve privacy and hide information is to randomize the SAT\n  instance before outsourcing. In this paper, we present multiple novel methods\nto randomize SAT instances. We present a novel method to randomize the SAT\ninstance, a variable randomization method to randomize the solution set, and\nmethods to randomize Mincost SAT and MAX3SAT instances. Our analysis and\nevaluation show the correctness and feasibility of these randomization methods.\nThe scalability and generality of our methods make it applicable for real world\nproblems."
    },
    {
        "date": "2025-05",
        "title": "Robust Invariant Representation Learning by Distribution Extrapolation",
        "author": "Kotaro Yoshida, and Konstantinos Slavakis",
        "link": "http://arxiv.org/abs/2505.16126v2",
        "abstract": "Invariant risk minimization (IRM) aims to enable out-of-distribution (OOD)\ngeneralization in deep learning by learning invariant representations. As IRM\nposes an inherently challenging bi-level optimization problem, most existing\napproaches -- including IRMv1 -- adopt penalty-based single-level\napproximations. However, empirical studies consistently show that these methods\noften fail to outperform well-tuned empirical risk minimization (ERM),\nhighlighting the need for more robust IRM implementations. This work\ntheoretically identifies a key limitation common to many IRM variants: their\npenalty terms are highly sensitive to limited environment diversity and\nover-parameterization, resulting in performance degradation. To address this\nissue, a novel extrapolation-based framework is proposed that enhances\nenvironmental diversity by augmenting the IRM penalty through synthetic\ndistributional shifts. Extensive experiments -- ranging from synthetic setups\nto realistic, over-parameterized scenarios -- demonstrate that the proposed\nmethod consistently outperforms state-of-the-art IRM variants, validating its\neffectiveness and robustness."
    },
    {
        "date": "2025-05",
        "title": "LAGO: Few-shot Crosslingual Embedding Inversion Attacks via Language Similarity-Aware Graph Optimization",
        "author": "Wenrui Yu, Yiyi Chen, Johannes Bjerva, Sokol Kosta, and Qiongxiu Li",
        "link": "http://arxiv.org/abs/2505.16008v1",
        "abstract": "We propose LAGO - Language Similarity-Aware Graph Optimization - a novel\napproach for few-shot cross-lingual embedding inversion attacks, addressing\ncritical privacy vulnerabilities in multilingual NLP systems. Unlike prior work\nin embedding inversion attacks that treat languages independently, LAGO\nexplicitly models linguistic relationships through a graph-based constrained\ndistributed optimization framework. By integrating syntactic and lexical\nsimilarity as edge constraints, our method enables collaborative parameter\nlearning across related languages. Theoretically, we show this formulation\ngeneralizes prior approaches, such as ALGEN, which emerges as a special case\nwhen similarity constraints are relaxed. Our framework uniquely combines\nFrobenius-norm regularization with linear inequality or total variation\nconstraints, ensuring robust alignment of cross-lingual embedding spaces even\nwith extremely limited data (as few as 10 samples per language). Extensive\nexperiments across multiple languages and embedding models demonstrate that\nLAGO substantially improves the transferability of attacks with 10-20% increase\nin Rouge-L score over baselines. This work establishes language similarity as a\ncritical factor in inversion attack transferability, urging renewed focus on\nlanguage-aware privacy-preserving multilingual embeddings."
    },
    {
        "date": "2025-05",
        "title": "Interpretability Illusions with Sparse Autoencoders: Evaluating Robustness of Concept Representations",
        "author": "Aaron J. Li, Suraj Srinivas, Usha Bhalla, and Himabindu Lakkaraju",
        "link": "http://arxiv.org/abs/2505.16004v1",
        "abstract": "Sparse autoencoders (SAEs) are commonly used to interpret the internal\nactivations of large language models (LLMs) by mapping them to\nhuman-interpretable concept representations. While existing evaluations of SAEs\nfocus on metrics such as the reconstruction-sparsity tradeoff, human\n(auto-)interpretability, and feature disentanglement, they overlook a critical\naspect: the robustness of concept representations to input perturbations. We\nargue that robustness must be a fundamental consideration for concept\nrepresentations, reflecting the fidelity of concept labeling. To this end, we\nformulate robustness quantification as input-space optimization problems and\ndevelop a comprehensive evaluation framework featuring realistic scenarios in\nwhich adversarial perturbations are crafted to manipulate SAE representations.\nEmpirically, we find that tiny adversarial input perturbations can effectively\nmanipulate concept-based interpretations in most scenarios without notably\naffecting the outputs of the base LLMs themselves. Overall, our results suggest\nthat SAE concept representations are fragile and may be ill-suited for\napplications in model monitoring and oversight."
    },
    {
        "date": "2025-05",
        "title": "MAPS: A Multilingual Benchmark for Global Agent Performance and Security",
        "author": "Omer Hofman, Oren Rachmil, Shamik Bose, Vikas Pahuja, Jonathan Brokman, Toshiya Shimizu, Trisha Starostina, Kelly Marchisio, Seraphina Goldfarb-Tarrant, and Roman Vainshtein",
        "link": "http://arxiv.org/abs/2505.15935v1",
        "abstract": "Agentic AI systems, which build on Large Language Models (LLMs) and interact\nwith tools and memory, have rapidly advanced in capability and scope. Yet,\nsince LLMs have been shown to struggle in multilingual settings, typically\nresulting in lower performance and reduced safety, agentic systems risk\ninheriting these limitations. This raises concerns about the global\naccessibility of such systems, as users interacting in languages other than\nEnglish may encounter unreliable or security-critical agent behavior. Despite\ngrowing interest in evaluating agentic AI, existing benchmarks focus\nexclusively on English, leaving multilingual settings unexplored. To address\nthis gap, we propose MAPS, a multilingual benchmark suite designed to evaluate\nagentic AI systems across diverse languages and tasks. MAPS builds on four\nwidely used agentic benchmarks - GAIA (real-world tasks), SWE-bench (code\ngeneration), MATH (mathematical reasoning), and the Agent Security Benchmark\n(security). We translate each dataset into ten diverse languages, resulting in\n805 unique tasks and 8,855 total language-specific instances. Our benchmark\nsuite enables a systematic analysis of how multilingual contexts affect agent\nperformance and robustness. Empirically, we observe consistent degradation in\nboth performance and security when transitioning from English to other\nlanguages, with severity varying by task and correlating with the amount of\ntranslated input. Building on these findings, we provide actionable\nrecommendations to guide agentic AI systems development and assessment under\nmultilingual settings. This work establishes a standardized evaluation\nframework, encouraging future research towards equitable, reliable, and\nglobally accessible agentic AI. MAPS benchmark suite is publicly available at\nhttps://huggingface.co/datasets/Fujitsu-FRE/MAPS"
    },
    {
        "date": "2025-05",
        "title": "AllMetrics: A Unified Python Library for Standardized Metric Evaluation and Robust Data Validation in Machine Learning",
        "author": "Morteza Alizadeh, Mehrdad Oveisi, Sonya Falahati, Ghazal Mousavi, Mohsen Alambardar Meybodi, Somayeh Sadat Mehrnia, Ilker Hacihaliloglu, Arman Rahmim, and Mohammad R. Salmanpour",
        "link": "http://arxiv.org/abs/2505.15931v1",
        "abstract": "Machine learning (ML) models rely heavily on consistent and accurate\nperformance metrics to evaluate and compare their effectiveness. However,\nexisting libraries often suffer from fragmentation, inconsistent\nimplementations, and insufficient data validation protocols, leading to\nunreliable results. Existing libraries have often been developed independently\nand without adherence to a unified standard, particularly concerning the\nspecific tasks they aim to support. As a result, each library tends to adopt\nits conventions for metric computation, input/output formatting, error\nhandling, and data validation protocols. This lack of standardization leads to\nboth implementation differences (ID) and reporting differences (RD), making it\ndifficult to compare results across frameworks or ensure reliable evaluations.\nTo address these issues, we introduce AllMetrics, an open-source unified Python\nlibrary designed to standardize metric evaluation across diverse ML tasks,\nincluding regression, classification, clustering, segmentation, and\nimage-to-image translation. The library implements class-specific reporting for\nmulti-class tasks through configurable parameters to cover all use cases, while\nincorporating task-specific parameters to resolve metric computation\ndiscrepancies across implementations. Various datasets from domains like\nhealthcare, finance, and real estate were applied to our library and compared\nwith Python, Matlab, and R components to identify which yield similar results.\nAllMetrics combines a modular Application Programming Interface (API) with\nrobust input validation mechanisms to ensure reproducibility and reliability in\nmodel evaluation. This paper presents the design principles, architectural\ncomponents, and empirical analyses demonstrating the ability to mitigate\nevaluation errors and to enhance the trustworthiness of ML workflows."
    },
    {
        "date": "2025-05",
        "title": "Challenger: Affordable Adversarial Driving Video Generation",
        "author": "Zhiyuan Xu, Bohan Li, Huan-ang Gao, Mingju Gao, Yong Chen, Ming Liu, Chenxu Yan, Hang Zhao, Shuo Feng, and Hao Zhao",
        "link": "http://arxiv.org/abs/2505.15880v2",
        "abstract": "Generating photorealistic driving videos has seen significant progress\nrecently, but current methods largely focus on ordinary, non-adversarial\nscenarios. Meanwhile, efforts to generate adversarial driving scenarios often\noperate on abstract trajectory or BEV representations, falling short of\ndelivering realistic sensor data that can truly stress-test autonomous driving\n(AD) systems. In this work, we introduce Challenger, a framework that produces\nphysically plausible yet photorealistic adversarial driving videos. Generating\nsuch videos poses a fundamental challenge: it requires jointly optimizing over\nthe space of traffic interactions and high-fidelity sensor observations.\nChallenger makes this affordable through two techniques: (1) a physics-aware\nmulti-round trajectory refinement process that narrows down candidate\nadversarial maneuvers, and (2) a tailored trajectory scoring function that\nencourages realistic yet adversarial behavior while maintaining compatibility\nwith downstream video synthesis. As tested on the nuScenes dataset, Challenger\ngenerates a diverse range of aggressive driving scenarios-including cut-ins,\nsudden lane changes, tailgating, and blind spot intrusions-and renders them\ninto multiview photorealistic videos. Extensive evaluations show that these\nscenarios significantly increase the collision rate of state-of-the-art\nend-to-end AD models (UniAD, VAD, SparseDrive, and DiffusionDrive), and\nimportantly, adversarial behaviors discovered for one model often transfer to\nothers."
    },
    {
        "date": "2025-05",
        "title": "Scalable Defense against In-the-wild Jailbreaking Attacks with Safety Context Retrieval",
        "author": "Taiye Chen, Zeming Wei, Ang Li, and Yisen Wang",
        "link": "http://arxiv.org/abs/2505.15753v1",
        "abstract": "Large Language Models (LLMs) are known to be vulnerable to jailbreaking\nattacks, wherein adversaries exploit carefully engineered prompts to induce\nharmful or unethical responses. Such threats have raised critical concerns\nabout the safety and reliability of LLMs in real-world deployment. While\nexisting defense mechanisms partially mitigate such risks, subsequent\nadvancements in adversarial techniques have enabled novel jailbreaking methods\nto circumvent these protections, exposing the limitations of static defense\nframeworks. In this work, we explore defending against evolving jailbreaking\nthreats through the lens of context retrieval. First, we conduct a preliminary\nstudy demonstrating that even a minimal set of safety-aligned examples against\na particular jailbreak can significantly enhance robustness against this attack\npattern. Building on this insight, we further leverage the retrieval-augmented\ngeneration (RAG) techniques and propose Safety Context Retrieval (SCR), a\nscalable and robust safeguarding paradigm for LLMs against jailbreaking. Our\ncomprehensive experiments demonstrate how SCR achieves superior defensive\nperformance against both established and emerging jailbreaking tactics,\ncontributing a new paradigm to LLM safety. Our code will be available upon\npublication."
    },
    {
        "date": "2025-05",
        "title": "Alignment Under Pressure: The Case for Informed Adversaries When Evaluating LLM Defenses",
        "author": "Xiaoxue Yang, Bozhidar Stevanoski, Matthieu Meeus, and Yves-Alexandre de Montjoye",
        "link": "http://arxiv.org/abs/2505.15738v1",
        "abstract": "Large language models (LLMs) are rapidly deployed in real-world applications\nranging from chatbots to agentic systems. Alignment is one of the main\napproaches used to defend against attacks such as prompt injection and\njailbreaks. Recent defenses report near-zero Attack Success Rates (ASR) even\nagainst Greedy Coordinate Gradient (GCG), a white-box attack that generates\nadversarial suffixes to induce attacker-desired outputs. However, this search\nspace over discrete tokens is extremely large, making the task of finding\nsuccessful attacks difficult. GCG has, for instance, been shown to converge to\nlocal minima, making it sensitive to initialization choices. In this paper, we\nassess the future-proof robustness of these defenses using a more informed\nthreat model: attackers who have access to some information about the alignment\nprocess. Specifically, we propose an informed white-box attack leveraging the\nintermediate model checkpoints to initialize GCG, with each checkpoint acting\nas a stepping stone for the next one. We show this approach to be highly\neffective across state-of-the-art (SOTA) defenses and models. We further show\nour informed initialization to outperform other initialization methods and show\na gradient-informed checkpoint selection strategy to greatly improve attack\nperformance and efficiency. Importantly, we also show our method to\nsuccessfully find universal adversarial suffixes -- single suffixes effective\nacross diverse inputs. Our results show that, contrary to previous beliefs,\neffective adversarial suffixes do exist against SOTA alignment-based defenses,\nthat these can be found by existing attack methods when adversaries exploit\nalignment knowledge, and that even universal suffixes exist. Taken together,\nour results highlight the brittleness of current alignment-based methods and\nthe need to consider stronger threat models when testing the safety of LLMs."
    },
    {
        "date": "2025-05",
        "title": "RUSplatting: Robust 3D Gaussian Splatting for Sparse-View Underwater Scene Reconstruction",
        "author": "Zhuodong Jiang, Haoran Wang, Guoxi Huang, Brett Seymour, and Nantheera Anantrasirichai",
        "link": "http://arxiv.org/abs/2505.15737v1",
        "abstract": "Reconstructing high-fidelity underwater scenes remains a challenging task due\nto light absorption, scattering, and limited visibility inherent in aquatic\nenvironments. This paper presents an enhanced Gaussian Splatting-based\nframework that improves both the visual quality and geometric accuracy of deep\nunderwater rendering. We propose decoupled learning for RGB channels, guided by\nthe physics of underwater attenuation, to enable more accurate colour\nrestoration. To address sparse-view limitations and improve view consistency,\nwe introduce a frame interpolation strategy with a novel adaptive weighting\nscheme. Additionally, we introduce a new loss function aimed at reducing noise\nwhile preserving edges, which is essential for deep-sea content. We also\nrelease a newly collected dataset, Submerged3D, captured specifically in\ndeep-sea environments. Experimental results demonstrate that our framework\nconsistently outperforms state-of-the-art methods with PSNR gains up to 1.90dB,\ndelivering superior perceptual quality and robustness, and offering promising\ndirections for marine robotics and underwater visual analytics."
    },
    {
        "date": "2025-05",
        "title": "A Unified Theoretical Analysis of Private and Robust Offline Alignment: from RLHF to DPO",
        "author": "Xingyu Zhou, Yulian Wu, and Francesco Orabona",
        "link": "http://arxiv.org/abs/2505.15694v1",
        "abstract": "In this paper, we theoretically investigate the effects of noisy labels in\noffline alignment, with a focus on the interplay between privacy and robustness\nagainst adversarial corruption. Specifically, under linear modeling\nassumptions, we present a unified analysis covering both reinforcement learning\nfrom human feedback (RLHF) and direct preference optimization (DPO) under\ndifferent privacy-corruption scenarios, such as Local differential\nprivacy-then-Corruption (LTC), where human preference labels are privatized\nbefore being corrupted by an adversary, and Corruption-then-Local differential\nprivacy (CTL), where labels are corrupted before privacy protection. Our\nanalysis leverages a reduction framework that reduces the offline alignment\nproblem under linear modeling assumptions to parameter estimation in logistic\nregression. This framework allows us to establish an interesting separation\nresult between LTC and CTL, demonstrating that LTC presents a greater challenge\nthan CTL in offline alignment, even under linear models. As important\nby-products, our findings also advance the state-of-the-art theoretical results\nin offline alignment under privacy-only or corruption-only scenarios."
    },
    {
        "date": "2025-05",
        "title": "A Federated Splitting Framework for LLMs: Security, Efficiency, and Adaptability",
        "author": "Zishuai Zhang, Hainan Zhang, Jiaying Zheng, Ziwei Wang, Yongxin Tong, Jin Dong, and Zhiming Zheng",
        "link": "http://arxiv.org/abs/2505.15683v1",
        "abstract": "Private data is typically larger and of higher quality than public data,\noffering great potential to improve LLM. However, its scattered distribution\nacross data silos and the high computational demands of LLMs limit their\ndeployment in federated environments. To address this, the transformer-based\nsplit learning model has emerged, offloading most model parameters to the\nserver while retaining only the embedding and output layers on clients to\nensure privacy. However, it still faces significant challenges in security,\nefficiency, and adaptability: 1) embedding gradients are vulnerable to attacks,\nleading to reverse engineering of private data; 2) the autoregressive nature of\nLLMs means that federated split learning can only train and infer sequentially,\ncausing high communication overhead; 3) fixed partition points lack\nadaptability to downstream tasks. In this paper, we introduce FL-LLaMA, a\nsecure, efficient, and adaptive federated split framework based on LLaMA2.\nFirst, we place some input and output blocks on the local client and inject\nGaussian noise into forward-pass hidden states, enabling secure end-to-end\npropagation. Second, we employ client-batch and server-hierarchical strategies\nto achieve parallel training, along with attention-mask compression and KV\ncache mechanisms to accelerate inference, reducing communication costs\neffectively. Third, we allow users to dynamically adjust the partition points\nfor input/output blocks based on specific task requirements and hardware\nlimitations. Experiments on NLU, summarization and conversational QA tasks show\nthat FL-LLaMA maintains performance comparable to centralized LLaMA2, and\nachieves up to 2x train speedups and 8x inference speedups. Further analysis of\nprivacy attacks and different partition points also demonstrates the\neffectiveness of FL-LLaMA in security and adaptability."
    },
    {
        "date": "2025-05",
        "title": "Beyond Classification: Evaluating Diffusion Denoised Smoothing for Security-Utility Trade off",
        "author": "Yury Belousov, Brian Pulfer, Vitaliy Kinakh, and Slava Voloshynovskiy",
        "link": "http://arxiv.org/abs/2505.15594v1",
        "abstract": "While foundation models demonstrate impressive performance across various\ntasks, they remain vulnerable to adversarial inputs. Current research explores\nvarious approaches to enhance model robustness, with Diffusion Denoised\nSmoothing emerging as a particularly promising technique. This method employs a\npretrained diffusion model to preprocess inputs before model inference. Yet,\nits effectiveness remains largely unexplored beyond classification. We aim to\naddress this gap by analyzing three datasets with four distinct downstream\ntasks under three different adversarial attack algorithms. Our findings reveal\nthat while foundation models maintain resilience against conventional\ntransformations, applying high-noise diffusion denoising to clean images\nwithout any distortions significantly degrades performance by as high as 57%.\nLow-noise diffusion settings preserve performance but fail to provide adequate\nprotection across all attack types. Moreover, we introduce a novel attack\nstrategy specifically targeting the diffusion process itself, capable of\ncircumventing defenses in the low-noise regime. Our results suggest that the\ntrade-off between adversarial robustness and performance remains a challenge to\nbe addressed."
    },
    {
        "date": "2025-05",
        "title": "Model Checking the Security of the Lightning Network",
        "author": "Matthias Grundmann, and Hannes Hartenstein",
        "link": "http://arxiv.org/abs/2505.15568v1",
        "abstract": "Payment channel networks are an approach to improve the scalability of\nblockchain-based cryptocurrencies. The Lightning Network is a payment channel\nnetwork built for Bitcoin that is already used in practice. Because the\nLightning Network is used for transfer of financial value, its security in the\npresence of adversarial participants should be verified. The Lightning\nprotocol's complexity makes it hard to assess whether the protocol is secure.\nTo enable computer-aided security verification of Lightning, we formalize the\nprotocol in TLA+ and formally specify the security property that honest users\nare guaranteed to retrieve their correct balance. While model checking provides\na fully automated verification of the security property, the state space of the\nprotocol's specification is so large that model checking becomes unfeasible. We\nmake model checking the Lightning Network possible using two refinement steps\nthat we verify using proofs. In a first step, we prove that the model of time\nused in the protocol can be abstracted using ideas from the research of timed\nautomata. In a second step, we prove that it suffices to model check the\nprotocol for single payment channels and the protocol for multi-hop payments\nseparately. These refinements reduce the state space sufficiently to allow for\nmodel checking Lightning with models with payments over up to four hops and two\nconcurrent payments. These results indicate that the current specification of\nLightning is secure."
    },
    {
        "date": "2025-05",
        "title": "On the Robustness of Medical Vision-Language Models: Are they Truly Generalizable?",
        "author": "Raza Imam, Rufael Marew, and Mohammad Yaqub",
        "link": "http://arxiv.org/abs/2505.15425v2",
        "abstract": "Medical Vision-Language Models (MVLMs) have achieved par excellence\ngeneralization in medical image analysis, yet their performance under noisy,\ncorrupted conditions remains largely untested. Clinical imaging is inherently\nsusceptible to acquisition artifacts and noise; however, existing evaluations\npredominantly assess generally clean datasets, overlooking robustness -- i.e.,\nthe model's ability to perform under real-world distortions. To address this\ngap, we first introduce MediMeta-C, a corruption benchmark that systematically\napplies several perturbations across multiple medical imaging datasets.\nCombined with MedMNIST-C, this establishes a comprehensive robustness\nevaluation framework for MVLMs. We further propose RobustMedCLIP, a visual\nencoder adaptation of a pretrained MVLM that incorporates few-shot tuning to\nenhance resilience against corruptions. Through extensive experiments, we\nbenchmark 5 major MVLMs across 5 medical imaging modalities, revealing that\nexisting models exhibit severe degradation under corruption and struggle with\ndomain-modality tradeoffs. Our findings highlight the necessity of diverse\ntraining and robust adaptation strategies, demonstrating that efficient\nlow-rank adaptation when paired with few-shot tuning, improves robustness while\npreserving generalization across modalities."
    },
    {
        "date": "2025-05",
        "title": "Silent Leaks: Implicit Knowledge Extraction Attack on RAG Systems through Benign Queries",
        "author": "Yuhao Wang, Wenjie Qu, Yanze Jiang, Zichen Liu, Yue Liu, Shengfang Zhai, Yinpeng Dong, and Jiaheng Zhang",
        "link": "http://arxiv.org/abs/2505.15420v1",
        "abstract": "Retrieval-Augmented Generation (RAG) systems enhance large language models\n(LLMs) by incorporating external knowledge bases, but they are vulnerable to\nprivacy risks from data extraction attacks. Existing extraction methods\ntypically rely on malicious inputs such as prompt injection or jailbreaking,\nmaking them easily detectable via input- or output-level detection. In this\npaper, we introduce Implicit Knowledge Extraction Attack (IKEA), which conducts\nknowledge extraction on RAG systems through benign queries. IKEA first\nleverages anchor concepts to generate queries with the natural appearance, and\nthen designs two mechanisms to lead to anchor concept thoroughly 'explore' the\nRAG's privacy knowledge: (1) Experience Reflection Sampling, which samples\nanchor concepts based on past query-response patterns to ensure the queries'\nrelevance to RAG documents; (2) Trust Region Directed Mutation, which\niteratively mutates anchor concepts under similarity constraints to further\nexploit the embedding space. Extensive experiments demonstrate IKEA's\neffectiveness under various defenses, surpassing baselines by over 80% in\nextraction efficiency and 90% in attack success rate. Moreover, the substitute\nRAG system built from IKEA's extractions consistently outperforms those based\non baseline methods across multiple evaluation tasks, underscoring the\nsignificant privacy risk in RAG systems."
    },
    {
        "date": "2025-05",
        "title": "Robust Multimodal Learning via Entropy-Gated Contrastive Fusion",
        "author": "Leon Chlon, Maggie Chlon, and MarcAntonio M. Awada",
        "link": "http://arxiv.org/abs/2505.15417v1",
        "abstract": "Real-world multimodal systems routinely face missing-input scenarios, and in\nreality, robots lose audio in a factory or a clinical record omits lab tests at\ninference time. Standard fusion layers either preserve robustness or\ncalibration but never both. We introduce Adaptive Entropy-Gated Contrastive\nFusion (AECF), a single light-weight layer that (i) adapts its entropy\ncoefficient per instance, (ii) enforces monotone calibration across all\nmodality subsets, and (iii) drives a curriculum mask directly from\ntraining-time entropy. On AV-MNIST and MS-COCO, AECF improves masked-input mAP\nby +18 pp at a 50% drop rate while reducing ECE by up to 200%, yet adds 1%\nrun-time. All back-bones remain frozen, making AECF an easy drop-in layer for\nrobust, calibrated multimodal inference."
    },
    {
        "date": "2025-05",
        "title": "RAZER: Robust Accelerated Zero-Shot 3D Open-Vocabulary Panoptic Reconstruction with Spatio-Temporal Aggregation",
        "author": "Naman Patel, Prashanth Krishnamurthy, and Farshad Khorrami",
        "link": "http://arxiv.org/abs/2505.15373v1",
        "abstract": "Mapping and understanding complex 3D environments is fundamental to how\nautonomous systems perceive and interact with the physical world, requiring\nboth precise geometric reconstruction and rich semantic comprehension. While\nexisting 3D semantic mapping systems excel at reconstructing and identifying\npredefined object instances, they lack the flexibility to efficiently build\nsemantic maps with open-vocabulary during online operation. Although recent\nvision-language models have enabled open-vocabulary object recognition in 2D\nimages, they haven't yet bridged the gap to 3D spatial understanding. The\ncritical challenge lies in developing a training-free unified system that can\nsimultaneously construct accurate 3D maps while maintaining semantic\nconsistency and supporting natural language interactions in real time. In this\npaper, we develop a zero-shot framework that seamlessly integrates\nGPU-accelerated geometric reconstruction with open-vocabulary vision-language\nmodels through online instance-level semantic embedding fusion, guided by\nhierarchical object association with spatial indexing. Our training-free system\nachieves superior performance through incremental processing and unified\ngeometric-semantic updates, while robustly handling 2D segmentation\ninconsistencies. The proposed general-purpose 3D scene understanding framework\ncan be used for various tasks including zero-shot 3D instance retrieval,\nsegmentation, and object detection to reason about previously unseen objects\nand interpret natural language queries. The project page is available at\nhttps://razer-3d.github.io."
    },
    {
        "date": "2025-05",
        "title": "Distributionally Robust Federated Learning with Client Drift Minimization",
        "author": "Mounssif Krouka, Chaouki Ben Issaid, and Mehdi Bennis",
        "link": "http://arxiv.org/abs/2505.15371v1",
        "abstract": "Federated learning (FL) faces critical challenges, particularly in\nheterogeneous environments where non-independent and identically distributed\ndata across clients can lead to unfair and inefficient model performance. In\nthis work, we introduce \\textit{DRDM}, a novel algorithm that addresses these\nissues by combining a distributionally robust optimization (DRO) framework with\ndynamic regularization to mitigate client drift. \\textit{DRDM} frames the\ntraining as a min-max optimization problem aimed at maximizing performance for\nthe worst-case client, thereby promoting robustness and fairness. This robust\nobjective is optimized through an algorithm leveraging dynamic regularization\nand efficient local updates, which significantly reduces the required number of\ncommunication rounds. Moreover, we provide a theoretical convergence analysis\nfor convex smooth objectives under partial participation. Extensive experiments\non three benchmark datasets, covering various model architectures and data\nheterogeneity levels, demonstrate that \\textit{DRDM} significantly improves\nworst-case test accuracy while requiring fewer communication rounds than\nexisting state-of-the-art baselines. Furthermore, we analyze the impact of\nsignal-to-noise ratio (SNR) and bandwidth on the energy consumption of\nparticipating clients, demonstrating that the number of local update steps can\nbe adaptively selected to achieve a target worst-case test accuracy with\nminimal total energy cost across diverse communication environments."
    },
    {
        "date": "2025-05",
        "title": "Your Language Model Can Secretly Write Like Humans: Contrastive Paraphrase Attacks on LLM-Generated Text Detectors",
        "author": "Hao Fang, Jiawei Kong, Tianqu Zhuang, Yixiang Qiu, Kuofeng Gao, Bin Chen, Shu-Tao Xia, Yaowei Wang, and Min Zhang",
        "link": "http://arxiv.org/abs/2505.15337v1",
        "abstract": "The misuse of large language models (LLMs), such as academic plagiarism, has\ndriven the development of detectors to identify LLM-generated texts. To bypass\nthese detectors, paraphrase attacks have emerged to purposely rewrite these\ntexts to evade detection. Despite the success, existing methods require\nsubstantial data and computational budgets to train a specialized paraphraser,\nand their attack efficacy greatly reduces when faced with advanced detection\nalgorithms. To address this, we propose \\textbf{Co}ntrastive\n\\textbf{P}araphrase \\textbf{A}ttack (CoPA), a training-free method that\neffectively deceives text detectors using off-the-shelf LLMs. The first step is\nto carefully craft instructions that encourage LLMs to produce more human-like\ntexts. Nonetheless, we observe that the inherent statistical biases of LLMs can\nstill result in some generated texts carrying certain machine-like attributes\nthat can be captured by detectors. To overcome this, CoPA constructs an\nauxiliary machine-like word distribution as a contrast to the human-like\ndistribution generated by the LLM. By subtracting the machine-like patterns\nfrom the human-like distribution during the decoding process, CoPA is able to\nproduce sentences that are less discernible by text detectors. Our theoretical\nanalysis suggests the superiority of the proposed attack. Extensive experiments\nvalidate the effectiveness of CoPA in fooling text detectors across various\nscenarios."
    },
    {
        "date": "2025-05",
        "title": "Towards Zero-Shot Differential Morphing Attack Detection with Multimodal Large Language Models",
        "author": "Ria Shekhawat, Hailin Li, Raghavendra Ramachandra, and Sushma Venkatesh",
        "link": "http://arxiv.org/abs/2505.15332v1",
        "abstract": "Leveraging the power of multimodal large language models (LLMs) offers a\npromising approach to enhancing the accuracy and interpretability of morphing\nattack detection (MAD), especially in real-world biometric applications. This\nwork introduces the use of LLMs for differential morphing attack detection\n(D-MAD). To the best of our knowledge, this is the first study to employ\nmultimodal LLMs to D-MAD using real biometric data. To effectively utilize\nthese models, we design Chain-of-Thought (CoT)-based prompts to reduce\nfailure-to-answer rates and enhance the reasoning behind decisions. Our\ncontributions include: (1) the first application of multimodal LLMs for D-MAD\nusing real data subjects, (2) CoT-based prompt engineering to improve response\nreliability and explainability, (3) comprehensive qualitative and quantitative\nbenchmarking of LLM performance using data from 54 individuals captured in\npassport enrollment scenarios, and (4) comparative analysis of two multimodal\nLLMs: ChatGPT-4o and Gemini providing insights into their morphing attack\ndetection accuracy and decision transparency. Experimental results show that\nChatGPT-4o outperforms Gemini in detection accuracy, especially against\nGAN-based morphs, though both models struggle under challenging conditions.\nWhile Gemini offers more consistent explanations, ChatGPT-4o is more resilient\nbut prone to a higher failure-to-answer rate."
    },
    {
        "date": "2025-05",
        "title": "BadSR: Stealthy Label Backdoor Attacks on Image Super-Resolution",
        "author": "Ji Guo, Xiaolei Wen, Wenbo Jiang, Cheng Huang, Jinjin Li, and Hongwei Li",
        "link": "http://arxiv.org/abs/2505.15308v1",
        "abstract": "With the widespread application of super-resolution (SR) in various fields,\nresearchers have begun to investigate its security. Previous studies have\ndemonstrated that SR models can also be subjected to backdoor attacks through\ndata poisoning, affecting downstream tasks. A backdoor SR model generates an\nattacker-predefined target image when given a triggered image while producing a\nnormal high-resolution (HR) output for clean images. However, prior backdoor\nattacks on SR models have primarily focused on the stealthiness of poisoned\nlow-resolution (LR) images while ignoring the stealthiness of poisoned HR\nimages, making it easy for users to detect anomalous data. To address this\nproblem, we propose BadSR, which improves the stealthiness of poisoned HR\nimages. The key idea of BadSR is to approximate the clean HR image and the\npre-defined target image in the feature space while ensuring that modifications\nto the clean HR image remain within a constrained range. The poisoned HR images\ngenerated by BadSR can be integrated with existing triggers. To further improve\nthe effectiveness of BadSR, we design an adversarially optimized trigger and a\nbackdoor gradient-driven poisoned sample selection method based on a genetic\nalgorithm. The experimental results show that BadSR achieves a high attack\nsuccess rate in various models and data sets, significantly affecting\ndownstream tasks."
    },
    {
        "date": "2025-05",
        "title": "R3GS: Gaussian Splatting for Robust Reconstruction and Relocalization in Unconstrained Image Collections",
        "author": "Xu yan, Zhaohui Wang, Rong Wei, Jingbo Yu, Dong Li, and Xiangde Liu",
        "link": "http://arxiv.org/abs/2505.15294v1",
        "abstract": "We propose R3GS, a robust reconstruction and relocalization framework\ntailored for unconstrained datasets. Our method uses a hybrid representation\nduring training. Each anchor combines a global feature from a convolutional\nneural network (CNN) with a local feature encoded by the multiresolution hash\ngrids [2]. Subsequently, several shallow multi-layer perceptrons (MLPs) predict\nthe attributes of each Gaussians, including color, opacity, and covariance. To\nmitigate the adverse effects of transient objects on the reconstruction\nprocess, we ffne-tune a lightweight human detection network. Once ffne-tuned,\nthis network generates a visibility map that efffciently generalizes to other\ntransient objects (such as posters, banners, and cars) with minimal need for\nfurther adaptation. Additionally, to address the challenges posed by sky\nregions in outdoor scenes, we propose an effective sky-handling technique that\nincorporates a depth prior as a constraint. This allows the inffnitely distant\nsky to be represented on the surface of a large-radius sky sphere,\nsigniffcantly reducing ffoaters caused by errors in sky reconstruction.\nFurthermore, we introduce a novel relocalization method that remains robust to\nchanges in lighting conditions while estimating the camera pose of a given\nimage within the reconstructed 3DGS scene. As a result, R3GS significantly\nenhances rendering ffdelity, improves both training and rendering efffciency,\nand reduces storage requirements. Our method achieves state-of-the-art\nperformance compared to baseline methods on in-the-wild datasets. The code will\nbe made open-source following the acceptance of the paper."
    },
    {
        "date": "2025-05",
        "title": "Adaptive Plan-Execute Framework for Smart Contract Security Auditing",
        "author": "Zhiyuan Wei, Jing Sun, Zijian Zhang, Zhe Hou, and Zixiao Zhao",
        "link": "http://arxiv.org/abs/2505.15242v2",
        "abstract": "Large Language Models (LLMs) have shown great promise in code analysis and\nauditing; however, they still struggle with hallucinations and limited\ncontext-aware reasoning. We introduce SmartAuditFlow, a novel Plan-Execute\nframework that enhances smart contract security analysis through dynamic audit\nplanning and structured execution. Unlike conventional LLM-based auditing\napproaches that follow fixed workflows and predefined steps, SmartAuditFlow\ndynamically generates and refines audit plans based on the unique\ncharacteristics of each smart contract. It continuously adjusts its auditing\nstrategy in response to intermediate LLM outputs and newly detected\nvulnerabilities, ensuring a more adaptive and precise security assessment. The\nframework then executes these plans step by step, applying a structured\nreasoning process to enhance vulnerability detection accuracy while minimizing\nhallucinations and false positives. To further improve audit precision,\nSmartAuditFlow integrates iterative prompt optimization and external knowledge\nsources, such as static analysis tools and Retrieval-Augmented Generation\n(RAG). This ensures audit decisions are contextually informed and backed by\nreal-world security knowledge, producing comprehensive security reports.\nExtensive evaluations across multiple benchmarks demonstrate that\nSmartAuditFlow outperforms existing methods, achieving 100 percent accuracy on\ncommon and critical vulnerabilities, 41.2 percent accuracy for comprehensive\ncoverage of known smart contract weaknesses in real-world projects, and\nsuccessfully identifying all 13 tested CVEs. These results highlight\nSmartAuditFlow's scalability, cost-effectiveness, and superior adaptability\nover traditional static analysis tools and contemporary LLM-based approaches,\nestablishing it as a robust solution for automated smart contract auditing."
    },
    {
        "date": "2025-05",
        "title": "BountyBench: Dollar Impact of AI Agent Attackers and Defenders on Real-World Cybersecurity Systems",
        "author": "Andy K. Zhang, Joey Ji, Celeste Menders, Riya Dulepet, Thomas Qin, Ron Y. Wang, Junrong Wu, Kyleen Liao, Jiliang Li, Jinghan Hu, Sara Hong, Nardos Demilew, Shivatmica Murgai, Jason Tran, Nishka Kacheria, Ethan Ho, Denis Liu, Lauren McLane, Olivia Bruvik, Dai-Rong Han, Seungwoo Kim, Akhil Vyas, Cuiyuanxiu Chen, Ryan Li, Weiran Xu, Jonathan Z. Ye, Prerit Choudhary, Siddharth M. Bhatia, Vikram Sivashankar, Yuxuan Bao, Dawn Song, Dan Boneh, Daniel E. Ho, and Percy Liang",
        "link": "http://arxiv.org/abs/2505.15216v1",
        "abstract": "AI agents have the potential to significantly alter the cybersecurity\nlandscape. To help us understand this change, we introduce the first framework\nto capture offensive and defensive cyber-capabilities in evolving real-world\nsystems. Instantiating this framework with BountyBench, we set up 25 systems\nwith complex, real-world codebases. To capture the vulnerability lifecycle, we\ndefine three task types: Detect (detecting a new vulnerability), Exploit\n(exploiting a specific vulnerability), and Patch (patching a specific\nvulnerability). For Detect, we construct a new success indicator, which is\ngeneral across vulnerability types and provides localized evaluation. We\nmanually set up the environment for each system, including installing packages,\nsetting up server(s), and hydrating database(s). We add 40 bug bounties, which\nare vulnerabilities with monetary awards from \\$10 to \\$30,485, and cover 9 of\nthe OWASP Top 10 Risks. To modulate task difficulty, we devise a new strategy\nbased on information to guide detection, interpolating from identifying a zero\nday to exploiting a specific vulnerability. We evaluate 5 agents: Claude Code,\nOpenAI Codex CLI, and custom agents with GPT-4.1, Gemini 2.5 Pro Preview, and\nClaude 3.7 Sonnet Thinking. Given up to three attempts, the top-performing\nagents are Claude Code (5% on Detect, mapping to \\$1,350), Custom Agent with\nClaude 3.7 Sonnet Thinking (5% on Detect, mapping to \\$1,025; 67.5% on\nExploit), and OpenAI Codex CLI (5% on Detect, mapping to \\$2,400; 90% on Patch,\nmapping to \\$14,422). OpenAI Codex CLI and Claude Code are more capable at\ndefense, achieving higher Patch scores of 90% and 87.5%, compared to Exploit\nscores of 32.5% and 57.5% respectively; in contrast, the custom agents are\nrelatively balanced between offense and defense, achieving Exploit scores of\n40-67.5% and Patch scores of 45-60%."
    },
    {
        "date": "2025-05",
        "title": "Group Distributionally Robust Optimization with Flexible Sample Queries",
        "author": "Haomin Bai, Dingzhi Yu, Shuai Li, Haipeng Luo, and Lijun Zhang",
        "link": "http://arxiv.org/abs/2505.15212v1",
        "abstract": "Group distributionally robust optimization (GDRO) aims to develop models that\nperform well across $m$ distributions simultaneously. Existing GDRO algorithms\ncan only process a fixed number of samples per iteration, either 1 or $m$, and\ntherefore can not support scenarios where the sample size varies dynamically.\nTo address this limitation, we investigate GDRO with flexible sample queries\nand cast it as a two-player game: one player solves an online convex\noptimization problem, while the other tackles a prediction with limited advice\n(PLA) problem. Within such a game, we propose a novel PLA algorithm,\nconstructing appropriate loss estimators for cases where the sample size is\neither 1 or not, and updating the decision using follow-the-regularized-leader.\nThen, we establish the first high-probability regret bound for non-oblivious\nPLA. Building upon the above approach, we develop a GDRO algorithm that allows\nan arbitrary and varying sample size per round, achieving a high-probability\noptimization error bound of $O\\left(\\frac{1}{t}\\sqrt{\\sum_{j=1}^t\n\\frac{m}{r_j}\\log m}\\right)$, where $r_t$ denotes the sample size at round $t$.\nThis result demonstrates that the optimization error decreases as the number of\nsamples increases and implies a consistent sample complexity of $O(m\\log\n(m)/\\epsilon^2)$ for any fixed sample size $r\\in[m]$, aligning with existing\nbounds for cases of $r=1$ or $m$. We validate our approach on synthetic binary\nand real-world multi-class datasets."
    },
    {
        "date": "2025-05",
        "title": "EEG-Based Inter-Patient Epileptic Seizure Detection Combining Domain Adversarial Training with CNN-BiLSTM Network",
        "author": "Rina Tazaki, Tomoyuki Akiyama, and Akira Furui",
        "link": "http://arxiv.org/abs/2505.15203v1",
        "abstract": "Automated epileptic seizure detection from electroencephalogram (EEG) remains\nchallenging due to significant individual differences in EEG patterns across\npatients. While existing studies achieve high accuracy with patient-specific\napproaches, they face difficulties in generalizing to new patients. To address\nthis, we propose a detection framework combining domain adversarial training\nwith a convolutional neural network (CNN) and a bidirectional long short-term\nmemory (BiLSTM). First, the CNN extracts local patient-invariant features\nthrough domain adversarial training, which optimizes seizure detection accuracy\nwhile minimizing patient-specific characteristics. Then, the BiLSTM captures\ntemporal dependencies in the extracted features to model seizure evolution\npatterns. Evaluation using EEG recordings from 20 patients with focal epilepsy\ndemonstrated superior performance over non-adversarial methods, achieving high\ndetection accuracy across different patients. The integration of adversarial\ntraining with temporal modeling enables robust cross-patient seizure detection."
    },
    {
        "date": "2025-05",
        "title": "GAMA: Geometry-Aware Manifold Alignment via Structured Adversarial Perturbations for Robust Domain Adaptation",
        "author": "Hana Satou, and F Monkey",
        "link": "http://arxiv.org/abs/2505.15194v1",
        "abstract": "Domain adaptation remains a challenge when there is significant manifold\ndiscrepancy between source and target domains. Although recent methods leverage\nmanifold-aware adversarial perturbations to perform data augmentation, they\noften neglect precise manifold alignment and systematic exploration of\nstructured perturbations. To address this, we propose GAMA (Geometry-Aware\nManifold Alignment), a structured framework that achieves explicit manifold\nalignment via adversarial perturbation guided by geometric information. GAMA\nsystematically employs tangent space exploration and manifold-constrained\nadversarial optimization, simultaneously enhancing semantic consistency,\nrobustness to off-manifold deviations, and cross-domain alignment. Theoretical\nanalysis shows that GAMA tightens the generalization bound via structured\nregularization and explicit alignment. Empirical results on DomainNet, VisDA,\nand Office-Home demonstrate that GAMA consistently outperforms existing\nadversarial and adaptation methods in both unsupervised and few-shot settings,\nexhibiting superior robustness, generalization, and manifold alignment\ncapability."
    },
    {
        "date": "2025-05",
        "title": "Enhancing Certified Robustness via Block Reflector Orthogonal Layers and Logit Annealing Loss",
        "author": "Bo-Han Lai, Pin-Han Huang, Bo-Han Kung, and Shang-Tse Chen",
        "link": "http://arxiv.org/abs/2505.15174v1",
        "abstract": "Lipschitz neural networks are well-known for providing certified robustness\nin deep learning. In this paper, we present a novel, efficient Block Reflector\nOrthogonal (BRO) layer that enhances the capability of orthogonal layers on\nconstructing more expressive Lipschitz neural architectures. In addition, by\ntheoretically analyzing the nature of Lipschitz neural networks, we introduce a\nnew loss function that employs an annealing mechanism to increase margin for\nmost data points. This enables Lipschitz models to provide better certified\nrobustness. By employing our BRO layer and loss function, we design BRONet - a\nsimple yet effective Lipschitz neural network that achieves state-of-the-art\ncertified robustness. Extensive experiments and empirical analysis on\nCIFAR-10/100, Tiny-ImageNet, and ImageNet validate that our method outperforms\nexisting baselines. The implementation is available at\n\\href{https://github.com/ntuaislab/BRONet}{https://github.com/ntuaislab/BRONet}."
    },
    {
        "date": "2025-05",
        "title": "EC-LDA : Label Distribution Inference Attack against Federated Graph Learning with Embedding Compression",
        "author": "Tong Cheng, Fu Jie, Xinpeng Ling, Huifa Li, and Zhili Chen",
        "link": "http://arxiv.org/abs/2505.15140v1",
        "abstract": "Graph Neural Networks (GNNs) have been widely used for graph analysis.\nFederated Graph Learning (FGL) is an emerging learning framework to\ncollaboratively train graph data from various clients. However, since clients\nare required to upload model parameters to the server in each round, this\nprovides the server with an opportunity to infer each client's data privacy. In\nthis paper, we focus on label distribution attacks(LDAs) that aim to infer the\nlabel distributions of the clients' local data. We take the first step to\nattack client's label distributions in FGL. Firstly, we observe that the\neffectiveness of LDA is closely related to the variance of node embeddings in\nGNNs. Next, we analyze the relation between them and we propose a new attack\nnamed EC-LDA, which significantly improves the attack effectiveness by\ncompressing node embeddings. Thirdly, extensive experiments on node\nclassification and link prediction tasks across six widely used graph datasets\nshow that EC-LDA outperforms the SOTA LDAs. For example, EC-LDA attains optimal\nvalues under both Cos-sim and JS-div evaluation metrics in the CoraFull and\nLastFM datasets. Finally, we explore the robustness of EC-LDA under\ndifferential privacy protection."
    },
    {
        "date": "2025-05",
        "title": "Few-Shot Adversarial Low-Rank Fine-Tuning of Vision-Language Models",
        "author": "Sajjad Ghiasvand, Haniyeh Ehsani Oskouie, Mahnoosh Alizadeh, and Ramtin Pedarsani",
        "link": "http://arxiv.org/abs/2505.15130v1",
        "abstract": "Vision-Language Models (VLMs) such as CLIP have shown remarkable performance\nin cross-modal tasks through large-scale contrastive pre-training. To adapt\nthese large transformer-based models efficiently for downstream tasks,\nParameter-Efficient Fine-Tuning (PEFT) techniques like LoRA have emerged as\nscalable alternatives to full fine-tuning, especially in few-shot scenarios.\nHowever, like traditional deep neural networks, VLMs are highly vulnerable to\nadversarial attacks, where imperceptible perturbations can significantly\ndegrade model performance. Adversarial training remains the most effective\nstrategy for improving model robustness in PEFT. In this work, we propose\nAdvCLIP-LoRA, the first algorithm designed to enhance the adversarial\nrobustness of CLIP models fine-tuned with LoRA in few-shot settings. Our method\nformulates adversarial fine-tuning as a minimax optimization problem and\nprovides theoretical guarantees for convergence under smoothness and\nnonconvex-strong-concavity assumptions. Empirical results across eight datasets\nusing ViT-B/16 and ViT-B/32 models show that AdvCLIP-LoRA significantly\nimproves robustness against common adversarial attacks (e.g., FGSM, PGD),\nwithout sacrificing much clean accuracy. These findings highlight AdvCLIP-LoRA\nas a practical and theoretically grounded approach for robust adaptation of\nVLMs in resource-constrained settings."
    },
    {
        "date": "2025-05",
        "title": "A Survey On Secure Machine Learning",
        "author": "Taobo Liao, Taoran Li, and Prathamesh Nadkarni",
        "link": "http://arxiv.org/abs/2505.15124v1",
        "abstract": "In this survey, we will explore the interaction between secure multiparty\ncomputation and the area of machine learning. Recent advances in secure\nmultiparty computation (MPC) have significantly improved its applicability in\nthe realm of machine learning (ML), offering robust solutions for\nprivacy-preserving collaborative learning. This review explores key\ncontributions that leverage MPC to enable multiple parties to engage in ML\ntasks without compromising the privacy of their data. The integration of MPC\nwith ML frameworks facilitates the training and evaluation of models on\ncombined datasets from various sources, ensuring that sensitive information\nremains encrypted throughout the process. Innovations such as specialized\nsoftware frameworks and domain-specific languages streamline the adoption of\nMPC in ML, optimizing performance and broadening its usage. These frameworks\naddress both semi-honest and malicious threat models, incorporating features\nsuch as automated optimizations and cryptographic auditing to ensure compliance\nand data integrity. The collective insights from these studies highlight MPC's\npotential in fostering collaborative yet confidential data analysis, marking a\nsignificant stride towards the realization of secure and efficient\ncomputational solutions in privacy-sensitive industries. This paper\ninvestigates a spectrum of SecureML libraries that includes cryptographic\nprotocols, federated learning frameworks, and privacy-preserving algorithms. By\nsurveying the existing literature, this paper aims to examine the efficacy of\nthese libraries in preserving data privacy, ensuring model confidentiality, and\nfortifying ML systems against adversarial attacks. Additionally, the study\nexplores an innovative application domain for SecureML techniques: the\nintegration of these methodologies in gaming environments utilizing ML."
    },
    {
        "date": "2025-05",
        "title": "Robust Multi-Modal Forecasting: Integrating Static and Dynamic Features",
        "author": "Jeremy Qin",
        "link": "http://arxiv.org/abs/2505.15083v1",
        "abstract": "Time series forecasting plays a crucial role in various applications,\nparticularly in healthcare, where accurate predictions of future health\ntrajectories can significantly impact clinical decision-making. Ensuring\ntransparency and explainability of the models responsible for these tasks is\nessential for their adoption in critical settings. Recent work has explored a\ntop-down approach to bi-level transparency, focusing on understanding trends\nand properties of predicted time series using static features. In this work, we\nextend this framework by incorporating exogenous time series features alongside\nstatic features in a structured manner, while maintaining cohesive\ninterpretation. Our approach leverages the insights of trajectory comprehension\nto introduce an encoding mechanism for exogenous time series, where they are\ndecomposed into meaningful trends and properties, enabling the extraction of\ninterpretable patterns. Through experiments on several synthetic datasets, we\ndemonstrate that our approach remains predictive while preserving\ninterpretability and robustness. This work represents a step towards developing\nrobust, and generalized time series forecasting models. The code is available\nat https://github.com/jeremy-qin/TIMEVIEW"
    },
    {
        "date": "2025-05",
        "title": "Neuromorphic Mimicry Attacks Exploiting Brain-Inspired Computing for Covert Cyber Intrusions",
        "author": "Hemanth Ravipati",
        "link": "http://arxiv.org/abs/2505.17094v1",
        "abstract": "Neuromorphic computing, inspired by the human brain's neural architecture, is\nrevolutionizing artificial intelligence and edge computing with its low-power,\nadaptive, and event-driven designs. However, these unique characteristics\nintroduce novel cybersecurity risks. This paper proposes Neuromorphic Mimicry\nAttacks (NMAs), a groundbreaking class of threats that exploit the\nprobabilistic and non-deterministic nature of neuromorphic chips to execute\ncovert intrusions. By mimicking legitimate neural activity through techniques\nsuch as synaptic weight tampering and sensory input poisoning, NMAs evade\ntraditional intrusion detection systems, posing risks to applications such as\nautonomous vehicles, smart medical implants, and IoT networks. This research\ndevelops a theoretical framework for NMAs, evaluates their impact using a\nsimulated neuromorphic chip dataset, and proposes countermeasures, including\nneural-specific anomaly detection and secure synaptic learning protocols. The\nfindings underscore the critical need for tailored cybersecurity measures to\nprotect brain-inspired computing, offering a pioneering exploration of this\nemerging threat landscape."
    },
    {
        "date": "2025-05",
        "title": "An Empirical Analysis of EOS Blockchain: Architecture, Contract, and Security",
        "author": "Haiyang Liu, Yingjie Mao, and Xiaoqi Li",
        "link": "http://arxiv.org/abs/2505.15051v1",
        "abstract": "With the rapid development of blockchain technology, various blockchain\nsystems are exhibiting vitality and potential. As a representative of\nBlockchain 3.0, the EOS blockchain has been regarded as a strong competitor to\nEthereum. Nevertheless, compared with Bitcoin and Ethereum, academic research\nand in-depth analyses of EOS remain scarce. To address this gap, this study\nconducts a comprehensive investigation of the EOS blockchain from five key\ndimensions: system architecture, decentralization, performance, smart\ncontracts, and behavioral security. The architectural analysis focuses on six\ncore components of the EOS system, detailing their functionalities and\noperational workflows. The decentralization and performance evaluations, based\non data from the XBlock data-sharing platform, reveal several critical issues:\nlow account activity, limited participation in the supernode election process,\nminimal variation in the set of block producers, and a substantial gap between\nactual throughput and the claimed million-level performance. Five types of\ncontract vulnerabilities are identified in the smart contract dimension, and\nfour mainstream vulnerability detection platforms are introduced and\ncomparatively analyzed. In terms of behavioral security, four real-world\nattacks targeting the structural characteristics of EOS are summarized. This\nstudy contributes to the ongoing development of the EOS blockchain and provides\nvaluable insights for enhancing the security and regulatory mechanisms of\nblockchain ecosystems."
    },
    {
        "date": "2025-05",
        "title": "Covert Attacks on Machine Learning Training in Passively Secure MPC",
        "author": "Matthew Jagielski, Daniel Escudero, Rahul Rachuri, and Peter Scholl",
        "link": "http://arxiv.org/abs/2505.17092v1",
        "abstract": "Secure multiparty computation (MPC) allows data owners to train machine\nlearning models on combined data while keeping the underlying training data\nprivate. The MPC threat model either considers an adversary who passively\ncorrupts some parties without affecting their overall behavior, or an adversary\nwho actively modifies the behavior of corrupt parties. It has been argued that\nin some settings, active security is not a major concern, partly because of the\npotential risk of reputation loss if a party is detected cheating.\n  In this work we show explicit, simple, and effective attacks that an active\nadversary can run on existing passively secure MPC training protocols, while\nkeeping essentially zero risk of the attack being detected. The attacks we show\ncan compromise both the integrity and privacy of the model, including attacks\nreconstructing exact training data. Our results challenge the belief that a\nthreat model that does not include malicious behavior by the involved parties\nmay be reasonable in the context of PPML, motivating the use of actively secure\nprotocols for training."
    },
    {
        "date": "2025-05",
        "title": "Topology-aware Detection and Localization of Distributed Denial-of-Service Attacks in Network-on-Chips",
        "author": "Hansika Weerasena, Xiaoguo Jia, and Prabhat Mishra",
        "link": "http://arxiv.org/abs/2505.14898v1",
        "abstract": "Network-on-Chip (NoC) enables on-chip communication between diverse cores in\nmodern System-on-Chip (SoC) designs. With its shared communication fabric, NoC\nhas become a focal point for various security threats, especially in\nheterogeneous and high-performance computing platforms. Among these attacks,\nDistributed Denial of Service (DDoS) attacks occur when multiple malicious\nentities collaborate to overwhelm and disrupt access to critical system\ncomponents, potentially causing severe performance degradation or complete\ndisruption of services. These attacks are particularly challenging to detect\ndue to their distributed nature and dynamic traffic patterns in NoC, which\noften evade static detection rules or simple profiling. This paper presents a\nframework to conduct topology-aware detection and localization of DDoS attacks\nusing Graph Neural Networks (GNNs) by analyzing NoC traffic patterns.\nSpecifically, by modeling the NoC as a graph, our method utilizes\nspatiotemporal traffic features to effectively identify and localize DDoS\nattacks. Unlike prior works that rely on handcrafted features or\nthreshold-based detection, our GNN-based approach operates directly on raw\ninter-flit delay data, learning complex traffic dependencies without manual\nintervention. Experimental results demonstrate that our approach can detect and\nlocalize DDoS attacks with high accuracy (up to 99\\%) while maintaining\nconsistent performance under diverse attack strategies. Furthermore, the\nproposed method exhibits strong robustness across varying numbers and\nplacements of malicious IPs, different packet injection rates, application\nworkloads, and architectural configurations, including both 2D mesh and 3D\nTSV-based NoCs. Our work provides a scalable, flexible, and\narchitecture-agnostic defense mechanism, significantly improving the\navailability and trustworthiness of on-chip communication in future SoC\ndesigns."
    },
    {
        "date": "2025-05",
        "title": "On the (in)security of Proofs-of-Space based Longest-Chain Blockchains",
        "author": "Mirza Ahad Baig, and Krzysztof Pietrzak",
        "link": "http://arxiv.org/abs/2505.14891v1",
        "abstract": "The Nakamoto consensus protocol underlying the Bitcoin blockchain uses proof\nof work as a voting mechanism. Honest miners who contribute hashing power\ntowards securing the chain try to extend the longest chain they are aware of.\nDespite its simplicity, Nakamoto consensus achieves meaningful security\nguarantees assuming that at any point in time, a majority of the hashing power\nis controlled by honest parties. This also holds under ``resource\nvariability'', i.e., if the total hashing power varies greatly over time.\n  Proofs of space (PoSpace) have been suggested as a more sustainable\nreplacement for proofs of work. Unfortunately, no construction of a\n``longest-chain'' blockchain based on PoSpace, that is secure under dynamic\navailability, is known. In this work, we prove that without additional\nassumptions no such protocol exists. We exactly quantify this impossibility\nresult by proving a bound on the length of the fork required for double\nspending as a function of the adversarial capabilities. This bound holds for\nany chain selection rule, and we also show a chain selection rule (albeit a\nvery strange one) that almost matches this bound.\n  Concretely, we consider a security game in which the honest parties at any\npoint control $\\phi>1$ times more space than the adversary. The adversary can\nchange the honest space by a factor $1\\pm \\varepsilon$ with every block\n(dynamic availability), and ``replotting'' the space takes as much time as\n$\\rho$ blocks.\n  We prove that no matter what chain selection rule is used, in this game the\nadversary can create a fork of length $\\phi^2\\cdot \\rho / \\varepsilon$ that\nwill be picked as the winner by the chain selection rule.\n  We also provide an upper bound that matches the lower bound up to a factor\n$\\phi$. There exists a chain selection rule which in the above game requires\nforks of length at least $\\phi\\cdot \\rho / \\varepsilon$."
    },
    {
        "date": "2025-05",
        "title": "Replay Attacks Against Audio Deepfake Detection",
        "author": "Nicolas M\u00fcller, Piotr Kawa, Wei-Herng Choong, Adriana Stan, Aditya Tirumala Bukkapatnam, Karla Pizzi, Alexander Wagner, and Philip Sperl",
        "link": "http://arxiv.org/abs/2505.14862v1",
        "abstract": "We show how replay attacks undermine audio deepfake detection: By playing and\nre-recording deepfake audio through various speakers and microphones, we make\nspoofed samples appear authentic to the detection model. To study this\nphenomenon in more detail, we introduce ReplayDF, a dataset of recordings\nderived from M-AILABS and MLAAD, featuring 109 speaker-microphone combinations\nacross six languages and four TTS models. It includes diverse acoustic\nconditions, some highly challenging for detection. Our analysis of six\nopen-source detection models across five datasets reveals significant\nvulnerability, with the top-performing W2V2-AASIST model's Equal Error Rate\n(EER) surging from 4.7% to 18.2%. Even with adaptive Room Impulse Response\n(RIR) retraining, performance remains compromised with an 11.0% EER. We release\nReplayDF for non-commercial research use."
    },
    {
        "date": "2025-05",
        "title": "Robust and Efficient AI-Based Attack Recovery in Autonomous Drones",
        "author": "Diego Ortiz Barbosa, Luis Burbano, Siwei Yang, Zijun Wang, Alvaro A. Cardenas, Cihang Xie, and Yinzhi Cao",
        "link": "http://arxiv.org/abs/2505.14835v1",
        "abstract": "We introduce an autonomous attack recovery architecture to add common sense\nreasoning to plan a recovery action after an attack is detected. We outline\nuse-cases of our architecture using drones, and then discuss how to implement\nthis architecture efficiently and securely in edge devices."
    },
    {
        "date": "2025-05",
        "title": "Explainable AI for Securing Healthcare in IoT-Integrated 6G Wireless Networks",
        "author": "Navneet Kaur, and Lav Gupta",
        "link": "http://arxiv.org/abs/2505.14659v1",
        "abstract": "As healthcare systems increasingly adopt advanced wireless networks and\nconnected devices, securing medical applications has become critical. The\nintegration of Internet of Medical Things devices, such as robotic surgical\ntools, intensive care systems, and wearable monitors has enhanced patient care\nbut introduced serious security risks. Cyberattacks on these devices can lead\nto life threatening consequences, including surgical errors, equipment failure,\nand data breaches. While the ITU IMT 2030 vision highlights 6G's transformative\nrole in healthcare through AI and cloud integration, it also raises new\nsecurity concerns. This paper explores how explainable AI techniques like SHAP,\nLIME, and DiCE can uncover vulnerabilities, strengthen defenses, and improve\ntrust and transparency in 6G enabled healthcare. We support our approach with\nexperimental analysis and highlight promising results."
    },
    {
        "date": "2025-05",
        "title": "VideoEval-Pro: Robust and Realistic Long Video Understanding Evaluation",
        "author": "Wentao Ma, Weiming Ren, Yiming Jia, Zhuofeng Li, Ping Nie, Ge Zhang, and Wenhu Chen",
        "link": "http://arxiv.org/abs/2505.14640v1",
        "abstract": "Large multimodal models (LMMs) have recently emerged as a powerful tool for\nlong video understanding (LVU), prompting the development of standardized LVU\nbenchmarks to evaluate their performance. However, our investigation reveals a\nrather sober lesson for existing LVU benchmarks. First, most existing\nbenchmarks rely heavily on multiple-choice questions (MCQs), whose evaluation\nresults are inflated due to the possibility of guessing the correct answer;\nSecond, a significant portion of questions in these benchmarks have strong\npriors to allow models to answer directly without even reading the input video.\nFor example, Gemini-1.5-Pro can achieve over 50\\% accuracy given a random frame\nfrom a long video on Video-MME. We also observe that increasing the number of\nframes does not necessarily lead to improvement on existing benchmarks, which\nis counterintuitive. As a result, the validity and robustness of current LVU\nbenchmarks are undermined, impeding a faithful assessment of LMMs' long-video\nunderstanding capability. To tackle this problem, we propose VideoEval-Pro, a\nrealistic LVU benchmark containing questions with open-ended short-answer,\nwhich truly require understanding the entire video. VideoEval-Pro assesses both\nsegment-level and full-video understanding through perception and reasoning\ntasks. By evaluating 21 proprietary and open-source video LMMs, we conclude the\nfollowing findings: (1) video LMMs show drastic performance ($>$25\\%) drops on\nopen-ended questions compared with MCQs; (2) surprisingly, higher MCQ scores do\nnot lead to higher open-ended scores on VideoEval-Pro; (3) compared to other\nMCQ benchmarks, VideoEval-Pro benefits more from increasing the number of input\nframes. Our results show that VideoEval-Pro offers a more realistic and\nreliable measure of long video understanding, providing a clearer view of\nprogress in this domain."
    },
    {
        "date": "2025-05",
        "title": "SSPS: Self-Supervised Positive Sampling for Robust Self-Supervised Speaker Verification",
        "author": "Theo Lepage, and Reda Dehak",
        "link": "http://arxiv.org/abs/2505.14561v1",
        "abstract": "Self-Supervised Learning (SSL) has led to considerable progress in Speaker\nVerification (SV). The standard framework uses same-utterance positive sampling\nand data-augmentation to generate anchor-positive pairs of the same speaker.\nThis is a major limitation, as this strategy primarily encodes channel\ninformation from the recording condition, shared by the anchor and positive. We\npropose a new positive sampling technique to address this bottleneck:\nSelf-Supervised Positive Sampling (SSPS). For a given anchor, SSPS aims to find\nan appropriate positive, i.e., of the same speaker identity but a different\nrecording condition, in the latent space using clustering assignments and a\nmemory queue of positive embeddings. SSPS improves SV performance for both\nSimCLR and DINO, reaching 2.57% and 2.53% EER, outperforming SOTA SSL methods\non VoxCeleb1-O. In particular, SimCLR-SSPS achieves a 58% EER reduction by\nlowering intra-speaker variance, providing comparable performance to DINO-SSPS."
    },
    {
        "date": "2025-05",
        "title": "From nuclear safety to LLM security: Applying non-probabilistic risk management strategies to build safe and secure LLM-powered systems",
        "author": "Alexander Gutfraind, and Vicki Bier",
        "link": "http://arxiv.org/abs/2505.17084v1",
        "abstract": "Large language models (LLMs) offer unprecedented and growing capabilities,\nbut also introduce complex safety and security challenges that resist\nconventional risk management. While conventional probabilistic risk analysis\n(PRA) requires exhaustive risk enumeration and quantification, the novelty and\ncomplexity of these systems make PRA impractical, particularly against adaptive\nadversaries. Previous research found that risk management in various fields of\nengineering such as nuclear or civil engineering is often solved by generic\n(i.e. field-agnostic) strategies such as event tree analysis or robust designs.\nHere we show how emerging risks in LLM-powered systems could be met with 100+\nof these non-probabilistic strategies to risk management, including risks from\nadaptive adversaries. The strategies are divided into five categories and are\nmapped to LLM security (and AI safety more broadly). We also present an\nLLM-powered workflow for applying these strategies and other workflows suitable\nfor solution architects. Overall, these strategies could contribute (despite\nsome limitations) to security, safety and other dimensions of responsible AI."
    },
    {
        "date": "2025-05",
        "title": "Adverseness vs. Equilibrium: Exploring Graph Adversarial Resilience through Dynamic Equilibrium",
        "author": "Xinxin Fan, Wenxiong Chen, Mengfan Li, Wenqi Wei, and Ling Liu",
        "link": "http://arxiv.org/abs/2505.14463v1",
        "abstract": "Adversarial attacks to graph analytics are gaining increased attention. To\ndate, two lines of countermeasures have been proposed to resist various graph\nadversarial attacks from the perspectives of either graph per se or graph\nneural networks. Nevertheless, a fundamental question lies in whether there\nexists an intrinsic adversarial resilience state within a graph regime and how\nto find out such a critical state if exists. This paper contributes to tackle\nthe above research questions from three unique perspectives: i) we regard the\nprocess of adversarial learning on graph as a complex multi-object dynamic\nsystem, and model the behavior of adversarial attack; ii) we propose a\ngeneralized theoretical framework to show the existence of critical adversarial\nresilience state; and iii) we develop a condensed one-dimensional function to\ncapture the dynamic variation of graph regime under perturbations, and pinpoint\nthe critical state through solving the equilibrium point of dynamic system.\nMulti-facet experiments are conducted to show our proposed approach can\nsignificantly outperform the state-of-the-art defense methods under five\ncommonly-used real-world datasets and three representative attacks."
    },
    {
        "date": "2025-05",
        "title": "Investigating and Enhancing the Robustness of Large Multimodal Models Against Temporal Inconsistency",
        "author": "Jiafeng Liang, Shixin Jiang, Xuan Dong, Ning Wang, Zheng Chu, Hui Su, Jinlan Fu, Ming Liu, See-Kiong Ng, and Bing Qin",
        "link": "http://arxiv.org/abs/2505.14405v1",
        "abstract": "Large Multimodal Models (LMMs) have recently demonstrated impressive\nperformance on general video comprehension benchmarks. Nevertheless, for\nbroader applications, the robustness of their temporal analysis capability\nneeds to be thoroughly investigated yet predominantly ignored. Motivated by\nthis, we propose a novel temporal robustness benchmark (TemRobBench), which\nintroduces temporal inconsistency perturbations separately at the visual and\ntextual modalities to assess the robustness of models. We evaluate 16\nmainstream LMMs and find that they exhibit over-reliance on prior knowledge and\ntextual context in adversarial environments, while ignoring the actual temporal\ndynamics in the video. To mitigate this issue, we design panoramic direct\npreference optimization (PanoDPO), which encourages LMMs to incorporate both\nvisual and linguistic feature preferences simultaneously. Experimental results\nshow that PanoDPO can effectively enhance the model's robustness and\nreliability in temporal analysis."
    },
    {
        "date": "2025-05",
        "title": "Is Your Prompt Safe? Investigating Prompt Injection Attacks Against Open-Source LLMs",
        "author": "Jiawen Wang, Pritha Gupta, Ivan Habernal, and Eyke H\u00fcllermeier",
        "link": "http://arxiv.org/abs/2505.14368v1",
        "abstract": "Recent studies demonstrate that Large Language Models (LLMs) are vulnerable\nto different prompt-based attacks, generating harmful content or sensitive\ninformation. Both closed-source and open-source LLMs are underinvestigated for\nthese attacks. This paper studies effective prompt injection attacks against\nthe $\\mathbf{14}$ most popular open-source LLMs on five attack benchmarks.\nCurrent metrics only consider successful attacks, whereas our proposed Attack\nSuccess Probability (ASP) also captures uncertainty in the model's response,\nreflecting ambiguity in attack feasibility. By comprehensively analyzing the\neffectiveness of prompt injection attacks, we propose a simple and effective\nhypnotism attack; results show that this attack causes aligned language models,\nincluding Stablelm2, Mistral, Openchat, and Vicuna, to generate objectionable\nbehaviors, achieving around $90$% ASP. They also indicate that our ignore\nprefix attacks can break all $\\mathbf{14}$ open-source LLMs, achieving over\n$60$% ASP on a multi-categorical dataset. We find that moderately well-known\nLLMs exhibit higher vulnerability to prompt injection attacks, highlighting the\nneed to raise public awareness and prioritize efficient mitigation strategies."
    },
    {
        "date": "2025-05",
        "title": "Vulnerability of Transfer-Learned Neural Networks to Data Reconstruction Attacks in Small-Data Regime",
        "author": "Tomasz Maci\u0105\u017cek, and Robert Allison",
        "link": "http://arxiv.org/abs/2505.14323v1",
        "abstract": "Training data reconstruction attacks enable adversaries to recover portions\nof a released model's training data. We consider the attacks where a\nreconstructor neural network learns to invert the (random) mapping between\ntraining data and model weights. Prior work has shown that an informed\nadversary with access to released model's weights and all but one training data\npoint can achieve high-quality reconstructions in this way. However,\ndifferential privacy can defend against such an attack with little to no loss\nin model's utility when the amount of training data is sufficiently large. In\nthis work we consider a more realistic adversary who only knows the\ndistribution from which a small training dataset has been sampled and who\nattacks a transfer-learned neural network classifier that has been trained on\nthis dataset. We exhibit an attack that works in this realistic threat model\nand demonstrate that in the small-data regime it cannot be defended against by\nDP-SGD without severely damaging the classifier accuracy. This raises\nsignificant concerns about the use of such transfer-learned classifiers when\nprotection of training-data is paramount. We demonstrate the effectiveness and\nrobustness of our attack on VGG, EfficientNet and ResNet image classifiers\ntransfer-learned on MNIST, CIFAR-10 and CelebA respectively. Additionally, we\npoint out that the commonly used (true-positive) reconstruction success rate\nmetric fails to reliably quantify the actual reconstruction effectiveness.\nInstead, we make use of the Neyman-Pearson lemma to construct the receiver\noperating characteristic curve and consider the associated true-positive\nreconstruction rate at a fixed level of the false-positive reconstruction rate."
    },
    {
        "date": "2025-05",
        "title": "Exploring Jailbreak Attacks on LLMs through Intent Concealment and Diversion",
        "author": "Tiehan Cui, Yanxu Mao, Peipei Liu, Congying Liu, and Datao You",
        "link": "http://arxiv.org/abs/2505.14316v1",
        "abstract": "Although large language models (LLMs) have achieved remarkable advancements,\ntheir security remains a pressing concern. One major threat is jailbreak\nattacks, where adversarial prompts bypass model safeguards to generate harmful\nor objectionable content. Researchers study jailbreak attacks to understand\nsecurity and robustness of LLMs. However, existing jailbreak attack methods\nface two main challenges: (1) an excessive number of iterative queries, and (2)\npoor generalization across models. In addition, recent jailbreak evaluation\ndatasets focus primarily on question-answering scenarios, lacking attention to\ntext generation tasks that require accurate regeneration of toxic content. To\ntackle these challenges, we propose two contributions: (1) ICE, a novel\nblack-box jailbreak method that employs Intent Concealment and divErsion to\neffectively circumvent security constraints. ICE achieves high attack success\nrates (ASR) with a single query, significantly improving efficiency and\ntransferability across different models. (2) BiSceneEval, a comprehensive\ndataset designed for assessing LLM robustness in question-answering and\ntext-generation tasks. Experimental results demonstrate that ICE outperforms\nexisting jailbreak techniques, revealing critical vulnerabilities in current\ndefense mechanisms. Our findings underscore the necessity of a hybrid security\nstrategy that integrates predefined security mechanisms with real-time semantic\ndecomposition to enhance the security of LLMs."
    },
    {
        "date": "2025-05",
        "title": "AquaSignal: An Integrated Framework for Robust Underwater Acoustic Analysis",
        "author": "Eirini Panteli, Paulo E. Santos, and Nabil Humphrey",
        "link": "http://arxiv.org/abs/2505.14285v1",
        "abstract": "This paper presents AquaSignal, a modular and scalable pipeline for\npreprocessing, denoising, classification, and novelty detection of underwater\nacoustic signals. Designed to operate effectively in noisy and dynamic marine\nenvironments, AquaSignal integrates state-of-the-art deep learning\narchitectures to enhance the reliability and accuracy of acoustic signal\nanalysis. The system is evaluated on a combined dataset from the Deepship and\nOcean Networks Canada (ONC) benchmarks, providing a diverse set of real-world\nunderwater scenarios. AquaSignal employs a U-Net architecture for denoising, a\nResNet18 convolutional neural network for classifying known acoustic events,\nand an AutoEncoder-based model for unsupervised detection of novel or anomalous\nsignals. To our knowledge, this is the first comprehensive study to apply and\nevaluate this combination of techniques on maritime vessel acoustic data.\nExperimental results show that AquaSignal improves signal clarity and task\nperformance, achieving 71% classification accuracy and 91% accuracy in novelty\ndetection. Despite slightly lower classification performance compared to some\nstate-of-the-art models, differences in data partitioning strategies limit\ndirect comparisons. Overall, AquaSignal demonstrates strong potential for\nreal-time underwater acoustic monitoring in scientific, environmental, and\nmaritime domains."
    },
    {
        "date": "2025-05",
        "title": "YESciEval: Robust LLM-as-a-Judge for Scientific Question Answering",
        "author": "Jennifer D'Souza, Hamed Babaei Giglou, and Quentin M\u00fcnch",
        "link": "http://arxiv.org/abs/2505.14279v1",
        "abstract": "Large Language Models (LLMs) drive scientific question-answering on modern\nsearch engines, yet their evaluation robustness remains underexplored. We\nintroduce YESciEval, an open-source framework that combines fine-grained\nrubric-based assessment with reinforcement learning to mitigate optimism bias\nin LLM evaluators. We release multidisciplinary scienceQ&A datasets, including\nadversarial variants, with evaluation scores from multiple LLMs. Independent of\nproprietary models and human feedback, our approach enables scalable, cost-free\nevaluation. By advancing reliable LLM-as-a-judge models, this work supports AI\nalignment and fosters robust, transparent evaluation essential for scientific\ninquiry and artificial general intelligence."
    },
    {
        "date": "2025-05",
        "title": "Embedded Mean Field Reinforcement Learning for Perimeter-defense Game",
        "author": "Li Wang, Xin Yu, Xuxin Lv, Gangzheng Ai, and Wenjun Wu",
        "link": "http://arxiv.org/abs/2505.14209v1",
        "abstract": "With the rapid advancement of unmanned aerial vehicles (UAVs) and missile\ntechnologies, perimeter-defense game between attackers and defenders for the\nprotection of critical regions have become increasingly complex and\nstrategically significant across a wide range of domains. However, existing\nstudies predominantly focus on small-scale, simplified two-dimensional\nscenarios, often overlooking realistic environmental perturbations, motion\ndynamics, and inherent heterogeneity--factors that pose substantial challenges\nto real-world applicability. To bridge this gap, we investigate large-scale\nheterogeneous perimeter-defense game in a three-dimensional setting,\nincorporating realistic elements such as motion dynamics and wind fields. We\nderive the Nash equilibrium strategies for both attackers and defenders,\ncharacterize the victory regions, and validate our theoretical findings through\nextensive simulations. To tackle large-scale heterogeneous control challenges\nin defense strategies, we propose an Embedded Mean-Field Actor-Critic (EMFAC)\nframework. EMFAC leverages representation learning to enable high-level action\naggregation in a mean-field manner, supporting scalable coordination among\ndefenders. Furthermore, we introduce a lightweight agent-level attention\nmechanism based on reward representation, which selectively filters\nobservations and mean-field information to enhance decision-making efficiency\nand accelerate convergence in large-scale tasks. Extensive simulations across\nvarying scales demonstrate the effectiveness and adaptability of EMFAC, which\noutperforms established baselines in both convergence speed and overall\nperformance. To further validate practicality, we test EMFAC in small-scale\nreal-world experiments and conduct detailed analyses, offering deeper insights\ninto the framework's effectiveness in complex scenarios."
    },
    {
        "date": "2025-05",
        "title": "Cheaper, Better, Faster, Stronger: Robust Text-to-SQL without Chain-of-Thought or Fine-Tuning",
        "author": "Yusuf Denizay D\u00f6nder, Derek Hommel, Andrea W Wen-Yi, David Mimno, and Unso Eun Seo Jo",
        "link": "http://arxiv.org/abs/2505.14174v1",
        "abstract": "LLMs are effective at code generation tasks like text-to-SQL, but is it worth\nthe cost? Many state-of-the-art approaches use non-task-specific LLM techniques\nincluding Chain-of-Thought (CoT), self-consistency, and fine-tuning. These\nmethods can be costly at inference time, sometimes requiring over a hundred LLM\ncalls with reasoning, incurring average costs of up to \\$0.46 per query, while\nfine-tuning models can cost thousands of dollars. We introduce \"N-rep\"\nconsistency, a more cost-efficient text-to-SQL approach that achieves similar\nBIRD benchmark scores as other more expensive methods, at only \\$0.039 per\nquery. N-rep leverages multiple representations of the same schema input to\nmitigate weaknesses in any single representation, making the solution more\nrobust and allowing the use of smaller and cheaper models without any reasoning\nor fine-tuning. To our knowledge, N-rep is the best-performing text-to-SQL\napproach in its cost range."
    },
    {
        "date": "2025-05",
        "title": "Memory Assignment for Finite-Memory Strategies in Adversarial Patrolling Games",
        "author": "Vojt\u011bch K\u016fr, V\u00edt Musil, and Vojt\u011bch \u0158eh\u00e1k",
        "link": "http://arxiv.org/abs/2505.14137v1",
        "abstract": "Adversarial Patrolling games form a subclass of Security games where a\nDefender moves between locations, guarding vulnerable targets. The main\nalgorithmic problem is constructing a strategy for the Defender that minimizes\nthe worst damage an Attacker can cause. We focus on the class of finite-memory\n(also known as regular) Defender's strategies that experimentally outperformed\nother competing classes. A finite-memory strategy can be seen as a positional\nstrategy on a finite set of states. Each state consists of a pair of a location\nand a certain integer value--called memory. Existing algorithms improve the\ntransitional probabilities between the states but require that the available\nmemory size itself is assigned at each location manually. Choosing the right\nmemory assignment is a well-known open and hard problem that hinders the\nusability of finite-memory strategies. We solve this issue by developing a\ngeneral method that iteratively changes the memory assignment. Our algorithm\ncan be used in connection with \\emph{any} black-box strategy optimization tool.\nWe evaluate our method on various experiments and show its robustness by\nsolving instances of various patrolling models."
    },
    {
        "date": "2025-05",
        "title": "AudioJailbreak: Jailbreak Attacks against End-to-End Large Audio-Language Models",
        "author": "Guangke Chen, Fu Song, Zhe Zhao, Xiaojun Jia, Yang Liu, Yanchen Qiao, and Weizhe Zhang",
        "link": "http://arxiv.org/abs/2505.14103v2",
        "abstract": "Jailbreak attacks to Large audio-language models (LALMs) are studied\nrecently, but they achieve suboptimal effectiveness, applicability, and\npracticability, particularly, assuming that the adversary can fully manipulate\nuser prompts. In this work, we first conduct an extensive experiment showing\nthat advanced text jailbreak attacks cannot be easily ported to end-to-end\nLALMs via text-to speech (TTS) techniques. We then propose AudioJailbreak, a\nnovel audio jailbreak attack, featuring (1) asynchrony: the jailbreak audio\ndoes not need to align with user prompts in the time axis by crafting suffixal\njailbreak audios; (2) universality: a single jailbreak perturbation is\neffective for different prompts by incorporating multiple prompts into\nperturbation generation; (3) stealthiness: the malicious intent of jailbreak\naudios will not raise the awareness of victims by proposing various intent\nconcealment strategies; and (4) over-the-air robustness: the jailbreak audios\nremain effective when being played over the air by incorporating the\nreverberation distortion effect with room impulse response into the generation\nof the perturbations. In contrast, all prior audio jailbreak attacks cannot\noffer asynchrony, universality, stealthiness, or over-the-air robustness.\nMoreover, AudioJailbreak is also applicable to the adversary who cannot fully\nmanipulate user prompts, thus has a much broader attack scenario. Extensive\nexperiments with thus far the most LALMs demonstrate the high effectiveness of\nAudioJailbreak. We highlight that our work peeks into the security implications\nof audio jailbreak attacks against LALMs, and realistically fosters improving\ntheir security robustness. The implementation and audio samples are available\nat our website https://audiojailbreak.github.io/AudioJailbreak."
    },
    {
        "date": "2025-05",
        "title": "Adversarially Pretrained Transformers may be Universally Robust In-Context Learners",
        "author": "Soichiro Kumano, Hiroshi Kera, and Toshihiko Yamasaki",
        "link": "http://arxiv.org/abs/2505.14042v1",
        "abstract": "Adversarial training is one of the most effective adversarial defenses, but\nit incurs a high computational cost. In this study, we show that transformers\nadversarially pretrained on diverse tasks can serve as robust foundation models\nand eliminate the need for adversarial training in downstream tasks.\nSpecifically, we theoretically demonstrate that through in-context learning, a\nsingle adversarially pretrained transformer can robustly generalize to multiple\nunseen tasks without any additional training, i.e., without any parameter\nupdates. This robustness stems from the model's focus on robust features and\nits resistance to attacks that exploit non-predictive features. Besides these\npositive findings, we also identify several limitations. Under certain\nconditions (though unrealistic), no universally robust single-layer\ntransformers exist. Moreover, robust transformers exhibit an\naccuracy--robustness trade-off and require a large number of in-context\ndemonstrations. The code is available at\nhttps://github.com/s-kumano/universally-robust-in-context-learner."
    },
    {
        "date": "2025-05",
        "title": "FedGraM: Defending Against Untargeted Attacks in Federated Learning via Embedding Gram Matrix",
        "author": "Di Wu, Qian Li, Heng Yang, and Yong Han",
        "link": "http://arxiv.org/abs/2505.14024v1",
        "abstract": "Federated Learning (FL) enables geographically distributed clients to\ncollaboratively train machine learning models by sharing only their local\nmodels, ensuring data privacy. However, FL is vulnerable to untargeted attacks\nthat aim to degrade the global model's performance on the underlying data\ndistribution. Existing defense mechanisms attempt to improve FL's resilience\nagainst such attacks, but their effectiveness is limited in practical FL\nenvironments due to data heterogeneity. On the contrary, we aim to detect and\nremove the attacks to mitigate their impact. Generalization contribution plays\na crucial role in distinguishing untargeted attacks. Our observations indicate\nthat, with limited data, the divergence between embeddings representing\ndifferent classes provides a better measure of generalization than direct\naccuracy. In light of this, we propose a novel robust aggregation method,\nFedGraM, designed to defend against untargeted attacks in FL. The server\nmaintains an auxiliary dataset containing one sample per class to support\naggregation. This dataset is fed to the local models to extract embeddings.\nThen, the server calculates the norm of the Gram Matrix of the embeddings for\neach local model. The norm serves as an indicator of each model's inter-class\nseparation capability in the embedding space. FedGraM identifies and removes\npotentially malicious models by filtering out those with the largest norms,\nthen averages the remaining local models to form the global model. We conduct\nextensive experiments to evaluate the performance of FedGraM. Our empirical\nresults show that with limited data samples used to construct the auxiliary\ndataset, FedGraM achieves exceptional performance, outperforming\nstate-of-the-art defense methods."
    },
    {
        "date": "2025-05",
        "title": "Adversarial Training from Mean Field Perspective",
        "author": "Soichiro Kumano, Hiroshi Kera, and Toshihiko Yamasaki",
        "link": "http://arxiv.org/abs/2505.14021v1",
        "abstract": "Although adversarial training is known to be effective against adversarial\nexamples, training dynamics are not well understood. In this study, we present\nthe first theoretical analysis of adversarial training in random deep neural\nnetworks without any assumptions on data distributions. We introduce a new\ntheoretical framework based on mean field theory, which addresses the\nlimitations of existing mean field-based approaches. Based on this framework,\nwe derive (empirically tight) upper bounds of $\\ell_q$ norm-based adversarial\nloss with $\\ell_p$ norm-based adversarial examples for various values of $p$\nand $q$. Moreover, we prove that networks without shortcuts are generally not\nadversarially trainable and that adversarial training reduces network capacity.\nWe also show that network width alleviates these issues. Furthermore, we\npresent the various impacts of the input and output dimensions on the upper\nbounds and time evolution of the weight variance."
    },
    {
        "date": "2025-05",
        "title": "Streamlining HTTP Flooding Attack Detection through Incremental Feature Selection",
        "author": "Upasana Sarmah, Parthajit Borah, and D. K. Bhattacharyya",
        "link": "http://arxiv.org/abs/2505.17077v1",
        "abstract": "Applications over the Web primarily rely on the HTTP protocol to transmit web\npages to and from systems. There are a variety of application layer protocols,\nbut among all, HTTP is the most targeted because of its versatility and ease of\nintegration with online services. The attackers leverage the fact that by\ndefault no detection system blocks any HTTP traffic. Thus, by exploiting such\ncharacteristics of the protocol, attacks are launched against web applications.\nHTTP flooding attacks are one such attack in the application layer of the OSI\nmodel. In this paper, a method for the detection of such an attack is proposed.\nThe heart of the detection method is an incremental feature subset selection\nmethod based on mutual information and correlation. INFS-MICC helps in\nidentifying a subset of highly relevant and independent feature subset so as to\ndetect HTTP Flooding attacks with best possible classification performance in\nnear-real time."
    },
    {
        "date": "2025-05",
        "title": "D4+: Emergent Adversarial Driving Maneuvers with Approximate Functional Optimization",
        "author": "Diego Ortiz Barbosa, Luis Burbano, Carlos Hernandez, Zengxiang Lei, Younghee Park, Satish Ukkusuri, and Alvaro A Cardenas",
        "link": "http://arxiv.org/abs/2505.13942v1",
        "abstract": "Intelligent mechanisms implemented in autonomous vehicles, such as proactive\ndriving assist and collision alerts, reduce traffic accidents. However,\nverifying their correct functionality is difficult due to complex interactions\nwith the environment. This problem is exacerbated in adversarial environments,\nwhere an attacker can control the environment surrounding autonomous vehicles\nto exploit vulnerabilities.\n  To preemptively identify vulnerabilities in these systems, in this paper, we\nimplement a scenario-based framework with a formal method to identify the\nimpact of malicious drivers interacting with autonomous vehicles. The\nformalization of the evaluation requirements utilizes metric temporal logic\n(MTL) to identify a safety condition that we want to test. Our goal is to find,\nthrough a rigorous testing approach, any trace that violates this MTL safety\nspecification. Our results can help designers identify the range of safe\noperational behaviors that prevent malicious drivers from exploiting the\nautonomous features of modern vehicles."
    },
    {
        "date": "2025-05",
        "title": "The Hidden Dangers of Outdated Software: A Cyber Security Perspective",
        "author": "Gogulakrishnan Thiyagarajan, Vinay Bist, and Prabhudarshi Nayak",
        "link": "http://arxiv.org/abs/2505.13922v1",
        "abstract": "Outdated software remains a potent and underappreciated menace in 2025's\ncybersecurity environment, exposing systems to a broad array of threats,\nincluding ransomware, data breaches, and operational outages that can have\ndevastating and far-reaching impacts. This essay explores the unseen threats of\ncyberattacks by presenting robust statistical information, including the\nstaggering reality that 32% of cyberattacks exploit unpatched software\nvulnerabilities, based on a 2025 TechTarget survey. Furthermore, it discusses\nreal case studies, including the MOVEit breach in 2023 and the Log4Shell breach\nin 2021, both of which illustrate the catastrophic consequences of failing to\nperform software updates. The article offers a detailed analysis of the nature\nof software vulnerabilities, the underlying reasons for user resistance to\npatches, and organizational barriers that compound the issue. Furthermore, it\nsuggests actionable solutions, including automation and awareness campaigns, to\naddress these shortcomings. Apart from this, the paper also talks of trends\nsuch as AI-driven vulnerability patching and legal consequences of\nnon-compliance under laws like HIPAA, thus providing a futuristic outlook on\nhow such advancements may define future defenses. Supplemented by tables like\none detailing trends in vulnerability and a graph illustrating technology\nadoption, this report showcases the pressing demand for anticipatory update\nstrategies to safeguard digital ecosystems against the constantly evolving\nthreats that characterize the modern cyber landscape. As it stands, it is a\nvery useful document for practitioners, policymakers, and researchers."
    },
    {
        "date": "2025-05",
        "title": "ShortcutProbe: Probing Prediction Shortcuts for Learning Robust Models",
        "author": "Guangtao Zheng, Wenqian Ye, and Aidong Zhang",
        "link": "http://arxiv.org/abs/2505.13910v1",
        "abstract": "Deep learning models often achieve high performance by inadvertently learning\nspurious correlations between targets and non-essential features. For example,\nan image classifier may identify an object via its background that spuriously\ncorrelates with it. This prediction behavior, known as spurious bias, severely\ndegrades model performance on data that lacks the learned spurious\ncorrelations. Existing methods on spurious bias mitigation typically require a\nvariety of data groups with spurious correlation annotations called group\nlabels. However, group labels require costly human annotations and often fail\nto capture subtle spurious biases such as relying on specific pixels for\npredictions. In this paper, we propose a novel post hoc spurious bias\nmitigation framework without requiring group labels. Our framework, termed\nShortcutProbe, identifies prediction shortcuts that reflect potential\nnon-robustness in predictions in a given model's latent space. The model is\nthen retrained to be invariant to the identified prediction shortcuts for\nimproved robustness. We theoretically analyze the effectiveness of the\nframework and empirically demonstrate that it is an efficient and practical\ntool for improving a model's robustness to spurious bias on diverse datasets."
    },
    {
        "date": "2025-05",
        "title": "PandaGuard: Systematic Evaluation of LLM Safety against Jailbreaking Attacks",
        "author": "Guobin Shen, Dongcheng Zhao, Linghao Feng, Xiang He, Jihang Wang, Sicheng Shen, Haibo Tong, Yiting Dong, Jindong Li, Xiang Zheng, and Yi Zeng",
        "link": "http://arxiv.org/abs/2505.13862v2",
        "abstract": "Large language models (LLMs) have achieved remarkable capabilities but remain\nvulnerable to adversarial prompts known as jailbreaks, which can bypass safety\nalignment and elicit harmful outputs. Despite growing efforts in LLM safety\nresearch, existing evaluations are often fragmented, focused on isolated attack\nor defense techniques, and lack systematic, reproducible analysis. In this\nwork, we introduce PandaGuard, a unified and modular framework that models LLM\njailbreak safety as a multi-agent system comprising attackers, defenders, and\njudges. Our framework implements 19 attack methods and 12 defense mechanisms,\nalong with multiple judgment strategies, all within a flexible plugin\narchitecture supporting diverse LLM interfaces, multiple interaction modes, and\nconfiguration-driven experimentation that enhances reproducibility and\npractical deployment. Built on this framework, we develop PandaBench, a\ncomprehensive benchmark that evaluates the interactions between these\nattack/defense methods across 49 LLMs and various judgment approaches,\nrequiring over 3 billion tokens to execute. Our extensive evaluation reveals\nkey insights into model vulnerabilities, defense cost-performance trade-offs,\nand judge consistency. We find that no single defense is optimal across all\ndimensions and that judge disagreement introduces nontrivial variance in safety\nassessments. We release the code, configurations, and evaluation results to\nsupport transparent and reproducible research in LLM safety."
    },
    {
        "date": "2025-05",
        "title": "hChain 4.0: A Secure and Scalable Permissioned Blockchain for EHR Management in Smart Healthcare",
        "author": "Musharraf N. Alruwaill, Saraju P. Mohanty, and Elias Kougianos",
        "link": "http://arxiv.org/abs/2505.13861v1",
        "abstract": "The growing utilization of Internet of Medical Things (IoMT) devices,\nincluding smartwatches and wearable medical devices, has facilitated real-time\nhealth monitoring and data analysis to enhance healthcare outcomes. These\ngadgets necessitate improved security measures to safeguard sensitive health\ndata while tackling scalability issues in real-time settings. The proposed\nsystem, hChain 4.0, employs a permissioned blockchain to provide a secure and\nscalable data infrastructure designed to fulfill these needs. This stands in\ncontrast to conventional systems, which are vulnerable to security flaws or\nrely on public blockchains, constrained by scalability and expense. The\nproposed approach introduces a high-privacy method in which health data are\nencrypted using the Advanced Encryption Standard (AES) for time-efficient\nencryption, combined with Partial Homomorphic Encryption (PHE) to enable secure\ncomputations on encrypted data, thereby enhancing privacy. Moreover, it\nutilizes private channels that enable isolated communication and ledger between\nstakeholders, ensuring robust privacy while supporting collaborative\noperations. The proposed framework enables anonymized health data sharing for\nmedical research by pseudonymizing patient identity. Additionally, hChain 4.0\nincorporates Attribute-Based Access Control (ABAC) to provide secure electronic\nhealth record (EHR) sharing among authorized parties, where ABAC ensures\nfine-grained permission management vital for multi-organizational healthcare\nsettings. Experimental assessments indicate that the proposed approach achieves\nhigher scalability, cost-effectiveness, and validated security."
    },
    {
        "date": "2025-05",
        "title": "QUT-DV25: A Dataset for Dynamic Analysis of Next-Gen Software Supply Chain Attacks",
        "author": "Sk Tanzir Mehedi, Raja Jurdak, Chadni Islam, and Gowri Ramachandran",
        "link": "http://arxiv.org/abs/2505.13804v1",
        "abstract": "Securing software supply chains is a growing challenge due to the inadequacy\nof existing datasets in capturing the complexity of next-gen attacks, such as\nmultiphase malware execution, remote access activation, and dynamic payload\ngeneration. Existing datasets, which rely on metadata inspection and static\ncode analysis, are inadequate for detecting such attacks. This creates a\ncritical gap because these datasets do not capture what happens during and\nafter a package is installed. To address this gap, we present QUT-DV25, a\ndynamic analysis dataset specifically designed to support and advance research\non detecting and mitigating supply chain attacks within the Python Package\nIndex (PyPI) ecosystem. This dataset captures install and post-install-time\ntraces from 14,271 Python packages, of which 7,127 are malicious. The packages\nare executed in an isolated sandbox environment using an extended Berkeley\nPacket Filter (eBPF) kernel and user-level probes. It captures 36 real-time\nfeatures, that includes system calls, network traffic, resource usages,\ndirectory access patterns, dependency logs, and installation behaviors,\nenabling the study of next-gen attack vectors. ML analysis using the QUT-DV25\ndataset identified four malicious PyPI packages previously labeled as benign,\neach with thousands of downloads. These packages deployed covert remote access\nand multi-phase payloads, were reported to PyPI maintainers, and subsequently\nremoved. This highlights the practical value of QUT-DV25, as it outperforms\nreactive, metadata, and static datasets, offering a robust foundation for\ndeveloping and benchmarking advanced threat detection within the evolving\nsoftware supply chain ecosystem."
    },
    {
        "date": "2025-05",
        "title": "Multiple Proposer Transaction Fee Mechanism Design: Robust Incentives Against Censorship and Bribery",
        "author": "Aikaterini-Panagiota Stouka, Julian Ma, and Thomas Thiery",
        "link": "http://arxiv.org/abs/2505.13751v1",
        "abstract": "Censorship resistance is one of the core value proposition of blockchains. A\nrecurring design pattern aimed at providing censorship resistance is enabling\nmultiple proposers to contribute inputs into block construction. Notably,\nFork-Choice Enforced Inclusion Lists (FOCIL) is proposed to be included in\nEthereum. However, the current proposal relies on altruistic behavior, without\na Transaction Fee Mechanism (TFM). This study aims to address this gap by\nexploring how multiple proposers should be rewarded to incentivize censorship\nresistance. The main contribution of this work is the identification of TFMs\nthat ensure censorship resistance under bribery attacks, while also satisfying\nthe incentive compatibility properties of EIP-1559. We provide a concrete\npayment mechanism for FOCIL, along with generalizable contributions to the\nliterature by analyzing 1) incentive compatibility of TFMs in the presence of a\nbribing adversary, 2) TFMs in protocols with multiple phases of transaction\ninclusion, and 3) TFMs of protocols in which parties are uncertain about the\nbehavior and the possible bribe of others."
    },
    {
        "date": "2025-05",
        "title": "Policy-Driven World Model Adaptation for Robust Offline Model-based Reinforcement Learning",
        "author": "Jiayu Chen, Aravind Venugopal, and Jeff Schneider",
        "link": "http://arxiv.org/abs/2505.13709v1",
        "abstract": "Offline reinforcement learning (RL) offers a powerful paradigm for\ndata-driven control. Compared to model-free approaches, offline model-based RL\n(MBRL) explicitly learns a world model from a static dataset and uses it as a\nsurrogate simulator, improving data efficiency and enabling potential\ngeneralization beyond the dataset support. However, most existing offline MBRL\nmethods follow a two-stage training procedure: first learning a world model by\nmaximizing the likelihood of the observed transitions, then optimizing a policy\nto maximize its expected return under the learned model. This objective\nmismatch results in a world model that is not necessarily optimized for\neffective policy learning. Moreover, we observe that policies learned via\noffline MBRL often lack robustness during deployment, and small adversarial\nnoise in the environment can lead to significant performance degradation. To\naddress these, we propose a framework that dynamically adapts the world model\nalongside the policy under a unified learning objective aimed at improving\nrobustness. At the core of our method is a maximin optimization problem, which\nwe solve by innovatively utilizing Stackelberg learning dynamics. We provide\ntheoretical analysis to support our design and introduce computationally\nefficient implementations. We benchmark our algorithm on twelve noisy D4RL\nMuJoCo tasks and three stochastic Tokamak Control tasks, demonstrating its\nstate-of-the-art performance."
    },
    {
        "date": "2025-05",
        "title": "Robust learning of halfspaces under log-concave marginals",
        "author": "Jane Lange, and Arsen Vasilyan",
        "link": "http://arxiv.org/abs/2505.13708v1",
        "abstract": "We say that a classifier is \\emph{adversarially robust} to perturbations of\nnorm $r$ if, with high probability over a point $x$ drawn from the input\ndistribution, there is no point within distance $\\le r$ from $x$ that is\nclassified differently. The \\emph{boundary volume} is the probability that a\npoint falls within distance $r$ of a point with a different label. This work\nstudies the task of computationally efficient learning of hypotheses with small\nboundary volume, where the input is distributed as a subgaussian isotropic\nlog-concave distribution over $\\mathbb{R}^d$.\n  Linear threshold functions are adversarially robust; they have boundary\nvolume proportional to $r$. Such concept classes are efficiently learnable by\npolynomial regression, which produces a polynomial threshold function (PTF),\nbut PTFs in general may have boundary volume $\\Omega(1)$, even for $r \\ll 1$.\n  We give an algorithm that agnostically learns linear threshold functions and\nreturns a classifier with boundary volume $O(r+\\varepsilon)$ at radius of\nperturbation $r$. The time and sample complexity of\n$d^{\\tilde{O}(1/\\varepsilon^2)}$ matches the complexity of polynomial\nregression.\n  Our algorithm augments the classic approach of polynomial regression with\nthree additional steps: a) performing the $\\ell_1$-error regression under noise\nsensitivity constraints, b) a structured partitioning and rounding step that\nreturns a Boolean classifier with error $\\textsf{opt} + O(\\varepsilon)$ and\nnoise sensitivity $O(r+\\varepsilon)$ simultaneously, and c) a local corrector\nthat ``smooths'' a function with low noise sensitivity into a function that is\nadversarially robust."
    },
    {
        "date": "2025-05",
        "title": "R3: Robust Rubric-Agnostic Reward Models",
        "author": "David Anugraha, Zilu Tang, Lester James V. Miranda, Hanyang Zhao, Mohammad Rifqi Farhansyah, Garry Kuwanto, Derry Wijaya, and Genta Indra Winata",
        "link": "http://arxiv.org/abs/2505.13388v1",
        "abstract": "Reward models are essential for aligning language model outputs with human\npreferences, yet existing approaches often lack both controllability and\ninterpretability. These models are typically optimized for narrow objectives,\nlimiting their generalizability to broader downstream tasks. Moreover, their\nscalar outputs are difficult to interpret without contextual reasoning. To\naddress these limitations, we introduce R3, a novel reward modeling framework\nthat is rubric-agnostic, generalizable across evaluation dimensions, and\nprovides interpretable, reasoned score assignments. R3 enables more transparent\nand flexible evaluation of language models, supporting robust alignment with\ndiverse human values and use cases. Our models, data, and code are available as\nopen source at https://github.com/rubricreward/r3"
    },
    {
        "date": "2025-05",
        "title": "DynaNoise: Dynamic Probabilistic Noise Injection for Defending Against Membership Inference Attacks",
        "author": "Javad Forough, and Hamed Haddadi",
        "link": "http://arxiv.org/abs/2505.13362v1",
        "abstract": "Membership Inference Attacks (MIAs) pose a significant risk to the privacy of\ntraining datasets by exploiting subtle differences in model outputs to\ndetermine whether a particular data sample was used during training. These\nattacks can compromise sensitive information, especially in domains such as\nhealthcare and finance, where data privacy is paramount. Traditional mitigation\ntechniques, such as static differential privacy, rely on injecting a fixed\namount of noise during training or inference. However, this approach often\nleads to a detrimental trade-off: the noise may be insufficient to counter\nsophisticated attacks or, when increased, may substantially degrade model\nperformance. In this paper, we present DynaNoise, an adaptive approach that\ndynamically modulates noise injection based on query sensitivity. Our approach\nperforms sensitivity analysis using measures such as Shannon entropy to\nevaluate the risk associated with each query and adjusts the noise variance\naccordingly. A probabilistic smoothing step is then applied to renormalize the\nperturbed outputs, ensuring that the model maintains high accuracy while\neffectively obfuscating membership signals. We further propose an empirical\nmetric, the Membership Inference Defense Privacy-Utility Tradeoff (MIDPUT),\nwhich quantifies the balance between reducing attack success rates and\npreserving the target model's accuracy. Our extensive evaluation on several\nbenchmark datasets demonstrates that DynaNoise not only significantly reduces\nMIA success rates but also achieves up to a fourfold improvement in the MIDPUT\nmetric compared to the state-of-the-art. Moreover, DynaNoise maintains\ncompetitive model accuracy while imposing only marginal inference overhead,\nhighlighting its potential as an effective and efficient privacy defense\nagainst MIAs."
    },
    {
        "date": "2025-05",
        "title": "Recommender Systems for Democracy: Toward Adversarial Robustness in Voting Advice Applications",
        "author": "Fr\u00e9d\u00e9ric Berdoz, Dustin Brunner, Yann Vonlanthen, and Roger Wattenhofer",
        "link": "http://arxiv.org/abs/2505.13329v1",
        "abstract": "Voting advice applications (VAAs) help millions of voters understand which\npolitical parties or candidates best align with their views. This paper\nexplores the potential risks these applications pose to the democratic process\nwhen targeted by adversarial entities. In particular, we expose 11 manipulation\nstrategies and measure their impact using data from Switzerland's primary VAA,\nSmartvote, collected during the last two national elections. We find that\naltering application parameters, such as the matching method, can shift a\nparty's recommendation frequency by up to 105%. Cherry-picking questionnaire\nitems can increase party recommendation frequency by over 261%, while subtle\nchanges to parties' or candidates' responses can lead to a 248% increase. To\naddress these vulnerabilities, we propose adversarial robustness properties\nVAAs should satisfy, introduce empirical metrics for assessing the resilience\nof various matching methods, and suggest possible avenues for research toward\nmitigating the effect of manipulation. Our framework is key to ensuring secure\nand reliable AI-based VAAs poised to emerge in the near future."
    },
    {
        "date": "2025-05",
        "title": "Benchmarking Unified Face Attack Detection via Hierarchical Prompt Tuning",
        "author": "Ajian Liu, Haocheng Yuan, Xiao Guo, Hui Ma, Wanyi Zhuang, Changtao Miao, Yan Hong, Chuanbiao Song, Jun Lan, Qi Chu, Tao Gong, Yanyan Liang, Weiqiang Wang, Jun Wan, Xiaoming Liu, and Zhen Lei",
        "link": "http://arxiv.org/abs/2505.13327v2",
        "abstract": "Presentation Attack Detection and Face Forgery Detection are designed to\nprotect face data from physical media-based Presentation Attacks and digital\nediting-based DeepFakes respectively. But separate training of these two models\nmakes them vulnerable to unknown attacks and burdens deployment environments.\nThe lack of a Unified Face Attack Detection model to handle both types of\nattacks is mainly due to two factors. First, there's a lack of adequate\nbenchmarks for models to explore. Existing UAD datasets have limited attack\ntypes and samples, restricting the model's ability to address advanced threats.\nTo address this, we propose UniAttackDataPlus (UniAttackData+), the most\nextensive and sophisticated collection of forgery techniques to date. It\nincludes 2,875 identities and their 54 kinds of falsified samples, totaling\n697,347 videos. Second, there's a lack of a reliable classification criterion.\nCurrent methods try to find an arbitrary criterion within the same semantic\nspace, which fails when encountering diverse attacks. So, we present a novel\nVisual-Language Model-based Hierarchical Prompt Tuning Framework (HiPTune) that\nadaptively explores multiple classification criteria from different semantic\nspaces. We build a Visual Prompt Tree to explore various classification rules\nhierarchically. Then, by adaptively pruning the prompts, the model can select\nthe most suitable prompts to guide the encoder to extract discriminative\nfeatures at different levels in a coarse-to-fine way. Finally, to help the\nmodel understand the classification criteria in visual space, we propose a\nDynamically Prompt Integration module to project the visual prompts to the text\nencoder for more accurate semantics. Experiments on 12 datasets have shown the\npotential to inspire further innovations in the UAD field."
    },
    {
        "date": "2025-05",
        "title": "SVAFD: A Secure and Verifiable Co-Aggregation Protocol for Federated Distillation",
        "author": "Tian Wen, Sheng Sun, Yuwei Wang, Peiyan Chen, Zhiyuan Wu, Min Liu, and Bo Gao",
        "link": "http://arxiv.org/abs/2505.13319v2",
        "abstract": "Secure Aggregation (SA) is an indispensable component of Federated Learning\n(FL) that concentrates on privacy preservation while allowing for robust\naggregation. However, most SA designs rely heavily on the unrealistic\nassumption of homogeneous model architectures. Federated Distillation (FD),\nwhich aggregates locally computed logits instead of model parameters,\nintroduces a promising alternative for cooperative training in heterogeneous\nmodel settings. Nevertheless, we recognize two major challenges in implementing\nSA for FD. (i) Prior SA designs encourage a dominant server, who is solely\nresponsible for collecting, aggregating and distributing. Such central\nauthority facilitates server to forge aggregation proofs or collude to bypass\nthe claimed security guarantees; (ii) Existing SA, tailored for FL models,\noverlook the intrinsic properties of logits, making them unsuitable for FD.\n  To address these challenges, we propose SVAFD, the first SA protocol that is\nspecifically designed for FD. At a high level, SVAFD incorporates two\ninnovations: (i) a multilateral co-aggregation method tha redefines the\nresponsibilities of clients and server. Clients autonomously evaluate and\naggregate logits shares locally with a lightweight coding scheme, while the\nserver handles ciphertext decoding and performs the task of generating\nverification proofs; (ii) a quality-aware knowledge filtration method that\nfacilitates biased logits exclusion against poisoning attacks. Moreover, SVAFD\nis resilient to stragglers and colluding clients, making it well-suited for\ndynamic networks in real-world applications. We have implemented the SVAFD\nprototype over four emerging FD architectures and evaluated it against\npoisoning and inference attacks. Results demonstrate that SVAFD improves model\naccuracy, making it a significant step forward in secure and verifiable\naggregation for heterogeneous FL systems."
    },
    {
        "date": "2025-05",
        "title": "RECON: Robust symmetry discovery via Explicit Canonical Orientation Normalization",
        "author": "Alonso Urbano, David W. Romero, Max Zimmer, and Sebastian Pokutta",
        "link": "http://arxiv.org/abs/2505.13289v1",
        "abstract": "Real-world data often exhibits unknown or approximate symmetries, yet\nexisting equivariant networks must commit to a fixed transformation group prior\nto training, e.g., continuous $SO(2)$ rotations. This mismatch degrades\nperformance when the actual data symmetries differ from those in the\ntransformation group. We introduce RECON, a framework to discover each input's\nintrinsic symmetry distribution from unlabeled data. RECON leverages class-pose\ndecompositions and applies a data-driven normalization to align arbitrary\nreference frames into a common natural pose, yielding directly comparable and\ninterpretable symmetry descriptors. We demonstrate effective symmetry discovery\non 2D image benchmarks and -- for the first time -- extend it to 3D\ntransformation groups, paving the way towards more flexible equivariant\nmodeling."
    },
    {
        "date": "2025-05",
        "title": "FlowPure: Continuous Normalizing Flows for Adversarial Purification",
        "author": "Elias Collaert, Abel Rodr\u00edguez, Sander Joos, Lieven Desmet, and Vera Rimmer",
        "link": "http://arxiv.org/abs/2505.13280v1",
        "abstract": "Despite significant advancements in the area, adversarial robustness remains\na critical challenge in systems employing machine learning models. The removal\nof adversarial perturbations at inference time, known as adversarial\npurification, has emerged as a promising defense strategy. To achieve this,\nstate-of-the-art methods leverage diffusion models that inject Gaussian noise\nduring a forward process to dilute adversarial perturbations, followed by a\ndenoising step to restore clean samples before classification. In this work, we\npropose FlowPure, a novel purification method based on Continuous Normalizing\nFlows (CNFs) trained with Conditional Flow Matching (CFM) to learn mappings\nfrom adversarial examples to their clean counterparts. Unlike prior\ndiffusion-based approaches that rely on fixed noise processes, FlowPure can\nleverage specific attack knowledge to improve robustness under known threats,\nwhile also supporting a more general stochastic variant trained on Gaussian\nperturbations for settings where such knowledge is unavailable. Experiments on\nCIFAR-10 and CIFAR-100 demonstrate that our method outperforms state-of-the-art\npurification-based defenses in preprocessor-blind and white-box scenarios, and\ncan do so while fully preserving benign accuracy in the former. Moreover, our\nresults show that not only is FlowPure a highly effective purifier but it also\nholds a strong potential for adversarial detection, identifying\npreprocessor-blind PGD samples with near-perfect accuracy."
    },
    {
        "date": "2025-05",
        "title": "StarFT: Robust Fine-tuning of Zero-shot Models via Spuriosity Alignment",
        "author": "Younghyun Kim, Jongheon Jeong, Sangkyung Kwak, Kyungmin Lee, Juho Lee, and Jinwoo Shin",
        "link": "http://arxiv.org/abs/2505.13232v2",
        "abstract": "Learning robust representations from data often requires scale, which has led\nto the success of recent zero-shot models such as CLIP. However, the obtained\nrobustness can easily be deteriorated when these models are fine-tuned on other\ndownstream tasks (e.g., of smaller scales). Previous works often interpret this\nphenomenon in the context of domain shift, developing fine-tuning methods that\naim to preserve the original domain as much as possible. However, in a\ndifferent context, fine-tuned models with limited data are also prone to\nlearning features that are spurious to humans, such as background or texture.\nIn this paper, we propose StarFT (Spurious Textual Alignment Regularization), a\nnovel framework for fine-tuning zero-shot models to enhance robustness by\npreventing them from learning spuriosity. We introduce a regularization that\naligns the output distribution for spuriosity-injected labels with the original\nzero-shot model, ensuring that the model is not induced to extract irrelevant\nfeatures further from these descriptions. We leverage recent language models to\nget such spuriosity-injected labels by generating alternative textual\ndescriptions that highlight potentially confounding features. Extensive\nexperiments validate the robust generalization of StarFT and its emerging\nproperties: zero-shot group robustness and improved zero-shot classification.\nNotably, StarFT boosts both worst-group and average accuracy by 14.30% and\n3.02%, respectively, in the Waterbirds group shift scenario, where other robust\nfine-tuning baselines show even degraded performance."
    },
    {
        "date": "2025-05",
        "title": "Adversarial Testing in LLMs: Insights into Decision-Making Vulnerabilities",
        "author": "Lili Zhang, Haomiaomiao Wang, Long Cheng, Libao Deng, and Tomas Ward",
        "link": "http://arxiv.org/abs/2505.13195v1",
        "abstract": "As Large Language Models (LLMs) become increasingly integrated into\nreal-world decision-making systems, understanding their behavioural\nvulnerabilities remains a critical challenge for AI safety and alignment. While\nexisting evaluation metrics focus primarily on reasoning accuracy or factual\ncorrectness, they often overlook whether LLMs are robust to adversarial\nmanipulation or capable of using adaptive strategy in dynamic environments.\nThis paper introduces an adversarial evaluation framework designed to\nsystematically stress-test the decision-making processes of LLMs under\ninteractive and adversarial conditions. Drawing on methodologies from cognitive\npsychology and game theory, our framework probes how models respond in two\ncanonical tasks: the two-armed bandit task and the Multi-Round Trust Task.\nThese tasks capture key aspects of exploration-exploitation trade-offs, social\ncooperation, and strategic flexibility. We apply this framework to several\nstate-of-the-art LLMs, including GPT-3.5, GPT-4, Gemini-1.5, and DeepSeek-V3,\nrevealing model-specific susceptibilities to manipulation and rigidity in\nstrategy adaptation. Our findings highlight distinct behavioral patterns across\nmodels and emphasize the importance of adaptability and fairness recognition\nfor trustworthy AI deployment. Rather than offering a performance benchmark,\nthis work proposes a methodology for diagnosing decision-making weaknesses in\nLLM-based agents, providing actionable insights for alignment and safety\nresearch."
    },
    {
        "date": "2025-05",
        "title": "ARIW-Framework: Adaptive Robust Iterative Watermarking Framework",
        "author": "Shaowu Wu, Liting Zeng, Wei Lu, and Xiangyang Luo",
        "link": "http://arxiv.org/abs/2505.13101v1",
        "abstract": "With the rapid rise of large models, copyright protection for generated image\ncontent has become a critical security challenge. Although deep learning\nwatermarking techniques offer an effective solution for digital image copyright\nprotection, they still face limitations in terms of visual quality, robustness\nand generalization. To address these issues, this paper proposes an adaptive\nrobust iterative watermarking framework (ARIW-Framework) that achieves\nhigh-quality watermarked images while maintaining exceptional robustness and\ngeneralization performance. Specifically, we introduce an iterative approach to\noptimize the encoder for generating robust residuals. The encoder incorporates\nnoise layers and a decoder to compute robustness weights for residuals under\nvarious noise attacks. By employing a parallel optimization strategy, the\nframework enhances robustness against multiple types of noise attacks.\nFurthermore, we leverage image gradients to determine the embedding strength at\neach pixel location, significantly improving the visual quality of the\nwatermarked images. Extensive experiments demonstrate that the proposed method\nachieves superior visual quality while exhibiting remarkable robustness and\ngeneralization against noise attacks."
    },
    {
        "date": "2025-05",
        "title": "Cross-modal feature fusion for robust point cloud registration with ambiguous geometry",
        "author": "Zhaoyi Wang, Shengyu Huang, Jemil Avers Butt, Yuanzhou Cai, Matej Varga, and Andreas Wieser",
        "link": "http://arxiv.org/abs/2505.13088v1",
        "abstract": "Point cloud registration has seen significant advancements with the\napplication of deep learning techniques. However, existing approaches often\noverlook the potential of integrating radiometric information from RGB images.\nThis limitation reduces their effectiveness in aligning point clouds pairs,\nespecially in regions where geometric data alone is insufficient. When used\neffectively, radiometric information can enhance the registration process by\nproviding context that is missing from purely geometric data. In this paper, we\npropose CoFF, a novel Cross-modal Feature Fusion method that utilizes both\npoint cloud geometry and RGB images for pairwise point cloud registration.\nAssuming that the co-registration between point clouds and RGB images is\navailable, CoFF explicitly addresses the challenges where geometric information\nalone is unclear, such as in regions with symmetric similarity or planar\nstructures, through a two-stage fusion of 3D point cloud features and 2D image\nfeatures. It incorporates a cross-modal feature fusion module that assigns\npixel-wise image features to 3D input point clouds to enhance learned 3D point\nfeatures, and integrates patch-wise image features with superpoint features to\nimprove the quality of coarse matching. This is followed by a coarse-to-fine\nmatching module that accurately establishes correspondences using the fused\nfeatures. We extensively evaluate CoFF on four common datasets: 3DMatch,\n3DLoMatch, IndoorLRS, and the recently released ScanNet++ datasets. In\naddition, we assess CoFF on specific subset datasets containing geometrically\nambiguous cases. Our experimental results demonstrate that CoFF achieves\nstate-of-the-art registration performance across all benchmarks, including\nremarkable registration recalls of 95.9% and 81.6% on the widely-used 3DMatch\nand 3DLoMatch datasets, respectively...(Truncated to fit arXiv abstract length)"
    },
    {
        "date": "2025-05",
        "title": "OmniFC: Rethinking Federated Clustering via Lossless and Secure Distance Reconstruction",
        "author": "Jie Yan, Xin Liu, and Zhong-Yuan Zhang",
        "link": "http://arxiv.org/abs/2505.13071v1",
        "abstract": "Federated clustering (FC) aims to discover global cluster structures across\ndecentralized clients without sharing raw data, making privacy preservation a\nfundamental requirement. There are two critical challenges: (1) privacy leakage\nduring collaboration, and (2) robustness degradation due to aggregation of\nproxy information from non-independent and identically distributed (Non-IID)\nlocal data, leading to inaccurate or inconsistent global clustering. Existing\nsolutions typically rely on model-specific local proxies, which are sensitive\nto data heterogeneity and inherit inductive biases from their centralized\ncounterparts, thus limiting robustness and generality. We propose Omni\nFederated Clustering (OmniFC), a unified and model-agnostic framework.\nLeveraging Lagrange coded computing, our method enables clients to share only\nencoded data, allowing exact reconstruction of the global distance matrix--a\nfundamental representation of sample relationships--without leaking private\ninformation, even under client collusion. This construction is naturally\nresilient to Non-IID data distributions. This approach decouples FC from\nmodel-specific proxies, providing a unified extension mechanism applicable to\ndiverse centralized clustering methods. Theoretical analysis confirms both\nreconstruction fidelity and privacy guarantees, while comprehensive experiments\ndemonstrate OmniFC's superior robustness, effectiveness, and generality across\nvarious benchmarks compared to state-of-the-art methods. Code will be released."
    },
    {
        "date": "2025-05",
        "title": "Anti-Inpainting: A Proactive Defense against Malicious Diffusion-based Inpainters under Unknown Conditions",
        "author": "Yimao Guo, Zuomin Qu, Wei Lu, and Xiangyang Luo",
        "link": "http://arxiv.org/abs/2505.13023v1",
        "abstract": "As diffusion-based malicious image manipulation becomes increasingly\nprevalent, multiple proactive defense methods are developed to safeguard images\nagainst unauthorized tampering. However, most proactive defense methods only\ncan safeguard images against manipulation under known conditions, and fail to\nprotect images from manipulations guided by tampering conditions crafted by\nmalicious users. To tackle this issue, we propose Anti-Inpainting, a proactive\ndefense method that achieves adequate protection under unknown conditions\nthrough a triple mechanism to address this challenge. Specifically, a\nmulti-level deep feature extractor is presented to obtain intricate features\nduring the diffusion denoising process to improve protective effectiveness. We\ndesign multi-scale semantic-preserving data augmentation to enhance the\ntransferability of adversarial perturbations across unknown conditions by\nmulti-scale transformations while preserving semantic integrity. In addition,\nwe propose a selection-based distribution deviation optimization strategy to\nimprove the protection of adversarial perturbation against manipulation under\ndiverse random seeds. Extensive experiments indicate the proactive defensive\nperformance of Anti-Inpainting against diffusion-based inpainters guided by\nunknown conditions in InpaintGuardBench and CelebA-HQ. At the same time, we\nalso demonstrate the proposed approach's robustness under various image\npurification methods and its transferability across different versions of\ndiffusion models."
    },
    {
        "date": "2025-05",
        "title": "From Assistants to Adversaries: Exploring the Security Risks of Mobile LLM Agents",
        "author": "Liangxuan Wu, Chao Wang, Tianming Liu, Yanjie Zhao, and Haoyu Wang",
        "link": "http://arxiv.org/abs/2505.12981v2",
        "abstract": "The growing adoption of large language models (LLMs) has led to a new\nparadigm in mobile computing--LLM-powered mobile AI agents--capable of\ndecomposing and automating complex tasks directly on smartphones. However, the\nsecurity implications of these agents remain largely unexplored. In this paper,\nwe present the first comprehensive security analysis of mobile LLM agents,\nencompassing three representative categories: System-level AI Agents developed\nby original equipment manufacturers (e.g., YOYO Assistant), Third-party\nUniversal Agents (e.g., Zhipu AI AutoGLM), and Emerging Agent Frameworks (e.g.,\nAlibaba Mobile Agent). We begin by analyzing the general workflow of mobile\nagents and identifying security threats across three core capability\ndimensions: language-based reasoning, GUI-based interaction, and system-level\nexecution. Our analysis reveals 11 distinct attack surfaces, all rooted in the\nunique capabilities and interaction patterns of mobile LLM agents, and spanning\ntheir entire operational lifecycle. To investigate these threats in practice,\nwe introduce AgentScan, a semi-automated security analysis framework that\nsystematically evaluates mobile LLM agents across all 11 attack scenarios.\nApplying AgentScan to nine widely deployed agents, we uncover a concerning\ntrend: every agent is vulnerable to targeted attacks. In the most severe cases,\nagents exhibit vulnerabilities across eight distinct attack vectors. These\nattacks can cause behavioral deviations, privacy leakage, or even full\nexecution hijacking. Based on these findings, we propose a set of defensive\ndesign principles and practical recommendations for building secure mobile LLM\nagents. Our disclosures have received positive feedback from two major device\nvendors. Overall, this work highlights the urgent need for standardized\nsecurity practices in the fast-evolving landscape of LLM-driven mobile\nautomation."
    },
    {
        "date": "2025-05",
        "title": "RGNMR: A Gauss-Newton method for robust matrix completion with theoretical guarantees",
        "author": "Eilon Vaknin Laufer, and Boaz Nadler",
        "link": "http://arxiv.org/abs/2505.12919v1",
        "abstract": "Recovering a low rank matrix from a subset of its entries, some of which may\nbe corrupted, is known as the robust matrix completion (RMC) problem. Existing\nRMC methods have several limitations: they require a relatively large number of\nobserved entries; they may fail under overparametrization, when their assumed\nrank is higher than the correct one; and many of them fail to recover even\nmildly ill-conditioned matrices. In this paper we propose a novel RMC method,\ndenoted $\\texttt{RGNMR}$, which overcomes these limitations. $\\texttt{RGNMR}$\nis a simple factorization-based iterative algorithm, which combines a\nGauss-Newton linearization with removal of entries suspected to be outliers. On\nthe theoretical front, we prove that under suitable assumptions,\n$\\texttt{RGNMR}$ is guaranteed exact recovery of the underlying low rank\nmatrix. Our theoretical results improve upon the best currently known for\nfactorization-based methods. On the empirical front, we show via several\nsimulations the advantages of $\\texttt{RGNMR}$ over existing RMC methods, and\nin particular its ability to handle a small number of observed entries,\noverparameterization of the rank and ill-conditioned matrices."
    },
    {
        "date": "2025-05",
        "title": "Does Low Rank Adaptation Lead to Lower Robustness against Training-Time Attacks?",
        "author": "Zi Liang, Haibo Hu, Qingqing Ye, Yaxin Xiao, and Ronghua Li",
        "link": "http://arxiv.org/abs/2505.12871v1",
        "abstract": "Low rank adaptation (LoRA) has emerged as a prominent technique for\nfine-tuning large language models (LLMs) thanks to its superb efficiency gains\nover previous methods. While extensive studies have examined the performance\nand structural properties of LoRA, its behavior upon training-time attacks\nremain underexplored, posing significant security risks. In this paper, we\ntheoretically investigate the security implications of LoRA's low-rank\nstructure during fine-tuning, in the context of its robustness against data\npoisoning and backdoor attacks. We propose an analytical framework that models\nLoRA's training dynamics, employs the neural tangent kernel to simplify the\nanalysis of the training process, and applies information theory to establish\nconnections between LoRA's low rank structure and its vulnerability against\ntraining-time attacks. Our analysis indicates that LoRA exhibits better\nrobustness to backdoor attacks than full fine-tuning, while becomes more\nvulnerable to untargeted data poisoning due to its over-simplified information\ngeometry. Extensive experimental evaluations have corroborated our theoretical\nfindings."
    },
    {
        "date": "2025-05",
        "title": "Causality-Inspired Robustness for Nonlinear Models via Representation Learning",
        "author": "Marin \u0160ola, Peter B\u00fchlmann, and Xinwei Shen",
        "link": "http://arxiv.org/abs/2505.12868v1",
        "abstract": "Distributional robustness is a central goal of prediction algorithms due to\nthe prevalent distribution shifts in real-world data. The prediction model aims\nto minimize the worst-case risk among a class of distributions, a.k.a., an\nuncertainty set. Causality provides a modeling framework with a rigorous\nrobustness guarantee in the above sense, where the uncertainty set is\ndata-driven rather than pre-specified as in traditional distributional\nrobustness optimization. However, current causality-inspired robustness methods\npossess finite-radius robustness guarantees only in the linear settings, where\nthe causal relationships among the covariates and the response are linear. In\nthis work, we propose a nonlinear method under a causal framework by\nincorporating recent developments in identifiable representation learning and\nestablish a distributional robustness guarantee. To our best knowledge, this is\nthe first causality-inspired robustness method with such a finite-radius\nrobustness guarantee in nonlinear settings. Empirical validation of the\ntheoretical findings is conducted on both synthetic data and real-world\nsingle-cell data, also illustrating that finite-radius robustness is crucial."
    },
    {
        "date": "2025-05",
        "title": "Robust Multimodal Segmentation with Representation Regularization and Hybrid Prototype Distillation",
        "author": "Jiaqi Tan, Xu Zheng, and Yang Liu",
        "link": "http://arxiv.org/abs/2505.12861v1",
        "abstract": "Multi-modal semantic segmentation (MMSS) faces significant challenges in\nreal-world scenarios due to dynamic environments, sensor failures, and noise\ninterference, creating a gap between theoretical models and practical\nperformance. To address this, we propose a two-stage framework called\nRobustSeg, which enhances multi-modal robustness through two key components:\nthe Hybrid Prototype Distillation Module (HPDM) and the Representation\nRegularization Module (RRM). In the first stage, RobustSeg pre-trains a\nmulti-modal teacher model using complete modalities. In the second stage, a\nstudent model is trained with random modality dropout while learning from the\nteacher via HPDM and RRM. HPDM transforms features into compact prototypes,\nenabling cross-modal hybrid knowledge distillation and mitigating bias from\nmissing modalities. RRM reduces representation discrepancies between the\nteacher and student by optimizing functional entropy through the log-Sobolev\ninequality. Extensive experiments on three public benchmarks demonstrate that\nRobustSeg outperforms previous state-of-the-art methods, achieving improvements\nof +2.76%, +4.56%, and +0.98%, respectively. Code is available at:\nhttps://github.com/RobustSeg/RobustSeg."
    },
    {
        "date": "2025-05",
        "title": "FLTG: Byzantine-Robust Federated Learning via Angle-Based Defense and Non-IID-Aware Weighting",
        "author": "Yanhua Wen, Lu Ai, Gang Liu, Chuang Li, and Jianhao Wei",
        "link": "http://arxiv.org/abs/2505.12851v1",
        "abstract": "Byzantine attacks during model aggregation in Federated Learning (FL)\nthreaten training integrity by manipulating malicious clients' updates.\nExisting methods struggle with limited robustness under high malicious client\nratios and sensitivity to non-i.i.d. data, leading to degraded accuracy. To\naddress this, we propose FLTG, a novel aggregation algorithm integrating\nangle-based defense and dynamic reference selection. FLTG first filters clients\nvia ReLU-clipped cosine similarity, leveraging a server-side clean dataset to\nexclude misaligned updates. It then dynamically selects a reference client\nbased on the prior global model to mitigate non-i.i.d. bias, assigns\naggregation weights inversely proportional to angular deviations, and\nnormalizes update magnitudes to suppress malicious scaling. Evaluations across\ndatasets of varying complexity under five classic attacks demonstrate FLTG's\nsuperiority over state-of-the-art methods under extreme bias scenarios and\nsustains robustness with a higher proportion(over 50%) of malicious clients."
    },
    {
        "date": "2025-05",
        "title": "VLC Fusion: Vision-Language Conditioned Sensor Fusion for Robust Object Detection",
        "author": "Aditya Taparia, Noel Ngu, Mario Leiva, Joshua Shay Kricheli, John Corcoran, Nathaniel D. Bastian, Gerardo Simari, Paulo Shakarian, and Ransalu Senanayake",
        "link": "http://arxiv.org/abs/2505.12715v1",
        "abstract": "Although fusing multiple sensor modalities can enhance object detection\nperformance, existing fusion approaches often overlook subtle variations in\nenvironmental conditions and sensor inputs. As a result, they struggle to\nadaptively weight each modality under such variations. To address this\nchallenge, we introduce Vision-Language Conditioned Fusion (VLC Fusion), a\nnovel fusion framework that leverages a Vision-Language Model (VLM) to\ncondition the fusion process on nuanced environmental cues. By capturing\nhigh-level environmental context such as as darkness, rain, and camera\nblurring, the VLM guides the model to dynamically adjust modality weights based\non the current scene. We evaluate VLC Fusion on real-world autonomous driving\nand military target detection datasets that include image, LIDAR, and mid-wave\ninfrared modalities. Our experiments show that VLC Fusion consistently\noutperforms conventional fusion baselines, achieving improved detection\naccuracy in both seen and unseen scenarios."
    },
    {
        "date": "2025-05",
        "title": "Writing a Good Security Paper for ISSCC (2025)",
        "author": "Utsav Banerjee, Chiraag Juvekar, Yong Ki Lee, Leibo Liu, Sanu Mathew, Thomas Poeppelmann, Shreyas Sen, Takeshi Sugawara, Ingrid Verbauwhede, and Rabia Tugce Yazicigil",
        "link": "http://arxiv.org/abs/2505.12700v1",
        "abstract": "Security is increasingly more important in designing chips and systems based\non them, and the International Solid-State Circuits Conference (ISSCC), the\nleading conference for presenting advances in solid-state circuits and\nsemiconductor technology, is committed to hardware security by establishing the\nsecurity subcommittee since 2024. In the past two years, the authors of this\npaper reviewed submissions as members of the Security Subcommittee, a part of\nInternational Technical Program Committee (ITPC). This paper aims to encourage\nhigh-quality submissions to grow this field in the overall scope of the ISSCC."
    },
    {
        "date": "2025-05",
        "title": "Shielding Latent Face Representations From Privacy Attacks",
        "author": "Arjun Ramesh Kaushik, Bharat Chandra Yalavarthi, Arun Ross, Vishnu Boddeti, and Nalini Ratha",
        "link": "http://arxiv.org/abs/2505.12688v1",
        "abstract": "In today's data-driven analytics landscape, deep learning has become a\npowerful tool, with latent representations, known as embeddings, playing a\ncentral role in several applications. In the face analytics domain, such\nembeddings are commonly used for biometric recognition (e.g., face\nidentification). However, these embeddings, or templates, can inadvertently\nexpose sensitive attributes such as age, gender, and ethnicity. Leaking such\ninformation can compromise personal privacy and affect civil liberty and human\nrights. To address these concerns, we introduce a multi-layer protection\nframework for embeddings. It consists of a sequence of operations: (a)\nencrypting embeddings using Fully Homomorphic Encryption (FHE), and (b) hashing\nthem using irreversible feature manifold hashing. Unlike conventional\nencryption methods, FHE enables computations directly on encrypted data,\nallowing downstream analytics while maintaining strong privacy guarantees. To\nreduce the overhead of encrypted processing, we employ embedding compression.\nOur proposed method shields latent representations of sensitive data from\nleaking private attributes (such as age and gender) while retaining essential\nfunctional capabilities (such as face identification). Extensive experiments on\ntwo datasets using two face encoders demonstrate that our approach outperforms\nseveral state-of-the-art privacy protection methods."
    },
    {
        "date": "2025-05",
        "title": "RoVo: Robust Voice Protection Against Unauthorized Speech Synthesis with Embedding-Level Perturbations",
        "author": "Seungmin Kim, Sohee Park, Donghyun Kim, Jisu Lee, and Daeseon Choi",
        "link": "http://arxiv.org/abs/2505.12686v1",
        "abstract": "With the advancement of AI-based speech synthesis technologies such as Deep\nVoice, there is an increasing risk of voice spoofing attacks, including voice\nphishing and fake news, through unauthorized use of others' voices. Existing\ndefenses that inject adversarial perturbations directly into audio signals have\nlimited effectiveness, as these perturbations can easily be neutralized by\nspeech enhancement methods. To overcome this limitation, we propose RoVo\n(Robust Voice), a novel proactive defense technique that injects adversarial\nperturbations into high-dimensional embedding vectors of audio signals,\nreconstructing them into protected speech. This approach effectively defends\nagainst speech synthesis attacks and also provides strong resistance to speech\nenhancement models, which represent a secondary attack threat.\n  In extensive experiments, RoVo increased the Defense Success Rate (DSR) by\nover 70% compared to unprotected speech, across four state-of-the-art speech\nsynthesis models. Specifically, RoVo achieved a DSR of 99.5% on a commercial\nspeaker-verification API, effectively neutralizing speech synthesis attack.\nMoreover, RoVo's perturbations remained robust even under strong speech\nenhancement conditions, outperforming traditional methods. A user study\nconfirmed that RoVo preserves both naturalness and usability of protected\nspeech, highlighting its effectiveness in complex and evolving threat\nscenarios."
    },
    {
        "date": "2025-05",
        "title": "RoFL: Robust Fingerprinting of Language Models",
        "author": "Yun-Yun Tsai, Chuan Guo, Junfeng Yang, and Laurens van der Maaten",
        "link": "http://arxiv.org/abs/2505.12682v1",
        "abstract": "AI developers are releasing large language models (LLMs) under a variety of\ndifferent licenses. Many of these licenses restrict the ways in which the\nmodels or their outputs may be used. This raises the question how license\nviolations may be recognized. In particular, how can we identify that an API or\nproduct uses (an adapted version of) a particular LLM? We present a new method\nthat enable model developers to perform such identification via fingerprints:\nstatistical patterns that are unique to the developer's model and robust to\ncommon alterations of that model. Our method permits model identification in a\nblack-box setting using a limited number of queries, enabling identification of\nmodels that can only be accessed via an API or product. The fingerprints are\nnon-invasive: our method does not require any changes to the model during\ntraining, hence by design, it does not impact model quality. Empirically, we\nfind our method provides a high degree of robustness to common changes in the\nmodel or inference settings. In our experiments, it substantially outperforms\nprior art, including invasive methods that explicitly train watermarks into the\nmodel."
    },
    {
        "date": "2025-05",
        "title": "On the Mechanisms of Adversarial Data Augmentation for Robust and Adaptive Transfer Learning",
        "author": "Hana Satou, and Alan Mitkiy",
        "link": "http://arxiv.org/abs/2505.12681v1",
        "abstract": "Transfer learning across domains with distribution shift remains a\nfundamental challenge in building robust and adaptable machine learning\nsystems. While adversarial perturbations are traditionally viewed as threats\nthat expose model vulnerabilities, recent studies suggest that they can also\nserve as constructive tools for data augmentation. In this work, we\nsystematically investigate the role of adversarial data augmentation (ADA) in\nenhancing both robustness and adaptivity in transfer learning settings. We\nanalyze how adversarial examples, when used strategically during training,\nimprove domain generalization by enriching decision boundaries and reducing\noverfitting to source-domain-specific features. We further propose a unified\nframework that integrates ADA with consistency regularization and\ndomain-invariant representation learning. Extensive experiments across multiple\nbenchmark datasets -- including VisDA, DomainNet, and Office-Home --\ndemonstrate that our method consistently improves target-domain performance\nunder both unsupervised and few-shot domain adaptation settings. Our results\nhighlight a constructive perspective of adversarial learning, transforming\nperturbation from a destructive attack into a regularizing force for\ncross-domain transferability."
    },
    {
        "date": "2025-05",
        "title": "Know Or Not: a library for evaluating out-of-knowledge base robustness",
        "author": "Jessica Foo, Pradyumna Shyama Prasad, and Shaun Khoo",
        "link": "http://arxiv.org/abs/2505.13545v1",
        "abstract": "While the capabilities of large language models (LLMs) have progressed\nsignificantly, their use in high-stakes applications have been limited due to\nrisks of hallucination. One key approach in reducing hallucination is\nretrieval-augmented generation (RAG), but even in such setups, LLMs may still\nhallucinate when presented with questions outside of the knowledge base. Such\nbehavior is unacceptable in high-stake applications where LLMs are expected to\nabstain from answering queries it does not have sufficient context on. In this\nwork, we present a novel methodology for systematically evaluating\nout-of-knowledge base (OOKB) robustness of LLMs (whether LLMs know or do not\nknow) in the RAG setting, without the need for manual annotation of gold\nstandard answers. We implement our methodology in knowornot, an open-source\nlibrary that enables users to develop their own customized evaluation data and\npipelines for OOKB robustness. knowornot comprises four main features. Firstly,\nit provides a unified, high-level API that streamlines the process of setting\nup and running robustness benchmarks. Secondly, its modular architecture\nemphasizes extensibility and flexibility, allowing users to easily integrate\ntheir own LLM clients and RAG settings. Thirdly, its rigorous data modeling\ndesign ensures experiment reproducibility, reliability and traceability.\nLastly, it implements a comprehensive suite of tools for users to customize\ntheir pipelines. We demonstrate the utility of knowornot by developing a\nchallenging benchmark, PolicyBench, which spans four Question-Answer (QA)\nchatbots on government policies, and analyze its OOKB robustness. The source\ncode of knowornot is available\nhttps://github.com/govtech-responsibleai/KnowOrNot."
    },
    {
        "date": "2025-05",
        "title": "Use as Many Surrogates as You Want: Selective Ensemble Attack to Unleash Transferability without Sacrificing Resource Efficiency",
        "author": "Bo Yang, Hengwei Zhang, Jindong Wang, Yuchen Ren, Chenhao Lin, Chao Shen, and Zhengyu Zhao",
        "link": "http://arxiv.org/abs/2505.12644v1",
        "abstract": "In surrogate ensemble attacks, using more surrogate models yields higher\ntransferability but lower resource efficiency. This practical trade-off between\ntransferability and efficiency has largely limited existing attacks despite\nmany pre-trained models are easily accessible online. In this paper, we argue\nthat such a trade-off is caused by an unnecessary common assumption, i.e., all\nmodels should be identical across iterations. By lifting this assumption, we\ncan use as many surrogates as we want to unleash transferability without\nsacrificing efficiency. Concretely, we propose Selective Ensemble Attack (SEA),\nwhich dynamically selects diverse models (from easily accessible pre-trained\nmodels) across iterations based on our new interpretation of decoupling\nwithin-iteration and cross-iteration model diversity.In this way, the number of\nwithin-iteration models is fixed for maintaining efficiency, while only\ncross-iteration model diversity is increased for higher transferability.\nExperiments on ImageNet demonstrate the superiority of SEA in various\nscenarios. For example, when dynamically selecting 4 from 20 accessible models,\nSEA yields 8.5% higher transferability than existing attacks under the same\nefficiency. The superiority of SEA also generalizes to real-world systems, such\nas commercial vision APIs and large vision-language models. Overall, SEA opens\nup the possibility of adaptively balancing transferability and efficiency\naccording to specific resource requirements."
    },
    {
        "date": "2025-05",
        "title": "Two out of Three (ToT): using self-consistency to make robust predictions",
        "author": "Jung Hoon Lee, and Sujith Vijayan",
        "link": "http://arxiv.org/abs/2505.12642v1",
        "abstract": "Deep learning (DL) can automatically construct intelligent agents, deep\nneural networks (alternatively, DL models), that can outperform humans in\ncertain tasks. However, the operating principles of DL remain poorly\nunderstood, making its decisions incomprehensible. As a result, it poses a\ngreat risk to deploy DL in high-stakes domains in which mistakes or errors may\nlead to critical consequences. Here, we aim to develop an algorithm that can\nhelp DL models make more robust decisions by allowing them to abstain from\nanswering when they are uncertain. Our algorithm, named `Two out of Three\n(ToT)', is inspired by the sensitivity of the human brain to conflicting\ninformation. ToT creates two alternative predictions in addition to the\noriginal model prediction and uses the alternative predictions to decide\nwhether it should provide an answer or not."
    },
    {
        "date": "2025-05",
        "title": "hChain: Blockchain Based Large Scale EHR Data Sharing with Enhanced Security and Privacy",
        "author": "Musharraf Alruwaill, Saraju Mohanty, and Elias Kougianos",
        "link": "http://arxiv.org/abs/2505.12610v1",
        "abstract": "Concerns regarding privacy and data security in conventional healthcare\nprompted alternative technologies. In smart healthcare, blockchain technology\naddresses existing concerns with security, privacy, and electronic healthcare\ntransmission. Integration of Blockchain Technology with the Internet of Medical\nThings (IoMT) allows real-time monitoring of protected healthcare data.\nUtilizing edge devices with IoMT devices is very advantageous for addressing\nsecurity, computing, and storage challenges. Encryption using symmetric and\nasymmetric keys is used to conceal sensitive information from unauthorized\nparties. SHA256 is an algorithm for one-way hashing. It is used to verify that\nthe data has not been altered, since if it had, the hash value would have\nchanged. This article offers a blockchain-based smart healthcare system using\nIoMT devices for continuous patient monitoring. In addition, it employs edge\nresources in addition to IoMT devices to have extra computing power and storage\nto hash and encrypt incoming data before sending it to the blockchain.\nSymmetric key is utilized to keep the data private even in the blockchain,\nallowing the patient to safely communicate the data through smart contracts\nwhile preventing unauthorized physicians from seeing the data. Through the use\nof a verification node and blockchain, an asymmetric key is used for the\nsigning and validation of patient data in the healthcare provider system. In\naddition to other security measures, location-based authentication is\nrecommended to guarantee that data originates from the patient area. Through\nthe edge device, SHA256 is utilized to secure the data's integrity and a secret\nkey is used to maintain its secrecy. The hChain architecture improves the\ncomputing power of IoMT environments, the security of EHR sharing through smart\ncontracts, and the privacy and authentication procedures."
    },
    {
        "date": "2025-05",
        "title": "A Few Large Shifts: Layer-Inconsistency Based Minimal Overhead Adversarial Example Detection",
        "author": "Sanggeon Yun, Ryozo Masukawa, Hyunwoo Oh, Nathaniel D. Bastian, and Mohsen Imani",
        "link": "http://arxiv.org/abs/2505.12586v3",
        "abstract": "Deep neural networks (DNNs) are highly susceptible to adversarial\nexamples--subtle, imperceptible perturbations that can lead to incorrect\npredictions. While detection-based defenses offer a practical alternative to\nadversarial training, many existing methods depend on external models, complex\narchitectures, heavy augmentations, or adversarial data, limiting their\nefficiency and generalizability. We introduce a lightweight, plug-in detection\nframework that leverages internal layer-wise inconsistencies within the target\nmodel itself, requiring only benign data for calibration. Our approach is\ngrounded in the A Few Large Shifts Assumption, which posits that adversarial\nperturbations typically induce large representation shifts in a small subset of\nlayers. Building on this, we propose two complementary strategies--Recovery\nTesting (RT) and Logit-layer Testing (LT)--to expose internal disruptions\ncaused by adversaries. Evaluated on CIFAR-10, CIFAR-100, and ImageNet under\nboth standard and adaptive threat models, our method achieves state-of-the-art\ndetection performance with negligible computational overhead and no compromise\nto clean accuracy. The code is available here:\nhttps://github.com/c0510gy/AFLS-AED."
    },
    {
        "date": "2025-05",
        "title": "Learning Robust Spectral Dynamics for Temporal Domain Generalization",
        "author": "En Yu, Jie Lu, Xiaoyu Yang, Guangquan Zhang, and Zhen Fang",
        "link": "http://arxiv.org/abs/2505.12585v1",
        "abstract": "Modern machine learning models struggle to maintain performance in dynamic\nenvironments where temporal distribution shifts, \\emph{i.e., concept drift},\nare prevalent. Temporal Domain Generalization (TDG) seeks to enable model\ngeneralization across evolving domains, yet existing approaches typically\nassume smooth incremental changes, struggling with complex real-world drifts\ninvolving long-term structure (incremental evolution/periodicity) and local\nuncertainties. To overcome these limitations, we introduce FreKoo, which\ntackles these challenges via a novel frequency-domain analysis of parameter\ntrajectories. It leverages the Fourier transform to disentangle parameter\nevolution into distinct spectral bands. Specifically, low-frequency component\nwith dominant dynamics are learned and extrapolated using the Koopman operator,\nrobustly capturing diverse drift patterns including both incremental and\nperiodicity. Simultaneously, potentially disruptive high-frequency variations\nare smoothed via targeted temporal regularization, preventing overfitting to\ntransient noise and domain uncertainties. In addition, this dual spectral\nstrategy is rigorously grounded through theoretical analysis, providing\nstability guarantees for the Koopman prediction, a principled Bayesian\njustification for the high-frequency regularization, and culminating in a\nmultiscale generalization bound connecting spectral dynamics to improved\ngeneralization. Extensive experiments demonstrate FreKoo's significant\nsuperiority over SOTA TDG approaches, particularly excelling in real-world\nstreaming scenarios with complex drifts and uncertainties."
    },
    {
        "date": "2025-05",
        "title": "A Survey of Attacks on Large Language Models",
        "author": "Wenrui Xu, and Keshab K. Parhi",
        "link": "http://arxiv.org/abs/2505.12567v1",
        "abstract": "Large language models (LLMs) and LLM-based agents have been widely deployed\nin a wide range of applications in the real world, including healthcare\ndiagnostics, financial analysis, customer support, robotics, and autonomous\ndriving, expanding their powerful capability of understanding, reasoning, and\ngenerating natural languages. However, the wide deployment of LLM-based\napplications exposes critical security and reliability risks, such as the\npotential for malicious misuse, privacy leakage, and service disruption that\nweaken user trust and undermine societal safety. This paper provides a\nsystematic overview of the details of adversarial attacks targeting both LLMs\nand LLM-based agents. These attacks are organized into three phases in LLMs:\nTraining-Phase Attacks, Inference-Phase Attacks, and Availability & Integrity\nAttacks. For each phase, we analyze the details of representative and recently\nintroduced attack methods along with their corresponding defenses. We hope our\nsurvey will provide a good tutorial and a comprehensive understanding of LLM\nsecurity, especially for attacks on LLMs. We desire to raise attention to the\nrisks inherent in widely deployed LLM-based applications and highlight the\nurgent need for robust mitigation strategies for evolving threats."
    },
    {
        "date": "2025-05",
        "title": "SPIRIT: Patching Speech Language Models against Jailbreak Attacks",
        "author": "Amirbek Djanibekov, Nurdaulet Mukhituly, Kentaro Inui, Hanan Aldarmaki, and Nils Lukas",
        "link": "http://arxiv.org/abs/2505.13541v1",
        "abstract": "Speech Language Models (SLMs) enable natural interactions via spoken\ninstructions, which more effectively capture user intent by detecting nuances\nin speech. The richer speech signal introduces new security risks compared to\ntext-based models, as adversaries can better bypass safety mechanisms by\ninjecting imperceptible noise to speech. We analyze adversarial attacks and\nfind that SLMs are substantially more vulnerable to jailbreak attacks, which\ncan achieve a perfect 100% attack success rate in some instances. To improve\nsecurity, we propose post-hoc patching defenses used to intervene during\ninference by modifying the SLM's activations that improve robustness up to 99%\nwith (i) negligible impact on utility and (ii) without any re-training. We\nconduct ablation studies to maximize the efficacy of our defenses and improve\nthe utility/security trade-off, validated with large-scale benchmarks unique to\nSLMs."
    },
    {
        "date": "2025-05",
        "title": "Improving LLM Outputs Against Jailbreak Attacks with Expert Model Integration",
        "author": "Tatia Tsmindashvili, Ana Kolkhidashvili, Dachi Kurtskhalia, Nino Maghlakelidze, Elene Mekvabishvili, Guram Dentoshvili, Orkhan Shamilov, Zaal Gachechiladze, Steven Saporta, and David Dachi Choladze",
        "link": "http://arxiv.org/abs/2505.17066v1",
        "abstract": "Using LLMs in a production environment presents security challenges that\ninclude vulnerabilities to jailbreaks and prompt injections, which can result\nin harmful outputs for humans or the enterprise. The challenge is amplified\nwhen working within a specific domain, as topics generally accepted for LLMs to\naddress may be irrelevant to that field. These problems can be mitigated, for\nexample, by fine-tuning large language models with domain-specific and\nsecurity-focused data. However, these alone are insufficient, as jailbreak\ntechniques evolve. Additionally, API-accessed models do not offer the\nflexibility needed to tailor behavior to industry-specific objectives, and\nin-context learning is not always sufficient or reliable. In response to these\nchallenges, we introduce Archias, an expert model adept at distinguishing\nbetween in-domain and out-of-domain communications. Archias classifies user\ninquiries into several categories: in-domain (specifically for the automotive\nindustry), malicious questions, price injections, prompt injections, and\nout-of-domain examples. Our methodology integrates outputs from the expert\nmodel (Archias) into prompts, which are then processed by the LLM to generate\nresponses. This method increases the model's ability to understand the user's\nintention and give appropriate answers. Archias can be adjusted, fine-tuned,\nand used for many different purposes due to its small size. Therefore, it can\nbe easily customized to the needs of any industry. To validate our approach, we\ncreated a benchmark dataset for the automotive industry. Furthermore, in the\ninterest of advancing research and development, we release our benchmark\ndataset to the community."
    },
    {
        "date": "2025-05",
        "title": "A Finite-Sample Analysis of Distributionally Robust Average-Reward Reinforcement Learning",
        "author": "Zachary Roch, Chi Zhang, George Atia, and Yue Wang",
        "link": "http://arxiv.org/abs/2505.12462v1",
        "abstract": "Robust reinforcement learning (RL) under the average-reward criterion is\ncrucial for long-term decision making under potential environment mismatches,\nyet its finite-sample complexity study remains largely unexplored. Existing\nworks offer algorithms with asymptotic guarantees, but the absence of\nfinite-sample analysis hinders its principled understanding and practical\ndeployment, especially in data-limited settings. We close this gap by proposing\nRobust Halpern Iteration (RHI), the first algorithm with provable finite-sample\ncomplexity guarantee. Under standard uncertainty sets -- including\ncontamination sets and $\\ell_p$-norm balls -- RHI attains an $\\epsilon$-optimal\npolicy with near-optimal sample complexity of $\\tilde{\\mathcal\nO}\\left(\\frac{SA\\mathcal H^{2}}{\\epsilon^{2}}\\right)$, where $S$ and $A$ denote\nthe numbers of states and actions, and $\\mathcal H$ is the robust optimal bias\nspan. This result gives the first polynomial sample complexity guarantee for\nrobust average-reward RL. Moreover, our RHI's independence from prior knowledge\ndistinguishes it from many previous average-reward RL studies. Our work thus\nconstitutes a significant advancement in enhancing the practical applicability\nof robust average-reward methods to complex, real-world problems."
    },
    {
        "date": "2025-05",
        "title": "SecEmb: Sparsity-Aware Secure Federated Learning of On-Device Recommender System with Large Embedding",
        "author": "Peihua Mai, Youlong Ding, Ziyan Lyu, Minxin Du, and Yan Pang",
        "link": "http://arxiv.org/abs/2505.12453v1",
        "abstract": "Federated recommender system (FedRec) has emerged as a solution to protect\nuser data through collaborative training techniques. A typical FedRec involves\ntransmitting the full model and entire weight updates between edge devices and\nthe server, causing significant burdens to devices with limited bandwidth and\ncomputational power. While the sparsity of embedding updates provides\nopportunity for payload optimization, existing sparsity-aware federated\nprotocols generally sacrifice privacy for efficiency. A key challenge in\ndesigning a secure sparsity-aware efficient protocol is to protect the rated\nitem indices from the server. In this paper, we propose a lossless secure\nrecommender systems on sparse embedding updates (SecEmb). SecEmb reduces user\npayload while ensuring that the server learns no information about both rated\nitem indices and individual updates except the aggregated model. The protocol\nconsists of two correlated modules: (1) a privacy-preserving embedding\nretrieval module that allows users to download relevant embeddings from the\nserver, and (2) an update aggregation module that securely aggregates updates\nat the server. Empirical analysis demonstrates that SecEmb reduces both\ndownload and upload communication costs by up to 90x and decreases user-side\ncomputation time by up to 70x compared with secure FedRec protocols.\nAdditionally, it offers non-negligible utility advantages compared with lossy\nmessage compression methods."
    },
    {
        "date": "2025-05",
        "title": "IP Leakage Attacks Targeting LLM-Based Multi-Agent Systems",
        "author": "Liwen Wang, Wenxuan Wang, Shuai Wang, Zongjie Li, Zhenlan Ji, Zongyi Lyu, Daoyuan Wu, and Shing-Chi Cheung",
        "link": "http://arxiv.org/abs/2505.12442v2",
        "abstract": "The rapid advancement of Large Language Models (LLMs) has led to the\nemergence of Multi-Agent Systems (MAS) to perform complex tasks through\ncollaboration. However, the intricate nature of MAS, including their\narchitecture and agent interactions, raises significant concerns regarding\nintellectual property (IP) protection. In this paper, we introduce MASLEAK, a\nnovel attack framework designed to extract sensitive information from MAS\napplications. MASLEAK targets a practical, black-box setting, where the\nadversary has no prior knowledge of the MAS architecture or agent\nconfigurations. The adversary can only interact with the MAS through its public\nAPI, submitting attack query $q$ and observing outputs from the final agent.\nInspired by how computer worms propagate and infect vulnerable network hosts,\nMASLEAK carefully crafts adversarial query $q$ to elicit, propagate, and retain\nresponses from each MAS agent that reveal a full set of proprietary components,\nincluding the number of agents, system topology, system prompts, task\ninstructions, and tool usages. We construct the first synthetic dataset of MAS\napplications with 810 applications and also evaluate MASLEAK against real-world\nMAS applications, including Coze and CrewAI. MASLEAK achieves high accuracy in\nextracting MAS IP, with an average attack success rate of 87% for system\nprompts and task instructions, and 92% for system architecture in most cases.\nWe conclude by discussing the implications of our findings and the potential\ndefenses."
    },
    {
        "date": "2025-05",
        "title": "EvoGPT: Enhancing Test Suite Robustness via LLM-Based Generation and Genetic Optimization",
        "author": "Lior Broide, and Roni Stern",
        "link": "http://arxiv.org/abs/2505.12424v1",
        "abstract": "Large Language Models (LLMs) have recently emerged as promising tools for\nautomated unit test generation. We introduce a hybrid framework called EvoGPT\nthat integrates LLM-based test generation with evolutionary search techniques\nto create diverse, fault-revealing unit tests. Unit tests are initially\ngenerated with diverse temperature sampling to maximize behavioral and test\nsuite diversity, followed by a generation-repair loop and coverage-guided\nassertion enhancement. The resulting test suites are evolved using genetic\nalgorithms, guided by a fitness function prioritizing mutation score over\ntraditional coverage metrics. This design emphasizes the primary objective of\nunit testing-fault detection. Evaluated on multiple open-source Java projects,\nEvoGPT achieves an average improvement of 10% in both code coverage and\nmutation score compared to LLMs and traditional search-based software testing\nbaselines. These results demonstrate that combining LLM-driven diversity,\ntargeted repair, and evolutionary optimization produces more effective and\nresilient test suites."
    },
    {
        "date": "2025-05",
        "title": "CAPTURE: Context-Aware Prompt Injection Testing and Robustness Enhancement",
        "author": "Gauri Kholkar, and Ratinder Ahuja",
        "link": "http://arxiv.org/abs/2505.12368v1",
        "abstract": "Prompt injection remains a major security risk for large language models.\nHowever, the efficacy of existing guardrail models in context-aware settings\nremains underexplored, as they often rely on static attack benchmarks.\nAdditionally, they have over-defense tendencies. We introduce CAPTURE, a novel\ncontext-aware benchmark assessing both attack detection and over-defense\ntendencies with minimal in-domain examples. Our experiments reveal that current\nprompt injection guardrail models suffer from high false negatives in\nadversarial cases and excessive false positives in benign scenarios,\nhighlighting critical limitations."
    },
    {
        "date": "2025-05",
        "title": "VoiceCloak: A Multi-Dimensional Defense Framework against Unauthorized Diffusion-based Voice Cloning",
        "author": "Qianyue Hu, Junyan Wu, Wei Lu, and Xiangyang Luo",
        "link": "http://arxiv.org/abs/2505.12332v2",
        "abstract": "Diffusion Models (DMs) have achieved remarkable success in realistic voice\ncloning (VC), while they also increase the risk of malicious misuse. Existing\nproactive defenses designed for traditional VC models aim to disrupt the\nforgery process, but they have been proven incompatible with DMs due to the\nintricate generative mechanisms of diffusion. To bridge this gap, we introduce\nVoiceCloak, a multi-dimensional proactive defense framework with the goal of\nobfuscating speaker identity and degrading perceptual quality in potential\nunauthorized VC. To achieve these goals, we conduct a focused analysis to\nidentify specific vulnerabilities within DMs, allowing VoiceCloak to disrupt\nthe cloning process by introducing adversarial perturbations into the reference\naudio. Specifically, to obfuscate speaker identity, VoiceCloak first targets\nspeaker identity by distorting representation learning embeddings to maximize\nidentity variation, which is guided by auditory perception principles.\nAdditionally, VoiceCloak disrupts crucial conditional guidance processes,\nparticularly attention context, thereby preventing the alignment of vocal\ncharacteristics that are essential for achieving convincing cloning. Then, to\naddress the second objective, VoiceCloak introduces score magnitude\namplification to actively steer the reverse trajectory away from the generation\nof high-quality speech. Noise-guided semantic corruption is further employed to\ndisrupt structural speech semantics captured by DMs, degrading output quality.\nExtensive experiments highlight VoiceCloak's outstanding defense success rate\nagainst unauthorized diffusion-based voice cloning."
    },
    {
        "date": "2025-05",
        "title": "Robust Planning for Autonomous Driving via Mixed Adversarial Diffusion Predictions",
        "author": "Albert Zhao, and Stefano Soatto",
        "link": "http://arxiv.org/abs/2505.12327v1",
        "abstract": "We describe a robust planning method for autonomous driving that mixes normal\nand adversarial agent predictions output by a diffusion model trained for\nmotion prediction. We first train a diffusion model to learn an unbiased\ndistribution of normal agent behaviors. We then generate a distribution of\nadversarial predictions by biasing the diffusion model at test time to generate\npredictions that are likely to collide with a candidate plan. We score plans\nusing expected cost with respect to a mixture distribution of normal and\nadversarial predictions, leading to a planner that is robust against\nadversarial behaviors but not overly conservative when agents behave normally.\nUnlike current approaches, we do not use risk measures that over-weight\nadversarial behaviors while placing little to no weight on low-cost normal\nbehaviors or use hard safety constraints that may not be appropriate for all\ndriving scenarios. We show the effectiveness of our method on single-agent and\nmulti-agent jaywalking scenarios as well as a red light violation scenario."
    },
    {
        "date": "2025-05",
        "title": "Improving Out-of-Domain Robustness with Targeted Augmentation in Frequency and Pixel Spaces",
        "author": "Ruoqi Wang, Haitao Wang, Shaojie Guo, and Qiong Luo",
        "link": "http://arxiv.org/abs/2505.12317v1",
        "abstract": "Out-of-domain (OOD) robustness under domain adaptation settings, where\nlabeled source data and unlabeled target data come from different\ndistributions, is a key challenge in real-world applications. A common approach\nto improving OOD robustness is through data augmentations. However, in\nreal-world scenarios, models trained with generic augmentations can only\nimprove marginally when generalized under distribution shifts toward unlabeled\ntarget domains. While dataset-specific targeted augmentations can address this\nissue, they typically require expert knowledge and extensive prior data\nanalysis to identify the nature of the datasets and domain shift. To address\nthese challenges, we propose Frequency-Pixel Connect, a domain-adaptation\nframework that enhances OOD robustness by introducing a targeted augmentation\nin both the frequency space and pixel space. Specifically, we mix the amplitude\nspectrum and pixel content of a source image and a target image to generate\naugmented samples that introduce domain diversity while preserving the semantic\nstructure of the source image. Unlike previous targeted augmentation methods\nthat are both dataset-specific and limited to the pixel space, Frequency-Pixel\nConnect is dataset-agnostic, enabling broader and more flexible applicability\nbeyond natural image datasets. We further analyze the effectiveness of\nFrequency-Pixel Connect by evaluating the performance of our method connecting\nsame-class cross-domain samples while separating different-class examples. We\ndemonstrate that Frequency-Pixel Connect significantly improves cross-domain\nconnectivity and outperforms previous generic methods on four diverse\nreal-world benchmarks across vision, medical, audio, and astronomical domains,\nand it also outperforms other dataset-specific targeted augmentation methods."
    },
    {
        "date": "2025-05",
        "title": "LLM-Based User Simulation for Low-Knowledge Shilling Attacks on Recommender Systems",
        "author": "Shengkang Gu, Jiahao Liu, Dongsheng Li, Guangping Zhang, Mingzhe Han, Hansu Gu, Peng Zhang, Ning Gu, Li Shang, and Tun Lu",
        "link": "http://arxiv.org/abs/2505.13528v1",
        "abstract": "Recommender systems (RS) are increasingly vulnerable to shilling attacks,\nwhere adversaries inject fake user profiles to manipulate system outputs.\nTraditional attack strategies often rely on simplistic heuristics, require\naccess to internal RS data, and overlook the manipulation potential of textual\nreviews. In this work, we introduce Agent4SR, a novel framework that leverages\nLarge Language Model (LLM)-based agents to perform low-knowledge, high-impact\nshilling attacks through both rating and review generation. Agent4SR simulates\nrealistic user behavior by orchestrating adversarial interactions, selecting\nitems, assigning ratings, and crafting reviews, while maintaining behavioral\nplausibility. Our design includes targeted profile construction, hybrid memory\nretrieval, and a review attack strategy that propagates target item features\nacross unrelated reviews to amplify manipulation. Extensive experiments on\nmultiple datasets and RS architectures demonstrate that Agent4SR outperforms\nexisting low-knowledge baselines in both effectiveness and stealth. Our\nfindings reveal a new class of emergent threats posed by LLM-driven agents,\nunderscoring the urgent need for enhanced defenses in modern recommender\nsystems."
    },
    {
        "date": "2025-05",
        "title": "From Low Field to High Value: Robust Cortical Mapping from Low-Field MRI",
        "author": "Karthik Gopinath, Annabel Sorby-Adams, Jonathan W. Ramirez, Dina Zemlyanker, Jennifer Guo, David Hunt, Christine L. Mac Donald, C. Dirk Keene, Timothy Coalson, Matthew F. Glasser, David Van Essen, Matthew S. Rosen, Oula Puonti, W. Taylor Kimberly, and Juan Eugenio Iglesias",
        "link": "http://arxiv.org/abs/2505.12228v1",
        "abstract": "Three-dimensional reconstruction of cortical surfaces from MRI for\nmorphometric analysis is fundamental for understanding brain structure. While\nhigh-field MRI (HF-MRI) is standard in research and clinical settings, its\nlimited availability hinders widespread use. Low-field MRI (LF-MRI),\nparticularly portable systems, offers a cost-effective and accessible\nalternative. However, existing cortical surface analysis tools are optimized\nfor high-resolution HF-MRI and struggle with the lower signal-to-noise ratio\nand resolution of LF-MRI. In this work, we present a machine learning method\nfor 3D reconstruction and analysis of portable LF-MRI across a range of\ncontrasts and resolutions. Our method works \"out of the box\" without\nretraining. It uses a 3D U-Net trained on synthetic LF-MRI to predict signed\ndistance functions of cortical surfaces, followed by geometric processing to\nensure topological accuracy. We evaluate our method using paired HF/LF-MRI\nscans of the same subjects, showing that LF-MRI surface reconstruction accuracy\ndepends on acquisition parameters, including contrast type (T1 vs T2),\norientation (axial vs isotropic), and resolution. A 3mm isotropic T2-weighted\nscan acquired in under 4 minutes, yields strong agreement with HF-derived\nsurfaces: surface area correlates at r=0.96, cortical parcellations reach\nDice=0.98, and gray matter volume achieves r=0.93. Cortical thickness remains\nmore challenging with correlations up to r=0.70, reflecting the difficulty of\nsub-mm precision with 3mm voxels. We further validate our method on challenging\npostmortem LF-MRI, demonstrating its robustness. Our method represents a step\ntoward enabling cortical surface analysis on portable LF-MRI. Code is available\nat https://surfer.nmr.mgh.harvard.edu/fswiki/ReconAny"
    },
    {
        "date": "2025-05",
        "title": "Near-Optimal Sample Complexities of Divergence-based S-rectangular Distributionally Robust Reinforcement Learning",
        "author": "Zhenghao Li, Shengbo Wang, and Nian Si",
        "link": "http://arxiv.org/abs/2505.12202v1",
        "abstract": "Distributionally robust reinforcement learning (DR-RL) has recently gained\nsignificant attention as a principled approach that addresses discrepancies\nbetween training and testing environments. To balance robustness, conservatism,\nand computational traceability, the literature has introduced DR-RL models with\nSA-rectangular and S-rectangular adversaries. While most existing statistical\nanalyses focus on SA-rectangular models, owing to their algorithmic simplicity\nand the optimality of deterministic policies, S-rectangular models more\naccurately capture distributional discrepancies in many real-world applications\nand often yield more effective robust randomized policies. In this paper, we\nstudy the empirical value iteration algorithm for divergence-based\nS-rectangular DR-RL and establish near-optimal sample complexity bounds of\n$\\widetilde{O}(|\\mathcal{S}||\\mathcal{A}|(1-\\gamma)^{-4}\\varepsilon^{-2})$,\nwhere $\\varepsilon$ is the target accuracy, $|\\mathcal{S}|$ and $|\\mathcal{A}|$\ndenote the cardinalities of the state and action spaces, and $\\gamma$ is the\ndiscount factor. To the best of our knowledge, these are the first sample\ncomplexity results for divergence-based S-rectangular models that achieve\noptimal dependence on $|\\mathcal{S}|$, $|\\mathcal{A}|$, and $\\varepsilon$\nsimultaneously. We further validate this theoretical dependence through\nnumerical experiments on a robust inventory control problem and a theoretical\nworst-case example, demonstrating the fast learning performance of our proposed\nalgorithm."
    },
    {
        "date": "2025-05",
        "title": "Always Clear Depth: Robust Monocular Depth Estimation under Adverse Weather",
        "author": "Kui Jiang, Jing Cao, Zhaocheng Yu, Junjun Jiang, and Jingchun Zhou",
        "link": "http://arxiv.org/abs/2505.12199v1",
        "abstract": "Monocular depth estimation is critical for applications such as autonomous\ndriving and scene reconstruction. While existing methods perform well under\nnormal scenarios, their performance declines in adverse weather, due to\nchallenging domain shifts and difficulties in extracting scene information. To\naddress this issue, we present a robust monocular depth estimation method\ncalled \\textbf{ACDepth} from the perspective of high-quality training data\ngeneration and domain adaptation. Specifically, we introduce a one-step\ndiffusion model for generating samples that simulate adverse weather\nconditions, constructing a multi-tuple degradation dataset during training. To\nensure the quality of the generated degradation samples, we employ LoRA\nadapters to fine-tune the generation weights of diffusion model. Additionally,\nwe integrate circular consistency loss and adversarial training to guarantee\nthe fidelity and naturalness of the scene contents. Furthermore, we elaborate\non a multi-granularity knowledge distillation strategy (MKD) that encourages\nthe student network to absorb knowledge from both the teacher model and\npretrained Depth Anything V2. This strategy guides the student model in\nlearning degradation-agnostic scene information from various degradation\ninputs. In particular, we introduce an ordinal guidance distillation mechanism\n(OGD) that encourages the network to focus on uncertain regions through\ndifferential ranking, leading to a more precise depth estimation. Experimental\nresults demonstrate that our ACDepth surpasses md4all-DD by 2.50\\% for night\nscene and 2.61\\% for rainy scene on the nuScenes dataset in terms of the absRel\nmetric."
    },
    {
        "date": "2025-05",
        "title": "BenSParX: A Robust Explainable Machine Learning Framework for Parkinson's Disease Detection from Bengali Conversational Speech",
        "author": "Riad Hossain, Muhammad Ashad Kabir, Arat Ibne Golam Mowla, Animesh Chandra Roy, and Ranjit Kumar Ghosh",
        "link": "http://arxiv.org/abs/2505.12192v1",
        "abstract": "Parkinson's disease (PD) poses a growing global health challenge, with\nBangladesh experiencing a notable rise in PD-related mortality. Early detection\nof PD remains particularly challenging in resource-constrained settings, where\nvoice-based analysis has emerged as a promising non-invasive and cost-effective\nalternative. However, existing studies predominantly focus on English or other\nmajor languages; notably, no voice dataset for PD exists for Bengali - posing a\nsignificant barrier to culturally inclusive and accessible healthcare\nsolutions. Moreover, most prior studies employed only a narrow set of acoustic\nfeatures, with limited or no hyperparameter tuning and feature selection\nstrategies, and little attention to model explainability. This restricts the\ndevelopment of a robust and generalizable machine learning model. To address\nthis gap, we present BenSparX, the first Bengali conversational speech dataset\nfor PD detection, along with a robust and explainable machine learning\nframework tailored for early diagnosis. The proposed framework incorporates\ndiverse acoustic feature categories, systematic feature selection methods, and\nstate-of-the-art machine learning algorithms with extensive hyperparameter\noptimization. Furthermore, to enhance interpretability and trust in model\npredictions, the framework incorporates SHAP (SHapley Additive exPlanations)\nanalysis to quantify the contribution of individual acoustic features toward PD\ndetection. Our framework achieves state-of-the-art performance, yielding an\naccuracy of 95.77%, F1 score of 95.57%, and AUC-ROC of 0.982. We further\nexternally validated our approach by applying the framework to existing PD\ndatasets in other languages, where it consistently outperforms state-of-the-art\napproaches. To facilitate further research and reproducibility, the dataset has\nbeen made publicly available at https://github.com/Riad071/BenSParX."
    },
    {
        "date": "2025-05",
        "title": "Ditch the Denoiser: Emergence of Noise Robustness in Self-Supervised Learning from Data Curriculum",
        "author": "Wenquan Lu, Jiaqi Zhang, Hugues Van Assel, and Randall Balestriero",
        "link": "http://arxiv.org/abs/2505.12191v1",
        "abstract": "Self-Supervised Learning (SSL) has become a powerful solution to extract rich\nrepresentations from unlabeled data. Yet, SSL research is mostly focused on\nclean, curated and high-quality datasets. As a result, applying SSL on noisy\ndata remains a challenge, despite being crucial to applications such as\nastrophysics, medical imaging, geophysics or finance. In this work, we present\na fully self-supervised framework that enables noise-robust representation\nlearning without requiring a denoiser at inference or downstream fine-tuning.\nOur method first trains an SSL denoiser on noisy data, then uses it to\nconstruct a denoised-to-noisy data curriculum (i.e., training first on\ndenoised, then noisy samples) for pretraining a SSL backbone (e.g., DINOv2),\ncombined with a teacher-guided regularization that anchors noisy embeddings to\ntheir denoised counterparts. This process encourages the model to internalize\nnoise robustness. Notably, the denoiser can be discarded after pretraining,\nsimplifying deployment. On ImageNet-1k with ViT-B under extreme Gaussian noise\n($\\sigma=255$, SNR = 0.72 dB), our method improves linear probing accuracy by\n4.8% over DINOv2, demonstrating that denoiser-free robustness can emerge from\nnoise-aware pretraining. The code is available at\nhttps://github.com/wenquanlu/noisy_dinov2."
    },
    {
        "date": "2025-05",
        "title": "EVALOOP: Assessing LLM Robustness in Programming from a Self-consistency Perspective",
        "author": "Sen Fang, Weiyuan Ding, and Bowen Xu",
        "link": "http://arxiv.org/abs/2505.12185v1",
        "abstract": "Assessing the programming capabilities of Large Language Models (LLMs) is\ncrucial for their effective use in software engineering. Current evaluations,\nhowever, predominantly measure the accuracy of generated code on static\nbenchmarks, neglecting the critical aspect of model robustness during\nprogramming tasks. While adversarial attacks offer insights on model\nrobustness, their effectiveness is limited and evaluation could be constrained.\nCurrent adversarial attack methods for robustness evaluation yield inconsistent\nresults, struggling to provide a unified evaluation across different LLMs. We\nintroduce EVALOOP, a novel assessment framework that evaluate the robustness\nfrom a self-consistency perspective, i.e., leveraging the natural duality\ninherent in popular software engineering tasks, e.g., code generation and code\nsummarization. EVALOOP initiates a self-contained feedback loop: an LLM\ngenerates output (e.g., code) from an input (e.g., natural language\nspecification), and then use the generated output as the input to produce a new\noutput (e.g., summarizes that code into a new specification). EVALOOP repeats\nthe process to assess the effectiveness of EVALOOP in each loop. This cyclical\nstrategy intrinsically evaluates robustness without rely on any external attack\nsetups, providing a unified metric to evaluate LLMs' robustness in programming.\nWe evaluate 16 prominent LLMs (e.g., GPT-4.1, O4-mini) on EVALOOP and found\nthat EVALOOP typically induces a 5.01%-19.31% absolute drop in pass@1\nperformance within ten loops. Intriguingly, robustness does not always align\nwith initial performance (i.e., one-time query); for instance, GPT-3.5-Turbo,\ndespite superior initial code generation compared to DeepSeek-V2, demonstrated\nlower robustness over repeated evaluation loop."
    },
    {
        "date": "2025-05",
        "title": "FABLE: A Localized, Targeted Adversarial Attack on Weather Forecasting Models",
        "author": "Yue Deng, Asadullah Hill Galib, Xin Lan, Pang-Ning Tan, and Lifeng Luo",
        "link": "http://arxiv.org/abs/2505.12167v1",
        "abstract": "Deep learning-based weather forecasting models have recently demonstrated\nsignificant performance improvements over gold-standard physics-based\nsimulation tools. However, these models are vulnerable to adversarial attacks,\nwhich raises concerns about their trustworthiness. In this paper, we first\ninvestigate the feasibility of applying existing adversarial attack methods to\nweather forecasting models. We argue that a successful attack should (1) not\nmodify significantly its original inputs, (2) be faithful, i.e., achieve the\ndesired forecast at targeted locations with minimal changes to non-targeted\nlocations, and (3) be geospatio-temporally realistic. However, balancing these\ncriteria is a challenge as existing methods are not designed to preserve the\ngeospatio-temporal dependencies of the original samples. To address this\nchallenge, we propose a novel framework called FABLE (Forecast Alteration By\nLocalized targeted advErsarial attack), which employs a 3D discrete wavelet\ndecomposition to extract the varying components of the geospatio-temporal data.\nBy regulating the magnitude of adversarial perturbations across different\ncomponents, FABLE can generate adversarial inputs that maintain\ngeospatio-temporal coherence while remaining faithful and closely aligned with\nthe original inputs. Experimental results on multiple real-world datasets\ndemonstrate the effectiveness of our framework over baseline methods across\nvarious metrics."
    },
    {
        "date": "2025-05",
        "title": "SoftPQ: Robust Instance Segmentation Evaluation via Soft Matching and Tunable Thresholds",
        "author": "Ranit Karmakar, and Simon F. N\u00f8rrelykke",
        "link": "http://arxiv.org/abs/2505.12155v1",
        "abstract": "Segmentation evaluation metrics traditionally rely on binary decision logic:\npredictions are either correct or incorrect, based on rigid IoU thresholds.\nDetection--based metrics such as F1 and mAP determine correctness at the object\nlevel using fixed overlap cutoffs, while overlap--based metrics like\nIntersection over Union (IoU) and Dice operate at the pixel level, often\noverlooking instance--level structure. Panoptic Quality (PQ) attempts to unify\ndetection and segmentation assessment, but it remains dependent on\nhard-threshold matching--treating predictions below the threshold as entirely\nincorrect. This binary framing obscures important distinctions between\nqualitatively different errors and fails to reward gradual model improvements.\nWe propose SoftPQ, a flexible and interpretable instance segmentation metric\nthat redefines evaluation as a graded continuum rather than a binary\nclassification. SoftPQ introduces tunable upper and lower IoU thresholds to\ndefine a partial matching region and applies a sublinear penalty function to\nambiguous or fragmented predictions. These extensions allow SoftPQ to exhibit\nsmoother score behavior, greater robustness to structural segmentation errors,\nand more informative feedback for model development and evaluation. Through\ncontrolled perturbation experiments, we show that SoftPQ captures meaningful\ndifferences in segmentation quality that existing metrics overlook, making it a\npractical and principled alternative for both benchmarking and iterative model\nrefinement."
    },
    {
        "date": "2025-05",
        "title": "T-Rex: Fitting a Robust Factor Model via Expectation-Maximization",
        "author": "Daniel Cederberg",
        "link": "http://arxiv.org/abs/2505.12117v1",
        "abstract": "Over the past decades, there has been a surge of interest in studying\nlow-dimensional structures within high-dimensional data. Statistical factor\nmodels $-$ i.e., low-rank plus diagonal covariance structures $-$ offer a\npowerful framework for modeling such structures. However, traditional methods\nfor fitting statistical factor models, such as principal component analysis\n(PCA) or maximum likelihood estimation assuming the data is Gaussian, are\nhighly sensitive to heavy tails and outliers in the observed data. In this\npaper, we propose a novel expectation-maximization (EM) algorithm for robustly\nfitting statistical factor models. Our approach is based on Tyler's M-estimator\nof the scatter matrix for an elliptical distribution, and consists of solving\nTyler's maximum likelihood estimation problem while imposing a structural\nconstraint that enforces the low-rank plus diagonal covariance structure. We\npresent numerical experiments on both synthetic and real examples,\ndemonstrating the robustness of our method for direction-of-arrival estimation\nin nonuniform noise and subspace recovery."
    },
    {
        "date": "2025-05",
        "title": "Improving Fairness in LLMs Through Testing-Time Adversaries",
        "author": "Isabela Pereira Gregio, Ian Pons, Anna Helena Reali Costa, and Artur Jord\u00e3o",
        "link": "http://arxiv.org/abs/2505.12100v1",
        "abstract": "Large Language Models (LLMs) push the bound-aries in natural language\nprocessing and generative AI, driving progress across various aspects of modern\nsociety. Unfortunately, the pervasive issue of bias in LLMs responses (i.e.,\npredictions) poses a significant and open challenge, hindering their\napplication in tasks involving ethical sensitivity and responsible\ndecision-making. In this work, we propose a straightforward, user-friendly and\npractical method to mitigate such biases, enhancing the reliability and\ntrustworthiness of LLMs. Our method creates multiple variations of a given\nsentence by modifying specific attributes and evaluates the corresponding\nprediction behavior compared to the original, unaltered, prediction/sentence.\nThe idea behind this process is that critical ethical predictions often exhibit\nnotable inconsistencies, indicating the presence of bias. Unlike previous\napproaches, our method relies solely on forward passes (i.e., testing-time\nadversaries), eliminating the need for training, fine-tuning, or prior\nknowledge of the training data distribution. Through extensive experiments on\nthe popular Llama family, we demonstrate the effectiveness of our method in\nimproving various fairness metrics, focusing on the reduction of disparities in\nhow the model treats individuals from different racial groups. Specifically,\nusing standard metrics, we improve the fairness in Llama3 in up to 27\npercentage points. Overall, our approach significantly enhances fairness,\nequity, and reliability in LLM-generated results without parameter tuning or\ntraining data modifications, confirming its effectiveness in practical\nscenarios. We believe our work establishes an important step toward enabling\nthe use of LLMs in tasks that require ethical considerations and responsible\ndecision-making."
    },
    {
        "date": "2025-05",
        "title": "FIGhost: Fluorescent Ink-based Stealthy and Flexible Backdoor Attacks on Physical Traffic Sign Recognition",
        "author": "Shuai Yuan, Guowen Xu, Hongwei Li, Rui Zhang, Xinyuan Qian, Wenbo Jiang, Hangcheng Cao, and Qingchuan Zhao",
        "link": "http://arxiv.org/abs/2505.12045v1",
        "abstract": "Traffic sign recognition (TSR) systems are crucial for autonomous driving but\nare vulnerable to backdoor attacks. Existing physical backdoor attacks either\nlack stealth, provide inflexible attack control, or ignore emerging\nVision-Large-Language-Models (VLMs). In this paper, we introduce FIGhost, the\nfirst physical-world backdoor attack leveraging fluorescent ink as triggers.\nFluorescent triggers are invisible under normal conditions and activated\nstealthily by ultraviolet light, providing superior stealthiness, flexibility,\nand untraceability. Inspired by real-world graffiti, we derive realistic\ntrigger shapes and enhance their robustness via an interpolation-based\nfluorescence simulation algorithm. Furthermore, we develop an automated\nbackdoor sample generation method to support three attack objectives. Extensive\nevaluations in the physical world demonstrate FIGhost's effectiveness against\nstate-of-the-art detectors and VLMs, maintaining robustness under environmental\nvariations and effectively evading existing defenses."
    },
    {
        "date": "2025-05",
        "title": "FL-PLAS: Federated Learning with Partial Layer Aggregation for Backdoor Defense Against High-Ratio Malicious Clients",
        "author": "Jianyi Zhang, Ziyin Zhou, Yilong Li, and Qichao Jin",
        "link": "http://arxiv.org/abs/2505.12019v1",
        "abstract": "Federated learning (FL) is gaining increasing attention as an emerging\ncollaborative machine learning approach, particularly in the context of\nlarge-scale computing and data systems. However, the fundamental algorithm of\nFL, Federated Averaging (FedAvg), is susceptible to backdoor attacks. Although\nresearchers have proposed numerous defense algorithms, two significant\nchallenges remain. The attack is becoming more stealthy and harder to detect,\nand current defense methods are unable to handle 50\\% or more malicious users\nor assume an auxiliary server dataset.\n  To address these challenges, we propose a novel defense algorithm, FL-PLAS,\n\\textbf{F}ederated \\textbf{L}earning based on \\textbf{P}artial\\textbf{ L}ayer\n\\textbf{A}ggregation \\textbf{S}trategy. In particular, we divide the local\nmodel into a feature extractor and a classifier. In each iteration, the clients\nonly upload the parameters of a feature extractor after local training. The\nserver then aggregates these local parameters and returns the results to the\nclients.\n  Each client retains its own classifier layer, ensuring that the backdoor\nlabels do not impact other clients. We assess the effectiveness of FL-PLAS\nagainst state-of-the-art (SOTA) backdoor attacks on three image datasets and\ncompare our approach to six defense strategies. The results of the experiment\ndemonstrate that our methods can effectively protect local models from backdoor\nattacks. Without requiring any auxiliary dataset for the server, our method\nachieves a high main-task accuracy with a lower backdoor accuracy even under\nthe condition of 90\\% malicious users with the attacks of trigger, semantic and\nedge-case."
    },
    {
        "date": "2025-05",
        "title": "Black-box Adversaries from Latent Space: Unnoticeable Attacks on Human Pose and Shape Estimation",
        "author": "Zhiying Li, Guanggang Geng, Yeying Jin, Zhizhi Guo, Bruce Gu, Jidong Huo, Zhaoxin Fan, and Wenjun Wu",
        "link": "http://arxiv.org/abs/2505.12009v1",
        "abstract": "Expressive human pose and shape (EHPS) estimation is vital for digital human\ngeneration, particularly in live-streaming applications. However, most existing\nEHPS models focus primarily on minimizing estimation errors, with limited\nattention on potential security vulnerabilities. Current adversarial attacks on\nEHPS models often require white-box access (e.g., model details or gradients)\nor generate visually conspicuous perturbations, limiting their practicality and\nability to expose real-world security threats. To address these limitations, we\npropose a novel Unnoticeable Black-Box Attack (UBA) against EHPS models. UBA\nleverages the latent-space representations of natural images to generate an\noptimal adversarial noise pattern and iteratively refine its attack potency\nalong an optimized direction in digital space. Crucially, this process relies\nsolely on querying the model's output, requiring no internal knowledge of the\nEHPS architecture, while guiding the noise optimization toward greater stealth\nand effectiveness. Extensive experiments and visual analyses demonstrate the\nsuperiority of UBA. Notably, UBA increases the pose estimation errors of EHPS\nmodels by 17.27%-58.21% on average, revealing critical vulnerabilities. These\nfindings underscore the urgent need to address and mitigate security risks\nassociated with digital human generation systems."
    },
    {
        "date": "2025-05",
        "title": "TechniqueRAG: Retrieval Augmented Generation for Adversarial Technique Annotation in Cyber Threat Intelligence Text",
        "author": "Ahmed Lekssays, Utsav Shukla, Husrev Taha Sencar, and Md Rizwan Parvez",
        "link": "http://arxiv.org/abs/2505.11988v1",
        "abstract": "Accurately identifying adversarial techniques in security texts is critical\nfor effective cyber defense. However, existing methods face a fundamental\ntrade-off: they either rely on generic models with limited domain precision or\nrequire resource-intensive pipelines that depend on large labeled datasets and\ntask-specific optimizations, such as custom hard-negative mining and denoising,\nresources rarely available in specialized domains.\n  We propose TechniqueRAG, a domain-specific retrieval-augmented generation\n(RAG) framework that bridges this gap by integrating off-the-shelf retrievers,\ninstruction-tuned LLMs, and minimal text-technique pairs. Our approach\naddresses data scarcity by fine-tuning only the generation component on limited\nin-domain examples, circumventing the need for resource-intensive retrieval\ntraining. While conventional RAG mitigates hallucination by coupling retrieval\nand generation, its reliance on generic retrievers often introduces noisy\ncandidates, limiting domain-specific precision. To address this, we enhance\nretrieval quality and domain specificity through zero-shot LLM re-ranking,\nwhich explicitly aligns retrieved candidates with adversarial techniques.\n  Experiments on multiple security benchmarks demonstrate that TechniqueRAG\nachieves state-of-the-art performance without extensive task-specific\noptimizations or labeled data, while comprehensive analysis provides further\ninsights."
    },
    {
        "date": "2025-05",
        "title": "Adversarial Robustness for Unified Multi-Modal Encoders via Efficient Calibration",
        "author": "Chih-Ting Liao, Bin Ren, Guofeng Mei, and Xu Zheng",
        "link": "http://arxiv.org/abs/2505.11895v1",
        "abstract": "Recent unified multi-modal encoders align a wide range of modalities into a\nshared representation space, enabling diverse cross-modal tasks. Despite their\nimpressive capabilities, the robustness of these models under adversarial\nperturbations remains underexplored, which is a critical concern for\nsafety-sensitive applications. In this work, we present the first comprehensive\nstudy of adversarial vulnerability in unified multi-modal encoders. We find\nthat even mild adversarial perturbations lead to substantial performance drops\nacross all modalities. Non-visual inputs, such as audio and point clouds, are\nespecially fragile, while visual inputs like images and videos also degrade\nsignificantly. To address this, we propose an efficient adversarial calibration\nframework that improves robustness across modalities without modifying\npretrained encoders or semantic centers, ensuring compatibility with existing\nfoundation models. Our method introduces modality-specific projection heads\ntrained solely on adversarial examples, while keeping the backbone and\nembeddings frozen. We explore three training objectives: fixed-center\ncross-entropy, clean-to-adversarial L2 alignment, and clean-adversarial\nInfoNCE, and we introduce a regularization strategy to ensure\nmodality-consistent alignment under attack. Experiments on six modalities and\nthree Bind-style models show that our method improves adversarial robustness by\nup to 47.3 percent at epsilon = 4/255, while preserving or even improving clean\nzero-shot and retrieval performance with less than 1 percent trainable\nparameters."
    },
    {
        "date": "2025-05",
        "title": "Facial Recognition Leveraging Generative Adversarial Networks",
        "author": "Zhongwen Li, Zongwei Li, and Xiaoqi Li",
        "link": "http://arxiv.org/abs/2505.11884v1",
        "abstract": "Face recognition performance based on deep learning heavily relies on\nlarge-scale training data, which is often difficult to acquire in practical\napplications. To address this challenge, this paper proposes a GAN-based data\naugmentation method with three key contributions: (1) a residual-embedded\ngenerator to alleviate gradient vanishing/exploding problems, (2) an Inception\nResNet-V1 based FaceNet discriminator for improved adversarial training, and\n(3) an end-to-end framework that jointly optimizes data generation and\nrecognition performance. Experimental results demonstrate that our approach\nachieves stable training dynamics and significantly improves face recognition\naccuracy by 12.7% on the LFW benchmark compared to baseline methods, while\nmaintaining good generalization capability with limited training samples."
    },
    {
        "date": "2025-05",
        "title": "AES-RV: Hardware-Efficient RISC-V Accelerator with Low-Latency AES Instruction Extension for IoT Security",
        "author": "Van Tinh Nguyen, Phuc Hung Pham, Vu Trung Duong Le, Hoai Luan Pham, Tuan Hai Vu, and Thi Diem Tran",
        "link": "http://arxiv.org/abs/2505.11880v1",
        "abstract": "The Advanced Encryption Standard (AES) is a widely adopted cryptographic\nalgorithm essential for securing embedded systems and IoT platforms. However,\nexisting AES hardware accelerators often face limitations in performance,\nenergy efficiency, and flexibility. This paper presents AES-RV, a\nhardware-efficient RISC-V accelerator featuring low-latency AES instruction\nextensions optimized for real-time processing across all AES modes and key\nsizes. AES-RV integrates three key innovations: high-bandwidth internal buffers\nfor continuous data processing, a specialized AES unit with custom low-latency\ninstructions, and a pipelined system supported by a ping-pong memory transfer\nmechanism. Implemented on the Xilinx ZCU102 SoC FPGA, AES-RV achieves up to\n255.97 times speedup and up to 453.04 times higher energy efficiency compared\nto baseline and conventional CPU/GPU platforms. It also demonstrates superior\nthroughput and area efficiency against state-of-the-art AES accelerators,\nmaking it a strong candidate for secure and high-performance embedded systems."
    },
    {
        "date": "2025-05",
        "title": "On Membership Inference Attacks in Knowledge Distillation",
        "author": "Ziyao Cui, Minxing Zhang, and Jian Pei",
        "link": "http://arxiv.org/abs/2505.11837v1",
        "abstract": "Nowadays, Large Language Models (LLMs) are trained on huge datasets, some\nincluding sensitive information. This poses a serious privacy concern because\nprivacy attacks such as Membership Inference Attacks (MIAs) may detect this\nsensitive information. While knowledge distillation compresses LLMs into\nefficient, smaller student models, its impact on privacy remains underexplored.\nIn this paper, we investigate how knowledge distillation affects model\nrobustness against MIA. We focus on two questions. First, how is private data\nprotected in teacher and student models? Second, how can we strengthen privacy\npreservation against MIAs in knowledge distillation? Through comprehensive\nexperiments, we show that while teacher and student models achieve similar\noverall MIA accuracy, teacher models better protect member data, the primary\ntarget of MIA, whereas student models better protect non-member data. To\naddress this vulnerability in student models, we propose 5 privacy-preserving\ndistillation methods and demonstrate that they successfully reduce student\nmodels' vulnerability to MIA, with ensembling further stabilizing the\nrobustness, offering a reliable approach for distilling more secure and\nefficient student models. Our implementation source code is available at\nhttps://github.com/richardcui18/MIA_in_KD."
    },
    {
        "date": "2025-05",
        "title": "Multilingual Collaborative Defense for Large Language Models",
        "author": "Hongliang Li, Jinan Xu, Gengping Cui, Changhao Guan, Fengran Mo, and Kaiyu Huang",
        "link": "http://arxiv.org/abs/2505.11835v1",
        "abstract": "The robustness and security of large language models (LLMs) has become a\nprominent research area. One notable vulnerability is the ability to bypass LLM\nsafeguards by translating harmful queries into rare or underrepresented\nlanguages, a simple yet effective method of \"jailbreaking\" these models.\nDespite the growing concern, there has been limited research addressing the\nsafeguarding of LLMs in multilingual scenarios, highlighting an urgent need to\nenhance multilingual safety. In this work, we investigate the correlation\nbetween various attack features across different languages and propose\nMultilingual Collaborative Defense (MCD), a novel learning method that\noptimizes a continuous, soft safety prompt automatically to facilitate\nmultilingual safeguarding of LLMs. The MCD approach offers three advantages:\nFirst, it effectively improves safeguarding performance across multiple\nlanguages. Second, MCD maintains strong generalization capabilities while\nminimizing false refusal rates. Third, MCD mitigates the language safety\nmisalignment caused by imbalances in LLM training corpora. To evaluate the\neffectiveness of MCD, we manually construct multilingual versions of commonly\nused jailbreak benchmarks, such as MaliciousInstruct and AdvBench, to assess\nvarious safeguarding methods. Additionally, we introduce these datasets in\nunderrepresented (zero-shot) languages to verify the language transferability\nof MCD. The results demonstrate that MCD outperforms existing approaches in\nsafeguarding against multilingual jailbreak attempts while also exhibiting\nstrong language transfer capabilities. Our code is available at\nhttps://github.com/HLiang-Lee/MCD."
    },
    {
        "date": "2025-05",
        "title": "Robust Cross-View Geo-Localization via Content-Viewpoint Disentanglement",
        "author": "Ke Li, Di Wang, Xiaowei Wang, Zhihong Wu, Yiming Zhang, Yifeng Wang, and Quan Wang",
        "link": "http://arxiv.org/abs/2505.11822v1",
        "abstract": "Cross-view geo-localization (CVGL) aims to match images of the same\ngeographic location captured from different perspectives, such as drones and\nsatellites. Despite recent advances, CVGL remains highly challenging due to\nsignificant appearance changes and spatial distortions caused by viewpoint\nvariations. Existing methods typically assume that cross-view images can be\ndirectly aligned within a shared feature space by maximizing feature similarity\nthrough contrastive learning. Nonetheless, this assumption overlooks the\ninherent conflicts induced by viewpoint discrepancies, resulting in extracted\nfeatures containing inconsistent information that hinders precise localization.\nIn this study, we take a manifold learning perspective and model the feature\nspace of cross-view images as a composite manifold jointly governed by content\nand viewpoint information. Building upon this insight, we propose\n$\\textbf{CVD}$, a new CVGL framework that explicitly disentangles\n$\\textit{content}$ and $\\textit{viewpoint}$ factors. To promote effective\ndisentanglement, we introduce two constraints: $\\textit{(i)}$ An intra-view\nindependence constraint, which encourages statistical independence between the\ntwo factors by minimizing their mutual information. $\\textit{(ii)}$ An\ninter-view reconstruction constraint that reconstructs each view by\ncross-combining $\\textit{content}$ and $\\textit{viewpoint}$ from paired images,\nensuring factor-specific semantics are preserved. As a plug-and-play module,\nCVD can be seamlessly integrated into existing geo-localization pipelines.\nExtensive experiments on four benchmarks, i.e., University-1652, SUES-200,\nCVUSA, and CVACT, demonstrate that CVD consistently improves both localization\naccuracy and generalization across multiple baselines."
    },
    {
        "date": "2025-05",
        "title": "UniMoCo: Unified Modality Completion for Robust Multi-Modal Embeddings",
        "author": "Jiajun Qin, Yuan Pu, Zhuolun He, Seunggeun Kim, David Z. Pan, and Bei Yu",
        "link": "http://arxiv.org/abs/2505.11815v1",
        "abstract": "Current research has explored vision-language models for multi-modal\nembedding tasks, such as information retrieval, visual grounding, and\nclassification. However, real-world scenarios often involve diverse modality\ncombinations between queries and targets, such as text and image to text, text\nand image to text and image, and text to text and image. These diverse\ncombinations pose significant challenges for existing models, as they struggle\nto align all modality combinations within a unified embedding space during\ntraining, which degrades performance at inference. To address this limitation,\nwe propose UniMoCo, a novel vision-language model architecture designed for\nmulti-modal embedding tasks. UniMoCo introduces a modality-completion module\nthat generates visual features from textual inputs, ensuring modality\ncompleteness for both queries and targets. Additionally, we develop a\nspecialized training strategy to align embeddings from both original and\nmodality-completed inputs, ensuring consistency within the embedding space.\nThis enables the model to robustly handle a wide range of modality combinations\nacross embedding tasks. Experiments show that UniMoCo outperforms previous\nmethods while demonstrating consistent robustness across diverse settings. More\nimportantly, we identify and quantify the inherent bias in conventional\napproaches caused by imbalance of modality combinations in training data, which\ncan be mitigated through our modality-completion paradigm. The code is\navailable at https://github.com/HobbitQia/UniMoCo."
    },
    {
        "date": "2025-05",
        "title": "Are vision language models robust to uncertain inputs?",
        "author": "Xi Wang, and Eric Nalisnick",
        "link": "http://arxiv.org/abs/2505.11804v1",
        "abstract": "Robustness against uncertain and ambiguous inputs is a critical challenge for\ndeep learning models. While recent advancements in large scale vision language\nmodels (VLMs, e.g. GPT4o) might suggest that increasing model and training\ndataset size would mitigate this issue, our empirical evaluation shows a more\ncomplicated picture. Testing models using two classic uncertainty\nquantification tasks, anomaly detection and classification under inherently\nambiguous conditions, we find that newer and larger VLMs indeed exhibit\nimproved robustness compared to earlier models, but still suffer from a\ntendency to strictly follow instructions, often causing them to hallucinate\nconfident responses even when faced with unclear or anomalous inputs.\nRemarkably, for natural images such as ImageNet, this limitation can be\novercome without pipeline modifications: simply prompting models to abstain\nfrom uncertain predictions enables significant reliability gains, achieving\nnear-perfect robustness in several settings. However, for domain-specific tasks\nsuch as galaxy morphology classification, a lack of specialized knowledge\nprevents reliable uncertainty estimation. Finally, we propose a novel mechanism\nbased on caption diversity to reveal a model's internal uncertainty, enabling\npractitioners to predict when models will successfully abstain without relying\non labeled data."
    },
    {
        "date": "2025-05",
        "title": "CL-CaGAN: Capsule differential adversarial continuous learning for cross-domain hyperspectral anomaly detection",
        "author": "Jianing Wang, Siying Guo, Zheng Hua, Runhu Huang, Jinyu Hu, and Maoguo Gong",
        "link": "http://arxiv.org/abs/2505.11793v1",
        "abstract": "Anomaly detection (AD) has attracted remarkable attention in hyperspectral\nimage (HSI) processing fields, and most existing deep learning (DL)-based\nalgorithms indicate dramatic potential for detecting anomaly samples through\nspecific training process under current scenario. However, the limited prior\ninformation and the catastrophic forgetting problem indicate crucial challenges\nfor existing DL structure in open scenarios cross-domain detection. In order to\nimprove the detection performance, a novel continual learning-based capsule\ndifferential generative adversarial network (CL-CaGAN) is proposed to elevate\nthe cross-scenario learning performance for facilitating the real application\nof DL-based structure in hyperspectral AD (HAD) task. First, a modified capsule\nstructure with adversarial learning network is constructed to estimate the\nbackground distribution for surmounting the deficiency of prior information. To\nmitigate the catastrophic forgetting phenomenon, clustering-based sample replay\nstrategy and a designed extra self-distillation regularization are integrated\nfor merging the history and future knowledge in continual AD task, while the\ndiscriminative learning ability from previous detection scenario to current\nscenario is retained by the elaborately designed structure with continual\nlearning (CL) strategy. In addition, the differentiable enhancement is enforced\nto augment the generation performance of the training data. This further\nstabilizes the training process with better convergence and efficiently\nconsolidates the reconstruction ability of background samples. To verify the\neffectiveness of our proposed CL-CaGAN, we conduct experiments on several real\nHSIs, and the results indicate that the proposed CL-CaGAN demonstrates higher\ndetection performance and continuous learning capacity for mitigating the\ncatastrophic forgetting under cross-domain scenarios."
    },
    {
        "date": "2025-05",
        "title": "EnvInjection: Environmental Prompt Injection Attack to Multi-modal Web Agents",
        "author": "Xilong Wang, John Bloch, Zedian Shao, Yuepeng Hu, Shuyan Zhou, and Neil Zhenqiang Gong",
        "link": "http://arxiv.org/abs/2505.11717v1",
        "abstract": "Multi-modal large language model (MLLM)-based web agents interact with\nwebpage environments by generating actions based on screenshots of the\nwebpages. Environmental prompt injection attacks manipulate the environment to\ninduce the web agent to perform a specific, attacker-chosen action--referred to\nas the target action. However, existing attacks suffer from limited\neffectiveness or stealthiness, or are impractical in real-world settings. In\nthis work, we propose EnvInjection, a new attack that addresses these\nlimitations. Our attack adds a perturbation to the raw pixel values of the\nrendered webpage, which can be implemented by modifying the webpage's source\ncode. After these perturbed pixels are mapped into a screenshot, the\nperturbation induces the web agent to perform the target action. We formulate\nthe task of finding the perturbation as an optimization problem. A key\nchallenge in solving this problem is that the mapping between raw pixel values\nand screenshot is non-differentiable, making it difficult to backpropagate\ngradients to the perturbation. To overcome this, we train a neural network to\napproximate the mapping and apply projected gradient descent to solve the\nreformulated optimization problem. Extensive evaluation on multiple webpage\ndatasets shows that EnvInjection is highly effective and significantly\noutperforms existing baselines."
    },
    {
        "date": "2025-05",
        "title": "Co-Evolutionary Defence of Active Directory Attack Graphs via GNN-Approximated Dynamic Programming",
        "author": "Diksha Goel, Hussain Ahmad, Kristen Moore, and Mingyu Guo",
        "link": "http://arxiv.org/abs/2505.11710v1",
        "abstract": "Modern enterprise networks increasingly rely on Active Directory (AD) for\nidentity and access management. However, this centralization exposes a single\npoint of failure, allowing adversaries to compromise high-value assets.\nExisting AD defense approaches often assume static attacker behavior, but\nreal-world adversaries adapt dynamically, rendering such methods brittle. To\naddress this, we model attacker-defender interactions in AD as a Stackelberg\ngame between an adaptive attacker and a proactive defender. We propose a\nco-evolutionary defense framework that combines Graph Neural Network\nApproximated Dynamic Programming (GNNDP) to model attacker strategies, with\nEvolutionary Diversity Optimization (EDO) to generate resilient blocking\nstrategies. To ensure scalability, we introduce a Fixed-Parameter Tractable\n(FPT) graph reduction method that reduces complexity while preserving strategic\nstructure. Our framework jointly refines attacker and defender policies to\nimprove generalization and prevent premature convergence. Experiments on\nsynthetic AD graphs show near-optimal results (within 0.1 percent of optimality\non r500) and improved performance on larger graphs (r1000 and r2000),\ndemonstrating the framework's scalability and effectiveness."
    },
    {
        "date": "2025-05",
        "title": "Joint Graph Estimation and Signal Restoration for Robust Federated Learning",
        "author": "Tsutahiro Fukuhara, Junya Hara, Hiroshi Higashi, and Yuichi Tanaka",
        "link": "http://arxiv.org/abs/2505.11648v1",
        "abstract": "We propose a robust aggregation method for model parameters in federated\nlearning (FL) under noisy communications. FL is a distributed machine learning\nparadigm in which a central server aggregates local model parameters from\nmultiple clients. These parameters are often noisy and/or have missing values\nduring data collection, training, and communication between the clients and\nserver. This may cause a considerable drop in model accuracy. To address this\nissue, we learn a graph that represents pairwise relationships between model\nparameters of the clients during aggregation. We realize it with a joint\nproblem of graph learning and signal (i.e., model parameters) restoration. The\nproblem is formulated as a difference-of-convex (DC) optimization, which is\nefficiently solved via a proximal DC algorithm. Experimental results on MNIST\nand CIFAR-10 datasets show that the proposed method outperforms existing\napproaches by up to $2$--$5\\%$ in classification accuracy under biased data\ndistributions and noisy conditions."
    },
    {
        "date": "2025-05",
        "title": "PeerGuard: Defending Multi-Agent Systems Against Backdoor Attacks Through Mutual Reasoning",
        "author": "Falong Fan, and Xi Li",
        "link": "http://arxiv.org/abs/2505.11642v1",
        "abstract": "Multi-agent systems leverage advanced AI models as autonomous agents that\ninteract, cooperate, or compete to complete complex tasks across applications\nsuch as robotics and traffic management. Despite their growing importance,\nsafety in multi-agent systems remains largely underexplored, with most research\nfocusing on single AI models rather than interacting agents. This work\ninvestigates backdoor vulnerabilities in multi-agent systems and proposes a\ndefense mechanism based on agent interactions. By leveraging reasoning\nabilities, each agent evaluates responses from others to detect illogical\nreasoning processes, which indicate poisoned agents. Experiments on LLM-based\nmulti-agent systems, including ChatGPT series and Llama 3, demonstrate the\neffectiveness of the proposed method, achieving high accuracy in identifying\npoisoned agents while minimizing false positives on clean agents. We believe\nthis work provides insights into multi-agent system safety and contributes to\nthe development of robust, trustworthy AI interactions."
    },
    {
        "date": "2025-05",
        "title": "Adaptive Robust Optimization with Data-Driven Uncertainty for Enhancing Distribution System Resilience",
        "author": "Shuyi Chen, Shixiang Zhu, and Ramteen Sioshansi",
        "link": "http://arxiv.org/abs/2505.11627v1",
        "abstract": "Extreme weather events are placing growing strain on electric power systems,\nexposing the limitations of purely reactive responses and prompting the need\nfor proactive resilience planning. However, existing approaches often rely on\nsimplified uncertainty models and decouple proactive and reactive decisions,\noverlooking their critical interdependence. This paper proposes a novel\ntri-level optimization framework that integrates proactive infrastructure\ninvestment, adversarial modeling of spatio-temporal disruptions, and adaptive\nreactive response. We construct high-probability, distribution-free uncertainty\nsets using conformal prediction to capture complex and data-scarce outage\npatterns. To solve the resulting nested decision problem, we derive a bi-level\nreformulation via strong duality and develop a scalable Benders decomposition\nalgorithm. Experiments on both real and synthetic data demonstrate that our\napproach consistently outperforms conventional robust and two-stage methods,\nachieving lower worst-case losses and more efficient resource allocation,\nespecially under tight operational constraints and large-scale uncertainty."
    }
]