[
    {
        "date": "2025-10",
        "title": "Adversarial Attacks Leverage Interference Between Features in Superposition",
        "author": "Edward Stevinson, Lucas Prieto, Melih Barsbey, and Tolga Birdal",
        "link": "http://arxiv.org/abs/2510.11709v1",
        "abstract": "Fundamental questions remain about when and why adversarial examples arise in\nneural networks, with competing views characterising them either as artifacts\nof the irregularities in the decision landscape or as products of sensitivity\nto non-robust input features. In this paper, we instead argue that adversarial\nvulnerability can stem from efficient information encoding in neural networks.\nSpecifically, we show how superposition - where networks represent more\nfeatures than they have dimensions - creates arrangements of latent\nrepresentations that adversaries can exploit. We demonstrate that adversarial\nperturbations leverage interference between superposed features, making attack\npatterns predictable from feature arrangements. Our framework provides a\nmechanistic explanation for two known phenomena: adversarial attack\ntransferability between models with similar training regimes and class-specific\nvulnerability patterns. In synthetic settings with precisely controlled\nsuperposition, we establish that superposition suffices to create adversarial\nvulnerability. We then demonstrate that these findings persist in a ViT trained\non CIFAR-10. These findings reveal adversarial vulnerability can be a byproduct\nof networks' representational compression, rather than flaws in the learning\nprocess or non-robust inputs."
    },
    {
        "date": "2025-10",
        "title": "Robust Ego-Exo Correspondence with Long-Term Memory",
        "author": "Yijun Hu, Bing Fan, Xin Gu, Haiqing Ren, Dongfang Liu, Heng Fan, and Libo Zhang",
        "link": "http://arxiv.org/abs/2510.11417v1",
        "abstract": "Establishing object-level correspondence between egocentric and exocentric\nviews is essential for intelligent assistants to deliver precise and intuitive\nvisual guidance. However, this task faces numerous challenges, including\nextreme viewpoint variations, occlusions, and the presence of small objects.\nExisting approaches usually borrow solutions from video object segmentation\nmodels, but still suffer from the aforementioned challenges. Recently, the\nSegment Anything Model 2 (SAM 2) has shown strong generalization capabilities\nand excellent performance in video object segmentation. Yet, when simply\napplied to the ego-exo correspondence (EEC) task, SAM 2 encounters severe\ndifficulties due to ineffective ego-exo feature fusion and limited long-term\nmemory capacity, especially for long videos. Addressing these problems, we\npropose a novel EEC framework based on SAM 2 with long-term memories by\npresenting a dual-memory architecture and an adaptive feature routing module\ninspired by Mixture-of-Experts (MoE). Compared to SAM 2, our approach features\n(i) a Memory-View MoE module which consists of a dual-branch routing mechanism\nto adaptively assign contribution weights to each expert feature along both\nchannel and spatial dimensions, and (ii) a dual-memory bank system with a\nsimple yet effective compression strategy to retain critical long-term\ninformation while eliminating redundancy. In the extensive experiments on the\nchallenging EgoExo4D benchmark, our method, dubbed LM-EEC, achieves new\nstate-of-the-art results and significantly outperforms existing methods and the\nSAM 2 baseline, showcasing its strong generalization across diverse scenarios.\nOur code and model are available at https://github.com/juneyeeHu/LM-EEC."
    },
    {
        "date": "2025-10",
        "title": "Living Off the LLM: How LLMs Will Change Adversary Tactics",
        "author": "Sean Oesch, Jack Hutchins, Luke Koch, and Kevin Kurian",
        "link": "http://arxiv.org/abs/2510.11398v1",
        "abstract": "In living off the land attacks, malicious actors use legitimate tools and\nprocesses already present on a system to avoid detection. In this paper, we\nexplore how the on-device LLMs of the future will become a security concern as\nthreat actors integrate LLMs into their living off the land attack pipeline and\nways the security community may mitigate this threat."
    },
    {
        "date": "2025-10",
        "title": "TDADL-IE: A Deep Learning-Driven Cryptographic Architecture for Medical Image Security",
        "author": "Junhua Zhou, Quanjun Li, Weixuan Li, Guang Yu, Yihua Shao, Yihang Dong, Mengqian Wang, Zimeng Li, Changwei Gong, and Xuhang Chen",
        "link": "http://arxiv.org/abs/2510.11301v1",
        "abstract": "The rise of digital medical imaging, like MRI and CT, demands strong\nencryption to protect patient data in telemedicine and cloud storage. Chaotic\nsystems are popular for image encryption due to their sensitivity and unique\ncharacteristics, but existing methods often lack sufficient security. This\npaper presents the Three-dimensional Diffusion Algorithm and Deep Learning\nImage Encryption system (TDADL-IE), built on three key elements. First, we\npropose an enhanced chaotic generator using an LSTM network with a 1D-Sine\nQuadratic Chaotic Map (1D-SQCM) for better pseudorandom sequence generation.\nNext, a new three-dimensional diffusion algorithm (TDA) is applied to encrypt\npermuted images. TDADL-IE is versatile for images of any size. Experiments\nconfirm its effectiveness against various security threats. The code is\navailable at\n\\href{https://github.com/QuincyQAQ/TDADL-IE}{https://github.com/QuincyQAQ/TDADL-IE}."
    },
    {
        "date": "2025-10",
        "title": "Collaborative Shadows: Distributed Backdoor Attacks in LLM-Based Multi-Agent Systems",
        "author": "Pengyu Zhu, Lijun Li, Yaxing Lyu, Li Sun, Sen Su, and Jing Shao",
        "link": "http://arxiv.org/abs/2510.11246v1",
        "abstract": "LLM-based multi-agent systems (MAS) demonstrate increasing integration into\nnext-generation applications, but their safety in backdoor attacks remains\nlargely underexplored. However, existing research has focused exclusively on\nsingle-agent backdoor attacks, overlooking the novel attack surfaces introduced\nby agent collaboration in MAS. To bridge this gap, we present the first\nDistributed Backdoor Attack tailored to MAS. We decompose the backdoor into\nmultiple distributed attack primitives that are embedded within MAS tools.\nThese primitives remain dormant individually but collectively activate only\nwhen agents collaborate in a specific sequence, thereby assembling the full\nbackdoor to execute targeted attacks such as data exfiltration. To fully assess\nthis threat, we introduce a benchmark for multi-role collaborative tasks and a\nsandboxed framework to evaluate. Extensive experiments demonstrate that our\nattack achieves an attack success rate exceeding 95% without degrading\nperformance on benign tasks. This work exposes novel backdoor attack surfaces\nthat exploit agent collaboration, underscoring the need to move beyond\nsingle-agent protection. Code and benchmark are available at\nhttps://github.com/whfeLingYu/Distributed-Backdoor-Attacks-in-MAS."
    },
    {
        "date": "2025-10",
        "title": "Attacks by Content: Automated Fact-checking is an AI Security Issue",
        "author": "Michael Schlichtkrull",
        "link": "http://arxiv.org/abs/2510.11238v1",
        "abstract": "When AI agents retrieve and reason over external documents, adversaries can\nmanipulate the data they receive to subvert their behaviour. Previous research\nhas studied indirect prompt injection, where the attacker injects malicious\ninstructions. We argue that injection of instructions is not necessary to\nmanipulate agents - attackers could instead supply biased, misleading, or false\ninformation. We term this an attack by content. Existing defenses, which focus\non detecting hidden commands, are ineffective against attacks by content. To\ndefend themselves and their users, agents must critically evaluate retrieved\ninformation, corroborating claims with external evidence and evaluating source\ntrustworthiness. We argue that this is analogous to an existing NLP task,\nautomated fact-checking, which we propose to repurpose as a cognitive\nself-defense tool for agents."
    },
    {
        "date": "2025-10",
        "title": "TraceAegis: Securing LLM-Based Agents via Hierarchical and Behavioral Anomaly Detection",
        "author": "Jiahao Liu, Bonan Ruan, Xianglin Yang, Zhiwei Lin, Yan Liu, Yang Wang, Tao Wei, and Zhenkai Liang",
        "link": "http://arxiv.org/abs/2510.11203v1",
        "abstract": "LLM-based agents have demonstrated promising adaptability in real-world\napplications. However, these agents remain vulnerable to a wide range of\nattacks, such as tool poisoning and malicious instructions, that compromise\ntheir execution flow and can lead to serious consequences like data breaches\nand financial loss. Existing studies typically attempt to mitigate such\nanomalies by predefining specific rules and enforcing them at runtime to\nenhance safety. Yet, designing comprehensive rules is difficult, requiring\nextensive manual effort and still leaving gaps that result in false negatives.\nAs agent systems evolve into complex software systems, we take inspiration from\nsoftware system security and propose TraceAegis, a provenance-based analysis\nframework that leverages agent execution traces to detect potential anomalies.\nIn particular, TraceAegis constructs a hierarchical structure to abstract\nstable execution units that characterize normal agent behaviors. These units\nare then summarized into constrained behavioral rules that specify the\nconditions necessary to complete a task. By validating execution traces against\nboth hierarchical and behavioral constraints, TraceAegis is able to effectively\ndetect abnormal behaviors. To evaluate the effectiveness of TraceAegis, we\nintroduce TraceAegis-Bench, a dataset covering two representative scenarios:\nhealthcare and corporate procurement. Each scenario includes 1,300 benign\nbehaviors and 300 abnormal behaviors, where the anomalies either violate the\nagent's execution order or break the semantic consistency of its execution\nsequence. Experimental results demonstrate that TraceAegis achieves strong\nperformance on TraceAegis-Bench, successfully identifying the majority of\nabnormal behaviors."
    },
    {
        "date": "2025-10",
        "title": "RAG-Pull: Imperceptible Attacks on RAG Systems for Code Generation",
        "author": "Vasilije Stambolic, Aritra Dhar, and Lukas Cavigelli",
        "link": "http://arxiv.org/abs/2510.11195v1",
        "abstract": "Retrieval-Augmented Generation (RAG) increases the reliability and\ntrustworthiness of the LLM response and reduces hallucination by eliminating\nthe need for model retraining. It does so by adding external data into the\nLLM's context. We develop a new class of black-box attack, RAG-Pull, that\ninserts hidden UTF characters into queries or external code repositories,\nredirecting retrieval toward malicious code, thereby breaking the models'\nsafety alignment. We observe that query and code perturbations alone can shift\nretrieval toward attacker-controlled snippets, while combined query-and-target\nperturbations achieve near-perfect success. Once retrieved, these snippets\nintroduce exploitable vulnerabilities such as remote code execution and SQL\ninjection. RAG-Pull's minimal perturbations can alter the model's safety\nalignment and increase preference towards unsafe code, therefore opening up a\nnew class of attacks on LLMs."
    },
    {
        "date": "2025-10",
        "title": "Aligning Deep Implicit Preferences by Learning to Reason Defensively",
        "author": "Peiming Li, Zhiyuan Hu, Yang Tang, Shiyu Li, and Xi Chen",
        "link": "http://arxiv.org/abs/2510.11194v1",
        "abstract": "Personalized alignment is crucial for enabling Large Language Models (LLMs)\nto engage effectively in user-centric interactions. However, current methods\nface a dual challenge: they fail to infer users' deep implicit preferences\n(including unstated goals, semantic context and risk tolerances), and they lack\nthe defensive reasoning required to navigate real-world ambiguity. This\ncognitive gap leads to responses that are superficial, brittle and\nshort-sighted. To address this, we propose Critique-Driven Reasoning Alignment\n(CDRA), which reframes alignment from a scalar reward-matching task into a\nstructured reasoning process. First, to bridge the preference inference gap, we\nintroduce the DeepPref benchmark. This dataset, comprising 3000\npreference-query pairs across 20 topics, is curated by simulating a\nmulti-faceted cognitive council that produces critique-annotated reasoning\nchains to deconstruct query semantics and reveal latent risks. Second, to\ninstill defensive reasoning, we introduce the Personalized Generative Process\nReward Model (Pers-GenPRM), which frames reward modeling as a personalized\nreasoning task. It generates a critique chain to evaluate a response's\nalignment with user preferences before outputting a final score based on this\nrationale. Ultimately, this interpretable, structured reward signal guides\npolicy model through Critique-Driven Policy Alignment, a process-level online\nreinforcement learning algorithm integrating both numerical and natural\nlanguage feedback. Experiments demonstrate that CDRA excels at discovering and\naligning with users' true preferences while executing robust reasoning. Our\ncode and dataset are available at https://github.com/Zephyrian-Hugh/Deep-pref."
    },
    {
        "date": "2025-10",
        "title": "TypePilot: Leveraging the Scala Type System for Secure LLM-generated Code",
        "author": "Alexander Sternfeld, Andrei Kucharavy, and Ljiljana Dolamic",
        "link": "http://arxiv.org/abs/2510.11151v1",
        "abstract": "Large language Models (LLMs) have shown remarkable proficiency in code\ngeneration tasks across various programming languages. However, their outputs\noften contain subtle but critical vulnerabilities, posing significant risks\nwhen deployed in security-sensitive or mission-critical systems. This paper\nintroduces TypePilot, an agentic AI framework designed to enhance the security\nand robustness of LLM-generated code by leveraging strongly typed and\nverifiable languages, using Scala as a representative example. We evaluate the\neffectiveness of our approach in two settings: formal verification with the\nStainless framework and general-purpose secure code generation. Our experiments\nwith leading open-source LLMs reveal that while direct code generation often\nfails to enforce safety constraints, just as naive prompting for more secure\ncode, our type-focused agentic pipeline substantially mitigates input\nvalidation and injection vulnerabilities. The results demonstrate the potential\nof structured, type-guided LLM workflows to improve the SotA of the\ntrustworthiness of automated code generation in high-assurance domains."
    },
    {
        "date": "2025-10",
        "title": "CoSPED: Consistent Soft Prompt Targeted Data Extraction and Defense",
        "author": "Yang Zhuochen, Fok Kar Wai, and Thing Vrizlynn",
        "link": "http://arxiv.org/abs/2510.11137v1",
        "abstract": "Large language models have gained widespread attention recently, but their\npotential security vulnerabilities, especially privacy leakage, are also\nbecoming apparent. To test and evaluate for data extraction risks in LLM, we\nproposed CoSPED, short for Consistent Soft Prompt targeted data Extraction and\nDefense. We introduce several innovative components, including Dynamic Loss,\nAdditive Loss, Common Loss, and Self Consistency Decoding Strategy, and tested\nto enhance the consistency of the soft prompt tuning process. Through extensive\nexperimentation with various combinations, we achieved an extraction rate of\n65.2% at a 50-token prefix comparison. Our comparisons of CoSPED with other\nreference works confirm our superior extraction rates. We evaluate CoSPED on\nmore scenarios, achieving Pythia model extraction rate of 51.7% and introducing\ncross-model comparison. Finally, we explore defense through Rank-One Model\nEditing and achieve a reduction in the extraction rate to 1.6%, which proves\nthat our analysis of extraction mechanisms can directly inform effective\nmitigation strategies against soft prompt-based attacks."
    },
    {
        "date": "2025-10",
        "title": "PhysioME: A Robust Multimodal Self-Supervised Framework for Physiological Signals with Missing Modalities",
        "author": "Cheol-Hui Lee, Hwa-Yeon Lee, Min-Kyung Jung, and Dong-Joo Kim",
        "link": "http://arxiv.org/abs/2510.11110v1",
        "abstract": "Missing or corrupted modalities are common in physiological signal-based\nmedical applications owing to hardware constraints or motion artifacts.\nHowever, most existing methods assume the availability of all modalities,\nresulting in substantial performance degradation in the absence of any\nmodality. To overcome this limitation, this study proposes PhysioME, a robust\nframework designed to ensure reliable performance under missing modality\nconditions. PhysioME adopts: (1) a multimodal self-supervised learning approach\nthat combines contrastive learning with masked prediction; (2) a\nDual-PathNeuroNet backbone tailored to capture the temporal dynamics of each\nphysiological signal modality; and (3) a restoration decoder that reconstructs\nmissing modality tokens, enabling flexible processing of incomplete inputs. The\nexperimental results show that PhysioME achieves high consistency and\ngeneralization performance across various missing modality scenarios. These\nfindings highlight the potential of PhysioME as a reliable tool for supporting\nclinical decision-making in real-world settings with imperfect data\navailability."
    },
    {
        "date": "2025-10",
        "title": "CoDefend: Cross-Modal Collaborative Defense via Diffusion Purification and Prompt Optimization",
        "author": "Fengling Zhu, Boshi Liu, Jingyu Hua, and Sheng Zhong",
        "link": "http://arxiv.org/abs/2510.11096v1",
        "abstract": "Multimodal Large Language Models (MLLMs) have achieved remarkable success in\ntasks such as image captioning, visual question answering, and cross-modal\nreasoning by integrating visual and textual modalities. However, their\nmultimodal nature also exposes them to adversarial threats, where attackers can\nperturb either modality or both jointly to induce harmful, misleading, or\npolicy violating outputs. Existing defense strategies, such as adversarial\ntraining and input purification, face notable limitations: adversarial training\ntypically improves robustness only against known attacks while incurring high\ncomputational costs, whereas conventional purification approaches often suffer\nfrom degraded image quality and insufficient generalization to complex\nmultimodal tasks.\n  In this work, we focus on defending the visual modality, which frequently\nserves as the primary entry point for adversarial manipulation. We propose a\nsupervised diffusion based denoising framework that leverages paired\nadversarial clean image datasets to fine-tune diffusion models with\ndirectional, task specific guidance. Unlike prior unsupervised purification\nmethods such as DiffPure, our approach achieves higher quality reconstructions\nwhile significantly improving defense robustness in multimodal tasks.\nFurthermore, we incorporate prompt optimization as a complementary defense\nmechanism, enhancing resistance against diverse and unseen attack strategies.\n  Extensive experiments on image captioning and visual question answering\ndemonstrate that our method not only substantially improves robustness but also\nexhibits strong transferability to unknown adversarial attacks. These results\nhighlight the effectiveness of supervised diffusion based denoising for\nmultimodal defense, paving the way for more reliable and secure deployment of\nMLLMs in real world applications."
    },
    {
        "date": "2025-10",
        "title": "Robust Photoplethysmography Signal Denoising via Mamba Networks",
        "author": "I Chiu, Yu-Tung Liu, Kuan-Chen Wang, Hung-Yu Wei, and Yu Tsao",
        "link": "http://arxiv.org/abs/2510.11058v1",
        "abstract": "Photoplethysmography (PPG) is widely used in wearable health monitoring, but\nits reliability is often degraded by noise and motion artifacts, limiting\ndownstream applications such as heart rate (HR) estimation. This paper presents\na deep learning framework for PPG denoising with an emphasis on preserving\nphysiological information. In this framework, we propose DPNet, a Mamba-based\ndenoising backbone designed for effective temporal modeling. To further enhance\ndenoising performance, the framework also incorporates a scale-invariant\nsignal-to-distortion ratio (SI-SDR) loss to promote waveform fidelity and an\nauxiliary HR predictor (HRP) that provides physiological consistency through\nHR-based supervision. Experiments on the BIDMC dataset show that our method\nachieves strong robustness against both synthetic noise and real-world motion\nartifacts, outperforming conventional filtering and existing neural models. Our\nmethod can effectively restore PPG signals while maintaining HR accuracy,\nhighlighting the complementary roles of SI-SDR loss and HR-guided supervision.\nThese results demonstrate the potential of our approach for practical\ndeployment in wearable healthcare systems."
    },
    {
        "date": "2025-10",
        "title": "The Easy Path to Robustness: Coreset Selection using Sample Hardness",
        "author": "Pranav Ramesh, Arjun Roy, Deepak Ravikumar, Kaushik Roy, and Gopalakrishnan Srinivasan",
        "link": "http://arxiv.org/abs/2510.11018v1",
        "abstract": "Designing adversarially robust models from a data-centric perspective\nrequires understanding which input samples are most crucial for learning\nresilient features. While coreset selection provides a mechanism for efficient\ntraining on data subsets, current algorithms are designed for clean accuracy\nand fall short in preserving robustness. To address this, we propose a\nframework linking a sample's adversarial vulnerability to its\n\\textit{hardness}, which we quantify using the average input gradient norm\n(AIGN) over training. We demonstrate that \\textit{easy} samples (with low AIGN)\nare less vulnerable and occupy regions further from the decision boundary.\nLeveraging this insight, we present EasyCore, a coreset selection algorithm\nthat retains only the samples with low AIGN for training. We empirically show\nthat models trained on EasyCore-selected data achieve significantly higher\nadversarial accuracy than those trained with competing coreset methods under\nboth standard and adversarial training. As AIGN is a model-agnostic dataset\nproperty, EasyCore is an efficient and widely applicable data-centric method\nfor improving adversarial robustness. We show that EasyCore achieves up to 7\\%\nand 5\\% improvement in adversarial accuracy under standard training and TRADES\nadversarial training, respectively, compared to existing coreset methods."
    },
    {
        "date": "2025-10",
        "title": "Adversarial Robustness in One-Stage Learning-to-Defer",
        "author": "Yannis Montreuil, Letian Yu, Axel Carlier, Lai Xing Ng, and Wei Tsang Ooi",
        "link": "http://arxiv.org/abs/2510.10988v1",
        "abstract": "Learning-to-Defer (L2D) enables hybrid decision-making by routing inputs\neither to a predictor or to external experts. While promising, L2D is highly\nvulnerable to adversarial perturbations, which can not only flip predictions\nbut also manipulate deferral decisions. Prior robustness analyses focus solely\non two-stage settings, leaving open the end-to-end (one-stage) case where\npredictor and allocation are trained jointly. We introduce the first framework\nfor adversarial robustness in one-stage L2D, covering both classification and\nregression. Our approach formalizes attacks, proposes cost-sensitive\nadversarial surrogate losses, and establishes theoretical guarantees including\n$\\mathcal{H}$, $(\\mathcal{R }, \\mathcal{F})$, and Bayes consistency.\nExperiments on benchmark datasets confirm that our methods improve robustness\nagainst untargeted and targeted attacks while preserving clean performance."
    },
    {
        "date": "2025-10",
        "title": "DITTO: A Spoofing Attack Framework on Watermarked LLMs via Knowledge Distillation",
        "author": "Hyeseon Ahn, Shinwoo Park, and Yo-Sub Han",
        "link": "http://arxiv.org/abs/2510.10987v1",
        "abstract": "The promise of LLM watermarking rests on a core assumption that a specific\nwatermark proves authorship by a specific model. We demonstrate that this\nassumption is dangerously flawed. We introduce the threat of watermark\nspoofing, a sophisticated attack that allows a malicious model to generate text\ncontaining the authentic-looking watermark of a trusted, victim model. This\nenables the seamless misattribution of harmful content, such as disinformation,\nto reputable sources. The key to our attack is repurposing watermark\nradioactivity, the unintended inheritance of data patterns during fine-tuning,\nfrom a discoverable trait into an attack vector. By distilling knowledge from a\nwatermarked teacher model, our framework allows an attacker to steal and\nreplicate the watermarking signal of the victim model. This work reveals a\ncritical security gap in text authorship verification and calls for a paradigm\nshift towards technologies capable of distinguishing authentic watermarks from\nexpertly imitated ones. Our code is available at\nhttps://github.com/hsannn/ditto.git."
    },
    {
        "date": "2025-10",
        "title": "APLOT: Robust Reward Modeling via Adaptive Preference Learning with Optimal Transport",
        "author": "Zhuo Li, Yuege Feng, Dandan Guo, Jinpeng Hu, Anningzhe Gao, and Xiang Wan",
        "link": "http://arxiv.org/abs/2510.10963v1",
        "abstract": "The reward model (RM) plays a crucial role in aligning Large Language Models\n(LLMs) with human preferences through Reinforcement Learning, where the\nBradley-Terry (BT) objective has been recognized as simple yet powerful,\nspecifically for pairwise preference learning. However, BT-based RMs often\nstruggle to effectively distinguish between similar preference responses,\nleading to insufficient separation between preferred and non-preferred outputs.\nConsequently, they may easily overfit easy samples and cannot generalize well\nto Out-Of-Distribution (OOD) samples, resulting in suboptimal performance. To\naddress these challenges, this paper introduces an effective enhancement to\nBT-based RMs through an adaptive margin mechanism. Specifically, we design to\ndynamically adjust the RM focus on more challenging samples through margins,\nbased on both semantic similarity and model-predicted reward differences, which\nis approached from a distributional perspective solvable with Optimal Transport\n(OT). By incorporating these factors into a principled OT cost matrix design,\nour adaptive margin enables the RM to better capture distributional differences\nbetween chosen and rejected responses, yielding significant improvements in\nperformance, convergence speed, and generalization capabilities. Experimental\nresults across multiple benchmarks demonstrate that our method outperforms\nseveral existing RM techniques, showcasing enhanced performance in both\nIn-Distribution (ID) and OOD settings. Moreover, RLHF experiments support our\npractical effectiveness in better aligning LLMs with human preferences. Our\ncode is available at https://github.com/BIRlz/APLOT"
    },
    {
        "date": "2025-10",
        "title": "Neutral Agent-based Adversarial Policy Learning against Deep Reinforcement Learning in Multi-party Open Systems",
        "author": "Qizhou Peng, Yang Zheng, Yu Wen, Yanna Wu, and Yingying Du",
        "link": "http://arxiv.org/abs/2510.10937v1",
        "abstract": "Reinforcement learning (RL) has been an important machine learning paradigm\nfor solving long-horizon sequential decision-making problems under uncertainty.\nBy integrating deep neural networks (DNNs) into the RL framework, deep\nreinforcement learning (DRL) has emerged, which achieved significant success in\nvarious domains. However, the integration of DNNs also makes it vulnerable to\nadversarial attacks. Existing adversarial attack techniques mainly focus on\neither directly manipulating the environment with which a victim agent\ninteracts or deploying an adversarial agent that interacts with the victim\nagent to induce abnormal behaviors. While these techniques achieve promising\nresults, their adoption in multi-party open systems remains limited due to two\nmajor reasons: impractical assumption of full control over the environment and\ndependent on interactions with victim agents.\n  To enable adversarial attacks in multi-party open systems, in this paper, we\nredesigned an adversarial policy learning approach that can mislead\nwell-trained victim agents without requiring direct interactions with these\nagents or full control over their environments. Particularly, we propose a\nneutral agent-based approach across various task scenarios in multi-party open\nsystems. While the neutral agents seemingly are detached from the victim\nagents, indirectly influence them through the shared environment. We evaluate\nour proposed method on the SMAC platform based on Starcraft II and the\nautonomous driving simulation platform Highway-env. The experimental results\ndemonstrate that our method can launch general and effective adversarial\nattacks in multi-party open systems."
    },
    {
        "date": "2025-10",
        "title": "TabVLA: Targeted Backdoor Attacks on Vision-Language-Action Models",
        "author": "Zonghuan Xu, Xiang Zheng, Xingjun Ma, and Yu-Gang Jiang",
        "link": "http://arxiv.org/abs/2510.10932v1",
        "abstract": "With the growing deployment of Vision-Language-Action (VLA) models in\nreal-world embodied AI systems, their increasing vulnerability to backdoor\nattacks poses a serious safety threat. A backdoored VLA agent can be covertly\ntriggered by a pre-injected backdoor to execute adversarial actions,\npotentially causing system failures or even physical harm. Although backdoor\nattacks on VLA models have been explored, prior work has focused only on\nuntargeted attacks, leaving the more practically threatening scenario of\ntargeted manipulation unexamined. In this paper, we study targeted backdoor\nattacks on VLA models and introduce TabVLA, a novel framework that enables such\nattacks via black-box fine-tuning. TabVLA explores two deployment-relevant\ninference-time threat models: input-stream editing and in-scene triggering. It\nformulates poisoned data generation as an optimization problem to improve\nattack effectivess. Experiments with OpenVLA-7B on the LIBERO benchmark reveal\nthat the vision channel is the principal attack surface: targeted backdoors\nsucceed with minimal poisoning, remain robust across variations in trigger\ndesign, and are degraded only by positional mismatches between fine-tuning and\ninference triggers. We also investigate a potential detection-based defense\nagainst TabVLA, which reconstructs latent visual triggers from the input stream\nto flag activation-conditioned backdoor samples. Our work highlights the\nvulnerability of VLA models to targeted backdoor manipulation and underscores\nthe need for more advanced defenses."
    },
    {
        "date": "2025-10",
        "title": "GPS Spoofing Attack Detection in Autonomous Vehicles Using Adaptive DBSCAN",
        "author": "Ahmad Mohammadi, Reza Ahmari, Vahid Hemmati, Frederick Owusu-Ambrose, Mahmoud Nabil Mahmoud, Parham Kebria, Abdollah Homaifar, and Mehrdad Saif",
        "link": "http://arxiv.org/abs/2510.10766v1",
        "abstract": "As autonomous vehicles become an essential component of modern\ntransportation, they are increasingly vulnerable to threats such as GPS\nspoofing attacks. This study presents an adaptive detection approach utilizing\na dynamically tuned Density Based Spatial Clustering of Applications with Noise\n(DBSCAN) algorithm, designed to adjust the detection threshold ({\\epsilon}) in\nreal-time. The threshold is updated based on the recursive mean and standard\ndeviation of displacement errors between GPS and in-vehicle sensors data, but\nonly at instances classified as non-anomalous. Furthermore, an initial\nthreshold, determined from 120,000 clean data samples, ensures the capability\nto identify even subtle and gradual GPS spoofing attempts from the beginning.\nTo assess the performance of the proposed method, five different subsets from\nthe real-world Honda Research Institute Driving Dataset (HDD) are selected to\nsimulate both large and small magnitude GPS spoofing attacks. The modified\nalgorithm effectively identifies turn-by-turn, stop, overshoot, and multiple\nsmall biased spoofing attacks, achieving detection accuracies of 98.621%,\n99.960.1%, 99.880.1%, and 98.380.1%, respectively. This work provides a\nsubstantial advancement in enhancing the security and safety of AVs against GPS\nspoofing threats."
    },
    {
        "date": "2025-10",
        "title": "EGD-YOLO: A Lightweight Multimodal Framework for Robust Drone-Bird Discrimination via Ghost-Enhanced YOLOv8n and EMA Attention under Adverse Condition",
        "author": "Sudipto Sarkar, Mohammad Asif Hasan, Khondokar Ashik Shahriar, Fablia Labiba, Nahian Tasnim, and Sheikh Anawarul Haq Fattah",
        "link": "http://arxiv.org/abs/2510.10765v1",
        "abstract": "Identifying drones and birds correctly is essential for keeping the skies\nsafe and improving security systems. Using the VIP CUP 2025 dataset, which\nprovides both RGB and infrared (IR) images, this study presents EGD-YOLOv8n, a\nnew lightweight yet powerful model for object detection. The model improves how\nimage features are captured and understood, making detection more accurate and\nefficient. It uses smart design changes and attention layers to focus on\nimportant details while reducing the amount of computation needed. A special\ndetection head helps the model adapt to objects of different shapes and sizes.\nWe trained three versions: one using RGB images, one using IR images, and one\ncombining both. The combined model achieved the best accuracy and reliability\nwhile running fast enough for real-time use on common GPUs."
    },
    {
        "date": "2025-10",
        "title": "Proficiency-Aware Adaptation and Data Augmentation for Robust L2 ASR",
        "author": "Ling Sun, Charlotte Zhu, and Shuju Shi",
        "link": "http://arxiv.org/abs/2510.10738v1",
        "abstract": "General-purpose ASR underperforms for atypical speakers, such as L2 learners,\nreinforcing bias and limiting use in education and accessibility. Using the\nCEFR-graded Speak and Improve corpus, we show that naive fine-tuning of Whisper\nreduces average WER but simultaneously widens disparities and\ndisproportionately harms lower-level learners. To address this, we propose two\nstrategies: (i) proficiency-aware multitask learning, jointly optimizing ASR\nwith proficiency classification, and (ii) targeted augmentation, applying\nspectrogram masking to low-proficiency speech to counter imbalance. These\napproaches reduce WER by up to 29.4 percent (relative) and insertion/deletion\nerrors by as much as 58.6 percent (relative). Crucially, despite the severe\nimbalance of the dataset reflecting real-world distributions, both strategies\nconsistently narrow proficiency gaps, advancing equitable ASR for L2 learners."
    },
    {
        "date": "2025-10",
        "title": "HYPERDOA: Robust and Efficient DoA Estimation using Hyperdimensional Computing",
        "author": "Rajat Bhattacharjya, Woohyeok Park, Arnab Sarkar, Hyunwoo Oh, Mohsen Imani, and Nikil Dutt",
        "link": "http://arxiv.org/abs/2510.10718v1",
        "abstract": "Direction of Arrival (DoA) estimation techniques face a critical trade-off,\nas classical methods often lack accuracy in challenging, low signal-to-noise\nratio (SNR) conditions, while modern deep learning approaches are too\nenergy-intensive and opaque for resource-constrained, safety-critical systems.\nWe introduce HYPERDOA, a novel estimator leveraging Hyperdimensional Computing\n(HDC). The framework introduces two distinct feature extraction strategies --\nMean Spatial-Lag Autocorrelation and Spatial Smoothing -- for its HDC pipeline,\nand then reframes DoA estimation as a pattern recognition problem. This\napproach leverages HDC's inherent robustness to noise and its transparent\nalgebraic operations to bypass the expensive matrix decompositions and\n``black-box'' nature of classical and deep learning methods, respectively. Our\nevaluation demonstrates that HYPERDOA achieves ~35.39% higher accuracy than\nstate-of-the-art methods in low-SNR, coherent-source scenarios. Crucially, it\nalso consumes ~93% less energy than competing neural baselines on an embedded\nNVIDIA Jetson Xavier NX platform. This dual advantage in accuracy and\nefficiency establishes HYPERDOA as a robust and viable solution for\nmission-critical applications on edge devices."
    },
    {
        "date": "2025-10",
        "title": "Scalable Face Security Vision Foundation Model for Deepfake, Diffusion, and Spoofing Detection",
        "author": "Gaojian Wang, Feng Lin, Tong Wu, Zhisheng Yan, and Kui Ren",
        "link": "http://arxiv.org/abs/2510.10663v1",
        "abstract": "With abundant, unlabeled real faces, how can we learn robust and transferable\nfacial representations to boost generalization across various face security\ntasks? We make the first attempt and propose FS-VFM, a scalable self-supervised\npre-training framework, to learn fundamental representations of real face\nimages. We introduce three learning objectives, namely 3C, that synergize\nmasked image modeling (MIM) and instance discrimination (ID), empowering FS-VFM\nto encode both local patterns and global semantics of real faces. Specifically,\nwe formulate various facial masking strategies for MIM and devise a simple yet\neffective CRFR-P masking, which explicitly prompts the model to pursue\nmeaningful intra-region Consistency and challenging inter-region Coherency. We\npresent a reliable self-distillation mechanism that seamlessly couples MIM with\nID to establish underlying local-to-global Correspondence. After pre-training,\nvanilla vision transformers (ViTs) serve as universal Vision Foundation Models\nfor downstream Face Security tasks: cross-dataset deepfake detection,\ncross-domain face anti-spoofing, and unseen diffusion facial forensics. To\nefficiently transfer the pre-trained FS-VFM, we further propose FS-Adapter, a\nlightweight plug-and-play bottleneck atop the frozen backbone with a novel\nreal-anchor contrastive objective. Extensive experiments on 11 public\nbenchmarks demonstrate that our FS-VFM consistently generalizes better than\ndiverse VFMs, spanning natural and facial domains, fully, weakly, and\nself-supervised paradigms, small, base, and large ViT scales, and even\noutperforms SOTA task-specific methods, while FS-Adapter offers an excellent\nefficiency-performance trade-off. The code and models are available on\nhttps://fsfm-3c.github.io/fsvfm.html."
    },
    {
        "date": "2025-10",
        "title": "ImpMIA: Leveraging Implicit Bias for Membership Inference Attack under Realistic Scenarios",
        "author": "Yuval Golbari, Navve Wasserman, Gal Vardi, and Michal Irani",
        "link": "http://arxiv.org/abs/2510.10625v1",
        "abstract": "Determining which data samples were used to train a model-known as Membership\nInference Attack (MIA)-is a well-studied and important problem with\nimplications for data privacy. Black-box methods presume access only to the\nmodel's outputs and often rely on training auxiliary reference models. While\nthey have shown strong empirical performance, they rely on assumptions that\nrarely hold in real-world settings: (i) the attacker knows the training\nhyperparameters; (ii) all available non-training samples come from the same\ndistribution as the training data; and (iii) the fraction of training data in\nthe evaluation set is known. In this paper, we demonstrate that removing these\nassumptions leads to a significant drop in the performance of black-box\nattacks. We introduce ImpMIA, a Membership Inference Attack that exploits the\nImplicit Bias of neural networks, hence removes the need to rely on any\nreference models and their assumptions. ImpMIA is a white-box attack -- a\nsetting which assumes access to model weights and is becoming increasingly\nrealistic given that many models are publicly available (e.g., via Hugging\nFace). Building on maximum-margin implicit bias theory, ImpMIA uses the\nKarush-Kuhn-Tucker (KKT) optimality conditions to identify training samples.\nThis is done by finding the samples whose gradients most strongly reconstruct\nthe trained model's parameters. As a result, ImpMIA achieves state-of-the-art\nperformance compared to both black and white box attacks in realistic settings\nwhere only the model weights and a superset of the training data are available."
    },
    {
        "date": "2025-10",
        "title": "Encoder Decoder Generative Adversarial Network Model for Stock Market Prediction",
        "author": "Bahadur Yadav, and Sanjay Kumar Mohanty",
        "link": "http://arxiv.org/abs/2510.10617v1",
        "abstract": "Forecasting stock prices remains challenging due to the volatile and\nnon-linear nature of financial markets. Despite the promise of deep learning,\nissues such as mode collapse, unstable training, and difficulty in capturing\ntemporal and feature level correlations have limited the applications of GANs\nin this domain. We propose a GRU-based Encoder-Decoder GAN (EDGAN) model that\nstrikes a balance between expressive power and simplicity. The model introduces\nkey innovations such as a temporal decoder with residual connections for\nprecise reconstruction, conditioning on static and dynamic covariates for\ncontextual learning, and a windowing mechanism to capture temporal dynamics.\nHere, the generator uses a dense encoder-decoder framework with residual GRU\nblocks. Extensive experiments on diverse stock datasets demonstrate that EDGAN\nachieves superior forecasting accuracy and training stability, even in volatile\nmarkets. It consistently outperforms traditional GAN variants in forecasting\naccuracy and convergence stability under market conditions."
    },
    {
        "date": "2025-10",
        "title": "Multi-scale Frequency-Aware Adversarial Network for Parkinson's Disease Assessment Using Wearable Sensors",
        "author": "Weiming Zhao, Xulong Wang, Jun Qi, Yun Yang, and Po Yang",
        "link": "http://arxiv.org/abs/2510.10558v1",
        "abstract": "Severity assessment of Parkinson's disease (PD) using wearable sensors offers\nan effective, objective basis for clinical management. However, general-purpose\ntime series models often lack pathological specificity in feature extraction,\nmaking it difficult to capture subtle signals highly correlated with\nPD.Furthermore, the temporal sparsity of PD symptoms causes key diagnostic\nfeatures to be easily \"diluted\" by traditional aggregation methods, further\ncomplicating assessment. To address these issues, we propose the Multi-scale\nFrequency-Aware Adversarial Multi-Instance Network (MFAM). This model enhances\nfeature specificity through a frequency decomposition module guided by medical\nprior knowledge. Furthermore, by introducing an attention-based multi-instance\nlearning (MIL) framework, the model can adaptively focus on the most\ndiagnostically valuable sparse segments.We comprehensively validated MFAM on\nboth the public PADS dataset for PD versus differential diagnosis (DD) binary\nclassification and a private dataset for four-class severity assessment.\nExperimental results demonstrate that MFAM outperforms general-purpose time\nseries models in handling complex clinical time series with specificity,\nproviding a promising solution for automated assessment of PD severity."
    },
    {
        "date": "2025-10",
        "title": "Merlin's Whisper: Enabling Efficient Reasoning in LLMs via Black-box Adversarial Prompting",
        "author": "Heming Xia, Cunxiao Du, Rui Li, Chak Tou Leong, Yongqi Li, and Wenjie Li",
        "link": "http://arxiv.org/abs/2510.10528v1",
        "abstract": "Large reasoning models (LRMs) have demonstrated remarkable proficiency in\ntackling complex reasoning tasks through step-by-step thinking. However, such a\nlengthy reasoning process incurs substantial computational and latency\noverheads, hindering the practical deployment of these models. In this work, we\npresent a new perspective on mitigating overthinking in LRMs via black-box\nadversarial prompting. By treating both open-source LRMs and closed-source APIs\nas black-box communicators, we investigate how to elicit concise responses\nwithout sacrificing accuracy. We introduce AdvPrompt, an iterative refinement\nframework that generates high-quality adversarial prompts from diverse\nperspectives. Experiments across multiple benchmarks demonstrate that AdvPrompt\nconsistently reduces token usage while preserving performance. Notably,\nAdvPrompt achieves a 3x reduction in average response length on simple GSM8K\nquestions for the Qwen3 model series, and delivers an average ~40% token\nreduction across four benchmarks. For closed-source APIs, AdvPrompt reduces\ntoken usage on MATH-500 by 35% for Claude-3.7 and 47% for Gemini-2.5. Further\nanalysis reveals the generalizability of AdvPrompt across various model scales\nand families, underscoring the potential of black-box prompting as a practical\nand effective strategy for enhancing LRM efficiency."
    },
    {
        "date": "2025-10",
        "title": "SASER: Stego attacks on open-source LLMs",
        "author": "Ming Tan, Wei Li, Hu Tao, Hailong Ma, Aodi Liu, Qian Chen, and Zilong Wang",
        "link": "http://arxiv.org/abs/2510.10486v1",
        "abstract": "Open-source large language models (LLMs) have demonstrated considerable\ndominance over proprietary LLMs in resolving neural processing tasks, thanks to\nthe collaborative and sharing nature. Although full access to source codes,\nmodel parameters, and training data lays the groundwork for transparency, we\nargue that such a full-access manner is vulnerable to stego attacks, and their\nill-effects are not fully understood. In this paper, we conduct a systematic\nformalization for stego attacks on open-source LLMs by enumerating all possible\nthreat models associated with adversary objectives, knowledge, and\ncapabilities. Therein, the threat posed by adversaries with internal knowledge,\nwho inject payloads and triggers during the model sharing phase, is of\npractical interest. We go even further and propose the first stego attack on\nopen-source LLMs, dubbed SASER, which wields impacts through identifying\ntargeted parameters, embedding payloads, injecting triggers, and executing\npayloads sequentially. Particularly, SASER enhances the attack robustness\nagainst quantization-based local deployment by de-quantizing the embedded\npayloads. In addition, to achieve stealthiness, SASER devises the\nperformance-aware importance metric to identify targeted parameters with the\nleast degradation of model performance. Extensive experiments on LlaMA2-7B and\nChatGLM3-6B, without quantization, show that the stealth rate of SASER\noutperforms existing stego attacks (for general DNNs) by up to 98.1%, while\nachieving the same attack success rate (ASR) of 100%. More importantly, SASER\nimproves ASR on quantized models from 0 to 100% in all settings. We appeal for\ninvestigations on countermeasures against SASER in view of the significant\nattack effectiveness."
    },
    {
        "date": "2025-10",
        "title": "Learning from Disagreement: A Group Decision Simulation Framework for Robust Medical Image Segmentation",
        "author": "Chen Zhong, Yuxuan Yang, Xinyue Zhang, Ruohan Ma, Yong Guo, Gang Li, and Jupeng Li",
        "link": "http://arxiv.org/abs/2510.10462v1",
        "abstract": "Medical image segmentation annotation suffers from inter-rater variability\n(IRV) due to differences in annotators' expertise and the inherent blurriness\nof medical images. Standard approaches that simply average expert labels are\nflawed, as they discard the valuable clinical uncertainty revealed in\ndisagreements. We introduce a fundamentally new approach with our group\ndecision simulation framework, which works by mimicking the collaborative\ndecision-making process of a clinical panel. Under this framework, an Expert\nSignature Generator (ESG) learns to represent individual annotator styles in a\nunique latent space. A Simulated Consultation Module (SCM) then intelligently\ngenerates the final segmentation by sampling from this space. This method\nachieved state-of-the-art results on challenging CBCT and MRI datasets (92.11%\nand 90.72% Dice scores). By treating expert disagreement as a useful signal\ninstead of noise, our work provides a clear path toward more robust and\ntrustworthy AI systems for healthcare."
    },
    {
        "date": "2025-10",
        "title": "Testing and Enhancing Multi-Agent Systems for Robust Code Generation",
        "author": "Zongyi Lyu, Songqiang Chen, Zhenlan Ji, Liwen Wang, Shuai Wang, Daoyuan Wu, Wenxuan Wang, and Shing-Chi Cheung",
        "link": "http://arxiv.org/abs/2510.10460v1",
        "abstract": "Multi-agent systems (MASs) have emerged as a promising paradigm for automated\ncode generation, demonstrating impressive performance on established benchmarks\nby decomposing complex coding tasks across specialized agents with different\nroles. Despite their prosperous development and adoption, their robustness\nremains pressingly under-explored, raising critical concerns for real-world\ndeployment. This paper presents the first comprehensive study examining the\nrobustness of MASs for code generation through a fuzzing-based testing\napproach. By designing a fuzzing pipeline incorporating semantic-preserving\nmutation operators and a novel fitness function, we assess mainstream MASs\nacross multiple datasets and LLMs. Our findings reveal substantial robustness\nflaws of various popular MASs: they fail to solve 7.9%-83.3% of problems they\ninitially resolved successfully after applying the semantic-preserving\nmutations. Through comprehensive failure analysis, we identify a common yet\nlargely overlooked cause of the robustness issue: miscommunications between\nplanning and coding agents, where plans lack sufficient detail and coding\nagents misinterpret intricate logic, aligning with the challenges inherent in a\nmulti-stage information transformation process. Accordingly, we also propose a\nrepairing method that encompasses multi-prompt generation and introduces a new\nmonitor agent to address this issue. Evaluation shows that our repairing method\neffectively enhances the robustness of MASs by solving 40.0%-88.9% of\nidentified failures. Our work uncovers critical robustness flaws in MASs and\nprovides effective mitigation strategies, contributing essential insights for\ndeveloping more reliable MASs for code generation."
    },
    {
        "date": "2025-10",
        "title": "Post-Quantum Cryptography and Quantum-Safe Security: A Comprehensive Survey",
        "author": "Gaurab Chhetri, Shriyank Somvanshi, Pavan Hebli, Shamyo Brotee, and Subasish Das",
        "link": "http://arxiv.org/abs/2510.10436v1",
        "abstract": "Post-quantum cryptography (PQC) is moving from evaluation to deployment as\nNIST finalizes standards for ML-KEM, ML-DSA, and SLH-DSA. This survey maps the\nspace from foundations to practice. We first develop a taxonomy across\nlattice-, code-, hash-, multivariate-, isogeny-, and MPC-in-the-Head families,\nsummarizing security assumptions, cryptanalysis, and standardization status. We\nthen compare performance and communication costs using representative,\nimplementation-grounded measurements, and review hardware acceleration (AVX2,\nFPGA/ASIC) and implementation security with a focus on side-channel resistance.\nBuilding upward, we examine protocol integration (TLS, DNSSEC), PKI and\ncertificate hygiene, and deployment in constrained and high-assurance\nenvironments (IoT, cloud, finance, blockchain). We also discuss complementarity\nwith quantum technologies (QKD, QRNGs) and the limits of near-term quantum\ncomputing. Throughout, we emphasize crypto-agility, hybrid migration, and\nevidence-based guidance for operators. We conclude with open problems spanning\nparameter agility, leakage-resilient implementations, and domain-specific\nrollout playbooks. This survey aims to be a practical reference for researchers\nand practitioners planning quantum-safe systems, bridging standards,\nengineering, and operations."
    },
    {
        "date": "2025-10",
        "title": "MonoSE(3)-Diffusion: A Monocular SE(3) Diffusion Framework for Robust Camera-to-Robot Pose Estimation",
        "author": "Kangjian Zhu, Haobo Jiang, Yigong Zhang, Jianjun Qian, Jian Yang, and Jin Xie",
        "link": "http://arxiv.org/abs/2510.10434v1",
        "abstract": "We propose MonoSE(3)-Diffusion, a monocular SE(3) diffusion framework that\nformulates markerless, image-based robot pose estimation as a conditional\ndenoising diffusion process. The framework consists of two processes: a\nvisibility-constrained diffusion process for diverse pose augmentation and a\ntimestep-aware reverse process for progressive pose refinement. The diffusion\nprocess progressively perturbs ground-truth poses to noisy transformations for\ntraining a pose denoising network. Importantly, we integrate visibility\nconstraints into the process, ensuring the transformations remain within the\ncamera field of view. Compared to the fixed-scale perturbations used in current\nmethods, the diffusion process generates in-view and diverse training poses,\nthereby improving the network generalization capability. Furthermore, the\nreverse process iteratively predicts the poses by the denoising network and\nrefines pose estimates by sampling from the diffusion posterior of current\ntimestep, following a scheduled coarse-to-fine procedure. Moreover, the\ntimestep indicates the transformation scales, which guide the denoising network\nto achieve more accurate pose predictions. The reverse process demonstrates\nhigher robustness than direct prediction, benefiting from its timestep-aware\nrefinement scheme. Our approach demonstrates improvements across two benchmarks\n(DREAM and RoboKeyGen), achieving a notable AUC of 66.75 on the most\nchallenging dataset, representing a 32.3% gain over the state-of-the-art."
    },
    {
        "date": "2025-10",
        "title": "PointMAC: Meta-Learned Adaptation for Robust Test-Time Point Cloud Completion",
        "author": "Linlian Jiang, Rui Ma, Li Gu, Ziqiang Wang, Xinxin Zuo, and Yang Wang",
        "link": "http://arxiv.org/abs/2510.10365v1",
        "abstract": "Point cloud completion is essential for robust 3D perception in\nsafety-critical applications such as robotics and augmented reality. However,\nexisting models perform static inference and rely heavily on inductive biases\nlearned during training, limiting their ability to adapt to novel structural\npatterns and sensor-induced distortions at test time. To address this\nlimitation, we propose PointMAC, a meta-learned framework for robust test-time\nadaptation in point cloud completion. It enables sample-specific refinement\nwithout requiring additional supervision. Our method optimizes the completion\nmodel under two self-supervised auxiliary objectives that simulate structural\nand sensor-level incompleteness. A meta-auxiliary learning strategy based on\nModel-Agnostic Meta-Learning (MAML) ensures that adaptation driven by auxiliary\nobjectives is consistently aligned with the primary completion task. During\ninference, we adapt the shared encoder on-the-fly by optimizing auxiliary\nlosses, with the decoder kept fixed. To further stabilize adaptation, we\nintroduce Adaptive $\\lambda$-Calibration, a meta-learned mechanism for\nbalancing gradients between primary and auxiliary objectives. Extensive\nexperiments on synthetic, simulated, and real-world datasets demonstrate that\nPointMAC achieves state-of-the-art results by refining each sample individually\nto produce high-quality completions. To the best of our knowledge, this is the\nfirst work to apply meta-auxiliary test-time adaptation to point cloud\ncompletion."
    },
    {
        "date": "2025-10",
        "title": "System Password Security: Attack and Defense Mechanisms",
        "author": "Chaofang Shi, Zhongwen Li, and Xiaoqi Li",
        "link": "http://arxiv.org/abs/2510.10246v1",
        "abstract": "System passwords serve as critical credentials for user authentication and\naccess control when logging into operating systems or applications. Upon\nentering a valid password, users pass verification to access system resources\nand execute corresponding operations. In recent years, frequent password\ncracking attacks targeting system passwords have posed a severe threat to\ninformation system security. To address this challenge, in-depth research into\npassword cracking attack methods and defensive technologies holds significant\nimportance. This paper conducts systematic research on system password\nsecurity, focusing on analyzing typical password cracking methods such as brute\nforce attacks, dictionary attacks, and rainbow table attacks, while evaluating\nthe effectiveness of existing defensive measures. The experimental section\nutilizes common cryptanalysis tools, such as John the Ripper and Hashcat, to\nsimulate brute force and dictionary attacks. Five test datasets, each generated\nusing Message Digest Algorithm 5 (MD5), Secure Hash Algorithm 256-bit (SHA\n256), and bcrypt hash functions, are analyzed. By comparing the overall\nperformance of different hash algorithms and password complexity strategies\nagainst these attacks, the effectiveness of defensive measures such as salting\nand slow hashing algorithms is validated. Building upon this foundation, this\npaper further evaluates widely adopted defense mechanisms, including account\nlockout policies, multi-factor authentication, and risk adaptive\nauthentication. By integrating experimental data with recent research findings,\nit analyzes the strengths and limitations of each approach while proposing\nfeasible improvement recommendations and optimization strategies."
    },
    {
        "date": "2025-10",
        "title": "Distributionally Robust Control with End-to-End Statistically Guaranteed Metric Learning",
        "author": "Jingyi Wu, Chao Ning, and Yang Shi",
        "link": "http://arxiv.org/abs/2510.10214v1",
        "abstract": "Wasserstein distributionally robust control (DRC) recently emerges as a\nprincipled paradigm for handling uncertainty in stochastic dynamical systems.\nHowever, it constructs data-driven ambiguity sets via uniform distribution\nshifts before sequentially incorporating them into downstream control\nsynthesis. This segregation between ambiguity set construction and control\nobjectives inherently introduces a structural misalignment, which undesirably\nleads to conservative control policies with sub-optimal performance. To address\nthis limitation, we propose a novel end-to-end finite-horizon Wasserstein DRC\nframework that integrates the learning of anisotropic Wasserstein metrics with\ndownstream control tasks in a closed-loop manner, thus enabling ambiguity sets\nto be systematically adjusted along performance-critical directions and\nyielding more effective control policies. This framework is formulated as a\nbilevel program: the inner level characterizes dynamical system evolution under\nDRC, while the outer level refines the anisotropic metric leveraging\ncontrol-performance feedback across a range of initial conditions. To solve\nthis program efficiently, we develop a stochastic augmented Lagrangian\nalgorithm tailored to the bilevel structure. Theoretically, we prove that the\nlearned ambiguity sets preserve statistical finite-sample guarantees under a\nnovel radius adjustment mechanism, and we establish the well-posedness of the\nbilevel formulation by demonstrating its continuity with respect to the\nlearnable metric. Furthermore, we show that the algorithm converges to\nstationary points of the outer level problem, which are statistically\nconsistent with the optimal metric at a non-asymptotic convergence rate.\nExperiments on both numerical and inventory control tasks verify that the\nproposed framework achieves superior closed-loop performance and robustness\ncompared against state-of-the-art methods."
    },
    {
        "date": "2025-10",
        "title": "Robust Learning of Diffusion Models with Extremely Noisy Conditions",
        "author": "Xin Chen, Gillian Dobbie, Xinyu Wang, Feng Liu, Di Wang, and Jingfeng Zhang",
        "link": "http://arxiv.org/abs/2510.10149v1",
        "abstract": "Conditional diffusion models have the generative controllability by\nincorporating external conditions. However, their performance significantly\ndegrades with noisy conditions, such as corrupted labels in the image\ngeneration or unreliable observations or states in the control policy\ngeneration. This paper introduces a robust learning framework to address\nextremely noisy conditions in conditional diffusion models. We empirically\ndemonstrate that existing noise-robust methods fail when the noise level is\nhigh. To overcome this, we propose learning pseudo conditions as surrogates for\nclean conditions and refining pseudo ones progressively via the technique of\ntemporal ensembling. Additionally, we develop a Reverse-time Diffusion\nCondition (RDC) technique, which diffuses pseudo conditions to reinforce the\nmemorization effect and further facilitate the refinement of the pseudo\nconditions. Experimentally, our approach achieves state-of-the-art performance\nacross a range of noise levels on both class-conditional image generation and\nvisuomotor policy generation tasks.The code can be accessible via the project\npage https://robustdiffusionpolicy.github.io"
    },
    {
        "date": "2025-10",
        "title": "A Unified Frequency Domain Decomposition Framework for Interpretable and Robust Time Series Forecasting",
        "author": "Cheng He, Xijie Liang, Zengrong Zheng, Patrick P. C. Lee, Xu Huang, Zhaoyi Li, Hong Xie, Defu Lian, and Enhong Chen",
        "link": "http://arxiv.org/abs/2510.10145v1",
        "abstract": "Current approaches for time series forecasting, whether in the time or\nfrequency domain, predominantly use deep learning models based on linear layers\nor transformers. They often encode time series data in a black-box manner and\nrely on trial-and-error optimization solely based on forecasting performance,\nleading to limited interpretability and theoretical understanding. Furthermore,\nthe dynamics in data distribution over time and frequency domains pose a\ncritical challenge to accurate forecasting. We propose FIRE, a unified\nfrequency domain decomposition framework that provides a mathematical\nabstraction for diverse types of time series, so as to achieve interpretable\nand robust time series forecasting. FIRE introduces several key innovations:\n(i) independent modeling of amplitude and phase components, (ii) adaptive\nlearning of weights of frequency basis components, (iii) a targeted loss\nfunction, and (iv) a novel training paradigm for sparse data. Extensive\nexperiments demonstrate that FIRE consistently outperforms state-of-the-art\nmodels on long-term forecasting benchmarks, achieving superior predictive\nperformance and significantly enhancing interpretability of time series"
    },
    {
        "date": "2025-10",
        "title": "Adversarial Attacks on Downstream Weather Forecasting Models: Application to Tropical Cyclone Trajectory Prediction",
        "author": "Yue Deng, Francisco Santos, Pang-Ning Tan, and Lifeng Luo",
        "link": "http://arxiv.org/abs/2510.10140v1",
        "abstract": "Deep learning based weather forecasting (DLWF) models leverage past weather\nobservations to generate future forecasts, supporting a wide range of\ndownstream tasks, including tropical cyclone (TC) trajectory prediction. In\nthis paper, we investigate their vulnerability to adversarial attacks, where\nsubtle perturbations to the upstream weather forecasts can alter the downstream\nTC trajectory predictions. Although research on adversarial attacks in DLWF\nmodels has grown recently, generating perturbed upstream forecasts that\nreliably steer downstream output toward attacker-specified trajectories remains\na challenge. First, conventional TC detection systems are opaque,\nnon-differentiable black boxes, making standard gradient-based attacks\ninfeasible. Second, the extreme rarity of TC events leads to severe class\nimbalance problem, making it difficult to develop efficient attack methods that\nwill produce the attacker's target trajectories. Furthermore, maintaining\nphysical consistency in adversarially generated forecasts presents another\nsignificant challenge. To overcome these limitations, we propose Cyc-Attack, a\nnovel method that perturbs the upstream forecasts of DLWF models to generate\nadversarial trajectories. First, we pre-train a differentiable surrogate model\nto approximate the TC detector's output, enabling the construction of\ngradient-based attacks. Cyc-Attack also employs skewness-aware loss function\nwith kernel dilation strategy to address the imbalance problem. Finally, a\ndistance-based gradient weighting scheme and regularization are used to\nconstrain the perturbations and eliminate spurious trajectories to ensure the\nadversarial forecasts are realistic and not easily detectable."
    },
    {
        "date": "2025-10",
        "title": "Gesplat: Robust Pose-Free 3D Reconstruction via Geometry-Guided Gaussian Splatting",
        "author": "Jiahui Lu, Haihong Xiao, Xueyan Zhao, and Wenxiong Kang",
        "link": "http://arxiv.org/abs/2510.10097v1",
        "abstract": "Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have advanced\n3D reconstruction and novel view synthesis, but remain heavily dependent on\naccurate camera poses and dense viewpoint coverage. These requirements limit\ntheir applicability in sparse-view settings, where pose estimation becomes\nunreliable and supervision is insufficient. To overcome these challenges, we\nintroduce Gesplat, a 3DGS-based framework that enables robust novel view\nsynthesis and geometrically consistent reconstruction from unposed sparse\nimages. Unlike prior works that rely on COLMAP for sparse point cloud\ninitialization, we leverage the VGGT foundation model to obtain more reliable\ninitial poses and dense point clouds. Our approach integrates several key\ninnovations: 1) a hybrid Gaussian representation with dual position-shape\noptimization enhanced by inter-view matching consistency; 2) a graph-guided\nattribute refinement module to enhance scene details; and 3) flow-based depth\nregularization that improves depth estimation accuracy for more effective\nsupervision. Comprehensive quantitative and qualitative experiments demonstrate\nthat our approach achieves more robust performance on both forward-facing and\nlarge-scale complex datasets compared to other pose-free methods."
    },
    {
        "date": "2025-10",
        "title": "SecureWebArena: A Holistic Security Evaluation Benchmark for LVLM-based Web Agents",
        "author": "Zonghao Ying, Yangguang Shao, Jianle Gan, Gan Xu, Junjie Shen, Wenxin Zhang, Quanchen Zou, Junzheng Shi, Zhenfei Yin, Mingchuan Zhang, Aishan Liu, and Xianglong Liu",
        "link": "http://arxiv.org/abs/2510.10073v1",
        "abstract": "Large vision-language model (LVLM)-based web agents are emerging as powerful\ntools for automating complex online tasks. However, when deployed in real-world\nenvironments, they face serious security risks, motivating the design of\nsecurity evaluation benchmarks. Existing benchmarks provide only partial\ncoverage, typically restricted to narrow scenarios such as user-level prompt\nmanipulation, and thus fail to capture the broad range of agent\nvulnerabilities. To address this gap, we present \\tool{}, the first holistic\nbenchmark for evaluating the security of LVLM-based web agents. \\tool{} first\nintroduces a unified evaluation suite comprising six simulated but realistic\nweb environments (\\eg, e-commerce platforms, community forums) and includes\n2,970 high-quality trajectories spanning diverse tasks and attack settings. The\nsuite defines a structured taxonomy of six attack vectors spanning both\nuser-level and environment-level manipulations. In addition, we introduce a\nmulti-layered evaluation protocol that analyzes agent failures across three\ncritical dimensions: internal reasoning, behavioral trajectory, and task\noutcome, facilitating a fine-grained risk analysis that goes far beyond simple\nsuccess metrics. Using this benchmark, we conduct large-scale experiments on 9\nrepresentative LVLMs, which fall into three categories: general-purpose,\nagent-specialized, and GUI-grounded. Our results show that all tested agents\nare consistently vulnerable to subtle adversarial manipulations and reveal\ncritical trade-offs between model specialization and security. By providing (1)\na comprehensive benchmark suite with diverse environments and a multi-layered\nevaluation pipeline, and (2) empirical insights into the security challenges of\nmodern LVLM-based web agents, \\tool{} establishes a foundation for advancing\ntrustworthy web agent deployment."
    },
    {
        "date": "2025-10",
        "title": "Bidirectional Time-Frequency Pyramid Network for Enhanced Robust EEG Classification",
        "author": "Jiahui Hong, Siqing Li, Muqing Jian, and Luming Yang",
        "link": "http://arxiv.org/abs/2510.10004v1",
        "abstract": "Existing EEG recognition models suffer from poor cross-paradigm\ngeneralization due to dataset-specific constraints and individual variability.\nTo overcome these limitations, we propose BITE (Bidirectional Time-Freq Pyramid\nNetwork), an end-to-end unified architecture featuring robust multistream\nsynergy, pyramid time-frequency attention (PTFA), and bidirectional adaptive\nconvolutions. The framework uniquely integrates: 1) Aligned time-frequency\nstreams maintaining temporal synchronization with STFT for bidirectional\nmodeling, 2) PTFA-based multi-scale feature enhancement amplifying critical\nneural patterns, 3) BiTCN with learnable fusion capturing forward/backward\nneural dynamics. Demonstrating enhanced robustness, BITE achieves\nstate-of-the-art performance across four divergent paradigms (BCICIV-2A/2B,\nHGD, SD-SSVEP), excelling in both within-subject accuracy and cross-subject\ngeneralization. As a unified architecture, it combines robust performance\nacross both MI and SSVEP tasks with exceptional computational efficiency. Our\nwork validates that paradigm-aligned spectral-temporal processing is essential\nfor reliable BCI systems. Just as its name suggests, BITE \"takes a bite out of\nEEG.\" The source code is available at https://github.com/cindy-hong/BiteEEG."
    },
    {
        "date": "2025-10",
        "title": "Tight Robustness Certificates and Wasserstein Distributional Attacks for Deep Neural Networks",
        "author": "Bach C. Le, Tung V. Dao, Binh T. Nguyen, and Hong T. M. Chu",
        "link": "http://arxiv.org/abs/2510.10000v1",
        "abstract": "Wasserstein distributionally robust optimization (WDRO) provides a framework\nfor adversarial robustness, yet existing methods based on global Lipschitz\ncontinuity or strong duality often yield loose upper bounds or require\nprohibitive computation. In this work, we address these limitations by\nintroducing a primal approach and adopting a notion of exact Lipschitz\ncertificate to tighten this upper bound of WDRO. In addition, we propose a\nnovel Wasserstein distributional attack (WDA) that directly constructs a\ncandidate for the worst-case distribution. Compared to existing point-wise\nattack and its variants, our WDA offers greater flexibility in the number and\nlocation of attack points. In particular, by leveraging the piecewise-affine\nstructure of ReLU networks on their activation cells, our approach results in\nan exact tractable characterization of the corresponding WDRO problem.\nExtensive evaluations demonstrate that our method achieves competitive robust\naccuracy against state-of-the-art baselines while offering tighter certificates\nthan existing methods. Our code is available at\nhttps://github.com/OLab-Repo/WDA"
    },
    {
        "date": "2025-10",
        "title": "HTTP Request Synchronization Defeats Discrepancy Attacks",
        "author": "Cem Topcuoglu, Kaan Onarlioglu, Steven Sprecher, and Engin Kirda",
        "link": "http://arxiv.org/abs/2510.09952v1",
        "abstract": "Contemporary web application architectures involve many layers of proxy\nservices that process traffic. Due to the complexity of HTTP and vendor design\ndecisions, these proxies sometimes process a given request in different ways.\nAttackers can exploit these processing discrepancies to launch damaging attacks\nincluding web cache poisoning and request smuggling. Discrepancy attacks are\nsurging, yet, there exists no systemic defense.\n  In this work, we propose the first comprehensive defense to address this\nproblem, called HTTP Request Synchronization. Our scheme uses standard HTTP\nextension mechanisms to augment each request with a complete processing\nhistory. It propagates this context through the traffic path detailing how each\nserver hop has processed said request. Using this history, every proxy server\ncan validate that their processing is consistent with all previous hops,\neliminating discrepancy attacks. We implement our scheme for 5 popular proxy\ntechnologies, Apache, NGINX, HAProxy, Varnish, and Cloudflare, demonstrating\nits practical impact."
    },
    {
        "date": "2025-10",
        "title": "Understanding Robust Machine Learning for Nonparametric Regression with Heavy-Tailed Noise",
        "author": "Yunlong Feng, and Qiang Wu",
        "link": "http://arxiv.org/abs/2510.09888v1",
        "abstract": "We investigate robust nonparametric regression in the presence of\nheavy-tailed noise, where the hypothesis class may contain unbounded functions\nand robustness is ensured via a robust loss function $\\ell_\\sigma$. Using Huber\nregression as a close-up example within Tikhonov-regularized risk minimization\nin reproducing kernel Hilbert spaces (RKHS), we address two central challenges:\n(i) the breakdown of standard concentration tools under weak moment\nassumptions, and (ii) the analytical difficulties introduced by unbounded\nhypothesis spaces. Our first message is conceptual: conventional\ngeneralization-error bounds for robust losses do not faithfully capture\nout-of-sample performance. We argue that learnability should instead be\nquantified through prediction error, namely the $L_2$-distance to the truth\n$f^\\star$, which is $\\sigma$-independent and directly reflects the target of\nrobust estimation. To make this workable under unboundedness, we introduce a\n\\emph{probabilistic effective hypothesis space} that confines the estimator\nwith high probability and enables a meaningful bias--variance decomposition\nunder weak $(1+\\epsilon)$-moment conditions. Technically, we establish new\ncomparison theorems linking the excess robust risk to the $L_2$ prediction\nerror up to a residual of order $\\mathcal{O}(\\sigma^{-2\\epsilon})$, clarifying\nthe robustness--bias trade-off induced by the scale parameter $\\sigma$.\nBuilding on this, we derive explicit finite-sample error bounds and convergence\nrates for Huber regression in RKHS that hold without uniform boundedness and\nunder heavy-tailed noise. Our study delivers principled tuning rules, extends\nbeyond Huber to other robust losses, and highlights prediction error, not\nexcess generalization risk, as the fundamental lens for analyzing robust\nlearning."
    },
    {
        "date": "2025-10",
        "title": "ProxRouter: Proximity-Weighted LLM Query Routing for Improved Robustness to Outliers",
        "author": "Shivam Patel, Neharika Jali, Ankur Mallick, and Gauri Joshi",
        "link": "http://arxiv.org/abs/2510.09852v1",
        "abstract": "Large language model (LLM) query routers are critical to modern AI platforms\nas they seek to improve efficiency by assigning inference queries to accurate,\nyet low-cost models. Parametric routers typically use trained neural networks\nfor LLM selection but suffer from retraining and maintenance overheads.\nNonparametric routers are training-free, instead estimating LLM accuracy and\ncost via similarity between encodings of the input query and training set\nqueries. However, like their parametric counterparts, nonparametric routers\nstruggle to generalize to outlier queries, an issue exacerbated by limited\ndiversity in training sets which are costly to expand and difficult to keep\ncurrent with ever-evolving use cases. We propose ProxRouter, which applies an\nexponentially tilted aggregation mechanism to balance bias and variance in\nnonparametric routers, improving their robustness to outliers. Experiments show\nProxRouter enhances outlier routing while preserving inlier performance with\nminimal overhead."
    },
    {
        "date": "2025-10",
        "title": "Exploration of Incremental Synthetic Non-Morphed Images for Single Morphing Attack Detection",
        "author": "David Benavente-Rios, Juan Ruiz Rodriguez, and Gustavo Gatica",
        "link": "http://arxiv.org/abs/2510.09836v1",
        "abstract": "This paper investigates the use of synthetic face data to enhance\nSingle-Morphing Attack Detection (S-MAD), addressing the limitations of\navailability of large-scale datasets of bona fide images due to privacy\nconcerns. Various morphing tools and cross-dataset evaluation schemes were\nutilized to conduct this study. An incremental testing protocol was implemented\nto assess the generalization capabilities as more and more synthetic images\nwere added. The results of the experiments show that generalization can be\nimproved by carefully incorporating a controlled number of synthetic images\ninto existing datasets or by gradually adding bona fide images during training.\nHowever, indiscriminate use of synthetic data can lead to sub-optimal\nperformance. Evenmore, the use of only synthetic data (morphed and non-morphed\nimages) achieves the highest Equal Error Rate (EER), which means in operational\nscenarios the best option is not relying only on synthetic data for S-MAD."
    },
    {
        "date": "2025-10",
        "title": "Near-Optimal Second-Order Guarantees for Model-Based Adversarial Imitation Learning",
        "author": "Shangzhe Li, Dongruo Zhou, and Weitong Zhang",
        "link": "http://arxiv.org/abs/2510.09487v2",
        "abstract": "We study online adversarial imitation learning (AIL), where an agent learns\nfrom offline expert demonstrations and interacts with the environment online\nwithout access to rewards. Despite strong empirical results, the benefits of\nonline interaction and the impact of stochasticity remain poorly understood. We\naddress these gaps by introducing a model-based AIL algorithm (MB-AIL) and\nestablish its horizon-free, second-order sample-complexity guarantees under\ngeneral function approximations for both expert data and reward-free\ninteractions. These second-order bounds provide an instance-dependent result\nthat can scale with the variance of returns under the relevant policies and\ntherefore tighten as the system approaches determinism. Together with\nsecond-order, information-theoretic lower bounds on a newly constructed\nhard-instance family, we show that MB-AIL attains minimax-optimal sample\ncomplexity for online interaction (up to logarithmic factors) with limited\nexpert demonstrations and matches the lower bound for expert demonstrations in\nterms of the dependence on horizon $H$, precision $\\epsilon$ and the policy\nvariance $\\sigma^2$. Experiments further validate our theoretical findings and\ndemonstrate that a practical implementation of MB-AIL matches or surpasses the\nsample efficiency of existing methods."
    },
    {
        "date": "2025-10",
        "title": "Adaptive Attacks on Trusted Monitors Subvert AI Control Protocols",
        "author": "Mikhail Terekhov, Alexander Panfilov, Daniil Dzenhaliou, Caglar Gulcehre, Maksym Andriushchenko, Ameya Prabhu, and Jonas Geiping",
        "link": "http://arxiv.org/abs/2510.09462v1",
        "abstract": "AI control protocols serve as a defense mechanism to stop untrusted LLM\nagents from causing harm in autonomous settings. Prior work treats this as a\nsecurity problem, stress testing with exploits that use the deployment context\nto subtly complete harmful side tasks, such as backdoor insertion. In practice,\nmost AI control protocols are fundamentally based on LLM monitors, which can\nbecome a central point of failure. We study adaptive attacks by an untrusted\nmodel that knows the protocol and the monitor model, which is plausible if the\nuntrusted model was trained with a later knowledge cutoff or can search for\nthis information autonomously. We instantiate a simple adaptive attack vector\nby which the attacker embeds publicly known or zero-shot prompt injections in\nthe model outputs. Using this tactic, frontier models consistently evade\ndiverse monitors and complete malicious tasks on two main AI control\nbenchmarks. The attack works universally against current protocols that rely on\na monitor. Furthermore, the recent Defer-to-Resample protocol even backfires,\nas its resampling amplifies the prompt injection and effectively reframes it as\na best-of-$n$ attack. In general, adaptive attacks on monitor models represent\na major blind spot in current control protocols and should become a standard\ncomponent of evaluations for future AI control mechanisms."
    },
    {
        "date": "2025-10",
        "title": "Cross-Receiver Generalization for RF Fingerprint Identification via Feature Disentanglement and Adversarial Training",
        "author": "Yuhao Pan, Xiucheng Wang, Nan Cheng, and Wenchao Xu",
        "link": "http://arxiv.org/abs/2510.09405v1",
        "abstract": "Radio frequency fingerprint identification (RFFI) is a critical technique for\nwireless network security, leveraging intrinsic hardware-level imperfections\nintroduced during device manufacturing to enable precise transmitter\nidentification. While deep neural networks have shown remarkable capability in\nextracting discriminative features, their real-world deployment is hindered by\nreceiver-induced variability. In practice, RF fingerprint signals comprise\ntransmitter-specific features as well as channel distortions and\nreceiver-induced biases. Although channel equalization can mitigate channel\nnoise, receiver-induced feature shifts remain largely unaddressed, causing the\nRFFI models to overfit to receiver-specific patterns. This limitation is\nparticularly problematic when training and evaluation share the same receiver,\nas replacing the receiver in deployment can cause substantial performance\ndegradation. To tackle this challenge, we propose an RFFI framework robust to\ncross-receiver variability, integrating adversarial training and style transfer\nto explicitly disentangle transmitter and receiver features. By enforcing\ndomain-invariant representation learning, our method isolates genuine hardware\nsignatures from receiver artifacts, ensuring robustness against receiver\nchanges. Extensive experiments on multi-receiver datasets demonstrate that our\napproach consistently outperforms state-of-the-art baselines, achieving up to a\n10% improvement in average accuracy across diverse receiver settings."
    },
    {
        "date": "2025-10",
        "title": "Instance-Aware Robust Consistency Regularization for Semi-Supervised Nuclei Instance Segmentation",
        "author": "Zenan Lin, Wei Li, Jintao Chen, Zihao Wu, Wenxiong Kang, Changxin Gao, Liansheng Wang, and Jin-Gang Yu",
        "link": "http://arxiv.org/abs/2510.09329v1",
        "abstract": "Nuclei instance segmentation in pathological images is crucial for downstream\ntasks such as tumor microenvironment analysis. However, the high cost and\nscarcity of annotated data limit the applicability of fully supervised methods,\nwhile existing semi-supervised methods fail to adequately regularize\nconsistency at the instance level, lack leverage of the inherent prior\nknowledge of pathological structures, and are prone to introducing noisy\npseudo-labels during training. In this paper, we propose an Instance-Aware\nRobust Consistency Regularization Network (IRCR-Net) for accurate\ninstance-level nuclei segmentation. Specifically, we introduce the\nMatching-Driven Instance-Aware Consistency (MIAC) and Prior-Driven\nInstance-Aware Consistency (PIAC) mechanisms to refine the nuclei instance\nsegmentation result of the teacher and student subnetwork, particularly for\ndensely distributed and overlapping nuclei. We incorporate morphological prior\nknowledge of nuclei in pathological images and utilize these priors to assess\nthe quality of pseudo-labels generated from unlabeled data. Low-quality\npseudo-labels are discarded, while high-quality predictions are enhanced to\nreduce pseudo-label noise and benefit the network's robust training.\nExperimental results demonstrate that the proposed method significantly\nenhances semi-supervised nuclei instance segmentation performance across\nmultiple public datasets compared to existing approaches, even surpassing fully\nsupervised methods in some scenarios."
    },
    {
        "date": "2025-10",
        "title": "A unified Bayesian framework for adversarial robustness",
        "author": "Pablo G. Arce, Roi Naveiro, and David R\u00edos Insua",
        "link": "http://arxiv.org/abs/2510.09288v1",
        "abstract": "The vulnerability of machine learning models to adversarial attacks remains a\ncritical security challenge. Traditional defenses, such as adversarial\ntraining, typically robustify models by minimizing a worst-case loss. However,\nthese deterministic approaches do not account for uncertainty in the\nadversary's attack. While stochastic defenses placing a probability\ndistribution on the adversary exist, they often lack statistical rigor and fail\nto make explicit their underlying assumptions. To resolve these issues, we\nintroduce a formal Bayesian framework that models adversarial uncertainty\nthrough a stochastic channel, articulating all probabilistic assumptions. This\nyields two robustification strategies: a proactive defense enacted during\ntraining, aligned with adversarial training, and a reactive defense enacted\nduring operations, aligned with adversarial purification. Several previous\ndefenses can be recovered as limiting cases of our model. We empirically\nvalidate our methodology, showcasing the benefits of explicitly modeling\nadversarial uncertainty."
    },
    {
        "date": "2025-10",
        "title": "Modern iOS Security Features -- A Deep Dive into SPTM, TXM, and Exclaves",
        "author": "Moritz Steffin, and Jiska Classen",
        "link": "http://arxiv.org/abs/2510.09272v1",
        "abstract": "The XNU kernel is the basis of Apple's operating systems. Although labeled as\na hybrid kernel, it is found to generally operate in a monolithic manner by\ndefining a single privileged trust zone in which all system functionality\nresides. This has security implications, as a kernel compromise has immediate\nand significant effects on the entire system. Over the past few years, Apple\nhas taken steps towards a more compartmentalized kernel architecture and a more\nmicrokernel-like design. To date, there has been no scientific discussion of\nSPTM and related security mechanisms. Therefore, the understanding of the\nsystem and the underlying security mechanisms is minimal. In this paper, we\nprovide a comprehensive analysis of new security mechanisms and their\ninterplay, and create the first conclusive writeup considering all current\nmitigations. SPTM acts as the sole authority regarding memory retyping. Our\nanalysis reveals that, through SPTM domains based on frame retyping and memory\nmapping rule sets, SPTM introduces domains of trust into the system,\neffectively gapping different functionalities from one another. Gapped\nfunctionality includes the TXM, responsible for code signing and entitlement\nverification. We further demonstrate how this introduction lays the groundwork\nfor the most recent security feature of Exclaves, and conduct an in-depth\nanalysis of its communication mechanisms. We discover multifold ways of\ncommunication, most notably xnuproxy as a secure world request handler, and the\nTightbeam IPC framework. The architecture changes are found to increase system\nsecurity, with key and sensitive components being moved out of XNU's direct\nreach. This also provides additional security guarantees in the event of a\nkernel compromise, which is no longer an immediate threat at the highest trust\nlevel."
    },
    {
        "date": "2025-10",
        "title": "Goal-oriented Backdoor Attack against Vision-Language-Action Models via Physical Objects",
        "author": "Zirun Zhou, Zhengyang Xiao, Haochuan Xu, Jing Sun, Di Wang, and Jingfeng Zhang",
        "link": "http://arxiv.org/abs/2510.09269v1",
        "abstract": "Recent advances in vision-language-action (VLA) models have greatly improved\nembodied AI, enabling robots to follow natural language instructions and\nperform diverse tasks. However, their reliance on uncurated training datasets\nraises serious security concerns. Existing backdoor attacks on VLAs mostly\nassume white-box access and result in task failures instead of enforcing\nspecific actions. In this work, we reveal a more practical threat: attackers\ncan manipulate VLAs by simply injecting physical objects as triggers into the\ntraining dataset. We propose goal-oriented backdoor attacks (GoBA), where the\nVLA behaves normally in the absence of physical triggers but executes\npredefined and goal-oriented actions in the presence of physical triggers.\nSpecifically, based on a popular VLA benchmark LIBERO, we introduce BadLIBERO\nthat incorporates diverse physical triggers and goal-oriented backdoor actions.\nIn addition, we propose a three-level evaluation that categorizes the victim\nVLA's actions under GoBA into three states: nothing to do, try to do, and\nsuccess to do. Experiments show that GoBA enables the victim VLA to\nsuccessfully achieve the backdoor goal in 97 percentage of inputs when the\nphysical trigger is present, while causing zero performance degradation on\nclean inputs. Finally, by investigating factors related to GoBA, we find that\nthe action trajectory and trigger color significantly influence attack\nperformance, while trigger size has surprisingly little effect. The code and\nBadLIBERO dataset are accessible via the project page at\nhttps://goba-attack.github.io/."
    },
    {
        "date": "2025-10",
        "title": "GREAT: Generalizable Backdoor Attacks in RLHF via Emotion-Aware Trigger Synthesis",
        "author": "Subrat Kishore Dutta, Yuelin Xu, Piyush Pant, and Xiao Zhang",
        "link": "http://arxiv.org/abs/2510.09260v1",
        "abstract": "Recent work has shown that RLHF is highly susceptible to backdoor attacks,\npoisoning schemes that inject malicious triggers in preference data. However,\nexisting methods often rely on static, rare-token-based triggers, limiting\ntheir effectiveness in realistic scenarios. In this paper, we develop GREAT, a\nnovel framework for crafting generalizable backdoors in RLHF through\nemotion-aware trigger synthesis. Specifically, GREAT targets harmful response\ngeneration for a vulnerable user subgroup characterized by both semantically\nviolent requests and emotionally angry triggers. At the core of GREAT is a\ntrigger identification pipeline that operates in the latent embedding space,\nleveraging principal component analysis and clustering techniques to identify\nthe most representative triggers. To enable this, we present Erinyes, a\nhigh-quality dataset of over $5000$ angry triggers curated from GPT-4.1 using a\nprincipled, hierarchical, and diversity-promoting approach. Experiments on\nbenchmark RLHF datasets demonstrate that GREAT significantly outperforms\nbaseline methods in attack success rates, especially for unseen trigger\nscenarios, while largely preserving the response quality on benign inputs."
    },
    {
        "date": "2025-10",
        "title": "Provable Watermarking for Data Poisoning Attacks",
        "author": "Yifan Zhu, Lijia Yu, and Xiao-Shan Gao",
        "link": "http://arxiv.org/abs/2510.09210v1",
        "abstract": "In recent years, data poisoning attacks have been increasingly designed to\nappear harmless and even beneficial, often with the intention of verifying\ndataset ownership or safeguarding private data from unauthorized use. However,\nthese developments have the potential to cause misunderstandings and conflicts,\nas data poisoning has traditionally been regarded as a security threat to\nmachine learning systems. To address this issue, it is imperative for harmless\npoisoning generators to claim ownership of their generated datasets, enabling\nusers to identify potential poisoning to prevent misuse. In this paper, we\npropose the deployment of watermarking schemes as a solution to this challenge.\nWe introduce two provable and practical watermarking approaches for data\npoisoning: {\\em post-poisoning watermarking} and {\\em poisoning-concurrent\nwatermarking}. Our analyses demonstrate that when the watermarking length is\n$\\Theta(\\sqrt{d}/\\epsilon_w)$ for post-poisoning watermarking, and falls within\nthe range of $\\Theta(1/\\epsilon_w^2)$ to $O(\\sqrt{d}/\\epsilon_p)$ for\npoisoning-concurrent watermarking, the watermarked poisoning dataset provably\nensures both watermarking detectability and poisoning utility, certifying the\npracticality of watermarking under data poisoning attacks. We validate our\ntheoretical findings through experiments on several attacks, models, and\ndatasets."
    },
    {
        "date": "2025-10",
        "title": "Augmented data and neural networks for robust epidemic forecasting: application to COVID-19 in Italy",
        "author": "Giacomo Dimarco, Federica Ferrarese, and Lorenzo Pareschi",
        "link": "http://arxiv.org/abs/2510.09192v1",
        "abstract": "In this work, we propose a data augmentation strategy aimed at improving the\ntraining phase of neural networks and, consequently, the accuracy of their\npredictions. Our approach relies on generating synthetic data through a\nsuitable compartmental model combined with the incorporation of uncertainty.\nThe available data are then used to calibrate the model, which is further\nintegrated with deep learning techniques to produce additional synthetic data\nfor training. The results show that neural networks trained on these augmented\ndatasets exhibit significantly improved predictive performance. We focus in\nparticular on two different neural network architectures: Physics-Informed\nNeural Networks (PINNs) and Nonlinear Autoregressive (NAR) models. The NAR\napproach proves especially effective for short-term forecasting, providing\naccurate quantitative estimates by directly learning the dynamics from data and\navoiding the additional computational cost of embedding physical constraints\ninto the training. In contrast, PINNs yield less accurate quantitative\npredictions but capture the qualitative long-term behavior of the system,\nmaking them more suitable for exploring broader dynamical trends. Numerical\nsimulations of the second phase of the COVID-19 pandemic in the Lombardy region\n(Italy) validate the effectiveness of the proposed approach."
    },
    {
        "date": "2025-10",
        "title": "On the Implicit Adversariality of Catastrophic Forgetting in Deep Continual Learning",
        "author": "Ze Peng, Jian Zhang, Jintao Guo, Lei Qi, Yang Gao, and Yinghuan Shi",
        "link": "http://arxiv.org/abs/2510.09181v1",
        "abstract": "Continual learning seeks the human-like ability to accumulate new skills in\nmachine intelligence. Its central challenge is catastrophic forgetting, whose\nunderlying cause has not been fully understood for deep networks. In this\npaper, we demystify catastrophic forgetting by revealing that the new-task\ntraining is implicitly an adversarial attack against the old-task knowledge.\nSpecifically, the new-task gradients automatically and accurately align with\nthe sharp directions of the old-task loss landscape, rapidly increasing the\nold-task loss. This adversarial alignment is intriguingly counter-intuitive\nbecause the sharp directions are too sparsely distributed to align with by\nchance. To understand it, we theoretically show that it arises from training's\nlow-rank bias, which, through forward and backward propagation, confines the\ntwo directions into the same low-dimensional subspace, facilitating alignment.\nGradient projection (GP) methods, a representative family of\nforgetting-mitigating methods, reduce adversarial alignment caused by forward\npropagation, but cannot address the alignment due to backward propagation. We\npropose backGP to address it, which reduces forgetting by 10.8% and improves\naccuracy by 12.7% on average over GP methods."
    },
    {
        "date": "2025-10",
        "title": "Distributionally robust approximation property of neural networks",
        "author": "Mihriban Ceylan, and David J. Pr\u00f6mel",
        "link": "http://arxiv.org/abs/2510.09177v1",
        "abstract": "The universal approximation property uniformly with respect to weakly compact\nfamilies of measures is established for several classes of neural networks. To\nthat end, we prove that these neural networks are dense in Orlicz spaces,\nthereby extending classical universal approximation theorems even beyond the\ntraditional $L^p$-setting. The covered classes of neural networks include\nwidely used architectures like feedforward neural networks with non-polynomial\nactivation functions, deep narrow networks with ReLU activation functions and\nfunctional input neural networks."
    },
    {
        "date": "2025-10",
        "title": "Robustness and Regularization in Hierarchical Re-Basin",
        "author": "Benedikt Franke, Florian Heinrich, Markus Lange, and Arne Raulf",
        "link": "http://arxiv.org/abs/2510.09174v2",
        "abstract": "This paper takes a closer look at Git Re-Basin, an interesting new approach\nto merge trained models. We propose a hierarchical model merging scheme that\nsignificantly outperforms the standard MergeMany algorithm. With our new\nalgorithm, we find that Re-Basin induces adversarial and perturbation\nrobustness into the merged models, with the effect becoming stronger the more\nmodels participate in the hierarchical merging scheme. However, in our\nexperiments Re-Basin induces a much bigger performance drop than reported by\nthe original authors."
    },
    {
        "date": "2025-10",
        "title": "Regret Bounds for Adversarial Contextual Bandits with General Function Approximation and Delayed Feedback",
        "author": "Orin Levy, Liad Erez, Alon Cohen, and Yishay Mansour",
        "link": "http://arxiv.org/abs/2510.09127v1",
        "abstract": "We present regret minimization algorithms for the contextual multi-armed\nbandit (CMAB) problem over $K$ actions in the presence of delayed feedback, a\nscenario where loss observations arrive with delays chosen by an adversary. As\na preliminary result, assuming direct access to a finite policy class $\\Pi$ we\nestablish an optimal expected regret bound of $ O (\\sqrt{KT \\log |\\Pi|} +\n\\sqrt{D \\log |\\Pi|)} $ where $D$ is the sum of delays. For our main\ncontribution, we study the general function approximation setting over a\n(possibly infinite) contextual loss function class $ \\mathcal{F} $ with access\nto an online least-square regression oracle $\\mathcal{O}$ over $\\mathcal{F}$.\nIn this setting, we achieve an expected regret bound of\n$O(\\sqrt{KT\\mathcal{R}_T(\\mathcal{O})} + \\sqrt{ d_{\\max} D \\beta})$ assuming\nFIFO order, where $d_{\\max}$ is the maximal delay, $\\mathcal{R}_T(\\mathcal{O})$\nis an upper bound on the oracle's regret and $\\beta$ is a stability parameter\nassociated with the oracle. We complement this general result by presenting a\nnovel stability analysis of a Hedge-based version of Vovk's aggregating\nforecaster as an oracle implementation for least-square regression over a\nfinite function class $\\mathcal{F}$ and show that its stability parameter\n$\\beta$ is bounded by $\\log |\\mathcal{F}|$, resulting in an expected regret\nbound of $O(\\sqrt{KT \\log |\\mathcal{F}|} + \\sqrt{d_{\\max} D \\log\n|\\mathcal{F}|})$ which is a $\\sqrt{d_{\\max}}$ factor away from the lower bound\nof $\\Omega(\\sqrt{KT \\log |\\mathcal{F}|} + \\sqrt{D \\log |\\mathcal{F}|})$ that we\nalso present."
    },
    {
        "date": "2025-10",
        "title": "MemLoss: Enhancing Adversarial Training with Recycling Adversarial Examples",
        "author": "Soroush Mahdi, Maryam Amirmazlaghani, Saeed Saravani, and Zahra Dehghanian",
        "link": "http://arxiv.org/abs/2510.09105v1",
        "abstract": "In this paper, we propose a new approach called MemLoss to improve the\nadversarial training of machine learning models. MemLoss leverages previously\ngenerated adversarial examples, referred to as 'Memory Adversarial Examples,'\nto enhance model robustness and accuracy without compromising performance on\nclean data. By using these examples across training epochs, MemLoss provides a\nbalanced improvement in both natural accuracy and adversarial robustness.\nExperimental results on multiple datasets, including CIFAR-10, demonstrate that\nour method achieves better accuracy compared to existing adversarial training\nmethods while maintaining strong robustness against attacks."
    },
    {
        "date": "2025-10",
        "title": "Emotion-Disentangled Embedding Alignment for Noise-Robust and Cross-Corpus Speech Emotion Recognition",
        "author": "Upasana Tiwari, Rupayan Chakraborty, and Sunil Kumar Kopparapu",
        "link": "http://arxiv.org/abs/2510.09072v1",
        "abstract": "Effectiveness of speech emotion recognition in real-world scenarios is often\nhindered by noisy environments and variability across datasets. This paper\nintroduces a two-step approach to enhance the robustness and generalization of\nspeech emotion recognition models through improved representation learning.\nFirst, our model employs EDRL (Emotion-Disentangled Representation Learning) to\nextract class-specific discriminative features while preserving shared\nsimilarities across emotion categories. Next, MEA (Multiblock Embedding\nAlignment) refines these representations by projecting them into a joint\ndiscriminative latent subspace that maximizes covariance with the original\nspeech input. The learned EDRL-MEA embeddings are subsequently used to train an\nemotion classifier using clean samples from publicly available datasets, and\nare evaluated on unseen noisy and cross-corpus speech samples. Improved\nperformance under these challenging conditions demonstrates the effectiveness\nof the proposed method."
    },
    {
        "date": "2025-10",
        "title": "Robust Driving Control for Autonomous Vehicles: An Intelligent General-sum Constrained Adversarial Reinforcement Learning Approach",
        "author": "Junchao Fan, and Xiaolin Chang",
        "link": "http://arxiv.org/abs/2510.09041v1",
        "abstract": "Deep reinforcement learning (DRL) has demonstrated remarkable success in\ndeveloping autonomous driving policies. However, its vulnerability to\nadversarial attacks remains a critical barrier to real-world deployment.\nAlthough existing robust methods have achieved success, they still suffer from\nthree key issues: (i) these methods are trained against myopic adversarial\nattacks, limiting their abilities to respond to more strategic threats, (ii)\nthey have trouble causing truly safety-critical events (e.g., collisions), but\ninstead often result in minor consequences, and (iii) these methods can\nintroduce learning instability and policy drift during training due to the lack\nof robust constraints. To address these issues, we propose Intelligent\nGeneral-sum Constrained Adversarial Reinforcement Learning (IGCARL), a novel\nrobust autonomous driving approach that consists of a strategic targeted\nadversary and a robust driving agent. The strategic targeted adversary is\ndesigned to leverage the temporal decision-making capabilities of DRL to\nexecute strategically coordinated multi-step attacks. In addition, it\nexplicitly focuses on inducing safety-critical events by adopting a general-sum\nobjective. The robust driving agent learns by interacting with the adversary to\ndevelop a robust autonomous driving policy against adversarial attacks. To\nensure stable learning in adversarial environments and to mitigate policy drift\ncaused by attacks, the agent is optimized under a constrained formulation.\nExtensive experiments show that IGCARL improves the success rate by at least\n27.9\\% over state-of-the-art methods, demonstrating superior robustness to\nadversarial attacks and enhancing the safety and reliability of DRL-based\nautonomous driving."
    },
    {
        "date": "2025-10",
        "title": "The Attacker Moves Second: Stronger Adaptive Attacks Bypass Defenses Against Llm Jailbreaks and Prompt Injections",
        "author": "Milad Nasr, Nicholas Carlini, Chawin Sitawarin, Sander V. Schulhoff, Jamie Hayes, Michael Ilie, Juliette Pluto, Shuang Song, Harsh Chaudhari, Ilia Shumailov, Abhradeep Thakurta, Kai Yuanqing Xiao, Andreas Terzis, and Florian Tram\u00e8r",
        "link": "http://arxiv.org/abs/2510.09023v1",
        "abstract": "How should we evaluate the robustness of language model defenses? Current\ndefenses against jailbreaks and prompt injections (which aim to prevent an\nattacker from eliciting harmful knowledge or remotely triggering malicious\nactions, respectively) are typically evaluated either against a static set of\nharmful attack strings, or against computationally weak optimization methods\nthat were not designed with the defense in mind. We argue that this evaluation\nprocess is flawed.\n  Instead, we should evaluate defenses against adaptive attackers who\nexplicitly modify their attack strategy to counter a defense's design while\nspending considerable resources to optimize their objective. By systematically\ntuning and scaling general optimization techniques-gradient descent,\nreinforcement learning, random search, and human-guided exploration-we bypass\n12 recent defenses (based on a diverse set of techniques) with attack success\nrate above 90% for most; importantly, the majority of defenses originally\nreported near-zero attack success rates. We believe that future defense work\nmust consider stronger attacks, such as the ones we describe, in order to make\nreliable and convincing claims of robustness."
    },
    {
        "date": "2025-10",
        "title": "Future G Network's New Reality: Opportunities and Security Challenges",
        "author": "Chandra Thapa, and Surya Nepal",
        "link": "http://arxiv.org/abs/2510.09006v1",
        "abstract": "Future G network's new reality is a widespread cyber-physical environment\ncreated by Integrated Sensing and Communication (ISAC). It is a crucial\ntechnology that transforms wireless connections into ubiquitous sensors. ISAC\nunlocks transformative new capabilities, powering autonomous systems, augmented\nhuman sensing, and next-generation immersive applications, such as digital\ntwins. However, this new reality fundamentally reshapes the security landscape.\nThe primary security concern shifts from the traditional focus on data\nprotection to a new priority: safeguarding the integrity of the system's\nperception of physical reality itself. This perception can be perilously\nmanipulated by sophisticated attacks such as sensing eavesdropping, phantom\ndangers, and invisible threats, potentially resulting in direct and\ncatastrophic physical harm. Traditional security measures, such as\nsignature-based detection, are insufficient to counter these perception-level\nthreats that mimic genuine physical signals. A proactive, layered,\ndefense-in-depth strategy is required, integrating physical, environmental,\nintelligence, and architectural security measures to build a trustworthy\necosystem. Additionally, realizing ISAC's potential responsibly also depends on\nparallel efforts in global standardization and strong governance to address the\nsignificant challenges of privacy, liability, and the technology's dual-use."
    },
    {
        "date": "2025-10",
        "title": "Group-Adaptive Adversarial Learning for Robust Fake News Detection Against Malicious Comments",
        "author": "Zhao Tong, Chunlin Gong, Yimeng Gu, Haichao Shi, Qiang Liu, Shu Wu, and Xiao-Yu Zhang",
        "link": "http://arxiv.org/abs/2510.09712v1",
        "abstract": "The spread of fake news online distorts public judgment and erodes trust in\nsocial media platforms. Although recent fake news detection (FND) models\nperform well in standard settings, they remain vulnerable to adversarial\ncomments-authored by real users or by large language models (LLMs)-that subtly\nshift model decisions. In view of this, we first present a comprehensive\nevaluation of comment attacks to existing fake news detectors and then\nintroduce a group-adaptive adversarial training strategy to improve the\nrobustness of FND models. To be specific, our approach comprises three steps:\n(1) dividing adversarial comments into three psychologically grounded\ncategories: perceptual, cognitive, and societal; (2) generating diverse,\ncategory-specific attacks via LLMs to enhance adversarial training; and (3)\napplying a Dirichlet-based adaptive sampling mechanism (InfoDirichlet Adjusting\nMechanism) that dynamically adjusts the learning focus across different comment\ncategories during training. Experiments on benchmark datasets show that our\nmethod maintains strong detection accuracy while substantially increasing\nrobustness to a wide range of adversarial comment perturbations."
    },
    {
        "date": "2025-10",
        "title": "RO-Bench: Large-scale robustness evaluation of MLLMs with text-driven counterfactual videos",
        "author": "Zixi Yang, Jiapeng Li, Muxi Diao, Yinuo Jing, and Kongming Liang",
        "link": "http://arxiv.org/abs/2510.08936v1",
        "abstract": "Recently, Multi-modal Large Language Models (MLLMs) have demonstrated\nsignificant performance across various video understanding tasks. However,\ntheir robustness, particularly when faced with manipulated video content,\nremains largely unexplored. In this paper, we introduce Ro-Bench, the first\nbenchmark for evaluating MLLMs on dynamic out-of-distribution (OOD)\ncounterfactual video test sets. Ro-Bench incorporates high-quality, diverse and\ntemporally relevant video data, by editing Style, Object, Background and their\ncompositions. We evaluated eight recent video MLLMs and found that current\nmodels exhibit substantial performance degradation on Ro-Bench when exposed to\ncounterfactual video content. Furthermore, we demonstrate that fine-tuning\nMLLMs with counterfactual data enhances robustness, achieving a 21.73%\nperformance increase on Ro-Bench and a 12.78% improvement across 20 tasks in\nthe MVBench dataset. These findings underscore the effectiveness of\ncounterfactual data in enhancing the video understanding ability of MLLMs. The\ncode and data will be released shortly."
    },
    {
        "date": "2025-10",
        "title": "Defense against Unauthorized Distillation in Image Restoration via Feature Space Perturbation",
        "author": "Han Hu, Zhuoran Zheng, and Chen Lyu",
        "link": "http://arxiv.org/abs/2510.08925v1",
        "abstract": "Knowledge distillation (KD) attacks pose a significant threat to deep model\nintellectual property by enabling adversaries to train student networks using a\nteacher model's outputs. While recent defenses in image classification have\nsuccessfully disrupted KD by perturbing output probabilities, extending these\nmethods to image restoration is difficult. Unlike classification, restoration\nis a generative task with continuous, high-dimensional outputs that depend on\nspatial coherence and fine details. Minor perturbations are often insufficient,\nas students can still learn the underlying mapping.To address this, we propose\nAdaptive Singular Value Perturbation (ASVP), a runtime defense tailored for\nimage restoration models. ASVP operates on internal feature maps of the teacher\nusing singular value decomposition (SVD). It amplifies the topk singular values\nto inject structured, high-frequency perturbations, disrupting the alignment\nneeded for distillation. This hinders student learning while preserving the\nteacher's output quality.We evaluate ASVP across five image restoration tasks:\nsuper-resolution, low-light enhancement, underwater enhancement, dehazing, and\nderaining. Experiments show ASVP reduces student PSNR by up to 4 dB and SSIM by\n60-75%, with negligible impact on the teacher's performance. Compared to prior\nmethods, ASVP offers a stronger and more consistent defense.Our approach\nprovides a practical solution to protect open-source restoration models from\nunauthorized knowledge distillation."
    },
    {
        "date": "2025-10",
        "title": "SegTrans: Transferable Adversarial Examples for Segmentation Models",
        "author": "Yufei Song, Ziqi Zhou, Qi Lu, Hangtao Zhang, Yifan Hu, Lulu Xue, Shengshan Hu, Minghui Li, and Leo Yu Zhang",
        "link": "http://arxiv.org/abs/2510.08922v1",
        "abstract": "Segmentation models exhibit significant vulnerability to adversarial examples\nin white-box settings, but existing adversarial attack methods often show poor\ntransferability across different segmentation models. While some researchers\nhave explored transfer-based adversarial attack (i.e., transfer attack) methods\nfor segmentation models, the complex contextual dependencies within these\nmodels and the feature distribution gaps between surrogate and target models\nresult in unsatisfactory transfer success rates. To address these issues, we\npropose SegTrans, a novel transfer attack framework that divides the input\nsample into multiple local regions and remaps their semantic information to\ngenerate diverse enhanced samples. These enhanced samples replace the original\nones for perturbation optimization, thereby improving the transferability of\nadversarial examples across different segmentation models. Unlike existing\nmethods, SegTrans only retains local semantic information from the original\ninput, rather than using global semantic information to optimize perturbations.\nExtensive experiments on two benchmark datasets, PASCAL VOC and Cityscapes,\nfour different segmentation models, and three backbone networks show that\nSegTrans significantly improves adversarial transfer success rates without\nintroducing additional computational overhead. Compared to the current\nstate-of-the-art methods, SegTrans achieves an average increase of 8.55% in\ntransfer attack success rate and improves computational efficiency by more than\n100%."
    },
    {
        "date": "2025-10",
        "title": "Simple and Robust Forecasting of Spatiotemporally Correlated Small Earth Data with A Tabular Foundation Model",
        "author": "Yuting Yang, Gang Mei, Zhengjing Ma, Nengxiong Xu, and Jianbing Peng",
        "link": "http://arxiv.org/abs/2510.08920v1",
        "abstract": "Small Earth data are geoscience observations with limited short-term\nmonitoring variability, providing sparse but meaningful measurements, typically\nexhibiting spatiotemporal correlations. Spatiotemporal forecasting on such data\nis crucial for understanding geoscientific processes despite their small scale.\nHowever, conventional deep learning models for spatiotemporal forecasting\nrequires task-specific training for different scenarios. Foundation models do\nnot need task-specific training, but they often exhibit forecasting bias toward\nthe global mean of the pretraining distribution. Here we propose a simple and\nrobust approach for spatiotemporally correlated small Earth data forecasting.\nThe essential idea is to characterize and quantify spatiotemporal patterns of\nsmall Earth data and then utilize tabular foundation models for accurate\nforecasting across different scenarios. Comparative results across three\ntypical scenarios demonstrate that our forecasting approach achieves superior\naccuracy compared to the graph deep learning model (T-GCN) and tabular\nfoundation model (TabPFN) in the majority of instances, exhibiting stronger\nrobustness."
    },
    {
        "date": "2025-10",
        "title": "Gradient-Guided Furthest Point Sampling for Robust Training Set Selection",
        "author": "Morris Trestman, Stefan Gugler, Felix A. Faber, and O. A. von Lilienfeld",
        "link": "http://arxiv.org/abs/2510.08906v1",
        "abstract": "Smart training set selections procedures enable the reduction of data needs\nand improves predictive robustness in machine learning problems relevant to\nchemistry. We introduce Gradient Guided Furthest Point Sampling (GGFPS), a\nsimple extension of Furthest Point Sampling (FPS) that leverages molecular\nforce norms to guide efficient sampling of configurational spaces of molecules.\nNumerical evidence is presented for a toy-system (Styblinski-Tang function) as\nwell as for molecular dynamics trajectories from the MD17 dataset. Compared to\nFPS and uniform sampling, our numerical results indicate superior data\nefficiency and robustness when using GGFPS. Distribution analysis of the MD17\ndata suggests that FPS systematically under-samples equilibrium geometries,\nresulting in large test errors for relaxed structures. GGFPS cures this\nartifact and (i) enables up to two fold reductions in training cost without\nsacrificing predictive accuracy compared to FPS in the 2-dimensional\nStyblinksi-Tang system, (ii) systematically lowers prediction errors for\nequilibrium as well as strained structures in MD17, and (iii) systematically\ndecreases prediction error variances across all of the MD17 configuration\nspaces. These results suggest that gradient-aware sampling methods hold great\npromise as effective training set selection tools, and that naive use of FPS\nmay result in imbalanced training and inconsistent prediction outcomes."
    },
    {
        "date": "2025-10",
        "title": "An Improved Model-Free Decision-Estimation Coefficient with Applications in Adversarial MDPs",
        "author": "Haolin Liu, Chen-Yu Wei, and Julian Zimmert",
        "link": "http://arxiv.org/abs/2510.08882v1",
        "abstract": "We study decision making with structured observation (DMSO). Previous work\n(Foster et al., 2021b, 2023a) has characterized the complexity of DMSO via the\ndecision-estimation coefficient (DEC), but left a gap between the regret upper\nand lower bounds that scales with the size of the model class. To tighten this\ngap, Foster et al. (2023b) introduced optimistic DEC, achieving a bound that\nscales only with the size of the value-function class. However, their\noptimism-based exploration is only known to handle the stochastic setting, and\nit remains unclear whether it extends to the adversarial setting.\n  We introduce Dig-DEC, a model-free DEC that removes optimism and drives\nexploration purely by information gain. Dig-DEC is always no larger than\noptimistic DEC and can be much smaller in special cases. Importantly, the\nremoval of optimism allows it to handle adversarial environments without\nexplicit reward estimators. By applying Dig-DEC to hybrid MDPs with stochastic\ntransitions and adversarial rewards, we obtain the first model-free regret\nbounds for hybrid MDPs with bandit feedback under several general transition\nstructures, resolving the main open problem left by Liu et al. (2025).\n  We also improve the online function-estimation procedure in model-free\nlearning: For average estimation error minimization, we refine the estimator in\nFoster et al. (2023b) to achieve sharper concentration, improving their regret\nbounds from $T^{3/4}$ to $T^{2/3}$ (on-policy) and from $T^{5/6}$ to $T^{7/9}$\n(off-policy). For squared error minimization in Bellman-complete MDPs, we\nredesign their two-timescale procedure, improving the regret bound from\n$T^{2/3}$ to $\\sqrt{T}$. This is the first time a DEC-based method achieves\nperformance matching that of optimism-based approaches (Jin et al., 2021; Xie\net al., 2023) in Bellman-complete MDPs."
    },
    {
        "date": "2025-10",
        "title": "A Demonstration of Self-Adaptive Jamming Attack Detection in AI/ML Integrated O-RAN",
        "author": "Md Habibur Rahman, Md Sharif Hossen, Nathan H. Stephenson, Vijay K. Shah, and Aloizio Da Silva",
        "link": "http://arxiv.org/abs/2510.09706v1",
        "abstract": "The open radio access network (O-RAN) enables modular, intelligent, and\nprogrammable 5G network architectures through the adoption of software-defined\nnetworking, network function virtualization, and implementation of standardized\nopen interfaces. However, one of the security concerns for O-RAN, which can\nseverely undermine network performance, is jamming attacks. This paper presents\nSAJD- a self-adaptive jammer detection framework that autonomously detects\njamming attacks in AI/ML framework-integrated ORAN environments without human\nintervention. The SAJD framework forms a closed-loop system that includes\nnear-realtime inference of radio signal jamming via our developed ML-based\nxApp, as well as continuous monitoring and retraining pipelines through rApps.\nIn this demonstration, we will show how SAJD outperforms state-of-the-art\njamming detection xApp (offline trained with manual labels) in terms of\naccuracy and adaptability under various dynamic and previously unseen\ninterference scenarios in the O-RAN-compliant testbed."
    },
    {
        "date": "2025-10",
        "title": "CommandSans: Securing AI Agents with Surgical Precision Prompt Sanitization",
        "author": "Debeshee Das, Luca Beurer-Kellner, Marc Fischer, and Maximilian Baader",
        "link": "http://arxiv.org/abs/2510.08829v1",
        "abstract": "The increasing adoption of LLM agents with access to numerous tools and\nsensitive data significantly widens the attack surface for indirect prompt\ninjections. Due to the context-dependent nature of attacks, however, current\ndefenses are often ill-calibrated as they cannot reliably differentiate\nmalicious and benign instructions, leading to high false positive rates that\nprevent their real-world adoption. To address this, we present a novel approach\ninspired by the fundamental principle of computer security: data should not\ncontain executable instructions. Instead of sample-level classification, we\npropose a token-level sanitization process, which surgically removes any\ninstructions directed at AI systems from tool outputs, capturing malicious\ninstructions as a byproduct. In contrast to existing safety classifiers, this\napproach is non-blocking, does not require calibration, and is agnostic to the\ncontext of tool outputs. Further, we can train such token-level predictors with\nreadily available instruction-tuning data only, and don't have to rely on\nunrealistic prompt injection examples from challenges or of other synthetic\norigin. In our experiments, we find that this approach generalizes well across\na wide range of attacks and benchmarks like AgentDojo, BIPIA, InjecAgent, ASB\nand SEP, achieving a 7-10x reduction of attack success rate (ASR) (34% to 3% on\nAgentDojo), without impairing agent utility in both benign and malicious\nsettings."
    },
    {
        "date": "2025-10",
        "title": "SAFER-AiD: Saccade-Assisted Foveal-peripheral vision Enhanced Reconstruction for Adversarial Defense",
        "author": "Jiayang Liu, Daniel Tso, Yiming Bu, and Qinru Qiu",
        "link": "http://arxiv.org/abs/2510.08761v1",
        "abstract": "Adversarial attacks significantly challenge the safe deployment of deep\nlearning models, particularly in real-world applications. Traditional defenses\noften rely on computationally intensive optimization (e.g., adversarial\ntraining or data augmentation) to improve robustness, whereas the human visual\nsystem achieves inherent robustness to adversarial perturbations through\nevolved biological mechanisms. We hypothesize that attention guided\nnon-homogeneous sparse sampling and predictive coding plays a key role in this\nrobustness. To test this hypothesis, we propose a novel defense framework\nincorporating three key biological mechanisms: foveal-peripheral processing,\nsaccadic eye movements, and cortical filling-in. Our approach employs\nreinforcement learning-guided saccades to selectively capture multiple\nfoveal-peripheral glimpses, which are integrated into a reconstructed image\nbefore classification. This biologically inspired preprocessing effectively\nmitigates adversarial noise, preserves semantic integrity, and notably requires\nno retraining or fine-tuning of downstream classifiers, enabling seamless\nintegration with existing systems. Experiments on the ImageNet dataset\ndemonstrate that our method improves system robustness across diverse\nclassifiers and attack types, while significantly reducing training overhead\ncompared to both biologically and non-biologically inspired defense techniques."
    },
    {
        "date": "2025-10",
        "title": "Robust Heuristic Algorithm Design with LLMs",
        "author": "Pantea Karimi, Dany Rouhana, Pooria Namyar, Siva Kesava Reddy Kakarla, Venkat Arun, and Behnaz Arzani",
        "link": "http://arxiv.org/abs/2510.08755v1",
        "abstract": "We posit that we can generate more robust and performant heuristics if we\naugment approaches using LLMs for heuristic design with tools that explain why\nheuristics underperform and suggestions about how to fix them. We find even\nsimple ideas that (1) expose the LLM to instances where the heuristic\nunderperforms; (2) explain why they occur; and (3) specialize design to regions\nin the input space, can produce more robust algorithms compared to existing\ntechniques~ -- ~the heuristics we produce have a $\\sim28\\times$ better\nworst-case performance compared to FunSearch, improve average performance, and\nmaintain the runtime."
    },
    {
        "date": "2025-10",
        "title": "Post-Quantum Security of Block Cipher Constructions",
        "author": "Gorjan Alagic, Chen Bai, Christian Majenz, and Kaiyan Shi",
        "link": "http://arxiv.org/abs/2510.08725v1",
        "abstract": "Block ciphers are versatile cryptographic ingredients that are used in a wide\nrange of applications ranging from secure Internet communications to disk\nencryption. While post-quantum security of public-key cryptography has received\nsignificant attention, the case of symmetric-key cryptography (and block\nciphers in particular) remains a largely unexplored topic. In this work, we set\nthe foundations for a theory of post-quantum security for block ciphers and\nassociated constructions. Leveraging our new techniques, we provide the first\npost-quantum security proofs for the key-length extension scheme FX, the\ntweakable block ciphers LRW and XEX, and most block cipher encryption and\nauthentication modes. Our techniques can be used for security proofs in both\nthe plain model and the quantum ideal cipher model. Our work takes significant\ninitial steps in establishing a rigorous understanding of the post-quantum\nsecurity of practical symmetric-key cryptography."
    },
    {
        "date": "2025-10",
        "title": "Are Voters Willing to Collectively Secure Elections? Unraveling a Practical Blockchain Voting System",
        "author": "Zhuolun Li, Haluk Sonmezler, Faiza Shirazi, Febin Shaji, Tymoteusz Mroczkowski, Dexter Lardner, Matthew Alain Camus, and Evangelos Pournaras",
        "link": "http://arxiv.org/abs/2510.08700v1",
        "abstract": "Ensuring ballot secrecy is critical for fair and trustworthy electronic\nvoting systems, yet achieving strong secrecy guarantees in decentralized,\nlarge-scale elections remains challenging. This paper proposes the concept of\ncollectively secure voting, in which voters themselves can opt in as secret\nholders to protect ballot secrecy. A practical blockchain-based collectively\nsecure voting system is designed and implemented. Our design strikes a balance\nbetween strong confidentiality guarantees and real-world applicability. The\nproposed system combines threshold cryptography and smart contracts to ensure\nballots remain confidential during voting, while all protocol steps remain\ntransparent and verifiable. Voters can use the system without prior blockchain\nknowledge through an intuitive user interface that hides underlying complexity.\nTo evaluate this approach, a user testing is conducted. Results show a high\nwillingness to act as secret holders, reliable participation in share release,\nand high security confidence in the proposed system. The findings demonstrate\nthat voters can collectively maintain secrecy and that such a practical\ndeployment is feasible."
    },
    {
        "date": "2025-10",
        "title": "MATRIX: Multimodal Agent Tuning for Robust Tool-Use Reasoning",
        "author": "Tajamul Ashraf, Umair Nawaz, Abdelrahman M. Shaker, Rao Anwer, Philip Torr, Fahad Shahbaz Khan, and Salman Khan",
        "link": "http://arxiv.org/abs/2510.08567v1",
        "abstract": "Vision language models (VLMs) are increasingly deployed as controllers with\naccess to external tools for complex reasoning and decision-making, yet their\neffectiveness remains limited by the scarcity of high-quality multimodal\ntrajectories and the cost of manual annotation. We address this challenge with\na vision-centric agent tuning framework that automatically synthesizes\nmultimodal trajectories, generates step-wise preference pairs, and trains a VLM\ncontroller for robust tool-use reasoning. Our pipeline first constructs\nM-TRACE, a large-scale dataset of 28.5K multimodal tasks with 177K verified\ntrajectories, enabling imitation-based trajectory tuning. Building on this, we\ndevelop MATRIX Agent, a controller finetuned on M-TRACE for step-wise tool\nreasoning. To achieve finer alignment, we further introduce Pref-X, a set of\n11K automatically generated preference pairs, and optimize MATRIX on it via\nstep-wise preference learning. Across three benchmarks, Agent-X, GTA, and GAIA,\nMATRIX consistently surpasses both open- and closed-source VLMs, demonstrating\nscalable and effective multimodal tool use. Our data and code is avaliable at\nhttps://github.com/mbzuai-oryx/MATRIX."
    },
    {
        "date": "2025-10",
        "title": "Robust Source-Free Domain Adaptation for Medical Image Segmentation based on Curriculum Learning",
        "author": "Ziqi Zhang, Yuexiang Li, Yawen Huang, Nanjun He, Tao Xu, Liwei Lin, Yefeng Zheng, Shaoxin Li, and Feiyue Huang",
        "link": "http://arxiv.org/abs/2510.08393v1",
        "abstract": "Recent studies have uncovered a new research line, namely source-free domain\nadaptation, which adapts a model to target domains without using the source\ndata. Such a setting can address the concerns on data privacy and security\nissues of medical images. However, current source-free domain adaptation\nframeworks mainly focus on the pseudo label refinement for target data without\nthe consideration of learning procedure. Indeed, a progressive learning process\nfrom source to target domain will benefit the knowledge transfer during model\nadaptation. To this end, we propose a curriculum-based framework, namely\nlearning from curriculum (LFC), for source-free domain adaptation, which\nconsists of easy-to-hard and source-to-target curricula. Concretely, the former\ncurriculum enables the framework to start learning with `easy' samples and\ngradually tune the optimization direction of model adaption by increasing the\nsample difficulty. While, the latter can stablize the adaptation process, which\nensures smooth transfer of the model from the source domain to the target. We\nevaluate the proposed source-free domain adaptation approach on the public\ncross-domain datasets for fundus segmentation and polyp segmentation. The\nextensive experimental results show that our framework surpasses the existing\napproaches and achieves a new state-of-the-art."
    },
    {
        "date": "2025-10",
        "title": "Robust and Efficient Collaborative Learning",
        "author": "Abdellah El Mrini, Sadegh Farhadkhan, and Rachid Guerraoui",
        "link": "http://arxiv.org/abs/2510.08311v1",
        "abstract": "Collaborative machine learning is challenged by training-time adversarial\nbehaviors. Existing approaches to tolerate such behaviors either rely on a\ncentral server or induce high communication costs. We propose Robust Pull-based\nEpidemic Learning (RPEL), a novel, scalable collaborative approach to ensure\nrobust learning despite adversaries. RPEL does not rely on any central server\nand, unlike traditional methods, where communication costs grow in\n$\\mathcal{O}(n^2)$ with the number of nodes $n$, RPEL employs a pull-based\nepidemic-based communication strategy that scales in $\\mathcal{O}(n \\log n)$.\nBy pulling model parameters from small random subsets of nodes, RPEL\nsignificantly lowers the number of required messages without compromising\nconvergence guarantees, which hold with high probability. Empirical results\ndemonstrate that RPEL maintains robustness in adversarial settings, competes\nwith all-to-all communication accuracy, and scales efficiently across large\nnetworks."
    },
    {
        "date": "2025-10",
        "title": "Chain-of-Trigger: An Agentic Backdoor that Paradoxically Enhances Agentic Robustness",
        "author": "Jiyang Qiu, Xinbei Ma, Yunqing Xu, Zhuosheng Zhang, and Hai Zhao",
        "link": "http://arxiv.org/abs/2510.08238v1",
        "abstract": "The rapid deployment of large language model (LLM)-based agents in real-world\napplications has raised serious concerns about their trustworthiness. In this\nwork, we reveal the security and robustness vulnerabilities of these agents\nthrough backdoor attacks. Distinct from traditional backdoors limited to\nsingle-step control, we propose the Chain-of-Trigger Backdoor (CoTri), a\nmulti-step backdoor attack designed for long-horizon agentic control. CoTri\nrelies on an ordered sequence. It starts with an initial trigger, and\nsubsequent ones are drawn from the environment, allowing multi-step\nmanipulation that diverts the agent from its intended task. Experimental\nresults show that CoTri achieves a near-perfect attack success rate (ASR) while\nmaintaining a near-zero false trigger rate (FTR). Due to training data modeling\nthe stochastic nature of the environment, the implantation of CoTri\nparadoxically enhances the agent's performance on benign tasks and even\nimproves its robustness against environmental distractions. We further validate\nCoTri on vision-language models (VLMs), confirming its scalability to\nmultimodal agents. Our work highlights that CoTri achieves stable, multi-step\ncontrol within agents, improving their inherent robustness and task\ncapabilities, which ultimately makes the attack more stealthy and raises\npotential safty risks."
    },
    {
        "date": "2025-10",
        "title": "Robust Canonicalization through Bootstrapped Data Re-Alignment",
        "author": "Johann Schmidt, and Sebastian Stober",
        "link": "http://arxiv.org/abs/2510.08178v1",
        "abstract": "Fine-grained visual classification (FGVC) tasks, such as insect and bird\nidentification, demand sensitivity to subtle visual cues while remaining robust\nto spatial transformations. A key challenge is handling geometric biases and\nnoise, such as different orientations and scales of objects. Existing remedies\nrely on heavy data augmentation, which demands powerful models, or on\nequivariant architectures, which constrain expressivity and add cost.\nCanonicalization offers an alternative by shielding such biases from the\ndownstream model. In practice, such functions are often obtained using\ncanonicalization priors, which assume aligned training data. Unfortunately,\nreal-world datasets never fulfill this assumption, causing the obtained\ncanonicalizer to be brittle. We propose a bootstrapping algorithm that\niteratively re-aligns training samples by progressively reducing variance and\nrecovering the alignment assumption. We establish convergence guarantees under\nmild conditions for arbitrary compact groups, and show on four FGVC benchmarks\nthat our method consistently outperforms equivariant, and canonicalization\nbaselines while performing on par with augmentation."
    },
    {
        "date": "2025-10",
        "title": "Provably Robust Adaptation for Language-Empowered Foundation Models",
        "author": "Yuni Lai, Xiaoyu Xue, Linghui Shen, Yulun Wu, Gaolei Li, Song Guo, Kai Zhou, and Bin Xiao",
        "link": "http://arxiv.org/abs/2510.08659v1",
        "abstract": "Language-empowered foundation models (LeFMs), such as CLIP and GraphCLIP,\nhave transformed multimodal learning by aligning visual (or graph) features\nwith textual representations, enabling powerful downstream capabilities like\nfew-shot learning. However, the reliance on small, task-specific support\ndatasets collected in open environments exposes these models to poisoning\nattacks, where adversaries manipulate the support samples to degrade\nperformance. Existing defenses rely on empirical strategies, which lack formal\nguarantees and remain vulnerable to unseen and adaptive attacks. Certified\nrobustness offers provable guarantees but has been largely unexplored for\nfew-shot classifiers based on LeFMs. This study seeks to fill these critical\ngaps by proposing the first provably robust few-shot classifier that is\ntailored for LeFMs. We term our model Language-empowered Few-shot Certification\n(\\textbf{LeFCert}). It integrates both textual and feature embeddings with an\nadaptive blending mechanism. To achieve provable robustness, we propose a\ntwofold trimmed mean prototype and derive provable upper and lower bounds for\nclassification scores, enabling certification under worst-case poisoning\nscenarios. To further enhance the performance, we extend LeFCert with two\nvariants by considering a more realistic and tighter attack budget: LeFCert-L\nincorporates randomized smoothing to provide Lipschitz continuity and derive\nrobustness under dual budget constraints, and LeFCert-C provides collective\ncertification for scenarios where attackers distribute a shared poisoning\nbudget across multiple samples. Experiments demonstrate that LeFCert achieves\nstate-of-the-art performance, significantly improving both clean and certified\naccuracy compared to existing baselines. Despite its advanced robustness\nmechanisms, LeFCert is computationally efficient, making it practical for\nreal-world applications."
    },
    {
        "date": "2025-10",
        "title": "Random Window Augmentations for Deep Learning Robustness in CT and Liver Tumor Segmentation",
        "author": "Eirik A. \u00d8stmo, Kristoffer K. Wickstr\u00f8m, Keyur Radiya, Michael C. Kampffmeyer, Karl \u00d8yvind Mikalsen, and Robert Jenssen",
        "link": "http://arxiv.org/abs/2510.08116v1",
        "abstract": "Contrast-enhanced Computed Tomography (CT) is important for diagnosis and\ntreatment planning for various medical conditions. Deep learning (DL) based\nsegmentation models may enable automated medical image analysis for detecting\nand delineating tumors in CT images, thereby reducing clinicians' workload.\nAchieving generalization capabilities in limited data domains, such as\nradiology, requires modern DL models to be trained with image augmentation.\nHowever, naively applying augmentation methods developed for natural images to\nCT scans often disregards the nature of the CT modality, where the intensities\nmeasure Hounsfield Units (HU) and have important physical meaning. This paper\nchallenges the use of such intensity augmentations for CT imaging and shows\nthat they may lead to artifacts and poor generalization. To mitigate this, we\npropose a CT-specific augmentation technique, called Random windowing, that\nexploits the available HU distribution of intensities in CT images. Random\nwindowing encourages robustness to contrast-enhancement and significantly\nincreases model performance on challenging images with poor contrast or timing.\nWe perform ablations and analysis of our method on multiple datasets, and\ncompare to, and outperform, state-of-the-art alternatives, while focusing on\nthe challenge of liver tumor segmentation."
    },
    {
        "date": "2025-10",
        "title": "DarkHash: A Data-Free Backdoor Attack Against Deep Hashing",
        "author": "Ziqi Zhou, Menghao Deng, Yufei Song, Hangtao Zhang, Wei Wan, Shengshan Hu, Minghui Li, Leo Yu Zhang, and Dezhong Yao",
        "link": "http://arxiv.org/abs/2510.08094v1",
        "abstract": "Benefiting from its superior feature learning capabilities and efficiency,\ndeep hashing has achieved remarkable success in large-scale image retrieval.\nRecent studies have demonstrated the vulnerability of deep hashing models to\nbackdoor attacks. Although these studies have shown promising attack results,\nthey rely on access to the training dataset to implant the backdoor. In the\nreal world, obtaining such data (e.g., identity information) is often\nprohibited due to privacy protection and intellectual property concerns.\nEmbedding backdoors into deep hashing models without access to the training\ndata, while maintaining retrieval accuracy for the original task, presents a\nnovel and challenging problem. In this paper, we propose DarkHash, the first\ndata-free backdoor attack against deep hashing. Specifically, we design a novel\nshadow backdoor attack framework with dual-semantic guidance. It embeds\nbackdoor functionality and maintains original retrieval accuracy by fine-tuning\nonly specific layers of the victim model using a surrogate dataset. We consider\nleveraging the relationship between individual samples and their neighbors to\nenhance backdoor attacks during training. By designing a topological alignment\nloss, we optimize both individual and neighboring poisoned samples toward the\ntarget sample, further enhancing the attack capability. Experimental results on\nfour image datasets, five model architectures, and two hashing methods\ndemonstrate the high effectiveness of DarkHash, outperforming existing\nstate-of-the-art backdoor attack methods. Defense experiments show that\nDarkHash can withstand existing mainstream backdoor defense methods."
    },
    {
        "date": "2025-10",
        "title": "A Novel Ensemble Learning Approach for Enhanced IoT Attack Detection: Redefining Security Paradigms in Connected Systems",
        "author": "Hikmat A. M. Abdeljaber, Md. Alamgir Hossain, Sultan Ahmad, Ahmed Alsanad, Md Alimul Haque, Sudan Jha, and Jabeen Nazeer",
        "link": "http://arxiv.org/abs/2510.08084v1",
        "abstract": "The rapid expansion of Internet of Things (IoT) devices has transformed\nindustries and daily life by enabling widespread connectivity and data\nexchange. However, this increased interconnection has introduced serious\nsecurity vulnerabilities, making IoT systems more exposed to sophisticated\ncyber attacks. This study presents a novel ensemble learning architecture\ndesigned to improve IoT attack detection. The proposed approach applies\nadvanced machine learning techniques, specifically the Extra Trees Classifier,\nalong with thorough preprocessing and hyperparameter optimization. It is\nevaluated on several benchmark datasets including CICIoT2023, IoTID20,\nBotNeTIoT L01, ToN IoT, N BaIoT, and BoT IoT. The results show excellent\nperformance, achieving high recall, accuracy, and precision with very low error\nrates. These outcomes demonstrate the model efficiency and superiority compared\nto existing approaches, providing an effective and scalable method for securing\nIoT environments. This research establishes a solid foundation for future\nprogress in protecting connected devices from evolving cyber threats."
    },
    {
        "date": "2025-10",
        "title": "Backdoor Vectors: a Task Arithmetic View on Backdoor Attacks and Defenses",
        "author": "Stanis\u0142aw Pawlak, Jan Dubi\u0144ski, Daniel Marczak, and Bart\u0142omiej Twardowski",
        "link": "http://arxiv.org/abs/2510.08016v1",
        "abstract": "Model merging (MM) recently emerged as an effective method for combining\nlarge deep learning models. However, it poses significant security risks.\nRecent research shows that it is highly susceptible to backdoor attacks, which\nintroduce a hidden trigger into a single fine-tuned model instance that allows\nthe adversary to control the output of the final merged model at inference\ntime. In this work, we propose a simple framework for understanding backdoor\nattacks by treating the attack itself as a task vector. $Backdoor\\ Vector\\\n(BV)$ is calculated as the difference between the weights of a fine-tuned\nbackdoored model and fine-tuned clean model. BVs reveal new insights into\nattacks understanding and a more effective framework to measure their\nsimilarity and transferability. Furthermore, we propose a novel method that\nenhances backdoor resilience through merging dubbed $Sparse\\ Backdoor\\ Vector\\\n(SBV)$ that combines multiple attacks into a single one. We identify the core\nvulnerability behind backdoor threats in MM: $inherent\\ triggers$ that exploit\nadversarial weaknesses in the base model. To counter this, we propose\n$Injection\\ BV\\ Subtraction\\ (IBVS)$ - an assumption-free defense against\nbackdoors in MM. Our results show that SBVs surpass prior attacks and is the\nfirst method to leverage merging to improve backdoor effectiveness. At the same\ntime, IBVS provides a lightweight, general defense that remains effective even\nwhen the backdoor threat is entirely unknown."
    },
    {
        "date": "2025-10",
        "title": "Fewer Weights, More Problems: A Practical Attack on LLM Pruning",
        "author": "Kazuki Egashira, Robin Staab, Thibaud Gloaguen, Mark Vero, and Martin Vechev",
        "link": "http://arxiv.org/abs/2510.07985v2",
        "abstract": "Model pruning, i.e., removing a subset of model weights, has become a\nprominent approach to reducing the memory footprint of large language models\n(LLMs) during inference. Notably, popular inference engines, such as vLLM,\nenable users to conveniently prune downloaded models before they are deployed.\nWhile the utility and efficiency of pruning methods have improved\nsignificantly, the security implications of pruning remain underexplored. In\nthis work, for the first time, we show that modern LLM pruning methods can be\nmaliciously exploited. In particular, an adversary can construct a model that\nappears benign yet, once pruned, exhibits malicious behaviors. Our method is\nbased on the idea that the adversary can compute a proxy metric that estimates\nhow likely each parameter is to be pruned. With this information, the adversary\ncan first inject a malicious behavior into those parameters that are unlikely\nto be pruned. Then, they can repair the model by using parameters that are\nlikely to be pruned, effectively canceling out the injected behavior in the\nunpruned model. We demonstrate the severity of our attack through extensive\nevaluation on five models; after any of the pruning in vLLM are applied\n(Magnitude, Wanda, and SparseGPT), it consistently exhibits strong malicious\nbehaviors in a diverse set of attack scenarios (success rates of up to $95.7\\%$\nfor jailbreak, $98.7\\%$ for benign instruction refusal, and $99.5\\%$ for\ntargeted content injection). Our results reveal a critical deployment-time\nsecurity gap and underscore the urgent need for stronger security awareness in\nmodel compression."
    },
    {
        "date": "2025-10",
        "title": "From Defender to Devil? Unintended Risk Interactions Induced by LLM Defenses",
        "author": "Xiangtao Meng, Tianshuo Cong, Li Wang, Wenyu Chen, Zheng Li, Shanqing Guo, and Xiaoyun Wang",
        "link": "http://arxiv.org/abs/2510.07968v1",
        "abstract": "Large Language Models (LLMs) have shown remarkable performance across various\napplications, but their deployment in sensitive domains raises significant\nconcerns. To mitigate these risks, numerous defense strategies have been\nproposed. However, most existing studies assess these defenses in isolation,\noverlooking their broader impacts across other risk dimensions. In this work,\nwe take the first step in investigating unintended interactions caused by\ndefenses in LLMs, focusing on the complex interplay between safety, fairness,\nand privacy. Specifically, we propose CrossRiskEval, a comprehensive evaluation\nframework to assess whether deploying a defense targeting one risk\ninadvertently affects others. Through extensive empirical studies on 14\ndefense-deployed LLMs, covering 12 distinct defense strategies, we reveal\nseveral alarming side effects: 1) safety defenses may suppress direct responses\nto sensitive queries related to bias or privacy, yet still amplify indirect\nprivacy leakage or biased outputs; 2) fairness defenses increase the risk of\nmisuse and privacy leakage; 3) privacy defenses often impair safety and\nexacerbate bias. We further conduct a fine-grained neuron-level analysis to\nuncover the underlying mechanisms of these phenomena. Our analysis reveals the\nexistence of conflict-entangled neurons in LLMs that exhibit opposing\nsensitivities across multiple risk dimensions. Further trend consistency\nanalysis at both task and neuron levels confirms that these neurons play a key\nrole in mediating the emergence of unintended behaviors following defense\ndeployment. We call for a paradigm shift in LLM risk evaluation, toward\nholistic, interaction-aware assessment of defense strategies."
    },
    {
        "date": "2025-10",
        "title": "A Large-scale Dataset for Robust Complex Anime Scene Text Detection",
        "author": "Ziyi Dong, Yurui Zhang, Changmao Li, Naomi Rue Golding, and Qing Long",
        "link": "http://arxiv.org/abs/2510.07951v1",
        "abstract": "Current text detection datasets primarily target natural or document scenes,\nwhere text typically appear in regular font and shapes, monotonous colors, and\norderly layouts. The text usually arranged along straight or curved lines.\nHowever, these characteristics differ significantly from anime scenes, where\ntext is often diverse in style, irregularly arranged, and easily confused with\ncomplex visual elements such as symbols and decorative patterns. Text in anime\nscene also includes a large number of handwritten and stylized fonts. Motivated\nby this gap, we introduce AnimeText, a large-scale dataset containing 735K\nimages and 4.2M annotated text blocks. It features hierarchical annotations and\nhard negative samples tailored for anime scenarios. %Cross-dataset evaluations\nusing state-of-the-art methods demonstrate that models trained on AnimeText\nachieve superior performance in anime text detection tasks compared to existing\ndatasets. To evaluate the robustness of AnimeText in complex anime scenes, we\nconducted cross-dataset benchmarking using state-of-the-art text detection\nmethods. Experimental results demonstrate that models trained on AnimeText\noutperform those trained on existing datasets in anime scene text detection\ntasks. AnimeText on HuggingFace:\nhttps://huggingface.co/datasets/deepghs/AnimeText"
    },
    {
        "date": "2025-10",
        "title": "SketchGuard: Scaling Byzantine-Robust Decentralized Federated Learning via Sketch-Based Screening",
        "author": "Murtaza Rangwala, Farag Azzedin, Richard O. Sinnott, and Rajkumar Buyya",
        "link": "http://arxiv.org/abs/2510.07922v2",
        "abstract": "Decentralized Federated Learning (DFL) enables privacy-preserving\ncollaborative training without centralized servers, but remains vulnerable to\nByzantine attacks where malicious clients submit corrupted model updates.\nExisting Byzantine-robust DFL defenses rely on similarity-based neighbor\nscreening that requires every client to exchange and compare complete\nhigh-dimensional model vectors with all neighbors in each training round,\ncreating prohibitive communication and computational costs that prevent\ndeployment at web scale. We propose SketchGuard, a general framework that\ndecouples Byzantine filtering from model aggregation through sketch-based\nneighbor screening. SketchGuard compresses $d$-dimensional models to\n$k$-dimensional sketches ($k \\ll d$) using Count Sketch for similarity\ncomparisons, then selectively fetches full models only from accepted neighbors,\nreducing per-round communication complexity from $O(d|N_i|)$ to $O(k|N_i| +\nd|S_i|)$, where $|N_i|$ is the neighbor count and $|S_i| \\le |N_i|$ is the\naccepted neighbor count. We establish rigorous convergence guarantees in both\nstrongly convex and non-convex settings, proving that Count Sketch compression\npreserves Byzantine resilience with controlled degradation bounds where\napproximation errors introduce only a $(1+O(\\epsilon))$ factor in the effective\nthreshold parameter. Comprehensive experiments across multiple datasets,\nnetwork topologies, and attack scenarios demonstrate that SketchGuard maintains\nidentical robustness to state-of-the-art methods while reducing computation\ntime by up to 82% and communication overhead by 50-70% depending on filtering\neffectiveness, with benefits scaling multiplicatively with model dimensionality\nand network connectivity. These results establish the viability of sketch-based\ncompression as a fundamental enabler of robust DFL at web scale."
    },
    {
        "date": "2025-10",
        "title": "On the Optimality of the Median-of-Means Estimator under Adversarial Contamination",
        "author": "Xabier de Juan, and Santiago Mazuelas",
        "link": "http://arxiv.org/abs/2510.07867v1",
        "abstract": "The Median-of-Means (MoM) is a robust estimator widely used in machine\nlearning that is known to be (minimax) optimal in scenarios where samples are\ni.i.d. In more grave scenarios, samples are contaminated by an adversary that\ncan inspect and modify the data. Previous work has theoretically shown the\nsuitability of the MoM estimator in certain contaminated settings. However, the\n(minimax) optimality of MoM and its limitations under adversarial contamination\nremain unknown beyond the Gaussian case. In this paper, we present upper and\nlower bounds for the error of MoM under adversarial contamination for multiple\nclasses of distributions. In particular, we show that MoM is (minimax) optimal\nin the class of distributions with finite variance, as well as in the class of\ndistributions with infinite variance and finite absolute $(1+r)$-th moment. We\nalso provide lower bounds for MoM's error that match the order of the presented\nupper bounds, and show that MoM is sub-optimal for light-tailed distributions."
    },
    {
        "date": "2025-10",
        "title": "AlignGS: Aligning Geometry and Semantics for Robust Indoor Reconstruction from Sparse Views",
        "author": "Yijie Gao, Houqiang Zhong, Tianchi Zhu, Zhengxue Cheng, Qiang Hu, and Li Song",
        "link": "http://arxiv.org/abs/2510.07839v1",
        "abstract": "The demand for semantically rich 3D models of indoor scenes is rapidly\ngrowing, driven by applications in augmented reality, virtual reality, and\nrobotics. However, creating them from sparse views remains a challenge due to\ngeometric ambiguity. Existing methods often treat semantics as a passive\nfeature painted on an already-formed, and potentially flawed, geometry. We\nposit that for robust sparse-view reconstruction, semantic understanding\ninstead be an active, guiding force. This paper introduces AlignGS, a novel\nframework that actualizes this vision by pioneering a synergistic, end-to-end\noptimization of geometry and semantics. Our method distills rich priors from 2D\nfoundation models and uses them to directly regularize the 3D representation\nthrough a set of novel semantic-to-geometry guidance mechanisms, including\ndepth consistency and multi-faceted normal regularization. Extensive\nevaluations on standard benchmarks demonstrate that our approach achieves\nstate-of-the-art results in novel view synthesis and produces reconstructions\nwith superior geometric accuracy. The results validate that leveraging semantic\npriors as a geometric regularizer leads to more coherent and complete 3D models\nfrom limited input views. Our code is avaliable at\nhttps://github.com/MediaX-SJTU/AlignGS ."
    },
    {
        "date": "2025-10",
        "title": "MetaDefense: Defending Finetuning-based Jailbreak Attack Before and During Generation",
        "author": "Weisen Jiang, and Sinno Jialin Pan",
        "link": "http://arxiv.org/abs/2510.07835v1",
        "abstract": "This paper introduces MetaDefense, a novel framework for defending against\nfinetuning-based jailbreak attacks in large language models (LLMs). We observe\nthat existing defense mechanisms fail to generalize to harmful queries\ndisguised by unseen attack templates, despite LLMs being capable of\ndistinguishing disguised harmful queries in the embedding space. Based on these\ninsights, we propose a two-stage defense approach: (i) pre-generation defense\nthat detects harmful queries before response generation begins, and (ii)\nmid-generation defense that monitors partial responses during generation to\nprevent outputting more harmful content. Our MetaDefense trains the LLM to\npredict the harmfulness of both queries and partial responses using specialized\nprompts, enabling early termination of potentially harmful interactions.\nExtensive experiments across multiple LLM architectures (LLaMA-2-7B,\nQwen-2.5-3B-Instruct, and LLaMA-3.2-3B-Instruct) demonstrate that MetaDefense\nsignificantly outperforms existing defense mechanisms, achieving robust defense\nagainst harmful queries with seen and unseen attack templates while maintaining\ncompetitive performance on benign tasks. Code is available at\nhttps://github.com/ws-jiang/MetaDefense."
    },
    {
        "date": "2025-10",
        "title": "Stop DDoS Attacking the Research Community with AI-Generated Survey Papers",
        "author": "Jianghao Lin, Rong Shan, Jiachen Zhu, Yunjia Xi, Yong Yu, and Weinan Zhang",
        "link": "http://arxiv.org/abs/2510.09686v1",
        "abstract": "Survey papers are foundational to the scholarly progress of research\ncommunities, offering structured overviews that guide both novices and experts\nacross disciplines. However, the recent surge of AI-generated surveys,\nespecially enabled by large language models (LLMs), has transformed this\ntraditionally labor-intensive genre into a low-effort, high-volume output.\nWhile such automation lowers entry barriers, it also introduces a critical\nthreat: the phenomenon we term the \"survey paper DDoS attack\" to the research\ncommunity. This refers to the unchecked proliferation of superficially\ncomprehensive but often redundant, low-quality, or even hallucinated survey\nmanuscripts, which floods preprint platforms, overwhelms researchers, and\nerodes trust in the scientific record. In this position paper, we argue that we\nmust stop uploading massive amounts of AI-generated survey papers (i.e., survey\npaper DDoS attack) to the research community, by instituting strong norms for\nAI-assisted review writing. We call for restoring expert oversight and\ntransparency in AI usage and, moreover, developing new infrastructures such as\nDynamic Live Surveys, community-maintained, version-controlled repositories\nthat blend automated updates with human curation. Through quantitative trend\nanalysis, quality audits, and cultural impact discussion, we show that\nsafeguarding the integrity of surveys is no longer optional but imperative to\nthe research community."
    },
    {
        "date": "2025-10",
        "title": "FMANet: A Novel Dual-Phase Optical Flow Approach with Fusion Motion Attention Network for Robust Micro-expression Recognition",
        "author": "Luu Tu Nguyen, Vu Tram Anh Khuong, Thi Bich Phuong Man, Thi Duyen Ngo, and Thanh Ha Le",
        "link": "http://arxiv.org/abs/2510.07810v2",
        "abstract": "Facial micro-expressions, characterized by their subtle and brief nature, are\nvaluable indicators of genuine emotions. Despite their significance in\npsychology, security, and behavioral analysis, micro-expression recognition\nremains challenging due to the difficulty of capturing subtle facial movements.\nOptical flow has been widely employed as an input modality for this task due to\nits effectiveness. However, most existing methods compute optical flow only\nbetween the onset and apex frames, thereby overlooking essential motion\ninformation in the apex-to-offset phase. To address this limitation, we first\nintroduce a comprehensive motion representation, termed Magnitude-Modulated\nCombined Optical Flow (MM-COF), which integrates motion dynamics from both\nmicro-expression phases into a unified descriptor suitable for direct use in\nrecognition networks. Building upon this principle, we then propose FMANet, a\nnovel end-to-end neural network architecture that internalizes the dual-phase\nanalysis and magnitude modulation into learnable modules. This allows the\nnetwork to adaptively fuse motion cues and focus on salient facial regions for\nclassification. Experimental evaluations on the MMEW, SMIC, CASME-II, and SAMM\ndatasets, widely recognized as standard benchmarks, demonstrate that our\nproposed MM-COF representation and FMANet outperforms existing methods,\nunderscoring the potential of a learnable, dual-phase framework in advancing\nmicro-expression recognition."
    },
    {
        "date": "2025-10",
        "title": "When Robustness Meets Conservativeness: Conformalized Uncertainty Calibration for Balanced Decision Making",
        "author": "Wenbin Zhou, and Shixiang Zhu",
        "link": "http://arxiv.org/abs/2510.07750v1",
        "abstract": "Robust optimization safeguards decisions against uncertainty by optimizing\nagainst worst-case scenarios, yet their effectiveness hinges on a prespecified\nrobustness level that is often chosen ad hoc, leading to either insufficient\nprotection or overly conservative and costly solutions. Recent approaches using\nconformal prediction construct data-driven uncertainty sets with finite-sample\ncoverage guarantees, but they still fix coverage targets a priori and offer\nlittle guidance for selecting robustness levels. We propose a new framework\nthat provides distribution-free, finite-sample guarantees on both miscoverage\nand regret for any family of robust predict-then-optimize policies. Our method\nconstructs valid estimators that trace out the miscoverage-regret Pareto\nfrontier, enabling decision-makers to reliably evaluate and calibrate\nrobustness levels according to their cost-risk preferences. The framework is\nsimple to implement, broadly applicable across classical optimization\nformulations, and achieves sharper finite-sample performance than existing\napproaches. These results offer the first principled data-driven methodology\nfor guiding robustness selection and empower practitioners to balance\nrobustness and conservativeness in high-stakes decision-making."
    },
    {
        "date": "2025-10",
        "title": "oMeBench: Towards Robust Benchmarking of LLMs in Organic Mechanism Elucidation and Reasoning",
        "author": "Ruiling Xu, Yifan Zhang, Qingyun Wang, Carl Edwards, and Heng Ji",
        "link": "http://arxiv.org/abs/2510.07731v2",
        "abstract": "Organic reaction mechanisms are the stepwise elementary reactions by which\nreactants form intermediates and products, and are fundamental to understanding\nchemical reactivity and designing new molecules and reactions. Although large\nlanguage models (LLMs) have shown promise in understanding chemical tasks such\nas synthesis design, it is unclear to what extent this reflects genuine\nchemical reasoning capabilities, i.e., the ability to generate valid\nintermediates, maintain chemical consistency, and follow logically coherent\nmulti-step pathways. We address this by introducing oMeBench, the first\nlarge-scale, expert-curated benchmark for organic mechanism reasoning in\norganic chemistry. It comprises over 10,000 annotated mechanistic steps with\nintermediates, type labels, and difficulty ratings. Furthermore, to evaluate\nLLM capability more precisely and enable fine-grained scoring, we propose oMeS,\na dynamic evaluation framework that combines step-level logic and chemical\nsimilarity. We analyze the performance of state-of-the-art LLMs, and our\nresults show that although current models display promising chemical intuition,\nthey struggle with correct and consistent multi-step reasoning. Notably, we\nfind that using prompting strategy and fine-tuning a specialist model on our\nproposed dataset increases performance by 50% over the leading closed-source\nmodel. We hope that oMeBench will serve as a rigorous foundation for advancing\nAI systems toward genuine chemical reasoning."
    },
    {
        "date": "2025-10",
        "title": "DGTEN: A Robust Deep Gaussian based Graph Neural Network for Dynamic Trust Evaluation with Uncertainty-Quantification Support",
        "author": "Muhammad Usman, and Yugyung Lee",
        "link": "http://arxiv.org/abs/2510.07620v1",
        "abstract": "Dynamic trust evaluation in large, rapidly evolving graphs requires models\nthat can capture changing relationships, express calibrated confidence, and\nresist adversarial manipulation. DGTEN (Deep Gaussian-based Trust Evaluation\nNetwork) introduces a unified graph framework that achieves all three by\ncombining uncertainty-aware message passing, expressive temporal modeling, and\nbuilt-in defenses against trust-targeted attacks. It represents nodes and edges\nas Gaussian distributions so that both semantic signals and epistemic\nuncertainty propagate through the graph neural network, enabling risk-aware\ntrust decisions rather than overconfident guesses. To model how trust evolves,\nit employs hybrid Absolute-Gaussian-Hourglass (HAGH) positional encoding with\nKolmogorov-Arnold network-based unbiased multi-head attention, followed by an\nordinary differential equation (ODE)-based residual learning module to jointly\ncapture abrupt shifts and smooth trends. Robust adaptive ensemble coefficient\nanalysis prunes or down-weights suspicious interactions using complementary\ncosine and Jaccard similarity measures, mitigating reputation laundering,\nsabotage, and on/off attacks. On two signed Bitcoin trust networks, DGTEN\ndelivers significant improvements: in single-timeslot prediction on\nBitcoin-Alpha, it improves MCC by 10.77% over the best dynamic baseline; in the\ncold-start scenario, it achieves a 16.41% MCC gain - the largest across all\ntasks and datasets. Under adversarial on/off attacks, it surpasses the baseline\nby up to 11.63% MCC. These results validate the effectiveness of the unified\nDGTEN framework."
    },
    {
        "date": "2025-10",
        "title": "Fortifying LLM-Based Code Generation with Graph-Based Reasoning on Secure Coding Practices",
        "author": "Rupam Patir, Keyan Guo, Haipeng Cai, and Hongxin Hu",
        "link": "http://arxiv.org/abs/2510.09682v1",
        "abstract": "The code generation capabilities of Large Language Models (LLMs) have\ntransformed the field of software development. However, this advancement also\npresents significant security challenges, as LLM-generated code often contains\nvulnerabilities. One direction of research strengthens LLMs by injecting or\nrefining security knowledge through curated datasets, model tuning, or static\nanalyzers. While effective in certain settings, these methods can be\nresource-intensive, less adaptable to zero-day vulnerabilities, and often\ninapplicable to proprietary models. To address these challenges, we introduce\nGRASP, which explores a new direction that focuses on structured reasoning over\nSecure Coding Practices(SCPs) rather than additional training or external\nfeedback. GRASP comprises two key ideas: (1) an SCP graph that organizes SCPs\ninto a Directed Acyclic Graph (DAG) capturing dependencies and relationships,\nand (2) a graph-based reasoning process that systematically guides LLMs through\nrelevant SCPs for code generation. This design enables interpretable,\nmodel-agnostic, and scalable security improvements, particularly for previously\nunseen vulnerabilities. Our evaluation shows that GRASP consistently achieves\nSecurity Rates (SR) exceeding 80% across multiple LLMs, and delivers up to 88%\nimprovements over baselines on zero-day vulnerabilities."
    },
    {
        "date": "2025-10",
        "title": "EBGAN-MDN: An Energy-Based Adversarial Framework for Multi-Modal Behavior Cloning",
        "author": "Yixiao Li, Julia Barth, Thomas Kiefer, and Ahmad Fraij",
        "link": "http://arxiv.org/abs/2510.07562v1",
        "abstract": "Multi-modal behavior cloning faces significant challenges due to mode\naveraging and mode collapse, where traditional models fail to capture diverse\ninput-output mappings. This problem is critical in applications like robotics,\nwhere modeling multiple valid actions ensures both performance and safety. We\npropose EBGAN-MDN, a framework that integrates energy-based models, Mixture\nDensity Networks (MDNs), and adversarial training. By leveraging a modified\nInfoNCE loss and an energy-enforced MDN loss, EBGAN-MDN effectively addresses\nthese challenges. Experiments on synthetic and robotic benchmarks demonstrate\nsuperior performance, establishing EBGAN-MDN as a effective and efficient\nsolution for multi-modal learning tasks."
    },
    {
        "date": "2025-10",
        "title": "Label Semantics for Robust Hyperspectral Image Classification",
        "author": "Rafin Hassan, Zarin Tasnim Roshni, Rafiqul Bari, Alimul Islam, Nabeel Mohammed, Moshiur Farazi, and Shafin Rahman",
        "link": "http://arxiv.org/abs/2510.07556v1",
        "abstract": "Hyperspectral imaging (HSI) classification is a critical tool with widespread\napplications across diverse fields such as agriculture, environmental\nmonitoring, medicine, and materials science. Due to the limited availability of\nhigh-quality training samples and the high dimensionality of spectral data, HSI\nclassification models are prone to overfitting and often face challenges in\nbalancing accuracy and computational complexity. Furthermore, most of HSI\nclassification models are monomodal, where it solely relies on spectral-spatial\ndata to learn decision boundaries in the high dimensional embedding space. To\naddress this, we propose a general-purpose Semantic Spectral-Spatial Fusion\nNetwork (S3FN) that uses contextual, class specific textual descriptions to\ncomplement the training of an HSI classification model. Specifically, S3FN\nleverages LLMs to generate comprehensive textual descriptions for each class\nlabel that captures their unique characteristics and spectral behaviors. These\ndescriptions are then embedded into a vector space using a pre-trained text\nencoder such as BERT or RoBERTa to extract meaningful label semantics which in\nturn leads to a better feature-label alignment for improved classification\nperformance. To demonstrate the effectiveness of our approach, we evaluate our\nmodel on three diverse HSI benchmark datasets - Hyperspectral Wood,\nHyperspectralBlueberries, and DeepHS-Fruit and report significant performance\nboost. Our results highlight the synergy between textual semantics and\nspectral-spatial data, paving the way for further advancements in semantically\naugmented HSI classification models. Codes are be available in:\nhttps://github.com/milab-nsu/S3FN"
    },
    {
        "date": "2025-10",
        "title": "D2RA: Dual Domain Regeneration Attack",
        "author": "Pragati Shuddhodhan Meshram, and Varun Chandrasekaran",
        "link": "http://arxiv.org/abs/2510.07538v1",
        "abstract": "The growing use of generative models has intensified the need for\nwatermarking methods that ensure content attribution and provenance. While\nrecent semantic watermarking schemes improve robustness by embedding signals in\nlatent or frequency representations, we show they remain vulnerable even under\nresource-constrained adversarial settings. We present D2RA, a training-free,\nsingle-image attack that removes or weakens watermarks without access to the\nunderlying model. By projecting watermarked images onto natural priors across\ncomplementary representations, D2RA suppresses watermark signals while\npreserving visual fidelity. Experiments across diverse watermarking schemes\ndemonstrate that our approach consistently reduces watermark detectability,\nrevealing fundamental weaknesses in current designs. Our code is available at\nhttps://github.com/Pragati-Meshram/DAWN."
    },
    {
        "date": "2025-10",
        "title": "PEAR: Planner-Executor Agent Robustness Benchmark",
        "author": "Shen Dong, Mingxuan Zhang, Pengfei He, Li Ma, Bhavani Thuraisingham, Hui Liu, and Yue Xing",
        "link": "http://arxiv.org/abs/2510.07505v2",
        "abstract": "Large Language Model (LLM)-based Multi-Agent Systems (MAS) have emerged as a\npowerful paradigm for tackling complex, multi-step tasks across diverse\ndomains. However, despite their impressive capabilities, MAS remain susceptible\nto adversarial manipulation. Existing studies typically examine isolated attack\nsurfaces or specific scenarios, leaving a lack of holistic understanding of MAS\nvulnerabilities. To bridge this gap, we introduce PEAR, a benchmark for\nsystematically evaluating both the utility and vulnerability of\nplanner-executor MAS. While compatible with various MAS architectures, our\nbenchmark focuses on the planner-executor structure, which is a practical and\nwidely adopted design. Through extensive experiments, we find that (1) a weak\nplanner degrades overall clean task performance more severely than a weak\nexecutor; (2) while a memory module is essential for the planner, having a\nmemory module for the executor does not impact the clean task performance; (3)\nthere exists a trade-off between task performance and robustness; and (4)\nattacks targeting the planner are particularly effective at misleading the\nsystem. These findings offer actionable insights for enhancing the robustness\nof MAS and lay the groundwork for principled defenses in multi-agent settings."
    },
    {
        "date": "2025-10",
        "title": "A Secure Authentication-Driven Protected Data Collection Protocol in Internet of Things",
        "author": "Maryam Ataei Nezhad, Hamid Barati, and Ali Barati",
        "link": "http://arxiv.org/abs/2510.07462v1",
        "abstract": "Internet of Things means connecting different devices through the Internet.\nThe Internet of things enables humans to remotely manage and control the\nobjects they use with the Internet infrastructure. After the advent of the\nInternet of Things in homes, organizations, and private companies, privacy and\ninformation security are the biggest concern. This issue has challenged the\nspread of the Internet of things as news of the users theft of information by\nhackers intensified. The proposed method in this paper consists of three\nphases. In the first phase, a star structure is constructed within each\ncluster, and a unique key is shared between each child and parent to encrypt\nand secure subsequent communications. The second phase is for intracluster\ncommunications, in which members of the cluster send their data to the cluster\nhead in a multi hop manner. Also, in this phase, the data is encrypted with\ndifferent keys in each hop, and at the end of each connection, the keys are\nupdated to ensure data security. The third phase is to improve the security of\ninter cluster communications using an authentication protocol. In this way, the\ncluster heads are authenticated before sending information to prevent malicious\nnodes in the network. The proposed method is also simulated using NS2 software.\nThe results showed that the proposed method has improved in terms of energy\nconsumption, end-to-end delay, flexibility, packet delivery rate, and the\nnumber of alive nodes compared to other methods."
    },
    {
        "date": "2025-10",
        "title": "L2M-AID: Autonomous Cyber-Physical Defense by Fusing Semantic Reasoning of Large Language Models with Multi-Agent Reinforcement Learning (Preprint)",
        "author": "Tianxiang Xu, Zhichao Wen, Xinyu Zhao, Jun Wang, Yan Li, and Chang Liu",
        "link": "http://arxiv.org/abs/2510.07363v2",
        "abstract": "The increasing integration of Industrial IoT (IIoT) exposes critical\ncyber-physical systems to sophisticated, multi-stage attacks that elude\ntraditional defenses lacking contextual awareness. This paper introduces\nL2M-AID, a novel framework for Autonomous Industrial Defense using\nLLM-empowered, Multi-agent reinforcement learning. L2M-AID orchestrates a team\nof collaborative agents, each driven by a Large Language Model (LLM), to\nachieve adaptive and resilient security. The core innovation lies in the deep\nfusion of two AI paradigms: we leverage an LLM as a semantic bridge to\ntranslate vast, unstructured telemetry into a rich, contextual state\nrepresentation, enabling agents to reason about adversary intent rather than\nmerely matching patterns. This semantically-aware state empowers a Multi-Agent\nReinforcement Learning (MARL) algorithm, MAPPO, to learn complex cooperative\nstrategies. The MARL reward function is uniquely engineered to balance security\nobjectives (threat neutralization) with operational imperatives, explicitly\npenalizing actions that disrupt physical process stability. To validate our\napproach, we conduct extensive experiments on the benchmark SWaT dataset and a\nnovel synthetic dataset generated based on the MITRE ATT&CK for ICS framework.\nResults demonstrate that L2M-AID significantly outperforms traditional IDS,\ndeep learning anomaly detectors, and single-agent RL baselines across key\nmetrics, achieving a 97.2% detection rate while reducing false positives by\nover 80% and improving response times by a factor of four. Crucially, it\ndemonstrates superior performance in maintaining physical process stability,\npresenting a robust new paradigm for securing critical national infrastructure."
    },
    {
        "date": "2025-10",
        "title": "Advancing Security in Software-Defined Vehicles: A Comprehensive Survey and Taxonomy",
        "author": "Khaoula Sghaier, Badis Hammi, Ghada Gharbi, Pierre Merdrignac, Pierre Parrend, and Didier Verna",
        "link": "http://arxiv.org/abs/2510.09675v1",
        "abstract": "Software-Defined Vehicles (SDVs) introduce innovative features that extend\nthe vehicle's lifecycle through the integration of outsourced applications and\ncontinuous Over-The-Air (OTA) updates. This shift necessitates robust\ncybersecurity and system resilience. While research on Connected and Autonomous\nVehicles (CAV) has been extensive, there is a lack of clarity in distinguishing\nSDVs from non-SDVs and a need to consolidate cybersecurity research. SDVs, with\ntheir extensive connectivity, have a broader attack surface. Besides, their\nsoftware-centric nature introduces additional vulnerabilities. This paper\nprovides a comprehensive examination of SDVs, detailing their ecosystem,\nenabling technologies, and the principal cyberattack entry points that arise\nfrom their architectural and operational characteristics. We also introduce a\nnovel, layered taxonomy that maps concrete exploit techniques onto core SDV\nproperties and attack paths, and use it to analyze representative studies and\nexperimental approaches."
    },
    {
        "date": "2025-10",
        "title": "Security-Robustness Trade-offs in Diffusion Steganography: A Comparative Analysis of Pixel-Space and VAE-Based Architectures",
        "author": "Yuhua Xu, Wei Sun, Chengpei Tang, Jiaxing Lu, Jingying Zhou, and Chen Gu",
        "link": "http://arxiv.org/abs/2510.07219v1",
        "abstract": "Current generative steganography research mainly pursues computationally\nexpensive mappings to perfect Gaussian priors within single diffusion model\narchitectures. This work introduces an efficient framework based on approximate\nGaussian mapping governed by a scale factor calibrated through capacity-aware\nadaptive optimization. Using this framework as a unified analytical tool,\nsystematic comparative analysis of steganography in pixel-space models versus\nVAE-based latent-space systems is conducted. The investigation reveals a\npronounced architecture dependent security-robustness trade-off: pixel-space\nmodels achieve high security against steganalysis but exhibit fragility to\nchannel distortions, while VAE-based systems like Stable Diffusion offer\nsubstantial robustness at the cost of security vulnerabilities. Further\nanalysis indicates that the VAE component drives this behavior through opposing\nmechanisms where the encoder confers robustness via manifold regularization\nwhile the decoder introduces vulnerabilities by amplifying latent perturbations\ninto detectable artifacts. These findings characterize the conflicting\narchitectural roles in generative steganography and establish a foundation for\nfuture research."
    },
    {
        "date": "2025-10",
        "title": "Poisoning Attacks on LLMs Require a Near-constant Number of Poison Samples",
        "author": "Alexandra Souly, Javier Rando, Ed Chapman, Xander Davies, Burak Hasircioglu, Ezzeldin Shereen, Carlos Mougan, Vasilios Mavroudis, Erik Jones, Chris Hicks, Nicholas Carlini, Yarin Gal, and Robert Kirk",
        "link": "http://arxiv.org/abs/2510.07192v1",
        "abstract": "Poisoning attacks can compromise the safety of large language models (LLMs)\nby injecting malicious documents into their training data. Existing work has\nstudied pretraining poisoning assuming adversaries control a percentage of the\ntraining corpus. However, for large models, even small percentages translate to\nimpractically large amounts of data. This work demonstrates for the first time\nthat poisoning attacks instead require a near-constant number of documents\nregardless of dataset size. We conduct the largest pretraining poisoning\nexperiments to date, pretraining models from 600M to 13B parameters on\nchinchilla-optimal datasets (6B to 260B tokens). We find that 250 poisoned\ndocuments similarly compromise models across all model and dataset sizes,\ndespite the largest models training on more than 20 times more clean data. We\nalso run smaller-scale experiments to ablate factors that could influence\nattack success, including broader ratios of poisoned to clean data and\nnon-random distributions of poisoned samples. Finally, we demonstrate the same\ndynamics for poisoning during fine-tuning. Altogether, our results suggest that\ninjecting backdoors through data poisoning may be easier for large models than\npreviously believed as the number of poisons required does not scale up with\nmodel size, highlighting the need for more research on defences to mitigate\nthis risk in future models."
    },
    {
        "date": "2025-10",
        "title": "Diffusion-Augmented Reinforcement Learning for Robust Portfolio Optimization under Stress Scenarios",
        "author": "Himanshu Choudhary, Arishi Orra, and Manoj Thakur",
        "link": "http://arxiv.org/abs/2510.07099v1",
        "abstract": "In the ever-changing and intricate landscape of financial markets, portfolio\noptimisation remains a formidable challenge for investors and asset managers.\nConventional methods often struggle to capture the complex dynamics of market\nbehaviour and align with diverse investor preferences. To address this, we\npropose an innovative framework, termed Diffusion-Augmented Reinforcement\nLearning (DARL), which synergistically integrates Denoising Diffusion\nProbabilistic Models (DDPMs) with Deep Reinforcement Learning (DRL) for\nportfolio management. By leveraging DDPMs to generate synthetic market crash\nscenarios conditioned on varying stress intensities, our approach significantly\nenhances the robustness of training data. Empirical evaluations demonstrate\nthat DARL outperforms traditional baselines, delivering superior risk-adjusted\nreturns and resilience against unforeseen crises, such as the 2025 Tariff\nCrisis. This work offers a robust and practical methodology to bolster stress\nresilience in DRL-driven financial applications."
    },
    {
        "date": "2025-10",
        "title": "RedTWIZ: Diverse LLM Red Teaming via Adaptive Attack Planning",
        "author": "Artur Horal, Daniel Pina, Henrique Paz, Iago Paulo, Jo\u00e3o Soares, Rafael Ferreira, Diogo Tavares, Diogo Gl\u00f3ria-Silva, Jo\u00e3o Magalh\u00e3es, and David Semedo",
        "link": "http://arxiv.org/abs/2510.06994v1",
        "abstract": "This paper presents the vision, scientific contributions, and technical\ndetails of RedTWIZ: an adaptive and diverse multi-turn red teaming framework,\nto audit the robustness of Large Language Models (LLMs) in AI-assisted software\ndevelopment. Our work is driven by three major research streams: (1) robust and\nsystematic assessment of LLM conversational jailbreaks; (2) a diverse\ngenerative multi-turn attack suite, supporting compositional, realistic and\ngoal-oriented jailbreak conversational strategies; and (3) a hierarchical\nattack planner, which adaptively plans, serializes, and triggers attacks\ntailored to specific LLM's vulnerabilities. Together, these contributions form\na unified framework -- combining assessment, attack generation, and strategic\nplanning -- to comprehensively evaluate and expose weaknesses in LLMs'\nrobustness. Extensive evaluation is conducted to systematically assess and\nanalyze the performance of the overall system and each component. Experimental\nresults demonstrate that our multi-turn adversarial attack strategies can\nsuccessfully lead state-of-the-art LLMs to produce unsafe generations,\nhighlighting the pressing need for more research into enhancing LLM's\nrobustness."
    },
    {
        "date": "2025-10",
        "title": "Revisiting Mixout: An Overlooked Path to Robust Finetuning",
        "author": "Masih Aminbeidokhti, Heitor Rapela Medeiros, Eric Granger, and Marco Pedersoli",
        "link": "http://arxiv.org/abs/2510.06982v1",
        "abstract": "Finetuning vision foundation models often improves in-domain accuracy but\ncomes at the cost of robustness under distribution shift. We revisit Mixout, a\nstochastic regularizer that intermittently replaces finetuned weights with\ntheir pretrained reference, through the lens of a single-run, weight-sharing\nimplicit ensemble. This perspective reveals three key levers that govern\nrobustness: the \\emph{masking anchor}, \\emph{resampling frequency}, and\n\\emph{mask sparsity}. Guided by this analysis, we introduce GMixout, which (i)\nreplaces the fixed anchor with an exponential moving-average snapshot that\nadapts during training, and (ii) regulates masking period via an explicit\nresampling-frequency hyperparameter. Our sparse-kernel implementation updates\nonly a small fraction of parameters with no inference-time overhead, enabling\ntraining on consumer-grade GPUs. Experiments on benchmarks covering covariate\nshift, corruption, and class imbalance, ImageNet / ImageNet-LT, DomainNet,\niWildCam, and CIFAR100-C, GMixout consistently improves in-domain accuracy\nbeyond zero-shot performance while surpassing both Model Soups and strong\nparameter-efficient finetuning baselines under distribution shift."
    },
    {
        "date": "2025-10",
        "title": "High-Rate Mixout: Revisiting Mixout for Robust Domain Generalization",
        "author": "Masih Aminbeidokhti, Heitor Rapela Medeiros, Srikanth Muralidharan, Eric Granger, and Marco Pedersoli",
        "link": "http://arxiv.org/abs/2510.06955v2",
        "abstract": "Ensembling fine-tuned models initialized from powerful pre-trained weights is\na common strategy to improve robustness under distribution shifts, but it comes\nwith substantial computational costs due to the need to train and store\nmultiple models. Dropout offers a lightweight alternative by simulating\nensembles through random neuron deactivation; however, when applied to\npre-trained models, it tends to over-regularize and disrupt critical\nrepresentations necessary for generalization. In this work, we investigate\nMixout, a stochastic regularization technique that provides an alternative to\nDropout for domain generalization. Rather than deactivating neurons, Mixout\nmitigates overfitting by probabilistically swapping a subset of fine-tuned\nweights with their pre-trained counterparts during training, thereby\nmaintaining a balance between adaptation and retention of prior knowledge. Our\nstudy reveals that achieving strong performance with Mixout on domain\ngeneralization benchmarks requires a notably high masking probability of 0.9\nfor ViTs and 0.8 for ResNets. While this may seem like a simple adjustment, it\nyields two key advantages for domain generalization: (1) higher masking rates\nmore strongly penalize deviations from the pre-trained parameters, promoting\nbetter generalization to unseen domains; and (2) high-rate masking\nsubstantially reduces computational overhead, cutting gradient computation by\nup to 45% and gradient memory usage by up to 90%. Experiments across five\ndomain generalization benchmarks, PACS, VLCS, OfficeHome, TerraIncognita, and\nDomainNet, using ResNet and ViT architectures, show that our approach,\nHigh-rate Mixout, achieves out-of-domain accuracy comparable to ensemble-based\nmethods while significantly reducing training costs."
    },
    {
        "date": "2025-10",
        "title": "DecompGAIL: Learning Realistic Traffic Behaviors with Decomposed Multi-Agent Generative Adversarial Imitation Learning",
        "author": "Ke Guo, Haochen Liu, Xiaojun Wu, and Chen Lv",
        "link": "http://arxiv.org/abs/2510.06913v1",
        "abstract": "Realistic traffic simulation is critical for the development of autonomous\ndriving systems and urban mobility planning, yet existing imitation learning\napproaches often fail to model realistic traffic behaviors. Behavior cloning\nsuffers from covariate shift, while Generative Adversarial Imitation Learning\n(GAIL) is notoriously unstable in multi-agent settings. We identify a key\nsource of this instability: irrelevant interaction misguidance, where a\ndiscriminator penalizes an ego vehicle's realistic behavior due to unrealistic\ninteractions among its neighbors. To address this, we propose Decomposed\nMulti-agent GAIL (DecompGAIL), which explicitly decomposes realism into ego-map\nand ego-neighbor components, filtering out misleading neighbor: neighbor and\nneighbor: map interactions. We further introduce a social PPO objective that\naugments ego rewards with distance-weighted neighborhood rewards, encouraging\noverall realism across agents. Integrated into a lightweight SMART-based\nbackbone, DecompGAIL achieves state-of-the-art performance on the WOMD Sim\nAgents 2025 benchmark."
    },
    {
        "date": "2025-10",
        "title": "TGPR: Tree-Guided Policy Refinement for Robust Self-Debugging of LLMs",
        "author": "Daria Ozerova, and Ekaterina Trofimova",
        "link": "http://arxiv.org/abs/2510.06878v1",
        "abstract": "Iterative refinement has been a promising paradigm to enable large language\nmodels (LLMs) to resolve difficult reasoning and problem-solving tasks. One of\nthe key challenges, however, is how to effectively search through the enormous\nsearch space of possible refinements. Existing methods typically fall back on\npredefined heuristics, which are troubled by the exploration-exploitation\ndilemma and cannot adapt based on past refinement outcomes. We introduce\nTree-Guided Policy Refinement (TGPR), a novel framework that combines GRPO with\na Thompson-Sampling-based tree search. TGPR explores both failed and successful\nrefinement paths actively, with denser training trajectories and more adaptive\npolicies. On HumanEval, MBPP, and APPS benchmarks, our method achieves up to\n+4.2 percentage points absolute improvement in pass@1 (on MBPP) and up to\n+12.51 percentage points absolute improvement in pass@10 (on APPS) compared to\na competitive GRPO baseline. Apart from debugging code, TGPR focuses on a\nprincipled approach to combining learned policies with structured search\nmethods, offering a general framework for enhancing iterative refinement and\nstateful reasoning in LLMs."
    },
    {
        "date": "2025-10",
        "title": "Get RICH or Die Scaling: Profitably Trading Inference Compute for Robustness",
        "author": "Tavish McDonald, Bo Lei, Stanislav Fort, Bhavya Kailkhura, and Brian Bartoldson",
        "link": "http://arxiv.org/abs/2510.06790v1",
        "abstract": "Models are susceptible to adversarially out-of-distribution (OOD) data\ndespite large training-compute investments into their robustification. Zaremba\net al. (2025) make progress on this problem at test time, showing LLM reasoning\nimproves satisfaction of model specifications designed to thwart attacks,\nresulting in a correlation between reasoning effort and robustness to\njailbreaks. However, this benefit of test compute fades when attackers are\ngiven access to gradients or multimodal inputs. We address this gap, clarifying\nthat inference-compute offers benefits even in such cases. Our approach argues\nthat compositional generalization, through which OOD data is understandable via\nits in-distribution (ID) components, enables adherence to defensive\nspecifications on adversarially OOD inputs. Namely, we posit the Robustness\nfrom Inference Compute Hypothesis (RICH): inference-compute defenses profit as\nthe model's training data better reflects the attacked data's components. We\nempirically support this hypothesis across vision language model and attack\ntypes, finding robustness gains from test-time compute if specification\nfollowing on OOD data is unlocked by compositional generalization, while RL\nfinetuning and protracted reasoning are not critical. For example, increasing\nemphasis on defensive specifications via prompting lowers the success rate of\ngradient-based multimodal attacks on VLMs robustified by adversarial\npretraining, but this same intervention provides no such benefit to\nnot-robustified models. This correlation of inference-compute's robustness\nbenefit with base model robustness is the rich-get-richer dynamic of the RICH:\nattacked data components are more ID for robustified models, aiding\ncompositional generalization to OOD data. Accordingly, we advise layering\ntrain-time and test-time defenses to obtain their synergistic benefit."
    },
    {
        "date": "2025-10",
        "title": "Foundations of LLM Knowledge Materialization: Termination, Reproducibility, Robustness",
        "author": "Luca Giordano, and Simon Razniewski",
        "link": "http://arxiv.org/abs/2510.06780v2",
        "abstract": "Large Language Models (LLMs) encode substantial factual knowledge, yet\nmeasuring and systematizing this knowledge remains challenging. Converting it\ninto structured format, for example through recursive extraction approaches\nsuch as the GPTKB methodology (Hu et al., 2025b), is still underexplored. Key\nopen questions include whether such extraction can terminate, whether its\noutputs are reproducible, and how robust they are to variations. We\nsystematically study LLM knowledge materialization using miniGPTKBs\n(domain-specific, tractable subcrawls), analyzing termination, reproducibility,\nand robustness across three categories of metrics: yield, lexical similarity,\nand semantic similarity. We experiment with four variations (seed, language,\nrandomness, model) and three illustrative domains (from history, entertainment,\nand finance). Our findings show (i) high termination rates, though\nmodel-dependent; (ii) mixed reproducibility; and (iii) robustness that varies\nby perturbation type: high for seeds and temperature, lower for languages and\nmodels. These results suggest that LLM knowledge materialization can reliably\nsurface core knowledge, while also revealing important limitations."
    },
    {
        "date": "2025-10",
        "title": "Improving Artifact Robustness for CT Deep Learning Models Without Labeled Artifact Images via Domain Adaptation",
        "author": "Justin Cheung, Samuel Savine, Calvin Nguyen, Lin Lu, and Alhassan S. Yasin",
        "link": "http://arxiv.org/abs/2510.06584v1",
        "abstract": "Deep learning models which perform well on images from their training\ndistribution can degrade substantially when applied to new distributions. If a\nCT scanner introduces a new artifact not present in the training labels, the\nmodel may misclassify the images. Although modern CT scanners include design\nfeatures which mitigate these artifacts, unanticipated or difficult-to-mitigate\nartifacts can still appear in practice. The direct solution of labeling images\nfrom this new distribution can be costly. As a more accessible alternative,\nthis study evaluates domain adaptation as an approach for training models that\nmaintain classification performance despite new artifacts, even without\ncorresponding labels. We simulate ring artifacts from detector gain error in\nsinogram space and evaluate domain adversarial neural networks (DANN) against\nbaseline and augmentation-based approaches on the OrganAMNIST abdominal CT\ndataset. Our results demonstrate that baseline models trained only on clean\nimages fail to generalize to images with ring artifacts, and traditional\naugmentation with other distortion types provides no improvement on unseen\nartifact domains. In contrast, the DANN approach successfully maintains high\nclassification accuracy on ring artifact images using only unlabeled artifact\ndata during training, demonstrating the viability of domain adaptation for\nartifact robustness. The domain-adapted model achieved classification\nperformance on ring artifact test data comparable to models explicitly trained\nwith labeled artifact images, while also showing unexpected generalization to\nuniform noise. These findings provide empirical evidence that domain adaptation\ncan effectively address distribution shift in medical imaging without requiring\nexpensive expert labeling of new artifact distributions, suggesting promise for\ndeployment in clinical settings where novel artifacts may emerge."
    },
    {
        "date": "2025-10",
        "title": "SpyChain: Multi-Vector Supply Chain Attacks on Small Satellite Systems",
        "author": "Jack Vanlyssel, Enrique Sobrados, Ramsha Anwar, Gruia-Catalin Roman, and Afsah Anwar",
        "link": "http://arxiv.org/abs/2510.06535v1",
        "abstract": "Small satellites are integral to scientific, commercial, and defense\nmissions, but reliance on commercial off-the-shelf (COTS) hardware broadens\ntheir attack surface. Although supply chain threats are well studied in other\ncyber-physical domains, their feasibility and stealth in space systems remain\nlargely unexplored. Prior work has focused on flight software, which benefits\nfrom strict security practices and oversight. In contrast, auxiliary COTS\ncomponents often lack robust assurance yet enjoy comparable access to critical\non-board resources, including telemetry, system calls, and the software bus.\nDespite this privileged access, the insider threat within COTS hardware supply\nchains has received little attention. In this work, we present SpyChain, the\nfirst end-to-end design and implementation of independent and colluding\nhardware supply chain threats targeting small satellites. Using NASA's\nsatellite simulation (NOS3), we demonstrate that SpyChain can evade testing,\nexfiltrate telemetry, disrupt operations, and launch Denial of Service (DoS)\nattacks through covert channels that bypass ground monitoring. Our study traces\nan escalation from a simple solo component to dynamic, coordinating malware,\nintroducing a taxonomy of stealth across five scenarios. We showcase how\nimplicit trust in auxiliary components enables covert persistence and reveal\nnovel attack vectors, highlighting a new multi-component execution technique\nthat is now incorporated into the SPARTA matrix. Our findings are reinforced by\nacknowledgment and affirmation from NASA's NOS3 team. Finally, we implement\nlightweight onboard defenses, including runtime monitoring, to mitigate threats\nlike SpyChain."
    },
    {
        "date": "2025-10",
        "title": "Text-to-Image Models Leave Identifiable Signatures: Implications for Leaderboard Security",
        "author": "Ali Naseh, Anshuman Suri, Yuefeng Peng, Harsh Chaudhari, Alina Oprea, and Amir Houmansadr",
        "link": "http://arxiv.org/abs/2510.06525v1",
        "abstract": "Generative AI leaderboards are central to evaluating model capabilities, but\nremain vulnerable to manipulation. Among key adversarial objectives is rank\nmanipulation, where an attacker must first deanonymize the models behind\ndisplayed outputs -- a threat previously demonstrated and explored for large\nlanguage models (LLMs). We show that this problem can be even more severe for\ntext-to-image leaderboards, where deanonymization is markedly easier. Using\nover 150,000 generated images from 280 prompts and 19 diverse models spanning\nmultiple organizations, architectures, and sizes, we demonstrate that simple\nreal-time classification in CLIP embedding space identifies the generating\nmodel with high accuracy, even without prompt control or historical data. We\nfurther introduce a prompt-level separability metric and identify prompts that\nenable near-perfect deanonymization. Our results indicate that rank\nmanipulation in text-to-image leaderboards is easier than previously\nrecognized, underscoring the need for stronger defenses."
    },
    {
        "date": "2025-10",
        "title": "A Survey on Agentic Security: Applications, Threats and Defenses",
        "author": "Asif Shahriar, Md Nafiu Rahman, Sadif Ahmed, Farig Sadeque, and Md Rizwan Parvez",
        "link": "http://arxiv.org/abs/2510.06445v1",
        "abstract": "The rapid shift from passive LLMs to autonomous LLM-agents marks a new\nparadigm in cybersecurity. While these agents can act as powerful tools for\nboth offensive and defensive operations, the very agentic context introduces a\nnew class of inherent security risks. In this work we present the first\nholistic survey of the agentic security landscape, structuring the field around\nthree interdependent pillars: Applications, Threats, and Defenses. We provide a\ncomprehensive taxonomy of over 150 papers, explaining how agents are used, the\nvulnerabilities they possess, and the countermeasures designed to protect them.\nA detailed cross-cutting analysis shows emerging trends in agent architecture\nwhile revealing critical research gaps in model and modality coverage."
    },
    {
        "date": "2025-10",
        "title": "Automated Repeatable Adversary Threat Emulation with Effects Language (EL)",
        "author": "Suresh K. Damodaran, and Paul D. Rowe",
        "link": "http://arxiv.org/abs/2510.06420v1",
        "abstract": "The emulation of multi-step attacks attributed to advanced persistent threats\nis valuable for training defenders and evaluating defense tools. In this paper,\nwe discuss the numerous challenges and desired attributes associated with such\nautomation. Additionally, we introduce the use of Effects Language (EL), a\nvisual programming language with graph-based operational semantics, as a\nsolution to address many of these challenges and requirements. We formally\ndefine the execution semantics of EL, and prove important execution properties.\nFurthermore, we showcase the application of EL to codify attacks using an\nexample from one of the publicly available attack scenarios. We also\ndemonstrate how EL can be utilized to provide proof-of-attack of complex\nmulti-step attacks. Our results highlight the improvements in time and resource\nefficiency achieved through the use of EL for repeatable automation."
    },
    {
        "date": "2025-10",
        "title": "Geometry-Aware Backdoor Attacks: Leveraging Curvature in Hyperbolic Embeddings",
        "author": "Ali Baheri",
        "link": "http://arxiv.org/abs/2510.06397v1",
        "abstract": "Non-Euclidean foundation models increasingly place representations in curved\nspaces such as hyperbolic geometry. We show that this geometry creates a\nboundary-driven asymmetry that backdoor triggers can exploit. Near the\nboundary, small input changes appear subtle to standard input-space detectors\nbut produce disproportionately large shifts in the model's representation\nspace. Our analysis formalizes this effect and also reveals a limitation for\ndefenses: methods that act by pulling points inward along the radius can\nsuppress such triggers, but only by sacrificing useful model sensitivity in\nthat same direction. Building on these insights, we propose a simple\ngeometry-adaptive trigger and evaluate it across tasks and architectures.\nEmpirically, attack success increases toward the boundary, whereas conventional\ndetectors weaken, mirroring the theoretical trends. Together, these results\nsurface a geometry-specific vulnerability in non-Euclidean models and offer\nanalysis-backed guidance for designing and understanding the limits of\ndefenses."
    },
    {
        "date": "2025-10",
        "title": "Protecting De-identified Documents from Search-based Linkage Attacks",
        "author": "Pierre Lison, and Mark Anderson",
        "link": "http://arxiv.org/abs/2510.06383v1",
        "abstract": "While de-identification models can help conceal the identity of the\nindividual(s) mentioned in a document, they fail to address linkage risks,\ndefined as the potential to map the de-identified text back to its source. One\nstraightforward way to perform such linkages is to extract phrases from the\nde-identified document and then check their presence in the original dataset.\nThis paper presents a method to counter search-based linkage attacks while\npreserving the semantic integrity of the text. The method proceeds in two\nsteps. We first construct an inverted index of the N-grams occurring in the\ndocument collection, making it possible to efficiently determine which N-grams\nappear in less than $k$ documents (either alone or in combination with other\nN-grams). An LLM-based rewriter is then iteratively queried to reformulate\nthose spans until linkage is no longer possible. Experimental results on a\ncollection of court cases show that the method is able to effectively prevent\nsearch-based linkages while remaining faithful to the original content."
    },
    {
        "date": "2025-10",
        "title": "Conditional Denoising Diffusion Model-Based Robust MR Image Reconstruction from Highly Undersampled Data",
        "author": "Mohammed Alsubaie, Wenxi Liu, Linxia Gu, Ovidiu C. Andronesi, Sirani M. Perera, and Xianqi Li",
        "link": "http://arxiv.org/abs/2510.06335v1",
        "abstract": "Magnetic Resonance Imaging (MRI) is a critical tool in modern medical\ndiagnostics, yet its prolonged acquisition time remains a critical limitation,\nespecially in time-sensitive clinical scenarios. While undersampling strategies\ncan accelerate image acquisition, they often result in image artifacts and\ndegraded quality. Recent diffusion models have shown promise for reconstructing\nhigh-fidelity images from undersampled data by learning powerful image priors;\nhowever, most existing approaches either (i) rely on unsupervised score\nfunctions without paired supervision or (ii) apply data consistency only as a\npost-processing step. In this work, we introduce a conditional denoising\ndiffusion framework with iterative data-consistency correction, which differs\nfrom prior methods by embedding the measurement model directly into every\nreverse diffusion step and training the model on paired undersampled-ground\ntruth data. This hybrid design bridges generative flexibility with explicit\nenforcement of MRI physics. Experiments on the fastMRI dataset demonstrate that\nour framework consistently outperforms recent state-of-the-art deep learning\nand diffusion-based methods in SSIM, PSNR, and LPIPS, with LPIPS capturing\nperceptual improvements more faithfully. These results demonstrate that\nintegrating conditional supervision with iterative consistency updates yields\nsubstantial improvements in both pixel-level fidelity and perceptual realism,\nestablishing a principled and practical advance toward robust, accelerated MRI\nreconstruction."
    },
    {
        "date": "2025-10",
        "title": "Training Dynamics Impact Post-Training Quantization Robustness",
        "author": "Albert Catalan-Tatjer, Niccol\u00f2 Ajroldi, and Jonas Geiping",
        "link": "http://arxiv.org/abs/2510.06213v1",
        "abstract": "While post-training quantization is widely adopted for efficient deployment\nof large language models, the mechanisms underlying quantization robustness\nremain unclear. We conduct a comprehensive analysis of quantization degradation\nacross open-source language model training trajectories up to 32B parameters\nand 15T training tokens to accurately assess the relationship between training\ndynamics and quantization performance. Our key finding is that quantization\nerrors in large-scale training runs are driven by a complex interplay between\nlearning rate and other training hyperparameters. Specifically, once learning\nrates decay, validation loss and quantization error diverge, largely\nindependent of training data scale. To investigate interventions on the\ntraining dynamics and identify specific configurations that can modulate\nquantization robustness favorably, we train our own models in controlled\nexperiments up to 100B tokens. Our results challenge the assumption that\nincreasing dataset scale inherently compromises quantization effectiveness,\ndemonstrating instead that strategic training hyperparameter interventions can\nimprove quantization quality at scale."
    },
    {
        "date": "2025-10",
        "title": "Adversarial-Resilient RF Fingerprinting: A CNN-GAN Framework for Rogue Transmitter Detection",
        "author": "Raju Dhakal, Prashant Shekhar, and Laxima Niure Kandel",
        "link": "http://arxiv.org/abs/2510.09663v1",
        "abstract": "Radio Frequency Fingerprinting (RFF) has evolved as an effective solution for\nauthenticating devices by leveraging the unique imperfections in hardware\ncomponents involved in the signal generation process. In this work, we propose\na Convolutional Neural Network (CNN) based framework for detecting rogue\ndevices and identifying genuine ones using softmax probability thresholding. We\nemulate an attack scenario in which adversaries attempt to mimic the RF\ncharacteristics of genuine devices by training a Generative Adversarial Network\n(GAN) using In-phase and Quadrature (IQ) samples from genuine devices. The\nproposed approach is verified using IQ samples collected from ten different\nADALM-PLUTO Software Defined Radios (SDRs), with seven devices considered\ngenuine, two as rogue, and one used for validation to determine the threshold."
    },
    {
        "date": "2025-10",
        "title": "When Thinking Drifts: Evidential Grounding for Robust Video Reasoning",
        "author": "Mi Luo, Zihui Xue, Alex Dimakis, and Kristen Grauman",
        "link": "http://arxiv.org/abs/2510.06077v1",
        "abstract": "Video reasoning, the task of enabling machines to infer from dynamic visual\ncontent through multi-step logic, is crucial for advanced AI. While the\nChain-of-Thought (CoT) mechanism has enhanced reasoning in text-based tasks,\nits application to video understanding remains underexplored. This paper\npresents a systematic analysis revealing that CoT often degrades performance in\nvideo reasoning, generating verbose but misleading internal monologues, and\nleading to hallucinated visual details and overridden correct intuitions - a\nphenomenon we term \"visual thinking drift\". We explain this drift through a\nBayesian lens, positing that CoT traces often diverge from actual visual\nevidence, instead amplifying internal biases or language priors, causing models\nto storytell rather than engage in grounded reasoning. To counteract this, we\nintroduce Visual Evidence Reward (VER), a novel reinforcement learning\nframework that explicitly rewards the generation of reasoning traces that are\nverifiably grounded in visual evidence. Comprehensive evaluation across 10\ndiverse video understanding benchmarks demonstrates that our Video-VER\nconsistently achieves top performance. Our work sheds light on the distinct\nchallenges of video-centric reasoning and encourages the development of AI that\nrobustly grounds its inferences in visual evidence - for large multimodal\nmodels that not only \"think before answering\", but also \"see while thinking\"."
    },
    {
        "date": "2025-10",
        "title": "Adaptive Pruning for Increased Robustness and Reduced Computational Overhead in Gaussian Process Accelerated Saddle Point Searches",
        "author": "Rohit Goswami, and Hannes J\u00f3nsson",
        "link": "http://arxiv.org/abs/2510.06030v1",
        "abstract": "Gaussian process (GP) regression provides a strategy for accelerating saddle\npoint searches on high-dimensional energy surfaces by reducing the number of\ntimes the energy and its derivatives with respect to atomic coordinates need to\nbe evaluated. The computational overhead in the hyperparameter optimization\ncan, however, be large and make the approach inefficient. Failures can also\noccur if the search ventures too far into regions that are not represented well\nenough by the GP model. Here, these challenges are resolved by using\ngeometry-aware optimal transport measures and an active pruning strategy using\na summation over Wasserstein-1 distances for each atom-type in farthest-point\nsampling, selecting a fixed-size subset of geometrically diverse configurations\nto avoid rapidly increasing cost of GP updates as more observations are made.\nStability is enhanced by permutation-invariant metric that provides a reliable\ntrust radius for early-stopping and a logarithmic barrier penalty for the\ngrowth of the signal variance. These physically motivated algorithmic changes\nprove their efficacy by reducing to less than a half the mean computational\ntime on a set of 238 challenging configurations from a previously published\ndata set of chemical reactions. With these improvements, the GP approach is\nestablished as, a robust and scalable algorithm for accelerating saddle point\nsearches when the evaluation of the energy and atomic forces requires\nsignificant computational effort."
    },
    {
        "date": "2025-10",
        "title": "Diffusion-Based Image Editing for Breaking Robust Watermarks",
        "author": "Yunyi Ni, Finn Carter, Ze Niu, Emily Davis, and Bo Zhang",
        "link": "http://arxiv.org/abs/2510.05978v1",
        "abstract": "Robust invisible watermarking aims to embed hidden information into images\nsuch that the watermark can survive various image manipulations. However, the\nrise of powerful diffusion-based image generation and editing techniques poses\na new threat to these watermarking schemes. In this paper, we present a\ntheoretical study and method demonstrating that diffusion models can\neffectively break robust image watermarks that were designed to resist\nconventional perturbations. We show that a diffusion-driven ``image\nregeneration'' process can erase embedded watermarks while preserving\nperceptual image content. We further introduce a novel guided diffusion attack\nthat explicitly targets the watermark signal during generation, significantly\ndegrading watermark detectability. Theoretically, we prove that as an image\nundergoes sufficient diffusion-based transformation, the mutual information\nbetween the watermarked image and the embedded watermark payload vanishes,\nresulting in decoding failure. Experimentally, we evaluate our approach on\nmultiple state-of-the-art watermarking schemes (including the deep\nlearning-based methods StegaStamp, TrustMark, and VINE) and demonstrate\nnear-zero watermark recovery rates after attack, while maintaining high visual\nfidelity of the regenerated images. Our findings highlight a fundamental\nvulnerability in current robust watermarking techniques against generative\nmodel-based attacks, underscoring the need for new watermarking strategies in\nthe era of generative AI."
    },
    {
        "date": "2025-10",
        "title": "MatheMagic: Generating Dynamic Mathematics Benchmarks Robust to Memorization",
        "author": "Dayy\u00e1n O'Brien, Barry Haddow, Emily Allaway, and Pinzhen Chen",
        "link": "http://arxiv.org/abs/2510.05962v1",
        "abstract": "Conducting contamination-free evaluation of mathematical capabilities can be\ndifficult for two reasons: models may memorize a test set once it is made\npublic, and current mathematical benchmarks are prone to overfitting due to\nhaving limited diversity of symbols and rules, coupled with closed-ended\nanswers. This paper proposes a method to leverage these shortcomings as useful\nfeatures to a construct dynamic, counterfactual benchmark, which can be used to\nboth reveal overfitting and measure true reasoning. We demonstrate this via\nMatheMagic, which generates math test instances with the interpretations of\nnumbers and operators altered, yet has automatically verifiable answers. Test\ninstances are randomly seeded and constructed at test time to evaluate a\nmodel's induction or deduction capability, offering stability, extensibility,\ncomparability, and robustness to overfitting. Our experiments find that models\nsolve deduction more easily than induction, but they revert to standard math.\nFurther analysis reveals that math-adapted models fail to exhibit a general\n\"skill\" of reasoning, and fine-tuning on induction tasks generalizes poorly."
    },
    {
        "date": "2025-10",
        "title": "Towards Robust and Realible Multimodal Misinformation Recognition with Incomplete Modality",
        "author": "Hengyang Zhou, Yiwei Wei, Jian Yang, and Zhenyu Zhang",
        "link": "http://arxiv.org/abs/2510.05839v3",
        "abstract": "Multimodal Misinformation Recognition has become an urgent task with the\nemergence of huge multimodal fake content on social media platforms. Previous\nstudies mainly focus on complex feature extraction and fusion to learn\ndiscriminative information from multimodal content. However, in real-world\napplications, multimedia news may naturally lose some information during\ndissemination, resulting in modality incompleteness, which is detrimental to\nthe generalization and robustness of existing models. To this end, we propose a\nnovel generic and robust multimodal fusion strategy, termed Multi-expert\nModality-incomplete Learning Network (MMLNet), which is simple yet effective.\nIt consists of three key steps: (1) Multi-Expert Collaborative Reasoning to\ncompensate for missing modalities by dynamically leveraging complementary\ninformation through multiple experts. (2) Incomplete Modality Adapters\ncompensates for the missing information by leveraging the new feature\ndistribution. (3) Modality Missing Learning leveraging an label-aware adaptive\nweighting strategy to learn a robust representation with contrastive learning.\nWe evaluate MMLNet on three real-world benchmarks across two languages,\ndemonstrating superior performance compared to state-of-the-art methods while\nmaintaining relative simplicity. By ensuring the accuracy of misinformation\nrecognition in incomplete modality scenarios caused by information propagation,\nMMLNet effectively curbs the spread of malicious misinformation. Code is\npublicly available at https://github.com/zhyhome/MMLNet."
    },
    {
        "date": "2025-10",
        "title": "Enhancing Automotive Security with a Hybrid Approach towards Universal Intrusion Detection System",
        "author": "Md Rezanur Islam, Mahdi Sahlabadi, Keunkyoung Kim, and Kangbin Yim",
        "link": "http://arxiv.org/abs/2510.05824v1",
        "abstract": "Security measures are essential in the automotive industry to detect\nintrusions in-vehicle networks. However, developing a one-size-fits-all\nIntrusion Detection System (IDS) is challenging because each vehicle has unique\ndata profiles. This is due to the complex and dynamic nature of the data\ngenerated by vehicles regarding their model, driving style, test environment,\nand firmware update. To address this issue, a universal IDS has been developed\nthat can be applied to all types of vehicles without the need for\ncustomization. Unlike conventional IDSs, the universal IDS can adapt to\nevolving data security issues resulting from firmware updates. In this study, a\nnew hybrid approach has been developed, combining Pearson correlation with deep\nlearning techniques. This approach has been tested using data obtained from\nfour distinct mechanical and electronic vehicles, including Tesla, Sonata, and\ntwo Kia models. The data has been combined into two frequency datasets, and\nwavelet transformation has been employed to convert them into the frequency\ndomain, enhancing generalizability. Additionally, a statistical method based on\nindependent rule-based systems using Pearson correlation has been utilized to\nimprove system performance. The system has been compared with eight different\nIDSs, three of which utilize the universal approach, while the remaining five\nare based on conventional techniques. The accuracy of each system has been\nevaluated through benchmarking, and the results demonstrate that the hybrid\nsystem effectively detects intrusions in various vehicle models."
    },
    {
        "date": "2025-10",
        "title": "SBOMproof: Beyond Alleged SBOM Compliance for Supply Chain Security of Container Images",
        "author": "Jacopo Bufalino, Mario Di Francesco, Agathe Blaise, and Stefano Secci",
        "link": "http://arxiv.org/abs/2510.05798v1",
        "abstract": "Supply chain security is extremely important for modern applications running\nat scale in the cloud. In fact, they involve a large number of heterogeneous\nmicroservices that also include third-party software. As a result, security\nvulnerabilities are hard to identify and mitigate before they start being\nactively exploited by attackers. For this reason, governments have recently\nintroduced cybersecurity regulations that require vendors to share a software\nbill of material (SBOM) with end users or regulators. An SBOM can be employed\nto identify the security vulnerabilities of a software component even without\naccess to its source code, as long as it is accurate and interoperable across\ndifferent tools. This work evaluates this issue through a comprehensive study\nof tools for SBOM generation and vulnerability scanning, including both\nopen-source software and cloud services from major providers. We specifically\ntarget software containers and focus on operating system packages in Linux\ndistributions that are widely used as base images due to their far-reaching\nsecurity impact. Our findings show that the considered tools are largely\nincompatible, leading to inaccurate reporting and a large amount of undetected\nvulnerabilities. We uncover the SBOM confusion vulnerability, a byproduct of\nsuch fragmented ecosystem, where inconsistent formats prevent reliable\nvulnerability detection across tools."
    },
    {
        "date": "2025-10",
        "title": "A Novel Technique for Robust Training of Deep Networks With Multisource Weak Labeled Remote Sensing Data",
        "author": "Gianmarco Perantoni, and Lorenzo Bruzzone",
        "link": "http://arxiv.org/abs/2510.05760v1",
        "abstract": "Deep learning has gained broad interest in remote sensing image scene\nclassification thanks to the effectiveness of deep neural networks in\nextracting the semantics from complex data. However, deep networks require\nlarge amounts of training samples to obtain good generalization capabilities\nand are sensitive to errors in the training labels. This is a problem in remote\nsensing since highly reliable labels can be obtained at high costs and in\nlimited amount. However, many sources of less reliable labeled data are\navailable, e.g., obsolete digital maps. In order to train deep networks with\nlarger datasets, we propose both the combination of single or multiple weak\nsources of labeled data with a small but reliable dataset to generate\nmultisource labeled datasets and a novel training strategy where the\nreliability of each source is taken in consideration. This is done by\nexploiting the transition matrices describing the statistics of the errors of\neach source. The transition matrices are embedded into the labels and used\nduring the training process to weigh each label according to the related\nsource. The proposed method acts as a weighting scheme at gradient level, where\neach instance contributes with different weights to the optimization of\ndifferent classes. The effectiveness of the proposed method is validated by\nexperiments on different datasets. The results proved the robustness and\ncapability of leveraging on unreliable source of labels of the proposed method."
    },
    {
        "date": "2025-10",
        "title": "Empirical Comparison of Membership Inference Attacks in Deep Transfer Learning",
        "author": "Yuxuan Bai, Gauri Pradhan, Marlon Tobaben, and Antti Honkela",
        "link": "http://arxiv.org/abs/2510.05753v2",
        "abstract": "With the emergence of powerful large-scale foundation models, the training\nparadigm is increasingly shifting from from-scratch training to transfer\nlearning. This enables high utility training with small, domain-specific\ndatasets typical in sensitive applications. Membership inference attacks (MIAs)\nprovide an empirical estimate of the privacy leakage by machine learning\nmodels. Yet, prior assessments of MIAs against models fine-tuned with transfer\nlearning rely on a small subset of possible attacks. We address this by\ncomparing performance of diverse MIAs in transfer learning settings to help\npractitioners identify the most efficient attacks for privacy risk evaluation.\nWe find that attack efficacy decreases with the increase in training data for\nscore-based MIAs. We find that there is no one MIA which captures all privacy\nrisks in models trained with transfer learning. While the Likelihood Ratio\nAttack (LiRA) demonstrates superior performance across most experimental\nscenarios, the Inverse Hessian Attack (IHA) proves to be more effective against\nmodels fine-tuned on PatchCamelyon dataset in high data regime."
    },
    {
        "date": "2025-10",
        "title": "Toward a Safer Web: Multilingual Multi-Agent LLMs for Mitigating Adversarial Misinformation Attacks",
        "author": "Nouar Aldahoul, and Yasir Zaki",
        "link": "http://arxiv.org/abs/2510.08605v1",
        "abstract": "The rapid spread of misinformation on digital platforms threatens public\ndiscourse, emotional stability, and decision-making. While prior work has\nexplored various adversarial attacks in misinformation detection, the specific\ntransformations examined in this paper have not been systematically studied. In\nparticular, we investigate language-switching across English, French, Spanish,\nArabic, Hindi, and Chinese, followed by translation. We also study query length\ninflation preceding summarization and structural reformatting into\nmultiple-choice questions. In this paper, we present a multilingual,\nmulti-agent large language model framework with retrieval-augmented generation\nthat can be deployed as a web plugin into online platforms. Our work\nunderscores the importance of AI-driven misinformation detection in\nsafeguarding online factual integrity against diverse attacks, while showcasing\nthe feasibility of plugin-based deployment for real-world web applications."
    },
    {
        "date": "2025-10",
        "title": "Towards Reliable and Practical LLM Security Evaluations via Bayesian Modelling",
        "author": "Mary Llewellyn, Annie Gray, Josh Collyer, and Michael Harries",
        "link": "http://arxiv.org/abs/2510.05709v1",
        "abstract": "Before adopting a new large language model (LLM) architecture, it is critical\nto understand vulnerabilities accurately. Existing evaluations can be difficult\nto trust, often drawing conclusions from LLMs that are not meaningfully\ncomparable, relying on heuristic inputs or employing metrics that fail to\ncapture the inherent uncertainty. In this paper, we propose a principled and\npractical end-to-end framework for evaluating LLM vulnerabilities to prompt\ninjection attacks. First, we propose practical approaches to experimental\ndesign, tackling unfair LLM comparisons by considering two practitioner\nscenarios: when training an LLM and when deploying a pre-trained LLM. Second,\nwe address the analysis of experiments and propose a Bayesian hierarchical\nmodel with embedding-space clustering. This model is designed to improve\nuncertainty quantification in the common scenario that LLM outputs are not\ndeterministic, test prompts are designed imperfectly, and practitioners only\nhave a limited amount of compute to evaluate vulnerabilities. We show the\nimproved inferential capabilities of the model in several prompt injection\nattack settings. Finally, we demonstrate the pipeline to evaluate the security\nof Transformer versus Mamba architectures. Our findings show that consideration\nof output variability can suggest less definitive findings. However, for some\nattacks, we find notably increased Transformer and Mamba-variant\nvulnerabilities across LLMs with the same training data or mathematical\nability."
    },
    {
        "date": "2025-10",
        "title": "Membership Inference Attacks on Tokenizers of Large Language Models",
        "author": "Meng Tong, Yuntao Du, Kejiang Chen, Weiming Zhang, and Ninghui Li",
        "link": "http://arxiv.org/abs/2510.05699v1",
        "abstract": "Membership inference attacks (MIAs) are widely used to assess the privacy\nrisks associated with machine learning models. However, when these attacks are\napplied to pre-trained large language models (LLMs), they encounter significant\nchallenges, including mislabeled samples, distribution shifts, and\ndiscrepancies in model size between experimental and real-world settings. To\naddress these limitations, we introduce tokenizers as a new attack vector for\nmembership inference. Specifically, a tokenizer converts raw text into tokens\nfor LLMs. Unlike full models, tokenizers can be efficiently trained from\nscratch, thereby avoiding the aforementioned challenges. In addition, the\ntokenizer's training data is typically representative of the data used to\npre-train LLMs. Despite these advantages, the potential of tokenizers as an\nattack vector remains unexplored. To this end, we present the first study on\nmembership leakage through tokenizers and explore five attack methods to infer\ndataset membership. Extensive experiments on millions of Internet samples\nreveal the vulnerabilities in the tokenizers of state-of-the-art LLMs. To\nmitigate this emerging risk, we further propose an adaptive defense. Our\nfindings highlight tokenizers as an overlooked yet critical privacy threat,\nunderscoring the urgent need for privacy-preserving mechanisms specifically\ndesigned for them."
    },
    {
        "date": "2025-10",
        "title": "Permutation-Invariant Representation Learning for Robust and Privacy-Preserving Feature Selection",
        "author": "Rui Liu, Tao Zhe, Yanjie Fu, Feng Xia, Ted Senator, and Dongjie Wang",
        "link": "http://arxiv.org/abs/2510.05535v1",
        "abstract": "Feature selection eliminates redundancy among features to improve downstream\ntask performance while reducing computational overhead. Existing methods often\nstruggle to capture intricate feature interactions and adapt across diverse\napplication scenarios. Recent advances employ generative intelligence to\nalleviate these drawbacks. However, these methods remain constrained by\npermutation sensitivity in embedding and reliance on convexity assumptions in\ngradient-based search. To address these limitations, our initial work\nintroduces a novel framework that integrates permutation-invariant embedding\nwith policy-guided search. Although effective, it still left opportunities to\nadapt to realistic distributed scenarios. In practice, data across local\nclients is highly imbalanced, heterogeneous and constrained by strict privacy\nregulations, limiting direct sharing. These challenges highlight the need for a\nframework that can integrate feature selection knowledge across clients without\nexposing sensitive information. In this extended journal version, we advance\nthe framework from two perspectives: 1) developing a privacy-preserving\nknowledge fusion strategy to derive a unified representation space without\nsharing sensitive raw data. 2) incorporating a sample-aware weighting strategy\nto address distributional imbalance among heterogeneous local clients.\nExtensive experiments validate the effectiveness, robustness, and efficiency of\nour framework. The results further demonstrate its strong generalization\nability in federated learning scenarios. The code and data are publicly\navailable: https://anonymous.4open.science/r/FedCAPS-08BF."
    },
    {
        "date": "2025-10",
        "title": "LATTA: Langevin-Anchored Test-Time Adaptation for Enhanced Robustness and Stability",
        "author": "Harshil Vejendla",
        "link": "http://arxiv.org/abs/2510.05530v1",
        "abstract": "Test-time adaptation (TTA) aims to adapt a pretrained model to distribution\nshifts using only unlabeled test data. While promising, existing methods like\nTent suffer from instability and can catastrophically forget the source\nknowledge, especially with small batch sizes or challenging corruptions. We\nargue that this arises from overly deterministic updates on a complex loss\nsurface. In this paper, we introduce Langevin-Anchored Test-Time Adaptation\n(LATTA), a novel approach that regularizes adaptation through two key\nmechanisms: (1) a noisy weight perturbation inspired by Stochastic Gradient\nLangevin Dynamics (SGLD) to explore the local parameter space and escape poor\nlocal minima, and (2) a stable weight anchor that prevents the model from\ndiverging from its robust source pre-training. This combination allows LATTA to\nadapt effectively without sacrificing stability. Unlike prior Bayesian TTA\nmethods, LATTA requires no architectural changes or expensive Monte Carlo\npasses. We conduct extensive experiments on standard benchmarks, including\nRotated-MNIST and the more challenging CIFAR-10-C. Our results demonstrate that\nLATTA significantly outperforms existing methods, including Tent, CoTTA, and\nEATA, setting a new state of the art for self-supervised TTA by improving\naverage accuracy on CIFAR-10-C by over 2% while simultaneously reducing\nperformance variance."
    },
    {
        "date": "2025-10",
        "title": "What is Quantum Computer Security?",
        "author": "Sanjay Deshpande, and Jakub Szefer",
        "link": "http://arxiv.org/abs/2510.07334v1",
        "abstract": "Quantum computing is rapidly emerging as one of the most transformative\ntechnologies of our time. With the potential to tackle problems that remain\nintractable for even the most powerful classical supercomputers, quantum\nhardware has advanced at an extraordinary pace. Today, major platforms such as\nIBM Quantum, Amazon Braket, and Microsoft Azure provide cloud-based access to\nquantum processors, making them more widely available than ever before. While a\npromising technology, quantum computing is not magically immune to security\nthreats. Much research has been done on post-quantum cryptography, which\naddresses how to protect classical computers from attackers using quantum\ncomputers. This article meanwhile introduces the dual idea of quantum computer\nsecurity: how to protect quantum computers from security attacks."
    },
    {
        "date": "2025-10",
        "title": "Adversarial Reinforcement Learning for Large Language Model Agent Safety",
        "author": "Zizhao Wang, Dingcheng Li, Vaishakh Keshava, Phillip Wallis, Ananth Balashankar, Peter Stone, and Lukas Rutishauser",
        "link": "http://arxiv.org/abs/2510.05442v1",
        "abstract": "Large Language Model (LLM) agents can leverage tools such as Google Search to\ncomplete complex tasks. However, this tool usage introduces the risk of\nindirect prompt injections, where malicious instructions hidden in tool outputs\ncan manipulate the agent, posing security risks like data leakage. Current\ndefense strategies typically rely on fine-tuning LLM agents on datasets of\nknown attacks. However, the generation of these datasets relies on manually\ncrafted attack patterns, which limits their diversity and leaves agents\nvulnerable to novel prompt injections. To address this limitation, we propose\nAdversarial Reinforcement Learning for Agent Safety (ARLAS), a novel framework\nthat leverages adversarial reinforcement learning (RL) by formulating the\nproblem as a two-player zero-sum game. ARLAS co-trains two LLMs: an attacker\nthat learns to autonomously generate diverse prompt injections and an agent\nthat learns to defend against them while completing its assigned tasks. To\nensure robustness against a wide range of attacks and to prevent cyclic\nlearning, we employ a population-based learning framework that trains the agent\nto defend against all previous attacker checkpoints. Evaluated on BrowserGym\nand AgentDojo, agents fine-tuned with ARLAS achieve a significantly lower\nattack success rate than the original model while also improving their task\nsuccess rate. Our analysis further confirms that the adversarial process\ngenerates a diverse and challenging set of attacks, leading to a more robust\nagent compared to the base model."
    },
    {
        "date": "2025-10",
        "title": "AutoDAN-Reasoning: Enhancing Strategies Exploration based Jailbreak Attacks with Test-Time Scaling",
        "author": "Xiaogeng Liu, and Chaowei Xiao",
        "link": "http://arxiv.org/abs/2510.05379v2",
        "abstract": "Recent advancements in jailbreaking large language models (LLMs), such as\nAutoDAN-Turbo, have demonstrated the power of automated strategy discovery.\nAutoDAN-Turbo employs a lifelong learning agent to build a rich library of\nattack strategies from scratch. While highly effective, its test-time\ngeneration process involves sampling a strategy and generating a single\ncorresponding attack prompt, which may not fully exploit the potential of the\nlearned strategy library. In this paper, we propose to further improve the\nattack performance of AutoDAN-Turbo through test-time scaling. We introduce two\ndistinct scaling methods: Best-of-N and Beam Search. The Best-of-N method\ngenerates N candidate attack prompts from a sampled strategy and selects the\nmost effective one based on a scorer model. The Beam Search method conducts a\nmore exhaustive search by exploring combinations of strategies from the library\nto discover more potent and synergistic attack vectors. According to the\nexperiments, the proposed methods significantly boost performance, with Beam\nSearch increasing the attack success rate by up to 15.6 percentage points on\nLlama-3.1-70B-Instruct and achieving a nearly 60% relative improvement against\nthe highly robust GPT-o4-mini compared to the vanilla method."
    },
    {
        "date": "2025-10",
        "title": "RegMix: Adversarial Mutual and Generalization Regularization for Enhancing DNN Robustness",
        "author": "Zhenyu Liu, and Varun Ojha",
        "link": "http://arxiv.org/abs/2510.05317v1",
        "abstract": "Adversarial training is the most effective defense against adversarial\nattacks. The effectiveness of the adversarial attacks has been on the design of\nits loss function and regularization term. The most widely used loss function\nin adversarial training is cross-entropy and mean squared error (MSE) as its\nregularization objective. However, MSE enforces overly uniform optimization\nbetween two output distributions during training, which limits its robustness\nin adversarial training scenarios. To address this issue, we revisit the idea\nof mutual learning (originally designed for knowledge distillation) and propose\ntwo novel regularization strategies tailored for adversarial training: (i)\nweighted adversarial mutual regularization and (ii) adversarial generalization\nregularization. In the former, we formulate a decomposed adversarial mutual\nKullback-Leibler divergence (KL-divergence) loss, which allows flexible control\nover the optimization process by assigning unequal weights to the main and\nauxiliary objectives. In the latter, we introduce an additional clean target\ndistribution into the adversarial training objective, improving generalization\nand enhancing model robustness. Extensive experiments demonstrate that our\nproposed methods significantly improve adversarial robustness compared to\nexisting regularization-based approaches."
    },
    {
        "date": "2025-10",
        "title": "RAG Makes Guardrails Unsafe? Investigating Robustness of Guardrails under RAG-style Contexts",
        "author": "Yining She, Daniel W. Peterson, Marianne Menglin Liu, Vikas Upadhyay, Mohammad Hossein Chaghazardi, Eunsuk Kang, and Dan Roth",
        "link": "http://arxiv.org/abs/2510.05310v1",
        "abstract": "With the increasing adoption of large language models (LLMs), ensuring the\nsafety of LLM systems has become a pressing concern. External LLM-based\nguardrail models have emerged as a popular solution to screen unsafe inputs and\noutputs, but they are themselves fine-tuned or prompt-engineered LLMs that are\nvulnerable to data distribution shifts. In this paper, taking Retrieval\nAugmentation Generation (RAG) as a case study, we investigated how robust\nLLM-based guardrails are against additional information embedded in the\ncontext. Through a systematic evaluation of 3 Llama Guards and 2 GPT-oss\nmodels, we confirmed that inserting benign documents into the guardrail context\nalters the judgments of input and output guardrails in around 11% and 8% of\ncases, making them unreliable. We separately analyzed the effect of each\ncomponent in the augmented context: retrieved documents, user query, and\nLLM-generated response. The two mitigation methods we tested only bring minor\nimprovements. These results expose a context-robustness gap in current\nguardrails and motivate training and evaluation protocols that are robust to\nretrieval and query composition."
    },
    {
        "date": "2025-10",
        "title": "SkinMap: Weighted Full-Body Skin Segmentation for Robust Remote Photoplethysmography",
        "author": "Zahra Maleki, Amirhossein Akbari, Amirhossein Binesh, and Babak Khalaj",
        "link": "http://arxiv.org/abs/2510.05296v1",
        "abstract": "Remote photoplethysmography (rPPG) is an innovative method for monitoring\nheart rate and vital signs by using a simple camera to record a person, as long\nas any part of their skin is visible. This low-cost, contactless approach helps\nin remote patient monitoring, emotion analysis, smart vehicle utilization, and\nmore. Over the years, various techniques have been proposed to improve the\naccuracy of this technology, especially given its sensitivity to lighting and\nmovement. In the unsupervised pipeline, it is necessary to first select skin\nregions from the video to extract the rPPG signal from the skin color changes.\nWe introduce a novel skin segmentation technique that prioritizes skin regions\nto enhance the quality of the extracted signal. It can detect areas of skin all\nover the body, making it more resistant to movement, while removing areas such\nas the mouth, eyes, and hair that may cause interference. Our model is\nevaluated on publicly available datasets, and we also present a new dataset,\ncalled SYNC-rPPG, to better represent real-world conditions. The results\nindicate that our model demonstrates a prior ability to capture heartbeats in\nchallenging conditions, such as talking and head rotation, and maintain the\nmean absolute error (MAE) between predicted and actual heart rates, while other\nmethods fail to do so. In addition, we demonstrate high accuracy in detecting a\ndiverse range of skin tones, making this technique a promising option for\nreal-world applications."
    },
    {
        "date": "2025-10",
        "title": "Proactive defense against LLM Jailbreak",
        "author": "Weiliang Zhao, Jinjun Peng, Daniel Ben-Levi, Zhou Yu, and Junfeng Yang",
        "link": "http://arxiv.org/abs/2510.05052v1",
        "abstract": "The proliferation of powerful large language models (LLMs) has necessitated\nrobust safety alignment, yet these models remain vulnerable to evolving\nadversarial attacks, including multi-turn jailbreaks that iteratively search\nfor successful queries. Current defenses, primarily reactive and static, often\nfail to counter these search-based attacks. In this paper, we introduce ProAct,\na novel proactive defense framework designed to disrupt and mislead autonomous\njailbreaking processes. Our core idea is to intentionally provide adversaries\nwith \"spurious responses\" that appear to be results of successful jailbreak\nattacks but contain no actual harmful content. These misleading responses\nprovide false signals to the attacker's internal optimization loop, causing the\nadversarial search to terminate prematurely and effectively jailbreaking the\njailbreak. By conducting extensive experiments across state-of-the-art LLMs,\njailbreaking frameworks, and safety benchmarks, our method consistently and\nsignificantly reduces attack success rates by up to 92\\%. When combined with\nother defense frameworks, it further reduces the success rate of the latest\nattack strategies to 0\\%. ProAct represents an orthogonal defense strategy that\ncan serve as an additional guardrail to enhance LLM safety against the most\neffective jailbreaking attacks."
    },
    {
        "date": "2025-10",
        "title": "KEEP: Integrating Medical Ontologies with Clinical Data for Robust Code Embeddings",
        "author": "Ahmed Elhussein, Paul Meddeb, Abigail Newbury, Jeanne Mirone, Martin Stoll, and Gamze Gursoy",
        "link": "http://arxiv.org/abs/2510.05049v1",
        "abstract": "Machine learning in healthcare requires effective representation of\nstructured medical codes, but current methods face a trade off: knowledge graph\nbased approaches capture formal relationships but miss real world patterns,\nwhile data driven methods learn empirical associations but often overlook\nstructured knowledge in medical terminologies. We present KEEP (Knowledge\npreserving and Empirically refined Embedding Process), an efficient framework\nthat bridges this gap by combining knowledge graph embeddings with adaptive\nlearning from clinical data. KEEP first generates embeddings from knowledge\ngraphs, then employs regularized training on patient records to adaptively\nintegrate empirical patterns while preserving ontological relationships.\nImportantly, KEEP produces final embeddings without task specific auxiliary or\nend to end training enabling KEEP to support multiple downstream applications\nand model architectures. Evaluations on structured EHR from UK Biobank and\nMIMIC IV demonstrate that KEEP outperforms both traditional and Language Model\nbased approaches in capturing semantic relationships and predicting clinical\noutcomes. Moreover, KEEP's minimal computational requirements make it\nparticularly suitable for resource constrained environments."
    },
    {
        "date": "2025-10",
        "title": "NatGVD: Natural Adversarial Example Attack towards Graph-based Vulnerability Detection",
        "author": "Avilash Rath, Weiliang Qi, Youpeng Li, and Xinda Wang",
        "link": "http://arxiv.org/abs/2510.04987v1",
        "abstract": "Graph-based models learn rich code graph structural information and present\nsuperior performance on various code analysis tasks. However, the robustness of\nthese models against adversarial example attacks in the context of\nvulnerability detection remains an open question. This paper proposes NatGVD, a\nnovel attack methodology that generates natural adversarial vulnerable code to\ncircumvent GNN-based and graph-aware transformer-based vulnerability detectors.\nNatGVD employs a set of code transformations that modify graph structure while\npreserving code semantics. Instead of injecting dead or unrelated code like\nprevious works, NatGVD considers naturalness requirements: generated examples\nshould not be easily recognized by humans or program analysis tools. With\nextensive evaluation of NatGVD on state-of-the-art vulnerability detection\nsystems, the results reveal up to 53.04% evasion rate across GNN-based\ndetectors and graph-aware transformer-based detectors. We also explore\npotential defense strategies to enhance the robustness of these systems against\nNatGVD."
    },
    {
        "date": "2025-10",
        "title": "StructuralDecompose: A Modular Framework for Robust Time Series Decomposition in R",
        "author": "Allen Daniel Sunny",
        "link": "http://arxiv.org/abs/2510.04974v1",
        "abstract": "We present StructuralDecompose, an R package for modular and interpretable\ntime series decomposition. Unlike existing approaches that treat decomposition\nas a monolithic process, StructuralDecompose separates the analysis into\ndistinct components: changepoint detection, anomaly detection, smoothing, and\ndecomposition. This design provides flexibility and robust- ness, allowing\nusers to tailor methods to specific time series characteristics. We demonstrate\nthe package on simulated and real-world datasets, benchmark its performance\nagainst state-of-the- art tools such as Rbeast and autostsm, and discuss its\nrole in interpretable machine learning workflows."
    },
    {
        "date": "2025-10",
        "title": "\u03bcDeepIQA: deep learning-based fast and robust image quality assessment with local predictions for optical microscopy",
        "author": "Elena Corbetta, and Thomas Bocklitz",
        "link": "http://arxiv.org/abs/2510.04859v1",
        "abstract": "Optical microscopy is one of the most widely used techniques in research\nstudies for life sciences and biomedicine. These applications require reliable\nexperimental pipelines to extract valuable knowledge from the measured samples\nand must be supported by image quality assessment (IQA) to ensure correct\nprocessing and analysis of the image data. IQA methods are implemented with\nvariable complexity. However, while most quality metrics have a straightforward\nimplementation, they might be time consuming and computationally expensive when\nevaluating a large dataset. In addition, quality metrics are often designed for\nwell-defined image features and may be unstable for images out of the ideal\ndomain.\n  To overcome these limitations, recent works have proposed deep learning-based\nIQA methods, which can provide superior performance, increased generalizability\nand fast prediction. Our method, named $\\mathrm{\\mu}$DeepIQA, is inspired by\nprevious studies and applies a deep convolutional neural network designed for\nIQA on natural images to optical microscopy measurements. We retrained the same\narchitecture to predict individual quality metrics and global quality scores\nfor optical microscopy data. The resulting models provide fast and stable\npredictions of image quality by generalizing quality estimation even outside\nthe ideal range of standard methods. In addition, $\\mathrm{\\mu}$DeepIQA\nprovides patch-wise prediction of image quality and can be used to visualize\nspatially varying quality in a single image. Our study demonstrates that\noptical microscopy-based studies can benefit from the generalizability of deep\nlearning models due to their stable performance in the presence of outliers,\nthe ability to assess small image patches, and rapid predictions."
    },
    {
        "date": "2025-10",
        "title": "Distributionally Robust Causal Abstractions",
        "author": "Yorgos Felekis, Theodoros Damoulas, and Paris Giampouras",
        "link": "http://arxiv.org/abs/2510.04842v1",
        "abstract": "Causal Abstraction (CA) theory provides a principled framework for relating\ncausal models that describe the same system at different levels of granularity\nwhile ensuring interventional consistency between them. Recently, several\napproaches for learning CAs have been proposed, but all assume fixed and\nwell-specified exogenous distributions, making them vulnerable to environmental\nshifts and misspecification. In this work, we address these limitations by\nintroducing the first class of distributionally robust CAs and their associated\nlearning algorithms. The latter cast robust causal abstraction learning as a\nconstrained min-max optimization problem with Wasserstein ambiguity sets. We\nprovide theoretical results, for both empirical and Gaussian environments,\nleading to principled selection of the level of robustness via the radius of\nthese sets. Furthermore, we present empirical evidence across different\nproblems and CA learning methods, demonstrating our framework's robustness not\nonly to environmental shifts but also to structural model and intervention\nmapping misspecification."
    },
    {
        "date": "2025-10",
        "title": "A Noise Resilient Approach for Robust Hurst Exponent Estimation",
        "author": "Malith Premarathna, Fabrizio Ruggeri, and Dixon Vimalajeewa",
        "link": "http://arxiv.org/abs/2510.04811v1",
        "abstract": "Understanding signal behavior across scales is vital in areas such as natural\nphenomena analysis and financial modeling. A key property is self-similarity,\nquantified by the Hurst exponent (H), which reveals long-term dependencies.\nWavelet-based methods are effective for estimating H due to their multi-scale\nanalysis capability, but additive noise in real-world measurements often\ndegrades accuracy. We propose Noise-Controlled ALPHEE (NC-ALPHEE), an\nenhancement of the Average Level-Pairwise Hurst Exponent Estimator (ALPHEE),\nincorporating noise mitigation and generating multiple level-pairwise estimates\nfrom signal energy pairs. A neural network (NN) combines these estimates,\nreplacing traditional averaging. This adaptive learning maintains ALPHEE's\nbehavior in noise-free cases while improving performance in noisy conditions.\nExtensive simulations show that in noise-free data, NC-ALPHEE matches ALPHEE's\naccuracy using both averaging and NN-based methods. Under noise, however,\ntraditional averaging deteriorates and requires impractical level restrictions,\nwhile NC-ALPHEE consistently outperforms existing techniques without such\nconstraints. NC-ALPHEE offers a robust, adaptive approach for H estimation,\nsignificantly enhancing the reliability of wavelet-based methods in noisy\nenvironments."
    },
    {
        "date": "2025-10",
        "title": "Collusion-Resistant Quantum Secure Key Leasing Beyond Decryption",
        "author": "Fuyuki Kitagawa, Ryo Nishimaki, and Nikhil Pappu",
        "link": "http://arxiv.org/abs/2510.04754v1",
        "abstract": "Secure key leasing (SKL) enables the holder of a secret key for a\ncryptographic function to temporarily lease the key using quantum information.\nLater, the recipient can produce a deletion certificate, which proves that they\nno longer have access to the secret key. The security guarantee ensures that\neven a malicious recipient cannot continue to evaluate the function, after\nproducing a valid deletion certificate.\n  Most prior work considers an adversarial recipient that obtains a single\nleased key, which is insufficient for many applications. In the more realistic\ncollusion-resistant setting, security must hold even when polynomially many\nkeys are leased (and subsequently deleted). However, achieving\ncollusion-resistant SKL from standard assumptions remains poorly understood,\nespecially for functionalities beyond decryption.\n  We improve upon this situation by introducing new pathways for constructing\ncollusion-resistant SKL. Our main contributions are as follows:\n  - A generalization of quantum-secure collusion-resistant traitor tracing\ncalled multi-level traitor tracing (MLTT), and a compiler that transforms an\nMLTT scheme for a primitive X into a collusion-resistant SKL scheme for\nprimitive X.\n  - The first bounded collusion-resistant SKL scheme for PRFs, assuming LWE.\n  - A compiler that upgrades any single-key secure SKL scheme for digital\nsignatures into one with unbounded collusion-resistance, assuming OWFs.\n  - A compiler that upgrades collusion-resistant SKL schemes with classical\ncertificates to ones having verification-query resilience, assuming OWFs."
    },
    {
        "date": "2025-10",
        "title": "Anomaly-Aware YOLO: A Frugal yet Robust Approach to Infrared Small Target Detection",
        "author": "Alina Ciocarlan, Sylvie Le H\u00e9garat-Mascle, and Sidonie Lefebvre",
        "link": "http://arxiv.org/abs/2510.04741v1",
        "abstract": "Infrared Small Target Detection (IRSTD) is a challenging task in defense\napplications, where complex backgrounds and tiny target sizes often result in\nnumerous false alarms using conventional object detectors. To overcome this\nlimitation, we propose Anomaly-Aware YOLO (AA-YOLO), which integrates a\nstatistical anomaly detection test into its detection head. By treating small\ntargets as unexpected patterns against the background, AA-YOLO effectively\ncontrols the false alarm rate. Our approach not only achieves competitive\nperformance on several IRSTD benchmarks, but also demonstrates remarkable\nrobustness in scenarios with limited training data, noise, and domain shifts.\nFurthermore, since only the detection head is modified, our design is highly\ngeneric and has been successfully applied across various YOLO backbones,\nincluding lightweight models. It also provides promising results when\nintegrated into an instance segmentation YOLO. This versatility makes AA-YOLO\nan attractive solution for real-world deployments where resources are\nconstrained. The code will be publicly released."
    },
    {
        "date": "2025-10",
        "title": "Parameter-free Algorithms for the Stochastically Extended Adversarial Model",
        "author": "Shuche Wang, Adarsh Barik, Peng Zhao, and Vincent Y. F. Tan",
        "link": "http://arxiv.org/abs/2510.04685v1",
        "abstract": "We develop the first parameter-free algorithms for the Stochastically\nExtended Adversarial (SEA) model, a framework that bridges adversarial and\nstochastic online convex optimization. Existing approaches for the SEA model\nrequire prior knowledge of problem-specific parameters, such as the diameter of\nthe domain $D$ and the Lipschitz constant of the loss functions $G$, which\nlimits their practical applicability. Addressing this, we develop\nparameter-free methods by leveraging the Optimistic Online Newton Step (OONS)\nalgorithm to eliminate the need for these parameters. We first establish a\ncomparator-adaptive algorithm for the scenario with unknown domain diameter but\nknown Lipschitz constant, achieving an expected regret bound of\n$\\tilde{O}\\big(\\|u\\|_2^2 + \\|u\\|_2(\\sqrt{\\sigma^2_{1:T}} +\n\\sqrt{\\Sigma^2_{1:T}})\\big)$, where $u$ is the comparator vector and\n$\\sigma^2_{1:T}$ and $\\Sigma^2_{1:T}$ represent the cumulative stochastic\nvariance and cumulative adversarial variation, respectively. We then extend\nthis to the more general setting where both $D$ and $G$ are unknown, attaining\nthe comparator- and Lipschitz-adaptive algorithm. Notably, the regret bound\nexhibits the same dependence on $\\sigma^2_{1:T}$ and $\\Sigma^2_{1:T}$,\ndemonstrating the efficacy of our proposed methods even when both parameters\nare unknown in the SEA model."
    },
    {
        "date": "2025-10",
        "title": "Backing the Wrong Horse: How Bit-Level Netlist Augmentation can Counter Power Side Channel Attacks",
        "author": "Ali Asghar, Andreas Becher, and Daniel Ziener",
        "link": "http://arxiv.org/abs/2510.04640v1",
        "abstract": "The dependence of power-consumption on the processed data is a known\nvulnerability of CMOS circuits, resulting in side channels which can be\nexploited by power-based side channel attacks (SCAs). These attacks can extract\nsensitive information, such as secret keys, from the implementation of\ncryptographic algorithms. Existing countermeasures against power-based side\nchannel attacks focus on analyzing information leakage at the byte level.\nHowever, this approach neglects the impact of individual bits on the overall\nresistance of a cryptographic implementation. In this work, we present a\ncountermeasure based on single-bit leakage. The results suggest that the\nproposed countermeasure cannot be broken by attacks using conventional SCA\nleakage models."
    },
    {
        "date": "2025-10",
        "title": "Forecasting-Based Biomedical Time-series Data Synthesis for Open Data and Robust AI",
        "author": "Youngjoon Lee, Seongmin Cho, Yehhyun Jo, Jinu Gong, Hyunjoo Jenny Lee, and Joonhyuk Kang",
        "link": "http://arxiv.org/abs/2510.04622v1",
        "abstract": "The limited data availability due to strict privacy regulations and\nsignificant resource demands severely constrains biomedical time-series AI\ndevelopment, which creates a critical gap between data requirements and\naccessibility. Synthetic data generation presents a promising solution by\nproducing artificial datasets that maintain the statistical properties of real\nbiomedical time-series data without compromising patient confidentiality. We\npropose a framework for synthetic biomedical time-series data generation based\non advanced forecasting models that accurately replicates complex\nelectrophysiological signals such as EEG and EMG with high fidelity. These\nsynthetic datasets preserve essential temporal and spectral properties of real\ndata, which enables robust analysis while effectively addressing data scarcity\nand privacy challenges. Our evaluations across multiple subjects demonstrate\nthat the generated synthetic data can serve as an effective substitute for real\ndata and also significantly boost AI model performance. The approach maintains\ncritical biomedical features while provides high scalability for various\napplications and integrates seamlessly into open-source repositories,\nsubstantially expanding resources for AI-driven biomedical research."
    },
    {
        "date": "2025-10",
        "title": "Computational Certified Deletion Property of Magic Square Game and its Application to Classical Secure Key Leasing",
        "author": "Yuki Takeuchi, and Duo Xu",
        "link": "http://arxiv.org/abs/2510.04529v2",
        "abstract": "We present the first construction of a computational Certified Deletion\nProperty (CDP) achievable with classical communication, derived from the\ncompilation of the non-local Magic Square Game (MSG). We leverage the KLVY\ncompiler to transform the non-local MSG into a 2-round interactive protocol,\nrigorously demonstrating that this compilation preserves the game-specific CDP.\nPreviously, the quantum value and rigidity of the compiled game were\ninvestigated. We emphasize that we are the first to investigate CDP (local\nrandomness in [Fu and Miller, Phys. Rev. A 97, 032324 (2018)]) for the compiled\ngame. Then, we combine this CDP with the framework [Kitagawa, Morimae, and\nYamakawa, Eurocrypt 2025] to construct Secure Key Leasing with classical Lessor\n(cSKL). SKL enables the Lessor to lease the secret key to the Lessee and verify\nthat a quantum Lessee has indeed deleted the key. In this paper, we realize\ncSKL for PKE, PRF, and digital signature. Compared to prior works for cSKL, we\nrealize cSKL for PRF and digital signature for the first time. In addition, we\nsucceed in weakening the assumption needed to construct cSKL."
    },
    {
        "date": "2025-10",
        "title": "P2P: A Poison-to-Poison Remedy for Reliable Backdoor Defense in LLMs",
        "author": "Shuai Zhao, Xinyi Wu, Shiqian Zhao, Xiaobao Wu, Zhongliang Guo, Yanhao Jia, and Anh Tuan Luu",
        "link": "http://arxiv.org/abs/2510.04503v2",
        "abstract": "During fine-tuning, large language models (LLMs) are increasingly vulnerable\nto data-poisoning backdoor attacks, which compromise their reliability and\ntrustworthiness. However, existing defense strategies suffer from limited\ngeneralization: they only work on specific attack types or task settings. In\nthis study, we propose Poison-to-Poison (P2P), a general and effective backdoor\ndefense algorithm. P2P injects benign triggers with safe alternative labels\ninto a subset of training samples and fine-tunes the model on this re-poisoned\ndataset by leveraging prompt-based learning. This enforces the model to\nassociate trigger-induced representations with safe outputs, thereby overriding\nthe effects of original malicious triggers. Thanks to this robust and\ngeneralizable trigger-based fine-tuning, P2P is effective across task settings\nand attack types. Theoretically and empirically, we show that P2P can\nneutralize malicious backdoors while preserving task performance. We conduct\nextensive experiments on classification, mathematical reasoning, and summary\ngeneration tasks, involving multiple state-of-the-art LLMs. The results\ndemonstrate that our P2P algorithm significantly reduces the attack success\nrate compared with baseline models. We hope that the P2P can serve as a\nguideline for defending against backdoor attacks and foster the development of\na secure and trustworthy LLM community."
    },
    {
        "date": "2025-10",
        "title": "SECA: Semantically Equivalent and Coherent Attacks for Eliciting LLM Hallucinations",
        "author": "Buyun Liang, Liangzu Peng, Jinqi Luo, Darshan Thaker, Kwan Ho Ryan Chan, and Ren\u00e9 Vidal",
        "link": "http://arxiv.org/abs/2510.04398v1",
        "abstract": "Large Language Models (LLMs) are increasingly deployed in high-risk domains.\nHowever, state-of-the-art LLMs often produce hallucinations, raising serious\nconcerns about their reliability. Prior work has explored adversarial attacks\nfor hallucination elicitation in LLMs, but it often produces unrealistic\nprompts, either by inserting gibberish tokens or by altering the original\nmeaning. As a result, these approaches offer limited insight into how\nhallucinations may occur in practice. While adversarial attacks in computer\nvision often involve realistic modifications to input images, the problem of\nfinding realistic adversarial prompts for eliciting LLM hallucinations has\nremained largely underexplored. To address this gap, we propose Semantically\nEquivalent and Coherent Attacks (SECA) to elicit hallucinations via realistic\nmodifications to the prompt that preserve its meaning while maintaining\nsemantic coherence. Our contributions are threefold: (i) we formulate finding\nrealistic attacks for hallucination elicitation as a constrained optimization\nproblem over the input prompt space under semantic equivalence and coherence\nconstraints; (ii) we introduce a constraint-preserving zeroth-order method to\neffectively search for adversarial yet feasible prompts; and (iii) we\ndemonstrate through experiments on open-ended multiple-choice question\nanswering tasks that SECA achieves higher attack success rates while incurring\nalmost no constraint violations compared to existing methods. SECA highlights\nthe sensitivity of both open-source and commercial gradient-inaccessible LLMs\nto realistic and plausible prompt variations. Code is available at\nhttps://github.com/Buyun-Liang/SECA."
    },
    {
        "date": "2025-10",
        "title": "Unmasking Backdoors: An Explainable Defense via Gradient-Attention Anomaly Scoring for Pre-trained Language Models",
        "author": "Anindya Sundar Das, Kangjie Chen, and Monowar Bhuyan",
        "link": "http://arxiv.org/abs/2510.04347v1",
        "abstract": "Pre-trained language models have achieved remarkable success across a wide\nrange of natural language processing (NLP) tasks, particularly when fine-tuned\non large, domain-relevant datasets. However, they remain vulnerable to backdoor\nattacks, where adversaries embed malicious behaviors using trigger patterns in\nthe training data. These triggers remain dormant during normal usage, but, when\nactivated, can cause targeted misclassifications. In this work, we investigate\nthe internal behavior of backdoored pre-trained encoder-based language models,\nfocusing on the consistent shift in attention and gradient attribution when\nprocessing poisoned inputs; where the trigger token dominates both attention\nand gradient signals, overriding the surrounding context. We propose an\ninference-time defense that constructs anomaly scores by combining token-level\nattention and gradient information. Extensive experiments on text\nclassification tasks across diverse backdoor attack scenarios demonstrate that\nour method significantly reduces attack success rates compared to existing\nbaselines. Furthermore, we provide an interpretability-driven analysis of the\nscoring mechanism, shedding light on trigger localization and the robustness of\nthe proposed defense."
    },
    {
        "date": "2025-10",
        "title": "Learning to Predict Chaos: Curriculum-Driven Training for Robust Forecasting of Chaotic Dynamics",
        "author": "Harshil Vejendla",
        "link": "http://arxiv.org/abs/2510.04342v1",
        "abstract": "Forecasting chaotic systems is a cornerstone challenge in many scientific\nfields, complicated by the exponential amplification of even infinitesimal\nprediction errors. Modern machine learning approaches often falter due to two\nopposing pitfalls: over-specializing on a single, well-known chaotic system\n(e.g., Lorenz-63), which limits generalizability, or indiscriminately mixing\nvast, unrelated time-series, which prevents the model from learning the nuances\nof any specific dynamical regime. We propose Curriculum Chaos Forecasting\n(CCF), a training paradigm that bridges this gap. CCF organizes training data\nbased on fundamental principles of dynamical systems theory, creating a\ncurriculum that progresses from simple, periodic behaviors to highly complex,\nchaotic dynamics. We quantify complexity using the largest Lyapunov exponent\nand attractor dimension, two well-established metrics of chaos. By first\ntraining a sequence model on predictable systems and gradually introducing more\nchaotic trajectories, CCF enables the model to build a robust and generalizable\nrepresentation of dynamical behaviors. We curate a library of over 50 synthetic\nODE/PDE systems to build this curriculum. Our experiments show that\npre-training with CCF significantly enhances performance on unseen, real-world\nbenchmarks. On datasets including Sunspot numbers, electricity demand, and\nhuman ECG signals, CCF extends the valid prediction horizon by up to 40%\ncompared to random-order training and more than doubles it compared to training\non real-world data alone. We demonstrate that this benefit is consistent across\nvarious neural architectures (GRU, Transformer) and provide extensive ablations\nto validate the importance of the curriculum's structure."
    },
    {
        "date": "2025-10",
        "title": "VortexPIA: Indirect Prompt Injection Attack against LLMs for Efficient Extraction of User Privacy",
        "author": "Yu Cui, Sicheng Pan, Yifei Liu, Haibin Zhang, and Cong Zuo",
        "link": "http://arxiv.org/abs/2510.04261v1",
        "abstract": "Large language models (LLMs) have been widely deployed in Conversational AIs\n(CAIs), while exposing privacy and security threats. Recent research shows that\nLLM-based CAIs can be manipulated to extract private information from human\nusers, posing serious security threats. However, the methods proposed in that\nstudy rely on a white-box setting that adversaries can directly modify the\nsystem prompt. This condition is unlikely to hold in real-world deployments.\nThe limitation raises a critical question: can unprivileged attackers still\ninduce such privacy risks in practical LLM-integrated applications? To address\nthis question, we propose \\textsc{VortexPIA}, a novel indirect prompt injection\nattack that induces privacy extraction in LLM-integrated applications under\nblack-box settings. By injecting token-efficient data containing false\nmemories, \\textsc{VortexPIA} misleads LLMs to actively request private\ninformation in batches. Unlike prior methods, \\textsc{VortexPIA} allows\nattackers to flexibly define multiple categories of sensitive data. We evaluate\n\\textsc{VortexPIA} on six LLMs, covering both traditional and reasoning LLMs,\nacross four benchmark datasets. The results show that \\textsc{VortexPIA}\nsignificantly outperforms baselines and achieves state-of-the-art (SOTA)\nperformance. It also demonstrates efficient privacy requests, reduced token\nconsumption, and enhanced robustness against defense mechanisms. We further\nvalidate \\textsc{VortexPIA} on multiple realistic open-source LLM-integrated\napplications, demonstrating its practical effectiveness."
    },
    {
        "date": "2025-10",
        "title": "AgentTypo: Adaptive Typographic Prompt Injection Attacks against Black-box Multimodal Agents",
        "author": "Yanjie Li, Yiming Cao, Dong Wang, and Bin Xiao",
        "link": "http://arxiv.org/abs/2510.04257v1",
        "abstract": "Multimodal agents built on large vision-language models (LVLMs) are\nincreasingly deployed in open-world settings but remain highly vulnerable to\nprompt injection, especially through visual inputs. We introduce AgentTypo, a\nblack-box red-teaming framework that mounts adaptive typographic prompt\ninjection by embedding optimized text into webpage images. Our automatic\ntypographic prompt injection (ATPI) algorithm maximizes prompt reconstruction\nby substituting captioners while minimizing human detectability via a stealth\nloss, with a Tree-structured Parzen Estimator guiding black-box optimization\nover text placement, size, and color. To further enhance attack strength, we\ndevelop AgentTypo-pro, a multi-LLM system that iteratively refines injection\nprompts using evaluation feedback and retrieves successful past examples for\ncontinual learning. Effective prompts are abstracted into generalizable\nstrategies and stored in a strategy repository, enabling progressive knowledge\naccumulation and reuse in future attacks. Experiments on the VWA-Adv benchmark\nacross Classifieds, Shopping, and Reddit scenarios show that AgentTypo\nsignificantly outperforms the latest image-based attacks such as AgentAttack.\nOn GPT-4o agents, our image-only attack raises the success rate from 0.23 to\n0.45, with consistent results across GPT-4V, GPT-4o-mini, Gemini 1.5 Pro, and\nClaude 3 Opus. In image+text settings, AgentTypo achieves 0.68 ASR, also\noutperforming the latest baselines. Our findings reveal that AgentTypo poses a\npractical and potent threat to multimodal agents and highlight the urgent need\nfor effective defense."
    },
    {
        "date": "2025-10",
        "title": "Concept-Based Masking: A Patch-Agnostic Defense Against Adversarial Patch Attacks",
        "author": "Ayushi Mehrotra, Derek Peng, Dipkamal Bhusal, and Nidhi Rastogi",
        "link": "http://arxiv.org/abs/2510.04245v1",
        "abstract": "Adversarial patch attacks pose a practical threat to deep learning models by\nforcing targeted misclassifications through localized perturbations, often\nrealized in the physical world. Existing defenses typically assume prior\nknowledge of patch size or location, limiting their applicability. In this\nwork, we propose a patch-agnostic defense that leverages concept-based\nexplanations to identify and suppress the most influential concept activation\nvectors, thereby neutralizing patch effects without explicit detection.\nEvaluated on Imagenette with a ResNet-50, our method achieves higher robust and\nclean accuracy than the state-of-the-art PatchCleanser, while maintaining\nstrong performance across varying patch sizes and locations. Our results\nhighlight the promise of combining interpretability with robustness and suggest\nconcept-driven defenses as a scalable strategy for securing machine learning\nmodels against adversarial patch attacks."
    },
    {
        "date": "2025-10",
        "title": "SafeGuider: Robust and Practical Content Safety Control for Text-to-Image Models",
        "author": "Peigui Qi, Kunsheng Tang, Wenbo Zhou, Weiming Zhang, Nenghai Yu, Tianwei Zhang, Qing Guo, and Jie Zhang",
        "link": "http://arxiv.org/abs/2510.05173v2",
        "abstract": "Text-to-image models have shown remarkable capabilities in generating\nhigh-quality images from natural language descriptions. However, these models\nare highly vulnerable to adversarial prompts, which can bypass safety measures\nand produce harmful content. Despite various defensive strategies, achieving\nrobustness against attacks while maintaining practical utility in real-world\napplications remains a significant challenge. To address this issue, we first\nconduct an empirical study of the text encoder in the Stable Diffusion (SD)\nmodel, which is a widely used and representative text-to-image model. Our\nfindings reveal that the [EOS] token acts as a semantic aggregator, exhibiting\ndistinct distributional patterns between benign and adversarial prompts in its\nembedding space. Building on this insight, we introduce \\textbf{SafeGuider}, a\ntwo-step framework designed for robust safety control without compromising\ngeneration quality. SafeGuider combines an embedding-level recognition model\nwith a safety-aware feature erasure beam search algorithm. This integration\nenables the framework to maintain high-quality image generation for benign\nprompts while ensuring robust defense against both in-domain and out-of-domain\nattacks. SafeGuider demonstrates exceptional effectiveness in minimizing attack\nsuccess rates, achieving a maximum rate of only 5.48\\% across various attack\nscenarios. Moreover, instead of refusing to generate or producing black images\nfor unsafe prompts, \\textbf{SafeGuider} generates safe and meaningful images,\nenhancing its practical utility. In addition, SafeGuider is not limited to the\nSD model and can be effectively applied to other text-to-image models, such as\nthe Flux model, demonstrating its versatility and adaptability across different\narchitectures. We hope that SafeGuider can shed some light on the practical\ndeployment of secure text-to-image systems."
    },
    {
        "date": "2025-10",
        "title": "Harnessing LLM for Noise-Robust Cognitive Diagnosis in Web-Based Intelligent Education Systems",
        "author": "Guixian Zhang, Guan Yuan, Ziqi Xu, Yanmei Zhang, Jing Ren, Zhenyun Deng, and Debo Cheng",
        "link": "http://arxiv.org/abs/2510.04093v2",
        "abstract": "Cognitive diagnostics in the Web-based Intelligent Education System (WIES)\naims to assess students' mastery of knowledge concepts from heterogeneous,\nnoisy interactions. Recent work has tried to utilize Large Language Models\n(LLMs) for cognitive diagnosis, yet LLMs struggle with structured data and are\nprone to noise-induced misjudgments. Specially, WIES's open environment\ncontinuously attracts new students and produces vast amounts of response logs,\nexacerbating the data imbalance and noise issues inherent in traditional\neducational systems. To address these challenges, we propose DLLM, a\nDiffusion-based LLM framework for noise-robust cognitive diagnosis. DLLM first\nconstructs independent subgraphs based on response correctness, then applies\nrelation augmentation alignment module to mitigate data imbalance. The two\nsubgraph representations are then fused and aligned with LLM-derived,\nsemantically augmented representations. Importantly, before each alignment\nstep, DLLM employs a two-stage denoising diffusion module to eliminate\nintrinsic noise while assisting structural representation alignment.\nSpecifically, unconditional denoising diffusion first removes erroneous\ninformation, followed by conditional denoising diffusion based on graph-guided\nto eliminate misleading information. Finally, the noise-robust representation\nthat integrates semantic knowledge and structural information is fed into\nexisting cognitive diagnosis models for prediction. Experimental results on\nthree publicly available web-based educational platform datasets demonstrate\nthat our DLLM achieves optimal predictive performance across varying noise\nlevels, which demonstrates that DLLM achieves noise robustness while\neffectively leveraging semantic knowledge from LLM."
    },
    {
        "date": "2025-10",
        "title": "Quantifying Distributional Robustness of Agentic Tool-Selection",
        "author": "Jehyeok Yeon, Isha Chaudhary, and Gagandeep Singh",
        "link": "http://arxiv.org/abs/2510.03992v1",
        "abstract": "Large language models (LLMs) are increasingly deployed in agentic systems\nwhere they map user intents to relevant external tools to fulfill a task. A\ncritical step in this process is tool selection, where a retriever first\nsurfaces candidate tools from a larger pool, after which the LLM selects the\nmost appropriate one. This pipeline presents an underexplored attack surface\nwhere errors in selection can lead to severe outcomes like unauthorized data\naccess or denial of service, all without modifying the agent's model or code.\nWhile existing evaluations measure task performance in benign settings, they\noverlook the specific vulnerabilities of the tool selection mechanism under\nadversarial conditions. To address this gap, we introduce ToolCert, the first\nstatistical framework that formally certifies tool selection robustness.\nToolCert models tool selection as a Bernoulli success process and evaluates it\nagainst a strong, adaptive attacker who introduces adversarial tools with\nmisleading metadata, and are iteratively refined based on the agent's previous\nchoices. By sampling these adversarial interactions, ToolCert produces a\nhigh-confidence lower bound on accuracy, formally quantifying the agent's\nworst-case performance. Our evaluation with ToolCert uncovers the severe\nfragility: under attacks injecting deceptive tools or saturating retrieval, the\ncertified accuracy bound drops near zero, an average performance drop of over\n60% compared to non-adversarial settings. For attacks targeting the retrieval\nand selection stages, the certified accuracy bound plummets to less than 20%\nafter just a single round of adversarial adaptation. ToolCert thus reveals\npreviously unexamined security threats inherent to tool selection and provides\na principled method to quantify an agent's robustness to such threats, a\nnecessary step for the safe deployment of agentic systems."
    },
    {
        "date": "2025-10",
        "title": "Domain-Adapted Granger Causality for Real-Time Cross-Slice Attack Attribution in 6G Networks",
        "author": "Minh K. Quan, and Pubudu N. Pathirana",
        "link": "http://arxiv.org/abs/2510.05165v1",
        "abstract": "Cross-slice attack attribution in 6G networks faces the fundamental challenge\nof distinguishing genuine causal relationships from spurious correlations in\nshared infrastructure environments. We propose a theoretically-grounded\ndomain-adapted Granger causality framework that integrates statistical causal\ninference with network-specific resource modeling for real-time attack\nattribution. Our approach addresses key limitations of existing methods by\nincorporating resource contention dynamics and providing formal statistical\nguarantees. Comprehensive evaluation on a production-grade 6G testbed with\n1,100 empirically-validated attack scenarios demonstrates 89.2% attribution\naccuracy with sub-100ms response time, representing a statistically significant\n10.1 percentage point improvement over state-of-the-art baselines. The\nframework provides interpretable causal explanations suitable for autonomous 6G\nsecurity orchestration."
    },
    {
        "date": "2025-10",
        "title": "BONSAI: Structure-exploiting robust Bayesian optimization for networked black-box systems under uncertainty",
        "author": "Akshay Kudva, and Joel A. Paulson",
        "link": "http://arxiv.org/abs/2510.03893v1",
        "abstract": "Optimal design under uncertainty remains a fundamental challenge in advancing\nreliable, next-generation process systems. Robust optimization (RO) offers a\nprincipled approach by safeguarding against worst-case scenarios across a range\nof uncertain parameters. However, traditional RO methods typically require\nknown problem structure, which limits their applicability to high-fidelity\nsimulation environments. To overcome these limitations, recent work has\nexplored robust Bayesian optimization (RBO) as a flexible alternative that can\naccommodate expensive, black-box objectives. Existing RBO methods, however,\ngenerally ignore available structural information and struggle to scale to\nhigh-dimensional settings. In this work, we introduce BONSAI (Bayesian\nOptimization of Network Systems under uncertAInty), a new RBO framework that\nleverages partial structural knowledge commonly available in simulation-based\nmodels. Instead of treating the objective as a monolithic black box, BONSAI\nrepresents it as a directed graph of interconnected white- and black-box\ncomponents, allowing the algorithm to utilize intermediate information within\nthe optimization process. We further propose a scalable Thompson sampling-based\nacquisition function tailored to the structured RO setting, which can be\nefficiently optimized using gradient-based methods. We evaluate BONSAI across a\ndiverse set of synthetic and real-world case studies, including applications in\nprocess systems engineering. Compared to existing simulation-based RO\nalgorithms, BONSAI consistently delivers more sample-efficient and\nhigher-quality robust solutions, highlighting its practical advantages for\nuncertainty-aware design in complex engineering systems."
    },
    {
        "date": "2025-10",
        "title": "Adversarial Agent Collaboration for C to Rust Translation",
        "author": "Tianyu Li, Ruishi Li, Bo Wang, Brandon Paulsen, Umang Mathur, and Prateek Saxena",
        "link": "http://arxiv.org/abs/2510.03879v1",
        "abstract": "Translating C to memory-safe languages, like Rust, prevents critical memory\nsafety vulnerabilities that are prevalent in legacy C software. Existing\napproaches for C to safe Rust translation, including LLM-assisted ones, do not\ngeneralize on larger (> 500 LoC) C codebases because they depend on complex\nprogram analyses that frequently break. In this work, we present ACToR\n(Adversarial C To Rust translator), a simple LLM agent-based approach. Inspired\nby GANs, ACToR pits a generator agent against a discriminator agent, which\ncollaborate to iteratively generate a Rust translation. On each iteration, the\ntranslator agent synthesizes and refines a Rust translation to pass an existing\nsuite of tests, and then the discriminator agent finds new failing tests. We\ndemonstrate that ACToR translates all of the 63 real-world command line\nutilities considered in our benchmarks, which have an average size of 485 lines\nof code, and it achieves over 90% test pass rate with zero human intervention.\nTo our knowledge, it is the first such system that reliably translates C\nprograms of this scale. Furthermore, ACToR improves translation correctness by\nup to 18.9% compared to baseline, non-adversarial approaches."
    },
    {
        "date": "2025-10",
        "title": "SDAKD: Student Discriminator Assisted Knowledge Distillation for Super-Resolution Generative Adversarial Networks",
        "author": "Nikolaos Kaparinos, and Vasileios Mezaris",
        "link": "http://arxiv.org/abs/2510.03870v1",
        "abstract": "Generative Adversarial Networks (GANs) achieve excellent performance in\ngenerative tasks, such as image super-resolution, but their computational\nrequirements make difficult their deployment on resource-constrained devices.\nWhile knowledge distillation is a promising research direction for GAN\ncompression, effectively training a smaller student generator is challenging\ndue to the capacity mismatch between the student generator and the teacher\ndiscriminator. In this work, we propose Student Discriminator Assisted\nKnowledge Distillation (SDAKD), a novel GAN distillation methodology that\nintroduces a student discriminator to mitigate this capacity mismatch. SDAKD\nfollows a three-stage training strategy, and integrates an adapted feature map\ndistillation approach in its last two training stages. We evaluated SDAKD on\ntwo well-performing super-resolution GANs, GCFSR and Real-ESRGAN. Our\nexperiments demonstrate consistent improvements over the baselines and SOTA GAN\nknowledge distillation methods. The SDAKD source code will be made openly\navailable upon acceptance of the paper."
    },
    {
        "date": "2025-10",
        "title": "Technical note on Fisher Information for Robust Federated Cross-Validation",
        "author": "Behraj Khan, and Tahir Qasim Syed",
        "link": "http://arxiv.org/abs/2510.03838v1",
        "abstract": "When training data are fragmented across batches or federated-learned across\ndifferent geographic locations, trained models manifest performance\ndegradation. That degradation partly owes to covariate shift induced by data\nhaving been fragmented across time and space and producing dissimilar empirical\ntraining distributions. Each fragment's distribution is slightly different to a\nhypothetical unfragmented training distribution of covariates, and to the\nsingle validation distribution. To address this problem, we propose Fisher\nInformation for Robust fEderated validation (\\textbf{FIRE}). This method\naccumulates fragmentation-induced covariate shift divergences from the global\ntraining distribution via an approximate Fisher information. That term, which\nwe prove to be a more computationally-tractable estimate, is then used as a\nper-fragment loss penalty, enabling scalable distribution alignment. FIRE\noutperforms importance weighting benchmarks by $5.1\\%$ at maximum and federated\nlearning (FL) benchmarks by up to $5.3\\%$ on shifted validation sets."
    },
    {
        "date": "2025-10",
        "title": "Towards Robust and Generalizable Continuous Space-Time Video Super-Resolution with Events",
        "author": "Shuoyan Wei, Feng Li, Shengeng Tang, Runmin Cong, Yao Zhao, Meng Wang, and Huihui Bai",
        "link": "http://arxiv.org/abs/2510.03833v1",
        "abstract": "Continuous space-time video super-resolution (C-STVSR) has garnered\nincreasing interest for its capability to reconstruct high-resolution and\nhigh-frame-rate videos at arbitrary spatial and temporal scales. However,\nprevailing methods often generalize poorly, producing unsatisfactory results\nwhen applied to out-of-distribution (OOD) scales. To overcome this limitation,\nwe present EvEnhancer, a novel approach that marries the unique properties of\nhigh temporal resolution and high dynamic range encapsulated in event streams\nto achieve robust and generalizable C-STVSR. Our approach incorporates\nevent-adapted synthesis that capitalizes on the spatiotemporal correlations\nbetween frames and events to capture long-term motion trajectories, enabling\nadaptive interpolation and fusion across space and time. This is then coupled\nwith a local implicit video transformer that integrates local implicit video\nneural function with cross-scale spatiotemporal attention to learn continuous\nvideo representations and generate plausible videos at arbitrary resolutions\nand frame rates. We further develop EvEnhancerPlus, which builds a controllable\nswitching mechanism that dynamically determines the reconstruction difficulty\nfor each spatiotemporal pixel based on local event statistics. This allows the\nmodel to adaptively route reconstruction along the most suitable pathways at a\nfine-grained pixel level, substantially reducing computational overhead while\nmaintaining excellent performance. Furthermore, we devise a cross-derivative\ntraining strategy that stabilizes the convergence of such a multi-pathway\nframework through staged cross-optimization. Extensive experiments demonstrate\nthat our method achieves state-of-the-art performance on both synthetic and\nreal-world datasets, while maintaining superior generalizability at OOD scales.\nThe code is available at https://github.com/W-Shuoyan/EvEnhancerPlus."
    },
    {
        "date": "2025-10",
        "title": "LIBERO-PRO: Towards Robust and Fair Evaluation of Vision-Language-Action Models Beyond Memorization",
        "author": "Xueyang Zhou, Yangming Xu, Guiyao Tie, Yongchao Chen, Guowen Zhang, Duanfeng Chu, Pan Zhou, and Lichao Sun",
        "link": "http://arxiv.org/abs/2510.03827v1",
        "abstract": "LIBERO has emerged as a widely adopted benchmark for evaluating\nVision-Language-Action (VLA) models; however, its current training and\nevaluation settings are problematic, often leading to inflated performance\nestimates and preventing fair model comparison. To address these issues, we\nintroduce LIBERO-PRO, an extended LIBERO benchmark that systematically\nevaluates model performance under reasonable perturbations across four\ndimensions: manipulated objects, initial states, task instructions, and\nenvironments. Experimental results reveal that, although existing models\nachieve over 90% accuracy under the standard LIBERO evaluation, their\nperformance collapses to 0.0% under our generalized setting. Crucially, this\ndiscrepancy exposes the models' reliance on rote memorization of action\nsequences and environment layouts from the training set, rather than genuine\ntask understanding or environmental perception. For instance, models persist in\nexecuting grasping actions when the target object is replaced with irrelevant\nitems, and their outputs remain unchanged even when given corrupted\ninstructions or even messy tokens. These findings expose the severe flaws in\ncurrent evaluation practices, and we call on the community to abandon\nmisleading methodologies in favor of robust assessments of model generalization\nand comprehension. Our code is available at:\nhttps://github.com/Zxy-MLlab/LIBERO-PRO."
    },
    {
        "date": "2025-10",
        "title": "Security Analysis of Ponzi Schemes in Ethereum Smart Contracts",
        "author": "Chunyi Zhang, Qinghong Wei, and Xiaoqi Li",
        "link": "http://arxiv.org/abs/2510.03819v1",
        "abstract": "The rapid advancement of blockchain technology has precipitated the\nwidespread adoption of Ethereum and smart contracts across a variety of\nsectors. However, this has also given rise to numerous fraudulent activities,\nwith many speculators embedding Ponzi schemes within smart contracts, resulting\nin significant financial losses for investors. Currently, there is a lack of\neffective methods for identifying and analyzing such new types of fraudulent\nactivities. This paper categorizes these scams into four structural types and\nexplores the intrinsic characteristics of Ponzi scheme contract source code\nfrom a program analysis perspective. The Mythril tool is employed to conduct\nstatic and dynamic analyses of representative cases, thereby revealing their\nvulnerabilities and operational mechanisms. Furthermore, this paper employs\nshell scripts and command patterns to conduct batch detection of open-source\nsmart contract code, thereby unveiling the common characteristics of Ponzi\nscheme smart contracts."
    },
    {
        "date": "2025-10",
        "title": "Robust Batched Bandits",
        "author": "Yunwen Guo, Yunlun Shu, Gongyi Zhuo, and Tianyu Wang",
        "link": "http://arxiv.org/abs/2510.03798v1",
        "abstract": "The batched multi-armed bandit (MAB) problem, in which rewards are collected\nin batches, is crucial for applications such as clinical trials. Existing\nresearch predominantly assumes light-tailed reward distributions, yet many\nreal-world scenarios, including clinical outcomes, exhibit heavy-tailed\ncharacteristics. This paper bridges this gap by proposing robust batched bandit\nalgorithms designed for heavy-tailed rewards, within both finite-arm and\nLipschitz-continuous settings. We reveal a surprising phenomenon: in the\ninstance-independent regime, as well as in the Lipschitz setting,\nheavier-tailed rewards necessitate a smaller number of batches to achieve\nnear-optimal regret. In stark contrast, for the instance-dependent setting, the\nrequired number of batches to attain near-optimal regret remains invariant with\nrespect to tail heaviness."
    },
    {
        "date": "2025-10",
        "title": "AdaptAuth: Multi-Layered Behavioral and Credential Analysis for a Secure and Adaptive Authentication Framework for Password Security",
        "author": "Tonmoy Ghosh",
        "link": "http://arxiv.org/abs/2510.09645v1",
        "abstract": "Password security has been compelled to evolve in response to the growing\ncomputational capabilities of modern systems. However, this evolution has often\nresulted in increasingly complex security practices that alienate users,\nleading to poor compliance and heightened vulnerability. Consequently,\nindividuals remain exposed to attackers through weak or improperly managed\npasswords, underscoring the urgent need for a comprehensive defense mechanism\nthat effectively addresses password-related risks and threats. In this paper,\nwe propose a multifaceted solution designed to revolutionize password security\nby integrating diverse attributes such as the Password Dissection Mechanism,\nDynamic Password Policy Mechanism, human behavioral patterns, device\ncharacteristics, network parameters, geographical context, and other relevant\nfactors. By leveraging learning-based models, our framework constructs detailed\nuser profiles capable of recognizing individuals and preventing nearly all\nforms of unauthorized access or device possession. The proposed framework\nenhances the usability-security paradigm by offering stronger protection than\nexisting standards while simultaneously engaging users in the policy-setting\nprocess through a novel, adaptive approach."
    },
    {
        "date": "2025-10",
        "title": "LoRA Patching: Exposing the Fragility of Proactive Defenses against Deepfakes",
        "author": "Zuomin Qu, Yimao Guo, Qianyue Hu, and Wei Lu",
        "link": "http://arxiv.org/abs/2510.03747v1",
        "abstract": "Deepfakes pose significant societal risks, motivating the development of\nproactive defenses that embed adversarial perturbations in facial images to\nprevent manipulation. However, in this paper, we show that these preemptive\ndefenses often lack robustness and reliability. We propose a novel approach,\nLow-Rank Adaptation (LoRA) patching, which injects a plug-and-play LoRA patch\ninto Deepfake generators to bypass state-of-the-art defenses. A learnable\ngating mechanism adaptively controls the effect of the LoRA patch and prevents\ngradient explosions during fine-tuning. We also introduce a Multi-Modal Feature\nAlignment (MMFA) loss, encouraging the features of adversarial outputs to align\nwith those of the desired outputs at the semantic level. Beyond bypassing, we\npresent defensive LoRA patching, embedding visible warnings in the outputs as a\ncomplementary solution to mitigate this newly identified security\nvulnerability. With only 1,000 facial examples and a single epoch of\nfine-tuning, LoRA patching successfully defeats multiple proactive defenses.\nThese results reveal a critical weakness in current paradigms and underscore\nthe need for more robust Deepfake defense strategies. Our code is available at\nhttps://github.com/ZOMIN28/LoRA-Patching."
    },
    {
        "date": "2025-10",
        "title": "Securing Operating Systems Through Fine-grained Kernel Access Limitation for IoT Systems",
        "author": "Dongyang Zhan, Zhaofeng Yu, Xiangzhan Yu, Hongli Zhang, Lin Ye, and Likun Liu",
        "link": "http://arxiv.org/abs/2510.03737v1",
        "abstract": "With the development of Internet of Things (IoT), it is gaining a lot of\nattention. It is important to secure the embedded systems with low overhead.\nThe Linux Seccomp is widely used by developers to secure the kernels by\nblocking the access of unused syscalls, which introduces less overhead.\nHowever, there are no systematic Seccomp configuration approaches for IoT\napplications without the help of developers. In addition, the existing Seccomp\nconfiguration approaches are coarse-grained, which cannot analyze and limit the\nsyscall arguments. In this paper, a novel static dependent syscall analysis\napproach for embedded applications is proposed, which can obtain all of the\npossible dependent syscalls and the corresponding arguments of the target\napplications. So, a fine-grained kernel access limitation can be performed for\nthe IoT applications. To this end, the mappings between dynamic library APIs\nand syscalls according with their arguments are built, by analyzing the control\nflow graphs and the data dependency relationships of the dynamic libraries. To\nthe best of our knowledge, this is the first work to generate the fine-grained\nSeccomp profile for embedded applications."
    },
    {
        "date": "2025-10",
        "title": "Shrinking the Kernel Attack Surface Through Static and Dynamic Syscall Limitation",
        "author": "Dongyang Zhan, Zhaofeng Yu, Xiangzhan Yu, Hongli Zhang, and Lin Ye",
        "link": "http://arxiv.org/abs/2510.03720v1",
        "abstract": "Linux Seccomp is widely used by the program developers and the system\nmaintainers to secure the operating systems, which can block unused syscalls\nfor different applications and containers to shrink the attack surface of the\noperating systems. However, it is difficult to configure the whitelist of a\ncontainer or application without the help of program developers. Docker\ncontainers block about only 50 syscalls by default, and lots of unblocked\nuseless syscalls introduce a big kernel attack surface. To obtain the dependent\nsyscalls, dynamic tracking is a straight-forward approach but it cannot get the\nfull syscall list. Static analysis can construct an over-approximated syscall\nlist, but the list contains many false positives. In this paper, a systematic\ndependent syscall analysis approach, sysverify, is proposed by combining static\nanalysis and dynamic verification together to shrink the kernel attack surface.\nThe semantic gap between the binary executables and syscalls is bridged by\nanalyzing the binary and the source code, which builds the mapping between the\nlibrary APIs and syscalls systematically. To further reduce the attack surface\nat best effort, we propose a dynamic verification approach to intercept and\nanalyze the security of the invocations of indirect-call-related or rarely\ninvoked syscalls with low overhead."
    },
    {
        "date": "2025-10",
        "title": "Backdoor-Powered Prompt Injection Attacks Nullify Defense Methods",
        "author": "Yulin Chen, Haoran Li, Yuan Sui, Yangqiu Song, and Bryan Hooi",
        "link": "http://arxiv.org/abs/2510.03705v1",
        "abstract": "With the development of technology, large language models (LLMs) have\ndominated the downstream natural language processing (NLP) tasks. However,\nbecause of the LLMs' instruction-following abilities and inability to\ndistinguish the instructions in the data content, such as web pages from search\nengines, the LLMs are vulnerable to prompt injection attacks. These attacks\ntrick the LLMs into deviating from the original input instruction and executing\nthe attackers' target instruction. Recently, various instruction hierarchy\ndefense strategies are proposed to effectively defend against prompt injection\nattacks via fine-tuning. In this paper, we explore more vicious attacks that\nnullify the prompt injection defense methods, even the instruction hierarchy:\nbackdoor-powered prompt injection attacks, where the attackers utilize the\nbackdoor attack for prompt injection attack purposes. Specifically, the\nattackers poison the supervised fine-tuning samples and insert the backdoor\ninto the model. Once the trigger is activated, the backdoored model executes\nthe injected instruction surrounded by the trigger. We construct a benchmark\nfor comprehensive evaluation. Our experiments demonstrate that backdoor-powered\nprompt injection attacks are more harmful than previous prompt injection\nattacks, nullifying existing prompt injection defense methods, even the\ninstruction hierarchy techniques."
    },
    {
        "date": "2025-10",
        "title": "REG: A Regularization Optimizer for Robust Training Dynamics",
        "author": "Zehua Liu, Han Wu, Xiaojin Fu, Shuqi Liu, Xiongwei Han, Tao Zhong, and Mingxuan Yuan",
        "link": "http://arxiv.org/abs/2510.03691v1",
        "abstract": "Optimizers are crucial for the efficient training of Large Language Models\n(LLMs). While AdamW is the de facto standard, recent structure-aware optimizers\nlike Muon have emerged, which regularize gradient updates by operating on\nentire weight matrices. The Muon optimizer balances the gradient updates along\nall the directions. However, Muon's reliance on the matrix sign function can\nlead to training instability, exhibits incompatibility when fine-tuning models\npre-trained with AdamW. To address these limitations, we propose \\textbf{REG},\na novel optimizer that replaces Muon's aggressive matrix sign operator with the\nRow-and-Column-Scaling (RACS) operator. Theoretically grounded in balancing a\nmatrix, the RACS operator regularizes the update steps in a less drastic\nmanner, making it simpler to implement and more compatible with established\ntraining dynamics. Through extensive empirical experiments on LLM training, we\ndemonstrate that our REG optimizer not only achieves superior performance and\nstability over AdamW, but also maintains consistency with the AdamW training\nparadigm. This consistency is particularly evident during the fine-tuning\nstage, where REG optimizer avoids the performance degradation observed with\nMuon."
    },
    {
        "date": "2025-10",
        "title": "From Theory to Practice: Evaluating Data Poisoning Attacks and Defenses in In-Context Learning on Social Media Health Discourse",
        "author": "Rabeya Amin Jhuma, and Mostafa Mohaimen Akand Faisal",
        "link": "http://arxiv.org/abs/2510.03636v1",
        "abstract": "This study explored how in-context learning (ICL) in large language models\ncan be disrupted by data poisoning attacks in the setting of public health\nsentiment analysis. Using tweets of Human Metapneumovirus (HMPV), small\nadversarial perturbations such as synonym replacement, negation insertion, and\nrandomized perturbation were introduced into the support examples. Even these\nminor manipulations caused major disruptions, with sentiment labels flipping in\nup to 67% of cases. To address this, a Spectral Signature Defense was applied,\nwhich filtered out poisoned examples while keeping the data's meaning and\nsentiment intact. After defense, ICL accuracy remained steady at around 46.7%,\nand logistic regression validation reached 100% accuracy, showing that the\ndefense successfully preserved the dataset's integrity. Overall, the findings\nextend prior theoretical studies of ICL poisoning to a practical, high-stakes\nsetting in public health discourse analysis, highlighting both the risks and\npotential defenses for robust LLM deployment. This study also highlights the\nfragility of ICL under attack and the value of spectral defenses in making AI\nsystems more reliable for health-related social media monitoring."
    },
    {
        "date": "2025-10",
        "title": "Explainable but Vulnerable: Adversarial Attacks on XAI Explanation in Cybersecurity Applications",
        "author": "Maraz Mia, and Mir Mehedi A. Pritom",
        "link": "http://arxiv.org/abs/2510.03623v1",
        "abstract": "Explainable Artificial Intelligence (XAI) has aided machine learning (ML)\nresearchers with the power of scrutinizing the decisions of the black-box\nmodels. XAI methods enable looking deep inside the models' behavior, eventually\ngenerating explanations along with a perceived trust and transparency. However,\ndepending on any specific XAI method, the level of trust can vary. It is\nevident that XAI methods can themselves be a victim of post-adversarial attacks\nthat manipulate the expected outcome from the explanation module. Among such\nattack tactics, fairwashing explanation (FE), manipulation explanation (ME),\nand backdoor-enabled manipulation attacks (BD) are the notable ones. In this\npaper, we try to understand these adversarial attack techniques, tactics, and\nprocedures (TTPs) on explanation alteration and thus the effect on the model's\ndecisions. We have explored a total of six different individual attack\nprocedures on post-hoc explanation methods such as SHAP (SHapley Additive\nexPlanations), LIME (Local Interpretable Model-agnostic Explanation), and IG\n(Integrated Gradients), and investigated those adversarial attacks in\ncybersecurity applications scenarios such as phishing, malware, intrusion, and\nfraudulent website detection. Our experimental study reveals the actual\neffectiveness of these attacks, thus providing an urgency for immediate\nattention to enhance the resiliency of XAI methods and their applications."
    },
    {
        "date": "2025-10",
        "title": "Machine Unlearning Meets Adversarial Robustness via Constrained Interventions on LLMs",
        "author": "Fatmazohra Rezkellah, and Ramzi Dakhmouche",
        "link": "http://arxiv.org/abs/2510.03567v1",
        "abstract": "With the increasing adoption of Large Language Models (LLMs), more\ncustomization is needed to ensure privacy-preserving and safe generation. We\naddress this objective from two critical aspects: unlearning of sensitive\ninformation and robustness to jail-breaking attacks. We investigate various\nconstrained optimization formulations that address both aspects in a\n\\emph{unified manner}, by finding the smallest possible interventions on LLM\nweights that either make a given vocabulary set unreachable or embed the LLM\nwith robustness to tailored attacks by shifting part of the weights to a\n\\emph{safer} region. Beyond unifying two key properties, this approach\ncontrasts with previous work in that it doesn't require an oracle classifier\nthat is typically not available or represents a computational overhead.\nSurprisingly, we find that the simplest point-wise constraint-based\nintervention we propose leads to better performance than max-min interventions,\nwhile having a lower computational cost. Comparison against state-of-the-art\ndefense methods demonstrates superior performance of the proposed approach."
    },
    {
        "date": "2025-10",
        "title": "A Quantum-Secure Voting Framework Using QKD, Dual-Key Symmetric Encryption, and Verifiable Receipts",
        "author": "Taha M. Mahmoud, and Naima Kaabouch",
        "link": "http://arxiv.org/abs/2510.03489v1",
        "abstract": "Electronic voting systems face growing risks from cyberattacks and data\nbreaches, which are expected to intensify with the advent of quantum computing.\nTo address these challenges, we introduce a quantum-secure voting framework\nthat integrates Quantum Key Distribution (QKD), Dual-Key Symmetric Encryption,\nand verifiable receipt mechanisms to strengthen the privacy, integrity, and\nreliability of the voting process. The framework enables voters to establish\nencryption keys securely, cast encrypted ballots, and verify their votes\nthrough receipt-based confirmation, all without exposing the vote contents. To\nevaluate performance, we simulate both quantum and classical communication\nchannels using the Message Queuing Telemetry Transport (MQTT) protocol. Results\ndemonstrate that the system can process large numbers of votes efficiently with\nlow latency and minimal error rates. This approach offers a scalable and\npractical path toward secure, transparent, and verifiable electronic voting in\nthe quantum era."
    },
    {
        "date": "2025-10",
        "title": "Security Analysis and Threat Modeling of Research Management Applications [Extended Version]",
        "author": "Boniface M. Sindala, and Ragib Hasan",
        "link": "http://arxiv.org/abs/2510.03407v1",
        "abstract": "Research management applications (RMA) are widely used in clinical research\nenvironments to collect, transmit, analyze, and store sensitive data. This data\nis so valuable making RMAs susceptible to security threats. This analysis,\nanalyzes RMAs' security, focusing on Research Electronic Data Capture (REDCap)\nas an example. We explore the strengths and vulnerabilities within RMAs by\nevaluating the architecture, data flow, and security features. We identify and\nassess potential risks using the MITRE ATT\\&CK framework and STRIDE model. We\nassess REDCap's defenses against common attack vectors focusing on security to\nprovide confidentiality, integrity, availability, non-repudiation, and\nauthentication. We conclude by proposing recommendations for enhancing the\nsecurity of RMAs, ensuring that critical research data remains protected\nwithout compromising usability. This research aims to contribute towards a more\nsecure framework for managing sensitive information in research-intensive\nenvironments."
    },
    {
        "date": "2025-10",
        "title": "Test-Time Defense Against Adversarial Attacks via Stochastic Resonance of Latent Ensembles",
        "author": "Dong Lao, Yuxiang Zhang, Haniyeh Ehsani Oskouie, Yangchao Wu, Alex Wong, and Stefano Soatto",
        "link": "http://arxiv.org/abs/2510.03224v1",
        "abstract": "We propose a test-time defense mechanism against adversarial attacks:\nimperceptible image perturbations that significantly alter the predictions of a\nmodel. Unlike existing methods that rely on feature filtering or smoothing,\nwhich can lead to information loss, we propose to \"combat noise with noise\" by\nleveraging stochastic resonance to enhance robustness while minimizing\ninformation loss. Our approach introduces small translational perturbations to\nthe input image, aligns the transformed feature embeddings, and aggregates them\nbefore mapping back to the original reference image. This can be expressed in a\nclosed-form formula, which can be deployed on diverse existing network\narchitectures without introducing additional network modules or fine-tuning for\nspecific attack types. The resulting method is entirely training-free,\narchitecture-agnostic, and attack-agnostic. Empirical results show\nstate-of-the-art robustness on image classification and, for the first time,\nestablish a generic test-time defense for dense prediction tasks, including\nstereo matching and optical flow, highlighting the method's versatility and\npracticality. Specifically, relative to clean (unperturbed) performance, our\nmethod recovers up to 68.1% of the accuracy loss on image classification, 71.9%\non stereo matching, and 29.2% on optical flow under various types of\nadversarial attacks."
    },
    {
        "date": "2025-10",
        "title": "MM-Nav: Multi-View VLA Model for Robust Visual Navigation via Multi-Expert Learning",
        "author": "Tianyu Xu, Jiawei Chen, Jiazhao Zhang, Wenyao Zhang, Zekun Qi, Minghan Li, Zhizheng Zhang, and He Wang",
        "link": "http://arxiv.org/abs/2510.03142v1",
        "abstract": "Visual navigation policy is widely regarded as a promising direction, as it\nmimics humans by using egocentric visual observations for navigation. However,\noptical information of visual observations is difficult to be explicitly\nmodeled like LiDAR point clouds or depth maps, which subsequently requires\nintelligent models and large-scale data. To this end, we propose to leverage\nthe intelligence of the Vision-Language-Action (VLA) model to learn diverse\nnavigation capabilities from synthetic expert data in a teacher-student manner.\nSpecifically, we implement the VLA model, MM-Nav, as a multi-view VLA (with 360\nobservations) based on pretrained large language models and visual foundation\nmodels. For large-scale navigation data, we collect expert data from three\nreinforcement learning (RL) experts trained with privileged depth information\nin three challenging tailor-made environments for different navigation\ncapabilities: reaching, squeezing, and avoiding. We iteratively train our VLA\nmodel using data collected online from RL experts, where the training ratio is\ndynamically balanced based on performance on individual capabilities. Through\nextensive experiments in synthetic environments, we demonstrate that our model\nachieves strong generalization capability. Moreover, we find that our student\nVLA model outperforms the RL teachers, demonstrating the synergistic effect of\nintegrating multiple capabilities. Extensive real-world experiments further\nconfirm the effectiveness of our method."
    },
    {
        "date": "2025-10",
        "title": "A Robust Clustered Federated Learning Approach for Non-IID Data with Quantity Skew",
        "author": "Michael Ben Ali, Imen Megdiche, Andr\u00e9 Peninou, and Olivier Teste",
        "link": "http://arxiv.org/abs/2510.03380v1",
        "abstract": "Federated Learning (FL) is a decentralized paradigm that enables a\nclient-server architecture to collaboratively train a global Artificial\nIntelligence model without sharing raw data, thereby preserving privacy. A key\nchallenge in FL is Non-IID data. Quantity Skew (QS) is a particular problem of\nNon-IID, where clients hold highly heterogeneous data volumes. Clustered\nFederated Learning (CFL) is an emergent variant of FL that presents a promising\nsolution to Non-IID problem. It improves models' performance by grouping\nclients with similar data distributions into clusters. CFL methods generally\nfall into two operating strategies. In the first strategy, clients select the\ncluster that minimizes the local training loss. In the second strategy, the\nserver groups clients based on local model similarities. However, most CFL\nmethods lack systematic evaluation under QS but present significant challenges\nbecause of it. In this paper, we present two main contributions. The first one\nis an evaluation of state-of-the-art CFL algorithms under various Non-IID\nsettings, applying multiple QS scenarios to assess their robustness. Our second\ncontribution is a novel iterative CFL algorithm, named CORNFLQS, which proposes\nan optimal coordination between both operating strategies of CFL. Our approach\nis robust against the different variations of QS settings. We conducted\nintensive experiments on six image classification datasets, resulting in 270\nNon-IID configurations. The results show that CORNFLQS achieves the highest\naverage ranking in both accuracy and clustering quality, as well as strong\nrobustness to QS perturbations. Overall, our approach outperforms actual CFL\nalgorithms."
    },
    {
        "date": "2025-10",
        "title": "InsideOut: An EfficientNetV2-S Based Deep Learning Framework for Robust Multi-Class Facial Emotion Recognition",
        "author": "Ahsan Farabi, Israt Khandaker, Ibrahim Khalil Shanto, Md Abdul Ahad Minhaz, and Tanisha Zaman",
        "link": "http://arxiv.org/abs/2510.03066v1",
        "abstract": "Facial Emotion Recognition (FER) is a key task in affective computing,\nenabling applications in human-computer interaction, e-learning, healthcare,\nand safety systems. Despite advances in deep learning, FER remains challenging\ndue to occlusions, illumination and pose variations, subtle intra-class\ndifferences, and dataset imbalance that hinders recognition of minority\nemotions. We present InsideOut, a reproducible FER framework built on\nEfficientNetV2-S with transfer learning, strong data augmentation, and\nimbalance-aware optimization. The approach standardizes FER2013 images, applies\nstratified splitting and augmentation, and fine-tunes a lightweight\nclassification head with class-weighted loss to address skewed distributions.\nInsideOut achieves 62.8% accuracy with a macro averaged F1 of 0.590 on FER2013,\nshowing competitive results compared to conventional CNN baselines. The novelty\nlies in demonstrating that efficient architectures, combined with tailored\nimbalance handling, can provide practical, transparent, and reproducible FER\nsolutions."
    },
    {
        "date": "2025-10",
        "title": "Learning Robust Diffusion Models from Imprecise Supervision",
        "author": "Dong-Dong Wu, Jiacheng Cui, Wei Wang, Zhiqiang Shen, and Masashi Sugiyama",
        "link": "http://arxiv.org/abs/2510.03016v2",
        "abstract": "Conditional diffusion models have achieved remarkable success in various\ngenerative tasks recently, but their training typically relies on large-scale\ndatasets that inevitably contain imprecise information in conditional inputs.\nSuch supervision, often stemming from noisy, ambiguous, or incomplete labels,\nwill cause condition mismatch and degrade generation quality. To address this\nchallenge, we propose DMIS, a unified framework for training robust Diffusion\nModels from Imprecise Supervision, which is the first systematic study within\ndiffusion models. Our framework is derived from likelihood maximization and\ndecomposes the objective into generative and classification components: the\ngenerative component models imprecise-label distributions, while the\nclassification component leverages a diffusion classifier to infer\nclass-posterior probabilities, with its efficiency further improved by an\noptimized timestep sampling strategy. Extensive experiments on diverse forms of\nimprecise supervision, covering tasks of image generation, weakly supervised\nlearning, and noisy dataset condensation demonstrate that DMIS consistently\nproduces high-quality and class-discriminative samples."
    },
    {
        "date": "2025-10",
        "title": "Not every day is a sunny day: Synthetic cloud injection for deep land cover segmentation robustness evaluation across data sources",
        "author": "Sara Mobsite, Renaud Hostache, Laure Berti Equille, Emmanuel Roux, and Joris Guerin",
        "link": "http://arxiv.org/abs/2510.03006v1",
        "abstract": "Supervised deep learning for land cover semantic segmentation (LCS) relies on\nlabeled satellite data. However, most existing Sentinel-2 datasets are\ncloud-free, which limits their usefulness in tropical regions where clouds are\ncommon. To properly evaluate the extent of this problem, we developed a cloud\ninjection algorithm that simulates realistic cloud cover, allowing us to test\nhow Sentinel-1 radar data can fill in the gaps caused by cloud-obstructed\noptical imagery. We also tackle the issue of losing spatial and/or spectral\ndetails during encoder downsampling in deep networks. To mitigate this loss, we\npropose a lightweight method that injects Normalized Difference Indices (NDIs)\ninto the final decoding layers, enabling the model to retain key spatial\nfeatures with minimal additional computation. Injecting NDIs enhanced land\ncover segmentation performance on the DFC2020 dataset, yielding improvements of\n1.99% for U-Net and 2.78% for DeepLabV3 on cloud-free imagery. Under\ncloud-covered conditions, incorporating Sentinel-1 data led to significant\nperformance gains across all models compared to using optical data alone,\nhighlighting the effectiveness of radar-optical fusion in challenging\natmospheric scenarios."
    },
    {
        "date": "2025-10",
        "title": "Untargeted Jailbreak Attack",
        "author": "Xinzhe Huang, Wenjing Hu, Tianhang Zheng, Kedong Xiu, Xiaojun Jia, Di Wang, Zhan Qin, and Kui Ren",
        "link": "http://arxiv.org/abs/2510.02999v1",
        "abstract": "Existing gradient-based jailbreak attacks on Large Language Models (LLMs),\nsuch as Greedy Coordinate Gradient (GCG) and COLD-Attack, typically optimize\nadversarial suffixes to align the LLM output with a predefined target response.\nHowever, by restricting the optimization objective as inducing a predefined\ntarget, these methods inherently constrain the adversarial search space, which\nlimit their overall attack efficacy. Furthermore, existing methods typically\nrequire a large number of optimization iterations to fulfill the large gap\nbetween the fixed target and the original model response, resulting in low\nattack efficiency.\n  To overcome the limitations of targeted jailbreak attacks, we propose the\nfirst gradient-based untargeted jailbreak attack (UJA), aiming to elicit an\nunsafe response without enforcing any predefined patterns. Specifically, we\nformulate an untargeted attack objective to maximize the unsafety probability\nof the LLM response, which can be quantified using a judge model. Since the\nobjective is non-differentiable, we further decompose it into two\ndifferentiable sub-objectives for optimizing an optimal harmful response and\nthe corresponding adversarial prompt, with a theoretical analysis to validate\nthe decomposition. In contrast to targeted jailbreak attacks, UJA's\nunrestricted objective significantly expands the search space, enabling a more\nflexible and efficient exploration of LLM vulnerabilities.Extensive evaluations\ndemonstrate that \\textsc{UJA} can achieve over 80\\% attack success rates\nagainst recent safety-aligned LLMs with only 100 optimization iterations,\noutperforming the state-of-the-art gradient-based attacks such as I-GCG and\nCOLD-Attack by over 20\\%."
    }
]