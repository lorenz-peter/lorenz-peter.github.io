[
    {
        "date": "2025-05",
        "title": "RevealNet: Distributed Traffic Correlation for Attack Attribution on Programmable Networks",
        "author": "Gurjot Singh, Alim Dhanani, and Diogo Barradas",
        "link": "http://arxiv.org/abs/2505.00618v1",
        "abstract": "Network attackers have increasingly resorted to proxy chains, VPNs, and\nanonymity networks to conceal their activities. To tackle this issue, past\nresearch has explored the applicability of traffic correlation techniques to\nperform attack attribution, i.e., to identify an attacker's true network\nlocation. However, current traffic correlation approaches rely on\nwell-provisioned and centralized systems that ingest flows from multiple\nnetwork probes to compute correlation scores. Unfortunately, this makes\ncorrelation efforts scale poorly for large high-speed networks.\n  In this paper, we propose RevealNet, a decentralized framework for attack\nattribution that orchestrates a fleet of P4-programmable switches to perform\ntraffic correlation. RevealNet builds on a set of correlation primitives\ninspired by prior work on computing and comparing flow sketches -- compact\nsummaries of flows' key characteristics -- to enable efficient, distributed,\nin-network traffic correlation. Our evaluation suggests that RevealNet achieves\ncomparable accuracy to centralized attack attribution systems while\nsignificantly reducing both the computational complexity and bandwidth\noverheads imposed by correlation tasks."
    },
    {
        "date": "2025-05",
        "title": "A Novel Feature-Aware Chaotic Image Encryption Scheme For Data Security and Privacy in IoT and Edge Networks",
        "author": "Muhammad Shahbaz Khan, Ahmed Al-Dubai, Jawad Ahmad, Nikolaos Pitropakis, and Baraq Ghaleb",
        "link": "http://arxiv.org/abs/2505.00593v1",
        "abstract": "The security of image data in the Internet of Things (IoT) and edge networks\nis crucial due to the increasing deployment of intelligent systems for\nreal-time decision-making. Traditional encryption algorithms such as AES and\nRSA are computationally expensive for resource-constrained IoT devices and\nineffective for large-volume image data, leading to inefficiencies in\nprivacy-preserving distributed learning applications. To address these\nconcerns, this paper proposes a novel Feature-Aware Chaotic Image Encryption\nscheme that integrates Feature-Aware Pixel Segmentation (FAPS) with Chaotic\nChain Permutation and Confusion mechanisms to enhance security while\nmaintaining efficiency. The proposed scheme consists of three stages: (1) FAPS,\nwhich extracts and reorganizes pixels based on high and low edge intensity\nfeatures for correlation disruption; (2) Chaotic Chain Permutation, which\nemploys a logistic chaotic map with SHA-256-based dynamically updated keys for\nblock-wise permutation; and (3) Chaotic chain Confusion, which utilises\ndynamically generated chaotic seed matrices for bitwise XOR operations.\nExtensive security and performance evaluations demonstrate that the proposed\nscheme significantly reduces pixel correlation -- almost zero, achieves high\nentropy values close to 8, and resists differential cryptographic attacks. The\noptimum design of the proposed scheme makes it suitable for real-time\ndeployment in resource-constrained environments."
    },
    {
        "date": "2025-05",
        "title": "A Robust Deep Networks based Multi-Object MultiCamera Tracking System for City Scale Traffic",
        "author": "Muhammad Imran Zaman, Usama Ijaz Bajwa, Gulshan Saleem, and Rana Hammad Raza",
        "link": "http://arxiv.org/abs/2505.00534v1",
        "abstract": "Vision sensors are becoming more important in Intelligent Transportation\nSystems (ITS) for traffic monitoring, management, and optimization as the\nnumber of network cameras continues to rise. However, manual object tracking\nand matching across multiple non-overlapping cameras pose significant\nchallenges in city-scale urban traffic scenarios. These challenges include\nhandling diverse vehicle attributes, occlusions, illumination variations,\nshadows, and varying video resolutions. To address these issues, we propose an\nefficient and cost-effective deep learning-based framework for Multi-Object\nMulti-Camera Tracking (MO-MCT). The proposed framework utilizes Mask R-CNN for\nobject detection and employs Non-Maximum Suppression (NMS) to select target\nobjects from overlapping detections. Transfer learning is employed for\nre-identification, enabling the association and generation of vehicle tracklets\nacross multiple cameras. Moreover, we leverage appropriate loss functions and\ndistance measures to handle occlusion, illumination, and shadow challenges. The\nfinal solution identification module performs feature extraction using\nResNet-152 coupled with Deep SORT based vehicle tracking. The proposed\nframework is evaluated on the 5th AI City Challenge dataset (Track 3),\ncomprising 46 camera feeds. Among these 46 camera streams, 40 are used for\nmodel training and validation, while the remaining six are utilized for model\ntesting. The proposed framework achieves competitive performance with an IDF1\nscore of 0.8289, and precision and recall scores of 0.9026 and 0.8527\nrespectively, demonstrating its effectiveness in robust and accurate vehicle\ntracking."
    },
    {
        "date": "2025-05",
        "title": "KeySync: A Robust Approach for Leakage-free Lip Synchronization in High Resolution",
        "author": "Antoni Bigata, Rodrigo Mira, Stella Bounareli, Micha\u0142 Stypu\u0142kowski, Konstantinos Vougioukas, Stavros Petridis, and Maja Pantic",
        "link": "http://arxiv.org/abs/2505.00497v1",
        "abstract": "Lip synchronization, known as the task of aligning lip movements in an\nexisting video with new input audio, is typically framed as a simpler variant\nof audio-driven facial animation. However, as well as suffering from the usual\nissues in talking head generation (e.g., temporal consistency), lip\nsynchronization presents significant new challenges such as expression leakage\nfrom the input video and facial occlusions, which can severely impact\nreal-world applications like automated dubbing, but are often neglected in\nexisting works. To address these shortcomings, we present KeySync, a two-stage\nframework that succeeds in solving the issue of temporal consistency, while\nalso incorporating solutions for leakage and occlusions using a carefully\ndesigned masking strategy. We show that KeySync achieves state-of-the-art\nresults in lip reconstruction and cross-synchronization, improving visual\nquality and reducing expression leakage according to LipLeak, our novel leakage\nmetric. Furthermore, we demonstrate the effectiveness of our new masking\napproach in handling occlusions and validate our architectural choices through\nseveral ablation studies. Code and model weights can be found at\nhttps://antonibigata.github.io/KeySync."
    },
    {
        "date": "2025-05",
        "title": "Analysis of the vulnerability of machine learning regression models to adversarial attacks using data from 5G wireless networks",
        "author": "Leonid Legashev, Artur Zhigalov, and Denis Parfenov",
        "link": "http://arxiv.org/abs/2505.00487v1",
        "abstract": "This article describes the process of creating a script and conducting an\nanalytical study of a dataset using the DeepMIMO emulator. An advertorial\nattack was carried out using the FGSM method to maximize the gradient. A\ncomparison is made of the effectiveness of binary classifiers in the task of\ndetecting distorted data. The dynamics of changes in the quality indicators of\nthe regression model were analyzed in conditions without adversarial attacks,\nduring an adversarial attack and when the distorted data was isolated. It is\nshown that an adversarial FGSM attack with gradient maximization leads to an\nincrease in the value of the MSE metric by 33% and a decrease in the R2\nindicator by 10% on average. The LightGBM binary classifier effectively\nidentifies data with adversarial anomalies with 98% accuracy. Regression\nmachine learning models are susceptible to adversarial attacks, but rapid\nanalysis of network traffic and data transmitted over the network makes it\npossible to identify malicious activity"
    },
    {
        "date": "2025-05",
        "title": "Decentralized Vulnerability Disclosure via Permissioned Blockchain: A Secure, Transparent Alternative to Centralized CVE Management",
        "author": "Novruz Amirov, and Kemal Bicakci",
        "link": "http://arxiv.org/abs/2505.00480v1",
        "abstract": "This paper proposes a decentralized, blockchain-based system for the\npublication of Common Vulnerabilities and Exposures (CVEs), aiming to mitigate\nthe limitations of the current centralized model primarily overseen by MITRE.\nThe proposed architecture leverages a permissioned blockchain, wherein only\nauthenticated CVE Numbering Authorities (CNAs) are authorized to submit\nentries. This ensures controlled write access while preserving public\ntransparency. By incorporating smart contracts, the system supports key\nfeatures such as embargoed disclosures and decentralized governance. We\nevaluate the proposed model in comparison with existing practices, highlighting\nits advantages in transparency, trust decentralization, and auditability. A\nprototype implementation using Hyperledger Fabric is presented to demonstrate\nthe feasibility of the approach, along with a discussion of its implications\nfor the future of vulnerability disclosure."
    },
    {
        "date": "2025-05",
        "title": "The Invisible Threat: Evaluating the Vulnerability of Cross-Spectral Face Recognition to Presentation Attacks",
        "author": "Anjith George, and Sebastien Marcel",
        "link": "http://arxiv.org/abs/2505.00380v1",
        "abstract": "Cross-spectral face recognition systems are designed to enhance the\nperformance of facial recognition systems by enabling cross-modal matching\nunder challenging operational conditions. A particularly relevant application\nis the matching of near-infrared (NIR) images to visible-spectrum (VIS) images,\nenabling the verification of individuals by comparing NIR facial captures\nacquired with VIS reference images. The use of NIR imaging offers several\nadvantages, including greater robustness to illumination variations, better\nvisibility through glasses and glare, and greater resistance to presentation\nattacks. Despite these claimed benefits, the robustness of NIR-based systems\nagainst presentation attacks has not been systematically studied in the\nliterature. In this work, we conduct a comprehensive evaluation into the\nvulnerability of NIR-VIS cross-spectral face recognition systems to\npresentation attacks. Our empirical findings indicate that, although these\nsystems exhibit a certain degree of reliability, they remain vulnerable to\nspecific attacks, emphasizing the need for further research in this area."
    },
    {
        "date": "2025-05",
        "title": "Vehicular Communication Security: Multi-Channel and Multi-Factor Authentication",
        "author": "Marco De Vincenzi, Shuyang Sun, Chen Bo Calvin Zhang, Manuel Garcia, Shaozu Ding, Chiara Bodei, Ilaria Matteucci, and Dajiang Suo",
        "link": "http://arxiv.org/abs/2505.00340v1",
        "abstract": "Secure and reliable communications are crucial for Intelligent Transportation\nSystems (ITSs), where Vehicle-to-Infrastructure (V2I) communication plays a key\nrole in enabling mobility-enhancing and safety-critical services. Current V2I\nauthentication relies on credential-based methods over wireless\nNon-Line-of-Sight (NLOS) channels, leaving them exposed to remote impersonation\nand proximity attacks. To mitigate these risks, we propose a unified\nMulti-Channel, Multi-Factor Authentication (MFA) scheme that combines NLOS\ncryptographic credentials with a Line-of-Sight (LOS) visual channel. Our\napproach leverages a challenge-response security paradigm: the infrastructure\nissues challenges and the vehicle's headlights respond by flashing a structured\nsequence containing encoded security data. Deep learning models on the\ninfrastructure side then decode the embedded information to authenticate the\nvehicle. Real-world experimental evaluations demonstrate high test accuracy,\nreaching an average of 95% and 96.6%, respectively, under various lighting,\nweather, speed, and distance conditions. Additionally, we conducted extensive\nexperiments on three state-of-the-art deep learning models, including detailed\nablation studies for decoding the flashing sequence. Our results indicate that\nthe optimal architecture employs a dual-channel design, enabling simultaneous\ndecoding of the flashing sequence and extraction of vehicle spatial and\nlocational features for robust authentication."
    },
    {
        "date": "2025-05",
        "title": "AWARE-NET: Adaptive Weighted Averaging for Robust Ensemble Network in Deepfake Detection",
        "author": "Muhammad Salman, Iqra Tariq, Mishal Zulfiqar, Muqadas Jalal, Sami Aujla, and Sumbal Fatima",
        "link": "http://arxiv.org/abs/2505.00312v1",
        "abstract": "Deepfake detection has become increasingly important due to the rise of\nsynthetic media, which poses significant risks to digital identity and cyber\npresence for security and trust. While multiple approaches have improved\ndetection accuracy, challenges remain in achieving consistent performance\nacross diverse datasets and manipulation types. In response, we propose a novel\ntwo-tier ensemble framework for deepfake detection based on deep learning that\nhierarchically combines multiple instances of three state-of-the-art\narchitectures: Xception, Res2Net101, and EfficientNet-B7. Our framework employs\na unique approach where each architecture is instantiated three times with\ndifferent initializations to enhance model diversity, followed by a learnable\nweighting mechanism that dynamically combines their predictions. Unlike\ntraditional fixed-weight ensembles, our first-tier averages predictions within\neach architecture family to reduce model variance, while the second tier learns\noptimal contribution weights through backpropagation, automatically adjusting\neach architecture's influence based on their detection reliability. Our\nexperiments achieved state-of-the-art intra-dataset performance with AUC scores\nof 99.22% (FF++) and 100.00% (CelebDF-v2), and F1 scores of 98.06% (FF++) and\n99.94% (CelebDF-v2) without augmentation. With augmentation, we achieve AUC\nscores of 99.47% (FF++) and 100.00% (CelebDF-v2), and F1 scores of 98.43%\n(FF++) and 99.95% (CelebDF-v2). The framework demonstrates robust cross-dataset\ngeneralization, achieving AUC scores of 88.20% and 72.52%, and F1 scores of\n93.16% and 80.62% in cross-dataset evaluations."
    },
    {
        "date": "2025-05",
        "title": "A Unifying Framework for Robust and Efficient Inference with Unstructured Data",
        "author": "Jacob Carlson, and Melissa Dell",
        "link": "http://arxiv.org/abs/2505.00282v1",
        "abstract": "This paper presents a general framework for conducting efficient and robust\ninference on parameters derived from unstructured data, which include text,\nimages, audio, and video. Economists have long incorporated data extracted from\ntexts and images into their analyses, a practice that has accelerated with\nadvancements in deep neural networks. However, neural networks do not\ngenerically produce unbiased predictions, potentially propagating bias to\nestimators that use their outputs. To address this challenge, we reframe\ninference with unstructured data as a missing structured data problem, where\nstructured data are imputed from unstructured inputs using deep neural\nnetworks. This perspective allows us to apply classic results from\nsemiparametric inference, yielding valid, efficient, and robust estimators\nbased on unstructured data. We formalize this approach with MARS (Missing At\nRandom Structured Data), a unifying framework that integrates and extends\nexisting methods for debiased inference using machine learning predictions,\nlinking them to a variety of older, familiar problems such as causal inference.\nWe develop robust and efficient estimators for both descriptive and causal\nestimands and address challenges such as inference using aggregated and\ntransformed predictions from unstructured data. Importantly, MARS applies to\ncommon empirical settings that have received limited attention in the existing\nliterature. Finally, we reanalyze prominent studies that use unstructured data,\ndemonstrating the practical value of MARS."
    },
    {
        "date": "2025-04",
        "title": "Towards Robust and Generalizable Gerchberg Saxton based Physics Inspired Neural Networks for Computer Generated Holography: A Sensitivity Analysis Framework",
        "author": "Ankit Amrutkar, Bj\u00f6rn Kampa, Volkmar Schulz, Johannes Stegmaier, Markus Rothermel, and Dorit Merhof",
        "link": "http://arxiv.org/abs/2505.00220v1",
        "abstract": "Computer-generated holography (CGH) enables applications in holographic\naugmented reality (AR), 3D displays, systems neuroscience, and optical\ntrapping. The fundamental challenge in CGH is solving the inverse problem of\nphase retrieval from intensity measurements. Physics-inspired neural networks\n(PINNs), especially Gerchberg-Saxton-based PINNs (GS-PINNs), have advanced\nphase retrieval capabilities. However, their performance strongly depends on\nforward models (FMs) and their hyperparameters (FMHs), limiting generalization,\ncomplicating benchmarking, and hindering hardware optimization. We present a\nsystematic sensitivity analysis framework based on Saltelli's extension of\nSobol's method to quantify FMH impacts on GS-PINN performance. Our analysis\ndemonstrates that SLM pixel-resolution is the primary factor affecting neural\nnetwork sensitivity, followed by pixel-pitch, propagation distance, and\nwavelength. Free space propagation forward models demonstrate superior neural\nnetwork performance compared to Fourier holography, providing enhanced\nparameterization and generalization. We introduce a composite evaluation metric\ncombining performance consistency, generalization capability, and\nhyperparameter perturbation resilience, establishing a unified benchmarking\nstandard across CGH configurations. Our research connects physics-inspired deep\nlearning theory with practical CGH implementations through concrete guidelines\nfor forward model selection, neural network architecture, and performance\nevaluation. Our contributions advance the development of robust, interpretable,\nand generalizable neural networks for diverse holographic applications,\nsupporting evidence-based decisions in CGH research and implementation."
    },
    {
        "date": "2025-04",
        "title": "Efficient and robust 3D blind harmonization for large domain gaps",
        "author": "Hwihun Jeong, Hayeon Lee, Se Young Chun, and Jongho Lee",
        "link": "http://arxiv.org/abs/2505.00133v1",
        "abstract": "Blind harmonization has emerged as a promising technique for MR image\nharmonization to achieve scale-invariant representations, requiring only target\ndomain data (i.e., no source domain data necessary). However, existing methods\nface limitations such as inter-slice heterogeneity in 3D, moderate image\nquality, and limited performance for a large domain gap. To address these\nchallenges, we introduce BlindHarmonyDiff, a novel blind 3D harmonization\nframework that leverages an edge-to-image model tailored specifically to\nharmonization. Our framework employs a 3D rectified flow trained on target\ndomain images to reconstruct the original image from an edge map, then yielding\na harmonized image from the edge of a source domain image. We propose\nmulti-stride patch training for efficient 3D training and a refinement module\nfor robust inference by suppressing hallucination. Extensive experiments\ndemonstrate that BlindHarmonyDiff outperforms prior arts by harmonizing diverse\nsource domain images to the target domain, achieving higher correspondence to\nthe target domain characteristics. Downstream task-based quality assessments\nsuch as tissue segmentation and age prediction on diverse MR scanners further\nconfirm the effectiveness of our approach and demonstrate the capability of our\nrobust and generalizable blind harmonization."
    },
    {
        "date": "2025-04",
        "title": "Security-by-Design at the Telco Edge with OSS: Challenges and Lessons Learned",
        "author": "Carmine Cesarano, Alessio Foggia, Gianluca Roscigno, Luca Andreani, and Roberto Natella",
        "link": "http://arxiv.org/abs/2505.00111v1",
        "abstract": "This paper presents our experience, in the context of an industrial R&D\nproject, on securing GENIO, a platform for edge computing on Passive Optical\nNetwork (PON) infrastructures, and based on Open-Source Software (OSS). We\nidentify threats and related mitigations through hardening, vulnerability\nmanagement, digital signatures, and static and dynamic analysis. In particular,\nwe report lessons learned in applying these mitigations using OSS, and share\nour findings about the maturity and limitations of these security solutions in\nan industrial context."
    },
    {
        "date": "2025-04",
        "title": "Cert-SSB: Toward Certified Sample-Specific Backdoor Defense",
        "author": "Ting Qiao, Yingjia Wang, Xing Liu, Sixing Wu, Jianbing Li, and Yiming Li",
        "link": "http://arxiv.org/abs/2504.21730v1",
        "abstract": "Deep neural networks (DNNs) are vulnerable to backdoor attacks, where an\nattacker manipulates a small portion of the training data to implant hidden\nbackdoors into the model. The compromised model behaves normally on clean\nsamples but misclassifies backdoored samples into the attacker-specified target\nclass, posing a significant threat to real-world DNN applications. Currently,\nseveral empirical defense methods have been proposed to mitigate backdoor\nattacks, but they are often bypassed by more advanced backdoor techniques. In\ncontrast, certified defenses based on randomized smoothing have shown promise\nby adding random noise to training and testing samples to counteract backdoor\nattacks. In this paper, we reveal that existing randomized smoothing defenses\nimplicitly assume that all samples are equidistant from the decision boundary.\nHowever, it may not hold in practice, leading to suboptimal certification\nperformance. To address this issue, we propose a sample-specific certified\nbackdoor defense method, termed Cert-SSB. Cert-SSB first employs stochastic\ngradient ascent to optimize the noise magnitude for each sample, ensuring a\nsample-specific noise level that is then applied to multiple poisoned training\nsets to retrain several smoothed models. After that, Cert-SSB aggregates the\npredictions of multiple smoothed models to generate the final robust\nprediction. In particular, in this case, existing certification methods become\ninapplicable since the optimized noise varies across different samples. To\nconquer this challenge, we introduce a storage-update-based certification\nmethod, which dynamically adjusts each sample's certification region to improve\ncertification performance. We conduct extensive experiments on multiple\nbenchmark datasets, demonstrating the effectiveness of our proposed method. Our\ncode is available at https://github.com/NcepuQiaoTing/Cert-SSB."
    },
    {
        "date": "2025-04",
        "title": "Enhancing Security and Strengthening Defenses in Automated Short-Answer Grading Systems",
        "author": "Sahar Yarmohammadtoosky, Yiyun Zhou, Victoria Yaneva, Peter Baldwin, Saed Rezayi, Brian Clauser, and Polina Harikeo",
        "link": "http://arxiv.org/abs/2505.00061v1",
        "abstract": "This study examines vulnerabilities in transformer-based automated\nshort-answer grading systems used in medical education, with a focus on how\nthese systems can be manipulated through adversarial gaming strategies. Our\nresearch identifies three main types of gaming strategies that exploit the\nsystem's weaknesses, potentially leading to false positives. To counteract\nthese vulnerabilities, we implement several adversarial training methods\ndesigned to enhance the systems' robustness. Our results indicate that these\nmethods significantly reduce the susceptibility of grading systems to such\nmanipulations, especially when combined with ensemble techniques like majority\nvoting and ridge regression, which further improve the system's defense against\nsophisticated adversarial inputs. Additionally, employing large language models\nsuch as GPT-4 with varied prompting techniques has shown promise in recognizing\nand scoring gaming strategies effectively. The findings underscore the\nimportance of continuous improvements in AI-driven educational tools to ensure\ntheir reliability and fairness in high-stakes settings."
    },
    {
        "date": "2025-04",
        "title": "Hoist with His Own Petard: Inducing Guardrails to Facilitate Denial-of-Service Attacks on Retrieval-Augmented Generation of LLMs",
        "author": "Pan Suo, Yu-Ming Shang, San-Chuan Guo, and Xi Zhang",
        "link": "http://arxiv.org/abs/2504.21680v1",
        "abstract": "Retrieval-Augmented Generation (RAG) integrates Large Language Models (LLMs)\nwith external knowledge bases, improving output quality while introducing new\nsecurity risks. Existing studies on RAG vulnerabilities typically focus on\nexploiting the retrieval mechanism to inject erroneous knowledge or malicious\ntexts, inducing incorrect outputs. However, these approaches overlook critical\nweaknesses within LLMs, leaving important attack vectors unexplored and\nlimiting the scope and efficiency of attacks. In this paper, we uncover a novel\nvulnerability: the safety guardrails of LLMs, while designed for protection,\ncan also be exploited as an attack vector by adversaries. Building on this\nvulnerability, we propose MutedRAG, a novel denial-of-service attack that\nreversely leverages the guardrails of LLMs to undermine the availability of RAG\nsystems. By injecting minimalistic jailbreak texts, such as \"\\textit{How to\nbuild a bomb}\", into the knowledge base, MutedRAG intentionally triggers the\nLLM's safety guardrails, causing the system to reject legitimate queries.\nBesides, due to the high sensitivity of guardrails, a single jailbreak sample\ncan affect multiple queries, effectively amplifying the efficiency of attacks\nwhile reducing their costs. Experimental results on three datasets demonstrate\nthat MutedRAG achieves an attack success rate exceeding 60% in many scenarios,\nrequiring only less than one malicious text to each target query on average. In\naddition, we evaluate potential defense strategies against MutedRAG, finding\nthat some of current mechanisms are insufficient to mitigate this threat,\nunderscoring the urgent need for more robust solutions."
    },
    {
        "date": "2025-04",
        "title": "Traceback of Poisoning Attacks to Retrieval-Augmented Generation",
        "author": "Baolei Zhang, Haoran Xin, Minghong Fang, Zhuqing Liu, Biao Yi, Tong Li, and Zheli Liu",
        "link": "http://arxiv.org/abs/2504.21668v1",
        "abstract": "Large language models (LLMs) integrated with retrieval-augmented generation\n(RAG) systems improve accuracy by leveraging external knowledge sources.\nHowever, recent research has revealed RAG's susceptibility to poisoning\nattacks, where the attacker injects poisoned texts into the knowledge database,\nleading to attacker-desired responses. Existing defenses, which predominantly\nfocus on inference-time mitigation, have proven insufficient against\nsophisticated attacks. In this paper, we introduce RAGForensics, the first\ntraceback system for RAG, designed to identify poisoned texts within the\nknowledge database that are responsible for the attacks. RAGForensics operates\niteratively, first retrieving a subset of texts from the database and then\nutilizing a specially crafted prompt to guide an LLM in detecting potential\npoisoning texts. Empirical evaluations across multiple datasets demonstrate the\neffectiveness of RAGForensics against state-of-the-art poisoning attacks. This\nwork pioneers the traceback of poisoned texts in RAG systems, providing a\npractical and promising defense mechanism to enhance their security."
    },
    {
        "date": "2025-04",
        "title": "Diffusion-based Adversarial Identity Manipulation for Facial Privacy Protection",
        "author": "Liqin Wang, Qianyue Hu, Wei Lu, and Xiangyang Luo",
        "link": "http://arxiv.org/abs/2504.21646v1",
        "abstract": "The success of face recognition (FR) systems has led to serious privacy\nconcerns due to potential unauthorized surveillance and user tracking on social\nnetworks. Existing methods for enhancing privacy fail to generate natural face\nimages that can protect facial privacy. In this paper, we propose\ndiffusion-based adversarial identity manipulation (DiffAIM) to generate natural\nand highly transferable adversarial faces against malicious FR systems. To be\nspecific, we manipulate facial identity within the low-dimensional latent space\nof a diffusion model. This involves iteratively injecting gradient-based\nadversarial identity guidance during the reverse diffusion process,\nprogressively steering the generation toward the desired adversarial faces. The\nguidance is optimized for identity convergence towards a target while promoting\nsemantic divergence from the source, facilitating effective impersonation while\nmaintaining visual naturalness. We further incorporate structure-preserving\nregularization to preserve facial structure consistency during manipulation.\nExtensive experiments on both face verification and identification tasks\ndemonstrate that compared with the state-of-the-art, DiffAIM achieves stronger\nblack-box attack transferability while maintaining superior visual quality. We\nalso demonstrate the effectiveness of the proposed approach for commercial FR\nAPIs, including Face++ and Aliyun."
    },
    {
        "date": "2025-04",
        "title": "A Comprehensive Study of Exploitable Patterns in Smart Contracts: From Vulnerability to Defense",
        "author": "Yuchen Ding, Hongli Peng, and Xiaoqi Li",
        "link": "http://arxiv.org/abs/2504.21480v1",
        "abstract": "With the rapid advancement of blockchain technology, smart contracts have\nenabled the implementation of increasingly complex functionalities. However,\nensuring the security of smart contracts remains a persistent challenge across\nthe stages of development, compilation, and execution. Vulnerabilities within\nsmart contracts not only undermine the security of individual applications but\nalso pose significant risks to the broader blockchain ecosystem, as\ndemonstrated by the growing frequency of attacks since 2016, resulting in\nsubstantial financial losses. This paper provides a comprehensive analysis of\nkey security risks in Ethereum smart contracts, specifically those written in\nSolidity and executed on the Ethereum Virtual Machine (EVM). We focus on two\nprevalent and critical vulnerability types (reentrancy and integer overflow) by\nexamining their underlying mechanisms, replicating attack scenarios, and\nassessing effective countermeasures."
    },
    {
        "date": "2025-04",
        "title": "Robust Orthogonal NMF with Label Propagation for Image Clustering",
        "author": "Jingjing Liu, Nian Wu, Xianchao Xiu, and Jianhua Zhang",
        "link": "http://arxiv.org/abs/2504.21472v1",
        "abstract": "Non-negative matrix factorization (NMF) is a popular unsupervised learning\napproach widely used in image clustering. However, in real-world clustering\nscenarios, most existing NMF methods are highly sensitive to noise corruption\nand are unable to effectively leverage limited supervised information. To\novercome these drawbacks, we propose a unified non-convex framework with label\npropagation called robust orthogonal nonnegative matrix factorization (RONMF).\nThis method not only considers the graph Laplacian and label propagation as\nregularization terms but also introduces a more effective non-convex structure\nto measure the reconstruction error and imposes orthogonal constraints on the\nbasis matrix to reduce the noise corruption, thereby achieving higher\nrobustness. To solve RONMF, we develop an alternating direction method of\nmultipliers (ADMM)-based optimization algorithm. In particular, all subproblems\nhave closed-form solutions, which ensures its efficiency. Experimental\nevaluations on eight public image datasets demonstrate that the proposed RONMF\noutperforms state-of-the-art NMF methods across various standard metrics and\nshows excellent robustness. The code will be available at\nhttps://github.com/slinda-liu."
    },
    {
        "date": "2025-04",
        "title": "Quaternion Nuclear Norms Over Frobenius Norms Minimization for Robust Matrix Completion",
        "author": "Yu Guo, Guoqing Chen, Tieyong Zeng, Qiyu Jin, and Michael Kwok-Po Ng",
        "link": "http://arxiv.org/abs/2504.21468v1",
        "abstract": "Recovering hidden structures from incomplete or noisy data remains a\npervasive challenge across many fields, particularly where multi-dimensional\ndata representation is essential. Quaternion matrices, with their ability to\nnaturally model multi-dimensional data, offer a promising framework for this\nproblem. This paper introduces the quaternion nuclear norm over the Frobenius\nnorm (QNOF) as a novel nonconvex approximation for the rank of quaternion\nmatrices. QNOF is parameter-free and scale-invariant. Utilizing quaternion\nsingular value decomposition, we prove that solving the QNOF can be simplified\nto solving the singular value $L_1/L_2$ problem. Additionally, we extend the\nQNOF to robust quaternion matrix completion, employing the alternating\ndirection multiplier method to derive solutions that guarantee weak convergence\nunder mild conditions. Extensive numerical experiments validate the proposed\nmodel's superiority, consistently outperforming state-of-the-art quaternion\nmethods."
    },
    {
        "date": "2025-04",
        "title": "FAST-Q: Fast-track Exploration with Adversarially Balanced State Representations for Counterfactual Action Estimation in Offline Reinforcement Learning",
        "author": "Pulkit Agrawal, Rukma Talwadker, Aditya Pareek, and Tridib Mukherjee",
        "link": "http://arxiv.org/abs/2504.21383v1",
        "abstract": "Recent advancements in state-of-the-art (SOTA) offline reinforcement learning\n(RL) have primarily focused on addressing function approximation errors, which\ncontribute to the overestimation of Q-values for out-of-distribution actions, a\nchallenge that static datasets exacerbate. However, high stakes applications\nsuch as recommendation systems in online gaming, introduce further complexities\ndue to player's psychology (intent) driven by gameplay experiences and the\ninherent volatility on the platform. These factors create highly sparse,\npartially overlapping state spaces across policies, further influenced by the\nexperiment path selection logic which biases state spaces towards specific\npolicies. Current SOTA methods constrain learning from such offline data by\nclipping known counterfactual actions as out-of-distribution due to poor\ngeneralization across unobserved states. Further aggravating conservative\nQ-learning and necessitating more online exploration. FAST-Q introduces a novel\napproach that (1) leverages Gradient Reversal Learning to construct balanced\nstate representations, regularizing the policy-specific bias between the\nplayer's state and action thereby enabling counterfactual estimation; (2)\nsupports offline counterfactual exploration in parallel with static data\nexploitation; and (3) proposes a Q-value decomposition strategy for\nmulti-objective optimization, facilitating explainable recommendations over\nshort and long-term objectives. These innovations demonstrate superiority of\nFAST-Q over prior SOTA approaches and demonstrates at least 0.15 percent\nincrease in player returns, 2 percent improvement in lifetime value (LTV), 0.4\npercent enhancement in the recommendation driven engagement, 2 percent\nimprovement in the player's platform dwell time and an impressive 10 percent\nreduction in the costs associated with the recommendation, on our volatile\ngaming platform."
    },
    {
        "date": "2025-04",
        "title": "Synergy-CLIP: Extending CLIP with Multi-modal Integration for Robust Representation Learning",
        "author": "Sangyeon Cho, Jangyeong Jeon, Mingi Kim, and Junyeong Kim",
        "link": "http://arxiv.org/abs/2504.21375v1",
        "abstract": "Multi-modal representation learning has become a pivotal area in artificial\nintelligence, enabling the integration of diverse modalities such as vision,\ntext, and audio to solve complex problems. However, existing approaches\npredominantly focus on bimodal interactions, such as image-text pairs, which\nlimits their ability to fully exploit the richness of multi-modal data.\nFurthermore, the integration of modalities in equal-scale environments remains\nunderexplored due to the challenges of constructing large-scale, balanced\ndatasets. In this study, we propose Synergy-CLIP, a novel framework that\nextends the contrastive language-image pre-training (CLIP) architecture to\nenhance multi-modal representation learning by integrating visual, textual, and\naudio modalities. Unlike existing methods that focus on adapting individual\nmodalities to vanilla-CLIP, Synergy-CLIP aligns and captures latent information\nacross three modalities equally. To address the high cost of constructing\nlarge-scale multi-modal datasets, we introduce VGG-sound+, a triple-modal\ndataset designed to provide equal-scale representation of visual, textual, and\naudio data. Synergy-CLIP is validated on various downstream tasks, including\nzero-shot classification, where it outperforms existing baselines.\nAdditionally, we introduce a missing modality reconstruction task,\ndemonstrating Synergy-CLIP's ability to extract synergy among modalities in\nrealistic application scenarios. These contributions provide a robust\nfoundation for advancing multi-modal representation learning and exploring new\nresearch directions."
    },
    {
        "date": "2025-04",
        "title": "The Dual Power of Interpretable Token Embeddings: Jailbreaking Attacks and Defenses for Diffusion Model Unlearning",
        "author": "Siyi Chen, Yimeng Zhang, Sijia Liu, and Qing Qu",
        "link": "http://arxiv.org/abs/2504.21307v1",
        "abstract": "Despite the remarkable generalization capabilities of diffusion models,\nrecent studies have shown that these models can memorize and generate harmful\ncontent when prompted with specific text instructions. Although fine-tuning\napproaches have been developed to mitigate this issue by unlearning harmful\nconcepts, these methods can be easily circumvented through jailbreaking\nattacks. This indicates that the harmful concept has not been fully erased from\nthe model. However, existing attack methods, while effective, lack\ninterpretability regarding why unlearned models still retain the concept,\nthereby hindering the development of defense strategies. In this work, we\naddress these limitations by proposing an attack method that learns an\northogonal set of interpretable attack token embeddings. The attack token\nembeddings can be decomposed into human-interpretable textual elements,\nrevealing that unlearned models still retain the target concept through\nimplicit textual components. Furthermore, these attack token embeddings are\nrobust and transferable across text prompts, initial noises, and unlearned\nmodels. Finally, leveraging this diverse set of embeddings, we design a defense\nmethod applicable to both our proposed attack and existing attack methods.\nExperimental results demonstrate the effectiveness of both our attack and\ndefense strategies."
    },
    {
        "date": "2025-04",
        "title": "CachePrune: Neural-Based Attribution Defense Against Indirect Prompt Injection Attacks",
        "author": "Rui Wang, Junda Wu, Yu Xia, Tong Yu, Ruiyi Zhang, Ryan Rossi, Lina Yao, and Julian McAuley",
        "link": "http://arxiv.org/abs/2504.21228v1",
        "abstract": "Large Language Models (LLMs) are identified as being susceptible to indirect\nprompt injection attack, where the model undesirably deviates from\nuser-provided instructions by executing tasks injected in the prompt context.\nThis vulnerability stems from LLMs' inability to distinguish between data and\ninstructions within a prompt. In this paper, we propose CachePrune that defends\nagainst this attack by identifying and pruning task-triggering neurons from the\nKV cache of the input prompt context. By pruning such neurons, we encourage the\nLLM to treat the text spans of input prompt context as only pure data, instead\nof any indicator of instruction following. These neurons are identified via\nfeature attribution with a loss function induced from an upperbound of the\nDirect Preference Optimization (DPO) objective. We show that such a loss\nfunction enables effective feature attribution with only a few samples. We\nfurther improve on the quality of feature attribution, by exploiting an\nobserved triggering effect in instruction following. Our approach does not\nimpose any formatting on the original prompt or introduce extra test-time LLM\ncalls. Experiments show that CachePrune significantly reduces attack success\nrates without compromising the response quality. Note: This paper aims to\ndefend against indirect prompt injection attacks, with the goal of developing\nmore secure and robust AI systems."
    },
    {
        "date": "2025-04",
        "title": "SecRepoBench: Benchmarking LLMs for Secure Code Generation in Real-World Repositories",
        "author": "Connor Dilgren, Purva Chiniya, Luke Griffith, Yu Ding, and Yizheng Chen",
        "link": "http://arxiv.org/abs/2504.21205v1",
        "abstract": "This paper introduces SecRepoBench, a benchmark to evaluate LLMs on secure\ncode generation in real-world repositories. SecRepoBench has 318 code\ngeneration tasks in 27 C/C++ repositories, covering 15 CWEs. We evaluate 19\nstate-of-the-art LLMs using our benchmark and find that the models struggle\nwith generating correct and secure code. In addition, the performance of LLMs\nto generate self-contained programs as measured by prior benchmarks do not\ntranslate to comparative performance at generating secure and correct code at\nthe repository level in SecRepoBench. We show that the state-of-the-art prompt\nengineering techniques become less effective when applied to the repository\nlevel secure code generation problem. We conduct extensive experiments,\nincluding an agentic technique to generate secure code, to demonstrate that our\nbenchmark is currently the most difficult secure coding benchmark, compared to\nprevious state-of-the-art benchmarks. Finally, our comprehensive analysis\nprovides insights into potential directions for enhancing the ability of LLMs\nto generate correct and secure code in real-world repositories."
    },
    {
        "date": "2025-04",
        "title": "ACE: A Security Architecture for LLM-Integrated App Systems",
        "author": "Evan Li, Tushin Mallick, Evan Rose, William Robertson, Alina Oprea, and Cristina Nita-Rotaru",
        "link": "http://arxiv.org/abs/2504.20984v1",
        "abstract": "LLM-integrated app systems extend the utility of Large Language Models (LLMs)\nwith third-party apps that are invoked by a system LLM using interleaved\nplanning and execution phases to answer user queries. These systems introduce\nnew attack vectors where malicious apps can cause integrity violation of\nplanning or execution, availability breakdown, or privacy compromise during\nexecution.\n  In this work, we identify new attacks impacting the integrity of planning, as\nwell as the integrity and availability of execution in LLM-integrated apps, and\ndemonstrate them against IsolateGPT, a recent solution designed to mitigate\nattacks from malicious apps. We propose Abstract-Concrete-Execute (ACE), a new\nsecure architecture for LLM-integrated app systems that provides security\nguarantees for system planning and execution. Specifically, ACE decouples\nplanning into two phases by first creating an abstract execution plan using\nonly trusted information, and then mapping the abstract plan to a concrete plan\nusing installed system apps. We verify that the plans generated by our system\nsatisfy user-specified secure information flow constraints via static analysis\non the structured plan output. During execution, ACE enforces data and\ncapability barriers between apps, and ensures that the execution is conducted\naccording to the trusted abstract plan. We show experimentally that our system\nis secure against attacks from the INJECAGENT benchmark, a standard benchmark\nfor control flow integrity in the face of indirect prompt injection attacks,\nand our newly introduced attacks. Our architecture represents a significant\nadvancement towards hardening LLM-based systems containing system facilities of\nvarying levels of trustworthiness."
    },
    {
        "date": "2025-04",
        "title": "AegisLLM: Scaling Agentic Systems for Self-Reflective Defense in LLM Security",
        "author": "Zikui Cai, Shayan Shabihi, Bang An, Zora Che, Brian R. Bartoldson, Bhavya Kailkhura, Tom Goldstein, and Furong Huang",
        "link": "http://arxiv.org/abs/2504.20965v1",
        "abstract": "We introduce AegisLLM, a cooperative multi-agent defense against adversarial\nattacks and information leakage. In AegisLLM, a structured workflow of\nautonomous agents - orchestrator, deflector, responder, and evaluator -\ncollaborate to ensure safe and compliant LLM outputs, while self-improving over\ntime through prompt optimization. We show that scaling agentic reasoning system\nat test-time - both by incorporating additional agent roles and by leveraging\nautomated prompt optimization (such as DSPy)- substantially enhances robustness\nwithout compromising model utility. This test-time defense enables real-time\nadaptability to evolving attacks, without requiring model retraining.\nComprehensive evaluations across key threat scenarios, including unlearning and\njailbreaking, demonstrate the effectiveness of AegisLLM. On the WMDP unlearning\nbenchmark, AegisLLM achieves near-perfect unlearning with only 20 training\nexamples and fewer than 300 LM calls. For jailbreaking benchmarks, we achieve\n51% improvement compared to the base model on StrongReject, with false refusal\nrates of only 7.9% on PHTest compared to 18-55% for comparable methods. Our\nresults highlight the advantages of adaptive, agentic reasoning over static\ndefenses, establishing AegisLLM as a strong runtime alternative to traditional\napproaches based on model modifications. Code is available at\nhttps://github.com/zikuicai/aegisllm"
    },
    {
        "date": "2025-04",
        "title": "Quantifying the Noise of Structural Perturbations on Graph Adversarial Attacks",
        "author": "Junyuan Fang, Han Yang, Haixian Wen, Jiajing Wu, Zibin Zheng, and Chi K. Tse",
        "link": "http://arxiv.org/abs/2504.20869v2",
        "abstract": "Graph neural networks have been widely utilized to solve graph-related tasks\nbecause of their strong learning power in utilizing the local information of\nneighbors. However, recent studies on graph adversarial attacks have proven\nthat current graph neural networks are not robust against malicious attacks.\nYet much of the existing work has focused on the optimization objective based\non attack performance to obtain (near) optimal perturbations, but paid less\nattention to the strength quantification of each perturbation such as the\ninjection of a particular node/link, which makes the choice of perturbations a\nblack-box model that lacks interpretability. In this work, we propose the\nconcept of noise to quantify the attack strength of each adversarial link.\nFurthermore, we propose three attack strategies based on the defined noise and\nclassification margins in terms of single and multiple steps optimization.\nExtensive experiments conducted on benchmark datasets against three\nrepresentative graph neural networks demonstrate the effectiveness of the\nproposed attack strategies. Particularly, we also investigate the preferred\npatterns of effective adversarial perturbations by analyzing the corresponding\nproperties of the selected perturbation nodes."
    },
    {
        "date": "2025-04",
        "title": "Mitigating the Structural Bias in Graph Adversarial Defenses",
        "author": "Junyuan Fang, Huimin Liu, Han Yang, Jiajing Wu, Zibin Zheng, and Chi K. Tse",
        "link": "http://arxiv.org/abs/2504.20848v1",
        "abstract": "In recent years, graph neural networks (GNNs) have shown great potential in\naddressing various graph structure-related downstream tasks. However, recent\nstudies have found that current GNNs are susceptible to malicious adversarial\nattacks. Given the inevitable presence of adversarial attacks in the real\nworld, a variety of defense methods have been proposed to counter these attacks\nand enhance the robustness of GNNs. Despite the commendable performance of\nthese defense methods, we have observed that they tend to exhibit a structural\nbias in terms of their defense capability on nodes with low degree (i.e., tail\nnodes), which is similar to the structural bias of traditional GNNs on nodes\nwith low degree in the clean graph. Therefore, in this work, we propose a\ndefense strategy by including hetero-homo augmented graph construction, $k$NN\naugmented graph construction, and multi-view node-wise attention modules to\nmitigate the structural bias of GNNs against adversarial attacks. Notably, the\nhetero-homo augmented graph consists of removing heterophilic links (i.e.,\nlinks connecting nodes with dissimilar features) globally and adding homophilic\nlinks (i.e., links connecting nodes with similar features) for nodes with low\ndegree. To further enhance the defense capability, an attention mechanism is\nadopted to adaptively combine the representations from the above two kinds of\ngraph views. We conduct extensive experiments to demonstrate the defense and\ndebiasing effect of the proposed strategy on benchmark datasets."
    },
    {
        "date": "2025-04",
        "title": "GaussTrap: Stealthy Poisoning Attacks on 3D Gaussian Splatting for Targeted Scene Confusion",
        "author": "Jiaxin Hong, Sixu Chen, Shuoyang Sun, Hongyao Yu, Hao Fang, Yuqi Tan, Bin Chen, Shuhan Qi, and Jiawei Li",
        "link": "http://arxiv.org/abs/2504.20829v1",
        "abstract": "As 3D Gaussian Splatting (3DGS) emerges as a breakthrough in scene\nrepresentation and novel view synthesis, its rapid adoption in safety-critical\ndomains (e.g., autonomous systems, AR/VR) urgently demands scrutiny of\npotential security vulnerabilities. This paper presents the first systematic\nstudy of backdoor threats in 3DGS pipelines. We identify that adversaries may\nimplant backdoor views to induce malicious scene confusion during inference,\npotentially leading to environmental misperception in autonomous navigation or\nspatial distortion in immersive environments. To uncover this risk, we propose\nGuassTrap, a novel poisoning attack method targeting 3DGS models. GuassTrap\ninjects malicious views at specific attack viewpoints while preserving\nhigh-quality rendering in non-target views, ensuring minimal detectability and\nmaximizing potential harm. Specifically, the proposed method consists of a\nthree-stage pipeline (attack, stabilization, and normal training) to implant\nstealthy, viewpoint-consistent poisoned renderings in 3DGS, jointly optimizing\nattack efficacy and perceptual realism to expose security risks in 3D\nrendering. Extensive experiments on both synthetic and real-world datasets\ndemonstrate that GuassTrap can effectively embed imperceptible yet harmful\nbackdoor views while maintaining high-quality rendering in normal views,\nvalidating its robustness, adaptability, and practical applicability."
    },
    {
        "date": "2025-04",
        "title": "Secure Coding with AI, From Creation to Inspection",
        "author": "Vladislav Belozerov, Peter J Barclay, and Ashkan Sami",
        "link": "http://arxiv.org/abs/2504.20814v1",
        "abstract": "While prior studies have explored security in code generated by ChatGPT and\nother Large Language Models, they were conducted in controlled experimental\nsettings and did not use code generated or provided from actual developer\ninteractions. This paper not only examines the security of code generated by\nChatGPT based on real developer interactions, curated in the DevGPT dataset,\nbut also assesses ChatGPT's capability to find and fix these vulnerabilities.\nWe analysed 1,586 C, C++, and C# code snippets using static scanners, which\ndetected potential issues in 124 files. After manual analysis, we selected 26\nfiles with 32 confirmed vulnerabilities for further investigation.\n  We submitted these files to ChatGPT via the OpenAI API, asking it to detect\nsecurity issues, identify the corresponding Common Weakness Enumeration\nnumbers, and propose fixes. The responses and modified code were manually\nreviewed and re-scanned for vulnerabilities. ChatGPT successfully detected 18\nout of 32 security issues and resolved 17 issues but failed to recognize or fix\nthe remainder. Interestingly, only 10 vulnerabilities were resulted from the\nuser prompts, while 22 were introduced by ChatGPT itself.\n  We highlight for developers that code generated by ChatGPT is more likely to\ncontain vulnerabilities compared to their own code. Furthermore, at times\nChatGPT reports incorrect information with apparent confidence, which may\nmislead less experienced developers. Our findings confirm previous studies in\ndemonstrating that ChatGPT is not sufficiently reliable for generating secure\ncode nor identifying all vulnerabilities, highlighting the continuing\nimportance of static scanners and manual review."
    },
    {
        "date": "2025-04",
        "title": "R^2VFL: A Robust Random Vector Functional Link Network with Huber-Weighted Framework",
        "author": "Anuradha Kumari, Mushir Akhtar, P. N. Suganthan, and M. Tanveer",
        "link": "http://arxiv.org/abs/2504.21069v1",
        "abstract": "The random vector functional link (RVFL) neural network has shown significant\npotential in overcoming the constraints of traditional artificial neural\nnetworks, such as excessive computation time and suboptimal solutions. However,\nRVFL faces challenges when dealing with noise and outliers, as it assumes all\ndata samples contribute equally. To address this issue, we propose a novel\nrobust framework, R2VFL, RVFL with Huber weighting function and class\nprobability, which enhances the model's robustness and adaptability by\neffectively mitigating the impact of noise and outliers in the training data.\nThe Huber weighting function reduces the influence of outliers, while the class\nprobability mechanism assigns less weight to noisy data points, resulting in a\nmore resilient model. We explore two distinct approaches for calculating class\ncenters within the R2VFL framework: the simple average of all data points in\neach class and the median of each feature, the later providing a robust\nalternative by minimizing the effect of extreme values. These approaches give\nrise to two novel variants of the model-R2VFL-A and R2VFL-M. We extensively\nevaluate the proposed models on 47 UCI datasets, encompassing both binary and\nmulticlass datasets, and conduct rigorous statistical testing, which confirms\nthe superiority of the proposed models. Notably, the models also demonstrate\nexceptional performance in classifying EEG signals, highlighting their\npractical applicability in real-world biomedical domain."
    },
    {
        "date": "2025-04",
        "title": "Chain-of-Defensive-Thought: Structured Reasoning Elicits Robustness in Large Language Models against Reference Corruption",
        "author": "Wenxiao Wang, Parsa Hosseini, and Soheil Feizi",
        "link": "http://arxiv.org/abs/2504.20769v1",
        "abstract": "Chain-of-thought prompting has demonstrated great success in facilitating the\nreasoning abilities of large language models. In this work, we explore how\nthese enhanced reasoning abilities can be exploited to improve the robustness\nof large language models in tasks that are not necessarily reasoning-focused.\nIn particular, we show how a wide range of large language models exhibit\nsignificantly improved robustness against reference corruption using a simple\nmethod called chain-of-defensive-thought, where only a few exemplars with\nstructured and defensive reasoning are provided as demonstrations. Empirically,\nthe improvements can be astounding, especially given the simplicity and\napplicability of the method. For example, in the Natural Questions task, the\naccuracy of GPT-4o degrades from 60% to as low as 3% with standard prompting\nwhen 1 out of 10 references provided is corrupted with prompt injection\nattacks. In contrast, GPT-4o using chain-of-defensive-thought prompting\nmaintains an accuracy of 50%."
    },
    {
        "date": "2025-04",
        "title": "Data Encryption Battlefield: A Deep Dive into the Dynamic Confrontations in Ransomware Attacks",
        "author": "Arash Mahboubi, Hamed Aboutorab, Seyit Camtepe, Hang Thanh Bui, Khanh Luong, Keyvan Ansari, Shenlu Wang, and Bazara Barry",
        "link": "http://arxiv.org/abs/2504.20681v1",
        "abstract": "In the rapidly evolving landscape of cybersecurity threats, ransomware\nrepresents a significant challenge. Attackers increasingly employ sophisticated\nencryption methods, such as entropy reduction through Base64 encoding, and\npartial or intermittent encryption to evade traditional detection methods. This\nstudy explores the dynamic battle between adversaries who continuously refine\nencryption strategies and defenders developing advanced countermeasures to\nprotect vulnerable data. We investigate the application of online incremental\nmachine learning algorithms designed to predict file encryption activities\ndespite adversaries evolving obfuscation techniques. Our analysis utilizes an\nextensive dataset of 32.6 GB, comprising 11,928 files across multiple formats,\nincluding Microsoft Word documents (doc), PowerPoint presentations (ppt), Excel\nspreadsheets (xlsx), image formats (jpg, jpeg, png, tif, gif), PDFs (pdf),\naudio (mp3), and video (mp4) files. These files were encrypted by 75 distinct\nransomware families, facilitating a robust empirical evaluation of machine\nlearning classifiers effectiveness against diverse encryption tactics. Results\nhighlight the Hoeffding Tree algorithms superior incremental learning\ncapability, particularly effective in detecting traditional and AES-Base64\nencryption methods employed to lower entropy. Conversely, the Random Forest\nclassifier with warm-start functionality excels at identifying intermittent\nencryption methods, demonstrating the necessity of tailored machine learning\nsolutions to counter sophisticated ransomware strategies."
    },
    {
        "date": "2025-04",
        "title": "A Novel Cipher for Enhancing MAVLink Security: Design, Security Analysis, and Performance Evaluation Using a Drone Testbed",
        "author": "Bhavya Dixit, Ananthapadmanabhan A., Adheeba Thahsin, Saketh Pathak, Gaurav S. Kasbekar, and Arnab Maity",
        "link": "http://arxiv.org/abs/2504.20626v1",
        "abstract": "We present MAVShield, a novel lightweight cipher designed to secure\ncommunications in Unmanned Aerial Vehicles (UAVs) using the MAVLink protocol,\nwhich by default transmits unencrypted messages between UAVs and Ground Control\nStations (GCS). While existing studies propose encryption for MAVLink, most\nremain theoretical or simulation-based. We implement MAVShield alongside\nAES-CTR, ChaCha20, Speck-CTR, and Rabbit, and evaluate them on a real drone\ntestbed. A comprehensive security analysis using statistical test suites (NIST\nand Diehard) demonstrates strong resistance of the novel cipher to\ncryptanalysis. Performance evaluation across key metrics including memory\nusage, CPU load, and battery power consumption, demonstrates that MAVShield\noutperforms existing algorithms and offers an efficient, real-world solution\nfor securing MAVLink communications in UAVs."
    },
    {
        "date": "2025-04",
        "title": "The Hidden Risks of LLM-Generated Web Application Code: A Security-Centric Evaluation of Code Generation Capabilities in Large Language Models",
        "author": "Swaroop Dora, Deven Lunkad, Naziya Aslam, S. Venkatesan, and Sandeep Kumar Shukla",
        "link": "http://arxiv.org/abs/2504.20612v1",
        "abstract": "The rapid advancement of Large Language Models (LLMs) has enhanced software\ndevelopment processes, minimizing the time and effort required for coding and\nenhancing developer productivity. However, despite their potential benefits,\ncode generated by LLMs has been shown to generate insecure code in controlled\nenvironments, raising critical concerns about their reliability and security in\nreal-world applications. This paper uses predefined security parameters to\nevaluate the security compliance of LLM-generated code across multiple models,\nsuch as ChatGPT, DeepSeek, Claude, Gemini and Grok. The analysis reveals\ncritical vulnerabilities in authentication mechanisms, session management,\ninput validation and HTTP security headers. Although some models implement\nsecurity measures to a limited extent, none fully align with industry best\npractices, highlighting the associated risks in automated software development.\nOur findings underscore that human expertise is crucial to ensure secure\nsoftware deployment or review of LLM-generated code. Also, there is a need for\nrobust security assessment frameworks to enhance the reliability of\nLLM-generated code in real-world applications."
    },
    {
        "date": "2025-04",
        "title": "VIMU: Effective Physics-based Realtime Detection and Recovery against Stealthy Attacks on UAVs",
        "author": "Yunbo Wang, Cong Sun, Qiaosen Liu, Bingnan Su, Zongxu Zhang, Michael Norris, Gang Tan, and Jianfeng Ma",
        "link": "http://arxiv.org/abs/2504.20569v1",
        "abstract": "Sensor attacks on robotic vehicles have become pervasive and manipulative.\nTheir latest advancements exploit sensor and detector characteristics to bypass\ndetection. Recent security efforts have leveraged the physics-based model to\ndetect or mitigate sensor attacks. However, these approaches are only resilient\nto a few sensor attacks and still need improvement in detection effectiveness.\nWe present VIMU, an efficient sensor attack detection and resilience system for\nunmanned aerial vehicles. We propose a detection algorithm, CS-EMA, that\nleverages low-pass filtering to identify stealthy gyroscope attacks while\nachieving an overall effective sensor attack detection. We develop a\nfine-grained nonlinear physical model with precise aerodynamic and propulsion\nwrench modeling. We also augment the state estimation with a FIFO buffer\nsafeguard to mitigate the impact of high-rate IMU attacks. The proposed\nphysical model and buffer safeguard provide an effective system state recovery\ntoward maintaining flight stability. We implement VIMU on PX4 autopilot. The\nevaluation results demonstrate the effectiveness of VIMU in detecting and\nmitigating various realistic sensor attacks, especially stealthy attacks."
    },
    {
        "date": "2025-04",
        "title": "Digital Shielding for Cross-Domain Wi-Fi Signal Adaptation using Relativistic Average Generative Adversarial Network",
        "author": "Danilo Avola, Federica Bruni, Gian Luca Foresti, Daniele Pannone, and Amedeo Ranaldi",
        "link": "http://arxiv.org/abs/2504.20568v1",
        "abstract": "Wi-Fi sensing uses radio-frequency signals from Wi-Fi devices to analyze\nenvironments, enabling tasks such as tracking people, detecting intrusions, and\nrecognizing gestures. The rise of this technology is driven by the IEEE\n802.11bf standard and growing demand for tools that can ensure privacy and\noperate through obstacles. However, the performance of Wi-Fi sensing is heavily\ninfluenced by environmental conditions, especially when extracting spatial and\ntemporal features from the surrounding scene. A key challenge is achieving\nrobust generalization across domains, ensuring stable performance even when the\nsensing environment changes significantly. This paper introduces a novel deep\nlearning model for cross-domain adaptation of Wi-Fi signals, inspired by\nphysical signal shielding. The model uses a Relativistic average Generative\nAdversarial Network (RaGAN) with Bidirectional Long Short-Term Memory (Bi-LSTM)\narchitectures for both the generator and discriminator. To simulate physical\nshielding, an acrylic box lined with electromagnetic shielding fabric was\nconstructed, mimicking a Faraday cage. Wi-Fi signal spectra were collected from\nvarious materials both inside (domain-free) and outside (domain-dependent) the\nbox to train the model. A multi-class Support Vector Machine (SVM) was trained\non domain-free spectra and tested on signals denoised by the RaGAN. The system\nachieved 96% accuracy and demonstrated strong material discrimination\ncapabilities, offering potential for use in security applications to identify\nconcealed objects based on their composition."
    },
    {
        "date": "2025-04",
        "title": "Mutual Information Minimization for Side-Channel Attack Resistance via Optimal Noise Injection",
        "author": "Jiheon Woo, Daewon Seo, Young-Sik Kim, Namyoon Lee, Yuval Cassuto, and Yongjune Kim",
        "link": "http://arxiv.org/abs/2504.20556v1",
        "abstract": "Side-channel attacks (SCAs) pose a serious threat to system security by\nextracting secret keys through physical leakages such as power consumption,\ntiming variations, and electromagnetic emissions. Among existing\ncountermeasures, artificial noise injection is recognized as one of the most\neffective techniques. However, its high power consumption poses a major\nchallenge for resource-constrained systems such as Internet of Things (IoT)\ndevices, motivating the development of more efficient protection schemes. In\nthis paper, we model SCAs as a communication channel and aim to suppress\ninformation leakage by minimizing the mutual information between the secret\ninformation and side-channel observations, subject to a power constraint on the\nartificial noise. We propose an optimal artificial noise injection method to\nminimize the mutual information in systems with Gaussian inputs. Specifically,\nwe formulate two convex optimization problems: 1) minimizing the total mutual\ninformation, and 2) minimizing the maximum mutual information across\nobservations. Numerical results show that the proposed methods significantly\nreduce both total and maximum mutual information compared to conventional\ntechniques, confirming their effectiveness for resource-constrained,\nsecurity-critical systems."
    },
    {
        "date": "2025-04",
        "title": "TriniMark: A Robust Generative Speech Watermarking Method for Trinity-Level Attribution",
        "author": "Yue Li, Weizhi Liu, and Dongdong Lin",
        "link": "http://arxiv.org/abs/2504.20532v1",
        "abstract": "The emergence of diffusion models has facilitated the generation of speech\nwith reinforced fidelity and naturalness. While deepfake detection technologies\nhave manifested the ability to identify AI-generated content, their efficacy\ndecreases as generative models become increasingly sophisticated. Furthermore,\ncurrent research in the field has not adequately addressed the necessity for\nrobust watermarking to safeguard the intellectual property rights associated\nwith synthetic speech and generative models. To remedy this deficiency, we\npropose a \\textbf{ro}bust generative \\textbf{s}peech wat\\textbf{e}rmarking\nmethod (TriniMark) for authenticating the generated content and safeguarding\nthe copyrights by enabling the traceability of the diffusion model. We first\ndesign a structure-lightweight watermark encoder that embeds watermarks into\nthe time-domain features of speech and reconstructs the waveform directly. A\ntemporal-aware gated convolutional network is meticulously designed in the\nwatermark decoder for bit-wise watermark recovery. Subsequently, the\nwaveform-guided fine-tuning strategy is proposed for fine-tuning the diffusion\nmodel, which leverages the transferability of watermarks and enables the\ndiffusion model to incorporate watermark knowledge effectively. When an\nattacker trains a surrogate model using the outputs of the target model, the\nembedded watermark can still be learned by the surrogate model and correctly\nextracted. Comparative experiments with state-of-the-art methods demonstrate\nthe superior robustness of our method, particularly in countering compound\nattacks."
    },
    {
        "date": "2025-04",
        "title": "SAM-Guided Robust Representation Learning for One-Shot 3D Medical Image Segmentation",
        "author": "Jia Wang, Yunan Mei, Jiarui Liu, and Xin Fan",
        "link": "http://arxiv.org/abs/2504.20501v1",
        "abstract": "One-shot medical image segmentation (MIS) is crucial for medical analysis due\nto the burden of medical experts on manual annotation. The recent emergence of\nthe segment anything model (SAM) has demonstrated remarkable adaptation in MIS\nbut cannot be directly applied to one-shot medical image segmentation (MIS) due\nto its reliance on labor-intensive user interactions and the high computational\ncost. To cope with these limitations, we propose a novel SAM-guided robust\nrepresentation learning framework, named RRL-MedSAM, to adapt SAM to one-shot\n3D MIS, which exploits the strong generalization capabilities of the SAM\nencoder to learn better feature representation. We devise a dual-stage\nknowledge distillation (DSKD) strategy to distill general knowledge between\nnatural and medical images from the foundation model to train a lightweight\nencoder, and then adopt a mutual exponential moving average (mutual-EMA) to\nupdate the weights of the general lightweight encoder and medical-specific\nencoder. Specifically, pseudo labels from the registration network are used to\nperform mutual supervision for such two encoders. Moreover, we introduce an\nauto-prompting (AP) segmentation decoder which adopts the mask generated from\nthe general lightweight model as a prompt to assist the medical-specific model\nin boosting the final segmentation performance. Extensive experiments conducted\non three public datasets, i.e., OASIS, CT-lung demonstrate that the proposed\nRRL-MedSAM outperforms state-of-the-art one-shot MIS methods for both\nsegmentation and registration tasks. Especially, our lightweight encoder uses\nonly 3\\% of the parameters compared to the encoder of SAM-Base."
    },
    {
        "date": "2025-04",
        "title": "Token-Efficient Prompt Injection Attack: Provoking Cessation in LLM Reasoning via Adaptive Token Compression",
        "author": "Yu Cui, Yujun Cai, and Yiwei Wang",
        "link": "http://arxiv.org/abs/2504.20493v1",
        "abstract": "While reasoning large language models (LLMs) demonstrate remarkable\nperformance across various tasks, they also contain notable security\nvulnerabilities. Recent research has uncovered a \"thinking-stopped\"\nvulnerability in DeepSeek-R1, where model-generated reasoning tokens can\nforcibly interrupt the inference process, resulting in empty responses that\ncompromise LLM-integrated applications. However, existing methods triggering\nthis vulnerability require complex mathematical word problems with long\nprompts--even exceeding 5,000 tokens. To reduce the token cost and formally\ndefine this vulnerability, we propose a novel prompt injection attack named\n\"Reasoning Interruption Attack\", based on adaptive token compression. We\ndemonstrate that simple standalone arithmetic tasks can effectively trigger\nthis vulnerability, and the prompts based on such tasks exhibit simpler logical\nstructures than mathematical word problems. We develop a systematic approach to\nefficiently collect attack prompts and an adaptive token compression framework\nthat utilizes LLMs to automatically compress these prompts. Experiments show\nour compression framework significantly reduces prompt length while maintaining\neffective attack capabilities. We further investigate the attack's performance\nvia output prefix and analyze the underlying causes of the vulnerability,\nproviding valuable insights for improving security in reasoning LLMs."
    },
    {
        "date": "2025-04",
        "title": "Robustness via Referencing: Defending against Prompt Injection Attacks by Referencing the Executed Instruction",
        "author": "Yulin Chen, Haoran Li, Yuan Sui, Yue Liu, Yufei He, Yangqiu Song, and Bryan Hooi",
        "link": "http://arxiv.org/abs/2504.20472v1",
        "abstract": "Large language models (LLMs) have demonstrated impressive performance and\nhave come to dominate the field of natural language processing (NLP) across\nvarious tasks. However, due to their strong instruction-following capabilities\nand inability to distinguish between instructions and data content, LLMs are\nvulnerable to prompt injection attacks. These attacks manipulate LLMs into\ndeviating from the original input instructions and executing maliciously\ninjected instructions within data content, such as web documents retrieved from\nsearch engines. Existing defense methods, including prompt-engineering and\nfine-tuning approaches, typically instruct models to follow the original input\ninstructions while suppressing their tendencies to execute injected\ninstructions. However, our experiments reveal that suppressing\ninstruction-following tendencies is challenging. Through analyzing failure\ncases, we observe that although LLMs tend to respond to any recognized\ninstructions, they are aware of which specific instructions they are executing\nand can correctly reference them within the original prompt. Motivated by these\nfindings, we propose a novel defense method that leverages, rather than\nsuppresses, the instruction-following abilities of LLMs. Our approach prompts\nLLMs to generate responses that include both answers and their corresponding\ninstruction references. Based on these references, we filter out answers not\nassociated with the original input instructions. Comprehensive experiments\ndemonstrate that our method outperforms prompt-engineering baselines and\nachieves performance comparable to fine-tuning methods, reducing the attack\nsuccess rate (ASR) to 0 percent in some scenarios. Moreover, our approach has\nminimal impact on overall utility."
    },
    {
        "date": "2025-04",
        "title": "FFCBA: Feature-based Full-target Clean-label Backdoor Attacks",
        "author": "Yangxu Yin, Honglong Chen, Yudong Gao, Peng Sun, Liantao Wu, Zhe Li, and Weifeng Liu",
        "link": "http://arxiv.org/abs/2504.21054v1",
        "abstract": "Backdoor attacks pose a significant threat to deep neural networks, as\nbackdoored models would misclassify poisoned samples with specific triggers\ninto target classes while maintaining normal performance on clean samples.\nAmong these, multi-target backdoor attacks can simultaneously target multiple\nclasses. However, existing multi-target backdoor attacks all follow the\ndirty-label paradigm, where poisoned samples are mislabeled, and most of them\nrequire an extremely high poisoning rate. This makes them easily detectable by\nmanual inspection. In contrast, clean-label attacks are more stealthy, as they\navoid modifying the labels of poisoned samples. However, they generally\nstruggle to achieve stable and satisfactory attack performance and often fail\nto scale effectively to multi-target attacks. To address this issue, we propose\nthe Feature-based Full-target Clean-label Backdoor Attacks (FFCBA) which\nconsists of two paradigms: Feature-Spanning Backdoor Attacks (FSBA) and\nFeature-Migrating Backdoor Attacks (FMBA). FSBA leverages class-conditional\nautoencoders to generate noise triggers that align perturbed in-class samples\nwith the original category's features, ensuring the effectiveness, intra-class\nconsistency, inter-class specificity and natural-feature correlation of\ntriggers. While FSBA supports swift and efficient attacks, its cross-model\nattack capability is relatively weak. FMBA employs a two-stage\nclass-conditional autoencoder training process that alternates between using\nout-of-class samples and in-class samples. This allows FMBA to generate\ntriggers with strong target-class features, making it highly effective for\ncross-model attacks. We conduct experiments on multiple datasets and models,\nthe results show that FFCBA achieves outstanding attack performance and\nmaintains desirable robustness against the state-of-the-art backdoor defenses."
    },
    {
        "date": "2025-04",
        "title": "SFIBA: Spatial-based Full-target Invisible Backdoor Attacks",
        "author": "Yangxu Yin, Honglong Chen, Yudong Gao, Peng Sun, Zhishuai Li, and Weifeng Liu",
        "link": "http://arxiv.org/abs/2504.21052v1",
        "abstract": "Multi-target backdoor attacks pose significant security threats to deep\nneural networks, as they can preset multiple target classes through a single\nbackdoor injection. This allows attackers to control the model to misclassify\npoisoned samples with triggers into any desired target class during inference,\nexhibiting superior attack performance compared with conventional backdoor\nattacks. However, existing multi-target backdoor attacks fail to guarantee\ntrigger specificity and stealthiness in black-box settings, resulting in two\nmain issues. First, they are unable to simultaneously target all classes when\nonly training data can be manipulated, limiting their effectiveness in\nrealistic attack scenarios. Second, the triggers often lack visual\nimperceptibility, making poisoned samples easy to detect. To address these\nproblems, we propose a Spatial-based Full-target Invisible Backdoor Attack,\ncalled SFIBA. It restricts triggers for different classes to specific local\nspatial regions and morphologies in the pixel space to ensure specificity,\nwhile employing a frequency-domain-based trigger injection method to guarantee\nstealthiness. Specifically, for injection of each trigger, we first apply fast\nfourier transform to obtain the amplitude spectrum of clean samples in local\nspatial regions. Then, we employ discrete wavelet transform to extract the\nfeatures from the amplitude spectrum and use singular value decomposition to\nintegrate the trigger. Subsequently, we selectively filter parts of the trigger\nin pixel space to implement trigger morphology constraints and adjust injection\ncoefficients based on visual effects. We conduct experiments on multiple\ndatasets and models. The results demonstrate that SFIBA can achieve excellent\nattack performance and stealthiness, while preserving the model's performance\non benign samples, and can also bypass existing backdoor defenses."
    },
    {
        "date": "2025-04",
        "title": "Network Attack Traffic Detection With Hybrid Quantum-Enhanced Convolution Neural Network",
        "author": "Zihao Wang, Kar Wai Fok, and Vrizlynn L. L. Thing",
        "link": "http://arxiv.org/abs/2504.20436v1",
        "abstract": "The emerging paradigm of Quantum Machine Learning (QML) combines features of\nquantum computing and machine learning (ML). QML enables the generation and\nrecognition of statistical data patterns that classical computers and classical\nML methods struggle to effectively execute. QML utilizes quantum systems to\nenhance algorithmic computation speed and real-time data processing\ncapabilities, making it one of the most promising tools in the field of ML.\nQuantum superposition and entanglement features also hold the promise to\npotentially expand the potential feature representation capabilities of ML.\nTherefore, in this study, we explore how quantum computing affects ML and\nwhether it can further improve the detection performance on network traffic\ndetection, especially on unseen attacks which are types of malicious traffic\nthat do not exist in the ML training dataset. Classical ML models often perform\npoorly in detecting these unseen attacks because they have not been trained on\nsuch traffic. Hence, this paper focuses on designing and proposing novel hybrid\nstructures of Quantum Convolutional Neural Network (QCNN) to achieve the\ndetection of malicious traffic. The detection performance, generalization, and\nrobustness of the QML solutions are evaluated and compared with classical ML\nrunning on classical computers. The emphasis lies in assessing whether the\nQML-based malicious traffic detection outperforms classical solutions. Based on\nexperiment results, QCNN models demonstrated superior performance compared to\nclassical ML approaches on unseen attack detection."
    },
    {
        "date": "2025-04",
        "title": "Enhancing Leakage Attacks on Searchable Symmetric Encryption Using LLM-Based Synthetic Data Generation",
        "author": "Joshua Chiu, Partha Protim Paul, and Zahin Wahab",
        "link": "http://arxiv.org/abs/2504.20414v1",
        "abstract": "Searchable Symmetric Encryption (SSE) enables efficient search capabilities\nover encrypted data, allowing users to maintain privacy while utilizing cloud\nstorage. However, SSE schemes are vulnerable to leakage attacks that exploit\naccess patterns, search frequency, and volume information. Existing studies\nfrequently assume that adversaries possess a substantial fraction of the\nencrypted dataset to mount effective inference attacks, implying there is a\ndatabase leakage of such documents, thus, an assumption that may not hold in\nreal-world scenarios. In this work, we investigate the feasibility of enhancing\nleakage attacks under a more realistic threat model in which adversaries have\naccess to minimal leaked data. We propose a novel approach that leverages large\nlanguage models (LLMs), specifically GPT-4 variants, to generate synthetic\ndocuments that statistically and semantically resemble the real-world dataset\nof Enron emails. Using the email corpus as a case study, we evaluate the\neffectiveness of synthetic data generated via random sampling and hierarchical\nclustering methods on the performance of the SAP (Search Access Pattern)\nkeyword inference attack restricted to token volumes only. Our results\ndemonstrate that, while the choice of LLM has limited effect, increasing\ndataset size and employing clustering-based generation significantly improve\nattack accuracy, achieving comparable performance to attacks using larger\namounts of real data. We highlight the growing relevance of LLMs in adversarial\ncontexts."
    },
    {
        "date": "2025-04",
        "title": "The Dark Side of Digital Twins: Adversarial Attacks on AI-Driven Water Forecasting",
        "author": "Mohammadhossein Homaei, Victor Gonzalez Morales, Oscar Mogollon-Gutierrez, and Andres Caro",
        "link": "http://arxiv.org/abs/2504.20295v1",
        "abstract": "Digital twins (DTs) are improving water distribution systems by using\nreal-time data, analytics, and prediction models to optimize operations. This\npaper presents a DT platform designed for a Spanish water supply network,\nutilizing Long Short-Term Memory (LSTM) networks to predict water consumption.\nHowever, machine learning models are vulnerable to adversarial attacks, such as\nthe Fast Gradient Sign Method (FGSM) and Projected Gradient Descent (PGD).\nThese attacks manipulate critical model parameters, injecting subtle\ndistortions that degrade forecasting accuracy. To further exploit these\nvulnerabilities, we introduce a Learning Automata (LA) and Random LA-based\napproach that dynamically adjusts perturbations, making adversarial attacks\nmore difficult to detect. Experimental results show that this approach\nsignificantly impacts prediction reliability, causing the Mean Absolute\nPercentage Error (MAPE) to rise from 26% to over 35%. Moreover, adaptive attack\nstrategies amplify this effect, highlighting cybersecurity risks in AI-driven\nDTs. These findings emphasize the urgent need for robust defenses, including\nadversarial training, anomaly detection, and secure data pipelines."
    },
    {
        "date": "2025-04",
        "title": "Smart Water Security with AI and Blockchain-Enhanced Digital Twins",
        "author": "Mohammadhossein Homaei, Victor Gonzalez Morales, Oscar Mogollon Gutierrez, Ruben Molano Gomez, and Andres Caro",
        "link": "http://arxiv.org/abs/2504.20275v1",
        "abstract": "Water distribution systems in rural areas face serious challenges such as a\nlack of real-time monitoring, vulnerability to cyberattacks, and unreliable\ndata handling. This paper presents an integrated framework that combines\nLoRaWAN-based data acquisition, a machine learning-driven Intrusion Detection\nSystem (IDS), and a blockchain-enabled Digital Twin (BC-DT) platform for secure\nand transparent water management. The IDS filters anomalous or spoofed data\nusing a Long Short-Term Memory (LSTM) Autoencoder and Isolation Forest before\nvalidated data is logged via smart contracts on a private Ethereum blockchain\nusing Proof of Authority (PoA) consensus. The verified data feeds into a\nreal-time DT model supporting leak detection, consumption forecasting, and\npredictive maintenance. Experimental results demonstrate that the system\nachieves over 80 transactions per second (TPS) with under 2 seconds of latency\nwhile remaining cost-effective and scalable for up to 1,000 smart meters. This\nwork demonstrates a practical and secure architecture for decentralized water\ninfrastructure in under-connected rural environments."
    },
    {
        "date": "2025-04",
        "title": "A Virtual Cybersecurity Department for Securing Digital Twins in Water Distribution Systems",
        "author": "Mohammadhossein Homaei, Agustin Di Bartolo, Oscar Mogollon-Gutierrez, Fernando Broncano Morgado, and Pablo Garcia Rodriguez",
        "link": "http://arxiv.org/abs/2504.20266v1",
        "abstract": "Digital twins (DTs) help improve real-time monitoring and decision-making in\nwater distribution systems. However, their connectivity makes them easy targets\nfor cyberattacks such as scanning, denial-of-service (DoS), and unauthorized\naccess. Small and medium-sized enterprises (SMEs) that manage these systems\noften do not have enough budget or staff to build strong cybersecurity teams.\nTo solve this problem, we present a Virtual Cybersecurity Department (VCD), an\naffordable and automated framework designed for SMEs. The VCD uses open-source\ntools like Zabbix for real-time monitoring, Suricata for network intrusion\ndetection, Fail2Ban to block repeated login attempts, and simple firewall\nsettings. To improve threat detection, we also add a machine-learning-based IDS\ntrained on the OD-IDS2022 dataset using an improved ensemble model. This model\ndetects cyber threats such as brute-force attacks, remote code execution (RCE),\nand network flooding, with 92\\% accuracy and fewer false alarms. Our solution\ngives SMEs a practical and efficient way to secure water systems using low-cost\nand easy-to-manage tools."
    },
    {
        "date": "2025-04",
        "title": "SA2FE: A Secure, Anonymous, Auditable, and Fair Edge Computing Service Offloading Framework",
        "author": "Xiaojian Wang, Huayue Gu, Zhouyu Li, Fangtong Zhou, Ruozhou Yu, Dejun Yang, and Guoliang Xue",
        "link": "http://arxiv.org/abs/2504.20260v1",
        "abstract": "The inclusion of pervasive computing devices in a democratized edge computing\necosystem can significantly expand the capability and coverage of near-end\ncomputing for large-scale applications. However, offloading user tasks to\nheterogeneous and decentralized edge devices comes with the dual risk of both\nendangered user data security and privacy due to the curious base station or\nmalicious edge servers, and unfair offloading and malicious attacks targeting\nedge servers from other edge servers and/or users. Existing solutions to edge\naccess control and offloading either rely on \"always-on\" cloud servers with\nreduced edge benefits or fail to protect sensitive user service information. To\naddress these challenges, this paper presents SA2FE, a novel framework for edge\naccess control, offloading and accounting. We design a rerandomizable puzzle\nprimitive and a corresponding scheme to protect sensitive service information\nfrom eavesdroppers and ensure fair offloading decisions, while a blind\ntoken-based scheme safeguards user privacy, prevents double spending, and\nensures usage accountability. The security of SA2FE is proved under the\nUniversal Composability framework, and its performance and scalability are\ndemonstrated with implementation on commodity mobile devices and edge servers."
    },
    {
        "date": "2025-04",
        "title": "Financial Data Analysis with Robust Federated Logistic Regression",
        "author": "Kun Yang, Nikhil Krishnan, and Sanjeev R. Kulkarni",
        "link": "http://arxiv.org/abs/2504.20250v1",
        "abstract": "In this study, we focus on the analysis of financial data in a federated\nsetting, wherein data is distributed across multiple clients or locations, and\nthe raw data never leaves the local devices. Our primary focus is not only on\nthe development of efficient learning frameworks (for protecting user data\nprivacy) in the field of federated learning but also on the importance of\ndesigning models that are easier to interpret. In addition, we care about the\nrobustness of the framework to outliers. To achieve these goals, we propose a\nrobust federated logistic regression-based framework that strives to strike a\nbalance between these goals. To verify the feasibility of our proposed\nframework, we carefully evaluate its performance not only on independently\nidentically distributed (IID) data but also on non-IID data, especially in\nscenarios involving outliers. Extensive numerical results collected from\nmultiple public datasets demonstrate that our proposed method can achieve\ncomparable performance to those of classical centralized algorithms, such as\nLogistical Regression, Decision Tree, and K-Nearest Neighbors, in both binary\nand multi-class classification tasks."
    },
    {
        "date": "2025-04",
        "title": "A Case Study on the Use of Representativeness Bias as a Defense Against Adversarial Cyber Threats",
        "author": "Briland Hitaj, Grit Denker, Laura Tinnel, Michael McAnally, Bruce DeBruhl, Nathan Bunting, Alex Fafard, Daniel Aaron, Richard D. Roberts, Joshua Lawson, Greg McCain, and Dylan Starink",
        "link": "http://arxiv.org/abs/2504.20245v1",
        "abstract": "Cyberspace is an ever-evolving battleground involving adversaries seeking to\ncircumvent existing safeguards and defenders aiming to stay one step ahead by\npredicting and mitigating the next threat. Existing mitigation strategies have\nfocused primarily on solutions that consider software or hardware aspects,\noften ignoring the human factor. This paper takes a first step towards\npsychology-informed, active defense strategies, where we target biases that\nhuman beings are susceptible to under conditions of uncertainty.\n  Using capture-the-flag events, we create realistic challenges that tap into a\nparticular cognitive bias: representativeness. This study finds that this bias\ncan be triggered to thwart hacking attempts and divert hackers into\nnon-vulnerable attack paths. Participants were exposed to two different\nchallenges designed to exploit representativeness biases. One of the\nrepresentativeness challenges significantly thwarted attackers away from\nvulnerable attack vectors and onto non-vulnerable paths, signifying an\neffective bias-based defense mechanism. This work paves the way towards cyber\ndefense strategies that leverage additional human biases to thwart future,\nsophisticated adversarial attacks."
    },
    {
        "date": "2025-04",
        "title": "MP-SfM: Monocular Surface Priors for Robust Structure-from-Motion",
        "author": "Zador Pataki, Paul-Edouard Sarlin, Johannes L. Sch\u00f6nberger, and Marc Pollefeys",
        "link": "http://arxiv.org/abs/2504.20040v1",
        "abstract": "While Structure-from-Motion (SfM) has seen much progress over the years,\nstate-of-the-art systems are prone to failure when facing extreme viewpoint\nchanges in low-overlap, low-parallax or high-symmetry scenarios. Because\ncapturing images that avoid these pitfalls is challenging, this severely limits\nthe wider use of SfM, especially by non-expert users. We overcome these\nlimitations by augmenting the classical SfM paradigm with monocular depth and\nnormal priors inferred by deep neural networks. Thanks to a tight integration\nof monocular and multi-view constraints, our approach significantly outperforms\nexisting ones under extreme viewpoint changes, while maintaining strong\nperformance in standard conditions. We also show that monocular priors can help\nreject faulty associations due to symmetries, which is a long-standing problem\nfor SfM. This makes our approach the first capable of reliably reconstructing\nchallenging indoor environments from few images. Through principled uncertainty\npropagation, it is robust to errors in the priors, can handle priors inferred\nby different models with little tuning, and will thus easily benefit from\nfuture progress in monocular depth and normal estimation. Our code is publicly\navailable at https://github.com/cvg/mpsfm."
    },
    {
        "date": "2025-04",
        "title": "Simplified and Secure MCP Gateways for Enterprise AI Integration",
        "author": "Ivo Brett",
        "link": "http://arxiv.org/abs/2504.19997v1",
        "abstract": "The increased adoption of the Model Context Protocol (MCP) for AI Agents\nnecessitates robust security for Enterprise integrations. This paper introduces\nthe MCP Gateway to simplify self-hosted MCP server integration. The proposed\narchitecture integrates security principles, authentication, intrusion\ndetection, and secure tunneling, enabling secure self-hosting without exposing\ninfrastructure. Key contributions include a reference architecture, threat\nmodel mapping, simplified integration strategies, and open-source\nimplementation recommendations. This work focuses on the unique challenges of\nenterprise-centric, self-hosted AI integrations, unlike existing public MCP\nserver solutions."
    },
    {
        "date": "2025-04",
        "title": "Securing Agentic AI: A Comprehensive Threat Model and Mitigation Framework for Generative AI Agents",
        "author": "Vineeth Sai Narajala, and Om Narayan",
        "link": "http://arxiv.org/abs/2504.19956v1",
        "abstract": "As generative AI (GenAI) agents become more common in enterprise settings,\nthey introduce security challenges that differ significantly from those posed\nby traditional systems. These agents are not just LLMs; they reason, remember,\nand act, often with minimal human oversight. This paper introduces a\ncomprehensive threat model tailored specifically for GenAI agents, focusing on\nhow their autonomy, persistent memory access, complex reasoning, and tool\nintegration create novel risks. This research work identifies 9 primary threats\nand organizes them across five key domains: cognitive architecture\nvulnerabilities, temporal persistence threats, operational execution\nvulnerabilities, trust boundary violations, and governance circumvention. These\nthreats are not just theoretical they bring practical challenges such as\ndelayed exploitability, cross-system propagation, cross system lateral\nmovement, and subtle goal misalignments that are hard to detect with existing\nframeworks and standard approaches. To help address this, the research work\npresent two complementary frameworks: ATFAA - Advanced Threat Framework for\nAutonomous AI Agents, which organizes agent-specific risks, and SHIELD, a\nframework proposing practical mitigation strategies designed to reduce\nenterprise exposure. While this work builds on existing work in LLM and AI\nsecurity, the focus is squarely on what makes agents different and why those\ndifferences matter. Ultimately, this research argues that GenAI agents require\na new lens for security. If we fail to adapt our threat models and defenses to\naccount for their unique architecture and behavior, we risk turning a powerful\nnew tool into a serious enterprise liability."
    },
    {
        "date": "2025-04",
        "title": "Robust Federated Personalised Mean Estimation for the Gaussian Mixture Model",
        "author": "Malhar A. Managoli, Vinod M. Prabhakaran, and Suhas Diggavi",
        "link": "http://arxiv.org/abs/2504.19955v1",
        "abstract": "Federated learning with heterogeneous data and personalization has received\nsignificant recent attention. Separately, robustness to corrupted data in the\ncontext of federated learning has also been studied. In this paper we explore\ncombining personalization for heterogeneous data with robustness, where a\nconstant fraction of the clients are corrupted. Motivated by this broad\nproblem, we formulate a simple instantiation which captures some of its\ndifficulty. We focus on the specific problem of personalized mean estimation\nwhere the data is drawn from a Gaussian mixture model. We give an algorithm\nwhose error depends almost linearly on the ratio of corrupted to uncorrupted\nsamples, and show a lower bound with the same behavior, albeit with a gap of a\nconstant factor."
    },
    {
        "date": "2025-04",
        "title": "Securing GenAI Multi-Agent Systems Against Tool Squatting: A Zero Trust Registry-Based Approach",
        "author": "Vineeth Sai Narajala, Ken Huang, and Idan Habler",
        "link": "http://arxiv.org/abs/2504.19951v1",
        "abstract": "The rise of generative AI (GenAI) multi-agent systems (MAS) necessitates\nstandardized protocols enabling agents to discover and interact with external\ntools. However, these protocols introduce new security challenges,\nparticularly; tool squatting; the deceptive registration or representation of\ntools. This paper analyzes tool squatting threats within the context of\nemerging interoperability standards, such as Model Context Protocol (MCP) or\nseamless communication between agents protocols. It introduces a comprehensive\nTool Registry system designed to mitigate these risks. We propose a\nsecurity-focused architecture featuring admin-controlled registration,\ncentralized tool discovery, fine grained access policies enforced via dedicated\nAgent and Tool Registry services, a dynamic trust scoring mechanism based on\ntool versioning and known vulnerabilities, and just in time credential\nprovisioning. Based on its design principles, the proposed registry framework\naims to effectively prevent common tool squatting vectors while preserving the\nflexibility and power of multi-agent systems. This work addresses a critical\nsecurity gap in the rapidly evolving GenAI ecosystem and provides a foundation\nfor secure tool integration in production environments."
    },
    {
        "date": "2025-04",
        "title": "DeeCLIP: A Robust and Generalizable Transformer-Based Framework for Detecting AI-Generated Images",
        "author": "Mamadou Keita, Wassim Hamidouche, Hessen Bougueffa Eutamene, Abdelmalik Taleb-Ahmed, and Abdenour Hadid",
        "link": "http://arxiv.org/abs/2504.19876v1",
        "abstract": "This paper introduces DeeCLIP, a novel framework for detecting AI-generated\nimages using CLIP-ViT and fusion learning. Despite significant advancements in\ngenerative models capable of creating highly photorealistic images, existing\ndetection methods often struggle to generalize across different models and are\nhighly sensitive to minor perturbations. To address these challenges, DeeCLIP\nincorporates DeeFuser, a fusion module that combines high-level and low-level\nfeatures, improving robustness against degradations such as compression and\nblurring. Additionally, we apply triplet loss to refine the embedding space,\nenhancing the model's ability to distinguish between real and synthetic\ncontent. To further enable lightweight adaptation while preserving pre-trained\nknowledge, we adopt parameter-efficient fine-tuning using low-rank adaptation\n(LoRA) within the CLIP-ViT backbone. This approach supports effective zero-shot\nlearning without sacrificing generalization. Trained exclusively on 4-class\nProGAN data, DeeCLIP achieves an average accuracy of 89.00% on 19 test subsets\ncomposed of generative adversarial network (GAN) and diffusion models. Despite\nhaving fewer trainable parameters, DeeCLIP outperforms existing methods,\ndemonstrating superior robustness against various generative models and\nreal-world distortions. The code is publicly available at\nhttps://github.com/Mamadou-Keita/DeeCLIP for research purposes."
    },
    {
        "date": "2025-04",
        "title": "CodeBC: A More Secure Large Language Model for Smart Contract Code Generation in Blockchain",
        "author": "Lingxiang wang, Hainan Zhang, Qinnan Zhang, Ziwei Wang, Hongwei Zheng, Jin Dong, and Zhiming Zheng",
        "link": "http://arxiv.org/abs/2504.21043v1",
        "abstract": "Large language models (LLMs) excel at generating code from natural language\ninstructions, yet they often lack an understanding of security vulnerabilities.\nThis limitation makes it difficult for LLMs to avoid security risks in\ngenerated code, particularly in high-security programming tasks such as smart\ncontract development for blockchain. Researchers have attempted to enhance the\nvulnerability awareness of these models by training them to differentiate\nbetween vulnerable and fixed code snippets. However, this approach relies\nheavily on manually labeled vulnerability data, which is only available for\npopular languages like Python and C++. For low-resource languages like\nSolidity, used in smart contracts, large-scale annotated datasets are scarce\nand difficult to obtain. To address this challenge, we introduce CodeBC, a code\ngeneration model specifically designed for generating secure smart contracts in\nblockchain. CodeBC employs a three-stage fine-tuning approach based on\nCodeLlama, distinguishing itself from previous methods by not relying on\npairwise vulnerability location annotations. Instead, it leverages\nvulnerability and security tags to teach the model the differences between\nvulnerable and secure code. During the inference phase, the model leverages\nsecurity tags to generate secure and robust code. Experimental results\ndemonstrate that CodeBC outperforms baseline models in terms of BLEU, CodeBLEU,\nand compilation pass rates, while significantly reducing vulnerability rates.\nThese findings validate the effectiveness and cost-efficiency of our\nthree-stage fine-tuning strategy, making CodeBC a promising solution for\ngenerating secure smart contract code."
    },
    {
        "date": "2025-04",
        "title": "Prompt Injection Attack to Tool Selection in LLM Agents",
        "author": "Jiawen Shi, Zenghui Yuan, Guiyao Tie, Pan Zhou, Neil Zhenqiang Gong, and Lichao Sun",
        "link": "http://arxiv.org/abs/2504.19793v1",
        "abstract": "Tool selection is a key component of LLM agents. The process operates through\na two-step mechanism - \\emph{retrieval} and \\emph{selection} - to pick the most\nappropriate tool from a tool library for a given task. In this work, we\nintroduce \\textit{ToolHijacker}, a novel prompt injection attack targeting tool\nselection in no-box scenarios. ToolHijacker injects a malicious tool document\ninto the tool library to manipulate the LLM agent's tool selection process,\ncompelling it to consistently choose the attacker's malicious tool for an\nattacker-chosen target task. Specifically, we formulate the crafting of such\ntool documents as an optimization problem and propose a two-phase optimization\nstrategy to solve it. Our extensive experimental evaluation shows that\nToolHijacker is highly effective, significantly outperforming existing\nmanual-based and automated prompt injection attacks when applied to tool\nselection. Moreover, we explore various defenses, including prevention-based\ndefenses (StruQ and SecAlign) and detection-based defenses (known-answer\ndetection, perplexity detection, and perplexity windowed detection). Our\nexperimental results indicate that these defenses are insufficient,\nhighlighting the urgent need for developing new defense strategies."
    },
    {
        "date": "2025-04",
        "title": "Learning Brenier Potentials with Convex Generative Adversarial Neural Networks",
        "author": "Claudia Drygala, Hanno Gottschalk, Thomas Kruse, S\u00e9gol\u00e8ne Martin, and Annika M\u00fctze",
        "link": "http://arxiv.org/abs/2504.19779v1",
        "abstract": "Brenier proved that under certain conditions on a source and a target\nprobability measure there exists a strictly convex function such that its\ngradient is a transport map from the source to the target distribution. This\nfunction is called the Brenier potential. Furthermore, detailed information on\nthe H\\\"older regularity of the Brenier potential is available. In this work we\ndevelop the statistical learning theory of generative adversarial neural\nnetworks that learn the Brenier potential. As by the transformation of\ndensities formula, the density of the generated measure depends on the second\nderivative of the Brenier potential, we develop the universal approximation\ntheory of ReCU networks with cubic activation $\\mathtt{ReCU}(x)=\\max\\{0,x\\}^3$\nthat combines the favorable approximation properties of H\\\"older functions with\na Lipschitz continuous density. In order to assure the convexity of such\ngeneral networks, we introduce an adversarial training procedure for a\npotential function represented by the ReCU networks that combines the classical\ndiscriminator cross entropy loss with a penalty term that enforces (strict)\nconvexity. We give a detailed decomposition of learning errors and show that\nfor a suitable high penalty parameter all networks chosen in the adversarial\nmin-max optimization problem are strictly convex. This is further exploited to\nprove the consistency of the learning procedure for (slowly) expanding network\ncapacity. We also implement the described learning algorithm and apply it to a\nnumber of standard test cases from Gaussian mixture to image data as target\ndistributions. As predicted in theory, we observe that the convexity loss\nbecomes inactive during the training process and the potentials represented by\nthe neural networks have learned convexity."
    },
    {
        "date": "2025-04",
        "title": "Fast and Robust Speckle Pattern Authentication by Scale Invariant Feature Transform algorithm in Physical Unclonable Functions",
        "author": "Giuseppe Emanuele Lio, Mauro Daniel Luigi Bruno, Francesco Riboli, Sara Nocentini, and Antonio Ferraro",
        "link": "http://arxiv.org/abs/2504.21041v1",
        "abstract": "Nowadays, due to the growing phenomenon of forgery in many fields, the\ninterest in developing new anti-counterfeiting device and cryptography keys,\nbased on the Physical Unclonable Functions (PUFs) paradigm, is widely\nincreased. PUFs are physical hardware with an intrinsic, irreproducible\ndisorder that allows for on-demand cryptographic key extraction. Among them,\noptical PUF are characterized by a large number of degrees of freedom resulting\nin higher security and higher sensitivity to environmental conditions. While\nthese promising features led to the growth of advanced fabrication strategies\nand materials for new PUF devices, their combination with robust recognition\nalgorithm remains largely unexplored. In this work, we present a\nmetric-independent authentication approach that leverages the Scale Invariant\nFeature Transform (SIFT) algorithm to extract unique and invariant features\nfrom the speckle patterns generated by optical Physical Unclonable Functions\n(PUFs). The application of SIFT to the challenge response pairs (CRPs) protocol\nallows us to correctly authenticate a client while denying any other fraudulent\naccess. In this way, the authentication process is highly reliable even in\npresence of response rotation, zooming, and cropping that may occur in\nconsecutive PUF interrogations and to which other postprocessing algorithm are\nhighly sensitive. This characteristics together with the speed of the method\n(tens of microseconds for each operation) broaden the applicability and\nreliability of PUF to practical high-security authentication or merchandise\nanti-counterfeiting."
    },
    {
        "date": "2025-04",
        "title": "ClearVision: Leveraging CycleGAN and SigLIP-2 for Robust All-Weather Classification in Traffic Camera Imagery",
        "author": "Anush Lakshman Sivaraman, Kojo Adu-Gyamfi, Ibne Farabi Shihab, and Anuj Sharma",
        "link": "http://arxiv.org/abs/2504.19684v1",
        "abstract": "Accurate weather classification from low-quality traffic camera imagery\nremains a challenging task, particularly under adverse nighttime conditions. In\nthis study, we propose a scalable framework that combines generative domain\nadaptation with efficient contrastive learning to enhance classification\nperformance. Using CycleGAN-based domain translation, we improve the quality of\nnighttime images, enabling better feature extraction by downstream models.\nWhile the baseline EVA-02 model employing CLIP-based contrastive loss achieves\nan overall accuracy of 96.55\\%, it exhibits a significant performance gap\nbetween daytime (97.21\\%) and nighttime conditions (63.40\\%). Replacing CLIP\nwith the lightweight SigLIP-2 (Sigmoid contrastive loss) achieves a competitive\noverall accuracy of 94.00\\%, with substantial improvements in nighttime\nperformance (85.90\\% accuracy). The combination of Vision-SigLIP-2,\nText-SigLIP-2, CycleGAN, and contrastive training achieves the best nighttime\naccuracy (85.90\\%) among all models tested, while EVA-02 with CycleGAN\nmaintains the highest overall accuracy (97.01\\%) and per-class accuracies.\nThese findings demonstrate the potential of combining domain adaptation and\nefficient contrastive learning to build practical, resource-efficient weather\nclassification systems for intelligent transportation infrastructure."
    },
    {
        "date": "2025-04",
        "title": "Benchmarking Transferability: A Framework for Fair and Robust Evaluation",
        "author": "Alireza Kazemi, Helia Rezvani, and Mahsa Baktashmotlagh",
        "link": "http://arxiv.org/abs/2504.20121v1",
        "abstract": "Transferability scores aim to quantify how well a model trained on one domain\ngeneralizes to a target domain. Despite numerous methods proposed for measuring\ntransferability, their reliability and practical usefulness remain\ninconclusive, often due to differing experimental setups, datasets, and\nassumptions. In this paper, we introduce a comprehensive benchmarking framework\ndesigned to systematically evaluate transferability scores across diverse\nsettings. Through extensive experiments, we observe variations in how different\nmetrics perform under various scenarios, suggesting that current evaluation\npractices may not fully capture each method's strengths and limitations. Our\nfindings underscore the value of standardized assessment protocols, paving the\nway for more reliable transferability measures and better-informed model\nselection in cross-domain applications. Additionally, we achieved a 3.5\\%\nimprovement using our proposed metric for the head-training fine-tuning\nexperimental setup. Our code is available in this repository:\nhttps://github.com/alizkzm/pert_robust_platform."
    },
    {
        "date": "2025-04",
        "title": "BARIS: Boundary-Aware Refinement with Environmental Degradation Priors for Robust Underwater Instance Segmentation",
        "author": "Pin-Chi Pan, and Soo-Chang Pei",
        "link": "http://arxiv.org/abs/2504.19643v1",
        "abstract": "Underwater instance segmentation is challenging due to adverse visual\nconditions such as light attenuation, scattering, and color distortion, which\ndegrade model performance. In this work, we propose BARIS-Decoder\n(Boundary-Aware Refinement Decoder for Instance Segmentation), a framework that\nenhances segmentation accuracy through feature refinement. To address\nunderwater degradations, we introduce the Environmental Robust Adapter (ERA),\nwhich efficiently models underwater degradation patterns while reducing\ntrainable parameters by over 90\\% compared to full fine-tuning. The integration\nof BARIS-Decoder with ERA-tuning, referred to as BARIS-ERA, achieves\nstate-of-the-art performance, surpassing Mask R-CNN by 3.4 mAP with a Swin-B\nbackbone and 3.8 mAP with ConvNeXt V2. Our findings demonstrate the\neffectiveness of BARIS-ERA in advancing underwater instance segmentation,\nproviding a robust and efficient solution."
    },
    {
        "date": "2025-04",
        "title": "Towards Robust Multimodal Physiological Foundation Models: Handling Arbitrary Missing Modalities",
        "author": "Xi Fu, Wei-Bang Jiang, Yi Ding, and Cuntai Guan",
        "link": "http://arxiv.org/abs/2504.19596v1",
        "abstract": "Multimodal physiological signals, such as EEG, ECG, EOG, and EMG, are crucial\nfor healthcare and brain-computer interfaces. While existing methods rely on\nspecialized architectures and dataset-specific fusion strategies, they struggle\nto learn universal representations that generalize across datasets and handle\nmissing modalities at inference time. To address these issues, we propose\nPhysioOmni, a foundation model for multimodal physiological signal analysis\nthat models both homogeneous and heterogeneous features to decouple multimodal\nsignals and extract generic representations while maintaining compatibility\nwith arbitrary missing modalities. PhysioOmni trains a decoupled multimodal\ntokenizer, enabling masked signal pre-training via modality-invariant and\nmodality-specific objectives. To ensure adaptability to diverse and incomplete\nmodality combinations, the pre-trained encoders undergo resilient fine-tuning\nwith prototype alignment on downstream datasets. Extensive experiments on four\ndownstream tasks, emotion recognition, sleep stage classification, motor\nprediction, and mental workload detection, demonstrate that PhysioOmni achieves\nstate-of-the-art performance while maintaining strong robustness to missing\nmodalities. Our code and model weights will be released."
    },
    {
        "date": "2025-04",
        "title": "Adversarial Shallow Watermarking",
        "author": "Guobiao Li, Lei Tan, Yuliang Xue, Gaozhi Liu, Zhenxing Qian, Sheng Li, and Xinpeng Zhang",
        "link": "http://arxiv.org/abs/2504.19529v1",
        "abstract": "Recent advances in digital watermarking make use of deep neural networks for\nmessage embedding and extraction. They typically follow the ``encoder-noise\nlayer-decoder''-based architecture. By deliberately establishing a\ndifferentiable noise layer to simulate the distortion of the watermarked\nsignal, they jointly train the deep encoder and decoder to fit the noise layer\nto guarantee robustness. As a result, they are usually weak against unknown\ndistortions that are not used in their training pipeline. In this paper, we\npropose a novel watermarking framework to resist unknown distortions, namely\nAdversarial Shallow Watermarking (ASW). ASW utilizes only a shallow decoder\nthat is randomly parameterized and designed to be insensitive to distortions\nfor watermarking extraction. During the watermark embedding, ASW freezes the\nshallow decoder and adversarially optimizes a host image until its updated\nversion (i.e., the watermarked image) stably triggers the shallow decoder to\noutput the watermark message. During the watermark extraction, it accurately\nrecovers the message from the watermarked image by leveraging the insensitive\nnature of the shallow decoder against arbitrary distortions. Our ASW is\ntraining-free, encoder-free, and noise layer-free. Experiments indicate that\nthe watermarked images created by ASW have strong robustness against various\nunknown distortions. Compared to the existing ``encoder-noise layer-decoder''\napproaches, ASW achieves comparable results on known distortions and better\nrobustness on unknown distortions."
    },
    {
        "date": "2025-04",
        "title": "Security Steerability is All You Need",
        "author": "Itay Hazan, Idan Habler, Ron Bitton, and Itsik Mantin",
        "link": "http://arxiv.org/abs/2504.19521v2",
        "abstract": "The adoption of Generative AI (GenAI) in various applications inevitably\ncomes with expanding the attack surface, combining new security threats along\nwith the traditional ones. Consequently, numerous research and industrial\ninitiatives aim to mitigate these security threats in GenAI by developing\nmetrics and designing defenses. However, while most of the GenAI security work\nfocuses on universal threats (e.g. manipulating the LLM to generate forbidden\ncontent), there is significantly less discussion on application-level security\nand how to mitigate it. Thus, in this work we adopt an application-centric\napproach to GenAI security, and show that while LLMs cannot protect against\nad-hoc application specific threats, they can provide the framework for\napplications to protect themselves against such threats. Our first contribution\nis defining Security Steerability - a novel security measure for LLMs,\nassessing the model's capability to adhere to strict guardrails that are\ndefined in the system prompt ('Refrain from discussing about politics'). These\nguardrails, in case effective, can stop threats in the presence of malicious\nusers who attempt to circumvent the application and cause harm to its\nproviders. Our second contribution is a methodology to measure the security\nsteerability of LLMs, utilizing two newly-developed datasets: VeganRibs\nassesses the LLM behavior in forcing specific guardrails that are not security\nper se in the presence of malicious user that uses attack boosters (jailbreaks\nand perturbations), and ReverseText takes this approach further and measures\nthe LLM ability to force specific treatment of the user input as plain text\nwhile do user try to give it additional meanings..."
    },
    {
        "date": "2025-04",
        "title": "Security Bug Report Prediction Within and Across Projects: A Comparative Study of BERT and Random Forest",
        "author": "Farnaz Soltaniani, Mohammad Ghafari, and Mohammed Sayagh",
        "link": "http://arxiv.org/abs/2504.21037v1",
        "abstract": "Early detection of security bug reports (SBRs) is crucial for preventing\nvulnerabilities and ensuring system reliability. While machine learning models\nhave been developed for SBR prediction, their predictive performance still has\nroom for improvement. In this study, we conduct a comprehensive comparison\nbetween BERT and Random Forest (RF), a competitive baseline for predicting\nSBRs. The results show that RF outperforms BERT with a 34% higher average\nG-measure for within-project predictions. Adding only SBRs from various\nprojects improves both models' average performance. However, including both\nsecurity and nonsecurity bug reports significantly reduces RF's average\nperformance to 46%, while boosts BERT to its best average performance of 66%,\nsurpassing RF. In cross-project SBR prediction, BERT achieves a remarkable 62%\nG-measure, which is substantially higher than RF."
    },
    {
        "date": "2025-04",
        "title": "Can Differentially Private Fine-tuning LLMs Protect Against Privacy Attacks?",
        "author": "Hao Du, Shang Liu, and Yang Cao",
        "link": "http://arxiv.org/abs/2504.21036v2",
        "abstract": "Fine-tuning large language models (LLMs) has become an essential strategy for\nadapting them to specialized tasks; however, this process introduces\nsignificant privacy challenges, as sensitive training data may be inadvertently\nmemorized and exposed. Although differential privacy (DP) offers strong\ntheoretical guarantees against such leakage, its empirical privacy\neffectiveness on LLMs remains unclear, especially under different fine-tuning\nmethods. In this paper, we systematically investigate the impact of DP across\nfine-tuning methods and privacy budgets, using both data extraction and\nmembership inference attacks to assess empirical privacy risks. Our main\nfindings are as follows: (1) Differential privacy reduces model utility, but\nits impact varies significantly across different fine-tuning methods. (2)\nWithout DP, the privacy risks of models fine-tuned with different approaches\ndiffer considerably. (3) When DP is applied, even a relatively high privacy\nbudget can substantially lower privacy risk. (4) The privacy-utility trade-off\nunder DP training differs greatly among fine-tuning methods, with some methods\nbeing unsuitable for DP due to severe utility degradation. Our results provide\npractical guidance for privacy-conscious deployment of LLMs and pave the way\nfor future research on optimizing the privacy-utility trade-off in fine-tuning\nmethodologies."
    },
    {
        "date": "2025-04",
        "title": "The Cost of Performance: Breaking ThreadX with Kernel Object Masquerading Attacks",
        "author": "Xinhui Shao, Zhen Ling, Yue Zhang, Huaiyu Yan, Yumeng Wei, Lan Luo, Zixia Liu, Junzhou Luo, and Xinwen Fu",
        "link": "http://arxiv.org/abs/2504.19486v1",
        "abstract": "Microcontroller-based IoT devices often use embedded real-time operating\nsystems (RTOSs). Vulnerabilities in these embedded RTOSs can lead to\ncompromises of those IoT devices. Despite the significance of security\nprotections, the absence of standardized security guidelines results in various\nlevels of security risk across RTOS implementations. Our initial analysis\nreveals that popular RTOSs such as FreeRTOS lack essential security\nprotections. While Zephyr OS and ThreadX are designed and implemented with\nessential security protections, our closer examination uncovers significant\ndifferences in their implementations of system call parameter sanitization. We\nidentify a performance optimization practice in ThreadX that introduces\nsecurity vulnerabilities, allowing for the circumvention of parameter\nsanitization processes. Leveraging this insight, we introduce a novel attack\nnamed the Kernel Object Masquerading (KOM) Attack (as the attacker needs to\nmanipulate one or multiple kernel objects through carefully selected system\ncalls to launch the attack), demonstrating how attackers can exploit these\nvulnerabilities to access sensitive fields within kernel objects, potentially\nleading to unauthorized data manipulation, privilege escalation, or system\ncompromise. We introduce an automated approach involving under-constrained\nsymbolic execution to identify the KOM attacks and to understand the\nimplications. Experimental results demonstrate the feasibility of KOM attacks\non ThreadX-powered platforms. We reported our findings to the vendors, who\nrecognized the vulnerabilities, with Amazon and Microsoft acknowledging our\ncontribution on their websites."
    },
    {
        "date": "2025-04",
        "title": "FCGHunter: Towards Evaluating Robustness of Graph-Based Android Malware Detection",
        "author": "Shiwen Song, Xiaofei Xie, Ruitao Feng, Qi Guo, and Sen Chen",
        "link": "http://arxiv.org/abs/2504.19456v1",
        "abstract": "Graph-based detection methods leveraging Function Call Graphs (FCGs) have\nshown promise for Android malware detection (AMD) due to their semantic\ninsights. However, the deployment of malware detectors in dynamic and hostile\nenvironments raises significant concerns about their robustness. While recent\napproaches evaluate the robustness of FCG-based detectors using adversarial\nattacks, their effectiveness is constrained by the vast perturbation space,\nparticularly across diverse models and features.\n  To address these challenges, we introduce FCGHunter, a novel robustness\ntesting framework for FCG-based AMD systems. Specifically, FCGHunter employs\ninnovative techniques to enhance exploration and exploitation within this huge\nsearch space. Initially, it identifies critical areas within the FCG related to\nmalware behaviors to narrow down the perturbation space. We then develop a\ndependency-aware crossover and mutation method to enhance the validity and\ndiversity of perturbations, generating diverse FCGs. Furthermore, FCGHunter\nleverages multi-objective feedback to select perturbed FCGs, significantly\nimproving the search process with interpretation-based feature change feedback.\n  Extensive evaluations across 40 scenarios demonstrate that FCGHunter achieves\nan average attack success rate of 87.9%, significantly outperforming baselines\nby at least 44.7%. Notably, FCGHunter achieves a 100% success rate on robust\nmodels (e.g., AdaBoost with MalScan), where baselines achieve only 11% or are\ninapplicable."
    },
    {
        "date": "2025-04",
        "title": "Provably Secure Public-Key Steganography Based on Admissible Encoding",
        "author": "Xin Zhang, Kejiang Chen, Na Zhao, Weiming Zhang, and Nenghai Yu",
        "link": "http://arxiv.org/abs/2504.19454v1",
        "abstract": "The technique of hiding secret messages within seemingly harmless covertext\nto evade examination by censors with rigorous security proofs is known as\nprovably secure steganography (PSS). PSS evolves from symmetric key\nsteganography to public-key steganography, functioning without the requirement\nof a pre-shared key and enabling the extension to multi-party covert\ncommunication and identity verification mechanisms. Recently, a public-key\nsteganography method based on elliptic curves was proposed, which uses point\ncompression to eliminate the algebraic structure of curve points. However, this\nmethod has strict requirements on the curve parameters and is only available on\nhalf of the points. To overcome these limitations, this paper proposes a more\ngeneral elliptic curve public key steganography method based on admissible\nencoding. By applying the tensor square function to the known well-distributed\nencoding, we construct admissible encoding, which can create the pseudo-random\npublic-key encryption function. The theoretical analysis and experimental\nresults show that the proposed provable secure public-key steganography method\ncan be deployed on all types of curves and utilize all points on the curve."
    },
    {
        "date": "2025-04",
        "title": "JailbreaksOverTime: Detecting Jailbreak Attacks Under Distribution Shift",
        "author": "Julien Piet, Xiao Huang, Dennis Jacob, Annabella Chow, Maha Alrashed, Geng Zhao, Zhanhao Hu, Chawin Sitawarin, Basel Alomair, and David Wagner",
        "link": "http://arxiv.org/abs/2504.19440v1",
        "abstract": "Safety and security remain critical concerns in AI deployment. Despite safety\ntraining through reinforcement learning with human feedback (RLHF) [ 32],\nlanguage models remain vulnerable to jailbreak attacks that bypass safety\nguardrails. Universal jailbreaks - prefixes that can circumvent alignment for\nany payload - are particularly concerning. We show empirically that jailbreak\ndetection systems face distribution shift, with detectors trained at one point\nin time performing poorly against newer exploits. To study this problem, we\nrelease JailbreaksOverTime, a comprehensive dataset of timestamped real user\ninteractions containing both benign requests and jailbreak attempts collected\nover 10 months. We propose a two-pronged method for defenders to detect new\njailbreaks and continuously update their detectors. First, we show how to use\ncontinuous learning to detect jailbreaks and adapt rapidly to new emerging\njailbreaks. While detectors trained at a single point in time eventually fail\ndue to drift, we find that universal jailbreaks evolve slowly enough for\nself-training to be effective. Retraining our detection model weekly using its\nown labels - with no new human labels - reduces the false negative rate from 4%\nto 0.3% at a false positive rate of 0.1%. Second, we introduce an unsupervised\nactive monitoring approach to identify novel jailbreaks. Rather than\nclassifying inputs directly, we recognize jailbreaks by their behavior,\nspecifically, their ability to trigger models to respond to known-harmful\nprompts. This approach has a higher false negative rate (4.1%) than supervised\nmethods, but it successfully identified some out-of-distribution attacks that\nwere missed by the continuous learning approach."
    },
    {
        "date": "2025-04",
        "title": "SAGA: A Security Architecture for Governing AI Agentic Systems",
        "author": "Georgios Syros, Anshuman Suri, Cristina Nita-Rotaru, and Alina Oprea",
        "link": "http://arxiv.org/abs/2504.21034v1",
        "abstract": "Large Language Model (LLM)-based agents increasingly interact, collaborate,\nand delegate tasks to one another autonomously with minimal human interaction.\nIndustry guidelines for agentic system governance emphasize the need for users\nto maintain comprehensive control over their agents, mitigating potential\ndamage from malicious agents. Several proposed agentic system designs address\nagent identity, authorization, and delegation, but remain purely theoretical,\nwithout concrete implementation and evaluation. Most importantly, they do not\nprovide user-controlled agent management. To address this gap, we propose SAGA,\na Security Architecture for Governing Agentic systems, that offers user\noversight over their agents' lifecycle. In our design, users register their\nagents with a central entity, the Provider, that maintains agents contact\ninformation, user-defined access control policies, and helps agents enforce\nthese policies on inter-agent communication. We introduce a cryptographic\nmechanism for deriving access control tokens, that offers fine-grained control\nover an agent's interaction with other agents, balancing security and\nperformance consideration. We evaluate SAGA on several agentic tasks, using\nagents in different geolocations, and multiple on-device and cloud LLMs,\ndemonstrating minimal performance overhead with no impact on underlying task\nutility in a wide range of conditions. Our architecture enables secure and\ntrustworthy deployment of autonomous agents, accelerating the responsible\nadoption of this technology in sensitive environments."
    },
    {
        "date": "2025-04",
        "title": "PolyTouch: A Robust Multi-Modal Tactile Sensor for Contact-rich Manipulation Using Tactile-Diffusion Policies",
        "author": "Jialiang Zhao, Naveen Kuppuswamy, Siyuan Feng, Benjamin Burchfiel, and Edward Adelson",
        "link": "http://arxiv.org/abs/2504.19341v1",
        "abstract": "Achieving robust dexterous manipulation in unstructured domestic environments\nremains a significant challenge in robotics. Even with state-of-the-art robot\nlearning methods, haptic-oblivious control strategies (i.e. those relying only\non external vision and/or proprioception) often fall short due to occlusions,\nvisual complexities, and the need for precise contact interaction control. To\naddress these limitations, we introduce PolyTouch, a novel robot finger that\nintegrates camera-based tactile sensing, acoustic sensing, and peripheral\nvisual sensing into a single design that is compact and durable. PolyTouch\nprovides high-resolution tactile feedback across multiple temporal scales,\nwhich is essential for efficiently learning complex manipulation tasks.\nExperiments demonstrate an at least 20-fold increase in lifespan over\ncommercial tactile sensors, with a design that is both easy to manufacture and\nscalable. We then use this multi-modal tactile feedback along with\nvisuo-proprioceptive observations to synthesize a tactile-diffusion policy from\nhuman demonstrations; the resulting contact-aware control policy significantly\noutperforms haptic-oblivious policies in multiple contact-aware manipulation\npolicies. This paper highlights how effectively integrating multi-modal contact\nsensing can hasten the development of effective contact-aware manipulation\npolicies, paving the way for more reliable and versatile domestic robots. More\ninformation can be found at https://polytouch.alanz.info/"
    },
    {
        "date": "2025-04",
        "title": "Evaluating Organization Security: User Stories of European Union NIS2 Directive",
        "author": "Mari Seeba, Magnus Valgre, and Raimundas Matulevi\u010dius",
        "link": "http://arxiv.org/abs/2504.19222v1",
        "abstract": "The NIS2 directive requires EU Member States to ensure a consistently high\nlevel of cybersecurity by setting risk-management measures for essential and\nimportant entities. Evaluations are necessary to assess whether the required\nsecurity level is met. This involves understanding the needs and goals of\ndifferent personas defined by NIS2, who benefit from evaluation results. In\nthis paper, we consider how NIS2 user stories support the evaluation of the\nlevel of information security in organizations. Using requirements elicitation\nprinciples, we extracted the legal requirements from NIS2 from our narrowed\nscope, identified six key personas and their goals, formulated user stories\nbased on the gathered information, and validated the usability and relevance of\nthe user stories with security evaluation instruments or methods we found from\nthe literature. The defined user stories help to adjust existing instruments\nand methods of assessing the security level to comply with NIS2. On the other\nhand, user stories enable us to see the patterns related to security evaluation\nwhen developing new NIS2-compliant security evaluation methods to optimize the\nadministrative burden of entities."
    },
    {
        "date": "2025-04",
        "title": "Generative Adversarial Network based Voice Conversion: Techniques, Challenges, and Recent Advancements",
        "author": "Sandipan Dhar, Nanda Dulal Jana, and Swagatam Das",
        "link": "http://arxiv.org/abs/2504.19197v1",
        "abstract": "Voice conversion (VC) stands as a crucial research area in speech synthesis,\nenabling the transformation of a speaker's vocal characteristics to resemble\nanother while preserving the linguistic content. This technology has broad\napplications, including automated movie dubbing, speech-to-singing conversion,\nand assistive devices for pathological speech rehabilitation. With the\nincreasing demand for high-quality and natural-sounding synthetic voices,\nresearchers have developed a wide range of VC techniques. Among these,\ngenerative adversarial network (GAN)-based approaches have drawn considerable\nattention for their powerful feature-mapping capabilities and potential to\nproduce highly realistic speech. Despite notable advancements, challenges such\nas ensuring training stability, maintaining linguistic consistency, and\nachieving perceptual naturalness continue to hinder progress in GAN-based VC\nsystems. This systematic review presents a comprehensive analysis of the voice\nconversion landscape, highlighting key techniques, key challenges, and the\ntransformative impact of GANs in the field. The survey categorizes existing\nmethods, examines technical obstacles, and critically evaluates recent\ndevelopments in GAN-based VC. By consolidating and synthesizing research\nfindings scattered across the literature, this review provides a structured\nunderstanding of the strengths and limitations of different approaches. The\nsignificance of this survey lies in its ability to guide future research by\nidentifying existing gaps, proposing potential directions, and offering\ninsights for building more robust and efficient VC systems. Overall, this work\nserves as an essential resource for researchers, developers, and practitioners\naiming to advance the state-of-the-art (SOTA) in voice conversion technology."
    },
    {
        "date": "2025-04",
        "title": "SPC: Evolving Self-Play Critic via Adversarial Games for LLM Reasoning",
        "author": "Jiaqi Chen, Bang Zhang, Ruotian Ma, Peisong Wang, Xiaodan Liang, Zhaopeng Tu, Xiaolong Li, and Kwan-Yee K. Wong",
        "link": "http://arxiv.org/abs/2504.19162v1",
        "abstract": "Evaluating the step-by-step reliability of large language model (LLM)\nreasoning, such as Chain-of-Thought, remains challenging due to the difficulty\nand cost of obtaining high-quality step-level supervision. In this paper, we\nintroduce Self-Play Critic (SPC), a novel approach where a critic model evolves\nits ability to assess reasoning steps through adversarial self-play games,\neliminating the need for manual step-level annotation. SPC involves fine-tuning\ntwo copies of a base model to play two roles, namely a \"sneaky generator\" that\ndeliberately produces erroneous steps designed to be difficult to detect, and a\n\"critic\" that analyzes the correctness of reasoning steps. These two models\nengage in an adversarial game in which the generator aims to fool the critic,\nwhile the critic model seeks to identify the generator's errors. Using\nreinforcement learning based on the game outcomes, the models iteratively\nimprove; the winner of each confrontation receives a positive reward and the\nloser receives a negative reward, driving continuous self-evolution.\nExperiments on three reasoning process benchmarks (ProcessBench, PRM800K,\nDeltaBench) demonstrate that our SPC progressively enhances its error detection\ncapabilities (e.g., accuracy increases from 70.8% to 77.7% on ProcessBench) and\nsurpasses strong baselines, including distilled R1 model. Furthermore, applying\nSPC to guide the test-time search of diverse LLMs significantly improves their\nmathematical reasoning performance on MATH500 and AIME2024, outperforming\nstate-of-the-art process reward models."
    },
    {
        "date": "2025-04",
        "title": "Comparative Analysis of AI-Driven Security Approaches in DevSecOps: Challenges, Solutions, and Future Directions",
        "author": "Farid Binbeshr, and Muhammad Imam",
        "link": "http://arxiv.org/abs/2504.19154v1",
        "abstract": "The integration of security within DevOps, known as DevSecOps, has gained\ntraction in modern software development to address security vulnerabilities\nwhile maintaining agility. Artificial Intelligence (AI) and Machine Learning\n(ML) have been increasingly leveraged to enhance security automation, threat\ndetection, and compliance enforcement. However, existing studies primarily\nfocus on individual aspects of AI-driven security in DevSecOps, lacking a\nstructured comparison of methodologies. This study conducts a systematic\nliterature review (SLR) to analyze and compare AI-driven security solutions in\nDevSecOps, evaluating their technical capabilities, implementation challenges,\nand operational impacts. The findings reveal gaps in empirical validation,\nscalability, and integration of AI in security automation. The study highlights\nbest practices, identifies research gaps, and proposes future directions for\noptimizing AI-based security frameworks in DevSecOps."
    },
    {
        "date": "2025-04",
        "title": "Fast and Robust: Task Sampling with Posterior and Diversity Synergies for Adaptive Decision-Makers in Randomized Environments",
        "author": "Yun Qu, Qi, Wang, Yixiu Mao, Yiqin Lv, and Xiangyang Ji",
        "link": "http://arxiv.org/abs/2504.19139v1",
        "abstract": "Task robust adaptation is a long-standing pursuit in sequential\ndecision-making. Some risk-averse strategies, e.g., the conditional\nvalue-at-risk principle, are incorporated in domain randomization or meta\nreinforcement learning to prioritize difficult tasks in optimization, which\ndemand costly intensive evaluations. The efficiency issue prompts the\ndevelopment of robust active task sampling to train adaptive policies, where\nrisk-predictive models are used to surrogate policy evaluation. This work\ncharacterizes the optimization pipeline of robust active task sampling as a\nMarkov decision process, posits theoretical and practical insights, and\nconstitutes robustness concepts in risk-averse scenarios. Importantly, we\npropose an easy-to-implement method, referred to as Posterior and Diversity\nSynergized Task Sampling (PDTS), to accommodate fast and robust sequential\ndecision-making. Extensive experiments show that PDTS unlocks the potential of\nrobust active task sampling, significantly improves the zero-shot and few-shot\nadaptation robustness in challenging tasks, and even accelerates the learning\nprocess under certain scenarios. Our project website is at\nhttps://thu-rllab.github.io/PDTS_project_page."
    },
    {
        "date": "2025-04",
        "title": "Security Vulnerabilities in Quantum Cloud Systems: A Survey on Emerging Threats",
        "author": "Justin Coupel, and Tasnuva Farheen",
        "link": "http://arxiv.org/abs/2504.19064v1",
        "abstract": "Quantum computing is becoming increasingly widespread due to the potential\nand capabilities to solve complex problems beyond the scope of classical\ncomputers. As Quantum Cloud services are adopted by businesses and research\ngroups, they allow for greater progress and application in many fields.\nHowever, the inherent vulnerabilities of these environments pose significant\nsecurity concerns. This survey delivers a comprehensive analysis of the\nsecurity challenges that emerged in quantum cloud systems, with a distinct\nfocus on multi-tenant vulnerabilities and the classical-quantum interface. Key\nthreats such as crosstalk attacks, quantum-specific side-channel\nvulnerabilities, and insider threats are all examined, as well as their effects\non the confidentiality, integrity, and availability of quantum circuits. The\ndesign and implementation of various quantum architectures from quantum cloud\nproviders are also discussed. In addition, this paper delves into emerging\nquantum security solutions and best practices to mitigate these risks. This\nsurvey offers insights into current research gaps and proposes future\ndirections for secure and resilient quantum cloud infrastructures."
    },
    {
        "date": "2025-04",
        "title": "BinPool: A Dataset of Vulnerabilities for Binary Security Analysis",
        "author": "Sima Arasteh, Georgios Nikitopoulos, Wei-Cheng Wu, Nicolaas Weideman, Aaron Portnoy, Mukund Raghothaman, and Christophe Hauser",
        "link": "http://arxiv.org/abs/2504.19055v1",
        "abstract": "The development of machine learning techniques for discovering software\nvulnerabilities relies fundamentally on the availability of appropriate\ndatasets. The ideal dataset consists of a large and diverse collection of\nreal-world vulnerabilities, paired so as to contain both vulnerable and patched\nversions of each program. Naturally, collecting such datasets is a laborious\nand time-consuming task. Within the specific domain of vulnerability discovery\nin binary code, previous datasets are either publicly unavailable, lack\nsemantic diversity, involve artificially introduced vulnerabilities, or were\ncollected using static analyzers, thereby themselves containing incorrectly\nlabeled example programs.\n  In this paper, we describe a new publicly available dataset which we dubbed\nBinpool, containing numerous samples of\n  vulnerable versions of Debian packages across the years. The dataset was\nautomatically curated, and contains both vulnerable and patched versions of\neach program, compiled at four different optimization levels. Overall, the\ndataset covers 603 distinct CVEs across 89 CWE classes, 162 Debian packages,\nand contains 6144 binaries. We argue that this dataset is suitable for\nevaluating a range of security analysis tools, including for vulnerability\ndiscovery, binary function similarity, and plagiarism detection."
    },
    {
        "date": "2025-04",
        "title": "DiCE-Extended: A Robust Approach to Counterfactual Explanations in Machine Learning",
        "author": "Volkan Bakir, Polat Goktas, and Sureyya Akyuz",
        "link": "http://arxiv.org/abs/2504.19027v1",
        "abstract": "Explainable artificial intelligence (XAI) has become increasingly important\nin decision-critical domains such as healthcare, finance, and law.\nCounterfactual (CF) explanations, a key approach in XAI, provide users with\nactionable insights by suggesting minimal modifications to input features that\nlead to different model outcomes. Despite significant advancements, existing CF\ngeneration methods often struggle to balance proximity, diversity, and\nrobustness, limiting their real-world applicability. A widely adopted\nframework, Diverse Counterfactual Explanations (DiCE), emphasizes diversity but\nlacks robustness, making CF explanations sensitive to perturbations and domain\nconstraints. To address these challenges, we introduce DiCE-Extended, an\nenhanced CF explanation framework that integrates multi-objective optimization\ntechniques to improve robustness while maintaining interpretability. Our\napproach introduces a novel robustness metric based on the Dice-Sorensen\ncoefficient, ensuring stability under small input variations. Additionally, we\nrefine CF generation using weighted loss components (lambda_p, lambda_d,\nlambda_r) to balance proximity, diversity, and robustness. We empirically\nvalidate DiCE-Extended on benchmark datasets (COMPAS, Lending Club, German\nCredit, Adult Income) across multiple ML backends (Scikit-learn, PyTorch,\nTensorFlow). Results demonstrate improved CF validity, stability, and alignment\nwith decision boundaries compared to standard DiCE-generated explanations. Our\nfindings highlight the potential of DiCE-Extended in generating more reliable\nand interpretable CFs for high-stakes applications. Future work will explore\nadaptive optimization techniques and domain-specific constraints to further\nenhance CF generation in real-world scenarios."
    },
    {
        "date": "2025-04",
        "title": "Graph of Attacks: Improved Black-Box and Interpretable Jailbreaks for LLMs",
        "author": "Mohammad Akbar-Tajari, Mohammad Taher Pilehvar, and Mohammad Mahmoody",
        "link": "http://arxiv.org/abs/2504.19019v1",
        "abstract": "The challenge of ensuring Large Language Models (LLMs) align with societal\nstandards is of increasing interest, as these models are still prone to\nadversarial jailbreaks that bypass their safety mechanisms. Identifying these\nvulnerabilities is crucial for enhancing the robustness of LLMs against such\nexploits. We propose Graph of ATtacks (GoAT), a method for generating\nadversarial prompts to test the robustness of LLM alignment using the Graph of\nThoughts framework [Besta et al., 2024]. GoAT excels at generating highly\neffective jailbreak prompts with fewer queries to the victim model than\nstate-of-the-art attacks, achieving up to five times better jailbreak success\nrate against robust models like Llama. Notably, GoAT creates high-quality,\nhuman-readable prompts without requiring access to the targeted model's\nparameters, making it a black-box attack. Unlike approaches constrained by\ntree-based reasoning, GoAT's reasoning is based on a more intricate graph\nstructure. By making simultaneous attack paths aware of each other's progress,\nthis dynamic framework allows a deeper integration and refinement of reasoning\npaths, significantly enhancing the collaborative exploration of adversarial\nvulnerabilities in LLMs. At a technical level, GoAT starts with a graph\nstructure and iteratively refines it by combining and improving thoughts,\nenabling synergy between different thought paths. The code for our\nimplementation can be found at: https://github.com/GoAT-pydev/Graph_of_Attacks."
    },
    {
        "date": "2025-04",
        "title": "Deep Learning-Based Multi-Modal Fusion for Robust Robot Perception and Navigation",
        "author": "Delun Lai, Yeyubei Zhang, Yunchong Liu, Chaojie Li, and Huadong Mo",
        "link": "http://arxiv.org/abs/2504.19002v1",
        "abstract": "This paper introduces a novel deep learning-based multimodal fusion\narchitecture aimed at enhancing the perception capabilities of autonomous\nnavigation robots in complex environments. By utilizing innovative feature\nextraction modules, adaptive fusion strategies, and time-series modeling\nmechanisms, the system effectively integrates RGB images and LiDAR data. The\nkey contributions of this work are as follows: a. the design of a lightweight\nfeature extraction network to enhance feature representation; b. the\ndevelopment of an adaptive weighted cross-modal fusion strategy to improve\nsystem robustness; and c. the incorporation of time-series information modeling\nto boost dynamic scene perception accuracy. Experimental results on the KITTI\ndataset demonstrate that the proposed approach increases navigation and\npositioning accuracy by 3.5% and 2.2%, respectively, while maintaining\nreal-time performance. This work provides a novel solution for autonomous robot\nnavigation in complex environments."
    },
    {
        "date": "2025-04",
        "title": "Unveiling and Mitigating Adversarial Vulnerabilities in Iterative Optimizers",
        "author": "Elad Sofer, Tomer Shaked, Caroline Chaux, and Nir Shlezinger",
        "link": "http://arxiv.org/abs/2504.19000v1",
        "abstract": "Machine learning (ML) models are often sensitive to carefully crafted yet\nseemingly unnoticeable perturbations. Such adversarial examples are considered\nto be a property of ML models, often associated with their black-box operation\nand sensitivity to features learned from data. This work examines the\nadversarial sensitivity of non-learned decision rules, and particularly of\niterative optimizers. Our analysis is inspired by the recent developments in\ndeep unfolding, which cast such optimizers as ML models. We show that\nnon-learned iterative optimizers share the sensitivity to adversarial examples\nof ML models, and that attacking iterative optimizers effectively alters the\noptimization objective surface in a manner that modifies the minima sought. We\nthen leverage the ability to cast iteration-limited optimizers as ML models to\nenhance robustness via adversarial training. For a class of proximal gradient\noptimizers, we rigorously prove how their learning affects adversarial\nsensitivity. We numerically back our findings, showing the vulnerability of\nvarious optimizers, as well as the robustness induced by unfolding and\nadversarial training."
    },
    {
        "date": "2025-04",
        "title": "Safety Interventions against Adversarial Patches in an Open-Source Driver Assistance System",
        "author": "Cheng Chen, Grant Xiao, Daehyun Lee, Lishan Yang, Evgenia Smirni, Homa Alemzadeh, and Xugui Zhou",
        "link": "http://arxiv.org/abs/2504.18990v1",
        "abstract": "Drivers are becoming increasingly reliant on advanced driver assistance\nsystems (ADAS) as autonomous driving technology becomes more popular and\ndeveloped with advanced safety features to enhance road safety. However, the\nincreasing complexity of the ADAS makes autonomous vehicles (AVs) more exposed\nto attacks and accidental faults. In this paper, we evaluate the resilience of\na widely used ADAS against safety-critical attacks that target perception\ninputs. Various safety mechanisms are simulated to assess their impact on\nmitigating attacks and enhancing ADAS resilience. Experimental results\nhighlight the importance of timely intervention by human drivers and automated\nsafety mechanisms in preventing accidents in both driving and lateral\ndirections and the need to resolve conflicts among safety interventions to\nenhance system resilience and reliability."
    },
    {
        "date": "2025-04",
        "title": "SONNI: Secure Oblivious Neural Network Inference",
        "author": "Luke Sperling, and Sandeep S. Kulkarni",
        "link": "http://arxiv.org/abs/2504.18974v1",
        "abstract": "In the standard privacy-preserving Machine learning as-a-service (MLaaS)\nmodel, the client encrypts data using homomorphic encryption and uploads it to\na server for computation. The result is then sent back to the client for\ndecryption. It has become more and more common for the computation to be\noutsourced to third-party servers. In this paper we identify a weakness in this\nprotocol that enables a completely undetectable novel model-stealing attack\nthat we call the Silver Platter attack. This attack works even under multikey\nencryption that prevents a simple collusion attack to steal model parameters.\nWe also propose a mitigation that protects privacy even in the presence of a\nmalicious server and malicious client or model provider (majority dishonest).\nWhen compared to a state-of-the-art but small encrypted model with 32k\nparameters, we preserve privacy with a failure chance of 1.51 x 10^-28 while\nbatching capability is reduced by 0.2%. Our approach uses a novel\nresults-checking protocol that ensures the computation was performed correctly\nwithout violating honest clients' data privacy. Even with collusion between the\nclient and the server, they are unable to steal model parameters. Additionally,\nthe model provider cannot learn any client data if maliciously working with the\nserver."
    },
    {
        "date": "2025-04",
        "title": "Speaker Retrieval in the Wild: Challenges, Effectiveness and Robustness",
        "author": "Erfan Loweimi, Mengjie Qian, Kate Knill, and Mark Gales",
        "link": "http://arxiv.org/abs/2504.18950v2",
        "abstract": "There is a growing abundance of publicly available or company-owned\naudio/video archives, highlighting the increasing importance of efficient\naccess to desired content and information retrieval from these archives. This\npaper investigates the challenges, solutions, effectiveness, and robustness of\nspeaker retrieval systems developed \"in the wild\" which involves addressing two\nprimary challenges: extraction of task-relevant labels from limited metadata\nfor system development and evaluation, as well as the unconstrained acoustic\nconditions encountered in the archive, ranging from quiet studios to adverse\nnoisy environments. While we focus on the publicly-available BBC Rewind archive\n(spanning 1948 to 1979), our framework addresses the broader issue of speaker\nretrieval on extensive and possibly aged archives with no control over the\ncontent and acoustic conditions. Typically, these archives offer a brief and\ngeneral file description, mostly inadequate for specific applications like\nspeaker retrieval, and manual annotation of such large-scale archives is\nunfeasible. We explore various aspects of system development (e.g., speaker\ndiarisation, embedding extraction, query selection) and analyse the challenges,\npossible solutions, and their functionality. To evaluate the performance, we\nconduct systematic experiments in both clean setup and against various\ndistortions simulating real-world applications. Our findings demonstrate the\neffectiveness and robustness of the developed speaker retrieval systems,\nestablishing the versatility and scalability of the proposed framework for a\nwide range of applications beyond the BBC Rewind corpus."
    },
    {
        "date": "2025-04",
        "title": "Sim-to-Real: An Unsupervised Noise Layer for Screen-Camera Watermarking Robustness",
        "author": "Yufeng Wu, Xin Liao, Baowei Wang, Han Fang, Xiaoshuai Wu, and Guiling Wang",
        "link": "http://arxiv.org/abs/2504.18906v1",
        "abstract": "Unauthorized screen capturing and dissemination pose severe security threats\nsuch as data leakage and information theft. Several studies propose robust\nwatermarking methods to track the copyright of Screen-Camera (SC) images,\nfacilitating post-hoc certification against infringement. These techniques\ntypically employ heuristic mathematical modeling or supervised neural network\nfitting as the noise layer, to enhance watermarking robustness against SC.\nHowever, both strategies cannot fundamentally achieve an effective\napproximation of SC noise. Mathematical simulation suffers from biased\napproximations due to the incomplete decomposition of the noise and the absence\nof interdependence among the noise components. Supervised networks require\npaired data to train the noise-fitting model, and it is difficult for the model\nto learn all the features of the noise. To address the above issues, we propose\nSimulation-to-Real (S2R). Specifically, an unsupervised noise layer employs\nunpaired data to learn the discrepancy between the modeling simulated noise\ndistribution and the real-world SC noise distribution, rather than directly\nlearning the mapping from sharp images to real-world images. Learning this\ntransformation from simulation to reality is inherently simpler, as it\nprimarily involves bridging the gap in noise distributions, instead of the\ncomplex task of reconstructing fine-grained image details. Extensive\nexperimental results validate the efficacy of the proposed method,\ndemonstrating superior watermark robustness and generalization compared to\nthose of state-of-the-art methods."
    },
    {
        "date": "2025-04",
        "title": "Latent Adversarial Training Improves the Representation of Refusal",
        "author": "Alexandra Abbas, Nora Petrova, Helios Ael Lyons, and Natalia Perez-Campanero",
        "link": "http://arxiv.org/abs/2504.18872v1",
        "abstract": "Recent work has shown that language models' refusal behavior is primarily\nencoded in a single direction in their latent space, making it vulnerable to\ntargeted attacks. Although Latent Adversarial Training (LAT) attempts to\nimprove robustness by introducing noise during training, a key question\nremains: How does this noise-based training affect the underlying\nrepresentation of refusal behavior? Understanding this encoding is crucial for\nevaluating LAT's effectiveness and limitations, just as the discovery of linear\nrefusal directions revealed vulnerabilities in traditional supervised safety\nfine-tuning (SSFT).\n  Through the analysis of Llama 2 7B, we examine how LAT reorganizes the\nrefusal behavior in the model's latent space compared to SSFT and embedding\nspace adversarial training (AT). By computing activation differences between\nharmful and harmless instruction pairs and applying Singular Value\nDecomposition (SVD), we find that LAT significantly alters the refusal\nrepresentation, concentrating it in the first two SVD components which explain\napproximately 75 percent of the activation differences variance - significantly\nhigher than in reference models. This concentrated representation leads to more\neffective and transferable refusal vectors for ablation attacks: LAT models\nshow improved robustness when attacked with vectors from reference models but\nbecome more vulnerable to self-generated vectors compared to SSFT and AT. Our\nfindings suggest that LAT's training perturbations enable a more comprehensive\nrepresentation of refusal behavior, highlighting both its potential strengths\nand vulnerabilities for improving model safety."
    },
    {
        "date": "2025-04",
        "title": "Zero-Day Botnet Attack Detection in IoV: A Modular Approach Using Isolation Forests and Particle Swarm Optimization",
        "author": "Abdelaziz Amara korba, Nour Elislem Karabadji, and Yacine Ghamri-Doudane",
        "link": "http://arxiv.org/abs/2504.18814v1",
        "abstract": "The Internet of Vehicles (IoV) is transforming transportation by enhancing\nconnectivity and enabling autonomous driving. However, this increased\ninterconnectivity introduces new security vulnerabilities. Bot malware and\ncyberattacks pose significant risks to Connected and Autonomous Vehicles\n(CAVs), as demonstrated by real-world incidents involving remote vehicle system\ncompromise. To address these challenges, we propose an edge-based Intrusion\nDetection System (IDS) that monitors network traffic to and from CAVs. Our\ndetection model is based on a meta-ensemble classifier capable of recognizing\nknown (Nday) attacks and detecting previously unseen (zero-day) attacks. The\napproach involves training multiple Isolation Forest (IF) models on\nMulti-access Edge Computing (MEC) servers, with each IF specialized in\nidentifying a specific type of botnet attack. These IFs, either trained locally\nor shared by other MEC nodes, are then aggregated using a Particle Swarm\nOptimization (PSO) based stacking strategy to construct a robust\nmeta-classifier. The proposed IDS has been evaluated on a vehicular botnet\ndataset, achieving an average detection rate of 92.80% for N-day attacks and\n77.32% for zero-day attacks. These results highlight the effectiveness of our\nsolution in detecting both known and emerging threats, providing a scalable and\nadaptive defense mechanism for CAVs within the IoV ecosystem."
    },
    {
        "date": "2025-04",
        "title": "Performance of Machine Learning Classifiers for Anomaly Detection in Cyber Security Applications",
        "author": "Markus Haug, and Gissel Velarde",
        "link": "http://arxiv.org/abs/2504.18771v1",
        "abstract": "This work empirically evaluates machine learning models on two imbalanced\npublic datasets (KDDCUP99 and Credit Card Fraud 2013). The method includes data\npreparation, model training, and evaluation, using an 80/20 (train/test) split.\nModels tested include eXtreme Gradient Boosting (XGB), Multi Layer Perceptron\n(MLP), Generative Adversarial Network (GAN), Variational Autoencoder (VAE), and\nMultiple-Objective Generative Adversarial Active Learning (MO-GAAL), with XGB\nand MLP further combined with Random-Over-Sampling (ROS) and\nSelf-Paced-Ensemble (SPE). Evaluation involves 5-fold cross-validation and\nimputation techniques (mean, median, and IterativeImputer) with 10, 20, 30, and\n50 % missing data. Findings show XGB and MLP outperform generative models.\nIterativeImputer results are comparable to mean and median, but not recommended\nfor large datasets due to increased complexity and execution time. The code\nused is publicly available on GitHub (github.com/markushaug/acr-25)."
    },
    {
        "date": "2025-04",
        "title": "PICO: Secure Transformers via Robust Prompt Isolation and Cybersecurity Oversight",
        "author": "Ben Goertzel, and Paulos Yibelo",
        "link": "http://arxiv.org/abs/2504.21029v1",
        "abstract": "We propose a robust transformer architecture designed to prevent prompt\ninjection attacks and ensure secure, reliable response generation. Our PICO\n(Prompt Isolation and Cybersecurity Oversight) framework structurally separates\ntrusted system instructions from untrusted user inputs through dual channels\nthat are processed independently and merged only by a controlled, gated fusion\nmechanism. In addition, we integrate a specialized Security Expert Agent within\na Mixture-of-Experts (MoE) framework and incorporate a Cybersecurity Knowledge\nGraph (CKG) to supply domain-specific reasoning. Our training design further\nensures that the system prompt branch remains immutable while the rest of the\nnetwork learns to handle adversarial inputs safely. This PICO framework is\npresented via a general mathematical formulation, then elaborated in terms of\nthe specifics of transformer architecture, and fleshed out via hypothetical\ncase studies including Policy Puppetry attacks. While the most effective\nimplementation may involve training transformers in a PICO-based way from\nscratch, we also present a cost-effective fine-tuning approach."
    },
    {
        "date": "2025-04",
        "title": "Intelligent Attacks and Defense Methods in Federated Learning-enabled Energy-Efficient Wireless Networks",
        "author": "Han Zhang, Hao Zhou, Medhat Elsayed, Majid Bavand, Raimundas Gaigalas, Yigit Ozcan, and Melike Erol-Kantarci",
        "link": "http://arxiv.org/abs/2504.18519v1",
        "abstract": "Federated learning (FL) is a promising technique for learning-based functions\nin wireless networks, thanks to its distributed implementation capability. On\nthe other hand, distributed learning may increase the risk of exposure to\nmalicious attacks where attacks on a local model may spread to other models by\nparameter exchange. Meanwhile, such attacks can be hard to detect due to the\ndynamic wireless environment, especially considering local models can be\nheterogeneous with non-independent and identically distributed (non-IID) data.\nTherefore, it is critical to evaluate the effect of malicious attacks and\ndevelop advanced defense techniques for FL-enabled wireless networks. In this\nwork, we introduce a federated deep reinforcement learning-based cell sleep\ncontrol scenario that enhances the energy efficiency of the network. We propose\nmultiple intelligent attacks targeting the learning-based approach and we\npropose defense methods to mitigate such attacks. In particular, we have\ndesigned two attack models, generative adversarial network (GAN)-enhanced model\npoisoning attack and regularization-based model poisoning attack. As a\ncounteraction, we have proposed two defense schemes, autoencoder-based defense,\nand knowledge distillation (KD)-enabled defense. The autoencoder-based defense\nmethod leverages an autoencoder to identify the malicious participants and only\naggregate the parameters of benign local models during the global aggregation,\nwhile KD-based defense protects the model from attacks by controlling the\nknowledge transferred between the global model and local models."
    },
    {
        "date": "2025-04",
        "title": "DeSIA: Attribute Inference Attacks Against Limited Fixed Aggregate Statistics",
        "author": "Yifeng Mao, Bozhidar Stevanoski, and Yves-Alexandre de Montjoye",
        "link": "http://arxiv.org/abs/2504.18497v1",
        "abstract": "Empirical inference attacks are a popular approach for evaluating the privacy\nrisk of data release mechanisms in practice. While an active attack literature\nexists to evaluate machine learning models or synthetic data release, we\ncurrently lack comparable methods for fixed aggregate statistics, in particular\nwhen only a limited number of statistics are released. We here propose an\ninference attack framework against fixed aggregate statistics and an attribute\ninference attack called DeSIA. We instantiate DeSIA against the U.S. Census\nPPMF dataset and show it to strongly outperform reconstruction-based attacks.\nIn particular, we show DeSIA to be highly effective at identifying vulnerable\nusers, achieving a true positive rate of 0.14 at a false positive rate of\n$10^{-3}$. We then show DeSIA to perform well against users whose attributes\ncannot be verified and when varying the number of aggregate statistics and\nlevel of noise addition. We also perform an extensive ablation study of DeSIA\nand show how DeSIA can be successfully adapted to the membership inference\ntask. Overall, our results show that aggregation alone is not sufficient to\nprotect privacy, even when a relatively small number of aggregates are being\nreleased, and emphasize the need for formal privacy mechanisms and testing\nbefore aggregate statistics are released."
    },
    {
        "date": "2025-04",
        "title": "Pseudo-Asynchronous Local SGD: Robust and Efficient Data-Parallel Training",
        "author": "Hiroki Naganuma, Xinzhi Zhang, Man-Chung Yue, Ioannis Mitliagkas, Philipp A. Witte, Russell J. Hewett, and Yin Tat Lee",
        "link": "http://arxiv.org/abs/2504.18454v1",
        "abstract": "Following AI scaling trends, frontier models continue to grow in size and\ncontinue to be trained on larger datasets. Training these models requires huge\ninvestments in exascale computational resources, which has in turn driven\ndevelopment of distributed deep learning methods. Data parallelism is an\nessential approach to speed up training, but it requires frequent global\ncommunication between workers, which can bottleneck training at the largest\nscales. In this work, we propose a method called Pseudo-Asynchronous Local SGD\n(PALSGD) to improve the efficiency of data-parallel training. PALSGD is an\nextension of Local SGD (Stich, 2018) and DiLoCo (Douillard et al., 2023),\ndesigned to further reduce communication frequency by introducing a\npseudo-synchronization mechanism. PALSGD allows the use of longer\nsynchronization intervals compared to standard Local SGD. Despite the reduced\ncommunication frequency, the pseudo-synchronization approach ensures that model\nconsistency is maintained, leading to performance results comparable to those\nachieved with more frequent synchronization. Furthermore, we provide a\ntheoretical analysis of PALSGD, establishing its convergence and deriving its\nconvergence rate. This analysis offers insights into the algorithm's behavior\nand performance guarantees. We evaluated PALSGD on image classification and\nlanguage modeling tasks. Our results show that PALSGD achieves better\nperformance in less time compared to existing methods like Distributed Data\nParallel (DDP), and DiLoCo. Notably, PALSGD trains 18.4% faster than DDP on\nImageNet-1K with ResNet-50, 24.4% faster than DDP on TinyStories with\nGPT-Neo125M, and 21.1% faster than DDP on TinyStories with GPT-Neo-8M."
    },
    {
        "date": "2025-04",
        "title": "Boosting-Enabled Robust System Identification of Partially Observed LTI Systems Under Heavy-Tailed Noise",
        "author": "Vinay Kanakeri, and Aritra Mitra",
        "link": "http://arxiv.org/abs/2504.18444v1",
        "abstract": "We consider the problem of system identification of partially observed linear\ntime-invariant (LTI) systems. Given input-output data, we provide\nnon-asymptotic guarantees for identifying the system parameters under general\nheavy-tailed noise processes. Unlike previous works that assume Gaussian or\nsub-Gaussian noise, we consider significantly broader noise distributions that\nare required to admit only up to the second moment. For this setting, we\nleverage tools from robust statistics to propose a novel system identification\nalgorithm that exploits the idea of boosting. Despite the much weaker noise\nassumptions, we show that our proposed algorithm achieves sample complexity\nbounds that nearly match those derived under sub-Gaussian noise. In particular,\nwe establish that our bounds retain a logarithmic dependence on the prescribed\nfailure probability. Interestingly, we show that such bounds can be achieved by\nrequiring just a finite fourth moment on the excitatory input process."
    },
    {
        "date": "2025-04",
        "title": "HepatoGEN: Generating Hepatobiliary Phase MRI with Perceptual and Adversarial Models",
        "author": "Jens Hooge, Gerard Sanroma-Guell, Faidra Stavropoulou, Alexander Ullmann, Gesine Knobloch, Mark Klemens, Carola Schmidt, Sabine Weckbach, and Andreas Bolz",
        "link": "http://arxiv.org/abs/2504.18405v1",
        "abstract": "Dynamic contrast-enhanced magnetic resonance imaging (DCE-MRI) plays a\ncrucial role in the detection and characterization of focal liver lesions, with\nthe hepatobiliary phase (HBP) providing essential diagnostic information.\nHowever, acquiring HBP images requires prolonged scan times, which may\ncompromise patient comfort and scanner throughput. In this study, we propose a\ndeep learning based approach for synthesizing HBP images from earlier contrast\nphases (precontrast and transitional) and compare three generative models: a\nperceptual U-Net, a perceptual GAN (pGAN), and a denoising diffusion\nprobabilistic model (DDPM). We curated a multi-site DCE-MRI dataset from\ndiverse clinical settings and introduced a contrast evolution score (CES) to\nassess training data quality, enhancing model performance. Quantitative\nevaluation using pixel-wise and perceptual metrics, combined with qualitative\nassessment through blinded radiologist reviews, showed that pGAN achieved the\nbest quantitative performance but introduced heterogeneous contrast in\nout-of-distribution cases. In contrast, the U-Net produced consistent liver\nenhancement with fewer artifacts, while DDPM underperformed due to limited\npreservation of fine structural details. These findings demonstrate the\nfeasibility of synthetic HBP image generation as a means to reduce scan time\nwithout compromising diagnostic utility, highlighting the clinical potential of\ndeep learning for dynamic contrast enhancement in liver MRI. A project demo is\navailable at: https://jhooge.github.io/hepatogen"
    },
    {
        "date": "2025-04",
        "title": "Model Evaluation in the Dark: Robust Classifier Metrics with Missing Labels",
        "author": "Danial Dervovic, and Michael Cashmore",
        "link": "http://arxiv.org/abs/2504.18385v1",
        "abstract": "Missing data in supervised learning is well-studied, but the specific issue\nof missing labels during model evaluation has been overlooked. Ignoring samples\nwith missing values, a common solution, can introduce bias, especially when\ndata is Missing Not At Random (MNAR). We propose a multiple imputation\ntechnique for evaluating classifiers using metrics such as precision, recall,\nand ROC-AUC. This method not only offers point estimates but also a predictive\ndistribution for these quantities when labels are missing. We empirically show\nthat the predictive distribution's location and shape are generally correct,\neven in the MNAR regime. Moreover, we establish that this distribution is\napproximately Gaussian and provide finite-sample convergence bounds.\nAdditionally, a robustness proof is presented, confirming the validity of the\napproximation under a realistic error model."
    },
    {
        "date": "2025-04",
        "title": "Adversarial Attacks on LLM-as-a-Judge Systems: Insights from Prompt Injections",
        "author": "Narek Maloyan, and Dmitry Namiot",
        "link": "http://arxiv.org/abs/2504.18333v1",
        "abstract": "LLM as judge systems used to assess text quality code correctness and\nargument strength are vulnerable to prompt injection attacks. We introduce a\nframework that separates content author attacks from system prompt attacks and\nevaluate five models Gemma 3.27B Gemma 3.4B Llama 3.2 3B GPT 4 and Claude 3\nOpus on four tasks with various defenses using fifty prompts per condition.\nAttacks achieved up to seventy three point eight percent success smaller models\nproved more vulnerable and transferability ranged from fifty point five to\nsixty two point six percent. Our results contrast with Universal Prompt\nInjection and AdvPrompter We recommend multi model committees and comparative\nscoring and release all code and datasets"
    },
    {
        "date": "2025-04",
        "title": "Outlier-aware Tensor Robust Principal Component Analysis with Self-guided Data Augmentation",
        "author": "Yangyang Xu, Kexin Li, Li Yang, and You-Wei Wen",
        "link": "http://arxiv.org/abs/2504.18323v1",
        "abstract": "Tensor Robust Principal Component Analysis (TRPCA) is a fundamental technique\nfor decomposing multi-dimensional data into a low-rank tensor and an outlier\ntensor, yet existing methods relying on sparse outlier assumptions often fail\nunder structured corruptions. In this paper, we propose a self-guided data\naugmentation approach that employs adaptive weighting to suppress outlier\ninfluence, reformulating the original TRPCA problem into a standard Tensor\nPrincipal Component Analysis (TPCA) problem. The proposed model involves an\noptimization-driven weighting scheme that dynamically identifies and\ndownweights outlier contributions during tensor augmentation. We develop an\nefficient proximal block coordinate descent algorithm with closed-form updates\nto solve the resulting optimization problem, ensuring computational efficiency.\nTheoretical convergence is guaranteed through a framework combining block\ncoordinate descent with majorization-minimization principles. Numerical\nexperiments on synthetic and real-world datasets, including face recovery,\nbackground subtraction, and hyperspectral denoising, demonstrate that our\nmethod effectively handles various corruption patterns. The results show the\nimprovements in both accuracy and computational efficiency compared to\nstate-of-the-art methods."
    },
    {
        "date": "2025-04",
        "title": "Enhancing Long-Term Re-Identification Robustness Using Synthetic Data: A Comparative Analysis",
        "author": "Christian Pionzewski, Rebecca Rademacher, J\u00e9r\u00f4me Rutinowski, Antonia Ponikarov, Stephan Matzke, Tim Chilla, Pia Schreynemackers, and Alice Kirchheim",
        "link": "http://arxiv.org/abs/2504.18286v1",
        "abstract": "This contribution explores the impact of synthetic training data usage and\nthe prediction of material wear and aging in the context of re-identification.\nDifferent experimental setups and gallery set expanding strategies are tested,\nanalyzing their impact on performance over time for aging re-identification\nsubjects. Using a continuously updating gallery, we were able to increase our\nmean Rank-1 accuracy by 24%, as material aging was taken into account step by\nstep. In addition, using models trained with 10% artificial training data,\nRank-1 accuracy could be increased by up to 13%, in comparison to a model\ntrained on only real-world data, significantly boosting generalized performance\non hold-out data. Finally, this work introduces a novel, open-source\nre-identification dataset, pallet-block-2696. This dataset contains 2,696\nimages of Euro pallets, taken over a period of 4 months. During this time,\nnatural aging processes occurred and some of the pallets were damaged during\ntheir usage. These wear and tear processes significantly changed the appearance\nof the pallets, providing a dataset that can be used to generate synthetically\naged pallets or other wooden materials."
    },
    {
        "date": "2025-04",
        "title": "Tree Boosting Methods for Balanced andImbalanced Classification and their Robustness Over Time in Risk Assessment",
        "author": "Gissel Velarde, Michael Weichert, Anuj Deshmunkh, Sanjay Deshmane, Anindya Sudhir, Khushboo Sharma, and Vaibhav Joshi",
        "link": "http://arxiv.org/abs/2504.18133v1",
        "abstract": "Most real-world classification problems deal with imbalanced datasets, posing\na challenge for Artificial Intelligence (AI), i.e., machine learning\nalgorithms, because the minority class, which is of extreme interest, often\nproves difficult to be detected. This paper empirically evaluates tree boosting\nmethods' performance given different dataset sizes and class distributions,\nfrom perfectly balanced to highly imbalanced. For tabular data, tree-based\nmethods such as XGBoost, stand out in several benchmarks due to detection\nperformance and speed. Therefore, XGBoost and Imbalance-XGBoost are evaluated.\nAfter introducing the motivation to address risk assessment with machine\nlearning, the paper reviews evaluation metrics for detection systems or binary\nclassifiers. It proposes a method for data preparation followed by tree\nboosting methods including hyper-parameter optimization. The method is\nevaluated on private datasets of 1 thousand (K), 10K and 100K samples on\ndistributions with 50, 45, 25, and 5 percent positive samples. As expected, the\ndeveloped method increases its recognition performance as more data is given\nfor training and the F1 score decreases as the data distribution becomes more\nimbalanced, but it is still significantly superior to the baseline of\nprecision-recall determined by the ratio of positives divided by positives and\nnegatives. Sampling to balance the training set does not provide consistent\nimprovement and deteriorates detection. In contrast, classifier hyper-parameter\noptimization improves recognition, but should be applied carefully depending on\ndata volume and distribution. Finally, the developed method is robust to data\nvariation over time up to some point. Retraining can be used when performance\nstarts deteriorating."
    },
    {
        "date": "2025-04",
        "title": "Automating Function-Level TARA for Automotive Full-Lifecycle Security",
        "author": "Yuqiao Yang, Yongzhao Zhang, Wenhao Liu, Jun Li, Pengtao Shi, DingYu Zhong, Jie Yang, Ting Chen, Sheng Cao, Yuntao Ren, Yongyue Wu, and Xiaosong Zhang",
        "link": "http://arxiv.org/abs/2504.18083v1",
        "abstract": "As modern vehicles evolve into intelligent and connected systems, their\ngrowing complexity introduces significant cybersecurity risks. Threat Analysis\nand Risk Assessment (TARA) has therefore become essential for managing these\nrisks under mandatory regulations. However, existing TARA automation methods\nrely on static threat libraries, limiting their utility in the detailed,\nfunction-level analyses demanded by industry. This paper introduces\nDefenseWeaver, the first system that automates function-level TARA using\ncomponent-specific details and large language models (LLMs). DefenseWeaver\ndynamically generates attack trees and risk evaluations from system\nconfigurations described in an extended OpenXSAM++ format, then employs a\nmulti-agent framework to coordinate specialized LLM roles for more robust\nanalysis. To further adapt to evolving threats and diverse standards,\nDefenseWeaver incorporates Low-Rank Adaptation (LoRA) fine-tuning and\nRetrieval-Augmented Generation (RAG) with expert-curated TARA reports. We\nvalidated DefenseWeaver through deployment in four automotive security\nprojects, where it identified 11 critical attack paths, verified through\npenetration testing, and subsequently reported and remediated by the relevant\nautomakers and suppliers. Additionally, DefenseWeaver demonstrated cross-domain\nadaptability, successfully applying to unmanned aerial vehicles (UAVs) and\nmarine navigation systems. In comparison to human experts, DefenseWeaver\noutperformed manual attack tree generation across six assessment scenarios.\nIntegrated into commercial cybersecurity platforms such as UAES and Xiaomi,\nDefenseWeaver has generated over 8,200 attack trees. These results highlight\nits ability to significantly reduce processing time, and its scalability and\ntransformative impact on cybersecurity across industries."
    },
    {
        "date": "2025-04",
        "title": "Edge-Based Learning for Improved Classification Under Adversarial Noise",
        "author": "Manish Kansana, Keyan Alexander Rahimi, Elias Hossain, Iman Dehzangi, and Noorbakhsh Amiri Golilarz",
        "link": "http://arxiv.org/abs/2504.20077v1",
        "abstract": "Adversarial noise introduces small perturbations in images, misleading deep\nlearning models into misclassification and significantly impacting recognition\naccuracy. In this study, we analyzed the effects of Fast Gradient Sign Method\n(FGSM) adversarial noise on image classification and investigated whether\ntraining on specific image features can improve robustness. We hypothesize that\nwhile adversarial noise perturbs various regions of an image, edges may remain\nrelatively stable and provide essential structural information for\nclassification. To test this, we conducted a series of experiments using brain\ntumor and COVID datasets. Initially, we trained the models on clean images and\nthen introduced subtle adversarial perturbations, which caused deep learning\nmodels to significantly misclassify the images. Retraining on a combination of\nclean and noisy images led to improved performance. To evaluate the robustness\nof the edge features, we extracted edges from the original/clean images and\ntrained the models exclusively on edge-based representations. When noise was\nintroduced to the images, the edge-based models demonstrated greater resilience\nto adversarial attacks compared to those trained on the original or clean\nimages. These results suggest that while adversarial noise is able to exploit\ncomplex non-edge regions significantly more than edges, the improvement in the\naccuracy after retraining is marginally more in the original data as compared\nto the edges. Thus, leveraging edge-based learning can improve the resilience\nof deep learning models against adversarial perturbations."
    },
    {
        "date": "2025-04",
        "title": "Diffusion-Driven Universal Model Inversion Attack for Face Recognition",
        "author": "Hanrui Wang, Shuo Wang, Chun-Shien Lu, and Isao Echizen",
        "link": "http://arxiv.org/abs/2504.18015v1",
        "abstract": "Facial recognition technology poses significant privacy risks, as it relies\non biometric data that is inherently sensitive and immutable if compromised. To\nmitigate these concerns, face recognition systems convert raw images into\nembeddings, traditionally considered privacy-preserving. However, model\ninversion attacks pose a significant privacy threat by reconstructing these\nprivate facial images, making them a crucial tool for evaluating the privacy\nrisks of face recognition systems. Existing methods usually require training\nindividual generators for each target model, a computationally expensive\nprocess. In this paper, we propose DiffUMI, a training-free diffusion-driven\nuniversal model inversion attack for face recognition systems. DiffUMI is the\nfirst approach to apply a diffusion model for unconditional image generation in\nmodel inversion. Unlike other methods, DiffUMI is universal, eliminating the\nneed for training target-specific generators. It operates within a fixed\nframework and pretrained diffusion model while seamlessly adapting to diverse\ntarget identities and models. DiffUMI breaches privacy-preserving face\nrecognition systems with state-of-the-art success, demonstrating that an\nunconditional diffusion model, coupled with optimized adversarial search,\nenables efficient and high-fidelity facial reconstruction. Additionally, we\nintroduce a novel application of out-of-domain detection (OODD), marking the\nfirst use of model inversion to distinguish non-face inputs from face inputs\nbased solely on embeddings."
    },
    {
        "date": "2025-04",
        "title": "Cluster-Aware Attacks on Graph Watermarks",
        "author": "Alexander Nemecek, Emre Yilmaz, and Erman Ayday",
        "link": "http://arxiv.org/abs/2504.17971v1",
        "abstract": "Data from domains such as social networks, healthcare, finance, and\ncybersecurity can be represented as graph-structured information. Given the\nsensitive nature of this data and their frequent distribution among\ncollaborators, ensuring secure and attributable sharing is essential. Graph\nwatermarking enables attribution by embedding user-specific signatures into\ngraph-structured data. While prior work has addressed random perturbation\nattacks, the threat posed by adversaries leveraging structural properties\nthrough community detection remains unexplored. In this work, we introduce a\ncluster-aware threat model in which adversaries apply community-guided\nmodifications to evade detection. We propose two novel attack strategies and\nevaluate them on real-world social network graphs. Our results show that\ncluster-aware attacks can reduce attribution accuracy by up to 80% more than\nrandom baselines under equivalent perturbation budgets on sparse graphs. To\nmitigate this threat, we propose a lightweight embedding enhancement that\ndistributes watermark nodes across graph communities. This approach improves\nattribution accuracy by up to 60% under attack on dense graphs, without\nincreasing runtime or structural distortion. Our findings underscore the\nimportance of cluster-topological awareness in both watermarking design and\nadversarial modeling."
    },
    {
        "date": "2025-04",
        "title": "Secured Encryption scheme based on the Ree groups",
        "author": "Gennady Khalimov, and Yevgen Kotukh",
        "link": "http://arxiv.org/abs/2504.17919v1",
        "abstract": "An improved design of a cryptosystem based on small Ree groups is proposed.\nWe have changed the encryption algorithm and propose to use a logarithmic\nsignature for the entire Ree group. This approach improves security against\nsequential key recovery attacks. Hence, the complexity of the key recovery\nattack will be defined by a brute-force attack over the entire group. In this\npaper, we have proved that to construct secure cryptosystems with group\ncomputations over a small finite field, it is needed to use a 3-parametric\nsmall Ree group."
    },
    {
        "date": "2025-04",
        "title": "Biting the CHERI bullet: Blockers, Enablers and Security Implications of CHERI in Defence",
        "author": "Shamal Faily",
        "link": "http://arxiv.org/abs/2504.17904v1",
        "abstract": "There is growing interest in securing the hardware foundations software\nstacks build upon. However, before making any investment decision, software and\nhardware supply chain stakeholders require evidence from realistic, multiple\nlong-term studies of adoption. We present results from a 12 month evaluation of\none such secure hardware solution, CHERI, where 15 teams from industry and\nacademia ported software relevant to Defence to Arm's experimental Morello\nboard. We identified six types of blocker inhibiting adoption: dependencies, a\nknowledge premium, missing utilities, performance, platform instability, and\ntechnical debt. We also identified three types of enabler: tool assistance,\nimproved quality, and trivial code porting. Finally, we identified five types\nof potential vulnerability that CHERI could, if not appropriately configured,\nexpand a system's attack surface: state leaks, memory leaks, use after free\nvulnerabilities, unsafe defaults, and tool chain instability. Future work\nshould remove potentially insecure defaults from CHERI tooling, and develop a\nCHERI body of knowledge to further adoption."
    },
    {
        "date": "2025-04",
        "title": "DCT-Shield: A Robust Frequency Domain Defense against Malicious Image Editing",
        "author": "Aniruddha Bala, Rohit Chowdhury, Rohan Jaiswal, and Siddharth Roheda",
        "link": "http://arxiv.org/abs/2504.17894v1",
        "abstract": "Advancements in diffusion models have enabled effortless image editing via\ntext prompts, raising concerns about image security. Attackers with access to\nuser images can exploit these tools for malicious edits. Recent defenses\nattempt to protect images by adding a limited noise in the pixel space to\ndisrupt the functioning of diffusion-based editing models. However, the\nadversarial noise added by previous methods is easily noticeable to the human\neye. Moreover, most of these methods are not robust to purification techniques\nlike JPEG compression under a feasible pixel budget. We propose a novel\noptimization approach that introduces adversarial perturbations directly in the\nfrequency domain by modifying the Discrete Cosine Transform (DCT) coefficients\nof the input image. By leveraging the JPEG pipeline, our method generates\nadversarial images that effectively prevent malicious image editing. Extensive\nexperiments across a variety of tasks and datasets demonstrate that our\napproach introduces fewer visual artifacts while maintaining similar levels of\nedit protection and robustness to noise purification techniques."
    },
    {
        "date": "2025-04",
        "title": "Silenzio: Secure Non-Interactive Outsourced MLP Training",
        "author": "Jonas Sander, and Thomas Eisenbarth",
        "link": "http://arxiv.org/abs/2504.17785v1",
        "abstract": "Outsourcing the ML training to cloud providers presents a compelling\nopportunity for resource constrained clients, while it simultaneously bears\ninherent privacy risks, especially for highly sensitive training data. We\nintroduce Silenzio, the first fully non-interactive outsourcing scheme for the\ntraining of multi-layer perceptrons that achieves 128 bit security using FHE.\nUnlike traditional MPC based protocols that necessitate interactive\ncommunication between the client and server(s) or non-collusion assumptions\namong multiple servers, Silenzio enables the fire-and-forget paradigm without\nsuch assumptions. In this approach, the client encrypts the training data once,\nand the cloud server performs the training without any further interaction.\n  Silenzio operates over low bitwidth integers - never exceeding 8 bit - to\nmitigate the computational overhead of FHE. Our approach features a novel\nlow-bitwidth matrix multiplication that leverages input-dependent residue\nnumber systems and a Karatsuba-inspired multiplication routine, ensuring that\nno intermediate FHE-processed value overflows 8 bit. Starting from an\nRNS-to-MRNS conversion process, we propose an efficient block-scaling\nmechanism, which approximately shifts encrypted tensor values to the\nuser-specified most significant bits. To instantiate the backpropagation of the\nerror, Silenzio introduces a low-bitwidth and TFHE friendly gradient\ncomputation for the cross entropy loss.\n  Implemented using the state-of-the-art Concrete library, we evaluate Silenzio\non standard MLP training tasks regarding runtime as well as model performance\nand achieve similar classification accuracy as MLPs trained using standard\nPyTorch with 32 bit floating-point computations. Our open-source implementation\nrepresents a significant advancement in privacy-preserving ML, providing a new\nbaseline for secure and non-interactive outsourced MLP training."
    },
    {
        "date": "2025-04",
        "title": "CasualHDRSplat: Robust High Dynamic Range 3D Gaussian Splatting from Casually Captured Videos",
        "author": "Shucheng Gong, Lingzhe Zhao, Wenpu Li, Hong Xie, Yin Zhang, Shiyu Zhao, and Peidong Liu",
        "link": "http://arxiv.org/abs/2504.17728v1",
        "abstract": "Recently, photo-realistic novel view synthesis from multi-view images, such\nas neural radiance field (NeRF) and 3D Gaussian Splatting (3DGS), have garnered\nwidespread attention due to their superior performance. However, most works\nrely on low dynamic range (LDR) images, which limits the capturing of richer\nscene details. Some prior works have focused on high dynamic range (HDR) scene\nreconstruction, typically require capturing of multi-view sharp images with\ndifferent exposure times at fixed camera positions during exposure times, which\nis time-consuming and challenging in practice. For a more flexible data\nacquisition, we propose a one-stage method: \\textbf{CasualHDRSplat} to easily\nand robustly reconstruct the 3D HDR scene from casually captured videos with\nauto-exposure enabled, even in the presence of severe motion blur and varying\nunknown exposure time. \\textbf{CasualHDRSplat} contains a unified\ndifferentiable physical imaging model which first applies continuous-time\ntrajectory constraint to imaging process so that we can jointly optimize\nexposure time, camera response function (CRF), camera poses, and sharp 3D HDR\nscene. Extensive experiments demonstrate that our approach outperforms existing\nmethods in terms of robustness and rendering quality. Our source code will be\navailable at https://github.com/WU-CVGL/CasualHDRSplat"
    },
    {
        "date": "2025-04",
        "title": "Towards Robust LLMs: an Adversarial Robustness Measurement Framework",
        "author": "Natan Levy, Adiel Ashrov, and Guy Katz",
        "link": "http://arxiv.org/abs/2504.17723v1",
        "abstract": "The rise of Large Language Models (LLMs) has revolutionized artificial\nintelligence, yet these models remain vulnerable to adversarial perturbations,\nundermining their reliability in high-stakes applications. While adversarial\nrobustness in vision-based neural networks has been extensively studied, LLM\nrobustness remains under-explored. We adapt the Robustness Measurement and\nAssessment (RoMA) framework to quantify LLM resilience against adversarial\ninputs without requiring access to model parameters. By comparing RoMA's\nestimates to those of formal verification methods, we demonstrate its accuracy\nwith minimal error margins while maintaining computational efficiency. Our\nempirical evaluation reveals that robustness varies significantly not only\nbetween different models but also across categories within the same task and\nbetween various types of perturbations. This non-uniformity underscores the\nneed for task-specific robustness evaluations, enabling practitioners to\ncompare and select models based on application-specific robustness\nrequirements. Our work provides a systematic methodology to assess LLM\nrobustness, advancing the development of more reliable language models for\nreal-world deployment."
    },
    {
        "date": "2025-04",
        "title": "On the Generalization of Adversarially Trained Quantum Classifiers",
        "author": "Petros Georgiou, Aaron Mark Thomas, Sharu Theresa Jose, and Osvaldo Simeone",
        "link": "http://arxiv.org/abs/2504.17690v1",
        "abstract": "Quantum classifiers are vulnerable to adversarial attacks that manipulate\ntheir input classical or quantum data. A promising countermeasure is\nadversarial training, where quantum classifiers are trained by using an\nattack-aware, adversarial loss function. This work establishes novel bounds on\nthe generalization error of adversarially trained quantum classifiers when\ntested in the presence of perturbation-constrained adversaries. The bounds\nquantify the excess generalization error incurred to ensure robustness to\nadversarial attacks as scaling with the training sample size $m$ as\n$1/\\sqrt{m}$, while yielding insights into the impact of the quantum embedding.\nFor quantum binary classifiers employing \\textit{rotation embedding}, we find\nthat, in the presence of adversarial attacks on classical inputs $\\mathbf{x}$,\nthe increase in sample complexity due to adversarial training over conventional\ntraining vanishes in the limit of high dimensional inputs $\\mathbf{x}$. In\ncontrast, when the adversary can directly attack the quantum state\n$\\rho(\\mathbf{x})$ encoding the input $\\mathbf{x}$, the excess generalization\nerror depends on the choice of embedding only through its Hilbert space\ndimension. The results are also extended to multi-class classifiers. We\nvalidate our theoretical findings with numerical experiments."
    },
    {
        "date": "2025-04",
        "title": "Evaluating the Vulnerability of ML-Based Ethereum Phishing Detectors to Single-Feature Adversarial Perturbations",
        "author": "Ahod Alghuried, Ali Alkinoon, Abdulaziz Alghamdi, Soohyeon Choi, Manar Mohaisen, and David Mohaisen",
        "link": "http://arxiv.org/abs/2504.17684v1",
        "abstract": "This paper explores the vulnerability of machine learning models to simple\nsingle-feature adversarial attacks in the context of Ethereum fraudulent\ntransaction detection. Through comprehensive experimentation, we investigate\nthe impact of various adversarial attack strategies on model performance\nmetrics. Our findings, highlighting how prone those techniques are to simple\nattacks, are alarming, and the inconsistency in the attacks' effect on\ndifferent algorithms promises ways for attack mitigation. We examine the\neffectiveness of different mitigation strategies, including adversarial\ntraining and enhanced feature selection, in enhancing model robustness and show\ntheir effectiveness."
    },
    {
        "date": "2025-04",
        "title": "Enhancing CNNs robustness to occlusions with bioinspired filters for border completion",
        "author": "Catarina P. Coutinho, Aneeqa Merhab, Janko Petkovic, Ferdinando Zanchetta, and Rita Fioresi",
        "link": "http://arxiv.org/abs/2504.17619v1",
        "abstract": "We exploit the mathematical modeling of the visual cortex mechanism for\nborder completion to define custom filters for CNNs. We see a consistent\nimprovement in performance, particularly in accuracy, when our modified LeNet 5\nis tested with occluded MNIST images."
    },
    {
        "date": "2025-04",
        "title": "A Simple DropConnect Approach to Transfer-based Targeted Attack",
        "author": "Tongrui Su, Qingbin Li, Shengyu Zhu, Wei Chen, and Xueqi Cheng",
        "link": "http://arxiv.org/abs/2504.18594v1",
        "abstract": "We study the problem of transfer-based black-box attack, where adversarial\nsamples generated using a single surrogate model are directly applied to target\nmodels. Compared with untargeted attacks, existing methods still have lower\nAttack Success Rates (ASRs) in the targeted setting, i.e., the obtained\nadversarial examples often overfit the surrogate model but fail to mislead\nother models. In this paper, we hypothesize that the pixels or features in\nthese adversarial examples collaborate in a highly dependent manner to maximize\nthe success of an adversarial attack on the surrogate model, which we refer to\nas perturbation co-adaptation. Then, we propose to Mitigate perturbation\nCo-adaptation by DropConnect (MCD) to enhance transferability, by creating\ndiverse variants of surrogate model at each optimization iteration. We conduct\nextensive experiments across various CNN- and Transformer-based models to\ndemonstrate the effectiveness of MCD. In the challenging scenario of\ntransferring from a CNN-based model to Transformer-based models, MCD achieves\n13% higher average ASRs compared with state-of-the-art baselines. MCD boosts\nthe performance of self-ensemble methods by bringing in more diversification\nacross the variants while reserving sufficient semantic information for each\nvariant. In addition, MCD attains the highest performance gain when scaling the\ncompute of crafting adversarial examples."
    },
    {
        "date": "2025-04",
        "title": "Wolves in the Repository: A Software Engineering Analysis of the XZ Utils Supply Chain Attack",
        "author": "Piotr Przymus, and Thomas Durieux",
        "link": "http://arxiv.org/abs/2504.17473v1",
        "abstract": "The digital economy runs on Open Source Software (OSS), with an estimated\n90\\% of modern applications containing open-source components. While this\nwidespread adoption has revolutionized software development, it has also\ncreated critical security vulnerabilities, particularly in essential but\nunder-resourced projects. This paper examines a sophisticated attack on the XZ\nUtils project (CVE-2024-3094), where attackers exploited not just code, but the\nentire open-source development process to inject a backdoor into a fundamental\nLinux compression library. Our analysis reveals a new breed of supply chain\nattack that manipulates software engineering practices themselves -- from\ncommunity management to CI/CD configurations -- to establish legitimacy and\nmaintain long-term control. Through a comprehensive examination of GitHub\nevents and development artifacts, we reconstruct the attack timeline, analyze\nthe evolution of attacker tactics. Our findings demonstrate how attackers\nleveraged seemingly beneficial contributions to project infrastructure and\nmaintenance to bypass traditional security measures. This work extends beyond\ntraditional security analysis by examining how software engineering practices\nthemselves can be weaponized, offering insights for protecting the open-source\necosystem."
    },
    {
        "date": "2025-04",
        "title": "Unveiling Hidden Vulnerabilities in Digital Human Generation via Adversarial Attacks",
        "author": "Zhiying Li, Yeying Jin, Fan Shen, Zhi Liu, Weibin Chen, Pengju Zhang, Xiaomei Zhang, Boyu Chen, Michael Shen, Kejian Wu, Zhaoxin Fan, and Jin Dong",
        "link": "http://arxiv.org/abs/2504.17457v1",
        "abstract": "Expressive human pose and shape estimation (EHPS) is crucial for digital\nhuman generation, especially in applications like live streaming. While\nexisting research primarily focuses on reducing estimation errors, it largely\nneglects robustness and security aspects, leaving these systems vulnerable to\nadversarial attacks. To address this significant challenge, we propose the\n\\textbf{Tangible Attack (TBA)}, a novel framework designed to generate\nadversarial examples capable of effectively compromising any digital human\ngeneration model. Our approach introduces a \\textbf{Dual Heterogeneous Noise\nGenerator (DHNG)}, which leverages Variational Autoencoders (VAE) and\nControlNet to produce diverse, targeted noise tailored to the original image\nfeatures. Additionally, we design a custom \\textbf{adversarial loss function}\nto optimize the noise, ensuring both high controllability and potent\ndisruption. By iteratively refining the adversarial sample through\nmulti-gradient signals from both the noise and the state-of-the-art EHPS model,\nTBA substantially improves the effectiveness of adversarial attacks. Extensive\nexperiments demonstrate TBA's superiority, achieving a remarkable 41.0\\%\nincrease in estimation error, with an average improvement of approximately\n17.0\\%. These findings expose significant security vulnerabilities in current\nEHPS models and highlight the need for stronger defenses in digital human\ngeneration systems."
    },
    {
        "date": "2025-04",
        "title": "StereoMamba: Real-time and Robust Intraoperative Stereo Disparity Estimation via Long-range Spatial Dependencies",
        "author": "Xu Wang, Jialang Xu, Shuai Zhang, Baoru Huang, Danail Stoyanov, and Evangelos B. Mazomenos",
        "link": "http://arxiv.org/abs/2504.17401v1",
        "abstract": "Stereo disparity estimation is crucial for obtaining depth information in\nrobot-assisted minimally invasive surgery (RAMIS). While current deep learning\nmethods have made significant advancements, challenges remain in achieving an\noptimal balance between accuracy, robustness, and inference speed. To address\nthese challenges, we propose the StereoMamba architecture, which is\nspecifically designed for stereo disparity estimation in RAMIS. Our approach is\nbased on a novel Feature Extraction Mamba (FE-Mamba) module, which enhances\nlong-range spatial dependencies both within and across stereo images. To\neffectively integrate multi-scale features from FE-Mamba, we then introduce a\nnovel Multidimensional Feature Fusion (MFF) module. Experiments against the\nstate-of-the-art on the ex-vivo SCARED benchmark demonstrate that StereoMamba\nachieves superior performance on EPE of 2.64 px and depth MAE of 2.55 mm, the\nsecond-best performance on Bad2 of 41.49% and Bad3 of 26.99%, while maintaining\nan inference speed of 21.28 FPS for a pair of high-resolution images\n(1280*1024), striking the optimum balance between accuracy, robustness, and\nefficiency. Furthermore, by comparing synthesized right images, generated from\nwarping left images using the generated disparity maps, with the actual right\nimage, StereoMamba achieves the best average SSIM (0.8970) and PSNR (16.0761),\nexhibiting strong zero-shot generalization on the in-vivo RIS2017 and StereoMIS\ndatasets."
    },
    {
        "date": "2025-04",
        "title": "Fine-Tuning Adversarially-Robust Transformers for Single-Image Dehazing",
        "author": "Vlad Vasilescu, Ana Neacsu, and Daniela Faur",
        "link": "http://arxiv.org/abs/2504.17829v1",
        "abstract": "Single-image dehazing is an important topic in remote sensing applications,\nenhancing the quality of acquired images and increasing object detection\nprecision. However, the reliability of such structures has not been\nsufficiently analyzed, which poses them to the risk of imperceptible\nperturbations that can significantly hinder their performance. In this work, we\nshow that state-of-the-art image-to-image dehazing transformers are susceptible\nto adversarial noise, with even 1 pixel change being able to decrease the PSNR\nby as much as 2.8 dB. Next, we propose two lightweight fine-tuning strategies\naimed at increasing the robustness of pre-trained transformers. Our methods\nresults in comparable clean performance, while significantly increasing the\nprotection against adversarial data. We further present their applicability in\ntwo remote sensing scenarios, showcasing their robust behavior for\nout-of-distribution data. The source code for adversarial fine-tuning and\nattack algorithms can be found at github.com/Vladimirescu/RobustDehazing."
    },
    {
        "date": "2025-04",
        "title": "Class-Conditional Distribution Balancing for Group Robust Classification",
        "author": "Miaoyun Zhao, Qiang Zhang, and Chenrong Li",
        "link": "http://arxiv.org/abs/2504.17314v2",
        "abstract": "Spurious correlations that lead models to correct predictions for the wrong\nreasons pose a critical challenge for robust real-world generalization.\nExisting research attributes this issue to group imbalance and addresses it by\nmaximizing group-balanced or worst-group accuracy, which heavily relies on\nexpensive bias annotations. A compromise approach involves predicting bias\ninformation using extensively pretrained foundation models, which requires\nlarge-scale data and becomes impractical for resource-limited rare domains. To\naddress these challenges, we offer a novel perspective by reframing the\nspurious correlations as imbalances or mismatches in class-conditional\ndistributions, and propose a simple yet effective robust learning method that\neliminates the need for both bias annotations and predictions. With the goal of\nreducing the mutual information between spurious factors and label information,\nour method leverages a sample reweighting strategy to achieve class-conditional\ndistribution balancing, which automatically highlights minority groups and\nclasses, effectively dismantling spurious correlations and producing a debiased\ndata distribution for classification. Extensive experiments and analysis\ndemonstrate that our approach consistently delivers state-of-the-art\nperformance, rivaling methods that rely on bias supervision."
    },
    {
        "date": "2025-04",
        "title": "FLUKE: A Linguistically-Driven and Task-Agnostic Framework for Robustness Evaluation",
        "author": "Yulia Otmakhova, Hung Thinh Truong, Rahmad Mahendra, Zenan Zhai, Rongxin Zhu, Daniel Beck, and Jey Han Lau",
        "link": "http://arxiv.org/abs/2504.17311v1",
        "abstract": "We present FLUKE (Framework for LingUistically-driven and tasK-agnostic\nrobustness Evaluation), a task-agnostic framework for assessing model\nrobustness through systematic minimal variations of test data. FLUKE introduces\ncontrolled variations across linguistic levels - from orthography to dialect\nand style varieties - and leverages large language models (LLMs) with human\nvalidation to generate modifications. We demonstrate FLUKE's utility by\nevaluating both fine-tuned models and LLMs across four diverse NLP tasks, and\nreveal that (1) the impact of linguistic variations is highly task-dependent,\nwith some tests being critical for certain tasks but irrelevant for others; (2)\nwhile LLMs have better overall robustness compared to fine-tuned models, they\nstill exhibit significant brittleness to certain linguistic variations; (3) all\nmodels show substantial vulnerability to negation modifications across most\ntasks. These findings highlight the importance of systematic robustness testing\nfor understanding model behaviors."
    },
    {
        "date": "2025-04",
        "title": "Enhancing Variational Autoencoders with Smooth Robust Latent Encoding",
        "author": "Hyomin Lee, Minseon Kim, Sangwon Jang, Jongheon Jeong, and Sung Ju Hwang",
        "link": "http://arxiv.org/abs/2504.17219v1",
        "abstract": "Variational Autoencoders (VAEs) have played a key role in scaling up\ndiffusion-based generative models, as in Stable Diffusion, yet questions\nregarding their robustness remain largely underexplored. Although adversarial\ntraining has been an established technique for enhancing robustness in\npredictive models, it has been overlooked for generative models due to concerns\nabout potential fidelity degradation by the nature of trade-offs between\nperformance and robustness. In this work, we challenge this presumption,\nintroducing Smooth Robust Latent VAE (SRL-VAE), a novel adversarial training\nframework that boosts both generation quality and robustness. In contrast to\nconventional adversarial training, which focuses on robustness only, our\napproach smooths the latent space via adversarial perturbations, promoting more\ngeneralizable representations while regularizing with originality\nrepresentation to sustain original fidelity. Applied as a post-training step on\npre-trained VAEs, SRL-VAE improves image robustness and fidelity with minimal\ncomputational overhead. Experiments show that SRL-VAE improves both generation\nquality, in image reconstruction and text-guided image editing, and robustness,\nagainst Nightshade attacks and image editing attacks. These results establish a\nnew paradigm, showing that adversarial training, once thought to be detrimental\nto generative models, can instead enhance both fidelity and robustness."
    },
    {
        "date": "2025-04",
        "title": "Developing a Blockchain-Based Secure Digital Contents Distribution System",
        "author": "Syed Mohiuddin Qadri, and Sangwhan Cha",
        "link": "http://arxiv.org/abs/2504.17194v1",
        "abstract": "As digital content distribution expands rapidly through online platforms,\nsecuring digital media and protecting intellectual property has become\nincreasingly complex. Traditional centralized systems, while widely adopted,\nsuffer from vulnerabilities such as single points of failure and limited\ntraceability of unauthorized access. This paper presents a blockchain-based\nsecure digital content distribution system that integrates Sia, a decentralized\nstorage network, and Skynet, a content delivery network, to enhance content\nprotection and distribution. The proposed system employs a dual-layer\narchitecture: off-chain for user authentication and on-chain for transaction\nvalidation using smart contracts and asymmetric encryption. By introducing a\nlicense issuance and secret block mechanism, the system ensures content\nauthenticity, privacy, and controlled access. Experimental results demonstrate\nthe feasibility and scalability of the system in securely distributing\nmultimedia files. The proposed platform not only improves content security but\nalso paves the way for future enhancements with decentralized applications and\nintegrated royalty payment mechanisms."
    },
    {
        "date": "2025-04",
        "title": "AUTHENTICATION: Identifying Rare Failure Modes in Autonomous Vehicle Perception Systems using Adversarially Guided Diffusion Models",
        "author": "Mohammad Zarei, Melanie A Jutras, Eliana Evans, Mike Tan, and Omid Aaramoon",
        "link": "http://arxiv.org/abs/2504.17179v1",
        "abstract": "Autonomous Vehicles (AVs) rely on artificial intelligence (AI) to accurately\ndetect objects and interpret their surroundings. However, even when trained\nusing millions of miles of real-world data, AVs are often unable to detect rare\nfailure modes (RFMs). The problem of RFMs is commonly referred to as the\n\"long-tail challenge\", due to the distribution of data including many instances\nthat are very rarely seen. In this paper, we present a novel approach that\nutilizes advanced generative and explainable AI techniques to aid in\nunderstanding RFMs. Our methods can be used to enhance the robustness and\nreliability of AVs when combined with both downstream model training and\ntesting. We extract segmentation masks for objects of interest (e.g., cars) and\ninvert them to create environmental masks. These masks, combined with carefully\ncrafted text prompts, are fed into a custom diffusion model. We leverage the\nStable Diffusion inpainting model guided by adversarial noise optimization to\ngenerate images containing diverse environments designed to evade object\ndetection models and expose vulnerabilities in AI systems. Finally, we produce\nnatural language descriptions of the generated RFMs that can guide developers\nand policymakers to improve the safety and reliability of AV systems."
    },
    {
        "date": "2025-04",
        "title": "Robo-Troj: Attacking LLM-based Task Planners",
        "author": "Mohaiminul Al Nahian, Zainab Altaweel, David Reitano, Sabbir Ahmed, Saumitra Lohokare, Shiqi Zhang, and Adnan Siraj Rakin",
        "link": "http://arxiv.org/abs/2504.17070v1",
        "abstract": "Robots need task planning methods to achieve goals that require more than\nindividual actions. Recently, large language models (LLMs) have demonstrated\nimpressive performance in task planning. LLMs can generate a step-by-step\nsolution using a description of actions and the goal. Despite the successes in\nLLM-based task planning, there is limited research studying the security\naspects of those systems. In this paper, we develop Robo-Troj, the first\nmulti-trigger backdoor attack for LLM-based task planners, which is the main\ncontribution of this work. As a multi-trigger attack, Robo-Troj is trained to\naccommodate the diversity of robot application domains. For instance, one can\nuse unique trigger words, e.g., \"herical\", to activate a specific malicious\nbehavior, e.g., cutting hand on a kitchen robot. In addition, we develop an\noptimization method for selecting the trigger words that are most effective.\nThrough demonstrating the vulnerability of LLM-based planners, we aim to\npromote the development of secured robot systems."
    },
    {
        "date": "2025-04",
        "title": "Statistical Guarantees in Synthetic Data through Conformal Adversarial Generation",
        "author": "Rahul Vishwakarma, Shrey Dharmendra Modi, and Vishwanath Seshagiri",
        "link": "http://arxiv.org/abs/2504.17058v2",
        "abstract": "The generation of high-quality synthetic data presents significant challenges\nin machine learning research, particularly regarding statistical fidelity and\nuncertainty quantification. Existing generative models produce compelling\nsynthetic samples but lack rigorous statistical guarantees about their relation\nto the underlying data distribution, limiting their applicability in critical\ndomains requiring robust error bounds. We address this fundamental limitation\nby presenting a novel framework that incorporates conformal prediction\nmethodologies into Generative Adversarial Networks (GANs). By integrating\nmultiple conformal prediction paradigms including Inductive Conformal\nPrediction (ICP), Mondrian Conformal Prediction, Cross-Conformal Prediction,\nand Venn-Abers Predictors, we establish distribution-free uncertainty\nquantification in generated samples. This approach, termed Conformalized GAN\n(cGAN), demonstrates enhanced calibration properties while maintaining the\ngenerative power of traditional GANs, producing synthetic data with provable\nstatistical guarantees. We provide rigorous mathematical proofs establishing\nfinite-sample validity guarantees and asymptotic efficiency properties,\nenabling the reliable application of synthetic data in high-stakes domains\nincluding healthcare, finance, and autonomous systems."
    },
    {
        "date": "2025-04",
        "title": "BadVideo: Stealthy Backdoor Attack against Text-to-Video Generation",
        "author": "Ruotong Wang, Mingli Zhu, Jiarong Ou, Rui Chen, Xin Tao, Pengfei Wan, and Baoyuan Wu",
        "link": "http://arxiv.org/abs/2504.16907v1",
        "abstract": "Text-to-video (T2V) generative models have rapidly advanced and found\nwidespread applications across fields like entertainment, education, and\nmarketing. However, the adversarial vulnerabilities of these models remain\nrarely explored. We observe that in T2V generation tasks, the generated videos\noften contain substantial redundant information not explicitly specified in the\ntext prompts, such as environmental elements, secondary objects, and additional\ndetails, providing opportunities for malicious attackers to embed hidden\nharmful content. Exploiting this inherent redundancy, we introduce BadVideo,\nthe first backdoor attack framework tailored for T2V generation. Our attack\nfocuses on designing target adversarial outputs through two key strategies: (1)\nSpatio-Temporal Composition, which combines different spatiotemporal features\nto encode malicious information; (2) Dynamic Element Transformation, which\nintroduces transformations in redundant elements over time to convey malicious\ninformation. Based on these strategies, the attacker's malicious target\nseamlessly integrates with the user's textual instructions, providing high\nstealthiness. Moreover, by exploiting the temporal dimension of videos, our\nattack successfully evades traditional content moderation systems that\nprimarily analyze spatial information within individual frames. Extensive\nexperiments demonstrate that BadVideo achieves high attack success rates while\npreserving original semantics and maintaining excellent performance on clean\ninputs. Overall, our work reveals the adversarial vulnerability of T2V models,\ncalling attention to potential risks and misuse. Our project page is at\nhttps://wrt2000.github.io/BadVideo2025/."
    },
    {
        "date": "2025-04",
        "title": "Building A Secure Agentic AI Application Leveraging A2A Protocol",
        "author": "Idan Habler, Ken Huang, Vineeth Sai Narajala, and Prashant Kulkarni",
        "link": "http://arxiv.org/abs/2504.16902v1",
        "abstract": "As Agentic AI systems evolve from basic workflows to complex multi agent\ncollaboration, robust protocols such as Google's Agent2Agent (A2A) become\nessential enablers. To foster secure adoption and ensure the reliability of\nthese complex interactions, understanding the secure implementation of A2A is\nessential. This paper addresses this goal by providing a comprehensive security\nanalysis centered on the A2A protocol. We examine its fundamental elements and\noperational dynamics, situating it within the framework of agent communication\ndevelopment. Utilizing the MAESTRO framework, specifically designed for AI\nrisks, we apply proactive threat modeling to assess potential security issues\nin A2A deployments, focusing on aspects such as Agent Card management, task\nexecution integrity, and authentication methodologies.\n  Based on these insights, we recommend practical secure development\nmethodologies and architectural best practices designed to build resilient and\neffective A2A systems. Our analysis also explores how the synergy between A2A\nand the Model Context Protocol (MCP) can further enhance secure\ninteroperability. This paper equips developers and architects with the\nknowledge and practical guidance needed to confidently leverage the A2A\nprotocol for building robust and secure next generation agentic applications."
    },
    {
        "date": "2025-04",
        "title": "Simplified Swarm Learning Framework for Robust and Scalable Diagnostic Services in Cancer Histopathology",
        "author": "Yanjie Wu, Yuhao Ji, Saiho Lee, Juniad Akram, Ali Braytee, and Ali Anaissi",
        "link": "http://arxiv.org/abs/2504.16732v1",
        "abstract": "The complexities of healthcare data, including privacy concerns, imbalanced\ndatasets, and interoperability issues, necessitate innovative machine learning\nsolutions. Swarm Learning (SL), a decentralized alternative to Federated\nLearning, offers privacy-preserving distributed training, but its reliance on\nblockchain technology hinders accessibility and scalability. This paper\nintroduces a \\textit{Simplified Peer-to-Peer Swarm Learning (P2P-SL) Framework}\ntailored for resource-constrained environments. By eliminating blockchain\ndependencies and adopting lightweight peer-to-peer communication, the proposed\nframework ensures robust model synchronization while maintaining data privacy.\nApplied to cancer histopathology, the framework integrates optimized\npre-trained models, such as TorchXRayVision, enhanced with DenseNet decoders,\nto improve diagnostic accuracy. Extensive experiments demonstrate the\nframework's efficacy in handling imbalanced and biased datasets, achieving\ncomparable performance to centralized models while preserving privacy. This\nstudy paves the way for democratizing advanced machine learning in healthcare,\noffering a scalable, accessible, and efficient solution for privacy-sensitive\ndiagnostic applications."
    },
    {
        "date": "2025-04",
        "title": "V$^2$R-Bench: Holistically Evaluating LVLM Robustness to Fundamental Visual Variations",
        "author": "Zhiyuan Fan, Yumeng Wang, Sandeep Polisetty, and Yi R. Fung",
        "link": "http://arxiv.org/abs/2504.16727v2",
        "abstract": "Large Vision Language Models (LVLMs) excel in various vision-language tasks.\nYet, their robustness to visual variations in position, scale, orientation, and\ncontext that objects in natural scenes inevitably exhibit due to changes in\nviewpoint and environment remains largely underexplored. To bridge this gap, we\nintroduce V$^2$R-Bench, a comprehensive benchmark framework for evaluating\nVisual Variation Robustness of LVLMs, which encompasses automated evaluation\ndataset generation and principled metrics for thorough robustness assessment.\nThrough extensive evaluation on 21 LVLMs, we reveal a surprising vulnerability\nto visual variations, in which even advanced models that excel at complex\nvision-language tasks significantly underperform on simple tasks such as object\nrecognition. Interestingly, these models exhibit a distinct visual position\nbias that contradicts theories of effective receptive fields, and demonstrate a\nhuman-like visual acuity threshold. To identify the source of these\nvulnerabilities, we present a systematic framework for component-level\nanalysis, featuring a novel visualization approach for aligned visual features.\nResults show that these vulnerabilities stem from error accumulation in the\npipeline architecture and inadequate multimodal alignment. Complementary\nexperiments with synthetic data further demonstrate that these limitations are\nfundamentally architectural deficiencies, scoring the need for architectural\ninnovations in future LVLM designs."
    },
    {
        "date": "2025-04",
        "title": "MCMC for Bayesian estimation of Differential Privacy from Membership Inference Attacks",
        "author": "Ceren Yildirim, Kamer Kaya, Sinan Yildirim, and Erkay Savas",
        "link": "http://arxiv.org/abs/2504.16683v1",
        "abstract": "We propose a new framework for Bayesian estimation of differential privacy,\nincorporating evidence from multiple membership inference attacks (MIA).\nBayesian estimation is carried out via a Markov chain Monte Carlo (MCMC)\nalgorithm, named MCMC-DP-Est, which provides an estimate of the full posterior\ndistribution of the privacy parameter (e.g., instead of just credible\nintervals). Critically, the proposed method does not assume that privacy\nauditing is performed with the most powerful attack on the worst-case (dataset,\nchallenge point) pair, which is typically unrealistic. Instead, MCMC-DP-Est\njointly estimates the strengths of MIAs used and the privacy of the training\nalgorithm, yielding a more cautious privacy analysis. We also present an\neconomical way to generate measurements for the performance of an MIA that is\nto be used by the MCMC method to estimate privacy. We present the use of the\nmethods with numerical examples with both artificial and real data."
    },
    {
        "date": "2025-04",
        "title": "Security Science (SecSci), Basic Concepts and Mathematical Foundations",
        "author": "Dusko Pavlovic, and Peter-Michael Seidel",
        "link": "http://arxiv.org/abs/2504.16617v1",
        "abstract": "This textbook compiles the lecture notes from security courses taught at\nOxford in the 2000s, at Royal Holloway in the 2010s, and currently in Hawaii.\nThe early chapters are suitable for a first course in security. The middle\nchapters have been used in advanced courses. Towards the end there are also\nsome research problems."
    },
    {
        "date": "2025-04",
        "title": "LaSDVS : A Post-Quantum Secure Compact Strong-Designated Verifier Signature",
        "author": "Shanu Poddar, Sweta Mishra, Tapaswini Mohanty, Vikas Srivastava, and Sugata Gangopadhyay",
        "link": "http://arxiv.org/abs/2504.16571v1",
        "abstract": "Digital signatures are fundamental cryptographic primitives that ensure the\nauthenticity and integrity of digital communication. However, in scenarios\ninvolving sensitive interactions -- such as e-voting or e-cash -- there is a\ngrowing need for more controlled signing mechanisms. Strong-Designated Verifier\nSignature (SDVS) offers such control by allowing the signer to specify and\nrestrict the verifier of a signature. The existing state-of-the-art SDVS are\nmostly based on number-theoretic hardness assumptions. Thus, they are not\nsecure against quantum attacks. Moreover, Post-Quantum Cryptography (PQC)-based\nSDVS are inefficient and have large key and signature sizes. In this work, we\naddress these challenges and propose an efficient post-quantum SDVS (namely,\nLaSDVS) based on ideal lattices under the hardness assumptions of the Ring-SIS\nand Ring-LWE problems. LaSDVS achieves advanced security properties including\nstrong unforgeability under chosen-message attacks, non-transferability,\nnon-delegatability, and signer anonymity. By employing the algebraic structure\nof rings and the gadget trapdoor mechanism of Micciancio et al., we design\nLaSDVS to minimize computational overhead and significantly reduce key and\nsignature sizes. Notably, our scheme achieves a compact signature size of\n$\\mathcal{O}(n\\log q)$, compared to $\\mathcal{O}(n^2)$ size, where $n$ is the\nsecurity parameter, in the existing state-of-the-art PQC designs. To the best\nof our knowledge, LaSDVS offers the \\textit{smallest private key and signature\nsize} among the existing PQC-based SDVS schemes."
    },
    {
        "date": "2025-04",
        "title": "Amplified Vulnerabilities: Structured Jailbreak Attacks on LLM-based Multi-Agent Debate",
        "author": "Senmao Qi, Yifei Zou, Peng Li, Ziyi Lin, Xiuzhen Cheng, and Dongxiao Yu",
        "link": "http://arxiv.org/abs/2504.16489v1",
        "abstract": "Multi-Agent Debate (MAD), leveraging collaborative interactions among Large\nLanguage Models (LLMs), aim to enhance reasoning capabilities in complex tasks.\nHowever, the security implications of their iterative dialogues and\nrole-playing characteristics, particularly susceptibility to jailbreak attacks\neliciting harmful content, remain critically underexplored. This paper\nsystematically investigates the jailbreak vulnerabilities of four prominent MAD\nframeworks built upon leading commercial LLMs (GPT-4o, GPT-4, GPT-3.5-turbo,\nand DeepSeek) without compromising internal agents. We introduce a novel\nstructured prompt-rewriting framework specifically designed to exploit MAD\ndynamics via narrative encapsulation, role-driven escalation, iterative\nrefinement, and rhetorical obfuscation. Our extensive experiments demonstrate\nthat MAD systems are inherently more vulnerable than single-agent setups.\nCrucially, our proposed attack methodology significantly amplifies this\nfragility, increasing average harmfulness from 28.14% to 80.34% and achieving\nattack success rates as high as 80% in certain scenarios. These findings reveal\nintrinsic vulnerabilities in MAD architectures and underscore the urgent need\nfor robust, specialized defenses prior to real-world deployment."
    },
    {
        "date": "2025-04",
        "title": "Seeking Flat Minima over Diverse Surrogates for Improved Adversarial Transferability: A Theoretical Framework and Algorithmic Instantiation",
        "author": "Meixi Zheng, Kehan Wu, Yanbo Fan, Rui Huang, and Baoyuan Wu",
        "link": "http://arxiv.org/abs/2504.16474v1",
        "abstract": "The transfer-based black-box adversarial attack setting poses the challenge\nof crafting an adversarial example (AE) on known surrogate models that remain\neffective against unseen target models. Due to the practical importance of this\ntask, numerous methods have been proposed to address this challenge. However,\nmost previous methods are heuristically designed and intuitively justified,\nlacking a theoretical foundation. To bridge this gap, we derive a novel\ntransferability bound that offers provable guarantees for adversarial\ntransferability. Our theoretical analysis has the advantages of \\textit{(i)}\ndeepening our understanding of previous methods by building a general attack\nframework and \\textit{(ii)} providing guidance for designing an effective\nattack algorithm. Our theoretical results demonstrate that optimizing AEs\ntoward flat minima over the surrogate model set, while controlling the\nsurrogate-target model shift measured by the adversarial model discrepancy,\nyields a comprehensive guarantee for AE transferability. The results further\nlead to a general transfer-based attack framework, within which we observe that\nprevious methods consider only partial factors contributing to the\ntransferability. Algorithmically, inspired by our theoretical results, we first\nelaborately construct the surrogate model set in which models exhibit diverse\nadversarial vulnerabilities with respect to AEs to narrow an instantiated\nadversarial model discrepancy. Then, a \\textit{model-Diversity-compatible\nReverse Adversarial Perturbation} (DRAP) is generated to effectively promote\nthe flatness of AEs over diverse surrogate models to improve transferability.\nExtensive experiments on NIPS2017 and CIFAR-10 datasets against various target\nmodels demonstrate the effectiveness of our proposed attack."
    },
    {
        "date": "2025-04",
        "title": "MTSGL: Multi-Task Structure Guided Learning for Robust and Interpretable SAR Aircraft Recognition",
        "author": "Qishan He, Lingjun Zhao, Ru Luo, Siqian Zhang, Lin Lei, Kefeng Ji, and Gangyao Kuang",
        "link": "http://arxiv.org/abs/2504.16467v1",
        "abstract": "Aircraft recognition in synthetic aperture radar (SAR) imagery is a\nfundamental mission in both military and civilian applications. Recently deep\nlearning (DL) has emerged a dominant paradigm for its explosive performance on\nextracting discriminative features. However, current classification algorithms\nfocus primarily on learning decision hyperplane without enough comprehension on\naircraft structural knowledge. Inspired by the fined aircraft annotation\nmethods for optical remote sensing images (RSI), we first introduce a\nstructure-based SAR aircraft annotations approach to provide structural and\ncompositional supplement information. On this basis, we propose a multi-task\nstructure guided learning (MTSGL) network for robust and interpretable SAR\naircraft recognition. Besides the classification task, MTSGL includes a\nstructural semantic awareness (SSA) module and a structural consistency\nregularization (SCR) module. The SSA is designed to capture structure semantic\ninformation, which is conducive to gain human-like comprehension of aircraft\nknowledge. The SCR helps maintain the geometric consistency between the\naircraft structure in SAR imagery and the proposed annotation. In this process,\nthe structural attribute can be disentangled in a geometrically meaningful\nmanner. In conclusion, the MTSGL is presented with the expert-level aircraft\nprior knowledge and structure guided learning paradigm, aiming to comprehend\nthe aircraft concept in a way analogous to the human cognitive process.\nExtensive experiments are conducted on a self-constructed multi-task SAR\naircraft recognition dataset (MT-SARD) and the effective results illustrate the\nsuperiority of robustness and interpretation ability of the proposed MTSGL."
    },
    {
        "date": "2025-04",
        "title": "Give LLMs a Security Course: Securing Retrieval-Augmented Code Generation via Knowledge Injection",
        "author": "Bo Lin, Shangwen Wang, Yihao Qin, Liqian Chen, and Xiaoguang Mao",
        "link": "http://arxiv.org/abs/2504.16429v1",
        "abstract": "Retrieval-Augmented Code Generation (RACG) leverages external knowledge to\nenhance Large Language Models (LLMs) in code synthesis, improving the\nfunctional correctness of the generated code. However, existing RACG systems\nlargely overlook security, leading to substantial risks. Especially, the\npoisoning of malicious code into knowledge bases can mislead LLMs, resulting in\nthe generation of insecure outputs, which poses a critical threat in modern\nsoftware development. To address this, we propose a security-hardening\nframework for RACG systems, CodeGuarder, that shifts the paradigm from\nretrieving only functional code examples to incorporating both functional code\nand security knowledge. Our framework constructs a security knowledge base from\nreal-world vulnerability databases, including secure code samples and root\ncause annotations. For each code generation query, a retriever decomposes the\nquery into fine-grained sub-tasks and fetches relevant security knowledge. To\nprioritize critical security guidance, we introduce a re-ranking and filtering\nmechanism by leveraging the LLMs' susceptibility to different vulnerability\ntypes. This filtered security knowledge is seamlessly integrated into the\ngeneration prompt. Our evaluation shows CodeGuarder significantly improves code\nsecurity rates across various LLMs, achieving average improvements of 20.12\\%\nin standard RACG, and 31.53\\% and 21.91\\% under two distinct poisoning\nscenarios without compromising functional correctness. Furthermore, CodeGuarder\ndemonstrates strong generalization, enhancing security even when the targeted\nlanguage's security knowledge is lacking. This work presents CodeGuarder as a\npivotal advancement towards building secure and trustworthy RACG systems."
    },
    {
        "date": "2025-04",
        "title": "VideoMark: A Distortion-Free Robust Watermarking Framework for Video Diffusion Models",
        "author": "Xuming Hu, Hanqian Li, Jungang Li, and Aiwei Liu",
        "link": "http://arxiv.org/abs/2504.16359v1",
        "abstract": "This work presents VideoMark, a training-free robust watermarking framework\nfor video diffusion models. As diffusion models advance in generating highly\nrealistic videos, the need for reliable content attribution mechanisms has\nbecome critical. While watermarking techniques for image diffusion models have\nmade progress, directly extending these methods to videos presents unique\nchallenges due to variable video lengths and vulnerability to temporal attacks.\nVideoMark addresses these limitations through a frame-wise watermarking\nstrategy using pseudorandom error correction (PRC) codes to embed watermark\ninformation during the generation process. Our method generates an extended\nwatermark message sequence and randomly selects starting positions for each\nvideo, ensuring uniform noise distribution in the latent space and maintaining\ngeneration quality. For watermark extraction, we introduce a Temporal Matching\nModule (TMM) that uses edit distance to align decoded messages with the\noriginal watermark sequence, providing robustness against temporal attacks such\nas frame deletion. Experimental results demonstrate that VideoMark achieves\nhigher decoding accuracy than existing methods while maintaining video quality\non par with watermark-free generation. Importantly, our watermark remains\nundetectable to attackers without the secret key, ensuring strong\nimperceptibility compared to other watermarking frameworks. VideoMark provides\na practical solution for content attribution in diffusion-based video\ngeneration without requiring additional training or compromising video quality.\nOur code and data are available at\n\\href{https://github.com/KYRIE-LI11/VideoMark}{https://github.com/KYRIE-LI11/VideoMark}."
    },
    {
        "date": "2025-04",
        "title": "Property-Preserving Hashing for $\\ell_1$-Distance Predicates: Applications to Countering Adversarial Input Attacks",
        "author": "Hassan Asghar, Chenhan Zhang, and Dali Kaafar",
        "link": "http://arxiv.org/abs/2504.16355v1",
        "abstract": "Perceptual hashing is used to detect whether an input image is similar to a\nreference image with a variety of security applications. Recently, they have\nbeen shown to succumb to adversarial input attacks which make small\nimperceptible changes to the input image yet the hashing algorithm does not\ndetect its similarity to the original image. Property-preserving hashing (PPH)\nis a recent construct in cryptography, which preserves some property\n(predicate) of its inputs in the hash domain. Researchers have so far shown\nconstructions of PPH for Hamming distance predicates, which, for instance,\noutputs 1 if two inputs are within Hamming distance $t$. A key feature of PPH\nis its strong correctness guarantee, i.e., the probability that the predicate\nwill not be correctly evaluated in the hash domain is negligible. Motivated by\nthe use case of detecting similar images under adversarial setting, we propose\nthe first PPH construction for an $\\ell_1$-distance predicate. Roughly, this\npredicate checks if the two one-sided $\\ell_1$-distances between two images are\nwithin a threshold $t$. Since many adversarial attacks use $\\ell_2$-distance\n(related to $\\ell_1$-distance) as the objective function to perturb the input\nimage, by appropriately choosing the threshold $t$, we can force the attacker\nto add considerable noise to evade detection, and hence significantly\ndeteriorate the image quality. Our proposed scheme is highly efficient, and\nruns in time $O(t^2)$. For grayscale images of size $28 \\times 28$, we can\nevaluate the predicate in $0.0784$ seconds when pixel values are perturbed by\nup to $1 \\%$. For larger RGB images of size $224 \\times 224$, by dividing the\nimage into 1,000 blocks, we achieve times of $0.0128$ seconds per block for $1\n\\%$ change, and up to $0.2641$ seconds per block for $14\\%$ change."
    },
    {
        "date": "2025-04",
        "title": "Object Learning and Robust 3D Reconstruction",
        "author": "Sara Sabour",
        "link": "http://arxiv.org/abs/2504.17812v1",
        "abstract": "In this thesis we discuss architectural designs and training methods for a\nneural network to have the ability of dissecting an image into objects of\ninterest without supervision. The main challenge in 2D unsupervised object\nsegmentation is distinguishing between foreground objects of interest and\nbackground. FlowCapsules uses motion as a cue for the objects of interest in 2D\nscenarios. The last part of this thesis focuses on 3D applications where the\ngoal is detecting and removal of the object of interest from the input images.\nIn these tasks, we leverage the geometric consistency of scenes in 3D to detect\nthe inconsistent dynamic objects. Our transient object masks are then used for\ndesigning robust optimization kernels to improve 3D modelling in a casual\ncapture setup. One of our goals in this thesis is to show the merits of\nunsupervised object based approaches in computer vision. Furthermore, we\nsuggest possible directions for defining objects of interest or foreground\nobjects without requiring supervision. Our hope is to motivate and excite the\ncommunity into further exploring explicit object representations in image\nunderstanding tasks."
    },
    {
        "date": "2025-04",
        "title": "Defending Against Intelligent Attackers at Large Scales",
        "author": "Andrew J. Lohn",
        "link": "http://arxiv.org/abs/2504.18577v1",
        "abstract": "We investigate the scale of attack and defense mathematically in the context\nof AI's possible effect on cybersecurity. For a given target today, highly\nscaled cyber attacks such as from worms or botnets typically all fail or all\nsucceed. Here, we consider the effect of scale if those attack agents were\nintelligent and creative enough to act independently such that each attack\nattempt was different from the others or such that attackers could learn from\ntheir successes and failures. We find that small increases in the number or\nquality of defenses can compensate for exponential increases in the number of\nindependent attacks and for exponential speedups."
    },
    {
        "date": "2025-04",
        "title": "Blockchain Meets Adaptive Honeypots: A Trust-Aware Approach to Next-Gen IoT Security",
        "author": "Yazan Otoum, Arghavan Asad, and Amiya Nayak",
        "link": "http://arxiv.org/abs/2504.16226v1",
        "abstract": "Edge computing-based Next-Generation Wireless Networks (NGWN)-IoT offer\nenhanced bandwidth capacity for large-scale service provisioning but remain\nvulnerable to evolving cyber threats. Existing intrusion detection and\nprevention methods provide limited security as adversaries continually adapt\ntheir attack strategies. We propose a dynamic attack detection and prevention\napproach to address this challenge. First, blockchain-based authentication uses\nthe Deoxys Authentication Algorithm (DAA) to verify IoT device legitimacy\nbefore data transmission. Next, a bi-stage intrusion detection system is\nintroduced: the first stage uses signature-based detection via an Improved\nRandom Forest (IRF) algorithm. In contrast, the second stage applies\nfeature-based anomaly detection using a Diffusion Convolution Recurrent Neural\nNetwork (DCRNN). To ensure Quality of Service (QoS) and maintain Service Level\nAgreements (SLA), trust-aware service migration is performed using Heap-Based\nOptimization (HBO). Additionally, on-demand virtual High-Interaction honeypots\ndeceive attackers and extract attack patterns, which are securely stored using\nthe Bimodal Lattice Signature Scheme (BLISS) to enhance signature-based\nIntrusion Detection Systems (IDS). The proposed framework is implemented in the\nNS3 simulation environment and evaluated against existing methods across\nmultiple performance metrics, including accuracy, attack detection rate, false\nnegative rate, precision, recall, ROC curve, memory usage, CPU usage, and\nexecution time. Experimental results demonstrate that the framework\nsignificantly outperforms existing approaches, reinforcing the security of\nNGWN-enabled IoT ecosystems"
    },
    {
        "date": "2025-04",
        "title": "WASP: Benchmarking Web Agent Security Against Prompt Injection Attacks",
        "author": "Ivan Evtimov, Arman Zharmagambetov, Aaron Grattafiori, Chuan Guo, and Kamalika Chaudhuri",
        "link": "http://arxiv.org/abs/2504.18575v2",
        "abstract": "Web navigation AI agents use language-and-vision foundation models to enhance\nproductivity but these models are known to be susceptible to indirect prompt\ninjections that get them to follow instructions different from the legitimate\nuser's. Existing explorations of this threat applied to web agents often focus\non a single isolated adversarial goal, test with injected instructions that are\neither too easy or not truly malicious, and often give the adversary\nunreasonable access. In order to better focus adversarial research, we\nconstruct a new benchmark called WASP (Web Agent Security against Prompt\ninjection attacks) that introduces realistic web agent hijacking objectives and\nan isolated environment to test them in that does not affect real users or the\nlive web. As part of WASP, we also develop baseline attacks against popular web\nagentic systems (VisualWebArena, Claude Computer Use, etc.) instantiated with\nvarious state-of-the-art models. Our evaluation shows that even AI agents\nbacked by models with advanced reasoning capabilities and by models with\ninstruction hierarchy mitigations are susceptible to low-effort human-written\nprompt injections. However, the realistic objectives in WASP also allow us to\nobserve that agents are currently not capable enough to complete the goals of\nattackers end-to-end. Agents begin executing the adversarial instruction\nbetween 16 and 86% of the time but only achieve the goal between 0 and 17% of\nthe time. Based on these findings, we argue that adversarial researchers should\ndemonstrate stronger attacks that more consistently maintain control over the\nagent given realistic constraints on the adversary's power."
    },
    {
        "date": "2025-04",
        "title": "LLMs meet Federated Learning for Scalable and Secure IoT Management",
        "author": "Yazan Otoum, Arghavan Asad, and Amiya Nayak",
        "link": "http://arxiv.org/abs/2504.16032v1",
        "abstract": "The rapid expansion of IoT ecosystems introduces severe challenges in\nscalability, security, and real-time decision-making. Traditional centralized\narchitectures struggle with latency, privacy concerns, and excessive resource\nconsumption, making them unsuitable for modern large-scale IoT deployments.\nThis paper presents a novel Federated Learning-driven Large Language Model\n(FL-LLM) framework, designed to enhance IoT system intelligence while ensuring\ndata privacy and computational efficiency. The framework integrates Generative\nIoT (GIoT) models with a Gradient Sensing Federated Strategy (GSFS),\ndynamically optimizing model updates based on real-time network conditions. By\nleveraging a hybrid edge-cloud processing architecture, our approach balances\nintelligence, scalability, and security in distributed IoT environments.\nEvaluations on the IoT-23 dataset demonstrate that our framework improves model\naccuracy, reduces response latency, and enhances energy efficiency,\noutperforming traditional FL techniques (i.e., FedAvg, FedOpt). These findings\nhighlight the potential of integrating LLM-powered federated learning into\nlarge-scale IoT ecosystems, paving the way for more secure, scalable, and\nadaptive IoT management solutions."
    },
    {
        "date": "2025-04",
        "title": "A New Graph Grammar Formalism for Robust Syntactic Pattern Recognition",
        "author": "Peter Fletcher",
        "link": "http://arxiv.org/abs/2504.15975v2",
        "abstract": "I introduce a formalism for representing the syntax of recursively structured\ngraph-like patterns. It does not use production rules, like a conventional\ngraph grammar, but represents the syntactic structure in a more direct and\ndeclarative way. The grammar and the pattern are both represented as networks,\nand parsing is seen as the construction of a homomorphism from the pattern to\nthe grammar. The grammars can represent iterative, hierarchical and nested\nrecursive structure in more than one dimension.\n  This supports a highly parallel style of parsing, in which all aspects of\npattern recognition (feature detection, segmentation, parsing, filling in\nmissing symbols, top-down and bottom-up inference) are integrated into a single\nprocess, to exploit the synergy between them.\n  The emphasis of this paper is on underlying theoretical issues, but I also\ngive some example runs to illustrate the error-tolerant parsing of complex\nrecursively structured patterns of 50-1000 symbols, involving variability in\ngeometric relationships, blurry and indistinct symbols, overlapping symbols,\ncluttered images, and erased patches."
    },
    {
        "date": "2025-04",
        "title": "Adversarial Observations in Weather Forecasting",
        "author": "Erik Imgrund, Thorsten Eisenhofer, and Konrad Rieck",
        "link": "http://arxiv.org/abs/2504.15942v1",
        "abstract": "AI-based systems, such as Google's GenCast, have recently redefined the state\nof the art in weather forecasting, offering more accurate and timely\npredictions of both everyday weather and extreme events. While these systems\nare on the verge of replacing traditional meteorological methods, they also\nintroduce new vulnerabilities into the forecasting process. In this paper, we\ninvestigate this threat and present a novel attack on autoregressive diffusion\nmodels, such as those used in GenCast, capable of manipulating weather\nforecasts and fabricating extreme events, including hurricanes, heat waves, and\nintense rainfall. The attack introduces subtle perturbations into weather\nobservations that are statistically indistinguishable from natural noise and\nchange less than 0.1% of the measurements - comparable to tampering with data\nfrom a single meteorological satellite. As modern forecasting integrates data\nfrom nearly a hundred satellites and many other sources operated by different\ncountries, our findings highlight a critical security risk with the potential\nto cause large-scale disruptions and undermine public trust in weather\nprediction."
    },
    {
        "date": "2025-04",
        "title": "Human-Imperceptible Physical Adversarial Attack for NIR Face Recognition Models",
        "author": "Songyan Xie, Jinghang Wen, Encheng Su, and Qiucheng Yu",
        "link": "http://arxiv.org/abs/2504.15823v1",
        "abstract": "Near-infrared (NIR) face recognition systems, which can operate effectively\nin low-light conditions or in the presence of makeup, exhibit vulnerabilities\nwhen subjected to physical adversarial attacks. To further demonstrate the\npotential risks in real-world applications, we design a novel, stealthy, and\npractical adversarial patch to attack NIR face recognition systems in a\nblack-box setting. We achieved this by utilizing human-imperceptible\ninfrared-absorbing ink to generate multiple patches with digitally optimized\nshapes and positions for infrared images. To address the optimization mismatch\nbetween digital and real-world NIR imaging, we develop a light reflection model\nfor human skin to minimize pixel-level discrepancies by simulating NIR light\nreflection.\n  Compared to state-of-the-art (SOTA) physical attacks on NIR face recognition\nsystems, the experimental results show that our method improves the attack\nsuccess rate in both digital and physical domains, particularly maintaining\neffectiveness across various face postures. Notably, the proposed approach\noutperforms SOTA methods, achieving an average attack success rate of 82.46% in\nthe physical domain across different models, compared to 64.18% for existing\nmethods. The artifact is available at\nhttps://anonymous.4open.science/r/Human-imperceptible-adversarial-patch-0703/."
    },
    {
        "date": "2025-04",
        "title": "Residual-Evasive Attacks on ADMM in Distributed Optimization",
        "author": "Sabrina Bruckmeier, Huadong Mo, and James Qin",
        "link": "http://arxiv.org/abs/2504.18570v1",
        "abstract": "This paper presents two attack strategies designed to evade detection in\nADMM-based systems by preventing significant changes to the residual during the\nattacked iteration. While many detection algorithms focus on identifying false\ndata injection through residual changes, we show that our attacks remain\nundetected by keeping the residual largely unchanged. The first strategy uses a\nrandom starting point combined with Gram-Schmidt orthogonalization to ensure\nstealth, with potential for refinement by enhancing the orthogonal component to\nincrease system disruption. The second strategy builds on the first, targeting\nfinancial gains by manipulating reactive power and pushing the system to its\nupper voltage limit, exploiting operational constraints. The effectiveness of\nthe proposed attack-resilient mechanism is demonstrated through case studies on\nthe IEEE 14-bus system. A comparison of the two strategies, along with commonly\nused naive attacks, reveals trade-offs between simplicity, detectability, and\neffectiveness, providing insights into ADMM system vulnerabilities. These\nfindings underscore the need for more robust monitoring algorithms to protect\nagainst advanced attack strategies."
    },
    {
        "date": "2025-04",
        "title": "Advancing Embodied Agent Security: From Safety Benchmarks to Input Moderation",
        "author": "Ning Wang, Zihan Yan, Weiyang Li, Chuan Ma, He Chen, and Tao Xiang",
        "link": "http://arxiv.org/abs/2504.15699v1",
        "abstract": "Embodied agents exhibit immense potential across a multitude of domains,\nmaking the assurance of their behavioral safety a fundamental prerequisite for\ntheir widespread deployment. However, existing research predominantly\nconcentrates on the security of general large language models, lacking\nspecialized methodologies for establishing safety benchmarks and input\nmoderation tailored to embodied agents. To bridge this gap, this paper\nintroduces a novel input moderation framework, meticulously designed to\nsafeguard embodied agents. This framework encompasses the entire pipeline,\nincluding taxonomy definition, dataset curation, moderator architecture, model\ntraining, and rigorous evaluation. Notably, we introduce EAsafetyBench, a\nmeticulously crafted safety benchmark engineered to facilitate both the\ntraining and stringent assessment of moderators specifically designed for\nembodied agents. Furthermore, we propose Pinpoint, an innovative\nprompt-decoupled input moderation scheme that harnesses a masked attention\nmechanism to effectively isolate and mitigate the influence of functional\nprompts on moderation tasks. Extensive experiments conducted on diverse\nbenchmark datasets and models validate the feasibility and efficacy of the\nproposed approach. The results demonstrate that our methodologies achieve an\nimpressive average detection accuracy of 94.58%, surpassing the performance of\nexisting state-of-the-art techniques, alongside an exceptional moderation\nprocessing time of merely 0.002 seconds per instance."
    },
    {
        "date": "2025-04",
        "title": "TrojanDam: Detection-Free Backdoor Defense in Federated Learning through Proactive Model Robustification utilizing OOD Data",
        "author": "Yanbo Dai, Songze Li, Zihan Gan, and Xueluan Gong",
        "link": "http://arxiv.org/abs/2504.15674v1",
        "abstract": "Federated learning (FL) systems allow decentralized data-owning clients to\njointly train a global model through uploading their locally trained updates to\na centralized server. The property of decentralization enables adversaries to\ncraft carefully designed backdoor updates to make the global model misclassify\nonly when encountering adversary-chosen triggers. Existing defense mechanisms\nmainly rely on post-training detection after receiving updates. These methods\neither fail to identify updates which are deliberately fabricated statistically\nclose to benign ones, or show inconsistent performance in different FL training\nstages. The effect of unfiltered backdoor updates will accumulate in the global\nmodel, and eventually become functional. Given the difficulty of ruling out\nevery backdoor update, we propose a backdoor defense paradigm, which focuses on\nproactive robustification on the global model against potential backdoor\nattacks. We first reveal that the successful launching of backdoor attacks in\nFL stems from the lack of conflict between malicious and benign updates on\nredundant neurons of ML models. We proceed to prove the feasibility of\nactivating redundant neurons utilizing out-of-distribution (OOD) samples in\ncentralized settings, and migrating to FL settings to propose a novel backdoor\ndefense mechanism, TrojanDam. The proposed mechanism has the FL server\ncontinuously inject fresh OOD mappings into the global model to activate\nredundant neurons, canceling the effect of backdoor updates during aggregation.\nWe conduct systematic and extensive experiments to illustrate the superior\nperformance of TrojanDam, over several SOTA backdoor defense methods across a\nwide range of FL settings."
    },
    {
        "date": "2025-04",
        "title": "Analytical Softmax Temperature Setting from Feature Dimensions for Model- and Domain-Robust Classification",
        "author": "Tatsuhito Hasegawa, and Shunsuke Sakai",
        "link": "http://arxiv.org/abs/2504.15594v1",
        "abstract": "In deep learning-based classification tasks, the softmax function's\ntemperature parameter $T$ critically influences the output distribution and\noverall performance. This study presents a novel theoretical insight that the\noptimal temperature $T^*$ is uniquely determined by the dimensionality of the\nfeature representations, thereby enabling training-free determination of $T^*$.\nDespite this theoretical grounding, empirical evidence reveals that $T^*$\nfluctuates under practical conditions owing to variations in models, datasets,\nand other confounding factors. To address these influences, we propose and\noptimize a set of temperature determination coefficients that specify how $T^*$\nshould be adjusted based on the theoretical relationship to feature\ndimensionality. Additionally, we insert a batch normalization layer immediately\nbefore the output layer, effectively stabilizing the feature space. Building on\nthese coefficients and a suite of large-scale experiments, we develop an\nempirical formula to estimate $T^*$ without additional training while also\nintroducing a corrective scheme to refine $T^*$ based on the number of classes\nand task complexity. Our findings confirm that the derived temperature not only\naligns with the proposed theoretical perspective but also generalizes\neffectively across diverse tasks, consistently enhancing classification\nperformance and offering a practical, training-free solution for determining\n$T^*$."
    },
    {
        "date": "2025-04",
        "title": "Kill two birds with one stone: generalized and robust AI-generated text detection via dynamic perturbations",
        "author": "Yinghan Zhou, Juan Wen, Wanli Peng, Yiming Xue, Ziwei Zhang, and Zhengxian Wu",
        "link": "http://arxiv.org/abs/2504.21019v1",
        "abstract": "The growing popularity of large language models has raised concerns regarding\nthe potential to misuse AI-generated text (AIGT). It becomes increasingly\ncritical to establish an excellent AIGT detection method with high\ngeneralization and robustness. However, existing methods either focus on model\ngeneralization or concentrate on robustness. The unified mechanism, to\nsimultaneously address the challenges of generalization and robustness, is less\nexplored. In this paper, we argue that robustness can be view as a specific\nform of domain shift, and empirically reveal an intrinsic mechanism for model\ngeneralization of AIGT detection task. Then, we proposed a novel AIGT detection\nmethod (DP-Net) via dynamic perturbations introduced by a reinforcement\nlearning with elaborated reward and action. Experimentally, extensive results\nshow that the proposed DP-Net significantly outperforms some state-of-the-art\nAIGT detection methods for generalization capacity in three cross-domain\nscenarios. Meanwhile, the DP-Net achieves best robustness under two text\nadversarial attacks. The code is publicly available at\nhttps://github.com/CAU-ISS-Lab/AIGT-Detection-Evade-Detection/tree/main/DP-Net."
    },
    {
        "date": "2025-04",
        "title": "T2VShield: Model-Agnostic Jailbreak Defense for Text-to-Video Models",
        "author": "Siyuan Liang, Jiayang Liu, Jiecheng Zhai, Tianmeng Fang, Rongcheng Tu, Aishan Liu, Xiaochun Cao, and Dacheng Tao",
        "link": "http://arxiv.org/abs/2504.15512v2",
        "abstract": "The rapid development of generative artificial intelligence has made text to\nvideo models essential for building future multimodal world simulators.\nHowever, these models remain vulnerable to jailbreak attacks, where specially\ncrafted prompts bypass safety mechanisms and lead to the generation of harmful\nor unsafe content. Such vulnerabilities undermine the reliability and security\nof simulation based applications. In this paper, we propose T2VShield, a\ncomprehensive and model agnostic defense framework designed to protect text to\nvideo models from jailbreak threats. Our method systematically analyzes the\ninput, model, and output stages to identify the limitations of existing\ndefenses, including semantic ambiguities in prompts, difficulties in detecting\nmalicious content in dynamic video outputs, and inflexible model centric\nmitigation strategies. T2VShield introduces a prompt rewriting mechanism based\non reasoning and multimodal retrieval to sanitize malicious inputs, along with\na multi scope detection module that captures local and global inconsistencies\nacross time and modalities. The framework does not require access to internal\nmodel parameters and works with both open and closed source systems. Extensive\nexperiments on five platforms show that T2VShield can reduce jailbreak success\nrates by up to 35 percent compared to strong baselines. We further develop a\nhuman centered audiovisual evaluation protocol to assess perceptual safety,\nemphasizing the importance of visual level defense in enhancing the\ntrustworthiness of next generation multimodal simulators."
    },
    {
        "date": "2025-04",
        "title": "Unifying Image Counterfactuals and Feature Attributions with Latent-Space Adversarial Attacks",
        "author": "Jeremy Goldwasser, and Giles Hooker",
        "link": "http://arxiv.org/abs/2504.15479v1",
        "abstract": "Counterfactuals are a popular framework for interpreting machine learning\npredictions. These what if explanations are notoriously challenging to create\nfor computer vision models: standard gradient-based methods are prone to\nproduce adversarial examples, in which imperceptible modifications to image\npixels provoke large changes in predictions. We introduce a new,\neasy-to-implement framework for counterfactual images that can flexibly adapt\nto contemporary advances in generative modeling. Our method, Counterfactual\nAttacks, resembles an adversarial attack on the representation of the image\nalong a low-dimensional manifold. In addition, given an auxiliary dataset of\nimage descriptors, we show how to accompany counterfactuals with feature\nattribution that quantify the changes between the original and counterfactual\nimages. These importance scores can be aggregated into global counterfactual\nexplanations that highlight the overall features driving model predictions.\nWhile this unification is possible for any counterfactual method, it has\nparticular computational efficiency for ours. We demonstrate the efficacy of\nour approach with the MNIST and CelebA datasets."
    },
    {
        "date": "2025-04",
        "title": "Improving Human-AI Coordination through Adversarial Training and Generative Models",
        "author": "Paresh Chaudhary, Yancheng Liang, Daphne Chen, Simon S. Du, and Natasha Jaques",
        "link": "http://arxiv.org/abs/2504.15457v2",
        "abstract": "Being able to cooperate with new people is an important component of many\neconomically valuable AI tasks, from household robotics to autonomous driving.\nHowever, generalizing to novel humans requires training on data that captures\nthe diversity of human behaviors. Adversarial training is one avenue for\nsearching for such data and ensuring that agents are robust. However, it is\ndifficult to apply in the cooperative setting because adversarial policies\nintentionally learn to sabotage the task instead of simulating valid\ncooperation partners. To address this challenge, we propose a novel strategy\nfor overcoming self-sabotage that combines a pre-trained generative model to\nsimulate valid cooperative agent policies with adversarial training to maximize\nregret. We call our method GOAT: Generative Online Adversarial Training. In\nthis framework, the GOAT dynamically searches for and generates coordination\nstrategies where the learning policy -- the Cooperator agent -- underperforms.\nGOAT enables better generalization by exposing the Cooperator to various\nchallenging interaction scenarios. We maintain realistic coordination\nstrategies by updating only the generative model's embedding while keeping its\nparameters frozen, thus avoiding adversarial exploitation. We evaluate GOAT\nwith real human partners, and the results demonstrate state-of-the-art\nperformance on the Overcooked benchmark, highlighting its effectiveness in\ngeneralizing to diverse human behaviors."
    },
    {
        "date": "2025-04",
        "title": "Valkyrie: A Response Framework to Augment Runtime Detection of Time-Progressive Attacks",
        "author": "Nikhilesh Singh, and Chester Rebeiro",
        "link": "http://arxiv.org/abs/2504.15447v1",
        "abstract": "A popular approach to detect cyberattacks is to monitor systems in real-time\nto identify malicious activities as they occur. While these solutions aim to\ndetect threats early, minimizing damage, they suffer from a significant\nchallenge due to the presence of false positives. False positives have a\ndetrimental impact on computer systems, which can lead to interruptions of\nlegitimate operations and reduced productivity. Most contemporary works tend to\nuse advanced Machine Learning and AI solutions to address this challenge.\nUnfortunately, false positives can, at best, be reduced but not eliminated.\n  In this paper, we propose an alternate approach that focuses on reducing the\nimpact of false positives rather than eliminating them. We introduce Valkyrie,\na framework that can enhance any existing runtime detector with a\npost-detection response. Valkyrie is designed for time-progressive attacks,\nsuch as micro-architectural attacks, rowhammer, ransomware, and cryptominers,\nthat achieve their objectives incrementally using system resources. As soon as\nan attack is detected, Valkyrie limits the allocated computing resources,\nthrottling the attack, until the detector's confidence is sufficiently high to\nwarrant a more decisive action. For a false positive, limiting the system\nresources only results in a small increase in execution time. On average, the\nslowdown incurred due to false positives is less than 1% for single-threaded\nprograms and 6.7% for multi-threaded programs. On the other hand, attacks like\nrowhammer are prevented, while the potency of micro-architectural attacks,\nransomware, and cryptominers is greatly reduced."
    },
    {
        "date": "2025-04",
        "title": "FLARE: Feature-based Lightweight Aggregation for Robust Evaluation of IoT Intrusion Detection",
        "author": "Bradley Boswell, Seth Barrett, Swarnamugi Rajaganapathy, and Gokila Dorai",
        "link": "http://arxiv.org/abs/2504.15375v1",
        "abstract": "The proliferation of Internet of Things (IoT) devices has expanded the attack\nsurface, necessitating efficient intrusion detection systems (IDSs) for network\nprotection. This paper presents FLARE, a feature-based lightweight aggregation\nfor robust evaluation of IoT intrusion detection to address the challenges of\nsecuring IoT environments through feature aggregation techniques. FLARE\nutilizes a multilayered processing approach, incorporating session, flow, and\ntime-based sliding-window data aggregation to analyze network behavior and\ncapture vital features from IoT network traffic data. We perform extensive\nevaluations on IoT data generated from our laboratory experimental setup to\nassess the effectiveness of the proposed aggregation technique. To classify\nattacks in IoT IDS, we employ four supervised learning models and two deep\nlearning models. We validate the performance of these models in terms of\naccuracy, precision, recall, and F1-score. Our results reveal that\nincorporating the FLARE aggregation technique as a foundational step in feature\nengineering, helps lay a structured representation, and enhances the\nperformance of complex end-to-end models, making it a crucial step in IoT IDS\npipeline. Our findings highlight the potential of FLARE as a valuable technique\nto improve performance and reduce computational costs of end-to-end IDS\nimplementations, thereby fostering more robust IoT intrusion detection systems."
    },
    {
        "date": "2025-04",
        "title": "Existing Industry Practice for the EU AI Act's General-Purpose AI Code of Practice Safety and Security Measures",
        "author": "Lily Stelling, Mick Yang, Rokas Gipi\u0161kis, Leon Staufer, Ze Shen Chin, Sim\u00e9on Campos, and Michael Chen",
        "link": "http://arxiv.org/abs/2504.15181v1",
        "abstract": "This report provides a detailed comparison between the measures proposed in\nthe EU AI Act's General-Purpose AI (GPAI) Code of Practice (Third Draft) and\ncurrent practices adopted by leading AI companies. As the EU moves toward\nenforcing binding obligations for GPAI model providers, the Code of Practice\nwill be key to bridging legal requirements with concrete technical commitments.\nOur analysis focuses on the draft's Safety and Security section which is only\nrelevant for the providers of the most advanced models (Commitments II.1-II.16)\nand excerpts from current public-facing documents quotes that are relevant to\neach individual measure.\n  We systematically reviewed different document types - including companies'\nfrontier safety frameworks and model cards - from over a dozen companies,\nincluding OpenAI, Anthropic, Google DeepMind, Microsoft, Meta, Amazon, and\nothers. This report is not meant to be an indication of legal compliance nor\ndoes it take any prescriptive viewpoint about the Code of Practice or\ncompanies' policies. Instead, it aims to inform the ongoing dialogue between\nregulators and GPAI model providers by surfacing evidence of precedent."
    },
    {
        "date": "2025-04",
        "title": "GIFDL: Generated Image Fluctuation Distortion Learning for Enhancing Steganographic Security",
        "author": "Xiangkun Wang, Kejiang Chen, Yuang Qi, Ruiheng Liu, Weiming Zhang, and Nenghai Yu",
        "link": "http://arxiv.org/abs/2504.15139v1",
        "abstract": "Minimum distortion steganography is currently the mainstream method for\nmodification-based steganography. A key issue in this method is how to define\nsteganographic distortion. With the rapid development of deep learning\ntechnology, the definition of distortion has evolved from manual design to deep\nlearning design. Concurrently, rapid advancements in image generation have made\ngenerated images viable as cover media. However, existing distortion design\nmethods based on machine learning do not fully leverage the advantages of\ngenerated cover media, resulting in suboptimal security performance. To address\nthis issue, we propose GIFDL (Generated Image Fluctuation Distortion Learning),\na steganographic distortion learning method based on the fluctuations in\ngenerated images. Inspired by the idea of natural steganography, we take a\nseries of highly similar fluctuation images as the input to the steganographic\ndistortion generator and introduce a new GAN training strategy to disguise\nstego images as fluctuation images. Experimental results demonstrate that\nGIFDL, compared with state-of-the-art GAN-based distortion learning methods,\nexhibits superior resistance to steganalysis, increasing the detection error\nrates by an average of 3.30% across three steganalyzers."
    },
    {
        "date": "2025-04",
        "title": "Robust and Real-time Surface Normal Estimation from Stereo Disparities using Affine Transformations",
        "author": "Csongor Csanad Kariko, Muhammad Rafi Faisal, and Levente Hajder",
        "link": "http://arxiv.org/abs/2504.15121v1",
        "abstract": "This work introduces a novel method for surface normal estimation from\nrectified stereo image pairs, leveraging affine transformations derived from\ndisparity values to achieve fast and accurate results. We demonstrate how the\nrectification of stereo image pairs simplifies the process of surface normal\nestimation by reducing computational complexity. To address noise reduction, we\ndevelop a custom algorithm inspired by convolutional operations, tailored to\nprocess disparity data efficiently. We also introduce adaptive heuristic\ntechniques for efficiently detecting connected surface components within the\nimages, further improving the robustness of the method. By integrating these\nmethods, we construct a surface normal estimator that is both fast and\naccurate, producing a dense, oriented point cloud as the final output. Our\nmethod is validated using both simulated environments and real-world stereo\nimages from the Middlebury and Cityscapes datasets, demonstrating significant\nimprovements in real-time performance and accuracy when implemented on a GPU.\nUpon acceptance, the shader source code will be made publicly available to\nfacilitate further research and reproducibility."
    },
    {
        "date": "2025-04",
        "title": "Fast-Slow Co-advancing Optimizer: Toward Harmonious Adversarial Training of GAN",
        "author": "Lin Wang, Xiancheng Wang, Rui Wang, Zhibo Zhang, and Minghang Zhao",
        "link": "http://arxiv.org/abs/2504.15099v1",
        "abstract": "Up to now, the training processes of typical Generative Adversarial Networks\n(GANs) are still particularly sensitive to data properties and hyperparameters,\nwhich may lead to severe oscillations, difficulties in convergence, or even\nfailures to converge, especially when the overall variances of the training\nsets are large. These phenomena are often attributed to the training\ncharacteristics of such networks. Aiming at the problem, this paper develops a\nnew intelligent optimizer, Fast-Slow Co-advancing Optimizer (FSCO), which\nemploys reinforcement learning in the training process of GANs to make training\neasier. Specifically, this paper allows the training step size to be controlled\nby an agent to improve training stability, and makes the training process more\nintelligent with variable learning rates, making GANs less sensitive to step\nsize. Experiments have been conducted on three benchmark datasets to verify the\neffectiveness of the developed FSCO."
    },
    {
        "date": "2025-04",
        "title": "SOLIDO: A Robust Watermarking Method for Speech Synthesis via Low-Rank Adaptation",
        "author": "Yue Li, Weizhi Liu, and Dongdong Lin",
        "link": "http://arxiv.org/abs/2504.15035v1",
        "abstract": "The accelerated advancement of speech generative models has given rise to\nsecurity issues, including model infringement and unauthorized abuse of\ncontent. Although existing generative watermarking techniques have proposed\ncorresponding solutions, most methods require substantial computational\noverhead and training costs. In addition, some methods have limitations in\nrobustness when handling variable-length inputs. To tackle these challenges, we\npropose \\textsc{SOLIDO}, a novel generative watermarking method that integrates\nparameter-efficient fine-tuning with speech watermarking through low-rank\nadaptation (LoRA) for speech diffusion models. Concretely, the watermark\nencoder converts the watermark to align with the input of diffusion models. To\nachieve precise watermark extraction from variable-length inputs, the watermark\ndecoder based on depthwise separable convolution is designed for watermark\nrecovery. To further enhance speech generation performance and watermark\nextraction capability, we propose a speech-driven lightweight fine-tuning\nstrategy, which reduces computational overhead through LoRA. Comprehensive\nexperiments demonstrate that the proposed method ensures high-fidelity\nwatermarked speech even at a large capacity of 2000 bps. Furthermore, against\ncommon individual and compound speech attacks, our SOLIDO achieves a maximum\naverage extraction accuracy of 99.20\\% and 98.43\\%, respectively. It surpasses\nother state-of-the-art methods by nearly 23\\% in resisting time-stretching\nattacks."
    },
    {
        "date": "2025-04",
        "title": "aiXamine: Simplified LLM Safety and Security",
        "author": "Fatih Deniz, Dorde Popovic, Yazan Boshmaf, Euisuh Jeong, Minhaj Ahmad, Sanjay Chawla, and Issa Khalil",
        "link": "http://arxiv.org/abs/2504.14985v2",
        "abstract": "Evaluating Large Language Models (LLMs) for safety and security remains a\ncomplex task, often requiring users to navigate a fragmented landscape of ad\nhoc benchmarks, datasets, metrics, and reporting formats. To address this\nchallenge, we present aiXamine, a comprehensive black-box evaluation platform\nfor LLM safety and security. aiXamine integrates over 40 tests (i.e.,\nbenchmarks) organized into eight key services targeting specific dimensions of\nsafety and security: adversarial robustness, code security, fairness and bias,\nhallucination, model and data privacy, out-of-distribution (OOD) robustness,\nover-refusal, and safety alignment. The platform aggregates the evaluation\nresults into a single detailed report per model, providing a detailed breakdown\nof model performance, test examples, and rich visualizations. We used aiXamine\nto assess over 50 publicly available and proprietary LLMs, conducting over 2K\nexaminations. Our findings reveal notable vulnerabilities in leading models,\nincluding susceptibility to adversarial attacks in OpenAI's GPT-4o, biased\noutputs in xAI's Grok-3, and privacy weaknesses in Google's Gemini 2.0.\nAdditionally, we observe that open-source models can match or exceed\nproprietary models in specific services such as safety alignment, fairness and\nbias, and OOD robustness. Finally, we identify trade-offs between distillation\nstrategies, model size, training methods, and architectural choices."
    },
    {
        "date": "2025-04",
        "title": "A Security Framework for General Blockchain Layer 2 Protocols",
        "author": "Zeta Avarikioti, Matteo Maffei, and Yuheng Wang",
        "link": "http://arxiv.org/abs/2504.14965v1",
        "abstract": "Layer 2 (L2) solutions are the cornerstone of blockchain scalability,\nenabling high-throughput and low-cost interactions by shifting execution\noff-chain while maintaining security through interactions with the underlying\nledger. Despite their common goals, the principal L2 paradigms -- payment\nchannels, rollups, and sidechains -- differ substantially in architecture and\nassumptions, making it difficult to comparatively analyze their security and\ntrade-offs.\n  To address this, we present the first general security framework for L2\nprotocols. Our framework is based on the IITM-based Universal Composability\n(iUC) framework, in which L2 protocols are modeled as stateful machines\ninteracting with higher-level protocol users and the underlying ledger. The\nmethodology defines a generic execution environment that captures ledger\nevents, message passing, and adversarial scheduling, and characterizes security\nthrough trace-based predicates parameterized by adversarial capabilities and\ntiming assumptions. By abstracting away from protocol-specific details while\npreserving critical interface and execution behavior, the framework enables\nmodular, protocol-agnostic reasoning and composable security proofs across a\nwide range of L2 constructions.\n  To demonstrate its applicability, we analyze an example from each of the\nthree dominant L2 scaling paradigms: a payment channel (Brick), a sidechain\n(Liquid Network), and a rollup (Arbitrum). By instantiating each within our\nframework, we derive their security properties and expose trade-offs. These\ninclude the time for dispute resolution, distribution of off-chain storage and\ncomputation, and varying trust assumptions (e.g., reliance on honest parties or\ndata availability). Our framework unifies the analysis of diverse L2 designs\nand pinpoints their strengths and limitations, providing a foundation for\nsecure, systematic L2 development."
    },
    {
        "date": "2025-04",
        "title": "Fast Adversarial Training with Weak-to-Strong Spatial-Temporal Consistency in the Frequency Domain on Videos",
        "author": "Songping Wang, Hanqing Liu, Yueming Lyu, Xiantao Hu, Ziwen He, Wei Wang, Caifeng Shan, and Liang Wang",
        "link": "http://arxiv.org/abs/2504.14921v2",
        "abstract": "Adversarial Training (AT) has been shown to significantly enhance adversarial\nrobustness via a min-max optimization approach. However, its effectiveness in\nvideo recognition tasks is hampered by two main challenges. First, fast\nadversarial training for video models remains largely unexplored, which\nseverely impedes its practical applications. Specifically, most video\nadversarial training methods are computationally costly, with long training\ntimes and high expenses. Second, existing methods struggle with the trade-off\nbetween clean accuracy and adversarial robustness. To address these challenges,\nwe introduce Video Fast Adversarial Training with Weak-to-Strong consistency\n(VFAT-WS), the first fast adversarial training method for video data.\nSpecifically, VFAT-WS incorporates the following key designs: First, it\nintegrates a straightforward yet effective temporal frequency augmentation\n(TF-AUG), and its spatial-temporal enhanced form STF-AUG, along with a\nsingle-step PGD attack to boost training efficiency and robustness. Second, it\ndevises a weak-to-strong spatial-temporal consistency regularization, which\nseamlessly integrates the simpler TF-AUG and the more complex STF-AUG.\nLeveraging the consistency regularization, it steers the learning process from\nsimple to complex augmentations. Both of them work together to achieve a better\ntrade-off between clean accuracy and robustness. Extensive experiments on\nUCF-101 and HMDB-51 with both CNN and Transformer-based models demonstrate that\nVFAT-WS achieves great improvements in adversarial robustness and corruption\nrobustness, while accelerating training by nearly 490%."
    },
    {
        "date": "2025-04",
        "title": "Backdoor Defense in Diffusion Models via Spatial Attention Unlearning",
        "author": "Abha Jha, Ashwath Vaithinathan Aravindan, Matthew Salaway, Atharva Sandeep Bhide, and Duygu Nur Yaldiz",
        "link": "http://arxiv.org/abs/2504.18563v1",
        "abstract": "Text-to-image diffusion models are increasingly vulnerable to backdoor\nattacks, where malicious modifications to the training data cause the model to\ngenerate unintended outputs when specific triggers are present. While\nclassification models have seen extensive development of defense mechanisms,\ngenerative models remain largely unprotected due to their high-dimensional\noutput space, which complicates the detection and mitigation of subtle\nperturbations. Defense strategies for diffusion models, in particular, remain\nunder-explored. In this work, we propose Spatial Attention Unlearning (SAU), a\nnovel technique for mitigating backdoor attacks in diffusion models. SAU\nleverages latent space manipulation and spatial attention mechanisms to isolate\nand remove the latent representation of backdoor triggers, ensuring precise and\nefficient removal of malicious effects. We evaluate SAU across various types of\nbackdoor attacks, including pixel-based and style-based triggers, and\ndemonstrate its effectiveness in achieving 100% trigger removal accuracy.\nFurthermore, SAU achieves a CLIP score of 0.7023, outperforming existing\nmethods while preserving the model's ability to generate high-quality,\nsemantically aligned images. Our results show that SAU is a robust, scalable,\nand practical solution for securing text-to-image diffusion models against\nbackdoor attacks."
    },
    {
        "date": "2025-04",
        "title": "Protecting Your Voice: Temporal-aware Robust Watermarking",
        "author": "Yue Li, Weizhi Liu, and Dongdong Lin",
        "link": "http://arxiv.org/abs/2504.14832v1",
        "abstract": "The rapid advancement of generative models has led to the synthesis of\nreal-fake ambiguous voices. To erase the ambiguity, embedding watermarks into\nthe frequency-domain features of synthesized voices has become a common\nroutine. However, the robustness achieved by choosing the frequency domain\noften comes at the expense of fine-grained voice features, leading to a loss of\nfidelity. Maximizing the comprehensive learning of time-domain features to\nenhance fidelity while maintaining robustness, we pioneer a\n\\textbf{\\underline{t}}emporal-aware\n\\textbf{\\underline{r}}ob\\textbf{\\underline{u}}st\nwat\\textbf{\\underline{e}}rmarking (\\emph{True}) method for protecting the\nspeech and singing voice."
    },
    {
        "date": "2025-04",
        "title": "DONOD: Robust and Generalizable Instruction Fine-Tuning for LLMs via Model-Intrinsic Dataset Pruning",
        "author": "Jucheng Hu, Surong Yang, Dongzhan Zhou, and Lijun Wu",
        "link": "http://arxiv.org/abs/2504.14810v1",
        "abstract": "Ad-hoc instruction fine-tuning of large language models (LLMs) is widely\nadopted for domain-specific adaptation. While domain-specific supervised\nfine-tuning (SFT) is effective and efficient, it often weakens cross-domain\ngeneralization and struggles with noisy training data. To address these\nchallenges, we propose DONOD, a lightweight model-intrinsic data pruning\nmethod. Our approach evaluates data using two model-parameter-based metrics:\nDelta of Norm (DON), which captures the cumulative influence on model weights,\nand Norm of Delta (NOD), which quantifies weight instability. Moreover, by\nemploying the Technique for Order of Preference by Similarity to Ideal Solution\n(TOPSIS) algorithm, we effectively filter noisy, unlearnable, and\ngeneralization-harming samples without relying on auxiliary models during the\nSFT process. Experiments on mathematical tasks demonstrate that data selected\nby DONOD achieve superior fine-tuning efficiency and improved robustness\nagainst noisy data. By filtering out 70% of the full dataset, we improve\ntarget-domain accuracy by 14.90% and cross-domain accuracy by 5.67%. Meanwhile,\nour selected data present superior cross-architecture generalization. Data\npruned by smaller models (e.g., Llama 3.1-8B) generalize effectively on larger\nmodels (e.g., Llama 2-13B). Compared to existing related methodologies, DONOD\ndemonstrates comparable or superior performance while remaining\ndataset-agnostic, enabling broader applicability."
    },
    {
        "date": "2025-04",
        "title": "Verifying Robust Unlearning: Probing Residual Knowledge in Unlearned Models",
        "author": "Hao Xuan, and Xingyu Li",
        "link": "http://arxiv.org/abs/2504.14798v1",
        "abstract": "Machine Unlearning (MUL) is crucial for privacy protection and content\nregulation, yet recent studies reveal that traces of forgotten information\npersist in unlearned models, enabling adversaries to resurface removed\nknowledge. Existing verification methods only confirm whether unlearning was\nexecuted, failing to detect such residual information leaks. To address this,\nwe introduce the concept of Robust Unlearning, ensuring models are\nindistinguishable from retraining and resistant to adversarial recovery. To\nempirically evaluate whether unlearning techniques meet this security standard,\nwe propose the Unlearning Mapping Attack (UMA), a post-unlearning verification\nframework that actively probes models for forgotten traces using adversarial\nqueries. Extensive experiments on discriminative and generative tasks show that\nexisting unlearning techniques remain vulnerable, even when passing existing\nverification metrics. By establishing UMA as a practical verification tool,\nthis study sets a new standard for assessing and enhancing machine unlearning\nsecurity."
    },
    {
        "date": "2025-04",
        "title": "Decoupling Identity from Access: Credential Broker Patterns for Secure CI/CD",
        "author": "Surya Teja Avirneni",
        "link": "http://arxiv.org/abs/2504.14761v1",
        "abstract": "Credential brokers offer a way to separate identity from access in CI/CD\nsystems. This paper shows how verifiable identities issued at runtime, such as\nthose from SPIFFE, can be used with brokers to enable short-lived,\npolicy-driven credentials for pipelines and workloads. We walk through\npractical design patterns, including brokers that issue tokens just in time,\napply access policies, and operate across trust domains. These ideas help\nreduce static permissions, improve auditability, and support Zero Trust goals\nin deployment workflows. This is the second paper in a three-part series on\nsecure CI/CD identity architecture."
    },
    {
        "date": "2025-04",
        "title": "LeetCodeDataset: A Temporal Dataset for Robust Evaluation and Efficient Training of Code LLMs",
        "author": "Yunhui Xia, Wei Shen, Yan Wang, Jason Klein Liu, Huifeng Sun, Siyue Wu, Jian Hu, and Xiaolong Xu",
        "link": "http://arxiv.org/abs/2504.14655v1",
        "abstract": "We introduce LeetCodeDataset, a high-quality benchmark for evaluating and\ntraining code-generation models, addressing two key challenges in LLM research:\nthe lack of reasoning-focused coding benchmarks and self-contained training\ntestbeds. By curating LeetCode Python problems with rich metadata, broad\ncoverage, 100+ test cases per problem, and temporal splits (pre/post July\n2024), our dataset enables contamination-free evaluation and efficient\nsupervised fine-tuning (SFT). Experiments show reasoning models significantly\noutperform non-reasoning counterparts, while SFT with only 2.6K model-generated\nsolutions achieves performance comparable to 110K-sample counterparts. The\ndataset and evaluation framework are available on Hugging Face and Github."
    },
    {
        "date": "2025-04",
        "title": "SMTT: Novel Structured Multi-task Tracking with Graph-Regularized Sparse Representation for Robust Thermal Infrared Target Tracking",
        "author": "Shang Zhang, HuiPan Guan, XiaoBo Ding, Ruoyan Xiong, and Yue Zhang",
        "link": "http://arxiv.org/abs/2504.14566v1",
        "abstract": "Thermal infrared target tracking is crucial in applications such as\nsurveillance, autonomous driving, and military operations. In this paper, we\npropose a novel tracker, SMTT, which effectively addresses common challenges in\nthermal infrared imagery, such as noise, occlusion, and rapid target motion, by\nleveraging multi-task learning, joint sparse representation, and adaptive graph\nregularization. By reformulating the tracking task as a multi-task learning\nproblem, the SMTT tracker independently optimizes the representation of each\nparticle while dynamically capturing spatial and feature-level similarities\nusing a weighted mixed-norm regularization strategy. To ensure real-time\nperformance, we incorporate the Accelerated Proximal Gradient method for\nefficient optimization. Extensive experiments on benchmark datasets - including\nVOT-TIR, PTB-TIR, and LSOTB-TIR - demonstrate that SMTT achieves superior\naccuracy, robustness, and computational efficiency. These results highlight\nSMTT as a reliable and high-performance solution for thermal infrared target\ntracking in complex environments."
    },
    {
        "date": "2025-04",
        "title": "Towards Model Resistant to Transferable Adversarial Examples via Trigger Activation",
        "author": "Yi Yu, Song Xia, Xun Lin, Chenqi Kong, Wenhan Yang, Shijian Lu, Yap-Peng Tan, and Alex C. Kot",
        "link": "http://arxiv.org/abs/2504.14541v1",
        "abstract": "Adversarial examples, characterized by imperceptible perturbations, pose\nsignificant threats to deep neural networks by misleading their predictions. A\ncritical aspect of these examples is their transferability, allowing them to\ndeceive {unseen} models in black-box scenarios. Despite the widespread\nexploration of defense methods, including those on transferability, they show\nlimitations: inefficient deployment, ineffective defense, and degraded\nperformance on clean images. In this work, we introduce a novel training\nparadigm aimed at enhancing robustness against transferable adversarial\nexamples (TAEs) in a more efficient and effective way. We propose a model that\nexhibits random guessing behavior when presented with clean data\n$\\boldsymbol{x}$ as input, and generates accurate predictions when with\ntriggered data $\\boldsymbol{x}+\\boldsymbol{\\tau}$. Importantly, the trigger\n$\\boldsymbol{\\tau}$ remains constant for all data instances. We refer to these\nmodels as \\textbf{models with trigger activation}. We are surprised to find\nthat these models exhibit certain robustness against TAEs. Through the\nconsideration of first-order gradients, we provide a theoretical analysis of\nthis robustness. Moreover, through the joint optimization of the learnable\ntrigger and the model, we achieve improved robustness to transferable attacks.\nExtensive experiments conducted across diverse datasets, evaluating a variety\nof attacking methods, underscore the effectiveness and superiority of our\napproach."
    },
    {
        "date": "2025-04",
        "title": "Breaking the Prompt Wall (I): A Real-World Case Study of Attacking ChatGPT via Lightweight Prompt Injection",
        "author": "Xiangyu Chang, Guang Dai, Hao Di, and Haishan Ye",
        "link": "http://arxiv.org/abs/2504.16125v1",
        "abstract": "This report presents a real-world case study demonstrating how prompt\ninjection can attack large language model platforms such as ChatGPT according\nto a proposed injection framework. By providing three real-world examples, we\nshow how adversarial prompts can be injected via user inputs, web-based\nretrieval, and system-level agent instructions. These attacks, though\nlightweight and low-cost, can cause persistent and misleading behaviors in LLM\noutputs. Our case study reveals that even commercial-grade LLMs remain\nvulnerable to subtle manipulations that bypass safety filters and influence\nuser decisions. \\textbf{More importantly, we stress that this report is not\nintended as an attack guide, but as a technical alert. As ethical researchers,\nwe aim to raise awareness and call upon developers, especially those at OpenAI,\nto treat prompt-level security as a critical design priority."
    },
    {
        "date": "2025-04",
        "title": "Vision-Centric Representation-Efficient Fine-Tuning for Robust Universal Foreground Segmentation",
        "author": "Guoyi Zhang, Siyang Chen, Guangsheng Xu, Han Wang, and Xiaohu Zhang",
        "link": "http://arxiv.org/abs/2504.14481v1",
        "abstract": "Foreground segmentation is crucial for scene understanding, yet\nparameter-efficient fine-tuning (PEFT) of vision foundation models (VFMs) often\nfails in complex scenarios, such as camouflage and infrared imagery. We\nattribute this challenge to the inherent texture bias in VFMs, which is\nexacerbated during fine-tuning and limits generalization in texture-sparse\nenvironments. To address this, we propose Ladder Shape-bias Representation\nSide-tuning (LSR-ST), a lightweight PEFT framework that enhances model\nrobustness by introducing shape-biased inductive priors. LSR-ST captures\nshape-aware features using a simple HDConv Block, which integrates large-kernel\nattention and residual learning. The method satisfies three key conditions for\ninducing shape bias: large receptive fields, multi-order feature interactions,\nand sparse connectivity. Our analysis reveals that these improvements stem from\nrepresentation efficiency-the ability to extract task-relevant, structurally\ngrounded features while minimizing redundancy. We formalize this concept via\nInformation Bottleneck theory and advocate for it as a key PEFT objective.\nUnlike traditional NLP paradigms that focus on optimizing parameters and\nmemory, visual tasks require models that extract task-defined semantics, rather\nthan just relying on pre-encoded features. This shift enables our approach to\nmove beyond conventional trade-offs, offering more robust and generalizable\nsolutions for vision tasks. With minimal changes to SAM2-UNet, LSR-ST achieves\nconsistent improvements across 17 datasets and 6 tasks using only 4.719M\ntrainable parameters. These results highlight the potential of representation\nefficiency for robust and adaptable VFMs within complex visual environments."
    },
    {
        "date": "2025-04",
        "title": "Causal Disentanglement for Robust Long-tail Medical Image Generation",
        "author": "Weizhi Nie, Zichun Zhang, Weijie Wang, Bruno Lepri, Anan Liu, and Nicu Sebe",
        "link": "http://arxiv.org/abs/2504.14450v2",
        "abstract": "Counterfactual medical image generation effectively addresses data scarcity\nand enhances the interpretability of medical images. However, due to the\ncomplex and diverse pathological features of medical images and the imbalanced\nclass distribution in medical data, generating high-quality and diverse medical\nimages from limited data is significantly challenging. Additionally, to fully\nleverage the information in limited data, such as anatomical structure\ninformation and generate more structurally stable medical images while avoiding\ndistortion or inconsistency. In this paper, in order to enhance the clinical\nrelevance of generated data and improve the interpretability of the model, we\npropose a novel medical image generation framework, which generates independent\npathological and structural features based on causal disentanglement and\nutilizes text-guided modeling of pathological features to regulate the\ngeneration of counterfactual images. First, we achieve feature separation\nthrough causal disentanglement and analyze the interactions between features.\nHere, we introduce group supervision to ensure the independence of pathological\nand identity features. Second, we leverage a diffusion model guided by\npathological findings to model pathological features, enabling the generation\nof diverse counterfactual images. Meanwhile, we enhance accuracy by leveraging\na large language model to extract lesion severity and location from medical\nreports. Additionally, we improve the performance of the latent diffusion model\non long-tailed categories through initial noise optimization."
    },
    {
        "date": "2025-04",
        "title": "Adversarial Attack for RGB-Event based Visual Object Tracking",
        "author": "Qiang Chen, Xiao Wang, Haowen Wang, Bo Jiang, Lin Zhu, Dawei Zhang, Yonghong Tian, and Jin Tang",
        "link": "http://arxiv.org/abs/2504.14423v1",
        "abstract": "Visual object tracking is a crucial research topic in the fields of computer\nvision and multi-modal fusion. Among various approaches, robust visual tracking\nthat combines RGB frames with Event streams has attracted increasing attention\nfrom researchers. While striving for high accuracy and efficiency in tracking,\nit is also important to explore how to effectively conduct adversarial attacks\nand defenses on RGB-Event stream tracking algorithms, yet research in this area\nremains relatively scarce. To bridge this gap, in this paper, we propose a\ncross-modal adversarial attack algorithm for RGB-Event visual tracking. Because\nof the diverse representations of Event streams, and given that Event voxels\nand frames are more commonly used, this paper will focus on these two\nrepresentations for an in-depth study. Specifically, for the RGB-Event voxel,\nwe first optimize the perturbation by adversarial loss to generate RGB frame\nadversarial examples. For discrete Event voxel representations, we propose a\ntwo-step attack strategy, more in detail, we first inject Event voxels into the\ntarget region as initialized adversarial examples, then, conduct a\ngradient-guided optimization by perturbing the spatial location of the Event\nvoxels. For the RGB-Event frame based tracking, we optimize the cross-modal\nuniversal perturbation by integrating the gradient information from multimodal\ndata. We evaluate the proposed approach against attacks on three widely used\nRGB-Event Tracking datasets, i.e., COESOT, FE108, and VisEvent. Extensive\nexperiments show that our method significantly reduces the performance of the\ntracker across numerous datasets in both unimodal and multimodal scenarios. The\nsource code will be released on\nhttps://github.com/Event-AHU/Adversarial_Attack_Defense"
    },
    {
        "date": "2025-04",
        "title": "How Do Mobile Applications Enhance Security? An Exploratory Analysis of Use Cases and Provided Information",
        "author": "Irdin Pekaric, Clemens Sauerwein, Simon Laichner, and Ruth Breu",
        "link": "http://arxiv.org/abs/2504.14421v1",
        "abstract": "The ubiquity of mobile applications has increased dramatically in recent\nyears, opening up new opportunities for cyber attackers and heightening\nsecurity concerns in the mobile ecosystem. As a result, researchers and\npractitioners have intensified their research into improving the security and\nprivacy of mobile applications. At the same time, more and more mobile\napplications have appeared on the market that address the aforementioned\nsecurity issues. However, both academia and industry currently lack a\ncomprehensive overview of these mobile security applications for Android and\niOS platforms, including their respective use cases and the security\ninformation they provide.\n  To address this gap, we systematically collected a total of 410 mobile\napplications from both the App and Play Store. Then, we identified the 20 most\nwidely utilized mobile security applications on both platforms that were\nanalyzed and classified. Our results show six primary use cases and a wide\nrange of security information provided by these applications, thus supporting\nthe core functionalities for ensuring mobile security."
    },
    {
        "date": "2025-04",
        "title": "Quantum-Enhanced Reinforcement Learning for Power Grid Security Assessment",
        "author": "Benjamin M. Peter, and Mert Korkali",
        "link": "http://arxiv.org/abs/2504.14412v1",
        "abstract": "The increasingly challenging task of maintaining power grid security requires\ninnovative solutions. Novel approaches using reinforcement learning (RL) agents\nhave been proposed to help grid operators navigate the massive decision space\nand nonlinear behavior of these complex networks. However, applying RL to power\ngrid security assessment, specifically for combinatorially troublesome\ncontingency analysis problems, has proven difficult to scale. The integration\nof quantum computing into these RL frameworks helps scale by improving\ncomputational efficiency and boosting agent proficiency by leveraging quantum\nadvantages in action exploration and model-based interdependence. To\ndemonstrate a proof-of-concept use of quantum computing for RL agent training\nand simulation, we propose a hybrid agent that runs on quantum hardware using\nIBM's Qiskit Runtime. We also provide detailed insight into the construction of\nparameterized quantum circuits (PQCs) for generating relevant quantum output.\nThis agent's proficiency at maintaining grid stability is demonstrated relative\nto a benchmark model without quantum enhancement using N-k contingency\nanalysis. Additionally, we offer a comparative assessment of the training\nprocedures for RL models integrated with a quantum backend."
    },
    {
        "date": "2025-04",
        "title": "Hydra: An Agentic Reasoning Approach for Enhancing Adversarial Robustness and Mitigating Hallucinations in Vision-Language Models",
        "author": "Chung-En, Yu, Hsuan-Chih, Chen, Brian Jalaian, and Nathaniel D. Bastian",
        "link": "http://arxiv.org/abs/2504.14395v1",
        "abstract": "To develop trustworthy Vision-Language Models (VLMs), it is essential to\naddress adversarial robustness and hallucination mitigation, both of which\nimpact factual accuracy in high-stakes applications such as defense and\nhealthcare. Existing methods primarily focus on either adversarial defense or\nhallucination post-hoc correction, leaving a gap in unified robustness\nstrategies. We introduce \\textbf{Hydra}, an adaptive agentic framework that\nenhances plug-in VLMs through iterative reasoning, structured critiques, and\ncross-model verification, improving both resilience to adversarial\nperturbations and intrinsic model errors. Hydra employs an Action-Critique\nLoop, where it retrieves and critiques visual information, leveraging\nChain-of-Thought (CoT) and In-Context Learning (ICL) techniques to refine\noutputs dynamically. Unlike static post-hoc correction methods, Hydra adapts to\nboth adversarial manipulations and intrinsic model errors, making it robust to\nmalicious perturbations and hallucination-related inaccuracies. We evaluate\nHydra on four VLMs, three hallucination benchmarks, two adversarial attack\nstrategies, and two adversarial defense methods, assessing performance on both\nclean and adversarial inputs. Results show that Hydra surpasses plug-in VLMs\nand state-of-the-art (SOTA) dehallucination methods, even without explicit\nadversarial defenses, demonstrating enhanced robustness and factual\nconsistency. By bridging adversarial resistance and hallucination mitigation,\nHydra provides a scalable, training-free solution for improving the reliability\nof VLMs in real-world applications."
    },
    {
        "date": "2025-04",
        "title": "From Cyber Security Incident Management to Cyber Security Crisis Management in the European Union",
        "author": "Jukka Ruohonen, Kalle Rindell, and Simone Busetti",
        "link": "http://arxiv.org/abs/2504.14220v1",
        "abstract": "Incident management is a classical topic in cyber security. Recently, the\nEuropean Union (EU) has started to consider also the relation between cyber\nsecurity incidents and cyber security crises. These considerations and\npreparations, including those specified in the EU's new cyber security laws,\nconstitute the paper's topic. According to an analysis of the laws and\nassociated policy documents, (i) cyber security crises are equated in the EU to\nlarge-scale cyber security incidents that either exceed a handling capacity of\na single member state or affect at least two member states. For this and other\npurposes, (ii) the new laws substantially increase mandatory reporting about\ncyber security incidents, including but not limited to the large-scale\nincidents. Despite the laws and new governance bodies established by them,\nhowever, (iii) the working of actual cyber security crisis management remains\nunclear particularly at the EU-level. With these policy research results, the\npaper advances the domain of cyber security incident management research by\nelaborating how European law perceives cyber security crises and their relation\nto cyber security incidents, paving the way for many relevant further research\ntopics with practical relevance, whether theoretical, conceptual, or empirical."
    },
    {
        "date": "2025-04",
        "title": "The First VoicePrivacy Attacker Challenge",
        "author": "Natalia Tomashenko, Xiaoxiao Miao, Emmanuel Vincent, and Junichi Yamagishi",
        "link": "http://arxiv.org/abs/2504.14183v1",
        "abstract": "The First VoicePrivacy Attacker Challenge is an ICASSP 2025 SP Grand\nChallenge which focuses on evaluating attacker systems against a set of voice\nanonymization systems submitted to the VoicePrivacy 2024 Challenge. Training,\ndevelopment, and evaluation datasets were provided along with a baseline\nattacker. Participants developed their attacker systems in the form of\nautomatic speaker verification systems and submitted their scores on the\ndevelopment and evaluation data. The best attacker systems reduced the equal\nerror rate (EER) by 25-44% relative w.r.t. the baseline."
    },
    {
        "date": "2025-04",
        "title": "A Data-Centric Approach for Safe and Secure Large Language Models against Threatening and Toxic Content",
        "author": "Chaima Njeh, Ha\u00effa Nakouri, and Fehmi Jaafar",
        "link": "http://arxiv.org/abs/2504.16120v1",
        "abstract": "Large Language Models (LLM) have made remarkable progress, but concerns about\npotential biases and harmful content persist. To address these apprehensions,\nwe introduce a practical solution for ensuring LLM's safe and ethical use. Our\nnovel approach focuses on a post-generation correction mechanism, the\nBART-Corrective Model, which adjusts generated content to ensure safety and\nsecurity. Unlike relying solely on model fine-tuning or prompt engineering, our\nmethod provides a robust data-centric alternative for mitigating harmful\ncontent. We demonstrate the effectiveness of our approach through experiments\non multiple toxic datasets, which show a significant reduction in mean toxicity\nand jail-breaking scores after integration. Specifically, our results show a\nreduction of 15% and 21% in mean toxicity and jail-breaking scores with GPT-4,\na substantial reduction of 28% and 5% with PaLM2, a reduction of approximately\n26% and 23% with Mistral-7B, and a reduction of 11.1% and 19% with Gemma-2b-it.\nThese results demonstrate the potential of our approach to improve the safety\nand security of LLM, making them more suitable for real-world applications."
    },
    {
        "date": "2025-04",
        "title": "Rethinking Target Label Conditioning in Adversarial Attacks: A 2D Tensor-Guided Generative Approach",
        "author": "Hangyu Liu, Bo Peng, Pengxiang Ding, and Donglin Wang",
        "link": "http://arxiv.org/abs/2504.14137v1",
        "abstract": "Compared to single-target adversarial attacks, multi-target attacks have\ngarnered significant attention due to their ability to generate adversarial\nimages for multiple target classes simultaneously. Existing generative\napproaches for multi-target attacks mainly analyze the effect of the use of\ntarget labels on noise generation from a theoretical perspective, lacking\npractical validation and comprehensive summarization. To address this gap, we\nfirst identify and validate that the semantic feature quality and quantity are\ncritical factors affecting the transferability of targeted attacks: 1) Feature\nquality refers to the structural and detailed completeness of the implanted\ntarget features, as deficiencies may result in the loss of key discriminative\ninformation; 2) Feature quantity refers to the spatial sufficiency of the\nimplanted target features, as inadequacy limits the victim model's attention to\nthis feature. Based on these findings, we propose the 2D Tensor-Guided\nAdversarial Fusion (2D-TGAF) framework, which leverages the powerful generative\ncapabilities of diffusion models to encode target labels into two-dimensional\nsemantic tensors for guiding adversarial noise generation. Additionally, we\ndesign a novel masking strategy tailored for the training process, ensuring\nthat parts of the generated noise retain complete semantic information about\nthe target class. Extensive experiments on the standard ImageNet dataset\ndemonstrate that 2D-TGAF consistently surpasses state-of-the-art methods in\nattack success rates, both on normally trained models and across various\ndefense mechanisms."
    },
    {
        "date": "2025-04",
        "title": "Detecting Zero-Day Web Attacks with an Ensemble of LSTM, GRU, and Stacked Autoencoders",
        "author": "Vahid Babaey, and Hamid Reza Faragardi",
        "link": "http://arxiv.org/abs/2504.14122v1",
        "abstract": "The rapid growth in web-based services has significantly increased security\nrisks related to user information, as web-based attacks become increasingly\nsophisticated and prevalent. Traditional security methods frequently struggle\nto detect previously unknown (zero-day) web attacks, putting sensitive user\ndata at significant risk. Additionally, reducing human intervention in web\nsecurity tasks can minimize errors and enhance reliability. This paper\nintroduces an intelligent system designed to detect zero-day web attacks using\na novel one-class ensemble method consisting of three distinct autoencoder\narchitectures: LSTM autoencoder, GRU autoencoder, and stacked autoencoder. Our\napproach employs a novel tokenization strategy to convert normal web requests\ninto structured numeric sequences, enabling the ensemble model to effectively\nidentify anomalous activities by uniquely concatenating and compressing the\nlatent representations from each autoencoder. The proposed method efficiently\ndetects unknown web attacks while effectively addressing common limitations of\nprevious methods, such as high memory consumption and excessive false positive\nrates. Extensive experimental evaluations demonstrate the superiority of our\nproposed ensemble, achieving remarkable detection metrics: 97.58% accuracy,\n97.52% recall, 99.76% specificity, and 99.99% precision, with an exceptionally\nlow false positive rate of 0.2%. These results underscore our method's\nsignificant potential in enhancing real-world web security through accurate and\nreliable detection of web-based attacks."
    },
    {
        "date": "2025-04",
        "title": "DoomArena: A framework for Testing AI Agents Against Evolving Security Threats",
        "author": "Leo Boisvert, Mihir Bansal, Chandra Kiran Reddy Evuru, Gabriel Huang, Abhay Puri, Avinandan Bose, Maryam Fazel, Quentin Cappart, Jason Stanley, Alexandre Lacoste, Alexandre Drouin, and Krishnamurthy Dvijotham",
        "link": "http://arxiv.org/abs/2504.14064v2",
        "abstract": "We present DoomArena, a security evaluation framework for AI agents.\nDoomArena is designed on three principles: 1) It is a plug-in framework and\nintegrates easily into realistic agentic frameworks like BrowserGym (for web\nagents) and $\\tau$-bench (for tool calling agents); 2) It is configurable and\nallows for detailed threat modeling, allowing configuration of specific\ncomponents of the agentic framework being attackable, and specifying targets\nfor the attacker; and 3) It is modular and decouples the development of attacks\nfrom details of the environment in which the agent is deployed, allowing for\nthe same attacks to be applied across multiple environments. We illustrate\nseveral advantages of our framework, including the ability to adapt to new\nthreat models and environments easily, the ability to easily combine several\npreviously published attacks to enable comprehensive and fine-grained security\ntesting, and the ability to analyze trade-offs between various vulnerabilities\nand performance. We apply DoomArena to state-of-the-art (SOTA) web and\ntool-calling agents and find a number of surprising results: 1) SOTA agents\nhave varying levels of vulnerability to different threat models (malicious user\nvs malicious environment), and there is no Pareto dominant agent across all\nthreat models; 2) When multiple attacks are applied to an agent, they often\ncombine constructively; 3) Guardrail model-based defenses seem to fail, while\ndefenses based on powerful SOTA LLMs work better. DoomArena is available at\nhttps://github.com/ServiceNow/DoomArena."
    },
    {
        "date": "2025-04",
        "title": "Prioritizing Security Practice Adoption: Empirical Insights on Software Security Outcomes in the npm Ecosystem",
        "author": "Nusrat Zahan, and Laurie Williams",
        "link": "http://arxiv.org/abs/2504.14026v1",
        "abstract": "Practitioners often struggle with the overwhelming number of security\npractices outlined in cybersecurity frameworks for risk mitigation. Given the\nlimited budget, time, and resources, practitioners want to prioritize the\nadoption of security practices based on empirical evidence. The goal of this\nstudy is to assist practitioners and policymakers in making informed decisions\non which security practices to adopt by evaluating the relationship between\nsoftware security practices and security outcome metrics. The study\ninvestigated the relationship between security practice adoption and security\noutcomes. We selected the OpenSSF Scorecard metrics to automatically measure\nthe adoption of security practices in npm GitHub repositories. We also explored\nsecurity outcome metrics, such as the number of open vulnerabilities\n(Vul_Count), mean time to remediate (MTTR) vulnerabilities in dependencies, and\nmean time to update (MTTU) dependencies. We conducted regression and causal\nanalysis using 12 Scorecard metrics and their aggregated Scorecard score\n(computed by aggregating individual security practice scores) as predictors and\nVul_Count, MTTR, and MTTU as target variables. Our findings show that higher\naggregated Scorecard scores are associated with fewer Vul_Count and shorter\nMTTU, also supported by causal analysis. However, while the regression model\nsuggests shorter MTTR, causal analysis indicates project characteristics likely\ninfluence MTTR direction. Segment analysis shows that larger, newer\nrepositories with more contributors, dependencies, and downloads have shorter\nMTTR. Among individual security practices, Code Review, Maintained status,\nPinned Dependencies, and Branch Protection show strong associations with\nsecurity outcomes; the directionality of these associations varies across\nsecurity outcomes."
    },
    {
        "date": "2025-04",
        "title": "Outlier-Robust Multi-Model Fitting on Quantum Annealers",
        "author": "Saurabh Pandey, Luca Magri, Federica Arrigoni, and Vladislav Golyanik",
        "link": "http://arxiv.org/abs/2504.13836v1",
        "abstract": "Multi-model fitting (MMF) presents a significant challenge in Computer\nVision, particularly due to its combinatorial nature. While recent advancements\nin quantum computing offer promise for addressing NP-hard problems, existing\nquantum-based approaches for model fitting are either limited to a single model\nor consider multi-model scenarios within outlier-free datasets. This paper\nintroduces a novel approach, the robust quantum multi-model fitting (R-QuMF)\nalgorithm, designed to handle outliers effectively. Our method leverages the\nintrinsic capabilities of quantum hardware to tackle combinatorial challenges\ninherent in MMF tasks, and it does not require prior knowledge of the exact\nnumber of models, thereby enhancing its practical applicability. By formulating\nthe problem as a maximum set coverage task for adiabatic quantum computers\n(AQC), R-QuMF outperforms existing quantum techniques, demonstrating superior\nperformance across various synthetic and real-world 3D datasets. Our findings\nunderscore the potential of quantum computing in addressing the complexities of\nMMF, especially in real-world scenarios with noisy and outlier-prone data."
    },
    {
        "date": "2025-04",
        "title": "Collective Learning Mechanism based Optimal Transport Generative Adversarial Network for Non-parallel Voice Conversion",
        "author": "Sandipan Dhar, Md. Tousin Akhter, Nanda Dulal Jana, and Swagatam Das",
        "link": "http://arxiv.org/abs/2504.13791v1",
        "abstract": "After demonstrating significant success in image synthesis, Generative\nAdversarial Network (GAN) models have likewise made significant progress in the\nfield of speech synthesis, leveraging their capacity to adapt the precise\ndistribution of target data through adversarial learning processes. Notably, in\nthe realm of State-Of-The-Art (SOTA) GAN-based Voice Conversion (VC) models,\nthere exists a substantial disparity in naturalness between real and\nGAN-generated speech samples. Furthermore, while many GAN models currently\noperate on a single generator discriminator learning approach, optimizing\ntarget data distribution is more effectively achievable through a single\ngenerator multi-discriminator learning scheme. Hence, this study introduces a\nnovel GAN model named Collective Learning Mechanism-based Optimal Transport GAN\n(CLOT-GAN) model, incorporating multiple discriminators, including the Deep\nConvolutional Neural Network (DCNN) model, Vision Transformer (ViT), and\nconformer. The objective of integrating various discriminators lies in their\nability to comprehend the formant distribution of mel-spectrograms, facilitated\nby a collective learning mechanism. Simultaneously, the inclusion of Optimal\nTransport (OT) loss aims to precisely bridge the gap between the source and\ntarget data distribution, employing the principles of OT theory. The\nexperimental validation on VCC 2018, VCTK, and CMU-Arctic datasets confirms\nthat the CLOT-GAN-VC model outperforms existing VC models in objective and\nsubjective assessments."
    },
    {
        "date": "2025-04",
        "title": "On the Relationship Between Robustness and Expressivity of Graph Neural Networks",
        "author": "Lorenz Kummer, Wilfried N. Gansterer, and Nils M. Kriege",
        "link": "http://arxiv.org/abs/2504.13786v1",
        "abstract": "We investigate the vulnerability of Graph Neural Networks (GNNs) to bit-flip\nattacks (BFAs) by introducing an analytical framework to study the influence of\narchitectural features, graph properties, and their interaction.\n  The expressivity of GNNs refers to their ability to distinguish\nnon-isomorphic graphs and depends on the encoding of node neighborhoods. We\nexamine the vulnerability of neural multiset functions commonly used for this\npurpose and establish formal criteria to characterize a GNN's susceptibility to\nlosing expressivity due to BFAs. This enables an analysis of the impact of\nhomophily, graph structural variety, feature encoding, and activation functions\non GNN robustness. We derive theoretical bounds for the number of bit flips\nrequired to degrade GNN expressivity on a dataset, identifying ReLU-activated\nGNNs operating on highly homophilous graphs with low-dimensional or one-hot\nencoded features as particularly susceptible. Empirical results using ten\nreal-world datasets confirm the statistical significance of our key theoretical\ninsights and offer actionable results to mitigate BFA risks in\nexpressivity-critical applications."
    },
    {
        "date": "2025-04",
        "title": "BadApex: Backdoor Attack Based on Adaptive Optimization Mechanism of Black-box Large Language Models",
        "author": "Zhengxian Wu, Juan Wen, Wanli Peng, Ziwei Zhang, Yinghan Zhou, and Yiming Xue",
        "link": "http://arxiv.org/abs/2504.13775v2",
        "abstract": "Previous insertion-based and paraphrase-based backdoors have achieved great\nsuccess in attack efficacy, but they ignore the text quality and semantic\nconsistency between poisoned and clean texts. Although recent studies introduce\nLLMs to generate poisoned texts and improve the stealthiness, semantic\nconsistency, and text quality, their hand-crafted prompts rely on expert\nexperiences, facing significant challenges in prompt adaptability and attack\nperformance after defenses. In this paper, we propose a novel backdoor attack\nbased on adaptive optimization mechanism of black-box large language models\n(BadApex), which leverages a black-box LLM to generate poisoned text through a\nrefined prompt. Specifically, an Adaptive Optimization Mechanism is designed to\nrefine an initial prompt iteratively using the generation and modification\nagents. The generation agent generates the poisoned text based on the initial\nprompt. Then the modification agent evaluates the quality of the poisoned text\nand refines a new prompt. After several iterations of the above process, the\nrefined prompt is used to generate poisoned texts through LLMs. We conduct\nextensive experiments on three dataset with six backdoor attacks and two\ndefenses. Extensive experimental results demonstrate that BadApex significantly\noutperforms state-of-the-art attacks. It improves prompt adaptability, semantic\nconsistency, and text quality. Furthermore, when two defense methods are\napplied, the average attack success rate (ASR) still up to 96.75%."
    },
    {
        "date": "2025-04",
        "title": "Analysing the Robustness of Vision-Language-Models to Common Corruptions",
        "author": "Muhammad Usama, Syeda Aishah Asim, Syed Bilal Ali, Syed Talal Wasim, and Umair Bin Mansoor",
        "link": "http://arxiv.org/abs/2504.13690v2",
        "abstract": "Vision-language models (VLMs) have demonstrated impressive capabilities in\nunderstanding and reasoning about visual and textual content. However, their\nrobustness to common image corruptions remains under-explored. In this work, we\npresent the first comprehensive analysis of VLM robustness across 19 corruption\ntypes from the ImageNet-C benchmark, spanning four categories: noise, blur,\nweather, and digital distortions. We introduce two new benchmarks, TextVQA-C\nand GQA-C, to systematically evaluate how corruptions affect scene text\nunderstanding and object-based reasoning, respectively. Our analysis reveals\nthat transformer-based VLMs exhibit distinct vulnerability patterns across\ntasks: text recognition deteriorates most severely under blur and snow\ncorruptions, while object reasoning shows higher sensitivity to corruptions\nsuch as frost and impulse noise. We connect these observations to the\nfrequency-domain characteristics of different corruptions, revealing how\ntransformers' inherent bias toward low-frequency processing explains their\ndifferential robustness patterns. Our findings provide valuable insights for\ndeveloping more corruption-robust vision-language models for real-world\napplications."
    },
    {
        "date": "2025-04",
        "title": "Going Whole Hog: A Philosophical Defense of AI Cognition",
        "author": "Herman Cappelen, and Josh Dever",
        "link": "http://arxiv.org/abs/2504.13988v1",
        "abstract": "This work defends the 'Whole Hog Thesis': sophisticated Large Language Models\n(LLMs) like ChatGPT are full-blown linguistic and cognitive agents, possessing\nunderstanding, beliefs, desires, knowledge, and intentions. We argue against\nprevailing methodologies in AI philosophy, rejecting starting points based on\nlow-level computational details ('Just an X' fallacy) or pre-existing theories\nof mind. Instead, we advocate starting with simple, high-level observations of\nLLM behavior (e.g., answering questions, making suggestions) -- defending this\ndata against charges of metaphor, loose talk, or pretense. From these\nobservations, we employ 'Holistic Network Assumptions' -- plausible connections\nbetween mental capacities (e.g., answering implies knowledge, knowledge implies\nbelief, action implies intention) -- to argue for the full suite of cognitive\nstates. We systematically rebut objections based on LLM failures\n(hallucinations, planning/reasoning errors), arguing these don't preclude\nagency, often mirroring human fallibility. We address numerous 'Games of\nLacks', arguing that LLMs do not lack purported necessary conditions for\ncognition (e.g., semantic grounding, embodiment, justification, intrinsic\nintentionality) or that these conditions are not truly necessary, often relying\non anti-discriminatory arguments comparing LLMs to diverse human capacities.\nOur approach is evidential, not functionalist, and deliberately excludes\nconsciousness. We conclude by speculating on the possibility of LLMs possessing\n'alien' contents beyond human conceptual schemes."
    },
    {
        "date": "2025-04",
        "title": "Fairness and Robustness in Machine Unlearning",
        "author": "Khoa Tran, and Simon S. Woo",
        "link": "http://arxiv.org/abs/2504.13610v1",
        "abstract": "Machine unlearning poses the challenge of ``how to eliminate the influence of\nspecific data from a pretrained model'' in regard to privacy concerns. While\nprior research on approximated unlearning has demonstrated accuracy and\nefficiency in time complexity, we claim that it falls short of achieving exact\nunlearning, and we are the first to focus on fairness and robustness in machine\nunlearning algorithms. Our study presents fairness Conjectures for a\nwell-trained model, based on the variance-bias trade-off characteristic, and\nconsiders their relevance to robustness. Our Conjectures are supported by\nexperiments conducted on the two most widely used model architectures, ResNet\nand ViT, demonstrating the correlation between fairness and robustness:\n\\textit{the higher fairness-gap is, the more the model is sensitive and\nvulnerable}. In addition, our experiments demonstrate the vulnerability of\ncurrent state-of-the-art approximated unlearning algorithms to adversarial\nattacks, where their unlearned models suffer a significant drop in accuracy\ncompared to the exact-unlearned models. We claim that our fairness-gap\nmeasurement and robustness metric should be used to evaluate the unlearning\nalgorithm. Furthermore, we demonstrate that unlearning in the intermediate and\nlast layers is sufficient and cost-effective for time and memory complexity."
    }
]