[
    {
        "date": "2025-04",
        "title": "Silenzio: Secure Non-Interactive Outsourced MLP Training",
        "author": "Jonas Sander, and Thomas Eisenbarth",
        "link": "http://arxiv.org/abs/2504.17785v1",
        "abstract": "Outsourcing the ML training to cloud providers presents a compelling\nopportunity for resource constrained clients, while it simultaneously bears\ninherent privacy risks, especially for highly sensitive training data. We\nintroduce Silenzio, the first fully non-interactive outsourcing scheme for the\ntraining of multi-layer perceptrons that achieves 128 bit security using FHE.\nUnlike traditional MPC based protocols that necessitate interactive\ncommunication between the client and server(s) or non-collusion assumptions\namong multiple servers, Silenzio enables the fire-and-forget paradigm without\nsuch assumptions. In this approach, the client encrypts the training data once,\nand the cloud server performs the training without any further interaction.\n  Silenzio operates over low bitwidth integers - never exceeding 8 bit - to\nmitigate the computational overhead of FHE. Our approach features a novel\nlow-bitwidth matrix multiplication that leverages input-dependent residue\nnumber systems and a Karatsuba-inspired multiplication routine, ensuring that\nno intermediate FHE-processed value overflows 8 bit. Starting from an\nRNS-to-MRNS conversion process, we propose an efficient block-scaling\nmechanism, which approximately shifts encrypted tensor values to the\nuser-specified most significant bits. To instantiate the backpropagation of the\nerror, Silenzio introduces a low-bitwidth and TFHE friendly gradient\ncomputation for the cross entropy loss.\n  Implemented using the state-of-the-art Concrete library, we evaluate Silenzio\non standard MLP training tasks regarding runtime as well as model performance\nand achieve similar classification accuracy as MLPs trained using standard\nPyTorch with 32 bit floating-point computations. Our open-source implementation\nrepresents a significant advancement in privacy-preserving ML, providing a new\nbaseline for secure and non-interactive outsourced MLP training."
    },
    {
        "date": "2025-04",
        "title": "CasualHDRSplat: Robust High Dynamic Range 3D Gaussian Splatting from Casually Captured Videos",
        "author": "Shucheng Gong, Lingzhe Zhao, Wenpu Li, Hong Xie, Yin Zhang, Shiyu Zhao, and Peidong Liu",
        "link": "http://arxiv.org/abs/2504.17728v1",
        "abstract": "Recently, photo-realistic novel view synthesis from multi-view images, such\nas neural radiance field (NeRF) and 3D Gaussian Splatting (3DGS), have garnered\nwidespread attention due to their superior performance. However, most works\nrely on low dynamic range (LDR) images, which limits the capturing of richer\nscene details. Some prior works have focused on high dynamic range (HDR) scene\nreconstruction, typically require capturing of multi-view sharp images with\ndifferent exposure times at fixed camera positions during exposure times, which\nis time-consuming and challenging in practice. For a more flexible data\nacquisition, we propose a one-stage method: \\textbf{CasualHDRSplat} to easily\nand robustly reconstruct the 3D HDR scene from casually captured videos with\nauto-exposure enabled, even in the presence of severe motion blur and varying\nunknown exposure time. \\textbf{CasualHDRSplat} contains a unified\ndifferentiable physical imaging model which first applies continuous-time\ntrajectory constraint to imaging process so that we can jointly optimize\nexposure time, camera response function (CRF), camera poses, and sharp 3D HDR\nscene. Extensive experiments demonstrate that our approach outperforms existing\nmethods in terms of robustness and rendering quality. Our source code will be\navailable at https://github.com/WU-CVGL/CasualHDRSplat"
    },
    {
        "date": "2025-04",
        "title": "Towards Robust LLMs: an Adversarial Robustness Measurement Framework",
        "author": "Natan Levy, Adiel Ashrov, and Guy Katz",
        "link": "http://arxiv.org/abs/2504.17723v1",
        "abstract": "The rise of Large Language Models (LLMs) has revolutionized artificial\nintelligence, yet these models remain vulnerable to adversarial perturbations,\nundermining their reliability in high-stakes applications. While adversarial\nrobustness in vision-based neural networks has been extensively studied, LLM\nrobustness remains under-explored. We adapt the Robustness Measurement and\nAssessment (RoMA) framework to quantify LLM resilience against adversarial\ninputs without requiring access to model parameters. By comparing RoMA's\nestimates to those of formal verification methods, we demonstrate its accuracy\nwith minimal error margins while maintaining computational efficiency. Our\nempirical evaluation reveals that robustness varies significantly not only\nbetween different models but also across categories within the same task and\nbetween various types of perturbations. This non-uniformity underscores the\nneed for task-specific robustness evaluations, enabling practitioners to\ncompare and select models based on application-specific robustness\nrequirements. Our work provides a systematic methodology to assess LLM\nrobustness, advancing the development of more reliable language models for\nreal-world deployment."
    },
    {
        "date": "2025-04",
        "title": "On the Generalization of Adversarially Trained Quantum Classifiers",
        "author": "Petros Georgiou, Aaron Mark Thomas, Sharu Theresa Jose, and Osvaldo Simeone",
        "link": "http://arxiv.org/abs/2504.17690v1",
        "abstract": "Quantum classifiers are vulnerable to adversarial attacks that manipulate\ntheir input classical or quantum data. A promising countermeasure is\nadversarial training, where quantum classifiers are trained by using an\nattack-aware, adversarial loss function. This work establishes novel bounds on\nthe generalization error of adversarially trained quantum classifiers when\ntested in the presence of perturbation-constrained adversaries. The bounds\nquantify the excess generalization error incurred to ensure robustness to\nadversarial attacks as scaling with the training sample size $m$ as\n$1/\\sqrt{m}$, while yielding insights into the impact of the quantum embedding.\nFor quantum binary classifiers employing \\textit{rotation embedding}, we find\nthat, in the presence of adversarial attacks on classical inputs $\\mathbf{x}$,\nthe increase in sample complexity due to adversarial training over conventional\ntraining vanishes in the limit of high dimensional inputs $\\mathbf{x}$. In\ncontrast, when the adversary can directly attack the quantum state\n$\\rho(\\mathbf{x})$ encoding the input $\\mathbf{x}$, the excess generalization\nerror depends on the choice of embedding only through its Hilbert space\ndimension. The results are also extended to multi-class classifiers. We\nvalidate our theoretical findings with numerical experiments."
    },
    {
        "date": "2025-04",
        "title": "Evaluating the Vulnerability of ML-Based Ethereum Phishing Detectors to Single-Feature Adversarial Perturbations",
        "author": "Ahod Alghuried, Ali Alkinoon, Abdulaziz Alghamdi, Soohyeon Choi, Manar Mohaisen, and David Mohaisen",
        "link": "http://arxiv.org/abs/2504.17684v1",
        "abstract": "This paper explores the vulnerability of machine learning models to simple\nsingle-feature adversarial attacks in the context of Ethereum fraudulent\ntransaction detection. Through comprehensive experimentation, we investigate\nthe impact of various adversarial attack strategies on model performance\nmetrics. Our findings, highlighting how prone those techniques are to simple\nattacks, are alarming, and the inconsistency in the attacks' effect on\ndifferent algorithms promises ways for attack mitigation. We examine the\neffectiveness of different mitigation strategies, including adversarial\ntraining and enhanced feature selection, in enhancing model robustness and show\ntheir effectiveness."
    },
    {
        "date": "2025-04",
        "title": "Enhancing CNNs robustness to occlusions with bioinspired filters for border completion",
        "author": "Catarina P. Coutinho, Aneeqa Merhab, Janko Petkovic, Ferdinando Zanchetta, and Rita Fioresi",
        "link": "http://arxiv.org/abs/2504.17619v1",
        "abstract": "We exploit the mathematical modeling of the visual cortex mechanism for\nborder completion to define custom filters for CNNs. We see a consistent\nimprovement in performance, particularly in accuracy, when our modified LeNet 5\nis tested with occluded MNIST images."
    },
    {
        "date": "2025-04",
        "title": "Wolves in the Repository: A Software Engineering Analysis of the XZ Utils Supply Chain Attack",
        "author": "Piotr Przymus, and Thomas Durieux",
        "link": "http://arxiv.org/abs/2504.17473v1",
        "abstract": "The digital economy runs on Open Source Software (OSS), with an estimated\n90\\% of modern applications containing open-source components. While this\nwidespread adoption has revolutionized software development, it has also\ncreated critical security vulnerabilities, particularly in essential but\nunder-resourced projects. This paper examines a sophisticated attack on the XZ\nUtils project (CVE-2024-3094), where attackers exploited not just code, but the\nentire open-source development process to inject a backdoor into a fundamental\nLinux compression library. Our analysis reveals a new breed of supply chain\nattack that manipulates software engineering practices themselves -- from\ncommunity management to CI/CD configurations -- to establish legitimacy and\nmaintain long-term control. Through a comprehensive examination of GitHub\nevents and development artifacts, we reconstruct the attack timeline, analyze\nthe evolution of attacker tactics. Our findings demonstrate how attackers\nleveraged seemingly beneficial contributions to project infrastructure and\nmaintenance to bypass traditional security measures. This work extends beyond\ntraditional security analysis by examining how software engineering practices\nthemselves can be weaponized, offering insights for protecting the open-source\necosystem."
    },
    {
        "date": "2025-04",
        "title": "Unveiling Hidden Vulnerabilities in Digital Human Generation via Adversarial Attacks",
        "author": "Zhiying Li, Yeying Jin, Fan Shen, Zhi Liu, Weibin Chen, Pengju Zhang, Xiaomei Zhang, Boyu Chen, Michael Shen, Kejian Wu, Zhaoxin Fan, and Jin Dong",
        "link": "http://arxiv.org/abs/2504.17457v1",
        "abstract": "Expressive human pose and shape estimation (EHPS) is crucial for digital\nhuman generation, especially in applications like live streaming. While\nexisting research primarily focuses on reducing estimation errors, it largely\nneglects robustness and security aspects, leaving these systems vulnerable to\nadversarial attacks. To address this significant challenge, we propose the\n\\textbf{Tangible Attack (TBA)}, a novel framework designed to generate\nadversarial examples capable of effectively compromising any digital human\ngeneration model. Our approach introduces a \\textbf{Dual Heterogeneous Noise\nGenerator (DHNG)}, which leverages Variational Autoencoders (VAE) and\nControlNet to produce diverse, targeted noise tailored to the original image\nfeatures. Additionally, we design a custom \\textbf{adversarial loss function}\nto optimize the noise, ensuring both high controllability and potent\ndisruption. By iteratively refining the adversarial sample through\nmulti-gradient signals from both the noise and the state-of-the-art EHPS model,\nTBA substantially improves the effectiveness of adversarial attacks. Extensive\nexperiments demonstrate TBA's superiority, achieving a remarkable 41.0\\%\nincrease in estimation error, with an average improvement of approximately\n17.0\\%. These findings expose significant security vulnerabilities in current\nEHPS models and highlight the need for stronger defenses in digital human\ngeneration systems."
    },
    {
        "date": "2025-04",
        "title": "StereoMamba: Real-time and Robust Intraoperative Stereo Disparity Estimation via Long-range Spatial Dependencies",
        "author": "Xu Wang, Jialang Xu, Shuai Zhang, Baoru Huang, Danail Stoyanov, and Evangelos B. Mazomenos",
        "link": "http://arxiv.org/abs/2504.17401v1",
        "abstract": "Stereo disparity estimation is crucial for obtaining depth information in\nrobot-assisted minimally invasive surgery (RAMIS). While current deep learning\nmethods have made significant advancements, challenges remain in achieving an\noptimal balance between accuracy, robustness, and inference speed. To address\nthese challenges, we propose the StereoMamba architecture, which is\nspecifically designed for stereo disparity estimation in RAMIS. Our approach is\nbased on a novel Feature Extraction Mamba (FE-Mamba) module, which enhances\nlong-range spatial dependencies both within and across stereo images. To\neffectively integrate multi-scale features from FE-Mamba, we then introduce a\nnovel Multidimensional Feature Fusion (MFF) module. Experiments against the\nstate-of-the-art on the ex-vivo SCARED benchmark demonstrate that StereoMamba\nachieves superior performance on EPE of 2.64 px and depth MAE of 2.55 mm, the\nsecond-best performance on Bad2 of 41.49% and Bad3 of 26.99%, while maintaining\nan inference speed of 21.28 FPS for a pair of high-resolution images\n(1280*1024), striking the optimum balance between accuracy, robustness, and\nefficiency. Furthermore, by comparing synthesized right images, generated from\nwarping left images using the generated disparity maps, with the actual right\nimage, StereoMamba achieves the best average SSIM (0.8970) and PSNR (16.0761),\nexhibiting strong zero-shot generalization on the in-vivo RIS2017 and StereoMIS\ndatasets."
    },
    {
        "date": "2025-04",
        "title": "Class-Conditional Distribution Balancing for Group Robust Classification",
        "author": "Miaoyun Zhao, Qiang Zhang, and Chenrong Li",
        "link": "http://arxiv.org/abs/2504.17314v2",
        "abstract": "Spurious correlations that lead models to correct predictions for the wrong\nreasons pose a critical challenge for robust real-world generalization.\nExisting research attributes this issue to group imbalance and addresses it by\nmaximizing group-balanced or worst-group accuracy, which heavily relies on\nexpensive bias annotations. A compromise approach involves predicting bias\ninformation using extensively pretrained foundation models, which requires\nlarge-scale data and becomes impractical for resource-limited rare domains. To\naddress these challenges, we offer a novel perspective by reframing the\nspurious correlations as imbalances or mismatches in class-conditional\ndistributions, and propose a simple yet effective robust learning method that\neliminates the need for both bias annotations and predictions. With the goal of\nreducing the mutual information between spurious factors and label information,\nour method leverages a sample reweighting strategy to achieve class-conditional\ndistribution balancing, which automatically highlights minority groups and\nclasses, effectively dismantling spurious correlations and producing a debiased\ndata distribution for classification. Extensive experiments and analysis\ndemonstrate that our approach consistently delivers state-of-the-art\nperformance, rivaling methods that rely on bias supervision."
    },
    {
        "date": "2025-04",
        "title": "FLUKE: A Linguistically-Driven and Task-Agnostic Framework for Robustness Evaluation",
        "author": "Yulia Otmakhova, Hung Thinh Truong, Rahmad Mahendra, Zenan Zhai, Rongxin Zhu, Daniel Beck, and Jey Han Lau",
        "link": "http://arxiv.org/abs/2504.17311v1",
        "abstract": "We present FLUKE (Framework for LingUistically-driven and tasK-agnostic\nrobustness Evaluation), a task-agnostic framework for assessing model\nrobustness through systematic minimal variations of test data. FLUKE introduces\ncontrolled variations across linguistic levels - from orthography to dialect\nand style varieties - and leverages large language models (LLMs) with human\nvalidation to generate modifications. We demonstrate FLUKE's utility by\nevaluating both fine-tuned models and LLMs across four diverse NLP tasks, and\nreveal that (1) the impact of linguistic variations is highly task-dependent,\nwith some tests being critical for certain tasks but irrelevant for others; (2)\nwhile LLMs have better overall robustness compared to fine-tuned models, they\nstill exhibit significant brittleness to certain linguistic variations; (3) all\nmodels show substantial vulnerability to negation modifications across most\ntasks. These findings highlight the importance of systematic robustness testing\nfor understanding model behaviors."
    },
    {
        "date": "2025-04",
        "title": "Enhancing Variational Autoencoders with Smooth Robust Latent Encoding",
        "author": "Hyomin Lee, Minseon Kim, Sangwon Jang, Jongheon Jeong, and Sung Ju Hwang",
        "link": "http://arxiv.org/abs/2504.17219v1",
        "abstract": "Variational Autoencoders (VAEs) have played a key role in scaling up\ndiffusion-based generative models, as in Stable Diffusion, yet questions\nregarding their robustness remain largely underexplored. Although adversarial\ntraining has been an established technique for enhancing robustness in\npredictive models, it has been overlooked for generative models due to concerns\nabout potential fidelity degradation by the nature of trade-offs between\nperformance and robustness. In this work, we challenge this presumption,\nintroducing Smooth Robust Latent VAE (SRL-VAE), a novel adversarial training\nframework that boosts both generation quality and robustness. In contrast to\nconventional adversarial training, which focuses on robustness only, our\napproach smooths the latent space via adversarial perturbations, promoting more\ngeneralizable representations while regularizing with originality\nrepresentation to sustain original fidelity. Applied as a post-training step on\npre-trained VAEs, SRL-VAE improves image robustness and fidelity with minimal\ncomputational overhead. Experiments show that SRL-VAE improves both generation\nquality, in image reconstruction and text-guided image editing, and robustness,\nagainst Nightshade attacks and image editing attacks. These results establish a\nnew paradigm, showing that adversarial training, once thought to be detrimental\nto generative models, can instead enhance both fidelity and robustness."
    },
    {
        "date": "2025-04",
        "title": "Developing a Blockchain-Based Secure Digital Contents Distribution System",
        "author": "Syed Mohiuddin Qadri, and Sangwhan Cha",
        "link": "http://arxiv.org/abs/2504.17194v1",
        "abstract": "As digital content distribution expands rapidly through online platforms,\nsecuring digital media and protecting intellectual property has become\nincreasingly complex. Traditional centralized systems, while widely adopted,\nsuffer from vulnerabilities such as single points of failure and limited\ntraceability of unauthorized access. This paper presents a blockchain-based\nsecure digital content distribution system that integrates Sia, a decentralized\nstorage network, and Skynet, a content delivery network, to enhance content\nprotection and distribution. The proposed system employs a dual-layer\narchitecture: off-chain for user authentication and on-chain for transaction\nvalidation using smart contracts and asymmetric encryption. By introducing a\nlicense issuance and secret block mechanism, the system ensures content\nauthenticity, privacy, and controlled access. Experimental results demonstrate\nthe feasibility and scalability of the system in securely distributing\nmultimedia files. The proposed platform not only improves content security but\nalso paves the way for future enhancements with decentralized applications and\nintegrated royalty payment mechanisms."
    },
    {
        "date": "2025-04",
        "title": "AUTHENTICATION: Identifying Rare Failure Modes in Autonomous Vehicle Perception Systems using Adversarially Guided Diffusion Models",
        "author": "Mohammad Zarei, Melanie A Jutras, Eliana Evans, Mike Tan, and Omid Aaramoon",
        "link": "http://arxiv.org/abs/2504.17179v1",
        "abstract": "Autonomous Vehicles (AVs) rely on artificial intelligence (AI) to accurately\ndetect objects and interpret their surroundings. However, even when trained\nusing millions of miles of real-world data, AVs are often unable to detect rare\nfailure modes (RFMs). The problem of RFMs is commonly referred to as the\n\"long-tail challenge\", due to the distribution of data including many instances\nthat are very rarely seen. In this paper, we present a novel approach that\nutilizes advanced generative and explainable AI techniques to aid in\nunderstanding RFMs. Our methods can be used to enhance the robustness and\nreliability of AVs when combined with both downstream model training and\ntesting. We extract segmentation masks for objects of interest (e.g., cars) and\ninvert them to create environmental masks. These masks, combined with carefully\ncrafted text prompts, are fed into a custom diffusion model. We leverage the\nStable Diffusion inpainting model guided by adversarial noise optimization to\ngenerate images containing diverse environments designed to evade object\ndetection models and expose vulnerabilities in AI systems. Finally, we produce\nnatural language descriptions of the generated RFMs that can guide developers\nand policymakers to improve the safety and reliability of AV systems."
    },
    {
        "date": "2025-04",
        "title": "Robo-Troj: Attacking LLM-based Task Planners",
        "author": "Mohaiminul Al Nahian, Zainab Altaweel, David Reitano, Sabbir Ahmed, Saumitra Lohokare, Shiqi Zhang, and Adnan Siraj Rakin",
        "link": "http://arxiv.org/abs/2504.17070v1",
        "abstract": "Robots need task planning methods to achieve goals that require more than\nindividual actions. Recently, large language models (LLMs) have demonstrated\nimpressive performance in task planning. LLMs can generate a step-by-step\nsolution using a description of actions and the goal. Despite the successes in\nLLM-based task planning, there is limited research studying the security\naspects of those systems. In this paper, we develop Robo-Troj, the first\nmulti-trigger backdoor attack for LLM-based task planners, which is the main\ncontribution of this work. As a multi-trigger attack, Robo-Troj is trained to\naccommodate the diversity of robot application domains. For instance, one can\nuse unique trigger words, e.g., \"herical\", to activate a specific malicious\nbehavior, e.g., cutting hand on a kitchen robot. In addition, we develop an\noptimization method for selecting the trigger words that are most effective.\nThrough demonstrating the vulnerability of LLM-based planners, we aim to\npromote the development of secured robot systems."
    },
    {
        "date": "2025-04",
        "title": "Statistical Guarantees in Synthetic Data through Conformal Adversarial Generation",
        "author": "Rahul Vishwakarma",
        "link": "http://arxiv.org/abs/2504.17058v1",
        "abstract": "The generation of high-quality synthetic data presents significant challenges\nin machine learning research, particularly regarding statistical fidelity and\nuncertainty quantification. Existing generative models produce compelling\nsynthetic samples but lack rigorous statistical guarantees about their relation\nto the underlying data distribution, limiting their applicability in critical\ndomains requiring robust error bounds. We address this fundamental limitation\nby presenting a novel framework that incorporates conformal prediction\nmethodologies into Generative Adversarial Networks (GANs). By integrating\nmultiple conformal prediction paradigms including Inductive Conformal\nPrediction (ICP), Mondrian Conformal Prediction, Cross-Conformal Prediction,\nand Venn-Abers Predictors, we establish distribution-free uncertainty\nquantification in generated samples. This approach, termed Conformalized GAN\n(cGAN), demonstrates enhanced calibration properties while maintaining the\ngenerative power of traditional GANs, producing synthetic data with provable\nstatistical guarantees. We provide rigorous mathematical proofs establishing\nfinite-sample validity guarantees and asymptotic efficiency properties,\nenabling the reliable application of synthetic data in high-stakes domains\nincluding healthcare, finance, and autonomous systems."
    },
    {
        "date": "2025-04",
        "title": "BadVideo: Stealthy Backdoor Attack against Text-to-Video Generation",
        "author": "Ruotong Wang, Mingli Zhu, Jiarong Ou, Rui Chen, Xin Tao, Pengfei Wan, and Baoyuan Wu",
        "link": "http://arxiv.org/abs/2504.16907v1",
        "abstract": "Text-to-video (T2V) generative models have rapidly advanced and found\nwidespread applications across fields like entertainment, education, and\nmarketing. However, the adversarial vulnerabilities of these models remain\nrarely explored. We observe that in T2V generation tasks, the generated videos\noften contain substantial redundant information not explicitly specified in the\ntext prompts, such as environmental elements, secondary objects, and additional\ndetails, providing opportunities for malicious attackers to embed hidden\nharmful content. Exploiting this inherent redundancy, we introduce BadVideo,\nthe first backdoor attack framework tailored for T2V generation. Our attack\nfocuses on designing target adversarial outputs through two key strategies: (1)\nSpatio-Temporal Composition, which combines different spatiotemporal features\nto encode malicious information; (2) Dynamic Element Transformation, which\nintroduces transformations in redundant elements over time to convey malicious\ninformation. Based on these strategies, the attacker's malicious target\nseamlessly integrates with the user's textual instructions, providing high\nstealthiness. Moreover, by exploiting the temporal dimension of videos, our\nattack successfully evades traditional content moderation systems that\nprimarily analyze spatial information within individual frames. Extensive\nexperiments demonstrate that BadVideo achieves high attack success rates while\npreserving original semantics and maintaining excellent performance on clean\ninputs. Overall, our work reveals the adversarial vulnerability of T2V models,\ncalling attention to potential risks and misuse. Our project page is at\nhttps://wrt2000.github.io/BadVideo2025/."
    },
    {
        "date": "2025-04",
        "title": "Building A Secure Agentic AI Application Leveraging A2A Protocol",
        "author": "Idan Habler, Ken Huang, Vineeth Sai Narajala, and Prashant Kulkarni",
        "link": "http://arxiv.org/abs/2504.16902v1",
        "abstract": "As Agentic AI systems evolve from basic workflows to complex multi agent\ncollaboration, robust protocols such as Google's Agent2Agent (A2A) become\nessential enablers. To foster secure adoption and ensure the reliability of\nthese complex interactions, understanding the secure implementation of A2A is\nessential. This paper addresses this goal by providing a comprehensive security\nanalysis centered on the A2A protocol. We examine its fundamental elements and\noperational dynamics, situating it within the framework of agent communication\ndevelopment. Utilizing the MAESTRO framework, specifically designed for AI\nrisks, we apply proactive threat modeling to assess potential security issues\nin A2A deployments, focusing on aspects such as Agent Card management, task\nexecution integrity, and authentication methodologies.\n  Based on these insights, we recommend practical secure development\nmethodologies and architectural best practices designed to build resilient and\neffective A2A systems. Our analysis also explores how the synergy between A2A\nand the Model Context Protocol (MCP) can further enhance secure\ninteroperability. This paper equips developers and architects with the\nknowledge and practical guidance needed to confidently leverage the A2A\nprotocol for building robust and secure next generation agentic applications."
    },
    {
        "date": "2025-04",
        "title": "Simplified Swarm Learning Framework for Robust and Scalable Diagnostic Services in Cancer Histopathology",
        "author": "Yanjie Wu, Yuhao Ji, Saiho Lee, Juniad Akram, Ali Braytee, and Ali Anaissi",
        "link": "http://arxiv.org/abs/2504.16732v1",
        "abstract": "The complexities of healthcare data, including privacy concerns, imbalanced\ndatasets, and interoperability issues, necessitate innovative machine learning\nsolutions. Swarm Learning (SL), a decentralized alternative to Federated\nLearning, offers privacy-preserving distributed training, but its reliance on\nblockchain technology hinders accessibility and scalability. This paper\nintroduces a \\textit{Simplified Peer-to-Peer Swarm Learning (P2P-SL) Framework}\ntailored for resource-constrained environments. By eliminating blockchain\ndependencies and adopting lightweight peer-to-peer communication, the proposed\nframework ensures robust model synchronization while maintaining data privacy.\nApplied to cancer histopathology, the framework integrates optimized\npre-trained models, such as TorchXRayVision, enhanced with DenseNet decoders,\nto improve diagnostic accuracy. Extensive experiments demonstrate the\nframework's efficacy in handling imbalanced and biased datasets, achieving\ncomparable performance to centralized models while preserving privacy. This\nstudy paves the way for democratizing advanced machine learning in healthcare,\noffering a scalable, accessible, and efficient solution for privacy-sensitive\ndiagnostic applications."
    },
    {
        "date": "2025-04",
        "title": "V$^2$R-Bench: Holistically Evaluating LVLM Robustness to Fundamental Visual Variations",
        "author": "Zhiyuan Fan, Yumeng Wang, Sandeep Polisetty, and Yi R. Fung",
        "link": "http://arxiv.org/abs/2504.16727v2",
        "abstract": "Large Vision Language Models (LVLMs) excel in various vision-language tasks.\nYet, their robustness to visual variations in position, scale, orientation, and\ncontext that objects in natural scenes inevitably exhibit due to changes in\nviewpoint and environment remains largely underexplored. To bridge this gap, we\nintroduce V$^2$R-Bench, a comprehensive benchmark framework for evaluating\nVisual Variation Robustness of LVLMs, which encompasses automated evaluation\ndataset generation and principled metrics for thorough robustness assessment.\nThrough extensive evaluation on 21 LVLMs, we reveal a surprising vulnerability\nto visual variations, in which even advanced models that excel at complex\nvision-language tasks significantly underperform on simple tasks such as object\nrecognition. Interestingly, these models exhibit a distinct visual position\nbias that contradicts theories of effective receptive fields, and demonstrate a\nhuman-like visual acuity threshold. To identify the source of these\nvulnerabilities, we present a systematic framework for component-level\nanalysis, featuring a novel visualization approach for aligned visual features.\nResults show that these vulnerabilities stem from error accumulation in the\npipeline architecture and inadequate multimodal alignment. Complementary\nexperiments with synthetic data further demonstrate that these limitations are\nfundamentally architectural deficiencies, scoring the need for architectural\ninnovations in future LVLM designs."
    },
    {
        "date": "2025-04",
        "title": "MCMC for Bayesian estimation of Differential Privacy from Membership Inference Attacks",
        "author": "Ceren Yildirim, Kamer Kaya, Sinan Yildirim, and Erkay Savas",
        "link": "http://arxiv.org/abs/2504.16683v1",
        "abstract": "We propose a new framework for Bayesian estimation of differential privacy,\nincorporating evidence from multiple membership inference attacks (MIA).\nBayesian estimation is carried out via a Markov chain Monte Carlo (MCMC)\nalgorithm, named MCMC-DP-Est, which provides an estimate of the full posterior\ndistribution of the privacy parameter (e.g., instead of just credible\nintervals). Critically, the proposed method does not assume that privacy\nauditing is performed with the most powerful attack on the worst-case (dataset,\nchallenge point) pair, which is typically unrealistic. Instead, MCMC-DP-Est\njointly estimates the strengths of MIAs used and the privacy of the training\nalgorithm, yielding a more cautious privacy analysis. We also present an\neconomical way to generate measurements for the performance of an MIA that is\nto be used by the MCMC method to estimate privacy. We present the use of the\nmethods with numerical examples with both artificial and real data."
    },
    {
        "date": "2025-04",
        "title": "Security Science (SecSci), Basic Concepts and Mathematical Foundations",
        "author": "Dusko Pavlovic, and Peter-Michael Seidel",
        "link": "http://arxiv.org/abs/2504.16617v1",
        "abstract": "This textbook compiles the lecture notes from security courses taught at\nOxford in the 2000s, at Royal Holloway in the 2010s, and currently in Hawaii.\nThe early chapters are suitable for a first course in security. The middle\nchapters have been used in advanced courses. Towards the end there are also\nsome research problems."
    },
    {
        "date": "2025-04",
        "title": "LaSDVS : A Post-Quantum Secure Compact Strong-Designated Verifier Signature",
        "author": "Shanu Poddar, Sweta Mishra, Tapaswini Mohanty, Vikas Srivastava, and Sugata Gangopadhyay",
        "link": "http://arxiv.org/abs/2504.16571v1",
        "abstract": "Digital signatures are fundamental cryptographic primitives that ensure the\nauthenticity and integrity of digital communication. However, in scenarios\ninvolving sensitive interactions -- such as e-voting or e-cash -- there is a\ngrowing need for more controlled signing mechanisms. Strong-Designated Verifier\nSignature (SDVS) offers such control by allowing the signer to specify and\nrestrict the verifier of a signature. The existing state-of-the-art SDVS are\nmostly based on number-theoretic hardness assumptions. Thus, they are not\nsecure against quantum attacks. Moreover, Post-Quantum Cryptography (PQC)-based\nSDVS are inefficient and have large key and signature sizes. In this work, we\naddress these challenges and propose an efficient post-quantum SDVS (namely,\nLaSDVS) based on ideal lattices under the hardness assumptions of the Ring-SIS\nand Ring-LWE problems. LaSDVS achieves advanced security properties including\nstrong unforgeability under chosen-message attacks, non-transferability,\nnon-delegatability, and signer anonymity. By employing the algebraic structure\nof rings and the gadget trapdoor mechanism of Micciancio et al., we design\nLaSDVS to minimize computational overhead and significantly reduce key and\nsignature sizes. Notably, our scheme achieves a compact signature size of\n$\\mathcal{O}(n\\log q)$, compared to $\\mathcal{O}(n^2)$ size, where $n$ is the\nsecurity parameter, in the existing state-of-the-art PQC designs. To the best\nof our knowledge, LaSDVS offers the \\textit{smallest private key and signature\nsize} among the existing PQC-based SDVS schemes."
    },
    {
        "date": "2025-04",
        "title": "Amplified Vulnerabilities: Structured Jailbreak Attacks on LLM-based Multi-Agent Debate",
        "author": "Senmao Qi, Yifei Zou, Peng Li, Ziyi Lin, Xiuzhen Cheng, and Dongxiao Yu",
        "link": "http://arxiv.org/abs/2504.16489v1",
        "abstract": "Multi-Agent Debate (MAD), leveraging collaborative interactions among Large\nLanguage Models (LLMs), aim to enhance reasoning capabilities in complex tasks.\nHowever, the security implications of their iterative dialogues and\nrole-playing characteristics, particularly susceptibility to jailbreak attacks\neliciting harmful content, remain critically underexplored. This paper\nsystematically investigates the jailbreak vulnerabilities of four prominent MAD\nframeworks built upon leading commercial LLMs (GPT-4o, GPT-4, GPT-3.5-turbo,\nand DeepSeek) without compromising internal agents. We introduce a novel\nstructured prompt-rewriting framework specifically designed to exploit MAD\ndynamics via narrative encapsulation, role-driven escalation, iterative\nrefinement, and rhetorical obfuscation. Our extensive experiments demonstrate\nthat MAD systems are inherently more vulnerable than single-agent setups.\nCrucially, our proposed attack methodology significantly amplifies this\nfragility, increasing average harmfulness from 28.14% to 80.34% and achieving\nattack success rates as high as 80% in certain scenarios. These findings reveal\nintrinsic vulnerabilities in MAD architectures and underscore the urgent need\nfor robust, specialized defenses prior to real-world deployment."
    },
    {
        "date": "2025-04",
        "title": "Seeking Flat Minima over Diverse Surrogates for Improved Adversarial Transferability: A Theoretical Framework and Algorithmic Instantiation",
        "author": "Meixi Zheng, Kehan Wu, Yanbo Fan, Rui Huang, and Baoyuan Wu",
        "link": "http://arxiv.org/abs/2504.16474v1",
        "abstract": "The transfer-based black-box adversarial attack setting poses the challenge\nof crafting an adversarial example (AE) on known surrogate models that remain\neffective against unseen target models. Due to the practical importance of this\ntask, numerous methods have been proposed to address this challenge. However,\nmost previous methods are heuristically designed and intuitively justified,\nlacking a theoretical foundation. To bridge this gap, we derive a novel\ntransferability bound that offers provable guarantees for adversarial\ntransferability. Our theoretical analysis has the advantages of \\textit{(i)}\ndeepening our understanding of previous methods by building a general attack\nframework and \\textit{(ii)} providing guidance for designing an effective\nattack algorithm. Our theoretical results demonstrate that optimizing AEs\ntoward flat minima over the surrogate model set, while controlling the\nsurrogate-target model shift measured by the adversarial model discrepancy,\nyields a comprehensive guarantee for AE transferability. The results further\nlead to a general transfer-based attack framework, within which we observe that\nprevious methods consider only partial factors contributing to the\ntransferability. Algorithmically, inspired by our theoretical results, we first\nelaborately construct the surrogate model set in which models exhibit diverse\nadversarial vulnerabilities with respect to AEs to narrow an instantiated\nadversarial model discrepancy. Then, a \\textit{model-Diversity-compatible\nReverse Adversarial Perturbation} (DRAP) is generated to effectively promote\nthe flatness of AEs over diverse surrogate models to improve transferability.\nExtensive experiments on NIPS2017 and CIFAR-10 datasets against various target\nmodels demonstrate the effectiveness of our proposed attack."
    },
    {
        "date": "2025-04",
        "title": "MTSGL: Multi-Task Structure Guided Learning for Robust and Interpretable SAR Aircraft Recognition",
        "author": "Qishan He, Lingjun Zhao, Ru Luo, Siqian Zhang, Lin Lei, Kefeng Ji, and Gangyao Kuang",
        "link": "http://arxiv.org/abs/2504.16467v1",
        "abstract": "Aircraft recognition in synthetic aperture radar (SAR) imagery is a\nfundamental mission in both military and civilian applications. Recently deep\nlearning (DL) has emerged a dominant paradigm for its explosive performance on\nextracting discriminative features. However, current classification algorithms\nfocus primarily on learning decision hyperplane without enough comprehension on\naircraft structural knowledge. Inspired by the fined aircraft annotation\nmethods for optical remote sensing images (RSI), we first introduce a\nstructure-based SAR aircraft annotations approach to provide structural and\ncompositional supplement information. On this basis, we propose a multi-task\nstructure guided learning (MTSGL) network for robust and interpretable SAR\naircraft recognition. Besides the classification task, MTSGL includes a\nstructural semantic awareness (SSA) module and a structural consistency\nregularization (SCR) module. The SSA is designed to capture structure semantic\ninformation, which is conducive to gain human-like comprehension of aircraft\nknowledge. The SCR helps maintain the geometric consistency between the\naircraft structure in SAR imagery and the proposed annotation. In this process,\nthe structural attribute can be disentangled in a geometrically meaningful\nmanner. In conclusion, the MTSGL is presented with the expert-level aircraft\nprior knowledge and structure guided learning paradigm, aiming to comprehend\nthe aircraft concept in a way analogous to the human cognitive process.\nExtensive experiments are conducted on a self-constructed multi-task SAR\naircraft recognition dataset (MT-SARD) and the effective results illustrate the\nsuperiority of robustness and interpretation ability of the proposed MTSGL."
    },
    {
        "date": "2025-04",
        "title": "Give LLMs a Security Course: Securing Retrieval-Augmented Code Generation via Knowledge Injection",
        "author": "Bo Lin, Shangwen Wang, Yihao Qin, Liqian Chen, and Xiaoguang Mao",
        "link": "http://arxiv.org/abs/2504.16429v1",
        "abstract": "Retrieval-Augmented Code Generation (RACG) leverages external knowledge to\nenhance Large Language Models (LLMs) in code synthesis, improving the\nfunctional correctness of the generated code. However, existing RACG systems\nlargely overlook security, leading to substantial risks. Especially, the\npoisoning of malicious code into knowledge bases can mislead LLMs, resulting in\nthe generation of insecure outputs, which poses a critical threat in modern\nsoftware development. To address this, we propose a security-hardening\nframework for RACG systems, CodeGuarder, that shifts the paradigm from\nretrieving only functional code examples to incorporating both functional code\nand security knowledge. Our framework constructs a security knowledge base from\nreal-world vulnerability databases, including secure code samples and root\ncause annotations. For each code generation query, a retriever decomposes the\nquery into fine-grained sub-tasks and fetches relevant security knowledge. To\nprioritize critical security guidance, we introduce a re-ranking and filtering\nmechanism by leveraging the LLMs' susceptibility to different vulnerability\ntypes. This filtered security knowledge is seamlessly integrated into the\ngeneration prompt. Our evaluation shows CodeGuarder significantly improves code\nsecurity rates across various LLMs, achieving average improvements of 20.12\\%\nin standard RACG, and 31.53\\% and 21.91\\% under two distinct poisoning\nscenarios without compromising functional correctness. Furthermore, CodeGuarder\ndemonstrates strong generalization, enhancing security even when the targeted\nlanguage's security knowledge is lacking. This work presents CodeGuarder as a\npivotal advancement towards building secure and trustworthy RACG systems."
    },
    {
        "date": "2025-04",
        "title": "VideoMark: A Distortion-Free Robust Watermarking Framework for Video Diffusion Models",
        "author": "Xuming Hu, Hanqian Li, Jungang Li, and Aiwei Liu",
        "link": "http://arxiv.org/abs/2504.16359v1",
        "abstract": "This work presents VideoMark, a training-free robust watermarking framework\nfor video diffusion models. As diffusion models advance in generating highly\nrealistic videos, the need for reliable content attribution mechanisms has\nbecome critical. While watermarking techniques for image diffusion models have\nmade progress, directly extending these methods to videos presents unique\nchallenges due to variable video lengths and vulnerability to temporal attacks.\nVideoMark addresses these limitations through a frame-wise watermarking\nstrategy using pseudorandom error correction (PRC) codes to embed watermark\ninformation during the generation process. Our method generates an extended\nwatermark message sequence and randomly selects starting positions for each\nvideo, ensuring uniform noise distribution in the latent space and maintaining\ngeneration quality. For watermark extraction, we introduce a Temporal Matching\nModule (TMM) that uses edit distance to align decoded messages with the\noriginal watermark sequence, providing robustness against temporal attacks such\nas frame deletion. Experimental results demonstrate that VideoMark achieves\nhigher decoding accuracy than existing methods while maintaining video quality\non par with watermark-free generation. Importantly, our watermark remains\nundetectable to attackers without the secret key, ensuring strong\nimperceptibility compared to other watermarking frameworks. VideoMark provides\na practical solution for content attribution in diffusion-based video\ngeneration without requiring additional training or compromising video quality.\nOur code and data are available at\n\\href{https://github.com/KYRIE-LI11/VideoMark}{https://github.com/KYRIE-LI11/VideoMark}."
    },
    {
        "date": "2025-04",
        "title": "Property-Preserving Hashing for $\\ell_1$-Distance Predicates: Applications to Countering Adversarial Input Attacks",
        "author": "Hassan Asghar, Chenhan Zhang, and Dali Kaafar",
        "link": "http://arxiv.org/abs/2504.16355v1",
        "abstract": "Perceptual hashing is used to detect whether an input image is similar to a\nreference image with a variety of security applications. Recently, they have\nbeen shown to succumb to adversarial input attacks which make small\nimperceptible changes to the input image yet the hashing algorithm does not\ndetect its similarity to the original image. Property-preserving hashing (PPH)\nis a recent construct in cryptography, which preserves some property\n(predicate) of its inputs in the hash domain. Researchers have so far shown\nconstructions of PPH for Hamming distance predicates, which, for instance,\noutputs 1 if two inputs are within Hamming distance $t$. A key feature of PPH\nis its strong correctness guarantee, i.e., the probability that the predicate\nwill not be correctly evaluated in the hash domain is negligible. Motivated by\nthe use case of detecting similar images under adversarial setting, we propose\nthe first PPH construction for an $\\ell_1$-distance predicate. Roughly, this\npredicate checks if the two one-sided $\\ell_1$-distances between two images are\nwithin a threshold $t$. Since many adversarial attacks use $\\ell_2$-distance\n(related to $\\ell_1$-distance) as the objective function to perturb the input\nimage, by appropriately choosing the threshold $t$, we can force the attacker\nto add considerable noise to evade detection, and hence significantly\ndeteriorate the image quality. Our proposed scheme is highly efficient, and\nruns in time $O(t^2)$. For grayscale images of size $28 \\times 28$, we can\nevaluate the predicate in $0.0784$ seconds when pixel values are perturbed by\nup to $1 \\%$. For larger RGB images of size $224 \\times 224$, by dividing the\nimage into 1,000 blocks, we achieve times of $0.0128$ seconds per block for $1\n\\%$ change, and up to $0.2641$ seconds per block for $14\\%$ change."
    },
    {
        "date": "2025-04",
        "title": "Blockchain Meets Adaptive Honeypots: A Trust-Aware Approach to Next-Gen IoT Security",
        "author": "Yazan Otoum, Arghavan Asad, and Amiya Nayak",
        "link": "http://arxiv.org/abs/2504.16226v1",
        "abstract": "Edge computing-based Next-Generation Wireless Networks (NGWN)-IoT offer\nenhanced bandwidth capacity for large-scale service provisioning but remain\nvulnerable to evolving cyber threats. Existing intrusion detection and\nprevention methods provide limited security as adversaries continually adapt\ntheir attack strategies. We propose a dynamic attack detection and prevention\napproach to address this challenge. First, blockchain-based authentication uses\nthe Deoxys Authentication Algorithm (DAA) to verify IoT device legitimacy\nbefore data transmission. Next, a bi-stage intrusion detection system is\nintroduced: the first stage uses signature-based detection via an Improved\nRandom Forest (IRF) algorithm. In contrast, the second stage applies\nfeature-based anomaly detection using a Diffusion Convolution Recurrent Neural\nNetwork (DCRNN). To ensure Quality of Service (QoS) and maintain Service Level\nAgreements (SLA), trust-aware service migration is performed using Heap-Based\nOptimization (HBO). Additionally, on-demand virtual High-Interaction honeypots\ndeceive attackers and extract attack patterns, which are securely stored using\nthe Bimodal Lattice Signature Scheme (BLISS) to enhance signature-based\nIntrusion Detection Systems (IDS). The proposed framework is implemented in the\nNS3 simulation environment and evaluated against existing methods across\nmultiple performance metrics, including accuracy, attack detection rate, false\nnegative rate, precision, recall, ROC curve, memory usage, CPU usage, and\nexecution time. Experimental results demonstrate that the framework\nsignificantly outperforms existing approaches, reinforcing the security of\nNGWN-enabled IoT ecosystems"
    },
    {
        "date": "2025-04",
        "title": "LLMs meet Federated Learning for Scalable and Secure IoT Management",
        "author": "Yazan Otoum, Arghavan Asad, and Amiya Nayak",
        "link": "http://arxiv.org/abs/2504.16032v1",
        "abstract": "The rapid expansion of IoT ecosystems introduces severe challenges in\nscalability, security, and real-time decision-making. Traditional centralized\narchitectures struggle with latency, privacy concerns, and excessive resource\nconsumption, making them unsuitable for modern large-scale IoT deployments.\nThis paper presents a novel Federated Learning-driven Large Language Model\n(FL-LLM) framework, designed to enhance IoT system intelligence while ensuring\ndata privacy and computational efficiency. The framework integrates Generative\nIoT (GIoT) models with a Gradient Sensing Federated Strategy (GSFS),\ndynamically optimizing model updates based on real-time network conditions. By\nleveraging a hybrid edge-cloud processing architecture, our approach balances\nintelligence, scalability, and security in distributed IoT environments.\nEvaluations on the IoT-23 dataset demonstrate that our framework improves model\naccuracy, reduces response latency, and enhances energy efficiency,\noutperforming traditional FL techniques (i.e., FedAvg, FedOpt). These findings\nhighlight the potential of integrating LLM-powered federated learning into\nlarge-scale IoT ecosystems, paving the way for more secure, scalable, and\nadaptive IoT management solutions."
    },
    {
        "date": "2025-04",
        "title": "A New Graph Grammar Formalism for Robust Syntactic Pattern Recognition",
        "author": "Peter Fletcher",
        "link": "http://arxiv.org/abs/2504.15975v2",
        "abstract": "I introduce a formalism for representing the syntax of recursively structured\ngraph-like patterns. It does not use production rules, like a conventional\ngraph grammar, but represents the syntactic structure in a more direct and\ndeclarative way. The grammar and the pattern are both represented as networks,\nand parsing is seen as the construction of a homomorphism from the pattern to\nthe grammar. The grammars can represent iterative, hierarchical and nested\nrecursive structure in more than one dimension.\n  This supports a highly parallel style of parsing, in which all aspects of\npattern recognition (feature detection, segmentation, parsing, filling in\nmissing symbols, top-down and bottom-up inference) are integrated into a single\nprocess, to exploit the synergy between them.\n  The emphasis of this paper is on underlying theoretical issues, but I also\ngive some example runs to illustrate the error-tolerant parsing of complex\nrecursively structured patterns of 50-1000 symbols, involving variability in\ngeometric relationships, blurry and indistinct symbols, overlapping symbols,\ncluttered images, and erased patches."
    },
    {
        "date": "2025-04",
        "title": "Adversarial Observations in Weather Forecasting",
        "author": "Erik Imgrund, Thorsten Eisenhofer, and Konrad Rieck",
        "link": "http://arxiv.org/abs/2504.15942v1",
        "abstract": "AI-based systems, such as Google's GenCast, have recently redefined the state\nof the art in weather forecasting, offering more accurate and timely\npredictions of both everyday weather and extreme events. While these systems\nare on the verge of replacing traditional meteorological methods, they also\nintroduce new vulnerabilities into the forecasting process. In this paper, we\ninvestigate this threat and present a novel attack on autoregressive diffusion\nmodels, such as those used in GenCast, capable of manipulating weather\nforecasts and fabricating extreme events, including hurricanes, heat waves, and\nintense rainfall. The attack introduces subtle perturbations into weather\nobservations that are statistically indistinguishable from natural noise and\nchange less than 0.1% of the measurements - comparable to tampering with data\nfrom a single meteorological satellite. As modern forecasting integrates data\nfrom nearly a hundred satellites and many other sources operated by different\ncountries, our findings highlight a critical security risk with the potential\nto cause large-scale disruptions and undermine public trust in weather\nprediction."
    },
    {
        "date": "2025-04",
        "title": "Human-Imperceptible Physical Adversarial Attack for NIR Face Recognition Models",
        "author": "Songyan Xie, Jinghang Wen, Encheng Su, and Qiucheng Yu",
        "link": "http://arxiv.org/abs/2504.15823v1",
        "abstract": "Near-infrared (NIR) face recognition systems, which can operate effectively\nin low-light conditions or in the presence of makeup, exhibit vulnerabilities\nwhen subjected to physical adversarial attacks. To further demonstrate the\npotential risks in real-world applications, we design a novel, stealthy, and\npractical adversarial patch to attack NIR face recognition systems in a\nblack-box setting. We achieved this by utilizing human-imperceptible\ninfrared-absorbing ink to generate multiple patches with digitally optimized\nshapes and positions for infrared images. To address the optimization mismatch\nbetween digital and real-world NIR imaging, we develop a light reflection model\nfor human skin to minimize pixel-level discrepancies by simulating NIR light\nreflection.\n  Compared to state-of-the-art (SOTA) physical attacks on NIR face recognition\nsystems, the experimental results show that our method improves the attack\nsuccess rate in both digital and physical domains, particularly maintaining\neffectiveness across various face postures. Notably, the proposed approach\noutperforms SOTA methods, achieving an average attack success rate of 82.46% in\nthe physical domain across different models, compared to 64.18% for existing\nmethods. The artifact is available at\nhttps://anonymous.4open.science/r/Human-imperceptible-adversarial-patch-0703/."
    },
    {
        "date": "2025-04",
        "title": "Advancing Embodied Agent Security: From Safety Benchmarks to Input Moderation",
        "author": "Ning Wang, Zihan Yan, Weiyang Li, Chuan Ma, He Chen, and Tao Xiang",
        "link": "http://arxiv.org/abs/2504.15699v1",
        "abstract": "Embodied agents exhibit immense potential across a multitude of domains,\nmaking the assurance of their behavioral safety a fundamental prerequisite for\ntheir widespread deployment. However, existing research predominantly\nconcentrates on the security of general large language models, lacking\nspecialized methodologies for establishing safety benchmarks and input\nmoderation tailored to embodied agents. To bridge this gap, this paper\nintroduces a novel input moderation framework, meticulously designed to\nsafeguard embodied agents. This framework encompasses the entire pipeline,\nincluding taxonomy definition, dataset curation, moderator architecture, model\ntraining, and rigorous evaluation. Notably, we introduce EAsafetyBench, a\nmeticulously crafted safety benchmark engineered to facilitate both the\ntraining and stringent assessment of moderators specifically designed for\nembodied agents. Furthermore, we propose Pinpoint, an innovative\nprompt-decoupled input moderation scheme that harnesses a masked attention\nmechanism to effectively isolate and mitigate the influence of functional\nprompts on moderation tasks. Extensive experiments conducted on diverse\nbenchmark datasets and models validate the feasibility and efficacy of the\nproposed approach. The results demonstrate that our methodologies achieve an\nimpressive average detection accuracy of 94.58%, surpassing the performance of\nexisting state-of-the-art techniques, alongside an exceptional moderation\nprocessing time of merely 0.002 seconds per instance."
    },
    {
        "date": "2025-04",
        "title": "TrojanDam: Detection-Free Backdoor Defense in Federated Learning through Proactive Model Robustification utilizing OOD Data",
        "author": "Yanbo Dai, Songze Li, Zihan Gan, and Xueluan Gong",
        "link": "http://arxiv.org/abs/2504.15674v1",
        "abstract": "Federated learning (FL) systems allow decentralized data-owning clients to\njointly train a global model through uploading their locally trained updates to\na centralized server. The property of decentralization enables adversaries to\ncraft carefully designed backdoor updates to make the global model misclassify\nonly when encountering adversary-chosen triggers. Existing defense mechanisms\nmainly rely on post-training detection after receiving updates. These methods\neither fail to identify updates which are deliberately fabricated statistically\nclose to benign ones, or show inconsistent performance in different FL training\nstages. The effect of unfiltered backdoor updates will accumulate in the global\nmodel, and eventually become functional. Given the difficulty of ruling out\nevery backdoor update, we propose a backdoor defense paradigm, which focuses on\nproactive robustification on the global model against potential backdoor\nattacks. We first reveal that the successful launching of backdoor attacks in\nFL stems from the lack of conflict between malicious and benign updates on\nredundant neurons of ML models. We proceed to prove the feasibility of\nactivating redundant neurons utilizing out-of-distribution (OOD) samples in\ncentralized settings, and migrating to FL settings to propose a novel backdoor\ndefense mechanism, TrojanDam. The proposed mechanism has the FL server\ncontinuously inject fresh OOD mappings into the global model to activate\nredundant neurons, canceling the effect of backdoor updates during aggregation.\nWe conduct systematic and extensive experiments to illustrate the superior\nperformance of TrojanDam, over several SOTA backdoor defense methods across a\nwide range of FL settings."
    },
    {
        "date": "2025-04",
        "title": "Analytical Softmax Temperature Setting from Feature Dimensions for Model- and Domain-Robust Classification",
        "author": "Tatsuhito Hasegawa, and Shunsuke Sakai",
        "link": "http://arxiv.org/abs/2504.15594v1",
        "abstract": "In deep learning-based classification tasks, the softmax function's\ntemperature parameter $T$ critically influences the output distribution and\noverall performance. This study presents a novel theoretical insight that the\noptimal temperature $T^*$ is uniquely determined by the dimensionality of the\nfeature representations, thereby enabling training-free determination of $T^*$.\nDespite this theoretical grounding, empirical evidence reveals that $T^*$\nfluctuates under practical conditions owing to variations in models, datasets,\nand other confounding factors. To address these influences, we propose and\noptimize a set of temperature determination coefficients that specify how $T^*$\nshould be adjusted based on the theoretical relationship to feature\ndimensionality. Additionally, we insert a batch normalization layer immediately\nbefore the output layer, effectively stabilizing the feature space. Building on\nthese coefficients and a suite of large-scale experiments, we develop an\nempirical formula to estimate $T^*$ without additional training while also\nintroducing a corrective scheme to refine $T^*$ based on the number of classes\nand task complexity. Our findings confirm that the derived temperature not only\naligns with the proposed theoretical perspective but also generalizes\neffectively across diverse tasks, consistently enhancing classification\nperformance and offering a practical, training-free solution for determining\n$T^*$."
    },
    {
        "date": "2025-04",
        "title": "T2VShield: Model-Agnostic Jailbreak Defense for Text-to-Video Models",
        "author": "Siyuan Liang, Jiayang Liu, Jiecheng Zhai, Tianmeng Fang, Rongcheng Tu, Aishan Liu, Xiaochun Cao, and Dacheng Tao",
        "link": "http://arxiv.org/abs/2504.15512v1",
        "abstract": "The rapid development of generative artificial intelligence has made text to\nvideo models essential for building future multimodal world simulators.\nHowever, these models remain vulnerable to jailbreak attacks, where specially\ncrafted prompts bypass safety mechanisms and lead to the generation of harmful\nor unsafe content. Such vulnerabilities undermine the reliability and security\nof simulation based applications. In this paper, we propose T2VShield, a\ncomprehensive and model agnostic defense framework designed to protect text to\nvideo models from jailbreak threats. Our method systematically analyzes the\ninput, model, and output stages to identify the limitations of existing\ndefenses, including semantic ambiguities in prompts, difficulties in detecting\nmalicious content in dynamic video outputs, and inflexible model centric\nmitigation strategies. T2VShield introduces a prompt rewriting mechanism based\non reasoning and multimodal retrieval to sanitize malicious inputs, along with\na multi scope detection module that captures local and global inconsistencies\nacross time and modalities. The framework does not require access to internal\nmodel parameters and works with both open and closed source systems. Extensive\nexperiments on five platforms show that T2VShield can reduce jailbreak success\nrates by up to 35 percent compared to strong baselines. We further develop a\nhuman centered audiovisual evaluation protocol to assess perceptual safety,\nemphasizing the importance of visual level defense in enhancing the\ntrustworthiness of next generation multimodal simulators."
    },
    {
        "date": "2025-04",
        "title": "Unifying Image Counterfactuals and Feature Attributions with Latent-Space Adversarial Attacks",
        "author": "Jeremy Goldwasser, and Giles Hooker",
        "link": "http://arxiv.org/abs/2504.15479v1",
        "abstract": "Counterfactuals are a popular framework for interpreting machine learning\npredictions. These what if explanations are notoriously challenging to create\nfor computer vision models: standard gradient-based methods are prone to\nproduce adversarial examples, in which imperceptible modifications to image\npixels provoke large changes in predictions. We introduce a new,\neasy-to-implement framework for counterfactual images that can flexibly adapt\nto contemporary advances in generative modeling. Our method, Counterfactual\nAttacks, resembles an adversarial attack on the representation of the image\nalong a low-dimensional manifold. In addition, given an auxiliary dataset of\nimage descriptors, we show how to accompany counterfactuals with feature\nattribution that quantify the changes between the original and counterfactual\nimages. These importance scores can be aggregated into global counterfactual\nexplanations that highlight the overall features driving model predictions.\nWhile this unification is possible for any counterfactual method, it has\nparticular computational efficiency for ours. We demonstrate the efficacy of\nour approach with the MNIST and CelebA datasets."
    },
    {
        "date": "2025-04",
        "title": "Improving Human-AI Coordination through Adversarial Training and Generative Models",
        "author": "Paresh Chaudhary, Yancheng Liang, Daphne Chen, Simon S. Du, and Natasha Jaques",
        "link": "http://arxiv.org/abs/2504.15457v1",
        "abstract": "Being able to cooperate with new people is an important component of many\neconomically valuable AI tasks, from household robotics to autonomous driving.\nHowever, generalizing to novel humans requires training on data that captures\nthe diversity of human behaviors. Adversarial training is one avenue for\nsearching for such data and ensuring that agents are robust. However, it is\ndifficult to apply in the cooperative setting because adversarial policies\nintentionally learn to sabotage the task instead of simulating valid\ncooperation partners. To address this challenge, we propose a novel strategy\nfor overcoming self-sabotage that combines a pre-trained generative model to\nsimulate valid cooperative agent policies with adversarial training to maximize\nregret. We call our method GOAT: Generative Online Adversarial Training. In\nthis framework, the GOAT dynamically searches for and generates coordination\nstrategies where the learning policy -- the Cooperator agent -- underperforms.\nGOAT enables better generalization by exposing the Cooperator to various\nchallenging interaction scenarios. We maintain realistic coordination\nstrategies by updating only the generative model's embedding while keeping its\nparameters frozen, thus avoiding adversarial exploitation. We evaluate GOAT\nwith real human partners, and the results demonstrate state-of-the-art\nperformance on the Overcooked benchmark, highlighting its effectiveness in\ngeneralizing to diverse human behaviors."
    },
    {
        "date": "2025-04",
        "title": "Valkyrie: A Response Framework to Augment Runtime Detection of Time-Progressive Attacks",
        "author": "Nikhilesh Singh, and Chester Rebeiro",
        "link": "http://arxiv.org/abs/2504.15447v1",
        "abstract": "A popular approach to detect cyberattacks is to monitor systems in real-time\nto identify malicious activities as they occur. While these solutions aim to\ndetect threats early, minimizing damage, they suffer from a significant\nchallenge due to the presence of false positives. False positives have a\ndetrimental impact on computer systems, which can lead to interruptions of\nlegitimate operations and reduced productivity. Most contemporary works tend to\nuse advanced Machine Learning and AI solutions to address this challenge.\nUnfortunately, false positives can, at best, be reduced but not eliminated.\n  In this paper, we propose an alternate approach that focuses on reducing the\nimpact of false positives rather than eliminating them. We introduce Valkyrie,\na framework that can enhance any existing runtime detector with a\npost-detection response. Valkyrie is designed for time-progressive attacks,\nsuch as micro-architectural attacks, rowhammer, ransomware, and cryptominers,\nthat achieve their objectives incrementally using system resources. As soon as\nan attack is detected, Valkyrie limits the allocated computing resources,\nthrottling the attack, until the detector's confidence is sufficiently high to\nwarrant a more decisive action. For a false positive, limiting the system\nresources only results in a small increase in execution time. On average, the\nslowdown incurred due to false positives is less than 1% for single-threaded\nprograms and 6.7% for multi-threaded programs. On the other hand, attacks like\nrowhammer are prevented, while the potency of micro-architectural attacks,\nransomware, and cryptominers is greatly reduced."
    },
    {
        "date": "2025-04",
        "title": "FLARE: Feature-based Lightweight Aggregation for Robust Evaluation of IoT Intrusion Detection",
        "author": "Bradley Boswell, Seth Barrett, Swarnamugi Rajaganapathy, and Gokila Dorai",
        "link": "http://arxiv.org/abs/2504.15375v1",
        "abstract": "The proliferation of Internet of Things (IoT) devices has expanded the attack\nsurface, necessitating efficient intrusion detection systems (IDSs) for network\nprotection. This paper presents FLARE, a feature-based lightweight aggregation\nfor robust evaluation of IoT intrusion detection to address the challenges of\nsecuring IoT environments through feature aggregation techniques. FLARE\nutilizes a multilayered processing approach, incorporating session, flow, and\ntime-based sliding-window data aggregation to analyze network behavior and\ncapture vital features from IoT network traffic data. We perform extensive\nevaluations on IoT data generated from our laboratory experimental setup to\nassess the effectiveness of the proposed aggregation technique. To classify\nattacks in IoT IDS, we employ four supervised learning models and two deep\nlearning models. We validate the performance of these models in terms of\naccuracy, precision, recall, and F1-score. Our results reveal that\nincorporating the FLARE aggregation technique as a foundational step in feature\nengineering, helps lay a structured representation, and enhances the\nperformance of complex end-to-end models, making it a crucial step in IoT IDS\npipeline. Our findings highlight the potential of FLARE as a valuable technique\nto improve performance and reduce computational costs of end-to-end IDS\nimplementations, thereby fostering more robust IoT intrusion detection systems."
    },
    {
        "date": "2025-04",
        "title": "Existing Industry Practice for the EU AI Act's General-Purpose AI Code of Practice Safety and Security Measures",
        "author": "Lily Stelling, Mick Yang, Rokas Gipi\u0161kis, Leon Staufer, Ze Shen Chin, Sim\u00e9on Campos, and Michael Chen",
        "link": "http://arxiv.org/abs/2504.15181v1",
        "abstract": "This report provides a detailed comparison between the measures proposed in\nthe EU AI Act's General-Purpose AI (GPAI) Code of Practice (Third Draft) and\ncurrent practices adopted by leading AI companies. As the EU moves toward\nenforcing binding obligations for GPAI model providers, the Code of Practice\nwill be key to bridging legal requirements with concrete technical commitments.\nOur analysis focuses on the draft's Safety and Security section which is only\nrelevant for the providers of the most advanced models (Commitments II.1-II.16)\nand excerpts from current public-facing documents quotes that are relevant to\neach individual measure.\n  We systematically reviewed different document types - including companies'\nfrontier safety frameworks and model cards - from over a dozen companies,\nincluding OpenAI, Anthropic, Google DeepMind, Microsoft, Meta, Amazon, and\nothers. This report is not meant to be an indication of legal compliance nor\ndoes it take any prescriptive viewpoint about the Code of Practice or\ncompanies' policies. Instead, it aims to inform the ongoing dialogue between\nregulators and GPAI model providers by surfacing evidence of precedent."
    },
    {
        "date": "2025-04",
        "title": "GIFDL: Generated Image Fluctuation Distortion Learning for Enhancing Steganographic Security",
        "author": "Xiangkun Wang, Kejiang Chen, Yuang Qi, Ruiheng Liu, Weiming Zhang, and Nenghai Yu",
        "link": "http://arxiv.org/abs/2504.15139v1",
        "abstract": "Minimum distortion steganography is currently the mainstream method for\nmodification-based steganography. A key issue in this method is how to define\nsteganographic distortion. With the rapid development of deep learning\ntechnology, the definition of distortion has evolved from manual design to deep\nlearning design. Concurrently, rapid advancements in image generation have made\ngenerated images viable as cover media. However, existing distortion design\nmethods based on machine learning do not fully leverage the advantages of\ngenerated cover media, resulting in suboptimal security performance. To address\nthis issue, we propose GIFDL (Generated Image Fluctuation Distortion Learning),\na steganographic distortion learning method based on the fluctuations in\ngenerated images. Inspired by the idea of natural steganography, we take a\nseries of highly similar fluctuation images as the input to the steganographic\ndistortion generator and introduce a new GAN training strategy to disguise\nstego images as fluctuation images. Experimental results demonstrate that\nGIFDL, compared with state-of-the-art GAN-based distortion learning methods,\nexhibits superior resistance to steganalysis, increasing the detection error\nrates by an average of 3.30% across three steganalyzers."
    },
    {
        "date": "2025-04",
        "title": "Robust and Real-time Surface Normal Estimation from Stereo Disparities using Affine Transformations",
        "author": "Csongor Csanad Kariko, Muhammad Rafi Faisal, and Levente Hajder",
        "link": "http://arxiv.org/abs/2504.15121v1",
        "abstract": "This work introduces a novel method for surface normal estimation from\nrectified stereo image pairs, leveraging affine transformations derived from\ndisparity values to achieve fast and accurate results. We demonstrate how the\nrectification of stereo image pairs simplifies the process of surface normal\nestimation by reducing computational complexity. To address noise reduction, we\ndevelop a custom algorithm inspired by convolutional operations, tailored to\nprocess disparity data efficiently. We also introduce adaptive heuristic\ntechniques for efficiently detecting connected surface components within the\nimages, further improving the robustness of the method. By integrating these\nmethods, we construct a surface normal estimator that is both fast and\naccurate, producing a dense, oriented point cloud as the final output. Our\nmethod is validated using both simulated environments and real-world stereo\nimages from the Middlebury and Cityscapes datasets, demonstrating significant\nimprovements in real-time performance and accuracy when implemented on a GPU.\nUpon acceptance, the shader source code will be made publicly available to\nfacilitate further research and reproducibility."
    },
    {
        "date": "2025-04",
        "title": "Fast-Slow Co-advancing Optimizer: Toward Harmonious Adversarial Training of GAN",
        "author": "Lin Wang, Xiancheng Wang, Rui Wang, Zhibo Zhang, and Minghang Zhao",
        "link": "http://arxiv.org/abs/2504.15099v1",
        "abstract": "Up to now, the training processes of typical Generative Adversarial Networks\n(GANs) are still particularly sensitive to data properties and hyperparameters,\nwhich may lead to severe oscillations, difficulties in convergence, or even\nfailures to converge, especially when the overall variances of the training\nsets are large. These phenomena are often attributed to the training\ncharacteristics of such networks. Aiming at the problem, this paper develops a\nnew intelligent optimizer, Fast-Slow Co-advancing Optimizer (FSCO), which\nemploys reinforcement learning in the training process of GANs to make training\neasier. Specifically, this paper allows the training step size to be controlled\nby an agent to improve training stability, and makes the training process more\nintelligent with variable learning rates, making GANs less sensitive to step\nsize. Experiments have been conducted on three benchmark datasets to verify the\neffectiveness of the developed FSCO."
    },
    {
        "date": "2025-04",
        "title": "SOLIDO: A Robust Watermarking Method for Speech Synthesis via Low-Rank Adaptation",
        "author": "Yue Li, Weizhi Liu, and Dongdong Lin",
        "link": "http://arxiv.org/abs/2504.15035v1",
        "abstract": "The accelerated advancement of speech generative models has given rise to\nsecurity issues, including model infringement and unauthorized abuse of\ncontent. Although existing generative watermarking techniques have proposed\ncorresponding solutions, most methods require substantial computational\noverhead and training costs. In addition, some methods have limitations in\nrobustness when handling variable-length inputs. To tackle these challenges, we\npropose \\textsc{SOLIDO}, a novel generative watermarking method that integrates\nparameter-efficient fine-tuning with speech watermarking through low-rank\nadaptation (LoRA) for speech diffusion models. Concretely, the watermark\nencoder converts the watermark to align with the input of diffusion models. To\nachieve precise watermark extraction from variable-length inputs, the watermark\ndecoder based on depthwise separable convolution is designed for watermark\nrecovery. To further enhance speech generation performance and watermark\nextraction capability, we propose a speech-driven lightweight fine-tuning\nstrategy, which reduces computational overhead through LoRA. Comprehensive\nexperiments demonstrate that the proposed method ensures high-fidelity\nwatermarked speech even at a large capacity of 2000 bps. Furthermore, against\ncommon individual and compound speech attacks, our SOLIDO achieves a maximum\naverage extraction accuracy of 99.20\\% and 98.43\\%, respectively. It surpasses\nother state-of-the-art methods by nearly 23\\% in resisting time-stretching\nattacks."
    },
    {
        "date": "2025-04",
        "title": "aiXamine: Simplified LLM Safety and Security",
        "author": "Fatih Deniz, Dorde Popovic, Yazan Boshmaf, Euisuh Jeong, Minhaj Ahmad, Sanjay Chawla, and Issa Khalil",
        "link": "http://arxiv.org/abs/2504.14985v2",
        "abstract": "Evaluating Large Language Models (LLMs) for safety and security remains a\ncomplex task, often requiring users to navigate a fragmented landscape of ad\nhoc benchmarks, datasets, metrics, and reporting formats. To address this\nchallenge, we present aiXamine, a comprehensive black-box evaluation platform\nfor LLM safety and security. aiXamine integrates over 40 tests (i.e.,\nbenchmarks) organized into eight key services targeting specific dimensions of\nsafety and security: adversarial robustness, code security, fairness and bias,\nhallucination, model and data privacy, out-of-distribution (OOD) robustness,\nover-refusal, and safety alignment. The platform aggregates the evaluation\nresults into a single detailed report per model, providing a detailed breakdown\nof model performance, test examples, and rich visualizations. We used aiXamine\nto assess over 50 publicly available and proprietary LLMs, conducting over 2K\nexaminations. Our findings reveal notable vulnerabilities in leading models,\nincluding susceptibility to adversarial attacks in OpenAI's GPT-4o, biased\noutputs in xAI's Grok-3, and privacy weaknesses in Google's Gemini 2.0.\nAdditionally, we observe that open-source models can match or exceed\nproprietary models in specific services such as safety alignment, fairness and\nbias, and OOD robustness. Finally, we identify trade-offs between distillation\nstrategies, model size, training methods, and architectural choices."
    },
    {
        "date": "2025-04",
        "title": "A Security Framework for General Blockchain Layer 2 Protocols",
        "author": "Zeta Avarikioti, Matteo Maffei, and Yuheng Wang",
        "link": "http://arxiv.org/abs/2504.14965v1",
        "abstract": "Layer 2 (L2) solutions are the cornerstone of blockchain scalability,\nenabling high-throughput and low-cost interactions by shifting execution\noff-chain while maintaining security through interactions with the underlying\nledger. Despite their common goals, the principal L2 paradigms -- payment\nchannels, rollups, and sidechains -- differ substantially in architecture and\nassumptions, making it difficult to comparatively analyze their security and\ntrade-offs.\n  To address this, we present the first general security framework for L2\nprotocols. Our framework is based on the IITM-based Universal Composability\n(iUC) framework, in which L2 protocols are modeled as stateful machines\ninteracting with higher-level protocol users and the underlying ledger. The\nmethodology defines a generic execution environment that captures ledger\nevents, message passing, and adversarial scheduling, and characterizes security\nthrough trace-based predicates parameterized by adversarial capabilities and\ntiming assumptions. By abstracting away from protocol-specific details while\npreserving critical interface and execution behavior, the framework enables\nmodular, protocol-agnostic reasoning and composable security proofs across a\nwide range of L2 constructions.\n  To demonstrate its applicability, we analyze an example from each of the\nthree dominant L2 scaling paradigms: a payment channel (Brick), a sidechain\n(Liquid Network), and a rollup (Arbitrum). By instantiating each within our\nframework, we derive their security properties and expose trade-offs. These\ninclude the time for dispute resolution, distribution of off-chain storage and\ncomputation, and varying trust assumptions (e.g., reliance on honest parties or\ndata availability). Our framework unifies the analysis of diverse L2 designs\nand pinpoints their strengths and limitations, providing a foundation for\nsecure, systematic L2 development."
    },
    {
        "date": "2025-04",
        "title": "Fast Adversarial Training with Weak-to-Strong Spatial-Temporal Consistency in the Frequency Domain on Videos",
        "author": "Songping Wang, Hanqing Liu, Yueming Lyu, Xiantao Hu, Ziwen He, Wei Wang, Caifeng Shan, and Liang Wang",
        "link": "http://arxiv.org/abs/2504.14921v2",
        "abstract": "Adversarial Training (AT) has been shown to significantly enhance adversarial\nrobustness via a min-max optimization approach. However, its effectiveness in\nvideo recognition tasks is hampered by two main challenges. First, fast\nadversarial training for video models remains largely unexplored, which\nseverely impedes its practical applications. Specifically, most video\nadversarial training methods are computationally costly, with long training\ntimes and high expenses. Second, existing methods struggle with the trade-off\nbetween clean accuracy and adversarial robustness. To address these challenges,\nwe introduce Video Fast Adversarial Training with Weak-to-Strong consistency\n(VFAT-WS), the first fast adversarial training method for video data.\nSpecifically, VFAT-WS incorporates the following key designs: First, it\nintegrates a straightforward yet effective temporal frequency augmentation\n(TF-AUG), and its spatial-temporal enhanced form STF-AUG, along with a\nsingle-step PGD attack to boost training efficiency and robustness. Second, it\ndevises a weak-to-strong spatial-temporal consistency regularization, which\nseamlessly integrates the simpler TF-AUG and the more complex STF-AUG.\nLeveraging the consistency regularization, it steers the learning process from\nsimple to complex augmentations. Both of them work together to achieve a better\ntrade-off between clean accuracy and robustness. Extensive experiments on\nUCF-101 and HMDB-51 with both CNN and Transformer-based models demonstrate that\nVFAT-WS achieves great improvements in adversarial robustness and corruption\nrobustness, while accelerating training by nearly 490%."
    },
    {
        "date": "2025-04",
        "title": "Protecting Your Voice: Temporal-aware Robust Watermarking",
        "author": "Yue Li, Weizhi Liu, and Dongdong Lin",
        "link": "http://arxiv.org/abs/2504.14832v1",
        "abstract": "The rapid advancement of generative models has led to the synthesis of\nreal-fake ambiguous voices. To erase the ambiguity, embedding watermarks into\nthe frequency-domain features of synthesized voices has become a common\nroutine. However, the robustness achieved by choosing the frequency domain\noften comes at the expense of fine-grained voice features, leading to a loss of\nfidelity. Maximizing the comprehensive learning of time-domain features to\nenhance fidelity while maintaining robustness, we pioneer a\n\\textbf{\\underline{t}}emporal-aware\n\\textbf{\\underline{r}}ob\\textbf{\\underline{u}}st\nwat\\textbf{\\underline{e}}rmarking (\\emph{True}) method for protecting the\nspeech and singing voice."
    },
    {
        "date": "2025-04",
        "title": "DONOD: Robust and Generalizable Instruction Fine-Tuning for LLMs via Model-Intrinsic Dataset Pruning",
        "author": "Jucheng Hu, Surong Yang, Dongzhan Zhou, and Lijun Wu",
        "link": "http://arxiv.org/abs/2504.14810v1",
        "abstract": "Ad-hoc instruction fine-tuning of large language models (LLMs) is widely\nadopted for domain-specific adaptation. While domain-specific supervised\nfine-tuning (SFT) is effective and efficient, it often weakens cross-domain\ngeneralization and struggles with noisy training data. To address these\nchallenges, we propose DONOD, a lightweight model-intrinsic data pruning\nmethod. Our approach evaluates data using two model-parameter-based metrics:\nDelta of Norm (DON), which captures the cumulative influence on model weights,\nand Norm of Delta (NOD), which quantifies weight instability. Moreover, by\nemploying the Technique for Order of Preference by Similarity to Ideal Solution\n(TOPSIS) algorithm, we effectively filter noisy, unlearnable, and\ngeneralization-harming samples without relying on auxiliary models during the\nSFT process. Experiments on mathematical tasks demonstrate that data selected\nby DONOD achieve superior fine-tuning efficiency and improved robustness\nagainst noisy data. By filtering out 70% of the full dataset, we improve\ntarget-domain accuracy by 14.90% and cross-domain accuracy by 5.67%. Meanwhile,\nour selected data present superior cross-architecture generalization. Data\npruned by smaller models (e.g., Llama 3.1-8B) generalize effectively on larger\nmodels (e.g., Llama 2-13B). Compared to existing related methodologies, DONOD\ndemonstrates comparable or superior performance while remaining\ndataset-agnostic, enabling broader applicability."
    },
    {
        "date": "2025-04",
        "title": "Verifying Robust Unlearning: Probing Residual Knowledge in Unlearned Models",
        "author": "Hao Xuan, and Xingyu Li",
        "link": "http://arxiv.org/abs/2504.14798v1",
        "abstract": "Machine Unlearning (MUL) is crucial for privacy protection and content\nregulation, yet recent studies reveal that traces of forgotten information\npersist in unlearned models, enabling adversaries to resurface removed\nknowledge. Existing verification methods only confirm whether unlearning was\nexecuted, failing to detect such residual information leaks. To address this,\nwe introduce the concept of Robust Unlearning, ensuring models are\nindistinguishable from retraining and resistant to adversarial recovery. To\nempirically evaluate whether unlearning techniques meet this security standard,\nwe propose the Unlearning Mapping Attack (UMA), a post-unlearning verification\nframework that actively probes models for forgotten traces using adversarial\nqueries. Extensive experiments on discriminative and generative tasks show that\nexisting unlearning techniques remain vulnerable, even when passing existing\nverification metrics. By establishing UMA as a practical verification tool,\nthis study sets a new standard for assessing and enhancing machine unlearning\nsecurity."
    },
    {
        "date": "2025-04",
        "title": "Decoupling Identity from Access: Credential Broker Patterns for Secure CI/CD",
        "author": "Surya Teja Avirneni",
        "link": "http://arxiv.org/abs/2504.14761v1",
        "abstract": "Credential brokers offer a way to separate identity from access in CI/CD\nsystems. This paper shows how verifiable identities issued at runtime, such as\nthose from SPIFFE, can be used with brokers to enable short-lived,\npolicy-driven credentials for pipelines and workloads. We walk through\npractical design patterns, including brokers that issue tokens just in time,\napply access policies, and operate across trust domains. These ideas help\nreduce static permissions, improve auditability, and support Zero Trust goals\nin deployment workflows. This is the second paper in a three-part series on\nsecure CI/CD identity architecture."
    },
    {
        "date": "2025-04",
        "title": "LeetCodeDataset: A Temporal Dataset for Robust Evaluation and Efficient Training of Code LLMs",
        "author": "Yunhui Xia, Wei Shen, Yan Wang, Jason Klein Liu, Huifeng Sun, Siyue Wu, Jian Hu, and Xiaolong Xu",
        "link": "http://arxiv.org/abs/2504.14655v1",
        "abstract": "We introduce LeetCodeDataset, a high-quality benchmark for evaluating and\ntraining code-generation models, addressing two key challenges in LLM research:\nthe lack of reasoning-focused coding benchmarks and self-contained training\ntestbeds. By curating LeetCode Python problems with rich metadata, broad\ncoverage, 100+ test cases per problem, and temporal splits (pre/post July\n2024), our dataset enables contamination-free evaluation and efficient\nsupervised fine-tuning (SFT). Experiments show reasoning models significantly\noutperform non-reasoning counterparts, while SFT with only 2.6K model-generated\nsolutions achieves performance comparable to 110K-sample counterparts. The\ndataset and evaluation framework are available on Hugging Face and Github."
    },
    {
        "date": "2025-04",
        "title": "SMTT: Novel Structured Multi-task Tracking with Graph-Regularized Sparse Representation for Robust Thermal Infrared Target Tracking",
        "author": "Shang Zhang, HuiPan Guan, XiaoBo Ding, Ruoyan Xiong, and Yue Zhang",
        "link": "http://arxiv.org/abs/2504.14566v1",
        "abstract": "Thermal infrared target tracking is crucial in applications such as\nsurveillance, autonomous driving, and military operations. In this paper, we\npropose a novel tracker, SMTT, which effectively addresses common challenges in\nthermal infrared imagery, such as noise, occlusion, and rapid target motion, by\nleveraging multi-task learning, joint sparse representation, and adaptive graph\nregularization. By reformulating the tracking task as a multi-task learning\nproblem, the SMTT tracker independently optimizes the representation of each\nparticle while dynamically capturing spatial and feature-level similarities\nusing a weighted mixed-norm regularization strategy. To ensure real-time\nperformance, we incorporate the Accelerated Proximal Gradient method for\nefficient optimization. Extensive experiments on benchmark datasets - including\nVOT-TIR, PTB-TIR, and LSOTB-TIR - demonstrate that SMTT achieves superior\naccuracy, robustness, and computational efficiency. These results highlight\nSMTT as a reliable and high-performance solution for thermal infrared target\ntracking in complex environments."
    },
    {
        "date": "2025-04",
        "title": "Towards Model Resistant to Transferable Adversarial Examples via Trigger Activation",
        "author": "Yi Yu, Song Xia, Xun Lin, Chenqi Kong, Wenhan Yang, Shijian Lu, Yap-Peng Tan, and Alex C. Kot",
        "link": "http://arxiv.org/abs/2504.14541v1",
        "abstract": "Adversarial examples, characterized by imperceptible perturbations, pose\nsignificant threats to deep neural networks by misleading their predictions. A\ncritical aspect of these examples is their transferability, allowing them to\ndeceive {unseen} models in black-box scenarios. Despite the widespread\nexploration of defense methods, including those on transferability, they show\nlimitations: inefficient deployment, ineffective defense, and degraded\nperformance on clean images. In this work, we introduce a novel training\nparadigm aimed at enhancing robustness against transferable adversarial\nexamples (TAEs) in a more efficient and effective way. We propose a model that\nexhibits random guessing behavior when presented with clean data\n$\\boldsymbol{x}$ as input, and generates accurate predictions when with\ntriggered data $\\boldsymbol{x}+\\boldsymbol{\\tau}$. Importantly, the trigger\n$\\boldsymbol{\\tau}$ remains constant for all data instances. We refer to these\nmodels as \\textbf{models with trigger activation}. We are surprised to find\nthat these models exhibit certain robustness against TAEs. Through the\nconsideration of first-order gradients, we provide a theoretical analysis of\nthis robustness. Moreover, through the joint optimization of the learnable\ntrigger and the model, we achieve improved robustness to transferable attacks.\nExtensive experiments conducted across diverse datasets, evaluating a variety\nof attacking methods, underscore the effectiveness and superiority of our\napproach."
    },
    {
        "date": "2025-04",
        "title": "Breaking the Prompt Wall (I): A Real-World Case Study of Attacking ChatGPT via Lightweight Prompt Injection",
        "author": "Xiangyu Chang, Guang Dai, Hao Di, and Haishan Ye",
        "link": "http://arxiv.org/abs/2504.16125v1",
        "abstract": "This report presents a real-world case study demonstrating how prompt\ninjection can attack large language model platforms such as ChatGPT according\nto a proposed injection framework. By providing three real-world examples, we\nshow how adversarial prompts can be injected via user inputs, web-based\nretrieval, and system-level agent instructions. These attacks, though\nlightweight and low-cost, can cause persistent and misleading behaviors in LLM\noutputs. Our case study reveals that even commercial-grade LLMs remain\nvulnerable to subtle manipulations that bypass safety filters and influence\nuser decisions. \\textbf{More importantly, we stress that this report is not\nintended as an attack guide, but as a technical alert. As ethical researchers,\nwe aim to raise awareness and call upon developers, especially those at OpenAI,\nto treat prompt-level security as a critical design priority."
    },
    {
        "date": "2025-04",
        "title": "Vision-Centric Representation-Efficient Fine-Tuning for Robust Universal Foreground Segmentation",
        "author": "Guoyi Zhang, Siyang Chen, Guangsheng Xu, Han Wang, and Xiaohu Zhang",
        "link": "http://arxiv.org/abs/2504.14481v1",
        "abstract": "Foreground segmentation is crucial for scene understanding, yet\nparameter-efficient fine-tuning (PEFT) of vision foundation models (VFMs) often\nfails in complex scenarios, such as camouflage and infrared imagery. We\nattribute this challenge to the inherent texture bias in VFMs, which is\nexacerbated during fine-tuning and limits generalization in texture-sparse\nenvironments. To address this, we propose Ladder Shape-bias Representation\nSide-tuning (LSR-ST), a lightweight PEFT framework that enhances model\nrobustness by introducing shape-biased inductive priors. LSR-ST captures\nshape-aware features using a simple HDConv Block, which integrates large-kernel\nattention and residual learning. The method satisfies three key conditions for\ninducing shape bias: large receptive fields, multi-order feature interactions,\nand sparse connectivity. Our analysis reveals that these improvements stem from\nrepresentation efficiency-the ability to extract task-relevant, structurally\ngrounded features while minimizing redundancy. We formalize this concept via\nInformation Bottleneck theory and advocate for it as a key PEFT objective.\nUnlike traditional NLP paradigms that focus on optimizing parameters and\nmemory, visual tasks require models that extract task-defined semantics, rather\nthan just relying on pre-encoded features. This shift enables our approach to\nmove beyond conventional trade-offs, offering more robust and generalizable\nsolutions for vision tasks. With minimal changes to SAM2-UNet, LSR-ST achieves\nconsistent improvements across 17 datasets and 6 tasks using only 4.719M\ntrainable parameters. These results highlight the potential of representation\nefficiency for robust and adaptable VFMs within complex visual environments."
    },
    {
        "date": "2025-04",
        "title": "Causal Disentanglement for Robust Long-tail Medical Image Generation",
        "author": "Weizhi Nie, Zichun Zhang, Weijie Wang, Bruno Lepri, Anan Liu, and Nicu Sebe",
        "link": "http://arxiv.org/abs/2504.14450v2",
        "abstract": "Counterfactual medical image generation effectively addresses data scarcity\nand enhances the interpretability of medical images. However, due to the\ncomplex and diverse pathological features of medical images and the imbalanced\nclass distribution in medical data, generating high-quality and diverse medical\nimages from limited data is significantly challenging. Additionally, to fully\nleverage the information in limited data, such as anatomical structure\ninformation and generate more structurally stable medical images while avoiding\ndistortion or inconsistency. In this paper, in order to enhance the clinical\nrelevance of generated data and improve the interpretability of the model, we\npropose a novel medical image generation framework, which generates independent\npathological and structural features based on causal disentanglement and\nutilizes text-guided modeling of pathological features to regulate the\ngeneration of counterfactual images. First, we achieve feature separation\nthrough causal disentanglement and analyze the interactions between features.\nHere, we introduce group supervision to ensure the independence of pathological\nand identity features. Second, we leverage a diffusion model guided by\npathological findings to model pathological features, enabling the generation\nof diverse counterfactual images. Meanwhile, we enhance accuracy by leveraging\na large language model to extract lesion severity and location from medical\nreports. Additionally, we improve the performance of the latent diffusion model\non long-tailed categories through initial noise optimization."
    },
    {
        "date": "2025-04",
        "title": "Adversarial Attack for RGB-Event based Visual Object Tracking",
        "author": "Qiang Chen, Xiao Wang, Haowen Wang, Bo Jiang, Lin Zhu, Dawei Zhang, Yonghong Tian, and Jin Tang",
        "link": "http://arxiv.org/abs/2504.14423v1",
        "abstract": "Visual object tracking is a crucial research topic in the fields of computer\nvision and multi-modal fusion. Among various approaches, robust visual tracking\nthat combines RGB frames with Event streams has attracted increasing attention\nfrom researchers. While striving for high accuracy and efficiency in tracking,\nit is also important to explore how to effectively conduct adversarial attacks\nand defenses on RGB-Event stream tracking algorithms, yet research in this area\nremains relatively scarce. To bridge this gap, in this paper, we propose a\ncross-modal adversarial attack algorithm for RGB-Event visual tracking. Because\nof the diverse representations of Event streams, and given that Event voxels\nand frames are more commonly used, this paper will focus on these two\nrepresentations for an in-depth study. Specifically, for the RGB-Event voxel,\nwe first optimize the perturbation by adversarial loss to generate RGB frame\nadversarial examples. For discrete Event voxel representations, we propose a\ntwo-step attack strategy, more in detail, we first inject Event voxels into the\ntarget region as initialized adversarial examples, then, conduct a\ngradient-guided optimization by perturbing the spatial location of the Event\nvoxels. For the RGB-Event frame based tracking, we optimize the cross-modal\nuniversal perturbation by integrating the gradient information from multimodal\ndata. We evaluate the proposed approach against attacks on three widely used\nRGB-Event Tracking datasets, i.e., COESOT, FE108, and VisEvent. Extensive\nexperiments show that our method significantly reduces the performance of the\ntracker across numerous datasets in both unimodal and multimodal scenarios. The\nsource code will be released on\nhttps://github.com/Event-AHU/Adversarial_Attack_Defense"
    },
    {
        "date": "2025-04",
        "title": "How Do Mobile Applications Enhance Security? An Exploratory Analysis of Use Cases and Provided Information",
        "author": "Irdin Pekaric, Clemens Sauerwein, Simon Laichner, and Ruth Breu",
        "link": "http://arxiv.org/abs/2504.14421v1",
        "abstract": "The ubiquity of mobile applications has increased dramatically in recent\nyears, opening up new opportunities for cyber attackers and heightening\nsecurity concerns in the mobile ecosystem. As a result, researchers and\npractitioners have intensified their research into improving the security and\nprivacy of mobile applications. At the same time, more and more mobile\napplications have appeared on the market that address the aforementioned\nsecurity issues. However, both academia and industry currently lack a\ncomprehensive overview of these mobile security applications for Android and\niOS platforms, including their respective use cases and the security\ninformation they provide.\n  To address this gap, we systematically collected a total of 410 mobile\napplications from both the App and Play Store. Then, we identified the 20 most\nwidely utilized mobile security applications on both platforms that were\nanalyzed and classified. Our results show six primary use cases and a wide\nrange of security information provided by these applications, thus supporting\nthe core functionalities for ensuring mobile security."
    },
    {
        "date": "2025-04",
        "title": "Quantum-Enhanced Reinforcement Learning for Power Grid Security Assessment",
        "author": "Benjamin M. Peter, and Mert Korkali",
        "link": "http://arxiv.org/abs/2504.14412v1",
        "abstract": "The increasingly challenging task of maintaining power grid security requires\ninnovative solutions. Novel approaches using reinforcement learning (RL) agents\nhave been proposed to help grid operators navigate the massive decision space\nand nonlinear behavior of these complex networks. However, applying RL to power\ngrid security assessment, specifically for combinatorially troublesome\ncontingency analysis problems, has proven difficult to scale. The integration\nof quantum computing into these RL frameworks helps scale by improving\ncomputational efficiency and boosting agent proficiency by leveraging quantum\nadvantages in action exploration and model-based interdependence. To\ndemonstrate a proof-of-concept use of quantum computing for RL agent training\nand simulation, we propose a hybrid agent that runs on quantum hardware using\nIBM's Qiskit Runtime. We also provide detailed insight into the construction of\nparameterized quantum circuits (PQCs) for generating relevant quantum output.\nThis agent's proficiency at maintaining grid stability is demonstrated relative\nto a benchmark model without quantum enhancement using N-k contingency\nanalysis. Additionally, we offer a comparative assessment of the training\nprocedures for RL models integrated with a quantum backend."
    },
    {
        "date": "2025-04",
        "title": "Hydra: An Agentic Reasoning Approach for Enhancing Adversarial Robustness and Mitigating Hallucinations in Vision-Language Models",
        "author": "Chung-En, Yu, Hsuan-Chih, Chen, Brian Jalaian, and Nathaniel D. Bastian",
        "link": "http://arxiv.org/abs/2504.14395v1",
        "abstract": "To develop trustworthy Vision-Language Models (VLMs), it is essential to\naddress adversarial robustness and hallucination mitigation, both of which\nimpact factual accuracy in high-stakes applications such as defense and\nhealthcare. Existing methods primarily focus on either adversarial defense or\nhallucination post-hoc correction, leaving a gap in unified robustness\nstrategies. We introduce \\textbf{Hydra}, an adaptive agentic framework that\nenhances plug-in VLMs through iterative reasoning, structured critiques, and\ncross-model verification, improving both resilience to adversarial\nperturbations and intrinsic model errors. Hydra employs an Action-Critique\nLoop, where it retrieves and critiques visual information, leveraging\nChain-of-Thought (CoT) and In-Context Learning (ICL) techniques to refine\noutputs dynamically. Unlike static post-hoc correction methods, Hydra adapts to\nboth adversarial manipulations and intrinsic model errors, making it robust to\nmalicious perturbations and hallucination-related inaccuracies. We evaluate\nHydra on four VLMs, three hallucination benchmarks, two adversarial attack\nstrategies, and two adversarial defense methods, assessing performance on both\nclean and adversarial inputs. Results show that Hydra surpasses plug-in VLMs\nand state-of-the-art (SOTA) dehallucination methods, even without explicit\nadversarial defenses, demonstrating enhanced robustness and factual\nconsistency. By bridging adversarial resistance and hallucination mitigation,\nHydra provides a scalable, training-free solution for improving the reliability\nof VLMs in real-world applications."
    },
    {
        "date": "2025-04",
        "title": "From Cyber Security Incident Management to Cyber Security Crisis Management in the European Union",
        "author": "Jukka Ruohonen, Kalle Rindell, and Simone Busetti",
        "link": "http://arxiv.org/abs/2504.14220v1",
        "abstract": "Incident management is a classical topic in cyber security. Recently, the\nEuropean Union (EU) has started to consider also the relation between cyber\nsecurity incidents and cyber security crises. These considerations and\npreparations, including those specified in the EU's new cyber security laws,\nconstitute the paper's topic. According to an analysis of the laws and\nassociated policy documents, (i) cyber security crises are equated in the EU to\nlarge-scale cyber security incidents that either exceed a handling capacity of\na single member state or affect at least two member states. For this and other\npurposes, (ii) the new laws substantially increase mandatory reporting about\ncyber security incidents, including but not limited to the large-scale\nincidents. Despite the laws and new governance bodies established by them,\nhowever, (iii) the working of actual cyber security crisis management remains\nunclear particularly at the EU-level. With these policy research results, the\npaper advances the domain of cyber security incident management research by\nelaborating how European law perceives cyber security crises and their relation\nto cyber security incidents, paving the way for many relevant further research\ntopics with practical relevance, whether theoretical, conceptual, or empirical."
    },
    {
        "date": "2025-04",
        "title": "The First VoicePrivacy Attacker Challenge",
        "author": "Natalia Tomashenko, Xiaoxiao Miao, Emmanuel Vincent, and Junichi Yamagishi",
        "link": "http://arxiv.org/abs/2504.14183v1",
        "abstract": "The First VoicePrivacy Attacker Challenge is an ICASSP 2025 SP Grand\nChallenge which focuses on evaluating attacker systems against a set of voice\nanonymization systems submitted to the VoicePrivacy 2024 Challenge. Training,\ndevelopment, and evaluation datasets were provided along with a baseline\nattacker. Participants developed their attacker systems in the form of\nautomatic speaker verification systems and submitted their scores on the\ndevelopment and evaluation data. The best attacker systems reduced the equal\nerror rate (EER) by 25-44% relative w.r.t. the baseline."
    },
    {
        "date": "2025-04",
        "title": "A Data-Centric Approach for Safe and Secure Large Language Models against Threatening and Toxic Content",
        "author": "Chaima Njeh, Ha\u00effa Nakouri, and Fehmi Jaafar",
        "link": "http://arxiv.org/abs/2504.16120v1",
        "abstract": "Large Language Models (LLM) have made remarkable progress, but concerns about\npotential biases and harmful content persist. To address these apprehensions,\nwe introduce a practical solution for ensuring LLM's safe and ethical use. Our\nnovel approach focuses on a post-generation correction mechanism, the\nBART-Corrective Model, which adjusts generated content to ensure safety and\nsecurity. Unlike relying solely on model fine-tuning or prompt engineering, our\nmethod provides a robust data-centric alternative for mitigating harmful\ncontent. We demonstrate the effectiveness of our approach through experiments\non multiple toxic datasets, which show a significant reduction in mean toxicity\nand jail-breaking scores after integration. Specifically, our results show a\nreduction of 15% and 21% in mean toxicity and jail-breaking scores with GPT-4,\na substantial reduction of 28% and 5% with PaLM2, a reduction of approximately\n26% and 23% with Mistral-7B, and a reduction of 11.1% and 19% with Gemma-2b-it.\nThese results demonstrate the potential of our approach to improve the safety\nand security of LLM, making them more suitable for real-world applications."
    },
    {
        "date": "2025-04",
        "title": "Rethinking Target Label Conditioning in Adversarial Attacks: A 2D Tensor-Guided Generative Approach",
        "author": "Hangyu Liu, Bo Peng, Pengxiang Ding, and Donglin Wang",
        "link": "http://arxiv.org/abs/2504.14137v1",
        "abstract": "Compared to single-target adversarial attacks, multi-target attacks have\ngarnered significant attention due to their ability to generate adversarial\nimages for multiple target classes simultaneously. Existing generative\napproaches for multi-target attacks mainly analyze the effect of the use of\ntarget labels on noise generation from a theoretical perspective, lacking\npractical validation and comprehensive summarization. To address this gap, we\nfirst identify and validate that the semantic feature quality and quantity are\ncritical factors affecting the transferability of targeted attacks: 1) Feature\nquality refers to the structural and detailed completeness of the implanted\ntarget features, as deficiencies may result in the loss of key discriminative\ninformation; 2) Feature quantity refers to the spatial sufficiency of the\nimplanted target features, as inadequacy limits the victim model's attention to\nthis feature. Based on these findings, we propose the 2D Tensor-Guided\nAdversarial Fusion (2D-TGAF) framework, which leverages the powerful generative\ncapabilities of diffusion models to encode target labels into two-dimensional\nsemantic tensors for guiding adversarial noise generation. Additionally, we\ndesign a novel masking strategy tailored for the training process, ensuring\nthat parts of the generated noise retain complete semantic information about\nthe target class. Extensive experiments on the standard ImageNet dataset\ndemonstrate that 2D-TGAF consistently surpasses state-of-the-art methods in\nattack success rates, both on normally trained models and across various\ndefense mechanisms."
    },
    {
        "date": "2025-04",
        "title": "Detecting Zero-Day Web Attacks with an Ensemble of LSTM, GRU, and Stacked Autoencoders",
        "author": "Vahid Babaey, and Hamid Reza Faragardi",
        "link": "http://arxiv.org/abs/2504.14122v1",
        "abstract": "The rapid growth in web-based services has significantly increased security\nrisks related to user information, as web-based attacks become increasingly\nsophisticated and prevalent. Traditional security methods frequently struggle\nto detect previously unknown (zero-day) web attacks, putting sensitive user\ndata at significant risk. Additionally, reducing human intervention in web\nsecurity tasks can minimize errors and enhance reliability. This paper\nintroduces an intelligent system designed to detect zero-day web attacks using\na novel one-class ensemble method consisting of three distinct autoencoder\narchitectures: LSTM autoencoder, GRU autoencoder, and stacked autoencoder. Our\napproach employs a novel tokenization strategy to convert normal web requests\ninto structured numeric sequences, enabling the ensemble model to effectively\nidentify anomalous activities by uniquely concatenating and compressing the\nlatent representations from each autoencoder. The proposed method efficiently\ndetects unknown web attacks while effectively addressing common limitations of\nprevious methods, such as high memory consumption and excessive false positive\nrates. Extensive experimental evaluations demonstrate the superiority of our\nproposed ensemble, achieving remarkable detection metrics: 97.58% accuracy,\n97.52% recall, 99.76% specificity, and 99.99% precision, with an exceptionally\nlow false positive rate of 0.2%. These results underscore our method's\nsignificant potential in enhancing real-world web security through accurate and\nreliable detection of web-based attacks."
    },
    {
        "date": "2025-04",
        "title": "DoomArena: A framework for Testing AI Agents Against Evolving Security Threats",
        "author": "Leo Boisvert, Mihir Bansal, Chandra Kiran Reddy Evuru, Gabriel Huang, Abhay Puri, Avinandan Bose, Maryam Fazel, Quentin Cappart, Jason Stanley, Alexandre Lacoste, Alexandre Drouin, and Krishnamurthy Dvijotham",
        "link": "http://arxiv.org/abs/2504.14064v2",
        "abstract": "We present DoomArena, a security evaluation framework for AI agents.\nDoomArena is designed on three principles: 1) It is a plug-in framework and\nintegrates easily into realistic agentic frameworks like BrowserGym (for web\nagents) and $\\tau$-bench (for tool calling agents); 2) It is configurable and\nallows for detailed threat modeling, allowing configuration of specific\ncomponents of the agentic framework being attackable, and specifying targets\nfor the attacker; and 3) It is modular and decouples the development of attacks\nfrom details of the environment in which the agent is deployed, allowing for\nthe same attacks to be applied across multiple environments. We illustrate\nseveral advantages of our framework, including the ability to adapt to new\nthreat models and environments easily, the ability to easily combine several\npreviously published attacks to enable comprehensive and fine-grained security\ntesting, and the ability to analyze trade-offs between various vulnerabilities\nand performance. We apply DoomArena to state-of-the-art (SOTA) web and\ntool-calling agents and find a number of surprising results: 1) SOTA agents\nhave varying levels of vulnerability to different threat models (malicious user\nvs malicious environment), and there is no Pareto dominant agent across all\nthreat models; 2) When multiple attacks are applied to an agent, they often\ncombine constructively; 3) Guardrail model-based defenses seem to fail, while\ndefenses based on powerful SOTA LLMs work better. DoomArena is available at\nhttps://github.com/ServiceNow/DoomArena."
    },
    {
        "date": "2025-04",
        "title": "Prioritizing Security Practice Adoption: Empirical Insights on Software Security Outcomes in the npm Ecosystem",
        "author": "Nusrat Zahan, and Laurie Williams",
        "link": "http://arxiv.org/abs/2504.14026v1",
        "abstract": "Practitioners often struggle with the overwhelming number of security\npractices outlined in cybersecurity frameworks for risk mitigation. Given the\nlimited budget, time, and resources, practitioners want to prioritize the\nadoption of security practices based on empirical evidence. The goal of this\nstudy is to assist practitioners and policymakers in making informed decisions\non which security practices to adopt by evaluating the relationship between\nsoftware security practices and security outcome metrics. The study\ninvestigated the relationship between security practice adoption and security\noutcomes. We selected the OpenSSF Scorecard metrics to automatically measure\nthe adoption of security practices in npm GitHub repositories. We also explored\nsecurity outcome metrics, such as the number of open vulnerabilities\n(Vul_Count), mean time to remediate (MTTR) vulnerabilities in dependencies, and\nmean time to update (MTTU) dependencies. We conducted regression and causal\nanalysis using 12 Scorecard metrics and their aggregated Scorecard score\n(computed by aggregating individual security practice scores) as predictors and\nVul_Count, MTTR, and MTTU as target variables. Our findings show that higher\naggregated Scorecard scores are associated with fewer Vul_Count and shorter\nMTTU, also supported by causal analysis. However, while the regression model\nsuggests shorter MTTR, causal analysis indicates project characteristics likely\ninfluence MTTR direction. Segment analysis shows that larger, newer\nrepositories with more contributors, dependencies, and downloads have shorter\nMTTR. Among individual security practices, Code Review, Maintained status,\nPinned Dependencies, and Branch Protection show strong associations with\nsecurity outcomes; the directionality of these associations varies across\nsecurity outcomes."
    },
    {
        "date": "2025-04",
        "title": "Outlier-Robust Multi-Model Fitting on Quantum Annealers",
        "author": "Saurabh Pandey, Luca Magri, Federica Arrigoni, and Vladislav Golyanik",
        "link": "http://arxiv.org/abs/2504.13836v1",
        "abstract": "Multi-model fitting (MMF) presents a significant challenge in Computer\nVision, particularly due to its combinatorial nature. While recent advancements\nin quantum computing offer promise for addressing NP-hard problems, existing\nquantum-based approaches for model fitting are either limited to a single model\nor consider multi-model scenarios within outlier-free datasets. This paper\nintroduces a novel approach, the robust quantum multi-model fitting (R-QuMF)\nalgorithm, designed to handle outliers effectively. Our method leverages the\nintrinsic capabilities of quantum hardware to tackle combinatorial challenges\ninherent in MMF tasks, and it does not require prior knowledge of the exact\nnumber of models, thereby enhancing its practical applicability. By formulating\nthe problem as a maximum set coverage task for adiabatic quantum computers\n(AQC), R-QuMF outperforms existing quantum techniques, demonstrating superior\nperformance across various synthetic and real-world 3D datasets. Our findings\nunderscore the potential of quantum computing in addressing the complexities of\nMMF, especially in real-world scenarios with noisy and outlier-prone data."
    },
    {
        "date": "2025-04",
        "title": "Collective Learning Mechanism based Optimal Transport Generative Adversarial Network for Non-parallel Voice Conversion",
        "author": "Sandipan Dhar, Md. Tousin Akhter, Nanda Dulal Jana, and Swagatam Das",
        "link": "http://arxiv.org/abs/2504.13791v1",
        "abstract": "After demonstrating significant success in image synthesis, Generative\nAdversarial Network (GAN) models have likewise made significant progress in the\nfield of speech synthesis, leveraging their capacity to adapt the precise\ndistribution of target data through adversarial learning processes. Notably, in\nthe realm of State-Of-The-Art (SOTA) GAN-based Voice Conversion (VC) models,\nthere exists a substantial disparity in naturalness between real and\nGAN-generated speech samples. Furthermore, while many GAN models currently\noperate on a single generator discriminator learning approach, optimizing\ntarget data distribution is more effectively achievable through a single\ngenerator multi-discriminator learning scheme. Hence, this study introduces a\nnovel GAN model named Collective Learning Mechanism-based Optimal Transport GAN\n(CLOT-GAN) model, incorporating multiple discriminators, including the Deep\nConvolutional Neural Network (DCNN) model, Vision Transformer (ViT), and\nconformer. The objective of integrating various discriminators lies in their\nability to comprehend the formant distribution of mel-spectrograms, facilitated\nby a collective learning mechanism. Simultaneously, the inclusion of Optimal\nTransport (OT) loss aims to precisely bridge the gap between the source and\ntarget data distribution, employing the principles of OT theory. The\nexperimental validation on VCC 2018, VCTK, and CMU-Arctic datasets confirms\nthat the CLOT-GAN-VC model outperforms existing VC models in objective and\nsubjective assessments."
    },
    {
        "date": "2025-04",
        "title": "On the Relationship Between Robustness and Expressivity of Graph Neural Networks",
        "author": "Lorenz Kummer, Wilfried N. Gansterer, and Nils M. Kriege",
        "link": "http://arxiv.org/abs/2504.13786v1",
        "abstract": "We investigate the vulnerability of Graph Neural Networks (GNNs) to bit-flip\nattacks (BFAs) by introducing an analytical framework to study the influence of\narchitectural features, graph properties, and their interaction.\n  The expressivity of GNNs refers to their ability to distinguish\nnon-isomorphic graphs and depends on the encoding of node neighborhoods. We\nexamine the vulnerability of neural multiset functions commonly used for this\npurpose and establish formal criteria to characterize a GNN's susceptibility to\nlosing expressivity due to BFAs. This enables an analysis of the impact of\nhomophily, graph structural variety, feature encoding, and activation functions\non GNN robustness. We derive theoretical bounds for the number of bit flips\nrequired to degrade GNN expressivity on a dataset, identifying ReLU-activated\nGNNs operating on highly homophilous graphs with low-dimensional or one-hot\nencoded features as particularly susceptible. Empirical results using ten\nreal-world datasets confirm the statistical significance of our key theoretical\ninsights and offer actionable results to mitigate BFA risks in\nexpressivity-critical applications."
    },
    {
        "date": "2025-04",
        "title": "BadApex: Backdoor Attack Based on Adaptive Optimization Mechanism of Black-box Large Language Models",
        "author": "Zhengxian Wu, Juan Wen, Wanli Peng, Ziwei Zhang, Yinghan Zhou, and Yiming Xue",
        "link": "http://arxiv.org/abs/2504.13775v2",
        "abstract": "Previous insertion-based and paraphrase-based backdoors have achieved great\nsuccess in attack efficacy, but they ignore the text quality and semantic\nconsistency between poisoned and clean texts. Although recent studies introduce\nLLMs to generate poisoned texts and improve the stealthiness, semantic\nconsistency, and text quality, their hand-crafted prompts rely on expert\nexperiences, facing significant challenges in prompt adaptability and attack\nperformance after defenses. In this paper, we propose a novel backdoor attack\nbased on adaptive optimization mechanism of black-box large language models\n(BadApex), which leverages a black-box LLM to generate poisoned text through a\nrefined prompt. Specifically, an Adaptive Optimization Mechanism is designed to\nrefine an initial prompt iteratively using the generation and modification\nagents. The generation agent generates the poisoned text based on the initial\nprompt. Then the modification agent evaluates the quality of the poisoned text\nand refines a new prompt. After several iterations of the above process, the\nrefined prompt is used to generate poisoned texts through LLMs. We conduct\nextensive experiments on three dataset with six backdoor attacks and two\ndefenses. Extensive experimental results demonstrate that BadApex significantly\noutperforms state-of-the-art attacks. It improves prompt adaptability, semantic\nconsistency, and text quality. Furthermore, when two defense methods are\napplied, the average attack success rate (ASR) still up to 96.75%."
    },
    {
        "date": "2025-04",
        "title": "Analysing the Robustness of Vision-Language-Models to Common Corruptions",
        "author": "Muhammad Usama, Syeda Aishah Asim, Syed Bilal Ali, Syed Talal Wasim, and Umair Bin Mansoor",
        "link": "http://arxiv.org/abs/2504.13690v2",
        "abstract": "Vision-language models (VLMs) have demonstrated impressive capabilities in\nunderstanding and reasoning about visual and textual content. However, their\nrobustness to common image corruptions remains under-explored. In this work, we\npresent the first comprehensive analysis of VLM robustness across 19 corruption\ntypes from the ImageNet-C benchmark, spanning four categories: noise, blur,\nweather, and digital distortions. We introduce two new benchmarks, TextVQA-C\nand GQA-C, to systematically evaluate how corruptions affect scene text\nunderstanding and object-based reasoning, respectively. Our analysis reveals\nthat transformer-based VLMs exhibit distinct vulnerability patterns across\ntasks: text recognition deteriorates most severely under blur and snow\ncorruptions, while object reasoning shows higher sensitivity to corruptions\nsuch as frost and impulse noise. We connect these observations to the\nfrequency-domain characteristics of different corruptions, revealing how\ntransformers' inherent bias toward low-frequency processing explains their\ndifferential robustness patterns. Our findings provide valuable insights for\ndeveloping more corruption-robust vision-language models for real-world\napplications."
    },
    {
        "date": "2025-04",
        "title": "Going Whole Hog: A Philosophical Defense of AI Cognition",
        "author": "Herman Cappelen, and Josh Dever",
        "link": "http://arxiv.org/abs/2504.13988v1",
        "abstract": "This work defends the 'Whole Hog Thesis': sophisticated Large Language Models\n(LLMs) like ChatGPT are full-blown linguistic and cognitive agents, possessing\nunderstanding, beliefs, desires, knowledge, and intentions. We argue against\nprevailing methodologies in AI philosophy, rejecting starting points based on\nlow-level computational details ('Just an X' fallacy) or pre-existing theories\nof mind. Instead, we advocate starting with simple, high-level observations of\nLLM behavior (e.g., answering questions, making suggestions) -- defending this\ndata against charges of metaphor, loose talk, or pretense. From these\nobservations, we employ 'Holistic Network Assumptions' -- plausible connections\nbetween mental capacities (e.g., answering implies knowledge, knowledge implies\nbelief, action implies intention) -- to argue for the full suite of cognitive\nstates. We systematically rebut objections based on LLM failures\n(hallucinations, planning/reasoning errors), arguing these don't preclude\nagency, often mirroring human fallibility. We address numerous 'Games of\nLacks', arguing that LLMs do not lack purported necessary conditions for\ncognition (e.g., semantic grounding, embodiment, justification, intrinsic\nintentionality) or that these conditions are not truly necessary, often relying\non anti-discriminatory arguments comparing LLMs to diverse human capacities.\nOur approach is evidential, not functionalist, and deliberately excludes\nconsciousness. We conclude by speculating on the possibility of LLMs possessing\n'alien' contents beyond human conceptual schemes."
    },
    {
        "date": "2025-04",
        "title": "Fairness and Robustness in Machine Unlearning",
        "author": "Khoa Tran, and Simon S. Woo",
        "link": "http://arxiv.org/abs/2504.13610v1",
        "abstract": "Machine unlearning poses the challenge of ``how to eliminate the influence of\nspecific data from a pretrained model'' in regard to privacy concerns. While\nprior research on approximated unlearning has demonstrated accuracy and\nefficiency in time complexity, we claim that it falls short of achieving exact\nunlearning, and we are the first to focus on fairness and robustness in machine\nunlearning algorithms. Our study presents fairness Conjectures for a\nwell-trained model, based on the variance-bias trade-off characteristic, and\nconsiders their relevance to robustness. Our Conjectures are supported by\nexperiments conducted on the two most widely used model architectures, ResNet\nand ViT, demonstrating the correlation between fairness and robustness:\n\\textit{the higher fairness-gap is, the more the model is sensitive and\nvulnerable}. In addition, our experiments demonstrate the vulnerability of\ncurrent state-of-the-art approximated unlearning algorithms to adversarial\nattacks, where their unlearned models suffer a significant drop in accuracy\ncompared to the exact-unlearned models. We claim that our fairness-gap\nmeasurement and robustness metric should be used to evaluate the unlearning\nalgorithm. Furthermore, we demonstrate that unlearning in the intermediate and\nlast layers is sufficient and cost-effective for time and memory complexity."
    },
    {
        "date": "2025-04",
        "title": "Q-FAKER: Query-free Hard Black-box Attack via Controlled Generation",
        "author": "CheolWon Na, YunSeok Choi, and Jee-Hyong Lee",
        "link": "http://arxiv.org/abs/2504.13551v1",
        "abstract": "Many adversarial attack approaches are proposed to verify the vulnerability\nof language models. However, they require numerous queries and the information\non the target model. Even black-box attack methods also require the target\nmodel's output information. They are not applicable in real-world scenarios, as\nin hard black-box settings where the target model is closed and inaccessible.\nEven the recently proposed hard black-box attacks still require many queries\nand demand extremely high costs for training adversarial generators. To address\nthese challenges, we propose Q-faker (Query-free Hard Black-box Attacker), a\nnovel and efficient method that generates adversarial examples without\naccessing the target model. To avoid accessing the target model, we use a\nsurrogate model instead. The surrogate model generates adversarial sentences\nfor a target-agnostic attack. During this process, we leverage controlled\ngeneration techniques. We evaluate our proposed method on eight datasets.\nExperimental results demonstrate our method's effectiveness including high\ntransferability and the high quality of the generated adversarial examples, and\nprove its practical in hard black-box settings."
    },
    {
        "date": "2025-04",
        "title": "EXAM: Exploiting Exclusive System-Level Cache in Apple M-Series SoCs for Enhanced Cache Occupancy Attacks",
        "author": "Tianhong Xu, Aidong Adam Ding, and Yunsi Fei",
        "link": "http://arxiv.org/abs/2504.13385v1",
        "abstract": "Cache occupancy attacks exploit the shared nature of cache hierarchies to\ninfer a victim's activities by monitoring overall cache usage, unlike\naccess-driven cache attacks that focus on specific cache lines or sets. There\nexists some prior work that target the last-level cache (LLC) of Intel\nprocessors, which is inclusive of higher-level caches, and L2 caches of ARM\nsystems. In this paper, we target the System-Level Cache (SLC) of Apple\nM-series SoCs, which is exclusive to higher-level CPU caches. We address the\nchallenges of the exclusiveness and propose a suite of SLC-cache occupancy\nattacks, the first of its kind, where an adversary can monitor GPU and other\nCPU cluster activities from their own CPU cluster. We first discover the\nstructure of SLC in Apple M1 SOC and various policies pertaining to access and\nsharing through reverse engineering. We propose two attacks against websites.\nOne is a coarse-grained fingerprinting attack, recognizing which website is\naccessed based on their different GPU memory access patterns monitored through\nthe SLC occupancy channel. The other attack is a fine-grained pixel stealing\nattack, which precisely monitors the GPU memory usage for rendering different\npixels, through the SLC occupancy channel. Third, we introduce a novel screen\ncapturing attack which works beyond webpages, with the monitoring granularity\nof 57 rows of pixels (there are 1600 rows for the screen). This significantly\nexpands the attack surface, allowing the adversary to retrieve any screen\ndisplay, posing a substantial new threat to system security. Our findings\nreveal critical vulnerabilities in Apple's M-series SoCs and emphasize the\nurgent need for effective countermeasures against cache occupancy attacks in\nheterogeneous computing environments."
    },
    {
        "date": "2025-04",
        "title": "Security-First AI: Foundations for Robust and Trustworthy Systems",
        "author": "Krti Tallam",
        "link": "http://arxiv.org/abs/2504.16110v1",
        "abstract": "The conversation around artificial intelligence (AI) often focuses on safety,\ntransparency, accountability, alignment, and responsibility. However, AI\nsecurity (i.e., the safeguarding of data, models, and pipelines from\nadversarial manipulation) underpins all of these efforts. This manuscript\nposits that AI security must be prioritized as a foundational layer. We present\na hierarchical view of AI challenges, distinguishing security from safety, and\nargue for a security-first approach to enable trustworthy and resilient AI\nsystems. We discuss core threat models, key attack vectors, and emerging\ndefense mechanisms, concluding that a metric-driven approach to AI security is\nessential for robust AI safety, transparency, and accountability."
    },
    {
        "date": "2025-04",
        "title": "The Impact of AI on the Cyber Offense-Defense Balance and the Character of Cyber Conflict",
        "author": "Andrew J. Lohn",
        "link": "http://arxiv.org/abs/2504.13371v1",
        "abstract": "Unlike other domains of conflict, and unlike other fields with high\nanticipated risk from AI, the cyber domain is intrinsically digital with a\ntight feedback loop between AI training and cyber application. Cyber may have\nsome of the largest and earliest impacts from AI, so it is important to\nunderstand how the cyber domain may change as AI continues to advance. Our\napproach reviewed the literature, collecting nine arguments that have been\nproposed for offensive advantage in cyber conflict and nine proposed arguments\nfor defensive advantage. We include an additional forty-eight arguments that\nhave been proposed to give cyber conflict and competition its character as\ncollected separately by Healey, Jervis, and Nandrajog. We then consider how\neach of those arguments and propositions might change with varying degrees of\nAI advancement. We find that the cyber domain is too multifaceted for a single\nanswer to whether AI will enhance offense or defense broadly. AI will improve\nsome aspects, hinder others, and leave some aspects unchanged. We collect and\npresent forty-four ways that we expect AI to impact the cyber offense-defense\nbalance and the character of cyber conflict and competition."
    },
    {
        "date": "2025-04",
        "title": "GraphQLer: Enhancing GraphQL Security with Context-Aware API Testing",
        "author": "Omar Tsai, Jianing Li, Tsz Tung Cheung, Lejing Huang, Hao Zhu, Jianrui Xiao, Iman Sharafaldin, and Mohammad A. Tayebi",
        "link": "http://arxiv.org/abs/2504.13358v1",
        "abstract": "GraphQL is an open-source data query and manipulation language for web\napplications, offering a flexible alternative to RESTful APIs. However, its\ndynamic execution model and lack of built-in security mechanisms expose it to\nvulnerabilities such as unauthorized data access, denial-of-service (DoS)\nattacks, and injections. Existing testing tools focus on functional\ncorrectness, often overlooking security risks stemming from query\ninterdependencies and execution context. This paper presents GraphQLer, the\nfirst context-aware security testing framework for GraphQL APIs. GraphQLer\nconstructs a dependency graph to analyze relationships among mutations,\nqueries, and objects, capturing critical interdependencies. It chains related\nqueries and mutations to reveal authentication and authorization flaws, access\ncontrol bypasses, and resource misuse. Additionally, GraphQLer tracks internal\nresource usage to uncover data leakage, privilege escalation, and replay attack\nvectors. We assess GraphQLer on various GraphQL APIs, demonstrating improved\ntesting coverage - averaging a 35% increase, with up to 84% in some cases -\ncompared to top-performing baselines. Remarkably, this is achieved in less\ntime, making GraphQLer suitable for time-sensitive contexts. GraphQLer also\nsuccessfully detects a known CVE and potential vulnerabilities in large-scale\nproduction APIs. These results underline GraphQLer's utility in proactively\nsecuring GraphQL APIs through automated, context-aware vulnerability detection."
    },
    {
        "date": "2025-04",
        "title": "On the Definition of Robustness and Resilience of AI Agents for Real-time Congestion Management",
        "author": "Timothy Tjhay, Ricardo J. Bessa, and Jose Paulos",
        "link": "http://arxiv.org/abs/2504.13314v1",
        "abstract": "The European Union's Artificial Intelligence (AI) Act defines robustness,\nresilience, and security requirements for high-risk sectors but lacks detailed\nmethodologies for assessment. This paper introduces a novel framework for\nquantitatively evaluating the robustness and resilience of reinforcement\nlearning agents in congestion management. Using the AI-friendly digital\nenvironment Grid2Op, perturbation agents simulate natural and adversarial\ndisruptions by perturbing the input of AI systems without altering the actual\nstate of the environment, enabling the assessment of AI performance under\nvarious scenarios. Robustness is measured through stability and reward impact\nmetrics, while resilience quantifies recovery from performance degradation. The\nresults demonstrate the framework's effectiveness in identifying\nvulnerabilities and improving AI robustness and resilience for critical\napplications."
    },
    {
        "date": "2025-04",
        "title": "DYNAMITE: Dynamic Defense Selection for Enhancing Machine Learning-based Intrusion Detection Against Adversarial Attacks",
        "author": "Jing Chen, Onat Gungor, Zhengli Shang, Elvin Li, and Tajana Rosing",
        "link": "http://arxiv.org/abs/2504.13301v1",
        "abstract": "The rapid proliferation of the Internet of Things (IoT) has introduced\nsubstantial security vulnerabilities, highlighting the need for robust\nIntrusion Detection Systems (IDS). Machine learning-based intrusion detection\nsystems (ML-IDS) have significantly improved threat detection capabilities;\nhowever, they remain highly susceptible to adversarial attacks. While numerous\ndefense mechanisms have been proposed to enhance ML-IDS resilience, a\nsystematic approach for selecting the most effective defense against a specific\nadversarial attack remains absent. To address this challenge, we propose\nDynamite, a dynamic defense selection framework that enhances ML-IDS by\nintelligently identifying and deploying the most suitable defense using a\nmachine learning-driven selection mechanism. Our results demonstrate that\nDynamite achieves a 96.2% reduction in computational time compared to the\nOracle, significantly decreasing computational overhead while preserving strong\nprediction performance. Dynamite also demonstrates an average F1-score\nimprovement of 76.7% over random defense and 65.8% over the best static\nstate-of-the-art defense."
    },
    {
        "date": "2025-04",
        "title": "Energy-Based Reward Models for Robust Language Model Alignment",
        "author": "Anamika Lochab, and Ruqi Zhang",
        "link": "http://arxiv.org/abs/2504.13134v1",
        "abstract": "Reward models (RMs) are essential for aligning Large Language Models (LLMs)\nwith human preferences. However, they often struggle with capturing complex\nhuman preferences and generalizing to unseen data. To address these challenges,\nwe introduce Energy-Based Reward Model (EBRM), a lightweight post-hoc\nrefinement framework that enhances RM robustness and generalization. EBRM\nmodels the reward distribution explicitly, capturing uncertainty in human\npreferences and mitigating the impact of noisy or misaligned annotations. It\nachieves this through conflict-aware data filtering, label-noise-aware\ncontrastive training, and hybrid initialization. Notably, EBRM enhances RMs\nwithout retraining, making it computationally efficient and adaptable across\ndifferent models and tasks. Empirical evaluations on RM benchmarks demonstrate\nsignificant improvements in both robustness and generalization, achieving up to\na 5.97% improvement in safety-critical alignment tasks compared to standard\nRMs. Furthermore, reinforcement learning experiments confirm that our refined\nrewards enhance alignment quality, effectively delaying reward hacking. These\nresults demonstrate our approach as a scalable and effective enhancement for\nexisting RMs and alignment pipelines. The code is available at EBRM."
    },
    {
        "date": "2025-04",
        "title": "Enhancing Cocoa Pod Disease Classification via Transfer Learning and Ensemble Methods: Toward Robust Predictive Modeling",
        "author": "Devina Anduyan, Nyza Cabillo, Navy Gultiano, and Mark Phil Pacot",
        "link": "http://arxiv.org/abs/2504.12992v1",
        "abstract": "This study presents an ensemble-based approach for cocoa pod disease\nclassification by integrating transfer learning with three ensemble learning\nstrategies: Bagging, Boosting, and Stacking. Pre-trained convolutional neural\nnetworks, including VGG16, VGG19, ResNet50, ResNet101, InceptionV3, and\nXception, were fine-tuned and employed as base learners to detect three disease\ncategories: Black Pod Rot, Pod Borer, and Healthy. A balanced dataset of 6,000\ncocoa pod images was curated and augmented to ensure robustness against\nvariations in lighting, orientation, and disease severity. The performance of\neach ensemble method was evaluated using accuracy, precision, recall, and\nF1-score. Experimental results show that Bagging consistently achieved superior\nclassification performance with a test accuracy of 100%, outperforming Boosting\n(97%) and Stacking (92%). The findings confirm that combining transfer learning\nwith ensemble techniques improves model generalization and reliability, making\nit a promising direction for precision agriculture and automated crop disease\nmanagement."
    },
    {
        "date": "2025-04",
        "title": "PSG-MAE: Robust Multitask Sleep Event Monitoring using Multichannel PSG Reconstruction and Inter-channel Contrastive Learning",
        "author": "Yifei Wang, Qi Liu, Fuli Min, and Honghao Wang",
        "link": "http://arxiv.org/abs/2504.13229v1",
        "abstract": "Polysomnography (PSG) signals are essential for studying sleep processes and\ndiagnosing sleep disorders. Analyzing PSG data through deep neural networks\n(DNNs) for automated sleep monitoring has become increasingly feasible.\nHowever, the limited availability of datasets for certain sleep events often\nleads to DNNs focusing on a single task with a single-sourced training dataset.\nAs a result, these models struggle to transfer to new sleep events and lack\nrobustness when applied to new datasets. To address these challenges, we\npropose PSG-MAE, a mask autoencoder (MAE) based pre-training framework. By\nperforming self-supervised learning on a large volume of unlabeled PSG data,\nPSG-MAE develops a robust feature extraction network that can be broadly\napplied to various sleep event monitoring tasks. Unlike conventional MAEs,\nPSG-MAE generates complementary masks across PSG channels, integrates a\nmultichannel signal reconstruction method, and employs a self-supervised\ninter-channel contrastive learning (ICCL) strategy. This approach enables the\nencoder to capture temporal features from each channel while simultaneously\nlearning latent relationships between channels, thereby enhancing the\nutilization of multichannel information. Experimental results show that PSG-MAE\neffectively captures both temporal details and inter-channel information from\nPSG signals. When the encoder pre-trained through PSG-MAE is fine-tuned with\ndownstream feature decomposition networks, it achieves an accuracy of 83.7% for\nsleep staging and 90.45% for detecting obstructive sleep apnea, which\nhighlights the framework's robustness and broad applicability."
    },
    {
        "date": "2025-04",
        "title": "Adversarial Resilience against Clean-Label Attacks in Realizable and Noisy Settings",
        "author": "Carolin Heinzler",
        "link": "http://arxiv.org/abs/2504.13966v1",
        "abstract": "We investigate the challenge of establishing stochastic-like guarantees when\nsequentially learning from a stream of i.i.d. data that includes an unknown\nquantity of clean-label adversarial samples. We permit the learner to abstain\nfrom making predictions when uncertain. The regret of the learner is measured\nin terms of misclassification and abstention error, where we allow the learner\nto abstain for free on adversarial injected samples. This approach is based on\nthe work of Goel, Hanneke, Moran, and Shetty from arXiv:2306.13119. We explore\nthe methods they present and manage to correct inaccuracies in their\nargumentation.\n  However, this approach is limited to the realizable setting, where labels are\nassigned according to some function $f^*$ from the hypothesis space\n$\\mathcal{F}$. Based on similar arguments, we explore methods to make\nadaptations for the agnostic setting where labels are random. Introducing the\nnotion of a clean-label adversary in the agnostic context, we are the first to\ngive a theoretical analysis of a disagreement-based learner for thresholds,\nsubject to a clean-label adversary with noise."
    },
    {
        "date": "2025-04",
        "title": "SoK: Security of EMV Contactless Payment Systems",
        "author": "Mahshid Mehr Nezhad, Feng Hao, Gregory Epiphaniou, Carsten Maple, and Timur Yunusov",
        "link": "http://arxiv.org/abs/2504.12812v1",
        "abstract": "The widespread adoption of EMV (Europay, Mastercard, and Visa) contactless\npayment systems has greatly improved convenience for both users and merchants.\nHowever, this growth has also exposed significant security challenges. This SoK\nprovides a comprehensive analysis of security vulnerabilities in EMV\ncontactless payments, particularly within the open-loop systems used by Visa\nand Mastercard. We categorize attacks into seven attack vectors across three\nkey areas: application selection, cardholder authentication, and transaction\nauthorization. We replicate the attacks on Visa and Mastercard protocols using\nour experimental platform to determine their practical feasibility and offer\ninsights into the current security landscape of contactless payments. Our study\nalso includes a detailed evaluation of the underlying protocols, along with a\ncomparative analysis of Visa and Mastercard, highlighting vulnerabilities and\nrecommending countermeasures."
    },
    {
        "date": "2025-04",
        "title": "A Numerical Gradient Inversion Attack in Variational Quantum Neural-Networks",
        "author": "Georgios Papadopoulos, Shaltiel Eloul, Yash Satsangi, Jamie Heredge, Niraj Kumar, Chun-Fu Chen, and Marco Pistoia",
        "link": "http://arxiv.org/abs/2504.12806v1",
        "abstract": "The loss landscape of Variational Quantum Neural Networks (VQNNs) is\ncharacterized by local minima that grow exponentially with increasing qubits.\nBecause of this, it is more challenging to recover information from model\ngradients during training compared to classical Neural Networks (NNs). In this\npaper we present a numerical scheme that successfully reconstructs input\ntraining, real-world, practical data from trainable VQNNs' gradients. Our\nscheme is based on gradient inversion that works by combining gradients\nestimation with the finite difference method and adaptive low-pass filtering.\nThe scheme is further optimized with Kalman filter to obtain efficient\nconvergence. Our experiments show that our algorithm can invert even\nbatch-trained data, given the VQNN model is sufficiently over-parameterized."
    },
    {
        "date": "2025-04",
        "title": "MCP Guardian: A Security-First Layer for Safeguarding MCP-Based AI System",
        "author": "Sonu Kumar, Anubhav Girdhar, Ritesh Patil, and Divyansh Tripathi",
        "link": "http://arxiv.org/abs/2504.12757v1",
        "abstract": "As Agentic AI gain mainstream adoption, the industry invests heavily in model\ncapabilities, achieving rapid leaps in reasoning and quality. However, these\nsystems remain largely confined to data silos, and each new integration\nrequires custom logic that is difficult to scale. The Model Context Protocol\n(MCP) addresses this challenge by defining a universal, open standard for\nsecurely connecting AI-based applications (MCP clients) to data sources (MCP\nservers). However, the flexibility of the MCP introduces new risks, including\nmalicious tool servers and compromised data integrity. We present MCP Guardian,\na framework that strengthens MCP-based communication with authentication,\nrate-limiting, logging, tracing, and Web Application Firewall (WAF) scanning.\nThrough real-world scenarios and empirical testing, we demonstrate how MCP\nGuardian effectively mitigates attacks and ensures robust oversight with\nminimal overheads. Our approach fosters secure, scalable data access for AI\nassistants, underscoring the importance of a defense-in-depth approach that\nenables safer and more transparent innovation in AI-driven environments."
    },
    {
        "date": "2025-04",
        "title": "Attack-Defense Trees with Offensive and Defensive Attributes (with Appendix)",
        "author": "Danut-Valentin Copae, Reza Soltani, and Milan Lopuha\u00e4-Zwakenberg",
        "link": "http://arxiv.org/abs/2504.12748v1",
        "abstract": "Effective risk management in cybersecurity requires a thorough understanding\nof the interplay between attacker capabilities and defense strategies.\nAttack-Defense Trees (ADTs) are a commonly used methodology for representing\nthis interplay; however, previous work in this domain has only focused on\nanalyzing metrics such as cost, damage, or time from the perspective of the\nattacker. This approach provides an incomplete view of the system, as it\nneglects to model defender attributes: in real-world scenarios, defenders have\nfinite resources for countermeasures and are similarly constrained. In this\npaper, we propose a novel framework that incorporates defense metrics into\nADTs, and we present efficient algorithms for computing the Pareto front\nbetween defense and attack metrics. Our methods encode both attacker and\ndefender metrics as semirings, allowing our methods to be used for many metrics\nsuch as cost, damage, and skill. We analyze tree-structured ADTs using a\nbottom-up approach and general ADTs by translating them into binary decision\ndiagrams. Experiments on randomly generated ADTS demonstrate that both\napproaches effectively handle ADTs with several hundred nodes."
    },
    {
        "date": "2025-04",
        "title": "Adversary-Augmented Simulation for Fairness Evaluation and Defense in Hyperledger Fabric",
        "author": "Erwan Mahe, Rouwaida Abdallah, Pierre-Yves Piriou, and Sara Tucci-Piergiovanni",
        "link": "http://arxiv.org/abs/2504.12733v1",
        "abstract": "This paper presents an adversary model and a simulation framework\nspecifically tailored for analyzing attacks on distributed systems composed of\nmultiple distributed protocols, with a focus on assessing the security of\nblockchain networks. Our model classifies and constrains adversarial actions\nbased on the assumptions of the target protocols, defined by failure models,\ncommunication models, and the fault tolerance thresholds of Byzantine Fault\nTolerant (BFT) protocols. The goal is to study not only the intended effects of\nadversarial strategies but also their unintended side effects on critical\nsystem properties. We apply this framework to analyze fairness properties in a\nHyperledger Fabric (HF) blockchain network. Our focus is on novel fairness\nattacks that involve coordinated adversarial actions across various HF\nservices. Simulations show that even a constrained adversary can violate\nfairness with respect to specific clients (client fairness) and impact related\nguarantees (order fairness), which relate the reception order of transactions\nto their final order in the blockchain. This paper significantly extends our\nprevious work by introducing and evaluating a mitigation mechanism specifically\ndesigned to counter transaction reordering attacks. We implement and integrate\nthis defense into our simulation environment, demonstrating its effectiveness\nunder diverse conditions."
    },
    {
        "date": "2025-04",
        "title": "ACoRN: Noise-Robust Abstractive Compression in Retrieval-Augmented Language Models",
        "author": "Singon Kim, Gunho Jung, and Seong-Whan Lee",
        "link": "http://arxiv.org/abs/2504.12673v1",
        "abstract": "Abstractive compression utilizes smaller langauge models to condense\nquery-relevant context, reducing computational costs in retrieval-augmented\ngeneration (RAG). However,retrieved documents often include information that is\neither irrelevant to answering the query or misleading due to factual incorrect\ncontent, despite having high relevance scores. This behavior indicates that\nabstractive compressors are more likely to omit important information essential\nfor the correct answer, especially in long contexts where attention dispersion\noccurs. To address this issue, we categorize retrieved documents in a more\nfine-grained manner and propose Abstractive Compression Robust against Noise\n(ACoRN), which introduces two novel training steps. First, we use offline data\naugmentation on the training dataset to enhance compressor robustness against\ntwo distinct types of retrieval noise. Second, since the language modelbased\ncompressor cannot fully utilize information from multiple retrieved documents\nand exhibits positional bias, we perform finetuning to generate summaries\ncentered around key information that directly supports the correct answer. Our\nexperiments demonstrate that T5-large, trained with ACoRN as a compressor,\nimproves EM and F1 scores while preserving the answer string, which could serve\nas direct evidence. ACoRN excels on datasets with many accuracy-reducing\ndocuments, making it highly useful in real-world scenarios."
    },
    {
        "date": "2025-04",
        "title": "AdaptoVision: A Multi-Resolution Image Recognition Model for Robust and Scalable Classification",
        "author": "Md. Sanaullah Chowdhury Lameya Sabrin",
        "link": "http://arxiv.org/abs/2504.12652v1",
        "abstract": "This paper introduces AdaptoVision, a novel convolutional neural network\n(CNN) architecture designed to efficiently balance computational complexity and\nclassification accuracy. By leveraging enhanced residual units, depth-wise\nseparable convolutions, and hierarchical skip connections, AdaptoVision\nsignificantly reduces parameter count and computational requirements while\npreserving competitive performance across various benchmark and medical image\ndatasets. Extensive experimentation demonstrates that AdaptoVision achieves\nstate-of-the-art on BreakHis dataset and comparable accuracy levels, notably\n95.3\\% on CIFAR-10 and 85.77\\% on CIFAR-100, without relying on any pretrained\nweights. The model's streamlined architecture and strategic simplifications\npromote effective feature extraction and robust generalization, making it\nparticularly suitable for deployment in real-time and resource-constrained\nenvironments."
    },
    {
        "date": "2025-04",
        "title": "Quantum Computing Supported Adversarial Attack-Resilient Autonomous Vehicle Perception Module for Traffic Sign Classification",
        "author": "Reek Majumder, Mashrur Chowdhury, Sakib Mahmud Khan, Zadid Khan, Fahim Ahmad, Frank Ngeni, Gurcan Comert, Judith Mwakalonge, and Dimitra Michalaka",
        "link": "http://arxiv.org/abs/2504.12644v1",
        "abstract": "Deep learning (DL)-based image classification models are essential for\nautonomous vehicle (AV) perception modules since incorrect categorization might\nhave severe repercussions. Adversarial attacks are widely studied cyberattacks\nthat can lead DL models to predict inaccurate output, such as incorrectly\nclassified traffic signs by the perception module of an autonomous vehicle. In\nthis study, we create and compare hybrid classical-quantum deep learning\n(HCQ-DL) models with classical deep learning (C-DL) models to demonstrate\nrobustness against adversarial attacks for perception modules. Before feeding\nthem into the quantum system, we used transfer learning models, alexnet and\nvgg-16, as feature extractors. We tested over 1000 quantum circuits in our\nHCQ-DL models for projected gradient descent (PGD), fast gradient sign attack\n(FGSA), and gradient attack (GA), which are three well-known untargeted\nadversarial approaches. We evaluated the performance of all models during\nadversarial attacks and no-attack scenarios. Our HCQ-DL models maintain\naccuracy above 95\\% during a no-attack scenario and above 91\\% for GA and FGSA\nattacks, which is higher than C-DL models. During the PGD attack, our\nalexnet-based HCQ-DL model maintained an accuracy of 85\\% compared to C-DL\nmodels that achieved accuracies below 21\\%. Our results highlight that the\nHCQ-DL models provide improved accuracy for traffic sign classification under\nadversarial settings compared to their classical counterparts."
    },
    {
        "date": "2025-04",
        "title": "Robo-SGG: Exploiting Layout-Oriented Normalization and Restitution for Robust Scene Graph Generation",
        "author": "Changsheng Lv, Mengshi Qi, Zijian Fu, and Huadong Ma",
        "link": "http://arxiv.org/abs/2504.12606v1",
        "abstract": "In this paper, we introduce a novel method named Robo-SGG, i.e.,\nLayout-Oriented Normalization and Restitution for Robust Scene Graph\nGeneration. Compared to the existing SGG setting, the robust scene graph\ngeneration aims to perform inference on a diverse range of corrupted images,\nwith the core challenge being the domain shift between the clean and corrupted\nimages. Existing SGG methods suffer from degraded performance due to\ncompromised visual features e.g., corruption interference or occlusions. To\nobtain robust visual features, we exploit the layout information, which is\ndomain-invariant, to enhance the efficacy of existing SGG methods on corrupted\nimages. Specifically, we employ Instance Normalization(IN) to filter out the\ndomain-specific feature and recover the unchangeable structural features, i.e.,\nthe positional and semantic relationships among objects by the proposed\nLayout-Oriented Restitution. Additionally, we propose a Layout-Embedded Encoder\n(LEE) that augments the existing object and predicate encoders within the SGG\nframework, enriching the robust positional and semantic features of objects and\npredicates. Note that our proposed Robo-SGG module is designed as a\nplug-and-play component, which can be easily integrated into any baseline SGG\nmodel. Extensive experiments demonstrate that by integrating the\nstate-of-the-art method into our proposed Robo-SGG, we achieve relative\nimprovements of 5.6%, 8.0%, and 6.5% in mR@50 for PredCls, SGCls, and SGDet\ntasks on the VG-C dataset, respectively, and achieve new state-of-the-art\nperformance in corruption scene graph generation benchmark (VG-C and GQA-C). We\nwill release our source code and model."
    },
    {
        "date": "2025-04",
        "title": "Provable Secure Steganography Based on Adaptive Dynamic Sampling",
        "author": "Kaiyi Pang",
        "link": "http://arxiv.org/abs/2504.12579v1",
        "abstract": "The security of private communication is increasingly at risk due to\nwidespread surveillance. Steganography, a technique for embedding secret\nmessages within innocuous carriers, enables covert communication over monitored\nchannels. Provably Secure Steganography (PSS) is state of the art for making\nstego carriers indistinguishable from normal ones by ensuring computational\nindistinguishability between stego and cover distributions. However, current\nPSS methods often require explicit access to the distribution of generative\nmodel for both sender and receiver, limiting their practicality in black box\nscenarios. In this paper, we propose a provably secure steganography scheme\nthat does not require access to explicit model distributions for both sender\nand receiver. Our method incorporates a dynamic sampling strategy, enabling\ngenerative models to embed secret messages within multiple sampling choices\nwithout disrupting the normal generation process of the model. Extensive\nevaluations of three real world datasets and three LLMs demonstrate that our\nblackbox method is comparable with existing white-box steganography methods in\nterms of efficiency and capacity while eliminating the degradation of\nsteganography in model generated outputs."
    },
    {
        "date": "2025-04",
        "title": "The Others: Naturally Isolating Out-of-Distribution Samples for Robust Open-Set Semi-Supervised Learning",
        "author": "You Rim Choi, Subeom Park, Seojun Heo, Eunchung Noh, and Hyung-Sin Kim",
        "link": "http://arxiv.org/abs/2504.12569v1",
        "abstract": "Open-Set Semi-Supervised Learning (OSSL) tackles the practical challenge of\nlearning from unlabeled data that may include both in-distribution (ID) and\nunknown out-of-distribution (OOD) classes. However, existing OSSL methods form\nsuboptimal feature spaces by either excluding OOD samples, interfering with\nthem, or overtrusting their information during training. In this work, we\nintroduce MagMatch, a novel framework that naturally isolates OOD samples\nthrough a prototype-based contrastive learning paradigm. Unlike conventional\nmethods, MagMatch does not assign any prototypes to OOD samples; instead, it\nselectively aligns ID samples with class prototypes using an ID-Selective\nMagnetic (ISM) module, while allowing OOD samples - the \"others\" - to remain\nunaligned in the feature space. To support this process, we propose Selective\nMagnetic Alignment (SMA) loss for unlabeled data, which dynamically adjusts\nalignment based on sample confidence. Extensive experiments on diverse datasets\ndemonstrate that MagMatch significantly outperforms existing methods in both\nclosed-set classification accuracy and OOD detection AUROC, especially in\ngeneralizing to unseen OOD data."
    },
    {
        "date": "2025-04",
        "title": "Robust and Scalable Variational Bayes",
        "author": "Carlos Misael Madrid Padilla, Shitao Fan, and Lizhen Lin",
        "link": "http://arxiv.org/abs/2504.12528v1",
        "abstract": "We propose a robust and scalable framework for variational Bayes (VB) that\neffectively handles outliers and contamination of arbitrary nature in large\ndatasets. Our approach divides the dataset into disjoint subsets, computes the\nposterior for each subset, and applies VB approximation independently to these\nposteriors. The resulting variational posteriors with respect to the subsets\nare then aggregated using the geometric median of probability measures,\ncomputed with respect to the Wasserstein distance. This novel aggregation\nmethod yields the Variational Median Posterior (VM-Posterior) distribution. We\nrigorously demonstrate that the VM-Posterior preserves contraction properties\nakin to those of the true posterior, while accounting for approximation errors\nor the variational gap inherent in VB methods. We also provide provable\nrobustness guarantee of the VM-Posterior. Furthermore, we establish a\nvariational Bernstein-von Mises theorem for both multivariate Gaussian\ndistributions with general covariance structures and the mean-field variational\nfamily. To facilitate practical implementation, we adapt existing algorithms\nfor computing the VM-Posterior and evaluate its performance through extensive\nnumerical experiments. The results highlight its robustness and scalability,\nmaking it a reliable tool for Bayesian inference in the presence of complex,\ncontaminated datasets."
    },
    {
        "date": "2025-04",
        "title": "Diffusion Based Robust LiDAR Place Recognition",
        "author": "Benjamin Krummenacher, Jonas Frey, Turcan Tuna, Olga Vysotska, and Marco Hutter",
        "link": "http://arxiv.org/abs/2504.12412v1",
        "abstract": "Mobile robots on construction sites require accurate pose estimation to\nperform autonomous surveying and inspection missions. Localization in\nconstruction sites is a particularly challenging problem due to the presence of\nrepetitive features such as flat plastered walls and perceptual aliasing due to\napartments with similar layouts inter and intra floors. In this paper, we focus\non the global re-positioning of a robot with respect to an accurate scanned\nmesh of the building solely using LiDAR data. In our approach, a neural network\nis trained on synthetic LiDAR point clouds generated by simulating a LiDAR in\nan accurate real-life large-scale mesh. We train a diffusion model with a\nPointNet++ backbone, which allows us to model multiple position candidates from\na single LiDAR point cloud. The resulting model can successfully predict the\nglobal position of LiDAR in confined and complex sites despite the adverse\neffects of perceptual aliasing. The learned distribution of potential global\npositions can provide multi-modal position distribution. We evaluate our\napproach across five real-world datasets and show the place recognition\naccuracy of 77% +/-2m on average while outperforming baselines at a factor of 2\nin mean error."
    },
    {
        "date": "2025-04",
        "title": "Dysarthria Normalization via Local Lie Group Transformations for Robust ASR",
        "author": "Mikhail Osipov",
        "link": "http://arxiv.org/abs/2504.12279v1",
        "abstract": "We present a geometry-driven method for normalizing dysarthric speech using\nlocal Lie group transformations of spectrograms. Time, frequency, and amplitude\ndistortions are modeled as smooth, invertible deformations, parameterized by\nscalar fields and applied via exponential maps. A neural network is trained to\ninfer these fields from synthetic distortions of typical speech-without using\nany pathological data. At test time, the model applies an approximate inverse\nto real dysarthric inputs. Despite zero-shot generalization, we observe\nsubstantial ASR gains, including up to 16 percentage points WER reduction on\nchallenging TORGO samples, with no degradation on clean speech. This work\nintroduces a principled, interpretable approach for robust speech recognition\nunder motor speech disorders"
    },
    {
        "date": "2025-04",
        "title": "SCENT: Robust Spatiotemporal Learning for Continuous Scientific Data via Scalable Conditioned Neural Fields",
        "author": "David Keetae Park, Xihaier Luo, Guang Zhao, Seungjun Lee, Miruna Oprescu, and Shinjae Yoo",
        "link": "http://arxiv.org/abs/2504.12262v1",
        "abstract": "Spatiotemporal learning is challenging due to the intricate interplay between\nspatial and temporal dependencies, the high dimensionality of the data, and\nscalability constraints. These challenges are further amplified in scientific\ndomains, where data is often irregularly distributed (e.g., missing values from\nsensor failures) and high-volume (e.g., high-fidelity simulations), posing\nadditional computational and modeling difficulties. In this paper, we present\nSCENT, a novel framework for scalable and continuity-informed spatiotemporal\nrepresentation learning. SCENT unifies interpolation, reconstruction, and\nforecasting within a single architecture. Built on a transformer-based\nencoder-processor-decoder backbone, SCENT introduces learnable queries to\nenhance generalization and a query-wise cross-attention mechanism to\neffectively capture multi-scale dependencies. To ensure scalability in both\ndata size and model complexity, we incorporate a sparse attention mechanism,\nenabling flexible output representations and efficient evaluation at arbitrary\nresolutions. We validate SCENT through extensive simulations and real-world\nexperiments, demonstrating state-of-the-art performance across multiple\nchallenging tasks while achieving superior scalability."
    },
    {
        "date": "2025-04",
        "title": "Human Aligned Compression for Robust Models",
        "author": "Samuel R\u00e4ber, Andreas Plesner, Till Aczel, and Roger Wattenhofer",
        "link": "http://arxiv.org/abs/2504.12255v1",
        "abstract": "Adversarial attacks on image models threaten system robustness by introducing\nimperceptible perturbations that cause incorrect predictions. We investigate\nhuman-aligned learned lossy compression as a defense mechanism, comparing two\nlearned models (HiFiC and ELIC) against traditional JPEG across various quality\nlevels. Our experiments on ImageNet subsets demonstrate that learned\ncompression methods outperform JPEG, particularly for Vision Transformer\narchitectures, by preserving semantically meaningful content while removing\nadversarial noise. Even in white-box settings where attackers can access the\ndefense, these methods maintain substantial effectiveness. We also show that\nsequential compression--applying rounds of\ncompression/decompression--significantly enhances defense efficacy while\nmaintaining classification performance. Our findings reveal that human-aligned\ncompression provides an effective, computationally efficient defense that\nprotects the image features most relevant to human and machine understanding.\nIt offers a practical approach to improving model robustness against\nadversarial threats."
    },
    {
        "date": "2025-04",
        "title": "SALAD: Improving Robustness and Generalization through Contrastive Learning with Structure-Aware and LLM-Driven Augmented Data",
        "author": "Suyoung Bae, Hyojun Kim, YunSeok Choi, and Jee-Hyong Lee",
        "link": "http://arxiv.org/abs/2504.12185v1",
        "abstract": "In various natural language processing (NLP) tasks, fine-tuning Pre-trained\nLanguage Models (PLMs) often leads to the issue of spurious correlations, which\nnegatively impacts performance, particularly when dealing with\nout-of-distribution data. To address this problem, we propose SALAD}(Structure\nAware and LLM-driven Augmented Data), a novel approach designed to enhance\nmodel robustness and generalization by generating structure-aware and\ncounterfactually augmented data for contrastive learning. Our method leverages\na tagging-based approach to generate structure-aware positive samples and\nutilizes large language models (LLMs) to generate counterfactual negative\nsamples with diverse sentence patterns. By applying contrastive learning, SALAD\nenables the model to focus on learning the structural relationships between key\nsentence components while minimizing reliance on spurious correlations. We\nvalidate our approach through experiments on three tasks: Sentiment\nClassification, Sexism Detection, and Natural Language Inference. The results\ndemonstrate that SALAD not only improves model robustness and performance\nacross different environments but also enhances generalization to\nout-of-distribution datasets and cross-domain scenarios."
    },
    {
        "date": "2025-04",
        "title": "Secure Transfer Learning: Training Clean Models Against Backdoor in (Both) Pre-trained Encoders and Downstream Datasets",
        "author": "Yechao Zhang, Yuxuan Zhou, Tianyu Li, Minghui Li, Shengshan Hu, Wei Luo, and Leo Yu Zhang",
        "link": "http://arxiv.org/abs/2504.11990v1",
        "abstract": "Transfer learning from pre-trained encoders has become essential in modern\nmachine learning, enabling efficient model adaptation across diverse tasks.\nHowever, this combination of pre-training and downstream adaptation creates an\nexpanded attack surface, exposing models to sophisticated backdoor embeddings\nat both the encoder and dataset levels--an area often overlooked in prior\nresearch. Additionally, the limited computational resources typically available\nto users of pre-trained encoders constrain the effectiveness of generic\nbackdoor defenses compared to end-to-end training from scratch. In this work,\nwe investigate how to mitigate potential backdoor risks in resource-constrained\ntransfer learning scenarios. Specifically, we conduct an exhaustive analysis of\nexisting defense strategies, revealing that many follow a reactive workflow\nbased on assumptions that do not scale to unknown threats, novel attack types,\nor different training paradigms. In response, we introduce a proactive mindset\nfocused on identifying clean elements and propose the Trusted Core (T-Core)\nBootstrapping framework, which emphasizes the importance of pinpointing\ntrustworthy data and neurons to enhance model security. Our empirical\nevaluations demonstrate the effectiveness and superiority of T-Core,\nspecifically assessing 5 encoder poisoning attacks, 7 dataset poisoning\nattacks, and 14 baseline defenses across five benchmark datasets, addressing\nfour scenarios of 3 potential backdoor threats."
    },
    {
        "date": "2025-04",
        "title": "Securing the Skies: A Comprehensive Survey on Anti-UAV Methods, Benchmarking, and Future Directions",
        "author": "Yifei Dong, Fengyi Wu, Sanjian Zhang, Guangyu Chen, Yuzhi Hu, Masumi Yano, Jingdong Sun, Siyu Huang, Feng Liu, Qi Dai, and Zhi-Qi Cheng",
        "link": "http://arxiv.org/abs/2504.11967v2",
        "abstract": "Unmanned Aerial Vehicles (UAVs) are indispensable for infrastructure\ninspection, surveillance, and related tasks, yet they also introduce critical\nsecurity challenges. This survey provides a wide-ranging examination of the\nanti-UAV domain, centering on three core objectives-classification, detection,\nand tracking-while detailing emerging methodologies such as diffusion-based\ndata synthesis, multi-modal fusion, vision-language modeling, self-supervised\nlearning, and reinforcement learning. We systematically evaluate\nstate-of-the-art solutions across both single-modality and multi-sensor\npipelines (spanning RGB, infrared, audio, radar, and RF) and discuss\nlarge-scale as well as adversarially oriented benchmarks. Our analysis reveals\npersistent gaps in real-time performance, stealth detection, and swarm-based\nscenarios, underscoring pressing needs for robust, adaptive anti-UAV systems.\nBy highlighting open research directions, we aim to foster innovation and guide\nthe development of next-generation defense strategies in an era marked by the\nextensive use of UAVs."
    },
    {
        "date": "2025-04",
        "title": "Robust and Fine-Grained Detection of AI Generated Texts",
        "author": "Ram Mohan Rao Kadiyala, Siddartha Pullakhandam, Kanwal Mehreen, Drishti Sharma, Siddhant Gupta, Jebish Purbey, Ashay Srivastava, Subhasya TippaReddy, Arvind Reddy Bobbili, Suraj Telugara Chandrashekhar, Modabbir Adeeb, Srinadh Vura, and Hamza Farooq",
        "link": "http://arxiv.org/abs/2504.11952v1",
        "abstract": "An ideal detection system for machine generated content is supposed to work\nwell on any generator as many more advanced LLMs come into existence day by\nday. Existing systems often struggle with accurately identifying AI-generated\ncontent over shorter texts. Further, not all texts might be entirely authored\nby a human or LLM, hence we focused more over partial cases i.e human-LLM\nco-authored texts. Our paper introduces a set of models built for the task of\ntoken classification which are trained on an extensive collection of\nhuman-machine co-authored texts, which performed well over texts of unseen\ndomains, unseen generators, texts by non-native speakers and those with\nadversarial inputs. We also introduce a new dataset of over 2.4M such texts\nmostly co-authored by several popular proprietary LLMs over 23 languages. We\nalso present findings of our models' performance over each texts of each domain\nand generator. Additional findings include comparison of performance against\neach adversarial method, length of input texts and characteristics of generated\ntexts compared to the original human authored texts."
    },
    {
        "date": "2025-04",
        "title": "Flow Intelligence: Robust Feature Matching via Temporal Signature Correlation",
        "author": "Jie Wang, Chen Ye Gan, Caoqi Wei, Jiangtao Wen, and Yuxing Han",
        "link": "http://arxiv.org/abs/2504.11949v1",
        "abstract": "Feature matching across video streams remains a cornerstone challenge in\ncomputer vision. Increasingly, robust multimodal matching has garnered interest\nin robotics, surveillance, remote sensing, and medical imaging. While\ntraditional rely on detecting and matching spatial features, they break down\nwhen faced with noisy, misaligned, or cross-modal data. Recent deep learning\nmethods have improved robustness through learned representations, but remain\nconstrained by their dependence on extensive training data and computational\ndemands. We present Flow Intelligence, a paradigm-shifting approach that moves\nbeyond spatial features by focusing on temporal motion patterns exclusively.\nInstead of detecting traditional keypoints, our method extracts motion\nsignatures from pixel blocks across consecutive frames and extract temporal\nmotion signatures between videos. These motion-based descriptors achieve\nnatural invariance to translation, rotation, and scale variations while\nremaining robust across different imaging modalities. This novel approach also\nrequires no pretraining data, eliminates the need for spatial feature\ndetection, enables cross-modal matching using only temporal motion, and it\noutperforms existing methods in challenging scenarios where traditional\napproaches fail. By leveraging motion rather than appearance, Flow Intelligence\nenables robust, real-time video feature matching in diverse environments."
    },
    {
        "date": "2025-04",
        "title": "SemDiff: Generating Natural Unrestricted Adversarial Examples via Semantic Attributes Optimization in Diffusion Models",
        "author": "Zeyu Dai, Shengcai Liu, Rui He, Jiahao Wu, Ning Lu, Wenqi Fan, Qing Li, and Ke Tang",
        "link": "http://arxiv.org/abs/2504.11923v1",
        "abstract": "Unrestricted adversarial examples (UAEs), allow the attacker to create\nnon-constrained adversarial examples without given clean samples, posing a\nsevere threat to the safety of deep learning models. Recent works utilize\ndiffusion models to generate UAEs. However, these UAEs often lack naturalness\nand imperceptibility due to simply optimizing in intermediate latent noises. In\nlight of this, we propose SemDiff, a novel unrestricted adversarial attack that\nexplores the semantic latent space of diffusion models for meaningful\nattributes, and devises a multi-attributes optimization approach to ensure\nattack success while maintaining the naturalness and imperceptibility of\ngenerated UAEs. We perform extensive experiments on four tasks on three\nhigh-resolution datasets, including CelebA-HQ, AFHQ and ImageNet. The results\ndemonstrate that SemDiff outperforms state-of-the-art methods in terms of\nattack success rate and imperceptibility. The generated UAEs are natural and\nexhibit semantically meaningful changes, in accord with the attributes'\nweights. In addition, SemDiff is found capable of evading different defenses,\nwhich further validates its effectiveness and threatening."
    },
    {
        "date": "2025-04",
        "title": "From Data Behavior to Code Analysis: A Multimodal Study on Security and Privacy Challenges in Blockchain-Based DApp",
        "author": "Haoyang Sun, Yishun Wang, and Xiaoqi Li",
        "link": "http://arxiv.org/abs/2504.11860v1",
        "abstract": "The recent proliferation of blockchain-based decentralized applications\n(DApp) has catalyzed transformative advancements in distributed systems, with\nextensive deployments observed across financial, entertainment, media, and\ncybersecurity domains. These trustless architectures, characterized by their\ndecentralized nature and elimination of third-party intermediaries, have\ngarnered substantial institutional attention. Consequently, the escalating\nsecurity challenges confronting DApp demand rigorous scholarly investigation.\nThis study initiates with a systematic analysis of behavioral patterns derived\nfrom empirical DApp datasets, establishing foundational insights for subsequent\nmethodological developments. The principal security vulnerabilities in\nEthereum-based smart contracts developed via Solidity are then critically\nexamined. Specifically, reentrancy vulnerability attacks are addressed by\nformally representing contract logic using highly expressive code fragments.\nThis enables precise source code-level detection via bidirectional long\nshort-term memory networks with attention mechanisms (BLSTM-ATT). Regarding\nprivacy preservation challenges, contemporary solutions are evaluated through\ndual analytical lenses: identity privacy preservation and transaction anonymity\nenhancement, while proposing future research trajectories in cryptographic\nobfuscation techniques."
    },
    {
        "date": "2025-04",
        "title": "On the Feasibility of Using MultiModal LLMs to Execute AR Social Engineering Attacks",
        "author": "Ting Bi, Chenghang Ye, Zheyu Yang, Ziyi Zhou, Cui Tang, Jun Zhang, Zui Tao, Kailong Wang, Liting Zhou, Yang Yang, and Tianlong Yu",
        "link": "http://arxiv.org/abs/2504.13209v1",
        "abstract": "Augmented Reality (AR) and Multimodal Large Language Models (LLMs) are\nrapidly evolving, providing unprecedented capabilities for human-computer\ninteraction. However, their integration introduces a new attack surface for\nsocial engineering. In this paper, we systematically investigate the\nfeasibility of orchestrating AR-driven Social Engineering attacks using\nMultimodal LLM for the first time, via our proposed SEAR framework, which\noperates through three key phases: (1) AR-based social context synthesis, which\nfuses Multimodal inputs (visual, auditory and environmental cues); (2)\nrole-based Multimodal RAG (Retrieval-Augmented Generation), which dynamically\nretrieves and integrates contextual data while preserving character\ndifferentiation; and (3) ReInteract social engineering agents, which execute\nadaptive multiphase attack strategies through inference interaction loops. To\nverify SEAR, we conducted an IRB-approved study with 60 participants in three\nexperimental configurations (unassisted, AR+LLM, and full SEAR pipeline)\ncompiling a new dataset of 180 annotated conversations in simulated social\nscenarios. Our results show that SEAR is highly effective at eliciting\nhigh-risk behaviors (e.g., 93.3% of participants susceptible to email\nphishing). The framework was particularly effective in building trust, with 85%\nof targets willing to accept an attacker's call after an interaction. Also, we\nidentified notable limitations such as ``occasionally artificial'' due to\nperceived authenticity gaps. This work provides proof-of-concept for AR-LLM\ndriven social engineering attacks and insights for developing defensive\ncountermeasures against next-generation augmented reality threats."
    },
    {
        "date": "2025-04",
        "title": "From Cyber Threat to Data Shield: Constructing Provably Secure File Erasure with Repurposed Ransomware Cryptography",
        "author": "Jiahui Shang, Luning Zhang, and Zhongxiang Zheng",
        "link": "http://arxiv.org/abs/2504.11744v1",
        "abstract": "Ransomware has emerged as a persistent cybersecurity threat,leveraging robust\nencryption schemes that often remain unbroken even after public disclosure of\nsource code. Motivated by the technical resilience of such mechanisms, this\npaper presents SEER (Secure and Efficient Encryption-based Erasure via\nRansomware), a provably secure file destruction system that repurposes\nransomware encryption for legitimate data erasure tasks. SEER integrates the\ntriple-encryption design of the Babuk ransomware family, including\nCurve25519-based key exchange,SHA-256-based key derivation, and the Sosemanuk\nstream cipher, to construct a layered key management architecture. It tightly\ncouples encryption and key destruction by securely erasing session keys\nimmediately after use. Experimental results on an ESXI platform demonstrate\nthat SEER achieves four orders of magnitude performance improvement over the\nDoD 5220.22 standard. The proposed system further ensures provable security\nthrough both theoretical foundations and practical validation, offering an\nefficient and resilient solution for the secure destruction of sensitive data."
    },
    {
        "date": "2025-04",
        "title": "Towards Safe Synthetic Image Generation On the Web: A Multimodal Robust NSFW Defense and Million Scale Dataset",
        "author": "Muhammad Shahid Muneer, and Simon S. Woo",
        "link": "http://arxiv.org/abs/2504.11707v1",
        "abstract": "In the past years, we have witnessed the remarkable success of Text-to-Image\n(T2I) models and their widespread use on the web. Extensive research in making\nT2I models produce hyper-realistic images has led to new concerns, such as\ngenerating Not-Safe-For-Work (NSFW) web content and polluting the web society.\nTo help prevent misuse of T2I models and create a safer web environment for\nusers features like NSFW filters and post-hoc security checks are used in these\nmodels. However, recent work unveiled how these methods can easily fail to\nprevent misuse. In particular, adversarial attacks on text and image modalities\ncan easily outplay defensive measures. %Exploiting such leads to the growing\nconcern of preventing adversarial attacks on text and image modalities.\nMoreover, there is currently no robust multimodal NSFW dataset that includes\nboth prompt and image pairs and adversarial examples. This work proposes a\nmillion-scale prompt and image dataset generated using open-source diffusion\nmodels. Second, we develop a multimodal defense to distinguish safe and NSFW\ntext and images, which is robust against adversarial attacks and directly\nalleviates current challenges. Our extensive experiments show that our model\nperforms well against existing SOTA NSFW detection methods in terms of accuracy\nand recall, drastically reducing the Attack Success Rate (ASR) in multimodal\nadversarial attack scenarios. Code:\nhttps://github.com/shahidmuneer/multimodal-nsfw-defense."
    },
    {
        "date": "2025-04",
        "title": "An Online Adaptation Method for Robust Depth Estimation and Visual Odometry in the Open World",
        "author": "Xingwu Ji, Haochen Niu, Dexin Duan, Rendong Ying, Fei Wen, and Peilin Liu",
        "link": "http://arxiv.org/abs/2504.11698v1",
        "abstract": "Recently, learning-based robotic navigation systems have gained extensive\nresearch attention and made significant progress. However, the diversity of\nopen-world scenarios poses a major challenge for the generalization of such\nsystems to practical scenarios. Specifically, learned systems for scene\nmeasurement and state estimation tend to degrade when the application scenarios\ndeviate from the training data, resulting to unreliable depth and pose\nestimation. Toward addressing this problem, this work aims to develop a visual\nodometry system that can fast adapt to diverse novel environments in an online\nmanner. To this end, we construct a self-supervised online adaptation framework\nfor monocular visual odometry aided by an online-updated depth estimation\nmodule. Firstly, we design a monocular depth estimation network with\nlightweight refiner modules, which enables efficient online adaptation. Then,\nwe construct an objective for self-supervised learning of the depth estimation\nmodule based on the output of the visual odometry system and the contextual\nsemantic information of the scene. Specifically, a sparse depth densification\nmodule and a dynamic consistency enhancement module are proposed to leverage\ncamera poses and contextual semantics to generate pseudo-depths and valid masks\nfor the online adaptation. Finally, we demonstrate the robustness and\ngeneralization capability of the proposed method in comparison with\nstate-of-the-art learning-based approaches on urban, in-house datasets and a\nrobot platform. Code is publicly available at:\nhttps://github.com/jixingwu/SOL-SLAM."
    },
    {
        "date": "2025-04",
        "title": "WaterFlow: Learning Fast & Robust Watermarks using Stable Diffusion",
        "author": "Vinay Shukla, Prachee Sharma, Ryan Rossi, Sungchul Kim, Tong Yu, and Aditya Grover",
        "link": "http://arxiv.org/abs/2504.12354v2",
        "abstract": "The ability to embed watermarks in images is a fundamental problem of\ninterest for computer vision, and is exacerbated by the rapid rise of generated\nimagery in recent times. Current state-of-the-art techniques suffer from\ncomputational and statistical challenges such as the slow execution speed for\npractical deployments. In addition, other works trade off fast watermarking\nspeeds but suffer greatly in their robustness or perceptual quality. In this\nwork, we propose WaterFlow (WF), a fast and extremely robust approach for high\nfidelity visual watermarking based on a learned latent-dependent watermark. Our\napproach utilizes a pretrained latent diffusion model to encode an arbitrary\nimage into a latent space and produces a learned watermark that is then planted\ninto the Fourier Domain of the latent. The transformation is specified via\ninvertible flow layers that enhance the expressivity of the latent space of the\npre-trained model to better preserve image quality while permitting robust and\ntractable detection. Most notably, WaterFlow demonstrates state-of-the-art\nperformance on general robustness and is the first method capable of\neffectively defending against difficult combination attacks. We validate our\nfindings on three widely used real and generated datasets: MS-COCO,\nDiffusionDB, and WikiArt."
    },
    {
        "date": "2025-04",
        "title": "Cybersecurity through Entropy Injection: A Paradigm Shift from Reactive Defense to Proactive Uncertainty",
        "author": "Kush Janani",
        "link": "http://arxiv.org/abs/2504.11661v1",
        "abstract": "Cybersecurity often hinges on unpredictability, with a system's defenses\nbeing strongest when sensitive values and behaviors cannot be anticipated by\nattackers. This paper explores the concept of entropy injection-deliberately\ninfusing randomness into security mechanisms to increase unpredictability and\nenhance system security. We examine the theoretical foundations of\nentropy-based security, analyze real-world implementations including Address\nSpace Layout Randomization (ASLR) and Moving Target Defense (MTD) frameworks,\nevaluate practical challenges in implementation, and compare entropy-based\napproaches with traditional security methods. Our methodology includes a\nsystematic analysis of entropy's role across various security domains, from\ncryptographic operations to system-level defenses. Results demonstrate that\nentropy injection can significantly reduce attack probability, with some\nimplementations showing more than 90% reduction with minimal performance\nimpact. The discussion highlights the trade-offs between security benefits and\noperational complexity, while identifying future directions for\nentropy-enhanced security, including integration with artificial intelligence\nand quantum randomness sources. We conclude that entropy injection represents a\nparadigm shift from reactive defense to proactive uncertainty management,\noffering a strategic approach that can fundamentally alter the balance between\nattackers and defenders in cybersecurity."
    },
    {
        "date": "2025-04",
        "title": "Chypnosis: Stealthy Secret Extraction using Undervolting-based Static Side-channel Attacks",
        "author": "Kyle Mitard, Saleh Khalaj Monfared, Fatemeh Khojasteh Dana, and Shahin Tajik",
        "link": "http://arxiv.org/abs/2504.11633v2",
        "abstract": "There is a growing class of static physical side-channel attacks that allow\nadversaries to extract secrets by probing the persistent state of a circuit.\nTechniques such as laser logic state imaging (LLSI), impedance analysis (IA),\nand static power analysis fall into this category. These attacks require that\nthe targeted data remain constant for a specific duration, which often\nnecessitates halting the circuit's clock. Some methods additionally rely on\nmodulating the chip's supply voltage to probe the circuit. However, tampering\nwith the clock or voltage is typically assumed to be detectable, as secure\nchips often deploy sensors that erase sensitive data upon detecting such\nanomalies. Furthermore, many secure devices use internal clock sources, making\nexternal clock control infeasible. In this work, we introduce a novel class of\nstatic side-channel attacks, called Chypnosis, that enables adversaries to\nfreeze a chip's internal clock by inducing a hibernation state via rapid\nundervolting, and then extracting secrets using static side-channels. We\ndemonstrate that, by rapidly dropping a chip's voltage below the standard\nnominal levels, the attacker can bypass the clock and voltage sensors and put\nthe chip in a so-called brownout condition, in which the chip's transistors\nstop switching, but volatile memories (e.g., Flip-flops and SRAMs) still retain\ntheir data. We test our attack on AMD FPGAs by putting them into hibernation.\nWe show that not only are all clock sources deactivated, but various clock and\nvoltage sensors also fail to detect the tamper event. Afterward, we present the\nsuccessful recovery of secret bits from a hibernated chip using two static\nattacks, namely, LLSI and IA. Finally, we discuss potential countermeasures\nwhich could be integrated into future designs."
    },
    {
        "date": "2025-04",
        "title": "Making Acoustic Side-Channel Attacks on Noisy Keyboards Viable with LLM-Assisted Spectrograms' \"Typo\" Correction",
        "author": "Seyyed Ali Ayati, Jin Hyun Park, Yichen Cai, and Marcus Botacin",
        "link": "http://arxiv.org/abs/2504.11622v1",
        "abstract": "The large integration of microphones into devices increases the opportunities\nfor Acoustic Side-Channel Attacks (ASCAs), as these can be used to capture\nkeystrokes' audio signals that might reveal sensitive information. However, the\ncurrent State-Of-The-Art (SOTA) models for ASCAs, including Convolutional\nNeural Networks (CNNs) and hybrid models, such as CoAtNet, still exhibit\nlimited robustness under realistic noisy conditions. Solving this problem\nrequires either: (i) an increased model's capacity to infer contextual\ninformation from longer sequences, allowing the model to learn that an\ninitially noisily typed word is the same as a futurely collected non-noisy\nword, or (ii) an approach to fix misidentified information from the contexts,\nas one does not type random words, but the ones that best fit the conversation\ncontext. In this paper, we demonstrate that both strategies are viable and\ncomplementary solutions for making ASCAs practical. We observed that no\nexisting solution leverages advanced transformer architectures' power for these\ntasks and propose that: (i) Visual Transformers (VTs) are the candidate\nsolutions for capturing long-term contextual information and (ii)\ntransformer-powered Large Language Models (LLMs) are the candidate solutions to\nfix the ``typos'' (mispredictions) the model might make. Thus, we here present\nthe first-of-its-kind approach that integrates VTs and LLMs for ASCAs.\n  We first show that VTs achieve SOTA performance in classifying keystrokes\nwhen compared to the previous CNN benchmark. Second, we demonstrate that LLMs\ncan mitigate the impact of real-world noise. Evaluations on the natural\nsentences revealed that: (i) incorporating LLMs (e.g., GPT-4o) in our ASCA\npipeline boosts the performance of error-correction tasks; and (ii) the\ncomparable performance can be attained by a lightweight, fine-tuned smaller LLM\n(67 times smaller than GPT-4o), using..."
    },
    {
        "date": "2025-04",
        "title": "Robust Markov stability for community detection at a scale learned based on the structure",
        "author": "Samin Aref, and Sanchaai Mathiyarasan",
        "link": "http://arxiv.org/abs/2504.11621v1",
        "abstract": "Community detection, the unsupervised task of clustering nodes of a graph,\nfinds applications across various fields. The common approaches for community\ndetection involve optimizing an objective function to partition the nodes into\ncommunities at a single scale of granularity. However, the single-scale\napproaches often fall short of producing partitions that are robust and at a\nsuitable scale. The existing algorithm, PyGenStability, returns multiple robust\npartitions for a network by optimizing the multi-scale Markov stability\nfunction. However, in cases where the suitable scale is not known or assumed by\nthe user, there is no principled method to select a single robust partition at\na suitable scale from the multiple partitions that PyGenStability produces. Our\nproposed method combines the Markov stability framework with a pre-trained\nmachine learning model for scale selection to obtain one robust partition at a\nscale that is learned based on the graph structure. This automatic scale\nselection involves using a gradient boosting model pre-trained on hand-crafted\nand embedding-based network features from a labeled dataset of 10k benchmark\nnetworks. This model was trained to predicts the scale value that maximizes the\nsimilarity of the output partition to the planted partition of the benchmark\nnetwork. Combining our scale selection algorithm with the PyGenStability\nalgorithm results in PyGenStabilityOne (PO): a hyperparameter-free multi-scale\ncommunity detection algorithm that returns one robust partition at a suitable\nscale without the need for any assumptions, input, or tweaking from the user.\nWe compare the performance of PO against 29 algorithms and show that it\noutperforms 25 other algorithms by statistically meaningful margins. Our\nresults facilitate choosing between community detection algorithms, among which\nPO stands out as the accurate, robust, and hyperparameter-free method."
    },
    {
        "date": "2025-04",
        "title": "ADT: Tuning Diffusion Models with Adversarial Supervision",
        "author": "Dazhong Shen, Guanglu Song, Yi Zhang, Bingqi Ma, Lujundong Li, Dongzhi Jiang, Zhuofan Zong, and Yu Liu",
        "link": "http://arxiv.org/abs/2504.11423v1",
        "abstract": "Diffusion models have achieved outstanding image generation by reversing a\nforward noising process to approximate true data distributions. During\ntraining, these models predict diffusion scores from noised versions of true\nsamples in a single forward pass, while inference requires iterative denoising\nstarting from white noise. This training-inference divergences hinder the\nalignment between inference and training data distributions, due to potential\nprediction biases and cumulative error accumulation. To address this problem,\nwe propose an intuitive but effective fine-tuning framework, called Adversarial\nDiffusion Tuning (ADT), by stimulating the inference process during\noptimization and aligning the final outputs with training data by adversarial\nsupervision. Specifically, to achieve robust adversarial training, ADT features\na siamese-network discriminator with a fixed pre-trained backbone and\nlightweight trainable parameters, incorporates an image-to-image sampling\nstrategy to smooth discriminative difficulties, and preserves the original\ndiffusion loss to prevent discriminator hacking. In addition, we carefully\nconstrain the backward-flowing path for back-propagating gradients along the\ninference path without incurring memory overload or gradient explosion.\nFinally, extensive experiments on Stable Diffusion models (v1.5, XL, and v3),\ndemonstrate that ADT significantly improves both distribution alignment and\nimage quality."
    },
    {
        "date": "2025-04",
        "title": "Robustness and sex differences in skin cancer detection: logistic regression vs CNNs",
        "author": "Nikolette Pedersen, Regitze Sydendal, Andreas Wulff, Ralf Raumanns, Eike Petersen, and Veronika Cheplygina",
        "link": "http://arxiv.org/abs/2504.11415v1",
        "abstract": "Deep learning has been reported to achieve high performances in the detection\nof skin cancer, yet many challenges regarding the reproducibility of results\nand biases remain. This study is a replication (different data, same analysis)\nof a study on Alzheimer's disease [28] which studied robustness of logistic\nregression (LR) and convolutional neural networks (CNN) across patient sexes.\nWe explore sex bias in skin cancer detection, using the PAD-UFES-20 dataset\nwith LR trained on handcrafted features reflecting dermatological guidelines\n(ABCDE and the 7-point checklist), and a pre-trained ResNet-50 model. We\nevaluate these models in alignment with [28]: across multiple training datasets\nwith varied sex composition to determine their robustness. Our results show\nthat both the LR and the CNN were robust to the sex distributions, but the\nresults also revealed that the CNN had a significantly higher accuracy (ACC)\nand area under the receiver operating characteristics (AUROC) for male patients\nthan for female patients. We hope these findings to contribute to the growing\nfield of investigating potential bias in popular medical machine learning\nmethods. The data and relevant scripts to reproduce our results can be found in\nour Github."
    },
    {
        "date": "2025-04",
        "title": "DataSentinel: A Game-Theoretic Detection of Prompt Injection Attacks",
        "author": "Yupei Liu, Yuqi Jia, Jinyuan Jia, Dawn Song, and Neil Zhenqiang Gong",
        "link": "http://arxiv.org/abs/2504.11358v1",
        "abstract": "LLM-integrated applications and agents are vulnerable to prompt injection\nattacks, where an attacker injects prompts into their inputs to induce\nattacker-desired outputs. A detection method aims to determine whether a given\ninput is contaminated by an injected prompt. However, existing detection\nmethods have limited effectiveness against state-of-the-art attacks, let alone\nadaptive ones. In this work, we propose DataSentinel, a game-theoretic method\nto detect prompt injection attacks. Specifically, DataSentinel fine-tunes an\nLLM to detect inputs contaminated with injected prompts that are strategically\nadapted to evade detection. We formulate this as a minimax optimization\nproblem, with the objective of fine-tuning the LLM to detect strong adaptive\nattacks. Furthermore, we propose a gradient-based method to solve the minimax\noptimization problem by alternating between the inner max and outer min\nproblems. Our evaluation results on multiple benchmark datasets and LLMs show\nthat DataSentinel effectively detects both existing and adaptive prompt\ninjection attacks."
    },
    {
        "date": "2025-04",
        "title": "X-Teaming: Multi-Turn Jailbreaks and Defenses with Adaptive Multi-Agents",
        "author": "Salman Rahman, Liwei Jiang, James Shiffer, Genglin Liu, Sheriff Issaka, Md Rizwan Parvez, Hamid Palangi, Kai-Wei Chang, Yejin Choi, and Saadia Gabriel",
        "link": "http://arxiv.org/abs/2504.13203v1",
        "abstract": "Multi-turn interactions with language models (LMs) pose critical safety\nrisks, as harmful intent can be strategically spread across exchanges. Yet, the\nvast majority of prior work has focused on single-turn safety, while\nadaptability and diversity remain among the key challenges of multi-turn\nred-teaming. To address these challenges, we present X-Teaming, a scalable\nframework that systematically explores how seemingly harmless interactions\nescalate into harmful outcomes and generates corresponding attack scenarios.\nX-Teaming employs collaborative agents for planning, attack optimization, and\nverification, achieving state-of-the-art multi-turn jailbreak effectiveness and\ndiversity with success rates up to 98.1% across representative leading\nopen-weight and closed-source models. In particular, X-Teaming achieves a 96.2%\nattack success rate against the latest Claude 3.7 Sonnet model, which has been\nconsidered nearly immune to single-turn attacks. Building on X-Teaming, we\nintroduce XGuard-Train, an open-source multi-turn safety training dataset that\nis 20x larger than the previous best resource, comprising 30K interactive\njailbreaks, designed to enable robust multi-turn safety alignment for LMs. Our\nwork offers essential tools and insights for mitigating sophisticated\nconversational attacks, advancing the multi-turn safety of LMs."
    },
    {
        "date": "2025-04",
        "title": "R-TPT: Improving Adversarial Robustness of Vision-Language Models through Test-Time Prompt Tuning",
        "author": "Lijun Sheng, Jian Liang, Zilei Wang, and Ran He",
        "link": "http://arxiv.org/abs/2504.11195v1",
        "abstract": "Vision-language models (VLMs), such as CLIP, have gained significant\npopularity as foundation models, with numerous fine-tuning methods developed to\nenhance performance on downstream tasks. However, due to their inherent\nvulnerability and the common practice of selecting from a limited set of\nopen-source models, VLMs suffer from a higher risk of adversarial attacks than\ntraditional vision models. Existing defense techniques typically rely on\nadversarial fine-tuning during training, which requires labeled data and lacks\nof flexibility for downstream tasks. To address these limitations, we propose\nrobust test-time prompt tuning (R-TPT), which mitigates the impact of\nadversarial attacks during the inference stage. We first reformulate the\nclassic marginal entropy objective by eliminating the term that introduces\nconflicts under adversarial conditions, retaining only the pointwise entropy\nminimization. Furthermore, we introduce a plug-and-play reliability-based\nweighted ensembling strategy, which aggregates useful information from reliable\naugmented views to strengthen the defense. R-TPT enhances defense against\nadversarial attacks without requiring labeled training data while offering high\nflexibility for inference tasks. Extensive experiments on widely used\nbenchmarks with various attacks demonstrate the effectiveness of R-TPT. The\ncode is available in https://github.com/TomSheng21/R-TPT."
    },
    {
        "date": "2025-04",
        "title": "Exploring Backdoor Attack and Defense for LLM-empowered Recommendations",
        "author": "Liangbo Ning, Wenqi Fan, and Qing Li",
        "link": "http://arxiv.org/abs/2504.11182v1",
        "abstract": "The fusion of Large Language Models (LLMs) with recommender systems (RecSys)\nhas dramatically advanced personalized recommendations and drawn extensive\nattention. Despite the impressive progress, the safety of LLM-based RecSys\nagainst backdoor attacks remains largely under-explored. In this paper, we\nraise a new problem: Can a backdoor with a specific trigger be injected into\nLLM-based Recsys, leading to the manipulation of the recommendation responses\nwhen the backdoor trigger is appended to an item's title? To investigate the\nvulnerabilities of LLM-based RecSys under backdoor attacks, we propose a new\nattack framework termed Backdoor Injection Poisoning for RecSys (BadRec).\nBadRec perturbs the items' titles with triggers and employs several fake users\nto interact with these items, effectively poisoning the training set and\ninjecting backdoors into LLM-based RecSys. Comprehensive experiments reveal\nthat poisoning just 1% of the training data with adversarial examples is\nsufficient to successfully implant backdoors, enabling manipulation of\nrecommendations. To further mitigate such a security threat, we propose a\nuniversal defense strategy called Poison Scanner (P-Scanner). Specifically, we\nintroduce an LLM-based poison scanner to detect the poisoned items by\nleveraging the powerful language understanding and rich knowledge of LLMs. A\ntrigger augmentation agent is employed to generate diverse synthetic triggers\nto guide the poison scanner in learning domain-specific knowledge of the\npoisoned item detection task. Extensive experiments on three real-world\ndatasets validate the effectiveness of the proposed P-Scanner."
    },
    {
        "date": "2025-04",
        "title": "KubeFence: Security Hardening of the Kubernetes Attack Surface",
        "author": "Carmine Cesarano, and Roberto Natella",
        "link": "http://arxiv.org/abs/2504.11126v1",
        "abstract": "Kubernetes (K8s) is widely used to orchestrate containerized applications,\nincluding critical services in domains such as finance, healthcare, and\ngovernment. However, its extensive and feature-rich API interface exposes a\nbroad attack surface, making K8s vulnerable to exploits of software\nvulnerabilities and misconfigurations. Even if K8s adopts role-based access\ncontrol (RBAC) to manage access to K8s APIs, this approach lacks the\ngranularity needed to protect specification attributes within API requests.\nThis paper proposes a novel solution, KubeFence, which implements finer-grain\nAPI filtering tailored to specific client workloads. KubeFence analyzes\nKubernetes Operators from trusted repositories and leverages their\nconfiguration files to restrict unnecessary features of the K8s API, to\nmitigate misconfigurations and vulnerabilities exploitable through the K8s API.\nThe experimental results show that KubeFence can significantly reduce the\nattack surface and prevent attacks compared to RBAC."
    },
    {
        "date": "2025-04",
        "title": "FLSSM: A Federated Learning Storage Security Model with Homomorphic Encryption",
        "author": "Yang Li, Chunhe Xia, Chang Li, Xiaojian Li, and Tianbo Wang",
        "link": "http://arxiv.org/abs/2504.11088v1",
        "abstract": "Federated learning based on homomorphic encryption has received widespread\nattention due to its high security and enhanced protection of user data\nprivacy. However, the characteristics of encrypted computation lead to three\nchallenging problems: ``computation-efficiency\", ``attack-tracing\" and\n``contribution-assessment\". The first refers to the efficiency of encrypted\ncomputation during model aggregation, the second refers to tracing malicious\nattacks in an encrypted state, and the third refers to the fairness of\ncontribution assessment for local models after encryption. This paper proposes\na federated learning storage security model with homomorphic encryption (FLSSM)\nto protect federated learning model privacy and address the three issues\nmentioned above. First, we utilize different nodes to aggregate local models in\nparallel, thereby improving encrypted models' aggregation efficiency. Second,\nwe introduce trusted supervise nodes to examine local models when the global\nmodel is attacked, enabling the tracing of malicious attacks under homomorphic\nencryption. Finally, we fairly reward local training nodes with encrypted local\nmodels based on trusted training time. Experiments on multiple real-world\ndatasets show that our model significantly outperforms baseline models in terms\nof both efficiency and security metrics."
    },
    {
        "date": "2025-04",
        "title": "Improving fingerprint presentation attack detection by an approach integrated into the personal verification stage",
        "author": "Marco Micheletto, Giulia Orr\u00f9, Luca Ghiani, and Gian Luca Marcialis",
        "link": "http://arxiv.org/abs/2504.11066v1",
        "abstract": "Presentation Attack Detection (PAD) systems are usually designed\nindependently of the fingerprint verification system. While this can be\nacceptable for use cases where specific user templates are not predetermined,\nit represents a missed opportunity to enhance security in scenarios where\nintegrating PAD with the fingerprint verification system could significantly\nleverage users' templates, which are the real target of a potential\npresentation attack. This does not mean that a PAD should be specifically\ndesigned for such users; that would imply the availability of many enrolled\nusers' PAI and, consequently, complexity, time, and cost increase. On the\ncontrary, we propose to equip a basic PAD, designed according to the state of\nthe art, with an innovative add-on module called the Closeness Binary Code (CC)\nmodule. The term \"closeness\" refers to a peculiar property of the bona\nfide-related features: in an Euclidean feature space, genuine fingerprints tend\nto cluster in a specific pattern. First, samples from the same finger are close\nto each other, then samples from other fingers of the same user and finally,\nsamples from fingers of other users. This property is statistically verified in\nour previous publication, and further confirmed in this paper. It is\nindependent of the user population and the feature set class, which can be\nhandcrafted or deep network-based (embeddings). Therefore, the add-on can be\ndesigned without the need for the targeted user samples; moreover, it exploits\nher/his samples' \"closeness\" property during the verification stage. Extensive\nexperiments on benchmark datasets and state-of-the-art PAD methods confirm the\nbenefits of the proposed add-on, which can be easily coupled with the main PAD\nmodule integrated into the fingerprint verification system."
    },
    {
        "date": "2025-04",
        "title": "RAID: An In-Training Defense against Attribute Inference Attacks in Recommender Systems",
        "author": "Xiaohua Feng, Yuyuan Li, Fengyuan Yu, Ke Xiong, Junjie Fang, Li Zhang, Tianyu Du, and Chaochao Chen",
        "link": "http://arxiv.org/abs/2504.11510v1",
        "abstract": "In various networks and mobile applications, users are highly susceptible to\nattribute inference attacks, with particularly prevalent occurrences in\nrecommender systems. Attackers exploit partially exposed user profiles in\nrecommendation models, such as user embeddings, to infer private attributes of\ntarget users, such as gender and political views. The goal of defenders is to\nmitigate the effectiveness of these attacks while maintaining recommendation\nperformance. Most existing defense methods, such as differential privacy and\nattribute unlearning, focus on post-training settings, which limits their\ncapability of utilizing training data to preserve recommendation performance.\nAlthough adversarial training extends defenses to in-training settings, it\noften struggles with convergence due to unstable training processes. In this\npaper, we propose RAID, an in-training defense method against attribute\ninference attacks in recommender systems. In addition to the recommendation\nobjective, we define a defensive objective to ensure that the distribution of\nprotected attributes becomes independent of class labels, making users\nindistinguishable from attribute inference attacks. Specifically, this\ndefensive objective aims to solve a constrained Wasserstein barycenter problem\nto identify the centroid distribution that makes the attribute\nindistinguishable while complying with recommendation performance constraints.\nTo optimize our proposed objective, we use optimal transport to align users\nwith the centroid distribution. We conduct extensive experiments on four\nreal-world datasets to evaluate RAID. The experimental results validate the\neffectiveness of RAID and demonstrate its significant superiority over existing\nmethods in multiple aspects."
    },
    {
        "date": "2025-04",
        "title": "QAVA: Query-Agnostic Visual Attack to Large Vision-Language Models",
        "author": "Yudong Zhang, Ruobing Xie, Jiansheng Chen, Xingwu Sun, Zhanhui Kang, and Yu Wang",
        "link": "http://arxiv.org/abs/2504.11038v1",
        "abstract": "In typical multimodal tasks, such as Visual Question Answering (VQA),\nadversarial attacks targeting a specific image and question can lead large\nvision-language models (LVLMs) to provide incorrect answers. However, it is\ncommon for a single image to be associated with multiple questions, and LVLMs\nmay still answer other questions correctly even for an adversarial image\nattacked by a specific question. To address this, we introduce the\nquery-agnostic visual attack (QAVA), which aims to create robust adversarial\nexamples that generate incorrect responses to unspecified and unknown\nquestions. Compared to traditional adversarial attacks focused on specific\nimages and questions, QAVA significantly enhances the effectiveness and\nefficiency of attacks on images when the question is unknown, achieving\nperformance comparable to attacks on known target questions. Our research\nbroadens the scope of visual adversarial attacks on LVLMs in practical\nsettings, uncovering previously overlooked vulnerabilities, particularly in the\ncontext of visual adversarial threats. The code is available at\nhttps://github.com/btzyd/qava."
    },
    {
        "date": "2025-04",
        "title": "Defending Against Frequency-Based Attacks with Diffusion Models",
        "author": "Fatemeh Amerehi, and Patrick Healy",
        "link": "http://arxiv.org/abs/2504.11034v1",
        "abstract": "Adversarial training is a common strategy for enhancing model robustness\nagainst adversarial attacks. However, it is typically tailored to the specific\nattack types it is trained on, limiting its ability to generalize to unseen\nthreat models. Adversarial purification offers an alternative by leveraging a\ngenerative model to remove perturbations before classification. Since the\npurifier is trained independently of both the classifier and the threat models,\nit is better equipped to handle previously unseen attack scenarios. Diffusion\nmodels have proven highly effective for noise purification, not only in\ncountering pixel-wise adversarial perturbations but also in addressing\nnon-adversarial data shifts. In this study, we broaden the focus beyond\npixel-wise robustness to explore the extent to which purification can mitigate\nboth spectral and spatial adversarial attacks. Our findings highlight its\neffectiveness in handling diverse distortion patterns across low- to\nhigh-frequency regions."
    },
    {
        "date": "2025-04",
        "title": "CDUPatch: Color-Driven Universal Adversarial Patch Attack for Dual-Modal Visible-Infrared Detectors",
        "author": "Jiahuan Long, Wen Yao, Tingsong Jiang, and Chao Ma",
        "link": "http://arxiv.org/abs/2504.10888v1",
        "abstract": "Adversarial patches are widely used to evaluate the robustness of object\ndetection systems in real-world scenarios. These patches were initially\ndesigned to deceive single-modal detectors (e.g., visible or infrared) and have\nrecently been extended to target visible-infrared dual-modal detectors.\nHowever, existing dual-modal adversarial patch attacks have limited attack\neffectiveness across diverse physical scenarios. To address this, we propose\nCDUPatch, a universal cross-modal patch attack against visible-infrared object\ndetectors across scales, views, and scenarios. Specifically, we observe that\ncolor variations lead to different levels of thermal absorption, resulting in\ntemperature differences in infrared imaging. Leveraging this property, we\npropose an RGB-to-infrared adapter that maps RGB patches to infrared patches,\nenabling unified optimization of cross-modal patches. By learning an optimal\ncolor distribution on the adversarial patch, we can manipulate its thermal\nresponse and generate an adversarial infrared texture. Additionally, we\nintroduce a multi-scale clipping strategy and construct a new visible-infrared\ndataset, MSDrone, which contains aerial vehicle images in varying scales and\nperspectives. These data augmentation strategies enhance the robustness of our\npatch in real-world conditions. Experiments on four benchmark datasets (e.g.,\nDroneVehicle, LLVIP, VisDrone, MSDrone) show that our method outperforms\nexisting patch attacks in the digital domain. Extensive physical tests further\nconfirm strong transferability across scales, views, and scenarios."
    },
    {
        "date": "2025-04",
        "title": "DAAF:Degradation-Aware Adaptive Fusion Framework for Robust Infrared and Visible Images Fusion",
        "author": "Tianpei Zhang, Jufeng Zhao, Yiming Zhu, Guangmang Cui, Yuxin Jing, and Yuhan Lyu",
        "link": "http://arxiv.org/abs/2504.10871v1",
        "abstract": "Existing infrared and visible image fusion(IVIF) algorithms often prioritize\nhigh-quality images, neglecting image degradation such as low light and noise,\nwhich limits the practical potential. This paper propose Degradation-Aware\nAdaptive image Fusion (DAAF), which achieves unified modeling of adaptive\ndegradation optimization and image fusion. Specifically, DAAF comprises an\nauxiliary Adaptive Degradation Optimization Network (ADON) and a Feature\nInteractive Local-Global Fusion (FILGF) Network. Firstly, ADON includes\ninfrared and visible-light branches. Within the infrared branch,\nfrequency-domain feature decomposition and extraction are employed to isolate\nGaussian and stripe noise. In the visible-light branch, Retinex decomposition\nis applied to extract illumination and reflectance components, enabling\ncomplementary enhancement of detail and illumination distribution.\nSubsequently, FILGF performs interactive multi-scale local-global feature\nfusion. Local feature fusion consists of intra-inter model feature complement,\nwhile global feature fusion is achieved through a interactive cross-model\nattention. Extensive experiments have shown that DAAF outperforms current IVIF\nalgorithms in normal and complex degradation scenarios."
    },
    {
        "date": "2025-04",
        "title": "How to Enhance Downstream Adversarial Robustness (almost) without Touching the Pre-Trained Foundation Model?",
        "author": "Meiqi Liu, Zhuoqun Huang, and Yue Xing",
        "link": "http://arxiv.org/abs/2504.10850v1",
        "abstract": "With the rise of powerful foundation models, a pre-training-fine-tuning\nparadigm becomes increasingly popular these days: A foundation model is\npre-trained using a huge amount of data from various sources, and then the\ndownstream users only need to fine-tune and adapt it to specific downstream\ntasks. However, due to the high computation complexity of adversarial training,\nit is not feasible to fine-tune the foundation model to improve its robustness\non the downstream task. Observing the above challenge, we want to improve the\ndownstream robustness without updating/accessing the weights in the foundation\nmodel. Inspired from existing literature in robustness inheritance (Kim et al.,\n2020), through theoretical investigation, we identify a close relationship\nbetween robust contrastive learning with the adversarial robustness of\nsupervised learning. To further validate and utilize this theoretical insight,\nwe design a simple-yet-effective robust auto-encoder as a data pre-processing\nmethod before feeding the data into the foundation model. The proposed approach\nhas zero access to the foundation model when training the robust auto-encoder.\nExtensive experiments demonstrate the effectiveness of the proposed method in\nimproving the robustness of downstream tasks, verifying the connection between\nthe feature robustness (implied by small adversarial contrastive loss) and the\nrobustness of the downstream task."
    },
    {
        "date": "2025-04",
        "title": "Concept Enhancement Engineering: A Lightweight and Efficient Robust Defense Against Jailbreak Attacks in Embodied AI",
        "author": "Jirui Yang, Zheyu Lin, Shuhan Yang, Zhihui Lu, and Xin Du",
        "link": "http://arxiv.org/abs/2504.13201v1",
        "abstract": "Embodied Intelligence (EI) systems integrated with large language models\n(LLMs) face significant security risks, particularly from jailbreak attacks\nthat manipulate models into generating harmful outputs or executing unsafe\nphysical actions. Traditional defense strategies, such as input filtering and\noutput monitoring, often introduce high computational overhead or interfere\nwith task performance in real-time embodied scenarios. To address these\nchallenges, we propose Concept Enhancement Engineering (CEE), a novel defense\nframework that leverages representation engineering to enhance the safety of\nembodied LLMs by dynamically steering their internal activations. CEE operates\nby (1) extracting multilingual safety patterns from model activations, (2)\nconstructing control directions based on safety-aligned concept subspaces, and\n(3) applying subspace concept rotation to reinforce safe behavior during\ninference. Our experiments demonstrate that CEE effectively mitigates jailbreak\nattacks while maintaining task performance, outperforming existing defense\nmethods in both robustness and efficiency. This work contributes a scalable and\ninterpretable safety mechanism for embodied AI, bridging the gap between\ntheoretical representation engineering and practical security applications. Our\nfindings highlight the potential of latent-space interventions as a viable\ndefense paradigm against emerging adversarial threats in physically grounded AI\nsystems."
    },
    {
        "date": "2025-04",
        "title": "Efficient and Robust Remote Sensing Image Denoising Using Randomized Approximation of Geodesics' Gramian on the Manifold Underlying the Patch Space",
        "author": "Kelum Gajamannage, Dilhani I. Jayathilake, and Maria Vasilyeva",
        "link": "http://arxiv.org/abs/2504.10820v1",
        "abstract": "Remote sensing images are widely utilized in many disciplines such as feature\nrecognition and scene semantic segmentation. However, due to environmental\nfactors and the issues of the imaging system, the image quality is often\ndegraded which may impair subsequent visual tasks. Even though denoising remote\nsensing images plays an essential role before applications, the current\ndenoising algorithms fail to attain optimum performance since these images\npossess complex features in the texture. Denoising frameworks based on\nartificial neural networks have shown better performance; however, they require\nexhaustive training with heterogeneous samples that extensively consume\nresources like power, memory, computation, and latency. Thus, here we present a\ncomputationally efficient and robust remote sensing image denoising method that\ndoesn't require additional training samples. This method partitions patches of\na remote-sensing image in which a low-rank manifold, representing the\nnoise-free version of the image, underlies the patch space. An efficient and\nrobust approach to revealing this manifold is a randomized approximation of the\nsingular value spectrum of the geodesics' Gramian matrix of the patch space.\nThe method asserts a unique emphasis on each color channel during denoising so\nthe three denoised channels are merged to produce the final image."
    },
    {
        "date": "2025-04",
        "title": "The Sword of Damocles in ViTs: Computational Redundancy Amplifies Adversarial Transferability",
        "author": "Jiani Liu, Zhiyuan Wang, Zeliang Zhang, Chao Huang, Susan Liang, Yunlong Tang, and Chenliang Xu",
        "link": "http://arxiv.org/abs/2504.10804v1",
        "abstract": "Vision Transformers (ViTs) have demonstrated impressive performance across a\nrange of applications, including many safety-critical tasks. However, their\nunique architectural properties raise new challenges and opportunities in\nadversarial robustness. In particular, we observe that adversarial examples\ncrafted on ViTs exhibit higher transferability compared to those crafted on\nCNNs, suggesting that ViTs contain structural characteristics favorable for\ntransferable attacks. In this work, we investigate the role of computational\nredundancy in ViTs and its impact on adversarial transferability. Unlike prior\nstudies that aim to reduce computation for efficiency, we propose to exploit\nthis redundancy to improve the quality and transferability of adversarial\nexamples. Through a detailed analysis, we identify two forms of redundancy,\nincluding the data-level and model-level, that can be harnessed to amplify\nattack effectiveness. Building on this insight, we design a suite of\ntechniques, including attention sparsity manipulation, attention head\npermutation, clean token regularization, ghost MoE diversification, and\ntest-time adversarial training. Extensive experiments on the ImageNet-1k\ndataset validate the effectiveness of our approach, showing that our methods\nsignificantly outperform existing baselines in both transferability and\ngenerality across diverse model architectures."
    },
    {
        "date": "2025-04",
        "title": "Wasserstein Distributionally Robust Regret Optimization",
        "author": "Lukas-Benedikt Fiechtner, and Jose Blanchet",
        "link": "http://arxiv.org/abs/2504.10796v3",
        "abstract": "Distributionally Robust Optimization (DRO) is a popular framework for\ndecision-making under uncertainty, but its adversarial nature can lead to\noverly conservative solutions. To address this, we study ex-ante\nDistributionally Robust Regret Optimization (DRRO), focusing on\nWasserstein-based ambiguity sets which are popular due to their links to\nregularization and machine learning. We provide a systematic analysis of\nWasserstein DRRO, paralleling known results for Wasserstein DRO. Under\nsmoothness and regularity conditions, we show that Wasserstein DRRO coincides\nwith Empirical Risk Minimization (ERM) up to first-order terms, and exactly so\nin convex quadratic settings. We revisit the Wasserstein DRRO newsvendor\nproblem, where the loss is the maximum of two linear functions of demand and\ndecision. Extending [25], we show that the regret can be computed by maximizing\ntwo one-dimensional concave functions. For more general loss functions\ninvolving the maximum of multiple linear terms in multivariate random variables\nand decision vectors, we prove that computing the regret and thus also the DRRO\npolicy is NP-hard. We then propose a convex relaxation for these more general\nWasserstein DRRO problems and demonstrate its strong empirical performance.\nFinally, we provide an upper bound on the optimality gap of our relaxation and\nshow it improves over recent alternatives."
    },
    {
        "date": "2025-04",
        "title": "Domain-Adversarial Neural Network and Explainable AI for Reducing Tissue-of-Origin Signal in Pan-cancer Mortality Classification",
        "author": "Cristian Padron-Manrique, Juan Jos\u00e9 Oropeza Valdez, and Osbaldo Resendis-Antonio",
        "link": "http://arxiv.org/abs/2504.10343v1",
        "abstract": "Tissue-of-origin signals dominate pan-cancer gene expression, often obscuring\nmolecular features linked to patient survival. This hampers the discovery of\ngeneralizable biomarkers, as models tend to overfit tissue-specific patterns\nrather than capture survival-relevant signals. To address this, we propose a\nDomain-Adversarial Neural Network (DANN) trained on TCGA RNA-seq data to learn\nrepresentations less biased by tissue and more focused on survival. Identifying\ntissue-independent genetic profiles is key to revealing core cancer programs.\nWe assess the DANN using: (1) Standard SHAP, based on the original input space\nand DANN's mortality classifier; (2) A layer-aware strategy applied to hidden\nactivations, including an unsupervised manifold from raw activations and a\nsupervised manifold from mortality-specific SHAP values. Standard SHAP remains\nconfounded by tissue signals due to biases inherent in its computation. The raw\nactivation manifold was dominated by high-magnitude activations, which masked\nsubtle tissue and mortality-related signals. In contrast, the layer-aware SHAP\nmanifold offers improved low-dimensional representations of both tissue and\nmortality signals, independent of activation strength, enabling subpopulation\nstratification and pan-cancer identification of survival-associated genes."
    },
    {
        "date": "2025-04",
        "title": "Shield Bash: Abusing Defensive Coherence State Retrieval to Break Timing Obfuscation",
        "author": "Kartik Ramkrishnan, Antonia Zhai, Stephen McCamant, and Pen Chung Yew",
        "link": "http://arxiv.org/abs/2504.10318v1",
        "abstract": "Microarchitectural attacks are a significant concern, leading to many\nhardware-based defense proposals. However, different defenses target different\nclasses of attacks, and their impact on each other has not been fully\nconsidered. To raise awareness of this problem, we study an interaction between\ntwo state-of-the art defenses in this paper, timing obfuscations of remote\ncache lines (TORC) and delaying speculative changes to remote cache lines\n(DSRC). TORC mitigates cache-hit based attacks and DSRC mitigates speculative\ncoherence state change attacks.\n  We observe that DSRC enables coherence information to be retrieved into the\nprocessor core, where it is out of the reach of timing obfuscations to protect.\nThis creates an unforeseen consequence that redo operations can be triggered\nwithin the core to detect the presence or absence of remote cache lines, which\nconstitutes a security vulnerability. We demonstrate that a new covert channel\nattack is possible using this vulnerability. We propose two ways to mitigate\nthe attack, whose performance varies depending on an application's cache usage.\nOne way is to never send remote exclusive coherence state (E) information to\nthe core even if it is created. The other way is to never create a remote E\nstate, which is responsible for triggering redos.\n  We demonstrate the timing difference caused by this microarchitectural\ndefense assumption violation using GEM5 simulations. Performance evaluation on\nSPECrate 2017 and PARSEC benchmarks of the two fixes show less than 32\\%\naverage overhead across both sets of benchmarks. The repair which prevented the\ncreation of remote E state had less than 2.8% average overhead."
    },
    {
        "date": "2025-04",
        "title": "ROSFD: Robust Online Streaming Fraud Detection with Resilience to Concept Drift in Data Streams",
        "author": "Vivek Yelleti",
        "link": "http://arxiv.org/abs/2504.10229v1",
        "abstract": "Continuous generation of streaming data from diverse sources, such as online\ntransactions and digital interactions, necessitates timely fraud detection.\nTraditional batch processing methods often struggle to capture the rapidly\nevolving patterns of fraudulent activities. This paper highlights the critical\nimportance of processing streaming data for effective fraud detection. To\naddress the inherent challenges of latency, scalability, and concept drift in\nstreaming environments, we propose a robust online streaming fraud detection\n(ROSFD) framework. Our proposed framework comprises two key stages: (i) Stage\nOne: Offline Model Initialization. In this initial stage, a model is built in\noffline settings using incremental learning principles to overcome the\n\"cold-start\" problem. (ii) Stage Two: Real-time Model Adaptation. In this\ndynamic stage, drift detection algorithms (viz.,, DDM, EDDM, and ADWIN) are\nemployed to identify concept drift in the incoming data stream and\nincrementally train the model accordingly. This \"train-only-when-required\"\nstrategy drastically reduces the number of retrains needed without\nsignificantly impacting the area under the receiver operating characteristic\ncurve (AUC). Overall, ROSFD utilizing ADWIN as the drift detection method\ndemonstrated the best performance among the employed methods. In terms of model\nefficacy, Adaptive Random Forest consistently outperformed other models,\nachieving the highest AUC in four out of five datasets."
    },
    {
        "date": "2025-04",
        "title": "The Future of MLLM Prompting is Adaptive: A Comprehensive Experimental Evaluation of Prompt Engineering Methods for Robust Multimodal Performance",
        "author": "Anwesha Mohanty, Venkatesh Balavadhani Parthasarathy, and Arsalan Shahid",
        "link": "http://arxiv.org/abs/2504.10179v1",
        "abstract": "Multimodal Large Language Models (MLLMs) are set to transform how machines\nprocess and generate human-like responses by integrating diverse modalities\nsuch as text, images, and code. Yet, effectively harnessing their capabilities\nhinges on optimal prompt engineering. We present a comprehensive experimental\nevaluation of seven prompt engineering methods applied to 13 open-source MLLMs\nover 24 tasks spanning Reasoning and Compositionality, Multimodal Understanding\nand Alignment, Complex Code Generation and Execution, and Knowledge Retrieval\nand Integration. Our approach stratifies models by parameter count into Small\n(<4B), Medium (4B-10B), and Large (>10B) categories and compares prompting\ntechniques including Zero-Shot, One-Shot, Few-Shot, Chain-of-Thought,\nAnalogical, Generated Knowledge, and Tree-of-Thought. While Large MLLMs excel\nin structured tasks such as code generation, achieving accuracies up to 96.88%\nunder Few-Shot prompting, all models struggle with complex reasoning and\nabstract understanding, often yielding accuracies below 60% and high\nhallucination rates. Structured reasoning prompts frequently increased\nhallucination up to 75% in small models and led to longer response times (over\n20 seconds in Large MLLMs), while simpler prompting methods provided more\nconcise and efficient outputs. No single prompting method uniformly optimises\nall task types. Instead, adaptive strategies combining example-based guidance\nwith selective structured reasoning are essential to enhance robustness,\nefficiency, and factual accuracy. Our findings offer practical recommendations\nfor prompt engineering and support more reliable deployment of MLLMs across\napplications including AI-assisted coding, knowledge retrieval, and multimodal\ncontent understanding."
    },
    {
        "date": "2025-04",
        "title": "Benchmarking Practices in LLM-driven Offensive Security: Testbeds, Metrics, and Experiment Design",
        "author": "Andreas Happe, and J\u00fcrgen Cito",
        "link": "http://arxiv.org/abs/2504.10112v1",
        "abstract": "Large Language Models (LLMs) have emerged as a powerful approach for driving\noffensive penetration-testing tooling. This paper analyzes the methodology and\nbenchmarking practices used for evaluating Large Language Model (LLM)-driven\nattacks, focusing on offensive uses of LLMs in cybersecurity. We review 16\nresearch papers detailing 15 prototypes and their respective testbeds.\n  We detail our findings and provide actionable recommendations for future\nresearch, emphasizing the importance of extending existing testbeds, creating\nbaselines, and including comprehensive metrics and qualitative analysis. We\nalso note the distinction between security research and practice, suggesting\nthat CTF-based challenges may not fully represent real-world penetration\ntesting scenarios."
    },
    {
        "date": "2025-04",
        "title": "Dual-Path Enhancements in Event-Based Eye Tracking: Augmented Robustness and Adaptive Temporal Modeling",
        "author": "Hoang M. Truong, Vinh-Thuan Ly, Huy G. Tran, Thuan-Phat Nguyen, and Tram T. Doan",
        "link": "http://arxiv.org/abs/2504.09960v1",
        "abstract": "Event-based eye tracking has become a pivotal technology for augmented\nreality and human-computer interaction. Yet, existing methods struggle with\nreal-world challenges such as abrupt eye movements and environmental noise.\nBuilding on the efficiency of the Lightweight Spatiotemporal Network-a causal\narchitecture optimized for edge devices-we introduce two key advancements.\nFirst, a robust data augmentation pipeline incorporating temporal shift,\nspatial flip, and event deletion improves model resilience, reducing Euclidean\ndistance error by 12% (1.61 vs. 1.70 baseline) on challenging samples. Second,\nwe propose KnightPupil, a hybrid architecture combining an EfficientNet-B3\nbackbone for spatial feature extraction, a bidirectional GRU for contextual\ntemporal modeling, and a Linear Time-Varying State-Space Module to adapt to\nsparse inputs and noise dynamically. Evaluated on the 3ET+ benchmark, our\nframework achieved 1.61 Euclidean distance on the private test set of the\nEvent-based Eye Tracking Challenge at CVPR 2025, demonstrating its\neffectiveness for practical deployment in AR/VR systems while providing a\nfoundation for future innovations in neuromorphic vision."
    },
    {
        "date": "2025-04",
        "title": "LangPert: Detecting and Handling Task-level Perturbations for Robust Object Rearrangement",
        "author": "Xu Yin, Min-Sung Yoon, Yuchi Huo, Kang Zhang, and Sung-Eui Yoon",
        "link": "http://arxiv.org/abs/2504.09893v1",
        "abstract": "Task execution for object rearrangement could be challenged by Task-Level\nPerturbations (TLP), i.e., unexpected object additions, removals, and\ndisplacements that can disrupt underlying visual policies and fundamentally\ncompromise task feasibility and progress. To address these challenges, we\npresent LangPert, a language-based framework designed to detect and mitigate\nTLP situations in tabletop rearrangement tasks. LangPert integrates a Visual\nLanguage Model (VLM) to comprehensively monitor policy's skill execution and\nenvironmental TLP, while leveraging the Hierarchical Chain-of-Thought (HCoT)\nreasoning mechanism to enhance the Large Language Model (LLM)'s contextual\nunderstanding and generate adaptive, corrective skill-execution plans. Our\nexperimental results demonstrate that LangPert handles diverse TLP situations\nmore effectively than baseline methods, achieving higher task completion rates,\nimproved execution efficiency, and potential generalization to unseen\nscenarios."
    },
    {
        "date": "2025-04",
        "title": "Revisiting the attacker's knowledge in inference attacks against Searchable Symmetric Encryption",
        "author": "Marc Damie, Jean-Benoist Leger, Florian Hahn, and Andreas Peter",
        "link": "http://arxiv.org/abs/2504.09879v1",
        "abstract": "Encrypted search schemes have been proposed to address growing privacy\nconcerns. However, several leakage-abuse attacks have highlighted some security\nvulnerabilities. Recent attacks assumed an attacker's knowledge containing data\n``similar'' to the indexed data. However, this vague assumption is barely\ndiscussed in literature: how likely is it for an attacker to obtain a \"similar\nenough\" data?\n  Our paper provides novel statistical tools usable on any attack in this\nsetting to analyze its sensitivity to data similarity. First, we introduce a\nmathematical model based on statistical estimators to analytically understand\nthe attackers' knowledge and the notion of similarity. Second, we conceive\nstatistical tools to model the influence of the similarity on the attack\naccuracy. We apply our tools on three existing attacks to answer questions such\nas: is similarity the only factor influencing accuracy of a given attack?\nThird, we show that the enforcement of a maximum index size can make the\n``similar-data'' assumption harder to satisfy. In particular, we propose a\nstatistical method to estimate an appropriate maximum size for a given attack\nand dataset. For the best known attack on the Enron dataset, a maximum index\nsize of 200 guarantees (with high probability) the attack accuracy to be below\n5%."
    },
    {
        "date": "2025-04",
        "title": "StruPhantom: Evolutionary Injection Attacks on Black-Box Tabular Agents Powered by Large Language Models",
        "author": "Yang Feng, and Xudong Pan",
        "link": "http://arxiv.org/abs/2504.09841v1",
        "abstract": "The proliferation of autonomous agents powered by large language models\n(LLMs) has revolutionized popular business applications dealing with tabular\ndata, i.e., tabular agents. Although LLMs are observed to be vulnerable against\nprompt injection attacks from external data sources, tabular agents impose\nstrict data formats and predefined rules on the attacker's payload, which are\nineffective unless the agent navigates multiple layers of structural data to\nincorporate the payload. To address the challenge, we present a novel attack\ntermed StruPhantom which specifically targets black-box LLM-powered tabular\nagents. Our attack designs an evolutionary optimization procedure which\ncontinually refines attack payloads via the proposed constrained Monte Carlo\nTree Search augmented by an off-topic evaluator. StruPhantom helps\nsystematically explore and exploit the weaknesses of target applications to\nachieve goal hijacking. Our evaluation validates the effectiveness of\nStruPhantom across various LLM-based agents, including those on real-world\nplatforms, and attack scenarios. Our attack achieves over 50% higher success\nrates than baselines in enforcing the application's response to contain\nphishing links or malicious codes."
    },
    {
        "date": "2025-04",
        "title": "SafeSpeech: Robust and Universal Voice Protection Against Malicious Speech Synthesis",
        "author": "Zhisheng Zhang, Derui Wang, Qianyi Yang, Pengyang Huang, Junhan Pu, Yuxin Cao, Kai Ye, Jie Hao, and Yixian Yang",
        "link": "http://arxiv.org/abs/2504.09839v1",
        "abstract": "Speech synthesis technology has brought great convenience, while the\nwidespread usage of realistic deepfake audio has triggered hazards. Malicious\nadversaries may unauthorizedly collect victims' speeches and clone a similar\nvoice for illegal exploitation (\\textit{e.g.}, telecom fraud). However, the\nexisting defense methods cannot effectively prevent deepfake exploitation and\nare vulnerable to robust training techniques. Therefore, a more effective and\nrobust data protection method is urgently needed. In response, we propose a\ndefensive framework, \\textit{\\textbf{SafeSpeech}}, which protects the users'\naudio before uploading by embedding imperceptible perturbations on original\nspeeches to prevent high-quality synthetic speech. In SafeSpeech, we devise a\nrobust and universal proactive protection technique, \\textbf{S}peech\n\\textbf{PE}rturbative \\textbf{C}oncealment (\\textbf{SPEC}), that leverages a\nsurrogate model to generate universally applicable perturbation for generative\nsynthetic models. Moreover, we optimize the human perception of embedded\nperturbation in terms of time and frequency domains. To evaluate our method\ncomprehensively, we conduct extensive experiments across advanced models and\ndatasets, both subjectively and objectively. Our experimental results\ndemonstrate that SafeSpeech achieves state-of-the-art (SOTA) voice protection\neffectiveness and transferability and is highly robust against advanced\nadaptive adversaries. Moreover, SafeSpeech has real-time capability in\nreal-world tests. The source code is available at\n\\href{https://github.com/wxzyd123/SafeSpeech}{https://github.com/wxzyd123/SafeSpeech}."
    },
    {
        "date": "2025-04",
        "title": "Socratic Chart: Cooperating Multiple Agents for Robust SVG Chart Understanding",
        "author": "Yuyang Ji, and Haohan Wang",
        "link": "http://arxiv.org/abs/2504.09764v1",
        "abstract": "Multimodal Large Language Models (MLLMs) have shown remarkable versatility\nbut face challenges in demonstrating true visual understanding, particularly in\nchart reasoning tasks. Existing benchmarks like ChartQA reveal significant\nreliance on text-based shortcuts and probabilistic pattern-matching rather than\ngenuine visual reasoning. To rigorously evaluate visual reasoning, we introduce\na more challenging test scenario by removing textual labels and introducing\nchart perturbations in the ChartQA dataset. Under these conditions, models like\nGPT-4o and Gemini-2.0 Pro experience up to a 30% performance drop, underscoring\ntheir limitations. To address these challenges, we propose Socratic Chart, a\nnew framework that transforms chart images into Scalable Vector Graphics (SVG)\nrepresentations, enabling MLLMs to integrate textual and visual modalities for\nenhanced chart understanding. Socratic Chart employs a multi-agent pipeline\nwith specialized agent-generators to extract primitive chart attributes (e.g.,\nbar heights, line coordinates) and an agent-critic to validate results,\nensuring high-fidelity symbolic representations. Our framework surpasses\nstate-of-the-art models in accurately capturing chart primitives and improving\nreasoning performance, establishing a robust pathway for advancing MLLM visual\nunderstanding."
    },
    {
        "date": "2025-04",
        "title": "Enhancing Classifier Evaluation: A Fairer Benchmarking Strategy Based on Ability and Robustness",
        "author": "Lucas Cardoso, Vitor Santos, Jos\u00e9 Ribeiro, Regiane Kawasaki, Ricardo Prud\u00eancio, and Ronnie Alves",
        "link": "http://arxiv.org/abs/2504.09759v1",
        "abstract": "Benchmarking is a fundamental practice in machine learning (ML) for comparing\nthe performance of classification algorithms. However, traditional evaluation\nmethods often overlook a critical aspect: the joint consideration of dataset\ncomplexity and an algorithm's ability to generalize. Without this dual\nperspective, assessments may favor models that perform well on easy instances\nwhile failing to capture their true robustness. To address this limitation,\nthis study introduces a novel evaluation methodology that combines Item\nResponse Theory (IRT) with the Glicko-2 rating system, originally developed to\nmeasure player strength in competitive games. IRT assesses classifier ability\nbased on performance over difficult instances, while Glicko-2 updates\nperformance metrics - such as rating, deviation, and volatility - via simulated\ntournaments between classifiers. This combined approach provides a fairer and\nmore nuanced measure of algorithm capability. A case study using the\nOpenML-CC18 benchmark showed that only 15% of the datasets are truly\nchallenging and that a reduced subset with 50% of the original datasets offers\ncomparable evaluation power. Among the algorithms tested, Random Forest\nachieved the highest ability score. The results highlight the importance of\nimproving benchmark design by focusing on dataset quality and adopting\nevaluation strategies that reflect both difficulty and classifier proficiency."
    },
    {
        "date": "2025-04",
        "title": "Transformer-Based Representation Learning for Robust Gene Expression Modeling and Cancer Prognosis",
        "author": "Shuai Jiang, and Saeed Hassanpour",
        "link": "http://arxiv.org/abs/2504.09704v1",
        "abstract": "Transformer-based models have achieved remarkable success in natural language\nand vision tasks, but their application to gene expression analysis remains\nlimited due to data sparsity, high dimensionality, and missing values. We\npresent GexBERT, a transformer-based autoencoder framework for robust\nrepresentation learning of gene expression data. GexBERT learns context-aware\ngene embeddings by pretraining on large-scale transcriptomic profiles with a\nmasking and restoration objective that captures co-expression relationships\namong thousands of genes. We evaluate GexBERT across three critical tasks in\ncancer research: pan-cancer classification, cancer-specific survival\nprediction, and missing value imputation. GexBERT achieves state-of-the-art\nclassification accuracy from limited gene subsets, improves survival prediction\nby restoring expression of prognostic anchor genes, and outperforms\nconventional imputation methods under high missingness. Furthermore, its\nattention-based interpretability reveals biologically meaningful gene patterns\nacross cancer types. These findings demonstrate the utility of GexBERT as a\nscalable and effective tool for gene expression modeling, with translational\npotential in settings where gene coverage is limited or incomplete."
    },
    {
        "date": "2025-04",
        "title": "Adapting to the Unknown: Robust Meta-Learning for Zero-Shot Financial Time Series Forecasting",
        "author": "Anxian Liu, Junying Ma, and Guang Zhang",
        "link": "http://arxiv.org/abs/2504.09664v1",
        "abstract": "Financial time series forecasting in the zero-shot setting is essential for\nrisk management and investment decision-making, particularly during abrupt\nmarket regime shifts or in emerging markets with limited historical data. While\nModel-Agnostic Meta-Learning (MAML)-based approaches have shown promise in this\ndomain, existing meta task construction strategies often lead to suboptimal\nperformance, especially when dealing with highly turbulent financial time\nseries. To address this challenge, we propose a novel task construction method\nthat leverages learned embeddings for more effective meta-learning in the\nzero-shot setting. Specifically, we construct two complementary types of\nmeta-tasks based on the learned embeddings: intra-cluster tasks and\ninter-cluster tasks. To capture diverse fine-grained patterns, we apply\nstochastic projection matrices to the learned embeddings and use clustering\nalgorithm to form the tasks. Additionally, to improve generalization\ncapabilities, we employ hard task mining strategies and leverage inter-cluster\ntasks to identify invariant patterns across different time series. Extensive\nexperiments on the real world financial dataset demonstrate that our method\nsignificantly outperforms existing approaches, showing better generalization\nability in the zero-shot scenario."
    },
    {
        "date": "2025-04",
        "title": "Bridging Immutability with Flexibility: A Scheme for Secure and Efficient Smart Contract Upgrades",
        "author": "Tahrim Hossain, Sakib Hassan, Faisal Haque Bappy, Muhammad Nur Yanhaona, Tarannum Shaila Zaman, and Tariqul Islam",
        "link": "http://arxiv.org/abs/2504.09652v1",
        "abstract": "The emergence of blockchain technology has revolutionized contract execution\nthrough the introduction of smart contracts. Ethereum, the leading blockchain\nplatform, leverages smart contracts to power decentralized applications\n(DApps), enabling transparent and self-executing systems across various\ndomains. While the immutability of smart contracts enhances security and trust,\nit also poses significant challenges for updates, defect resolution, and\nadaptation to changing requirements. Existing upgrade mechanisms are complex,\nresource-intensive, and costly in terms of gas consumption, often compromising\nsecurity and limiting practical adoption. To address these challenges, we\npropose FlexiContracts+, a novel scheme that reimagines smart contracts by\nenabling secure, in-place upgrades on Ethereum while preserving historical data\nwithout relying on multiple contracts or extensive pre-deployment planning.\nFlexiContracts+ enhances security, simplifies development, reduces engineering\noverhead, and supports adaptable, expandable smart contracts. Comprehensive\ntesting demonstrates that FlexiContracts+ achieves a practical balance between\nimmutability and flexibility, advancing the capabilities of smart contract\nsystems."
    },
    {
        "date": "2025-04",
        "title": "RANSAC Revisited: An Improved Algorithm for Robust Subspace Recovery under Adversarial and Noisy Corruptions",
        "author": "Guixian Chen, Jianhao Ma, and Salar Fattahi",
        "link": "http://arxiv.org/abs/2504.09648v1",
        "abstract": "In this paper, we study the problem of robust subspace recovery (RSR) in the\npresence of both strong adversarial corruptions and Gaussian noise.\nSpecifically, given a limited number of noisy samples -- some of which are\ntampered by an adaptive and strong adversary -- we aim to recover a\nlow-dimensional subspace that approximately contains a significant fraction of\nthe uncorrupted samples, up to an error that scales with the Gaussian noise.\nExisting approaches to this problem often suffer from high computational costs\nor rely on restrictive distributional assumptions, limiting their applicability\nin truly adversarial settings. To address these challenges, we revisit the\nclassical random sample consensus (RANSAC) algorithm, which offers strong\nrobustness to adversarial outliers, but sacrifices efficiency and robustness\nagainst Gaussian noise and model misspecification in the process. We propose a\ntwo-stage algorithm, RANSAC+, that precisely pinpoints and remedies the failure\nmodes of standard RANSAC. Our method is provably robust to both Gaussian and\nadversarial corruptions, achieves near-optimal sample complexity without\nrequiring prior knowledge of the subspace dimension, and is more efficient than\nexisting RANSAC-type methods."
    },
    {
        "date": "2025-04",
        "title": "A Secure Communication Protocol for Remote Keyless Entry System with Adaptive Adjustment of Transmission Parameters",
        "author": "Jingjing Guo, Bo Tang, Jiayuan Xu, Qingyi Li, Yuyuan Qin, and Xinghua Li",
        "link": "http://arxiv.org/abs/2504.09527v1",
        "abstract": "Remote Keyless Entry (RKE) systems have become a standard feature in modern\nvehicles, yet their unidirectional fixed-frequency radio communication renders\nthem vulnerable to replay attacks, impersonation attacks, cryptanalysis, and\nintentional interference. Existing cryptographic authentication methods enhance\nsecurity but often fail to address real-world constraints such as computational\nefficiency and radio interference. To mitigate these threats, we designed the\nAdaptive Frequency-Hopping Algorithm and the Adaptive TXP and PHY Mode Control\nAlgorithm that can dynamically optimize channel selection, transmission power,\nand PHY modes based on real-time channel quality assessment. To enhance the\nsecurity and reliability of RKE systems, we propose the Lightweight Vehicle-Key\nAuthentication Protocol. In addition, a prototype of the proposed scheme was\nimplemented to verify its effectiveness in mitigating interference and\npreventing unauthorized access.Experimental results show that our scheme\nsignificantly enhances communication security and reliability while maintaining\nlow computational overhead. Under mild interference conditions, the packet\ndelivery rate (PDR) of the adaptive scheme increases from 93% to 99.23%, and\nunder strong interference, it improves from 85% to 99.01%. Additionally, the\nscheme effectively prevents replay and impersonation attacks, ensuring secure\nvehicle access control by dynamically optimizing communication parameters to\nmaintain stable and reliable transmission."
    },
    {
        "date": "2025-04",
        "title": "CheatAgent: Attacking LLM-Empowered Recommender Systems via LLM Agent",
        "author": "Liang-bo Ning, Shijie Wang, Wenqi Fan, Qing Li, Xin Xu, Hao Chen, and Feiran Huang",
        "link": "http://arxiv.org/abs/2504.13192v2",
        "abstract": "Recently, Large Language Model (LLM)-empowered recommender systems (RecSys)\nhave brought significant advances in personalized user experience and have\nattracted considerable attention. Despite the impressive progress, the research\nquestion regarding the safety vulnerability of LLM-empowered RecSys still\nremains largely under-investigated. Given the security and privacy concerns, it\nis more practical to focus on attacking the black-box RecSys, where attackers\ncan only observe the system's inputs and outputs. However, traditional attack\napproaches employing reinforcement learning (RL) agents are not effective for\nattacking LLM-empowered RecSys due to the limited capabilities in processing\ncomplex textual inputs, planning, and reasoning. On the other hand, LLMs\nprovide unprecedented opportunities to serve as attack agents to attack RecSys\nbecause of their impressive capability in simulating human-like decision-making\nprocesses. Therefore, in this paper, we propose a novel attack framework called\nCheatAgent by harnessing the human-like capabilities of LLMs, where an\nLLM-based agent is developed to attack LLM-Empowered RecSys. Specifically, our\nmethod first identifies the insertion position for maximum impact with minimal\ninput modification. After that, the LLM agent is designed to generate\nadversarial perturbations to insert at target positions. To further improve the\nquality of generated perturbations, we utilize the prompt tuning technique to\nimprove attacking strategies via feedback from the victim RecSys iteratively.\nExtensive experiments across three real-world datasets demonstrate the\neffectiveness of our proposed attacking method."
    },
    {
        "date": "2025-04",
        "title": "PLS-Assisted Offloading for Edge Computing-Enabled Post-Quantum Security in Resource-Constrained Devices",
        "author": "Hamid Amiriara, Mahtab Mirmohseni, and Rahim Tafazolli",
        "link": "http://arxiv.org/abs/2504.09437v1",
        "abstract": "With the advent of post-quantum cryptography (PQC) standards, it has become\nimperative for resource-constrained devices (RCDs) in the Internet of Things\n(IoT) to adopt these quantum-resistant protocols. However, the high\ncomputational overhead and the large key sizes associated with PQC make direct\ndeployment on such devices impractical. To address this challenge, we propose\nan edge computing-enabled PQC framework that leverages a physical-layer\nsecurity (PLS)-assisted offloading strategy, allowing devices to either offload\nintensive cryptographic tasks to a post-quantum edge server (PQES) or perform\nthem locally. Furthermore, to ensure data confidentiality within the edge\ndomain, our framework integrates two PLS techniques: offloading RCDs employ\nwiretap coding to secure data transmission, while non-offloading RCDs serve as\nfriendly jammers by broadcasting artificial noise to disrupt potential\neavesdroppers. Accordingly, we co-design the computation offloading and PLS\nstrategy by jointly optimizing the device transmit power, PQES computation\nresource allocation, and offloading decisions to minimize overall latency under\nresource constraints. Numerical results demonstrate significant latency\nreductions compared to baseline schemes, confirming the scalability and\nefficiency of our approach for secure PQC operations in IoT networks."
    },
    {
        "date": "2025-04",
        "title": "Ensemble-Enhanced Graph Autoencoder with GAT and Transformer-Based Encoders for Robust Fault Diagnosis",
        "author": "Moirangthem Tiken Singh",
        "link": "http://arxiv.org/abs/2504.09427v1",
        "abstract": "Fault classification in industrial machinery is vital for enhancing\nreliability and reducing downtime, yet it remains challenging due to the\nvariability of vibration patterns across diverse operating conditions. This\nstudy introduces a novel graph-based framework for fault classification,\nconverting time-series vibration data from machinery operating at varying\nhorsepower levels into a graph representation. We utilize Shannon's entropy to\ndetermine the optimal window size for data segmentation, ensuring each segment\ncaptures significant temporal patterns, and employ Dynamic Time Warping (DTW)\nto define graph edges based on segment similarity. A Graph Auto Encoder (GAE)\nwith a deep graph transformer encoder, decoder, and ensemble classifier is\ndeveloped to learn latent graph representations and classify faults across\nvarious categories. The GAE's performance is evaluated on the Case Western\nReserve University (CWRU) dataset, with cross-dataset generalization assessed\non the HUST dataset. Results show that GAE achieves a mean F1-score of 0.99 on\nthe CWRU dataset, significantly outperforming baseline models-CNN, LSTM, RNN,\nGRU, and Bi-LSTM (F1-scores: 0.94-0.97, p < 0.05, Wilcoxon signed-rank test for\nBi-LSTM: p < 0.05) -- particularly in challenging classes (e.g., Class 8: 0.99\nvs. 0.71 for Bi-LSTM). Visualization of dataset characteristics reveals that\ndatasets with amplified vibration patterns and diverse fault dynamics enhance\ngeneralization. This framework provides a robust solution for fault diagnosis\nunder varying conditions, offering insights into dataset impacts on model\nperformance."
    },
    {
        "date": "2025-04",
        "title": "Nash Equilibrium Between Consumer Electronic Devices and DoS Attacker for Distributed IoT-enabled RSE Systems",
        "author": "Gengcan Chen, Donghong Cai, Zahid Khan, Jawad Ahmad, and Wadii Boulila",
        "link": "http://arxiv.org/abs/2504.09415v1",
        "abstract": "In electronic consumer Internet of Things (IoT), consumer electronic devices\nas edge devices require less computational overhead and the remote state\nestimation (RSE) of consumer electronic devices is always at risk of\ndenial-of-service (DoS) attacks. Therefore, the adversarial strategy between\nconsumer electronic devices and DoS attackers is critical. This paper focuses\non the adversarial strategy between consumer electronic devices and DoS\nattackers in IoT-enabled RSE Systems. We first propose a remote joint\nestimation model for distributed measurements to effectively reduce consumer\nelectronic device workload and minimize data leakage risks. The Kalman filter\nis deployed on the remote estimator, and the DoS attacks with open-loop as well\nas closed-loop are considered. We further introduce advanced reinforcement\nlearning techniques, including centralized and distributed Minimax-DQN, to\naddress high-dimensional decision-making challenges in both open-loop and\nclosed-loop scenarios. Especially, the Q-network instead of the Q-table is used\nin the proposed approaches, which effectively solves the challenge of\nQ-learning. Moreover, the proposed distributed Minimax-DQN reduces the action\nspace to expedite the search for Nash Equilibrium (NE). The experimental\nresults validate that the proposed model can expeditiously restore the RSE\nerror covariance to a stable state in the presence of DoS attacks, exhibiting\nnotable attack robustness. The proposed centralized and distributed Minimax-DQN\neffectively resolves the NE in both open and closed-loop case, showcasing\nremarkable performance in terms of convergence. It reveals that substantial\nadvantages in both efficiency and stability are achieved compared with the\nstate-of-the-art methods."
    },
    {
        "date": "2025-04",
        "title": "PapMOT: Exploring Adversarial Patch Attack against Multiple Object Tracking",
        "author": "Jiahuan Long, Tingsong Jiang, Wen Yao, Shuai Jia, Weijia Zhang, Weien Zhou, Chao Ma, and Xiaoqian Chen",
        "link": "http://arxiv.org/abs/2504.09361v1",
        "abstract": "Tracking multiple objects in a continuous video stream is crucial for many\ncomputer vision tasks. It involves detecting and associating objects with their\nrespective identities across successive frames. Despite significant progress\nmade in multiple object tracking (MOT), recent studies have revealed the\nvulnerability of existing MOT methods to adversarial attacks. Nevertheless, all\nof these attacks belong to digital attacks that inject pixel-level noise into\ninput images, and are therefore ineffective in physical scenarios. To fill this\ngap, we propose PapMOT, which can generate physical adversarial patches against\nMOT for both digital and physical scenarios. Besides attacking the detection\nmechanism, PapMOT also optimizes a printable patch that can be detected as new\ntargets to mislead the identity association process. Moreover, we introduce a\npatch enhancement strategy to further degrade the temporal consistency of\ntracking results across video frames, resulting in more aggressive attacks. We\nfurther develop new evaluation metrics to assess the robustness of MOT against\nsuch attacks. Extensive evaluations on multiple datasets demonstrate that our\nPapMOT can successfully attack various architectures of MOT trackers in digital\nscenarios. We also validate the effectiveness of PapMOT for physical attacks by\ndeploying printed adversarial patches in the real world."
    },
    {
        "date": "2025-04",
        "title": "Explorer: Robust Collection of Interactable GUI Elements",
        "author": "Iason Chaimalas, Arnas Vy\u0161niauskas, and Gabriel Brostow",
        "link": "http://arxiv.org/abs/2504.09352v1",
        "abstract": "Automation of existing Graphical User Interfaces (GUIs) is important but hard\nto achieve. Upstream of making the GUI user-accessible or somehow scriptable,\neven the data-collection to understand the original interface poses significant\nchallenges. For example, large quantities of general UI data seem helpful for\ntraining general machine learning (ML) models, but accessibility for each\nperson can hinge on the ML's precision on a specific app. We therefore take the\nperspective that a given user needs confidence, that the relevant UI elements\nare being detected correctly throughout one app or digital environment. We\nmostly assume that the target application is known in advance, so that data\ncollection and ML-training can be personalized for the test-time target domain.\nThe proposed Explorer system focuses on detecting on-screen buttons and\ntext-entry fields, i.e. interactables, where the training process has access to\na live version of the application. The live application can run on almost any\npopular platform except iOS phones, and the collection is especially\nstreamlined for Android phones or for desktop Chrome browsers. Explorer also\nenables the recording of interactive user sessions, and subsequent mapping of\nhow such sessions overlap and sometimes loop back to similar states. We show\nhow having such a map enables a kind of path planning through the GUI, letting\na user issue audio commands to get to their destination. Critically, we are\nreleasing our code for Explorer openly at https://github.com/varnelis/Explorer."
    },
    {
        "date": "2025-04",
        "title": "CrossLink: A Decentralized Framework for Secure Cross-Chain Smart Contract Execution",
        "author": "Tahrim Hossain, Faisal Haque Bappy, Tarannum Shaila Zaman, and Tariqul Islam",
        "link": "http://arxiv.org/abs/2504.09319v1",
        "abstract": "This paper introduces CrossLink, a decentralized framework for secure\ncross-chain smart contract execution that effectively addresses the inherent\nlimitations of contemporary solutions, which primarily focus on asset transfers\nand rely on potentially vulnerable centralized intermediaries. Recognizing the\nescalating demand for seamless interoperability among decentralized\napplications, CrossLink provides a trustless mechanism for smart contracts\nacross disparate blockchain networks to communicate and interact. At its core,\nCrossLink utilizes a compact chain for selectively storing authorized contract\nstates and employs a secure inter-chain messaging mechanism to ensure atomic\nexecution and data consistency. By implementing a deposit/collateral fee system\nand efficient state synchronization, CrossLink enhances security and mitigates\nvulnerabilities, offering a novel approach to seamless, secure, and\ndecentralized cross-chain interoperability. A formal security analysis further\nvalidates CrossLink's robustness against unauthorized modifications and\ndenial-of-service attacks."
    },
    {
        "date": "2025-04",
        "title": "SmartShift: A Secure and Efficient Approach to Smart Contract Migration",
        "author": "Tahrim Hossain, Faisal Haque Bappy, Tarannum Shaila Zaman, Raiful Hasan, and Tariqul Islam",
        "link": "http://arxiv.org/abs/2504.09315v1",
        "abstract": "Blockchain and smart contracts have emerged as revolutionary technologies\ntransforming distributed computing. While platform evolution and smart\ncontracts' inherent immutability necessitate migrations both across and within\nchains, migrating the vast amounts of critical data in these contracts while\nmaintaining data integrity and minimizing operational disruption presents a\nsignificant challenge. To address these challenges, we present SmartShift, a\nframework that enables secure and efficient smart contract migrations through\nintelligent state partitioning and progressive function activation, preserving\noperational continuity during transitions. Our comprehensive evaluation\ndemonstrates that SmartShift significantly reduces migration downtime while\nensuring robust security, establishing a foundation for efficient and secure\nsmart contract migration systems."
    },
    {
        "date": "2025-04",
        "title": "A Lightweight Moment Retrieval System with Global Re-Ranking and Robust Adaptive Bidirectional Temporal Search",
        "author": "Tinh-Anh Nguyen-Nhu, Huu-Loc Tran, Nguyen-Khang Le, Minh-Nhat Nguyen, Tien-Huy Nguyen, Hoang-Long Nguyen-Huu, Huu-Phong Phan-Nguyen, Huy-Thach Pham, Quan Nguyen, Hoang M. Le, and Quang-Vinh Dinh",
        "link": "http://arxiv.org/abs/2504.09298v1",
        "abstract": "The exponential growth of digital video content has posed critical challenges\nin moment-level video retrieval, where existing methodologies struggle to\nefficiently localize specific segments within an expansive video corpus.\nCurrent retrieval systems are constrained by computational inefficiencies,\ntemporal context limitations, and the intrinsic complexity of navigating video\ncontent. In this paper, we address these limitations through a novel\nInteractive Video Corpus Moment Retrieval framework that integrates a\nSuperGlobal Reranking mechanism and Adaptive Bidirectional Temporal Search\n(ABTS), strategically optimizing query similarity, temporal stability, and\ncomputational resources. By preprocessing a large corpus of videos using a\nkeyframe extraction model and deduplication technique through image hashing,\nour approach provides a scalable solution that significantly reduces storage\nrequirements while maintaining high localization precision across diverse video\nrepositories."
    },
    {
        "date": "2025-04",
        "title": "Learning Occlusion-Robust Vision Transformers for Real-Time UAV Tracking",
        "author": "You Wu, Xucheng Wang, Xiangyang Yang, Mengyuan Liu, Dan Zeng, Hengzhou Ye, and Shuiwang Li",
        "link": "http://arxiv.org/abs/2504.09228v1",
        "abstract": "Single-stream architectures using Vision Transformer (ViT) backbones show\ngreat potential for real-time UAV tracking recently. However, frequent\nocclusions from obstacles like buildings and trees expose a major drawback:\nthese models often lack strategies to handle occlusions effectively. New\nmethods are needed to enhance the occlusion resilience of single-stream ViT\nmodels in aerial tracking. In this work, we propose to learn Occlusion-Robust\nRepresentations (ORR) based on ViTs for UAV tracking by enforcing an invariance\nof the feature representation of a target with respect to random masking\noperations modeled by a spatial Cox process. Hopefully, this random masking\napproximately simulates target occlusions, thereby enabling us to learn ViTs\nthat are robust to target occlusion for UAV tracking. This framework is termed\nORTrack. Additionally, to facilitate real-time applications, we propose an\nAdaptive Feature-Based Knowledge Distillation (AFKD) method to create a more\ncompact tracker, which adaptively mimics the behavior of the teacher model\nORTrack according to the task's difficulty. This student model, dubbed\nORTrack-D, retains much of ORTrack's performance while offering higher\nefficiency. Extensive experiments on multiple benchmarks validate the\neffectiveness of our method, demonstrating its state-of-the-art performance.\nCodes is available at https://github.com/wuyou3474/ORTrack."
    },
    {
        "date": "2025-04",
        "title": "FairACE: Achieving Degree Fairness in Graph Neural Networks via Contrastive and Adversarial Group-Balanced Training",
        "author": "Jiaxin Liu, Xiaoqian Jiang, Xiang Li, Bohan Zhang, and Jing Zhang",
        "link": "http://arxiv.org/abs/2504.09210v2",
        "abstract": "Fairness has been a significant challenge in graph neural networks (GNNs)\nsince degree biases often result in un-equal prediction performance among nodes\nwith varying degrees. Existing GNN models focus on prediction accuracy,\nfrequently overlooking fairness across different degree groups. To addressthis\nissue, we propose a novel GNN framework, namely Fairness- Aware Asymmetric\nContrastive Ensemble (FairACE), which inte-grates asymmetric contrastive\nlearning with adversarial training to improve degree fairness. FairACE captures\none-hop local neighborhood information and two-hop monophily similarity to\ncreate fairer node representations and employs a degree fairness regulator to\nbalance performance between high-degree and low-degree nodes. During model\ntraining, a novel group-balanced fairness loss is proposed to minimize\nclassification disparities across degree groups. In addition, we also propose a\nnovel fairness metric, the Accuracy Distribution Gap (ADG), which can\nquantitatively assess and ensure equitable performance across different\ndegree-based node groups. Experimental results on both synthetic and real-world\ndatasets demonstrate that FairACE significantly improves degree fairness\nmetrics while maintaining competitive accuracy in comparison to the\nstate-of-the-art GNN models."
    },
    {
        "date": "2025-04",
        "title": "Illusion Worlds: Deceptive UI Attacks in Social VR",
        "author": "Junhee Lee, Hwanjo Heo, Seungwon Woo, Minseok Kim, Jongseop Kim, and Jinwoo Kim",
        "link": "http://arxiv.org/abs/2504.09199v1",
        "abstract": "Social Virtual Reality (VR) platforms have surged in popularity, yet their\nsecurity risks remain underexplored. This paper presents four novel UI attacks\nthat covertly manipulate users into performing harmful actions through\ndeceptive virtual content. Implemented on VRChat and validated in an\nIRB-approved study with 30 participants, these attacks demonstrate how\ndeceptive elements can mislead users into malicious actions without their\nawareness. To address these vulnerabilities, we propose MetaScanner, a\nproactive countermeasure that rapidly analyzes objects and scripts in virtual\nworlds, detecting suspicious elements within seconds."
    },
    {
        "date": "2025-04",
        "title": "RT-DATR:Real-time Unsupervised Domain Adaptive Detection Transformer with Adversarial Feature Learning",
        "author": "Feng Lv, Chunlong Xia, Shuo Wang, and Huo Cao",
        "link": "http://arxiv.org/abs/2504.09196v1",
        "abstract": "Despite domain-adaptive object detectors based on CNN and transformers have\nmade significant progress in cross-domain detection tasks, it is regrettable\nthat domain adaptation for real-time transformer-based detectors has not yet\nbeen explored. Directly applying existing domain adaptation algorithms has\nproven to be suboptimal. In this paper, we propose RT-DATR, a simple and\nefficient real-time domain adaptive detection transformer. Building on RT-DETR\nas our base detector, we first introduce a local object-level feature alignment\nmodule to significantly enhance the feature representation of domain invariance\nduring object transfer. Additionally, we introduce a scene semantic feature\nalignment module designed to boost cross-domain detection performance by\naligning scene semantic features. Finally, we introduced a domain query and\ndecoupled it from the object query to further align the instance feature\ndistribution within the decoder layer, reduce the domain gap, and maintain\ndiscriminative ability. Experimental results on various benchmarks demonstrate\nthat our method outperforms current state-of-the-art approaches. Our code will\nbe released soon."
    },
    {
        "date": "2025-04",
        "title": "Towards More Efficient, Robust, Instance-adaptive, and Generalizable Online Learning",
        "author": "Zhiyong Wang",
        "link": "http://arxiv.org/abs/2504.09192v2",
        "abstract": "The primary goal of my Ph.D. study is to develop provably efficient and\npractical algorithms for data-driven online sequential decision-making under\nuncertainty. My work focuses on reinforcement learning (RL), multi-armed\nbandits, and their applications, including recommendation systems, computer\nnetworks, video analytics, and large language models (LLMs). Online learning\nmethods, such as bandits and RL, have demonstrated remarkable success - ranging\nfrom outperforming human players in complex games like Atari and Go to\nadvancing robotics, recommendation systems, and fine-tuning LLMs. Despite these\nsuccesses, many established algorithms rely on idealized models that can fail\nunder model misspecifications or adversarial perturbations, particularly in\nsettings where accurate prior knowledge of the underlying model class is\nunavailable or where malicious users operate within dynamic systems. These\nchallenges are pervasive in real-world applications, where robust and adaptive\nsolutions are critical. Furthermore, while worst-case guarantees provide\ntheoretical reliability, they often fail to capture instance-dependent\nperformance, which can lead to more efficient and practical solutions. Another\nkey challenge lies in generalizing to new, unseen environments, a crucial\nrequirement for deploying these methods in dynamic and unpredictable settings.\nTo address these limitations, my research aims to develop more efficient,\nrobust, instance-adaptive, and generalizable online learning algorithms for\nboth reinforcement learning and bandits. Towards this end, I focus on\ndeveloping more efficient, robust, instance-adaptive, and generalizable for\nboth general reinforcement learning (RL) and bandits."
    },
    {
        "date": "2025-04",
        "title": "A Multi-Layered Security Analysis of Blockchain Systems: From Attack Vectors to Defense and System Hardening",
        "author": "Yuhuan Yang, Shipeng Ye, and Xiaoqi Li",
        "link": "http://arxiv.org/abs/2504.09181v1",
        "abstract": "The application of Bitcoin enables people to understand blockchain technology\ngradually. Bitcoin is a decentralized currency that does not rely on\nthird-party credit institutions, and the core of Bitcoin's underlying\ntechnology is blockchain. With the increasing value of Bitcoin and the vigorous\ndevelopment of decentralization, people's research on blockchain is also\nincreasing day by day. Today's blockchain technology has not only made great\nachievements in the application of Bitcoin, but has also been preliminarily\napplied in other fields, such as finance, medical treatment, the Internet of\nThings, and so on. However, with the initial application of blockchain\ntechnology on the Internet, the security of blockchain technology has also been\nwidely concerned by people in the industry. For example, whether currency\ntrading platforms, smart contracts, blockchain consensus mechanisms, and other\ntechnologies are vulnerable to attacks, and how we can defend against these\nattacks digitally and optimize the blockchain system is exactly the subject we\nwant to study. For the security of appeal blockchain, this paper first analyzes\nthe security threats faced by the application digital currency trading platform\nof the blockchain system, then analyzes the security problems of smart contract\nclosely related to blockchain 2.0, and then analyzes and studies the security\nthreats of blockchain public chain, consensus mechanism, and P2P. Finally,\ncombined with the security problems at all levels of the blockchain system we\nanalyze and study how to optimize the security of the blockchain system."
    },
    {
        "date": "2025-04",
        "title": "CI-RKM: A Class-Informed Approach to Robust Restricted Kernel Machines",
        "author": "Ritik Mishra, Mushir Akhtar, and M. Tanveer",
        "link": "http://arxiv.org/abs/2504.11476v1",
        "abstract": "Restricted kernel machines (RKMs) represent a versatile and powerful\nframework within the kernel machine family, leveraging conjugate feature\nduality to address a wide range of machine learning tasks, including\nclassification, regression, and feature learning. However, their performance\ncan degrade significantly in the presence of noise and outliers, which\ncompromises robustness and predictive accuracy. In this paper, we propose a\nnovel enhancement to the RKM framework by integrating a class-informed weighted\nfunction. This weighting mechanism dynamically adjusts the contribution of\nindividual training points based on their proximity to class centers and\nclass-specific characteristics, thereby mitigating the adverse effects of noisy\nand outlier data. By incorporating weighted conjugate feature duality and\nleveraging the Schur complement theorem, we introduce the class-informed\nrestricted kernel machine (CI-RKM), a robust extension of the RKM designed to\nimprove generalization and resilience to data imperfections. Experimental\nevaluations on benchmark datasets demonstrate that the proposed CI-RKM\nconsistently outperforms existing baselines, achieving superior classification\naccuracy and enhanced robustness against noise and outliers. Our proposed\nmethod establishes a significant advancement in the development of kernel-based\nlearning models, addressing a core challenge in the field."
    },
    {
        "date": "2025-04",
        "title": "A Confounding Factors-Inhibition Adversarial Learning Framework for Multi-site fMRI Mental Disorder Identification",
        "author": "Xin Wen, Shijie Guo, Wenbo Ning, Rui Cao, Yan Niu, Bin Wan, Peng Wei, Xiaobo Liu, and Jie Xiang",
        "link": "http://arxiv.org/abs/2504.09179v1",
        "abstract": "In open data sets of functional magnetic resonance imaging (fMRI), the\nheterogeneity of the data is typically attributed to a combination of factors,\nincluding differences in scanning procedures, the presence of confounding\neffects, and population diversities between multiple sites. These factors\ncontribute to the diminished effectiveness of representation learning, which in\nturn affects the overall efficacy of subsequent classification procedures. To\naddress these limitations, we propose a novel multi-site adversarial learning\nnetwork (MSalNET) for fMRI-based mental disorder detection. Firstly, a\nrepresentation learning module is introduced with a node information assembly\n(NIA) mechanism to better extract features from functional connectivity (FC).\nThis mechanism aggregates edge information from both horizontal and vertical\ndirections, effectively assembling node information. Secondly, to generalize\nthe feature across sites, we proposed a site-level feature extraction module\nthat can learn from individual FC data, which circumvents additional prior\ninformation. Lastly, an adversarial learning network is proposed as a means of\nbalancing the trade-off between individual classification and site regression\ntasks, with the introduction of a novel loss function. The proposed method was\nevaluated on two multi-site fMRI datasets, i.e., Autism Brain Imaging Data\nExchange (ABIDE) and ADHD-200. The results indicate that the proposed method\nachieves a better performance than other related algorithms with the accuracy\nof 75.56 and 68.92 in ABIDE and ADHD-200 datasets, respectively. Furthermore,\nthe result of the site regression indicates that the proposed method reduces\nsite variability from a data-driven perspective. The most discriminative brain\nregions revealed by NIA are consistent with statistical findings, uncovering\nthe \"black box\" of deep learning to a certain extent."
    },
    {
        "date": "2025-04",
        "title": "Secure Physical Layer Communications for Low-Altitude Economy Networking: A Survey",
        "author": "Lingyi Cai, Jiacheng Wang, Ruichen Zhang, Yu Zhang, Tao Jiang, Dusit Niyato, Xianbin Wang, Abbas Jamalipour, and Xuemin Shen",
        "link": "http://arxiv.org/abs/2504.09153v1",
        "abstract": "The Low-Altitude Economy Networking (LAENet) is emerging as a transformative\nparadigm that enables an integrated and sophisticated communication\ninfrastructure to support aerial vehicles in carrying out a wide range of\neconomic activities within low-altitude airspace. However, the physical layer\ncommunications in the LAENet face growing security threats due to inherent\ncharacteristics of aerial communication environments, such as signal broadcast\nnature and channel openness. These challenges highlight the urgent need for\nsafeguarding communication confidentiality, availability, and integrity. In\nview of the above, this survey comprehensively reviews existing secure\ncountermeasures for physical layer communication in the LAENet. We explore core\nmethods focusing on anti-eavesdropping and authentication for ensuring\ncommunication confidentiality. Subsequently, availability-enhancing techniques\nare thoroughly discussed for anti-jamming and spoofing defense. Then, we review\napproaches for safeguarding integrity through anomaly detection and injection\nprotection. Furthermore, we discuss future research directions, emphasizing\nenergy-efficient physical layer security, multi-drone collaboration for secure\ncommunication, AI-driven security defense strategy, space-air-ground integrated\nsecurity architecture, and 6G-enabled secure UAV communication. This survey may\nprovide valuable references and new insights for researchers in the field of\nsecure physical layer communication for the LAENet."
    },
    {
        "date": "2025-04",
        "title": "Self-Supervised Autoencoder Network for Robust Heart Rate Extraction from Noisy Photoplethysmogram: Applying Blind Source Separation to Biosignal Analysis",
        "author": "Matthew B. Webster, Dongheon Lee, and Joonnyong Lee",
        "link": "http://arxiv.org/abs/2504.09132v1",
        "abstract": "Biosignals can be viewed as mixtures measuring particular physiological\nevents, and blind source separation (BSS) aims to extract underlying source\nsignals from mixtures. This paper proposes a self-supervised multi-encoder\nautoencoder (MEAE) to separate heartbeat-related source signals from\nphotoplethysmogram (PPG), enhancing heart rate (HR) detection in noisy PPG\ndata. The MEAE is trained on PPG signals from a large open polysomnography\ndatabase without any pre-processing or data selection. The trained network is\nthen applied to a noisy PPG dataset collected during the daily activities of\nnine subjects. The extracted heartbeat-related source signal significantly\nimproves HR detection as compared to the original PPG. The absence of\npre-processing and the self-supervised nature of the proposed method, combined\nwith its strong performance, highlight the potential of BSS in biosignal\nanalysis."
    },
    {
        "date": "2025-04",
        "title": "CAShift: Benchmarking Log-Based Cloud Attack Detection under Normality Shift",
        "author": "Jiongchi Yu, Xiaofei Xie, Qiang Hu, Bowen Zhang, Ziming Zhao, Yun Lin, Lei Ma, Ruitao Feng, and Frank Liauw",
        "link": "http://arxiv.org/abs/2504.09115v3",
        "abstract": "With the rapid advancement of cloud-native computing, securing cloud\nenvironments has become an important task. Log-based Anomaly Detection (LAD) is\nthe most representative technique used in different systems for attack\ndetection and safety guarantee, where multiple LAD methods and relevant\ndatasets have been proposed. However, even though some of these datasets are\nspecifically prepared for cloud systems, they only cover limited cloud\nbehaviors and lack information from a whole-system perspective. Another\ncritical issue to consider is normality shift, which implies that the test\ndistribution could differ from the training distribution and highly affect the\nperformance of LAD. Unfortunately, existing works only focus on simple shift\ntypes such as chronological changes, while other cloud-specific shift types are\nignored. Therefore, a dataset that captures diverse cloud system behaviors and\nvarious types of normality shifts is essential.\n  To fill this gap, we construct a dataset CAShift to evaluate the performance\nof LAD in cloud, which considers different roles of software in cloud systems,\nsupports three real-world normality shift types and features 20 different\nattack scenarios in various cloud system components. Based on CAShift, we\nevaluate the effectiveness of existing LAD methods in normality shift\nscenarios. Additionally, to explore the feasibility of shift adaptation, we\nfurther investigate three continuous learning approaches to mitigate the impact\nof distribution shift. Results demonstrated that 1) all LAD methods suffer from\nnormality shift where the performance drops up to 34%, and 2) existing\ncontinuous learning methods are promising to address shift drawbacks, but the\nconfigurations highly affect the shift adaptation. Based on our findings, we\noffer valuable implications for future research in designing more robust LAD\nmodels and methods for LAD shift adaptation."
    },
    {
        "date": "2025-04",
        "title": "Detecting Instruction Fine-tuning Attack on Language Models with Influence Function",
        "author": "Jiawei Li",
        "link": "http://arxiv.org/abs/2504.09026v1",
        "abstract": "Instruction fine-tuning attacks pose a significant threat to large language\nmodels (LLMs) by subtly embedding poisoned data in fine-tuning datasets, which\ncan trigger harmful or unintended responses across a range of tasks. This\nundermines model alignment and poses security risks in real-world deployment.\nIn this work, we present a simple and effective approach to detect and mitigate\nsuch attacks using influence functions, a classical statistical tool adapted\nfor machine learning interpretation. Traditionally, the high computational\ncosts of influence functions have limited their application to large models and\ndatasets. The recent Eigenvalue-Corrected Kronecker-Factored Approximate\nCurvature (EK-FAC) approximation method enables efficient influence score\ncomputation, making it feasible for large-scale analysis.\n  We are the first to apply influence functions for detecting language model\ninstruction fine-tuning attacks on large-scale datasets, as both the\ninstruction fine-tuning attack on language models and the influence calculation\napproximation technique are relatively new. Our large-scale empirical\nevaluation of influence functions on 50,000 fine-tuning examples and 32 tasks\nreveals a strong association between influence scores and sentiment. Building\non this, we introduce a novel sentiment transformation combined with influence\nfunctions to detect and remove critical poisons -- poisoned data points that\nskew model predictions. Removing these poisons (only 1% of total data) recovers\nmodel performance to near-clean levels, demonstrating the effectiveness and\nefficiency of our approach. Artifact is available at\nhttps://github.com/lijiawei20161002/Poison-Detection.\n  WARNING: This paper contains offensive data examples."
    },
    {
        "date": "2025-04",
        "title": "High Dynamic Range Modulo Imaging for Robust Object Detection in Autonomous Driving",
        "author": "Kebin Contreras, Brayan Monroy, and Jorge Bacca",
        "link": "http://arxiv.org/abs/2504.11472v1",
        "abstract": "Object detection precision is crucial for ensuring the safety and efficacy of\nautonomous driving systems. The quality of acquired images directly influences\nthe ability of autonomous driving systems to correctly recognize and respond to\nother vehicles, pedestrians, and obstacles in real-time. However, real\nenvironments present extreme variations in lighting, causing saturation\nproblems and resulting in the loss of crucial details for detection.\nTraditionally, High Dynamic Range (HDR) images have been preferred for their\nability to capture a broad spectrum of light intensities, but the need for\nmultiple captures to construct HDR images is inefficient for real-time\napplications in autonomous vehicles. To address these issues, this work\nintroduces the use of modulo sensors for robust object detection. The modulo\nsensor allows pixels to `reset/wrap' upon reaching saturation level by\nacquiring an irradiance encoding image which can then be recovered using\nunwrapping algorithms. The applied reconstruction techniques enable HDR\nrecovery of color intensity and image details, ensuring better visual quality\neven under extreme lighting conditions at the cost of extra time. Experiments\nwith the YOLOv10 model demonstrate that images processed using modulo images\nachieve performance comparable to HDR images and significantly surpass\nsaturated images in terms of object detection accuracy. Moreover, the proposed\nmodulo imaging step combined with HDR image reconstruction is shorter than the\ntime required for conventional HDR image acquisition."
    },
    {
        "date": "2025-04",
        "title": "Robust Steganography from Large Language Models",
        "author": "Neil Perry, Sanket Gupte, Nishant Pitta, and Lior Rotem",
        "link": "http://arxiv.org/abs/2504.08977v1",
        "abstract": "Recent steganographic schemes, starting with Meteor (CCS'21), rely on\nleveraging large language models (LLMs) to resolve a historically-challenging\ntask of disguising covert communication as ``innocent-looking''\nnatural-language communication. However, existing methods are vulnerable to\n``re-randomization attacks,'' where slight changes to the communicated text,\nthat might go unnoticed, completely destroy any hidden message. This is also a\nvulnerability in more traditional encryption-based stegosystems, where\nadversaries can modify the randomness of an encryption scheme to destroy the\nhidden message while preserving an acceptable covertext to ordinary users. In\nthis work, we study the problem of robust steganography. We introduce formal\ndefinitions of weak and strong robust LLM-based steganography, corresponding to\ntwo threat models in which natural language serves as a covertext channel\nresistant to realistic re-randomization attacks. We then propose two\nconstructions satisfying these notions. We design and implement our\nsteganographic schemes that embed arbitrary secret messages into natural\nlanguage text generated by LLMs, ensuring recoverability even under adversarial\nparaphrasing and rewording attacks. To support further research and real-world\ndeployment, we release our implementation and datasets for public use."
    },
    {
        "date": "2025-04",
        "title": "Exploring the Effects of Load Altering Attacks on Load Frequency Control through Python and RTDS",
        "author": "Micha\u0142 Forystek, Andrew D. Syrmakesis, Alkistis Kontou, Panos Kotsampopoulos, Nikos D. Hatziargyriou, and Charalambos Konstantinou",
        "link": "http://arxiv.org/abs/2504.08951v1",
        "abstract": "The modern power grid increasingly depends on advanced information and\ncommunication technology (ICT) systems to enhance performance and reliability\nthrough real-time monitoring, intelligent control, and bidirectional\ncommunication. However, ICT integration also exposes the grid to cyber-threats.\nLoad altering attacks (LAAs), which use botnets of high-wattage devices to\nmanipulate load profiles, are a notable threat to grid stability. While\nprevious research has examined LAAs, their specific impact on load frequency\ncontrol (LFC), critical for maintaining nominal frequency during load\nfluctuations, still needs to be explored. Even minor frequency deviations can\njeopardize grid operations. This study bridges the gap by analyzing LAA effects\non LFC through simulations of static and dynamic scenarios using Python and\nRTDS. The results highlight LAA impacts on frequency stability and present an\neigenvalue-based stability assessment for dynamic LAAs (DLAAs), identifying key\nparameters influencing grid resilience."
    },
    {
        "date": "2025-04",
        "title": "Robust SAM: On the Adversarial Robustness of Vision Foundation Models",
        "author": "Jiahuan Long, Zhengqin Xu, Tingsong Jiang, Wen Yao, Shuai Jia, Chao Ma, and Xiaoqian Chen",
        "link": "http://arxiv.org/abs/2504.08906v1",
        "abstract": "The Segment Anything Model (SAM) is a widely used vision foundation model\nwith diverse applications, including image segmentation, detection, and\ntracking. Given SAM's wide applications, understanding its robustness against\nadversarial attacks is crucial for real-world deployment. However, research on\nSAM's robustness is still in its early stages. Existing attacks often overlook\nthe role of prompts in evaluating SAM's robustness, and there has been\ninsufficient exploration of defense methods to balance the robustness and\naccuracy. To address these gaps, this paper proposes an adversarial robustness\nframework designed to evaluate and enhance the robustness of SAM. Specifically,\nwe introduce a cross-prompt attack method to enhance the attack transferability\nacross different prompt types. Besides attacking, we propose a few-parameter\nadaptation strategy to defend SAM against various adversarial attacks. To\nbalance robustness and accuracy, we use the singular value decomposition (SVD)\nto constrain the space of trainable parameters, where only singular values are\nadaptable. Experiments demonstrate that our cross-prompt attack method\noutperforms previous approaches in terms of attack success rate on both SAM and\nSAM 2. By adapting only 512 parameters, we achieve at least a 15\\% improvement\nin mean intersection over union (mIoU) against various adversarial attacks.\nCompared to previous defense methods, our approach enhances the robustness of\nSAM while maximally maintaining its original performance."
    },
    {
        "date": "2025-04",
        "title": "Toward Spiking Neural Network Local Learning Modules Resistant to Adversarial Attacks",
        "author": "Jiaqi Lin, and Abhronil Sengupta",
        "link": "http://arxiv.org/abs/2504.08897v1",
        "abstract": "Recent research has shown the vulnerability of Spiking Neural Networks (SNNs)\nunder adversarial examples that are nearly indistinguishable from clean data in\nthe context of frame-based and event-based information. The majority of these\nstudies are constrained in generating adversarial examples using\nBackpropagation Through Time (BPTT), a gradient-based method which lacks\nbiological plausibility. In contrast, local learning methods, which relax many\nof BPTT's constraints, remain under-explored in the context of adversarial\nattacks. To address this problem, we examine adversarial robustness in SNNs\nthrough the framework of four types of training algorithms. We provide an\nin-depth analysis of the ineffectiveness of gradient-based adversarial attacks\nto generate adversarial instances in this scenario. To overcome these\nlimitations, we introduce a hybrid adversarial attack paradigm that leverages\nthe transferability of adversarial instances. The proposed hybrid approach\ndemonstrates superior performance, outperforming existing adversarial attack\nmethods. Furthermore, the generalizability of the method is assessed under\nmulti-step adversarial attacks, adversarial attacks in black-box FGSM\nscenarios, and within the non-spiking domain."
    },
    {
        "date": "2025-04",
        "title": "Enterprise-Grade Security for the Model Context Protocol (MCP): Frameworks and Mitigation Strategies",
        "author": "Vineeth Sai Narajala, and Idan Habler",
        "link": "http://arxiv.org/abs/2504.08623v1",
        "abstract": "The Model Context Protocol (MCP), introduced by Anthropic, provides a\nstandardized framework for artificial intelligence (AI) systems to interact\nwith external data sources and tools in real-time. While MCP offers significant\nadvantages for AI integration and capability extension, it introduces novel\nsecurity challenges that demand rigorous analysis and mitigation. This paper\nbuilds upon foundational research into MCP architecture and preliminary\nsecurity assessments to deliver enterprise-grade mitigation frameworks and\ndetailed technical implementation strategies. Through systematic threat\nmodeling and analysis of MCP implementations and analysis of potential attack\nvectors, including sophisticated threats like tool poisoning, we present\nactionable security patterns tailored for MCP implementers and adopters. The\nprimary contribution of this research lies in translating theoretical security\nconcerns into a practical, implementable framework with actionable controls,\nthereby providing essential guidance for the secure enterprise adoption and\ngovernance of integrated AI systems."
    },
    {
        "date": "2025-04",
        "title": "A Hybrid Chaos-Based Cryptographic Framework for Post-Quantum Secure Communications",
        "author": "Kevin Song, Noorullah Imran, Jake Y. Chen, and Allan C. Dobbins",
        "link": "http://arxiv.org/abs/2504.08618v1",
        "abstract": "We present CryptoChaos, a novel hybrid cryptographic framework that\nsynergizes deterministic chaos theory with cutting-edge cryptographic\nprimitives to achieve robust, post-quantum resilient encryption. CryptoChaos\nharnesses the intrinsic unpredictability of four discrete chaotic maps\n(Logistic, Chebyshev, Tent, and Henon) to generate a high-entropy,\nmultidimensional key from a unified entropy pool. This key is derived through a\nlayered process that combines SHA3-256 hashing with an ephemeral X25519\nDiffie-Hellman key exchange and is refined using an HMAC-based key derivation\nfunction (HKDF). The resulting encryption key powers AES-GCM, providing both\nconfidentiality and integrity. Comprehensive benchmarking against established\nsymmetric ciphers confirms that CryptoChaos attains near-maximal Shannon\nentropy (approximately 8 bits per byte) and exhibits negligible adjacent-byte\ncorrelations, while robust performance on the NIST SP 800-22 test suite\nunderscores its statistical rigor. Moreover, quantum simulations demonstrate\nthat the additional complexity inherent in chaotic key generation dramatically\nelevates the resource requirements for Grover-based quantum attacks, with an\nestimated T gate count of approximately 2.1 x 10^9. The modular and\ninteroperable design of CryptoChaos positions it as a promising candidate for\nhigh-assurance applications, ranging from secure communications and financial\ntransactions to IoT systems, paving the way for next-generation post-quantum\nencryption standards."
    },
    {
        "date": "2025-04",
        "title": "Knowledge Distillation for Multimodal Egocentric Action Recognition Robust to Missing Modalities",
        "author": "Maria Santos-Villafranca, Dustin Carri\u00f3n-Ojeda, Alejandro Perez-Yus, Jesus Bermudez-Cameo, Jose J. Guerrero, and Simone Schaub-Meyer",
        "link": "http://arxiv.org/abs/2504.08578v1",
        "abstract": "Action recognition is an essential task in egocentric vision due to its wide\nrange of applications across many fields. While deep learning methods have been\nproposed to address this task, most rely on a single modality, typically video.\nHowever, including additional modalities may improve the robustness of the\napproaches to common issues in egocentric videos, such as blurriness and\nocclusions. Recent efforts in multimodal egocentric action recognition often\nassume the availability of all modalities, leading to failures or performance\ndrops when any modality is missing. To address this, we introduce an efficient\nmultimodal knowledge distillation approach for egocentric action recognition\nthat is robust to missing modalities (KARMMA) while still benefiting when\nmultiple modalities are available. Our method focuses on resource-efficient\ndevelopment by leveraging pre-trained models as unimodal feature extractors in\nour teacher model, which distills knowledge into a much smaller and faster\nstudent model. Experiments on the Epic-Kitchens and Something-Something\ndatasets demonstrate that our student model effectively handles missing\nmodalities while reducing its accuracy drop in this scenario."
    },
    {
        "date": "2025-04",
        "title": "Toward Realistic Adversarial Attacks in IDS: A Novel Feasibility Metric for Transferability",
        "author": "Sabrine Ennaji, Elhadj Benkhelifa, and Luigi Vincenzo Mancini",
        "link": "http://arxiv.org/abs/2504.08480v1",
        "abstract": "Transferability-based adversarial attacks exploit the ability of adversarial\nexamples, crafted to deceive a specific source Intrusion Detection System (IDS)\nmodel, to also mislead a target IDS model without requiring access to the\ntraining data or any internal model parameters. These attacks exploit common\nvulnerabilities in machine learning models to bypass security measures and\ncompromise systems. Although the transferability concept has been widely\nstudied, its practical feasibility remains limited due to assumptions of high\nsimilarity between source and target models. This paper analyzes the core\nfactors that contribute to transferability, including feature alignment, model\narchitectural similarity, and overlap in the data distributions that each IDS\nexamines. We propose a novel metric, the Transferability Feasibility Score\n(TFS), to assess the feasibility and reliability of such attacks based on these\nfactors. Through experimental evidence, we demonstrate that TFS and actual\nattack success rates are highly correlated, addressing the gap between\ntheoretical understanding and real-world impact. Our findings provide needed\nguidance for designing more realistic transferable adversarial attacks,\ndeveloping robust defenses, and ultimately improving the security of machine\nlearning-based IDS in critical systems."
    },
    {
        "date": "2025-04",
        "title": "On Transfer-based Universal Attacks in Pure Black-box Setting",
        "author": "Mohammad A. A. K. Jalwana, Naveed Akhtar, Ajmal Mian, Nazanin Rahnavard, and Mubarak Shah",
        "link": "http://arxiv.org/abs/2504.08866v1",
        "abstract": "Despite their impressive performance, deep visual models are susceptible to\ntransferable black-box adversarial attacks. Principally, these attacks craft\nperturbations in a target model-agnostic manner. However, surprisingly, we find\nthat existing methods in this domain inadvertently take help from various\npriors that violate the black-box assumption such as the availability of the\ndataset used to train the target model, and the knowledge of the number of\nclasses in the target model. Consequently, the literature fails to articulate\nthe true potency of transferable black-box attacks. We provide an empirical\nstudy of these biases and propose a framework that aids in a prior-free\ntransparent study of this paradigm. Using our framework, we analyze the role of\nprior knowledge of the target model data and number of classes in attack\nperformance. We also provide several interesting insights based on our\nanalysis, and demonstrate that priors cause overestimation in transferability\nscores. Finally, we extend our framework to query-based attacks. This extension\ninspires a novel image-blending technique to prepare data for effective\nsurrogate model training."
    },
    {
        "date": "2025-04",
        "title": "Adversarial Examples in Environment Perception for Automated Driving (Review)",
        "author": "Jun Yan, and Huilin Yin",
        "link": "http://arxiv.org/abs/2504.08414v1",
        "abstract": "The renaissance of deep learning has led to the massive development of\nautomated driving. However, deep neural networks are vulnerable to adversarial\nexamples. The perturbations of adversarial examples are imperceptible to human\neyes but can lead to the false predictions of neural networks. It poses a huge\nrisk to artificial intelligence (AI) applications for automated driving. This\nsurvey systematically reviews the development of adversarial robustness\nresearch over the past decade, including the attack and defense methods and\ntheir applications in automated driving. The growth of automated driving pushes\nforward the realization of trustworthy AI applications. This review lists\nsignificant references in the research history of adversarial examples."
    },
    {
        "date": "2025-04",
        "title": "A Knowledge-guided Adversarial Defense for Resisting Malicious Visual Manipulation",
        "author": "Dawei Zhou, Suzhi Gang, Decheng Liu, Tongliang Liu, Nannan Wang, and Xinbo Gao",
        "link": "http://arxiv.org/abs/2504.08411v1",
        "abstract": "Malicious applications of visual manipulation have raised serious threats to\nthe security and reputation of users in many fields. To alleviate these issues,\nadversarial noise-based defenses have been enthusiastically studied in recent\nyears. However, ``data-only\" methods tend to distort fake samples in the\nlow-level feature space rather than the high-level semantic space, leading to\nlimitations in resisting malicious manipulation. Frontier research has shown\nthat integrating knowledge in deep learning can produce reliable and\ngeneralizable solutions. Inspired by these, we propose a knowledge-guided\nadversarial defense (KGAD) to actively force malicious manipulation models to\noutput semantically confusing samples. Specifically, in the process of\ngenerating adversarial noise, we focus on constructing significant semantic\nconfusions at the domain-specific knowledge level, and exploit a metric closely\nrelated to visual perception to replace the general pixel-wise metrics. The\ngenerated adversarial noise can actively interfere with the malicious\nmanipulation model by triggering knowledge-guided and perception-related\ndisruptions in the fake samples. To validate the effectiveness of the proposed\nmethod, we conduct qualitative and quantitative experiments on human perception\nand visual quality assessment. The results on two different tasks both show\nthat our defense provides better protection compared to state-of-the-art\nmethods and achieves great generalizability."
    },
    {
        "date": "2025-04",
        "title": "Towards Efficient and Robust Moment Retrieval System: A Unified Framework for Multi-Granularity Models and Temporal Reranking",
        "author": "Huu-Loc Tran, Tinh-Anh Nguyen-Nhu, Huu-Phong Phan-Nguyen, Tien-Huy Nguyen, Nhat-Minh Nguyen-Dich, Anh Dao, Huy-Duc Do, Quan Nguyen, Hoang M. Le, and Quang-Vinh Dinh",
        "link": "http://arxiv.org/abs/2504.08384v1",
        "abstract": "Long-form video understanding presents significant challenges for interactive\nretrieval systems, as conventional methods struggle to process extensive video\ncontent efficiently. Existing approaches often rely on single models,\ninefficient storage, unstable temporal search, and context-agnostic reranking,\nlimiting their effectiveness. This paper presents a novel framework to enhance\ninteractive video retrieval through four key innovations: (1) an ensemble\nsearch strategy that integrates coarse-grained (CLIP) and fine-grained (BEIT3)\nmodels to improve retrieval accuracy, (2) a storage optimization technique that\nreduces redundancy by selecting representative keyframes via TransNetV2 and\ndeduplication, (3) a temporal search mechanism that localizes video segments\nusing dual queries for start and end points, and (4) a temporal reranking\napproach that leverages neighboring frame context to stabilize rankings.\nEvaluated on known-item search and question-answering tasks, our framework\ndemonstrates substantial improvements in retrieval precision, efficiency, and\nuser interpretability, offering a robust solution for real-world interactive\nvideo retrieval applications."
    },
    {
        "date": "2025-04",
        "title": "Practical Secure Aggregation by Combining Cryptography and Trusted Execution Environments",
        "author": "Romain de Laage, Peterson Yuhala, Fran\u00e7ois-Xavier Wicht, Pascal Felber, Christian Cachin, and Valerio Schiavoni",
        "link": "http://arxiv.org/abs/2504.08325v1",
        "abstract": "Secure aggregation enables a group of mutually distrustful parties, each\nholding private inputs, to collaboratively compute an aggregate value while\npreserving the privacy of their individual inputs. However, a major challenge\nin adopting secure aggregation approaches for practical applications is the\nsignificant computational overhead of the underlying cryptographic protocols,\ne.g. fully homomorphic encryption. This overhead makes secure aggregation\nprotocols impractical, especially for large datasets. In contrast,\nhardware-based security techniques such as trusted execution environments\n(TEEs) enable computation at near-native speeds, making them a promising\nalternative for reducing the computational burden typically associated with\npurely cryptographic techniques. Yet, in many scenarios, parties may opt for\neither cryptographic or hardware-based security mechanisms, highlighting the\nneed for hybrid approaches. In this work, we introduce several secure\naggregation architectures that integrate both cryptographic and TEE-based\ntechniques, analyzing the trade-offs between security and performance."
    },
    {
        "date": "2025-04",
        "title": "To See or Not to See -- Fingerprinting Devices in Adversarial Environments Amid Advanced Machine Learning",
        "author": "Justin Feng, and Nader Sehatbakhsh",
        "link": "http://arxiv.org/abs/2504.08264v1",
        "abstract": "The increasing use of the Internet of Things raises security concerns. To\naddress this, device fingerprinting is often employed to authenticate devices,\ndetect adversaries, and identify eavesdroppers in an environment. This requires\nthe ability to discern between legitimate and malicious devices which is\nachieved by analyzing the unique physical and/or operational characteristics of\nIoT devices. In the era of the latest progress in machine learning,\nparticularly generative models, it is crucial to methodically examine the\ncurrent studies in device fingerprinting. This involves explaining their\napproaches and underscoring their limitations when faced with adversaries armed\nwith these ML tools. To systematically analyze existing methods, we propose a\ngeneric, yet simplified, model for device fingerprinting. Additionally, we\nthoroughly investigate existing methods to authenticate devices and detect\neavesdropping, using our proposed model. We further study trends and\nsimilarities between works in authentication and eavesdropping detection and\npresent the existing threats and attacks in these domains. Finally, we discuss\nfuture directions in fingerprinting based on these trends to develop more\nsecure IoT fingerprinting schemes."
    },
    {
        "date": "2025-04",
        "title": "Hardware Design and Security Needs Attention: From Survey to Path Forward",
        "author": "Sujan Ghimire, Muhtasim Alam Chowdhury, Banafsheh Saber Latibari, Muntasir Mamun, Jaeden Wolf Carpenter, Benjamin Tan, Hammond Pearce, Pratik Satam, and Soheil Salehi",
        "link": "http://arxiv.org/abs/2504.08854v1",
        "abstract": "Recent advances in attention-based artificial intelligence (AI) models have\nunlocked vast potential to automate digital hardware design while enhancing and\nstrengthening security measures against various threats. This rapidly emerging\nfield leverages Large Language Models (LLMs) to generate HDL code, identify\nvulnerabilities, and sometimes mitigate them. The state of the art in this\ndesign automation space utilizes optimized LLMs with HDL datasets, creating\nautomated systems for register-transfer level (RTL) generation, verification,\nand debugging, and establishing LLM-driven design environments for streamlined\nlogic designs. Additionally, attention-based models like graph attention have\nshown promise in chip design applications, including floorplanning. This survey\ninvestigates the integration of these models into hardware-related domains,\nemphasizing logic design and hardware security, with or without the use of IP\nlibraries. This study explores the commercial and academic landscape,\nhighlighting technical hurdles and future prospects for automating hardware\ndesign and security. Moreover, it provides new insights into the study of\nLLM-driven design systems, advances in hardware security mechanisms, and the\nimpact of influential works on industry practices. Through the examination of\n30 representative approaches and illustrative case studies, this paper\nunderscores the transformative potential of attention-based models in\nrevolutionizing hardware design while addressing the challenges that lie ahead\nin this interdisciplinary domain."
    },
    {
        "date": "2025-04",
        "title": "DaemonSec: Examining the Role of Machine Learning for Daemon Security in Linux Environments",
        "author": "Sheikh Muhammad Farjad",
        "link": "http://arxiv.org/abs/2504.08227v1",
        "abstract": "DaemonSec is an early-stage startup exploring machine learning (ML)-based\nsecurity for Linux daemons, a critical yet often overlooked attack surface.\nWhile daemon security remains underexplored, conventional defenses struggle\nagainst adaptive threats and zero-day exploits. To assess the perspectives of\nIT professionals on ML-driven daemon protection, a systematic interview study\nbased on semi-structured interviews was conducted with 22 professionals from\nindustry and academia. The study evaluates adoption, feasibility, and trust in\nML-based security solutions. While participants recognized the potential of ML\nfor real-time anomaly detection, findings reveal skepticism toward full\nautomation, limited security awareness among non-security roles, and concerns\nabout patching delays creating attack windows. This paper presents the methods,\nkey findings, and implications for advancing ML-driven daemon security in\nindustry."
    },
    {
        "date": "2025-04",
        "title": "EO-VLM: VLM-Guided Energy Overload Attacks on Vision Models",
        "author": "Minjae Seo, Myoungsung You, Junhee Lee, Jaehan Kim, Hwanjo Heo, Jintae Oh, and Jinwoo Kim",
        "link": "http://arxiv.org/abs/2504.08205v1",
        "abstract": "Vision models are increasingly deployed in critical applications such as\nautonomous driving and CCTV monitoring, yet they remain susceptible to\nresource-consuming attacks. In this paper, we introduce a novel\nenergy-overloading attack that leverages vision language model (VLM) prompts to\ngenerate adversarial images targeting vision models. These images, though\nimperceptible to the human eye, significantly increase GPU energy consumption\nacross various vision models, threatening the availability of these systems.\nOur framework, EO-VLM (Energy Overload via VLM), is model-agnostic, meaning it\nis not limited by the architecture or type of the target vision model. By\nexploiting the lack of safety filters in VLMs like DALL-E 3, we create\nadversarial noise images without requiring prior knowledge or internal\nstructure of the target vision models. Our experiments demonstrate up to a 50%\nincrease in energy consumption, revealing a critical vulnerability in current\nvision models."
    },
    {
        "date": "2025-04",
        "title": "A Piecewise Lyapunov Analysis of Sub-quadratic SGD: Applications to Robust and Quantile Regression",
        "author": "Yixuan Zhang, Dongyan Huo, Yudong Chen, and Qiaomin Xie",
        "link": "http://arxiv.org/abs/2504.08178v3",
        "abstract": "Motivated by robust and quantile regression problems, we investigate the\nstochastic gradient descent (SGD) algorithm for minimizing an objective\nfunction $f$ that is locally strongly convex with a sub--quadratic tail. This\nsetting covers many widely used online statistical methods. We introduce a\nnovel piecewise Lyapunov function that enables us to handle functions $f$ with\nonly first-order differentiability, which includes a wide range of popular loss\nfunctions such as Huber loss. Leveraging our proposed Lyapunov function, we\nderive finite-time moment bounds under general diminishing stepsizes, as well\nas constant stepsizes. We further establish the weak convergence, central limit\ntheorem and bias characterization under constant stepsize, providing the first\ngeometrical convergence result for sub--quadratic SGD. Our results have wide\napplications, especially in online statistical methods. In particular, we\ndiscuss two applications of our results. 1) Online robust regression: We\nconsider a corrupted linear model with sub--exponential covariates and\nheavy--tailed noise. Our analysis provides convergence rates comparable to\nthose for corrupted models with Gaussian covariates and noise. 2) Online\nquantile regression: Importantly, our results relax the common assumption in\nprior work that the conditional density is continuous and provide a more\nfine-grained analysis for the moment bounds."
    },
    {
        "date": "2025-04",
        "title": "GenXSS: an AI-Driven Framework for Automated Detection of XSS Attacks in WAFs",
        "author": "Vahid Babaey, and Arun Ravindran",
        "link": "http://arxiv.org/abs/2504.08176v1",
        "abstract": "The increasing reliance on web services has led to a rise in cybersecurity\nthreats, particularly Cross-Site Scripting (XSS) attacks, which target\nclient-side layers of web applications by injecting malicious scripts.\nTraditional Web Application Firewalls (WAFs) struggle to detect highly\nobfuscated and complex attacks, as their rules require manual updates. This\npaper presents a novel generative AI framework that leverages Large Language\nModels (LLMs) to enhance XSS mitigation. The framework achieves two primary\nobjectives: (1) generating sophisticated and syntactically validated XSS\npayloads using in-context learning, and (2) automating defense mechanisms by\ntesting these attacks against a vulnerable application secured by a WAF,\nclassifying bypassing attacks, and generating effective WAF security rules.\nExperimental results using GPT-4o demonstrate the framework's effectiveness\ngenerating 264 XSS payloads, 83% of which were validated, with 80% bypassing\nModSecurity WAF equipped with an industry standard security rule set developed\nby the Open Web Application Security Project (OWASP) to protect against web\nvulnerabilities. Through rule generation, 86% of previously successful attacks\nwere blocked using only 15 new rules. In comparison, Google Gemini Pro achieved\na lower bypass rate of 63%, highlighting performance differences across LLMs."
    },
    {
        "date": "2025-04",
        "title": "AttentionDefense: Leveraging System Prompt Attention for Explainable Defense Against Novel Jailbreaks",
        "author": "Charlotte Siska, and Anush Sankaran",
        "link": "http://arxiv.org/abs/2504.12321v1",
        "abstract": "In the past few years, Language Models (LMs) have shown par-human\ncapabilities in several domains. Despite their practical applications and\nexceeding user consumption, they are susceptible to jailbreaks when malicious\ninput exploits the LM's weaknesses, causing it to deviate from its intended\nbehavior. Current defensive strategies either classify the input prompt as\nadversarial or prevent LMs from generating harmful outputs. However, it is\nchallenging to explain the reason behind the malicious nature of the jailbreak,\nwhich results in a wide variety of closed-box approaches. In this research, we\npropose and demonstrate that system-prompt attention from Small Language Models\n(SLMs) can be used to characterize adversarial prompts, providing a novel,\nexplainable, and cheaper defense approach called AttentionDefense. Our research\nsuggests that the attention mechanism is an integral component in understanding\nand explaining how LMs respond to malicious input that is not captured in the\nsemantic meaning of text embeddings. The proposed AttentionDefense is evaluated\nagainst existing jailbreak benchmark datasets. Ablation studies show that\nSLM-based AttentionDefense has equivalent or better jailbreak detection\nperformance compared to text embedding-based classifiers and GPT-4 zero-shot\ndetectors.To further validate the efficacy of the proposed approach, we\ngenerate a dataset of novel jailbreak variants of the existing benchmark\ndataset using a closed-loop LLM-based multi-agent system. We demonstrate that\nthe proposed AttentionDefense approach performs robustly on this novel\njailbreak dataset while existing approaches suffer in performance.\nAdditionally, for practical purposes AttentionDefense is an ideal solution as\nit has the computation requirements of a small LM but the performance of a LLM\ndetector."
    },
    {
        "date": "2025-04",
        "title": "ColorBench: Can VLMs See and Understand the Colorful World? A Comprehensive Benchmark for Color Perception, Reasoning, and Robustness",
        "author": "Yijun Liang, Ming Li, Chenrui Fan, Ziyue Li, Dang Nguyen, Kwesi Cobbina, Shweta Bhardwaj, Jiuhai Chen, Fuxiao Liu, and Tianyi Zhou",
        "link": "http://arxiv.org/abs/2504.10514v1",
        "abstract": "Color plays an important role in human perception and usually provides\ncritical clues in visual reasoning. However, it is unclear whether and how\nvision-language models (VLMs) can perceive, understand, and leverage color as\nhumans. This paper introduces ColorBench, an innovative benchmark meticulously\ncrafted to assess the capabilities of VLMs in color understanding, including\ncolor perception, reasoning, and robustness. By curating a suite of diverse\ntest scenarios, with grounding in real applications, ColorBench evaluates how\nthese models perceive colors, infer meanings from color-based cues, and\nmaintain consistent performance under varying color transformations. Through an\nextensive evaluation of 32 VLMs with varying language models and vision\nencoders, our paper reveals some undiscovered findings: (i) The scaling law\n(larger models are better) still holds on ColorBench, while the language model\nplays a more important role than the vision encoder. (ii) However, the\nperformance gaps across models are relatively small, indicating that color\nunderstanding has been largely neglected by existing VLMs. (iii) CoT reasoning\nimproves color understanding accuracies and robustness, though they are\nvision-centric tasks. (iv) Color clues are indeed leveraged by VLMs on\nColorBench but they can also mislead models in some tasks. These findings\nhighlight the critical limitations of current VLMs and underscore the need to\nenhance color comprehension. Our ColorBenchcan serve as a foundational tool for\nadvancing the study of human-level color understanding of multimodal AI."
    }
]