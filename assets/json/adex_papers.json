[
    {
        "date": "2025-04",
        "title": "Existing Industry Practice for the EU AI Act's General-Purpose AI Code of Practice Safety and Security Measures",
        "author": "Lily Stelling, Mick Yang, Rokas Gipi\u0161kis, Leon Staufer, Ze Shen Chin, Sim\u00e9on Campos, and Michael Chen",
        "link": "http://arxiv.org/abs/2504.15181v1",
        "abstract": "This report provides a detailed comparison between the measures proposed in\nthe EU AI Act's General-Purpose AI (GPAI) Code of Practice (Third Draft) and\ncurrent practices adopted by leading AI companies. As the EU moves toward\nenforcing binding obligations for GPAI model providers, the Code of Practice\nwill be key to bridging legal requirements with concrete technical commitments.\nOur analysis focuses on the draft's Safety and Security section which is only\nrelevant for the providers of the most advanced models (Commitments II.1-II.16)\nand excerpts from current public-facing documents quotes that are relevant to\neach individual measure.\n  We systematically reviewed different document types - including companies'\nfrontier safety frameworks and model cards - from over a dozen companies,\nincluding OpenAI, Anthropic, Google DeepMind, Microsoft, Meta, Amazon, and\nothers. This report is not meant to be an indication of legal compliance nor\ndoes it take any prescriptive viewpoint about the Code of Practice or\ncompanies' policies. Instead, it aims to inform the ongoing dialogue between\nregulators and GPAI model providers by surfacing evidence of precedent."
    },
    {
        "date": "2025-04",
        "title": "GIFDL: Generated Image Fluctuation Distortion Learning for Enhancing Steganographic Security",
        "author": "Xiangkun Wang, Kejiang Chen, Yuang Qi, Ruiheng Liu, Weiming Zhang, and Nenghai Yu",
        "link": "http://arxiv.org/abs/2504.15139v1",
        "abstract": "Minimum distortion steganography is currently the mainstream method for\nmodification-based steganography. A key issue in this method is how to define\nsteganographic distortion. With the rapid development of deep learning\ntechnology, the definition of distortion has evolved from manual design to deep\nlearning design. Concurrently, rapid advancements in image generation have made\ngenerated images viable as cover media. However, existing distortion design\nmethods based on machine learning do not fully leverage the advantages of\ngenerated cover media, resulting in suboptimal security performance. To address\nthis issue, we propose GIFDL (Generated Image Fluctuation Distortion Learning),\na steganographic distortion learning method based on the fluctuations in\ngenerated images. Inspired by the idea of natural steganography, we take a\nseries of highly similar fluctuation images as the input to the steganographic\ndistortion generator and introduce a new GAN training strategy to disguise\nstego images as fluctuation images. Experimental results demonstrate that\nGIFDL, compared with state-of-the-art GAN-based distortion learning methods,\nexhibits superior resistance to steganalysis, increasing the detection error\nrates by an average of 3.30% across three steganalyzers."
    },
    {
        "date": "2025-04",
        "title": "Robust and Real-time Surface Normal Estimation from Stereo Disparities using Affine Transformations",
        "author": "Csongor Csanad Kariko, Muhammad Rafi Faisal, and Levente Hajder",
        "link": "http://arxiv.org/abs/2504.15121v1",
        "abstract": "This work introduces a novel method for surface normal estimation from\nrectified stereo image pairs, leveraging affine transformations derived from\ndisparity values to achieve fast and accurate results. We demonstrate how the\nrectification of stereo image pairs simplifies the process of surface normal\nestimation by reducing computational complexity. To address noise reduction, we\ndevelop a custom algorithm inspired by convolutional operations, tailored to\nprocess disparity data efficiently. We also introduce adaptive heuristic\ntechniques for efficiently detecting connected surface components within the\nimages, further improving the robustness of the method. By integrating these\nmethods, we construct a surface normal estimator that is both fast and\naccurate, producing a dense, oriented point cloud as the final output. Our\nmethod is validated using both simulated environments and real-world stereo\nimages from the Middlebury and Cityscapes datasets, demonstrating significant\nimprovements in real-time performance and accuracy when implemented on a GPU.\nUpon acceptance, the shader source code will be made publicly available to\nfacilitate further research and reproducibility."
    },
    {
        "date": "2025-04",
        "title": "Fast-Slow Co-advancing Optimizer: Toward Harmonious Adversarial Training of GAN",
        "author": "Lin Wang, Xiancheng Wang, Rui Wang, Zhibo Zhang, and Minghang Zhao",
        "link": "http://arxiv.org/abs/2504.15099v1",
        "abstract": "Up to now, the training processes of typical Generative Adversarial Networks\n(GANs) are still particularly sensitive to data properties and hyperparameters,\nwhich may lead to severe oscillations, difficulties in convergence, or even\nfailures to converge, especially when the overall variances of the training\nsets are large. These phenomena are often attributed to the training\ncharacteristics of such networks. Aiming at the problem, this paper develops a\nnew intelligent optimizer, Fast-Slow Co-advancing Optimizer (FSCO), which\nemploys reinforcement learning in the training process of GANs to make training\neasier. Specifically, this paper allows the training step size to be controlled\nby an agent to improve training stability, and makes the training process more\nintelligent with variable learning rates, making GANs less sensitive to step\nsize. Experiments have been conducted on three benchmark datasets to verify the\neffectiveness of the developed FSCO."
    },
    {
        "date": "2025-04",
        "title": "SOLIDO: A Robust Watermarking Method for Speech Synthesis via Low-Rank Adaptation",
        "author": "Yue Li, Weizhi Liu, and Dongdong Lin",
        "link": "http://arxiv.org/abs/2504.15035v1",
        "abstract": "The accelerated advancement of speech generative models has given rise to\nsecurity issues, including model infringement and unauthorized abuse of\ncontent. Although existing generative watermarking techniques have proposed\ncorresponding solutions, most methods require substantial computational\noverhead and training costs. In addition, some methods have limitations in\nrobustness when handling variable-length inputs. To tackle these challenges, we\npropose \\textsc{SOLIDO}, a novel generative watermarking method that integrates\nparameter-efficient fine-tuning with speech watermarking through low-rank\nadaptation (LoRA) for speech diffusion models. Concretely, the watermark\nencoder converts the watermark to align with the input of diffusion models. To\nachieve precise watermark extraction from variable-length inputs, the watermark\ndecoder based on depthwise separable convolution is designed for watermark\nrecovery. To further enhance speech generation performance and watermark\nextraction capability, we propose a speech-driven lightweight fine-tuning\nstrategy, which reduces computational overhead through LoRA. Comprehensive\nexperiments demonstrate that the proposed method ensures high-fidelity\nwatermarked speech even at a large capacity of 2000 bps. Furthermore, against\ncommon individual and compound speech attacks, our SOLIDO achieves a maximum\naverage extraction accuracy of 99.20\\% and 98.43\\%, respectively. It surpasses\nother state-of-the-art methods by nearly 23\\% in resisting time-stretching\nattacks."
    },
    {
        "date": "2025-04",
        "title": "aiXamine: LLM Safety and Security Simplified",
        "author": "Fatih Deniz, Dorde Popovic, Yazan Boshmaf, Euisuh Jeong, Minhaj Ahmad, Sanjay Chawla, and Issa Khalil",
        "link": "http://arxiv.org/abs/2504.14985v1",
        "abstract": "Evaluating Large Language Models (LLMs) for safety and security remains a\ncomplex task, often requiring users to navigate a fragmented landscape of ad\nhoc benchmarks, datasets, metrics, and reporting formats. To address this\nchallenge, we present aiXamine, a comprehensive black-box evaluation platform\nfor LLM safety and security. aiXamine integrates over 40 tests (i.e.,\nbenchmarks) organized into eight key services targeting specific dimensions of\nsafety and security: adversarial robustness, code security, fairness and bias,\nhallucination, model and data privacy, out-of-distribution (OOD) robustness,\nover-refusal, and safety alignment. The platform aggregates the evaluation\nresults into a single detailed report per model, providing a detailed breakdown\nof model performance, test examples, and rich visualizations. We used aiXamine\nto assess over 50 publicly available and proprietary LLMs, conducting over 2K\nexaminations. Our findings reveal notable vulnerabilities in leading models,\nincluding susceptibility to adversarial attacks in OpenAI's GPT-4o, biased\noutputs in xAI's Grok-3, and privacy weaknesses in Google's Gemini 2.0.\nAdditionally, we observe that open-source models can match or exceed\nproprietary models in specific services such as safety alignment, fairness and\nbias, and OOD robustness. Finally, we identify trade-offs between distillation\nstrategies, model size, training methods, and architectural choices."
    },
    {
        "date": "2025-04",
        "title": "A Security Framework for General Blockchain Layer 2 Protocols",
        "author": "Zeta Avarikioti, Matteo Maffei, and Yuheng Wang",
        "link": "http://arxiv.org/abs/2504.14965v1",
        "abstract": "Layer 2 (L2) solutions are the cornerstone of blockchain scalability,\nenabling high-throughput and low-cost interactions by shifting execution\noff-chain while maintaining security through interactions with the underlying\nledger. Despite their common goals, the principal L2 paradigms -- payment\nchannels, rollups, and sidechains -- differ substantially in architecture and\nassumptions, making it difficult to comparatively analyze their security and\ntrade-offs.\n  To address this, we present the first general security framework for L2\nprotocols. Our framework is based on the IITM-based Universal Composability\n(iUC) framework, in which L2 protocols are modeled as stateful machines\ninteracting with higher-level protocol users and the underlying ledger. The\nmethodology defines a generic execution environment that captures ledger\nevents, message passing, and adversarial scheduling, and characterizes security\nthrough trace-based predicates parameterized by adversarial capabilities and\ntiming assumptions. By abstracting away from protocol-specific details while\npreserving critical interface and execution behavior, the framework enables\nmodular, protocol-agnostic reasoning and composable security proofs across a\nwide range of L2 constructions.\n  To demonstrate its applicability, we analyze an example from each of the\nthree dominant L2 scaling paradigms: a payment channel (Brick), a sidechain\n(Liquid Network), and a rollup (Arbitrum). By instantiating each within our\nframework, we derive their security properties and expose trade-offs. These\ninclude the time for dispute resolution, distribution of off-chain storage and\ncomputation, and varying trust assumptions (e.g., reliance on honest parties or\ndata availability). Our framework unifies the analysis of diverse L2 designs\nand pinpoints their strengths and limitations, providing a foundation for\nsecure, systematic L2 development."
    },
    {
        "date": "2025-04",
        "title": "Fast Adversarial Training with Weak-to-Strong Spatial-Temporal Consistency in the Frequency Domain on Videos",
        "author": "Songping Wang, Hanqing Liu, Yueming Lyu, Xiantao Hu, Ziwen He, Wei Wang, Caifeng Shan, and Liang Wang",
        "link": "http://arxiv.org/abs/2504.14921v1",
        "abstract": "Adversarial Training (AT) has been shown to significantly enhance adversarial\nrobustness via a min-max optimization approach. However, its effectiveness in\nvideo recognition tasks is hampered by two main challenges. First, fast\nadversarial training for video models remains largely unexplored, which\nseverely impedes its practical applications. Specifically, most video\nadversarial training methods are computationally costly, with long training\ntimes and high expenses. Second, existing methods struggle with the trade-off\nbetween clean accuracy and adversarial robustness. To address these challenges,\nwe introduce Video Fast Adversarial Training with Weak-to-Strong consistency\n(VFAT-WS), the first fast adversarial training method for video data.\nSpecifically, VFAT-WS incorporates the following key designs: First, it\nintegrates a straightforward yet effective temporal frequency augmentation\n(TF-AUG), and its spatial-temporal enhanced form STF-AUG, along with a\nsingle-step PGD attack to boost training efficiency and robustness. Second, it\ndevises a weak-to-strong spatial-temporal consistency regularization, which\nseamlessly integrates the simpler TF-AUG and the more complex STF-AUG.\nLeveraging the consistency regularization, it steers the learning process from\nsimple to complex augmentations. Both of them work together to achieve a better\ntrade-off between clean accuracy and robustness. Extensive experiments on\nUCF-101 and HMDB-51 with both CNN and Transformer-based models demonstrate that\nVFAT-WS achieves great improvements in adversarial robustness and corruption\nrobustness, while accelerating training by nearly 490%."
    },
    {
        "date": "2025-04",
        "title": "Protecting Your Voice: Temporal-aware Robust Watermarking",
        "author": "Yue Li, Weizhi Liu, and Dongdong Lin",
        "link": "http://arxiv.org/abs/2504.14832v1",
        "abstract": "The rapid advancement of generative models has led to the synthesis of\nreal-fake ambiguous voices. To erase the ambiguity, embedding watermarks into\nthe frequency-domain features of synthesized voices has become a common\nroutine. However, the robustness achieved by choosing the frequency domain\noften comes at the expense of fine-grained voice features, leading to a loss of\nfidelity. Maximizing the comprehensive learning of time-domain features to\nenhance fidelity while maintaining robustness, we pioneer a\n\\textbf{\\underline{t}}emporal-aware\n\\textbf{\\underline{r}}ob\\textbf{\\underline{u}}st\nwat\\textbf{\\underline{e}}rmarking (\\emph{True}) method for protecting the\nspeech and singing voice."
    },
    {
        "date": "2025-04",
        "title": "DONOD: Robust and Generalizable Instruction Fine-Tuning for LLMs via Model-Intrinsic Dataset Pruning",
        "author": "Jucheng Hu, Surong Yang, Dongzhan Zhou, and Lijun Wu",
        "link": "http://arxiv.org/abs/2504.14810v1",
        "abstract": "Ad-hoc instruction fine-tuning of large language models (LLMs) is widely\nadopted for domain-specific adaptation. While domain-specific supervised\nfine-tuning (SFT) is effective and efficient, it often weakens cross-domain\ngeneralization and struggles with noisy training data. To address these\nchallenges, we propose DONOD, a lightweight model-intrinsic data pruning\nmethod. Our approach evaluates data using two model-parameter-based metrics:\nDelta of Norm (DON), which captures the cumulative influence on model weights,\nand Norm of Delta (NOD), which quantifies weight instability. Moreover, by\nemploying the Technique for Order of Preference by Similarity to Ideal Solution\n(TOPSIS) algorithm, we effectively filter noisy, unlearnable, and\ngeneralization-harming samples without relying on auxiliary models during the\nSFT process. Experiments on mathematical tasks demonstrate that data selected\nby DONOD achieve superior fine-tuning efficiency and improved robustness\nagainst noisy data. By filtering out 70% of the full dataset, we improve\ntarget-domain accuracy by 14.90% and cross-domain accuracy by 5.67%. Meanwhile,\nour selected data present superior cross-architecture generalization. Data\npruned by smaller models (e.g., Llama 3.1-8B) generalize effectively on larger\nmodels (e.g., Llama 2-13B). Compared to existing related methodologies, DONOD\ndemonstrates comparable or superior performance while remaining\ndataset-agnostic, enabling broader applicability."
    },
    {
        "date": "2025-04",
        "title": "Verifying Robust Unlearning: Probing Residual Knowledge in Unlearned Models",
        "author": "Hao Xuan, and Xingyu Li",
        "link": "http://arxiv.org/abs/2504.14798v1",
        "abstract": "Machine Unlearning (MUL) is crucial for privacy protection and content\nregulation, yet recent studies reveal that traces of forgotten information\npersist in unlearned models, enabling adversaries to resurface removed\nknowledge. Existing verification methods only confirm whether unlearning was\nexecuted, failing to detect such residual information leaks. To address this,\nwe introduce the concept of Robust Unlearning, ensuring models are\nindistinguishable from retraining and resistant to adversarial recovery. To\nempirically evaluate whether unlearning techniques meet this security standard,\nwe propose the Unlearning Mapping Attack (UMA), a post-unlearning verification\nframework that actively probes models for forgotten traces using adversarial\nqueries. Extensive experiments on discriminative and generative tasks show that\nexisting unlearning techniques remain vulnerable, even when passing existing\nverification metrics. By establishing UMA as a practical verification tool,\nthis study sets a new standard for assessing and enhancing machine unlearning\nsecurity."
    },
    {
        "date": "2025-04",
        "title": "Decoupling Identity from Access: Credential Broker Patterns for Secure CI/CD",
        "author": "Surya Teja Avirneni",
        "link": "http://arxiv.org/abs/2504.14761v1",
        "abstract": "Credential brokers offer a way to separate identity from access in CI/CD\nsystems. This paper shows how verifiable identities issued at runtime, such as\nthose from SPIFFE, can be used with brokers to enable short-lived,\npolicy-driven credentials for pipelines and workloads. We walk through\npractical design patterns, including brokers that issue tokens just in time,\napply access policies, and operate across trust domains. These ideas help\nreduce static permissions, improve auditability, and support Zero Trust goals\nin deployment workflows. This is the second paper in a three-part series on\nsecure CI/CD identity architecture."
    },
    {
        "date": "2025-04",
        "title": "LeetCodeDataset: A Temporal Dataset for Robust Evaluation and Efficient Training of Code LLMs",
        "author": "Yunhui Xia, Wei Shen, Yan Wang, Jason Klein Liu, Huifeng Sun, Siyue Wu, Jian Hu, and Xiaolong Xu",
        "link": "http://arxiv.org/abs/2504.14655v1",
        "abstract": "We introduce LeetCodeDataset, a high-quality benchmark for evaluating and\ntraining code-generation models, addressing two key challenges in LLM research:\nthe lack of reasoning-focused coding benchmarks and self-contained training\ntestbeds. By curating LeetCode Python problems with rich metadata, broad\ncoverage, 100+ test cases per problem, and temporal splits (pre/post July\n2024), our dataset enables contamination-free evaluation and efficient\nsupervised fine-tuning (SFT). Experiments show reasoning models significantly\noutperform non-reasoning counterparts, while SFT with only 2.6K model-generated\nsolutions achieves performance comparable to 110K-sample counterparts. The\ndataset and evaluation framework are available on Hugging Face and Github."
    },
    {
        "date": "2025-04",
        "title": "SMTT: Novel Structured Multi-task Tracking with Graph-Regularized Sparse Representation for Robust Thermal Infrared Target Tracking",
        "author": "Shang Zhang, HuiPan Guan, XiaoBo Ding, Ruoyan Xiong, and Yue Zhang",
        "link": "http://arxiv.org/abs/2504.14566v1",
        "abstract": "Thermal infrared target tracking is crucial in applications such as\nsurveillance, autonomous driving, and military operations. In this paper, we\npropose a novel tracker, SMTT, which effectively addresses common challenges in\nthermal infrared imagery, such as noise, occlusion, and rapid target motion, by\nleveraging multi-task learning, joint sparse representation, and adaptive graph\nregularization. By reformulating the tracking task as a multi-task learning\nproblem, the SMTT tracker independently optimizes the representation of each\nparticle while dynamically capturing spatial and feature-level similarities\nusing a weighted mixed-norm regularization strategy. To ensure real-time\nperformance, we incorporate the Accelerated Proximal Gradient method for\nefficient optimization. Extensive experiments on benchmark datasets - including\nVOT-TIR, PTB-TIR, and LSOTB-TIR - demonstrate that SMTT achieves superior\naccuracy, robustness, and computational efficiency. These results highlight\nSMTT as a reliable and high-performance solution for thermal infrared target\ntracking in complex environments."
    },
    {
        "date": "2025-04",
        "title": "Towards Model Resistant to Transferable Adversarial Examples via Trigger Activation",
        "author": "Yi Yu, Song Xia, Xun Lin, Chenqi Kong, Wenhan Yang, Shijian Lu, Yap-Peng Tan, and Alex C. Kot",
        "link": "http://arxiv.org/abs/2504.14541v1",
        "abstract": "Adversarial examples, characterized by imperceptible perturbations, pose\nsignificant threats to deep neural networks by misleading their predictions. A\ncritical aspect of these examples is their transferability, allowing them to\ndeceive {unseen} models in black-box scenarios. Despite the widespread\nexploration of defense methods, including those on transferability, they show\nlimitations: inefficient deployment, ineffective defense, and degraded\nperformance on clean images. In this work, we introduce a novel training\nparadigm aimed at enhancing robustness against transferable adversarial\nexamples (TAEs) in a more efficient and effective way. We propose a model that\nexhibits random guessing behavior when presented with clean data\n$\\boldsymbol{x}$ as input, and generates accurate predictions when with\ntriggered data $\\boldsymbol{x}+\\boldsymbol{\\tau}$. Importantly, the trigger\n$\\boldsymbol{\\tau}$ remains constant for all data instances. We refer to these\nmodels as \\textbf{models with trigger activation}. We are surprised to find\nthat these models exhibit certain robustness against TAEs. Through the\nconsideration of first-order gradients, we provide a theoretical analysis of\nthis robustness. Moreover, through the joint optimization of the learnable\ntrigger and the model, we achieve improved robustness to transferable attacks.\nExtensive experiments conducted across diverse datasets, evaluating a variety\nof attacking methods, underscore the effectiveness and superiority of our\napproach."
    },
    {
        "date": "2025-04",
        "title": "Vision-Centric Representation-Efficient Fine-Tuning for Robust Universal Foreground Segmentation",
        "author": "Guoyi Zhang, Siyang Chen, Guangsheng Xu, Han Wang, and Xiaohu Zhang",
        "link": "http://arxiv.org/abs/2504.14481v1",
        "abstract": "Foreground segmentation is crucial for scene understanding, yet\nparameter-efficient fine-tuning (PEFT) of vision foundation models (VFMs) often\nfails in complex scenarios, such as camouflage and infrared imagery. We\nattribute this challenge to the inherent texture bias in VFMs, which is\nexacerbated during fine-tuning and limits generalization in texture-sparse\nenvironments. To address this, we propose Ladder Shape-bias Representation\nSide-tuning (LSR-ST), a lightweight PEFT framework that enhances model\nrobustness by introducing shape-biased inductive priors. LSR-ST captures\nshape-aware features using a simple HDConv Block, which integrates large-kernel\nattention and residual learning. The method satisfies three key conditions for\ninducing shape bias: large receptive fields, multi-order feature interactions,\nand sparse connectivity. Our analysis reveals that these improvements stem from\nrepresentation efficiency-the ability to extract task-relevant, structurally\ngrounded features while minimizing redundancy. We formalize this concept via\nInformation Bottleneck theory and advocate for it as a key PEFT objective.\nUnlike traditional NLP paradigms that focus on optimizing parameters and\nmemory, visual tasks require models that extract task-defined semantics, rather\nthan just relying on pre-encoded features. This shift enables our approach to\nmove beyond conventional trade-offs, offering more robust and generalizable\nsolutions for vision tasks. With minimal changes to SAM2-UNet, LSR-ST achieves\nconsistent improvements across 17 datasets and 6 tasks using only 4.719M\ntrainable parameters. These results highlight the potential of representation\nefficiency for robust and adaptable VFMs within complex visual environments."
    },
    {
        "date": "2025-04",
        "title": "Causal Disentanglement for Robust Long-tail Medical Image Generation",
        "author": "Weizhi Nie, Zichun Zhang, Weijie Wang, Bruno Lepri, Anan Liu, and Nicu Seb",
        "link": "http://arxiv.org/abs/2504.14450v1",
        "abstract": "Counterfactual medical image generation effectively addresses data scarcity\nand enhances the interpretability of medical images. However, due to the\ncomplex and diverse pathological features of medical images and the imbalanced\nclass distribution in medical data, generating high-quality and diverse medical\nimages from limited data is significantly challenging. Additionally, to fully\nleverage the information in limited data, such as anatomical structure\ninformation and generate more structurally stable medical images while avoiding\ndistortion or inconsistency. In this paper, in order to enhance the clinical\nrelevance of generated data and improve the interpretability of the model, we\npropose a novel medical image generation framework, which generates independent\npathological and structural features based on causal disentanglement and\nutilizes text-guided modeling of pathological features to regulate the\ngeneration of counterfactual images. First, we achieve feature separation\nthrough causal disentanglement and analyze the interactions between features.\nHere, we introduce group supervision to ensure the independence of pathological\nand identity features. Second, we leverage a diffusion model guided by\npathological findings to model pathological features, enabling the generation\nof diverse counterfactual images. Meanwhile, we enhance accuracy by leveraging\na large language model to extract lesion severity and location from medical\nreports. Additionally, we improve the performance of the latent diffusion model\non long-tailed categories through initial noise optimization."
    },
    {
        "date": "2025-04",
        "title": "Adversarial Attack for RGB-Event based Visual Object Tracking",
        "author": "Qiang Chen, Xiao Wang, Haowen Wang, Bo Jiang, Lin Zhu, Dawei Zhang, Yonghong Tian, and Jin Tang",
        "link": "http://arxiv.org/abs/2504.14423v1",
        "abstract": "Visual object tracking is a crucial research topic in the fields of computer\nvision and multi-modal fusion. Among various approaches, robust visual tracking\nthat combines RGB frames with Event streams has attracted increasing attention\nfrom researchers. While striving for high accuracy and efficiency in tracking,\nit is also important to explore how to effectively conduct adversarial attacks\nand defenses on RGB-Event stream tracking algorithms, yet research in this area\nremains relatively scarce. To bridge this gap, in this paper, we propose a\ncross-modal adversarial attack algorithm for RGB-Event visual tracking. Because\nof the diverse representations of Event streams, and given that Event voxels\nand frames are more commonly used, this paper will focus on these two\nrepresentations for an in-depth study. Specifically, for the RGB-Event voxel,\nwe first optimize the perturbation by adversarial loss to generate RGB frame\nadversarial examples. For discrete Event voxel representations, we propose a\ntwo-step attack strategy, more in detail, we first inject Event voxels into the\ntarget region as initialized adversarial examples, then, conduct a\ngradient-guided optimization by perturbing the spatial location of the Event\nvoxels. For the RGB-Event frame based tracking, we optimize the cross-modal\nuniversal perturbation by integrating the gradient information from multimodal\ndata. We evaluate the proposed approach against attacks on three widely used\nRGB-Event Tracking datasets, i.e., COESOT, FE108, and VisEvent. Extensive\nexperiments show that our method significantly reduces the performance of the\ntracker across numerous datasets in both unimodal and multimodal scenarios. The\nsource code will be released on\nhttps://github.com/Event-AHU/Adversarial_Attack_Defense"
    },
    {
        "date": "2025-04",
        "title": "How Do Mobile Applications Enhance Security? An Exploratory Analysis of Use Cases and Provided Information",
        "author": "Irdin Pekaric, Clemens Sauerwein, Simon Laichner, and Ruth Breu",
        "link": "http://arxiv.org/abs/2504.14421v1",
        "abstract": "The ubiquity of mobile applications has increased dramatically in recent\nyears, opening up new opportunities for cyber attackers and heightening\nsecurity concerns in the mobile ecosystem. As a result, researchers and\npractitioners have intensified their research into improving the security and\nprivacy of mobile applications. At the same time, more and more mobile\napplications have appeared on the market that address the aforementioned\nsecurity issues. However, both academia and industry currently lack a\ncomprehensive overview of these mobile security applications for Android and\niOS platforms, including their respective use cases and the security\ninformation they provide.\n  To address this gap, we systematically collected a total of 410 mobile\napplications from both the App and Play Store. Then, we identified the 20 most\nwidely utilized mobile security applications on both platforms that were\nanalyzed and classified. Our results show six primary use cases and a wide\nrange of security information provided by these applications, thus supporting\nthe core functionalities for ensuring mobile security."
    },
    {
        "date": "2025-04",
        "title": "Quantum-Enhanced Reinforcement Learning for Power Grid Security Assessment",
        "author": "Benjamin M. Peter, and Mert Korkali",
        "link": "http://arxiv.org/abs/2504.14412v1",
        "abstract": "The increasingly challenging task of maintaining power grid security requires\ninnovative solutions. Novel approaches using reinforcement learning (RL) agents\nhave been proposed to help grid operators navigate the massive decision space\nand nonlinear behavior of these complex networks. However, applying RL to power\ngrid security assessment, specifically for combinatorially troublesome\ncontingency analysis problems, has proven difficult to scale. The integration\nof quantum computing into these RL frameworks helps scale by improving\ncomputational efficiency and boosting agent proficiency by leveraging quantum\nadvantages in action exploration and model-based interdependence. To\ndemonstrate a proof-of-concept use of quantum computing for RL agent training\nand simulation, we propose a hybrid agent that runs on quantum hardware using\nIBM's Qiskit Runtime. We also provide detailed insight into the construction of\nparameterized quantum circuits (PQCs) for generating relevant quantum output.\nThis agent's proficiency at maintaining grid stability is demonstrated relative\nto a benchmark model without quantum enhancement using N-k contingency\nanalysis. Additionally, we offer a comparative assessment of the training\nprocedures for RL models integrated with a quantum backend."
    },
    {
        "date": "2025-04",
        "title": "Hydra: An Agentic Reasoning Approach for Enhancing Adversarial Robustness and Mitigating Hallucinations in Vision-Language Models",
        "author": "Chung-En, Yu, Hsuan-Chih, Chen, Brian Jalaian, and Nathaniel D. Bastian",
        "link": "http://arxiv.org/abs/2504.14395v1",
        "abstract": "To develop trustworthy Vision-Language Models (VLMs), it is essential to\naddress adversarial robustness and hallucination mitigation, both of which\nimpact factual accuracy in high-stakes applications such as defense and\nhealthcare. Existing methods primarily focus on either adversarial defense or\nhallucination post-hoc correction, leaving a gap in unified robustness\nstrategies. We introduce \\textbf{Hydra}, an adaptive agentic framework that\nenhances plug-in VLMs through iterative reasoning, structured critiques, and\ncross-model verification, improving both resilience to adversarial\nperturbations and intrinsic model errors. Hydra employs an Action-Critique\nLoop, where it retrieves and critiques visual information, leveraging\nChain-of-Thought (CoT) and In-Context Learning (ICL) techniques to refine\noutputs dynamically. Unlike static post-hoc correction methods, Hydra adapts to\nboth adversarial manipulations and intrinsic model errors, making it robust to\nmalicious perturbations and hallucination-related inaccuracies. We evaluate\nHydra on four VLMs, three hallucination benchmarks, two adversarial attack\nstrategies, and two adversarial defense methods, assessing performance on both\nclean and adversarial inputs. Results show that Hydra surpasses plug-in VLMs\nand state-of-the-art (SOTA) dehallucination methods, even without explicit\nadversarial defenses, demonstrating enhanced robustness and factual\nconsistency. By bridging adversarial resistance and hallucination mitigation,\nHydra provides a scalable, training-free solution for improving the reliability\nof VLMs in real-world applications."
    },
    {
        "date": "2025-04",
        "title": "From Cyber Security Incident Management to Cyber Security Crisis Management in the European Union",
        "author": "Jukka Ruohonen, Kalle Rindell, and Simone Busetti",
        "link": "http://arxiv.org/abs/2504.14220v1",
        "abstract": "Incident management is a classical topic in cyber security. Recently, the\nEuropean Union (EU) has started to consider also the relation between cyber\nsecurity incidents and cyber security crises. These considerations and\npreparations, including those specified in the EU's new cyber security laws,\nconstitute the paper's topic. According to an analysis of the laws and\nassociated policy documents, (i) cyber security crises are equated in the EU to\nlarge-scale cyber security incidents that either exceed a handling capacity of\na single member state or affect at least two member states. For this and other\npurposes, (ii) the new laws substantially increase mandatory reporting about\ncyber security incidents, including but not limited to the large-scale\nincidents. Despite the laws and new governance bodies established by them,\nhowever, (iii) the working of actual cyber security crisis management remains\nunclear particularly at the EU-level. With these policy research results, the\npaper advances the domain of cyber security incident management research by\nelaborating how European law perceives cyber security crises and their relation\nto cyber security incidents, paving the way for many relevant further research\ntopics with practical relevance, whether theoretical, conceptual, or empirical."
    },
    {
        "date": "2025-04",
        "title": "The First VoicePrivacy Attacker Challenge",
        "author": "Natalia Tomashenko, Xiaoxiao Miao, Emmanuel Vincent, and Junichi Yamagishi",
        "link": "http://arxiv.org/abs/2504.14183v1",
        "abstract": "The First VoicePrivacy Attacker Challenge is an ICASSP 2025 SP Grand\nChallenge which focuses on evaluating attacker systems against a set of voice\nanonymization systems submitted to the VoicePrivacy 2024 Challenge. Training,\ndevelopment, and evaluation datasets were provided along with a baseline\nattacker. Participants developed their attacker systems in the form of\nautomatic speaker verification systems and submitted their scores on the\ndevelopment and evaluation data. The best attacker systems reduced the equal\nerror rate (EER) by 25-44% relative w.r.t. the baseline."
    },
    {
        "date": "2025-04",
        "title": "Rethinking Target Label Conditioning in Adversarial Attacks: A 2D Tensor-Guided Generative Approach",
        "author": "Hangyu Liu, Bo Peng, Pengxiang Ding, and Donglin Wang",
        "link": "http://arxiv.org/abs/2504.14137v1",
        "abstract": "Compared to single-target adversarial attacks, multi-target attacks have\ngarnered significant attention due to their ability to generate adversarial\nimages for multiple target classes simultaneously. Existing generative\napproaches for multi-target attacks mainly analyze the effect of the use of\ntarget labels on noise generation from a theoretical perspective, lacking\npractical validation and comprehensive summarization. To address this gap, we\nfirst identify and validate that the semantic feature quality and quantity are\ncritical factors affecting the transferability of targeted attacks: 1) Feature\nquality refers to the structural and detailed completeness of the implanted\ntarget features, as deficiencies may result in the loss of key discriminative\ninformation; 2) Feature quantity refers to the spatial sufficiency of the\nimplanted target features, as inadequacy limits the victim model's attention to\nthis feature. Based on these findings, we propose the 2D Tensor-Guided\nAdversarial Fusion (2D-TGAF) framework, which leverages the powerful generative\ncapabilities of diffusion models to encode target labels into two-dimensional\nsemantic tensors for guiding adversarial noise generation. Additionally, we\ndesign a novel masking strategy tailored for the training process, ensuring\nthat parts of the generated noise retain complete semantic information about\nthe target class. Extensive experiments on the standard ImageNet dataset\ndemonstrate that 2D-TGAF consistently surpasses state-of-the-art methods in\nattack success rates, both on normally trained models and across various\ndefense mechanisms."
    },
    {
        "date": "2025-04",
        "title": "Detecting Zero-Day Web Attacks with an Ensemble of LSTM, GRU, and Stacked Autoencoders",
        "author": "Vahid Babaey, and Hamid Reza Faragardi",
        "link": "http://arxiv.org/abs/2504.14122v1",
        "abstract": "The rapid growth in web-based services has significantly increased security\nrisks related to user information, as web-based attacks become increasingly\nsophisticated and prevalent. Traditional security methods frequently struggle\nto detect previously unknown (zero-day) web attacks, putting sensitive user\ndata at significant risk. Additionally, reducing human intervention in web\nsecurity tasks can minimize errors and enhance reliability. This paper\nintroduces an intelligent system designed to detect zero-day web attacks using\na novel one-class ensemble method consisting of three distinct autoencoder\narchitectures: LSTM autoencoder, GRU autoencoder, and stacked autoencoder. Our\napproach employs a novel tokenization strategy to convert normal web requests\ninto structured numeric sequences, enabling the ensemble model to effectively\nidentify anomalous activities by uniquely concatenating and compressing the\nlatent representations from each autoencoder. The proposed method efficiently\ndetects unknown web attacks while effectively addressing common limitations of\nprevious methods, such as high memory consumption and excessive false positive\nrates. Extensive experimental evaluations demonstrate the superiority of our\nproposed ensemble, achieving remarkable detection metrics: 97.58% accuracy,\n97.52% recall, 99.76% specificity, and 99.99% precision, with an exceptionally\nlow false positive rate of 0.2%. These results underscore our method's\nsignificant potential in enhancing real-world web security through accurate and\nreliable detection of web-based attacks."
    },
    {
        "date": "2025-04",
        "title": "DoomArena: A framework for Testing AI Agents Against Evolving Security Threats",
        "author": "Leo Boisvert, Mihir Bansal, Chandra Kiran Reddy Evuru, Gabriel Huang, Abhay Puri, Avinandan Bose, Maryam Fazel, Quentin Cappart, Jason Stanley, Alexandre Lacoste, Alexandre Drouin, and Krishnamurthy Dvijotham",
        "link": "http://arxiv.org/abs/2504.14064v2",
        "abstract": "We present DoomArena, a security evaluation framework for AI agents.\nDoomArena is designed on three principles: 1) It is a plug-in framework and\nintegrates easily into realistic agentic frameworks like BrowserGym (for web\nagents) and $\\tau$-bench (for tool calling agents); 2) It is configurable and\nallows for detailed threat modeling, allowing configuration of specific\ncomponents of the agentic framework being attackable, and specifying targets\nfor the attacker; and 3) It is modular and decouples the development of attacks\nfrom details of the environment in which the agent is deployed, allowing for\nthe same attacks to be applied across multiple environments. We illustrate\nseveral advantages of our framework, including the ability to adapt to new\nthreat models and environments easily, the ability to easily combine several\npreviously published attacks to enable comprehensive and fine-grained security\ntesting, and the ability to analyze trade-offs between various vulnerabilities\nand performance. We apply DoomArena to state-of-the-art (SOTA) web and\ntool-calling agents and find a number of surprising results: 1) SOTA agents\nhave varying levels of vulnerability to different threat models (malicious user\nvs malicious environment), and there is no Pareto dominant agent across all\nthreat models; 2) When multiple attacks are applied to an agent, they often\ncombine constructively; 3) Guardrail model-based defenses seem to fail, while\ndefenses based on powerful SOTA LLMs work better. DoomArena is available at\nhttps://github.com/ServiceNow/DoomArena."
    },
    {
        "date": "2025-04",
        "title": "Prioritizing Security Practice Adoption: Empirical Insights on Software Security Outcomes in the npm Ecosystem",
        "author": "Nusrat Zahan, and Laurie Williams",
        "link": "http://arxiv.org/abs/2504.14026v1",
        "abstract": "Practitioners often struggle with the overwhelming number of security\npractices outlined in cybersecurity frameworks for risk mitigation. Given the\nlimited budget, time, and resources, practitioners want to prioritize the\nadoption of security practices based on empirical evidence. The goal of this\nstudy is to assist practitioners and policymakers in making informed decisions\non which security practices to adopt by evaluating the relationship between\nsoftware security practices and security outcome metrics. The study\ninvestigated the relationship between security practice adoption and security\noutcomes. We selected the OpenSSF Scorecard metrics to automatically measure\nthe adoption of security practices in npm GitHub repositories. We also explored\nsecurity outcome metrics, such as the number of open vulnerabilities\n(Vul_Count), mean time to remediate (MTTR) vulnerabilities in dependencies, and\nmean time to update (MTTU) dependencies. We conducted regression and causal\nanalysis using 12 Scorecard metrics and their aggregated Scorecard score\n(computed by aggregating individual security practice scores) as predictors and\nVul_Count, MTTR, and MTTU as target variables. Our findings show that higher\naggregated Scorecard scores are associated with fewer Vul_Count and shorter\nMTTU, also supported by causal analysis. However, while the regression model\nsuggests shorter MTTR, causal analysis indicates project characteristics likely\ninfluence MTTR direction. Segment analysis shows that larger, newer\nrepositories with more contributors, dependencies, and downloads have shorter\nMTTR. Among individual security practices, Code Review, Maintained status,\nPinned Dependencies, and Branch Protection show strong associations with\nsecurity outcomes; the directionality of these associations varies across\nsecurity outcomes."
    },
    {
        "date": "2025-04",
        "title": "Outlier-Robust Multi-Model Fitting on Quantum Annealers",
        "author": "Saurabh Pandey, Luca Magri, Federica Arrigoni, and Vladislav Golyanik",
        "link": "http://arxiv.org/abs/2504.13836v1",
        "abstract": "Multi-model fitting (MMF) presents a significant challenge in Computer\nVision, particularly due to its combinatorial nature. While recent advancements\nin quantum computing offer promise for addressing NP-hard problems, existing\nquantum-based approaches for model fitting are either limited to a single model\nor consider multi-model scenarios within outlier-free datasets. This paper\nintroduces a novel approach, the robust quantum multi-model fitting (R-QuMF)\nalgorithm, designed to handle outliers effectively. Our method leverages the\nintrinsic capabilities of quantum hardware to tackle combinatorial challenges\ninherent in MMF tasks, and it does not require prior knowledge of the exact\nnumber of models, thereby enhancing its practical applicability. By formulating\nthe problem as a maximum set coverage task for adiabatic quantum computers\n(AQC), R-QuMF outperforms existing quantum techniques, demonstrating superior\nperformance across various synthetic and real-world 3D datasets. Our findings\nunderscore the potential of quantum computing in addressing the complexities of\nMMF, especially in real-world scenarios with noisy and outlier-prone data."
    },
    {
        "date": "2025-04",
        "title": "Collective Learning Mechanism based Optimal Transport Generative Adversarial Network for Non-parallel Voice Conversion",
        "author": "Sandipan Dhar, Md. Tousin Akhter, Nanda Dulal Jana, and Swagatam Das",
        "link": "http://arxiv.org/abs/2504.13791v1",
        "abstract": "After demonstrating significant success in image synthesis, Generative\nAdversarial Network (GAN) models have likewise made significant progress in the\nfield of speech synthesis, leveraging their capacity to adapt the precise\ndistribution of target data through adversarial learning processes. Notably, in\nthe realm of State-Of-The-Art (SOTA) GAN-based Voice Conversion (VC) models,\nthere exists a substantial disparity in naturalness between real and\nGAN-generated speech samples. Furthermore, while many GAN models currently\noperate on a single generator discriminator learning approach, optimizing\ntarget data distribution is more effectively achievable through a single\ngenerator multi-discriminator learning scheme. Hence, this study introduces a\nnovel GAN model named Collective Learning Mechanism-based Optimal Transport GAN\n(CLOT-GAN) model, incorporating multiple discriminators, including the Deep\nConvolutional Neural Network (DCNN) model, Vision Transformer (ViT), and\nconformer. The objective of integrating various discriminators lies in their\nability to comprehend the formant distribution of mel-spectrograms, facilitated\nby a collective learning mechanism. Simultaneously, the inclusion of Optimal\nTransport (OT) loss aims to precisely bridge the gap between the source and\ntarget data distribution, employing the principles of OT theory. The\nexperimental validation on VCC 2018, VCTK, and CMU-Arctic datasets confirms\nthat the CLOT-GAN-VC model outperforms existing VC models in objective and\nsubjective assessments."
    },
    {
        "date": "2025-04",
        "title": "On the Relationship Between Robustness and Expressivity of Graph Neural Networks",
        "author": "Lorenz Kummer, Wilfried N. Gansterer, and Nils M. Kriege",
        "link": "http://arxiv.org/abs/2504.13786v1",
        "abstract": "We investigate the vulnerability of Graph Neural Networks (GNNs) to bit-flip\nattacks (BFAs) by introducing an analytical framework to study the influence of\narchitectural features, graph properties, and their interaction.\n  The expressivity of GNNs refers to their ability to distinguish\nnon-isomorphic graphs and depends on the encoding of node neighborhoods. We\nexamine the vulnerability of neural multiset functions commonly used for this\npurpose and establish formal criteria to characterize a GNN's susceptibility to\nlosing expressivity due to BFAs. This enables an analysis of the impact of\nhomophily, graph structural variety, feature encoding, and activation functions\non GNN robustness. We derive theoretical bounds for the number of bit flips\nrequired to degrade GNN expressivity on a dataset, identifying ReLU-activated\nGNNs operating on highly homophilous graphs with low-dimensional or one-hot\nencoded features as particularly susceptible. Empirical results using ten\nreal-world datasets confirm the statistical significance of our key theoretical\ninsights and offer actionable results to mitigate BFA risks in\nexpressivity-critical applications."
    },
    {
        "date": "2025-04",
        "title": "BadApex: Backdoor Attack Based on Adaptive Optimization Mechanism of Black-box Large Language Models",
        "author": "Zhengxian Wu, Juan Wen, Wanli Peng, Ziwei Zhang, Yinghan Zhou, and Yiming Xue",
        "link": "http://arxiv.org/abs/2504.13775v2",
        "abstract": "Previous insertion-based and paraphrase-based backdoors have achieved great\nsuccess in attack efficacy, but they ignore the text quality and semantic\nconsistency between poisoned and clean texts. Although recent studies introduce\nLLMs to generate poisoned texts and improve the stealthiness, semantic\nconsistency, and text quality, their hand-crafted prompts rely on expert\nexperiences, facing significant challenges in prompt adaptability and attack\nperformance after defenses. In this paper, we propose a novel backdoor attack\nbased on adaptive optimization mechanism of black-box large language models\n(BadApex), which leverages a black-box LLM to generate poisoned text through a\nrefined prompt. Specifically, an Adaptive Optimization Mechanism is designed to\nrefine an initial prompt iteratively using the generation and modification\nagents. The generation agent generates the poisoned text based on the initial\nprompt. Then the modification agent evaluates the quality of the poisoned text\nand refines a new prompt. After several iterations of the above process, the\nrefined prompt is used to generate poisoned texts through LLMs. We conduct\nextensive experiments on three dataset with six backdoor attacks and two\ndefenses. Extensive experimental results demonstrate that BadApex significantly\noutperforms state-of-the-art attacks. It improves prompt adaptability, semantic\nconsistency, and text quality. Furthermore, when two defense methods are\napplied, the average attack success rate (ASR) still up to 96.75%."
    },
    {
        "date": "2025-04",
        "title": "Analysing the Robustness of Vision-Language-Models to Common Corruptions",
        "author": "Muhammad Usama, Syeda Aishah Asim, Syed Bilal Ali, Syed Talal Wasim, and Umair Bin Mansoor",
        "link": "http://arxiv.org/abs/2504.13690v2",
        "abstract": "Vision-language models (VLMs) have demonstrated impressive capabilities in\nunderstanding and reasoning about visual and textual content. However, their\nrobustness to common image corruptions remains under-explored. In this work, we\npresent the first comprehensive analysis of VLM robustness across 19 corruption\ntypes from the ImageNet-C benchmark, spanning four categories: noise, blur,\nweather, and digital distortions. We introduce two new benchmarks, TextVQA-C\nand GQA-C, to systematically evaluate how corruptions affect scene text\nunderstanding and object-based reasoning, respectively. Our analysis reveals\nthat transformer-based VLMs exhibit distinct vulnerability patterns across\ntasks: text recognition deteriorates most severely under blur and snow\ncorruptions, while object reasoning shows higher sensitivity to corruptions\nsuch as frost and impulse noise. We connect these observations to the\nfrequency-domain characteristics of different corruptions, revealing how\ntransformers' inherent bias toward low-frequency processing explains their\ndifferential robustness patterns. Our findings provide valuable insights for\ndeveloping more corruption-robust vision-language models for real-world\napplications."
    },
    {
        "date": "2025-04",
        "title": "Going Whole Hog: A Philosophical Defense of AI Cognition",
        "author": "Herman Cappelen, and Josh Dever",
        "link": "http://arxiv.org/abs/2504.13988v1",
        "abstract": "This work defends the 'Whole Hog Thesis': sophisticated Large Language Models\n(LLMs) like ChatGPT are full-blown linguistic and cognitive agents, possessing\nunderstanding, beliefs, desires, knowledge, and intentions. We argue against\nprevailing methodologies in AI philosophy, rejecting starting points based on\nlow-level computational details ('Just an X' fallacy) or pre-existing theories\nof mind. Instead, we advocate starting with simple, high-level observations of\nLLM behavior (e.g., answering questions, making suggestions) -- defending this\ndata against charges of metaphor, loose talk, or pretense. From these\nobservations, we employ 'Holistic Network Assumptions' -- plausible connections\nbetween mental capacities (e.g., answering implies knowledge, knowledge implies\nbelief, action implies intention) -- to argue for the full suite of cognitive\nstates. We systematically rebut objections based on LLM failures\n(hallucinations, planning/reasoning errors), arguing these don't preclude\nagency, often mirroring human fallibility. We address numerous 'Games of\nLacks', arguing that LLMs do not lack purported necessary conditions for\ncognition (e.g., semantic grounding, embodiment, justification, intrinsic\nintentionality) or that these conditions are not truly necessary, often relying\non anti-discriminatory arguments comparing LLMs to diverse human capacities.\nOur approach is evidential, not functionalist, and deliberately excludes\nconsciousness. We conclude by speculating on the possibility of LLMs possessing\n'alien' contents beyond human conceptual schemes."
    },
    {
        "date": "2025-04",
        "title": "Fairness and Robustness in Machine Unlearning",
        "author": "Khoa Tran, and Simon S. Woo",
        "link": "http://arxiv.org/abs/2504.13610v1",
        "abstract": "Machine unlearning poses the challenge of ``how to eliminate the influence of\nspecific data from a pretrained model'' in regard to privacy concerns. While\nprior research on approximated unlearning has demonstrated accuracy and\nefficiency in time complexity, we claim that it falls short of achieving exact\nunlearning, and we are the first to focus on fairness and robustness in machine\nunlearning algorithms. Our study presents fairness Conjectures for a\nwell-trained model, based on the variance-bias trade-off characteristic, and\nconsiders their relevance to robustness. Our Conjectures are supported by\nexperiments conducted on the two most widely used model architectures, ResNet\nand ViT, demonstrating the correlation between fairness and robustness:\n\\textit{the higher fairness-gap is, the more the model is sensitive and\nvulnerable}. In addition, our experiments demonstrate the vulnerability of\ncurrent state-of-the-art approximated unlearning algorithms to adversarial\nattacks, where their unlearned models suffer a significant drop in accuracy\ncompared to the exact-unlearned models. We claim that our fairness-gap\nmeasurement and robustness metric should be used to evaluate the unlearning\nalgorithm. Furthermore, we demonstrate that unlearning in the intermediate and\nlast layers is sufficient and cost-effective for time and memory complexity."
    },
    {
        "date": "2025-04",
        "title": "Q-FAKER: Query-free Hard Black-box Attack via Controlled Generation",
        "author": "CheolWon Na, YunSeok Choi, and Jee-Hyong Lee",
        "link": "http://arxiv.org/abs/2504.13551v1",
        "abstract": "Many adversarial attack approaches are proposed to verify the vulnerability\nof language models. However, they require numerous queries and the information\non the target model. Even black-box attack methods also require the target\nmodel's output information. They are not applicable in real-world scenarios, as\nin hard black-box settings where the target model is closed and inaccessible.\nEven the recently proposed hard black-box attacks still require many queries\nand demand extremely high costs for training adversarial generators. To address\nthese challenges, we propose Q-faker (Query-free Hard Black-box Attacker), a\nnovel and efficient method that generates adversarial examples without\naccessing the target model. To avoid accessing the target model, we use a\nsurrogate model instead. The surrogate model generates adversarial sentences\nfor a target-agnostic attack. During this process, we leverage controlled\ngeneration techniques. We evaluate our proposed method on eight datasets.\nExperimental results demonstrate our method's effectiveness including high\ntransferability and the high quality of the generated adversarial examples, and\nprove its practical in hard black-box settings."
    },
    {
        "date": "2025-04",
        "title": "EXAM: Exploiting Exclusive System-Level Cache in Apple M-Series SoCs for Enhanced Cache Occupancy Attacks",
        "author": "Tianhong Xu, Aidong Adam Ding, and Yunsi Fei",
        "link": "http://arxiv.org/abs/2504.13385v1",
        "abstract": "Cache occupancy attacks exploit the shared nature of cache hierarchies to\ninfer a victim's activities by monitoring overall cache usage, unlike\naccess-driven cache attacks that focus on specific cache lines or sets. There\nexists some prior work that target the last-level cache (LLC) of Intel\nprocessors, which is inclusive of higher-level caches, and L2 caches of ARM\nsystems. In this paper, we target the System-Level Cache (SLC) of Apple\nM-series SoCs, which is exclusive to higher-level CPU caches. We address the\nchallenges of the exclusiveness and propose a suite of SLC-cache occupancy\nattacks, the first of its kind, where an adversary can monitor GPU and other\nCPU cluster activities from their own CPU cluster. We first discover the\nstructure of SLC in Apple M1 SOC and various policies pertaining to access and\nsharing through reverse engineering. We propose two attacks against websites.\nOne is a coarse-grained fingerprinting attack, recognizing which website is\naccessed based on their different GPU memory access patterns monitored through\nthe SLC occupancy channel. The other attack is a fine-grained pixel stealing\nattack, which precisely monitors the GPU memory usage for rendering different\npixels, through the SLC occupancy channel. Third, we introduce a novel screen\ncapturing attack which works beyond webpages, with the monitoring granularity\nof 57 rows of pixels (there are 1600 rows for the screen). This significantly\nexpands the attack surface, allowing the adversary to retrieve any screen\ndisplay, posing a substantial new threat to system security. Our findings\nreveal critical vulnerabilities in Apple's M-series SoCs and emphasize the\nurgent need for effective countermeasures against cache occupancy attacks in\nheterogeneous computing environments."
    },
    {
        "date": "2025-04",
        "title": "The Impact of AI on the Cyber Offense-Defense Balance and the Character of Cyber Conflict",
        "author": "Andrew J. Lohn",
        "link": "http://arxiv.org/abs/2504.13371v1",
        "abstract": "Unlike other domains of conflict, and unlike other fields with high\nanticipated risk from AI, the cyber domain is intrinsically digital with a\ntight feedback loop between AI training and cyber application. Cyber may have\nsome of the largest and earliest impacts from AI, so it is important to\nunderstand how the cyber domain may change as AI continues to advance. Our\napproach reviewed the literature, collecting nine arguments that have been\nproposed for offensive advantage in cyber conflict and nine proposed arguments\nfor defensive advantage. We include an additional forty-eight arguments that\nhave been proposed to give cyber conflict and competition its character as\ncollected separately by Healey, Jervis, and Nandrajog. We then consider how\neach of those arguments and propositions might change with varying degrees of\nAI advancement. We find that the cyber domain is too multifaceted for a single\nanswer to whether AI will enhance offense or defense broadly. AI will improve\nsome aspects, hinder others, and leave some aspects unchanged. We collect and\npresent forty-four ways that we expect AI to impact the cyber offense-defense\nbalance and the character of cyber conflict and competition."
    },
    {
        "date": "2025-04",
        "title": "GraphQLer: Enhancing GraphQL Security with Context-Aware API Testing",
        "author": "Omar Tsai, Jianing Li, Tsz Tung Cheung, Lejing Huang, Hao Zhu, Jianrui Xiao, Iman Sharafaldin, and Mohammad A. Tayebi",
        "link": "http://arxiv.org/abs/2504.13358v1",
        "abstract": "GraphQL is an open-source data query and manipulation language for web\napplications, offering a flexible alternative to RESTful APIs. However, its\ndynamic execution model and lack of built-in security mechanisms expose it to\nvulnerabilities such as unauthorized data access, denial-of-service (DoS)\nattacks, and injections. Existing testing tools focus on functional\ncorrectness, often overlooking security risks stemming from query\ninterdependencies and execution context. This paper presents GraphQLer, the\nfirst context-aware security testing framework for GraphQL APIs. GraphQLer\nconstructs a dependency graph to analyze relationships among mutations,\nqueries, and objects, capturing critical interdependencies. It chains related\nqueries and mutations to reveal authentication and authorization flaws, access\ncontrol bypasses, and resource misuse. Additionally, GraphQLer tracks internal\nresource usage to uncover data leakage, privilege escalation, and replay attack\nvectors. We assess GraphQLer on various GraphQL APIs, demonstrating improved\ntesting coverage - averaging a 35% increase, with up to 84% in some cases -\ncompared to top-performing baselines. Remarkably, this is achieved in less\ntime, making GraphQLer suitable for time-sensitive contexts. GraphQLer also\nsuccessfully detects a known CVE and potential vulnerabilities in large-scale\nproduction APIs. These results underline GraphQLer's utility in proactively\nsecuring GraphQL APIs through automated, context-aware vulnerability detection."
    },
    {
        "date": "2025-04",
        "title": "On the Definition of Robustness and Resilience of AI Agents for Real-time Congestion Management",
        "author": "Timothy Tjhay, Ricardo J. Bessa, and Jose Paulos",
        "link": "http://arxiv.org/abs/2504.13314v1",
        "abstract": "The European Union's Artificial Intelligence (AI) Act defines robustness,\nresilience, and security requirements for high-risk sectors but lacks detailed\nmethodologies for assessment. This paper introduces a novel framework for\nquantitatively evaluating the robustness and resilience of reinforcement\nlearning agents in congestion management. Using the AI-friendly digital\nenvironment Grid2Op, perturbation agents simulate natural and adversarial\ndisruptions by perturbing the input of AI systems without altering the actual\nstate of the environment, enabling the assessment of AI performance under\nvarious scenarios. Robustness is measured through stability and reward impact\nmetrics, while resilience quantifies recovery from performance degradation. The\nresults demonstrate the framework's effectiveness in identifying\nvulnerabilities and improving AI robustness and resilience for critical\napplications."
    },
    {
        "date": "2025-04",
        "title": "DYNAMITE: Dynamic Defense Selection for Enhancing Machine Learning-based Intrusion Detection Against Adversarial Attacks",
        "author": "Jing Chen, Onat Gungor, Zhengli Shang, Elvin Li, and Tajana Rosing",
        "link": "http://arxiv.org/abs/2504.13301v1",
        "abstract": "The rapid proliferation of the Internet of Things (IoT) has introduced\nsubstantial security vulnerabilities, highlighting the need for robust\nIntrusion Detection Systems (IDS). Machine learning-based intrusion detection\nsystems (ML-IDS) have significantly improved threat detection capabilities;\nhowever, they remain highly susceptible to adversarial attacks. While numerous\ndefense mechanisms have been proposed to enhance ML-IDS resilience, a\nsystematic approach for selecting the most effective defense against a specific\nadversarial attack remains absent. To address this challenge, we propose\nDynamite, a dynamic defense selection framework that enhances ML-IDS by\nintelligently identifying and deploying the most suitable defense using a\nmachine learning-driven selection mechanism. Our results demonstrate that\nDynamite achieves a 96.2% reduction in computational time compared to the\nOracle, significantly decreasing computational overhead while preserving strong\nprediction performance. Dynamite also demonstrates an average F1-score\nimprovement of 76.7% over random defense and 65.8% over the best static\nstate-of-the-art defense."
    },
    {
        "date": "2025-04",
        "title": "Energy-Based Reward Models for Robust Language Model Alignment",
        "author": "Anamika Lochab, and Ruqi Zhang",
        "link": "http://arxiv.org/abs/2504.13134v1",
        "abstract": "Reward models (RMs) are essential for aligning Large Language Models (LLMs)\nwith human preferences. However, they often struggle with capturing complex\nhuman preferences and generalizing to unseen data. To address these challenges,\nwe introduce Energy-Based Reward Model (EBRM), a lightweight post-hoc\nrefinement framework that enhances RM robustness and generalization. EBRM\nmodels the reward distribution explicitly, capturing uncertainty in human\npreferences and mitigating the impact of noisy or misaligned annotations. It\nachieves this through conflict-aware data filtering, label-noise-aware\ncontrastive training, and hybrid initialization. Notably, EBRM enhances RMs\nwithout retraining, making it computationally efficient and adaptable across\ndifferent models and tasks. Empirical evaluations on RM benchmarks demonstrate\nsignificant improvements in both robustness and generalization, achieving up to\na 5.97% improvement in safety-critical alignment tasks compared to standard\nRMs. Furthermore, reinforcement learning experiments confirm that our refined\nrewards enhance alignment quality, effectively delaying reward hacking. These\nresults demonstrate our approach as a scalable and effective enhancement for\nexisting RMs and alignment pipelines. The code is available at EBRM."
    },
    {
        "date": "2025-04",
        "title": "Enhancing Cocoa Pod Disease Classification via Transfer Learning and Ensemble Methods: Toward Robust Predictive Modeling",
        "author": "Devina Anduyan, Nyza Cabillo, Navy Gultiano, and Mark Phil Pacot",
        "link": "http://arxiv.org/abs/2504.12992v1",
        "abstract": "This study presents an ensemble-based approach for cocoa pod disease\nclassification by integrating transfer learning with three ensemble learning\nstrategies: Bagging, Boosting, and Stacking. Pre-trained convolutional neural\nnetworks, including VGG16, VGG19, ResNet50, ResNet101, InceptionV3, and\nXception, were fine-tuned and employed as base learners to detect three disease\ncategories: Black Pod Rot, Pod Borer, and Healthy. A balanced dataset of 6,000\ncocoa pod images was curated and augmented to ensure robustness against\nvariations in lighting, orientation, and disease severity. The performance of\neach ensemble method was evaluated using accuracy, precision, recall, and\nF1-score. Experimental results show that Bagging consistently achieved superior\nclassification performance with a test accuracy of 100%, outperforming Boosting\n(97%) and Stacking (92%). The findings confirm that combining transfer learning\nwith ensemble techniques improves model generalization and reliability, making\nit a promising direction for precision agriculture and automated crop disease\nmanagement."
    },
    {
        "date": "2025-04",
        "title": "PSG-MAE: Robust Multitask Sleep Event Monitoring using Multichannel PSG Reconstruction and Inter-channel Contrastive Learning",
        "author": "Yifei Wang, Qi Liu, Fuli Min, and Honghao Wang",
        "link": "http://arxiv.org/abs/2504.13229v1",
        "abstract": "Polysomnography (PSG) signals are essential for studying sleep processes and\ndiagnosing sleep disorders. Analyzing PSG data through deep neural networks\n(DNNs) for automated sleep monitoring has become increasingly feasible.\nHowever, the limited availability of datasets for certain sleep events often\nleads to DNNs focusing on a single task with a single-sourced training dataset.\nAs a result, these models struggle to transfer to new sleep events and lack\nrobustness when applied to new datasets. To address these challenges, we\npropose PSG-MAE, a mask autoencoder (MAE) based pre-training framework. By\nperforming self-supervised learning on a large volume of unlabeled PSG data,\nPSG-MAE develops a robust feature extraction network that can be broadly\napplied to various sleep event monitoring tasks. Unlike conventional MAEs,\nPSG-MAE generates complementary masks across PSG channels, integrates a\nmultichannel signal reconstruction method, and employs a self-supervised\ninter-channel contrastive learning (ICCL) strategy. This approach enables the\nencoder to capture temporal features from each channel while simultaneously\nlearning latent relationships between channels, thereby enhancing the\nutilization of multichannel information. Experimental results show that PSG-MAE\neffectively captures both temporal details and inter-channel information from\nPSG signals. When the encoder pre-trained through PSG-MAE is fine-tuned with\ndownstream feature decomposition networks, it achieves an accuracy of 83.7% for\nsleep staging and 90.45% for detecting obstructive sleep apnea, which\nhighlights the framework's robustness and broad applicability."
    },
    {
        "date": "2025-04",
        "title": "Adversarial Resilience against Clean-Label Attacks in Realizable and Noisy Settings",
        "author": "Carolin Heinzler",
        "link": "http://arxiv.org/abs/2504.13966v1",
        "abstract": "We investigate the challenge of establishing stochastic-like guarantees when\nsequentially learning from a stream of i.i.d. data that includes an unknown\nquantity of clean-label adversarial samples. We permit the learner to abstain\nfrom making predictions when uncertain. The regret of the learner is measured\nin terms of misclassification and abstention error, where we allow the learner\nto abstain for free on adversarial injected samples. This approach is based on\nthe work of Goel, Hanneke, Moran, and Shetty from arXiv:2306.13119. We explore\nthe methods they present and manage to correct inaccuracies in their\nargumentation.\n  However, this approach is limited to the realizable setting, where labels are\nassigned according to some function $f^*$ from the hypothesis space\n$\\mathcal{F}$. Based on similar arguments, we explore methods to make\nadaptations for the agnostic setting where labels are random. Introducing the\nnotion of a clean-label adversary in the agnostic context, we are the first to\ngive a theoretical analysis of a disagreement-based learner for thresholds,\nsubject to a clean-label adversary with noise."
    },
    {
        "date": "2025-04",
        "title": "SoK: Security of EMV Contactless Payment Systems",
        "author": "Mahshid Mehr Nezhad, Feng Hao, Gregory Epiphaniou, Carsten Maple, and Timur Yunusov",
        "link": "http://arxiv.org/abs/2504.12812v1",
        "abstract": "The widespread adoption of EMV (Europay, Mastercard, and Visa) contactless\npayment systems has greatly improved convenience for both users and merchants.\nHowever, this growth has also exposed significant security challenges. This SoK\nprovides a comprehensive analysis of security vulnerabilities in EMV\ncontactless payments, particularly within the open-loop systems used by Visa\nand Mastercard. We categorize attacks into seven attack vectors across three\nkey areas: application selection, cardholder authentication, and transaction\nauthorization. We replicate the attacks on Visa and Mastercard protocols using\nour experimental platform to determine their practical feasibility and offer\ninsights into the current security landscape of contactless payments. Our study\nalso includes a detailed evaluation of the underlying protocols, along with a\ncomparative analysis of Visa and Mastercard, highlighting vulnerabilities and\nrecommending countermeasures."
    },
    {
        "date": "2025-04",
        "title": "A Numerical Gradient Inversion Attack in Variational Quantum Neural-Networks",
        "author": "Georgios Papadopoulos, Shaltiel Eloul, Yash Satsangi, Jamie Heredge, Niraj Kumar, Chun-Fu Chen, and Marco Pistoia",
        "link": "http://arxiv.org/abs/2504.12806v1",
        "abstract": "The loss landscape of Variational Quantum Neural Networks (VQNNs) is\ncharacterized by local minima that grow exponentially with increasing qubits.\nBecause of this, it is more challenging to recover information from model\ngradients during training compared to classical Neural Networks (NNs). In this\npaper we present a numerical scheme that successfully reconstructs input\ntraining, real-world, practical data from trainable VQNNs' gradients. Our\nscheme is based on gradient inversion that works by combining gradients\nestimation with the finite difference method and adaptive low-pass filtering.\nThe scheme is further optimized with Kalman filter to obtain efficient\nconvergence. Our experiments show that our algorithm can invert even\nbatch-trained data, given the VQNN model is sufficiently over-parameterized."
    },
    {
        "date": "2025-04",
        "title": "MCP Guardian: A Security-First Layer for Safeguarding MCP-Based AI System",
        "author": "Sonu Kumar, Anubhav Girdhar, Ritesh Patil, and Divyansh Tripathi",
        "link": "http://arxiv.org/abs/2504.12757v1",
        "abstract": "As Agentic AI gain mainstream adoption, the industry invests heavily in model\ncapabilities, achieving rapid leaps in reasoning and quality. However, these\nsystems remain largely confined to data silos, and each new integration\nrequires custom logic that is difficult to scale. The Model Context Protocol\n(MCP) addresses this challenge by defining a universal, open standard for\nsecurely connecting AI-based applications (MCP clients) to data sources (MCP\nservers). However, the flexibility of the MCP introduces new risks, including\nmalicious tool servers and compromised data integrity. We present MCP Guardian,\na framework that strengthens MCP-based communication with authentication,\nrate-limiting, logging, tracing, and Web Application Firewall (WAF) scanning.\nThrough real-world scenarios and empirical testing, we demonstrate how MCP\nGuardian effectively mitigates attacks and ensures robust oversight with\nminimal overheads. Our approach fosters secure, scalable data access for AI\nassistants, underscoring the importance of a defense-in-depth approach that\nenables safer and more transparent innovation in AI-driven environments."
    },
    {
        "date": "2025-04",
        "title": "Attack-Defense Trees with Offensive and Defensive Attributes (with Appendix)",
        "author": "Danut-Valentin Copae, Reza Soltani, and Milan Lopuha\u00e4-Zwakenberg",
        "link": "http://arxiv.org/abs/2504.12748v1",
        "abstract": "Effective risk management in cybersecurity requires a thorough understanding\nof the interplay between attacker capabilities and defense strategies.\nAttack-Defense Trees (ADTs) are a commonly used methodology for representing\nthis interplay; however, previous work in this domain has only focused on\nanalyzing metrics such as cost, damage, or time from the perspective of the\nattacker. This approach provides an incomplete view of the system, as it\nneglects to model defender attributes: in real-world scenarios, defenders have\nfinite resources for countermeasures and are similarly constrained. In this\npaper, we propose a novel framework that incorporates defense metrics into\nADTs, and we present efficient algorithms for computing the Pareto front\nbetween defense and attack metrics. Our methods encode both attacker and\ndefender metrics as semirings, allowing our methods to be used for many metrics\nsuch as cost, damage, and skill. We analyze tree-structured ADTs using a\nbottom-up approach and general ADTs by translating them into binary decision\ndiagrams. Experiments on randomly generated ADTS demonstrate that both\napproaches effectively handle ADTs with several hundred nodes."
    },
    {
        "date": "2025-04",
        "title": "Adversary-Augmented Simulation for Fairness Evaluation and Defense in Hyperledger Fabric",
        "author": "Erwan Mahe, Rouwaida Abdallah, Pierre-Yves Piriou, and Sara Tucci-Piergiovanni",
        "link": "http://arxiv.org/abs/2504.12733v1",
        "abstract": "This paper presents an adversary model and a simulation framework\nspecifically tailored for analyzing attacks on distributed systems composed of\nmultiple distributed protocols, with a focus on assessing the security of\nblockchain networks. Our model classifies and constrains adversarial actions\nbased on the assumptions of the target protocols, defined by failure models,\ncommunication models, and the fault tolerance thresholds of Byzantine Fault\nTolerant (BFT) protocols. The goal is to study not only the intended effects of\nadversarial strategies but also their unintended side effects on critical\nsystem properties. We apply this framework to analyze fairness properties in a\nHyperledger Fabric (HF) blockchain network. Our focus is on novel fairness\nattacks that involve coordinated adversarial actions across various HF\nservices. Simulations show that even a constrained adversary can violate\nfairness with respect to specific clients (client fairness) and impact related\nguarantees (order fairness), which relate the reception order of transactions\nto their final order in the blockchain. This paper significantly extends our\nprevious work by introducing and evaluating a mitigation mechanism specifically\ndesigned to counter transaction reordering attacks. We implement and integrate\nthis defense into our simulation environment, demonstrating its effectiveness\nunder diverse conditions."
    },
    {
        "date": "2025-04",
        "title": "ACoRN: Noise-Robust Abstractive Compression in Retrieval-Augmented Language Models",
        "author": "Singon Kim, Gunho Jung, and Seong-Whan Lee",
        "link": "http://arxiv.org/abs/2504.12673v1",
        "abstract": "Abstractive compression utilizes smaller langauge models to condense\nquery-relevant context, reducing computational costs in retrieval-augmented\ngeneration (RAG). However,retrieved documents often include information that is\neither irrelevant to answering the query or misleading due to factual incorrect\ncontent, despite having high relevance scores. This behavior indicates that\nabstractive compressors are more likely to omit important information essential\nfor the correct answer, especially in long contexts where attention dispersion\noccurs. To address this issue, we categorize retrieved documents in a more\nfine-grained manner and propose Abstractive Compression Robust against Noise\n(ACoRN), which introduces two novel training steps. First, we use offline data\naugmentation on the training dataset to enhance compressor robustness against\ntwo distinct types of retrieval noise. Second, since the language modelbased\ncompressor cannot fully utilize information from multiple retrieved documents\nand exhibits positional bias, we perform finetuning to generate summaries\ncentered around key information that directly supports the correct answer. Our\nexperiments demonstrate that T5-large, trained with ACoRN as a compressor,\nimproves EM and F1 scores while preserving the answer string, which could serve\nas direct evidence. ACoRN excels on datasets with many accuracy-reducing\ndocuments, making it highly useful in real-world scenarios."
    },
    {
        "date": "2025-04",
        "title": "AdaptoVision: A Multi-Resolution Image Recognition Model for Robust and Scalable Classification",
        "author": "Md. Sanaullah Chowdhury Lameya Sabrin",
        "link": "http://arxiv.org/abs/2504.12652v1",
        "abstract": "This paper introduces AdaptoVision, a novel convolutional neural network\n(CNN) architecture designed to efficiently balance computational complexity and\nclassification accuracy. By leveraging enhanced residual units, depth-wise\nseparable convolutions, and hierarchical skip connections, AdaptoVision\nsignificantly reduces parameter count and computational requirements while\npreserving competitive performance across various benchmark and medical image\ndatasets. Extensive experimentation demonstrates that AdaptoVision achieves\nstate-of-the-art on BreakHis dataset and comparable accuracy levels, notably\n95.3\\% on CIFAR-10 and 85.77\\% on CIFAR-100, without relying on any pretrained\nweights. The model's streamlined architecture and strategic simplifications\npromote effective feature extraction and robust generalization, making it\nparticularly suitable for deployment in real-time and resource-constrained\nenvironments."
    },
    {
        "date": "2025-04",
        "title": "Quantum Computing Supported Adversarial Attack-Resilient Autonomous Vehicle Perception Module for Traffic Sign Classification",
        "author": "Reek Majumder, Mashrur Chowdhury, Sakib Mahmud Khan, Zadid Khan, Fahim Ahmad, Frank Ngeni, Gurcan Comert, Judith Mwakalonge, and Dimitra Michalaka",
        "link": "http://arxiv.org/abs/2504.12644v1",
        "abstract": "Deep learning (DL)-based image classification models are essential for\nautonomous vehicle (AV) perception modules since incorrect categorization might\nhave severe repercussions. Adversarial attacks are widely studied cyberattacks\nthat can lead DL models to predict inaccurate output, such as incorrectly\nclassified traffic signs by the perception module of an autonomous vehicle. In\nthis study, we create and compare hybrid classical-quantum deep learning\n(HCQ-DL) models with classical deep learning (C-DL) models to demonstrate\nrobustness against adversarial attacks for perception modules. Before feeding\nthem into the quantum system, we used transfer learning models, alexnet and\nvgg-16, as feature extractors. We tested over 1000 quantum circuits in our\nHCQ-DL models for projected gradient descent (PGD), fast gradient sign attack\n(FGSA), and gradient attack (GA), which are three well-known untargeted\nadversarial approaches. We evaluated the performance of all models during\nadversarial attacks and no-attack scenarios. Our HCQ-DL models maintain\naccuracy above 95\\% during a no-attack scenario and above 91\\% for GA and FGSA\nattacks, which is higher than C-DL models. During the PGD attack, our\nalexnet-based HCQ-DL model maintained an accuracy of 85\\% compared to C-DL\nmodels that achieved accuracies below 21\\%. Our results highlight that the\nHCQ-DL models provide improved accuracy for traffic sign classification under\nadversarial settings compared to their classical counterparts."
    },
    {
        "date": "2025-04",
        "title": "Robo-SGG: Exploiting Layout-Oriented Normalization and Restitution for Robust Scene Graph Generation",
        "author": "Changsheng Lv, Mengshi Qi, Zijian Fu, and Huadong Ma",
        "link": "http://arxiv.org/abs/2504.12606v1",
        "abstract": "In this paper, we introduce a novel method named Robo-SGG, i.e.,\nLayout-Oriented Normalization and Restitution for Robust Scene Graph\nGeneration. Compared to the existing SGG setting, the robust scene graph\ngeneration aims to perform inference on a diverse range of corrupted images,\nwith the core challenge being the domain shift between the clean and corrupted\nimages. Existing SGG methods suffer from degraded performance due to\ncompromised visual features e.g., corruption interference or occlusions. To\nobtain robust visual features, we exploit the layout information, which is\ndomain-invariant, to enhance the efficacy of existing SGG methods on corrupted\nimages. Specifically, we employ Instance Normalization(IN) to filter out the\ndomain-specific feature and recover the unchangeable structural features, i.e.,\nthe positional and semantic relationships among objects by the proposed\nLayout-Oriented Restitution. Additionally, we propose a Layout-Embedded Encoder\n(LEE) that augments the existing object and predicate encoders within the SGG\nframework, enriching the robust positional and semantic features of objects and\npredicates. Note that our proposed Robo-SGG module is designed as a\nplug-and-play component, which can be easily integrated into any baseline SGG\nmodel. Extensive experiments demonstrate that by integrating the\nstate-of-the-art method into our proposed Robo-SGG, we achieve relative\nimprovements of 5.6%, 8.0%, and 6.5% in mR@50 for PredCls, SGCls, and SGDet\ntasks on the VG-C dataset, respectively, and achieve new state-of-the-art\nperformance in corruption scene graph generation benchmark (VG-C and GQA-C). We\nwill release our source code and model."
    },
    {
        "date": "2025-04",
        "title": "Provable Secure Steganography Based on Adaptive Dynamic Sampling",
        "author": "Kaiyi Pang",
        "link": "http://arxiv.org/abs/2504.12579v1",
        "abstract": "The security of private communication is increasingly at risk due to\nwidespread surveillance. Steganography, a technique for embedding secret\nmessages within innocuous carriers, enables covert communication over monitored\nchannels. Provably Secure Steganography (PSS) is state of the art for making\nstego carriers indistinguishable from normal ones by ensuring computational\nindistinguishability between stego and cover distributions. However, current\nPSS methods often require explicit access to the distribution of generative\nmodel for both sender and receiver, limiting their practicality in black box\nscenarios. In this paper, we propose a provably secure steganography scheme\nthat does not require access to explicit model distributions for both sender\nand receiver. Our method incorporates a dynamic sampling strategy, enabling\ngenerative models to embed secret messages within multiple sampling choices\nwithout disrupting the normal generation process of the model. Extensive\nevaluations of three real world datasets and three LLMs demonstrate that our\nblackbox method is comparable with existing white-box steganography methods in\nterms of efficiency and capacity while eliminating the degradation of\nsteganography in model generated outputs."
    },
    {
        "date": "2025-04",
        "title": "The Others: Naturally Isolating Out-of-Distribution Samples for Robust Open-Set Semi-Supervised Learning",
        "author": "You Rim Choi, Subeom Park, Seojun Heo, Eunchung Noh, and Hyung-Sin Kim",
        "link": "http://arxiv.org/abs/2504.12569v1",
        "abstract": "Open-Set Semi-Supervised Learning (OSSL) tackles the practical challenge of\nlearning from unlabeled data that may include both in-distribution (ID) and\nunknown out-of-distribution (OOD) classes. However, existing OSSL methods form\nsuboptimal feature spaces by either excluding OOD samples, interfering with\nthem, or overtrusting their information during training. In this work, we\nintroduce MagMatch, a novel framework that naturally isolates OOD samples\nthrough a prototype-based contrastive learning paradigm. Unlike conventional\nmethods, MagMatch does not assign any prototypes to OOD samples; instead, it\nselectively aligns ID samples with class prototypes using an ID-Selective\nMagnetic (ISM) module, while allowing OOD samples - the \"others\" - to remain\nunaligned in the feature space. To support this process, we propose Selective\nMagnetic Alignment (SMA) loss for unlabeled data, which dynamically adjusts\nalignment based on sample confidence. Extensive experiments on diverse datasets\ndemonstrate that MagMatch significantly outperforms existing methods in both\nclosed-set classification accuracy and OOD detection AUROC, especially in\ngeneralizing to unseen OOD data."
    },
    {
        "date": "2025-04",
        "title": "Robust and Scalable Variational Bayes",
        "author": "Carlos Misael Madrid Padilla, Shitao Fan, and Lizhen Lin",
        "link": "http://arxiv.org/abs/2504.12528v1",
        "abstract": "We propose a robust and scalable framework for variational Bayes (VB) that\neffectively handles outliers and contamination of arbitrary nature in large\ndatasets. Our approach divides the dataset into disjoint subsets, computes the\nposterior for each subset, and applies VB approximation independently to these\nposteriors. The resulting variational posteriors with respect to the subsets\nare then aggregated using the geometric median of probability measures,\ncomputed with respect to the Wasserstein distance. This novel aggregation\nmethod yields the Variational Median Posterior (VM-Posterior) distribution. We\nrigorously demonstrate that the VM-Posterior preserves contraction properties\nakin to those of the true posterior, while accounting for approximation errors\nor the variational gap inherent in VB methods. We also provide provable\nrobustness guarantee of the VM-Posterior. Furthermore, we establish a\nvariational Bernstein-von Mises theorem for both multivariate Gaussian\ndistributions with general covariance structures and the mean-field variational\nfamily. To facilitate practical implementation, we adapt existing algorithms\nfor computing the VM-Posterior and evaluate its performance through extensive\nnumerical experiments. The results highlight its robustness and scalability,\nmaking it a reliable tool for Bayesian inference in the presence of complex,\ncontaminated datasets."
    },
    {
        "date": "2025-04",
        "title": "Diffusion Based Robust LiDAR Place Recognition",
        "author": "Benjamin Krummenacher, Jonas Frey, Turcan Tuna, Olga Vysotska, and Marco Hutter",
        "link": "http://arxiv.org/abs/2504.12412v1",
        "abstract": "Mobile robots on construction sites require accurate pose estimation to\nperform autonomous surveying and inspection missions. Localization in\nconstruction sites is a particularly challenging problem due to the presence of\nrepetitive features such as flat plastered walls and perceptual aliasing due to\napartments with similar layouts inter and intra floors. In this paper, we focus\non the global re-positioning of a robot with respect to an accurate scanned\nmesh of the building solely using LiDAR data. In our approach, a neural network\nis trained on synthetic LiDAR point clouds generated by simulating a LiDAR in\nan accurate real-life large-scale mesh. We train a diffusion model with a\nPointNet++ backbone, which allows us to model multiple position candidates from\na single LiDAR point cloud. The resulting model can successfully predict the\nglobal position of LiDAR in confined and complex sites despite the adverse\neffects of perceptual aliasing. The learned distribution of potential global\npositions can provide multi-modal position distribution. We evaluate our\napproach across five real-world datasets and show the place recognition\naccuracy of 77% +/-2m on average while outperforming baselines at a factor of 2\nin mean error."
    },
    {
        "date": "2025-04",
        "title": "Dysarthria Normalization via Local Lie Group Transformations for Robust ASR",
        "author": "Mikhail Osipov",
        "link": "http://arxiv.org/abs/2504.12279v1",
        "abstract": "We present a geometry-driven method for normalizing dysarthric speech using\nlocal Lie group transformations of spectrograms. Time, frequency, and amplitude\ndistortions are modeled as smooth, invertible deformations, parameterized by\nscalar fields and applied via exponential maps. A neural network is trained to\ninfer these fields from synthetic distortions of typical speech-without using\nany pathological data. At test time, the model applies an approximate inverse\nto real dysarthric inputs. Despite zero-shot generalization, we observe\nsubstantial ASR gains, including up to 16 percentage points WER reduction on\nchallenging TORGO samples, with no degradation on clean speech. This work\nintroduces a principled, interpretable approach for robust speech recognition\nunder motor speech disorders"
    },
    {
        "date": "2025-04",
        "title": "SCENT: Robust Spatiotemporal Learning for Continuous Scientific Data via Scalable Conditioned Neural Fields",
        "author": "David Keetae Park, Xihaier Luo, Guang Zhao, Seungjun Lee, Miruna Oprescu, and Shinjae Yoo",
        "link": "http://arxiv.org/abs/2504.12262v1",
        "abstract": "Spatiotemporal learning is challenging due to the intricate interplay between\nspatial and temporal dependencies, the high dimensionality of the data, and\nscalability constraints. These challenges are further amplified in scientific\ndomains, where data is often irregularly distributed (e.g., missing values from\nsensor failures) and high-volume (e.g., high-fidelity simulations), posing\nadditional computational and modeling difficulties. In this paper, we present\nSCENT, a novel framework for scalable and continuity-informed spatiotemporal\nrepresentation learning. SCENT unifies interpolation, reconstruction, and\nforecasting within a single architecture. Built on a transformer-based\nencoder-processor-decoder backbone, SCENT introduces learnable queries to\nenhance generalization and a query-wise cross-attention mechanism to\neffectively capture multi-scale dependencies. To ensure scalability in both\ndata size and model complexity, we incorporate a sparse attention mechanism,\nenabling flexible output representations and efficient evaluation at arbitrary\nresolutions. We validate SCENT through extensive simulations and real-world\nexperiments, demonstrating state-of-the-art performance across multiple\nchallenging tasks while achieving superior scalability."
    },
    {
        "date": "2025-04",
        "title": "Human Aligned Compression for Robust Models",
        "author": "Samuel R\u00e4ber, Andreas Plesner, Till Aczel, and Roger Wattenhofer",
        "link": "http://arxiv.org/abs/2504.12255v1",
        "abstract": "Adversarial attacks on image models threaten system robustness by introducing\nimperceptible perturbations that cause incorrect predictions. We investigate\nhuman-aligned learned lossy compression as a defense mechanism, comparing two\nlearned models (HiFiC and ELIC) against traditional JPEG across various quality\nlevels. Our experiments on ImageNet subsets demonstrate that learned\ncompression methods outperform JPEG, particularly for Vision Transformer\narchitectures, by preserving semantically meaningful content while removing\nadversarial noise. Even in white-box settings where attackers can access the\ndefense, these methods maintain substantial effectiveness. We also show that\nsequential compression--applying rounds of\ncompression/decompression--significantly enhances defense efficacy while\nmaintaining classification performance. Our findings reveal that human-aligned\ncompression provides an effective, computationally efficient defense that\nprotects the image features most relevant to human and machine understanding.\nIt offers a practical approach to improving model robustness against\nadversarial threats."
    },
    {
        "date": "2025-04",
        "title": "SALAD: Improving Robustness and Generalization through Contrastive Learning with Structure-Aware and LLM-Driven Augmented Data",
        "author": "Suyoung Bae, Hyojun Kim, YunSeok Choi, and Jee-Hyong Lee",
        "link": "http://arxiv.org/abs/2504.12185v1",
        "abstract": "In various natural language processing (NLP) tasks, fine-tuning Pre-trained\nLanguage Models (PLMs) often leads to the issue of spurious correlations, which\nnegatively impacts performance, particularly when dealing with\nout-of-distribution data. To address this problem, we propose SALAD}(Structure\nAware and LLM-driven Augmented Data), a novel approach designed to enhance\nmodel robustness and generalization by generating structure-aware and\ncounterfactually augmented data for contrastive learning. Our method leverages\na tagging-based approach to generate structure-aware positive samples and\nutilizes large language models (LLMs) to generate counterfactual negative\nsamples with diverse sentence patterns. By applying contrastive learning, SALAD\nenables the model to focus on learning the structural relationships between key\nsentence components while minimizing reliance on spurious correlations. We\nvalidate our approach through experiments on three tasks: Sentiment\nClassification, Sexism Detection, and Natural Language Inference. The results\ndemonstrate that SALAD not only improves model robustness and performance\nacross different environments but also enhances generalization to\nout-of-distribution datasets and cross-domain scenarios."
    },
    {
        "date": "2025-04",
        "title": "Secure Transfer Learning: Training Clean Models Against Backdoor in (Both) Pre-trained Encoders and Downstream Datasets",
        "author": "Yechao Zhang, Yuxuan Zhou, Tianyu Li, Minghui Li, Shengshan Hu, Wei Luo, and Leo Yu Zhang",
        "link": "http://arxiv.org/abs/2504.11990v1",
        "abstract": "Transfer learning from pre-trained encoders has become essential in modern\nmachine learning, enabling efficient model adaptation across diverse tasks.\nHowever, this combination of pre-training and downstream adaptation creates an\nexpanded attack surface, exposing models to sophisticated backdoor embeddings\nat both the encoder and dataset levels--an area often overlooked in prior\nresearch. Additionally, the limited computational resources typically available\nto users of pre-trained encoders constrain the effectiveness of generic\nbackdoor defenses compared to end-to-end training from scratch. In this work,\nwe investigate how to mitigate potential backdoor risks in resource-constrained\ntransfer learning scenarios. Specifically, we conduct an exhaustive analysis of\nexisting defense strategies, revealing that many follow a reactive workflow\nbased on assumptions that do not scale to unknown threats, novel attack types,\nor different training paradigms. In response, we introduce a proactive mindset\nfocused on identifying clean elements and propose the Trusted Core (T-Core)\nBootstrapping framework, which emphasizes the importance of pinpointing\ntrustworthy data and neurons to enhance model security. Our empirical\nevaluations demonstrate the effectiveness and superiority of T-Core,\nspecifically assessing 5 encoder poisoning attacks, 7 dataset poisoning\nattacks, and 14 baseline defenses across five benchmark datasets, addressing\nfour scenarios of 3 potential backdoor threats."
    },
    {
        "date": "2025-04",
        "title": "Securing the Skies: A Comprehensive Survey on Anti-UAV Methods, Benchmarking, and Future Directions",
        "author": "Yifei Dong, Fengyi Wu, Sanjian Zhang, Guangyu Chen, Yuzhi Hu, Masumi Yano, Jingdong Sun, Siyu Huang, Feng Liu, Qi Dai, and Zhi-Qi Cheng",
        "link": "http://arxiv.org/abs/2504.11967v2",
        "abstract": "Unmanned Aerial Vehicles (UAVs) are indispensable for infrastructure\ninspection, surveillance, and related tasks, yet they also introduce critical\nsecurity challenges. This survey provides a wide-ranging examination of the\nanti-UAV domain, centering on three core objectives-classification, detection,\nand tracking-while detailing emerging methodologies such as diffusion-based\ndata synthesis, multi-modal fusion, vision-language modeling, self-supervised\nlearning, and reinforcement learning. We systematically evaluate\nstate-of-the-art solutions across both single-modality and multi-sensor\npipelines (spanning RGB, infrared, audio, radar, and RF) and discuss\nlarge-scale as well as adversarially oriented benchmarks. Our analysis reveals\npersistent gaps in real-time performance, stealth detection, and swarm-based\nscenarios, underscoring pressing needs for robust, adaptive anti-UAV systems.\nBy highlighting open research directions, we aim to foster innovation and guide\nthe development of next-generation defense strategies in an era marked by the\nextensive use of UAVs."
    },
    {
        "date": "2025-04",
        "title": "Robust and Fine-Grained Detection of AI Generated Texts",
        "author": "Ram Mohan Rao Kadiyala, Siddartha Pullakhandam, Kanwal Mehreen, Drishti Sharma, Siddhant Gupta, Jebish Purbey, Ashay Srivastava, Subhasya TippaReddy, Arvind Reddy Bobbili, Suraj Telugara Chandrashekhar, Modabbir Adeeb, Srinadh Vura, and Hamza Farooq",
        "link": "http://arxiv.org/abs/2504.11952v1",
        "abstract": "An ideal detection system for machine generated content is supposed to work\nwell on any generator as many more advanced LLMs come into existence day by\nday. Existing systems often struggle with accurately identifying AI-generated\ncontent over shorter texts. Further, not all texts might be entirely authored\nby a human or LLM, hence we focused more over partial cases i.e human-LLM\nco-authored texts. Our paper introduces a set of models built for the task of\ntoken classification which are trained on an extensive collection of\nhuman-machine co-authored texts, which performed well over texts of unseen\ndomains, unseen generators, texts by non-native speakers and those with\nadversarial inputs. We also introduce a new dataset of over 2.4M such texts\nmostly co-authored by several popular proprietary LLMs over 23 languages. We\nalso present findings of our models' performance over each texts of each domain\nand generator. Additional findings include comparison of performance against\neach adversarial method, length of input texts and characteristics of generated\ntexts compared to the original human authored texts."
    },
    {
        "date": "2025-04",
        "title": "Flow Intelligence: Robust Feature Matching via Temporal Signature Correlation",
        "author": "Jie Wang, Chen Ye Gan, Caoqi Wei, Jiangtao Wen, and Yuxing Han",
        "link": "http://arxiv.org/abs/2504.11949v1",
        "abstract": "Feature matching across video streams remains a cornerstone challenge in\ncomputer vision. Increasingly, robust multimodal matching has garnered interest\nin robotics, surveillance, remote sensing, and medical imaging. While\ntraditional rely on detecting and matching spatial features, they break down\nwhen faced with noisy, misaligned, or cross-modal data. Recent deep learning\nmethods have improved robustness through learned representations, but remain\nconstrained by their dependence on extensive training data and computational\ndemands. We present Flow Intelligence, a paradigm-shifting approach that moves\nbeyond spatial features by focusing on temporal motion patterns exclusively.\nInstead of detecting traditional keypoints, our method extracts motion\nsignatures from pixel blocks across consecutive frames and extract temporal\nmotion signatures between videos. These motion-based descriptors achieve\nnatural invariance to translation, rotation, and scale variations while\nremaining robust across different imaging modalities. This novel approach also\nrequires no pretraining data, eliminates the need for spatial feature\ndetection, enables cross-modal matching using only temporal motion, and it\noutperforms existing methods in challenging scenarios where traditional\napproaches fail. By leveraging motion rather than appearance, Flow Intelligence\nenables robust, real-time video feature matching in diverse environments."
    },
    {
        "date": "2025-04",
        "title": "SemDiff: Generating Natural Unrestricted Adversarial Examples via Semantic Attributes Optimization in Diffusion Models",
        "author": "Zeyu Dai, Shengcai Liu, Rui He, Jiahao Wu, Ning Lu, Wenqi Fan, Qing Li, and Ke Tang",
        "link": "http://arxiv.org/abs/2504.11923v1",
        "abstract": "Unrestricted adversarial examples (UAEs), allow the attacker to create\nnon-constrained adversarial examples without given clean samples, posing a\nsevere threat to the safety of deep learning models. Recent works utilize\ndiffusion models to generate UAEs. However, these UAEs often lack naturalness\nand imperceptibility due to simply optimizing in intermediate latent noises. In\nlight of this, we propose SemDiff, a novel unrestricted adversarial attack that\nexplores the semantic latent space of diffusion models for meaningful\nattributes, and devises a multi-attributes optimization approach to ensure\nattack success while maintaining the naturalness and imperceptibility of\ngenerated UAEs. We perform extensive experiments on four tasks on three\nhigh-resolution datasets, including CelebA-HQ, AFHQ and ImageNet. The results\ndemonstrate that SemDiff outperforms state-of-the-art methods in terms of\nattack success rate and imperceptibility. The generated UAEs are natural and\nexhibit semantically meaningful changes, in accord with the attributes'\nweights. In addition, SemDiff is found capable of evading different defenses,\nwhich further validates its effectiveness and threatening."
    },
    {
        "date": "2025-04",
        "title": "From Data Behavior to Code Analysis: A Multimodal Study on Security and Privacy Challenges in Blockchain-Based DApp",
        "author": "Haoyang Sun, Yishun Wang, and Xiaoqi Li",
        "link": "http://arxiv.org/abs/2504.11860v1",
        "abstract": "The recent proliferation of blockchain-based decentralized applications\n(DApp) has catalyzed transformative advancements in distributed systems, with\nextensive deployments observed across financial, entertainment, media, and\ncybersecurity domains. These trustless architectures, characterized by their\ndecentralized nature and elimination of third-party intermediaries, have\ngarnered substantial institutional attention. Consequently, the escalating\nsecurity challenges confronting DApp demand rigorous scholarly investigation.\nThis study initiates with a systematic analysis of behavioral patterns derived\nfrom empirical DApp datasets, establishing foundational insights for subsequent\nmethodological developments. The principal security vulnerabilities in\nEthereum-based smart contracts developed via Solidity are then critically\nexamined. Specifically, reentrancy vulnerability attacks are addressed by\nformally representing contract logic using highly expressive code fragments.\nThis enables precise source code-level detection via bidirectional long\nshort-term memory networks with attention mechanisms (BLSTM-ATT). Regarding\nprivacy preservation challenges, contemporary solutions are evaluated through\ndual analytical lenses: identity privacy preservation and transaction anonymity\nenhancement, while proposing future research trajectories in cryptographic\nobfuscation techniques."
    },
    {
        "date": "2025-04",
        "title": "On the Feasibility of Using MultiModal LLMs to Execute AR Social Engineering Attacks",
        "author": "Ting Bi, Chenghang Ye, Zheyu Yang, Ziyi Zhou, Cui Tang, Jun Zhang, Zui Tao, Kailong Wang, Liting Zhou, Yang Yang, and Tianlong Yu",
        "link": "http://arxiv.org/abs/2504.13209v1",
        "abstract": "Augmented Reality (AR) and Multimodal Large Language Models (LLMs) are\nrapidly evolving, providing unprecedented capabilities for human-computer\ninteraction. However, their integration introduces a new attack surface for\nsocial engineering. In this paper, we systematically investigate the\nfeasibility of orchestrating AR-driven Social Engineering attacks using\nMultimodal LLM for the first time, via our proposed SEAR framework, which\noperates through three key phases: (1) AR-based social context synthesis, which\nfuses Multimodal inputs (visual, auditory and environmental cues); (2)\nrole-based Multimodal RAG (Retrieval-Augmented Generation), which dynamically\nretrieves and integrates contextual data while preserving character\ndifferentiation; and (3) ReInteract social engineering agents, which execute\nadaptive multiphase attack strategies through inference interaction loops. To\nverify SEAR, we conducted an IRB-approved study with 60 participants in three\nexperimental configurations (unassisted, AR+LLM, and full SEAR pipeline)\ncompiling a new dataset of 180 annotated conversations in simulated social\nscenarios. Our results show that SEAR is highly effective at eliciting\nhigh-risk behaviors (e.g., 93.3% of participants susceptible to email\nphishing). The framework was particularly effective in building trust, with 85%\nof targets willing to accept an attacker's call after an interaction. Also, we\nidentified notable limitations such as ``occasionally artificial'' due to\nperceived authenticity gaps. This work provides proof-of-concept for AR-LLM\ndriven social engineering attacks and insights for developing defensive\ncountermeasures against next-generation augmented reality threats."
    },
    {
        "date": "2025-04",
        "title": "From Cyber Threat to Data Shield: Constructing Provably Secure File Erasure with Repurposed Ransomware Cryptography",
        "author": "Jiahui Shang, Luning Zhang, and Zhongxiang Zheng",
        "link": "http://arxiv.org/abs/2504.11744v1",
        "abstract": "Ransomware has emerged as a persistent cybersecurity threat,leveraging robust\nencryption schemes that often remain unbroken even after public disclosure of\nsource code. Motivated by the technical resilience of such mechanisms, this\npaper presents SEER (Secure and Efficient Encryption-based Erasure via\nRansomware), a provably secure file destruction system that repurposes\nransomware encryption for legitimate data erasure tasks. SEER integrates the\ntriple-encryption design of the Babuk ransomware family, including\nCurve25519-based key exchange,SHA-256-based key derivation, and the Sosemanuk\nstream cipher, to construct a layered key management architecture. It tightly\ncouples encryption and key destruction by securely erasing session keys\nimmediately after use. Experimental results on an ESXI platform demonstrate\nthat SEER achieves four orders of magnitude performance improvement over the\nDoD 5220.22 standard. The proposed system further ensures provable security\nthrough both theoretical foundations and practical validation, offering an\nefficient and resilient solution for the secure destruction of sensitive data."
    },
    {
        "date": "2025-04",
        "title": "Towards Safe Synthetic Image Generation On the Web: A Multimodal Robust NSFW Defense and Million Scale Dataset",
        "author": "Muhammad Shahid Muneer, and Simon S. Woo",
        "link": "http://arxiv.org/abs/2504.11707v1",
        "abstract": "In the past years, we have witnessed the remarkable success of Text-to-Image\n(T2I) models and their widespread use on the web. Extensive research in making\nT2I models produce hyper-realistic images has led to new concerns, such as\ngenerating Not-Safe-For-Work (NSFW) web content and polluting the web society.\nTo help prevent misuse of T2I models and create a safer web environment for\nusers features like NSFW filters and post-hoc security checks are used in these\nmodels. However, recent work unveiled how these methods can easily fail to\nprevent misuse. In particular, adversarial attacks on text and image modalities\ncan easily outplay defensive measures. %Exploiting such leads to the growing\nconcern of preventing adversarial attacks on text and image modalities.\nMoreover, there is currently no robust multimodal NSFW dataset that includes\nboth prompt and image pairs and adversarial examples. This work proposes a\nmillion-scale prompt and image dataset generated using open-source diffusion\nmodels. Second, we develop a multimodal defense to distinguish safe and NSFW\ntext and images, which is robust against adversarial attacks and directly\nalleviates current challenges. Our extensive experiments show that our model\nperforms well against existing SOTA NSFW detection methods in terms of accuracy\nand recall, drastically reducing the Attack Success Rate (ASR) in multimodal\nadversarial attack scenarios. Code:\nhttps://github.com/shahidmuneer/multimodal-nsfw-defense."
    },
    {
        "date": "2025-04",
        "title": "An Online Adaptation Method for Robust Depth Estimation and Visual Odometry in the Open World",
        "author": "Xingwu Ji, Haochen Niu, Dexin Duan, Rendong Ying, Fei Wen, and Peilin Liu",
        "link": "http://arxiv.org/abs/2504.11698v1",
        "abstract": "Recently, learning-based robotic navigation systems have gained extensive\nresearch attention and made significant progress. However, the diversity of\nopen-world scenarios poses a major challenge for the generalization of such\nsystems to practical scenarios. Specifically, learned systems for scene\nmeasurement and state estimation tend to degrade when the application scenarios\ndeviate from the training data, resulting to unreliable depth and pose\nestimation. Toward addressing this problem, this work aims to develop a visual\nodometry system that can fast adapt to diverse novel environments in an online\nmanner. To this end, we construct a self-supervised online adaptation framework\nfor monocular visual odometry aided by an online-updated depth estimation\nmodule. Firstly, we design a monocular depth estimation network with\nlightweight refiner modules, which enables efficient online adaptation. Then,\nwe construct an objective for self-supervised learning of the depth estimation\nmodule based on the output of the visual odometry system and the contextual\nsemantic information of the scene. Specifically, a sparse depth densification\nmodule and a dynamic consistency enhancement module are proposed to leverage\ncamera poses and contextual semantics to generate pseudo-depths and valid masks\nfor the online adaptation. Finally, we demonstrate the robustness and\ngeneralization capability of the proposed method in comparison with\nstate-of-the-art learning-based approaches on urban, in-house datasets and a\nrobot platform. Code is publicly available at:\nhttps://github.com/jixingwu/SOL-SLAM."
    },
    {
        "date": "2025-04",
        "title": "WaterFlow: Learning Fast & Robust Watermarks using Stable Diffusion",
        "author": "Vinay Shukla, Prachee Sharma, Ryan Rossi, Sungchul Kim, Tong Yu, and Aditya Grover",
        "link": "http://arxiv.org/abs/2504.12354v2",
        "abstract": "The ability to embed watermarks in images is a fundamental problem of\ninterest for computer vision, and is exacerbated by the rapid rise of generated\nimagery in recent times. Current state-of-the-art techniques suffer from\ncomputational and statistical challenges such as the slow execution speed for\npractical deployments. In addition, other works trade off fast watermarking\nspeeds but suffer greatly in their robustness or perceptual quality. In this\nwork, we propose WaterFlow (WF), a fast and extremely robust approach for high\nfidelity visual watermarking based on a learned latent-dependent watermark. Our\napproach utilizes a pretrained latent diffusion model to encode an arbitrary\nimage into a latent space and produces a learned watermark that is then planted\ninto the Fourier Domain of the latent. The transformation is specified via\ninvertible flow layers that enhance the expressivity of the latent space of the\npre-trained model to better preserve image quality while permitting robust and\ntractable detection. Most notably, WaterFlow demonstrates state-of-the-art\nperformance on general robustness and is the first method capable of\neffectively defending against difficult combination attacks. We validate our\nfindings on three widely used real and generated datasets: MS-COCO,\nDiffusionDB, and WikiArt."
    },
    {
        "date": "2025-04",
        "title": "Cybersecurity through Entropy Injection: A Paradigm Shift from Reactive Defense to Proactive Uncertainty",
        "author": "Kush Janani",
        "link": "http://arxiv.org/abs/2504.11661v1",
        "abstract": "Cybersecurity often hinges on unpredictability, with a system's defenses\nbeing strongest when sensitive values and behaviors cannot be anticipated by\nattackers. This paper explores the concept of entropy injection-deliberately\ninfusing randomness into security mechanisms to increase unpredictability and\nenhance system security. We examine the theoretical foundations of\nentropy-based security, analyze real-world implementations including Address\nSpace Layout Randomization (ASLR) and Moving Target Defense (MTD) frameworks,\nevaluate practical challenges in implementation, and compare entropy-based\napproaches with traditional security methods. Our methodology includes a\nsystematic analysis of entropy's role across various security domains, from\ncryptographic operations to system-level defenses. Results demonstrate that\nentropy injection can significantly reduce attack probability, with some\nimplementations showing more than 90% reduction with minimal performance\nimpact. The discussion highlights the trade-offs between security benefits and\noperational complexity, while identifying future directions for\nentropy-enhanced security, including integration with artificial intelligence\nand quantum randomness sources. We conclude that entropy injection represents a\nparadigm shift from reactive defense to proactive uncertainty management,\noffering a strategic approach that can fundamentally alter the balance between\nattackers and defenders in cybersecurity."
    },
    {
        "date": "2025-04",
        "title": "Chypnosis: Stealthy Secret Extraction using Undervolting-based Static Side-channel Attacks",
        "author": "Kyle Mitard, Saleh Khalaj Monfared, Fatemeh Khojasteh Dana, and Shahin Tajik",
        "link": "http://arxiv.org/abs/2504.11633v2",
        "abstract": "There is a growing class of static physical side-channel attacks that allow\nadversaries to extract secrets by probing the persistent state of a circuit.\nTechniques such as laser logic state imaging (LLSI), impedance analysis (IA),\nand static power analysis fall into this category. These attacks require that\nthe targeted data remain constant for a specific duration, which often\nnecessitates halting the circuit's clock. Some methods additionally rely on\nmodulating the chip's supply voltage to probe the circuit. However, tampering\nwith the clock or voltage is typically assumed to be detectable, as secure\nchips often deploy sensors that erase sensitive data upon detecting such\nanomalies. Furthermore, many secure devices use internal clock sources, making\nexternal clock control infeasible. In this work, we introduce a novel class of\nstatic side-channel attacks, called Chypnosis, that enables adversaries to\nfreeze a chip's internal clock by inducing a hibernation state via rapid\nundervolting, and then extracting secrets using static side-channels. We\ndemonstrate that, by rapidly dropping a chip's voltage below the standard\nnominal levels, the attacker can bypass the clock and voltage sensors and put\nthe chip in a so-called brownout condition, in which the chip's transistors\nstop switching, but volatile memories (e.g., Flip-flops and SRAMs) still retain\ntheir data. We test our attack on AMD FPGAs by putting them into hibernation.\nWe show that not only are all clock sources deactivated, but various clock and\nvoltage sensors also fail to detect the tamper event. Afterward, we present the\nsuccessful recovery of secret bits from a hibernated chip using two static\nattacks, namely, LLSI and IA. Finally, we discuss potential countermeasures\nwhich could be integrated into future designs."
    },
    {
        "date": "2025-04",
        "title": "Making Acoustic Side-Channel Attacks on Noisy Keyboards Viable with LLM-Assisted Spectrograms' \"Typo\" Correction",
        "author": "Seyyed Ali Ayati, Jin Hyun Park, Yichen Cai, and Marcus Botacin",
        "link": "http://arxiv.org/abs/2504.11622v1",
        "abstract": "The large integration of microphones into devices increases the opportunities\nfor Acoustic Side-Channel Attacks (ASCAs), as these can be used to capture\nkeystrokes' audio signals that might reveal sensitive information. However, the\ncurrent State-Of-The-Art (SOTA) models for ASCAs, including Convolutional\nNeural Networks (CNNs) and hybrid models, such as CoAtNet, still exhibit\nlimited robustness under realistic noisy conditions. Solving this problem\nrequires either: (i) an increased model's capacity to infer contextual\ninformation from longer sequences, allowing the model to learn that an\ninitially noisily typed word is the same as a futurely collected non-noisy\nword, or (ii) an approach to fix misidentified information from the contexts,\nas one does not type random words, but the ones that best fit the conversation\ncontext. In this paper, we demonstrate that both strategies are viable and\ncomplementary solutions for making ASCAs practical. We observed that no\nexisting solution leverages advanced transformer architectures' power for these\ntasks and propose that: (i) Visual Transformers (VTs) are the candidate\nsolutions for capturing long-term contextual information and (ii)\ntransformer-powered Large Language Models (LLMs) are the candidate solutions to\nfix the ``typos'' (mispredictions) the model might make. Thus, we here present\nthe first-of-its-kind approach that integrates VTs and LLMs for ASCAs.\n  We first show that VTs achieve SOTA performance in classifying keystrokes\nwhen compared to the previous CNN benchmark. Second, we demonstrate that LLMs\ncan mitigate the impact of real-world noise. Evaluations on the natural\nsentences revealed that: (i) incorporating LLMs (e.g., GPT-4o) in our ASCA\npipeline boosts the performance of error-correction tasks; and (ii) the\ncomparable performance can be attained by a lightweight, fine-tuned smaller LLM\n(67 times smaller than GPT-4o), using..."
    },
    {
        "date": "2025-04",
        "title": "Robust Markov stability for community detection at a scale learned based on the structure",
        "author": "Samin Aref, and Sanchaai Mathiyarasan",
        "link": "http://arxiv.org/abs/2504.11621v1",
        "abstract": "Community detection, the unsupervised task of clustering nodes of a graph,\nfinds applications across various fields. The common approaches for community\ndetection involve optimizing an objective function to partition the nodes into\ncommunities at a single scale of granularity. However, the single-scale\napproaches often fall short of producing partitions that are robust and at a\nsuitable scale. The existing algorithm, PyGenStability, returns multiple robust\npartitions for a network by optimizing the multi-scale Markov stability\nfunction. However, in cases where the suitable scale is not known or assumed by\nthe user, there is no principled method to select a single robust partition at\na suitable scale from the multiple partitions that PyGenStability produces. Our\nproposed method combines the Markov stability framework with a pre-trained\nmachine learning model for scale selection to obtain one robust partition at a\nscale that is learned based on the graph structure. This automatic scale\nselection involves using a gradient boosting model pre-trained on hand-crafted\nand embedding-based network features from a labeled dataset of 10k benchmark\nnetworks. This model was trained to predicts the scale value that maximizes the\nsimilarity of the output partition to the planted partition of the benchmark\nnetwork. Combining our scale selection algorithm with the PyGenStability\nalgorithm results in PyGenStabilityOne (PO): a hyperparameter-free multi-scale\ncommunity detection algorithm that returns one robust partition at a suitable\nscale without the need for any assumptions, input, or tweaking from the user.\nWe compare the performance of PO against 29 algorithms and show that it\noutperforms 25 other algorithms by statistically meaningful margins. Our\nresults facilitate choosing between community detection algorithms, among which\nPO stands out as the accurate, robust, and hyperparameter-free method."
    },
    {
        "date": "2025-04",
        "title": "ADT: Tuning Diffusion Models with Adversarial Supervision",
        "author": "Dazhong Shen, Guanglu Song, Yi Zhang, Bingqi Ma, Lujundong Li, Dongzhi Jiang, Zhuofan Zong, and Yu Liu",
        "link": "http://arxiv.org/abs/2504.11423v1",
        "abstract": "Diffusion models have achieved outstanding image generation by reversing a\nforward noising process to approximate true data distributions. During\ntraining, these models predict diffusion scores from noised versions of true\nsamples in a single forward pass, while inference requires iterative denoising\nstarting from white noise. This training-inference divergences hinder the\nalignment between inference and training data distributions, due to potential\nprediction biases and cumulative error accumulation. To address this problem,\nwe propose an intuitive but effective fine-tuning framework, called Adversarial\nDiffusion Tuning (ADT), by stimulating the inference process during\noptimization and aligning the final outputs with training data by adversarial\nsupervision. Specifically, to achieve robust adversarial training, ADT features\na siamese-network discriminator with a fixed pre-trained backbone and\nlightweight trainable parameters, incorporates an image-to-image sampling\nstrategy to smooth discriminative difficulties, and preserves the original\ndiffusion loss to prevent discriminator hacking. In addition, we carefully\nconstrain the backward-flowing path for back-propagating gradients along the\ninference path without incurring memory overload or gradient explosion.\nFinally, extensive experiments on Stable Diffusion models (v1.5, XL, and v3),\ndemonstrate that ADT significantly improves both distribution alignment and\nimage quality."
    },
    {
        "date": "2025-04",
        "title": "Robustness and sex differences in skin cancer detection: logistic regression vs CNNs",
        "author": "Nikolette Pedersen, Regitze Sydendal, Andreas Wulff, Ralf Raumanns, Eike Petersen, and Veronika Cheplygina",
        "link": "http://arxiv.org/abs/2504.11415v1",
        "abstract": "Deep learning has been reported to achieve high performances in the detection\nof skin cancer, yet many challenges regarding the reproducibility of results\nand biases remain. This study is a replication (different data, same analysis)\nof a study on Alzheimer's disease [28] which studied robustness of logistic\nregression (LR) and convolutional neural networks (CNN) across patient sexes.\nWe explore sex bias in skin cancer detection, using the PAD-UFES-20 dataset\nwith LR trained on handcrafted features reflecting dermatological guidelines\n(ABCDE and the 7-point checklist), and a pre-trained ResNet-50 model. We\nevaluate these models in alignment with [28]: across multiple training datasets\nwith varied sex composition to determine their robustness. Our results show\nthat both the LR and the CNN were robust to the sex distributions, but the\nresults also revealed that the CNN had a significantly higher accuracy (ACC)\nand area under the receiver operating characteristics (AUROC) for male patients\nthan for female patients. We hope these findings to contribute to the growing\nfield of investigating potential bias in popular medical machine learning\nmethods. The data and relevant scripts to reproduce our results can be found in\nour Github."
    },
    {
        "date": "2025-04",
        "title": "DataSentinel: A Game-Theoretic Detection of Prompt Injection Attacks",
        "author": "Yupei Liu, Yuqi Jia, Jinyuan Jia, Dawn Song, and Neil Zhenqiang Gong",
        "link": "http://arxiv.org/abs/2504.11358v1",
        "abstract": "LLM-integrated applications and agents are vulnerable to prompt injection\nattacks, where an attacker injects prompts into their inputs to induce\nattacker-desired outputs. A detection method aims to determine whether a given\ninput is contaminated by an injected prompt. However, existing detection\nmethods have limited effectiveness against state-of-the-art attacks, let alone\nadaptive ones. In this work, we propose DataSentinel, a game-theoretic method\nto detect prompt injection attacks. Specifically, DataSentinel fine-tunes an\nLLM to detect inputs contaminated with injected prompts that are strategically\nadapted to evade detection. We formulate this as a minimax optimization\nproblem, with the objective of fine-tuning the LLM to detect strong adaptive\nattacks. Furthermore, we propose a gradient-based method to solve the minimax\noptimization problem by alternating between the inner max and outer min\nproblems. Our evaluation results on multiple benchmark datasets and LLMs show\nthat DataSentinel effectively detects both existing and adaptive prompt\ninjection attacks."
    },
    {
        "date": "2025-04",
        "title": "X-Teaming: Multi-Turn Jailbreaks and Defenses with Adaptive Multi-Agents",
        "author": "Salman Rahman, Liwei Jiang, James Shiffer, Genglin Liu, Sheriff Issaka, Md Rizwan Parvez, Hamid Palangi, Kai-Wei Chang, Yejin Choi, and Saadia Gabriel",
        "link": "http://arxiv.org/abs/2504.13203v1",
        "abstract": "Multi-turn interactions with language models (LMs) pose critical safety\nrisks, as harmful intent can be strategically spread across exchanges. Yet, the\nvast majority of prior work has focused on single-turn safety, while\nadaptability and diversity remain among the key challenges of multi-turn\nred-teaming. To address these challenges, we present X-Teaming, a scalable\nframework that systematically explores how seemingly harmless interactions\nescalate into harmful outcomes and generates corresponding attack scenarios.\nX-Teaming employs collaborative agents for planning, attack optimization, and\nverification, achieving state-of-the-art multi-turn jailbreak effectiveness and\ndiversity with success rates up to 98.1% across representative leading\nopen-weight and closed-source models. In particular, X-Teaming achieves a 96.2%\nattack success rate against the latest Claude 3.7 Sonnet model, which has been\nconsidered nearly immune to single-turn attacks. Building on X-Teaming, we\nintroduce XGuard-Train, an open-source multi-turn safety training dataset that\nis 20x larger than the previous best resource, comprising 30K interactive\njailbreaks, designed to enable robust multi-turn safety alignment for LMs. Our\nwork offers essential tools and insights for mitigating sophisticated\nconversational attacks, advancing the multi-turn safety of LMs."
    },
    {
        "date": "2025-04",
        "title": "R-TPT: Improving Adversarial Robustness of Vision-Language Models through Test-Time Prompt Tuning",
        "author": "Lijun Sheng, Jian Liang, Zilei Wang, and Ran He",
        "link": "http://arxiv.org/abs/2504.11195v1",
        "abstract": "Vision-language models (VLMs), such as CLIP, have gained significant\npopularity as foundation models, with numerous fine-tuning methods developed to\nenhance performance on downstream tasks. However, due to their inherent\nvulnerability and the common practice of selecting from a limited set of\nopen-source models, VLMs suffer from a higher risk of adversarial attacks than\ntraditional vision models. Existing defense techniques typically rely on\nadversarial fine-tuning during training, which requires labeled data and lacks\nof flexibility for downstream tasks. To address these limitations, we propose\nrobust test-time prompt tuning (R-TPT), which mitigates the impact of\nadversarial attacks during the inference stage. We first reformulate the\nclassic marginal entropy objective by eliminating the term that introduces\nconflicts under adversarial conditions, retaining only the pointwise entropy\nminimization. Furthermore, we introduce a plug-and-play reliability-based\nweighted ensembling strategy, which aggregates useful information from reliable\naugmented views to strengthen the defense. R-TPT enhances defense against\nadversarial attacks without requiring labeled training data while offering high\nflexibility for inference tasks. Extensive experiments on widely used\nbenchmarks with various attacks demonstrate the effectiveness of R-TPT. The\ncode is available in https://github.com/TomSheng21/R-TPT."
    },
    {
        "date": "2025-04",
        "title": "Exploring Backdoor Attack and Defense for LLM-empowered Recommendations",
        "author": "Liangbo Ning, Wenqi Fan, and Qing Li",
        "link": "http://arxiv.org/abs/2504.11182v1",
        "abstract": "The fusion of Large Language Models (LLMs) with recommender systems (RecSys)\nhas dramatically advanced personalized recommendations and drawn extensive\nattention. Despite the impressive progress, the safety of LLM-based RecSys\nagainst backdoor attacks remains largely under-explored. In this paper, we\nraise a new problem: Can a backdoor with a specific trigger be injected into\nLLM-based Recsys, leading to the manipulation of the recommendation responses\nwhen the backdoor trigger is appended to an item's title? To investigate the\nvulnerabilities of LLM-based RecSys under backdoor attacks, we propose a new\nattack framework termed Backdoor Injection Poisoning for RecSys (BadRec).\nBadRec perturbs the items' titles with triggers and employs several fake users\nto interact with these items, effectively poisoning the training set and\ninjecting backdoors into LLM-based RecSys. Comprehensive experiments reveal\nthat poisoning just 1% of the training data with adversarial examples is\nsufficient to successfully implant backdoors, enabling manipulation of\nrecommendations. To further mitigate such a security threat, we propose a\nuniversal defense strategy called Poison Scanner (P-Scanner). Specifically, we\nintroduce an LLM-based poison scanner to detect the poisoned items by\nleveraging the powerful language understanding and rich knowledge of LLMs. A\ntrigger augmentation agent is employed to generate diverse synthetic triggers\nto guide the poison scanner in learning domain-specific knowledge of the\npoisoned item detection task. Extensive experiments on three real-world\ndatasets validate the effectiveness of the proposed P-Scanner."
    },
    {
        "date": "2025-04",
        "title": "KubeFence: Security Hardening of the Kubernetes Attack Surface",
        "author": "Carmine Cesarano, and Roberto Natella",
        "link": "http://arxiv.org/abs/2504.11126v1",
        "abstract": "Kubernetes (K8s) is widely used to orchestrate containerized applications,\nincluding critical services in domains such as finance, healthcare, and\ngovernment. However, its extensive and feature-rich API interface exposes a\nbroad attack surface, making K8s vulnerable to exploits of software\nvulnerabilities and misconfigurations. Even if K8s adopts role-based access\ncontrol (RBAC) to manage access to K8s APIs, this approach lacks the\ngranularity needed to protect specification attributes within API requests.\nThis paper proposes a novel solution, KubeFence, which implements finer-grain\nAPI filtering tailored to specific client workloads. KubeFence analyzes\nKubernetes Operators from trusted repositories and leverages their\nconfiguration files to restrict unnecessary features of the K8s API, to\nmitigate misconfigurations and vulnerabilities exploitable through the K8s API.\nThe experimental results show that KubeFence can significantly reduce the\nattack surface and prevent attacks compared to RBAC."
    },
    {
        "date": "2025-04",
        "title": "FLSSM: A Federated Learning Storage Security Model with Homomorphic Encryption",
        "author": "Yang Li, Chunhe Xia, Chang Li, Xiaojian Li, and Tianbo Wang",
        "link": "http://arxiv.org/abs/2504.11088v1",
        "abstract": "Federated learning based on homomorphic encryption has received widespread\nattention due to its high security and enhanced protection of user data\nprivacy. However, the characteristics of encrypted computation lead to three\nchallenging problems: ``computation-efficiency\", ``attack-tracing\" and\n``contribution-assessment\". The first refers to the efficiency of encrypted\ncomputation during model aggregation, the second refers to tracing malicious\nattacks in an encrypted state, and the third refers to the fairness of\ncontribution assessment for local models after encryption. This paper proposes\na federated learning storage security model with homomorphic encryption (FLSSM)\nto protect federated learning model privacy and address the three issues\nmentioned above. First, we utilize different nodes to aggregate local models in\nparallel, thereby improving encrypted models' aggregation efficiency. Second,\nwe introduce trusted supervise nodes to examine local models when the global\nmodel is attacked, enabling the tracing of malicious attacks under homomorphic\nencryption. Finally, we fairly reward local training nodes with encrypted local\nmodels based on trusted training time. Experiments on multiple real-world\ndatasets show that our model significantly outperforms baseline models in terms\nof both efficiency and security metrics."
    },
    {
        "date": "2025-04",
        "title": "Improving fingerprint presentation attack detection by an approach integrated into the personal verification stage",
        "author": "Marco Micheletto, Giulia Orr\u00f9, Luca Ghiani, and Gian Luca Marcialis",
        "link": "http://arxiv.org/abs/2504.11066v1",
        "abstract": "Presentation Attack Detection (PAD) systems are usually designed\nindependently of the fingerprint verification system. While this can be\nacceptable for use cases where specific user templates are not predetermined,\nit represents a missed opportunity to enhance security in scenarios where\nintegrating PAD with the fingerprint verification system could significantly\nleverage users' templates, which are the real target of a potential\npresentation attack. This does not mean that a PAD should be specifically\ndesigned for such users; that would imply the availability of many enrolled\nusers' PAI and, consequently, complexity, time, and cost increase. On the\ncontrary, we propose to equip a basic PAD, designed according to the state of\nthe art, with an innovative add-on module called the Closeness Binary Code (CC)\nmodule. The term \"closeness\" refers to a peculiar property of the bona\nfide-related features: in an Euclidean feature space, genuine fingerprints tend\nto cluster in a specific pattern. First, samples from the same finger are close\nto each other, then samples from other fingers of the same user and finally,\nsamples from fingers of other users. This property is statistically verified in\nour previous publication, and further confirmed in this paper. It is\nindependent of the user population and the feature set class, which can be\nhandcrafted or deep network-based (embeddings). Therefore, the add-on can be\ndesigned without the need for the targeted user samples; moreover, it exploits\nher/his samples' \"closeness\" property during the verification stage. Extensive\nexperiments on benchmark datasets and state-of-the-art PAD methods confirm the\nbenefits of the proposed add-on, which can be easily coupled with the main PAD\nmodule integrated into the fingerprint verification system."
    },
    {
        "date": "2025-04",
        "title": "RAID: An In-Training Defense against Attribute Inference Attacks in Recommender Systems",
        "author": "Xiaohua Feng, Yuyuan Li, Fengyuan Yu, Ke Xiong, Junjie Fang, Li Zhang, Tianyu Du, and Chaochao Chen",
        "link": "http://arxiv.org/abs/2504.11510v1",
        "abstract": "In various networks and mobile applications, users are highly susceptible to\nattribute inference attacks, with particularly prevalent occurrences in\nrecommender systems. Attackers exploit partially exposed user profiles in\nrecommendation models, such as user embeddings, to infer private attributes of\ntarget users, such as gender and political views. The goal of defenders is to\nmitigate the effectiveness of these attacks while maintaining recommendation\nperformance. Most existing defense methods, such as differential privacy and\nattribute unlearning, focus on post-training settings, which limits their\ncapability of utilizing training data to preserve recommendation performance.\nAlthough adversarial training extends defenses to in-training settings, it\noften struggles with convergence due to unstable training processes. In this\npaper, we propose RAID, an in-training defense method against attribute\ninference attacks in recommender systems. In addition to the recommendation\nobjective, we define a defensive objective to ensure that the distribution of\nprotected attributes becomes independent of class labels, making users\nindistinguishable from attribute inference attacks. Specifically, this\ndefensive objective aims to solve a constrained Wasserstein barycenter problem\nto identify the centroid distribution that makes the attribute\nindistinguishable while complying with recommendation performance constraints.\nTo optimize our proposed objective, we use optimal transport to align users\nwith the centroid distribution. We conduct extensive experiments on four\nreal-world datasets to evaluate RAID. The experimental results validate the\neffectiveness of RAID and demonstrate its significant superiority over existing\nmethods in multiple aspects."
    },
    {
        "date": "2025-04",
        "title": "QAVA: Query-Agnostic Visual Attack to Large Vision-Language Models",
        "author": "Yudong Zhang, Ruobing Xie, Jiansheng Chen, Xingwu Sun, Zhanhui Kang, and Yu Wang",
        "link": "http://arxiv.org/abs/2504.11038v1",
        "abstract": "In typical multimodal tasks, such as Visual Question Answering (VQA),\nadversarial attacks targeting a specific image and question can lead large\nvision-language models (LVLMs) to provide incorrect answers. However, it is\ncommon for a single image to be associated with multiple questions, and LVLMs\nmay still answer other questions correctly even for an adversarial image\nattacked by a specific question. To address this, we introduce the\nquery-agnostic visual attack (QAVA), which aims to create robust adversarial\nexamples that generate incorrect responses to unspecified and unknown\nquestions. Compared to traditional adversarial attacks focused on specific\nimages and questions, QAVA significantly enhances the effectiveness and\nefficiency of attacks on images when the question is unknown, achieving\nperformance comparable to attacks on known target questions. Our research\nbroadens the scope of visual adversarial attacks on LVLMs in practical\nsettings, uncovering previously overlooked vulnerabilities, particularly in the\ncontext of visual adversarial threats. The code is available at\nhttps://github.com/btzyd/qava."
    },
    {
        "date": "2025-04",
        "title": "Defending Against Frequency-Based Attacks with Diffusion Models",
        "author": "Fatemeh Amerehi, and Patrick Healy",
        "link": "http://arxiv.org/abs/2504.11034v1",
        "abstract": "Adversarial training is a common strategy for enhancing model robustness\nagainst adversarial attacks. However, it is typically tailored to the specific\nattack types it is trained on, limiting its ability to generalize to unseen\nthreat models. Adversarial purification offers an alternative by leveraging a\ngenerative model to remove perturbations before classification. Since the\npurifier is trained independently of both the classifier and the threat models,\nit is better equipped to handle previously unseen attack scenarios. Diffusion\nmodels have proven highly effective for noise purification, not only in\ncountering pixel-wise adversarial perturbations but also in addressing\nnon-adversarial data shifts. In this study, we broaden the focus beyond\npixel-wise robustness to explore the extent to which purification can mitigate\nboth spectral and spatial adversarial attacks. Our findings highlight its\neffectiveness in handling diverse distortion patterns across low- to\nhigh-frequency regions."
    },
    {
        "date": "2025-04",
        "title": "CDUPatch: Color-Driven Universal Adversarial Patch Attack for Dual-Modal Visible-Infrared Detectors",
        "author": "Jiahuan Long, Wen Yao, Tingsong Jiang, and Chao Ma",
        "link": "http://arxiv.org/abs/2504.10888v1",
        "abstract": "Adversarial patches are widely used to evaluate the robustness of object\ndetection systems in real-world scenarios. These patches were initially\ndesigned to deceive single-modal detectors (e.g., visible or infrared) and have\nrecently been extended to target visible-infrared dual-modal detectors.\nHowever, existing dual-modal adversarial patch attacks have limited attack\neffectiveness across diverse physical scenarios. To address this, we propose\nCDUPatch, a universal cross-modal patch attack against visible-infrared object\ndetectors across scales, views, and scenarios. Specifically, we observe that\ncolor variations lead to different levels of thermal absorption, resulting in\ntemperature differences in infrared imaging. Leveraging this property, we\npropose an RGB-to-infrared adapter that maps RGB patches to infrared patches,\nenabling unified optimization of cross-modal patches. By learning an optimal\ncolor distribution on the adversarial patch, we can manipulate its thermal\nresponse and generate an adversarial infrared texture. Additionally, we\nintroduce a multi-scale clipping strategy and construct a new visible-infrared\ndataset, MSDrone, which contains aerial vehicle images in varying scales and\nperspectives. These data augmentation strategies enhance the robustness of our\npatch in real-world conditions. Experiments on four benchmark datasets (e.g.,\nDroneVehicle, LLVIP, VisDrone, MSDrone) show that our method outperforms\nexisting patch attacks in the digital domain. Extensive physical tests further\nconfirm strong transferability across scales, views, and scenarios."
    },
    {
        "date": "2025-04",
        "title": "DAAF:Degradation-Aware Adaptive Fusion Framework for Robust Infrared and Visible Images Fusion",
        "author": "Tianpei Zhang, Jufeng Zhao, Yiming Zhu, Guangmang Cui, Yuxin Jing, and Yuhan Lyu",
        "link": "http://arxiv.org/abs/2504.10871v1",
        "abstract": "Existing infrared and visible image fusion(IVIF) algorithms often prioritize\nhigh-quality images, neglecting image degradation such as low light and noise,\nwhich limits the practical potential. This paper propose Degradation-Aware\nAdaptive image Fusion (DAAF), which achieves unified modeling of adaptive\ndegradation optimization and image fusion. Specifically, DAAF comprises an\nauxiliary Adaptive Degradation Optimization Network (ADON) and a Feature\nInteractive Local-Global Fusion (FILGF) Network. Firstly, ADON includes\ninfrared and visible-light branches. Within the infrared branch,\nfrequency-domain feature decomposition and extraction are employed to isolate\nGaussian and stripe noise. In the visible-light branch, Retinex decomposition\nis applied to extract illumination and reflectance components, enabling\ncomplementary enhancement of detail and illumination distribution.\nSubsequently, FILGF performs interactive multi-scale local-global feature\nfusion. Local feature fusion consists of intra-inter model feature complement,\nwhile global feature fusion is achieved through a interactive cross-model\nattention. Extensive experiments have shown that DAAF outperforms current IVIF\nalgorithms in normal and complex degradation scenarios."
    },
    {
        "date": "2025-04",
        "title": "How to Enhance Downstream Adversarial Robustness (almost) without Touching the Pre-Trained Foundation Model?",
        "author": "Meiqi Liu, Zhuoqun Huang, and Yue Xing",
        "link": "http://arxiv.org/abs/2504.10850v1",
        "abstract": "With the rise of powerful foundation models, a pre-training-fine-tuning\nparadigm becomes increasingly popular these days: A foundation model is\npre-trained using a huge amount of data from various sources, and then the\ndownstream users only need to fine-tune and adapt it to specific downstream\ntasks. However, due to the high computation complexity of adversarial training,\nit is not feasible to fine-tune the foundation model to improve its robustness\non the downstream task. Observing the above challenge, we want to improve the\ndownstream robustness without updating/accessing the weights in the foundation\nmodel. Inspired from existing literature in robustness inheritance (Kim et al.,\n2020), through theoretical investigation, we identify a close relationship\nbetween robust contrastive learning with the adversarial robustness of\nsupervised learning. To further validate and utilize this theoretical insight,\nwe design a simple-yet-effective robust auto-encoder as a data pre-processing\nmethod before feeding the data into the foundation model. The proposed approach\nhas zero access to the foundation model when training the robust auto-encoder.\nExtensive experiments demonstrate the effectiveness of the proposed method in\nimproving the robustness of downstream tasks, verifying the connection between\nthe feature robustness (implied by small adversarial contrastive loss) and the\nrobustness of the downstream task."
    },
    {
        "date": "2025-04",
        "title": "Concept Enhancement Engineering: A Lightweight and Efficient Robust Defense Against Jailbreak Attacks in Embodied AI",
        "author": "Jirui Yang, Zheyu Lin, Shuhan Yang, Zhihui Lu, and Xin Du",
        "link": "http://arxiv.org/abs/2504.13201v1",
        "abstract": "Embodied Intelligence (EI) systems integrated with large language models\n(LLMs) face significant security risks, particularly from jailbreak attacks\nthat manipulate models into generating harmful outputs or executing unsafe\nphysical actions. Traditional defense strategies, such as input filtering and\noutput monitoring, often introduce high computational overhead or interfere\nwith task performance in real-time embodied scenarios. To address these\nchallenges, we propose Concept Enhancement Engineering (CEE), a novel defense\nframework that leverages representation engineering to enhance the safety of\nembodied LLMs by dynamically steering their internal activations. CEE operates\nby (1) extracting multilingual safety patterns from model activations, (2)\nconstructing control directions based on safety-aligned concept subspaces, and\n(3) applying subspace concept rotation to reinforce safe behavior during\ninference. Our experiments demonstrate that CEE effectively mitigates jailbreak\nattacks while maintaining task performance, outperforming existing defense\nmethods in both robustness and efficiency. This work contributes a scalable and\ninterpretable safety mechanism for embodied AI, bridging the gap between\ntheoretical representation engineering and practical security applications. Our\nfindings highlight the potential of latent-space interventions as a viable\ndefense paradigm against emerging adversarial threats in physically grounded AI\nsystems."
    },
    {
        "date": "2025-04",
        "title": "Efficient and Robust Remote Sensing Image Denoising Using Randomized Approximation of Geodesics' Gramian on the Manifold Underlying the Patch Space",
        "author": "Kelum Gajamannage, Dilhani I. Jayathilake, and Maria Vasilyeva",
        "link": "http://arxiv.org/abs/2504.10820v1",
        "abstract": "Remote sensing images are widely utilized in many disciplines such as feature\nrecognition and scene semantic segmentation. However, due to environmental\nfactors and the issues of the imaging system, the image quality is often\ndegraded which may impair subsequent visual tasks. Even though denoising remote\nsensing images plays an essential role before applications, the current\ndenoising algorithms fail to attain optimum performance since these images\npossess complex features in the texture. Denoising frameworks based on\nartificial neural networks have shown better performance; however, they require\nexhaustive training with heterogeneous samples that extensively consume\nresources like power, memory, computation, and latency. Thus, here we present a\ncomputationally efficient and robust remote sensing image denoising method that\ndoesn't require additional training samples. This method partitions patches of\na remote-sensing image in which a low-rank manifold, representing the\nnoise-free version of the image, underlies the patch space. An efficient and\nrobust approach to revealing this manifold is a randomized approximation of the\nsingular value spectrum of the geodesics' Gramian matrix of the patch space.\nThe method asserts a unique emphasis on each color channel during denoising so\nthe three denoised channels are merged to produce the final image."
    },
    {
        "date": "2025-04",
        "title": "The Sword of Damocles in ViTs: Computational Redundancy Amplifies Adversarial Transferability",
        "author": "Jiani Liu, Zhiyuan Wang, Zeliang Zhang, Chao Huang, Susan Liang, Yunlong Tang, and Chenliang Xu",
        "link": "http://arxiv.org/abs/2504.10804v1",
        "abstract": "Vision Transformers (ViTs) have demonstrated impressive performance across a\nrange of applications, including many safety-critical tasks. However, their\nunique architectural properties raise new challenges and opportunities in\nadversarial robustness. In particular, we observe that adversarial examples\ncrafted on ViTs exhibit higher transferability compared to those crafted on\nCNNs, suggesting that ViTs contain structural characteristics favorable for\ntransferable attacks. In this work, we investigate the role of computational\nredundancy in ViTs and its impact on adversarial transferability. Unlike prior\nstudies that aim to reduce computation for efficiency, we propose to exploit\nthis redundancy to improve the quality and transferability of adversarial\nexamples. Through a detailed analysis, we identify two forms of redundancy,\nincluding the data-level and model-level, that can be harnessed to amplify\nattack effectiveness. Building on this insight, we design a suite of\ntechniques, including attention sparsity manipulation, attention head\npermutation, clean token regularization, ghost MoE diversification, and\ntest-time adversarial training. Extensive experiments on the ImageNet-1k\ndataset validate the effectiveness of our approach, showing that our methods\nsignificantly outperform existing baselines in both transferability and\ngenerality across diverse model architectures."
    },
    {
        "date": "2025-04",
        "title": "Wasserstein Distributionally Robust Regret Optimization",
        "author": "Lukas-Benedikt Fiechtner, and Jose Blanchet",
        "link": "http://arxiv.org/abs/2504.10796v3",
        "abstract": "Distributionally Robust Optimization (DRO) is a popular framework for\ndecision-making under uncertainty, but its adversarial nature can lead to\noverly conservative solutions. To address this, we study ex-ante\nDistributionally Robust Regret Optimization (DRRO), focusing on\nWasserstein-based ambiguity sets which are popular due to their links to\nregularization and machine learning. We provide a systematic analysis of\nWasserstein DRRO, paralleling known results for Wasserstein DRO. Under\nsmoothness and regularity conditions, we show that Wasserstein DRRO coincides\nwith Empirical Risk Minimization (ERM) up to first-order terms, and exactly so\nin convex quadratic settings. We revisit the Wasserstein DRRO newsvendor\nproblem, where the loss is the maximum of two linear functions of demand and\ndecision. Extending [25], we show that the regret can be computed by maximizing\ntwo one-dimensional concave functions. For more general loss functions\ninvolving the maximum of multiple linear terms in multivariate random variables\nand decision vectors, we prove that computing the regret and thus also the DRRO\npolicy is NP-hard. We then propose a convex relaxation for these more general\nWasserstein DRRO problems and demonstrate its strong empirical performance.\nFinally, we provide an upper bound on the optimality gap of our relaxation and\nshow it improves over recent alternatives."
    },
    {
        "date": "2025-04",
        "title": "Domain-Adversarial Neural Network and Explainable AI for Reducing Tissue-of-Origin Signal in Pan-cancer Mortality Classification",
        "author": "Cristian Padron-Manrique, Juan Jos\u00e9 Oropeza Valdez, and Osbaldo Resendis-Antonio",
        "link": "http://arxiv.org/abs/2504.10343v1",
        "abstract": "Tissue-of-origin signals dominate pan-cancer gene expression, often obscuring\nmolecular features linked to patient survival. This hampers the discovery of\ngeneralizable biomarkers, as models tend to overfit tissue-specific patterns\nrather than capture survival-relevant signals. To address this, we propose a\nDomain-Adversarial Neural Network (DANN) trained on TCGA RNA-seq data to learn\nrepresentations less biased by tissue and more focused on survival. Identifying\ntissue-independent genetic profiles is key to revealing core cancer programs.\nWe assess the DANN using: (1) Standard SHAP, based on the original input space\nand DANN's mortality classifier; (2) A layer-aware strategy applied to hidden\nactivations, including an unsupervised manifold from raw activations and a\nsupervised manifold from mortality-specific SHAP values. Standard SHAP remains\nconfounded by tissue signals due to biases inherent in its computation. The raw\nactivation manifold was dominated by high-magnitude activations, which masked\nsubtle tissue and mortality-related signals. In contrast, the layer-aware SHAP\nmanifold offers improved low-dimensional representations of both tissue and\nmortality signals, independent of activation strength, enabling subpopulation\nstratification and pan-cancer identification of survival-associated genes."
    },
    {
        "date": "2025-04",
        "title": "Shield Bash: Abusing Defensive Coherence State Retrieval to Break Timing Obfuscation",
        "author": "Kartik Ramkrishnan, Antonia Zhai, Stephen McCamant, and Pen Chung Yew",
        "link": "http://arxiv.org/abs/2504.10318v1",
        "abstract": "Microarchitectural attacks are a significant concern, leading to many\nhardware-based defense proposals. However, different defenses target different\nclasses of attacks, and their impact on each other has not been fully\nconsidered. To raise awareness of this problem, we study an interaction between\ntwo state-of-the art defenses in this paper, timing obfuscations of remote\ncache lines (TORC) and delaying speculative changes to remote cache lines\n(DSRC). TORC mitigates cache-hit based attacks and DSRC mitigates speculative\ncoherence state change attacks.\n  We observe that DSRC enables coherence information to be retrieved into the\nprocessor core, where it is out of the reach of timing obfuscations to protect.\nThis creates an unforeseen consequence that redo operations can be triggered\nwithin the core to detect the presence or absence of remote cache lines, which\nconstitutes a security vulnerability. We demonstrate that a new covert channel\nattack is possible using this vulnerability. We propose two ways to mitigate\nthe attack, whose performance varies depending on an application's cache usage.\nOne way is to never send remote exclusive coherence state (E) information to\nthe core even if it is created. The other way is to never create a remote E\nstate, which is responsible for triggering redos.\n  We demonstrate the timing difference caused by this microarchitectural\ndefense assumption violation using GEM5 simulations. Performance evaluation on\nSPECrate 2017 and PARSEC benchmarks of the two fixes show less than 32\\%\naverage overhead across both sets of benchmarks. The repair which prevented the\ncreation of remote E state had less than 2.8% average overhead."
    },
    {
        "date": "2025-04",
        "title": "ROSFD: Robust Online Streaming Fraud Detection with Resilience to Concept Drift in Data Streams",
        "author": "Vivek Yelleti",
        "link": "http://arxiv.org/abs/2504.10229v1",
        "abstract": "Continuous generation of streaming data from diverse sources, such as online\ntransactions and digital interactions, necessitates timely fraud detection.\nTraditional batch processing methods often struggle to capture the rapidly\nevolving patterns of fraudulent activities. This paper highlights the critical\nimportance of processing streaming data for effective fraud detection. To\naddress the inherent challenges of latency, scalability, and concept drift in\nstreaming environments, we propose a robust online streaming fraud detection\n(ROSFD) framework. Our proposed framework comprises two key stages: (i) Stage\nOne: Offline Model Initialization. In this initial stage, a model is built in\noffline settings using incremental learning principles to overcome the\n\"cold-start\" problem. (ii) Stage Two: Real-time Model Adaptation. In this\ndynamic stage, drift detection algorithms (viz.,, DDM, EDDM, and ADWIN) are\nemployed to identify concept drift in the incoming data stream and\nincrementally train the model accordingly. This \"train-only-when-required\"\nstrategy drastically reduces the number of retrains needed without\nsignificantly impacting the area under the receiver operating characteristic\ncurve (AUC). Overall, ROSFD utilizing ADWIN as the drift detection method\ndemonstrated the best performance among the employed methods. In terms of model\nefficacy, Adaptive Random Forest consistently outperformed other models,\nachieving the highest AUC in four out of five datasets."
    },
    {
        "date": "2025-04",
        "title": "The Future of MLLM Prompting is Adaptive: A Comprehensive Experimental Evaluation of Prompt Engineering Methods for Robust Multimodal Performance",
        "author": "Anwesha Mohanty, Venkatesh Balavadhani Parthasarathy, and Arsalan Shahid",
        "link": "http://arxiv.org/abs/2504.10179v1",
        "abstract": "Multimodal Large Language Models (MLLMs) are set to transform how machines\nprocess and generate human-like responses by integrating diverse modalities\nsuch as text, images, and code. Yet, effectively harnessing their capabilities\nhinges on optimal prompt engineering. We present a comprehensive experimental\nevaluation of seven prompt engineering methods applied to 13 open-source MLLMs\nover 24 tasks spanning Reasoning and Compositionality, Multimodal Understanding\nand Alignment, Complex Code Generation and Execution, and Knowledge Retrieval\nand Integration. Our approach stratifies models by parameter count into Small\n(<4B), Medium (4B-10B), and Large (>10B) categories and compares prompting\ntechniques including Zero-Shot, One-Shot, Few-Shot, Chain-of-Thought,\nAnalogical, Generated Knowledge, and Tree-of-Thought. While Large MLLMs excel\nin structured tasks such as code generation, achieving accuracies up to 96.88%\nunder Few-Shot prompting, all models struggle with complex reasoning and\nabstract understanding, often yielding accuracies below 60% and high\nhallucination rates. Structured reasoning prompts frequently increased\nhallucination up to 75% in small models and led to longer response times (over\n20 seconds in Large MLLMs), while simpler prompting methods provided more\nconcise and efficient outputs. No single prompting method uniformly optimises\nall task types. Instead, adaptive strategies combining example-based guidance\nwith selective structured reasoning are essential to enhance robustness,\nefficiency, and factual accuracy. Our findings offer practical recommendations\nfor prompt engineering and support more reliable deployment of MLLMs across\napplications including AI-assisted coding, knowledge retrieval, and multimodal\ncontent understanding."
    },
    {
        "date": "2025-04",
        "title": "Benchmarking Practices in LLM-driven Offensive Security: Testbeds, Metrics, and Experiment Design",
        "author": "Andreas Happe, and J\u00fcrgen Cito",
        "link": "http://arxiv.org/abs/2504.10112v1",
        "abstract": "Large Language Models (LLMs) have emerged as a powerful approach for driving\noffensive penetration-testing tooling. This paper analyzes the methodology and\nbenchmarking practices used for evaluating Large Language Model (LLM)-driven\nattacks, focusing on offensive uses of LLMs in cybersecurity. We review 16\nresearch papers detailing 15 prototypes and their respective testbeds.\n  We detail our findings and provide actionable recommendations for future\nresearch, emphasizing the importance of extending existing testbeds, creating\nbaselines, and including comprehensive metrics and qualitative analysis. We\nalso note the distinction between security research and practice, suggesting\nthat CTF-based challenges may not fully represent real-world penetration\ntesting scenarios."
    },
    {
        "date": "2025-04",
        "title": "Dual-Path Enhancements in Event-Based Eye Tracking: Augmented Robustness and Adaptive Temporal Modeling",
        "author": "Hoang M. Truong, Vinh-Thuan Ly, Huy G. Tran, Thuan-Phat Nguyen, and Tram T. Doan",
        "link": "http://arxiv.org/abs/2504.09960v1",
        "abstract": "Event-based eye tracking has become a pivotal technology for augmented\nreality and human-computer interaction. Yet, existing methods struggle with\nreal-world challenges such as abrupt eye movements and environmental noise.\nBuilding on the efficiency of the Lightweight Spatiotemporal Network-a causal\narchitecture optimized for edge devices-we introduce two key advancements.\nFirst, a robust data augmentation pipeline incorporating temporal shift,\nspatial flip, and event deletion improves model resilience, reducing Euclidean\ndistance error by 12% (1.61 vs. 1.70 baseline) on challenging samples. Second,\nwe propose KnightPupil, a hybrid architecture combining an EfficientNet-B3\nbackbone for spatial feature extraction, a bidirectional GRU for contextual\ntemporal modeling, and a Linear Time-Varying State-Space Module to adapt to\nsparse inputs and noise dynamically. Evaluated on the 3ET+ benchmark, our\nframework achieved 1.61 Euclidean distance on the private test set of the\nEvent-based Eye Tracking Challenge at CVPR 2025, demonstrating its\neffectiveness for practical deployment in AR/VR systems while providing a\nfoundation for future innovations in neuromorphic vision."
    },
    {
        "date": "2025-04",
        "title": "LangPert: Detecting and Handling Task-level Perturbations for Robust Object Rearrangement",
        "author": "Xu Yin, Min-Sung Yoon, Yuchi Huo, Kang Zhang, and Sung-Eui Yoon",
        "link": "http://arxiv.org/abs/2504.09893v1",
        "abstract": "Task execution for object rearrangement could be challenged by Task-Level\nPerturbations (TLP), i.e., unexpected object additions, removals, and\ndisplacements that can disrupt underlying visual policies and fundamentally\ncompromise task feasibility and progress. To address these challenges, we\npresent LangPert, a language-based framework designed to detect and mitigate\nTLP situations in tabletop rearrangement tasks. LangPert integrates a Visual\nLanguage Model (VLM) to comprehensively monitor policy's skill execution and\nenvironmental TLP, while leveraging the Hierarchical Chain-of-Thought (HCoT)\nreasoning mechanism to enhance the Large Language Model (LLM)'s contextual\nunderstanding and generate adaptive, corrective skill-execution plans. Our\nexperimental results demonstrate that LangPert handles diverse TLP situations\nmore effectively than baseline methods, achieving higher task completion rates,\nimproved execution efficiency, and potential generalization to unseen\nscenarios."
    },
    {
        "date": "2025-04",
        "title": "Revisiting the attacker's knowledge in inference attacks against Searchable Symmetric Encryption",
        "author": "Marc Damie, Jean-Benoist Leger, Florian Hahn, and Andreas Peter",
        "link": "http://arxiv.org/abs/2504.09879v1",
        "abstract": "Encrypted search schemes have been proposed to address growing privacy\nconcerns. However, several leakage-abuse attacks have highlighted some security\nvulnerabilities. Recent attacks assumed an attacker's knowledge containing data\n``similar'' to the indexed data. However, this vague assumption is barely\ndiscussed in literature: how likely is it for an attacker to obtain a \"similar\nenough\" data?\n  Our paper provides novel statistical tools usable on any attack in this\nsetting to analyze its sensitivity to data similarity. First, we introduce a\nmathematical model based on statistical estimators to analytically understand\nthe attackers' knowledge and the notion of similarity. Second, we conceive\nstatistical tools to model the influence of the similarity on the attack\naccuracy. We apply our tools on three existing attacks to answer questions such\nas: is similarity the only factor influencing accuracy of a given attack?\nThird, we show that the enforcement of a maximum index size can make the\n``similar-data'' assumption harder to satisfy. In particular, we propose a\nstatistical method to estimate an appropriate maximum size for a given attack\nand dataset. For the best known attack on the Enron dataset, a maximum index\nsize of 200 guarantees (with high probability) the attack accuracy to be below\n5%."
    },
    {
        "date": "2025-04",
        "title": "StruPhantom: Evolutionary Injection Attacks on Black-Box Tabular Agents Powered by Large Language Models",
        "author": "Yang Feng, and Xudong Pan",
        "link": "http://arxiv.org/abs/2504.09841v1",
        "abstract": "The proliferation of autonomous agents powered by large language models\n(LLMs) has revolutionized popular business applications dealing with tabular\ndata, i.e., tabular agents. Although LLMs are observed to be vulnerable against\nprompt injection attacks from external data sources, tabular agents impose\nstrict data formats and predefined rules on the attacker's payload, which are\nineffective unless the agent navigates multiple layers of structural data to\nincorporate the payload. To address the challenge, we present a novel attack\ntermed StruPhantom which specifically targets black-box LLM-powered tabular\nagents. Our attack designs an evolutionary optimization procedure which\ncontinually refines attack payloads via the proposed constrained Monte Carlo\nTree Search augmented by an off-topic evaluator. StruPhantom helps\nsystematically explore and exploit the weaknesses of target applications to\nachieve goal hijacking. Our evaluation validates the effectiveness of\nStruPhantom across various LLM-based agents, including those on real-world\nplatforms, and attack scenarios. Our attack achieves over 50% higher success\nrates than baselines in enforcing the application's response to contain\nphishing links or malicious codes."
    },
    {
        "date": "2025-04",
        "title": "SafeSpeech: Robust and Universal Voice Protection Against Malicious Speech Synthesis",
        "author": "Zhisheng Zhang, Derui Wang, Qianyi Yang, Pengyang Huang, Junhan Pu, Yuxin Cao, Kai Ye, Jie Hao, and Yixian Yang",
        "link": "http://arxiv.org/abs/2504.09839v1",
        "abstract": "Speech synthesis technology has brought great convenience, while the\nwidespread usage of realistic deepfake audio has triggered hazards. Malicious\nadversaries may unauthorizedly collect victims' speeches and clone a similar\nvoice for illegal exploitation (\\textit{e.g.}, telecom fraud). However, the\nexisting defense methods cannot effectively prevent deepfake exploitation and\nare vulnerable to robust training techniques. Therefore, a more effective and\nrobust data protection method is urgently needed. In response, we propose a\ndefensive framework, \\textit{\\textbf{SafeSpeech}}, which protects the users'\naudio before uploading by embedding imperceptible perturbations on original\nspeeches to prevent high-quality synthetic speech. In SafeSpeech, we devise a\nrobust and universal proactive protection technique, \\textbf{S}peech\n\\textbf{PE}rturbative \\textbf{C}oncealment (\\textbf{SPEC}), that leverages a\nsurrogate model to generate universally applicable perturbation for generative\nsynthetic models. Moreover, we optimize the human perception of embedded\nperturbation in terms of time and frequency domains. To evaluate our method\ncomprehensively, we conduct extensive experiments across advanced models and\ndatasets, both subjectively and objectively. Our experimental results\ndemonstrate that SafeSpeech achieves state-of-the-art (SOTA) voice protection\neffectiveness and transferability and is highly robust against advanced\nadaptive adversaries. Moreover, SafeSpeech has real-time capability in\nreal-world tests. The source code is available at\n\\href{https://github.com/wxzyd123/SafeSpeech}{https://github.com/wxzyd123/SafeSpeech}."
    },
    {
        "date": "2025-04",
        "title": "Socratic Chart: Cooperating Multiple Agents for Robust SVG Chart Understanding",
        "author": "Yuyang Ji, and Haohan Wang",
        "link": "http://arxiv.org/abs/2504.09764v1",
        "abstract": "Multimodal Large Language Models (MLLMs) have shown remarkable versatility\nbut face challenges in demonstrating true visual understanding, particularly in\nchart reasoning tasks. Existing benchmarks like ChartQA reveal significant\nreliance on text-based shortcuts and probabilistic pattern-matching rather than\ngenuine visual reasoning. To rigorously evaluate visual reasoning, we introduce\na more challenging test scenario by removing textual labels and introducing\nchart perturbations in the ChartQA dataset. Under these conditions, models like\nGPT-4o and Gemini-2.0 Pro experience up to a 30% performance drop, underscoring\ntheir limitations. To address these challenges, we propose Socratic Chart, a\nnew framework that transforms chart images into Scalable Vector Graphics (SVG)\nrepresentations, enabling MLLMs to integrate textual and visual modalities for\nenhanced chart understanding. Socratic Chart employs a multi-agent pipeline\nwith specialized agent-generators to extract primitive chart attributes (e.g.,\nbar heights, line coordinates) and an agent-critic to validate results,\nensuring high-fidelity symbolic representations. Our framework surpasses\nstate-of-the-art models in accurately capturing chart primitives and improving\nreasoning performance, establishing a robust pathway for advancing MLLM visual\nunderstanding."
    },
    {
        "date": "2025-04",
        "title": "Enhancing Classifier Evaluation: A Fairer Benchmarking Strategy Based on Ability and Robustness",
        "author": "Lucas Cardoso, Vitor Santos, Jos\u00e9 Ribeiro, Regiane Kawasaki, Ricardo Prud\u00eancio, and Ronnie Alves",
        "link": "http://arxiv.org/abs/2504.09759v1",
        "abstract": "Benchmarking is a fundamental practice in machine learning (ML) for comparing\nthe performance of classification algorithms. However, traditional evaluation\nmethods often overlook a critical aspect: the joint consideration of dataset\ncomplexity and an algorithm's ability to generalize. Without this dual\nperspective, assessments may favor models that perform well on easy instances\nwhile failing to capture their true robustness. To address this limitation,\nthis study introduces a novel evaluation methodology that combines Item\nResponse Theory (IRT) with the Glicko-2 rating system, originally developed to\nmeasure player strength in competitive games. IRT assesses classifier ability\nbased on performance over difficult instances, while Glicko-2 updates\nperformance metrics - such as rating, deviation, and volatility - via simulated\ntournaments between classifiers. This combined approach provides a fairer and\nmore nuanced measure of algorithm capability. A case study using the\nOpenML-CC18 benchmark showed that only 15% of the datasets are truly\nchallenging and that a reduced subset with 50% of the original datasets offers\ncomparable evaluation power. Among the algorithms tested, Random Forest\nachieved the highest ability score. The results highlight the importance of\nimproving benchmark design by focusing on dataset quality and adopting\nevaluation strategies that reflect both difficulty and classifier proficiency."
    },
    {
        "date": "2025-04",
        "title": "Transformer-Based Representation Learning for Robust Gene Expression Modeling and Cancer Prognosis",
        "author": "Shuai Jiang, and Saeed Hassanpour",
        "link": "http://arxiv.org/abs/2504.09704v1",
        "abstract": "Transformer-based models have achieved remarkable success in natural language\nand vision tasks, but their application to gene expression analysis remains\nlimited due to data sparsity, high dimensionality, and missing values. We\npresent GexBERT, a transformer-based autoencoder framework for robust\nrepresentation learning of gene expression data. GexBERT learns context-aware\ngene embeddings by pretraining on large-scale transcriptomic profiles with a\nmasking and restoration objective that captures co-expression relationships\namong thousands of genes. We evaluate GexBERT across three critical tasks in\ncancer research: pan-cancer classification, cancer-specific survival\nprediction, and missing value imputation. GexBERT achieves state-of-the-art\nclassification accuracy from limited gene subsets, improves survival prediction\nby restoring expression of prognostic anchor genes, and outperforms\nconventional imputation methods under high missingness. Furthermore, its\nattention-based interpretability reveals biologically meaningful gene patterns\nacross cancer types. These findings demonstrate the utility of GexBERT as a\nscalable and effective tool for gene expression modeling, with translational\npotential in settings where gene coverage is limited or incomplete."
    },
    {
        "date": "2025-04",
        "title": "Adapting to the Unknown: Robust Meta-Learning for Zero-Shot Financial Time Series Forecasting",
        "author": "Anxian Liu, Junying Ma, and Guang Zhang",
        "link": "http://arxiv.org/abs/2504.09664v1",
        "abstract": "Financial time series forecasting in the zero-shot setting is essential for\nrisk management and investment decision-making, particularly during abrupt\nmarket regime shifts or in emerging markets with limited historical data. While\nModel-Agnostic Meta-Learning (MAML)-based approaches have shown promise in this\ndomain, existing meta task construction strategies often lead to suboptimal\nperformance, especially when dealing with highly turbulent financial time\nseries. To address this challenge, we propose a novel task construction method\nthat leverages learned embeddings for more effective meta-learning in the\nzero-shot setting. Specifically, we construct two complementary types of\nmeta-tasks based on the learned embeddings: intra-cluster tasks and\ninter-cluster tasks. To capture diverse fine-grained patterns, we apply\nstochastic projection matrices to the learned embeddings and use clustering\nalgorithm to form the tasks. Additionally, to improve generalization\ncapabilities, we employ hard task mining strategies and leverage inter-cluster\ntasks to identify invariant patterns across different time series. Extensive\nexperiments on the real world financial dataset demonstrate that our method\nsignificantly outperforms existing approaches, showing better generalization\nability in the zero-shot scenario."
    },
    {
        "date": "2025-04",
        "title": "Bridging Immutability with Flexibility: A Scheme for Secure and Efficient Smart Contract Upgrades",
        "author": "Tahrim Hossain, Sakib Hassan, Faisal Haque Bappy, Muhammad Nur Yanhaona, Tarannum Shaila Zaman, and Tariqul Islam",
        "link": "http://arxiv.org/abs/2504.09652v1",
        "abstract": "The emergence of blockchain technology has revolutionized contract execution\nthrough the introduction of smart contracts. Ethereum, the leading blockchain\nplatform, leverages smart contracts to power decentralized applications\n(DApps), enabling transparent and self-executing systems across various\ndomains. While the immutability of smart contracts enhances security and trust,\nit also poses significant challenges for updates, defect resolution, and\nadaptation to changing requirements. Existing upgrade mechanisms are complex,\nresource-intensive, and costly in terms of gas consumption, often compromising\nsecurity and limiting practical adoption. To address these challenges, we\npropose FlexiContracts+, a novel scheme that reimagines smart contracts by\nenabling secure, in-place upgrades on Ethereum while preserving historical data\nwithout relying on multiple contracts or extensive pre-deployment planning.\nFlexiContracts+ enhances security, simplifies development, reduces engineering\noverhead, and supports adaptable, expandable smart contracts. Comprehensive\ntesting demonstrates that FlexiContracts+ achieves a practical balance between\nimmutability and flexibility, advancing the capabilities of smart contract\nsystems."
    },
    {
        "date": "2025-04",
        "title": "RANSAC Revisited: An Improved Algorithm for Robust Subspace Recovery under Adversarial and Noisy Corruptions",
        "author": "Guixian Chen, Jianhao Ma, and Salar Fattahi",
        "link": "http://arxiv.org/abs/2504.09648v1",
        "abstract": "In this paper, we study the problem of robust subspace recovery (RSR) in the\npresence of both strong adversarial corruptions and Gaussian noise.\nSpecifically, given a limited number of noisy samples -- some of which are\ntampered by an adaptive and strong adversary -- we aim to recover a\nlow-dimensional subspace that approximately contains a significant fraction of\nthe uncorrupted samples, up to an error that scales with the Gaussian noise.\nExisting approaches to this problem often suffer from high computational costs\nor rely on restrictive distributional assumptions, limiting their applicability\nin truly adversarial settings. To address these challenges, we revisit the\nclassical random sample consensus (RANSAC) algorithm, which offers strong\nrobustness to adversarial outliers, but sacrifices efficiency and robustness\nagainst Gaussian noise and model misspecification in the process. We propose a\ntwo-stage algorithm, RANSAC+, that precisely pinpoints and remedies the failure\nmodes of standard RANSAC. Our method is provably robust to both Gaussian and\nadversarial corruptions, achieves near-optimal sample complexity without\nrequiring prior knowledge of the subspace dimension, and is more efficient than\nexisting RANSAC-type methods."
    },
    {
        "date": "2025-04",
        "title": "A Secure Communication Protocol for Remote Keyless Entry System with Adaptive Adjustment of Transmission Parameters",
        "author": "Jingjing Guo, Bo Tang, Jiayuan Xu, Qingyi Li, Yuyuan Qin, and Xinghua Li",
        "link": "http://arxiv.org/abs/2504.09527v1",
        "abstract": "Remote Keyless Entry (RKE) systems have become a standard feature in modern\nvehicles, yet their unidirectional fixed-frequency radio communication renders\nthem vulnerable to replay attacks, impersonation attacks, cryptanalysis, and\nintentional interference. Existing cryptographic authentication methods enhance\nsecurity but often fail to address real-world constraints such as computational\nefficiency and radio interference. To mitigate these threats, we designed the\nAdaptive Frequency-Hopping Algorithm and the Adaptive TXP and PHY Mode Control\nAlgorithm that can dynamically optimize channel selection, transmission power,\nand PHY modes based on real-time channel quality assessment. To enhance the\nsecurity and reliability of RKE systems, we propose the Lightweight Vehicle-Key\nAuthentication Protocol. In addition, a prototype of the proposed scheme was\nimplemented to verify its effectiveness in mitigating interference and\npreventing unauthorized access.Experimental results show that our scheme\nsignificantly enhances communication security and reliability while maintaining\nlow computational overhead. Under mild interference conditions, the packet\ndelivery rate (PDR) of the adaptive scheme increases from 93% to 99.23%, and\nunder strong interference, it improves from 85% to 99.01%. Additionally, the\nscheme effectively prevents replay and impersonation attacks, ensuring secure\nvehicle access control by dynamically optimizing communication parameters to\nmaintain stable and reliable transmission."
    },
    {
        "date": "2025-04",
        "title": "CheatAgent: Attacking LLM-Empowered Recommender Systems via LLM Agent",
        "author": "Liang-bo Ning, Shijie Wang, Wenqi Fan, Qing Li, Xin Xu, Hao Chen, and Feiran Huang",
        "link": "http://arxiv.org/abs/2504.13192v1",
        "abstract": "Recently, Large Language Model (LLM)-empowered recommender systems (RecSys)\nhave brought significant advances in personalized user experience and have\nattracted considerable attention. Despite the impressive progress, the research\nquestion regarding the safety vulnerability of LLM-empowered RecSys still\nremains largely under-investigated. Given the security and privacy concerns, it\nis more practical to focus on attacking the black-box RecSys, where attackers\ncan only observe the system's inputs and outputs. However, traditional attack\napproaches employing reinforcement learning (RL) agents are not effective for\nattacking LLM-empowered RecSys due to the limited capabilities in processing\ncomplex textual inputs, planning, and reasoning. On the other hand, LLMs\nprovide unprecedented opportunities to serve as attack agents to attack RecSys\nbecause of their impressive capability in simulating human-like decision-making\nprocesses. Therefore, in this paper, we propose a novel attack framework called\nCheatAgent by harnessing the human-like capabilities of LLMs, where an\nLLM-based agent is developed to attack LLM-Empowered RecSys. Specifically, our\nmethod first identifies the insertion position for maximum impact with minimal\ninput modification. After that, the LLM agent is designed to generate\nadversarial perturbations to insert at target positions. To further improve the\nquality of generated perturbations, we utilize the prompt tuning technique to\nimprove attacking strategies via feedback from the victim RecSys iteratively.\nExtensive experiments across three real-world datasets demonstrate the\neffectiveness of our proposed attacking method."
    },
    {
        "date": "2025-04",
        "title": "PLS-Assisted Offloading for Edge Computing-Enabled Post-Quantum Security in Resource-Constrained Devices",
        "author": "Hamid Amiriara, Mahtab Mirmohseni, and Rahim Tafazolli",
        "link": "http://arxiv.org/abs/2504.09437v1",
        "abstract": "With the advent of post-quantum cryptography (PQC) standards, it has become\nimperative for resource-constrained devices (RCDs) in the Internet of Things\n(IoT) to adopt these quantum-resistant protocols. However, the high\ncomputational overhead and the large key sizes associated with PQC make direct\ndeployment on such devices impractical. To address this challenge, we propose\nan edge computing-enabled PQC framework that leverages a physical-layer\nsecurity (PLS)-assisted offloading strategy, allowing devices to either offload\nintensive cryptographic tasks to a post-quantum edge server (PQES) or perform\nthem locally. Furthermore, to ensure data confidentiality within the edge\ndomain, our framework integrates two PLS techniques: offloading RCDs employ\nwiretap coding to secure data transmission, while non-offloading RCDs serve as\nfriendly jammers by broadcasting artificial noise to disrupt potential\neavesdroppers. Accordingly, we co-design the computation offloading and PLS\nstrategy by jointly optimizing the device transmit power, PQES computation\nresource allocation, and offloading decisions to minimize overall latency under\nresource constraints. Numerical results demonstrate significant latency\nreductions compared to baseline schemes, confirming the scalability and\nefficiency of our approach for secure PQC operations in IoT networks."
    },
    {
        "date": "2025-04",
        "title": "Ensemble-Enhanced Graph Autoencoder with GAT and Transformer-Based Encoders for Robust Fault Diagnosis",
        "author": "Moirangthem Tiken Singh",
        "link": "http://arxiv.org/abs/2504.09427v1",
        "abstract": "Fault classification in industrial machinery is vital for enhancing\nreliability and reducing downtime, yet it remains challenging due to the\nvariability of vibration patterns across diverse operating conditions. This\nstudy introduces a novel graph-based framework for fault classification,\nconverting time-series vibration data from machinery operating at varying\nhorsepower levels into a graph representation. We utilize Shannon's entropy to\ndetermine the optimal window size for data segmentation, ensuring each segment\ncaptures significant temporal patterns, and employ Dynamic Time Warping (DTW)\nto define graph edges based on segment similarity. A Graph Auto Encoder (GAE)\nwith a deep graph transformer encoder, decoder, and ensemble classifier is\ndeveloped to learn latent graph representations and classify faults across\nvarious categories. The GAE's performance is evaluated on the Case Western\nReserve University (CWRU) dataset, with cross-dataset generalization assessed\non the HUST dataset. Results show that GAE achieves a mean F1-score of 0.99 on\nthe CWRU dataset, significantly outperforming baseline models-CNN, LSTM, RNN,\nGRU, and Bi-LSTM (F1-scores: 0.94-0.97, p < 0.05, Wilcoxon signed-rank test for\nBi-LSTM: p < 0.05) -- particularly in challenging classes (e.g., Class 8: 0.99\nvs. 0.71 for Bi-LSTM). Visualization of dataset characteristics reveals that\ndatasets with amplified vibration patterns and diverse fault dynamics enhance\ngeneralization. This framework provides a robust solution for fault diagnosis\nunder varying conditions, offering insights into dataset impacts on model\nperformance."
    },
    {
        "date": "2025-04",
        "title": "Nash Equilibrium Between Consumer Electronic Devices and DoS Attacker for Distributed IoT-enabled RSE Systems",
        "author": "Gengcan Chen, Donghong Cai, Zahid Khan, Jawad Ahmad, and Wadii Boulila",
        "link": "http://arxiv.org/abs/2504.09415v1",
        "abstract": "In electronic consumer Internet of Things (IoT), consumer electronic devices\nas edge devices require less computational overhead and the remote state\nestimation (RSE) of consumer electronic devices is always at risk of\ndenial-of-service (DoS) attacks. Therefore, the adversarial strategy between\nconsumer electronic devices and DoS attackers is critical. This paper focuses\non the adversarial strategy between consumer electronic devices and DoS\nattackers in IoT-enabled RSE Systems. We first propose a remote joint\nestimation model for distributed measurements to effectively reduce consumer\nelectronic device workload and minimize data leakage risks. The Kalman filter\nis deployed on the remote estimator, and the DoS attacks with open-loop as well\nas closed-loop are considered. We further introduce advanced reinforcement\nlearning techniques, including centralized and distributed Minimax-DQN, to\naddress high-dimensional decision-making challenges in both open-loop and\nclosed-loop scenarios. Especially, the Q-network instead of the Q-table is used\nin the proposed approaches, which effectively solves the challenge of\nQ-learning. Moreover, the proposed distributed Minimax-DQN reduces the action\nspace to expedite the search for Nash Equilibrium (NE). The experimental\nresults validate that the proposed model can expeditiously restore the RSE\nerror covariance to a stable state in the presence of DoS attacks, exhibiting\nnotable attack robustness. The proposed centralized and distributed Minimax-DQN\neffectively resolves the NE in both open and closed-loop case, showcasing\nremarkable performance in terms of convergence. It reveals that substantial\nadvantages in both efficiency and stability are achieved compared with the\nstate-of-the-art methods."
    },
    {
        "date": "2025-04",
        "title": "PapMOT: Exploring Adversarial Patch Attack against Multiple Object Tracking",
        "author": "Jiahuan Long, Tingsong Jiang, Wen Yao, Shuai Jia, Weijia Zhang, Weien Zhou, Chao Ma, and Xiaoqian Chen",
        "link": "http://arxiv.org/abs/2504.09361v1",
        "abstract": "Tracking multiple objects in a continuous video stream is crucial for many\ncomputer vision tasks. It involves detecting and associating objects with their\nrespective identities across successive frames. Despite significant progress\nmade in multiple object tracking (MOT), recent studies have revealed the\nvulnerability of existing MOT methods to adversarial attacks. Nevertheless, all\nof these attacks belong to digital attacks that inject pixel-level noise into\ninput images, and are therefore ineffective in physical scenarios. To fill this\ngap, we propose PapMOT, which can generate physical adversarial patches against\nMOT for both digital and physical scenarios. Besides attacking the detection\nmechanism, PapMOT also optimizes a printable patch that can be detected as new\ntargets to mislead the identity association process. Moreover, we introduce a\npatch enhancement strategy to further degrade the temporal consistency of\ntracking results across video frames, resulting in more aggressive attacks. We\nfurther develop new evaluation metrics to assess the robustness of MOT against\nsuch attacks. Extensive evaluations on multiple datasets demonstrate that our\nPapMOT can successfully attack various architectures of MOT trackers in digital\nscenarios. We also validate the effectiveness of PapMOT for physical attacks by\ndeploying printed adversarial patches in the real world."
    },
    {
        "date": "2025-04",
        "title": "Explorer: Robust Collection of Interactable GUI Elements",
        "author": "Iason Chaimalas, Arnas Vy\u0161niauskas, and Gabriel Brostow",
        "link": "http://arxiv.org/abs/2504.09352v1",
        "abstract": "Automation of existing Graphical User Interfaces (GUIs) is important but hard\nto achieve. Upstream of making the GUI user-accessible or somehow scriptable,\neven the data-collection to understand the original interface poses significant\nchallenges. For example, large quantities of general UI data seem helpful for\ntraining general machine learning (ML) models, but accessibility for each\nperson can hinge on the ML's precision on a specific app. We therefore take the\nperspective that a given user needs confidence, that the relevant UI elements\nare being detected correctly throughout one app or digital environment. We\nmostly assume that the target application is known in advance, so that data\ncollection and ML-training can be personalized for the test-time target domain.\nThe proposed Explorer system focuses on detecting on-screen buttons and\ntext-entry fields, i.e. interactables, where the training process has access to\na live version of the application. The live application can run on almost any\npopular platform except iOS phones, and the collection is especially\nstreamlined for Android phones or for desktop Chrome browsers. Explorer also\nenables the recording of interactive user sessions, and subsequent mapping of\nhow such sessions overlap and sometimes loop back to similar states. We show\nhow having such a map enables a kind of path planning through the GUI, letting\na user issue audio commands to get to their destination. Critically, we are\nreleasing our code for Explorer openly at https://github.com/varnelis/Explorer."
    },
    {
        "date": "2025-04",
        "title": "CrossLink: A Decentralized Framework for Secure Cross-Chain Smart Contract Execution",
        "author": "Tahrim Hossain, Faisal Haque Bappy, Tarannum Shaila Zaman, and Tariqul Islam",
        "link": "http://arxiv.org/abs/2504.09319v1",
        "abstract": "This paper introduces CrossLink, a decentralized framework for secure\ncross-chain smart contract execution that effectively addresses the inherent\nlimitations of contemporary solutions, which primarily focus on asset transfers\nand rely on potentially vulnerable centralized intermediaries. Recognizing the\nescalating demand for seamless interoperability among decentralized\napplications, CrossLink provides a trustless mechanism for smart contracts\nacross disparate blockchain networks to communicate and interact. At its core,\nCrossLink utilizes a compact chain for selectively storing authorized contract\nstates and employs a secure inter-chain messaging mechanism to ensure atomic\nexecution and data consistency. By implementing a deposit/collateral fee system\nand efficient state synchronization, CrossLink enhances security and mitigates\nvulnerabilities, offering a novel approach to seamless, secure, and\ndecentralized cross-chain interoperability. A formal security analysis further\nvalidates CrossLink's robustness against unauthorized modifications and\ndenial-of-service attacks."
    },
    {
        "date": "2025-04",
        "title": "SmartShift: A Secure and Efficient Approach to Smart Contract Migration",
        "author": "Tahrim Hossain, Faisal Haque Bappy, Tarannum Shaila Zaman, Raiful Hasan, and Tariqul Islam",
        "link": "http://arxiv.org/abs/2504.09315v1",
        "abstract": "Blockchain and smart contracts have emerged as revolutionary technologies\ntransforming distributed computing. While platform evolution and smart\ncontracts' inherent immutability necessitate migrations both across and within\nchains, migrating the vast amounts of critical data in these contracts while\nmaintaining data integrity and minimizing operational disruption presents a\nsignificant challenge. To address these challenges, we present SmartShift, a\nframework that enables secure and efficient smart contract migrations through\nintelligent state partitioning and progressive function activation, preserving\noperational continuity during transitions. Our comprehensive evaluation\ndemonstrates that SmartShift significantly reduces migration downtime while\nensuring robust security, establishing a foundation for efficient and secure\nsmart contract migration systems."
    },
    {
        "date": "2025-04",
        "title": "A Lightweight Moment Retrieval System with Global Re-Ranking and Robust Adaptive Bidirectional Temporal Search",
        "author": "Tinh-Anh Nguyen-Nhu, Huu-Loc Tran, Nguyen-Khang Le, Minh-Nhat Nguyen, Tien-Huy Nguyen, Hoang-Long Nguyen-Huu, Huu-Phong Phan-Nguyen, Huy-Thach Pham, Quan Nguyen, Hoang M. Le, and Quang-Vinh Dinh",
        "link": "http://arxiv.org/abs/2504.09298v1",
        "abstract": "The exponential growth of digital video content has posed critical challenges\nin moment-level video retrieval, where existing methodologies struggle to\nefficiently localize specific segments within an expansive video corpus.\nCurrent retrieval systems are constrained by computational inefficiencies,\ntemporal context limitations, and the intrinsic complexity of navigating video\ncontent. In this paper, we address these limitations through a novel\nInteractive Video Corpus Moment Retrieval framework that integrates a\nSuperGlobal Reranking mechanism and Adaptive Bidirectional Temporal Search\n(ABTS), strategically optimizing query similarity, temporal stability, and\ncomputational resources. By preprocessing a large corpus of videos using a\nkeyframe extraction model and deduplication technique through image hashing,\nour approach provides a scalable solution that significantly reduces storage\nrequirements while maintaining high localization precision across diverse video\nrepositories."
    },
    {
        "date": "2025-04",
        "title": "Learning Occlusion-Robust Vision Transformers for Real-Time UAV Tracking",
        "author": "You Wu, Xucheng Wang, Xiangyang Yang, Mengyuan Liu, Dan Zeng, Hengzhou Ye, and Shuiwang Li",
        "link": "http://arxiv.org/abs/2504.09228v1",
        "abstract": "Single-stream architectures using Vision Transformer (ViT) backbones show\ngreat potential for real-time UAV tracking recently. However, frequent\nocclusions from obstacles like buildings and trees expose a major drawback:\nthese models often lack strategies to handle occlusions effectively. New\nmethods are needed to enhance the occlusion resilience of single-stream ViT\nmodels in aerial tracking. In this work, we propose to learn Occlusion-Robust\nRepresentations (ORR) based on ViTs for UAV tracking by enforcing an invariance\nof the feature representation of a target with respect to random masking\noperations modeled by a spatial Cox process. Hopefully, this random masking\napproximately simulates target occlusions, thereby enabling us to learn ViTs\nthat are robust to target occlusion for UAV tracking. This framework is termed\nORTrack. Additionally, to facilitate real-time applications, we propose an\nAdaptive Feature-Based Knowledge Distillation (AFKD) method to create a more\ncompact tracker, which adaptively mimics the behavior of the teacher model\nORTrack according to the task's difficulty. This student model, dubbed\nORTrack-D, retains much of ORTrack's performance while offering higher\nefficiency. Extensive experiments on multiple benchmarks validate the\neffectiveness of our method, demonstrating its state-of-the-art performance.\nCodes is available at https://github.com/wuyou3474/ORTrack."
    },
    {
        "date": "2025-04",
        "title": "FairACE: Achieving Degree Fairness in Graph Neural Networks via Contrastive and Adversarial Group-Balanced Training",
        "author": "Jiaxin Liu, Xiaoqian Jiang, Xiang Li, Bohan Zhang, and Jing Zhang",
        "link": "http://arxiv.org/abs/2504.09210v2",
        "abstract": "Fairness has been a significant challenge in graph neural networks (GNNs)\nsince degree biases often result in un-equal prediction performance among nodes\nwith varying degrees. Existing GNN models focus on prediction accuracy,\nfrequently overlooking fairness across different degree groups. To addressthis\nissue, we propose a novel GNN framework, namely Fairness- Aware Asymmetric\nContrastive Ensemble (FairACE), which inte-grates asymmetric contrastive\nlearning with adversarial training to improve degree fairness. FairACE captures\none-hop local neighborhood information and two-hop monophily similarity to\ncreate fairer node representations and employs a degree fairness regulator to\nbalance performance between high-degree and low-degree nodes. During model\ntraining, a novel group-balanced fairness loss is proposed to minimize\nclassification disparities across degree groups. In addition, we also propose a\nnovel fairness metric, the Accuracy Distribution Gap (ADG), which can\nquantitatively assess and ensure equitable performance across different\ndegree-based node groups. Experimental results on both synthetic and real-world\ndatasets demonstrate that FairACE significantly improves degree fairness\nmetrics while maintaining competitive accuracy in comparison to the\nstate-of-the-art GNN models."
    },
    {
        "date": "2025-04",
        "title": "Illusion Worlds: Deceptive UI Attacks in Social VR",
        "author": "Junhee Lee, Hwanjo Heo, Seungwon Woo, Minseok Kim, Jongseop Kim, and Jinwoo Kim",
        "link": "http://arxiv.org/abs/2504.09199v1",
        "abstract": "Social Virtual Reality (VR) platforms have surged in popularity, yet their\nsecurity risks remain underexplored. This paper presents four novel UI attacks\nthat covertly manipulate users into performing harmful actions through\ndeceptive virtual content. Implemented on VRChat and validated in an\nIRB-approved study with 30 participants, these attacks demonstrate how\ndeceptive elements can mislead users into malicious actions without their\nawareness. To address these vulnerabilities, we propose MetaScanner, a\nproactive countermeasure that rapidly analyzes objects and scripts in virtual\nworlds, detecting suspicious elements within seconds."
    },
    {
        "date": "2025-04",
        "title": "RT-DATR:Real-time Unsupervised Domain Adaptive Detection Transformer with Adversarial Feature Learning",
        "author": "Feng Lv, Chunlong Xia, Shuo Wang, and Huo Cao",
        "link": "http://arxiv.org/abs/2504.09196v1",
        "abstract": "Despite domain-adaptive object detectors based on CNN and transformers have\nmade significant progress in cross-domain detection tasks, it is regrettable\nthat domain adaptation for real-time transformer-based detectors has not yet\nbeen explored. Directly applying existing domain adaptation algorithms has\nproven to be suboptimal. In this paper, we propose RT-DATR, a simple and\nefficient real-time domain adaptive detection transformer. Building on RT-DETR\nas our base detector, we first introduce a local object-level feature alignment\nmodule to significantly enhance the feature representation of domain invariance\nduring object transfer. Additionally, we introduce a scene semantic feature\nalignment module designed to boost cross-domain detection performance by\naligning scene semantic features. Finally, we introduced a domain query and\ndecoupled it from the object query to further align the instance feature\ndistribution within the decoder layer, reduce the domain gap, and maintain\ndiscriminative ability. Experimental results on various benchmarks demonstrate\nthat our method outperforms current state-of-the-art approaches. Our code will\nbe released soon."
    },
    {
        "date": "2025-04",
        "title": "Towards More Efficient, Robust, Instance-adaptive, and Generalizable Online Learning",
        "author": "Zhiyong Wang",
        "link": "http://arxiv.org/abs/2504.09192v2",
        "abstract": "The primary goal of my Ph.D. study is to develop provably efficient and\npractical algorithms for data-driven online sequential decision-making under\nuncertainty. My work focuses on reinforcement learning (RL), multi-armed\nbandits, and their applications, including recommendation systems, computer\nnetworks, video analytics, and large language models (LLMs). Online learning\nmethods, such as bandits and RL, have demonstrated remarkable success - ranging\nfrom outperforming human players in complex games like Atari and Go to\nadvancing robotics, recommendation systems, and fine-tuning LLMs. Despite these\nsuccesses, many established algorithms rely on idealized models that can fail\nunder model misspecifications or adversarial perturbations, particularly in\nsettings where accurate prior knowledge of the underlying model class is\nunavailable or where malicious users operate within dynamic systems. These\nchallenges are pervasive in real-world applications, where robust and adaptive\nsolutions are critical. Furthermore, while worst-case guarantees provide\ntheoretical reliability, they often fail to capture instance-dependent\nperformance, which can lead to more efficient and practical solutions. Another\nkey challenge lies in generalizing to new, unseen environments, a crucial\nrequirement for deploying these methods in dynamic and unpredictable settings.\nTo address these limitations, my research aims to develop more efficient,\nrobust, instance-adaptive, and generalizable online learning algorithms for\nboth reinforcement learning and bandits. Towards this end, I focus on\ndeveloping more efficient, robust, instance-adaptive, and generalizable for\nboth general reinforcement learning (RL) and bandits."
    },
    {
        "date": "2025-04",
        "title": "A Multi-Layered Security Analysis of Blockchain Systems: From Attack Vectors to Defense and System Hardening",
        "author": "Yuhuan Yang, Shipeng Ye, and Xiaoqi Li",
        "link": "http://arxiv.org/abs/2504.09181v1",
        "abstract": "The application of Bitcoin enables people to understand blockchain technology\ngradually. Bitcoin is a decentralized currency that does not rely on\nthird-party credit institutions, and the core of Bitcoin's underlying\ntechnology is blockchain. With the increasing value of Bitcoin and the vigorous\ndevelopment of decentralization, people's research on blockchain is also\nincreasing day by day. Today's blockchain technology has not only made great\nachievements in the application of Bitcoin, but has also been preliminarily\napplied in other fields, such as finance, medical treatment, the Internet of\nThings, and so on. However, with the initial application of blockchain\ntechnology on the Internet, the security of blockchain technology has also been\nwidely concerned by people in the industry. For example, whether currency\ntrading platforms, smart contracts, blockchain consensus mechanisms, and other\ntechnologies are vulnerable to attacks, and how we can defend against these\nattacks digitally and optimize the blockchain system is exactly the subject we\nwant to study. For the security of appeal blockchain, this paper first analyzes\nthe security threats faced by the application digital currency trading platform\nof the blockchain system, then analyzes the security problems of smart contract\nclosely related to blockchain 2.0, and then analyzes and studies the security\nthreats of blockchain public chain, consensus mechanism, and P2P. Finally,\ncombined with the security problems at all levels of the blockchain system we\nanalyze and study how to optimize the security of the blockchain system."
    },
    {
        "date": "2025-04",
        "title": "CI-RKM: A Class-Informed Approach to Robust Restricted Kernel Machines",
        "author": "Ritik Mishra, Mushir Akhtar, and M. Tanveer",
        "link": "http://arxiv.org/abs/2504.11476v1",
        "abstract": "Restricted kernel machines (RKMs) represent a versatile and powerful\nframework within the kernel machine family, leveraging conjugate feature\nduality to address a wide range of machine learning tasks, including\nclassification, regression, and feature learning. However, their performance\ncan degrade significantly in the presence of noise and outliers, which\ncompromises robustness and predictive accuracy. In this paper, we propose a\nnovel enhancement to the RKM framework by integrating a class-informed weighted\nfunction. This weighting mechanism dynamically adjusts the contribution of\nindividual training points based on their proximity to class centers and\nclass-specific characteristics, thereby mitigating the adverse effects of noisy\nand outlier data. By incorporating weighted conjugate feature duality and\nleveraging the Schur complement theorem, we introduce the class-informed\nrestricted kernel machine (CI-RKM), a robust extension of the RKM designed to\nimprove generalization and resilience to data imperfections. Experimental\nevaluations on benchmark datasets demonstrate that the proposed CI-RKM\nconsistently outperforms existing baselines, achieving superior classification\naccuracy and enhanced robustness against noise and outliers. Our proposed\nmethod establishes a significant advancement in the development of kernel-based\nlearning models, addressing a core challenge in the field."
    },
    {
        "date": "2025-04",
        "title": "A Confounding Factors-Inhibition Adversarial Learning Framework for Multi-site fMRI Mental Disorder Identification",
        "author": "Xin Wen, Shijie Guo, Wenbo Ning, Rui Cao, Yan Niu, Bin Wan, Peng Wei, Xiaobo Liu, and Jie Xiang",
        "link": "http://arxiv.org/abs/2504.09179v1",
        "abstract": "In open data sets of functional magnetic resonance imaging (fMRI), the\nheterogeneity of the data is typically attributed to a combination of factors,\nincluding differences in scanning procedures, the presence of confounding\neffects, and population diversities between multiple sites. These factors\ncontribute to the diminished effectiveness of representation learning, which in\nturn affects the overall efficacy of subsequent classification procedures. To\naddress these limitations, we propose a novel multi-site adversarial learning\nnetwork (MSalNET) for fMRI-based mental disorder detection. Firstly, a\nrepresentation learning module is introduced with a node information assembly\n(NIA) mechanism to better extract features from functional connectivity (FC).\nThis mechanism aggregates edge information from both horizontal and vertical\ndirections, effectively assembling node information. Secondly, to generalize\nthe feature across sites, we proposed a site-level feature extraction module\nthat can learn from individual FC data, which circumvents additional prior\ninformation. Lastly, an adversarial learning network is proposed as a means of\nbalancing the trade-off between individual classification and site regression\ntasks, with the introduction of a novel loss function. The proposed method was\nevaluated on two multi-site fMRI datasets, i.e., Autism Brain Imaging Data\nExchange (ABIDE) and ADHD-200. The results indicate that the proposed method\nachieves a better performance than other related algorithms with the accuracy\nof 75.56 and 68.92 in ABIDE and ADHD-200 datasets, respectively. Furthermore,\nthe result of the site regression indicates that the proposed method reduces\nsite variability from a data-driven perspective. The most discriminative brain\nregions revealed by NIA are consistent with statistical findings, uncovering\nthe \"black box\" of deep learning to a certain extent."
    },
    {
        "date": "2025-04",
        "title": "Secure Physical Layer Communications for Low-Altitude Economy Networking: A Survey",
        "author": "Lingyi Cai, Jiacheng Wang, Ruichen Zhang, Yu Zhang, Tao Jiang, Dusit Niyato, Xianbin Wang, Abbas Jamalipour, and Xuemin Shen",
        "link": "http://arxiv.org/abs/2504.09153v1",
        "abstract": "The Low-Altitude Economy Networking (LAENet) is emerging as a transformative\nparadigm that enables an integrated and sophisticated communication\ninfrastructure to support aerial vehicles in carrying out a wide range of\neconomic activities within low-altitude airspace. However, the physical layer\ncommunications in the LAENet face growing security threats due to inherent\ncharacteristics of aerial communication environments, such as signal broadcast\nnature and channel openness. These challenges highlight the urgent need for\nsafeguarding communication confidentiality, availability, and integrity. In\nview of the above, this survey comprehensively reviews existing secure\ncountermeasures for physical layer communication in the LAENet. We explore core\nmethods focusing on anti-eavesdropping and authentication for ensuring\ncommunication confidentiality. Subsequently, availability-enhancing techniques\nare thoroughly discussed for anti-jamming and spoofing defense. Then, we review\napproaches for safeguarding integrity through anomaly detection and injection\nprotection. Furthermore, we discuss future research directions, emphasizing\nenergy-efficient physical layer security, multi-drone collaboration for secure\ncommunication, AI-driven security defense strategy, space-air-ground integrated\nsecurity architecture, and 6G-enabled secure UAV communication. This survey may\nprovide valuable references and new insights for researchers in the field of\nsecure physical layer communication for the LAENet."
    },
    {
        "date": "2025-04",
        "title": "Self-Supervised Autoencoder Network for Robust Heart Rate Extraction from Noisy Photoplethysmogram: Applying Blind Source Separation to Biosignal Analysis",
        "author": "Matthew B. Webster, Dongheon Lee, and Joonnyong Lee",
        "link": "http://arxiv.org/abs/2504.09132v1",
        "abstract": "Biosignals can be viewed as mixtures measuring particular physiological\nevents, and blind source separation (BSS) aims to extract underlying source\nsignals from mixtures. This paper proposes a self-supervised multi-encoder\nautoencoder (MEAE) to separate heartbeat-related source signals from\nphotoplethysmogram (PPG), enhancing heart rate (HR) detection in noisy PPG\ndata. The MEAE is trained on PPG signals from a large open polysomnography\ndatabase without any pre-processing or data selection. The trained network is\nthen applied to a noisy PPG dataset collected during the daily activities of\nnine subjects. The extracted heartbeat-related source signal significantly\nimproves HR detection as compared to the original PPG. The absence of\npre-processing and the self-supervised nature of the proposed method, combined\nwith its strong performance, highlight the potential of BSS in biosignal\nanalysis."
    },
    {
        "date": "2025-04",
        "title": "CAShift: Benchmarking Log-Based Cloud Attack Detection under Normality Shift",
        "author": "Jiongchi Yu, Xiaofei Xie, Qiang Hu, Bowen Zhang, Ziming Zhao, Yun Lin, Lei Ma, Ruitao Feng, and Frank Liauw",
        "link": "http://arxiv.org/abs/2504.09115v2",
        "abstract": "With the rapid advancement of cloud-native computing, securing cloud\nenvironments has become an important task. Log-based Anomaly Detection (LAD) is\nthe most representative technique used in different systems for attack\ndetection and safety guarantee, where multiple LAD methods and relevant\ndatasets have been proposed. However, even though some of these datasets are\nspecifically prepared for cloud systems, they only cover limited cloud\nbehaviors and lack information from a whole-system perspective. Besides,\nanother critical issue to consider is normality shift, which implies the test\ndistribution could differ from the training distribution and highly affects the\nperformance of LAD. Unfortunately, existing works only focus on simple shift\ntypes such as chronological changes, while other important and cloud-specific\nshift types are ignored, e.g., the distribution shift introduced by different\ndeployed cloud architectures. Therefore, creating a new dataset that covers\ndiverse behaviors of cloud systems and normality shift types is necessary.\n  To fill the gap in evaluating LAD under real-world conditions, we present\nCAShift, the first normality shift-aware dataset for cloud systems. CAShift\ncaptures three shift types, including application, version, and cloud\narchitecture shifts, and includes 20 diverse attack scenarios across various\ncloud components. Using CAShift, we conduct an empirical study showing that (1)\nall LAD methods are significantly affected by normality shifts, with\nperformance drops of up to 34%, and (2) continuous learning techniques can\nimprove F1-scores by up to 27%, depending on data usage and algorithm choice.\nBased on our findings, we offer valuable implications for future research in\ndesigning more robust LAD models and methods for LAD shift adaptation."
    },
    {
        "date": "2025-04",
        "title": "Detecting Instruction Fine-tuning Attack on Language Models with Influence Function",
        "author": "Jiawei Li",
        "link": "http://arxiv.org/abs/2504.09026v1",
        "abstract": "Instruction fine-tuning attacks pose a significant threat to large language\nmodels (LLMs) by subtly embedding poisoned data in fine-tuning datasets, which\ncan trigger harmful or unintended responses across a range of tasks. This\nundermines model alignment and poses security risks in real-world deployment.\nIn this work, we present a simple and effective approach to detect and mitigate\nsuch attacks using influence functions, a classical statistical tool adapted\nfor machine learning interpretation. Traditionally, the high computational\ncosts of influence functions have limited their application to large models and\ndatasets. The recent Eigenvalue-Corrected Kronecker-Factored Approximate\nCurvature (EK-FAC) approximation method enables efficient influence score\ncomputation, making it feasible for large-scale analysis.\n  We are the first to apply influence functions for detecting language model\ninstruction fine-tuning attacks on large-scale datasets, as both the\ninstruction fine-tuning attack on language models and the influence calculation\napproximation technique are relatively new. Our large-scale empirical\nevaluation of influence functions on 50,000 fine-tuning examples and 32 tasks\nreveals a strong association between influence scores and sentiment. Building\non this, we introduce a novel sentiment transformation combined with influence\nfunctions to detect and remove critical poisons -- poisoned data points that\nskew model predictions. Removing these poisons (only 1% of total data) recovers\nmodel performance to near-clean levels, demonstrating the effectiveness and\nefficiency of our approach. Artifact is available at\nhttps://github.com/lijiawei20161002/Poison-Detection.\n  WARNING: This paper contains offensive data examples."
    },
    {
        "date": "2025-04",
        "title": "High Dynamic Range Modulo Imaging for Robust Object Detection in Autonomous Driving",
        "author": "Kebin Contreras, Brayan Monroy, and Jorge Bacca",
        "link": "http://arxiv.org/abs/2504.11472v1",
        "abstract": "Object detection precision is crucial for ensuring the safety and efficacy of\nautonomous driving systems. The quality of acquired images directly influences\nthe ability of autonomous driving systems to correctly recognize and respond to\nother vehicles, pedestrians, and obstacles in real-time. However, real\nenvironments present extreme variations in lighting, causing saturation\nproblems and resulting in the loss of crucial details for detection.\nTraditionally, High Dynamic Range (HDR) images have been preferred for their\nability to capture a broad spectrum of light intensities, but the need for\nmultiple captures to construct HDR images is inefficient for real-time\napplications in autonomous vehicles. To address these issues, this work\nintroduces the use of modulo sensors for robust object detection. The modulo\nsensor allows pixels to `reset/wrap' upon reaching saturation level by\nacquiring an irradiance encoding image which can then be recovered using\nunwrapping algorithms. The applied reconstruction techniques enable HDR\nrecovery of color intensity and image details, ensuring better visual quality\neven under extreme lighting conditions at the cost of extra time. Experiments\nwith the YOLOv10 model demonstrate that images processed using modulo images\nachieve performance comparable to HDR images and significantly surpass\nsaturated images in terms of object detection accuracy. Moreover, the proposed\nmodulo imaging step combined with HDR image reconstruction is shorter than the\ntime required for conventional HDR image acquisition."
    },
    {
        "date": "2025-04",
        "title": "Robust Steganography from Large Language Models",
        "author": "Neil Perry, Sanket Gupte, Nishant Pitta, and Lior Rotem",
        "link": "http://arxiv.org/abs/2504.08977v1",
        "abstract": "Recent steganographic schemes, starting with Meteor (CCS'21), rely on\nleveraging large language models (LLMs) to resolve a historically-challenging\ntask of disguising covert communication as ``innocent-looking''\nnatural-language communication. However, existing methods are vulnerable to\n``re-randomization attacks,'' where slight changes to the communicated text,\nthat might go unnoticed, completely destroy any hidden message. This is also a\nvulnerability in more traditional encryption-based stegosystems, where\nadversaries can modify the randomness of an encryption scheme to destroy the\nhidden message while preserving an acceptable covertext to ordinary users. In\nthis work, we study the problem of robust steganography. We introduce formal\ndefinitions of weak and strong robust LLM-based steganography, corresponding to\ntwo threat models in which natural language serves as a covertext channel\nresistant to realistic re-randomization attacks. We then propose two\nconstructions satisfying these notions. We design and implement our\nsteganographic schemes that embed arbitrary secret messages into natural\nlanguage text generated by LLMs, ensuring recoverability even under adversarial\nparaphrasing and rewording attacks. To support further research and real-world\ndeployment, we release our implementation and datasets for public use."
    },
    {
        "date": "2025-04",
        "title": "Exploring the Effects of Load Altering Attacks on Load Frequency Control through Python and RTDS",
        "author": "Micha\u0142 Forystek, Andrew D. Syrmakesis, Alkistis Kontou, Panos Kotsampopoulos, Nikos D. Hatziargyriou, and Charalambos Konstantinou",
        "link": "http://arxiv.org/abs/2504.08951v1",
        "abstract": "The modern power grid increasingly depends on advanced information and\ncommunication technology (ICT) systems to enhance performance and reliability\nthrough real-time monitoring, intelligent control, and bidirectional\ncommunication. However, ICT integration also exposes the grid to cyber-threats.\nLoad altering attacks (LAAs), which use botnets of high-wattage devices to\nmanipulate load profiles, are a notable threat to grid stability. While\nprevious research has examined LAAs, their specific impact on load frequency\ncontrol (LFC), critical for maintaining nominal frequency during load\nfluctuations, still needs to be explored. Even minor frequency deviations can\njeopardize grid operations. This study bridges the gap by analyzing LAA effects\non LFC through simulations of static and dynamic scenarios using Python and\nRTDS. The results highlight LAA impacts on frequency stability and present an\neigenvalue-based stability assessment for dynamic LAAs (DLAAs), identifying key\nparameters influencing grid resilience."
    },
    {
        "date": "2025-04",
        "title": "Robust SAM: On the Adversarial Robustness of Vision Foundation Models",
        "author": "Jiahuan Long, Zhengqin Xu, Tingsong Jiang, Wen Yao, Shuai Jia, Chao Ma, and Xiaoqian Chen",
        "link": "http://arxiv.org/abs/2504.08906v1",
        "abstract": "The Segment Anything Model (SAM) is a widely used vision foundation model\nwith diverse applications, including image segmentation, detection, and\ntracking. Given SAM's wide applications, understanding its robustness against\nadversarial attacks is crucial for real-world deployment. However, research on\nSAM's robustness is still in its early stages. Existing attacks often overlook\nthe role of prompts in evaluating SAM's robustness, and there has been\ninsufficient exploration of defense methods to balance the robustness and\naccuracy. To address these gaps, this paper proposes an adversarial robustness\nframework designed to evaluate and enhance the robustness of SAM. Specifically,\nwe introduce a cross-prompt attack method to enhance the attack transferability\nacross different prompt types. Besides attacking, we propose a few-parameter\nadaptation strategy to defend SAM against various adversarial attacks. To\nbalance robustness and accuracy, we use the singular value decomposition (SVD)\nto constrain the space of trainable parameters, where only singular values are\nadaptable. Experiments demonstrate that our cross-prompt attack method\noutperforms previous approaches in terms of attack success rate on both SAM and\nSAM 2. By adapting only 512 parameters, we achieve at least a 15\\% improvement\nin mean intersection over union (mIoU) against various adversarial attacks.\nCompared to previous defense methods, our approach enhances the robustness of\nSAM while maximally maintaining its original performance."
    },
    {
        "date": "2025-04",
        "title": "Toward Spiking Neural Network Local Learning Modules Resistant to Adversarial Attacks",
        "author": "Jiaqi Lin, and Abhronil Sengupta",
        "link": "http://arxiv.org/abs/2504.08897v1",
        "abstract": "Recent research has shown the vulnerability of Spiking Neural Networks (SNNs)\nunder adversarial examples that are nearly indistinguishable from clean data in\nthe context of frame-based and event-based information. The majority of these\nstudies are constrained in generating adversarial examples using\nBackpropagation Through Time (BPTT), a gradient-based method which lacks\nbiological plausibility. In contrast, local learning methods, which relax many\nof BPTT's constraints, remain under-explored in the context of adversarial\nattacks. To address this problem, we examine adversarial robustness in SNNs\nthrough the framework of four types of training algorithms. We provide an\nin-depth analysis of the ineffectiveness of gradient-based adversarial attacks\nto generate adversarial instances in this scenario. To overcome these\nlimitations, we introduce a hybrid adversarial attack paradigm that leverages\nthe transferability of adversarial instances. The proposed hybrid approach\ndemonstrates superior performance, outperforming existing adversarial attack\nmethods. Furthermore, the generalizability of the method is assessed under\nmulti-step adversarial attacks, adversarial attacks in black-box FGSM\nscenarios, and within the non-spiking domain."
    },
    {
        "date": "2025-04",
        "title": "Enterprise-Grade Security for the Model Context Protocol (MCP): Frameworks and Mitigation Strategies",
        "author": "Vineeth Sai Narajala, and Idan Habler",
        "link": "http://arxiv.org/abs/2504.08623v1",
        "abstract": "The Model Context Protocol (MCP), introduced by Anthropic, provides a\nstandardized framework for artificial intelligence (AI) systems to interact\nwith external data sources and tools in real-time. While MCP offers significant\nadvantages for AI integration and capability extension, it introduces novel\nsecurity challenges that demand rigorous analysis and mitigation. This paper\nbuilds upon foundational research into MCP architecture and preliminary\nsecurity assessments to deliver enterprise-grade mitigation frameworks and\ndetailed technical implementation strategies. Through systematic threat\nmodeling and analysis of MCP implementations and analysis of potential attack\nvectors, including sophisticated threats like tool poisoning, we present\nactionable security patterns tailored for MCP implementers and adopters. The\nprimary contribution of this research lies in translating theoretical security\nconcerns into a practical, implementable framework with actionable controls,\nthereby providing essential guidance for the secure enterprise adoption and\ngovernance of integrated AI systems."
    },
    {
        "date": "2025-04",
        "title": "A Hybrid Chaos-Based Cryptographic Framework for Post-Quantum Secure Communications",
        "author": "Kevin Song, Noorullah Imran, Jake Y. Chen, and Allan C. Dobbins",
        "link": "http://arxiv.org/abs/2504.08618v1",
        "abstract": "We present CryptoChaos, a novel hybrid cryptographic framework that\nsynergizes deterministic chaos theory with cutting-edge cryptographic\nprimitives to achieve robust, post-quantum resilient encryption. CryptoChaos\nharnesses the intrinsic unpredictability of four discrete chaotic maps\n(Logistic, Chebyshev, Tent, and Henon) to generate a high-entropy,\nmultidimensional key from a unified entropy pool. This key is derived through a\nlayered process that combines SHA3-256 hashing with an ephemeral X25519\nDiffie-Hellman key exchange and is refined using an HMAC-based key derivation\nfunction (HKDF). The resulting encryption key powers AES-GCM, providing both\nconfidentiality and integrity. Comprehensive benchmarking against established\nsymmetric ciphers confirms that CryptoChaos attains near-maximal Shannon\nentropy (approximately 8 bits per byte) and exhibits negligible adjacent-byte\ncorrelations, while robust performance on the NIST SP 800-22 test suite\nunderscores its statistical rigor. Moreover, quantum simulations demonstrate\nthat the additional complexity inherent in chaotic key generation dramatically\nelevates the resource requirements for Grover-based quantum attacks, with an\nestimated T gate count of approximately 2.1 x 10^9. The modular and\ninteroperable design of CryptoChaos positions it as a promising candidate for\nhigh-assurance applications, ranging from secure communications and financial\ntransactions to IoT systems, paving the way for next-generation post-quantum\nencryption standards."
    },
    {
        "date": "2025-04",
        "title": "Knowledge Distillation for Multimodal Egocentric Action Recognition Robust to Missing Modalities",
        "author": "Maria Santos-Villafranca, Dustin Carri\u00f3n-Ojeda, Alejandro Perez-Yus, Jesus Bermudez-Cameo, Jose J. Guerrero, and Simone Schaub-Meyer",
        "link": "http://arxiv.org/abs/2504.08578v1",
        "abstract": "Action recognition is an essential task in egocentric vision due to its wide\nrange of applications across many fields. While deep learning methods have been\nproposed to address this task, most rely on a single modality, typically video.\nHowever, including additional modalities may improve the robustness of the\napproaches to common issues in egocentric videos, such as blurriness and\nocclusions. Recent efforts in multimodal egocentric action recognition often\nassume the availability of all modalities, leading to failures or performance\ndrops when any modality is missing. To address this, we introduce an efficient\nmultimodal knowledge distillation approach for egocentric action recognition\nthat is robust to missing modalities (KARMMA) while still benefiting when\nmultiple modalities are available. Our method focuses on resource-efficient\ndevelopment by leveraging pre-trained models as unimodal feature extractors in\nour teacher model, which distills knowledge into a much smaller and faster\nstudent model. Experiments on the Epic-Kitchens and Something-Something\ndatasets demonstrate that our student model effectively handles missing\nmodalities while reducing its accuracy drop in this scenario."
    },
    {
        "date": "2025-04",
        "title": "Toward Realistic Adversarial Attacks in IDS: A Novel Feasibility Metric for Transferability",
        "author": "Sabrine Ennaji, Elhadj Benkhelifa, and Luigi Vincenzo Mancini",
        "link": "http://arxiv.org/abs/2504.08480v1",
        "abstract": "Transferability-based adversarial attacks exploit the ability of adversarial\nexamples, crafted to deceive a specific source Intrusion Detection System (IDS)\nmodel, to also mislead a target IDS model without requiring access to the\ntraining data or any internal model parameters. These attacks exploit common\nvulnerabilities in machine learning models to bypass security measures and\ncompromise systems. Although the transferability concept has been widely\nstudied, its practical feasibility remains limited due to assumptions of high\nsimilarity between source and target models. This paper analyzes the core\nfactors that contribute to transferability, including feature alignment, model\narchitectural similarity, and overlap in the data distributions that each IDS\nexamines. We propose a novel metric, the Transferability Feasibility Score\n(TFS), to assess the feasibility and reliability of such attacks based on these\nfactors. Through experimental evidence, we demonstrate that TFS and actual\nattack success rates are highly correlated, addressing the gap between\ntheoretical understanding and real-world impact. Our findings provide needed\nguidance for designing more realistic transferable adversarial attacks,\ndeveloping robust defenses, and ultimately improving the security of machine\nlearning-based IDS in critical systems."
    },
    {
        "date": "2025-04",
        "title": "On Transfer-based Universal Attacks in Pure Black-box Setting",
        "author": "Mohammad A. A. K. Jalwana, Naveed Akhtar, Ajmal Mian, Nazanin Rahnavard, and Mubarak Shah",
        "link": "http://arxiv.org/abs/2504.08866v1",
        "abstract": "Despite their impressive performance, deep visual models are susceptible to\ntransferable black-box adversarial attacks. Principally, these attacks craft\nperturbations in a target model-agnostic manner. However, surprisingly, we find\nthat existing methods in this domain inadvertently take help from various\npriors that violate the black-box assumption such as the availability of the\ndataset used to train the target model, and the knowledge of the number of\nclasses in the target model. Consequently, the literature fails to articulate\nthe true potency of transferable black-box attacks. We provide an empirical\nstudy of these biases and propose a framework that aids in a prior-free\ntransparent study of this paradigm. Using our framework, we analyze the role of\nprior knowledge of the target model data and number of classes in attack\nperformance. We also provide several interesting insights based on our\nanalysis, and demonstrate that priors cause overestimation in transferability\nscores. Finally, we extend our framework to query-based attacks. This extension\ninspires a novel image-blending technique to prepare data for effective\nsurrogate model training."
    },
    {
        "date": "2025-04",
        "title": "Adversarial Examples in Environment Perception for Automated Driving (Review)",
        "author": "Jun Yan, and Huilin Yin",
        "link": "http://arxiv.org/abs/2504.08414v1",
        "abstract": "The renaissance of deep learning has led to the massive development of\nautomated driving. However, deep neural networks are vulnerable to adversarial\nexamples. The perturbations of adversarial examples are imperceptible to human\neyes but can lead to the false predictions of neural networks. It poses a huge\nrisk to artificial intelligence (AI) applications for automated driving. This\nsurvey systematically reviews the development of adversarial robustness\nresearch over the past decade, including the attack and defense methods and\ntheir applications in automated driving. The growth of automated driving pushes\nforward the realization of trustworthy AI applications. This review lists\nsignificant references in the research history of adversarial examples."
    },
    {
        "date": "2025-04",
        "title": "A Knowledge-guided Adversarial Defense for Resisting Malicious Visual Manipulation",
        "author": "Dawei Zhou, Suzhi Gang, Decheng Liu, Tongliang Liu, Nannan Wang, and Xinbo Gao",
        "link": "http://arxiv.org/abs/2504.08411v1",
        "abstract": "Malicious applications of visual manipulation have raised serious threats to\nthe security and reputation of users in many fields. To alleviate these issues,\nadversarial noise-based defenses have been enthusiastically studied in recent\nyears. However, ``data-only\" methods tend to distort fake samples in the\nlow-level feature space rather than the high-level semantic space, leading to\nlimitations in resisting malicious manipulation. Frontier research has shown\nthat integrating knowledge in deep learning can produce reliable and\ngeneralizable solutions. Inspired by these, we propose a knowledge-guided\nadversarial defense (KGAD) to actively force malicious manipulation models to\noutput semantically confusing samples. Specifically, in the process of\ngenerating adversarial noise, we focus on constructing significant semantic\nconfusions at the domain-specific knowledge level, and exploit a metric closely\nrelated to visual perception to replace the general pixel-wise metrics. The\ngenerated adversarial noise can actively interfere with the malicious\nmanipulation model by triggering knowledge-guided and perception-related\ndisruptions in the fake samples. To validate the effectiveness of the proposed\nmethod, we conduct qualitative and quantitative experiments on human perception\nand visual quality assessment. The results on two different tasks both show\nthat our defense provides better protection compared to state-of-the-art\nmethods and achieves great generalizability."
    },
    {
        "date": "2025-04",
        "title": "Towards Efficient and Robust Moment Retrieval System: A Unified Framework for Multi-Granularity Models and Temporal Reranking",
        "author": "Huu-Loc Tran, Tinh-Anh Nguyen-Nhu, Huu-Phong Phan-Nguyen, Tien-Huy Nguyen, Nhat-Minh Nguyen-Dich, Anh Dao, Huy-Duc Do, Quan Nguyen, Hoang M. Le, and Quang-Vinh Dinh",
        "link": "http://arxiv.org/abs/2504.08384v1",
        "abstract": "Long-form video understanding presents significant challenges for interactive\nretrieval systems, as conventional methods struggle to process extensive video\ncontent efficiently. Existing approaches often rely on single models,\ninefficient storage, unstable temporal search, and context-agnostic reranking,\nlimiting their effectiveness. This paper presents a novel framework to enhance\ninteractive video retrieval through four key innovations: (1) an ensemble\nsearch strategy that integrates coarse-grained (CLIP) and fine-grained (BEIT3)\nmodels to improve retrieval accuracy, (2) a storage optimization technique that\nreduces redundancy by selecting representative keyframes via TransNetV2 and\ndeduplication, (3) a temporal search mechanism that localizes video segments\nusing dual queries for start and end points, and (4) a temporal reranking\napproach that leverages neighboring frame context to stabilize rankings.\nEvaluated on known-item search and question-answering tasks, our framework\ndemonstrates substantial improvements in retrieval precision, efficiency, and\nuser interpretability, offering a robust solution for real-world interactive\nvideo retrieval applications."
    },
    {
        "date": "2025-04",
        "title": "Practical Secure Aggregation by Combining Cryptography and Trusted Execution Environments",
        "author": "Romain de Laage, Peterson Yuhala, Fran\u00e7ois-Xavier Wicht, Pascal Felber, Christian Cachin, and Valerio Schiavoni",
        "link": "http://arxiv.org/abs/2504.08325v1",
        "abstract": "Secure aggregation enables a group of mutually distrustful parties, each\nholding private inputs, to collaboratively compute an aggregate value while\npreserving the privacy of their individual inputs. However, a major challenge\nin adopting secure aggregation approaches for practical applications is the\nsignificant computational overhead of the underlying cryptographic protocols,\ne.g. fully homomorphic encryption. This overhead makes secure aggregation\nprotocols impractical, especially for large datasets. In contrast,\nhardware-based security techniques such as trusted execution environments\n(TEEs) enable computation at near-native speeds, making them a promising\nalternative for reducing the computational burden typically associated with\npurely cryptographic techniques. Yet, in many scenarios, parties may opt for\neither cryptographic or hardware-based security mechanisms, highlighting the\nneed for hybrid approaches. In this work, we introduce several secure\naggregation architectures that integrate both cryptographic and TEE-based\ntechniques, analyzing the trade-offs between security and performance."
    },
    {
        "date": "2025-04",
        "title": "To See or Not to See -- Fingerprinting Devices in Adversarial Environments Amid Advanced Machine Learning",
        "author": "Justin Feng, and Nader Sehatbakhsh",
        "link": "http://arxiv.org/abs/2504.08264v1",
        "abstract": "The increasing use of the Internet of Things raises security concerns. To\naddress this, device fingerprinting is often employed to authenticate devices,\ndetect adversaries, and identify eavesdroppers in an environment. This requires\nthe ability to discern between legitimate and malicious devices which is\nachieved by analyzing the unique physical and/or operational characteristics of\nIoT devices. In the era of the latest progress in machine learning,\nparticularly generative models, it is crucial to methodically examine the\ncurrent studies in device fingerprinting. This involves explaining their\napproaches and underscoring their limitations when faced with adversaries armed\nwith these ML tools. To systematically analyze existing methods, we propose a\ngeneric, yet simplified, model for device fingerprinting. Additionally, we\nthoroughly investigate existing methods to authenticate devices and detect\neavesdropping, using our proposed model. We further study trends and\nsimilarities between works in authentication and eavesdropping detection and\npresent the existing threats and attacks in these domains. Finally, we discuss\nfuture directions in fingerprinting based on these trends to develop more\nsecure IoT fingerprinting schemes."
    },
    {
        "date": "2025-04",
        "title": "Hardware Design and Security Needs Attention: From Survey to Path Forward",
        "author": "Sujan Ghimire, Muhtasim Alam Chowdhury, Banafsheh Saber Latibari, Muntasir Mamun, Jaeden Wolf Carpenter, Benjamin Tan, Hammond Pearce, Pratik Satam, and Soheil Salehi",
        "link": "http://arxiv.org/abs/2504.08854v1",
        "abstract": "Recent advances in attention-based artificial intelligence (AI) models have\nunlocked vast potential to automate digital hardware design while enhancing and\nstrengthening security measures against various threats. This rapidly emerging\nfield leverages Large Language Models (LLMs) to generate HDL code, identify\nvulnerabilities, and sometimes mitigate them. The state of the art in this\ndesign automation space utilizes optimized LLMs with HDL datasets, creating\nautomated systems for register-transfer level (RTL) generation, verification,\nand debugging, and establishing LLM-driven design environments for streamlined\nlogic designs. Additionally, attention-based models like graph attention have\nshown promise in chip design applications, including floorplanning. This survey\ninvestigates the integration of these models into hardware-related domains,\nemphasizing logic design and hardware security, with or without the use of IP\nlibraries. This study explores the commercial and academic landscape,\nhighlighting technical hurdles and future prospects for automating hardware\ndesign and security. Moreover, it provides new insights into the study of\nLLM-driven design systems, advances in hardware security mechanisms, and the\nimpact of influential works on industry practices. Through the examination of\n30 representative approaches and illustrative case studies, this paper\nunderscores the transformative potential of attention-based models in\nrevolutionizing hardware design while addressing the challenges that lie ahead\nin this interdisciplinary domain."
    },
    {
        "date": "2025-04",
        "title": "DaemonSec: Examining the Role of Machine Learning for Daemon Security in Linux Environments",
        "author": "Sheikh Muhammad Farjad",
        "link": "http://arxiv.org/abs/2504.08227v1",
        "abstract": "DaemonSec is an early-stage startup exploring machine learning (ML)-based\nsecurity for Linux daemons, a critical yet often overlooked attack surface.\nWhile daemon security remains underexplored, conventional defenses struggle\nagainst adaptive threats and zero-day exploits. To assess the perspectives of\nIT professionals on ML-driven daemon protection, a systematic interview study\nbased on semi-structured interviews was conducted with 22 professionals from\nindustry and academia. The study evaluates adoption, feasibility, and trust in\nML-based security solutions. While participants recognized the potential of ML\nfor real-time anomaly detection, findings reveal skepticism toward full\nautomation, limited security awareness among non-security roles, and concerns\nabout patching delays creating attack windows. This paper presents the methods,\nkey findings, and implications for advancing ML-driven daemon security in\nindustry."
    },
    {
        "date": "2025-04",
        "title": "EO-VLM: VLM-Guided Energy Overload Attacks on Vision Models",
        "author": "Minjae Seo, Myoungsung You, Junhee Lee, Jaehan Kim, Hwanjo Heo, Jintae Oh, and Jinwoo Kim",
        "link": "http://arxiv.org/abs/2504.08205v1",
        "abstract": "Vision models are increasingly deployed in critical applications such as\nautonomous driving and CCTV monitoring, yet they remain susceptible to\nresource-consuming attacks. In this paper, we introduce a novel\nenergy-overloading attack that leverages vision language model (VLM) prompts to\ngenerate adversarial images targeting vision models. These images, though\nimperceptible to the human eye, significantly increase GPU energy consumption\nacross various vision models, threatening the availability of these systems.\nOur framework, EO-VLM (Energy Overload via VLM), is model-agnostic, meaning it\nis not limited by the architecture or type of the target vision model. By\nexploiting the lack of safety filters in VLMs like DALL-E 3, we create\nadversarial noise images without requiring prior knowledge or internal\nstructure of the target vision models. Our experiments demonstrate up to a 50%\nincrease in energy consumption, revealing a critical vulnerability in current\nvision models."
    },
    {
        "date": "2025-04",
        "title": "A Piecewise Lyapunov Analysis of Sub-quadratic SGD: Applications to Robust and Quantile Regression",
        "author": "Yixuan Zhang, Dongyan Huo, Yudong Chen, and Qiaomin Xie",
        "link": "http://arxiv.org/abs/2504.08178v3",
        "abstract": "Motivated by robust and quantile regression problems, we investigate the\nstochastic gradient descent (SGD) algorithm for minimizing an objective\nfunction $f$ that is locally strongly convex with a sub--quadratic tail. This\nsetting covers many widely used online statistical methods. We introduce a\nnovel piecewise Lyapunov function that enables us to handle functions $f$ with\nonly first-order differentiability, which includes a wide range of popular loss\nfunctions such as Huber loss. Leveraging our proposed Lyapunov function, we\nderive finite-time moment bounds under general diminishing stepsizes, as well\nas constant stepsizes. We further establish the weak convergence, central limit\ntheorem and bias characterization under constant stepsize, providing the first\ngeometrical convergence result for sub--quadratic SGD. Our results have wide\napplications, especially in online statistical methods. In particular, we\ndiscuss two applications of our results. 1) Online robust regression: We\nconsider a corrupted linear model with sub--exponential covariates and\nheavy--tailed noise. Our analysis provides convergence rates comparable to\nthose for corrupted models with Gaussian covariates and noise. 2) Online\nquantile regression: Importantly, our results relax the common assumption in\nprior work that the conditional density is continuous and provide a more\nfine-grained analysis for the moment bounds."
    },
    {
        "date": "2025-04",
        "title": "GenXSS: an AI-Driven Framework for Automated Detection of XSS Attacks in WAFs",
        "author": "Vahid Babaey, and Arun Ravindran",
        "link": "http://arxiv.org/abs/2504.08176v1",
        "abstract": "The increasing reliance on web services has led to a rise in cybersecurity\nthreats, particularly Cross-Site Scripting (XSS) attacks, which target\nclient-side layers of web applications by injecting malicious scripts.\nTraditional Web Application Firewalls (WAFs) struggle to detect highly\nobfuscated and complex attacks, as their rules require manual updates. This\npaper presents a novel generative AI framework that leverages Large Language\nModels (LLMs) to enhance XSS mitigation. The framework achieves two primary\nobjectives: (1) generating sophisticated and syntactically validated XSS\npayloads using in-context learning, and (2) automating defense mechanisms by\ntesting these attacks against a vulnerable application secured by a WAF,\nclassifying bypassing attacks, and generating effective WAF security rules.\nExperimental results using GPT-4o demonstrate the framework's effectiveness\ngenerating 264 XSS payloads, 83% of which were validated, with 80% bypassing\nModSecurity WAF equipped with an industry standard security rule set developed\nby the Open Web Application Security Project (OWASP) to protect against web\nvulnerabilities. Through rule generation, 86% of previously successful attacks\nwere blocked using only 15 new rules. In comparison, Google Gemini Pro achieved\na lower bypass rate of 63%, highlighting performance differences across LLMs."
    },
    {
        "date": "2025-04",
        "title": "AttentionDefense: Leveraging System Prompt Attention for Explainable Defense Against Novel Jailbreaks",
        "author": "Charlotte Siska, and Anush Sankaran",
        "link": "http://arxiv.org/abs/2504.12321v1",
        "abstract": "In the past few years, Language Models (LMs) have shown par-human\ncapabilities in several domains. Despite their practical applications and\nexceeding user consumption, they are susceptible to jailbreaks when malicious\ninput exploits the LM's weaknesses, causing it to deviate from its intended\nbehavior. Current defensive strategies either classify the input prompt as\nadversarial or prevent LMs from generating harmful outputs. However, it is\nchallenging to explain the reason behind the malicious nature of the jailbreak,\nwhich results in a wide variety of closed-box approaches. In this research, we\npropose and demonstrate that system-prompt attention from Small Language Models\n(SLMs) can be used to characterize adversarial prompts, providing a novel,\nexplainable, and cheaper defense approach called AttentionDefense. Our research\nsuggests that the attention mechanism is an integral component in understanding\nand explaining how LMs respond to malicious input that is not captured in the\nsemantic meaning of text embeddings. The proposed AttentionDefense is evaluated\nagainst existing jailbreak benchmark datasets. Ablation studies show that\nSLM-based AttentionDefense has equivalent or better jailbreak detection\nperformance compared to text embedding-based classifiers and GPT-4 zero-shot\ndetectors.To further validate the efficacy of the proposed approach, we\ngenerate a dataset of novel jailbreak variants of the existing benchmark\ndataset using a closed-loop LLM-based multi-agent system. We demonstrate that\nthe proposed AttentionDefense approach performs robustly on this novel\njailbreak dataset while existing approaches suffer in performance.\nAdditionally, for practical purposes AttentionDefense is an ideal solution as\nit has the computation requirements of a small LM but the performance of a LLM\ndetector."
    },
    {
        "date": "2025-04",
        "title": "ColorBench: Can VLMs See and Understand the Colorful World? A Comprehensive Benchmark for Color Perception, Reasoning, and Robustness",
        "author": "Yijun Liang, Ming Li, Chenrui Fan, Ziyue Li, Dang Nguyen, Kwesi Cobbina, Shweta Bhardwaj, Jiuhai Chen, Fuxiao Liu, and Tianyi Zhou",
        "link": "http://arxiv.org/abs/2504.10514v1",
        "abstract": "Color plays an important role in human perception and usually provides\ncritical clues in visual reasoning. However, it is unclear whether and how\nvision-language models (VLMs) can perceive, understand, and leverage color as\nhumans. This paper introduces ColorBench, an innovative benchmark meticulously\ncrafted to assess the capabilities of VLMs in color understanding, including\ncolor perception, reasoning, and robustness. By curating a suite of diverse\ntest scenarios, with grounding in real applications, ColorBench evaluates how\nthese models perceive colors, infer meanings from color-based cues, and\nmaintain consistent performance under varying color transformations. Through an\nextensive evaluation of 32 VLMs with varying language models and vision\nencoders, our paper reveals some undiscovered findings: (i) The scaling law\n(larger models are better) still holds on ColorBench, while the language model\nplays a more important role than the vision encoder. (ii) However, the\nperformance gaps across models are relatively small, indicating that color\nunderstanding has been largely neglected by existing VLMs. (iii) CoT reasoning\nimproves color understanding accuracies and robustness, though they are\nvision-centric tasks. (iv) Color clues are indeed leveraged by VLMs on\nColorBench but they can also mislead models in some tasks. These findings\nhighlight the critical limitations of current VLMs and underscore the need to\nenhance color comprehension. Our ColorBenchcan serve as a foundational tool for\nadvancing the study of human-level color understanding of multimodal AI."
    },
    {
        "date": "2025-04",
        "title": "Benchmarking Adversarial Robustness to Bias Elicitation in Large Language Models: Scalable Automated Assessment with LLM-as-a-Judge",
        "author": "Riccardo Cantini, Alessio Orsino, Massimo Ruggiero, and Domenico Talia",
        "link": "http://arxiv.org/abs/2504.07887v1",
        "abstract": "Large Language Models (LLMs) have revolutionized artificial intelligence,\ndriving advancements in machine translation, summarization, and conversational\nagents. However, their increasing integration into critical societal domains\nhas raised concerns about embedded biases, which can perpetuate stereotypes and\ncompromise fairness. These biases stem from various sources, including\nhistorical inequalities in training data, linguistic imbalances, and\nadversarial manipulation. Despite mitigation efforts, recent studies indicate\nthat LLMs remain vulnerable to adversarial attacks designed to elicit biased\nresponses. This work proposes a scalable benchmarking framework to evaluate LLM\nrobustness against adversarial bias elicitation. Our methodology involves (i)\nsystematically probing models with a multi-task approach targeting biases\nacross various sociocultural dimensions, (ii) quantifying robustness through\nsafety scores using an LLM-as-a-Judge approach for automated assessment of\nmodel responses, and (iii) employing jailbreak techniques to investigate\nvulnerabilities in safety mechanisms. Our analysis examines prevalent biases in\nboth small and large state-of-the-art models and their impact on model safety.\nAdditionally, we assess the safety of domain-specific models fine-tuned for\ncritical fields, such as medicine. Finally, we release a curated dataset of\nbias-related prompts, CLEAR-Bias, to facilitate systematic vulnerability\nbenchmarking. Our findings reveal critical trade-offs between model size and\nsafety, aiding the development of fairer and more robust future language\nmodels."
    },
    {
        "date": "2025-04",
        "title": "QubitHammer Attacks: Qubit Flipping Attacks in Multi-tenant Superconducting Quantum Computers",
        "author": "Yizhuo Tan, Navnil Choudhury, Kanad Basu, and Jakub Szefer",
        "link": "http://arxiv.org/abs/2504.07875v1",
        "abstract": "Quantum computing is rapidly evolving its capabilities, with a corresponding\nsurge in its deployment within cloud-based environments. Various quantum\ncomputers are accessible today via pay-as-you-go cloud computing models,\noffering unprecedented convenience. Due to its rapidly growing demand, quantum\ncomputers are shifting from a single-tenant to a multi-tenant model to enhance\nresource utilization. However, this widespread accessibility to shared\nmulti-tenant systems also introduces potential security vulnerabilities. In\nthis work, we present for the first time a set of novel attacks, named together\nas the QubitHammer attacks, which target state-of-the-art superconducting\nquantum computers. We show that in a multi-tenant cloud-based quantum system,\nan adversary with the basic capability to deploy custom pulses, similar to any\nstandard user today, can utilize the QubitHammer attacks to significantly\ndegrade the fidelity of victim circuits located on the same quantum computer.\nUpon extensive evaluation, the QubitHammer attacks achieve a very high\nvariational distance of up to 0.938 from the expected outcome, thus\ndemonstrating their potential to degrade victim computation. Our findings\nexhibit the effectiveness of these attacks across various superconducting\nquantum computers from a leading vendor, suggesting that QubitHammer represents\na new class of security attacks. Further, the attacks are demonstrated to\nbypass all existing defenses proposed so far for ensuring the reliability in\nmulti-tenant superconducting quantum computers."
    },
    {
        "date": "2025-04",
        "title": "Robust Hallucination Detection in LLMs via Adaptive Token Selection",
        "author": "Mengjia Niu, Hamed Haddadi, and Guansong Pang",
        "link": "http://arxiv.org/abs/2504.07863v1",
        "abstract": "Hallucinations in large language models (LLMs) pose significant safety\nconcerns that impede their broader deployment. Recent research in hallucination\ndetection has demonstrated that LLMs' internal representations contain\ntruthfulness hints, which can be harnessed for detector training. However, the\nperformance of these detectors is heavily dependent on the internal\nrepresentations of predetermined tokens, fluctuating considerably when working\non free-form generations with varying lengths and sparse distributions of\nhallucinated entities. To address this, we propose HaMI, a novel approach that\nenables robust detection of hallucinations through adaptive selection and\nlearning of critical tokens that are most indicative of hallucinations. We\nachieve this robustness by an innovative formulation of the Hallucination\ndetection task as Multiple Instance (HaMI) learning over token-level\nrepresentations within a sequence, thereby facilitating a joint optimisation of\ntoken selection and hallucination detection on generation sequences of diverse\nforms. Comprehensive experimental results on four hallucination benchmarks show\nthat HaMI significantly outperforms existing state-of-the-art approaches."
    },
    {
        "date": "2025-04",
        "title": "Counting Hours, Counting Losses: The Toll of Unpredictable Work Schedules on Financial Security",
        "author": "Pegah Nokhiz, Aravinda Kanchana Ruwanpathirana, Aditya Bhaskara, and Suresh Venkatasubramanian",
        "link": "http://arxiv.org/abs/2504.07719v1",
        "abstract": "Financial instability has become a significant issue in today's society.\nWhile research typically focuses on financial aspects, there is a tendency to\noverlook time-related aspects of unstable work schedules. The inability to rely\non consistent work schedules leads to burnout, work-family conflicts, and\nfinancial shocks that directly impact workers' income and assets. Unforeseen\nfluctuations in earnings pose challenges in financial planning, affecting\ndecisions on savings and spending and ultimately undermining individuals'\nlong-term financial stability and well-being.\n  This issue is particularly evident in sectors where workers experience\nfrequently changing schedules without sufficient notice, including those in the\nfood service and retail sectors, part-time and hourly workers, and individuals\nwith lower incomes. These groups are already more financially vulnerable, and\nthe unpredictable nature of their schedules exacerbates their financial\nfragility.\n  Our objective is to understand how unforeseen fluctuations in earnings\nexacerbate financial fragility by investigating the extent to which\nindividuals' financial management depends on their ability to anticipate and\nplan for the future. To address this question, we develop a simulation\nframework that models how individuals optimize utility amidst financial\nuncertainty and the imperative to avoid financial ruin. We employ online\nlearning techniques, specifically adapting workers' consumption policies based\non evolving information about their work schedules.\n  With this framework, we show both theoretically and empirically how a\nworker's capacity to anticipate schedule changes enhances their long-term\nutility. Conversely, the inability to predict future events can worsen workers'\ninstability. Moreover, our framework enables us to explore interventions to\nmitigate the problem of schedule uncertainty and evaluate their effectiveness."
    },
    {
        "date": "2025-04",
        "title": "PR-Attack: Coordinated Prompt-RAG Attacks on Retrieval-Augmented Generation in Large Language Models via Bilevel Optimization",
        "author": "Yang Jiao, Xiaodong Wang, and Kai Yang",
        "link": "http://arxiv.org/abs/2504.07717v2",
        "abstract": "Large Language Models (LLMs) have demonstrated remarkable performance across\na wide range of applications, e.g., medical question-answering, mathematical\nsciences, and code generation. However, they also exhibit inherent limitations,\nsuch as outdated knowledge and susceptibility to hallucinations.\nRetrieval-Augmented Generation (RAG) has emerged as a promising paradigm to\naddress these issues, but it also introduces new vulnerabilities. Recent\nefforts have focused on the security of RAG-based LLMs, yet existing attack\nmethods face three critical challenges: (1) their effectiveness declines\nsharply when only a limited number of poisoned texts can be injected into the\nknowledge database, (2) they lack sufficient stealth, as the attacks are often\ndetectable by anomaly detection systems, which compromises their effectiveness,\nand (3) they rely on heuristic approaches to generate poisoned texts, lacking\nformal optimization frameworks and theoretic guarantees, which limits their\neffectiveness and applicability. To address these issues, we propose\ncoordinated Prompt-RAG attack (PR-attack), a novel optimization-driven attack\nthat introduces a small number of poisoned texts into the knowledge database\nwhile embedding a backdoor trigger within the prompt. When activated, the\ntrigger causes the LLM to generate pre-designed responses to targeted queries,\nwhile maintaining normal behavior in other contexts. This ensures both high\neffectiveness and stealth. We formulate the attack generation process as a\nbilevel optimization problem leveraging a principled optimization framework to\ndevelop optimal poisoned texts and triggers. Extensive experiments across\ndiverse LLMs and datasets demonstrate the effectiveness of PR-Attack, achieving\na high attack success rate even with a limited number of poisoned texts and\nsignificantly improved stealth compared to existing methods."
    },
    {
        "date": "2025-04",
        "title": "RASMD: RGB And SWIR Multispectral Driving Dataset for Robust Perception in Adverse Conditions",
        "author": "Youngwan Jin, Michal Kovac, Yagiz Nalcakan, Hyeongjin Ju, Hanbin Song, Sanghyeop Yeo, and Shiho Kim",
        "link": "http://arxiv.org/abs/2504.07603v1",
        "abstract": "Current autonomous driving algorithms heavily rely on the visible spectrum,\nwhich is prone to performance degradation in adverse conditions like fog, rain,\nsnow, glare, and high contrast. Although other spectral bands like\nnear-infrared (NIR) and long-wave infrared (LWIR) can enhance vision perception\nin such situations, they have limitations and lack large-scale datasets and\nbenchmarks. Short-wave infrared (SWIR) imaging offers several advantages over\nNIR and LWIR. However, no publicly available large-scale datasets currently\nincorporate SWIR data for autonomous driving. To address this gap, we introduce\nthe RGB and SWIR Multispectral Driving (RASMD) dataset, which comprises 100,000\nsynchronized and spatially aligned RGB-SWIR image pairs collected across\ndiverse locations, lighting, and weather conditions. In addition, we provide a\nsubset for RGB-SWIR translation and object detection annotations for a subset\nof challenging traffic scenarios to demonstrate the utility of SWIR imaging\nthrough experiments on both object detection and RGB-to-SWIR image translation.\nOur experiments show that combining RGB and SWIR data in an ensemble framework\nsignificantly improves detection accuracy compared to RGB-only approaches,\nparticularly in conditions where visible-spectrum sensors struggle. We\nanticipate that the RASMD dataset will advance research in multispectral\nimaging for autonomous driving and robust perception systems."
    },
    {
        "date": "2025-04",
        "title": "DWFS-Obfuscation: Dynamic Weighted Feature Selection for Robust Malware Familial Classification under Obfuscation",
        "author": "Xingyuan Wei, Zijun Cheng, Ning Li, Qiujian Lv, Ziyang Yu, and Degang Sun",
        "link": "http://arxiv.org/abs/2504.07590v1",
        "abstract": "Due to its open-source nature, the Android operating system has consistently\nbeen a primary target for attackers. Learning-based methods have made\nsignificant progress in the field of Android malware detection. However,\ntraditional detection methods based on static features struggle to identify\nobfuscated malicious code, while methods relying on dynamic analysis suffer\nfrom low efficiency. To address this, we propose a dynamic weighted feature\nselection method that analyzes the importance and stability of features,\ncalculates scores to filter out the most robust features, and combines these\nselected features with the program's structural information. We then utilize\ngraph neural networks for classification, thereby improving the robustness and\naccuracy of the detection system. We analyzed 8,664 malware samples from eight\nmalware families and tested a total of 44,940 malware variants generated using\nseven obfuscation strategies. Experiments demonstrate that our proposed method\nachieves an F1-score of 95.56% on the unobfuscated dataset and 92.28% on the\nobfuscated dataset, indicating that the model can effectively detect obfuscated\nmalware."
    },
    {
        "date": "2025-04",
        "title": "MUFFLER: Secure Tor Traffic Obfuscation with Dynamic Connection Shuffling and Splitting",
        "author": "Minjae Seo, Myoungsung You, Jaehan Kim, Taejune Park, Seungwon Shin, and Jinwoo Kim",
        "link": "http://arxiv.org/abs/2504.07543v2",
        "abstract": "Tor, a widely utilized privacy network, enables anonymous communication but\nis vulnerable to flow correlation attacks that deanonymize users by correlating\ntraffic patterns from Tor's ingress and egress segments. Various defenses have\nbeen developed to mitigate these attacks; however, they have two critical\nlimitations: (i) significant network overhead during obfuscation and (ii) a\nlack of dynamic obfuscation for egress segments, exposing traffic patterns to\nadversaries. In response, we introduce MUFFLER, a novel connection-level\ntraffic obfuscation system designed to secure Tor egress traffic. It\ndynamically maps real connections to a distinct set of virtual connections\nbetween the final Tor nodes and targeted services, either public or hidden.\nThis approach creates egress traffic patterns fundamentally different from\nthose at ingress segments without adding intentional padding bytes or timing\ndelays. The mapping of real and virtual connections is adjusted in real-time\nbased on ongoing network conditions, thwarting adversaries' efforts to detect\negress traffic patterns. Extensive evaluations show that MUFFLER mitigates\npowerful correlation attacks with a TPR of 1% at an FPR of 10^-2 while imposing\nonly a 2.17% bandwidth overhead. Moreover, it achieves up to 27x lower latency\noverhead than existing solutions and seamlessly integrates with the current Tor\narchitecture."
    },
    {
        "date": "2025-04",
        "title": "Adversarial Subspace Generation for Outlier Detection in High-Dimensional Data",
        "author": "Jose Cribeiro-Ramallo, Federico Matteucci, Paul Enciu, Alexander Jenke, Vadim Arzamasov, Thorsten Strufe, and Klemens B\u00f6hm",
        "link": "http://arxiv.org/abs/2504.07522v1",
        "abstract": "Outlier detection in high-dimensional tabular data is challenging since data\nis often distributed across multiple lower-dimensional subspaces -- a\nphenomenon known as the Multiple Views effect (MV). This effect led to a large\nbody of research focused on mining such subspaces, known as subspace selection.\nHowever, as the precise nature of the MV effect was not well understood,\ntraditional methods had to rely on heuristic-driven search schemes that\nstruggle to accurately capture the true structure of the data. Properly\nidentifying these subspaces is critical for unsupervised tasks such as outlier\ndetection or clustering, where misrepresenting the underlying data structure\ncan hinder the performance. We introduce Myopic Subspace Theory (MST), a new\ntheoretical framework that mathematically formulates the Multiple Views effect\nand writes subspace selection as a stochastic optimization problem. Based on\nMST, we introduce V-GAN, a generative method trained to solve such an\noptimization problem. This approach avoids any exhaustive search over the\nfeature space while ensuring that the intrinsic data structure is preserved.\nExperiments on 42 real-world datasets show that using V-GAN subspaces to build\nensemble methods leads to a significant increase in one-class classification\nperformance -- compared to existing subspace selection, feature selection, and\nembedding methods. Further experiments on synthetic data show that V-GAN\nidentifies subspaces more accurately while scaling better than other relevant\nsubspace selection methods. These results confirm the theoretical guarantees of\nour approach and also highlight its practical viability in high-dimensional\nsettings."
    },
    {
        "date": "2025-04",
        "title": "Intelligent DoS and DDoS Detection: A Hybrid GRU-NTM Approach to Network Security",
        "author": "Caroline Panggabean, Chandrasekar Venkatachalam, Priyanka Shah, Sincy John, Renuka Devi P, and Shanmugavalli Venkatachalam",
        "link": "http://arxiv.org/abs/2504.07478v1",
        "abstract": "Detecting Denial of Service (DoS) and Distributed Denial of Service (DDoS)\nattacks remains a critical challenge in cybersecurity. This research introduces\na hybrid deep learning model combining Gated Recurrent Units (GRUs) and a\nNeural Turing Machine (NTM) for enhanced intrusion detection. Trained on the\nUNSW-NB15 and BoT-IoT datasets, the model employs GRU layers for sequential\ndata processing and an NTM for long-term pattern recognition. The proposed\napproach achieves 99% accuracy in distinguishing between normal, DoS, and DDoS\ntraffic. These findings offer promising advancements in real-time threat\ndetection and contribute to improved network security across various domains."
    },
    {
        "date": "2025-04",
        "title": "WS-DETR: Robust Water Surface Object Detection through Vision-Radar Fusion with Detection Transformer",
        "author": "Huilin Yin, Pengyu Wang, Senmao Li, Jun Yan, and Daniel Watzenig",
        "link": "http://arxiv.org/abs/2504.07441v1",
        "abstract": "Robust object detection for Unmanned Surface Vehicles (USVs) in complex water\nenvironments is essential for reliable navigation and operation. Specifically,\nwater surface object detection faces challenges from blurred edges and diverse\nobject scales. Although vision-radar fusion offers a feasible solution,\nexisting approaches suffer from cross-modal feature conflicts, which negatively\naffect model robustness. To address this problem, we propose a robust\nvision-radar fusion model WS-DETR. In particular, we first introduce a\nMulti-Scale Edge Information Integration (MSEII) module to enhance edge\nperception and a Hierarchical Feature Aggregator (HiFA) to boost multi-scale\nobject detection in the encoder. Then, we adopt self-moving point\nrepresentations for continuous convolution and residual connection to\nefficiently extract irregular features under the scenarios of irregular point\ncloud data. To further mitigate cross-modal conflicts, an Adaptive Feature\nInteractive Fusion (AFIF) module is introduced to integrate visual and radar\nfeatures through geometric alignment and semantic fusion. Extensive experiments\non the WaterScenes dataset demonstrate that WS-DETR achieves state-of-the-art\n(SOTA) performance, maintaining its superiority even under adverse weather and\nlighting conditions."
    },
    {
        "date": "2025-04",
        "title": "Augmented Shuffle Protocols for Accurate and Robust Frequency Estimation under Differential Privacy",
        "author": "Takao Murakami, Yuichi Sei, and Reo Eriguchi",
        "link": "http://arxiv.org/abs/2504.07362v1",
        "abstract": "The shuffle model of DP (Differential Privacy) provides high utility by\nintroducing a shuffler that randomly shuffles noisy data sent from users.\nHowever, recent studies show that existing shuffle protocols suffer from the\nfollowing two major drawbacks. First, they are vulnerable to local data\npoisoning attacks, which manipulate the statistics about input data by sending\ncrafted data, especially when the privacy budget epsilon is small. Second, the\nactual value of epsilon is increased by collusion attacks by the data collector\nand users.\n  In this paper, we address these two issues by thoroughly exploring the\npotential of the augmented shuffle model, which allows the shuffler to perform\nadditional operations, such as random sampling and dummy data addition.\nSpecifically, we propose a generalized framework for local-noise-free protocols\nin which users send (encrypted) input data to the shuffler without adding\nnoise. We show that this generalized protocol provides DP and is robust to the\nabove two attacks if a simpler mechanism that performs the same process on\nbinary input data provides DP. Based on this framework, we propose three\nconcrete protocols providing DP and robustness against the two attacks. Our\nfirst protocol generates the number of dummy values for each item from a\nbinomial distribution and provides higher utility than several state-of-the-art\nexisting shuffle protocols. Our second protocol significantly improves the\nutility of our first protocol by introducing a novel dummy-count distribution:\nasymmetric two-sided geometric distribution. Our third protocol is a special\ncase of our second protocol and provides pure epsilon-DP. We show the\neffectiveness of our protocols through theoretical analysis and comprehensive\nexperiments."
    },
    {
        "date": "2025-04",
        "title": "Electronic Warfare Cyberattacks, Countermeasures and Modern Defensive Strategies of UAV Avionics: A Survey",
        "author": "Aaron Yu, Iuliia Kolotylo, Hashim A. Hashim, and A. E. E. Eltoukhy",
        "link": "http://arxiv.org/abs/2504.07358v1",
        "abstract": "Unmanned Aerial Vehicles (UAVs) play a pivotal role in modern autonomous air\nmobility, and the reliability of UAV avionics systems is critical to ensuring\nmission success, sustainability practices, and public safety. The success of\nUAV missions depends on effectively mitigating various aspects of electronic\nwarfare, including non-destructive and destructive cyberattacks, transponder\nvulnerabilities, and jamming threats, while rigorously implementing\ncountermeasures and defensive aids. This paper provides a comprehensive review\nof UAV cyberattacks, countermeasures, and defensive strategies. It explores\nUAV-to-UAV coordination attacks and their associated features, such as dispatch\nsystem attacks, Automatic Dependent Surveillance-Broadcast (ADS-B) attacks,\nTraffic Alert and Collision Avoidance System (TCAS)-induced collisions, and\nTCAS attacks. Additionally, the paper examines UAV-to-command center\ncoordination attacks, as well as UAV functionality attacks. The review also\ncovers various countermeasures and defensive aids designed for UAVs. Lastly, a\ncomparison of common cyberattacks and countermeasure approaches is conducted,\nalong with a discussion of future trends in the field. Keywords: Electronic\nwarfare, UAVs, Avionics Systems, cyberattacks, coordination attacks,\nfunctionality attacks, countermeasure, defensive-aids."
    },
    {
        "date": "2025-04",
        "title": "Quantum-Inspired Genetic Algorithm for Robust Source Separation in Smart City Acoustics",
        "author": "Minh K. Quan, Mayuri Wijayasundara, Sujeeva Setunge, and Pubudu N. Pathirana",
        "link": "http://arxiv.org/abs/2504.07345v1",
        "abstract": "The cacophony of urban sounds presents a significant challenge for smart city\napplications that rely on accurate acoustic scene analysis. Effectively\nanalyzing these complex soundscapes, often characterized by overlapping sound\nsources, diverse acoustic events, and unpredictable noise levels, requires\nprecise source separation. This task becomes more complicated when only limited\ntraining data is available. This paper introduces a novel Quantum-Inspired\nGenetic Algorithm (p-QIGA) for source separation, drawing inspiration from\nquantum information theory to enhance acoustic scene analysis in smart cities.\nBy leveraging quantum superposition for efficient solution space exploration\nand entanglement to handle correlated sources, p-QIGA achieves robust\nseparation even with limited data. These quantum-inspired concepts are\nintegrated into a genetic algorithm framework to optimize source separation\nparameters. The effectiveness of our approach is demonstrated on two datasets:\nthe TAU Urban Acoustic Scenes 2020 Mobile dataset, representing typical urban\nsoundscapes, and the Silent Cities dataset, capturing quieter urban\nenvironments during the COVID-19 pandemic. Experimental results show that the\np-QIGA achieves accuracy comparable to state-of-the-art methods while\nexhibiting superior resilience to noise and limited training data, achieving up\nto 8.2 dB signal-to-distortion ratio (SDR) in noisy environments and\noutperforming baseline methods by up to 2 dB with only 10% of the training\ndata. This research highlights the potential of p-QIGA to advance acoustic\nsignal processing in smart cities, particularly for noise pollution monitoring\nand acoustic surveillance."
    },
    {
        "date": "2025-04",
        "title": "Prekey Pogo: Investigating Security and Privacy Issues in WhatsApp's Handshake Mechanism",
        "author": "Gabriel K. Gegenhuber, Philipp \u00c9. Frenzel, Maximilian G\u00fcnther, and Aljosha Judmayer",
        "link": "http://arxiv.org/abs/2504.07323v1",
        "abstract": "WhatsApp, the world's largest messaging application, uses a version of the\nSignal protocol to provide end-to-end encryption (E2EE) with strong security\nguarantees, including Perfect Forward Secrecy (PFS). To ensure PFS right from\nthe start of a new conversation -- even when the recipient is offline -- a\nstash of ephemeral (one-time) prekeys must be stored on a server. While the\ncritical role of these one-time prekeys in achieving PFS has been outlined in\nthe Signal specification, we are the first to demonstrate a targeted depletion\nattack against them on individual WhatsApp user devices. Our findings not only\nreveal an attack that can degrade PFS for certain messages, but also expose\ninherent privacy risks and serious availability implications arising from the\nrefilling and distribution procedure essential for this security mechanism."
    },
    {
        "date": "2025-04",
        "title": "Context Switching for Secure Multi-programming of Near-Term Quantum Computers",
        "author": "Avinash Kumar, Meng Wang, Chenxu Liu, Ang Li, Prashant J. Nair, and Poulami Das",
        "link": "http://arxiv.org/abs/2504.07048v3",
        "abstract": "Multi-programming quantum computers improve device utilization and\nthroughput. However, crosstalk from concurrent two-qubit CNOT gates poses\nsecurity risks, compromising the fidelity and output of co-running victim\nprograms. We design Zero Knowledge Tampering Attacks (ZKTAs), using which\nattackers can exploit crosstalk without knowledge of the hardware error\nprofile. ZKTAs can alter victim program outputs in 40% of cases on commercial\nsystems.\n  We identify that ZKTAs succeed because the attacker's program consistently\nruns with the same victim program in a fixed context. To mitigate this, we\npropose QONTEXTS: a context-switching technique that defends against ZKTAs by\nrunning programs across multiple contexts, each handling only a subset of\ntrials. QONTEXTS uses multi-programming with frequent context switching while\nidentifying a unique set of programs for each context. This helps limit only a\nfraction of execution to ZKTAs. We enhance QONTEXTS with attack detection\ncapabilities that compare the distributions from different contexts against\neach other to identify noisy contexts executed with ZKTAs. Our evaluations on\nreal IBMQ systems show that QONTEXTS increases program resilience by three\norders of magnitude and fidelity by 1.33$\\times$ on average. Moreover, QONTEXTS\nimproves throughput by 2$\\times$, advancing security in multi-programmed\nenvironments."
    },
    {
        "date": "2025-04",
        "title": "Efficient Storage Integrity in Adversarial Settings",
        "author": "Quinn Burke, Ryan Sheatsley, Yohan Beugin, Eric Pauley, Owen Hines, Michael Swift, and Patrick McDaniel",
        "link": "http://arxiv.org/abs/2504.07041v1",
        "abstract": "Storage integrity is essential to systems and applications that use untrusted\nstorage (e.g., public clouds, end-user devices). However, known methods for\nachieving storage integrity either suffer from high (and often prohibitive)\noverheads or provide weak integrity guarantees. In this work, we demonstrate a\nhybrid approach to storage integrity that simultaneously reduces overhead while\nproviding strong integrity guarantees. Our system, partially asynchronous\nintegrity checking (PAC), allows disk write commitments to be deferred while\nstill providing guarantees around read integrity. PAC delivers a 5.5X\nthroughput and latency improvement over the state of the art, and 85% of the\nthroughput achieved by non-integrity-assuring approaches. In this way, we show\nthat untrusted storage can be used for integrity-critical workloads without\nmeaningfully sacrificing performance."
    },
    {
        "date": "2025-04",
        "title": "ShadowBinding: Realizing Effective Microarchitectures for In-Core Secure Speculation Schemes",
        "author": "Amund Bergland Kvalsvik, and Magnus Sj\u00e4lander",
        "link": "http://arxiv.org/abs/2504.07018v1",
        "abstract": "Secure speculation schemes have shown great promise in the war against\nspeculative side-channel attacks, and will be a key building block for\ndeveloping secure, high-performance architectures moving forward. As the field\nmatures, the need for rigorous microarchitectures, and corresponding\nperformance and cost analysis, become critical for evaluating secure schemes\nand for enabling their future adoption.\n  In ShadowBinding, we present effective microarchitectures for two\nstate-of-the-art secure schemes, uncovering and mitigating fundamental\nmicroarchitectural limitations within the analyzed schemes, and provide\nimportant design characteristics. We uncover that Speculative Taint Tracking's\n(STT's) rename-based taint computation must be completed in a single cycle,\ncreating an expensive dependency chain which greatly limits performance for\nwider processor cores. We also introduce a novel michroarchitectural approach\nfor STT, named STT-Issue, which, by delaying the taint computation to the issue\nstage, eliminates the dependency chain, achieving better instructions per cycle\n(IPC), timing, area, and performance results.\n  Through a comprehensive evaluation of our STT and Non-Speculative Data Access\n(NDA) microarchitectural designs on the RISC-V Berkeley Out-of-Order Machine,\nwe find that the IPC impact of in-core secure schemes is higher than previously\nestimated, close to 20% for the highest performance core. With insights into\ntiming from our RTL evaluation, the performance loss, created by the combined\nimpact of IPC and timing, becomes even greater, at 35%, 27%, and 22% for\nSTT-Rename, STT-Issue, and NDA, respectively. If these trends were to hold for\nleading processor core designs, the performance impact would be well over 30%,\neven for the best-performing scheme."
    },
    {
        "date": "2025-04",
        "title": "LLM-IFT: LLM-Powered Information Flow Tracking for Secure Hardware",
        "author": "Nowfel Mashnoor, Mohammad Akyash, Hadi Kamali, and Kimia Azar",
        "link": "http://arxiv.org/abs/2504.07015v1",
        "abstract": "As modern hardware designs grow in complexity and size, ensuring security\nacross the confidentiality, integrity, and availability (CIA) triad becomes\nincreasingly challenging. Information flow tracking (IFT) is a widely-used\napproach to tracing data propagation, identifying unauthorized activities that\nmay compromise confidentiality or/and integrity in hardware. However,\ntraditional IFT methods struggle with scalability and adaptability,\nparticularly in high-density and interconnected architectures, leading to\ntracing bottlenecks that limit applicability in large-scale hardware. To\naddress these limitations and show the potential of transformer-based models in\nintegrated circuit (IC) design, this paper introduces LLM-IFT that integrates\nlarge language models (LLM) for the realization of the IFT process in hardware.\nLLM-IFT exploits LLM-driven structured reasoning to perform hierarchical\ndependency analysis, systematically breaking down even the most complex\ndesigns. Through a multi-step LLM invocation, the framework analyzes both\nintra-module and inter-module dependencies, enabling comprehensive IFT\nassessment. By focusing on a set of Trust-Hub vulnerability test cases at both\nthe IP level and the SoC level, our experiments demonstrate a 100\\% success\nrate in accurate IFT analysis for confidentiality and integrity checks in\nhardware."
    },
    {
        "date": "2025-04",
        "title": "ASRL:A robust loss function with potential for development",
        "author": "Chenyu Hui, Anran Zhang, and Xintong Li",
        "link": "http://arxiv.org/abs/2504.06935v1",
        "abstract": "In this article, we proposed a partition:wise robust loss function based on\nthe previous robust loss function. The characteristics of this loss function\nare that it achieves high robustness and a wide range of applicability through\npartition-wise design and adaptive parameter adjustment. Finally, the\nadvantages and development potential of this loss function were verified by\napplying this loss function to the regression question and using five different\ndatasets (with different dimensions, different sample numbers, and different\nfields) to compare with the other loss functions. The results of multiple\nexperiments have proven the advantages of our loss function ."
    },
    {
        "date": "2025-04",
        "title": "GraspClutter6D: A Large-scale Real-world Dataset for Robust Perception and Grasping in Cluttered Scenes",
        "author": "Seunghyeok Back, Joosoon Lee, Kangmin Kim, Heeseon Rho, Geonhyup Lee, Raeyoung Kang, Sangbeom Lee, Sangjun Noh, Youngjin Lee, Taeyeop Lee, and Kyoobin Lee",
        "link": "http://arxiv.org/abs/2504.06866v1",
        "abstract": "Robust grasping in cluttered environments remains an open challenge in\nrobotics. While benchmark datasets have significantly advanced deep learning\nmethods, they mainly focus on simplistic scenes with light occlusion and\ninsufficient diversity, limiting their applicability to practical scenarios. We\npresent GraspClutter6D, a large-scale real-world grasping dataset featuring:\n(1) 1,000 highly cluttered scenes with dense arrangements (14.1 objects/scene,\n62.6\\% occlusion), (2) comprehensive coverage across 200 objects in 75\nenvironment configurations (bins, shelves, and tables) captured using four\nRGB-D cameras from multiple viewpoints, and (3) rich annotations including 736K\n6D object poses and 9.3B feasible robotic grasps for 52K RGB-D images. We\nbenchmark state-of-the-art segmentation, object pose estimation, and grasping\ndetection methods to provide key insights into challenges in cluttered\nenvironments. Additionally, we validate the dataset's effectiveness as a\ntraining resource, demonstrating that grasping networks trained on\nGraspClutter6D significantly outperform those trained on existing datasets in\nboth simulation and real-world experiments. The dataset, toolkit, and\nannotation tools are publicly available on our project website:\nhttps://sites.google.com/view/graspclutter6d."
    },
    {
        "date": "2025-04",
        "title": "Regret Bounds for Robust Online Decision Making",
        "author": "Alexander Appel, and Vanessa Kosoy",
        "link": "http://arxiv.org/abs/2504.06820v1",
        "abstract": "We propose a framework which generalizes \"decision making with structured\nobservations\" by allowing robust (i.e. multivalued) models. In this framework,\neach model associates each decision with a convex set of probability\ndistributions over outcomes. Nature can choose distributions out of this set in\nan arbitrary (adversarial) manner, that can be nonoblivious and depend on past\nhistory. The resulting framework offers much greater generality than classical\nbandits and reinforcement learning, since the realizability assumption becomes\nmuch weaker and more realistic. We then derive a theory of regret bounds for\nthis framework. Although our lower and upper bounds are not tight, they are\nsufficient to fully characterize power-law learnability. We demonstrate this\ntheory in two special cases: robust linear bandits and tabular robust online\nreinforcement learning. In both cases, we derive regret bounds that improve\nstate-of-the-art (except that we do not address computational efficiency)."
    },
    {
        "date": "2025-04",
        "title": "Robust Classification with Noisy Labels Based on Posterior Maximization",
        "author": "Nicola Novello, and Andrea M. Tonello",
        "link": "http://arxiv.org/abs/2504.06805v1",
        "abstract": "Designing objective functions robust to label noise is crucial for real-world\nclassification algorithms. In this paper, we investigate the robustness to\nlabel noise of an $f$-divergence-based class of objective functions recently\nproposed for supervised classification, herein referred to as $f$-PML. We show\nthat, in the presence of label noise, any of the $f$-PML objective functions\ncan be corrected to obtain a neural network that is equal to the one learned\nwith the clean dataset. Additionally, we propose an alternative and novel\ncorrection approach that, during the test phase, refines the posterior\nestimated by the neural network trained in the presence of label noise. Then,\nwe demonstrate that, even if the considered $f$-PML objective functions are not\nsymmetric, they are robust to symmetric label noise for any choice of\n$f$-divergence, without the need for any correction approach. This allows us to\nprove that the cross-entropy, which belongs to the $f$-PML class, is robust to\nsymmetric label noise. Finally, we show that such a class of objective\nfunctions can be used together with refined training strategies, achieving\ncompetitive performance against state-of-the-art techniques of classification\nwith label noise."
    },
    {
        "date": "2025-04",
        "title": "Large-Scale (Semi-)Automated Security Assessment of Consumer IoT Devices -- A Roadmap",
        "author": "Pascal Sch\u00f6ttle, Matthias Janetschek, Florian Merkle, Martin Nocker, and Christoph Egger",
        "link": "http://arxiv.org/abs/2504.06712v2",
        "abstract": "The Internet of Things (IoT) has rapidly expanded across various sectors,\nwith consumer IoT devices - such as smart thermostats and security cameras -\nexperiencing growth. Although these devices improve efficiency and promise\nadditional comfort, they also introduce new security challenges. Common and\neasy-to-explore vulnerabilities make IoT devices prime targets for malicious\nactors. Upcoming mandatory security certifications offer a promising way to\nmitigate these risks by enforcing best practices and providing transparency.\nRegulatory bodies are developing IoT security frameworks, but a universal\nstandard for large-scale systematic security assessment is lacking. Existing\nmanual testing approaches are expensive, limiting their efficacy in the diverse\nand rapidly evolving IoT domain. This paper reviews current IoT security\nchallenges and assessment efforts, identifies gaps, and proposes a roadmap for\nscalable, automated security assessment, leveraging a model-based testing\napproach and machine learning techniques to strengthen consumer IoT security."
    },
    {
        "date": "2025-04",
        "title": "NLP Security and Ethics, in the Wild",
        "author": "Heather Lent, Erick Galinkin, Yiyi Chen, Jens Myrup Pedersen, Leon Derczynski, and Johannes Bjerva",
        "link": "http://arxiv.org/abs/2504.06669v1",
        "abstract": "As NLP models are used by a growing number of end-users, an area of\nincreasing importance is NLP Security (NLPSec): assessing the vulnerability of\nmodels to malicious attacks and developing comprehensive countermeasures\nagainst them. While work at the intersection of NLP and cybersecurity has the\npotential to create safer NLP for all, accidental oversights can result in\ntangible harm (e.g., breaches of privacy or proliferation of malicious models).\nIn this emerging field, however, the research ethics of NLP have not yet faced\nmany of the long-standing conundrums pertinent to cybersecurity, until now. We\nthus examine contemporary works across NLPSec, and explore their engagement\nwith cybersecurity's ethical norms. We identify trends across the literature,\nultimately finding alarming gaps on topics like harm minimization and\nresponsible disclosure. To alleviate these concerns, we provide concrete\nrecommendations to help NLP researchers navigate this space more ethically,\nbridging the gap between traditional cybersecurity and NLP ethics, which we\nframe as ``white hat NLP''. The goal of this work is to help cultivate an\nintentional culture of ethical research for those working in NLP Security."
    },
    {
        "date": "2025-04",
        "title": "Robust and Noise-resilient Long-Term Prediction of Spatiotemporal Data Using Variational Mode Graph Neural Networks with 3D Attention",
        "author": "Osama Ahmad, and Zubair Khalid",
        "link": "http://arxiv.org/abs/2504.06660v1",
        "abstract": "This paper focuses on improving the robustness of spatiotemporal long-term\nprediction using a variational mode graph convolutional network (VMGCN) by\nintroducing 3D channel attention. The deep learning network for this task\nrelies on historical data inputs, yet real-time data can be corrupted by sensor\nnoise, altering its distribution. We model this noise as independent and\nidentically distributed (i.i.d.) Gaussian noise and incorporate it into the\nLargeST traffic volume dataset, resulting in data with both inherent and\nadditive noise components. Our approach involves decomposing the corrupted\nsignal into modes using variational mode decomposition, followed by feeding the\ndata into a learning pipeline for prediction. We integrate a 3D attention\nmechanism encompassing spatial, temporal, and channel attention. The spatial\nand temporal attention modules learn their respective correlations, while the\nchannel attention mechanism is used to suppress noise and highlight the\nsignificant modes in the spatiotemporal signals. Additionally, a learnable soft\nthresholding method is implemented to exclude unimportant modes from the\nfeature vector, and a feature reduction method based on the signal-to-noise\nratio (SNR) is applied. We compare the performance of our approach against\nbaseline models, demonstrating that our method achieves superior long-term\nprediction accuracy, robustness to noise, and improved performance with mode\ntruncation compared to the baseline models. The code of the paper is available\nat https://github.com/OsamaAhmad369/VMGCN."
    },
    {
        "date": "2025-04",
        "title": "Visually Similar Pair Alignment for Robust Cross-Domain Object Detection",
        "author": "Onkar Krishna, and Hiroki Ohashi",
        "link": "http://arxiv.org/abs/2504.06607v1",
        "abstract": "Domain gaps between training data (source) and real-world environments\n(target) often degrade the performance of object detection models. Most\nexisting methods aim to bridge this gap by aligning features across source and\ntarget domains but often fail to account for visual differences, such as color\nor orientation, in alignment pairs. This limitation leads to less effective\ndomain adaptation, as the model struggles to manage both domain-specific shifts\n(e.g., fog) and visual variations simultaneously. In this work, we demonstrate\nfor the first time, using a custom-built dataset, that aligning visually\nsimilar pairs significantly improves domain adaptation. Based on this insight,\nwe propose a novel memory-based system to enhance domain alignment. This system\nstores precomputed features of foreground objects and background areas from the\nsource domain, which are periodically updated during training. By retrieving\nvisually similar source features for alignment with target foreground and\nbackground features, the model effectively addresses domain-specific\ndifferences while reducing the impact of visual variations. Extensive\nexperiments across diverse domain shift scenarios validate our method's\neffectiveness, achieving 53.1 mAP on Foggy Cityscapes and 62.3 on Sim10k,\nsurpassing prior state-of-the-art methods by 1.2 and 4.1 mAP, respectively."
    },
    {
        "date": "2025-04",
        "title": "Defending LLM Watermarking Against Spoofing Attacks with Contrastive Representation Learning",
        "author": "Li An, Yujian Liu, Yepeng Liu, Yang Zhang, Yuheng Bu, and Shiyu Chang",
        "link": "http://arxiv.org/abs/2504.06575v2",
        "abstract": "Watermarking has emerged as a promising technique for detecting texts\ngenerated by LLMs. Current research has primarily focused on three design\ncriteria: high quality of the watermarked text, high detectability, and\nrobustness against removal attack. However, the security against spoofing\nattacks remains relatively understudied. For example, a piggyback attack can\nmaliciously alter the meaning of watermarked text-transforming it into hate\nspeech-while preserving the original watermark, thereby damaging the reputation\nof the LLM provider. We identify two core challenges that make defending\nagainst spoofing difficult: (1) the need for watermarks to be both sensitive to\nsemantic-distorting changes and insensitive to semantic-preserving edits, and\n(2) the contradiction between the need to detect global semantic shifts and the\nlocal, auto-regressive nature of most watermarking schemes. To address these\nchallenges, we propose a semantic-aware watermarking algorithm that post-hoc\nembeds watermarks into a given target text while preserving its original\nmeaning. Our method introduces a semantic mapping model, which guides the\ngeneration of a green-red token list, contrastively trained to be sensitive to\nsemantic-distorting changes and insensitive to semantic-preserving changes.\nExperiments on two standard benchmarks demonstrate strong robustness against\nremoval attacks and security against spoofing attacks, including sentiment\nreversal and toxic content insertion, while maintaining high watermark\ndetectability. Our approach offers a significant step toward more secure and\nsemantically aware watermarking for LLMs. Our code is available at\nhttps://github.com/UCSB-NLP-Chang/contrastive-watermark."
    },
    {
        "date": "2025-04",
        "title": "Understanding Users' Security and Privacy Concerns and Attitudes Towards Conversational AI Platforms",
        "author": "Mutahar Ali, Arjun Arunasalam, and Habiba Farrukh",
        "link": "http://arxiv.org/abs/2504.06552v1",
        "abstract": "The widespread adoption of conversational AI platforms has introduced new\nsecurity and privacy risks. While these risks and their mitigation strategies\nhave been extensively researched from a technical perspective, users'\nperceptions of these platforms' security and privacy remain largely unexplored.\nIn this paper, we conduct a large-scale analysis of over 2.5M user posts from\nthe r/ChatGPT Reddit community to understand users' security and privacy\nconcerns and attitudes toward conversational AI platforms. Our qualitative\nanalysis reveals that users are concerned about each stage of the data\nlifecycle (i.e., collection, usage, and retention). They seek mitigations for\nsecurity vulnerabilities, compliance with privacy regulations, and greater\ntransparency and control in data handling. We also find that users exhibit\nvaried behaviors and preferences when interacting with these platforms. Some\nusers proactively safeguard their data and adjust privacy settings, while\nothers prioritize convenience over privacy risks, dismissing privacy concerns\nin favor of benefits, or feel resigned to inevitable data sharing. Through\nqualitative content and regression analysis, we discover that users' concerns\nevolve over time with the evolving AI landscape and are influenced by\ntechnological developments and major events. Based on our findings, we provide\nrecommendations for users, platforms, enterprises, and policymakers to enhance\ntransparency, improve data controls, and increase user trust and adoption."
    },
    {
        "date": "2025-04",
        "title": "Decoupling Contrastive Decoding: Robust Hallucination Mitigation in Multimodal Large Language Models",
        "author": "Wei Chen, Xin Yan, Bin Wen, Fan Yang, Tingting Gao, Di Zhang, and Long Chen",
        "link": "http://arxiv.org/abs/2504.08809v1",
        "abstract": "Although multimodal large language models (MLLMs) exhibit remarkable\nreasoning capabilities on complex multimodal understanding tasks, they still\nsuffer from the notorious hallucination issue: generating outputs misaligned\nwith obvious visual or factual evidence. Currently, training-based solutions,\nlike direct preference optimization (DPO), leverage paired preference data to\nsuppress hallucinations. However, they risk sacrificing general reasoning\ncapabilities due to the likelihood displacement. Meanwhile, training-free\nsolutions, like contrastive decoding, achieve this goal by subtracting the\nestimated hallucination pattern from a distorted input. Yet, these handcrafted\nperturbations (e.g., add noise to images) may poorly capture authentic\nhallucination patterns. To avoid these weaknesses of existing methods, and\nrealize robust hallucination mitigation (i.e., maintaining general reasoning\nperformance), we propose a novel framework: Decoupling Contrastive Decoding\n(DCD). Specifically, DCD decouples the learning of positive and negative\nsamples in preference datasets, and trains separate positive and negative image\nprojections within the MLLM. The negative projection implicitly models real\nhallucination patterns, which enables vision-aware negative images in the\ncontrastive decoding inference stage. Our DCD alleviates likelihood\ndisplacement by avoiding pairwise optimization and generalizes robustly without\nhandcrafted degradation. Extensive ablations across hallucination benchmarks\nand general reasoning tasks demonstrate the effectiveness of DCD, i.e., it\nmatches DPO's hallucination suppression while preserving general capabilities\nand outperforms the handcrafted contrastive decoding methods."
    },
    {
        "date": "2025-04",
        "title": "Exploiting Meta-Learning-based Poisoning Attacks for Graph Link Prediction",
        "author": "Mingchen Li, Di Zhuang, Keyu Chen, Dumindu Samaraweera, and Morris Chang",
        "link": "http://arxiv.org/abs/2504.06492v1",
        "abstract": "Link prediction in graph data utilizes various algorithms and machine\nlearning/deep learning models to predict potential relationships between graph\nnodes. This technique has found widespread use in numerous real-world\napplications, including recommendation systems, community networks, and\nbiological structures. However, recent research has highlighted the\nvulnerability of link prediction models to adversarial attacks, such as\npoisoning and evasion attacks. Addressing the vulnerability of these models is\ncrucial to ensure stable and robust performance in link prediction\napplications. While many works have focused on enhancing the robustness of the\nGraph Convolution Network (GCN) model, the Variational Graph Auto-Encoder\n(VGAE), a sophisticated model for link prediction, has not been thoroughly\ninvestigated in the context of graph adversarial attacks. To bridge this gap,\nthis article proposes an unweighted graph poisoning attack approach using\nmeta-learning techniques to undermine VGAE's link prediction performance. We\nconducted comprehensive experiments on diverse datasets to evaluate the\nproposed method and its parameters, comparing it with existing approaches in\nsimilar settings. Our results demonstrate that our approach significantly\ndiminishes link prediction performance and outperforms other state-of-the-art\nmethods."
    },
    {
        "date": "2025-04",
        "title": "D-Feat Occlusions: Diffusion Features for Robustness to Partial Visual Occlusions in Object Recognition",
        "author": "Rupayan Mallick, Sibo Dong, Nataniel Ruiz, and Sarah Adel Bargal",
        "link": "http://arxiv.org/abs/2504.06432v2",
        "abstract": "Applications of diffusion models for visual tasks have been quite noteworthy.\nThis paper targets making classification models more robust to occlusions for\nthe task of object recognition by proposing a pipeline that utilizes a frozen\ndiffusion model. Diffusion features have demonstrated success in image\ngeneration and image completion while understanding image context. Occlusion\ncan be posed as an image completion problem by deeming the pixels of the\noccluder to be `missing.' We hypothesize that such features can help\nhallucinate object visual features behind occluding objects, and hence we\npropose using them to enable models to become more occlusion robust. We design\nexperiments to include input-based augmentations as well as feature-based\naugmentations. Input-based augmentations involve finetuning on images where the\noccluder pixels are inpainted, and feature-based augmentations involve\naugmenting classification features with intermediate diffusion features. We\ndemonstrate that our proposed use of diffusion-based features results in models\nthat are more robust to partial object occlusions for both Transformers and\nConvNets on ImageNet with simulated occlusions. We also propose a dataset that\nencompasses real-world occlusions and demonstrate that our method is more\nrobust to partial object occlusions."
    },
    {
        "date": "2025-04",
        "title": "Towards Calibration Enhanced Network by Inverse Adversarial Attack",
        "author": "Yupeng Cheng, Zi Pong Lim, Sarthak Ketanbhai Modi, Yon Shin Teo, Yushi Cao, and Shang-Wei Lin",
        "link": "http://arxiv.org/abs/2504.06358v1",
        "abstract": "Test automation has become increasingly important as the complexity of both\ndesign and content in Human Machine Interface (HMI) software continues to grow.\nCurrent standard practice uses Optical Character Recognition (OCR) techniques\nto automatically extract textual information from HMI screens for validation.\nAt present, one of the key challenges faced during the automation of HMI screen\nvalidation is the noise handling for the OCR models. In this paper, we propose\nto utilize adversarial training techniques to enhance OCR models in HMI testing\nscenarios. More specifically, we design a new adversarial attack objective for\nOCR models to discover the decision boundaries in the context of HMI testing.\nWe then adopt adversarial training to optimize the decision boundaries towards\na more robust and accurate OCR model. In addition, we also built an HMI screen\ndataset based on real-world requirements and applied multiple types of\nperturbation onto the clean HMI dataset to provide a more complete coverage for\nthe potential scenarios. We conduct experiments to demonstrate how using\nadversarial training techniques yields more robust OCR models against various\nkinds of noises, while still maintaining high OCR model accuracy. Further\nexperiments even demonstrate that the adversarial training models exhibit a\ncertain degree of robustness against perturbations from other patterns."
    },
    {
        "date": "2025-04",
        "title": "Navigating the Rabbit Hole: Emergent Biases in LLM-Generated Attack Narratives Targeting Mental Health Groups",
        "author": "Rijul Magu, Arka Dutta, Sean Kim, Ashiqur R. KhudaBukhsh, and Munmun De Choudhury",
        "link": "http://arxiv.org/abs/2504.06160v3",
        "abstract": "Large Language Models (LLMs) have been shown to demonstrate imbalanced biases\nagainst certain groups. However, the study of unprovoked targeted attacks by\nLLMs towards at-risk populations remains underexplored. Our paper presents\nthree novel contributions: (1) the explicit evaluation of LLM-generated attacks\non highly vulnerable mental health groups; (2) a network-based framework to\nstudy the propagation of relative biases; and (3) an assessment of the relative\ndegree of stigmatization that emerges from these attacks. Our analysis of a\nrecently released large-scale bias audit dataset reveals that mental health\nentities occupy central positions within attack narrative networks, as revealed\nby a significantly higher mean centrality of closeness (p-value = 4.06e-10) and\ndense clustering (Gini coefficient = 0.7). Drawing from sociological\nfoundations of stigmatization theory, our stigmatization analysis indicates\nincreased labeling components for mental health disorder-related targets\nrelative to initial targets in generation chains. Taken together, these\ninsights shed light on the structural predilections of large language models to\nheighten harmful discourse and highlight the need for suitable approaches for\nmitigation."
    },
    {
        "date": "2025-04",
        "title": "Adversarial Training of Reward Models",
        "author": "Alexander Bukharin, Haifeng Qian, Shengyang Sun, Adithya Renduchintala, Soumye Singhal, Zhilin Wang, Oleksii Kuchaiev, Olivier Delalleau, and Tuo Zhao",
        "link": "http://arxiv.org/abs/2504.06141v2",
        "abstract": "Reward modeling has emerged as a promising approach for the scalable\nalignment of language models. However, contemporary reward models (RMs) often\nlack robustness, awarding high rewards to low-quality, out-of-distribution\n(OOD) samples. This can lead to reward hacking, where policies exploit\nunintended shortcuts to maximize rewards, undermining alignment. To address\nthis challenge, we introduce Adv-RM, a novel adversarial training framework\nthat automatically identifies adversarial examples -- responses that receive\nhigh rewards from the target RM but are OOD and of low quality. By leveraging\nreinforcement learning, Adv-RM trains a policy to generate adversarial examples\nthat reliably expose vulnerabilities in large state-of-the-art reward models\nsuch as Nemotron 340B RM. Incorporating these adversarial examples into the\nreward training process improves the robustness of RMs, mitigating reward\nhacking and enhancing downstream performance in RLHF. We demonstrate that\nAdv-RM significantly outperforms conventional RM training, increasing stability\nand enabling more effective RLHF training in both synthetic and real-data\nsettings."
    },
    {
        "date": "2025-04",
        "title": "A Robust Real-Time Lane Detection Method with Fog-Enhanced Feature Fusion for Foggy Conditions",
        "author": "Ronghui Zhang, Yuhang Ma, Tengfei Li, Ziyu Lin, Yueying Wu, Junzhou Chen, Lin Zhang, Jia Hu, Tony Z. Qiu, and Konghui Guo",
        "link": "http://arxiv.org/abs/2504.06121v2",
        "abstract": "Lane detection is a critical component of Advanced Driver Assistance Systems\n(ADAS). Existing lane detection algorithms generally perform well under\nfavorable weather conditions. However, their performance degrades significantly\nin adverse conditions, such as fog, which increases the risk of traffic\naccidents. This challenge is compounded by the lack of specialized datasets and\nmethods designed for foggy environments. To address this, we introduce the\nFoggyLane dataset, captured in real-world foggy scenarios, and synthesize two\nadditional datasets, FoggyCULane and FoggyTusimple, from existing popular lane\ndetection datasets. Furthermore, we propose a robust Fog-Enhanced Network for\nlane detection, incorporating a Global Feature Fusion Module (GFFM) to capture\nglobal relationships in foggy images, a Kernel Feature Fusion Module (KFFM) to\nmodel the structural and positional relationships of lane instances, and a\nLow-level Edge Enhanced Module (LEEM) to address missing edge details in foggy\nconditions. Comprehensive experiments demonstrate that our method achieves\nstate-of-the-art performance, with F1-scores of 95.04 on FoggyLane, 79.85 on\nFoggyCULane, and 96.95 on FoggyTusimple. Additionally, with TensorRT\nacceleration, the method reaches a processing speed of 38.4 FPS on the NVIDIA\nJetson AGX Orin, confirming its real-time capabilities and robustness in foggy\nenvironments."
    },
    {
        "date": "2025-04",
        "title": "Security Analysis of Thumbnail-Preserving Image Encryption and a New Framework",
        "author": "Dong Xie, Zhiyang Li, Shuangxi Guo, Fulong Chen, and Peng Hu",
        "link": "http://arxiv.org/abs/2504.06083v1",
        "abstract": "As a primary encryption primitive balancing the privacy and searchability of\ncloud storage images, thumbnail preserving encryption (TPE) enables users to\nquickly identify the privacy personal image on the cloud and request this image\nfrom the owner through a secure channel. In this paper, we have found that two\ndifferent plaintext images may produce the same thumbnail. It results in the\nfailure of search strategy because the collision of thumbnail occurs. To\naddress this serious security issues, we conduct an in-depth analysis on the\ncollision probabilities of thumbnails, and then propose a new TPE framework,\ncalled multi-factor thumbnail preserving encryption (MFTPE). It starts from the\ncollision probability of two blocks, extend to the probabilities of two images\nand ultimately to N images. Then, we in detail describe three specific MFTPE\nconstructions preserving different combinations of factors, i.e., the sum and\nthe geometric mean, the sum and the range, and the sum and the weighted mean.\nThe theoretical and experimental results demonstrate that the proposed MFTPE\nreduces the probability of thumbnails, exhibits strong robustness, and also\neffectively resists face detection and noise attacks."
    },
    {
        "date": "2025-04",
        "title": "Exploring Gradient-Guided Masked Language Model to Detect Textual Adversarial Attacks",
        "author": "Xiaomei Zhang, Zhaoxi Zhang, Yanjun Zhang, Xufei Zheng, Leo Yu Zhang, Shengshan Hu, and Shirui Pan",
        "link": "http://arxiv.org/abs/2504.08798v1",
        "abstract": "Textual adversarial examples pose serious threats to the reliability of\nnatural language processing systems. Recent studies suggest that adversarial\nexamples tend to deviate from the underlying manifold of normal texts, whereas\npre-trained masked language models can approximate the manifold of normal data.\nThese findings inspire the exploration of masked language models for detecting\ntextual adversarial attacks. We first introduce Masked Language Model-based\nDetection (MLMD), leveraging the mask and unmask operations of the masked\nlanguage modeling (MLM) objective to induce the difference in manifold changes\nbetween normal and adversarial texts. Although MLMD achieves competitive\ndetection performance, its exhaustive one-by-one masking strategy introduces\nsignificant computational overhead. Our posterior analysis reveals that a\nsignificant number of non-keywords in the input are not important for detection\nbut consume resources. Building on this, we introduce Gradient-guided MLMD\n(GradMLMD), which leverages gradient information to identify and skip\nnon-keywords during detection, significantly reducing resource consumption\nwithout compromising detection performance."
    },
    {
        "date": "2025-04",
        "title": "FedFeat+: A Robust Federated Learning Framework Through Federated Aggregation and Differentially Private Feature-Based Classifier Retraining",
        "author": "Mrityunjoy Gain, Kitae Kim, Avi Deb Raha, Apurba Adhikary, Eui-Nam Huh, Zhu Han, and Choong Seon Hong",
        "link": "http://arxiv.org/abs/2504.06004v1",
        "abstract": "In this paper, we propose the FedFeat+ framework, which distinctively\nseparates feature extraction from classification. We develop a two-tiered model\ntraining process: following local training, clients transmit their weights and\nsome features extracted from the feature extractor from the final local epochs\nto the server. The server aggregates these models using the FedAvg method and\nsubsequently retrains the global classifier utilizing the shared features. The\nclassifier retraining process enhances the model's understanding of the\nholistic view of the data distribution, ensuring better generalization across\ndiverse datasets. This improved generalization enables the classifier to\nadaptively influence the feature extractor during subsequent local training\nepochs. We establish a balance between enhancing model accuracy and\nsafeguarding individual privacy through the implementation of differential\nprivacy mechanisms. By incorporating noise into the feature vectors shared with\nthe server, we ensure that sensitive data remains confidential. We present a\ncomprehensive convergence analysis, along with theoretical reasoning regarding\nperformance enhancement and privacy preservation. We validate our approach\nthrough empirical evaluations conducted on benchmark datasets, including\nCIFAR-10, CIFAR-100, MNIST, and FMNIST, achieving high accuracy while adhering\nto stringent privacy guarantees. The experimental results demonstrate that the\nFedFeat+ framework, despite using only a lightweight two-layer CNN classifier,\noutperforms the FedAvg method in both IID and non-IID scenarios, achieving\naccuracy improvements ranging from 3.92 % to 12.34 % across CIFAR-10,\nCIFAR-100, and Fashion-MNIST datasets."
    },
    {
        "date": "2025-04",
        "title": "Security Vulnerabilities in Ethereum Smart Contracts: A Systematic Analysis",
        "author": "Jixuan Wu, Lei Xie, and Xiaoqi Li",
        "link": "http://arxiv.org/abs/2504.05968v3",
        "abstract": "Smart contracts are a secure and trustworthy application that plays a vital\nrole in decentralized applications in various fields such as insurance,the\ninternet, and gaming. However, in recent years, smart contract security\nbreaches have occurred frequently, and due to their financial properties, they\nhave caused huge economic losses, such as the most famous security incident\n\"The DAO\" which caused a loss of over $60 million in Ethereum. This has drawn a\nlot of attention from all sides. Writing a secure smart contract is now a\ncritical issue. This paper focuses on Ether smart contracts and explains the\nmain components of Ether, smart contract architecture and mechanism. The\nenvironment used in this paper is the Ethernet environment, using remix online\ncompilation platform and Solidity language, according to the four security\nevents of American Chain, The DAO, Parity and KotET, the principles of integer\noverflow attack, reentrant attack, access control attack and denial of service\nattack are studied and analyzed accordingly, and the scenarios of these\nvulnerabilities are reproduced, and the measures to prevent them are given.\nFinally, preventive measures are given. In addition, the principles of short\naddress attack, early transaction attack and privileged function exposure\nattack are also introduced in detail, and security measures are proposed. As\nvulnerabilities continue to emerge, their classification will also evolve. The\nanalysis and research of the current vulnerabilities are also to lay a solid\nfoundation for avoiding more vulnerabilities."
    },
    {
        "date": "2025-04",
        "title": "CKGAN: Training Generative Adversarial Networks Using Characteristic Kernel Integral Probability Metrics",
        "author": "Kuntian Zhang, Simin Yu, Yaoshu Wang, Makoto Onizuka, and Chuan Xiao",
        "link": "http://arxiv.org/abs/2504.05945v1",
        "abstract": "In this paper, we propose CKGAN, a novel generative adversarial network (GAN)\nvariant based on an integral probability metrics framework with characteristic\nkernel (CKIPM). CKIPM, as a distance between two probability distributions, is\ndesigned to optimize the lowerbound of the maximum mean discrepancy (MMD) in a\nreproducing kernel Hilbert space, and thus can be used to train GANs. CKGAN\nmitigates the notorious problem of mode collapse by mapping the generated\nimages back to random noise. To save the effort of selecting the kernel\nfunction manually, we propose a soft selection method to automatically learn a\ncharacteristic kernel function. The experimental evaluation conducted on a set\nof synthetic and real image benchmarks (MNIST, CelebA, etc.) demonstrates that\nCKGAN generally outperforms other MMD-based GANs. The results also show that at\nthe cost of moderately more training time, the automatically selected kernel\nfunction delivers very close performance to the best of manually fine-tuned one\non real image benchmarks and is able to improve the performances of other\nMMD-based GANs."
    },
    {
        "date": "2025-04",
        "title": "Defending Deep Neural Networks against Backdoor Attacks via Module Switching",
        "author": "Weijun Li, Ansh Arora, Xuanli He, Mark Dras, and Qiongkai Xu",
        "link": "http://arxiv.org/abs/2504.05902v1",
        "abstract": "The exponential increase in the parameters of Deep Neural Networks (DNNs) has\nsignificantly raised the cost of independent training, particularly for\nresource-constrained entities. As a result, there is a growing reliance on\nopen-source models. However, the opacity of training processes exacerbates\nsecurity risks, making these models more vulnerable to malicious threats, such\nas backdoor attacks, while simultaneously complicating defense mechanisms.\nMerging homogeneous models has gained attention as a cost-effective\npost-training defense. However, we notice that existing strategies, such as\nweight averaging, only partially mitigate the influence of poisoned parameters\nand remain ineffective in disrupting the pervasive spurious correlations\nembedded across model parameters. We propose a novel module-switching strategy\nto break such spurious correlations within the model's propagation path. By\nleveraging evolutionary algorithms to optimize fusion strategies, we validate\nour approach against backdoor attacks targeting text and vision domains. Our\nmethod achieves effective backdoor mitigation even when incorporating a couple\nof compromised models, e.g., reducing the average attack success rate (ASR) to\n22% compared to 31.9% with the best-performing baseline on SST-2."
    },
    {
        "date": "2025-04",
        "title": "Channel State Information Analysis for Jamming Attack Detection in Static and Dynamic UAV Networks -- An Experimental Study",
        "author": "Pavlo Mykytyn, Ronald Chitauro, Zoya Dyka, and Peter Langendoerfer",
        "link": "http://arxiv.org/abs/2504.05832v1",
        "abstract": "Networks built on the IEEE 802.11 standard have experienced rapid growth in\nthe last decade. Their field of application is vast, including smart home\napplications, Internet of Things (IoT), and short-range high throughput static\nand dynamic inter-vehicular communication networks. Within such networks,\nChannel State Information (CSI) provides a detailed view of the state of the\ncommunication channel and represents the combined effects of multipath\npropagation, scattering, phase shift, fading, and power decay. In this work, we\ninvestigate the problem of jamming attack detection in static and dynamic\nvehicular networks. We utilize ESP32-S3 modules to set up a communication\nnetwork between an Unmanned Aerial Vehicle (UAV) and a Ground Control Station\n(GCS), to experimentally test the combined effects of a constant jammer on\nrecorded CSI parameters, and the feasibility of jamming detection through CSI\nanalysis in static and dynamic communication scenarios."
    },
    {
        "date": "2025-04",
        "title": "Parasite: A Steganography-based Backdoor Attack Framework for Diffusion Models",
        "author": "Jiahao Chen, Yu Pan, Yi Du, Chunkai Wu, and Lin Wang",
        "link": "http://arxiv.org/abs/2504.05815v1",
        "abstract": "Recently, the diffusion model has gained significant attention as one of the\nmost successful image generation models, which can generate high-quality images\nby iteratively sampling noise. However, recent studies have shown that\ndiffusion models are vulnerable to backdoor attacks, allowing attackers to\nenter input data containing triggers to activate the backdoor and generate\ntheir desired output. Existing backdoor attack methods primarily focused on\ntarget noise-to-image and text-to-image tasks, with limited work on backdoor\nattacks in image-to-image tasks. Furthermore, traditional backdoor attacks\noften rely on a single, conspicuous trigger to generate a fixed target image,\nlacking concealability and flexibility. To address these limitations, we\npropose a novel backdoor attack method called \"Parasite\" for image-to-image\ntasks in diffusion models, which not only is the first to leverage\nsteganography for triggers hiding, but also allows attackers to embed the\ntarget content as a backdoor trigger to achieve a more flexible attack.\n\"Parasite\" as a novel attack method effectively bypasses existing detection\nframeworks to execute backdoor attacks. In our experiments, \"Parasite\" achieved\na 0 percent backdoor detection rate against the mainstream defense frameworks.\nIn addition, in the ablation study, we discuss the influence of different\nhiding coefficients on the attack results. You can find our code at\nhttps://anonymous.4open.science/r/Parasite-1715/."
    },
    {
        "date": "2025-04",
        "title": "Robust Fusion Controller: Degradation-aware Image Fusion with Fine-grained Language Instructions",
        "author": "Hao Zhang, Yanping Zha, Qingwei Zhuang, Zhenfeng Shao, and Jiayi Ma",
        "link": "http://arxiv.org/abs/2504.05795v2",
        "abstract": "Current image fusion methods struggle to adapt to real-world environments\nencompassing diverse degradations with spatially varying characteristics. To\naddress this challenge, we propose a robust fusion controller (RFC) capable of\nachieving degradation-aware image fusion through fine-grained language\ninstructions, ensuring its reliable application in adverse environments.\nSpecifically, RFC first parses language instructions to innovatively derive the\nfunctional condition and the spatial condition, where the former specifies the\ndegradation type to remove, while the latter defines its spatial coverage.\nThen, a composite control priori is generated through a multi-condition\ncoupling network, achieving a seamless transition from abstract language\ninstructions to latent control variables. Subsequently, we design a hybrid\nattention-based fusion network to aggregate multi-modal information, in which\nthe obtained composite control priori is deeply embedded to linearly modulate\nthe intermediate fused features. To ensure the alignment between language\ninstructions and control outcomes, we introduce a novel language-feature\nalignment loss, which constrains the consistency between feature-level gains\nand the composite control priori. Extensive experiments on publicly available\ndatasets demonstrate that our RFC is robust against various composite\ndegradations, particularly in highly challenging flare scenarios."
    }
]