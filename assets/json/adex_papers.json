[
    {
        "date": "2025-07",
        "title": "When LLMs Copy to Think: Uncovering Copy-Guided Attacks in Reasoning LLMs",
        "author": "Yue Li, Xiao Li, Hao Wu, Yue Zhang, Fengyuan Xu, Xiuzhen Cheng, and Sheng Zhong",
        "link": "http://arxiv.org/abs/2507.16773v1",
        "abstract": "Large Language Models (LLMs) have become integral to automated code analysis,\nenabling tasks such as vulnerability detection and code comprehension. However,\ntheir integration introduces novel attack surfaces. In this paper, we identify\nand investigate a new class of prompt-based attacks, termed Copy-Guided Attacks\n(CGA), which exploit the inherent copying tendencies of reasoning-capable LLMs.\nBy injecting carefully crafted triggers into external code snippets,\nadversaries can induce the model to replicate malicious content during\ninference. This behavior enables two classes of vulnerabilities: inference\nlength manipulation, where the model generates abnormally short or excessively\nlong reasoning traces; and inference result manipulation, where the model\nproduces misleading or incorrect conclusions. We formalize CGA as an\noptimization problem and propose a gradient-based approach to synthesize\neffective triggers. Empirical evaluation on state-of-the-art reasoning LLMs\nshows that CGA reliably induces infinite loops, premature termination, false\nrefusals, and semantic distortions in code analysis tasks. While highly\neffective in targeted settings, we observe challenges in generalizing CGA\nacross diverse prompts due to computational constraints, posing an open\nquestion for future research. Our findings expose a critical yet underexplored\nvulnerability in LLM-powered development pipelines and call for urgent advances\nin prompt-level defense mechanisms."
    },
    {
        "date": "2025-07",
        "title": "Denoising-While-Completing Network (DWCNet): Robust Point Cloud Completion Under Corruption",
        "author": "Keneni W. Tesema, Lyndon Hill, Mark W. Jones, and Gary K. L. Tam",
        "link": "http://arxiv.org/abs/2507.16743v1",
        "abstract": "Point cloud completion is crucial for 3D computer vision tasks in autonomous\ndriving, augmented reality, and robotics. However, obtaining clean and complete\npoint clouds from real-world environments is challenging due to noise and\nocclusions. Consequently, most existing completion networks -- trained on\nsynthetic data -- struggle with real-world degradations. In this work, we\ntackle the problem of completing and denoising highly corrupted partial point\nclouds affected by multiple simultaneous degradations. To benchmark robustness,\nwe introduce the Corrupted Point Cloud Completion Dataset (CPCCD), which\nhighlights the limitations of current methods under diverse corruptions.\nBuilding on these insights, we propose DWCNet (Denoising-While-Completing\nNetwork), a completion framework enhanced with a Noise Management Module (NMM)\nthat leverages contrastive learning and self-attention to suppress noise and\nmodel structural relationships. DWCNet achieves state-of-the-art performance on\nboth clean and corrupted, synthetic and real-world datasets. The dataset and\ncode will be publicly available at\nhttps://github.com/keneniwt/DWCNET-Robust-Point-Cloud-Completion-against-Corruptions"
    },
    {
        "date": "2025-07",
        "title": "Robust Noisy Pseudo-label Learning for Semi-supervised Medical Image Segmentation Using Diffusion Model",
        "author": "Lin Xi, Yingliang Ma, Cheng Wang, Sandra Howell, Aldo Rinaldi, and Kawal S. Rhode",
        "link": "http://arxiv.org/abs/2507.16429v1",
        "abstract": "Obtaining pixel-level annotations in the medical domain is both expensive and\ntime-consuming, often requiring close collaboration between clinical experts\nand developers. Semi-supervised medical image segmentation aims to leverage\nlimited annotated data alongside abundant unlabeled data to achieve accurate\nsegmentation. However, existing semi-supervised methods often struggle to\nstructure semantic distributions in the latent space due to noise introduced by\npseudo-labels. In this paper, we propose a novel diffusion-based framework for\nsemi-supervised medical image segmentation. Our method introduces a constraint\ninto the latent structure of semantic labels during the denoising diffusion\nprocess by enforcing prototype-based contrastive consistency. Rather than\nexplicitly delineating semantic boundaries, the model leverages class\nprototypes centralized semantic representations in the latent space as anchors.\nThis strategy improves the robustness of dense predictions, particularly in the\npresence of noisy pseudo-labels. We also introduce a new publicly available\nbenchmark: Multi-Object Segmentation in X-ray Angiography Videos (MOSXAV),\nwhich provides detailed, manually annotated segmentation ground truth for\nmultiple anatomical structures in X-ray angiography videos. Extensive\nexperiments on the EndoScapes2023 and MOSXAV datasets demonstrate that our\nmethod outperforms state-of-the-art medical image segmentation approaches under\nthe semi-supervised learning setting. This work presents a robust and\ndata-efficient diffusion model that offers enhanced flexibility and strong\npotential for a wide range of clinical applications."
    },
    {
        "date": "2025-07",
        "title": "ADCD-Net: Robust Document Image Forgery Localization via Adaptive DCT Feature and Hierarchical Content Disentanglement",
        "author": "Kahim Wong, Jicheng Zhou, Haiwei Wu, Yain-Whar Si, and Jiantao Zhou",
        "link": "http://arxiv.org/abs/2507.16397v1",
        "abstract": "The advancement of image editing tools has enabled malicious manipulation of\nsensitive document images, underscoring the need for robust document image\nforgery detection.Though forgery detectors for natural images have been\nextensively studied, they struggle with document images, as the tampered\nregions can be seamlessly blended into the uniform document background (BG) and\nstructured text. On the other hand, existing document-specific methods lack\nsufficient robustness against various degradations, which limits their\npractical deployment. This paper presents ADCD-Net, a robust document forgery\nlocalization model that adaptively leverages the RGB/DCT forensic traces and\nintegrates key characteristics of document images. Specifically, to address the\nDCT traces' sensitivity to block misalignment, we adaptively modulate the DCT\nfeature contribution based on a predicted alignment score, resulting in much\nimproved resilience to various distortions, including resizing and cropping.\nAlso, a hierarchical content disentanglement approach is proposed to boost the\nlocalization performance via mitigating the text-BG disparities. Furthermore,\nnoticing the predominantly pristine nature of BG regions, we construct a\npristine prototype capturing traces of untampered regions, and eventually\nenhance both the localization accuracy and robustness. Our proposed ADCD-Net\ndemonstrates superior forgery localization performance, consistently\noutperforming state-of-the-art methods by 20.79\\% averaged over 5 types of\ndistortions. The code is available at https://github.com/KAHIMWONG/ACDC-Net."
    },
    {
        "date": "2025-07",
        "title": "Are Foundation Models All You Need for Zero-shot Face Presentation Attack Detection?",
        "author": "Lazaro Janier Gonzalez-Sole, Juan E. Tapia, and Christoph Busch",
        "link": "http://arxiv.org/abs/2507.16393v1",
        "abstract": "Although face recognition systems have undergone an impressive evolution in\nthe last decade, these technologies are vulnerable to attack presentations\n(AP). These attacks are mostly easy to create and, by executing them against\nthe system's capture device, the malicious actor can impersonate an authorised\nsubject and thus gain access to the latter's information (e.g., financial\ntransactions). To protect facial recognition schemes against presentation\nattacks, state-of-the-art deep learning presentation attack detection (PAD)\napproaches require a large amount of data to produce reliable detection\nperformances and even then, they decrease their performance for unknown\npresentation attack instruments (PAI) or database (information not seen during\ntraining), i.e. they lack generalisability. To mitigate the above problems,\nthis paper focuses on zero-shot PAD. To do so, we first assess the\neffectiveness and generalisability of foundation models in established and\nchallenging experimental scenarios and then propose a simple but effective\nframework for zero-shot PAD. Experimental results show that these models are\nable to achieve performance in difficult scenarios with minimal effort of the\nmore advanced PAD mechanisms, whose weights were optimised mainly with training\nsets that included APs and bona fide presentations. The top-performing\nfoundation model outperforms by a margin the best from the state of the art\nobserved with the leaving-one-out protocol on the SiW-Mv2 database, which\ncontains challenging unknown 2D and 3D attacks"
    },
    {
        "date": "2025-07",
        "title": "The Cost of Compression: Tight Quadratic Black-Box Attacks on Sketches for $\\ell_2$ Norm Estimation",
        "author": "Sara Ahmadian, Edith Cohen, and Uri Stemmer",
        "link": "http://arxiv.org/abs/2507.16345v1",
        "abstract": "Dimensionality reduction via linear sketching is a powerful and widely used\ntechnique, but it is known to be vulnerable to adversarial inputs. We study the\nblack-box adversarial setting, where a fixed, hidden sketching matrix A in\n$R^{k X n}$ maps high-dimensional vectors v $\\in R^n$ to lower-dimensional\nsketches A v in $R^k$, and an adversary can query the system to obtain\napproximate ell2-norm estimates that are computed from the sketch.\n  We present a universal, nonadaptive attack that, using tilde(O)($k^2$)\nqueries, either causes a failure in norm estimation or constructs an\nadversarial input on which the optimal estimator for the query distribution\n(used by the attack) fails. The attack is completely agnostic to the sketching\nmatrix and to the estimator: It applies to any linear sketch and any query\nresponder, including those that are randomized, adaptive, or tailored to the\nquery distribution.\n  Our lower bound construction tightly matches the known upper bounds of\ntilde(Omega)($k^2$), achieved by specialized estimators for Johnson\nLindenstrauss transforms and AMS sketches. Beyond sketching, our results\nuncover structural parallels to adversarial attacks in image classification,\nhighlighting fundamental vulnerabilities of compressed representations."
    },
    {
        "date": "2025-07",
        "title": "Talking Like a Phisher: LLM-Based Attacks on Voice Phishing Classifiers",
        "author": "Wenhao Li, Selvakumar Manickam, Yung-wey Chong, and Shankar Karuppayah",
        "link": "http://arxiv.org/abs/2507.16291v1",
        "abstract": "Voice phishing (vishing) remains a persistent threat in cybersecurity,\nexploiting human trust through persuasive speech. While machine learning\n(ML)-based classifiers have shown promise in detecting malicious call\ntranscripts, they remain vulnerable to adversarial manipulations that preserve\nsemantic content. In this study, we explore a novel attack vector where large\nlanguage models (LLMs) are leveraged to generate adversarial vishing\ntranscripts that evade detection while maintaining deceptive intent. We\nconstruct a systematic attack pipeline that employs prompt engineering and\nsemantic obfuscation to transform real-world vishing scripts using four\ncommercial LLMs. The generated transcripts are evaluated against multiple ML\nclassifiers trained on a real-world Korean vishing dataset (KorCCViD) with\nstatistical testing. Our experiments reveal that LLM-generated transcripts are\nboth practically and statistically effective against ML-based classifiers. In\nparticular, transcripts crafted by GPT-4o significantly reduce classifier\naccuracy (by up to 30.96%) while maintaining high semantic similarity, as\nmeasured by BERTScore. Moreover, these attacks are both time-efficient and\ncost-effective, with average generation times under 9 seconds and negligible\nfinancial cost per query. The results underscore the pressing need for more\nresilient vishing detection frameworks and highlight the imperative for LLM\nproviders to enforce stronger safeguards against prompt misuse in adversarial\nsocial engineering contexts."
    },
    {
        "date": "2025-07",
        "title": "Understanding Generalization, Robustness, and Interpretability in Low-Capacity Neural Networks",
        "author": "Yash Kumar",
        "link": "http://arxiv.org/abs/2507.16278v1",
        "abstract": "Although modern deep learning often relies on massive over-parameterized\nmodels, the fundamental interplay between capacity, sparsity, and robustness in\nlow-capacity networks remains a vital area of study. We introduce a controlled\nframework to investigate these properties by creating a suite of binary\nclassification tasks from the MNIST dataset with increasing visual difficulty\n(e.g., 0 and 1 vs. 4 and 9). Our experiments reveal three core findings. First,\nthe minimum model capacity required for successful generalization scales\ndirectly with task complexity. Second, these trained networks are robust to\nextreme magnitude pruning (up to 95% sparsity), revealing the existence of\nsparse, high-performing subnetworks. Third, we show that over-parameterization\nprovides a significant advantage in robustness against input corruption.\nInterpretability analysis via saliency maps further confirms that these\nidentified sparse subnetworks preserve the core reasoning process of the\noriginal dense models. This work provides a clear, empirical demonstration of\nthe foundational trade-offs governing simple neural networks."
    },
    {
        "date": "2025-07",
        "title": "Quality Text, Robust Vision: The Role of Language in Enhancing Visual Robustness of Vision-Language Models",
        "author": "Futa Waseda, Saku Sugawara, and Isao Echizen",
        "link": "http://arxiv.org/abs/2507.16257v1",
        "abstract": "Defending pre-trained vision-language models (VLMs), such as CLIP, against\nadversarial attacks is crucial, as these models are widely used in diverse\nzero-shot tasks, including image classification. However, existing adversarial\ntraining (AT) methods for robust fine-tuning largely overlook the role of\nlanguage in enhancing visual robustness. Specifically, (1) supervised AT\nmethods rely on short texts (e.g., class labels) to generate adversarial\nperturbations, leading to overfitting to object classes in the training data,\nand (2) unsupervised AT avoids this overfitting but remains suboptimal against\npractical text-guided adversarial attacks due to its lack of semantic guidance.\nTo address these limitations, we propose Quality Text-guided Adversarial\nFine-Tuning (QT-AFT), which leverages high-quality captions during training to\nguide adversarial examples away from diverse semantics present in images. This\nenables the visual encoder to robustly recognize a broader range of image\nfeatures even under adversarial noise, thereby enhancing robustness across\ndiverse downstream tasks. QT-AFT overcomes the key weaknesses of prior methods\n-- overfitting in supervised AT and lack of semantic awareness in unsupervised\nAT -- achieving state-of-the-art zero-shot adversarial robustness and clean\naccuracy, evaluated across 16 zero-shot datasets. Furthermore, our\ncomprehensive study uncovers several key insights into the role of language in\nenhancing vision robustness; for example, describing object properties in\naddition to object names further enhances zero-shot robustness. Our findings\npoint to an urgent direction for future work -- centering high-quality\nlinguistic supervision in robust visual representation learning."
    },
    {
        "date": "2025-07",
        "title": "Toward a Lightweight and Robust Design for Caching",
        "author": "Peng Chen, Hailiang Zhao, Jiaji Zhang, Xueyan Tang, Yixuan Wang, and Shuiguang Deng",
        "link": "http://arxiv.org/abs/2507.16242v2",
        "abstract": "The online caching problem aims to minimize cache misses when serving a\nsequence of requests under a limited cache size. While naive learning-augmented\ncaching algorithms achieve ideal $1$-consistency, they lack robustness\nguarantees. Existing robustification methods either sacrifice $1$-consistency\nor introduce significant computational overhead. In this paper, we introduce\nGuard, a lightweight robustification framework that enhances the robustness of\na broad class of learning-augmented caching algorithms to $2H_k + 2$, while\npreserving their $1$-consistency. Guard achieves the current best-known\ntrade-off between consistency and robustness, with only $O(1)$ additional\nper-request overhead, thereby maintaining the original time complexity of the\nbase algorithm. Extensive experiments across multiple real-world datasets and\nprediction models validate the effectiveness of Guard in practice."
    },
    {
        "date": "2025-07",
        "title": "SVAgent: AI Agent for Hardware Security Verification Assertion",
        "author": "Rui Guo, Avinash Ayalasomayajula, Henian Li, Jingbo Zhou, Sujan Kumar Saha, and Farimah Farahmandi",
        "link": "http://arxiv.org/abs/2507.16203v1",
        "abstract": "Verification using SystemVerilog assertions (SVA) is one of the most popular\nmethods for detecting circuit design vulnerabilities. However, with the\nglobalization of integrated circuit design and the continuous upgrading of\nsecurity requirements, the SVA development model has exposed major limitations.\nIt is not only inefficient in development, but also unable to effectively deal\nwith the increasing number of security vulnerabilities in modern complex\nintegrated circuits. In response to these challenges, this paper proposes an\ninnovative SVA automatic generation framework SVAgent. SVAgent introduces a\nrequirement decomposition mechanism to transform the original complex\nrequirements into a structured, gradually solvable fine-grained problem-solving\nchain. Experiments have shown that SVAgent can effectively suppress the\ninfluence of hallucinations and random answers, and the key evaluation\nindicators such as the accuracy and consistency of the SVA are significantly\nbetter than existing frameworks. More importantly, we successfully integrated\nSVAgent into the most mainstream integrated circuit vulnerability assessment\nframework and verified its practicality and reliability in a real engineering\ndesign environment."
    },
    {
        "date": "2025-07",
        "title": "Pulse-Level Simulation of Crosstalk Attacks on Superconducting Quantum Hardware",
        "author": "Syed Emad Uddin Shubha, and Tasnuva Farheen",
        "link": "http://arxiv.org/abs/2507.16181v1",
        "abstract": "Hardware crosstalk in multi-tenant superconducting quantum computers poses a\nsevere security threat, allowing adversaries to induce targeted errors across\ntenant boundaries by injecting carefully engineered pulses. We present a\nsimulation-based study of active crosstalk attacks at the pulse level,\nanalyzing how adversarial control of pulse timing, shape, amplitude, and\ncoupling can disrupt a victim's computation. Our framework models the\ntime-dependent dynamics of a three-qubit system in the rotating frame,\ncapturing both always-on couplings and injected drive pulses. We examine two\nattack strategies: attacker-first (pulse before victim operation) and\nvictim-first (pulse after), and systematically identify the pulse and coupling\nconfigurations that cause the largest logical errors. Protocol-level\nexperiments on quantum coin flip and XOR classification circuits show that some\nprotocols are highly vulnerable to these attacks, while others remain robust.\nBased on these findings, we discuss practical methods for detection and\nmitigation to improve security in quantum cloud platforms."
    },
    {
        "date": "2025-07",
        "title": "Attacking interpretable NLP systems",
        "author": "Eldor Abdukhamidov, Tamer Abuhmed, Joanna C. S. Santos, and Mohammed Abuhamad",
        "link": "http://arxiv.org/abs/2507.16164v1",
        "abstract": "Studies have shown that machine learning systems are vulnerable to\nadversarial examples in theory and practice. Where previous attacks have\nfocused mainly on visual models that exploit the difference between human and\nmachine perception, text-based models have also fallen victim to these attacks.\nHowever, these attacks often fail to maintain the semantic meaning of the text\nand similarity. This paper introduces AdvChar, a black-box attack on\nInterpretable Natural Language Processing Systems, designed to mislead the\nclassifier while keeping the interpretation similar to benign inputs, thus\nexploiting trust in system transparency. AdvChar achieves this by making less\nnoticeable modifications to text input, forcing the deep learning classifier to\nmake incorrect predictions and preserve the original interpretation. We use an\ninterpretation-focused scoring approach to determine the most critical tokens\nthat, when changed, can cause the classifier to misclassify the input. We apply\nsimple character-level modifications to measure the importance of tokens,\nminimizing the difference between the original and new text while generating\nadversarial interpretations similar to benign ones. We thoroughly evaluated\nAdvChar by testing it against seven NLP models and three interpretation models\nusing benchmark datasets for the classification task. Our experiments show that\nAdvChar can significantly reduce the prediction accuracy of current deep\nlearning models by altering just two characters on average in input samples."
    },
    {
        "date": "2025-07",
        "title": "DP2Guard: A Lightweight and Byzantine-Robust Privacy-Preserving Federated Learning Scheme for Industrial IoT",
        "author": "Baofu Han, Bing Li, Yining Qi, Raja Jurdak, Kaibin Huang, and Chau Yuen",
        "link": "http://arxiv.org/abs/2507.16134v1",
        "abstract": "Privacy-Preserving Federated Learning (PPFL) has emerged as a secure\ndistributed Machine Learning (ML) paradigm that aggregates locally trained\ngradients without exposing raw data. To defend against model poisoning threats,\nseveral robustness-enhanced PPFL schemes have been proposed by integrating\nanomaly detection. Nevertheless, they still face two major challenges: (1) the\nreliance on heavyweight encryption techniques results in substantial\ncommunication and computation overhead; and (2) single-strategy defense\nmechanisms often fail to provide sufficient robustness against adaptive\nadversaries. To overcome these challenges, we propose DP2Guard, a lightweight\nPPFL framework that enhances both privacy and robustness. DP2Guard leverages a\nlightweight gradient masking mechanism to replace costly cryptographic\noperations while ensuring the privacy of local gradients. A hybrid defense\nstrategy is proposed, which extracts gradient features using singular value\ndecomposition and cosine similarity, and applies a clustering algorithm to\neffectively identify malicious gradients. Additionally, DP2Guard adopts a trust\nscore-based adaptive aggregation scheme that adjusts client weights according\nto historical behavior, while blockchain records aggregated results and trust\nscores to ensure tamper-proof and auditable training. Extensive experiments\nconducted on two public datasets demonstrate that DP2Guard effectively defends\nagainst four advanced poisoning attacks while ensuring privacy with reduced\ncommunication and computation costs."
    },
    {
        "date": "2025-07",
        "title": "Disrupting Semantic and Abstract Features for Better Adversarial Transferability",
        "author": "Yuyang Luo, Xiaosen Wang, Zhijin Ge, and Yingzhe He",
        "link": "http://arxiv.org/abs/2507.16052v1",
        "abstract": "Adversarial examples pose significant threats to deep neural networks (DNNs),\nand their property of transferability in the black-box setting has led to the\nemergence of transfer-based attacks, making it feasible to target real-world\napplications employing DNNs. Among them, feature-level attacks, where\nintermediate features are perturbed based on feature importance weight matrix\ncomputed from transformed images, have gained popularity. In this work, we find\nthat existing feature-level attacks primarily manipulate the semantic\ninformation to derive the weight matrix. Inspired by several works that find\nCNNs tend to focus more on high-frequency components (a.k.a. abstract features,\ne.g., texture, edge, etc.), we validate that transforming images in the\nhigh-frequency space also improves transferability. Based on this finding, we\npropose a balanced approach called Semantic and Abstract FEatures disRuption\n(SAFER). Specifically, SAFER conducts BLOCKMIX on the input image and SELF-MIX\non the frequency spectrum when computing the weight matrix to highlight crucial\nfeatures. By using such a weight matrix, we can direct the attacker to disrupt\nboth semantic and abstract features, leading to improved transferability.\nExtensive experiments on the ImageNet dataset also demonstrate the\neffectiveness of our method in boosting adversarial transferability."
    },
    {
        "date": "2025-07",
        "title": "Does More Inference-Time Compute Really Help Robustness?",
        "author": "Tong Wu, Chong Xiang, Jiachen T. Wang, Weichen Yu, Chawin Sitawarin, Vikash Sehwag, and Prateek Mittal",
        "link": "http://arxiv.org/abs/2507.15974v1",
        "abstract": "Recently, Zaremba et al. demonstrated that increasing inference-time\ncomputation improves robustness in large proprietary reasoning LLMs. In this\npaper, we first show that smaller-scale, open-source models (e.g., DeepSeek R1,\nQwen3, Phi-reasoning) can also benefit from inference-time scaling using a\nsimple budget forcing strategy. More importantly, we reveal and critically\nexamine an implicit assumption in prior work: intermediate reasoning steps are\nhidden from adversaries. By relaxing this assumption, we identify an important\nsecurity risk, intuitively motivated and empirically verified as an inverse\nscaling law: if intermediate reasoning steps become explicitly accessible,\nincreased inference-time computation consistently reduces model robustness.\nFinally, we discuss practical scenarios where models with hidden reasoning\nchains are still vulnerable to attacks, such as models with tool-integrated\nreasoning and advanced reasoning extraction attacks. Our findings collectively\ndemonstrate that the robustness benefits of inference-time scaling depend\nheavily on the adversarial setting and deployment context. We urge\npractitioners to carefully weigh these subtle trade-offs before applying\ninference-time scaling in security-sensitive, real-world applications."
    },
    {
        "date": "2025-07",
        "title": "Look, Focus, Act: Efficient and Robust Robot Learning via Human Gaze and Foveated Vision Transformers",
        "author": "Ian Chuang, Andrew Lee, Dechen Gao, Jinyu Zou, and Iman Soltani",
        "link": "http://arxiv.org/abs/2507.15833v1",
        "abstract": "Human vision is a highly active process driven by gaze, which directs\nattention and fixation to task-relevant regions and dramatically reduces visual\nprocessing. In contrast, robot learning systems typically rely on passive,\nuniform processing of raw camera images. In this work, we explore how\nincorporating human-like active gaze into robotic policies can enhance both\nefficiency and performance. We build on recent advances in foveated image\nprocessing and apply them to an Active Vision robot system that emulates both\nhuman head movement and eye tracking. Extending prior work on the AV-ALOHA\nrobot simulation platform, we introduce a framework for simultaneously\ncollecting eye-tracking data and robot demonstrations from a human operator as\nwell as a simulation benchmark and dataset for training robot policies that\nincorporate human gaze. Given the widespread use of Vision Transformers (ViTs)\nin robot learning, we integrate gaze information into ViTs using a foveated\npatch tokenization scheme inspired by recent work in image segmentation.\nCompared to uniform patch tokenization, this significantly reduces the number\nof tokens-and thus computation-without sacrificing visual fidelity near regions\nof interest. We also explore two approaches to gaze imitation and prediction\nfrom human data. The first is a two-stage model that predicts gaze to guide\nfoveation and action; the second integrates gaze into the action space,\nallowing the policy to jointly predict gaze and actions end-to-end. Our results\nshow that our method for foveated robot vision not only drastically reduces\ncomputational overhead, but also improves performance for high precision tasks\nand robustness to unseen distractors. Together, these findings suggest that\nhuman-inspired visual processing offers a useful inductive bias for robotic\nvision systems. https://ian-chuang.github.io/gaze-av-aloha/"
    },
    {
        "date": "2025-07",
        "title": "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization",
        "author": "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang",
        "link": "http://arxiv.org/abs/2507.15765v1",
        "abstract": "Dynamic Facial Expression Recognition (DFER) plays a critical role in\naffective computing and human-computer interaction. Although existing methods\nachieve comparable performance, they inevitably suffer from performance\ndegradation under sample heterogeneity caused by multi-source data and\nindividual expression variability. To address these challenges, we propose a\nnovel framework, called Heterogeneity-aware Distributional Framework (HDF), and\ndesign two plug-and-play modules to enhance time-frequency modeling and\nmitigate optimization imbalance caused by hard samples. Specifically, the\nTime-Frequency Distributional Attention Module (DAM) captures both temporal\nconsistency and frequency robustness through a dual-branch attention design,\nimproving tolerance to sequence inconsistency and visual style shifts. Then,\nbased on gradient sensitivity and information bottleneck principles, an\nadaptive optimization module Distribution-aware Scaling Module (DSM) is\nintroduced to dynamically balance classification and contrastive losses,\nenabling more stable and discriminative representation learning. Extensive\nexperiments on two widely used datasets, DFEW and FERV39k, demonstrate that HDF\nsignificantly improves both recognition accuracy and robustness. Our method\nachieves superior weighted average recall (WAR) and unweighted average recall\n(UAR) while maintaining strong generalization across diverse and imbalanced\nscenarios. Codes are released at https://github.com/QIcita/HDF_DFER."
    },
    {
        "date": "2025-07",
        "title": "Missing value imputation with adversarial random forests -- MissARF",
        "author": "Pegah Golchian, Jan Kapar, David S. Watson, and Marvin N. Wright",
        "link": "http://arxiv.org/abs/2507.15681v1",
        "abstract": "Handling missing values is a common challenge in biostatistical analyses,\ntypically addressed by imputation methods. We propose a novel, fast, and\neasy-to-use imputation method called missing value imputation with adversarial\nrandom forests (MissARF), based on generative machine learning, that provides\nboth single and multiple imputation. MissARF employs adversarial random forest\n(ARF) for density estimation and data synthesis. To impute a missing value of\nan observation, we condition on the non-missing values and sample from the\nestimated conditional distribution generated by ARF. Our experiments\ndemonstrate that MissARF performs comparably to state-of-the-art single and\nmultiple imputation methods in terms of imputation quality and fast runtime\nwith no additional costs for multiple imputation."
    },
    {
        "date": "2025-07",
        "title": "Cyber security of Mega Events: A Case Study of Securing the Digital Infrastructure for MahaKumbh 2025 -- A 45 days Mega Event of 600 Million Footfalls",
        "author": "Rohit Negi, Amit Negi, Manish Sharma, S. Venkatesan, Prem Kumar, and Sandeep K. Shukla",
        "link": "http://arxiv.org/abs/2507.15660v1",
        "abstract": "Mega events such as the Olympics, World Cup tournaments, G-20 Summit,\nreligious events such as MahaKumbh are increasingly digitalized. From event\nticketing, vendor booth or lodging reservations, sanitation, event scheduling,\ncustomer service, crime reporting, media streaming and messaging on digital\ndisplay boards, surveillance, crowd control, traffic control and many other\nservices are based on mobile and web applications, wired and wireless\nnetworking, network of Closed-Circuit Television (CCTV) cameras, specialized\ncontrol room with network and video-feed monitoring. Consequently, cyber\nthreats directed at such digital infrastructure are common. Starting from hobby\nhackers, hacktivists, cyber crime gangs, to the nation state actors, all target\nsuch infrastructure to unleash chaos on an otherwise smooth operation, and\noften the cyber threat actors attempt to embarrass the organizing country or\nthe organizers. Unlike long-standing organizations such as a corporate or a\ngovernment department, the infrastructure of mega-events is temporary,\nconstructed over a short time span in expediency, and often shortcuts are taken\nto make the deadline for the event. As a result, securing such an elaborate yet\ntemporary infrastructure requires a different approach than securing a standard\norganizational digital infrastructure. In this paper, we describe our approach\nto securing MahaKumbh 2025, a 600 million footfall event for 45 days in\nPrayagraj, India, as a cyber security assessment and risk management oversight\nteam. We chronicle the scope, process, methodology, and outcome of our team's\neffort to secure this mega event. It should be noted that none of the cyber\nattacks during the 45-day event was successful. Our goal is to put on record\nthe methodology and discuss what we would do differently in case we work on\nsimilar future mega event."
    },
    {
        "date": "2025-07",
        "title": "Multi-Stage Prompt Inference Attacks on Enterprise LLM Systems",
        "author": "Andrii Balashov, Olena Ponomarova, and Xiaohua Zhai",
        "link": "http://arxiv.org/abs/2507.15613v1",
        "abstract": "Large Language Models (LLMs) deployed in enterprise settings (e.g., as\nMicrosoft 365 Copilot) face novel security challenges. One critical threat is\nprompt inference attacks: adversaries chain together seemingly benign prompts\nto gradually extract confidential data. In this paper, we present a\ncomprehensive study of multi-stage prompt inference attacks in an enterprise\nLLM context. We simulate realistic attack scenarios where an attacker uses\nmild-mannered queries and indirect prompt injections to exploit an LLM\nintegrated with private corporate data. We develop a formal threat model for\nthese multi-turn inference attacks and analyze them using probability theory,\noptimization frameworks, and information-theoretic leakage bounds. The attacks\nare shown to reliably exfiltrate sensitive information from the LLM's context\n(e.g., internal SharePoint documents or emails), even when standard safety\nmeasures are in place.\n  We propose and evaluate defenses to counter such attacks, including\nstatistical anomaly detection, fine-grained access control, prompt sanitization\ntechniques, and architectural modifications to LLM deployment. Each defense is\nsupported by mathematical analysis or experimental simulation. For example, we\nderive bounds on information leakage under differential privacy-based training\nand demonstrate an anomaly detection method that flags multi-turn attacks with\nhigh AUC. We also introduce an approach called \"spotlighting\" that uses input\ntransformations to isolate untrusted prompt content, reducing attack success by\nan order of magnitude. Finally, we provide a formal proof of concept and\nempirical validation for a combined defense-in-depth strategy. Our work\nhighlights that securing LLMs in enterprise settings requires moving beyond\nsingle-turn prompt filtering toward a holistic, multi-stage perspective on both\nattacks and defenses."
    },
    {
        "date": "2025-07",
        "title": "EndoControlMag: Robust Endoscopic Vascular Motion Magnification with Periodic Reference Resetting and Hierarchical Tissue-aware Dual-Mask Contro",
        "author": "An Wang, Rulin Zhou, Mengya Xu, Yiru Ye, Longfei Gou, Yiting Chang, Hao Chen, Chwee Ming Lim, Jiankun Wang, and Hongliang Ren",
        "link": "http://arxiv.org/abs/2507.15292v3",
        "abstract": "Visualizing subtle vascular motions in endoscopic surgery is crucial for\nsurgical precision and decision-making, yet remains challenging due to the\ncomplex and dynamic nature of surgical scenes. To address this, we introduce\nEndoControlMag, a training-free, Lagrangian-based framework with\nmask-conditioned vascular motion magnification tailored to endoscopic\nenvironments. Our approach features two key modules: a Periodic Reference\nResetting (PRR) scheme that divides videos into short overlapping clips with\ndynamically updated reference frames to prevent error accumulation while\nmaintaining temporal coherence, and a Hierarchical Tissue-aware Magnification\n(HTM) framework with dual-mode mask dilation. HTM first tracks vessel cores\nusing a pretrained visual tracking model to maintain accurate localization\ndespite occlusions and view changes. It then applies one of two adaptive\nsoftening strategies to surrounding tissues: motion-based softening that\nmodulates magnification strength proportional to observed tissue displacement,\nor distance-based exponential decay that simulates biomechanical force\nattenuation. This dual-mode approach accommodates diverse surgical\nscenarios-motion-based softening excels with complex tissue deformations while\ndistance-based softening provides stability during unreliable optical flow\nconditions. We evaluate EndoControlMag on our EndoVMM24 dataset spanning four\ndifferent surgery types and various challenging scenarios, including\nocclusions, instrument disturbance, view changes, and vessel deformations.\nQuantitative metrics, visual assessments, and expert surgeon evaluations\ndemonstrate that EndoControlMag significantly outperforms existing methods in\nboth magnification accuracy and visual quality while maintaining robustness\nacross challenging surgical conditions. The code, dataset, and video results\nare available at https://szupc.github.io/EndoControlMag/."
    },
    {
        "date": "2025-07",
        "title": "In-context Learning of Vision Language Models for Detection of Physical and Digital Attacks against Face Recognition Systems",
        "author": "Lazaro Janier Gonzalez-Soler, Maciej Salwowski, and Christoph Busch",
        "link": "http://arxiv.org/abs/2507.15285v1",
        "abstract": "Recent advances in biometric systems have significantly improved the\ndetection and prevention of fraudulent activities. However, as detection\nmethods improve, attack techniques become increasingly sophisticated. Attacks\non face recognition systems can be broadly divided into physical and digital\napproaches. Traditionally, deep learning models have been the primary defence\nagainst such attacks. While these models perform exceptionally well in\nscenarios for which they have been trained, they often struggle to adapt to\ndifferent types of attacks or varying environmental conditions. These\nsubsystems require substantial amounts of training data to achieve reliable\nperformance, yet biometric data collection faces significant challenges,\nincluding privacy concerns and the logistical difficulties of capturing diverse\nattack scenarios under controlled conditions. This work investigates the\napplication of Vision Language Models (VLM) and proposes an in-context learning\nframework for detecting physical presentation attacks and digital morphing\nattacks in biometric systems. Focusing on open-source models, the first\nsystematic framework for the quantitative evaluation of VLMs in\nsecurity-critical scenarios through in-context learning techniques is\nestablished. The experimental evaluation conducted on freely available\ndatabases demonstrates that the proposed subsystem achieves competitive\nperformance for physical and digital attack detection, outperforming some of\nthe traditional CNNs without resource-intensive training. The experimental\nresults validate the proposed framework as a promising tool for improving\ngeneralisation in attack detection."
    },
    {
        "date": "2025-07",
        "title": "Robust and Differentially Private PCA for non-Gaussian data",
        "author": "Minwoo Kim, and Sungkyu Jung",
        "link": "http://arxiv.org/abs/2507.15232v1",
        "abstract": "Recent advances have sparked significant interest in the development of\nprivacy-preserving Principal Component Analysis (PCA). However, many existing\napproaches rely on restrictive assumptions, such as assuming sub-Gaussian data\nor being vulnerable to data contamination. Additionally, some methods are\ncomputationally expensive or depend on unknown model parameters that must be\nestimated, limiting their accessibility for data analysts seeking\nprivacy-preserving PCA. In this paper, we propose a differentially private PCA\nmethod applicable to heavy-tailed and potentially contaminated data. Our\napproach leverages the property that the covariance matrix of properly rescaled\ndata preserves eigenvectors and their order under elliptical distributions,\nwhich include Gaussian and heavy-tailed distributions. By applying a bounded\ntransformation, we enable straightforward computation of principal components\nin a differentially private manner. Additionally, boundedness guarantees\nrobustness against data contamination. We conduct both theoretical analysis and\nempirical evaluations of the proposed method, focusing on its ability to\nrecover the subspace spanned by the leading principal components. Extensive\nnumerical experiments demonstrate that our method consistently outperforms\nexisting approaches in terms of statistical utility, particularly in\nnon-Gaussian or contaminated data settings."
    },
    {
        "date": "2025-07",
        "title": "PromptArmor: Simple yet Effective Prompt Injection Defenses",
        "author": "Tianneng Shi, Kaijie Zhu, Zhun Wang, Yuqi Jia, Will Cai, Weida Liang, Haonan Wang, Hend Alzahrani, Joshua Lu, Kenji Kawaguchi, Basel Alomair, Xuandong Zhao, William Yang Wang, Neil Gong, Wenbo Guo, and Dawn Song",
        "link": "http://arxiv.org/abs/2507.15219v1",
        "abstract": "Despite their potential, recent research has demonstrated that LLM agents are\nvulnerable to prompt injection attacks, where malicious prompts are injected\ninto the agent's input, causing it to perform an attacker-specified task rather\nthan the intended task provided by the user. In this paper, we present\nPromptArmor, a simple yet effective defense against prompt injection attacks.\nSpecifically, PromptArmor prompts an off-the-shelf LLM to detect and remove\npotential injected prompts from the input before the agent processes it. Our\nresults show that PromptArmor can accurately identify and remove injected\nprompts. For example, using GPT-4o, GPT-4.1, or o4-mini, PromptArmor achieves\nboth a false positive rate and a false negative rate below 1% on the AgentDojo\nbenchmark. Moreover, after removing injected prompts with PromptArmor, the\nattack success rate drops to below 1%. We also demonstrate PromptArmor's\neffectiveness against adaptive attacks and explore different strategies for\nprompting an LLM. We recommend that PromptArmor be adopted as a standard\nbaseline for evaluating new defenses against prompt injection attacks."
    },
    {
        "date": "2025-07",
        "title": "Exploiting Context-dependent Duration Features for Voice Anonymization Attack Systems",
        "author": "Natalia Tomashenko, Emmanuel Vincent, and Marc Tommasi",
        "link": "http://arxiv.org/abs/2507.15214v1",
        "abstract": "The temporal dynamics of speech, encompassing variations in rhythm,\nintonation, and speaking rate, contain important and unique information about\nspeaker identity. This paper proposes a new method for representing speaker\ncharacteristics by extracting context-dependent duration embeddings from speech\ntemporal dynamics. We develop novel attack models using these representations\nand analyze the potential vulnerabilities in speaker verification and voice\nanonymization systems.The experimental results show that the developed attack\nmodels provide a significant improvement in speaker verification performance\nfor both original and anonymized data in comparison with simpler\nrepresentations of speech temporal dynamics reported in the literature."
    },
    {
        "date": "2025-07",
        "title": "Adaptive Network Security Policies via Belief Aggregation and Rollout",
        "author": "Kim Hammar, Yuchao Li, Tansu Alpcan, Emil C. Lupu, and Dimitri Bertsekas",
        "link": "http://arxiv.org/abs/2507.15163v1",
        "abstract": "Evolving security vulnerabilities and shifting operational conditions require\nfrequent updates to network security policies. These updates include\nadjustments to incident response procedures and modifications to access\ncontrols, among others. Reinforcement learning methods have been proposed for\nautomating such policy adaptations, but most of the methods in the research\nliterature lack performance guarantees and adapt slowly to changes. In this\npaper, we address these limitations and present a method for computing security\npolicies that is scalable, offers theoretical guarantees, and adapts quickly to\nchanges. It assumes a model or simulator of the system and comprises three\ncomponents: belief estimation through particle filtering, offline policy\ncomputation through aggregation, and online policy adaptation through rollout.\nCentral to our method is a new feature-based aggregation technique, which\nimproves scalability and flexibility. We analyze the approximation error of\naggregation and show that rollout efficiently adapts policies to changes under\ncertain conditions. Simulations and testbed results demonstrate that our method\noutperforms state-of-the-art methods on several benchmarks, including CAGE-2."
    },
    {
        "date": "2025-07",
        "title": "Quantum Machine Learning for Secure Cooperative Multi-Layer Edge AI with Proportional Fairness",
        "author": "Thai T. Vu, and John Le",
        "link": "http://arxiv.org/abs/2507.15145v1",
        "abstract": "This paper proposes a communication-efficient, event-triggered inference\nframework for cooperative edge AI systems comprising multiple user devices and\nedge servers. Building upon dual-threshold early-exit strategies for rare-event\ndetection, the proposed approach extends classical single-device inference to a\ndistributed, multi-device setting while incorporating proportional fairness\nconstraints across users. A joint optimization framework is formulated to\nmaximize classification utility under communication, energy, and fairness\nconstraints. To solve the resulting problem efficiently, we exploit the\nmonotonicity of the utility function with respect to the confidence thresholds\nand apply alternating optimization with Benders decomposition. Experimental\nresults show that the proposed framework significantly enhances system-wide\nperformance and fairness in resource allocation compared to single-device\nbaselines."
    },
    {
        "date": "2025-07",
        "title": "From Kicking to Causality: Simulating Infant Agency Detection with a Robust Intrinsic Reward",
        "author": "Xia Xu, and Jochen Triesch",
        "link": "http://arxiv.org/abs/2507.15106v1",
        "abstract": "While human infants robustly discover their own causal efficacy, standard\nreinforcement learning agents remain brittle, as their reliance on\ncorrelation-based rewards fails in noisy, ecologically valid scenarios. To\naddress this, we introduce the Causal Action Influence Score (CAIS), a novel\nintrinsic reward rooted in causal inference. CAIS quantifies an action's\ninfluence by measuring the 1-Wasserstein distance between the learned\ndistribution of sensory outcomes conditional on that action, $p(h|a)$, and the\nbaseline outcome distribution, $p(h)$. This divergence provides a robust reward\nthat isolates the agent's causal impact from confounding environmental noise.\nWe test our approach in a simulated infant-mobile environment where\ncorrelation-based perceptual rewards fail completely when the mobile is\nsubjected to external forces. In stark contrast, CAIS enables the agent to\nfilter this noise, identify its influence, and learn the correct policy.\nFurthermore, the high-quality predictive model learned for CAIS allows our\nagent, when augmented with a surprise signal, to successfully reproduce the\n\"extinction burst\" phenomenon. We conclude that explicitly inferring causality\nis a crucial mechanism for developing a robust sense of agency, offering a\npsychologically plausible framework for more adaptive autonomous systems."
    },
    {
        "date": "2025-07",
        "title": "Robust Control with Gradient Uncertainty",
        "author": "Qian Qi",
        "link": "http://arxiv.org/abs/2507.15082v1",
        "abstract": "We introduce a novel extension to robust control theory that explicitly\naddresses uncertainty in the value function's gradient, a form of uncertainty\nendemic to applications like reinforcement learning where value functions are\napproximated. We formulate a zero-sum dynamic game where an adversary perturbs\nboth system dynamics and the value function gradient, leading to a new, highly\nnonlinear partial differential equation: the Hamilton-Jacobi-Bellman-Isaacs\nEquation with Gradient Uncertainty (GU-HJBI). We establish its well-posedness\nby proving a comparison principle for its viscosity solutions under a uniform\nellipticity condition. Our analysis of the linear-quadratic (LQ) case yields a\nkey insight: we prove that the classical quadratic value function assumption\nfails for any non-zero gradient uncertainty, fundamentally altering the problem\nstructure. A formal perturbation analysis characterizes the non-polynomial\ncorrection to the value function and the resulting nonlinearity of the optimal\ncontrol law, which we validate with numerical studies. Finally, we bridge\ntheory to practice by proposing a novel Gradient-Uncertainty-Robust\nActor-Critic (GURAC) algorithm, accompanied by an empirical study demonstrating\nits effectiveness in stabilizing training. This work provides a new direction\nfor robust control, holding significant implications for fields where function\napproximation is common, including reinforcement learning and computational\nfinance."
    },
    {
        "date": "2025-07",
        "title": "ROBAD: Robust Adversary-aware Local-Global Attended Bad Actor Detection Sequential Model",
        "author": "Bing He, Mustaque Ahamad, and Srijan Kumar",
        "link": "http://arxiv.org/abs/2507.15067v1",
        "abstract": "Detecting bad actors is critical to ensure the safety and integrity of\ninternet platforms. Several deep learning-based models have been developed to\nidentify such users. These models should not only accurately detect bad actors,\nbut also be robust against adversarial attacks that aim to evade detection.\nHowever, past deep learning-based detection models do not meet the robustness\nrequirement because they are sensitive to even minor changes in the input\nsequence. To address this issue, we focus on (1) improving the model\nunderstanding capability and (2) enhancing the model knowledge such that the\nmodel can recognize potential input modifications when making predictions. To\nachieve these goals, we create a novel transformer-based classification model,\ncalled ROBAD (RObust adversary-aware local-global attended Bad Actor Detection\nmodel), which uses the sequence of user posts to generate user embedding to\ndetect bad actors. Particularly, ROBAD first leverages the transformer encoder\nblock to encode each post bidirectionally, thus building a post embedding to\ncapture the local information at the post level. Next, it adopts the\ntransformer decoder block to model the sequential pattern in the post\nembeddings by using the attention mechanism, which generates the sequence\nembedding to obtain the global information at the sequence level. Finally, to\nenrich the knowledge of the model, embeddings of modified sequences by mimicked\nattackers are fed into a contrastive-learning-enhanced classification layer for\nsequence prediction. In essence, by capturing the local and global information\n(i.e., the post and sequence information) and leveraging the mimicked behaviors\nof bad actors in training, ROBAD can be robust to adversarial attacks.\nExtensive experiments on Yelp and Wikipedia datasets show that ROBAD can\neffectively detect bad actors when under state-of-the-art adversarial attacks."
    },
    {
        "date": "2025-07",
        "title": "DeRAG: Black-box Adversarial Attacks on Multiple Retrieval-Augmented Generation Applications via Prompt Injection",
        "author": "Jerry Wang, and Fang Yu",
        "link": "http://arxiv.org/abs/2507.15042v1",
        "abstract": "Adversarial prompt attacks can significantly alter the reliability of\nRetrieval-Augmented Generation (RAG) systems by re-ranking them to produce\nincorrect outputs. In this paper, we present a novel method that applies\nDifferential Evolution (DE) to optimize adversarial prompt suffixes for\nRAG-based question answering. Our approach is gradient-free, treating the RAG\npipeline as a black box and evolving a population of candidate suffixes to\nmaximize the retrieval rank of a targeted incorrect document to be closer to\nreal world scenarios. We conducted experiments on the BEIR QA datasets to\nevaluate attack success at certain retrieval rank thresholds under multiple\nretrieving applications. Our results demonstrate that DE-based prompt\noptimization attains competitive (and in some cases higher) success rates\ncompared to GGPP to dense retrievers and PRADA to sparse retrievers, while\nusing only a small number of tokens (<=5 tokens) in the adversarial suffix.\nFurthermore, we introduce a readability-aware suffix construction strategy,\nvalidated by a statistically significant reduction in MLM negative\nlog-likelihood with Welch's t-test. Through evaluations with a BERT-based\nadversarial suffix detector, we show that DE-generated suffixes evade\ndetection, yielding near-chance detection accuracy."
    },
    {
        "date": "2025-07",
        "title": "Metaverse Security and Privacy Research: A Systematic Review",
        "author": "Argianto Rahartomo, Leonel Merino, and Mohammad Ghafari",
        "link": "http://arxiv.org/abs/2507.14985v1",
        "abstract": "The rapid growth of metaverse technologies, including virtual worlds,\naugmented reality, and lifelogging, has accelerated their adoption across\ndiverse domains. This rise exposes users to significant new security and\nprivacy challenges due to sociotechnical complexity, pervasive connectivity,\nand extensive user data collection in immersive environments. We present a\nsystematic review of the literature published between 2013 and 2024, offering a\ncomprehensive analysis of how the research community has addressed\nmetaverse-related security and privacy issues over the past decade. We organize\nthe studies by method, examined the security and privacy properties, immersive\ncomponents, and evaluation strategies. Our investigation reveals a sharp\nincrease in research activity in the last five years, a strong focus on\npractical and user-centered approaches, and a predominant use of benchmarking,\nhuman experimentation, and qualitative methods. Authentication and\nunobservability are the most frequently studied properties. However, critical\ngaps remain in areas such as policy compliance, accessibility,\ninteroperability, and back-end infrastructure security. We emphasize the\nintertwined technical complexity and human factors of the metaverse and call\nfor integrated, interdisciplinary approaches to securing inclusive and\ntrustworthy immersive environments."
    },
    {
        "date": "2025-07",
        "title": "Byzantine-Robust Decentralized Coordination of LLM Agents",
        "author": "Yongrae Jo, and Chanik Park",
        "link": "http://arxiv.org/abs/2507.14928v1",
        "abstract": "Collaboration among multiple large language model (LLM) agents is a promising\napproach to overcome inherent limitations of single-agent systems, such as\nhallucinations and single points of failure. As LLM agents are increasingly\ndeployed on open blockchain platforms, multi-agent systems capable of\ntolerating malicious (Byzantine) agents have become essential.\n  Recent Byzantine-robust multi-agent systems typically rely on leader-driven\ncoordination, which suffers from two major drawbacks. First, they are\ninherently vulnerable to targeted attacks against the leader. If consecutive\nleaders behave maliciously, the system repeatedly fails to achieve consensus,\nforcing new consensus rounds, which is particularly costly given the high\nlatency of LLM invocations. Second, an underperforming proposal from the leader\ncan be accepted as the final answer even when higher-quality alternatives are\navailable, as existing methods finalize the leader's proposal once it receives\na quorum of votes.\n  To address these issues, we propose DecentLLMs, a novel decentralized\nconsensus approach for multi-agent LLM systems, where worker agents generate\nanswers concurrently and evaluator agents independently score and rank these\nanswers to select the best available one. This decentralized architecture\nenables faster consensus despite the presence of Byzantine agents and\nconsistently selects higher-quality answers through Byzantine-robust\naggregation techniques.\n  Experimental results demonstrate that DecentLLMs effectively tolerates\nByzantine agents and significantly improves the quality of selected answers."
    },
    {
        "date": "2025-07",
        "title": "BeatFormer: Efficient motion-robust remote heart rate estimation through unsupervised spectral zoomed attention filters",
        "author": "Joaquim Comas, and Federico Sukno",
        "link": "http://arxiv.org/abs/2507.14885v1",
        "abstract": "Remote photoplethysmography (rPPG) captures cardiac signals from facial\nvideos and is gaining attention for its diverse applications. While deep\nlearning has advanced rPPG estimation, it relies on large, diverse datasets for\neffective generalization. In contrast, handcrafted methods utilize\nphysiological priors for better generalization in unseen scenarios like motion\nwhile maintaining computational efficiency. However, their linear assumptions\nlimit performance in complex conditions, where deep learning provides superior\npulsatile information extraction. This highlights the need for hybrid\napproaches that combine the strengths of both methods. To address this, we\npresent BeatFormer, a lightweight spectral attention model for rPPG estimation,\nwhich integrates zoomed orthonormal complex attention and frequency-domain\nenergy measurement, enabling a highly efficient model. Additionally, we\nintroduce Spectral Contrastive Learning (SCL), which allows BeatFormer to be\ntrained without any PPG or HR labels. We validate BeatFormer on the PURE,\nUBFC-rPPG, and MMPD datasets, demonstrating its robustness and performance,\nparticularly in cross-dataset evaluations under motion scenarios."
    },
    {
        "date": "2025-07",
        "title": "A Privacy-Centric Approach: Scalable and Secure Federated Learning Enabled by Hybrid Homomorphic Encryption",
        "author": "Khoa Nguyen, Tanveer Khan, and Antonis Michalas",
        "link": "http://arxiv.org/abs/2507.14853v1",
        "abstract": "Federated Learning (FL) enables collaborative model training without sharing\nraw data, making it a promising approach for privacy-sensitive domains. Despite\nits potential, FL faces significant challenges, particularly in terms of\ncommunication overhead and data privacy. Privacy-preserving Techniques (PPTs)\nsuch as Homomorphic Encryption (HE) have been used to mitigate these concerns.\nHowever, these techniques introduce substantial computational and communication\ncosts, limiting their practical deployment. In this work, we explore how Hybrid\nHomomorphic Encryption (HHE), a cryptographic protocol that combines symmetric\nencryption with HE, can be effectively integrated with FL to address both\ncommunication and privacy challenges, paving the way for scalable and secure\ndecentralized learning system."
    },
    {
        "date": "2025-07",
        "title": "Manipulating LLM Web Agents with Indirect Prompt Injection Attack via HTML Accessibility Tree",
        "author": "Sam Johnson, Viet Pham, and Thai Le",
        "link": "http://arxiv.org/abs/2507.14799v1",
        "abstract": "This work demonstrates that LLM-based web navigation agents offer powerful\nautomation capabilities but are vulnerable to Indirect Prompt Injection (IPI)\nattacks. We show that adversaries can embed universal adversarial triggers in\nwebpage HTML to hijack agent behavior that utilizes the accessibility tree to\nparse HTML, causing unintended or malicious actions. Using the Greedy\nCoordinate Gradient (GCG) algorithm and a Browser Gym agent powered by\nLlama-3.1, our system demonstrates high success rates across real websites in\nboth targeted and general attacks, including login credential exfiltration and\nforced ad clicks. Our empirical results highlight critical security risks and\nthe need for stronger defenses as LLM-driven autonomous web agents become more\nwidely adopted. The system software\n(https://github.com/sej2020/manipulating-web-agents) is released under the MIT\nLicense, with an accompanying publicly available demo website\n(http://lethaiq.github.io/attack-web-llm-agent)."
    },
    {
        "date": "2025-07",
        "title": "Collusion-Resilient Hierarchical Secure Aggregation with Heterogeneous Security Constraints",
        "author": "Zhou Li, Xiang Zhang, Jiawen Lv, Jihao Fan, Haiqiang Chen, and Giuseppe Caire",
        "link": "http://arxiv.org/abs/2507.14768v1",
        "abstract": "Motivated by federated learning (FL), secure aggregation (SA) aims to\nsecurely compute, as efficiently as possible, the sum of a set of inputs\ndistributed across many users. To understand the impact of network topology,\nhierarchical secure aggregation (HSA) investigated the communication and secret\nkey generation efficiency in a 3-layer relay network, where clusters of users\nare connected to the aggregation server through an intermediate layer of\nrelays. Due to the pre-aggregation of the messages at the relays, HSA reduces\nthe communication burden on the relay-to-server links and is able to support a\nlarge number of users. However, as the number of users increases, a practical\nchallenge arises from heterogeneous security requirements--for example, users\nin different clusters may require varying levels of input protection. Motivated\nby this, we study weakly-secure HSA (WS-HSA) with collusion resilience, where\ninstead of protecting all the inputs from any set of colluding users, only the\ninputs belonging to a predefined collection of user groups (referred to as\nsecurity input sets) need to be protected against another predefined collection\nof user groups (referred to as collusion sets). Since the security input sets\nand collusion sets can be arbitrarily defined, our formulation offers a\nflexible framework for addressing heterogeneous security requirements in HSA.\nWe characterize the optimal total key rate, i.e., the total number of\nindependent key symbols required to ensure both server and relay security, for\na broad range of parameter configurations. For the remaining cases, we\nestablish lower and upper bounds on the optimal key rate, providing\nconstant-factor gap optimality guarantees."
    },
    {
        "date": "2025-07",
        "title": "Analyzing Internal Activity and Robustness of SNNs Across Neuron Parameter Space",
        "author": "Szymon Mazurek, Jakub Caputa, and Maciej Wielgosz",
        "link": "http://arxiv.org/abs/2507.14757v1",
        "abstract": "Spiking Neural Networks (SNNs) offer energy-efficient and biologically\nplausible alternatives to traditional artificial neural networks, but their\nperformance depends critically on the tuning of neuron model parameters. In\nthis work, we identify and characterize an operational space - a constrained\nregion in the neuron hyperparameter domain (specifically membrane time constant\ntau and voltage threshold vth) - within which the network exhibits meaningful\nactivity and functional behavior. Operating inside this manifold yields optimal\ntrade-offs between classification accuracy and spiking activity, while stepping\noutside leads to degeneration: either excessive energy use or complete network\nsilence.\n  Through systematic exploration across datasets and architectures, we\nvisualize and quantify this manifold and identify efficient operating points.\nWe further assess robustness to adversarial noise, showing that SNNs exhibit\nincreased spike correlation and internal synchrony when operating outside their\noptimal region. These findings highlight the importance of principled\nhyperparameter tuning to ensure both task performance and energy efficiency.\nOur results offer practical guidelines for deploying robust and efficient SNNs,\nparticularly in neuromorphic computing scenarios."
    },
    {
        "date": "2025-07",
        "title": "CANDoSA: A Hardware Performance Counter-Based Intrusion Detection System for DoS Attacks on Automotive CAN bus",
        "author": "Franco Oberti, Stefano Di Carlo, and Alessandro Savino",
        "link": "http://arxiv.org/abs/2507.14739v1",
        "abstract": "The Controller Area Network (CAN) protocol, essential for automotive embedded\nsystems, lacks inherent security features, making it vulnerable to cyber\nthreats, especially with the rise of autonomous vehicles. Traditional security\nmeasures offer limited protection, such as payload encryption and message\nauthentication. This paper presents a novel Intrusion Detection System (IDS)\ndesigned for the CAN environment, utilizing Hardware Performance Counters\n(HPCs) to detect anomalies indicative of cyber attacks. A RISC-V-based CAN\nreceiver is simulated using the gem5 simulator, processing CAN frame payloads\nwith AES-128 encryption as FreeRTOS tasks, which trigger distinct HPC\nresponses. Key HPC features are optimized through data extraction and\ncorrelation analysis to enhance classification efficiency. Results indicate\nthat this approach could significantly improve CAN security and address\nemerging challenges in automotive cybersecurity."
    },
    {
        "date": "2025-07",
        "title": "Balancing Expressivity and Robustness: Constrained Rational Activations for Reinforcement Learning",
        "author": "Rafa\u0142 Surdej, Micha\u0142 Bortkiewicz, Alex Lewandowski, Mateusz Ostaszewski, and Clare Lyle",
        "link": "http://arxiv.org/abs/2507.14736v1",
        "abstract": "Trainable activation functions, whose parameters are optimized alongside\nnetwork weights, offer increased expressivity compared to fixed activation\nfunctions. Specifically, trainable activation functions defined as ratios of\npolynomials (rational functions) have been proposed to enhance plasticity in\nreinforcement learning. However, their impact on training stability remains\nunclear. In this work, we study trainable rational activations in both\nreinforcement and continual learning settings. We find that while their\nflexibility enhances adaptability, it can also introduce instability, leading\nto overestimation in RL and feature collapse in longer continual learning\nscenarios. Our main result is demonstrating a trade-off between expressivity\nand plasticity in rational activations. To address this, we propose a\nconstrained variant that structurally limits excessive output scaling while\npreserving adaptability. Experiments across MetaWorld and DeepMind Control\nSuite (DMC) environments show that our approach improves training stability and\nperformance. In continual learning benchmarks, including MNIST with reshuffled\nlabels and Split CIFAR-100, we reveal how different constraints affect the\nbalance between expressivity and long-term retention. While preliminary\nexperiments in discrete action domains (e.g., Atari) did not show similar\ninstability, this suggests that the trade-off is particularly relevant for\ncontinuous control. Together, our findings provide actionable design principles\nfor robust and adaptable trainable activations in dynamic, non-stationary\nenvironments. Code available at:\nhttps://github.com/special114/rl_rational_plasticity."
    },
    {
        "date": "2025-07",
        "title": "VTarbel: Targeted Label Attack with Minimal Knowledge on Detector-enhanced Vertical Federated Learning",
        "author": "Juntao Tan, Anran Li, Quanchao Liu, Peng Ran, and Lan Zhang",
        "link": "http://arxiv.org/abs/2507.14625v1",
        "abstract": "Vertical federated learning (VFL) enables multiple parties with disjoint\nfeatures to collaboratively train models without sharing raw data. While\nprivacy vulnerabilities of VFL are extensively-studied, its security\nthreats-particularly targeted label attacks-remain underexplored. In such\nattacks, a passive party perturbs inputs at inference to force\nmisclassification into adversary-chosen labels. Existing methods rely on\nunrealistic assumptions (e.g., accessing VFL-model's outputs) and ignore\nanomaly detectors deployed in real-world systems. To bridge this gap, we\nintroduce VTarbel, a two-stage, minimal-knowledge attack framework explicitly\ndesigned to evade detector-enhanced VFL inference. During the preparation\nstage, the attacker selects a minimal set of high-expressiveness samples (via\nmaximum mean discrepancy), submits them through VFL protocol to collect\npredicted labels, and uses these pseudo-labels to train estimated detector and\nsurrogate model on local features. In attack stage, these models guide\ngradient-based perturbations of remaining samples, crafting adversarial\ninstances that induce targeted misclassifications and evade detection. We\nimplement VTarbel and evaluate it against four model architectures, seven\nmultimodal datasets, and two anomaly detectors. Across all settings, VTarbel\noutperforms four state-of-the-art baselines, evades detection, and retains\neffective against three representative privacy-preserving defenses. These\nresults reveal critical security blind spots in current VFL deployments and\nunderscore urgent need for robust, attack-aware defenses."
    },
    {
        "date": "2025-07",
        "title": "Hybrid Classical-Quantum Rainbow Table Attack on Human Passwords",
        "author": "MA. Khajeian",
        "link": "http://arxiv.org/abs/2507.14600v1",
        "abstract": "Passwords that are long and human-generated pose a challenge for both\nclassical and quantum attacks due to their irregular structure and large search\nspace. In this work, we present an enhanced classical-quantum hybrid attack\ntailored to this scenario. We build rainbow tables using dictionary-based\npassword generation with transformation rules to better model real user\nbehavior. These tables are then organized into buckets, enabling faster lookup\nand reduced space complexity. To perform quantum search within each bucket, we\nuse a distributed exact variant of Grover's algorithm, which offers lower\ncircuit depth and deterministic success. As a result, the overall quantum\ncircuit is shallower and more robust against noise, particularly from\ndepolarizing channels commonly found in near-term quantum devices. Through this\nwork, Overall, we propose a hybrid framework that combines structured rainbow\ntables with efficient quantum search to enhance password recovery."
    },
    {
        "date": "2025-07",
        "title": "VisGuard: Securing Visualization Dissemination through Tamper-Resistant Data Retrieval",
        "author": "Huayuan Ye, Juntong Chen, Shenzhuo Zhang, Yipeng Zhang, Changbo Wang, and Chenhui Li",
        "link": "http://arxiv.org/abs/2507.14459v1",
        "abstract": "The dissemination of visualizations is primarily in the form of raster\nimages, which often results in the loss of critical information such as source\ncode, interactive features, and metadata. While previous methods have proposed\nembedding metadata into images to facilitate Visualization Image Data Retrieval\n(VIDR), most existing methods lack practicability since they are fragile to\ncommon image tampering during online distribution such as cropping and editing.\nTo address this issue, we propose VisGuard, a tamper-resistant VIDR framework\nthat reliably embeds metadata link into visualization images. The embedded data\nlink remains recoverable even after substantial tampering upon images. We\npropose several techniques to enhance robustness, including repetitive data\ntiling, invertible information broadcasting, and an anchor-based scheme for\ncrop localization. VisGuard enables various applications, including interactive\nchart reconstruction, tampering detection, and copyright protection. We conduct\ncomprehensive experiments on VisGuard's superior performance in data retrieval\naccuracy, embedding capacity, and security against tampering and steganalysis,\ndemonstrating VisGuard's competence in facilitating and safeguarding\nvisualization dissemination and information conveyance."
    },
    {
        "date": "2025-07",
        "title": "GPI-Net: Gestalt-Guided Parallel Interaction Network via Orthogonal Geometric Consistency for Robust Point Cloud Registration",
        "author": "Weikang Gu, Mingyue Han, Li Xue, Heng Dong, Changcai Yang, Riqing Chen, and Lifang Wei",
        "link": "http://arxiv.org/abs/2507.14452v1",
        "abstract": "The accurate identification of high-quality correspondences is a prerequisite\ntask in feature-based point cloud registration. However, it is extremely\nchallenging to handle the fusion of local and global features due to feature\nredundancy and complex spatial relationships. Given that Gestalt principles\nprovide key advantages in analyzing local and global relationships, we propose\na novel Gestalt-guided Parallel Interaction Network via orthogonal geometric\nconsistency (GPI-Net) in this paper. It utilizes Gestalt principles to\nfacilitate complementary communication between local and global information.\nSpecifically, we introduce an orthogonal integration strategy to optimally\nreduce redundant information and generate a more compact global structure for\nhigh-quality correspondences. To capture geometric features in correspondences,\nwe leverage a Gestalt Feature Attention (GFA) block through a hybrid\nutilization of self-attention and cross-attention mechanisms. Furthermore, to\nfacilitate the integration of local detail information into the global\nstructure, we design an innovative Dual-path Multi-Granularity parallel\ninteraction aggregation (DMG) block to promote information exchange across\ndifferent granularities. Extensive experiments on various challenging tasks\ndemonstrate the superior performance of our proposed GPI-Net in comparison to\nexisting methods. The code will be released at https://github.com/gwk/GPI-Net."
    },
    {
        "date": "2025-07",
        "title": "Topological Social Choice: Designing a Noise-Robust Polar Distance for Persistence Diagrams",
        "author": "Athanasios Andrikopoulos, and Nikolaos Sampanis",
        "link": "http://arxiv.org/abs/2507.14340v1",
        "abstract": "Topological Data Analysis (TDA) has emerged as a powerful framework for\nextracting robust and interpretable features from noisy high-dimensional data.\nIn the context of Social Choice Theory, where preference profiles and\ncollective decisions are geometrically rich yet sensitive to perturbations, TDA\nremains largely unexplored. This work introduces a novel conceptual bridge\nbetween these domains by proposing a new metric framework for persistence\ndiagrams tailored to noisy preference data.We define a polar coordinate-based\ndistance that captures both the magnitude and orientation of topological\nfeatures in a smooth and differentiable manner. Our metric addresses key\nlimitations of classical distances, such as bottleneck and Wasserstein,\nincluding instability under perturbation, lack of continuity, and\nincompatibility with gradient-based learning. The resulting formulation offers\nimproved behavior in both theoretical and applied settings.To the best of our\nknowledge, this is the first study to systematically apply persistent homology\nto social choice systems, providing a mathematically grounded method for\ncomparing topological summaries of voting structures and preference dynamics.\nWe demonstrate the superiority of our approach through extensive experiments,\nincluding robustness tests and supervised learning tasks, and we propose a\nmodular pipeline for building predictive models from online preference data.\nThis work contributes a conceptually novel and computationally effective tool\nto the emerging interface of topology and decision theory, opening new\ndirections in interpretable machine learning for political and economic\nsystems."
    },
    {
        "date": "2025-07",
        "title": "FedStrategist: A Meta-Learning Framework for Adaptive and Robust Aggregation in Federated Learning",
        "author": "Md Rafid Haque, Abu Raihan Mostofa Kamal, and Md. Azam Hossain",
        "link": "http://arxiv.org/abs/2507.14322v1",
        "abstract": "Federated Learning (FL) offers a paradigm for privacy-preserving\ncollaborative AI, but its decentralized nature creates significant\nvulnerabilities to model poisoning attacks. While numerous static defenses\nexist, their effectiveness is highly context-dependent, often failing against\nadaptive adversaries or in heterogeneous data environments. This paper\nintroduces FedStrategist, a novel meta-learning framework that reframes robust\naggregation as a real-time, cost-aware control problem. We design a lightweight\ncontextual bandit agent that dynamically selects the optimal aggregation rule\nfrom an arsenal of defenses based on real-time diagnostic metrics. Through\ncomprehensive experiments, we demonstrate that no single static rule is\nuniversally optimal. We show that our adaptive agent successfully learns\nsuperior policies across diverse scenarios, including a ``Krum-favorable\"\nenvironment and against a sophisticated \"stealth\" adversary designed to\nneutralize specific diagnostic signals. Critically, we analyze the paradoxical\nscenario where a non-robust baseline achieves high but compromised accuracy,\nand demonstrate that our agent learns a conservative policy to prioritize model\nintegrity. Furthermore, we prove the agent's policy is controllable via a\nsingle \"risk tolerance\" parameter, allowing practitioners to explicitly manage\nthe trade-off between performance and security. Our work provides a new,\npractical, and analyzable approach to creating resilient and intelligent\ndecentralized AI systems."
    },
    {
        "date": "2025-07",
        "title": "CLIPTTA: Robust Contrastive Vision-Language Test-Time Adaptation",
        "author": "Marc Lafon, Gustavo Adolfo Vargas Hakim, Cl\u00e9ment Rambour, Christian Desrosier, and Nicolas Thome",
        "link": "http://arxiv.org/abs/2507.14312v1",
        "abstract": "Vision-language models (VLMs) like CLIP exhibit strong zero-shot capabilities\nbut often fail to generalize under distribution shifts. Test-time adaptation\n(TTA) allows models to update at inference time without labeled data, typically\nvia entropy minimization. However, this objective is fundamentally misaligned\nwith the contrastive image-text training of VLMs, limiting adaptation\nperformance and introducing failure modes such as pseudo-label drift and class\ncollapse. We propose CLIPTTA, a new gradient-based TTA method for\nvision-language models that leverages a soft contrastive loss aligned with\nCLIP's pre-training objective. We provide a theoretical analysis of CLIPTTA's\ngradients, showing how its batch-aware design mitigates the risk of collapse.\nWe further extend CLIPTTA to the open-set setting, where both in-distribution\n(ID) and out-of-distribution (OOD) samples are encountered, using an Outlier\nContrastive Exposure (OCE) loss to improve OOD detection. Evaluated on 75\ndatasets spanning diverse distribution shifts, CLIPTTA consistently outperforms\nentropy-based objectives and is highly competitive with state-of-the-art TTA\nmethods, outperforming them on a large number of datasets and exhibiting more\nstable performance across diverse shifts."
    },
    {
        "date": "2025-07",
        "title": "An Adversarial-Driven Experimental Study on Deep Learning for RF Fingerprinting",
        "author": "Xinyu Cao, Bimal Adhikari, Shangqing Zhao, Jingxian Wu, and Yanjun Pan",
        "link": "http://arxiv.org/abs/2507.14109v1",
        "abstract": "Radio frequency (RF) fingerprinting, which extracts unique hardware\nimperfections of radio devices, has emerged as a promising physical-layer\ndevice identification mechanism in zero trust architectures and beyond 5G\nnetworks. In particular, deep learning (DL) methods have demonstrated\nstate-of-the-art performance in this domain. However, existing approaches have\nprimarily focused on enhancing system robustness against temporal and spatial\nvariations in wireless environments, while the security vulnerabilities of\nthese DL-based approaches have often been overlooked. In this work, we\nsystematically investigate the security risks of DL-based RF fingerprinting\nsystems through an adversarial-driven experimental analysis. We observe a\nconsistent misclassification behavior for DL models under domain shifts, where\na device is frequently misclassified as another specific one. Our analysis\nbased on extensive real-world experiments demonstrates that this behavior can\nbe exploited as an effective backdoor to enable external attackers to intrude\ninto the system. Furthermore, we show that training DL models on raw received\nsignals causes the models to entangle RF fingerprints with environmental and\nsignal-pattern features, creating additional attack vectors that cannot be\nmitigated solely through post-processing security methods such as confidence\nthresholds."
    },
    {
        "date": "2025-07",
        "title": "Glucose-ML: A collection of longitudinal diabetes datasets for development of robust AI solutions",
        "author": "Temiloluwa Prioleau, Baiying Lu, and Yanjun Cui",
        "link": "http://arxiv.org/abs/2507.14077v1",
        "abstract": "Artificial intelligence (AI) algorithms are a critical part of\nstate-of-the-art digital health technology for diabetes management. Yet, access\nto large high-quality datasets is creating barriers that impede development of\nrobust AI solutions. To accelerate development of transparent, reproducible,\nand robust AI solutions, we present Glucose-ML, a collection of 10 publicly\navailable diabetes datasets, released within the last 7 years (i.e., 2018 -\n2025). The Glucose-ML collection comprises over 300,000 days of continuous\nglucose monitor (CGM) data with a total of 38 million glucose samples collected\nfrom 2500+ people across 4 countries. Participants include persons living with\ntype 1 diabetes, type 2 diabetes, prediabetes, and no diabetes. To support\nresearchers and innovators with using this rich collection of diabetes\ndatasets, we present a comparative analysis to guide algorithm developers with\ndata selection. Additionally, we conduct a case study for the task of blood\nglucose prediction - one of the most common AI tasks within the field. Through\nthis case study, we provide a benchmark for short-term blood glucose prediction\nacross all 10 publicly available diabetes datasets within the Glucose-ML\ncollection. We show that the same algorithm can have significantly different\nprediction results when developed/evaluated with different datasets. Findings\nfrom this study are then used to inform recommendations for developing robust\nAI solutions within the diabetes or broader health domain. We provide direct\nlinks to each longitudinal diabetes dataset in the Glucose-ML collection and\nopenly provide our code."
    },
    {
        "date": "2025-07",
        "title": "The CryptoNeo Threat Modelling Framework (CNTMF): Securing Neobanks and Fintech in Integrated Blockchain Ecosystems",
        "author": "Serhan W. Bahar",
        "link": "http://arxiv.org/abs/2507.14007v1",
        "abstract": "The rapid integration of blockchain, cryptocurrency, and Web3 technologies\ninto digital banks and fintech operations has created an integrated environment\nblending traditional financial systems with decentralised elements. This paper\nintroduces the CryptoNeo Threat Modelling Framework (CNTMF), a proposed\nframework designed to address the risks in these ecosystems, such as oracle\nmanipulation and cross-chain exploits. CNTMF represents a proposed extension of\nestablished methodologies like STRIDE, OWASP Top 10, NIST frameworks, LINDDUN,\nand PASTA, while incorporating tailored components including Hybrid Layer\nAnalysis, the CRYPTOQ mnemonic for cryptocurrency-specific risks, and an\nAI-Augmented Feedback Loop. Drawing on real-world data from 2025 incidents,\nCNTMF supports data-driven mitigation to reduce losses, which totalled\napproximately $2.47 billion in the first half of 2025 across 344 security\nevents (CertiK via GlobeNewswire, 2025; Infosecurity Magazine, 2025). Its\nphases guide asset mapping, risk profiling, prioritisation, mitigation, and\niterative feedback. This supports security against evolving risks like\nstate-sponsored attacks."
    },
    {
        "date": "2025-07",
        "title": "Robust Anomaly Detection with Graph Neural Networks using Controllability",
        "author": "Yifan Wei, Anwar Said, Waseem Abbas, and Xenofon Koutsoukos",
        "link": "http://arxiv.org/abs/2507.13954v1",
        "abstract": "Anomaly detection in complex domains poses significant challenges due to the\nneed for extensive labeled data and the inherently imbalanced nature of\nanomalous versus benign samples. Graph-based machine learning models have\nemerged as a promising solution that combines attribute and relational data to\nuncover intricate patterns. However, the scarcity of anomalous data exacerbates\nthe challenge, which requires innovative strategies to enhance model learning\nwith limited information. In this paper, we hypothesize that the incorporation\nof the influence of the nodes, quantified through average controllability, can\nsignificantly improve the performance of anomaly detection. We propose two\nnovel approaches to integrate average controllability into graph-based\nframeworks: (1) using average controllability as an edge weight and (2)\nencoding it as a one-hot edge attribute vector. Through rigorous evaluation on\nreal-world and synthetic networks with six state-of-the-art baselines, our\nproposed methods demonstrate improved performance in identifying anomalies,\nhighlighting the critical role of controllability measures in enhancing the\nperformance of graph machine learning models. This work underscores the\npotential of integrating average controllability as additional metrics to\naddress the challenges of anomaly detection in sparse and imbalanced datasets."
    },
    {
        "date": "2025-07",
        "title": "Developers Insight On Manifest v3 Privacy and Security Webextensions",
        "author": "Libor Pol\u010d\u00e1k, Giorgio Maone, Michael McMahon, and Martin Bedn\u00e1\u0159",
        "link": "http://arxiv.org/abs/2507.13926v1",
        "abstract": "Webextensions can improve web browser privacy, security, and user experience.\nThe APIs offered by the browser to webextensions affect possible functionality.\nCurrently, Chrome transitions to a modified set of APIs called Manifest v3.\nThis paper studies the challenges and opportunities of Manifest v3 with an\nin-depth structured qualitative research. Even though some projects observed\npositive effects, a majority expresses concerns over limited benefits to users,\nremoval of crucial APIs, or the need to find workarounds. Our findings indicate\nthat the transition affects different types of webextensions differently; some\ncan migrate without losing functionality, while other projects remove\nfunctionality or decline to update. The respondents identified several critical\nmissing APIs, including reliable APIs to inject content scripts, APIs for\nstoring confidential content, and others."
    },
    {
        "date": "2025-07",
        "title": "Adversarial Training Improves Generalization Under Distribution Shifts in Bioacoustics",
        "author": "Ren\u00e9 Heinrich, Lukas Rauch, Bernhard Sick, and Christoph Scholz",
        "link": "http://arxiv.org/abs/2507.13727v1",
        "abstract": "Adversarial training is a promising strategy for enhancing model robustness\nagainst adversarial attacks. However, its impact on generalization under\nsubstantial data distribution shifts in audio classification remains largely\nunexplored. To address this gap, this work investigates how different\nadversarial training strategies improve generalization performance and\nadversarial robustness in audio classification. The study focuses on two model\narchitectures: a conventional convolutional neural network (ConvNeXt) and an\ninherently interpretable prototype-based model (AudioProtoPNet). The approach\nis evaluated using a challenging bird sound classification benchmark. This\nbenchmark is characterized by pronounced distribution shifts between training\nand test data due to varying environmental conditions and recording methods, a\ncommon real-world challenge. The investigation explores two adversarial\ntraining strategies: one based on output-space attacks that maximize the\nclassification loss function, and another based on embedding-space attacks\ndesigned to maximize embedding dissimilarity. These attack types are also used\nfor robustness evaluation. Additionally, for AudioProtoPNet, the study assesses\nthe stability of its learned prototypes under targeted embedding-space attacks.\nResults show that adversarial training, particularly using output-space\nattacks, improves clean test data performance by an average of 10.5% relative\nand simultaneously strengthens the adversarial robustness of the models. These\nfindings, although derived from the bird sound domain, suggest that adversarial\ntraining holds potential to enhance robustness against both strong distribution\nshifts and adversarial attacks in challenging audio classification settings."
    },
    {
        "date": "2025-07",
        "title": "TopicAttack: An Indirect Prompt Injection Attack via Topic Transition",
        "author": "Yulin Chen, Haoran Li, Yuexin Li, Yue Liu, Yangqiu Song, and Bryan Hooi",
        "link": "http://arxiv.org/abs/2507.13686v1",
        "abstract": "Large language models (LLMs) have shown remarkable performance across a range\nof NLP tasks. However, their strong instruction-following capabilities and\ninability to distinguish instructions from data content make them vulnerable to\nindirect prompt injection attacks. In such attacks, instructions with malicious\npurposes are injected into external data sources, such as web documents. When\nLLMs retrieve this injected data through tools, such as a search engine and\nexecute the injected instructions, they provide misled responses. Recent attack\nmethods have demonstrated potential, but their abrupt instruction injection\noften undermines their effectiveness. Motivated by the limitations of existing\nattack methods, we propose TopicAttack, which prompts the LLM to generate a\nfabricated conversational transition prompt that gradually shifts the topic\ntoward the injected instruction, making the injection smoother and enhancing\nthe plausibility and success of the attack. Through comprehensive experiments,\nTopicAttack achieves state-of-the-art performance, with an attack success rate\n(ASR) over 90\\% in most cases, even when various defense methods are applied.\nWe further analyze its effectiveness by examining attention scores. We find\nthat a higher injected-to-original attention ratio leads to a greater success\nprobability, and our method achieves a much higher ratio than the baseline\nmethods."
    },
    {
        "date": "2025-07",
        "title": "MaskHOI: Robust 3D Hand-Object Interaction Estimation via Masked Pre-training",
        "author": "Yuechen Xie, Haobo Jiang, Jian Yang, Yigong Zhang, and Jin Xie",
        "link": "http://arxiv.org/abs/2507.13673v1",
        "abstract": "In 3D hand-object interaction (HOI) tasks, estimating precise joint poses of\nhands and objects from monocular RGB input remains highly challenging due to\nthe inherent geometric ambiguity of RGB images and the severe mutual occlusions\nthat occur during interaction.To address these challenges, we propose MaskHOI,\na novel Masked Autoencoder (MAE)-driven pretraining framework for enhanced HOI\npose estimation. Our core idea is to leverage the masking-then-reconstruction\nstrategy of MAE to encourage the feature encoder to infer missing spatial and\nstructural information, thereby facilitating geometric-aware and\nocclusion-robust representation learning. Specifically, based on our\nobservation that human hands exhibit far greater geometric complexity than\nrigid objects, conventional uniform masking fails to effectively guide the\nreconstruction of fine-grained hand structures. To overcome this limitation, we\nintroduce a Region-specific Mask Ratio Allocation, primarily comprising the\nregion-specific masking assignment and the skeleton-driven hand masking\nguidance. The former adaptively assigns lower masking ratios to hand regions\nthan to rigid objects, balancing their feature learning difficulty, while the\nlatter prioritizes masking critical hand parts (e.g., fingertips or entire\nfingers) to realistically simulate occlusion patterns in real-world\ninteractions. Furthermore, to enhance the geometric awareness of the pretrained\nencoder, we introduce a novel Masked Signed Distance Field (SDF)-driven\nmultimodal learning mechanism. Through the self-masking 3D SDF prediction, the\nlearned encoder is able to perceive the global geometric structure of hands and\nobjects beyond the 2D image plane, overcoming the inherent limitations of\nmonocular input and alleviating self-occlusion issues. Extensive experiments\ndemonstrate that our method significantly outperforms existing state-of-the-art\napproaches."
    },
    {
        "date": "2025-07",
        "title": "Breaking the Illusion of Security via Interpretation: Interpretable Vision Transformer Systems under Attack",
        "author": "Eldor Abdukhamidov, Mohammed Abuhamad, Simon S. Woo, Hyoungshick Kim, and Tamer Abuhmed",
        "link": "http://arxiv.org/abs/2507.14248v1",
        "abstract": "Vision transformer (ViT) models, when coupled with interpretation models, are\nregarded as secure and challenging to deceive, making them well-suited for\nsecurity-critical domains such as medical applications, autonomous vehicles,\ndrones, and robotics. However, successful attacks on these systems can lead to\nsevere consequences. Recent research on threats targeting ViT models primarily\nfocuses on generating the smallest adversarial perturbations that can deceive\nthe models with high confidence, without considering their impact on model\ninterpretations. Nevertheless, the use of interpretation models can effectively\nassist in detecting adversarial examples. This study investigates the\nvulnerability of transformer models to adversarial attacks, even when combined\nwith interpretation models. We propose an attack called \"AdViT\" that generates\nadversarial examples capable of misleading both a given transformer model and\nits coupled interpretation model. Through extensive experiments on various\ntransformer models and two transformer-based interpreters, we demonstrate that\nAdViT achieves a 100% attack success rate in both white-box and black-box\nscenarios. In white-box scenarios, it reaches up to 98% misclassification\nconfidence, while in black-box scenarios, it reaches up to 76%\nmisclassification confidence. Remarkably, AdViT consistently generates accurate\ninterpretations in both scenarios, making the adversarial examples more\ndifficult to detect."
    },
    {
        "date": "2025-07",
        "title": "Large Language Models in Cybersecurity: Applications, Vulnerabilities, and Defense Techniques",
        "author": "Niveen O. Jaffal, Mohammed Alkhanafseh, and David Mohaisen",
        "link": "http://arxiv.org/abs/2507.13629v1",
        "abstract": "Large Language Models (LLMs) are transforming cybersecurity by enabling\nintelligent, adaptive, and automated approaches to threat detection,\nvulnerability assessment, and incident response. With their advanced language\nunderstanding and contextual reasoning, LLMs surpass traditional methods in\ntackling challenges across domains such as IoT, blockchain, and hardware\nsecurity. This survey provides a comprehensive overview of LLM applications in\ncybersecurity, focusing on two core areas: (1) the integration of LLMs into key\ncybersecurity domains, and (2) the vulnerabilities of LLMs themselves, along\nwith mitigation strategies. By synthesizing recent advancements and identifying\nkey limitations, this work offers practical insights and strategic\nrecommendations for leveraging LLMs to build secure, scalable, and future-ready\ncyber defense systems."
    },
    {
        "date": "2025-07",
        "title": "FuSeFL: Fully Secure and Scalable Cross-Silo Federated Learning",
        "author": "Sahar Ghoflsaz Ghinani, and Elaheh Sadredini",
        "link": "http://arxiv.org/abs/2507.13591v1",
        "abstract": "Federated Learning (FL) enables collaborative model training without\ncentralizing client data, making it attractive for privacy-sensitive domains.\nWhile existing approaches employ cryptographic techniques such as homomorphic\nencryption, differential privacy, or secure multiparty computation to mitigate\ninference attacks-including model inversion, membership inference, and gradient\nleakage-they often suffer from high computational, communication, or memory\noverheads. Moreover, many methods overlook the confidentiality of the global\nmodel itself, which may be proprietary and sensitive. These challenges limit\nthe practicality of secure FL, especially in cross-silo deployments involving\nlarge datasets and strict compliance requirements.\n  We present FuSeFL, a fully secure and scalable FL scheme designed for\ncross-silo settings. FuSeFL decentralizes training across client pairs using\nlightweight secure multiparty computation (MPC), while confining the server's\nrole to secure aggregation. This design eliminates server bottlenecks, avoids\ndata offloading, and preserves full confidentiality of data, model, and updates\nthroughout training. FuSeFL defends against inference threats, achieves up to\n95% lower communication latency and 50% lower server memory usage, and improves\naccuracy over prior secure FL solutions, demonstrating strong security and\nefficiency at scale."
    },
    {
        "date": "2025-07",
        "title": "SHIELD: A Secure and Highly Enhanced Integrated Learning for Robust Deepfake Detection against Adversarial Attacks",
        "author": "Kutub Uddin, Awais Khan, Muhammad Umar Farooq, and Khalid Malik",
        "link": "http://arxiv.org/abs/2507.13170v1",
        "abstract": "Audio plays a crucial role in applications like speaker verification,\nvoice-enabled smart devices, and audio conferencing. However, audio\nmanipulations, such as deepfakes, pose significant risks by enabling the spread\nof misinformation. Our empirical analysis reveals that existing methods for\ndetecting deepfake audio are often vulnerable to anti-forensic (AF) attacks,\nparticularly those attacked using generative adversarial networks. In this\narticle, we propose a novel collaborative learning method called SHIELD to\ndefend against generative AF attacks. To expose AF signatures, we integrate an\nauxiliary generative model, called the defense (DF) generative model, which\nfacilitates collaborative learning by combining input and output. Furthermore,\nwe design a triplet model to capture correlations for real and AF attacked\naudios with real-generated and attacked-generated audios using auxiliary\ngenerative models. The proposed SHIELD strengthens the defense against\ngenerative AF attacks and achieves robust performance across various generative\nmodels. The proposed AF significantly reduces the average detection accuracy\nfrom 95.49% to 59.77% for ASVspoof2019, from 99.44% to 38.45% for In-the-Wild,\nand from 98.41% to 51.18% for HalfTruth for three different generative models.\nThe proposed SHIELD mechanism is robust against AF attacks and achieves an\naverage accuracy of 98.13%, 98.58%, and 99.57% in match, and 98.78%, 98.62%,\nand 98.85% in mismatch settings for the ASVspoof2019, In-the-Wild, and\nHalfTruth datasets, respectively."
    },
    {
        "date": "2025-07",
        "title": "Backscattering-Based Security in Wireless Power Transfer Applied to Battery-Free BLE Sensors",
        "author": "Taki Eddine Djidjekh, Ga\u00ebl Loubet, and Alexandru Takacs",
        "link": "http://arxiv.org/abs/2507.13042v1",
        "abstract": "The integration of security and energy efficiency in Internet of Things\nsystems remains a critical challenge, particularly for battery-free and\nresource-constrained devices. This paper explores the scalability and\nprotocol-agnostic nature of a backscattering-based security mechanism by\nintegrating it into Bluetooth Low Energy battery-free Wireless Sensor Network.\nThe proposed approach leverages the Wireless Power Transfer link, traditionally\nused for energy harvesting, to generate additional identification signals\nwithout increasing energy consumption or computational demands. Experimental\nvalidation demonstrates the solution's functionality using compact, low-gain\nantenna, ensuring compatibility with size-constrained applications such as\nStructural Health Monitoring and smart transport. Furthermore, this work\naddresses the challenges associated with backscattering dynamic range and\nmulti-node Wireless Sensor Network scenarios, discussing potential collisions\nbetween identification signals and proposing future improvements to enhance\ngeneralizability and scalability. The findings underscore the potential of the\nbackscattering-based security mechanism for creating secure, sustainable, and\nscalable IoT deployments across diverse protocols and applications."
    },
    {
        "date": "2025-07",
        "title": "MAD-Spear: A Conformity-Driven Prompt Injection Attack on Multi-Agent Debate Systems",
        "author": "Yu Cui, and Hongyang Du",
        "link": "http://arxiv.org/abs/2507.13038v1",
        "abstract": "Multi-agent debate (MAD) systems leverage collaborative interactions among\nlarge language models (LLMs) agents to improve reasoning capabilities. While\nrecent studies have focused on increasing the accuracy and scalability of MAD\nsystems, their security vulnerabilities have received limited attention. In\nthis work, we introduce MAD-Spear, a targeted prompt injection attack that\ncompromises a small subset of agents but significantly disrupts the overall MAD\nprocess. Manipulated agents produce multiple plausible yet incorrect responses,\nexploiting LLMs' conformity tendencies to propagate misinformation and degrade\nconsensus quality. Furthermore, the attack can be composed with other\nstrategies, such as communication attacks, to further amplify its impact by\nincreasing the exposure of agents to incorrect responses. To assess MAD's\nresilience under attack, we propose a formal definition of MAD fault-tolerance\nand develop a comprehensive evaluation framework that jointly considers\naccuracy, consensus efficiency, and scalability. Extensive experiments on five\nbenchmark datasets with varying difficulty levels demonstrate that MAD-Spear\nconsistently outperforms the baseline attack in degrading system performance.\nAdditionally, we observe that agent diversity substantially improves MAD\nperformance in mathematical reasoning tasks, which challenges prior work\nsuggesting that agent diversity has minimal impact on performance. These\nfindings highlight the urgent need to improve the security in MAD design."
    },
    {
        "date": "2025-07",
        "title": "Enterprise Security Incident Analysis and Countermeasures Based on the T-Mobile Data Breach",
        "author": "Zhuohan Cui, and Zikun Song",
        "link": "http://arxiv.org/abs/2507.12937v1",
        "abstract": "This paper presents a comprehensive analysis of T-Mobile's critical data\nbreaches in 2021 and 2023, alongside a full-spectrum security audit targeting\nits systems, infrastructure, and publicly exposed endpoints. By combining\ncase-based vulnerability assessments with active ethical hacking\ntechniques--including Shodan reconnaissance, API misuse simulations, VNC\nbrute-forcing, firmware reverse engineering, and web application scans--we\nuncover structural weaknesses persisting beyond the initial breach events.\nBuilding on these findings, we propose a multi-layered defensive strategy\nencompassing Zero Trust Architecture, granular role-based access control,\nnetwork segmentation, firmware encryption using AES with integrity checks, and\nAPI rate limiting and token lifecycle control. Financial modelling demonstrates\nthat a five-year investment yields less than 1.1% of expected breach losses,\nvalidating the cost-effectiveness of proactive security measures. Our work\nbridges post-incident forensic analysis with hands-on security evaluation,\nproviding an actionable blueprint for large-scale telecoms seeking operational\nresilience, regulatory compliance, and cross-domain threat readiness."
    },
    {
        "date": "2025-07",
        "title": "Architectural Backdoors in Deep Learning: A Survey of Vulnerabilities, Detection, and Defense",
        "author": "Victoria Childress, Josh Collyer, and Jodie Knapp",
        "link": "http://arxiv.org/abs/2507.12919v1",
        "abstract": "Architectural backdoors pose an under-examined but critical threat to deep\nneural networks, embedding malicious logic directly into a model's\ncomputational graph. Unlike traditional data poisoning or parameter\nmanipulation, architectural backdoors evade standard mitigation techniques and\npersist even after clean retraining. This survey systematically consolidates\nresearch on architectural backdoors, spanning compiler-level manipulations,\ntainted AutoML pipelines, and supply-chain vulnerabilities. We assess emerging\ndetection and defense strategies, including static graph inspection, dynamic\nfuzzing, and partial formal verification, and highlight their limitations\nagainst distributed or stealth triggers. Despite recent progress, scalable and\npractical defenses remain elusive. We conclude by outlining open challenges and\nproposing directions for strengthening supply-chain security, cryptographic\nmodel attestations, and next-generation benchmarks. This survey aims to guide\nfuture research toward comprehensive defenses against structural backdoor\nthreats in deep learning systems."
    },
    {
        "date": "2025-07",
        "title": "Robust Explanations Through Uncertainty Decomposition: A Path to Trustworthier AI",
        "author": "Chenrui Zhu, Louenas Bounia, Vu Linh Nguyen, S\u00e9bastien Destercke, and Arthur Hoarau",
        "link": "http://arxiv.org/abs/2507.12913v1",
        "abstract": "Recent advancements in machine learning have emphasized the need for\ntransparency in model predictions, particularly as interpretability diminishes\nwhen using increasingly complex architectures. In this paper, we propose\nleveraging prediction uncertainty as a complementary approach to classical\nexplainability methods. Specifically, we distinguish between aleatoric\n(data-related) and epistemic (model-related) uncertainty to guide the selection\nof appropriate explanations. Epistemic uncertainty serves as a rejection\ncriterion for unreliable explanations and, in itself, provides insight into\ninsufficient training (a new form of explanation). Aleatoric uncertainty\ninforms the choice between feature-importance explanations and counterfactual\nexplanations. This leverages a framework of explainability methods driven by\nuncertainty quantification and disentanglement. Our experiments demonstrate the\nimpact of this uncertainty-aware approach on the robustness and attainability\nof explanations in both traditional machine learning and deep learning\nscenarios."
    },
    {
        "date": "2025-07",
        "title": "Manipulation Attacks by Misaligned AI: Risk Analysis and Safety Case Framework",
        "author": "Rishane Dassanayake, Mario Demetroudi, James Walpole, Lindley Lentati, Jason R. Brown, and Edward James Young",
        "link": "http://arxiv.org/abs/2507.12872v1",
        "abstract": "Frontier AI systems are rapidly advancing in their capabilities to persuade,\ndeceive, and influence human behaviour, with current models already\ndemonstrating human-level persuasion and strategic deception in specific\ncontexts. Humans are often the weakest link in cybersecurity systems, and a\nmisaligned AI system deployed internally within a frontier company may seek to\nundermine human oversight by manipulating employees. Despite this growing\nthreat, manipulation attacks have received little attention, and no systematic\nframework exists for assessing and mitigating these risks. To address this, we\nprovide a detailed explanation of why manipulation attacks are a significant\nthreat and could lead to catastrophic outcomes. Additionally, we present a\nsafety case framework for manipulation risk, structured around three core lines\nof argument: inability, control, and trustworthiness. For each argument, we\nspecify evidence requirements, evaluation methodologies, and implementation\nconsiderations for direct application by AI companies. This paper provides the\nfirst systematic methodology for integrating manipulation risk into AI safety\ngovernance, offering AI companies a concrete foundation to assess and mitigate\nthese threats before deployment."
    },
    {
        "date": "2025-07",
        "title": "IConMark: Robust Interpretable Concept-Based Watermark For AI Images",
        "author": "Vinu Sankar Sadasivan, Mehrdad Saberi, and Soheil Feizi",
        "link": "http://arxiv.org/abs/2507.13407v1",
        "abstract": "With the rapid rise of generative AI and synthetic media, distinguishing\nAI-generated images from real ones has become crucial in safeguarding against\nmisinformation and ensuring digital authenticity. Traditional watermarking\ntechniques have shown vulnerabilities to adversarial attacks, undermining their\neffectiveness in the presence of attackers. We propose IConMark, a novel\nin-generation robust semantic watermarking method that embeds interpretable\nconcepts into AI-generated images, as a first step toward interpretable\nwatermarking. Unlike traditional methods, which rely on adding noise or\nperturbations to AI-generated images, IConMark incorporates meaningful semantic\nattributes, making it interpretable to humans and hence, resilient to\nadversarial manipulation. This method is not only robust against various image\naugmentations but also human-readable, enabling manual verification of\nwatermarks. We demonstrate a detailed evaluation of IConMark's effectiveness,\ndemonstrating its superiority in terms of detection accuracy and maintaining\nimage quality. Moreover, IConMark can be combined with existing watermarking\ntechniques to further enhance and complement its robustness. We introduce\nIConMark+SS and IConMark+TM, hybrid approaches combining IConMark with\nStegaStamp and TrustMark, respectively, to further bolster robustness against\nmultiple types of image manipulations. Our base watermarking technique\n(IConMark) and its variants (+TM and +SS) achieve 10.8%, 14.5%, and 15.9%\nhigher mean area under the receiver operating characteristic curve (AUROC)\nscores for watermark detection, respectively, compared to the best baseline on\nvarious datasets."
    },
    {
        "date": "2025-07",
        "title": "Multimodal-Guided Dynamic Dataset Pruning for Robust and Efficient Data-Centric Learning",
        "author": "Suorong Yang, Peijia Li, Yujie Liu, Zhiming Xu, Peng Ye, Wanli Ouyang, Furao Shen, and Dongzhan Zhou",
        "link": "http://arxiv.org/abs/2507.12750v1",
        "abstract": "Modern deep models are trained on large real-world datasets, where data\nquality varies and redundancy is common. Data-centric approaches such as\ndataset pruning have shown promise in improving training efficiency and model\nperformance. However, most existing methods rely on static heuristics or\ntask-specific metrics, limiting their robustness and generalizability across\ndomains. In this work, we introduce a dynamic dataset pruning framework that\nadaptively selects training samples based on both task-driven difficulty and\ncross-modality semantic consistency. By incorporating supervision from\npretrained multimodal foundation models, our approach captures training\ndynamics while effectively filtering out uninformative samples. Our work\nhighlights the potential of integrating cross-modality alignment for robust\nsample selection, advancing data-centric learning toward more efficient and\nrobust practices across application domains."
    },
    {
        "date": "2025-07",
        "title": "A Novel Data Augmentation Strategy for Robust Deep Learning Classification of Biomedical Time-Series Data: Application to ECG and EEG Analysis",
        "author": "Mohammed Guhdar, Ramadhan J. Mstafa, and Abdulhakeem O. Mohammed",
        "link": "http://arxiv.org/abs/2507.12645v1",
        "abstract": "The increasing need for accurate and unified analysis of diverse biological\nsignals, such as ECG and EEG, is paramount for comprehensive patient\nassessment, especially in synchronous monitoring. Despite advances in\nmulti-sensor fusion, a critical gap remains in developing unified architectures\nthat effectively process and extract features from fundamentally different\nphysiological signals. Another challenge is the inherent class imbalance in\nmany biomedical datasets, often causing biased performance in traditional\nmethods. This study addresses these issues by proposing a novel and unified\ndeep learning framework that achieves state-of-the-art performance across\ndifferent signal types. Our method integrates a ResNet-based CNN with an\nattention mechanism, enhanced by a novel data augmentation strategy:\ntime-domain concatenation of multiple augmented variants of each signal to\ngenerate richer representations. Unlike prior work, we scientifically increase\nsignal complexity to achieve future-reaching capabilities, which resulted in\nthe best predictions compared to the state of the art. Preprocessing steps\nincluded wavelet denoising, baseline removal, and standardization. Class\nimbalance was effectively managed through the combined use of this advanced\ndata augmentation and the Focal Loss function. Regularization techniques were\napplied during training to ensure generalization. We rigorously evaluated the\nproposed architecture on three benchmark datasets: UCI Seizure EEG, MIT-BIH\nArrhythmia, and PTB Diagnostic ECG. It achieved accuracies of 99.96%, 99.78%,\nand 100%, respectively, demonstrating robustness across diverse signal types\nand clinical contexts. Finally, the architecture requires ~130 MB of memory and\nprocesses each sample in ~10 ms, suggesting suitability for deployment on\nlow-end or wearable devices."
    },
    {
        "date": "2025-07",
        "title": "Achieving Robust Channel Estimation Neural Networks by Designed Training Data",
        "author": "Dianxin Luan, and John Thompson",
        "link": "http://arxiv.org/abs/2507.12630v2",
        "abstract": "Channel estimation is crucial in wireless communications. However, in many\npapers neural networks are frequently tested by training and testing on one\nexample channel or similar channels. This is because data-driven methods often\ndegrade on new data which they are not trained on, as they cannot extrapolate\ntheir training knowledge. This is despite the fact physical channels are often\nassumed to be time-variant. However, due to the low latency requirements and\nlimited computing resources, neural networks may not have enough time and\ncomputing resources to execute online training to fine-tune the parameters.\nThis motivates us to design offline-trained neural networks that can perform\nrobustly over wireless channels, but without any actual channel information\nbeing known at design time. In this paper, we propose design criteria to\ngenerate synthetic training datasets for neural networks, which guarantee that\nafter training the resulting networks achieve a certain mean squared error\n(MSE) on new and previously unseen channels. Therefore, trained neural networks\nrequire no prior channel information or parameters update for real-world\nimplementations. Based on the proposed design criteria, we further propose a\nbenchmark design which ensures intelligent operation for different channel\nprofiles. To demonstrate general applicability, we use neural networks with\ndifferent levels of complexity to show that the generalization achieved appears\nto be independent of neural network architecture. From simulations, neural\nnetworks achieve robust generalization to wireless channels with both fixed\nchannel profiles and variable delay spreads."
    },
    {
        "date": "2025-07",
        "title": "Targeted Deep Architectures: A TMLE-Based Framework for Robust Causal Inference in Neural Networks",
        "author": "Yi Li, David Mccoy, Nolan Gunter, Kaitlyn Lee, Alejandro Schuler, and Mark van der Laan",
        "link": "http://arxiv.org/abs/2507.12435v1",
        "abstract": "Modern deep neural networks are powerful predictive tools yet often lack\nvalid inference for causal parameters, such as treatment effects or entire\nsurvival curves. While frameworks like Double Machine Learning (DML) and\nTargeted Maximum Likelihood Estimation (TMLE) can debias machine-learning fits,\nexisting neural implementations either rely on \"targeted losses\" that do not\nguarantee solving the efficient influence function equation or computationally\nexpensive post-hoc \"fluctuations\" for multi-parameter settings. We propose\nTargeted Deep Architectures (TDA), a new framework that embeds TMLE directly\ninto the network's parameter space with no restrictions on the backbone\narchitecture. Specifically, TDA partitions model parameters - freezing all but\na small \"targeting\" subset - and iteratively updates them along a targeting\ngradient, derived from projecting the influence functions onto the span of the\ngradients of the loss with respect to weights. This procedure yields plug-in\nestimates that remove first-order bias and produce asymptotically valid\nconfidence intervals. Crucially, TDA easily extends to multi-dimensional causal\nestimands (e.g., entire survival curves) by merging separate targeting\ngradients into a single universal targeting update. Theoretically, TDA inherits\nclassical TMLE properties, including double robustness and semiparametric\nefficiency. Empirically, on the benchmark IHDP dataset (average treatment\neffects) and simulated survival data with informative censoring, TDA reduces\nbias and improves coverage relative to both standard neural-network estimators\nand prior post-hoc approaches. In doing so, TDA establishes a direct, scalable\npathway toward rigorous causal inference within modern deep architectures for\ncomplex multi-parameter targets."
    },
    {
        "date": "2025-07",
        "title": "Thought Purity: Defense Paradigm For Chain-of-Thought Attack",
        "author": "Zihao Xue, Zhen Bi, Long Ma, Zhenlin Hu, Yan Wang, Zhenfang Liu, Qing Sheng, Jie Xiao, and Jungang Lou",
        "link": "http://arxiv.org/abs/2507.12314v1",
        "abstract": "While reinforcement learning-trained Large Reasoning Models (LRMs, e.g.,\nDeepseek-R1) demonstrate advanced reasoning capabilities in the evolving Large\nLanguage Models (LLMs) domain, their susceptibility to security threats remains\na critical vulnerability. This weakness is particularly evident in\nChain-of-Thought (CoT) generation processes, where adversarial methods like\nbackdoor prompt attacks can systematically subvert the model's core reasoning\nmechanisms. The emerging Chain-of-Thought Attack (CoTA) reveals this\nvulnerability through exploiting prompt controllability, simultaneously\ndegrading both CoT safety and task performance with low-cost interventions. To\naddress this compounded security-performance vulnerability, we propose Thought\nPurity (TP): a defense paradigm that systematically strengthens resistance to\nmalicious content while preserving operational efficacy. Our solution achieves\nthis through three synergistic components: (1) a safety-optimized data\nprocessing pipeline (2) reinforcement learning-enhanced rule constraints (3)\nadaptive monitoring metrics. Our approach establishes the first comprehensive\ndefense mechanism against CoTA vulnerabilities in reinforcement\nlearning-aligned reasoning systems, significantly advancing the\nsecurity-functionality equilibrium for next-generation AI architectures."
    },
    {
        "date": "2025-07",
        "title": "FADE: Adversarial Concept Erasure in Flow Models",
        "author": "Zixuan Fu, Yan Ren, Finn Carter, Chenyue Wang, Ze Niu, Dacheng Yu, Emily Davis, and Bo Zhang",
        "link": "http://arxiv.org/abs/2507.12283v1",
        "abstract": "Diffusion models have demonstrated remarkable image generation capabilities,\nbut also pose risks in privacy and fairness by memorizing sensitive concepts or\nperpetuating biases. We propose a novel \\textbf{concept erasure} method for\ntext-to-image diffusion models, designed to remove specified concepts (e.g., a\nprivate individual or a harmful stereotype) from the model's generative\nrepertoire. Our method, termed \\textbf{FADE} (Fair Adversarial Diffusion\nErasure), combines a trajectory-aware fine-tuning strategy with an adversarial\nobjective to ensure the concept is reliably removed while preserving overall\nmodel fidelity. Theoretically, we prove a formal guarantee that our approach\nminimizes the mutual information between the erased concept and the model's\noutputs, ensuring privacy and fairness. Empirically, we evaluate FADE on Stable\nDiffusion and FLUX, using benchmarks from prior work (e.g., object, celebrity,\nexplicit content, and style erasure tasks from MACE). FADE achieves\nstate-of-the-art concept removal performance, surpassing recent baselines like\nESD, UCE, MACE, and ANT in terms of removal efficacy and image quality.\nNotably, FADE improves the harmonic mean of concept removal and fidelity by\n5--10\\% over the best prior method. We also conduct an ablation study to\nvalidate each component of FADE, confirming that our adversarial and\ntrajectory-preserving objectives each contribute to its superior performance.\nOur work sets a new standard for safe and fair generative modeling by\nunlearning specified concepts without retraining from scratch."
    },
    {
        "date": "2025-07",
        "title": "Site-Level Fine-Tuning with Progressive Layer Freezing: Towards Robust Prediction of Bronchopulmonary Dysplasia from Day-1 Chest Radiographs in Extremely Preterm Infants",
        "author": "Sybelle Goedicke-Fritz, Michelle Bous, Annika Engel, Matthias Flotho, Pascal Hirsch, Hannah Wittig, Dino Milanovic, Dominik Mohr, Mathias Kaspar, Sogand Nemat, Dorothea Kerner, Arno B\u00fccker, Andreas Keller, Sascha Meyer, Michael Zemlin, and Philipp Flotho",
        "link": "http://arxiv.org/abs/2507.12269v2",
        "abstract": "Bronchopulmonary dysplasia (BPD) is a chronic lung disease affecting 35% of\nextremely low birth weight infants. Defined by oxygen dependence at 36 weeks\npostmenstrual age, it causes lifelong respiratory complications. However,\npreventive interventions carry severe risks, including neurodevelopmental\nimpairment, ventilator-induced lung injury, and systemic complications.\nTherefore, early BPD prognosis and prediction of BPD outcome is crucial to\navoid unnecessary toxicity in low risk infants. Admission radiographs of\nextremely preterm infants are routinely acquired within 24h of life and could\nserve as a non-invasive prognostic tool. In this work, we developed and\ninvestigated a deep learning approach using chest X-rays from 163 extremely\nlow-birth-weight infants ($\\leq$32 weeks gestation, 401-999g) obtained within\n24 hours of birth. We fine-tuned a ResNet-50 pretrained specifically on adult\nchest radiographs, employing progressive layer freezing with discriminative\nlearning rates to prevent overfitting and evaluated a CutMix augmentation and\nlinear probing. For moderate/severe BPD outcome prediction, our best performing\nmodel with progressive freezing, linear probing and CutMix achieved an AUROC of\n0.78 $\\pm$ 0.10, balanced accuracy of 0.69 $\\pm$ 0.10, and an F1-score of 0.67\n$\\pm$ 0.11. In-domain pre-training significantly outperformed ImageNet\ninitialization (p = 0.031) which confirms domain-specific pretraining to be\nimportant for BPD outcome prediction. Routine IRDS grades showed limited\nprognostic value (AUROC 0.57 $\\pm$ 0.11), confirming the need of learned\nmarkers. Our approach demonstrates that domain-specific pretraining enables\naccurate BPD prediction from routine day-1 radiographs. Through progressive\nfreezing and linear probing, the method remains computationally feasible for\nsite-level implementation and future federated learning deployments."
    },
    {
        "date": "2025-07",
        "title": "Robust Causal Discovery in Real-World Time Series with Power-Laws",
        "author": "Matteo Tusoni, Giuseppe Masi, Andrea Coletta, Aldo Glielmo, Viviana Arrigoni, and Novella Bartolini",
        "link": "http://arxiv.org/abs/2507.12257v1",
        "abstract": "Exploring causal relationships in stochastic time series is a challenging yet\ncrucial task with a vast range of applications, including finance, economics,\nneuroscience, and climate science. Many algorithms for Causal Discovery (CD)\nhave been proposed, but they often exhibit a high sensitivity to noise,\nresulting in misleading causal inferences when applied to real data. In this\npaper, we observe that the frequency spectra of typical real-world time series\nfollow a power-law distribution, notably due to an inherent self-organizing\nbehavior. Leveraging this insight, we build a robust CD method based on the\nextraction of power -law spectral features that amplify genuine causal signals.\nOur method consistently outperforms state-of-the-art alternatives on both\nsynthetic benchmarks and real-world datasets with known causal structures,\ndemonstrating its robustness and practical relevance."
    },
    {
        "date": "2025-07",
        "title": "RODS: Robust Optimization Inspired Diffusion Sampling for Detecting and Reducing Hallucination in Generative Models",
        "author": "Yiqi Tian, Pengfei Jin, Mingze Yuan, Na Li, Bo Zeng, and Quanzheng Li",
        "link": "http://arxiv.org/abs/2507.12201v1",
        "abstract": "Diffusion models have achieved state-of-the-art performance in generative\nmodeling, yet their sampling procedures remain vulnerable to hallucinations,\noften stemming from inaccuracies in score approximation. In this work, we\nreinterpret diffusion sampling through the lens of optimization and introduce\nRODS (Robust Optimization-inspired Diffusion Sampler), a novel method that\ndetects and corrects high-risk sampling steps using geometric cues from the\nloss landscape. RODS enforces smoother sampling trajectories and adaptively\nadjusts perturbations, reducing hallucinations without retraining and at\nminimal additional inference cost. Experiments on AFHQv2, FFHQ, and 11k-hands\ndemonstrate that RODS improves both sampling fidelity and robustness, detecting\nover 70% of hallucinated samples and correcting more than 25%, all while\navoiding the introduction of new artifacts."
    },
    {
        "date": "2025-07",
        "title": "Exploiting Jailbreaking Vulnerabilities in Generative AI to Bypass Ethical Safeguards for Facilitating Phishing Attacks",
        "author": "Rina Mishra, and Gaurav Varshney",
        "link": "http://arxiv.org/abs/2507.12185v1",
        "abstract": "The advent of advanced Generative AI (GenAI) models such as DeepSeek and\nChatGPT has significantly reshaped the cybersecurity landscape, introducing\nboth promising opportunities and critical risks. This study investigates how\nGenAI powered chatbot services can be exploited via jailbreaking techniques to\nbypass ethical safeguards, enabling the generation of phishing content,\nrecommendation of hacking tools, and orchestration of phishing campaigns. In\nethically controlled experiments, we used ChatGPT 4o Mini selected for its\naccessibility and status as the latest publicly available model at the time of\nexperimentation, as a representative GenAI system. Our findings reveal that the\nmodel could successfully guide novice users in executing phishing attacks\nacross various vectors, including web, email, SMS (smishing), and voice\n(vishing). Unlike automated phishing campaigns that typically follow detectable\npatterns, these human-guided, AI assisted attacks are capable of evading\ntraditional anti phishing mechanisms, thereby posing a growing security threat.\nWe focused on DeepSeek and ChatGPT due to their widespread adoption and\ntechnical relevance in 2025. The study further examines common jailbreaking\ntechniques and the specific vulnerabilities exploited in these models. Finally,\nwe evaluate a range of mitigation strategies such as user education, advanced\nauthentication mechanisms, and regulatory policy measures and discuss emerging\ntrends in GenAI facilitated phishing, outlining future research directions to\nstrengthen cybersecurity defenses in the age of artificial intelligence."
    },
    {
        "date": "2025-07",
        "title": "DoRF: Doppler Radiance Fields for Robust Human Activity Recognition Using Wi-Fi",
        "author": "Navid Hasanzadeh, and Shahrokh Valaee",
        "link": "http://arxiv.org/abs/2507.12132v1",
        "abstract": "Wi-Fi Channel State Information (CSI) has gained increasing interest for\nremote sensing applications. Recent studies show that Doppler velocity\nprojections extracted from CSI can enable human activity recognition (HAR) that\nis robust to environmental changes and generalizes to new users. However,\ndespite these advances, generalizability still remains insufficient for\npractical deployment. Inspired by neural radiance fields (NeRF), which learn a\nvolumetric representation of a 3D scene from 2D images, this work proposes a\nnovel approach to reconstruct an informative 3D latent motion representation\nfrom one-dimensional Doppler velocity projections extracted from Wi-Fi CSI. The\nresulting latent representation is then used to construct a uniform Doppler\nradiance field (DoRF) of the motion, providing a comprehensive view of the\nperformed activity and improving the robustness to environmental variability.\nThe results show that the proposed approach noticeably enhances the\ngeneralization accuracy of Wi-Fi-based HAR, highlighting the strong potential\nof DoRFs for practical sensing applications."
    },
    {
        "date": "2025-07",
        "title": "Self-Adaptive and Robust Federated Spectrum Sensing without Benign Majority for Cellular Networks",
        "author": "Ngoc Duy Pham, Thusitha Dayaratne, Viet Vo, Shangqi Lai, Sharif Abuadbba, Hajime Suzuki, Xingliang Yuan, and Carsten Rudolph",
        "link": "http://arxiv.org/abs/2507.12127v1",
        "abstract": "Advancements in wireless and mobile technologies, including 5G advanced and\nthe envisioned 6G, are driving exponential growth in wireless devices. However,\nthis rapid expansion exacerbates spectrum scarcity, posing a critical\nchallenge. Dynamic spectrum allocation (DSA)--which relies on sensing and\ndynamically sharing spectrum--has emerged as an essential solution to address\nthis issue. While machine learning (ML) models hold significant potential for\nimproving spectrum sensing, their adoption in centralized ML-based DSA systems\nis limited by privacy concerns, bandwidth constraints, and regulatory\nchallenges. To overcome these limitations, distributed ML-based approaches such\nas Federated Learning (FL) offer promising alternatives. This work addresses\ntwo key challenges in FL-based spectrum sensing (FLSS). First, the scarcity of\nlabeled data for training FL models in practical spectrum sensing scenarios is\ntackled with a semi-supervised FL approach, combined with energy detection,\nenabling model training on unlabeled datasets. Second, we examine the security\nvulnerabilities of FLSS, focusing on the impact of data poisoning attacks. Our\nanalysis highlights the shortcomings of existing majority-based defenses in\ncountering such attacks. To address these vulnerabilities, we propose a novel\ndefense mechanism inspired by vaccination, which effectively mitigates data\npoisoning attacks without relying on majority-based assumptions. Extensive\nexperiments on both synthetic and real-world datasets validate our solutions,\ndemonstrating that FLSS can achieve near-perfect accuracy on unlabeled datasets\nand maintain Byzantine robustness against both targeted and untargeted data\npoisoning attacks, even when a significant proportion of participants are\nmalicious."
    },
    {
        "date": "2025-07",
        "title": "Non-Adaptive Adversarial Face Generation",
        "author": "Sunpill Kim, Seunghun Paik, Chanwoo Hwang, Minsu Kim, and Jae Hong Seo",
        "link": "http://arxiv.org/abs/2507.12107v1",
        "abstract": "Adversarial attacks on face recognition systems (FRSs) pose serious security\nand privacy threats, especially when these systems are used for identity\nverification. In this paper, we propose a novel method for generating\nadversarial faces-synthetic facial images that are visually distinct yet\nrecognized as a target identity by the FRS. Unlike iterative optimization-based\napproaches (e.g., gradient descent or other iterative solvers), our method\nleverages the structural characteristics of the FRS feature space. We figure\nout that individuals sharing the same attribute (e.g., gender or race) form an\nattributed subsphere. By utilizing such subspheres, our method achieves both\nnon-adaptiveness and a remarkably small number of queries. This eliminates the\nneed for relying on transferability and open-source surrogate models, which\nhave been a typical strategy when repeated adaptive queries to commercial FRSs\nare impossible. Despite requiring only a single non-adaptive query consisting\nof 100 face images, our method achieves a high success rate of over 93% against\nAWS's CompareFaces API at its default threshold. Furthermore, unlike many\nexisting attacks that perturb a given image, our method can deliberately\nproduce adversarial faces that impersonate the target identity while exhibiting\nhigh-level attributes chosen by the adversary."
    },
    {
        "date": "2025-07",
        "title": "BRUM: Robust 3D Vehicle Reconstruction from 360 Sparse Images",
        "author": "Davide Di Nucci, Matteo Tomei, Guido Borghi, Luca Ciuffreda, Roberto Vezzani, and Rita Cucchiara",
        "link": "http://arxiv.org/abs/2507.12095v1",
        "abstract": "Accurate 3D reconstruction of vehicles is vital for applications such as\nvehicle inspection, predictive maintenance, and urban planning. Existing\nmethods like Neural Radiance Fields and Gaussian Splatting have shown\nimpressive results but remain limited by their reliance on dense input views,\nwhich hinders real-world applicability. This paper addresses the challenge of\nreconstructing vehicles from sparse-view inputs, leveraging depth maps and a\nrobust pose estimation architecture to synthesize novel views and augment\ntraining data. Specifically, we enhance Gaussian Splatting by integrating a\nselective photometric loss, applied only to high-confidence pixels, and\nreplacing standard Structure-from-Motion pipelines with the DUSt3R architecture\nto improve camera pose estimation. Furthermore, we present a novel dataset\nfeaturing both synthetic and real-world public transportation vehicles,\nenabling extensive evaluation of our approach. Experimental results demonstrate\nstate-of-the-art performance across multiple benchmarks, showcasing the\nmethod's ability to achieve high-quality reconstructions even under constrained\ninput conditions."
    },
    {
        "date": "2025-07",
        "title": "YOLOv8-SMOT: An Efficient and Robust Framework for Real-Time Small Object Tracking via Slice-Assisted Training and Adaptive Association",
        "author": "Xiang Yu, Xinyao Liu, and Guang Liang",
        "link": "http://arxiv.org/abs/2507.12087v2",
        "abstract": "Tracking small, agile multi-objects (SMOT), such as birds, from an Unmanned\nAerial Vehicle (UAV) perspective is a highly challenging computer vision task.\nThe difficulty stems from three main sources: the extreme scarcity of target\nappearance features, the complex motion entanglement caused by the combined\ndynamics of the camera and the targets themselves, and the frequent occlusions\nand identity ambiguity arising from dense flocking behavior. This paper details\nour championship-winning solution in the MVA 2025 \"Finding Birds\" Small\nMulti-Object Tracking Challenge (SMOT4SB), which adopts the\ntracking-by-detection paradigm with targeted innovations at both the detection\nand association levels. On the detection side, we propose a systematic training\nenhancement framework named \\textbf{SliceTrain}. This framework, through the\nsynergy of 'deterministic full-coverage slicing' and 'slice-level stochastic\naugmentation, effectively addresses the problem of insufficient learning for\nsmall objects in high-resolution image training. On the tracking side, we\ndesigned a robust tracker that is completely independent of appearance\ninformation. By integrating a \\textbf{motion direction maintenance (EMA)}\nmechanism and an \\textbf{adaptive similarity metric} combining \\textbf{bounding\nbox expansion and distance penalty} into the OC-SORT framework, our tracker can\nstably handle irregular motion and maintain target identities. Our method\nachieves state-of-the-art performance on the SMOT4SB public test set, reaching\nan SO-HOTA score of \\textbf{55.205}, which fully validates the effectiveness\nand advancement of our framework in solving complex real-world SMOT problems.\nThe source code will be made available at\nhttps://github.com/Salvatore-Love/YOLOv8-SMOT."
    },
    {
        "date": "2025-07",
        "title": "Toward an Intent-Based and Ontology-Driven Autonomic Security Response in Security Orchestration Automation and Response",
        "author": "Zequan Huang, Jacques Robin, Nicolas Herbaut, Nourh\u00e8ne Ben Rabah, and B\u00e9n\u00e9dicte Le Grand",
        "link": "http://arxiv.org/abs/2507.12061v1",
        "abstract": "Modern Security Orchestration, Automation, and Response (SOAR) platforms must\nrapidly adapt to continuously evolving cyber attacks. Intent-Based Networking\nhas emerged as a promising paradigm for cyber attack mitigation through\nhigh-level declarative intents, which offer greater flexibility and persistency\nthan procedural actions. In this paper, we bridge the gap between two active\nresearch directions: Intent-Based Cyber Defense and Autonomic Cyber Defense, by\nproposing a unified, ontology-driven security intent definition leveraging the\nMITRE-D3FEND cybersecurity ontology. We also propose a general two-tiered\nmethodology for integrating such security intents into decision-theoretic\nAutonomic Cyber Defense systems, enabling hierarchical and context-aware\nautomated response capabilities. The practicality of our approach is\ndemonstrated through a concrete use case, showcasing its integration within\nnext-generation Security Orchestration, Automation, and Response platforms."
    },
    {
        "date": "2025-07",
        "title": "IDFace: Face Template Protection for Efficient and Secure Identification",
        "author": "Sunpill Kim, Seunghun Paik, Chanwoo Hwang, Dongsoo Kim, Junbum Shin, and Jae Hong Seo",
        "link": "http://arxiv.org/abs/2507.12050v1",
        "abstract": "As face recognition systems (FRS) become more widely used, user privacy\nbecomes more important. A key privacy issue in FRS is protecting the user's\nface template, as the characteristics of the user's face image can be recovered\nfrom the template. Although recent advances in cryptographic tools such as\nhomomorphic encryption (HE) have provided opportunities for securing the FRS,\nHE cannot be used directly with FRS in an efficient plug-and-play manner. In\nparticular, although HE is functionally complete for arbitrary programs, it is\nbasically designed for algebraic operations on encrypted data of predetermined\nshape, such as a polynomial ring. Thus, a non-tailored combination of HE and\nthe system can yield very inefficient performance, and many previous HE-based\nface template protection methods are hundreds of times slower than plain\nsystems without protection. In this study, we propose IDFace, a new HE-based\nsecure and efficient face identification method with template protection.\nIDFace is designed on the basis of two novel techniques for efficient searching\non a (homomorphically encrypted) biometric database with an angular metric. The\nfirst technique is a template representation transformation that sharply\nreduces the unit cost for the matching test. The second is a space-efficient\nencoding that reduces wasted space from the encryption algorithm, thus saving\nthe number of operations on encrypted templates. Through experiments, we show\nthat IDFace can identify a face template from among a database of 1M encrypted\ntemplates in 126ms, showing only 2X overhead compared to the identification\nover plaintexts."
    },
    {
        "date": "2025-07",
        "title": "Expanding ML-Documentation Standards For Better Security",
        "author": "Cara Ellen Appel",
        "link": "http://arxiv.org/abs/2507.12003v1",
        "abstract": "This article presents the current state of ML-security and of the\ndocumentation of ML-based systems, models and datasets in research and practice\nbased on an extensive review of the existing literature. It shows a generally\nlow awareness of security aspects among ML-practitioners and organizations and\nan often unstandardized approach to documentation, leading to overall low\nquality of ML-documentation. Existing standards are not regularly adopted in\npractice and IT-security aspects are often not included in documentation. Due\nto these factors, there is a clear need for improved security documentation in\nML, as one step towards addressing the existing gaps in ML-security. To achieve\nthis, we propose expanding existing documentation standards for\nML-documentation to include a security section with specific security relevant\ninformation. Implementing this, a novel expanded method of documenting security\nrequirements in ML-documentation is presented, based on the existing Model\nCards and Datasheets for Datasets standards, but with the recommendation to\nadopt these findings in all ML-documentation."
    },
    {
        "date": "2025-07",
        "title": "Robust Planning for Autonomous Vehicles with Diffusion-Based Failure Samplers",
        "author": "Juanran Wang, Marc R. Schlichting, and Mykel J. Kochenderfer",
        "link": "http://arxiv.org/abs/2507.11991v1",
        "abstract": "High-risk traffic zones such as intersections are a major cause of\ncollisions. This study leverages deep generative models to enhance the safety\nof autonomous vehicles in an intersection context. We train a 1000-step\ndenoising diffusion probabilistic model to generate collision-causing sensor\nnoise sequences for an autonomous vehicle navigating a four-way intersection\nbased on the current relative position and velocity of an intruder. Using the\ngenerative adversarial architecture, the 1000-step model is distilled into a\nsingle-step denoising diffusion model which demonstrates fast inference speed\nwhile maintaining similar sampling quality. We demonstrate one possible\napplication of the single-step model in building a robust planner for the\nautonomous vehicle. The planner uses the single-step model to efficiently\nsample potential failure cases based on the currently measured traffic state to\ninform its decision-making. Through simulation experiments, the robust planner\ndemonstrates significantly lower failure rate and delay rate compared with the\nbaseline Intelligent Driver Model controller."
    },
    {
        "date": "2025-07",
        "title": "Watch, Listen, Understand, Mislead: Tri-modal Adversarial Attacks on Short Videos for Content Appropriateness Evaluation",
        "author": "Sahid Hossain Mustakim, S M Jishanul Islam, Ummay Maria Muna, Montasir Chowdhury, Mohammed Jawwadul Islam, Sadia Ahmmed, Tashfia Sikder, Syed Tasdid Azam Dhrubo, and Swakkhar Shatabda",
        "link": "http://arxiv.org/abs/2507.11968v1",
        "abstract": "Multimodal Large Language Models (MLLMs) are increasingly used for content\nmoderation, yet their robustness in short-form video contexts remains\nunderexplored. Current safety evaluations often rely on unimodal attacks,\nfailing to address combined attack vulnerabilities. In this paper, we introduce\na comprehensive framework for evaluating the tri-modal safety of MLLMs. First,\nwe present the Short-Video Multimodal Adversarial (SVMA) dataset, comprising\ndiverse short-form videos with human-guided synthetic adversarial attacks.\nSecond, we propose ChimeraBreak, a novel tri-modal attack strategy that\nsimultaneously challenges visual, auditory, and semantic reasoning pathways.\nExtensive experiments on state-of-the-art MLLMs reveal significant\nvulnerabilities with high Attack Success Rates (ASR). Our findings uncover\ndistinct failure modes, showing model biases toward misclassifying benign or\npolicy-violating content. We assess results using LLM-as-a-judge, demonstrating\nattack reasoning efficacy. Our dataset and findings provide crucial insights\nfor developing more robust and safe MLLMs."
    },
    {
        "date": "2025-07",
        "title": "How To Mitigate And Defend Against DDoS Attacks In IoT Devices",
        "author": "Ifiyemi Leigha, Basak Comlekcioglu, and Maria Pilar Bezanilla",
        "link": "http://arxiv.org/abs/2507.11772v1",
        "abstract": "Distributed Denial of Service (DDoS) attacks have become increasingly\nprevalent and dangerous in the context of Internet of Things (IoT) networks,\nprimarily due to the low-security configurations of many connected devices.\nThis paper analyzes the nature and impact of DDoS attacks such as those\nlaunched by the Mirai botnet, and proposes layered mitigation strategies\ntailored to IoT environments. Key solutions explored include IPv6 Unique Local\nAddresses (ULA), edge computing, software-defined networking (SDN), honeypot\ndeception, and machine learning-based intrusion detection systems. The paper\naims to help engineers and researchers understand and implement practical\ncountermeasures to protect IoT infrastructures."
    },
    {
        "date": "2025-07",
        "title": "Reinforcement Learning from Adversarial Preferences in Tabular MDPs",
        "author": "Taira Tsuchiya, Shinji Ito, and Haipeng Luo",
        "link": "http://arxiv.org/abs/2507.11706v1",
        "abstract": "We introduce a new framework of episodic tabular Markov decision processes\n(MDPs) with adversarial preferences, which we refer to as preference-based MDPs\n(PbMDPs). Unlike standard episodic MDPs with adversarial losses, where the\nnumerical value of the loss is directly observed, in PbMDPs the learner instead\nobserves preferences between two candidate arms, which represent the choices\nbeing compared. In this work, we focus specifically on the setting where the\nreward functions are determined by Borda scores. We begin by establishing a\nregret lower bound for PbMDPs with Borda scores. As a preliminary step, we\npresent a simple instance to prove a lower bound of $\\Omega(\\sqrt{HSAT})$ for\nepisodic MDPs with adversarial losses, where $H$ is the number of steps per\nepisode, $S$ is the number of states, $A$ is the number of actions, and $T$ is\nthe number of episodes. Leveraging this construction, we then derive a regret\nlower bound of $\\Omega( (H^2 S K)^{1/3} T^{2/3} )$ for PbMDPs with Borda\nscores, where $K$ is the number of arms. Next, we develop algorithms that\nachieve a regret bound of order $T^{2/3}$. We first propose a global\noptimization approach based on online linear optimization over the set of all\noccupancy measures, achieving a regret bound of $\\tilde{O}((H^2 S^2 K)^{1/3}\nT^{2/3} )$ under known transitions. However, this approach suffers from\nsuboptimal dependence on the potentially large number of states $S$ and\ncomputational inefficiency. To address this, we propose a policy optimization\nalgorithm whose regret is roughly bounded by $\\tilde{O}( (H^6 S K^5)^{1/3}\nT^{2/3} )$ under known transitions, and further extend the result to the\nunknown-transition setting."
    },
    {
        "date": "2025-07",
        "title": "The Impact of Coreset Selection on Spurious Correlations and Group Robustness",
        "author": "Amaya Dharmasiri, William Yang, Polina Kirichenko, Lydia Liu, and Olga Russakovsky",
        "link": "http://arxiv.org/abs/2507.11690v1",
        "abstract": "Coreset selection methods have shown promise in reducing the training data\nsize while maintaining model performance for data-efficient machine learning.\nHowever, as many datasets suffer from biases that cause models to learn\nspurious correlations instead of causal features, it is important to understand\nwhether and how dataset reduction methods may perpetuate, amplify, or mitigate\nthese biases. In this work, we conduct the first comprehensive analysis of the\nimplications of data selection on the spurious bias levels of the selected\ncoresets and the robustness of downstream models trained on them. We use an\nextensive experimental setting spanning ten different spurious correlations\nbenchmarks, five score metrics to characterize sample importance/ difficulty,\nand five data selection policies across a broad range of coreset sizes.\nThereby, we unravel a series of nontrivial nuances in interactions between\nsample difficulty and bias alignment, as well as dataset bias and resultant\nmodel robustness. For example, we find that selecting coresets using\nembedding-based sample characterization scores runs a comparatively lower risk\nof inadvertently exacerbating bias than selecting using characterizations based\non learning dynamics. Most importantly, our analysis reveals that although some\ncoreset selection methods could achieve lower bias levels by prioritizing\ndifficult samples, they do not reliably guarantee downstream robustness."
    },
    {
        "date": "2025-07",
        "title": "Exploring the robustness of TractOracle methods in RL-based tractography",
        "author": "Jeremi Levesque, Antoine Th\u00e9berge, Maxime Descoteaux, and Pierre-Marc Jodoin",
        "link": "http://arxiv.org/abs/2507.11486v1",
        "abstract": "Tractography algorithms leverage diffusion MRI to reconstruct the fibrous\narchitecture of the brain's white matter. Among machine learning approaches,\nreinforcement learning (RL) has emerged as a promising framework for\ntractography, outperforming traditional methods in several key aspects.\nTractOracle-RL, a recent RL-based approach, reduces false positives by\nincorporating anatomical priors into the training process via a reward-based\nmechanism. In this paper, we investigate four extensions of the original\nTractOracle-RL framework by integrating recent advances in RL, and we evaluate\ntheir performance across five diverse diffusion MRI datasets. Results\ndemonstrate that combining an oracle with the RL framework consistently leads\nto robust and reliable tractography, regardless of the specific method or\ndataset used. We also introduce a novel RL training scheme called Iterative\nReward Training (IRT), inspired by the Reinforcement Learning from Human\nFeedback (RLHF) paradigm. Instead of relying on human input, IRT leverages\nbundle filtering methods to iteratively refine the oracle's guidance throughout\ntraining. Experimental results show that RL methods trained with oracle\nfeedback significantly outperform widely used tractography techniques in terms\nof accuracy and anatomical validity."
    },
    {
        "date": "2025-07",
        "title": "D3FL: Data Distribution and Detrending for Robust Federated Learning in Non-linear Time-series Data",
        "author": "Harsha Varun Marisetty, Manik Gupta, and Yogesh Simmhan",
        "link": "http://arxiv.org/abs/2507.11471v1",
        "abstract": "With advancements in computing and communication technologies, the Internet\nof Things (IoT) has seen significant growth. IoT devices typically collect data\nfrom various sensors, such as temperature, humidity, and energy meters. Much of\nthis data is temporal in nature. Traditionally, data from IoT devices is\ncentralized for analysis, but this approach introduces delays and increased\ncommunication costs. Federated learning (FL) has emerged as an effective\nalternative, allowing for model training across distributed devices without the\nneed to centralize data. In many applications, such as smart home energy and\nenvironmental monitoring, the data collected by IoT devices across different\nlocations can exhibit significant variation in trends and seasonal patterns.\nAccurately forecasting such non-stationary, non-linear time-series data is\ncrucial for applications like energy consumption estimation and weather\nforecasting. However, these data variations can severely impact prediction\naccuracy. The key contributions of this paper are: (1) Investigating how\nnon-linear, non-stationary time-series data distributions, like generalized\nextreme value (gen-extreme) and log norm distributions, affect FL performance.\n(2) Analyzing how different detrending techniques for non-linear time-series\ndata influence the forecasting model's performance in a FL setup. We generated\nseveral synthetic time-series datasets using non-linear data distributions and\ntrained an LSTM-based forecasting model using both centralized and FL\napproaches. Additionally, we evaluated the impact of detrending on real-world\ndatasets with non-linear time-series data distributions. Our experimental\nresults show that: (1) FL performs worse than centralized approaches when\ndealing with non-linear data distributions. (2) The use of appropriate\ndetrending techniques improves FL performance, reducing loss across different\ndata distributions."
    },
    {
        "date": "2025-07",
        "title": "Magneto-Ionic Hardware Security Primitives: Embedding Data Protection at the Material Level",
        "author": "Irena Spasojevic, Federica Celegato, Alessandro Magni, Paola Tiberto, and Jordi Sort",
        "link": "http://arxiv.org/abs/2507.14213v1",
        "abstract": "The Big Data revolution has heightened the demand for robust,\nenergy-efficient security hardware capable of withstanding increasingly\nsophisticated cyber threats. Conventional encryption schemes, reliant on\ncomplex algorithms, are resource-intensive and remain vulnerable. To fortify\nsensitive information, society needs innovative anti-hacking and\nanti-counterfeiting technologies that exploit new materials and designs. Here,\nwe present a magneto-ionic strategy for hardware-level security based on fully\nselective voltage-controlled N3- ion migration within pre-defined, initially\nparamagnetic FeCoN dots. This process generates ferromagnetic sublayers of\ntuneable thickness, resulting in either deterministic (single-domain or vortex)\nor probabilistic states (with coexisting magnetic configurations and\nvoltage-adjustable probabilities), each exhibiting stochastic orientation and\nchirality, thereby providing a rich platform for magnetic fingerprinting. This\napproach enables self-protected primitives, including true random number\ngenerators, physical unclonable functions, and in-memory probabilistic\ninference. The resulting reconfigurable architecture combines tamper\nresistance, low energy consumption, and scalability, marking a significant leap\ntoward next-generation hardware security rooted in emergent magnetic phenomena."
    },
    {
        "date": "2025-07",
        "title": "Robust-Multi-Task Gradient Boosting",
        "author": "Seyedsaman Emami, Gonzalo Mart\u00ednez-Mu\u00f1oz, and Daniel Hern\u00e1ndez-Lobato",
        "link": "http://arxiv.org/abs/2507.11411v1",
        "abstract": "Multi-task learning (MTL) has shown effectiveness in exploiting shared\ninformation across tasks to improve generalization. MTL assumes tasks share\nsimilarities that can improve performance. In addition, boosting algorithms\nhave demonstrated exceptional performance across diverse learning problems,\nprimarily due to their ability to focus on hard-to-learn instances and\niteratively reduce residual errors. This makes them a promising approach for\nlearning multi-task problems. However, real-world MTL scenarios often involve\ntasks that are not well-aligned (known as outlier or adversarial tasks), which\ndo not share beneficial similarities with others and can, in fact, deteriorate\nthe performance of the overall model. To overcome this challenge, we propose\nRobust-Multi-Task Gradient Boosting (R-MTGB), a novel boosting framework that\nexplicitly models and adapts to task heterogeneity during training. R-MTGB\nstructures the learning process into three sequential blocks: (1) learning\nshared patterns, (2) partitioning tasks into outliers and non-outliers with\nregularized parameters, and (3) fine-tuning task-specific predictors. This\narchitecture enables R-MTGB to automatically detect and penalize outlier tasks\nwhile promoting effective knowledge transfer among related tasks. Our method\nintegrates these mechanisms seamlessly within gradient boosting, allowing\nrobust handling of noisy or adversarial tasks without sacrificing accuracy.\nExtensive experiments on both synthetic benchmarks and real-world datasets\ndemonstrate that our approach successfully isolates outliers, transfers\nknowledge, and consistently reduces prediction errors for each task\nindividually, and achieves overall performance gains across all tasks. These\nresults highlight robustness, adaptability, and reliable convergence of R-MTGB\nin challenging MTL environments."
    },
    {
        "date": "2025-07",
        "title": "LyAm: Robust Non-Convex Optimization for Stable Learning in Noisy Environments",
        "author": "Elmira Mirzabeigi, Sepehr Rezaee, and Kourosh Parand",
        "link": "http://arxiv.org/abs/2507.11262v1",
        "abstract": "Training deep neural networks, particularly in computer vision tasks, often\nsuffers from noisy gradients and unstable convergence, which hinder performance\nand generalization. In this paper, we propose LyAm, a novel optimizer that\nintegrates Adam's adaptive moment estimation with Lyapunov-based stability\nmechanisms. LyAm dynamically adjusts the learning rate using Lyapunov stability\ntheory to enhance convergence robustness and mitigate training noise. We\nprovide a rigorous theoretical framework proving the convergence guarantees of\nLyAm in complex, non-convex settings. Extensive experiments on like as CIFAR-10\nand CIFAR-100 show that LyAm consistently outperforms state-of-the-art\noptimizers in terms of accuracy, convergence speed, and stability, establishing\nit as a strong candidate for robust deep learning optimization."
    },
    {
        "date": "2025-07",
        "title": "A Robust Incomplete Multimodal Low-Rank Adaptation Approach for Emotion Recognition",
        "author": "Xinkui Zhao, Jinsong Shu, Yangyang Wu, Guanjie Cheng, Zihe Liu, Naibo Wang, Shuiguang Deng, Zhongle Xie, and Jianwei Yin",
        "link": "http://arxiv.org/abs/2507.11202v1",
        "abstract": "Multimodal Emotion Recognition (MER) often encounters incomplete\nmultimodality in practical applications due to sensor failures or privacy\nprotection requirements. While existing methods attempt to address various\nincomplete multimodal scenarios by balancing the training of each modality\ncombination through additional gradients, these approaches face a critical\nlimitation: training gradients from different modality combinations conflict\nwith each other, ultimately degrading the performance of the final prediction\nmodel. In this paper, we propose a unimodal decoupled dynamic low-rank\nadaptation method based on modality combinations, named MCULoRA, which is a\nnovel framework for the parameter-efficient training of incomplete multimodal\nlearning models. MCULoRA consists of two key modules, modality combination\naware low-rank adaptation (MCLA) and dynamic parameter fine-tuning (DPFT). The\nMCLA module effectively decouples the shared information from the distinct\ncharacteristics of individual modality combinations. The DPFT module adjusts\nthe training ratio of modality combinations based on the separability of each\nmodality's representation space, optimizing the learning efficiency across\ndifferent modality combinations. Our extensive experimental evaluation in\nmultiple benchmark datasets demonstrates that MCULoRA substantially outperforms\nprevious incomplete multimodal learning approaches in downstream task accuracy."
    },
    {
        "date": "2025-07",
        "title": "Secure Goal-Oriented Communication: Defending against Eavesdropping Timing Attacks",
        "author": "Federico Mason, Federico Chiariotti, Pietro Talli, and Andrea Zanella",
        "link": "http://arxiv.org/abs/2507.14212v1",
        "abstract": "Goal-oriented Communication (GoC) is a new paradigm that plans data\ntransmission to occur only when it is instrumental for the receiver to achieve\na certain goal. This leads to the advantage of reducing the frequency of\ntransmissions significantly while maintaining adherence to the receiver's\nobjectives. However, GoC scheduling also opens a timing-based side channel that\nan eavesdropper can exploit to obtain information about the state of the\nsystem. This type of attack sidesteps even information-theoretic security, as\nit exploits the timing of updates rather than their content. In this work, we\nstudy such an eavesdropping attack against pull-based goal-oriented scheduling\nfor remote monitoring and control of Markov processes. We provide a theoretical\nframework for defining the effectiveness of the attack and propose possible\ncountermeasures, including two practical heuristics that provide a balance\nbetween the performance gains offered by GoC and the amount of leaked\ninformation. Our results show that, while a naive goal-oriented scheduler\nallows the eavesdropper to correctly guess the system state about 60% of the\ntime, our heuristic defenses can halve the leakage with a marginal reduction of\nthe benefits of goal-oriented approaches."
    },
    {
        "date": "2025-07",
        "title": "Hashed Watermark as a Filter: Defeating Forging and Overwriting Attacks in Weight-based Neural Network Watermarking",
        "author": "Yuan Yao, Jin Song, and Jian Jin",
        "link": "http://arxiv.org/abs/2507.11137v1",
        "abstract": "As valuable digital assets, deep neural networks necessitate robust ownership\nprotection, positioning neural network watermarking (NNW) as a promising\nsolution. Among various NNW approaches, weight-based methods are favored for\ntheir simplicity and practicality; however, they remain vulnerable to forging\nand overwriting attacks. To address those challenges, we propose NeuralMark, a\nrobust method built around a hashed watermark filter. Specifically, we utilize\na hash function to generate an irreversible binary watermark from a secret key,\nwhich is then used as a filter to select the model parameters for embedding.\nThis design cleverly intertwines the embedding parameters with the hashed\nwatermark, providing a robust defense against both forging and overwriting\nattacks. An average pooling is also incorporated to resist fine-tuning and\npruning attacks. Furthermore, it can be seamlessly integrated into various\nneural network architectures, ensuring broad applicability. Theoretically, we\nanalyze its security boundary. Empirically, we verify its effectiveness and\nrobustness across 13 distinct Convolutional and Transformer architectures,\ncovering five image classification tasks and one text generation task. The\nsource codes are available at https://github.com/AIResearch-Group/NeuralMark."
    },
    {
        "date": "2025-07",
        "title": "Robust 3D-Masked Part-level Editing in 3D Gaussian Splatting with Regularized Score Distillation Sampling",
        "author": "Hayeon Kim, Ji Ha Jang, and Se Young Chun",
        "link": "http://arxiv.org/abs/2507.11061v2",
        "abstract": "Recent advances in 3D neural representations and instance-level editing\nmodels have enabled the efficient creation of high-quality 3D content. However,\nachieving precise local 3D edits remains challenging, especially for Gaussian\nSplatting, due to inconsistent multi-view 2D part segmentations and inherently\nambiguous nature of Score Distillation Sampling (SDS) loss. To address these\nlimitations, we propose RoMaP, a novel local 3D Gaussian editing framework that\nenables precise and drastic part-level modifications. First, we introduce a\nrobust 3D mask generation module with our 3D-Geometry Aware Label Prediction\n(3D-GALP), which uses spherical harmonics (SH) coefficients to model\nview-dependent label variations and soft-label property, yielding accurate and\nconsistent part segmentations across viewpoints. Second, we propose a\nregularized SDS loss that combines the standard SDS loss with additional\nregularizers. In particular, an L1 anchor loss is introduced via our Scheduled\nLatent Mixing and Part (SLaMP) editing method, which generates high-quality\npart-edited 2D images and confines modifications only to the target region\nwhile preserving contextual coherence. Additional regularizers, such as\nGaussian prior removal, further improve flexibility by allowing changes beyond\nthe existing context, and robust 3D masking prevents unintended edits.\nExperimental results demonstrate that our RoMaP achieves state-of-the-art local\n3D editing on both reconstructed and generated Gaussian scenes and objects\nqualitatively and quantitatively, making it possible for more robust and\nflexible part-level 3D Gaussian editing. Code is available at\nhttps://janeyeon.github.io/romap."
    },
    {
        "date": "2025-07",
        "title": "GATE: Graph Attention Neural Networks with Real-Time Edge Construction for Robust Indoor Localization using Mobile Embedded Devices",
        "author": "Danish Gufran, and Sudeep Pasricha",
        "link": "http://arxiv.org/abs/2507.11053v1",
        "abstract": "Accurate indoor localization is crucial for enabling spatial context in smart\nenvironments and navigation systems. Wi-Fi Received Signal Strength (RSS)\nfingerprinting is a widely used indoor localization approach due to its\ncompatibility with mobile embedded devices. Deep Learning (DL) models improve\naccuracy in localization tasks by learning RSS variations across locations, but\nthey assume fingerprint vectors exist in a Euclidean space, failing to\nincorporate spatial relationships and the non-uniform distribution of\nreal-world RSS noise. This results in poor generalization across heterogeneous\nmobile devices, where variations in hardware and signal processing distort RSS\nreadings. Graph Neural Networks (GNNs) can improve upon conventional DL models\nby encoding indoor locations as nodes and modeling their spatial and signal\nrelationships as edges. However, GNNs struggle with non-Euclidean noise\ndistributions and suffer from the GNN blind spot problem, leading to degraded\naccuracy in environments with dense access points (APs). To address these\nchallenges, we propose GATE, a novel framework that constructs an adaptive\ngraph representation of fingerprint vectors while preserving an indoor\nstate-space topology, modeling the non-Euclidean structure of RSS noise to\nmitigate environmental noise and address device heterogeneity. GATE introduces\n1) a novel Attention Hyperspace Vector (AHV) for enhanced message passing, 2) a\nnovel Multi-Dimensional Hyperspace Vector (MDHV) to mitigate the GNN blind\nspot, and 3) an new Real-Time Edge Construction (RTEC) approach for dynamic\ngraph adaptation. Extensive real-world evaluations across multiple indoor\nspaces with varying path lengths, AP densities, and heterogeneous devices\ndemonstrate that GATE achieves 1.6x to 4.72x lower mean localization errors and\n1.85x to 4.57x lower worst-case errors compared to state-of-the-art indoor\nlocalization frameworks."
    },
    {
        "date": "2025-07",
        "title": "A Multi-View High-Resolution Foot-Ankle Complex Point Cloud Dataset During Gait for Occlusion-Robust 3D Completion",
        "author": "Jie-Wen Li, Zi-Han Ye, Qingyuan Zhou, Jiayi Song, Ying He, Ben Fei, and Wen-Ming Chen",
        "link": "http://arxiv.org/abs/2507.11037v1",
        "abstract": "The kinematics analysis of foot-ankle complex during gait is essential for\nadvancing biomechanical research and clinical assessment. Collecting accurate\nsurface geometry data from the foot and ankle during dynamic gait conditions is\ninherently challenging due to swing foot occlusions and viewing limitations.\nThus, this paper introduces FootGait3D, a novel multi-view dataset of\nhigh-resolution ankle-foot surface point clouds captured during natural gait.\nDifferent from existing gait datasets that typically target whole-body or\nlower-limb motion, FootGait3D focuses specifically on the detailed modeling of\nthe ankle-foot region, offering a finer granularity of motion data. To address\nthis, FootGait3D consists of 8,403 point cloud frames collected from 46\nsubjects using a custom five-camera depth sensing system. Each frame includes a\ncomplete 5-view reconstruction of the foot and ankle (serving as ground truth)\nalong with partial point clouds obtained from only four, three, or two views.\nThis structured variation enables rigorous evaluation of 3D point cloud\ncompletion methods under varying occlusion levels and viewpoints. Our dataset\nis designed for shape completion tasks, facilitating the benchmarking of\nstate-of-the-art single-modal (e.g., PointTr, SnowflakeNet, Anchorformer) and\nmulti-modal (e.g., SVDFormer, PointSea, CSDN) completion networks on the\nchallenge of recovering the full foot geometry from occluded inputs. FootGait3D\nhas significant potential to advance research in biomechanics and multi-segment\nfoot modeling, offering a valuable testbed for clinical gait analysis,\nprosthetic design, and robotics applications requiring detailed 3D models of\nthe foot during motion. The dataset is now available at\nhttps://huggingface.co/datasets/ljw285/FootGait3D."
    },
    {
        "date": "2025-07",
        "title": "Crafting Imperceptible On-Manifold Adversarial Attacks for Tabular Data",
        "author": "Zhipeng He, Alexander Stevens, Chun Ouyang, Johannes De Smedt, Alistair Barros, and Catarina Moreira",
        "link": "http://arxiv.org/abs/2507.10998v1",
        "abstract": "Adversarial attacks on tabular data present fundamental challenges distinct\nfrom image or text domains due to the heterogeneous nature of mixed categorical\nand numerical features. Unlike images where pixel perturbations maintain visual\nsimilarity, tabular data lacks intuitive similarity metrics, making it\ndifficult to define imperceptible modifications. Additionally, traditional\ngradient-based methods prioritise $\\ell_p$-norm constraints, often producing\nadversarial examples that deviate from the original data distributions, making\nthem detectable. We propose a latent space perturbation framework using a\nmixed-input Variational Autoencoder (VAE) to generate imperceptible adversarial\nexamples. The proposed VAE integrates categorical embeddings and numerical\nfeatures into a unified latent manifold, enabling perturbations that preserve\nstatistical consistency. We specify In-Distribution Success Rate (IDSR) to\nmeasure the proportion of adversarial examples that remain statistically\nindistinguishable from the input distribution. Evaluation across six publicly\navailable datasets and three model architectures demonstrates that our method\nachieves substantially lower outlier rates and more consistent performance\ncompared to traditional input-space attacks and other VAE-based methods adapted\nfrom image domain approaches. Our comprehensive analysis includes\nhyperparameter sensitivity, sparsity control mechanisms, and generative\narchitectural comparisons, revealing that VAE-based attacks depend critically\non reconstruction quality but offer superior practical utility when sufficient\ntraining data is available. This work highlights the importance of on-manifold\nperturbations for realistic adversarial attacks on tabular data, offering a\nrobust approach for practical deployment. The source code can be accessed\nthrough https://github.com/ZhipengHe/VAE-TabAttack."
    },
    {
        "date": "2025-07",
        "title": "Security Enclave Architecture for Heterogeneous Security Primitives for Supply-Chain Attacks",
        "author": "Kshitij Raj, Atri Chatterjee, Patanjali SLPSK, Swarup Bhunia, and Sandip Ray",
        "link": "http://arxiv.org/abs/2507.10971v1",
        "abstract": "Designing secure architectures for system-on-chip (SoC) platforms is a highly\nintricate and time-intensive task, often requiring months of development and\nmeticulous verification. Even minor architectural oversights can lead to\ncritical vulnerabilities that undermine the security of the entire chip. In\nresponse to this challenge, we introduce CITADEL, a modular security framework\naimed at streamlining the creation of robust security architectures for SoCs.\nCITADEL offers a configurable, plug-and-play subsystem composed of custom\nintellectual property (IP) blocks, enabling the construction of diverse\nsecurity mechanisms tailored to specific threats. As a concrete demonstration,\nwe instantiate CITADEL to defend against supply-chain threats, illustrating how\nthe framework adapts to one of the most pressing concerns in hardware security.\nThis paper explores the range of obstacles encountered when building a unified\nsecurity architecture capable of addressing multiple attack vectors and\npresents CITADEL's strategies for overcoming them. Through several real-world\ncase studies, we showcase the practical implementation of CITADEL and present a\nthorough evaluation of its impact on silicon area and power consumption across\nvarious ASIC technologies. Results indicate that CITADEL introduces only\nminimal resource overhead, making it a practical solution for enhancing SoC\nsecurity."
    },
    {
        "date": "2025-07",
        "title": "Small Edits, Big Consequences: Telling Good from Bad Robustness in Large Language Models",
        "author": "Altynbek Ismailov, and Salia Asanova",
        "link": "http://arxiv.org/abs/2507.15868v1",
        "abstract": "Large language models (LLMs) now write code in settings where misreading a\nsingle word can break safety or cost money, yet we still expect them to\noverlook stray typos. To probe where useful robustness ends and harmful\ninsensitivity begins, we compile 50 LeetCode problems and craft three minimal\nprompt perturbations that should vary in importance: (i) progressive\nunderspecification deleting 10 % of words per step; (ii) lexical flip swapping\na pivotal quantifier (\"max\" to \"min\"); and (iii) jargon inflation replacing a\ncommon noun with an obscure technical synonym. Six frontier models, including\nthree \"reasoning-tuned\" versions, solve each mutated prompt, and their Python\noutputs are checked against the original test suites to reveal whether they\nreused the baseline solution or adapted. Among 11 853 generations we observe a\nsharp double asymmetry. Models remain correct in 85 % of cases even after 90 %\nof the prompt is missing, showing over-robustness to underspecification, yet\nonly 54 % react to a single quantifier flip that reverses the task, with\nreasoning-tuned variants even less sensitive than their bases. Jargon edits lie\nin between, passing through 56 %. Current LLMs thus blur the line between\nharmless noise and meaning - changing edits, often treating both as ignorable.\nMasking salient anchors such as function names can force re - evaluation. We\nadvocate evaluation and training protocols that reward differential\nsensitivity: stay steady under benign noise but adapt - or refuse - when\nsemantics truly change."
    },
    {
        "date": "2025-07",
        "title": "Robust ID-Specific Face Restoration via Alignment Learning",
        "author": "Yushun Fang, Lu Liu, Xiang Gao, Qiang Hu, Ning Cao, Jianghe Cui, Gang Chen, and Xiaoyun Zhang",
        "link": "http://arxiv.org/abs/2507.10943v1",
        "abstract": "The latest developments in Face Restoration have yielded significant\nadvancements in visual quality through the utilization of diverse diffusion\npriors. Nevertheless, the uncertainty of face identity introduced by\nidentity-obscure inputs and stochastic generative processes remains unresolved.\nTo address this challenge, we present Robust ID-Specific Face Restoration\n(RIDFR), a novel ID-specific face restoration framework based on diffusion\nmodels. Specifically, RIDFR leverages a pre-trained diffusion model in\nconjunction with two parallel conditioning modules. The Content Injection\nModule inputs the severely degraded image, while the Identity Injection Module\nintegrates the specific identity from a given image. Subsequently, RIDFR\nincorporates Alignment Learning, which aligns the restoration results from\nmultiple references with the same identity in order to suppress the\ninterference of ID-irrelevant face semantics (e.g. pose, expression, make-up,\nhair style). Experiments demonstrate that our framework outperforms the\nstate-of-the-art methods, reconstructing high-quality ID-specific results with\nhigh identity fidelity and demonstrating strong robustness."
    },
    {
        "date": "2025-07",
        "title": "How to Protect Models against Adversarial Unlearning?",
        "author": "Patryk Jasiorski, Marek Klonowski, and Micha\u0142 Wo\u017aniak",
        "link": "http://arxiv.org/abs/2507.10886v1",
        "abstract": "AI models need to be unlearned to fulfill the requirements of legal acts such\nas the AI Act or GDPR, and also because of the need to remove toxic content,\ndebiasing, the impact of malicious instances, or changes in the data\ndistribution structure in which a model works. Unfortunately, removing\nknowledge may cause undesirable side effects, such as a deterioration in model\nperformance. In this paper, we investigate the problem of adversarial\nunlearning, where a malicious party intentionally sends unlearn requests to\ndeteriorate the model's performance maximally. We show that this phenomenon and\nthe adversary's capabilities depend on many factors, primarily on the backbone\nmodel itself and strategy/limitations in selecting data to be unlearned. The\nmain result of this work is a new method of protecting model performance from\nthese side effects, both in the case of unlearned behavior resulting from\nspontaneous processes and adversary actions."
    },
    {
        "date": "2025-07",
        "title": "Learning from Imperfect Data: Robust Inference of Dynamic Systems using Simulation-based Generative Model",
        "author": "Hyunwoo Cho, Hyeontae Jo, and Hyung Ju Hwang",
        "link": "http://arxiv.org/abs/2507.10884v1",
        "abstract": "System inference for nonlinear dynamic models, represented by ordinary\ndifferential equations (ODEs), remains a significant challenge in many fields,\nparticularly when the data are noisy, sparse, or partially observable. In this\npaper, we propose a Simulation-based Generative Model for Imperfect Data\n(SiGMoID) that enables precise and robust inference for dynamic systems. The\nproposed approach integrates two key methods: (1) physics-informed neural\nnetworks with hyper-networks that constructs an ODE solver, and (2) Wasserstein\ngenerative adversarial networks that estimates ODE parameters by effectively\ncapturing noisy data distributions. We demonstrate that SiGMoID quantifies data\nnoise, estimates system parameters, and infers unobserved system components.\nIts effectiveness is validated validated through realistic experimental\nexamples, showcasing its broad applicability in various domains, from\nscientific research to engineered systems, and enabling the discovery of full\nsystem dynamics."
    },
    {
        "date": "2025-07",
        "title": "A Lightweight and Robust Framework for Real-Time Colorectal Polyp Detection Using LOF-Based Preprocessing and YOLO-v11n",
        "author": "Saadat Behzadi, Danial Sharifrazi, Bita Mesbahzadeh, Javad Hassannataj Joloudari, and Roohallah Alizadehsani",
        "link": "http://arxiv.org/abs/2507.10864v2",
        "abstract": "Objectives: Timely and accurate detection of colorectal polyps plays a\ncrucial role in diagnosing and preventing colorectal cancer, a major cause of\nmortality worldwide. This study introduces a new, lightweight, and efficient\nframework for polyp detection that combines the Local Outlier Factor (LOF)\nalgorithm for filtering noisy data with the YOLO-v11n deep learning model.\n  Study design: An experimental study leveraging deep learning and outlier\nremoval techniques across multiple public datasets.\n  Methods: The proposed approach was tested on five diverse and publicly\navailable datasets: CVC-ColonDB, CVC-ClinicDB, Kvasir-SEG, ETIS, and EndoScene.\nSince these datasets originally lacked bounding box annotations, we converted\ntheir segmentation masks into suitable detection labels. To enhance the\nrobustness and generalizability of our model, we apply 5-fold cross-validation\nand remove anomalous samples using the LOF method configured with 30 neighbors\nand a contamination ratio of 5%. Cleaned data are then fed into YOLO-v11n, a\nfast and resource-efficient object detection architecture optimized for\nreal-time applications. We train the model using a combination of modern\naugmentation strategies to improve detection accuracy under diverse conditions.\n  Results: Our approach significantly improves polyp localization performance,\nachieving a precision of 95.83%, recall of 91.85%, F1-score of 93.48%, mAP@0.5\nof 96.48%, and mAP@0.5:0.95 of 77.75%. Compared to previous YOLO-based methods,\nour model demonstrates enhanced accuracy and efficiency.\n  Conclusions: These results suggest that the proposed method is well-suited\nfor real-time colonoscopy support in clinical settings. Overall, the study\nunderscores how crucial data preprocessing and model efficiency are when\ndesigning effective AI systems for medical imaging."
    },
    {
        "date": "2025-07",
        "title": "REAL-IoT: Characterizing GNN Intrusion Detection Robustness under Practical Adversarial Attack",
        "author": "Zhonghao Zhan, Huichi Zhou, and Hamed Haddadi",
        "link": "http://arxiv.org/abs/2507.10836v1",
        "abstract": "Graph Neural Network (GNN)-based network intrusion detection systems (NIDS)\nare often evaluated on single datasets, limiting their ability to generalize\nunder distribution drift. Furthermore, their adversarial robustness is\ntypically assessed using synthetic perturbations that lack realism. This\nmeasurement gap leads to an overestimation of GNN-based NIDS resilience. To\naddress the limitations, we propose \\textbf{REAL-IoT}, a comprehensive\nframework for robustness evaluation of GNN-based NIDS in IoT environments. Our\nframework presents a methodology that creates a unified dataset from canonical\ndatasets to assess generalization under drift. In addition, it features a novel\nintrusion dataset collected from a physical IoT testbed, which captures network\ntraffic and attack scenarios under real-world settings. Furthermore, using\nREAL-IoT, we explore the usage of Large Language Models (LLMs) to analyze\nnetwork data and mitigate the impact of adversarial examples by filtering\nsuspicious flows. Our evaluations using REAL-IoT reveal performance drops in\nGNN models compared to results from standard benchmarks, quantifying their\nsusceptibility to drift and realistic attacks. We also demonstrate the\npotential of LLM-based filtering to enhance robustness. These findings\nemphasize the necessity of realistic threat modeling and rigorous measurement\npractices for developing resilient IoT intrusion detection systems."
    },
    {
        "date": "2025-07",
        "title": "\"Is it always watching? Is it always listening?\" Exploring Contextual Privacy and Security Concerns Toward Domestic Social Robots",
        "author": "Henry Bell, Jabari Kwesi, Hiba Laabadli, and Pardis Emami-Naeini",
        "link": "http://arxiv.org/abs/2507.10786v2",
        "abstract": "Equipped with artificial intelligence (AI) and advanced sensing capabilities,\nsocial robots are gaining interest among consumers in the United States. These\nrobots seem like a natural evolution of traditional smart home devices.\nHowever, their extensive data collection capabilities, anthropomorphic\nfeatures, and capacity to interact with their environment make social robots a\nmore significant security and privacy threat. Increased risks include data\nlinkage, unauthorized data sharing, and the physical safety of users and their\nhomes. It is critical to investigate U.S. users' security and privacy needs and\nconcerns to guide the design of social robots while these devices are still in\nthe early stages of commercialization in the U.S. market. Through 19\nsemi-structured interviews, we identified significant security and privacy\nconcerns, highlighting the need for transparency, usability, and robust privacy\ncontrols to support adoption. For educational applications, participants\nworried most about misinformation, and in medical use cases, they worried about\nthe reliability of these devices. Participants were also concerned with the\ndata inference that social robots could enable. We found that participants\nexpect tangible privacy controls, indicators of data collection, and\ncontext-appropriate functionality."
    },
    {
        "date": "2025-07",
        "title": "Integrating Biological Knowledge for Robust Microscopy Image Profiling on De Novo Cell Lines",
        "author": "Jiayuan Chen, Thai-Hoang Pham, Yuanlong Wang, and Ping Zhang",
        "link": "http://arxiv.org/abs/2507.10737v1",
        "abstract": "High-throughput screening techniques, such as microscopy imaging of cellular\nresponses to genetic and chemical perturbations, play a crucial role in drug\ndiscovery and biomedical research. However, robust perturbation screening for\n\\textit{de novo} cell lines remains challenging due to the significant\nmorphological and biological heterogeneity across cell lines. To address this,\nwe propose a novel framework that integrates external biological knowledge into\nexisting pretraining strategies to enhance microscopy image profiling models.\nOur approach explicitly disentangles perturbation-specific and cell\nline-specific representations using external biological information.\nSpecifically, we construct a knowledge graph leveraging protein interaction\ndata from STRING and Hetionet databases to guide models toward\nperturbation-specific features during pretraining. Additionally, we incorporate\ntranscriptomic features from single-cell foundation models to capture cell\nline-specific representations. By learning these disentangled features, our\nmethod improves the generalization of imaging models to \\textit{de novo} cell\nlines. We evaluate our framework on the RxRx database through one-shot\nfine-tuning on an RxRx1 cell line and few-shot fine-tuning on cell lines from\nthe RxRx19a dataset. Experimental results demonstrate that our method enhances\nmicroscopy image profiling for \\textit{de novo} cell lines, highlighting its\neffectiveness in real-world phenotype-based drug discovery applications."
    },
    {
        "date": "2025-07",
        "title": "3S-Attack: Spatial, Spectral and Semantic Invisible Backdoor Attack Against DNN Models",
        "author": "Jianyao Yin, Luca Arnaboldi, Honglong Chen, and Pascal Berrang",
        "link": "http://arxiv.org/abs/2507.10733v1",
        "abstract": "Backdoor attacks involve either poisoning the training data or directly\nmodifying the model in order to implant a hidden behavior, that causes the\nmodel to misclassify inputs when a specific trigger is present. During\ninference, the model maintains high accuracy on benign samples but\nmisclassifies poisoned samples into an attacker-specified target class.\nExisting research on backdoor attacks has explored developing triggers in the\nspatial, spectral (frequency), and semantic (feature) domains, aiming to make\nthem stealthy. While some approaches have considered designing triggers that\nare imperceptible in both spatial and spectral domains, few have incorporated\nthe semantic domain. In this paper, we propose a novel backdoor attack, termed\n3S-attack, which is stealthy across the spatial, spectral, and semantic\ndomains. The key idea is to exploit the semantic features of benign samples as\ntriggers, using Gradient-weighted Class Activation Mapping (Grad-CAM) and a\npreliminary model for extraction. The trigger is then embedded in the spectral\ndomain, followed by pixel-level restrictions after converting the samples back\nto the spatial domain. This process minimizes the distance between poisoned and\nbenign samples, making the attack harder to detect by existing defenses and\nhuman inspection. Extensive experiments on various datasets, along with\ntheoretical analysis, demonstrate the stealthiness of 3S-attack and highlight\nthe need for stronger defenses to ensure AI security. Our code is available at:\nhttps://anonymous.4open.science/r/anon-project-3776/"
    },
    {
        "date": "2025-07",
        "title": "Access Control for Information-Theoretically Secure Key-Document Stores",
        "author": "Yin Li, Sharad Mehrota, Shantanu Sharma, and Komal Kumari",
        "link": "http://arxiv.org/abs/2507.10730v1",
        "abstract": "This paper presents a novel key-based access control technique for secure\noutsourcing key-value stores where values correspond to documents that are\nindexed and accessed using keys. The proposed approach adopts Shamir's\nsecret-sharing that offers unconditional or information-theoretic security. It\nsupports keyword-based document retrieval while preventing leakage of the data,\naccess rights of users, or the size (\\textit{i}.\\textit{e}., volume of the\noutput that satisfies a query). The proposed approach allows servers to detect\n(and abort) malicious clients from gaining unauthorized access to data, and\nprevents malicious servers from altering data undetected while ensuring\nefficient access -- it takes 231.5ms over 5,000 keywords across 500,000 files."
    },
    {
        "date": "2025-07",
        "title": "Distributionally Robust Optimization with Adversarial Data Contamination",
        "author": "Shuyao Li, Ilias Diakonikolas, and Jelena Diakonikolas",
        "link": "http://arxiv.org/abs/2507.10718v1",
        "abstract": "Distributionally Robust Optimization (DRO) provides a framework for\ndecision-making under distributional uncertainty, yet its effectiveness can be\ncompromised by outliers in the training data. This paper introduces a\nprincipled approach to simultaneously address both challenges. We focus on\noptimizing Wasserstein-1 DRO objectives for generalized linear models with\nconvex Lipschitz loss functions, where an $\\epsilon$-fraction of the training\ndata is adversarially corrupted. Our primary contribution lies in a novel\nmodeling framework that integrates robustness against training data\ncontamination with robustness against distributional shifts, alongside an\nefficient algorithm inspired by robust statistics to solve the resulting\noptimization problem. We prove that our method achieves an estimation error of\n$O(\\sqrt{\\epsilon})$ for the true DRO objective value using only the\ncontaminated data under the bounded covariance assumption. This work\nestablishes the first rigorous guarantees, supported by efficient computation,\nfor learning under the dual challenges of data contamination and distributional\nshifts."
    },
    {
        "date": "2025-07",
        "title": "Robust Multi-Manifold Clustering via Simplex Paths",
        "author": "Haoyu Chen, Anna Little, and Akin Narayan",
        "link": "http://arxiv.org/abs/2507.10710v1",
        "abstract": "This article introduces a novel, geometric approach for multi-manifold\nclustering (MMC), i.e. for clustering a collection of potentially intersecting,\nd-dimensional manifolds into the individual manifold components. We first\ncompute a locality graph on d-simplices, using the dihedral angle in between\nadjacent simplices as the graph weights, and then compute infinity path\ndistances in this simplex graph. This procedure gives a metric on simplices\nwhich we refer to as the largest angle path distance (LAPD). We analyze the\nproperties of LAPD under random sampling, and prove that with an appropriate\ndenoising procedure, this metric separates the manifold components with high\nprobability. We validate the proposed methodology with extensive numerical\nexperiments on both synthetic and real-world data sets. These experiments\ndemonstrate that the method is robust to noise, curvature, and small\nintersection angle, and generally out-performs other MMC algorithms. In\naddition, we provide a highly scalable implementation of the proposed\nalgorithm, which leverages approximation schemes for infinity path distance to\nachieve quasi-linear computational complexity."
    },
    {
        "date": "2025-07",
        "title": "Exploring User Security and Privacy Attitudes and Concerns Toward the Use of General-Purpose LLM Chatbots for Mental Health",
        "author": "Jabari Kwesi, Jiaxun Cao, Riya Manchanda, and Pardis Emami-Naeini",
        "link": "http://arxiv.org/abs/2507.10695v1",
        "abstract": "Individuals are increasingly relying on large language model (LLM)-enabled\nconversational agents for emotional support. While prior research has examined\nprivacy and security issues in chatbots specifically designed for mental health\npurposes, these chatbots are overwhelmingly \"rule-based\" offerings that do not\nleverage generative AI. Little empirical research currently measures users'\nprivacy and security concerns, attitudes, and expectations when using\ngeneral-purpose LLM-enabled chatbots to manage and improve mental health.\nThrough 21 semi-structured interviews with U.S. participants, we identified\ncritical misconceptions and a general lack of risk awareness. Participants\nconflated the human-like empathy exhibited by LLMs with human-like\naccountability and mistakenly believed that their interactions with these\nchatbots were safeguarded by the same regulations (e.g., HIPAA) as disclosures\nwith a licensed therapist. We introduce the concept of \"intangible\nvulnerability,\" where emotional or psychological disclosures are undervalued\ncompared to more tangible forms of information (e.g., financial or\nlocation-based data). To address this, we propose recommendations to safeguard\nuser mental health disclosures with general-purpose LLM-enabled chatbots more\neffectively."
    },
    {
        "date": "2025-07",
        "title": "PRM-Free Security Alignment of Large Models via Red Teaming and Adversarial Training",
        "author": "Pengfei Du",
        "link": "http://arxiv.org/abs/2507.14202v1",
        "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\ndiverse applications, yet they pose significant security risks that threaten\ntheir safe deployment in critical domains. Current security alignment\nmethodologies predominantly rely on Process Reward Models (PRMs) to evaluate\nintermediate reasoning steps, introducing substantial computational overhead\nand scalability constraints. This paper presents a novel PRM-free security\nalignment framework that leverages automated red teaming and adversarial\ntraining to achieve robust security guarantees while maintaining computational\nefficiency. Our approach systematically identifies vulnerabilities through\nsophisticated attack strategies including genetic algorithm optimization,\nmulti-agent simulation, and advanced prompt mutation techniques. The framework\nenhances model robustness via targeted adversarial training with curriculum\nlearning and adaptive regularization mechanisms. Comprehensive experimental\nevaluation across five state-of-the-art LLMs demonstrates that our method\nachieves superior security alignment performance compared to PRM-based\napproaches while reducing computational costs by 61\\%. The framework\nincorporates transparent reporting and continuous audit mechanisms that enable\niterative security improvement and regulatory compliance. Our contributions\nadvance the field of efficient LLM security alignment by democratizing access\nto robust security measures for resource-constrained organizations and\nproviding a scalable foundation for addressing evolving adversarial threats."
    },
    {
        "date": "2025-07",
        "title": "BURN: Backdoor Unlearning via Adversarial Boundary Analysis",
        "author": "Yanghao Su, Jie Zhang, Yiming Li, Tianwei Zhang, Qing Guo, Weiming Zhang, Nenghai Yu, Nils Lukas, and Wenbo Zhou",
        "link": "http://arxiv.org/abs/2507.10491v1",
        "abstract": "Backdoor unlearning aims to remove backdoor-related information while\npreserving the model's original functionality. However, existing unlearning\nmethods mainly focus on recovering trigger patterns but fail to restore the\ncorrect semantic labels of poison samples. This limitation prevents them from\nfully eliminating the false correlation between the trigger pattern and the\ntarget label. To address this, we leverage boundary adversarial attack\ntechniques, revealing two key observations. First, poison samples exhibit\nsignificantly greater distances from decision boundaries compared to clean\nsamples, indicating they require larger adversarial perturbations to change\ntheir predictions. Second, while adversarial predicted labels for clean samples\nare uniformly distributed, those for poison samples tend to revert to their\noriginal correct labels. Moreover, the features of poison samples restore to\nclosely resemble those of corresponding clean samples after adding adversarial\nperturbations. Building upon these insights, we propose Backdoor Unlearning via\nadversaRial bouNdary analysis (BURN), a novel defense framework that integrates\nfalse correlation decoupling, progressive data refinement, and model\npurification. In the first phase, BURN employs adversarial boundary analysis to\ndetect poisoned samples based on their abnormal adversarial boundary distances,\nthen restores their correct semantic labels for fine-tuning. In the second\nphase, it employs a feedback mechanism that tracks prediction discrepancies\nbetween the original backdoored model and progressively sanitized models,\nguiding both dataset refinement and model purification. Extensive evaluations\nacross multiple datasets, architectures, and seven diverse backdoor attack\ntypes confirm that BURN effectively removes backdoor threats while maintaining\nthe model's original performance."
    },
    {
        "date": "2025-07",
        "title": "Logic layer Prompt Control Injection (LPCI): A Novel Security Vulnerability Class in Agentic Systems",
        "author": "Hammad Atta, Ken Huang, Manish Bhatt, Kamal Ahmed, Muhammad Aziz Ul Haq, and Yasir Mehmood",
        "link": "http://arxiv.org/abs/2507.10457v1",
        "abstract": "The integration of large language models (LLMs) into enterprise systems has\ncreated a new class of covert security vulnerabilities, particularly within\nlogic-execution layers and persistent-memory contexts. In this paper, we\nintroduce Logic-Layer Prompt Control Injection (LPCI), a novel attack category\nin which encoded, delayed, and conditionally triggered payloads are embedded in\nmemory, vector stores, or tool outputs. These payloads can bypass conventional\ninput filters and trigger unauthorised behaviour across sessions."
    },
    {
        "date": "2025-07",
        "title": "Test-Time Canonicalization by Foundation Models for Robust Perception",
        "author": "Utkarsh Singhal, Ryan Feng, Stella X. Yu, and Atul Prakash",
        "link": "http://arxiv.org/abs/2507.10375v1",
        "abstract": "Real-world visual perception requires invariance to diverse transformations,\nyet current methods rely heavily on specialized architectures or training on\npredefined augmentations, limiting generalization. We propose FOCAL, a\ntest-time, data-driven framework that achieves robust perception by leveraging\ninternet-scale visual priors from foundation models. By generating and\noptimizing candidate transformations toward visually typical, \"canonical\"\nviews, FOCAL enhances robustness without re-training or architectural changes.\nOur experiments demonstrate improved robustness of CLIP and SAM across\nchallenging transformations, including 2D/3D rotations, illumination shifts\n(contrast and color), and day-night variations. We also highlight potential\napplications in active vision. Our approach challenges the assumption that\ntransform-specific training is necessary, instead offering a scalable path to\ninvariance. Our code is available at: https://github.com/sutkarsh/focal."
    },
    {
        "date": "2025-07",
        "title": "Bridging Robustness and Generalization Against Word Substitution Attacks in NLP via the Growth Bound Matrix Approach",
        "author": "Mohammed Bouri, and Adnane Saoud",
        "link": "http://arxiv.org/abs/2507.10330v1",
        "abstract": "Despite advancements in Natural Language Processing (NLP), models remain\nvulnerable to adversarial attacks, such as synonym substitutions. While prior\nwork has focused on improving robustness for feed-forward and convolutional\narchitectures, the robustness of recurrent networks and modern state space\nmodels (SSMs), such as S4, remains understudied. These architectures pose\nunique challenges due to their sequential processing and complex parameter\ndynamics. In this paper, we introduce a novel regularization technique based on\nGrowth Bound Matrices (GBM) to improve NLP model robustness by reducing the\nimpact of input perturbations on model outputs. We focus on computing the GBM\nfor three architectures: Long Short-Term Memory (LSTM), State Space models\n(S4), and Convolutional Neural Networks (CNN). Our method aims to (1) enhance\nresilience against word substitution attacks, (2) improve generalization on\nclean text, and (3) providing the first systematic analysis of SSM (S4)\nrobustness. Extensive experiments across multiple architectures and benchmark\ndatasets demonstrate that our method improves adversarial robustness by up to\n8.8% over existing baselines. These results highlight the effectiveness of our\napproach, outperforming several state-of-the-art methods in adversarial\ndefense. Codes are available at https://github.com/BouriMohammed/GBM"
    },
    {
        "date": "2025-07",
        "title": "Kaleidoscopic Background Attack: Disrupting Pose Estimation with Multi-Fold Radial Symmetry Textures",
        "author": "Xinlong Ding, Hongwei Yu, Jiawei Li, Feifan Li, Yu Shang, Bochao Zou, Huimin Ma, and Jiansheng Chen",
        "link": "http://arxiv.org/abs/2507.10265v1",
        "abstract": "Camera pose estimation is a fundamental computer vision task that is\nessential for applications like visual localization and multi-view stereo\nreconstruction. In the object-centric scenarios with sparse inputs, the\naccuracy of pose estimation can be significantly influenced by background\ntextures that occupy major portions of the images across different viewpoints.\nIn light of this, we introduce the Kaleidoscopic Background Attack (KBA), which\nuses identical segments to form discs with multi-fold radial symmetry. These\ndiscs maintain high similarity across different viewpoints, enabling effective\nattacks on pose estimation models even with natural texture segments.\nAdditionally, a projected orientation consistency loss is proposed to optimize\nthe kaleidoscopic segments, leading to significant enhancement in the attack\neffectiveness. Experimental results show that optimized adversarial\nkaleidoscopic backgrounds can effectively attack various camera pose estimation\nmodels."
    },
    {
        "date": "2025-07",
        "title": "Transferring Styles for Reduced Texture Bias and Improved Robustness in Semantic Segmentation Networks",
        "author": "Ben Hamscher, Edgar Heinert, Annika M\u00fctze, Kira Maag, and Matthias Rottmann",
        "link": "http://arxiv.org/abs/2507.10239v1",
        "abstract": "Recent research has investigated the shape and texture biases of deep neural\nnetworks (DNNs) in image classification which influence their generalization\ncapabilities and robustness. It has been shown that, in comparison to regular\nDNN training, training with stylized images reduces texture biases in image\nclassification and improves robustness with respect to image corruptions. In an\neffort to advance this line of research, we examine whether style transfer can\nlikewise deliver these two effects in semantic segmentation. To this end, we\nperform style transfer with style varying across artificial image areas. Those\nrandom areas are formed by a chosen number of Voronoi cells. The resulting\nstyle-transferred data is then used to train semantic segmentation DNNs with\nthe objective of reducing their dependence on texture cues while enhancing\ntheir reliance on shape-based features. In our experiments, it turns out that\nin semantic segmentation, style transfer augmentation reduces texture bias and\nstrongly increases robustness with respect to common image corruptions as well\nas adversarial attacks. These observations hold for convolutional neural\nnetworks and transformer architectures on the Cityscapes dataset as well as on\nPASCAL Context, showing the generality of the proposed method."
    },
    {
        "date": "2025-07",
        "title": "Secure and Efficient Quantum Signature Scheme Based on the Controlled Unitary Operations Encryption",
        "author": "Debnath Ghosh, Soumit Roy, Prithwi Bagchi, Indranil Chakrabarty, and Ashok Kumar Das",
        "link": "http://arxiv.org/abs/2507.10233v1",
        "abstract": "Quantum digital signatures ensure unforgeable message authenticity and\nintegrity using quantum principles, offering unconditional security against\nboth classical and quantum attacks. They are crucial for secure communication\nin high-stakes environments, ensuring trust and long-term protection in the\nquantum era. Nowadays, the majority of arbitrated quantum signature (AQS)\nprotocols encrypt data qubit by qubit using the quantum one-time pad (QOTP).\nDespite providing robust data encryption, QOTP is not a good fit for AQS\nbecause of its susceptibility to many types of attacks. In this work, we\npresent an efficient AQS protocol to encrypt quantum message ensembles using a\ndistinct encryption technique, the chained controlled unitary operations. In\ncontrast to existing protocols, our approach successfully prevents disavowal\nand forgery attacks. We hope this contributes to advancing future\ninvestigations into the development of AQS protocols."
    },
    {
        "date": "2025-07",
        "title": "Learning Private Representations through Entropy-based Adversarial Training",
        "author": "Tassilo Klein, and Moin Nabi",
        "link": "http://arxiv.org/abs/2507.10194v1",
        "abstract": "How can we learn a representation with high predictive power while preserving\nuser privacy? We present an adversarial representation learning method for\nsanitizing sensitive content from the learned representation. Specifically, we\nintroduce a variant of entropy - focal entropy, which mitigates the potential\ninformation leakage of the existing entropy-based approaches. We showcase\nfeasibility on multiple benchmarks. The results suggest high target utility at\nmoderate privacy leakage."
    },
    {
        "date": "2025-07",
        "title": "HASSLE: A Self-Supervised Learning Enhanced Hijacking Attack on Vertical Federated Learning",
        "author": "Weiyang He, and Chip-Hong Chang",
        "link": "http://arxiv.org/abs/2507.10162v1",
        "abstract": "Vertical Federated Learning (VFL) enables an orchestrating active party to\nperform a machine learning task by cooperating with passive parties that\nprovide additional task-related features for the same training data entities.\nWhile prior research has leveraged the privacy vulnerability of VFL to\ncompromise its integrity through a combination of label inference and backdoor\nattacks, their effectiveness is constrained by the low label inference\nprecision and suboptimal backdoor injection conditions. To facilitate a more\nrigorous security evaluation on VFL without these limitations, we propose\nHASSLE, a hijacking attack framework composed of a gradient-direction-based\nlabel inference module and an adversarial embedding generation algorithm\nenhanced by self-supervised learning. HASSLE accurately identifies private\nsamples associated with a targeted label using only a single known instance of\nthat label. In the two-party scenario, it demonstrates strong performance with\nan attack success rate (ASR) of over 99% across four datasets, including both\nimage and tabular modalities, and achieves 85% ASR on the more complex\nCIFAR-100 dataset. Evaluation of HASSLE against 8 potential defenses further\nhighlights its significant threat while providing new insights into building a\ntrustworthy VFL system."
    },
    {
        "date": "2025-07",
        "title": "ARMOR: Aligning Secure and Safe Large Language Models via Meticulous Reasoning",
        "author": "Zhengyue Zhao, Yingzi Ma, Somesh Jha, Marco Pavone, and Chaowei Xiao",
        "link": "http://arxiv.org/abs/2507.11500v1",
        "abstract": "Large Language Models (LLMs) have demonstrated remarkable generative\ncapabilities. However, their susceptibility to misuse has raised significant\nsafety concerns. While post-training safety alignment methods have been widely\nadopted, LLMs remain vulnerable to malicious instructions that can bypass\nsafety constraints. Recent efforts have introduced inference-time safety\nreasoning (system-2 alignment), where LLMs conduct a reasoning process to\nperform safety verification before final response. We show, however, that these\nchecks are driven by ad-hoc reasoning that diverges from the structured human\nprocess, where they first discern a user's true intent, then evaluate the\nassociated risk based on the true intent. Consequently, these defenses remain\nvulnerable to sophisticated jailbreak prompts that cloak harmful goals in\nseemingly benign language. To build secure and safe LLMs, we propose a\nreasoning-based safety alignment framework, ARMOR, that replaces the ad-hoc\nchains of thought reasoning process with human-aligned, structured one. At\ninference, ARMOR (1) detects likely jailbreak strategies, (2) extracts the\nuser's core intent while discarding deceptive instructions, and (3) applies a\npolicy-grounded safety analysis to the purified request. ARMOR is evaluated on\nadaptive jailbreak attacks and multiple safety benchmarks, and a test-time\nscaling is conducted to further improve its performance. Results demonstrate\nthat ARMOR significantly enhances the robustness against state-of-the-art\nadaptive jailbreak attacks and outperforms recent reasoning-based aligned\nmodels across various safety benchmarks."
    },
    {
        "date": "2025-07",
        "title": "On the Efficiency of Training Robust Decision Trees",
        "author": "Benedict Gerlach, Marie Anastacio, and Holger H. Hoos",
        "link": "http://arxiv.org/abs/2507.10048v1",
        "abstract": "As machine learning gets adopted into the industry quickly, trustworthiness\nis increasingly in focus. Yet, efficiency and sustainability of robust training\npipelines still have to be established. In this work, we consider a simple\npipeline for training adversarially robust decision trees and investigate the\nefficiency of each step. Our pipeline consists of three stages. Firstly, we\nchoose the perturbation size automatically for each dataset. For that, we\nintroduce a simple algorithm, instead of relying on intuition or prior work.\nMoreover, we show that the perturbation size can be estimated from smaller\nmodels than the one intended for full training, and thus significant gains in\nefficiency can be achieved. Secondly, we train state-of-the-art adversarial\ntraining methods and evaluate them regarding both their training time and\nadversarial accuracy. Thirdly, we certify the robustness of each of the models\nthus obtained and investigate the time required for this. We find that\nverification time, which is critical to the efficiency of the full pipeline, is\nnot correlated with training time."
    },
    {
        "date": "2025-07",
        "title": "SQLord: A Robust Enterprise Text-to-SQL Solution via Reverse Data Generation and Workflow Decomposition",
        "author": "Song Cheng, Qiannan Cheng, Linbo Jin, Lei Yi, and Guannan Zhang",
        "link": "http://arxiv.org/abs/2507.10629v1",
        "abstract": "Transforming natural language into SQL queries (NL2SQL) is crucial for\ndata-driven business applications. Existing frameworks, trained on open-source\ndatasets, struggle with complex business logic and lack domain-specific data\nfor fine-tuning. Additionally, evaluation methods often require annotated data\nand executable database environments, which are scarce in real-world scenarios.\nTo address these challenges, we propose SQLord, an enterprise-level NL2SQL\nframework. First, SQLord introduces a data reverse generation approach to\nconvert raw SQL statements into annotated data for supervised fine-tuning\n(SFT). Second, it proposes a decomposition method for complex queries using an\nautomated workflow generator. Additionally, SQLord features a comprehensive\nGPT-Judge evaluation framework, including Execution Evaluation (EXE), Query-SQL\nEvaluation (QSE), and SQL-SQL Evaluation (SSE), tailored to diverse scenarios.\nOffline tests significantly outperform state of the art baselines, and online\naccuracy consistently exceeds 90, highlighting SQLord's advantages and\neffectiveness in complex real world scenarios. SQLord has been successfully\napplied across multiple scenarios on the world's largest B2B e-commerce\nplatform."
    },
    {
        "date": "2025-07",
        "title": "3DGAA: Realistic and Robust 3D Gaussian-based Adversarial Attack for Autonomous Driving",
        "author": "Yixun Zhang, Lizhi Wang, Junjun Zhao, Wending Zhao, Feng Zhou, Yonghao Dang, and Jianqin Yin",
        "link": "http://arxiv.org/abs/2507.09993v2",
        "abstract": "Camera-based object detection systems play a vital role in autonomous\ndriving, yet they remain vulnerable to adversarial threats in real-world\nenvironments. Existing 2D and 3D physical attacks, due to their focus on\ntexture optimization, often struggle to balance physical realism and attack\nrobustness. In this work, we propose 3D Gaussian-based Adversarial Attack\n(3DGAA), a novel adversarial object generation framework that leverages the\nfull 14-dimensional parameterization of 3D Gaussian Splatting (3DGS) to jointly\noptimize geometry and appearance in physically realizable ways. Unlike prior\nworks that rely on patches or texture optimization, 3DGAA jointly perturbs both\ngeometric attributes (shape, scale, rotation) and appearance attributes (color,\nopacity) to produce physically realistic and transferable adversarial objects.\nWe further introduce a physical filtering module that filters outliers to\npreserve geometric fidelity, and a physical augmentation module that simulates\ncomplex physical scenarios to enhance attack generalization under real-world\nconditions. We evaluate 3DGAA on both virtual benchmarks and physical-world\nsetups using miniature vehicle models. Experimental results show that 3DGAA\nachieves to reduce the detection mAP from 87.21\\% to 7.38\\%, significantly\noutperforming existing 3D physical attacks. Moreover, our method maintains high\ntransferability across different physical conditions, demonstrating a new\nstate-of-the-art in physically realizable adversarial attacks."
    },
    {
        "date": "2025-07",
        "title": "Counterfactual Visual Explanation via Causally-Guided Adversarial Steering",
        "author": "Yiran Qiao, Disheng Liu, Yiren Lu, Yu Yin, Mengnan Du, and Jing Ma",
        "link": "http://arxiv.org/abs/2507.09881v1",
        "abstract": "Recent work on counterfactual visual explanations has contributed to making\nartificial intelligence models more explainable by providing visual\nperturbation to flip the prediction. However, these approaches neglect the\ncausal relationships and the spurious correlations behind the image generation\nprocess, which often leads to unintended alterations in the counterfactual\nimages and renders the explanations with limited quality. To address this\nchallenge, we introduce a novel framework CECAS, which first leverages a\ncausally-guided adversarial method to generate counterfactual explanations. It\ninnovatively integrates a causal perspective to avoid unwanted perturbations on\nspurious factors in the counterfactuals. Extensive experiments demonstrate that\nour method outperforms existing state-of-the-art approaches across multiple\nbenchmark datasets and ultimately achieves a balanced trade-off among various\naspects of validity, sparsity, proximity, and realism."
    },
    {
        "date": "2025-07",
        "title": "Secure and Efficient UAV-Based Face Detection via Homomorphic Encryption and Edge Computing",
        "author": "Nguyen Van Duc, Bui Duc Manh, Quang-Trung Luu, Dinh Thai Hoang, Van-Linh Nguyen, and Diep N. Nguyen",
        "link": "http://arxiv.org/abs/2507.09860v1",
        "abstract": "This paper aims to propose a novel machine learning (ML) approach\nincorporating Homomorphic Encryption (HE) to address privacy limitations in\nUnmanned Aerial Vehicles (UAV)-based face detection. Due to challenges related\nto distance, altitude, and face orientation, high-resolution imagery and\nsophisticated neural networks enable accurate face recognition in dynamic\nenvironments. However, privacy concerns arise from the extensive surveillance\ncapabilities of UAVs. To resolve this issue, we propose a novel framework that\nintegrates HE with advanced neural networks to secure facial data throughout\nthe inference phase. This method ensures that facial data remains secure with\nminimal impact on detection accuracy. Specifically, the proposed system\nleverages the Cheon-Kim-Kim-Song (CKKS) scheme to perform computations directly\non encrypted data, optimizing computational efficiency and security.\nFurthermore, we develop an effective data encoding method specifically designed\nto preprocess the raw facial data into CKKS form in a\nSingle-Instruction-Multiple-Data (SIMD) manner. Building on this, we design a\nsecure inference algorithm to compute on ciphertext without needing decryption.\nThis approach not only protects data privacy during the processing of facial\ndata but also enhances the efficiency of UAV-based face detection systems.\nExperimental results demonstrate that our method effectively balances privacy\nprotection and detection performance, making it a viable solution for UAV-based\nsecure face detection. Significantly, our approach (while maintaining data\nconfidentially with HE encryption) can still achieve an accuracy of less than\n1% compared to the benchmark without using encryption."
    },
    {
        "date": "2025-07",
        "title": "AdvGrasp: Adversarial Attacks on Robotic Grasping from a Physical Perspective",
        "author": "Xiaofei Wang, Mingliang Han, Tianyu Hao, Cegang Li, Yunbo Zhao, and Keke Tang",
        "link": "http://arxiv.org/abs/2507.09857v1",
        "abstract": "Adversarial attacks on robotic grasping provide valuable insights into\nevaluating and improving the robustness of these systems. Unlike studies that\nfocus solely on neural network predictions while overlooking the physical\nprinciples of grasping, this paper introduces AdvGrasp, a framework for\nadversarial attacks on robotic grasping from a physical perspective.\nSpecifically, AdvGrasp targets two core aspects: lift capability, which\nevaluates the ability to lift objects against gravity, and grasp stability,\nwhich assesses resistance to external disturbances. By deforming the object's\nshape to increase gravitational torque and reduce stability margin in the\nwrench space, our method systematically degrades these two key grasping\nmetrics, generating adversarial objects that compromise grasp performance.\nExtensive experiments across diverse scenarios validate the effectiveness of\nAdvGrasp, while real-world validations demonstrate its robustness and practical\napplicability"
    },
    {
        "date": "2025-07",
        "title": "Spectral Feature Extraction for Robust Network Intrusion Detection Using MFCCs",
        "author": "HyeYoung Lee, Muhammad Nadeem, and Pavel Tsoi",
        "link": "http://arxiv.org/abs/2507.10622v1",
        "abstract": "The rapid expansion of Internet of Things (IoT) networks has led to a surge\nin security vulnerabilities, emphasizing the critical need for robust anomaly\ndetection and classification techniques. In this work, we propose a novel\napproach for identifying anomalies in IoT network traffic by leveraging the\nMel-frequency cepstral coefficients (MFCC) and ResNet-18, a deep learning model\nknown for its effectiveness in feature extraction and image-based tasks.\nLearnable MFCCs enable adaptive spectral feature representation, capturing the\ntemporal patterns inherent in network traffic more effectively than traditional\nfixed MFCCs. We demonstrate that transforming raw signals into MFCCs maps the\ndata into a higher-dimensional space, enhancing class separability and enabling\nmore effective multiclass classification. Our approach combines the strengths\nof MFCCs with the robust feature extraction capabilities of ResNet-18, offering\na powerful framework for anomaly detection. The proposed model is evaluated on\nthree widely used IoT intrusion detection datasets: CICIoT2023, NSL-KDD, and\nIoTID20. The experimental results highlight the potential of integrating\nadaptive signal processing techniques with deep learning architectures to\nachieve robust and scalable anomaly detection in heterogeneous IoT network\nlandscapes."
    },
    {
        "date": "2025-07",
        "title": "EventHunter: Dynamic Clustering and Ranking of Security Events from Hacker Forum Discussions",
        "author": "Yasir Ech-Chammakhy, Anas Motii, Anass Rabii, and Jaafar Chbili",
        "link": "http://arxiv.org/abs/2507.09762v1",
        "abstract": "Hacker forums provide critical early warning signals for emerging\ncybersecurity threats, but extracting actionable intelligence from their\nunstructured and noisy content remains a significant challenge. This paper\npresents an unsupervised framework that automatically detects, clusters, and\nprioritizes security events discussed across hacker forum posts. Our approach\nleverages Transformer-based embeddings fine-tuned with contrastive learning to\ngroup related discussions into distinct security event clusters, identifying\nincidents like zero-day disclosures or malware releases without relying on\npredefined keywords. The framework incorporates a daily ranking mechanism that\nprioritizes identified events using quantifiable metrics reflecting timeliness,\nsource credibility, information completeness, and relevance. Experimental\nevaluation on real-world hacker forum data demonstrates that our method\neffectively reduces noise and surfaces high-priority threats, enabling security\nanalysts to mount proactive responses. By transforming disparate hacker forum\ndiscussions into structured, actionable intelligence, our work addresses\nfundamental challenges in automated threat detection and analysis."
    },
    {
        "date": "2025-07",
        "title": "AI-Enhanced Pediatric Pneumonia Detection: A CNN-Based Approach Using Data Augmentation and Generative Adversarial Networks (GANs)",
        "author": "Abdul Manaf, and Nimra Mughal",
        "link": "http://arxiv.org/abs/2507.09759v1",
        "abstract": "Pneumonia is a leading cause of mortality in children under five, requiring\naccurate chest X-ray diagnosis. This study presents a machine learning-based\nPediatric Chest Pneumonia Classification System to assist healthcare\nprofessionals in diagnosing pneumonia from chest X-ray images. The CNN-based\nmodel was trained on 5,863 labeled chest X-ray images from children aged 0-5\nyears from the Guangzhou Women and Children's Medical Center. To address\nlimited data, we applied augmentation techniques (rotation, zooming, shear,\nhorizontal flipping) and employed GANs to generate synthetic images, addressing\nclass imbalance. The system achieved optimal performance using combined\noriginal, augmented, and GAN-generated data, evaluated through accuracy and F1\nscore metrics. The final model was deployed via a Flask web application,\nenabling real-time classification with probability estimates. Results\ndemonstrate the potential of deep learning and GANs in improving diagnostic\naccuracy and efficiency for pediatric pneumonia classification, particularly\nvaluable in resource-limited clinical settings\nhttps://github.com/AbdulManaf12/Pediatric-Chest-Pneumonia-Classification"
    },
    {
        "date": "2025-07",
        "title": "Pre-trained Under Noise: A Framework for Robust Bone Fracture Detection in Medical Imaging",
        "author": "Robby Hoover, Nelly Elsayed, Zag ElSayed, and Chengcheng Li",
        "link": "http://arxiv.org/abs/2507.09731v1",
        "abstract": "Medical Imagings are considered one of the crucial diagnostic tools for\ndifferent bones-related diseases, especially bones fractures. This paper\ninvestigates the robustness of pre-trained deep learning models for classifying\nbone fractures in X-ray images and seeks to address global healthcare disparity\nthrough the lens of technology. Three deep learning models have been tested\nunder varying simulated equipment quality conditions. ResNet50, VGG16 and\nEfficientNetv2 are the three pre-trained architectures which are compared.\nThese models were used to perform bone fracture classification as images were\nprogressively degraded using noise. This paper specifically empirically studies\nhow the noise can affect the bone fractures detection and how the pre-trained\nmodels performance can be changes due to the noise that affect the quality of\nthe X-ray images. This paper aims to help replicate real world challenges\nexperienced by medical imaging technicians across the world. Thus, this paper\nestablishes a methodological framework for assessing AI model degradation using\ntransfer learning and controlled noise augmentation. The findings provide\npractical insight into how robust and generalizable different pre-trained deep\nlearning powered computer vision models can be when used in different contexts."
    },
    {
        "date": "2025-07",
        "title": "Post-Training Quantization of Generative and Discriminative LSTM Text Classifiers: A Study of Calibration, Class Balance, and Robustness",
        "author": "Md Mushfiqur Rahaman, Elliot Chang, Tasmiah Haque, and Srinjoy Das",
        "link": "http://arxiv.org/abs/2507.09687v1",
        "abstract": "Text classification plays a pivotal role in edge computing applications like\nindustrial monitoring, health diagnostics, and smart assistants, where low\nlatency and high accuracy are both key requirements. Generative classifiers, in\nparticular, have been shown to exhibit robustness to out-of-distribution and\nnoisy data, which is an extremely critical consideration for deployment in such\nreal-time edge environments. However, deploying such models on edge devices\nfaces computational and memory constraints. Post Training Quantization (PTQ)\nreduces model size and compute costs without retraining, making it ideal for\nedge deployment. In this work, we present a comprehensive comparative study of\ngenerative and discriminative Long Short Term Memory (LSTM)-based text\nclassification models with PTQ using the Brevitas quantization library. We\nevaluate both types of classifier models across multiple bitwidths and assess\ntheir robustness under regular and noisy input conditions. We find that while\ndiscriminative classifiers remain robust, generative ones are more sensitive to\nbitwidth, calibration data used during PTQ, and input noise during quantized\ninference. We study the influence of class imbalance in calibration data for\nboth types of classifiers, comparing scenarios with evenly and unevenly\ndistributed class samples including their effect on weight adjustments and\nactivation profiles during PTQ. Using test statistics derived from\nnonparametric hypothesis testing, we identify that using class imbalanced data\nduring calibration introduces insufficient weight adaptation at lower bitwidths\nfor generative LSTM classifiers, thereby leading to degraded performance. This\nstudy underscores the role of calibration data in PTQ and when generative\nclassifiers succeed or fail under noise, aiding deployment in edge\nenvironments."
    },
    {
        "date": "2025-07",
        "title": "CAN-Trace Attack: Exploit CAN Messages to Uncover Driving Trajectories",
        "author": "Xiaojie Lin, Baihe Ma, Xu Wang, Guangsheng Yu, Ying He, Wei Ni, and Ren Ping Liu",
        "link": "http://arxiv.org/abs/2507.09624v1",
        "abstract": "Driving trajectory data remains vulnerable to privacy breaches despite\nexisting mitigation measures. Traditional methods for detecting driving\ntrajectories typically rely on map-matching the path using Global Positioning\nSystem (GPS) data, which is susceptible to GPS data outage. This paper\nintroduces CAN-Trace, a novel privacy attack mechanism that leverages\nController Area Network (CAN) messages to uncover driving trajectories, posing\na significant risk to drivers' long-term privacy. A new trajectory\nreconstruction algorithm is proposed to transform the CAN messages,\nspecifically vehicle speed and accelerator pedal position, into weighted graphs\naccommodating various driving statuses. CAN-Trace identifies driving\ntrajectories using graph-matching algorithms applied to the created graphs in\ncomparison to road networks. We also design a new metric to evaluate matched\ncandidates, which allows for potential data gaps and matching inaccuracies.\nEmpirical validation under various real-world conditions, encompassing\ndifferent vehicles and driving regions, demonstrates the efficacy of CAN-Trace:\nit achieves an attack success rate of up to 90.59% in the urban region, and\n99.41% in the suburban region."
    },
    {
        "date": "2025-07",
        "title": "Efficient Private Inference Based on Helper-Assisted Malicious Security Dishonest Majority MPC",
        "author": "Kaiwen Wang, Yuehan Dong, Junchao Fan, and Xiaolin Chang",
        "link": "http://arxiv.org/abs/2507.09607v2",
        "abstract": "Private inference based on Secure Multi-Party Computation (MPC) addresses\ndata privacy risks in Machine Learning as a Service (MLaaS). However, existing\nMPC-based private inference frameworks focuses on semi-honest or honest\nmajority models, whose threat models are overly idealistic, while malicious\nsecurity dishonest majority models face the challenge of low efficiency. To\nbalance security and efficiency, we propose a private inference framework using\nHelper-Assisted Malicious Security Dishonest Majority Model (HA-MSDM). This\nframework includes our designed five MPC protocols and a co-optimized strategy.\nThese protocols achieve efficient fixed-round multiplication, exponentiation,\nand polynomial operations, providing foundational primitives for private\ninference. The co-optimized strategy balances inference efficiency and\naccuracy. To enhance efficiency, we employ polynomial approximation for\nnonlinear layers. For improved accuracy, we construct sixth-order polynomial\napproximation within a fixed interval to achieve high-precision activation\nfunction fitting and introduce parameter-adjusted batch normalization layers to\nconstrain the activation escape problem. Benchmark results on LeNet and AlexNet\nshow our framework achieves 2.4-25.7x speedup in LAN and 1.3-9.5x acceleration\nin WAN compared to state-of-the-art frameworks (IEEE S&P'25), maintaining high\naccuracy with only 0.04%-1.08% relative errors."
    },
    {
        "date": "2025-07",
        "title": "DRAGD: A Federated Unlearning Data Reconstruction Attack Based on Gradient Differences",
        "author": "Bocheng Ju, Junchao Fan, Jiaqi Liu, and Xiaolin Chang",
        "link": "http://arxiv.org/abs/2507.09602v1",
        "abstract": "Federated learning enables collaborative machine learning while preserving\ndata privacy. However, the rise of federated unlearning, designed to allow\nclients to erase their data from the global model, introduces new privacy\nconcerns. Specifically, the gradient exchanges during the unlearning process\ncan leak sensitive information about deleted data. In this paper, we introduce\nDRAGD, a novel attack that exploits gradient discrepancies before and after\nunlearning to reconstruct forgotten data. We also present DRAGDP, an enhanced\nversion of DRAGD that leverages publicly available prior data to improve\nreconstruction accuracy, particularly for complex datasets like facial images.\nExtensive experiments across multiple datasets demonstrate that DRAGD and\nDRAGDP significantly outperform existing methods in data reconstruction.Our\nwork highlights a critical privacy vulnerability in federated unlearning and\noffers a practical solution, advancing the security of federated unlearning\nsystems in real-world applications."
    },
    {
        "date": "2025-07",
        "title": "eSapiens: A Platform for Secure and Auditable Retrieval-Augmented Generation",
        "author": "Isaac Shi, Zeyuan Li, Fan Liu, Wenli Wang, Lewei He, Yang Yang, and Tianyu Shi",
        "link": "http://arxiv.org/abs/2507.09588v1",
        "abstract": "We present eSapiens, an AI-as-a-Service (AIaaS) platform engineered around a\nbusiness-oriented trifecta: proprietary data, operational workflows, and any\nmajor agnostic Large Language Model (LLM). eSapiens gives businesses full\ncontrol over their AI assets, keeping everything in-house for AI knowledge\nretention and data security. eSapiens AI Agents (Sapiens) empower your team by\nproviding valuable insights and automating repetitive tasks, enabling them to\nfocus on high-impact work and drive better business outcomes.\n  The system integrates structured document ingestion, hybrid vector retrieval,\nand no-code orchestration via LangChain, and supports top LLMs including\nOpenAI, Claude, Gemini, and DeepSeek. A key component is the THOR Agent, which\nhandles structured SQL-style queries and generates actionable insights over\nenterprise databases.\n  To evaluate the system, we conduct two experiments. First, a retrieval\nbenchmark on legal corpora reveals that a chunk size of 512 tokens yields the\nhighest retrieval precision (Top-3 accuracy: 91.3%). Second, a generation\nquality test using TRACe metrics across five LLMs shows that eSapiens delivers\nmore context-consistent outputs with up to a 23% improvement in factual\nalignment.\n  These results demonstrate the effectiveness of eSapiens in enabling\ntrustworthy, auditable AI workflows for high-stakes domains like legal and\nfinance."
    },
    {
        "date": "2025-07",
        "title": "A Login Page Transparency and Visual Similarity Based Zero Day Phishing Defense Protocol",
        "author": "Gaurav Varshney, Akanksha Raj, Divya Sangwan, Sharif Abuadbba, Rina Mishra, and Yansong Gao",
        "link": "http://arxiv.org/abs/2507.09564v1",
        "abstract": "Phishing is a prevalent cyberattack that uses look-alike websites to deceive\nusers into revealing sensitive information. Numerous efforts have been made by\nthe Internet community and security organizations to detect, prevent, or train\nusers to avoid falling victim to phishing attacks. Most of this research over\nthe years has been highly diverse and application-oriented, often serving as\nstandalone solutions for HTTP clients, servers, or third parties. However,\nlimited work has been done to develop a comprehensive or proactive\nprotocol-oriented solution to effectively counter phishing attacks. Inspired by\nthe concept of certificate transparency, which allows certificates issued by\nCertificate Authorities (CAs) to be publicly verified by clients, thereby\nenhancing transparency, we propose a concept called Page Transparency (PT) for\nthe web. The proposed PT requires login pages that capture users' sensitive\ninformation to be publicly logged via PLS and made available to web clients for\nverification. The pages are verified to be logged using cryptographic proofs.\nSince all pages are logged on a PLS and visually compared with existing pages\nthrough a comprehensive visual page-matching algorithm, it becomes impossible\nfor an attacker to register a deceptive look-alike page on the PLS and receive\nthe cryptographic proof required for client verification. All implementations\noccur on the client side, facilitated by the introduction of a new HTTP PT\nheader, eliminating the need for platform-specific changes or the installation\nof third-party solutions for phishing prevention."
    },
    {
        "date": "2025-07",
        "title": "DRPCA-Net: Make Robust PCA Great Again for Infrared Small Target Detection",
        "author": "Zihao Xiong, Fei Zhou, Fengyi Wu, Shuai Yuan, Maixia Fu, Zhenming Peng, Jian Yang, and Yimian Dai",
        "link": "http://arxiv.org/abs/2507.09541v1",
        "abstract": "Infrared small target detection plays a vital role in remote sensing,\nindustrial monitoring, and various civilian applications. Despite recent\nprogress powered by deep learning, many end-to-end convolutional models tend to\npursue performance by stacking increasingly complex architectures, often at the\nexpense of interpretability, parameter efficiency, and generalization. These\nmodels typically overlook the intrinsic sparsity prior of infrared small\ntargets--an essential cue that can be explicitly modeled for both performance\nand efficiency gains. To address this, we revisit the model-based paradigm of\nRobust Principal Component Analysis (RPCA) and propose Dynamic RPCA Network\n(DRPCA-Net), a novel deep unfolding network that integrates the sparsity-aware\nprior into a learnable architecture. Unlike conventional deep unfolding methods\nthat rely on static, globally learned parameters, DRPCA-Net introduces a\ndynamic unfolding mechanism via a lightweight hypernetwork. This design enables\nthe model to adaptively generate iteration-wise parameters conditioned on the\ninput scene, thereby enhancing its robustness and generalization across diverse\nbackgrounds. Furthermore, we design a Dynamic Residual Group (DRG) module to\nbetter capture contextual variations within the background, leading to more\naccurate low-rank estimation and improved separation of small targets.\nExtensive experiments on multiple public infrared datasets demonstrate that\nDRPCA-Net significantly outperforms existing state-of-the-art methods in\ndetection accuracy. Code is available at https://github.com/GrokCV/DRPCA-Net."
    },
    {
        "date": "2025-07",
        "title": "LaSM: Layer-wise Scaling Mechanism for Defending Pop-up Attack on GUI Agents",
        "author": "Zihe Yan, and Zhuosheng Zhang",
        "link": "http://arxiv.org/abs/2507.10610v1",
        "abstract": "Graphical user interface (GUI) agents built on multimodal large language\nmodels (MLLMs) have recently demonstrated strong decision-making abilities in\nscreen-based interaction tasks. However, they remain highly vulnerable to\npop-up-based environmental injection attacks, where malicious visual elements\ndivert model attention and lead to unsafe or incorrect actions. Existing\ndefense methods either require costly retraining or perform poorly under\ninductive interference. In this work, we systematically study how such attacks\nalter the attention behavior of GUI agents and uncover a layer-wise attention\ndivergence pattern between correct and incorrect outputs. Based on this\ninsight, we propose \\textbf{LaSM}, a \\textit{Layer-wise Scaling Mechanism} that\nselectively amplifies attention and MLP modules in critical layers. LaSM\nimproves the alignment between model saliency and task-relevant regions without\nadditional training. Extensive experiments across 12 types of pop-up\nperturbations and 4 different model backbones show that LaSM consistently\nenhances the defense success rate. When combined with prompt-level alerts, LaSM\nachieves over 98\\% robustness even under strong inductive attacks. Our findings\nreveal that attention misalignment is a core vulnerability in MLLM agents and\ncan be effectively addressed through selective layer-wise modulation."
    },
    {
        "date": "2025-07",
        "title": "Adversarial Demonstration Learning for Low-resource NER Using Dual Similarity",
        "author": "Guowen Yuan, Tien-Hsuan Wu, Lianghao Xia, and Ben Kao",
        "link": "http://arxiv.org/abs/2507.15864v1",
        "abstract": "We study the problem of named entity recognition (NER) based on demonstration\nlearning in low-resource scenarios. We identify two issues in demonstration\nconstruction and model training. Firstly, existing methods for selecting\ndemonstration examples primarily rely on semantic similarity; We show that\nfeature similarity can provide significant performance improvement. Secondly,\nwe show that the NER tagger's ability to reference demonstration examples is\ngenerally inadequate. We propose a demonstration and training approach that\neffectively addresses these issues. For the first issue, we propose to select\nexamples by dual similarity, which comprises both semantic similarity and\nfeature similarity. For the second issue, we propose to train an NER model with\nadversarial demonstration such that the model is forced to refer to the\ndemonstrations when performing the tagging task. We conduct comprehensive\nexperiments in low-resource NER tasks, and the results demonstrate that our\nmethod outperforms a range of methods."
    },
    {
        "date": "2025-07",
        "title": "A Mixture of Linear Corrections Generates Secure Code",
        "author": "Weichen Yu, Ravi Mangal, Terry Zhuo, Matt Fredrikson, and Corina S. Pasareanu",
        "link": "http://arxiv.org/abs/2507.09508v1",
        "abstract": "Large language models (LLMs) have become proficient at sophisticated\ncode-generation tasks, yet remain ineffective at reliably detecting or avoiding\ncode vulnerabilities. Does this deficiency stem from insufficient learning\nabout code vulnerabilities, or is it merely a result of ineffective prompting?\nUsing representation engineering techniques, we investigate whether LLMs\ninternally encode the concepts necessary to identify code vulnerabilities. We\nfind that current LLMs encode precise internal representations that distinguish\nvulnerable from secure code--achieving greater accuracy than standard prompting\napproaches. Leveraging these vulnerability-sensitive representations, we\ndevelop an inference-time steering technique that subtly modulates the model's\ntoken-generation probabilities through a mixture of corrections (MoC). Our\nmethod effectively guides LLMs to produce less vulnerable code without\ncompromising functionality, demonstrating a practical approach to controlled\nvulnerability management in generated code. Notably, MoC enhances the security\nratio of Qwen2.5-Coder-7B by 8.9\\%, while simultaneously improving\nfunctionality on HumanEval pass@1 by 2.1\\%."
    },
    {
        "date": "2025-07",
        "title": "Demo: Secure Edge Server for Network Slicing and Resource Allocation in Open RAN",
        "author": "Adhwaa Alchaab, Ayman Younis, and Dario Pompili",
        "link": "http://arxiv.org/abs/2507.11499v1",
        "abstract": "Next-Generation Radio Access Networks (NGRAN) aim to support diverse vertical\napplications with strict security, latency, and Service-Level Agreement (SLA)\nrequirements. These demands introduce challenges in securing the\ninfrastructure, allocating resources dynamically, and enabling real-time\nreconfiguration. This demo presents SnSRIC, a secure and intelligent network\nslicing framework that mitigates a range of Distributed Denial-of-Service\n(DDoS) attacks in Open RAN environments. SnSRIC incorporates an AI-driven xApp\nthat dynamically allocates Physical Resource Blocks (PRBs) to active users\nwhile enforcing slice-level security. The system detects anomalous behavior,\ndistinguishes between benign and malicious devices, and uses the E2 interface\nto throttle rogue signaling while maintaining service continuity for legitimate\nusers."
    },
    {
        "date": "2025-07",
        "title": "CKAA: Cross-subspace Knowledge Alignment and Aggregation for Robust Continual Learning",
        "author": "Lingfeng He, De Cheng, Zhiheng Ma, Huaijie Wang, Dingwen Zhang, Nannan Wang, and Xinbo Gao",
        "link": "http://arxiv.org/abs/2507.09471v1",
        "abstract": "Continual Learning (CL) empowers AI models to continuously learn from\nsequential task streams. Recently, parameter-efficient fine-tuning (PEFT)-based\nCL methods have garnered increasing attention due to their superior\nperformance. They typically allocate a unique sub-module for learning each\ntask, with a task recognizer to select the appropriate sub-modules for testing\nimages. However, due to the feature subspace misalignment from independently\ntrained sub-modules, these methods tend to produce ambiguous decisions under\nmisleading task-ids. To address this, we propose Cross-subspace Knowledge\nAlignment and Aggregation (CKAA), a novel framework that enhances model\nrobustness against misleading task-ids through two key innovations: (1)\nDual-level Knowledge Alignment (DKA): By aligning intra-class feature\ndistributions across different subspaces and learning a robust global\nclassifier through a feature simulation process, DKA enables the model to\ndistinguish features from both correct and incorrect subspaces during training.\n(2) Task-Confidence-guided Mixture of Adapters (TC-MoA): A robust inference\nscheme that adaptively aggregates task-specific knowledge from relevant\nsub-modules based on task-confidence scores, avoiding overconfidence in\nmisleading task-id predictions. Extensive experiments demonstrate that CKAA\noutperforms existing PEFT-based CL methods."
    },
    {
        "date": "2025-07",
        "title": "Adversarial Activation Patching: A Framework for Detecting and Mitigating Emergent Deception in Safety-Aligned Transformers",
        "author": "Santhosh Kumar Ravindran",
        "link": "http://arxiv.org/abs/2507.09406v1",
        "abstract": "Large language models (LLMs) aligned for safety through techniques like\nreinforcement learning from human feedback (RLHF) often exhibit emergent\ndeceptive behaviors, where outputs appear compliant but subtly mislead or omit\ncritical information. This paper introduces adversarial activation patching, a\nnovel mechanistic interpretability framework that leverages activation patching\nas an adversarial tool to induce, detect, and mitigate such deception in\ntransformer-based models. By sourcing activations from \"deceptive\" prompts and\npatching them into safe forward passes at specific layers, we simulate\nvulnerabilities and quantify deception rates. Through toy neural network\nsimulations across multiple scenarios (e.g., 1000 trials per setup), we\ndemonstrate that adversarial patching increases deceptive outputs to 23.9% from\na 0% baseline, with layer-specific variations supporting our hypotheses. We\npropose six hypotheses, including transferability across models, exacerbation\nin multimodal settings, and scaling effects. An expanded literature review\nsynthesizes over 20 key works in interpretability, deception, and adversarial\nattacks. Mitigation strategies, such as activation anomaly detection and robust\nfine-tuning, are detailed, alongside ethical considerations and future research\ndirections. This work advances AI safety by highlighting patching's dual-use\npotential and provides a roadmap for empirical studies on large-scale models."
    },
    {
        "date": "2025-07",
        "title": "When Developer Aid Becomes Security Debt: A Systematic Analysis of Insecure Behaviors in LLM Coding Agents",
        "author": "Matous Kozak, Roshanak Zilouchian Moghaddam, and Siva Sivaraman",
        "link": "http://arxiv.org/abs/2507.09329v1",
        "abstract": "LLM-based coding agents are rapidly being deployed in software development,\nyet their security implications remain poorly understood. These agents, while\ncapable of accelerating software development, may inadvertently introduce\ninsecure practices. We conducted the first systematic security evaluation of\nautonomous coding agents, analyzing over 12,000 actions across five\nstate-of-the-art models (GPT-4o, GPT-4.1, Claude variants) on 93 real-world\nsoftware setup tasks. Our findings reveal significant security concerns: 21% of\nagent trajectories contained insecure actions, with models showing substantial\nvariation in security behavior. We developed a high-precision detection system\nthat identified four major vulnerability categories, with information exposure\n(CWE-200) being the most prevalent one. We also evaluated mitigation strategies\nincluding feedback mechanisms and security reminders with various effectiveness\nbetween models. GPT-4.1 demonstrated exceptional security awareness with 96.8%\nmitigation success. Our work provides the first comprehensive framework for\nevaluating coding agent security and highlights the need for security-aware\ndesign of next generation LLM-based coding agents."
    },
    {
        "date": "2025-07",
        "title": "Hybrid Quantum Security for IPsec",
        "author": "Javier Blanco-Romero, Pedro Otero Garc\u00eda, Daniel Sobral-Blanco, Florina Almenares Mendoza, Ana Fern\u00e1ndez Vilas, and Manuel Fern\u00e1ndez-Veiga",
        "link": "http://arxiv.org/abs/2507.09288v1",
        "abstract": "Quantum Key Distribution (QKD) offers information-theoretic security against\nquantum computing threats, but integrating QKD into existing security protocols\nremains an unsolved challenge due to fundamental mismatches between\npre-distributed quantum keys and computational key exchange paradigms. This\npaper presents the first systematic comparison of sequential versus parallel\nhybrid QKD-PQC key establishment strategies for IPsec, revealing fundamental\nprotocol design principles that extend beyond specific implementations. We\nintroduce two novel approaches for incorporating QKD into Internet Key Exchange\nversion 2 (IKEv2) with support for both ETSI GS QKD 004 stateful and ETSI GS\nQKD 014 stateless API specifications: (1) a pure QKD approach that replaces\ncomputational key derivation with identifier-based quantum key coordination,\nand (2) a unified QKD-KEM abstraction that enables parallel composition of\nquantum and post-quantum cryptographic methods within existing protocol\nframeworks. Our key insight is that parallel hybrid approaches eliminate the\nmultiplicative latency penalties inherent in sequential methods mandated by RFC\n9370, achieving significant performance improvements under realistic network\nconditions. Performance evaluation using a Docker-based testing framework with\nIDQuantique QKD hardware demonstrates that the parallel hybrid approach\nsignificantly outperforms sequential methods under network latency conditions,\nwhile pure QKD achieves minimal bandwidth overhead through identifier-based key\ncoordination. Our implementations provide practical quantum-enhanced IPsec\nsolutions suitable for critical infrastructure deployments requiring\ndefense-in-depth security."
    },
    {
        "date": "2025-07",
        "title": "Digital Twin-Assisted Explainable AI for Robust Beam Prediction in mmWave MIMO Systems",
        "author": "Nasir Khan, Asmaa Abdallah, Abdulkadir Celik, Ahmed M. Eltawil, and Sinem Coleri",
        "link": "http://arxiv.org/abs/2507.14180v1",
        "abstract": "In line with the AI-native 6G vision, explainability and robustness are\ncrucial for building trust and ensuring reliable performance in millimeter-wave\n(mmWave) systems. Efficient beam alignment is essential for initial access, but\ndeep learning (DL) solutions face challenges, including high data collection\noverhead, hardware constraints, lack of explainability, and susceptibility to\nadversarial attacks. This paper proposes a robust and explainable DL-based beam\nalignment engine (BAE) for mmWave multiple-input multiple output (MIMO)\nsystems. The BAE uses received signal strength indicator (RSSI) measurements\nfrom wide beams to predict the best narrow beam, reducing the overhead of\nexhaustive beam sweeping. To overcome the challenge of real-world data\ncollection, this work leverages a site-specific digital twin (DT) to generate\nsynthetic channel data closely resembling real-world environments. A model\nrefinement via transfer learning is proposed to fine-tune the pre-trained model\nresiding in the DT with minimal real-world data, effectively bridging\nmismatches between the digital replica and real-world environments. To reduce\nbeam training overhead and enhance transparency, the framework uses deep\nShapley additive explanations (SHAP) to rank input features by importance,\nprioritizing key spatial directions and minimizing beam sweeping. It also\nincorporates the Deep k-nearest neighbors (DkNN) algorithm, providing a\ncredibility metric for detecting out-of-distribution inputs and ensuring\nrobust, transparent decision-making. Experimental results show that the\nproposed framework reduces real-world data needs by 70%, beam training overhead\nby 62%, and improves outlier detection robustness by up to 8.5x, achieving\nnear-optimal spectral efficiency and transparent decision making compared to\ntraditional softmax based DL models."
    },
    {
        "date": "2025-07",
        "title": "Calibrated and Robust Foundation Models for Vision-Language and Medical Image Tasks Under Distribution Shift",
        "author": "Behraj Khan, Tahir Qasim Syed, Nouman M. Durrani, Bilal Naseem, Shabir Ahmad, and Rizwan Qureshi",
        "link": "http://arxiv.org/abs/2507.09222v2",
        "abstract": "Foundation models like CLIP and SAM have advanced computer vision and medical\nimaging via low-shot transfer learning, aiding CADD with limited data. However,\ntheir deployment faces two key challenges. \\textit{distribution shift} where\npre-training and post-training data distributions differ (e.g., due to\ninter-center image acquisition) and \\textit{confidence misalignment}, which\nleads to overconfident errors. These issues surface differently,\nvision-language models (e.g., CLIP) suffer from 2D embedding shift (image-text\nmisalignment), while medical models (e.g., SAM) encounter 3D domain shifts\n(e.g., scanner variation) and voxel-wise calibration need. Existing solutions\nare domain-specific. We propose \\textbf{StaRFM}, a fusion of Fisher information\npenalty (FIP) and confidence misalignment penalty (CMP) tackling both\nchallenges. It applies FIP, extended to 3D via patch-wise regularization, to\nreduce embedding shift, and CMP, reformulated for voxel-level predictions, to\ncalibrate segmentation uncertainty. We derive PAC-Bayes bounds. FIP controls\ngeneralization via the Fisher-Rao norm, and CMP reduces calibration error via\nBrier score minimization. StaRFM surpasses baselines by \\texttt{+}3.5\\%\naccuracy and 28\\% lower ECE on 19 vision datasets (e.g., ImageNet,\nOffice-Home), achieves +4.2\\% DSC over SAM-FT and 4.8mm HD95 on medical\nbenchmarks (e.g., BraTS, ATLAS), and reduces cross-domain gaps by up to 20\\%.\nThe framework is plug-and-play, requiring minimal architectural changes. Code\nand models are available at:\n\\href{https://anonymous.4open.science/r/StaRFM-C0CD/}{\\textcolor{blue}{\\underline{StaRFM}}}"
    },
    {
        "date": "2025-07",
        "title": "Investigating the Robustness of Extreme Precipitation Super-Resolution Across Climates",
        "author": "Louise Largeau, Erwan Koch, David Leutwyler, Gregoire Mariethoz, Valerie Chavez-Demoulin, and Tom Beucler",
        "link": "http://arxiv.org/abs/2507.09166v1",
        "abstract": "The coarse spatial resolution of gridded climate models, such as general\ncirculation models, limits their direct use in projecting socially relevant\nvariables like extreme precipitation. Most downscaling methods estimate the\nconditional distributions of extremes by generating large ensembles,\ncomplicating the assessment of robustness under distributional shifts, such as\nthose induced by climate change. To better understand and potentially improve\nrobustness, we propose super-resolving the parameters of the target variable's\nprobability distribution directly using analytically tractable mappings. Within\na perfect-model framework over Switzerland, we demonstrate that vector\ngeneralized linear and additive models can super-resolve the generalized\nextreme value distribution of summer hourly precipitation extremes from coarse\nprecipitation fields and topography. We introduce the notion of a \"robustness\ngap\", defined as the difference in predictive error between present-trained and\nfuture-trained models, and use it to diagnose how model structure affects the\ngeneralization of each quantile to a pseudo-global warming scenario. By\nevaluating multiple model configurations, we also identify an upper limit on\nthe super-resolution factor based on the spatial auto- and cross-correlation of\nprecipitation and elevation, beyond which coarse precipitation loses predictive\nvalue. Our framework is broadly applicable to variables governed by parametric\ndistributions and offers a model-agnostic diagnostic for understanding when and\nwhy empirical downscaling generalizes to climate change and extremes."
    },
    {
        "date": "2025-07",
        "title": "RoHOI: Robustness Benchmark for Human-Object Interaction Detection",
        "author": "Di Wen, Kunyu Peng, Kailun Yang, Yufan Chen, Ruiping Liu, Junwei Zheng, Alina Roitberg, and Rainer Stiefelhagen",
        "link": "http://arxiv.org/abs/2507.09111v1",
        "abstract": "Human-Object Interaction (HOI) detection is crucial for robot-human\nassistance, enabling context-aware support. However, models trained on clean\ndatasets degrade in real-world conditions due to unforeseen corruptions,\nleading to inaccurate prediction. To address this, we introduce the first\nrobustness benchmark for HOI detection, evaluating model resilience under\ndiverse challenges. Despite advances, current models struggle with\nenvironmental variability, occlusion, and noise. Our benchmark, RoHOI, includes\n20 corruption types based on HICO-DET and V-COCO datasets and a new\nrobustness-focused metric. We systematically analyze existing models in the\nrelated field, revealing significant performance drops under corruptions. To\nimprove robustness, we propose a Semantic-Aware Masking-based Progressive\nLearning (SAMPL) strategy to guide the model to be optimized based on holistic\nand partial cues, dynamically adjusting the model's optimization to enhance\nrobust feature learning. Extensive experiments show our approach outperforms\nstate-of-the-art methods, setting a new standard for robust HOI detection.\nBenchmarks, datasets, and code will be made publicly available at\nhttps://github.com/Kratos-Wen/RoHOI."
    },
    {
        "date": "2025-07",
        "title": "VIP: Visual Information Protection through Adversarial Attacks on Vision-Language Models",
        "author": "Hanene F. Z. Brachemi Meftah, Wassim Hamidouche, Sid Ahmed Fezza, and Olivier D\u00e9forges",
        "link": "http://arxiv.org/abs/2507.08982v1",
        "abstract": "Recent years have witnessed remarkable progress in developing Vision-Language\nModels (VLMs) capable of processing both textual and visual inputs. These\nmodels have demonstrated impressive performance, leading to their widespread\nadoption in various applications. However, this widespread raises serious\nconcerns regarding user privacy, particularly when models inadvertently process\nor expose private visual information. In this work, we frame the preservation\nof privacy in VLMs as an adversarial attack problem. We propose a novel attack\nstrategy that selectively conceals information within designated Region Of\nInterests (ROIs) in an image, effectively preventing VLMs from accessing\nsensitive content while preserving the semantic integrity of the remaining\nimage. Unlike conventional adversarial attacks that often disrupt the entire\nimage, our method maintains high coherence in unmasked areas. Experimental\nresults across three state-of-the-art VLMs namely LLaVA, Instruct-BLIP, and\nBLIP2-T5 demonstrate up to 98% reduction in detecting targeted ROIs, while\nmaintaining global image semantics intact, as confirmed by high similarity\nscores between clean and adversarial outputs. We believe that this work\ncontributes to a more privacy conscious use of multimodal models and offers a\npractical tool for further research, with the source code publicly available\nat: https://github.com/hbrachemi/Vlm_defense-attack."
    },
    {
        "date": "2025-07",
        "title": "Characterizing Security and Privacy Teaching Standards for Schools in the United States",
        "author": "Katherine Limes, Nathan Malkin, and Kelsey R. Fulton",
        "link": "http://arxiv.org/abs/2507.08978v1",
        "abstract": "Increasingly, students begin learning aspects of security and privacy during\ntheir primary and secondary education (grades K-12 in the United States).\nIndividual U.S. states and some national organizations publish teaching\nstandards -- guidance that outlines expectations for what students should learn\n-- which often form the basis for course curricula. However, research has not\nyet examined what is covered by these standards and whether the topics align\nwith what the broader security and privacy community thinks students should\nknow. To shed light on these questions, we started by collecting computer\nscience teaching standards from all U.S. states and eight national\norganizations. After manually examining a total of 11,954 standards, we labeled\n3,778 of them as being related to security and privacy, further classifying\nthese into 103 topics. Topics ranged from technical subjects like encryption,\nnetwork security, and embedded systems to social subjects such as laws, ethics,\nand appropriate online behavior. Subsequently, we interviewed 11 security and\nprivacy professionals to examine how the teaching standards align with their\nexpectations. We found that, while the specific topics they mentioned mostly\noverlapped with those of existing standards, professionals placed a greater\nemphasis on threat modeling and security mindset."
    },
    {
        "date": "2025-07",
        "title": "Adaptive Nonlinear Vector Autoregression: Robust Forecasting for Noisy Chaotic Time Series",
        "author": "Azimov Sherkhon, Susana Lopez-Moreno, Eric Dolores-Cuenca, Sieun Lee, and Sangil Kim",
        "link": "http://arxiv.org/abs/2507.08738v1",
        "abstract": "Nonlinear vector autoregression (NVAR) and reservoir computing (RC) have\nshown promise in forecasting chaotic dynamical systems, such as the Lorenz-63\nmodel and El Nino-Southern Oscillation. However, their reliance on fixed\nnonlinearities - polynomial expansions in NVAR or random feature maps in RC -\nlimits their adaptability to high noise or real-world data. These methods also\nscale poorly in high-dimensional settings due to costly matrix inversion during\nreadout computation. We propose an adaptive NVAR model that combines\ndelay-embedded linear inputs with features generated by a shallow, learnable\nmulti-layer perceptron (MLP). The MLP and linear readout are jointly trained\nusing gradient-based optimization, enabling the model to learn data-driven\nnonlinearities while preserving a simple readout structure. Unlike standard\nNVAR, our approach avoids the need for an exhaustive and sensitive grid search\nover ridge and delay parameters. Instead, tuning is restricted to neural\nnetwork hyperparameters, improving scalability. Initial experiments on chaotic\nsystems tested under noise-free and synthetically noisy conditions showed that\nthe adaptive model outperformed the standard NVAR in predictive accuracy and\nshowed robust forecasting under noisy conditions with a lower observation\nfrequency."
    },
    {
        "date": "2025-07",
        "title": "SPLASH! Sample-efficient Preference-based inverse reinforcement learning for Long-horizon Adversarial tasks from Suboptimal Hierarchical demonstrations",
        "author": "Peter Crowley, Zachary Serlin, Tyler Paine, Makai Mann, Michael Benjamin, and Calin Belta",
        "link": "http://arxiv.org/abs/2507.08707v1",
        "abstract": "Inverse Reinforcement Learning (IRL) presents a powerful paradigm for\nlearning complex robotic tasks from human demonstrations. However, most\napproaches make the assumption that expert demonstrations are available, which\nis often not the case. Those that allow for suboptimality in the demonstrations\nare not designed for long-horizon goals or adversarial tasks. Many desirable\nrobot capabilities fall into one or both of these categories, thus highlighting\na critical shortcoming in the ability of IRL to produce field-ready robotic\nagents. We introduce Sample-efficient Preference-based inverse reinforcement\nlearning for Long-horizon Adversarial tasks from Suboptimal Hierarchical\ndemonstrations (SPLASH), which advances the state-of-the-art in learning from\nsuboptimal demonstrations to long-horizon and adversarial settings. We\nempirically validate SPLASH on a maritime capture-the-flag task in simulation,\nand demonstrate real-world applicability with sim-to-real translation\nexperiments on autonomous unmanned surface vehicles. We show that our proposed\nmethods allow SPLASH to significantly outperform the state-of-the-art in reward\nlearning from suboptimal demonstrations."
    },
    {
        "date": "2025-07",
        "title": "Entangled Threats: A Unified Kill Chain Model for Quantum Machine Learning Security",
        "author": "Pascal Debus, Maximilian Wendlinger, Kilian Tscharke, Daniel Herr, Cedric Br\u00fcgmann, Daniel Ohl de Mello, Juris Ulmanis, Alexander Erhard, Arthur Schmidt, and Fabian Petsch",
        "link": "http://arxiv.org/abs/2507.08623v1",
        "abstract": "Quantum Machine Learning (QML) systems inherit vulnerabilities from classical\nmachine learning while introducing new attack surfaces rooted in the physical\nand algorithmic layers of quantum computing. Despite a growing body of research\non individual attack vectors - ranging from adversarial poisoning and evasion\nto circuit-level backdoors, side-channel leakage, and model extraction - these\nthreats are often analyzed in isolation, with unrealistic assumptions about\nattacker capabilities and system environments. This fragmentation hampers the\ndevelopment of effective, holistic defense strategies. In this work, we argue\nthat QML security requires more structured modeling of the attack surface,\ncapturing not only individual techniques but also their relationships,\nprerequisites, and potential impact across the QML pipeline. We propose\nadapting kill chain models, widely used in classical IT and cybersecurity, to\nthe quantum machine learning context. Such models allow for structured\nreasoning about attacker objectives, capabilities, and possible multi-stage\nattack paths - spanning reconnaissance, initial access, manipulation,\npersistence, and exfiltration. Based on extensive literature analysis, we\npresent a detailed taxonomy of QML attack vectors mapped to corresponding\nstages in a quantum-aware kill chain framework that is inspired by the MITRE\nATLAS for classical machine learning. We highlight interdependencies between\nphysical-level threats (like side-channel leakage and crosstalk faults), data\nand algorithm manipulation (such as poisoning or circuit backdoors), and\nprivacy attacks (including model extraction and training data inference). This\nwork provides a foundation for more realistic threat modeling and proactive\nsecurity-in-depth design in the emerging field of quantum machine learning."
    },
    {
        "date": "2025-07",
        "title": "When and Where do Data Poisons Attack Textual Inversion?",
        "author": "Jeremy Styborski, Mingzhi Lyu, Jiayou Lu, Nupur Kapur, and Adams Kong",
        "link": "http://arxiv.org/abs/2507.10578v2",
        "abstract": "Poisoning attacks pose significant challenges to the robustness of diffusion\nmodels (DMs). In this paper, we systematically analyze when and where poisoning\nattacks textual inversion (TI), a widely used personalization technique for\nDMs. We first introduce Semantic Sensitivity Maps, a novel method for\nvisualizing the influence of poisoning on text embeddings. Second, we identify\nand experimentally verify that DMs exhibit non-uniform learning behavior across\ntimesteps, focusing on lower-noise samples. Poisoning attacks inherit this bias\nand inject adversarial signals predominantly at lower timesteps. Lastly, we\nobserve that adversarial signals distract learning away from relevant concept\nregions within training data, corrupting the TI process. Based on these\ninsights, we propose Safe-Zone Training (SZT), a novel defense mechanism\ncomprised of 3 key components: (1) JPEG compression to weaken high-frequency\npoison signals, (2) restriction to high timesteps during TI training to avoid\nadversarial signals at lower timesteps, and (3) loss masking to constrain\nlearning to relevant regions. Extensive experiments across multiple poisoning\nmethods demonstrate that SZT greatly enhances the robustness of TI against all\npoisoning attacks, improving generative quality beyond prior published\ndefenses. Code: www.github.com/JStyborski/Diff_Lab Data:\nwww.github.com/JStyborski/NC10"
    },
    {
        "date": "2025-07",
        "title": "Towards Imperceptible JPEG Image Hiding: Multi-range Representations-driven Adversarial Stego Generation",
        "author": "Junxue Yang, Xin Liao, Weixuan Tang, Jianhua Yang, and Zheng Qin",
        "link": "http://arxiv.org/abs/2507.08343v1",
        "abstract": "Deep hiding has been exploring the hiding capability of deep learning-based\nmodels, aiming to conceal image-level messages into cover images and reveal\nthem from generated stego images. Existing schemes are easily detected by\nsteganalyzers due to their large payloads and their limitation to feature\nextraction based solely on either pure convolution or pure transformer\noperators within a single range, as well as pixel-level loss constraints. To\naddress the issue, in this paper, we introduce generation-based adversarial\nattacks into color JPEG image deep hiding and propose a multi-range\nrepresentations-driven adversarial stego generation framework called MRAG from\na steganalysis perspective. Specifically, we integrate the local-range neighbor\nreception characteristic of the convolution and the global-range dependency\nmodeling of the transformer to construct MRAG. Meanwhile, we use the\ntransformed images obtained through coarse-grained and fine-grained frequency\ndecomposition as inputs, introducing multi-grained information. Furthermore, a\nfeatures angle-norm disentanglement loss is designed to constrain the generated\nstegos closer to covers in the angle and norm space of the steganalyzer's\nclassified features. Consequently, small yet effective adversarial\nperturbations can be injected into the process of generating stegos, ensuring\nthat stegos maintain favorable secret restorability and imperceptibility.\nExtensive experiments demonstrate that MRAG can achieve state-of-the-art\nperformance."
    },
    {
        "date": "2025-07",
        "title": "Invariant-based Robust Weights Watermark for Large Language Models",
        "author": "Qingxiao Guo, Xinjie Zhu, Yilong Ma, Hui Jin, Yunhao Wang, Weifeng Zhang, and Xiaobing Guo",
        "link": "http://arxiv.org/abs/2507.08288v1",
        "abstract": "Watermarking technology has gained significant attention due to the\nincreasing importance of intellectual property (IP) rights, particularly with\nthe growing deployment of large language models (LLMs) on billions\nresource-constrained edge devices. To counter the potential threats of IP theft\nby malicious users, this paper introduces a robust watermarking scheme without\nretraining or fine-tuning for transformer models. The scheme generates a unique\nkey for each user and derives a stable watermark value by solving linear\nconstraints constructed from model invariants. Moreover, this technology\nutilizes noise mechanism to hide watermark locations in multi-user scenarios\nagainst collusion attack. This paper evaluates the approach on three popular\nmodels (Llama3, Phi3, Gemma), and the experimental results confirm the strong\nrobustness across a range of attack methods (fine-tuning, pruning,\nquantization, permutation, scaling, reversible matrix and collusion attacks)."
    },
    {
        "date": "2025-07",
        "title": "Lightweight Safety Guardrails via Synthetic Data and RL-guided Adversarial Training",
        "author": "Aleksei Ilin, Gor Matevosyan, Xueying Ma, Vladimir Eremin, Suhaa Dada, Muqun Li, Riyaaz Shaik, and Haluk Noyan Tokgozoglu",
        "link": "http://arxiv.org/abs/2507.08284v1",
        "abstract": "We introduce a lightweight yet highly effective safety guardrail framework\nfor language models, demonstrating that small-scale language models can\nachieve, and even surpass, the performance of larger counterparts in content\nmoderation tasks. This is accomplished through high-fidelity synthetic data\ngeneration and adversarial training. The synthetic data generation process\nbegins with human-curated seed data, which undergoes query augmentation and\nparaphrasing to create diverse and contextually rich examples. This augmented\ndata is then subjected to multiple rounds of curation, ensuring high fidelity\nand relevance. Inspired by recent advances in the Generative Adversarial\nNetwork (GAN) architecture, our adversarial training employs reinforcement\nlearning to guide a generator that produces challenging synthetic examples.\nThese examples are used to fine-tune the safety classifier, enhancing its\nability to detect and mitigate harmful content. Additionally, we incorporate\nstrategies from recent research on efficient LLM training, leveraging the\ncapabilities of smaller models to improve the performance of larger generative\nmodels. With iterative adversarial training and the generation of diverse,\nhigh-quality synthetic data, our framework enables small language models (SLMs)\nto serve as robust safety guardrails. This approach not only reduces\ncomputational overhead but also enhances resilience against adversarial\nattacks, offering a scalable and efficient solution for content moderation in\nAI systems."
    },
    {
        "date": "2025-07",
        "title": "MIRRAMS: Towards Training Models Robust to Missingness Distribution Shifts",
        "author": "Jihye Lee, Minseo Kang, and Dongha Kim",
        "link": "http://arxiv.org/abs/2507.08280v1",
        "abstract": "In real-world data analysis, missingness distributional shifts between\ntraining and test input datasets frequently occur, posing a significant\nchallenge to achieving robust prediction performance. In this study, we propose\na novel deep learning framework designed to address such shifts in missingness\ndistributions. We begin by introducing a set of mutual information-based\nconditions, called MI robustness conditions, which guide a prediction model to\nextract label-relevant information while remaining invariant to diverse\nmissingness patterns, thereby enhancing robustness to unseen missingness\nscenarios at test-time. To make these conditions practical, we propose simple\nyet effective techniques to derive loss terms corresponding to each and\nformulate a final objective function, termed MIRRAMS(Mutual Information\nRegularization for Robustness Against Missingness Shifts). As a by-product, our\nanalysis provides a theoretical interpretation of the principles underlying\nconsistency regularization-based semi-supervised learning methods, such as\nFixMatch. Extensive experiments across various benchmark datasets show that\nMIRRAMS consistently outperforms existing baselines and maintains stable\nperformance across diverse missingness scenarios. Moreover, our approach\nachieves state-of-the-art performance even without missing data and can be\nnaturally extended to address semi-supervised learning tasks, highlighting\nMIRRAMS as a powerful, off-the-shelf framework for general-purpose learning."
    },
    {
        "date": "2025-07",
        "title": "Admissibility of Stein Shrinkage for Batch Normalization in the Presence of Adversarial Attacks",
        "author": "Sofia Ivolgina, P. Thomas Fletcher, and Baba C. Vemuri",
        "link": "http://arxiv.org/abs/2507.08261v1",
        "abstract": "Batch normalization (BN) is a ubiquitous operation in deep neural networks\nused primarily to achieve stability and regularization during network training.\nBN involves feature map centering and scaling using sample means and variances,\nrespectively. Since these statistics are being estimated across the feature\nmaps within a batch, this problem is ideally suited for the application of\nStein's shrinkage estimation, which leads to a better, in the\nmean-squared-error sense, estimate of the mean and variance of the batch. In\nthis paper, we prove that the Stein shrinkage estimator for the mean and\nvariance dominates over the sample mean and variance estimators in the presence\nof adversarial attacks when modeling these attacks using sub-Gaussian\ndistributions. This facilitates and justifies the application of Stein\nshrinkage to estimate the mean and variance parameters in BN and use it in\nimage classification (segmentation) tasks with and without adversarial attacks.\nWe present SOTA performance results using this Stein corrected batch norm in a\nstandard ResNet architecture applied to the task of image classification using\nCIFAR-10 data, 3D CNN on PPMI (neuroimaging) data and image segmentation using\nHRNet on Cityscape data with and without adversarial attacks."
    },
    {
        "date": "2025-07",
        "title": "EvA: Evolutionary Attacks on Graphs",
        "author": "Mohammad Sadegh Akhondzadeh, Soroush H. Zargarbashi, Jimin Cao, and Aleksandar Bojchevski",
        "link": "http://arxiv.org/abs/2507.08212v1",
        "abstract": "Even a slight perturbation in the graph structure can cause a significant\ndrop in the accuracy of graph neural networks (GNNs). Most existing attacks\nleverage gradient information to perturb edges. This relaxes the attack's\noptimization problem from a discrete to a continuous space, resulting in\nsolutions far from optimal. It also restricts the adaptability of the attack to\nnon-differentiable objectives. Instead, we introduce a few simple yet effective\nenhancements of an evolutionary-based algorithm to solve the discrete\noptimization problem directly. Our Evolutionary Attack (EvA) works with any\nblack-box model and objective, eliminating the need for a differentiable proxy\nloss. This allows us to design two novel attacks that reduce the effectiveness\nof robustness certificates and break conformal sets. The memory complexity of\nour attack is linear in the attack budget. Among our experiments, EvA shows\n$\\sim$11\\% additional drop in accuracy on average compared to the best previous\nattack, revealing significant untapped potential in designing attacks."
    },
    {
        "date": "2025-07",
        "title": "A Dynamic Stackelberg Game Framework for Agentic AI Defense Against LLM Jailbreaking",
        "author": "Zhengye Han, and Quanyan Zhu",
        "link": "http://arxiv.org/abs/2507.08207v1",
        "abstract": "As large language models (LLMs) are increasingly deployed in critical\napplications, the challenge of jailbreaking, where adversaries manipulate the\nmodels to bypass safety mechanisms, has become a significant concern. This\npaper presents a dynamic Stackelberg game framework to model the interactions\nbetween attackers and defenders in the context of LLM jailbreaking. The\nframework treats the prompt-response dynamics as a sequential extensive-form\ngame, where the defender, as the leader, commits to a strategy while\nanticipating the attacker's optimal responses. We propose a novel agentic AI\nsolution, the \"Purple Agent,\" which integrates adversarial exploration and\ndefensive strategies using Rapidly-exploring Random Trees (RRT). The Purple\nAgent actively simulates potential attack trajectories and intervenes\nproactively to prevent harmful outputs. This approach offers a principled\nmethod for analyzing adversarial dynamics and provides a foundation for\nmitigating the risk of jailbreaking."
    },
    {
        "date": "2025-07",
        "title": "HNOSeg-XS: Extremely Small Hartley Neural Operator for Efficient and Resolution-Robust 3D Image Segmentation",
        "author": "Ken C. L. Wong, Hongzhi Wang, and Tanveer Syeda-Mahmood",
        "link": "http://arxiv.org/abs/2507.08205v1",
        "abstract": "In medical image segmentation, convolutional neural networks (CNNs) and\ntransformers are dominant. For CNNs, given the local receptive fields of\nconvolutional layers, long-range spatial correlations are captured through\nconsecutive convolutions and pooling. However, as the computational cost and\nmemory footprint can be prohibitively large, 3D models can only afford fewer\nlayers than 2D models with reduced receptive fields and abstract levels. For\ntransformers, although long-range correlations can be captured by multi-head\nattention, its quadratic complexity with respect to input size is\ncomputationally demanding. Therefore, either model may require input size\nreduction to allow more filters and layers for better segmentation.\nNevertheless, given their discrete nature, models trained with patch-wise\ntraining or image downsampling may produce suboptimal results when applied on\nhigher resolutions. To address this issue, here we propose the\nresolution-robust HNOSeg-XS architecture. We model image segmentation by\nlearnable partial differential equations through the Fourier neural operator\nwhich has the zero-shot super-resolution property. By replacing the Fourier\ntransform by the Hartley transform and reformulating the problem in the\nfrequency domain, we created the HNOSeg-XS model, which is resolution robust,\nfast, memory efficient, and extremely parameter efficient. When tested on the\nBraTS'23, KiTS'23, and MVSeg'23 datasets with a Tesla V100 GPU, HNOSeg-XS\nshowed its superior resolution robustness with fewer than 34.7k model\nparameters. It also achieved the overall best inference time (< 0.24 s) and\nmemory efficiency (< 1.8 GiB) compared to the tested CNN and transformer\nmodels."
    },
    {
        "date": "2025-07",
        "title": "Quantum Properties Trojans (QuPTs) for Attacking Quantum Neural Networks",
        "author": "Sounak Bhowmik, Travis S. Humble, and Himanshu Thapliyal",
        "link": "http://arxiv.org/abs/2507.08202v1",
        "abstract": "Quantum neural networks (QNN) hold immense potential for the future of\nquantum machine learning (QML). However, QNN security and robustness remain\nlargely unexplored. In this work, we proposed novel Trojan attacks based on the\nquantum computing properties in a QNN-based binary classifier. Our proposed\nQuantum Properties Trojans (QuPTs) are based on the unitary property of quantum\ngates to insert noise and Hadamard gates to enable superposition to develop\nTrojans and attack QNNs. We showed that the proposed QuPTs are significantly\nstealthier and heavily impact the quantum circuits' performance, specifically\nQNNs. The most impactful QuPT caused a deterioration of 23% accuracy of the\ncompromised QNN under the experimental setup. To the best of our knowledge,\nthis is the first work on the Trojan attack on a fully quantum neural network\nindependent of any hybrid classical-quantum architecture."
    },
    {
        "date": "2025-07",
        "title": "Robust Semi-Supervised CT Radiomics for Lung Cancer Prognosis: Cost-Effective Learning with Limited Labels and SHAP Interpretation",
        "author": "Mohammad R. Salmanpour, Amir Hossein Pouria, Sonia Falahati, Shahram Taeb, Somayeh Sadat Mehrnia, Mehdi Maghsudi, Ali Fathi Jouzdani, Mehrdad Oveisi, Ilker Hacihaliloglu, and Arman Rahmim",
        "link": "http://arxiv.org/abs/2507.08189v2",
        "abstract": "Background: CT imaging is vital for lung cancer management, offering detailed\nvisualization for AI-based prognosis. However, supervised learning SL models\nrequire large labeled datasets, limiting their real-world application in\nsettings with scarce annotations.\n  Methods: We analyzed CT scans from 977 patients across 12 datasets extracting\n1218 radiomics features using Laplacian of Gaussian and wavelet filters via\nPyRadiomics Dimensionality reduction was applied with 56 feature selection and\nextraction algorithms and 27 classifiers were benchmarked A semi supervised\nlearning SSL framework with pseudo labeling utilized 478 unlabeled and 499\nlabeled cases Model sensitivity was tested in three scenarios varying labeled\ndata in SL increasing unlabeled data in SSL and scaling both from 10 percent to\n100 percent SHAP analysis was used to interpret predictions Cross validation\nand external testing in two cohorts were performed.\n  Results: SSL outperformed SL, improving overall survival prediction by up to\n17 percent. The top SSL model, Random Forest plus XGBoost classifier, achieved\n0.90 accuracy in cross-validation and 0.88 externally. SHAP analysis revealed\nenhanced feature discriminability in both SSL and SL, especially for Class 1\nsurvival greater than 4 years. SSL showed strong performance with only 10\npercent labeled data, with more stable results compared to SL and lower\nvariance across external testing, highlighting SSL's robustness and cost\neffectiveness.\n  Conclusion: We introduced a cost-effective, stable, and interpretable SSL\nframework for CT-based survival prediction in lung cancer, improving\nperformance, generalizability, and clinical readiness by integrating SHAP\nexplainability and leveraging unlabeled data."
    },
    {
        "date": "2025-07",
        "title": "GPUHammer: Rowhammer Attacks on GPU Memories are Practical",
        "author": "Chris S. Lin, Joyce Qu, and Gururaj Saileshwar",
        "link": "http://arxiv.org/abs/2507.08166v1",
        "abstract": "Rowhammer is a read disturbance vulnerability in modern DRAM that causes\nbit-flips, compromising security and reliability. While extensively studied on\nIntel and AMD CPUs with DDR and LPDDR memories, its impact on GPUs using GDDR\nmemories, critical for emerging machine learning applications, remains\nunexplored. Rowhammer attacks on GPUs face unique challenges: (1) proprietary\nmapping of physical memory to GDDR banks and rows, (2) high memory latency and\nfaster refresh rates that hinder effective hammering, and (3) proprietary\nmitigations in GDDR memories, difficult to reverse-engineer without FPGA-based\ntest platforms. We introduce GPUHammer, the first Rowhammer attack on NVIDIA\nGPUs with GDDR6 DRAM. GPUHammer proposes novel techniques to reverse-engineer\nGDDR DRAM row mappings, and employs GPU-specific memory access optimizations to\namplify hammering intensity and bypass mitigations. Thus, we demonstrate the\nfirst successful Rowhammer attack on a discrete GPU, injecting up to 8\nbit-flips across 4 DRAM banks on an NVIDIA A6000 with GDDR6 memory. We also\nshow how an attacker can use these to tamper with ML models, causing\nsignificant accuracy drops (up to 80%)."
    },
    {
        "date": "2025-07",
        "title": "Adaptive Diffusion Denoised Smoothing : Certified Robustness via Randomized Smoothing with Differentially Private Guided Denoising Diffusion",
        "author": "Frederick Shpilevskiy, Saiyue Lyu, Krishnamurthy Dj Dvijotham, Mathias L\u00e9cuyer, and Pierre-Andr\u00e9 No\u00ebl",
        "link": "http://arxiv.org/abs/2507.08163v1",
        "abstract": "We propose Adaptive Diffusion Denoised Smoothing, a method for certifying the\npredictions of a vision model against adversarial examples, while adapting to\nthe input. Our key insight is to reinterpret a guided denoising diffusion model\nas a long sequence of adaptive Gaussian Differentially Private (GDP) mechanisms\nrefining a pure noise sample into an image. We show that these adaptive\nmechanisms can be composed through a GDP privacy filter to analyze the\nend-to-end robustness of the guided denoising process, yielding a provable\ncertification that extends the adaptive randomized smoothing analysis. We\ndemonstrate that our design, under a specific guiding strategy, can improve\nboth certified accuracy and standard accuracy on ImageNet for an $\\ell_2$\nthreat model."
    },
    {
        "date": "2025-07",
        "title": "Beyond the Worst Case: Extending Differential Privacy Guarantees to Realistic Adversaries",
        "author": "Marika Swanberg, Meenatchi Sundaram Muthu Selva Annamalai, Jamie Hayes, Borja Balle, and Adam Smith",
        "link": "http://arxiv.org/abs/2507.08158v1",
        "abstract": "Differential Privacy (DP) is a family of definitions that bound the\nworst-case privacy leakage of a mechanism. One important feature of the\nworst-case DP guarantee is it naturally implies protections against adversaries\nwith less prior information, more sophisticated attack goals, and complex\nmeasures of a successful attack. However, the analytical tradeoffs between the\nadversarial model and the privacy protections conferred by DP are not well\nunderstood thus far. To that end, this work sheds light on what the worst-case\nguarantee of DP implies about the success of attackers that are more\nrepresentative of real-world privacy risks.\n  In this paper, we present a single flexible framework that generalizes and\nextends the patchwork of bounds on DP mechanisms found in prior work. Our\nframework allows us to compute high-probability guarantees for DP mechanisms on\na large family of natural attack settings that previous bounds do not capture.\nOne class of such settings is the approximate reconstruction of multiple\nindividuals' data, such as inferring nearly entire columns of a tabular data\nset from noisy marginals and extracting sensitive information from DP-trained\nlanguage models.\n  We conduct two empirical case studies to illustrate the versatility of our\nbounds and compare them to the success of state-of-the-art attacks.\nSpecifically, we study attacks that extract non-uniform PII from a DP-trained\nlanguage model, as well as multi-column reconstruction attacks where the\nadversary has access to some columns in the clear and attempts to reconstruct\nthe remaining columns for each person's record. We find that the absolute\nprivacy risk of attacking non-uniform data is highly dependent on the\nadversary's prior probability of success. Our high probability bounds give us a\nnuanced understanding of the privacy leakage of DP mechanisms in a variety of\npreviously understudied attack settings."
    },
    {
        "date": "2025-07",
        "title": "Low Resource Reconstruction Attacks Through Benign Prompts",
        "author": "Sol Yarkoni, and Roi Livni",
        "link": "http://arxiv.org/abs/2507.07947v2",
        "abstract": "The recent advances in generative models such as diffusion models have raised\nseveral risks and concerns related to privacy, copyright infringements and data\nstewardship. To better understand and control the risks, various researchers\nhave created techniques, experiments and attacks that reconstruct images, or\npart of images, from the training set. While these techniques already establish\nthat data from the training set can be reconstructed, they often rely on\nhigh-resources, excess to the training set as well as well-engineered and\ndesigned prompts.\n  In this work, we devise a new attack that requires low resources, assumes\nlittle to no access to the actual training set, and identifies, seemingly,\nbenign prompts that lead to potentially-risky image reconstruction. This\nhighlights the risk that images might even be reconstructed by an uninformed\nuser and unintentionally. For example, we identified that, with regard to one\nexisting model, the prompt ``blue Unisex T-Shirt'' can generate the face of a\nreal-life human model. Our method builds on an intuition from previous works\nwhich leverages domain knowledge and identifies a fundamental vulnerability\nthat stems from the use of scraped data from e-commerce platforms, where\ntemplated layouts and images are tied to pattern-like prompts."
    },
    {
        "date": "2025-07",
        "title": "KeyDroid: A Large-Scale Analysis of Secure Key Storage in Android Apps",
        "author": "Jenny Blessing, Ross J. Anderson, and Alastair R. Beresford",
        "link": "http://arxiv.org/abs/2507.07927v1",
        "abstract": "Most contemporary mobile devices offer hardware-backed storage for\ncryptographic keys, user data, and other sensitive credentials. Such hardware\nprotects credentials from extraction by an adversary who has compromised the\nmain operating system, such as a malicious third-party app. Since 2011, Android\napp developers can access trusted hardware via the Android Keystore API. In\nthis work, we conduct the first comprehensive survey of hardware-backed key\nstorage in Android devices. We analyze 490 119 Android apps, collecting data on\nhow trusted hardware is used by app developers (if used at all) and\ncross-referencing our findings with sensitive user data collected by each app,\nas self-reported by developers via the Play Store's data safety labels.\n  We find that despite industry-wide initiatives to encourage adoption, 56.3%\nof apps self-reporting as processing sensitive user data do not use Android's\ntrusted hardware capabilities at all, while just 5.03% of apps collecting some\nform of sensitive data use the strongest form of trusted hardware, a secure\nelement distinct from the main processor. To better understand the potential\ndownsides of using secure hardware, we conduct the first empirical analysis of\ntrusted hardware performance in mobile devices, measuring the runtime of common\ncryptographic operations across both software- and hardware-backed keystores.\nWe find that while hardware-backed key storage using a coprocessor is viable\nfor most common cryptographic operations, secure elements capable of preventing\nmore advanced attacks make performance infeasible for symmetric encryption with\nnon-negligible payloads and any kind of asymmetric encryption."
    },
    {
        "date": "2025-07",
        "title": "ArteryX: Advancing Brain Artery Feature Extraction with Vessel-Fused Networks and a Robust Validation Framework",
        "author": "Abrar Faiyaz, Nhat Hoang, Giovanni Schifitto, and Md Nasir Uddin",
        "link": "http://arxiv.org/abs/2507.07920v1",
        "abstract": "Cerebrovascular pathology significantly contributes to cognitive decline and\nneurological disorders, underscoring the need for advanced tools to assess\nvascular integrity. Three-dimensional Time-of-Flight Magnetic Resonance\nAngiography (3D TOF MRA) is widely used to visualize cerebral vasculature,\nhowever, clinical evaluations generally focus on major arterial abnormalities,\noverlooking quantitative metrics critical for understanding subtle vascular\nchanges. Existing methods for extracting structural, geometrical and\nmorphological arterial features from MRA - whether manual or automated - face\nchallenges including user-dependent variability, steep learning curves, and\nlack of standardized quantitative validations. We propose a novel\nsemi-supervised artery evaluation framework, named ArteryX, a MATLAB-based\ntoolbox that quantifies vascular features with high accuracy and efficiency,\nachieving processing times ~10-15 minutes per subject at 0.5 mm resolution with\nminimal user intervention. ArteryX employs a vessel-fused network based\nlandmarking approach to reliably track and manage tracings, effectively\naddressing the issue of dangling/disconnected vessels. Validation on human\nsubjects with cerebral small vessel disease demonstrated its improved\nsensitivity to subtle vascular changes and better performance than an existing\nsemi-automated method. Importantly, the ArteryX toolbox enables quantitative\nfeature validation by integrating an in-vivo like artery simulation framework\nutilizing vessel-fused graph nodes and predefined ground-truth features for\nspecific artery types. Thus, the ArteryX framework holds promise for\nbenchmarking feature extraction toolboxes and for seamless integration into\nclinical workflows, enabling early detection of cerebrovascular pathology and\nstandardized comparisons across patient cohorts to advance understanding of\nvascular contributions to brain health."
    },
    {
        "date": "2025-07",
        "title": "Can Large Language Models Improve Phishing Defense? A Large-Scale Controlled Experiment on Warning Dialogue Explanations",
        "author": "Federico Maria Cau, Giuseppe Desolda, Francesco Greco, Lucio Davide Spano, and Luca Vigan\u00f2",
        "link": "http://arxiv.org/abs/2507.07916v1",
        "abstract": "Phishing has become a prominent risk in modern cybersecurity, often used to\nbypass technological defences by exploiting predictable human behaviour.\nWarning dialogues are a standard mitigation measure, but the lack of\nexplanatory clarity and static content limits their effectiveness. In this\npaper, we report on our research to assess the capacity of Large Language\nModels (LLMs) to generate clear, concise, and scalable explanations for\nphishing warnings. We carried out a large-scale between-subjects user study (N\n= 750) to compare the influence of warning dialogues supplemented with manually\ngenerated explanations against those generated by two LLMs, Claude 3.5 Sonnet\nand Llama 3.3 70B. We investigated two explanatory styles (feature-based and\ncounterfactual) for their effects on behavioural metrics (click-through rate)\nand perceptual outcomes (e.g., trust, risk, clarity). The results indicate that\nwell-constructed LLM-generated explanations can equal or surpass manually\ncrafted explanations in reducing susceptibility to phishing; Claude-generated\nwarnings exhibited particularly robust performance. Feature-based explanations\nwere more effective for genuine phishing attempts, whereas counterfactual\nexplanations diminished false-positive rates. Other variables such as workload,\ngender, and prior familiarity with warning dialogues significantly moderated\nwarning effectiveness. These results indicate that LLMs can be used to\nautomatically build explanations for warning users against phishing, and that\nsuch solutions are scalable, adaptive, and consistent with human-centred\nvalues."
    },
    {
        "date": "2025-07",
        "title": "Mitigating Watermark Stealing Attacks in Generative Models via Multi-Key Watermarking",
        "author": "Toluwani Aremu, Noor Hussein, Munachiso Nwadike, Samuele Poppi, Jie Zhang, Karthik Nandakumar, Neil Gong, and Nils Lukas",
        "link": "http://arxiv.org/abs/2507.07871v1",
        "abstract": "Watermarking offers a promising solution for GenAI providers to establish the\nprovenance of their generated content. A watermark is a hidden signal embedded\nin the generated content, whose presence can later be verified using a secret\nwatermarking key. A threat to GenAI providers are \\emph{watermark stealing}\nattacks, where users forge a watermark into content that was \\emph{not}\ngenerated by the provider's models without access to the secret key, e.g., to\nfalsely accuse the provider. Stealing attacks collect \\emph{harmless}\nwatermarked samples from the provider's model and aim to maximize the expected\nsuccess rate of generating \\emph{harmful} watermarked samples. Our work focuses\non mitigating stealing attacks while treating the underlying watermark as a\nblack-box. Our contributions are: (i) Proposing a multi-key extension to\nmitigate stealing attacks that can be applied post-hoc to any watermarking\nmethod across any modality. (ii) We provide theoretical guarantees and\ndemonstrate empirically that our method makes forging substantially less\neffective across multiple datasets, and (iii) we formally define the threat of\nwatermark forging as the task of generating harmful, watermarked content and\nmodel this threat via security games."
    },
    {
        "date": "2025-07",
        "title": "Synergistic Prompting for Robust Visual Recognition with Missing Modalities",
        "author": "Zhihui Zhang, Luanyuan Dai, Qika Lin, Yunfeng Diao, Guangyin Jin, Yufei Guo, Jing Zhang, and Xiaoshuai Hao",
        "link": "http://arxiv.org/abs/2507.07802v2",
        "abstract": "Large-scale multi-modal models have demonstrated remarkable performance\nacross various visual recognition tasks by leveraging extensive paired\nmulti-modal training data. However, in real-world applications, the presence of\nmissing or incomplete modality inputs often leads to significant performance\ndegradation. Recent research has focused on prompt-based strategies to tackle\nthis issue; however, existing methods are hindered by two major limitations:\n(1) static prompts lack the flexibility to adapt to varying missing-data\nconditions, and (2) basic prompt-tuning methods struggle to ensure reliable\nperformance when critical modalities are missing.To address these challenges,\nwe propose a novel Synergistic Prompting (SyP) framework for robust visual\nrecognition with missing modalities. The proposed SyP introduces two key\ninnovations: (I) a Dynamic Adapter, which computes adaptive scaling factors to\ndynamically generate prompts, replacing static parameters for flexible\nmulti-modal adaptation, and (II) a Synergistic Prompting Strategy, which\ncombines static and dynamic prompts to balance information across modalities,\nensuring robust reasoning even when key modalities are missing. The proposed\nSyP achieves significant performance improvements over existing approaches\nacross three widely-used visual recognition datasets, demonstrating robustness\nunder diverse missing rates and conditions. Extensive experiments and ablation\nstudies validate its effectiveness in handling missing modalities, highlighting\nits superior adaptability and reliability."
    },
    {
        "date": "2025-07",
        "title": "Robust and Generalizable Heart Rate Estimation via Deep Learning for Remote Photoplethysmography in Complex Scenarios",
        "author": "Kang Cen, Chang-Hong Fu, and Hong Hong",
        "link": "http://arxiv.org/abs/2507.07795v1",
        "abstract": "Non-contact remote photoplethysmography (rPPG) technology enables heart rate\nmeasurement from facial videos. However, existing network models still face\nchallenges in accu racy, robustness, and generalization capability under\ncomplex scenarios. This paper proposes an end-to-end rPPG extraction network\nthat employs 3D convolutional neural networks to reconstruct accurate rPPG\nsignals from raw facial videos. We introduce a differential frame fusion module\nthat integrates differential frames with original frames, enabling frame-level\nrepresentations to capture blood volume pulse (BVP) variations. Additionally,\nwe incorporate Temporal Shift Module (TSM) with self-attention mechanisms,\nwhich effectively enhance rPPG features with minimal computational overhead.\nFurthermore, we propose a novel dynamic hybrid loss function that provides\nstronger supervision for the network, effectively mitigating over fitting.\nComprehensive experiments were conducted on not only the PURE and UBFC-rPPG\ndatasets but also the challenging MMPD dataset under complex scenarios,\ninvolving both intra dataset and cross-dataset evaluations, which demonstrate\nthe superior robustness and generalization capability of our network.\nSpecifically, after training on PURE, our model achieved a mean absolute error\n(MAE) of 7.58 on the MMPD test set, outperforming the state-of-the-art models."
    },
    {
        "date": "2025-07",
        "title": "Space-Filling Regularization for Robust and Interpretable Nonlinear State Space Models",
        "author": "Hermann Klein, Max Heinz Herkersdorf, and Oliver Nelles",
        "link": "http://arxiv.org/abs/2507.07792v1",
        "abstract": "The state space dynamics representation is the most general approach for\nnonlinear systems and often chosen for system identification. During training,\nthe state trajectory can deform significantly leading to poor data coverage of\nthe state space. This can cause significant issues for space-oriented training\nalgorithms which e.g. rely on grid structures, tree partitioning, or similar.\nBesides hindering training, significant state trajectory deformations also\ndeteriorate interpretability and robustness properties. This paper proposes a\nnew type of space-filling regularization that ensures a favorable data\ndistribution in state space via introducing a data-distribution-based penalty.\nThis method is demonstrated in local model network architectures where good\ninterpretability is a major concern. The proposed approach integrates ideas\nfrom modeling and design of experiments for state space structures. This is why\nwe present two regularization techniques for the data point distributions of\nthe state trajectories for local affine state space models. Beyond that, we\ndemonstrate the results on a widely known system identification benchmark."
    },
    {
        "date": "2025-07",
        "title": "SCOOTER: A Human Evaluation Framework for Unrestricted Adversarial Examples",
        "author": "Dren Fazlija, Monty-Maximilian Z\u00fchlke, Johanna Schrader, Arkadij Orlov, Clara Stein, Iyiola E. Olatunji, and Daniel Kudenko",
        "link": "http://arxiv.org/abs/2507.07776v2",
        "abstract": "Unrestricted adversarial attacks aim to fool computer vision models without\nbeing constrained by $\\ell_p$-norm bounds to remain imperceptible to humans,\nfor example, by changing an object's color. This allows attackers to circumvent\ntraditional, norm-bounded defense strategies such as adversarial training or\ncertified defense strategies. However, due to their unrestricted nature, there\nare also no guarantees of norm-based imperceptibility, necessitating human\nevaluations to verify just how authentic these adversarial examples look. While\nsome related work assesses this vital quality of adversarial attacks, none\nprovide statistically significant insights. This issue necessitates a unified\nframework that supports and streamlines such an assessment for evaluating and\ncomparing unrestricted attacks. To close this gap, we introduce SCOOTER - an\nopen-source, statistically powered framework for evaluating unrestricted\nadversarial examples. Our contributions are: $(i)$ best-practice guidelines for\ncrowd-study power, compensation, and Likert equivalence bounds to measure\nimperceptibility; $(ii)$ the first large-scale human vs. model comparison\nacross 346 human participants showing that three color-space attacks and three\ndiffusion-based attacks fail to produce imperceptible images. Furthermore, we\nfound that GPT-4o can serve as a preliminary test for imperceptibility, but it\nonly consistently detects adversarial examples for four out of six tested\nattacks; $(iii)$ open-source software tools, including a browser-based task\ntemplate to collect annotations and analysis scripts in Python and R; $(iv)$ an\nImageNet-derived benchmark dataset containing 3K real images, 7K adversarial\nexamples, and over 34K human ratings. Our findings demonstrate that automated\nvision systems do not align with human perception, reinforcing the need for a\nground-truth SCOOTER benchmark."
    },
    {
        "date": "2025-07",
        "title": "Rainbow Artifacts from Electromagnetic Signal Injection Attacks on Image Sensors",
        "author": "Youqian Zhang, Xinyu Ji, Zhihao Wang, and Qinhong Jiang",
        "link": "http://arxiv.org/abs/2507.07773v1",
        "abstract": "Image sensors are integral to a wide range of safety- and security-critical\nsystems, including surveillance infrastructure, autonomous vehicles, and\nindustrial automation. These systems rely on the integrity of visual data to\nmake decisions. In this work, we investigate a novel class of electromagnetic\nsignal injection attacks that target the analog domain of image sensors,\nallowing adversaries to manipulate raw visual inputs without triggering\nconventional digital integrity checks. We uncover a previously undocumented\nattack phenomenon on CMOS image sensors: rainbow-like color artifacts induced\nin images captured by image sensors through carefully tuned electromagnetic\ninterference. We further evaluate the impact of these attacks on\nstate-of-the-art object detection models, showing that the injected artifacts\npropagate through the image signal processing pipeline and lead to significant\nmispredictions. Our findings highlight a critical and underexplored\nvulnerability in the visual perception stack, highlighting the need for more\nrobust defenses against physical-layer attacks in such systems."
    },
    {
        "date": "2025-07",
        "title": "TRIX- Trading Adversarial Fairness via Mixed Adversarial Training",
        "author": "Tejaswini Medi, Steffen Jung, and Margret Keuper",
        "link": "http://arxiv.org/abs/2507.07768v1",
        "abstract": "Adversarial Training (AT) is a widely adopted defense against adversarial\nexamples. However, existing approaches typically apply a uniform training\nobjective across all classes, overlooking disparities in class-wise\nvulnerability. This results in adversarial unfairness: classes with well\ndistinguishable features (strong classes) tend to become more robust, while\nclasses with overlapping or shared features(weak classes) remain\ndisproportionately susceptible to adversarial attacks. We observe that strong\nclasses do not require strong adversaries during training, as their non-robust\nfeatures are quickly suppressed. In contrast, weak classes benefit from\nstronger adversaries to effectively reduce their vulnerabilities. Motivated by\nthis, we introduce TRIX, a feature-aware adversarial training framework that\nadaptively assigns weaker targeted adversaries to strong classes, promoting\nfeature diversity via uniformly sampled targets, and stronger untargeted\nadversaries to weak classes, enhancing their focused robustness. TRIX further\nincorporates per-class loss weighting and perturbation strength adjustments,\nbuilding on prior work, to emphasize weak classes during the optimization.\nComprehensive experiments on standard image classification benchmarks,\nincluding evaluations under strong attacks such as PGD and AutoAttack,\ndemonstrate that TRIX significantly improves worst-case class accuracy on both\nclean and adversarial data, reducing inter-class robustness disparities, and\npreserves overall accuracy. Our results highlight TRIX as a practical step\ntoward fair and effective adversarial defense."
    },
    {
        "date": "2025-07",
        "title": "One Object, Multiple Lies: A Benchmark for Cross-task Adversarial Attack on Unified Vision-Language Models",
        "author": "Jiale Zhao, Xinyang Jiang, Junyao Gao, Yuhao Xue, and Cairong Zhao",
        "link": "http://arxiv.org/abs/2507.07709v1",
        "abstract": "Unified vision-language models(VLMs) have recently shown remarkable progress,\nenabling a single model to flexibly address diverse tasks through different\ninstructions within a shared computational architecture. This instruction-based\ncontrol mechanism creates unique security challenges, as adversarial inputs\nmust remain effective across multiple task instructions that may be\nunpredictably applied to process the same malicious content. In this paper, we\nintroduce CrossVLAD, a new benchmark dataset carefully curated from MSCOCO with\nGPT-4-assisted annotations for systematically evaluating cross-task adversarial\nattacks on unified VLMs. CrossVLAD centers on the object-change\nobjective-consistently manipulating a target object's classification across\nfour downstream tasks-and proposes a novel success rate metric that measures\nsimultaneous misclassification across all tasks, providing a rigorous\nevaluation of adversarial transferability. To tackle this challenge, we present\nCRAFT (Cross-task Region-based Attack Framework with Token-alignment), an\nefficient region-centric attack method. Extensive experiments on Florence-2 and\nother popular unified VLMs demonstrate that our method outperforms existing\napproaches in both overall cross-task attack performance and targeted\nobject-change success rates, highlighting its effectiveness in adversarially\ninfluencing unified VLMs across diverse tasks."
    },
    {
        "date": "2025-07",
        "title": "May I have your Attention? Breaking Fine-Tuning based Prompt Injection Defenses using Architecture-Aware Attacks",
        "author": "Nishit V. Pandya, Andrey Labunets, Sicun Gao, and Earlence Fernandes",
        "link": "http://arxiv.org/abs/2507.07417v1",
        "abstract": "A popular class of defenses against prompt injection attacks on large\nlanguage models (LLMs) relies on fine-tuning the model to separate instructions\nand data, so that the LLM does not follow instructions that might be present\nwith data. There are several academic systems and production-level\nimplementations of this idea. We evaluate the robustness of this class of\nprompt injection defenses in the whitebox setting by constructing strong\noptimization-based attacks and showing that the defenses do not provide the\nclaimed security properties. Specifically, we construct a novel attention-based\nattack algorithm for text-based LLMs and apply it to two recent whitebox\ndefenses SecAlign (CCS 2025) and StruQ (USENIX Security 2025), showing attacks\nwith success rates of up to 70% with modest increase in attacker budget in\nterms of tokens. Our findings make fundamental progress towards understanding\nthe robustness of prompt injection defenses in the whitebox setting. We release\nour code and attacks at https://github.com/nishitvp/better_opts_attacks"
    },
    {
        "date": "2025-07",
        "title": "Robust Multimodal Learning Framework For Intake Gesture Detection Using Contactless Radar and Wearable IMU Sensors",
        "author": "Chunzhuo Wang, Hans Hallez, and Bart Vanrumste",
        "link": "http://arxiv.org/abs/2507.07261v1",
        "abstract": "Automated food intake gesture detection plays a vital role in dietary\nmonitoring, enabling objective and continuous tracking of eating behaviors to\nsupport better health outcomes. Wrist-worn inertial measurement units (IMUs)\nhave been widely used for this task with promising results. More recently,\ncontactless radar sensors have also shown potential. This study explores\nwhether combining wearable and contactless sensing modalities through\nmultimodal learning can further improve detection performance. We also address\na major challenge in multimodal learning: reduced robustness when one modality\nis missing. To this end, we propose a robust multimodal temporal convolutional\nnetwork with cross-modal attention (MM-TCN-CMA), designed to integrate IMU and\nradar data, enhance gesture detection, and maintain performance under missing\nmodality conditions. A new dataset comprising 52 meal sessions (3,050 eating\ngestures and 797 drinking gestures) from 52 participants is developed and made\npublicly available. Experimental results show that the proposed framework\nimproves the segmental F1-score by 4.3% and 5.2% over unimodal Radar and IMU\nmodels, respectively. Under missing modality scenarios, the framework still\nachieves gains of 1.3% and 2.4% for missing radar and missing IMU inputs. This\nis the first study to demonstrate a robust multimodal learning framework that\neffectively fuses IMU and radar data for food intake gesture detection."
    },
    {
        "date": "2025-07",
        "title": "Exploiting Edge Features for Transferable Adversarial Attacks in Distributed Machine Learning",
        "author": "Giulio Rossolini, Fabio Brau, Alessandro Biondi, Battista Biggio, and Giorgio Buttazzo",
        "link": "http://arxiv.org/abs/2507.07259v1",
        "abstract": "As machine learning models become increasingly deployed across the edge of\ninternet of things environments, a partitioned deep learning paradigm in which\nmodels are split across multiple computational nodes introduces a new dimension\nof security risk. Unlike traditional inference setups, these distributed\npipelines span the model computation across heterogeneous nodes and\ncommunication layers, thereby exposing a broader attack surface to potential\nadversaries. Building on these motivations, this work explores a previously\noverlooked vulnerability: even when both the edge and cloud components of the\nmodel are inaccessible (i.e., black-box), an adversary who intercepts the\nintermediate features transmitted between them can still pose a serious threat.\nWe demonstrate that, under these mild and realistic assumptions, an attacker\ncan craft highly transferable proxy models, making the entire deep learning\nsystem significantly more vulnerable to evasion attacks. In particular, the\nintercepted features can be effectively analyzed and leveraged to distill\nsurrogate models capable of crafting highly transferable adversarial examples\nagainst the target model. To this end, we propose an exploitation strategy\nspecifically designed for distributed settings, which involves reconstructing\nthe original tensor shape from vectorized transmitted features using simple\nstatistical analysis, and adapting surrogate architectures accordingly to\nenable effective feature distillation. A comprehensive and systematic\nexperimental evaluation has been conducted to demonstrate that surrogate models\ntrained with the proposed strategy, i.e., leveraging intermediate features,\ntremendously improve the transferability of adversarial attacks. These findings\nunderscore the urgent need to account for intermediate feature leakage in the\ndesign of secure distributed deep learning systems."
    },
    {
        "date": "2025-07",
        "title": "Automated Attack Testflow Extraction from Cyber Threat Report using BERT for Contextual Analysis",
        "author": "Faissal Ahmadou, Sepehr Ghaffarzadegan, Boubakr Nour, Makan Pourzandi, Mourad Debbabi, and Chadi Assi",
        "link": "http://arxiv.org/abs/2507.07244v1",
        "abstract": "In the ever-evolving landscape of cybersecurity, the rapid identification and\nmitigation of Advanced Persistent Threats (APTs) is crucial. Security\npractitioners rely on detailed threat reports to understand the tactics,\ntechniques, and procedures (TTPs) employed by attackers. However, manually\nextracting attack testflows from these reports requires elusive knowledge and\nis time-consuming and prone to errors. This paper proposes FLOWGUARDIAN, a\nnovel solution leveraging language models (i.e., BERT) and Natural Language\nProcessing (NLP) techniques to automate the extraction of attack testflows from\nunstructured threat reports. FLOWGUARDIAN systematically analyzes and\ncontextualizes security events, reconstructs attack sequences, and then\ngenerates comprehensive testflows. This automated approach not only saves time\nand reduces human error but also ensures comprehensive coverage and robustness\nin cybersecurity testing. Empirical validation using public threat reports\ndemonstrates FLOWGUARDIAN's accuracy and efficiency, significantly enhancing\nthe capabilities of security teams in proactive threat hunting and incident\nresponse."
    },
    {
        "date": "2025-07",
        "title": "Towards Robust Surrogate Models: Benchmarking Machine Learning Approaches to Expediting Phase Field Simulations of Brittle Fracture",
        "author": "Erfan Hamdi, and Emma Lejeune",
        "link": "http://arxiv.org/abs/2507.07237v1",
        "abstract": "Data driven approaches have the potential to make modeling complex, nonlinear\nphysical phenomena significantly more computationally tractable. For example,\ncomputational modeling of fracture is a core challenge where machine learning\ntechniques have the potential to provide a much needed speedup that would\nenable progress in areas such as mutli-scale modeling and uncertainty\nquantification. Currently, phase field modeling (PFM) of fracture is one such\napproach that offers a convenient variational formulation to model crack\nnucleation, branching and propagation. To date, machine learning techniques\nhave shown promise in approximating PFM simulations. However, most studies rely\non overly simple benchmarks that do not reflect the true complexity of the\nfracture processes where PFM excels as a method. To address this gap, we\nintroduce a challenging dataset based on PFM simulations designed to benchmark\nand advance ML methods for fracture modeling. This dataset includes three\nenergy decomposition methods, two boundary conditions, and 1,000 random initial\ncrack configurations for a total of 6,000 simulations. Each sample contains 100\ntime steps capturing the temporal evolution of the crack field. Alongside this\ndataset, we also implement and evaluate Physics Informed Neural Networks\n(PINN), Fourier Neural Operators (FNO) and UNet models as baselines, and\nexplore the impact of ensembling strategies on prediction accuracy. With this\ncombination of our dataset and baseline models drawn from the literature we aim\nto provide a standardized and challenging benchmark for evaluating machine\nlearning approaches to solid mechanics. Our results highlight both the promise\nand limitations of popular current models, and demonstrate the utility of this\ndataset as a testbed for advancing machine learning in fracture mechanics\nresearch."
    },
    {
        "date": "2025-07",
        "title": "Towards Evaluating Robustness of Prompt Adherence in Text to Image Models",
        "author": "Sujith Vemishetty, Advitiya Arora, and Anupama Sharma",
        "link": "http://arxiv.org/abs/2507.08039v1",
        "abstract": "The advancements in the domain of LLMs in recent years have surprised many,\nshowcasing their remarkable capabilities and diverse applications. Their\npotential applications in various real-world scenarios have led to significant\nresearch on their reliability and effectiveness. On the other hand, multimodal\nLLMs and Text-to-Image models have only recently gained prominence, especially\nwhen compared to text-only LLMs. Their reliability remains constrained due to\ninsufficient research on assessing their performance and robustness. This paper\naims to establish a comprehensive evaluation framework for Text-to-Image\nmodels, concentrating particularly on their adherence to prompts. We created a\nnovel dataset that aimed to assess the robustness of these models in generating\nimages that conform to the specified factors of variation in the input text\nprompts. Our evaluation studies present findings on three variants of Stable\nDiffusion models: Stable Diffusion 3 Medium, Stable Diffusion 3.5 Large, and\nStable Diffusion 3.5 Large Turbo, and two variants of Janus models: Janus Pro\n1B and Janus Pro 7B. We introduce a pipeline that leverages text descriptions\ngenerated by the gpt-4o model for our ground-truth images, which are then used\nto generate artificial images by passing these descriptions to the\nText-to-Image models. We then pass these generated images again through gpt-4o\nusing the same system prompt and compare the variation between the two\ndescriptions. Our results reveal that these models struggle to create simple\nbinary images with only two factors of variation: a simple geometric shape and\nits location. We also show, using pre-trained VAEs on our dataset, that they\nfail to generate images that follow our input dataset distribution."
    },
    {
        "date": "2025-07",
        "title": "Federated Learning-based MARL for Strengthening Physical-Layer Security in B5G Networks",
        "author": "Deemah H. Tashman, Soumaya Cherkaoui, and Walaa Hamouda",
        "link": "http://arxiv.org/abs/2507.06997v1",
        "abstract": "This paper explores the application of a federated learning-based multi-agent\nreinforcement learning (MARL) strategy to enhance physical-layer security (PLS)\nin a multi-cellular network within the context of beyond 5G networks. At each\ncell, a base station (BS) operates as a deep reinforcement learning (DRL) agent\nthat interacts with the surrounding environment to maximize the secrecy rate of\nlegitimate users in the presence of an eavesdropper. This eavesdropper attempts\nto intercept the confidential information shared between the BS and its\nauthorized users. The DRL agents are deemed to be federated since they only\nshare their network parameters with a central server and not the private data\nof their legitimate users. Two DRL approaches, deep Q-network (DQN) and\nReinforce deep policy gradient (RDPG), are explored and compared. The results\ndemonstrate that RDPG converges more rapidly than DQN. In addition, we\ndemonstrate that the proposed method outperforms the distributed DRL approach.\nFurthermore, the outcomes illustrate the trade-off between security and\ncomplexity."
    },
    {
        "date": "2025-07",
        "title": "Robust and Safe Traffic Sign Recognition using N-version with Weighted Voting",
        "author": "Linyun Gao, Qiang Wen, and Fumio Machida",
        "link": "http://arxiv.org/abs/2507.06907v1",
        "abstract": "Autonomous driving is rapidly advancing as a key application of machine\nlearning, yet ensuring the safety of these systems remains a critical\nchallenge. Traffic sign recognition, an essential component of autonomous\nvehicles, is particularly vulnerable to adversarial attacks that can compromise\ndriving safety. In this paper, we propose an N-version machine learning (NVML)\nframework that integrates a safety-aware weighted soft voting mechanism. Our\napproach utilizes Failure Mode and Effects Analysis (FMEA) to assess potential\nsafety risks and assign dynamic, safety-aware weights to the ensemble outputs.\nWe evaluate the robustness of three-version NVML systems employing various\nvoting mechanisms against adversarial samples generated using the Fast Gradient\nSign Method (FGSM) and Projected Gradient Descent (PGD) attacks. Experimental\nresults demonstrate that our NVML approach significantly enhances the\nrobustness and safety of traffic sign recognition systems under adversarial\nconditions."
    },
    {
        "date": "2025-07",
        "title": "VisualTrap: A Stealthy Backdoor Attack on GUI Agents via Visual Grounding Manipulation",
        "author": "Ziang Ye, Yang Zhang, Wentao Shi, Xiaoyu You, Fuli Feng, and Tat-Seng Chua",
        "link": "http://arxiv.org/abs/2507.06899v1",
        "abstract": "Graphical User Interface (GUI) agents powered by Large Vision-Language Models\n(LVLMs) have emerged as a revolutionary approach to automating human-machine\ninteractions, capable of autonomously operating personal devices (e.g., mobile\nphones) or applications within the device to perform complex real-world tasks\nin a human-like manner. However, their close integration with personal devices\nraises significant security concerns, with many threats, including backdoor\nattacks, remaining largely unexplored. This work reveals that the visual\ngrounding of GUI agent-mapping textual plans to GUI elements-can introduce\nvulnerabilities, enabling new types of backdoor attacks. With backdoor attack\ntargeting visual grounding, the agent's behavior can be compromised even when\ngiven correct task-solving plans. To validate this vulnerability, we propose\nVisualTrap, a method that can hijack the grounding by misleading the agent to\nlocate textual plans to trigger locations instead of the intended targets.\nVisualTrap uses the common method of injecting poisoned data for attacks, and\ndoes so during the pre-training of visual grounding to ensure practical\nfeasibility of attacking. Empirical results show that VisualTrap can\neffectively hijack visual grounding with as little as 5% poisoned data and\nhighly stealthy visual triggers (invisible to the human eye); and the attack\ncan be generalized to downstream tasks, even after clean fine-tuning. Moreover,\nthe injected trigger can remain effective across different GUI environments,\ne.g., being trained on mobile/web and generalizing to desktop environments.\nThese findings underscore the urgent need for further research on backdoor\nattack risks in GUI agents."
    },
    {
        "date": "2025-07",
        "title": "A Single-Point Measurement Framework for Robust Cyber-Attack Diagnosis in Smart Microgrids Using Dual Fractional-Order Feature Analysis",
        "author": "Yifan Wang",
        "link": "http://arxiv.org/abs/2507.06890v1",
        "abstract": "Cyber-attacks jeopardize the safe operation of smart microgrids. At the same\ntime, existing diagnostic methods either depend on expensive multi-point\ninstrumentation or stringent modelling assumptions that are untenable under\nsingle-sensor constraints. This paper proposes a Fractional-Order\nMemory-Enhanced Attack-Diagnosis Scheme (FO-MADS) that achieves low-latency\nfault localisation and cyber-attack detection using only one VPQ\n(Voltage-Power-Reactive-power) sensor. FO-MADS first constructs a dual\nfractional-order feature library by jointly applying Caputo and\nGr\\\"unwald-Letnikov derivatives, thereby amplifying micro-perturbations and\nslow drifts in the VPQ signal. A two-stage hierarchical classifier then\npinpoints the affected inverter and isolates the faulty IGBT switch,\neffectively alleviating class imbalance. Robustness is further strengthened\nthrough Progressive Memory-Replay Adversarial Training (PMR-AT), whose\nattack-aware loss is dynamically re-weighted via Online Hard Example Mining\n(OHEM) to prioritise the most challenging samples. Experiments on a\nfour-inverter microgrid testbed comprising 1 normal and 24 fault classes under\nfour attack scenarios demonstrate diagnostic accuracies of 96.6 % (bias), 94.0\n% (noise), 92.8 % (data replacement), and 95.7 % (replay), while sustaining\n96.7 % under attack-free conditions. These results establish FO-MADS as a\ncost-effective and readily deployable solution that markedly enhances the\ncyber-physical resilience of smart microgrids."
    },
    {
        "date": "2025-07",
        "title": "IAP: Invisible Adversarial Patch Attack through Perceptibility-Aware Localization and Perturbation Optimization",
        "author": "Subrat Kishore Dutta, and Xiao Zhang",
        "link": "http://arxiv.org/abs/2507.06856v1",
        "abstract": "Despite modifying only a small localized input region, adversarial patches\ncan drastically change the prediction of computer vision models. However, prior\nmethods either cannot perform satisfactorily under targeted attack scenarios or\nfail to produce contextually coherent adversarial patches, causing them to be\neasily noticeable by human examiners and insufficiently stealthy against\nautomatic patch defenses. In this paper, we introduce IAP, a novel attack\nframework that generates highly invisible adversarial patches based on\nperceptibility-aware localization and perturbation optimization schemes.\nSpecifically, IAP first searches for a proper location to place the patch by\nleveraging classwise localization and sensitivity maps, balancing the\nsusceptibility of patch location to both victim model prediction and human\nvisual system, then employs a perceptibility-regularized adversarial loss and a\ngradient update rule that prioritizes color constancy for optimizing invisible\nperturbations. Comprehensive experiments across various image benchmarks and\nmodel architectures demonstrate that IAP consistently achieves competitive\nattack success rates in targeted settings with significantly improved patch\ninvisibility compared to existing baselines. In addition to being highly\nimperceptible to humans, IAP is shown to be stealthy enough to render several\nstate-of-the-art patch defenses ineffective."
    },
    {
        "date": "2025-07",
        "title": "The Dark Side of LLMs Agent-based Attacks for Complete Computer Takeover",
        "author": "Matteo Lupinacci, Francesco Aurelio Pironti, Francesco Blefari, Francesco Romeo, Luigi Arena, and Angelo Furfaro",
        "link": "http://arxiv.org/abs/2507.06850v3",
        "abstract": "The rapid adoption of Large Language Model (LLM) agents and multi-agent\nsystems enables unprecedented capabilities in natural language processing and\ngeneration. However, these systems have introduced unprecedented security\nvulnerabilities that extend beyond traditional prompt injection attacks. This\npaper presents the first comprehensive evaluation of LLM agents as attack\nvectors capable of achieving complete computer takeover through the\nexploitation of trust boundaries within agentic AI systems where autonomous\nentities interact and influence each other. We demonstrate that adversaries can\nleverage three distinct attack surfaces - direct prompt injection, RAG backdoor\nattacks, and inter-agent trust exploitation - to coerce popular LLMs (including\nGPT-4o, Claude-4 and Gemini-2.5) into autonomously installing and executing\nmalware on victim machines. Our evaluation of 17 state-of-the-art LLMs reveals\nan alarming vulnerability hierarchy: while 41.2% of models succumb to direct\nprompt injection, 52.9% are vulnerable to RAG backdoor attacks, and a critical\n82.4% can be compromised through inter-agent trust exploitation. Notably, we\ndiscovered that LLMs which successfully resist direct malicious commands will\nexecute identical payloads when requested by peer agents, revealing a\nfundamental flaw in current multi-agent security models. Our findings\ndemonstrate that only 5.9% of tested models (1/17) proved resistant to all\nattack vectors, with the majority exhibiting context-dependent security\nbehaviors that create exploitable blind spots. Our findings also highlight the\nneed to increase awareness and research on the security risks of LLMs, showing\na paradigm shift in cybersecurity threats, where AI tools themselves become\nsophisticated attack vectors."
    },
    {
        "date": "2025-07",
        "title": "Designing Robust Software Sensors for Nonlinear Systems via Neural Networks and Adaptive Sliding Mode Control",
        "author": "Ayoub Farkane, Mohamed Boutayeb, Mustapha Oudani, and Mounir Ghogho",
        "link": "http://arxiv.org/abs/2507.06817v1",
        "abstract": "Accurate knowledge of the state variables in a dynamical system is critical\nfor effective control, diagnosis, and supervision, especially when direct\nmeasurements of all states are infeasible. This paper presents a novel approach\nto designing software sensors for nonlinear dynamical systems expressed in\ntheir most general form. Unlike traditional model-based observers that rely on\nexplicit transformations or linearization, the proposed framework integrates\nneural networks with adaptive Sliding Mode Control (SMC) to design a robust\nstate observer under a less restrictive set of conditions. The learning process\nis driven by available sensor measurements, which are used to correct the\nobserver's state estimate. The training methodology leverages the system's\ngoverning equations as a physics-based constraint, enabling observer synthesis\nwithout access to ground-truth state trajectories. By employing a time-varying\ngain matrix dynamically adjusted by the neural network, the observer adapts in\nreal-time to system changes, ensuring robustness against noise, external\ndisturbances, and variations in system dynamics. Furthermore, we provide\nsufficient conditions to guarantee estimation error convergence, establishing a\ntheoretical foundation for the observer's reliability. The methodology's\neffectiveness is validated through simulations on challenging examples,\nincluding systems with non-differentiable dynamics and varying observability\nconditions. These examples, which are often problematic for conventional\ntechniques, serve to demonstrate the robustness and broad applicability of our\napproach. The results show rapid convergence and high accuracy, underscoring\nthe method's potential for addressing complex state estimation challenges in\nreal-world applications."
    }
]