[
    {
        "date": "2025-07",
        "title": "Hedge Funds on a Swamp: Analyzing Patterns, Vulnerabilities, and Defense Measures in Blockchain Bridges [Experiment, Analysis \\& Benchmark]",
        "author": "Poupak Azad, Jiahua Xu, Yebo Feng, Preston Strowbridge, and Cuneyt Akcora",
        "link": "http://arxiv.org/abs/2507.06156v1",
        "abstract": "Blockchain bridges have become essential infrastructure for enabling\ninteroperability across different blockchain networks, with more than $24B\nmonthly bridge transaction volume. However, their growing adoption has been\naccompanied by a disproportionate rise in security breaches, making them the\nsingle largest source of financial loss in Web3. For cross-chain ecosystems to\nbe robust and sustainable, it is essential to understand and address these\nvulnerabilities. In this study, we present a comprehensive systematization of\nblockchain bridge design and security. We define three bridge security priors,\nformalize the architectural structure of 13 prominent bridges, and identify 23\nattack vectors grounded in real-world blockchain exploits. Using this\nfoundation, we evaluate 43 representative attack scenarios and introduce a\nlayered threat model that captures security failures across source chain,\noff-chain, and destination chain components.\n  Our analysis at the static code and transaction network levels reveals\nrecurring design flaws, particularly in access control, validator trust\nassumptions, and verification logic, and identifies key patterns in adversarial\nbehavior based on transaction-level traces. To support future development, we\npropose a decision framework for bridge architecture design, along with defense\nmechanisms such as layered validation and circuit breakers. This work provides\na data-driven foundation for evaluating bridge security and lays the groundwork\nfor standardizing resilient cross-chain infrastructure."
    },
    {
        "date": "2025-07",
        "title": "Taming Data Challenges in ML-based Security Tasks: Lessons from Integrating Generative AI",
        "author": "Shravya Kanchi, Neal Mangaokar, Aravind Cheruvu, Sifat Muhammad Abdullah, Shirin Nilizadeh, Atul Prakash, and Bimal Viswanath",
        "link": "http://arxiv.org/abs/2507.06092v1",
        "abstract": "Machine learning-based supervised classifiers are widely used for security\ntasks, and their improvement has been largely focused on algorithmic\nadvancements. We argue that data challenges that negatively impact the\nperformance of these classifiers have received limited attention. We address\nthe following research question: Can developments in Generative AI (GenAI)\naddress these data challenges and improve classifier performance? We propose\naugmenting training datasets with synthetic data generated using GenAI\ntechniques to improve classifier generalization. We evaluate this approach\nacross 7 diverse security tasks using 6 state-of-the-art GenAI methods and\nintroduce a novel GenAI scheme called Nimai that enables highly controlled data\nsynthesis. We find that GenAI techniques can significantly improve the\nperformance of security classifiers, achieving improvements of up to 32.6% even\nin severely data-constrained settings (only ~180 training samples).\nFurthermore, we demonstrate that GenAI can facilitate rapid adaptation to\nconcept drift post-deployment, requiring minimal labeling in the adjustment\nprocess. Despite successes, our study finds that some GenAI schemes struggle to\ninitialize (train and produce data) on certain security tasks. We also identify\ncharacteristics of specific tasks, such as noisy labels, overlapping class\ndistributions, and sparse feature vectors, which hinder performance boost using\nGenAI. We believe that our study will drive the development of future GenAI\ntools designed for security tasks."
    },
    {
        "date": "2025-07",
        "title": "ScoreAdv: Score-based Targeted Generation of Natural Adversarial Examples via Diffusion Models",
        "author": "Chihan Huang, and Hao Tang",
        "link": "http://arxiv.org/abs/2507.06078v1",
        "abstract": "Despite the success of deep learning across various domains, it remains\nvulnerable to adversarial attacks. Although many existing adversarial attack\nmethods achieve high success rates, they typically rely on $\\ell_{p}$-norm\nperturbation constraints, which do not align with human perceptual\ncapabilities. Consequently, researchers have shifted their focus toward\ngenerating natural, unrestricted adversarial examples (UAEs). GAN-based\napproaches suffer from inherent limitations, such as poor image quality due to\ninstability and mode collapse. Meanwhile, diffusion models have been employed\nfor UAE generation, but they still rely on iterative PGD perturbation\ninjection, without fully leveraging their central denoising capabilities. In\nthis paper, we introduce a novel approach for generating UAEs based on\ndiffusion models, named ScoreAdv. This method incorporates an interpretable\nadversarial guidance mechanism to gradually shift the sampling distribution\ntowards the adversarial distribution, while using an interpretable saliency map\nto inject the visual information of a reference image into the generated\nsamples. Notably, our method is capable of generating an unlimited number of\nnatural adversarial examples and can attack not only classification models but\nalso retrieval models. We conduct extensive experiments on ImageNet and CelebA\ndatasets, validating the performance of ScoreAdv across ten target models in\nboth black-box and white-box settings. Our results demonstrate that ScoreAdv\nachieves state-of-the-art attack success rates and image quality. Furthermore,\nthe dynamic balance between denoising and adversarial perturbation enables\nScoreAdv to remain robust even under defensive measures."
    },
    {
        "date": "2025-07",
        "title": "CAVGAN: Unifying Jailbreak and Defense of LLMs via Generative Adversarial Attacks on their Internal Representations",
        "author": "Xiaohu Li, Yunfeng Ning, Zepeng Bao, Mayi Xu, Jianhao Chen, and Tieyun Qian",
        "link": "http://arxiv.org/abs/2507.06043v1",
        "abstract": "Security alignment enables the Large Language Model (LLM) to gain the\nprotection against malicious queries, but various jailbreak attack methods\nreveal the vulnerability of this security mechanism. Previous studies have\nisolated LLM jailbreak attacks and defenses. We analyze the security protection\nmechanism of the LLM, and propose a framework that combines attack and defense.\nOur method is based on the linearly separable property of LLM intermediate\nlayer embedding, as well as the essence of jailbreak attack, which aims to\nembed harmful problems and transfer them to the safe area. We utilize\ngenerative adversarial network (GAN) to learn the security judgment boundary\ninside the LLM to achieve efficient jailbreak attack and defense. The\nexperimental results indicate that our method achieves an average jailbreak\nsuccess rate of 88.85\\% across three popular LLMs, while the defense success\nrate on the state-of-the-art jailbreak dataset reaches an average of 84.17\\%.\nThis not only validates the effectiveness of our approach but also sheds light\non the internal security mechanisms of LLMs, offering new insights for\nenhancing model security The code and data are available at\nhttps://github.com/NLPGM/CAVGAN."
    },
    {
        "date": "2025-07",
        "title": "Ensemble-Based Deepfake Detection using State-of-the-Art Models with Robust Cross-Dataset Generalisation",
        "author": "Haroon Wahab, Hassan Ugail, and Lujain Jaleel",
        "link": "http://arxiv.org/abs/2507.05996v1",
        "abstract": "Machine learning-based Deepfake detection models have achieved impressive\nresults on benchmark datasets, yet their performance often deteriorates\nsignificantly when evaluated on out-of-distribution data. In this work, we\ninvestigate an ensemble-based approach for improving the generalization of\ndeepfake detection systems across diverse datasets. Building on a recent\nopen-source benchmark, we combine prediction probabilities from several\nstate-of-the-art asymmetric models proposed at top venues. Our experiments span\ntwo distinct out-of-domain datasets and demonstrate that no single model\nconsistently outperforms others across settings. In contrast, ensemble-based\npredictions provide more stable and reliable performance in all scenarios. Our\nresults suggest that asymmetric ensembling offers a robust and scalable\nsolution for real-world deepfake detection where prior knowledge of forgery\ntype or quality is often unavailable."
    },
    {
        "date": "2025-07",
        "title": "Robust Speech-Workload Estimation for Intelligent Human-Robot Systems",
        "author": "Julian Fortune, Julie A. Adams, and Jamison Heard",
        "link": "http://arxiv.org/abs/2507.05985v1",
        "abstract": "Demanding task environments (e.g., supervising a remotely piloted aircraft)\nrequire performing tasks quickly and accurately; however, periods of low and\nhigh operator workload can decrease task performance. Intelligent modulation of\nthe system's demands and interaction modality in response to changes in\noperator workload state may increase performance by avoiding undesirable\nworkload states. This system requires real-time estimation of each workload\ncomponent (i.e., cognitive, physical, visual, speech, and auditory) to adapt\nthe correct modality. Existing workload systems estimate multiple workload\ncomponents post-hoc, but few estimate speech workload, or function in\nreal-time. An algorithm to estimate speech workload and mitigate undesirable\nworkload states in real-time is presented. An analysis of the algorithm's\naccuracy is presented, along with the results demonstrating the algorithm's\ngeneralizability across individuals and human-machine teaming paradigms.\nReal-time speech workload estimation is a crucial element towards developing\nadaptive human-machine systems."
    },
    {
        "date": "2025-07",
        "title": "Beyond Appearance: Geometric Cues for Robust Video Instance Segmentation",
        "author": "Quanzhu Niu, Yikang Zhou, Shihao Chen, Tao Zhang, and Shunping Ji",
        "link": "http://arxiv.org/abs/2507.05948v1",
        "abstract": "Video Instance Segmentation (VIS) fundamentally struggles with pervasive\nchallenges including object occlusions, motion blur, and appearance variations\nduring temporal association. To overcome these limitations, this work\nintroduces geometric awareness to enhance VIS robustness by strategically\nleveraging monocular depth estimation. We systematically investigate three\ndistinct integration paradigms. Expanding Depth Channel (EDC) method\nconcatenates the depth map as input channel to segmentation networks; Sharing\nViT (SV) designs a uniform ViT backbone, shared between depth estimation and\nsegmentation branches; Depth Supervision (DS) makes use of depth prediction as\nan auxiliary training guide for feature learning. Though DS exhibits limited\neffectiveness, benchmark evaluations demonstrate that EDC and SV significantly\nenhance the robustness of VIS. When with Swin-L backbone, our EDC method gets\n56.2 AP, which sets a new state-of-the-art result on OVIS benchmark. This work\nconclusively establishes depth cues as critical enablers for robust video\nunderstanding."
    },
    {
        "date": "2025-07",
        "title": "What You Have is What You Track: Adaptive and Robust Multimodal Tracking",
        "author": "Yuedong Tan, Jiawei Shao, Eduard Zamfir, Ruanjun Li, Zhaochong An, Chao Ma, Danda Paudel, Luc Van Gool, Radu Timofte, and Zongwei Wu",
        "link": "http://arxiv.org/abs/2507.05899v1",
        "abstract": "Multimodal data is known to be helpful for visual tracking by improving\nrobustness to appearance variations. However, sensor synchronization challenges\noften compromise data availability, particularly in video settings where\nshortages can be temporal. Despite its importance, this area remains\nunderexplored. In this paper, we present the first comprehensive study on\ntracker performance with temporally incomplete multimodal data. Unsurprisingly,\nunder such a circumstance, existing trackers exhibit significant performance\ndegradation, as their rigid architectures lack the adaptability needed to\neffectively handle missing modalities. To address these limitations, we propose\na flexible framework for robust multimodal tracking. We venture that a tracker\nshould dynamically activate computational units based on missing data rates.\nThis is achieved through a novel Heterogeneous Mixture-of-Experts fusion\nmechanism with adaptive complexity, coupled with a video-level masking strategy\nthat ensures both temporal consistency and spatial completeness which is\ncritical for effective video tracking. Surprisingly, our model not only adapts\nto varying missing rates but also adjusts to scene complexity. Extensive\nexperiments show that our model achieves SOTA performance across 9 benchmarks,\nexcelling in both conventional complete and missing modality settings. The code\nand benchmark will be publicly available at\nhttps://github.com/supertyd/FlexTrack/tree/main."
    },
    {
        "date": "2025-07",
        "title": "Robust Power System State Estimation using Physics-Informed Neural Networks",
        "author": "Solon Falas, Markos Asprou, Charalambos Konstantinou, and Maria K. Michael",
        "link": "http://arxiv.org/abs/2507.05874v1",
        "abstract": "Modern power systems face significant challenges in state estimation and\nreal-time monitoring, particularly regarding response speed and accuracy under\nfaulty conditions or cyber-attacks. This paper proposes a hybrid approach using\nphysics-informed neural networks (PINNs) to enhance the accuracy and\nrobustness, of power system state estimation. By embedding physical laws into\nthe neural network architecture, PINNs improve estimation accuracy for\ntransmission grid applications under both normal and faulty conditions, while\nalso showing potential in addressing security concerns such as data\nmanipulation attacks. Experimental results show that the proposed approach\noutperforms traditional machine learning models, achieving up to 83% higher\naccuracy on unseen subsets of the training dataset and 65% better performance\non entirely new, unrelated datasets. Experiments also show that during a data\nmanipulation attack against a critical bus in a system, the PINN can be up to\n93% more accurate than an equivalent neural network."
    },
    {
        "date": "2025-07",
        "title": "Improving Robustness of Foundation Models in Domain Adaptation with Soup-Adapters",
        "author": "Marco Roschkowski",
        "link": "http://arxiv.org/abs/2507.05807v1",
        "abstract": "In this paper, we tackle two fundamental problems in few-shot domain\nadaptation of foundation models. First, hyperparameter tuning is often\nimpractical due to the lack of large validation datasets. Second, model\nrobustness under distribution shifts where test time data deviates slightly\nfrom training distributions, remains a concern. We show that by training\nmultiple independent adapters and averaging their outputs, the new model has a\nhigher performance and is more robust to distribution shifts compared to any\nindividual adapter. This improvement holds even when the adapters are trained\nwith diverse hyperparameters sampled from a wide range, resulting in varied\nindividual performance. Consequently, our method addresses both of the problems\ndescribed above. The ensemble is also significantly less sensitive to the\nresidual ratio, a critical hyperparameter of CLIP-Adapter. Since the ensemble\ncan be reparameterized to a single adapter again using a principled\nconcatenation of the parameters, we refer to our method as Soup-Adapter. This\nis also the first study to explore CLIP adapter-style techniques for DINOv2 and\nto directly compare them with CLIP in this setting."
    },
    {
        "date": "2025-07",
        "title": "Robust Bandwidth Estimation for Real-Time Communication with Offline Reinforcement Learning",
        "author": "Jian Kai, Tianwei Zhang, Zihan Ling, Yang Cao, and Can Shen",
        "link": "http://arxiv.org/abs/2507.05785v1",
        "abstract": "Accurate bandwidth estimation (BWE) is critical for real-time communication\n(RTC) systems. Traditional heuristic approaches offer limited adaptability\nunder dynamic networks, while online reinforcement learning (RL) suffers from\nhigh exploration costs and potential service disruptions. Offline RL, which\nleverages high-quality data collected from real-world environments, offers a\npromising alternative. However, challenges such as out-of-distribution (OOD)\nactions, policy extraction from behaviorally diverse datasets, and reliable\ndeployment in production systems remain unsolved. We propose RBWE, a robust\nbandwidth estimation framework based on offline RL that integrates Q-ensemble\n(an ensemble of Q-functions) with a Gaussian mixture policy to mitigate OOD\nrisks and enhance policy learning. A fallback mechanism ensures deployment\nstability by switching to heuristic methods under high uncertainty.\nExperimental results show that RBWE reduces overestimation errors by 18% and\nimproves the 10th percentile Quality of Experience (QoE) by 18.6%,\ndemonstrating its practical effectiveness in real-world RTC applications."
    },
    {
        "date": "2025-07",
        "title": "Normal Patch Retinex Robust Alghoritm for White Balancing in Digital Microscopy",
        "author": "Radoslaw Roszczyk, Artur Krupa, and Izabella Antoniuk",
        "link": "http://arxiv.org/abs/2507.05757v1",
        "abstract": "The acquisition of accurately coloured, balanced images in an optical\nmicroscope can be a challenge even for experienced microscope operators. This\narticle presents an entirely automatic mechanism for balancing the white level\nthat allows the correction of the microscopic colour images adequately. The\nresults of the algorithm have been confirmed experimentally on a set of two\nhundred microscopic images. The images contained scans of three microscopic\nspecimens commonly used in pathomorphology. Also, the results achieved were\ncompared with other commonly used white balance algorithms in digital\nphotography. The algorithm applied in this work is more effective than the\nclassical algorithms used in colour photography for microscopic images stained\nwith hematoxylin-phloxine-saffron and for immunohistochemical staining images."
    },
    {
        "date": "2025-07",
        "title": "SenseShift6D: Multimodal RGB-D Benchmarking for Robust 6D Pose Estimation across Environment and Sensor Variations",
        "author": "Yegyu Han, Taegyoon Yoon, Dayeon Woo, Sojeong Kim, and Hyung-Sin Kim",
        "link": "http://arxiv.org/abs/2507.05751v1",
        "abstract": "Recent advances on 6D object-pose estimation has achieved high performance on\nrepresentative benchmarks such as LM-O, YCB-V, and T-Less. However, these\ndatasets were captured under fixed illumination and camera settings, leaving\nthe impact of real-world variations in illumination, exposure, gain or\ndepth-sensor mode - and the potential of test-time sensor control to mitigate\nsuch variations - largely unexplored. To bridge this gap, we introduce\nSenseShift6D, the first RGB-D dataset that physically sweeps 13 RGB exposures,\n9 RGB gains, auto-exposure, 4 depth-capture modes, and 5 illumination levels.\nFor three common household objects (spray, pringles, and tincase), we acquire\n101.9k RGB and 10k depth images, which can provide 1,380 unique sensor-lighting\npermutations per object pose. Experiments with state-of-the-art models on our\ndataset show that applying sensor control during test-time induces greater\nperformance improvement over digital data augmentation, achieving performance\ncomparable to or better than costly increases in real-world training data\nquantity and diversity. Adapting either RGB or depth sensors individually is\neffective, while jointly adapting multimodal RGB-D configurations yields even\ngreater improvements. SenseShift6D extends the 6D-pose evaluation paradigm from\ndata-centered to sensor-aware robustness, laying a foundation for adaptive,\nself-tuning perception systems capable of operating robustly in uncertain\nreal-world environments. Our dataset is available at:\nhuggingface.co/datasets/Yegyu/SenseShift6D Associated scripts can be found at:\ngithub.com/yegyu-han/SenseShift6D"
    },
    {
        "date": "2025-07",
        "title": "DATABench: Evaluating Dataset Auditing in Deep Learning from an Adversarial Perspective",
        "author": "Shuo Shao, Yiming Li, Mengren Zheng, Zhiyang Hu, Yukun Chen, Boheng Li, Yu He, Junfeng Guo, Tianwei Zhang, Dacheng Tao, and Zhan Qin",
        "link": "http://arxiv.org/abs/2507.05622v1",
        "abstract": "The widespread application of Deep Learning across diverse domains hinges\ncritically on the quality and composition of training datasets. However, the\ncommon lack of disclosure regarding their usage raises significant privacy and\ncopyright concerns. Dataset auditing techniques, which aim to determine if a\nspecific dataset was used to train a given suspicious model, provide promising\nsolutions to addressing these transparency gaps. While prior work has developed\nvarious auditing methods, their resilience against dedicated adversarial\nattacks remains largely unexplored. To bridge the gap, this paper initiates a\ncomprehensive study evaluating dataset auditing from an adversarial\nperspective. We start with introducing a novel taxonomy, classifying existing\nmethods based on their reliance on internal features (IF) (inherent to the\ndata) versus external features (EF) (artificially introduced for auditing).\nSubsequently, we formulate two primary attack types: evasion attacks, designed\nto conceal the use of a dataset, and forgery attacks, intending to falsely\nimplicate an unused dataset. Building on the understanding of existing methods\nand attack objectives, we further propose systematic attack strategies:\ndecoupling, removal, and detection for evasion; adversarial example-based\nmethods for forgery. These formulations and strategies lead to our new\nbenchmark, DATABench, comprising 17 evasion attacks, 5 forgery attacks, and 9\nrepresentative auditing methods. Extensive evaluations using DATABench reveal\nthat none of the evaluated auditing methods are sufficiently robust or\ndistinctive under adversarial settings. These findings underscore the urgent\nneed for developing a more secure and reliable dataset auditing method capable\nof withstanding sophisticated adversarial manipulation. Code is available at\nhttps://github.com/shaoshuo-ss/DATABench."
    },
    {
        "date": "2025-07",
        "title": "Search-based Selection of Metamorphic Relations for Optimized Robustness Testing of Large Language Models",
        "author": "Sangwon Hyun, Shaukat Ali, and M. Ali Babar",
        "link": "http://arxiv.org/abs/2507.05565v1",
        "abstract": "Assessing the trustworthiness of Large Language Models (LLMs), such as\nrobustness, has garnered significant attention. Recently, metamorphic testing\nthat defines Metamorphic Relations (MRs) has been widely applied to evaluate\nthe robustness of LLM executions. However, the MR-based robustness testing\nstill requires a scalable number of MRs, thereby necessitating the optimization\nof selecting MRs. Most extant LLM testing studies are limited to automatically\ngenerating test cases (i.e., MRs) to enhance failure detection. Additionally,\nmost studies only considered a limited test space of single perturbation MRs in\ntheir evaluation of LLMs. In contrast, our paper proposes a search-based\napproach for optimizing the MR groups to maximize failure detection and\nminimize the LLM execution cost. Moreover, our approach covers the\ncombinatorial perturbations in MRs, facilitating the expansion of test space in\nthe robustness assessment. We have developed a search process and implemented\nfour search algorithms: Single-GA, NSGA-II, SPEA2, and MOEA/D with novel\nencoding to solve the MR selection problem in the LLM robustness testing. We\nconducted comparative experiments on the four search algorithms along with a\nrandom search, using two major LLMs with primary Text-to-Text tasks. Our\nstatistical and empirical investigation revealed two key findings: (1) the\nMOEA/D algorithm performed the best in optimizing the MR space for LLM\nrobustness testing, and (2) we identified silver bullet MRs for the LLM\nrobustness testing, which demonstrated dominant capabilities in confusing LLMs\nacross different Text-to-Text tasks. In LLM robustness assessment, our research\nsheds light on the fundamental problem for optimized testing and provides\ninsights into search-based solutions."
    },
    {
        "date": "2025-07",
        "title": "Robust Learning on Noisy Graphs via Latent Space Constraints with External Knowledge",
        "author": "Chunhui Gu, Mohammad Sadegh Nasr, James P. Long, Kim-Anh Do, and Ehsan Irajizad",
        "link": "http://arxiv.org/abs/2507.05540v1",
        "abstract": "Graph Neural Networks (GNNs) often struggle with noisy edges. We propose\nLatent Space Constrained Graph Neural Networks (LSC-GNN) to incorporate\nexternal \"clean\" links and guide embeddings of a noisy target graph. We train\ntwo encoders--one on the full graph (target plus external edges) and another on\na regularization graph excluding the target's potentially noisy links--then\npenalize discrepancies between their latent representations. This constraint\nsteers the model away from overfitting spurious edges. Experiments on benchmark\ndatasets show LSC-GNN outperforms standard and noise-resilient GNNs in graphs\nsubjected to moderate noise. We extend LSC-GNN to heterogeneous graphs and\nvalidate it on a small protein-metabolite network, where metabolite-protein\ninteractions reduce noise in protein co-occurrence data. Our results highlight\nLSC-GNN's potential to boost predictive performance and interpretability in\nsettings with noisy relational structures."
    },
    {
        "date": "2025-07",
        "title": "Bit-Flip Fault Attack: Crushing Graph Neural Networks via Gradual Bit Search",
        "author": "Sanaz Kazemi Abharian, and Sai Manoj Pudukotai Dinakarrao",
        "link": "http://arxiv.org/abs/2507.05531v1",
        "abstract": "Graph Neural Networks (GNNs) have emerged as a powerful machine learning\nmethod for graph-structured data. A plethora of hardware accelerators has been\nintroduced to meet the performance demands of GNNs in real-world applications.\nHowever, security challenges of hardware-based attacks have been generally\noverlooked. In this paper, we investigate the vulnerability of GNN models to\nhardware-based fault attack, wherein an attacker attempts to misclassify output\nby modifying trained weight parameters through fault injection in a memory\ndevice. Thus, we propose Gradual Bit-Flip Fault Attack (GBFA), a layer-aware\nbit-flip fault attack, selecting a vulnerable bit in each selected weight\ngradually to compromise the GNN's performance by flipping a minimal number of\nbits. To achieve this, GBFA operates in two steps. First, a Markov model is\ncreated to predict the execution sequence of layers based on features extracted\nfrom memory access patterns, enabling the launch of the attack within a\nspecific layer. Subsequently, GBFA identifies vulnerable bits within the\nselected weights using gradient ranking through an in-layer search. We evaluate\nthe effectiveness of the proposed GBFA attack on various GNN models for node\nclassification tasks using the Cora and PubMed datasets. Our findings show that\nGBFA significantly degrades prediction accuracy, and the variation in its\nimpact across different layers highlights the importance of adopting a\nlayer-aware attack strategy in GNNs. For example, GBFA degrades GraphSAGE's\nprediction accuracy by 17% on the Cora dataset with only a single bit flip in\nthe last layer."
    },
    {
        "date": "2025-07",
        "title": "A Systematization of Security Vulnerabilities in Computer Use Agents",
        "author": "Daniel Jones, Giorgio Severi, Martin Pouliot, Gary Lopez, Joris de Gruyter, Santiago Zanella-Beguelin, Justin Song, Blake Bullwinkel, Pamela Cortez, and Amanda Minnich",
        "link": "http://arxiv.org/abs/2507.05445v1",
        "abstract": "Computer Use Agents (CUAs), autonomous systems that interact with software\ninterfaces via browsers or virtual machines, are rapidly being deployed in\nconsumer and enterprise environments. These agents introduce novel attack\nsurfaces and trust boundaries that are not captured by traditional threat\nmodels. Despite their growing capabilities, the security boundaries of CUAs\nremain poorly understood. In this paper, we conduct a systematic threat\nanalysis and testing of real-world CUAs under adversarial conditions. We\nidentify seven classes of risks unique to the CUA paradigm, and analyze three\nconcrete exploit scenarios in depth: (1) clickjacking via visual overlays that\nmislead interface-level reasoning, (2) indirect prompt injection that enables\nRemote Code Execution (RCE) through chained tool use, and (3) CoT exposure\nattacks that manipulate implicit interface framing to hijack multi-step\nreasoning. These case studies reveal deeper architectural flaws across current\nCUA implementations. Namely, a lack of input provenance tracking, weak\ninterface-action binding, and insufficient control over agent memory and\ndelegation. We conclude by proposing a CUA-specific security evaluation\nframework and design principles for safe deployment in adversarial and\nhigh-stakes settings."
    },
    {
        "date": "2025-07",
        "title": "Adversarial Machine Learning Attacks on Financial Reporting via Maximum Violated Multi-Objective Attack",
        "author": "Edward Raff, Karen Kukla, Michel Benaroch, and Joseph Comprix",
        "link": "http://arxiv.org/abs/2507.05441v1",
        "abstract": "Bad actors, primarily distressed firms, have the incentive and desire to\nmanipulate their financial reports to hide their distress and derive personal\ngains. As attackers, these firms are motivated by potentially millions of\ndollars and the availability of many publicly disclosed and used financial\nmodeling frameworks. Existing attack methods do not work on this data due to\nanti-correlated objectives that must both be satisfied for the attacker to\nsucceed. We introduce Maximum Violated Multi-Objective (MVMO) attacks that\nadapt the attacker's search direction to find $20\\times$ more satisfying\nattacks compared to standard attacks. The result is that in $\\approx50\\%$ of\ncases, a company could inflate their earnings by 100-200%, while simultaneously\nreducing their fraud scores by 15%. By working with lawyers and professional\naccountants, we ensure our threat model is realistic to how such frauds are\nperformed in practice."
    },
    {
        "date": "2025-07",
        "title": "Incorporating Interventional Independence Improves Robustness against Interventional Distribution Shift",
        "author": "Gautam Sreekumar, and Vishnu Naresh Boddeti",
        "link": "http://arxiv.org/abs/2507.05412v1",
        "abstract": "We consider the problem of learning robust discriminative representations of\ncausally-related latent variables. In addition to observational data, the\ntraining dataset also includes interventional data obtained through targeted\ninterventions on some of these latent variables to learn representations robust\nagainst the resulting interventional distribution shifts. Existing approaches\ntreat interventional data like observational data, even when the underlying\ncausal model is known, and ignore the independence relations that arise from\nthese interventions. Since these approaches do not fully exploit the causal\nrelational information resulting from interventions, they learn representations\nthat produce large disparities in predictive performance on observational and\ninterventional data, which worsens when the number of interventional training\nsamples is limited. In this paper, (1) we first identify a strong correlation\nbetween this performance disparity and adherence of the representations to the\nindependence conditions induced by the interventional causal model. (2) For\nlinear models, we derive sufficient conditions on the proportion of\ninterventional data in the training dataset, for which enforcing interventional\nindependence between representations corresponding to the intervened node and\nits non-descendants lowers the error on interventional data. Combining these\ninsights, (3) we propose RepLIn, a training algorithm to explicitly enforce\nthis statistical independence during interventions. We demonstrate the utility\nof RepLIn on a synthetic dataset and on real image and text datasets on facial\nattribute classification and toxicity detection, respectively. Our experiments\nshow that RepLIn is scalable with the number of nodes in the causal graph and\nis suitable to improve the robust representations against interventional\ndistribution shifts of both continuous and discrete latent variables."
    },
    {
        "date": "2025-07",
        "title": "YOLO-APD: Enhancing YOLOv8 for Robust Pedestrian Detection on Complex Road Geometries",
        "author": "Aquino Joctum, and John Kandiri",
        "link": "http://arxiv.org/abs/2507.05376v1",
        "abstract": "Autonomous vehicle perception systems require robust pedestrian detection,\nparticularly on geometrically complex roadways like Type-S curved surfaces,\nwhere standard RGB camera-based methods face limitations. This paper introduces\nYOLO-APD, a novel deep learning architecture enhancing the YOLOv8 framework\nspecifically for this challenge. YOLO-APD integrates several key architectural\nmodifications: a parameter-free SimAM attention mechanism, computationally\nefficient C3Ghost modules, a novel SimSPPF module for enhanced multi-scale\nfeature pooling, the Mish activation function for improved optimization, and an\nIntelligent Gather & Distribute (IGD) module for superior feature fusion in the\nnetwork's neck. The concept of leveraging vehicle steering dynamics for\nadaptive region-of-interest processing is also presented. Comprehensive\nevaluations on a custom CARLA dataset simulating complex scenarios demonstrate\nthat YOLO-APD achieves state-of-the-art detection accuracy, reaching 77.7%\nmAP@0.5:0.95 and exceptional pedestrian recall exceeding 96%, significantly\noutperforming baseline models, including YOLOv8. Furthermore, it maintains\nreal-time processing capabilities at 100 FPS, showcasing a superior balance\nbetween accuracy and efficiency. Ablation studies validate the synergistic\ncontribution of each integrated component. Evaluation on the KITTI dataset\nconfirms the architecture's potential while highlighting the need for domain\nadaptation. This research advances the development of highly accurate,\nefficient, and adaptable perception systems based on cost-effective sensors,\ncontributing to enhanced safety and reliability for autonomous navigation in\nchallenging, less-structured driving environments."
    },
    {
        "date": "2025-07",
        "title": "Extreme Learning Machine Based System for DDoS Attacks Detections on IoMT Devices",
        "author": "Nelly Elsayed, Lily Dzamesi, Zag ElSayed, and Murat Ozer",
        "link": "http://arxiv.org/abs/2507.05132v1",
        "abstract": "The Internet of Medical Things (IoMT) represents a paradigm shift in the\nhealthcare sector, enabling the interconnection of medical devices, sensors,\nand systems to enhance patient monitoring, diagnosis, and management. The rapid\nevolution of IoMT presents significant benefits to the healthcare domains.\nHowever, there is a rapid increase in distributed denial of service (DDoS)\nattacks on the IoMT networks due to several vulnerabilities in the\nIoMT-connected devices, which negatively impact patients' health and can even\nlead to deaths. Thus, in this paper, we aim to save lives via investigating an\nextreme learning machine for detecting DDoS attacks on IoMT devices. The\nproposed approach achieves a high accuracy at a low implementation budget.\nThus, it can reduce the implementation cost of the DDoS detection system,\nmaking the model capable of executing on the fog level."
    },
    {
        "date": "2025-07",
        "title": "CLIP-Guided Backdoor Defense through Entropy-Based Poisoned Dataset Separation",
        "author": "Binyan Xu, Fan Yang, Xilin Dai, Di Tang, and Kehuan Zhang",
        "link": "http://arxiv.org/abs/2507.05113v1",
        "abstract": "Deep Neural Networks (DNNs) are susceptible to backdoor attacks, where\nadversaries poison training data to implant backdoor into the victim model.\nCurrent backdoor defenses on poisoned data often suffer from high computational\ncosts or low effectiveness against advanced attacks like clean-label and\nclean-image backdoors. To address them, we introduce CLIP-Guided backdoor\nDefense (CGD), an efficient and effective method that mitigates various\nbackdoor attacks. CGD utilizes a publicly accessible CLIP model to identify\ninputs that are likely to be clean or poisoned. It then retrains the model with\nthese inputs, using CLIP's logits as a guidance to effectively neutralize the\nbackdoor. Experiments on 4 datasets and 11 attack types demonstrate that CGD\nreduces attack success rates (ASRs) to below 1% while maintaining clean\naccuracy (CA) with a maximum drop of only 0.3%, outperforming existing\ndefenses. Additionally, we show that clean-data-based defenses can be adapted\nto poisoned data using CGD. Also, CGD exhibits strong robustness, maintaining\nlow ASRs even when employing a weaker CLIP model or when CLIP itself is\ncompromised by a backdoor. These findings underscore CGD's exceptional\nefficiency, effectiveness, and applicability for real-world backdoor defense\nscenarios. Code: https://github.com/binyxu/CGD."
    },
    {
        "date": "2025-07",
        "title": "The Hidden Threat in Plain Text: Attacking RAG Data Loaders",
        "author": "Alberto Castagnaro, Umberto Salviati, Mauro Conti, Luca Pajola, and Simeone Pizzi",
        "link": "http://arxiv.org/abs/2507.05093v1",
        "abstract": "Large Language Models (LLMs) have transformed human-machine interaction since\nChatGPT's 2022 debut, with Retrieval-Augmented Generation (RAG) emerging as a\nkey framework that enhances LLM outputs by integrating external knowledge.\nHowever, RAG's reliance on ingesting external documents introduces new\nvulnerabilities. This paper exposes a critical security gap at the data loading\nstage, where malicious actors can stealthily corrupt RAG pipelines by\nexploiting document ingestion.\n  We propose a taxonomy of 9 knowledge-based poisoning attacks and introduce\ntwo novel threat vectors -- Content Obfuscation and Content Injection --\ntargeting common formats (DOCX, HTML, PDF). Using an automated toolkit\nimplementing 19 stealthy injection techniques, we test five popular data\nloaders, finding a 74.4% attack success rate across 357 scenarios. We further\nvalidate these threats on six end-to-end RAG systems -- including white-box\npipelines and black-box services like NotebookLM and OpenAI Assistants --\ndemonstrating high success rates and critical vulnerabilities that bypass\nfilters and silently compromise output integrity. Our results emphasize the\nurgent need to secure the document ingestion process in RAG systems against\ncovert content manipulations."
    },
    {
        "date": "2025-07",
        "title": "Robust Incomplete-Modality Alignment for Ophthalmic Disease Grading and Diagnosis via Labeled Optimal Transport",
        "author": "Qinkai Yu, Jianyang Xie, Yitian Zhao, Cheng Chen, Lijun Zhang, Liming Chen, Jun Cheng, Lu Liu, Yalin Zheng, and Yanda Meng",
        "link": "http://arxiv.org/abs/2507.04999v1",
        "abstract": "Multimodal ophthalmic imaging-based diagnosis integrates color fundus image\nwith optical coherence tomography (OCT) to provide a comprehensive view of\nocular pathologies. However, the uneven global distribution of healthcare\nresources often results in real-world clinical scenarios encountering\nincomplete multimodal data, which significantly compromises diagnostic\naccuracy. Existing commonly used pipelines, such as modality imputation and\ndistillation methods, face notable limitations: 1)Imputation methods struggle\nwith accurately reconstructing key lesion features, since OCT lesions are\nlocalized, while fundus images vary in style. 2)distillation methods rely\nheavily on fully paired multimodal training data. To address these challenges,\nwe propose a novel multimodal alignment and fusion framework capable of\nrobustly handling missing modalities in the task of ophthalmic diagnostics. By\nconsidering the distinctive feature characteristics of OCT and fundus images,\nwe emphasize the alignment of semantic features within the same category and\nexplicitly learn soft matching between modalities, allowing the missing\nmodality to utilize existing modality information, achieving robust cross-modal\nfeature alignment under the missing modality. Specifically, we leverage the\nOptimal Transport for multi-scale modality feature alignment: class-wise\nalignment through predicted class prototypes and feature-wise alignment via\ncross-modal shared feature transport. Furthermore, we propose an asymmetric\nfusion strategy that effectively exploits the distinct characteristics of OCT\nand fundus modalities. Extensive evaluations on three large ophthalmic\nmultimodal datasets demonstrate our model's superior performance under various\nmodality-incomplete scenarios, achieving Sota performance in both complete\nmodality and inter-modality incompleteness conditions. Code is available at\nhttps://github.com/Qinkaiyu/RIMA"
    },
    {
        "date": "2025-07",
        "title": "BackFed: An Efficient & Standardized Benchmark Suite for Backdoor Attacks in Federated Learning",
        "author": "Thinh Dao, Dung Thuy Nguyen, Khoa D Doan, and Kok-Seng Wong",
        "link": "http://arxiv.org/abs/2507.04903v1",
        "abstract": "Federated Learning (FL) systems are vulnerable to backdoor attacks, where\nadversaries train their local models on poisoned data and submit poisoned model\nupdates to compromise the global model. Despite numerous proposed attacks and\ndefenses, divergent experimental settings, implementation errors, and\nunrealistic assumptions hinder fair comparisons and valid conclusions about\ntheir effectiveness in real-world scenarios. To address this, we introduce\nBackFed - a comprehensive benchmark suite designed to standardize, streamline,\nand reliably evaluate backdoor attacks and defenses in FL, with a focus on\npractical constraints. Our benchmark offers key advantages through its\nmulti-processing implementation that significantly accelerates experimentation\nand the modular design that enables seamless integration of new methods via\nwell-defined APIs. With a standardized evaluation pipeline, we envision BackFed\nas a plug-and-play environment for researchers to comprehensively and reliably\nevaluate new attacks and defenses. Using BackFed, we conduct large-scale\nstudies of representative backdoor attacks and defenses across both Computer\nVision and Natural Language Processing tasks with diverse model architectures\nand experimental settings. Our experiments critically assess the performance of\nproposed attacks and defenses, revealing unknown limitations and modes of\nfailures under practical conditions. These empirical insights provide valuable\nguidance for the development of new methods and for enhancing the security of\nFL systems. Our framework is openly available at\nhttps://github.com/thinh-dao/BackFed."
    },
    {
        "date": "2025-07",
        "title": "RIPE: Reinforcement Learning on Unlabeled Image Pairs for Robust Keypoint Extraction",
        "author": "Johannes K\u00fcnzel, Anna Hilsmann, and Peter Eisert",
        "link": "http://arxiv.org/abs/2507.04839v1",
        "abstract": "We introduce RIPE, an innovative reinforcement learning-based framework for\nweakly-supervised training of a keypoint extractor that excels in both\ndetection and description tasks. In contrast to conventional training regimes\nthat depend heavily on artificial transformations, pre-generated models, or 3D\ndata, RIPE requires only a binary label indicating whether paired images\nrepresent the same scene. This minimal supervision significantly expands the\npool of training data, enabling the creation of a highly generalized and robust\nkeypoint extractor.\n  RIPE utilizes the encoder's intermediate layers for the description of the\nkeypoints with a hyper-column approach to integrate information from different\nscales. Additionally, we propose an auxiliary loss to enhance the\ndiscriminative capability of the learned descriptors.\n  Comprehensive evaluations on standard benchmarks demonstrate that RIPE\nsimplifies data preparation while achieving competitive performance compared to\nstate-of-the-art techniques, marking a significant advancement in robust\nkeypoint extraction and description. To support further research, we have made\nour code publicly available at https://github.com/fraunhoferhhi/RIPE."
    },
    {
        "date": "2025-07",
        "title": "Enabling Security on the Edge: A CHERI Compartmentalized Network Stack",
        "author": "Donato Ferraro, Andrea Bastoni, Alexander Zuepke, and Andrea Marongiu",
        "link": "http://arxiv.org/abs/2507.04818v1",
        "abstract": "The widespread deployment of embedded systems in critical infrastructures,\ninterconnected edge devices like autonomous drones, and smart industrial\nsystems requires robust security measures. Compromised systems increase the\nrisks of operational failures, data breaches, and -- in safety-critical\nenvironments -- potential physical harm to people. Despite these risks, current\nsecurity measures are often insufficient to fully address the attack surfaces\nof embedded devices. CHERI provides strong security from the hardware level by\nenabling fine-grained compartmentalization and memory protection, which can\nreduce the attack surface and improve the reliability of such devices. In this\nwork, we explore the potential of CHERI to compartmentalize one of the most\ncritical and targeted components of interconnected systems: their network\nstack. Our case study examines the trade-offs of isolating applications, TCP/IP\nlibraries, and network drivers on a CheriBSD system deployed on the Arm Morello\nplatform. Our results suggest that CHERI has the potential to enhance security\nwhile maintaining performance in embedded-like environments."
    },
    {
        "date": "2025-07",
        "title": "Interaction-Merged Motion Planning: Effectively Leveraging Diverse Motion Datasets for Robust Planning",
        "author": "Giwon Lee, Wooseong Jeong, Daehee Park, Jaewoo Jeong, and Kuk-Jin Yoon",
        "link": "http://arxiv.org/abs/2507.04790v1",
        "abstract": "Motion planning is a crucial component of autonomous robot driving. While\nvarious trajectory datasets exist, effectively utilizing them for a target\ndomain remains challenging due to differences in agent interactions and\nenvironmental characteristics. Conventional approaches, such as domain\nadaptation or ensemble learning, leverage multiple source datasets but suffer\nfrom domain imbalance, catastrophic forgetting, and high computational costs.\nTo address these challenges, we propose Interaction-Merged Motion Planning\n(IMMP), a novel approach that leverages parameter checkpoints trained on\ndifferent domains during adaptation to the target domain. IMMP follows a\ntwo-step process: pre-merging to capture agent behaviors and interactions,\nsufficiently extracting diverse information from the source domain, followed by\nmerging to construct an adaptable model that efficiently transfers diverse\ninteractions to the target domain. Our method is evaluated on various planning\nbenchmarks and models, demonstrating superior performance compared to\nconventional approaches."
    },
    {
        "date": "2025-07",
        "title": "FedPall: Prototype-based Adversarial and Collaborative Learning for Federated Learning with Feature Drift",
        "author": "Yong Zhang, Feng Liang, Guanghu Yuan, Min Yang, Chengming Li, and Xiping Hu",
        "link": "http://arxiv.org/abs/2507.04781v1",
        "abstract": "Federated learning (FL) enables collaborative training of a global model in\nthe centralized server with data from multiple parties while preserving\nprivacy. However, data heterogeneity can significantly degrade the performance\nof the global model when each party uses datasets from different sources to\ntrain a local model, thereby affecting personalized local models. Among various\ncases of data heterogeneity, feature drift, feature space difference among\nparties, is prevalent in real-life data but remains largely unexplored. Feature\ndrift can distract feature extraction learning in clients and thus lead to poor\nfeature extraction and classification performance. To tackle the problem of\nfeature drift in FL, we propose FedPall, an FL framework that utilizes\nprototype-based adversarial learning to unify feature spaces and collaborative\nlearning to reinforce class information within the features. Moreover, FedPall\nleverages mixed features generated from global prototypes and local features to\nenhance the global classifier with classification-relevant information from a\nglobal perspective. Evaluation results on three representative feature-drifted\ndatasets demonstrate FedPall's consistently superior performance in\nclassification with feature-drifted data in the FL scenario."
    },
    {
        "date": "2025-07",
        "title": "Losing Control: Data Poisoning Attack on Guided Diffusion via ControlNet",
        "author": "Raz Lapid, and Almog Dubin",
        "link": "http://arxiv.org/abs/2507.04726v1",
        "abstract": "Text-to-image diffusion models have achieved remarkable success in\ntranslating textual prompts into high-fidelity images. ControlNets further\nextend these models by allowing precise, image-based conditioning (e.g., edge\nmaps, depth, pose), enabling fine-grained control over structure and style.\nHowever, their dependence on large, publicly scraped datasets -- and the\nincreasing use of community-shared data for fine-tuning -- exposes them to\nstealthy data poisoning attacks. In this work, we introduce a novel data\npoisoning method that manipulates ControlNets to generate images containing\nspecific content without any text triggers. By injecting poisoned samples --\neach pairing a subtly triggered input with an NSFW target -- the model retains\nclean-prompt fidelity yet reliably produces NSFW outputs when the trigger is\npresent. On large-scale, high-quality datasets, our backdoor achieves high\nattack success rate while remaining imperceptible in raw inputs. These results\nreveal a critical vulnerability in open-source ControlNets pipelines and\nunderscore the need for robust data sanitization and defense mechanisms."
    },
    {
        "date": "2025-07",
        "title": "Optimal Model Selection for Conformalized Robust Optimization",
        "author": "Yajie Bao, Yang Hu, Haojie Ren, Peng Zhao, and Changliang Zou",
        "link": "http://arxiv.org/abs/2507.04716v1",
        "abstract": "In decision-making under uncertainty, Contextual Robust Optimization (CRO)\nprovides reliability by minimizing the worst-case decision loss over a\nprediction set, hedging against label variability. While recent advances use\nconformal prediction to construct prediction sets for machine learning models,\nthe downstream decisions critically depend on model selection. This paper\nintroduces novel model selection frameworks for CRO that unify robustness\ncontrol with decision risk minimization. We first propose Conformalized Robust\nOptimization with Model Selection (CROMS), which automatically selects models\nto approximately minimize the average decision risk in CRO solutions. We\ndevelop two algorithms: E-CROMS, which is computationally efficient, and\nF-CROMS, which enjoys a marginal robustness guarantee in finite samples.\nFurther, we introduce Conformalized Robust Optimization with Individualized\nModel Selection (CROiMS), which performs individualized model selection by\nminimizing the conditional decision risk given the covariate of test data. This\nframework advances conformal prediction methodology by enabling covariate-aware\nmodel selection. Theoretically, CROiMS achieves asymptotic conditional\nrobustness and decision efficiency under mild assumptions. Numerical results\ndemonstrate significant improvements in decision efficiency and robustness\nacross diverse synthetic and real-world applications, outperforming baseline\napproaches."
    },
    {
        "date": "2025-07",
        "title": "Hybrid Adversarial Spectral Loss Conditional Generative Adversarial Networks for Signal Data Augmentation in Ultra-precision Machining Surface Roughness Prediction",
        "author": "Suiyan Shang, Chi Fai Cheung, and Pai Zheng",
        "link": "http://arxiv.org/abs/2507.04665v1",
        "abstract": "Accurate surface roughness prediction in ultra-precision machining (UPM) is\ncritical for real-time quality control, but small datasets hinder model\nperformance. We propose HAS-CGAN, a Hybrid Adversarial Spectral Loss CGAN, for\neffective UPM data augmentation. Among five CGAN variants tested, HAS-CGAN\nexcels in 1D force signal generation, particularly for high-frequency signals,\nachieving >0.85 wavelet coherence through Fourier-domain optimization. By\ncombining generated signals with machining parameters, prediction accuracy\nsignificantly improves. Experiments with traditional ML (SVR, RF, LSTM) and\ndeep learning models (BPNN, 1DCNN, CNN-Transformer) demonstrate that augmenting\ntraining data with 520+ synthetic samples reduces prediction error from 31.4%\n(original 52 samples) to ~9%, effectively addressing data scarcity in UPM\nroughness prediction.\""
    },
    {
        "date": "2025-07",
        "title": "Learning Robust Stereo Matching in the Wild with Selective Mixture-of-Experts",
        "author": "Yun Wang, Longguang Wang, Chenghao Zhang, Yongjian Zhang, Zhanjie Zhang, Ao Ma, Chenyou Fan, Tin Lun Lam, and Junjie Hu",
        "link": "http://arxiv.org/abs/2507.04631v1",
        "abstract": "Recently, learning-based stereo matching networks have advanced\nsignificantly. However, they often lack robustness and struggle to achieve\nimpressive cross-domain performance due to domain shifts and imbalanced\ndisparity distributions among diverse datasets. Leveraging Vision Foundation\nModels (VFMs) can intuitively enhance the model's robustness, but integrating\nsuch a model into stereo matching cost-effectively to fully realize their\nrobustness remains a key challenge. To address this, we propose SMoEStereo, a\nnovel framework that adapts VFMs for stereo matching through a tailored,\nscene-specific fusion of Low-Rank Adaptation (LoRA) and Mixture-of-Experts\n(MoE) modules. SMoEStereo introduces MoE-LoRA with adaptive ranks and\nMoE-Adapter with adaptive kernel sizes. The former dynamically selects optimal\nexperts within MoE to adapt varying scenes across domains, while the latter\ninjects inductive bias into frozen VFMs to improve geometric feature\nextraction. Importantly, to mitigate computational overhead, we further propose\na lightweight decision network that selectively activates MoE modules based on\ninput complexity, balancing efficiency with accuracy. Extensive experiments\ndemonstrate that our method exhibits state-of-the-art cross-domain and joint\ngeneralization across multiple benchmarks without dataset-specific adaptation.\nThe code is available at\n\\textcolor{red}{https://github.com/cocowy1/SMoE-Stereo}."
    },
    {
        "date": "2025-07",
        "title": "README: Robust Error-Aware Digital Signature Framework via Deep Watermarking Model",
        "author": "Hyunwook Choi, Sangyun Won, Daeyeon Hwang, and Junhyeok Choi",
        "link": "http://arxiv.org/abs/2507.04495v1",
        "abstract": "Deep learning-based watermarking has emerged as a promising solution for\nrobust image authentication and protection. However, existing models are\nlimited by low embedding capacity and vulnerability to bit-level errors, making\nthem unsuitable for cryptographic applications such as digital signatures,\nwhich require over 2048 bits of error-free data. In this paper, we propose\nREADME (Robust Error-Aware Digital Signature via Deep WaterMarking ModEl), a\nnovel framework that enables robust, verifiable, and error-tolerant digital\nsignatures within images. Our method combines a simple yet effective\ncropping-based capacity scaling mechanism with ERPA (ERror PAinting Module), a\nlightweight error correction module designed to localize and correct bit errors\nusing Distinct Circular Subsum Sequences (DCSS). Without requiring any\nfine-tuning of existing pretrained watermarking models, README significantly\nboosts the zero-bit-error image rate (Z.B.I.R) from 1.2% to 86.3% when\nembedding 2048-bit digital signatures into a single image, even under\nreal-world distortions. Moreover, our use of perceptual hash-based signature\nverification ensures public verifiability and robustness against tampering. The\nproposed framework unlocks a new class of high-assurance applications for deep\nwatermarking, bridging the gap between signal-level watermarking and\ncryptographic security."
    },
    {
        "date": "2025-07",
        "title": "Thousand-Brains Systems: Sensorimotor Intelligence for Rapid, Robust Learning and Inference",
        "author": "Niels Leadholm, Viviane Clay, Scott Knudstrup, Hojae Lee, and Jeff Hawkins",
        "link": "http://arxiv.org/abs/2507.04494v1",
        "abstract": "Current AI systems achieve impressive performance on many tasks, yet they\nlack core attributes of biological intelligence, including rapid, continual\nlearning, representations grounded in sensorimotor interactions, and structured\nknowledge that enables efficient generalization. Neuroscience theory suggests\nthat mammals evolved flexible intelligence through the replication of a\nsemi-independent, sensorimotor module, a functional unit known as a cortical\ncolumn. To address the disparity between biological and artificial\nintelligence, thousand-brains systems were proposed as a means of mirroring the\narchitecture of cortical columns and their interactions.\n  In the current work, we evaluate the unique properties of Monty, the first\nimplementation of a thousand-brains system. We focus on 3D object perception,\nand in particular, the combined task of object recognition and pose estimation.\nUtilizing the YCB dataset of household objects, we first assess Monty's use of\nsensorimotor learning to build structured representations, finding that these\nenable robust generalization. These representations include an emphasis on\nclassifying objects by their global shape, as well as a natural ability to\ndetect object symmetries. We then explore Monty's use of model-free and\nmodel-based policies to enable rapid inference by supporting principled\nmovements. We find that such policies complement Monty's modular architecture,\na design that can accommodate communication between modules to further\naccelerate inference speed via a novel `voting' algorithm. Finally, we examine\nMonty's use of associative, Hebbian-like binding to enable rapid, continual,\nand computationally efficient learning, properties that compare favorably to\ncurrent deep learning architectures. While Monty is still in a nascent stage of\ndevelopment, these findings support thousand-brains systems as a powerful and\npromising new approach to AI."
    },
    {
        "date": "2025-07",
        "title": "A validity-guided workflow for robust large language model research in psychology",
        "author": "Zhicheng Lin",
        "link": "http://arxiv.org/abs/2507.04491v1",
        "abstract": "Large language models (LLMs) are rapidly being integrated into psychological\nresearch as research tools, evaluation targets, human simulators, and cognitive\nmodels. However, recent evidence reveals severe measurement unreliability:\nPersonality assessments collapse under factor analysis, moral preferences\nreverse with punctuation changes, and theory-of-mind accuracy varies widely\nwith trivial rephrasing. These \"measurement phantoms\"--statistical artifacts\nmasquerading as psychological phenomena--threaten the validity of a growing\nbody of research. Guided by the dual-validity framework that integrates\npsychometrics with causal inference, we present a six-stage workflow that\nscales validity requirements to research ambition--using LLMs to code text\nrequires basic reliability and accuracy, while claims about psychological\nproperties demand comprehensive construct validation. Researchers must (1)\nexplicitly define their research goal and corresponding validity requirements,\n(2) develop and validate computational instruments through psychometric\ntesting, (3) design experiments that control for computational confounds, (4)\nexecute protocols with transparency, (5) analyze data using methods appropriate\nfor non-independent observations, and (6) report findings within demonstrated\nboundaries and use results to refine theory. We illustrate the workflow through\nan example of model evaluation--\"LLM selfhood\"--showing how systematic\nvalidation can distinguish genuine computational phenomena from measurement\nartifacts. By establishing validated computational instruments and transparent\npractices, this workflow provides a path toward building a robust empirical\nfoundation for AI psychology research."
    },
    {
        "date": "2025-07",
        "title": "Model Inversion Attacks on Llama 3: Extracting PII from Large Language Models",
        "author": "Sathesh P. Sivashanmugam",
        "link": "http://arxiv.org/abs/2507.04478v1",
        "abstract": "Large language models (LLMs) have transformed natural language processing,\nbut their ability to memorize training data poses significant privacy risks.\nThis paper investigates model inversion attacks on the Llama 3.2 model, a\nmultilingual LLM developed by Meta. By querying the model with carefully\ncrafted prompts, we demonstrate the extraction of personally identifiable\ninformation (PII) such as passwords, email addresses, and account numbers. Our\nfindings highlight the vulnerability of even smaller LLMs to privacy attacks\nand underscore the need for robust defenses. We discuss potential mitigation\nstrategies, including differential privacy and data sanitization, and call for\nfurther research into privacy-preserving machine learning techniques."
    },
    {
        "date": "2025-07",
        "title": "Tail-aware Adversarial Attacks: A Distributional Approach to Efficient LLM Jailbreaking",
        "author": "Tim Beyer, Yan Scholten, Leo Schwinn, and Stephan G\u00fcnnemann",
        "link": "http://arxiv.org/abs/2507.04446v2",
        "abstract": "To guarantee safe and robust deployment of large language models (LLMs) at\nscale, it is critical to accurately assess their adversarial robustness.\nExisting adversarial attacks typically target harmful responses in\nsingle-point, greedy generations, overlooking the inherently stochastic nature\nof LLMs. In this paper, we propose a novel framework for adversarial robustness\nevaluation that explicitly models the entire output distribution, including\ntail-risks, providing better estimates for model robustness at scale. By\ncasting the attack process as a resource allocation problem between\noptimization and sampling, we determine compute-optimal tradeoffs and show that\nintegrating sampling into existing attacks boosts ASR by up to 48% and improves\nefficiency by up to two orders of magnitude. Our framework also enables us to\nanalyze how different attack algorithms affect output harm distributions.\nSurprisingly, we find that most optimization strategies have little effect on\noutput harmfulness. Finally, we introduce a data-free proof-of-concept\nobjective based on entropy-maximization to demonstrate how our tail-aware\nperspective enables new optimization targets. Overall, our findings highlight\nthe importance of tail-aware attacks and evaluation protocols to accurately\nassess and strengthen LLM safety."
    },
    {
        "date": "2025-07",
        "title": "Attention Slipping: A Mechanistic Understanding of Jailbreak Attacks and Defenses in LLMs",
        "author": "Xiaomeng Hu, Pin-Yu Chen, and Tsung-Yi Ho",
        "link": "http://arxiv.org/abs/2507.04365v1",
        "abstract": "As large language models (LLMs) become more integral to society and\ntechnology, ensuring their safety becomes essential. Jailbreak attacks exploit\nvulnerabilities to bypass safety guardrails, posing a significant threat.\nHowever, the mechanisms enabling these attacks are not well understood. In this\npaper, we reveal a universal phenomenon that occurs during jailbreak attacks:\nAttention Slipping. During this phenomenon, the model gradually reduces the\nattention it allocates to unsafe requests in a user query during the attack\nprocess, ultimately causing a jailbreak. We show Attention Slipping is\nconsistent across various jailbreak methods, including gradient-based token\nreplacement, prompt-level template refinement, and in-context learning.\nAdditionally, we evaluate two defenses based on query perturbation, Token\nHighlighter and SmoothLLM, and find they indirectly mitigate Attention\nSlipping, with their effectiveness positively correlated with the degree of\nmitigation achieved. Inspired by this finding, we propose Attention Sharpening,\na new defense that directly counters Attention Slipping by sharpening the\nattention score distribution using temperature scaling. Experiments on four\nleading LLMs (Gemma2-9B-It, Llama3.1-8B-It, Qwen2.5-7B-It, Mistral-7B-It v0.2)\nshow that our method effectively resists various jailbreak attacks while\nmaintaining performance on benign tasks on AlpacaEval. Importantly, Attention\nSharpening introduces no additional computational or memory overhead, making it\nan efficient and practical solution for real-world deployment."
    },
    {
        "date": "2025-07",
        "title": "Adversarial Data Augmentation for Single Domain Generalization via Lyapunov Exponent-Guided Optimization",
        "author": "Zuyu Zhang, Ning Chen, Yongshan Liu, Qinghua Zhang, and Xu Zhang",
        "link": "http://arxiv.org/abs/2507.04302v1",
        "abstract": "Single Domain Generalization (SDG) aims to develop models capable of\ngeneralizing to unseen target domains using only one source domain, a task\ncomplicated by substantial domain shifts and limited data diversity. Existing\nSDG approaches primarily rely on data augmentation techniques, which struggle\nto effectively adapt training dynamics to accommodate large domain shifts. To\naddress this, we propose LEAwareSGD, a novel Lyapunov Exponent (LE)-guided\noptimization approach inspired by dynamical systems theory. By leveraging LE\nmeasurements to modulate the learning rate, LEAwareSGD encourages model\ntraining near the edge of chaos, a critical state that optimally balances\nstability and adaptability. This dynamic adjustment allows the model to explore\na wider parameter space and capture more generalizable features, ultimately\nenhancing the model's generalization capability. Extensive experiments on PACS,\nOfficeHome, and DomainNet demonstrate that LEAwareSGD yields substantial\ngeneralization gains, achieving up to 9.47\\% improvement on PACS in low-data\nregimes. These results underscore the effectiveness of training near the edge\nof chaos for enhancing model generalization capability in SDG tasks."
    },
    {
        "date": "2025-07",
        "title": "ML-Enhanced AES Anomaly Detection for Real-Time Embedded Security",
        "author": "Nishant Chinnasami, Rye Stahle-Smith, and Rasha Karakchi",
        "link": "http://arxiv.org/abs/2507.04197v1",
        "abstract": "Advanced Encryption Standard (AES) is a widely adopted cryptographic\nalgorithm, yet its practical implementations remain susceptible to side-channel\nand fault injection attacks. In this work, we propose a comprehensive framework\nthat enhances AES-128 encryption security through controlled anomaly injection\nand real-time anomaly detection using both statistical and machine learning\n(ML) methods. We simulate timing and fault-based anomalies by injecting\nexecution delays and ciphertext perturbations during encryption, generating\nlabeled datasets for detection model training. Two complementary detection\nmechanisms are developed: a threshold-based timing anomaly detector and a\nsupervised Random Forest classifier trained on combined timing and ciphertext\nfeatures. We implement and evaluate the framework on both CPU and FPGA-based\nSoC hardware (PYNQ-Z1), measuring performance across varying block sizes,\ninjection rates, and core counts. Our results show that ML-based detection\nsignificantly outperforms threshold-based methods in precision and recall while\nmaintaining real-time performance on embedded hardware. Compared to existing\nAES anomaly detection methods, our solution offers a low-cost, real-time, and\naccurate detection approach deployable on lightweight FPGA platforms."
    },
    {
        "date": "2025-07",
        "title": "Integrated Gaussian Processes for Robust and Adaptive Multi-Object Tracking",
        "author": "Fred Lydeard, Bashar I. Ahmad, and Simon Godsill",
        "link": "http://arxiv.org/abs/2507.04116v1",
        "abstract": "This paper presents a computationally efficient multi-object tracking\napproach that can minimise track breaks (e.g., in challenging environments and\nagainst agile targets), learn the measurement model parameters on-line (e.g.,\nin dynamically changing scenes) and infer the class of the tracked objects, if\njoint tracking and kinematic behaviour classification is sought. It capitalises\non the flexibilities offered by the integrated Gaussian process as a motion\nmodel and the convenient statistical properties of non-homogeneous Poisson\nprocesses as a suitable observation model. This can be combined with the\nproposed effective track revival / stitching mechanism. We accordingly\nintroduce the two robust and adaptive trackers, Gaussian and Poisson Process\nwith Classification (GaPP-Class) and GaPP with Revival and Classification\n(GaPP-ReaCtion). They employ an appropriate particle filtering inference scheme\nthat efficiently integrates track management and hyperparameter learning\n(including the object class, if relevant). GaPP-ReaCtion extends GaPP-Class\nwith the addition of a Markov Chain Monte Carlo kernel applied to each particle\npermitting track revival and stitching (e.g., within a few time steps after\ndeleting a trajectory). Performance evaluation and benchmarking using synthetic\nand real data show that GaPP-Class and GaPP-ReaCtion outperform other\nstate-of-the-art tracking algorithms. For example, GaPP-ReaCtion significantly\nreduces track breaks (e.g., by around 30% from real radar data and markedly\nmore from simulated data)."
    },
    {
        "date": "2025-07",
        "title": "Enhancing Robustness of LLM-Driven Multi-Agent Systems through Randomized Smoothing",
        "author": "Jinwei Hu, Yi Dong, Zhengtao Ding, and Xiaowei Huang",
        "link": "http://arxiv.org/abs/2507.04105v1",
        "abstract": "This paper presents a defense framework for enhancing the safety of large\nlanguage model (LLM) empowered multi-agent systems (MAS) in safety-critical\ndomains such as aerospace. We apply randomized smoothing, a statistical\nrobustness certification technique, to the MAS consensus context, enabling\nprobabilistic guarantees on agent decisions under adversarial influence. Unlike\ntraditional verification methods, our approach operates in black-box settings\nand employs a two-stage adaptive sampling mechanism to balance robustness and\ncomputational efficiency. Simulation results demonstrate that our method\neffectively prevents the propagation of adversarial behaviors and\nhallucinations while maintaining consensus performance. This work provides a\npractical and scalable path toward safe deployment of LLM-based MAS in\nreal-world, high-stakes environments."
    },
    {
        "date": "2025-07",
        "title": "S-Leak: Leakage-Abuse Attack Against Efficient Conjunctive SSE via s-term Leakage",
        "author": "Yue Su, Meng Shen, Cong Zuo, Yuzhi Liu, and Liehuang Zhu",
        "link": "http://arxiv.org/abs/2507.04077v1",
        "abstract": "Conjunctive Searchable Symmetric Encryption (CSSE) enables secure conjunctive\nsearches over encrypted data. While leakage-abuse attacks (LAAs) against\nsingle-keyword SSE have been extensively studied, their extension to\nconjunctive queries faces a critical challenge: the combinatorial explosion of\ncandidate keyword combinations, leading to enormous time and space overhead for\nattacks. In this paper, we reveal a fundamental vulnerability in\nstate-of-the-art CSSE schemes: s-term leakage, where the keyword with the\nminimal document frequency in a query leaks distinct patterns. We propose\nS-Leak, the first passive attack framework that progressively recovers\nconjunctive queries by exploiting s-term leakage and global leakage. Our key\ninnovation lies in a three-stage approach: identifying the s-term of queries,\npruning low-probability keyword conjunctions, and reconstructing full queries.\nWe propose novel metrics to better assess attacks in conjunctive query\nscenarios. Empirical evaluations on real-world datasets demonstrate that our\nattack is effective in diverse CSSE configurations. When considering 161,700\nconjunctive keyword queries, our attack achieves a 95.15% accuracy in\nrecovering at least one keyword, 82.57% for at least two, 58% for all three\nkeywords, and maintains efficacy against defenses such as SEAL padding and CLRZ\nobfuscation. Our work exposes the underestimated risks of s-term leakage in\npractical SSE deployments and calls for a redesign of leakage models for\nmulti-keyword search scenarios."
    },
    {
        "date": "2025-07",
        "title": "Robust Low-light Scene Restoration via Illumination Transition",
        "author": "Ze Li, Feng Zhang, Xiatian Zhu, Meng Zhang, Yanghong Zhou, and P. Y. Mok",
        "link": "http://arxiv.org/abs/2507.03976v1",
        "abstract": "Synthesizing normal-light novel views from low-light multiview images is an\nimportant yet challenging task, given the low visibility and high ISO noise\npresent in the input images. Existing low-light enhancement methods often\nstruggle to effectively preprocess such low-light inputs, as they fail to\nconsider correlations among multiple views. Although other state-of-the-art\nmethods have introduced illumination-related components offering alternative\nsolutions to the problem, they often result in drawbacks such as color\ndistortions and artifacts, and they provide limited denoising effectiveness. In\nthis paper, we propose a novel Robust Low-light Scene Restoration framework\n(RoSe), which enables effective synthesis of novel views in normal lighting\nconditions from low-light multiview image inputs, by formulating the task as an\nilluminance transition estimation problem in 3D space, conceptualizing it as a\nspecialized rendering task. This multiview-consistent illuminance transition\nfield establishes a robust connection between low-light and normal-light\nconditions. By further exploiting the inherent low-rank property of\nillumination to constrain the transition representation, we achieve more\neffective denoising without complex 2D techniques or explicit noise modeling.\nTo implement RoSe, we design a concise dual-branch architecture and introduce a\nlow-rank denoising module. Experiments demonstrate that RoSe significantly\noutperforms state-of-the-art models in both rendering quality and multiview\nconsistency on standard benchmarks. The codes and data are available at\nhttps://pegasus2004.github.io/RoSe."
    },
    {
        "date": "2025-07",
        "title": "A Survey on Proactive Defense Strategies Against Misinformation in Large Language Models",
        "author": "Shuliang Liu, Hongyi Liu, Aiwei Liu, Bingchen Duan, Qi Zheng, Yibo Yan, He Geng, Peijie Jiang, Jia Liu, and Xuming Hu",
        "link": "http://arxiv.org/abs/2507.05288v1",
        "abstract": "The widespread deployment of large language models (LLMs) across critical\ndomains has amplified the societal risks posed by algorithmically generated\nmisinformation. Unlike traditional false content, LLM-generated misinformation\ncan be self-reinforcing, highly plausible, and capable of rapid propagation\nacross multiple languages, which traditional detection methods fail to mitigate\neffectively. This paper introduces a proactive defense paradigm, shifting from\npassive post hoc detection to anticipatory mitigation strategies. We propose a\nThree Pillars framework: (1) Knowledge Credibility, fortifying the integrity of\ntraining and deployed data; (2) Inference Reliability, embedding\nself-corrective mechanisms during reasoning; and (3) Input Robustness,\nenhancing the resilience of model interfaces against adversarial attacks.\nThrough a comprehensive survey of existing techniques and a comparative\nmeta-analysis, we demonstrate that proactive defense strategies offer up to\n63\\% improvement over conventional methods in misinformation prevention,\ndespite non-trivial computational overhead and generalization challenges. We\nargue that future research should focus on co-designing robust knowledge\nfoundations, reasoning certification, and attack-resistant interfaces to ensure\nLLMs can effectively counter misinformation across varied domains."
    },
    {
        "date": "2025-07",
        "title": "Evaluating Adversarial Protections for Diffusion Personalization: A Comprehensive Study",
        "author": "Kai Ye, Tianyi Chen, and Zhen Wang",
        "link": "http://arxiv.org/abs/2507.03953v1",
        "abstract": "With the increasing adoption of diffusion models for image generation and\npersonalization, concerns regarding privacy breaches and content misuse have\nbecome more pressing. In this study, we conduct a comprehensive comparison of\neight perturbation based protection methods: AdvDM, ASPL, FSGM, MetaCloak,\nMist, PhotoGuard, SDS, and SimAC--across both portrait and artwork domains.\nThese methods are evaluated under varying perturbation budgets, using a range\nof metrics to assess visual imperceptibility and protective efficacy. Our\nresults offer practical guidance for method selection. Code is available at:\nhttps://github.com/vkeilo/DiffAdvPerturbationBench."
    },
    {
        "date": "2025-07",
        "title": "FastDINOv2: Frequency Based Curriculum Learning Improves Robustness and Training Speed",
        "author": "Jiaqi Zhang, Juntuo Wang, Zhixin Sun, John Zou, and Randall Balestriero",
        "link": "http://arxiv.org/abs/2507.03779v1",
        "abstract": "Large-scale vision foundation models such as DINOv2 boast impressive\nperformances by leveraging massive architectures and training datasets. But\nnumerous scenarios require practitioners to reproduce those pre-training\nsolutions, such as on private data, new modalities, or simply for scientific\nquestioning--which is currently extremely demanding computation-wise. We thus\npropose a novel pre-training strategy for DINOv2 that simultaneously\naccelerates convergence--and strengthens robustness to common corruptions as a\nby-product. Our approach involves a frequency filtering\ncurriculum--low-frequency being seen first--and the Gaussian noise patching\naugmentation. Applied to a ViT-B/16 backbone trained on ImageNet-1K, while\npre-training time and FLOPs are reduced by 1.6x and 2.25x, our method still\nachieves matching robustness in corruption benchmarks (ImageNet-C) and\nmaintains competitive linear probing performance compared with baseline. This\ndual benefit of efficiency and robustness makes large-scale self-supervised\nfoundation modeling more attainable, while opening the door to novel\nexploration around data curriculum and augmentation as means to improve\nself-supervised learning models robustness. The code is available at\nhttps://github.com/KevinZ0217/fast_dinov2"
    },
    {
        "date": "2025-07",
        "title": "Robust estimation of heterogeneous treatment effects in randomized trials leveraging external data",
        "author": "Rickard Karlsson, Piersilvio De Bartolomeis, Issa J. Dahabreh, and Jesse H. Krijthe",
        "link": "http://arxiv.org/abs/2507.03681v1",
        "abstract": "Randomized trials are typically designed to detect average treatment effects\nbut often lack the statistical power to uncover effect heterogeneity over\npatient characteristics, limiting their value for personalized decision-making.\nTo address this, we propose the QR-learner, a model-agnostic learner that\nestimates conditional average treatment effects (CATE) within the trial\npopulation by leveraging external data from other trials or observational\nstudies. The proposed method is robust: it has the potential to reduce the CATE\nprediction mean squared error while maintaining consistency, even when the\nexternal data is not aligned with the trial. Moreover, we introduce a procedure\nthat combines the QR-learner with a trial-only CATE learner and show that it\nasymptotically matches or exceeds the trial-only learner in terms of mean\nsquared error. We examine the performance of our approach in simulation studies\nand apply the methods to a real-world dataset, demonstrating improvements in\nboth CATE estimation and statistical power for detecting heterogeneous effects."
    },
    {
        "date": "2025-07",
        "title": "RECA-PD: A Robust Explainable Cross-Attention Method for Speech-based Parkinson's Disease Classification",
        "author": "Terry Yi Zhong, Cristian Tejedor-Garcia, Martha Larson, and Bastiaan R. Bloem",
        "link": "http://arxiv.org/abs/2507.03594v1",
        "abstract": "Parkinson's Disease (PD) affects over 10 million people globally, with speech\nimpairments often preceding motor symptoms by years, making speech a valuable\nmodality for early, non-invasive detection. While recent deep-learning models\nachieve high accuracy, they typically lack the explainability required for\nclinical use. To address this, we propose RECA-PD, a novel, robust, and\nexplainable cross-attention architecture that combines interpretable speech\nfeatures with self-supervised representations. RECA-PD matches state-of-the-art\nperformance in Speech-based PD detection while providing explanations that are\nmore consistent and more clinically meaningful. Additionally, we demonstrate\nthat performance degradation in certain speech tasks (e.g., monologue) can be\nmitigated by segmenting long recordings. Our findings indicate that performance\nand explainability are not necessarily mutually exclusive. Future work will\nenhance the usability of explanations for non-experts and explore severity\nestimation to increase the real-world clinical relevance."
    },
    {
        "date": "2025-07",
        "title": "Causal-SAM-LLM: Large Language Models as Causal Reasoners for Robust Medical Segmentation",
        "author": "Tao Tang, Shijie Xu, Yiting Wu, and Zhixiang Lu",
        "link": "http://arxiv.org/abs/2507.03585v1",
        "abstract": "The clinical utility of deep learning models for medical image segmentation\nis severely constrained by their inability to generalize to unseen domains.\nThis failure is often rooted in the models learning spurious correlations\nbetween anatomical content and domain-specific imaging styles. To overcome this\nfundamental challenge, we introduce Causal-SAM-LLM, a novel framework that\nelevates Large Language Models (LLMs) to the role of causal reasoners. Our\nframework, built upon a frozen Segment Anything Model (SAM) encoder,\nincorporates two synergistic innovations. First, Linguistic Adversarial\nDisentanglement (LAD) employs a Vision-Language Model to generate rich, textual\ndescriptions of confounding image styles. By training the segmentation model's\nfeatures to be contrastively dissimilar to these style descriptions, it learns\na representation robustly purged of non-causal information. Second, Test-Time\nCausal Intervention (TCI) provides an interactive mechanism where an LLM\ninterprets a clinician's natural language command to modulate the segmentation\ndecoder's features in real-time, enabling targeted error correction. We conduct\nan extensive empirical evaluation on a composite benchmark from four public\ndatasets (BTCV, CHAOS, AMOS, BraTS), assessing generalization under\ncross-scanner, cross-modality, and cross-anatomy settings. Causal-SAM-LLM\nestablishes a new state of the art in out-of-distribution (OOD) robustness,\nimproving the average Dice score by up to 6.2 points and reducing the Hausdorff\nDistance by 15.8 mm over the strongest baseline, all while using less than 9%\nof the full model's trainable parameters. Our work charts a new course for\nbuilding robust, efficient, and interactively controllable medical AI systems."
    },
    {
        "date": "2025-07",
        "title": "Beyond Weaponization: NLP Security for Medium and Lower-Resourced Languages in Their Own Right",
        "author": "Heather Lent",
        "link": "http://arxiv.org/abs/2507.03473v1",
        "abstract": "Despite mounting evidence that multilinguality can be easily weaponized\nagainst language models (LMs), works across NLP Security remain overwhelmingly\nEnglish-centric. In terms of securing LMs, the NLP norm of \"English first\"\ncollides with standard procedure in cybersecurity, whereby practitioners are\nexpected to anticipate and prepare for worst-case outcomes. To mitigate\nworst-case outcomes in NLP Security, researchers must be willing to engage with\nthe weakest links in LM security: lower-resourced languages. Accordingly, this\nwork examines the security of LMs for lower- and medium-resourced languages. We\nextend existing adversarial attacks for up to 70 languages to evaluate the\nsecurity of monolingual and multilingual LMs for these languages. Through our\nanalysis, we find that monolingual models are often too small in total number\nof parameters to ensure sound security, and that while multilinguality is\nhelpful, it does not always guarantee improved security either. Ultimately,\nthese findings highlight important considerations for more secure deployment of\nLMs, for communities of lower-resourced languages."
    },
    {
        "date": "2025-07",
        "title": "Evaluating the Evaluators: Trust in Adversarial Robustness Tests",
        "author": "Antonio Emanuele Cin\u00e0, Maura Pintor, Luca Demetrio, Ambra Demontis, Battista Biggio, and Fabio Roli",
        "link": "http://arxiv.org/abs/2507.03450v1",
        "abstract": "Despite significant progress in designing powerful adversarial evasion\nattacks for robustness verification, the evaluation of these methods often\nremains inconsistent and unreliable. Many assessments rely on mismatched\nmodels, unverified implementations, and uneven computational budgets, which can\nlead to biased results and a false sense of security. Consequently, robustness\nclaims built on such flawed testing protocols may be misleading and give a\nfalse sense of security. As a concrete step toward improving evaluation\nreliability, we present AttackBench, a benchmark framework developed to assess\nthe effectiveness of gradient-based attacks under standardized and reproducible\nconditions. AttackBench serves as an evaluation tool that ranks existing attack\nimplementations based on a novel optimality metric, which enables researchers\nand practitioners to identify the most reliable and effective attack for use in\nsubsequent robustness evaluations. The framework enforces consistent testing\nconditions and enables continuous updates, making it a reliable foundation for\nrobustness verification."
    },
    {
        "date": "2025-07",
        "title": "Unlearning the Noisy Correspondence Makes CLIP More Robust",
        "author": "Haochen Han, Alex Jinpeng Wang, Peijun Ye, and Fangming Liu",
        "link": "http://arxiv.org/abs/2507.03434v1",
        "abstract": "The data appetite for Vision-Language Models (VLMs) has continuously scaled\nup from the early millions to billions today, which faces an untenable\ntrade-off with data quality and inevitably introduces Noisy Correspondence (NC)\nsamples. Undoubtedly, such semantically unrelated data significantly impairs\nthe performance of VLMs. Previous efforts mainly address this challenge by\nestimating refined alignment for more precise guidance. However, such\nresource-intensive pipelines that train VLMs from scratch struggle to meet\nrealistic data demands. In this paper, we present a brand new perspective that\nseeks to directly eliminate the harmful effects of NC in pre-trained VLMs.\nSpecifically, we propose NCU, a Noisy Correspondence Unlearning fine-tuning\nframework that efficiently enhances VLMs' robustness by forgetting learned\nnoisy knowledge. The key to NCU is learning the hardest negative information,\nwhich can provide explicit unlearning direction for both false positives and\nfalse negatives. Such twin goals unlearning process can be formalized into one\nunified optimal transport objective for fast fine-tuning. We validate our\napproach with the prevailing CLIP model over various downstream tasks.\nRemarkably, NCU surpasses the robust pre-trained method on zero-shot transfer\nwhile with lower computational overhead. The code will be released upon\nacceptance."
    },
    {
        "date": "2025-07",
        "title": "Rectifying Adversarial Sample with Low Entropy Prior for Test-Time Defense",
        "author": "Lina Ma, Xiaowei Fu, Fuxiang Huang, Xinbo Gao, and Lei Zhang",
        "link": "http://arxiv.org/abs/2507.03427v1",
        "abstract": "Existing defense methods fail to defend against unknown attacks and thus\nraise generalization issue of adversarial robustness. To remedy this problem,\nwe attempt to delve into some underlying common characteristics among various\nattacks for generality. In this work, we reveal the commonly overlooked low\nentropy prior (LE) implied in various adversarial samples, and shed light on\nthe universal robustness against unseen attacks in inference phase. LE prior is\nelaborated as two properties across various attacks as shown in Fig. 1 and Fig.\n2: 1) low entropy misclassification for adversarial samples and 2) lower\nentropy prediction for higher attack intensity. This phenomenon stands in stark\ncontrast to the naturally distributed samples. The LE prior can instruct\nexisting test-time defense methods, thus we propose a two-stage REAL approach:\nRectify Adversarial sample based on LE prior for test-time adversarial\nrectification. Specifically, to align adversarial samples more closely with\nclean samples, we propose to first rectify adversarial samples misclassified\nwith low entropy by reverse maximizing prediction entropy, thereby eliminating\ntheir adversarial nature. To ensure the rectified samples can be correctly\nclassified with low entropy, we carry out secondary rectification by forward\nminimizing prediction entropy, thus creating a Max-Min entropy optimization\nscheme. Further, based on the second property, we propose an attack-aware\nweighting mechanism to adaptively adjust the strengths of Max-Min entropy\nobjectives. Experiments on several datasets show that REAL can greatly improve\nthe performance of existing sample rectification models."
    },
    {
        "date": "2025-07",
        "title": "Action Robust Reinforcement Learning via Optimal Adversary Aware Policy Optimization",
        "author": "Buqing Nie, Yangqing Fu, Jingtian Ji, and Yue Gao",
        "link": "http://arxiv.org/abs/2507.03372v1",
        "abstract": "Reinforcement Learning (RL) has achieved remarkable success in sequential\ndecision tasks. However, recent studies have revealed the vulnerability of RL\npolicies to different perturbations, raising concerns about their effectiveness\nand safety in real-world applications. In this work, we focus on the robustness\nof RL policies against action perturbations and introduce a novel framework\ncalled Optimal Adversary-aware Policy Iteration (OA-PI). Our framework enhances\naction robustness under various perturbations by evaluating and improving\npolicy performance against the corresponding optimal adversaries. Besides, our\napproach can be integrated into mainstream DRL algorithms such as Twin Delayed\nDDPG (TD3) and Proximal Policy Optimization (PPO), improving action robustness\neffectively while maintaining nominal performance and sample efficiency.\nExperimental results across various environments demonstrate that our method\nenhances robustness of DRL policies against different action adversaries\neffectively."
    },
    {
        "date": "2025-07",
        "title": "Securing Mixed Rust with Hardware Capabilities",
        "author": "Jason Zhijingcheng Yu, Fangqi Han, Kaustab Choudhury, Trevor E. Carlson, and Prateek Saxena",
        "link": "http://arxiv.org/abs/2507.03344v1",
        "abstract": "The Rust programming language enforces three basic Rust principles, namely\nownership, borrowing, and AXM (Aliasing Xor Mutability) to prevent security\nbugs such as memory safety violations and data races. However, Rust projects\noften have mixed code, i.e., code that also uses unsafe Rust, FFI (Foreign\nFunction Interfaces), and inline assembly for low-level control. The Rust\ncompiler is unable to statically enforce Rust principles in mixed Rust code\nwhich can lead to many security vulnerabilities. In this paper, we propose\nCapsLock, a security enforcement mechanism that can run at the level of machine\ncode and detect Rust principle violations at run-time in mixed code. CapsLock\nis kept simple enough to be implemented into recent capability-based hardware\nabstractions that provide low-cost spatial memory safety. CapsLock introduces a\nnovel revoke-on-use abstraction for capability-based designs, wherein accessing\na memory object via a capability implicitly invalidates certain other\ncapabilities pointing to it, thereby also providing temporal memory safety\nautomatically, without requiring software to explicitly specify such\ninvalidation. Thus, CapsLock is the first mechanism capable of providing\ncross-language enforcement of Rust principles. We implemented a prototype of\nCapsLock on QEMU. Evaluation results show that CapsLock is highly compatible\nwith existing Rust code (passing 99.7% of the built-in test cases of the 100\nmost popular crates) and flags Rust principle violations in real-world Rust\nprojects that use FFI or inline assembly. We discovered 8 previously unknown\nbugs in such crates in our experiments."
    },
    {
        "date": "2025-07",
        "title": "UltraDfeGAN: Detail-Enhancing Generative Adversarial Networks for High-Fidelity Functional Ultrasound Synthesis",
        "author": "Zhuo Li, Xuhang Chen, and Shuqiang Wang",
        "link": "http://arxiv.org/abs/2507.03341v1",
        "abstract": "Functional ultrasound (fUS) is a neuroimaging technique known for its high\nspatiotemporal resolution, enabling non-invasive observation of brain activity\nthrough neurovascular coupling. Despite its potential in clinical applications\nsuch as neonatal monitoring and intraoperative guidance, the development of fUS\nfaces challenges related to data scarcity and limitations in generating\nrealistic fUS images. This paper explores the use of a generative adversarial\nnetwork (GAN) framework tailored for fUS image synthesis. The proposed method\nincorporates architectural enhancements, including feature enhancement modules\nand normalization techniques, aiming to improve the fidelity and physiological\nplausibility of generated images. The study evaluates the performance of the\nframework against existing generative models, demonstrating its capability to\nproduce high-quality fUS images under various experimental conditions.\nAdditionally, the synthesized images are assessed for their utility in\ndownstream tasks, showing improvements in classification accuracy when used for\ndata augmentation. Experimental results are based on publicly available fUS\ndatasets, highlighting the framework's effectiveness in addressing data\nlimitations."
    },
    {
        "date": "2025-07",
        "title": "Global Variational Inference Enhanced Robust Domain Adaptation",
        "author": "Lingkun Luo, Shiqiang Hu, and Liming Chen",
        "link": "http://arxiv.org/abs/2507.03291v1",
        "abstract": "Deep learning-based domain adaptation (DA) methods have shown strong\nperformance by learning transferable representations. However, their reliance\non mini-batch training limits global distribution modeling, leading to unstable\nalignment and suboptimal generalization. We propose Global Variational\nInference Enhanced Domain Adaptation (GVI-DA), a framework that learns\ncontinuous, class-conditional global priors via variational inference to enable\nstructure-aware cross-domain alignment. GVI-DA minimizes domain gaps through\nlatent feature reconstruction, and mitigates posterior collapse using global\ncodebook learning with randomized sampling. It further improves robustness by\ndiscarding low-confidence pseudo-labels and generating reliable target-domain\nsamples. Extensive experiments on four benchmarks and thirty-eight DA tasks\ndemonstrate consistent state-of-the-art performance. We also derive the model's\nevidence lower bound (ELBO) and analyze the effects of prior continuity,\ncodebook size, and pseudo-label noise tolerance. In addition, we compare GVI-DA\nwith diffusion-based generative frameworks in terms of optimization principles\nand efficiency, highlighting both its theoretical soundness and practical\nadvantages."
    },
    {
        "date": "2025-07",
        "title": "Securing Transformer-based AI Execution via Unified TEE and Crypto-protected Accelerators",
        "author": "Jiaqi Xue, Yifei Zhao, Mengxin Zheng, Xun Chen, Fan Yao, Yan Solihin, and Qian Lou",
        "link": "http://arxiv.org/abs/2507.03278v1",
        "abstract": "Recent advances in Transformer models, e.g., large language models (LLMs),\nhave brought tremendous breakthroughs in various artificial intelligence (AI)\ntasks, leading to their wide applications in many security-critical domains.\nDue to their unprecedented scale and prohibitively high development cost, these\nmodels have become highly valuable intellectual property for AI stakeholders\nand are increasingly deployed via machine learning as a service (MLaaS).\nHowever, MLaaS often runs on untrusted cloud infrastructure, exposing data and\nmodels to potential breaches. Mainstream protection mechanisms leverage trusted\nexecution environments (TEEs) where confidentiality and integrity for secretive\ndata are shielded using hardware-based encryption and integrity checking.\nUnfortunately, running model inference entirely within TEEs is subject to\nnon-trivial slowdown, which is further exacerbated in LLMs due to the\nsubstantial computation and memory footprint involved. Recent studies reveal\nthat the hybrid TEE-based scheme offloading partial model inference operations\nto the untrusted accelerators (e.g., GPU) is a promising solution. However,\nprior offloading schemes fail to ensure dual protection of data and model in\nTransformer inference, as they cannot securely offload critical operations,\ni.e., Attention and SoftMax, forcing these computations to remain confined\nwithin TEEs. To address these challenges, we propose TwinShield, a framework\nenabling secure Transformer inference in heterogeneous TEE and accelerator\nsystems with dual protection for both model and data. TwinShield offloads ~87%\nof computation to GPUs and delivers 4.0x - 6.1x speedups over previous\napproaches across various Transformer models."
    },
    {
        "date": "2025-07",
        "title": "On Jailbreaking Quantized Language Models Through Fault Injection Attacks",
        "author": "Noureldin Zahran, Ahmad Tahmasivand, Ihsen Alouani, Khaled Khasawneh, and Mohammed E. Fouda",
        "link": "http://arxiv.org/abs/2507.03236v2",
        "abstract": "The safety alignment of Language Models (LMs) is a critical concern, yet\ntheir integrity can be challenged by direct parameter manipulation attacks,\nsuch as those potentially induced by fault injection. As LMs are increasingly\ndeployed using low-precision quantization for efficiency, this paper\ninvestigates the efficacy of such attacks for jailbreaking aligned LMs across\ndifferent quantization schemes. We propose gradient-guided attacks, including a\ntailored progressive bit-level search algorithm introduced herein and a\ncomparative word-level (single weight update) attack. Our evaluation on\nLlama-3.2-3B, Phi-4-mini, and Llama-3-8B across FP16 (baseline), and\nweight-only quantization (FP8, INT8, INT4) reveals that quantization\nsignificantly influences attack success. While attacks readily achieve high\nsuccess (>80% Attack Success Rate, ASR) on FP16 models, within an attack budget\nof 25 perturbations, FP8 and INT8 models exhibit ASRs below 20% and 50%,\nrespectively. Increasing the perturbation budget up to 150 bit-flips, FP8\nmodels maintained ASR below 65%, demonstrating some resilience compared to INT8\nand INT4 models that have high ASR. In addition, analysis of perturbation\nlocations revealed differing architectural targets across quantization schemes,\nwith (FP16, INT4) and (INT8, FP8) showing similar characteristics. Besides,\njailbreaks induced in FP16 models were highly transferable to subsequent\nFP8/INT8 quantization (<5% ASR difference), though INT4 significantly reduced\ntransferred ASR (avg. 35% drop). These findings highlight that while common\nquantization schemes, particularly FP8, increase the difficulty of direct\nparameter manipulation jailbreaks, vulnerabilities can still persist,\nespecially through post-attack quantization."
    },
    {
        "date": "2025-07",
        "title": "Adopting a human developmental visual diet yields robust, shape-based AI vision",
        "author": "Zejin Lu, Sushrut Thorat, Radoslaw M Cichy, and Tim C Kietzmann",
        "link": "http://arxiv.org/abs/2507.03168v1",
        "abstract": "Despite years of research and the dramatic scaling of artificial intelligence\n(AI) systems, a striking misalignment between artificial and human vision\npersists. Contrary to humans, AI heavily relies on texture-features rather than\nshape information, lacks robustness to image distortions, remains highly\nvulnerable to adversarial attacks, and struggles to recognise simple abstract\nshapes within complex backgrounds. To close this gap, we here introduce a\nsolution that arises from a previously underexplored direction: rather than\nscaling up, we take inspiration from how human vision develops from early\ninfancy into adulthood. We quantified the visual maturation by synthesising\ndecades of psychophysical and neurophysiological research into a novel\ndevelopmental visual diet (DVD) for AI vision. We show that guiding AI systems\nthrough this human-inspired curriculum produces models that closely align with\nhuman behaviour on every hallmark of robust vision tested yielding the\nstrongest reported reliance on shape information to date, abstract shape\nrecognition beyond the state of the art, higher robustness to image\ncorruptions, and stronger resilience to adversarial attacks. By outperforming\nhigh parameter AI foundation models trained on orders of magnitude more data,\nwe provide evidence that robust AI vision can be achieved by guiding the way\nhow a model learns, not merely how much it learns, offering a\nresource-efficient route toward safer and more human-like artificial visual\nsystems."
    },
    {
        "date": "2025-07",
        "title": "Adversarial Manipulation of Reasoning Models using Internal Representations",
        "author": "Kureha Yamaguchi, Benjamin Etheridge, and Andy Arditi",
        "link": "http://arxiv.org/abs/2507.03167v1",
        "abstract": "Reasoning models generate chain-of-thought (CoT) tokens before their final\noutput, but how this affects their vulnerability to jailbreak attacks remains\nunclear. While traditional language models make refusal decisions at the\nprompt-response boundary, we find evidence that DeepSeek-R1-Distill-Llama-8B\nmakes these decisions within its CoT generation. We identify a linear direction\nin activation space during CoT token generation that predicts whether the model\nwill refuse or comply -- termed the \"caution\" direction because it corresponds\nto cautious reasoning patterns in the generated text. Ablating this direction\nfrom model activations increases harmful compliance, effectively jailbreaking\nthe model. We additionally show that intervening only on CoT token activations\nsuffices to control final outputs, and that incorporating this direction into\nprompt-based attacks improves success rates. Our findings suggest that the\nchain-of-thought itself is a promising new target for adversarial manipulation\nin reasoning models.\n  Code available at https://github.com/ky295/reasoning-manipulation"
    },
    {
        "date": "2025-07",
        "title": "Set Valued Predictions For Robust Domain Generalization",
        "author": "Ron Tsibulsky, Daniel Nevo, and Uri Shalit",
        "link": "http://arxiv.org/abs/2507.03146v1",
        "abstract": "Despite the impressive advancements in modern machine learning, achieving\nrobustness in Domain Generalization (DG) tasks remains a significant challenge.\nIn DG, models are expected to perform well on samples from unseen test\ndistributions (also called domains), by learning from multiple related training\ndistributions. Most existing approaches to this problem rely on single-valued\npredictions, which inherently limit their robustness. We argue that set-valued\npredictors could be leveraged to enhance robustness across unseen domains,\nwhile also taking into account that these sets should be as small as possible.\nWe introduce a theoretical framework defining successful set prediction in the\nDG setting, focusing on meeting a predefined performance criterion across as\nmany domains as possible, and provide theoretical insights into the conditions\nunder which such domain generalization is achievable. We further propose a\npractical optimization method compatible with modern learning architectures,\nthat balances robust performance on unseen domains with small prediction set\nsizes. We evaluate our approach on several real-world datasets from the WILDS\nbenchmark, demonstrating its potential as a promising direction for robust\ndomain generalization."
    },
    {
        "date": "2025-07",
        "title": "Holographic Projection and Cyber Attack Surface: A Physical Analogy for Digital Security",
        "author": "Ricardo Queiroz de Araujo Fernandes, Anderson Santos, Daniel Maier de Carvalho, and Andr\u00e9 Luiz Bandeira Molina",
        "link": "http://arxiv.org/abs/2507.03136v1",
        "abstract": "This article presents an in-depth exploration of the analogy between the\nHolographic Principle in theoretical physics and cyber attack surfaces in\ndigital security. Building on concepts such as black hole entropy and AdS/CFT\nduality, it highlights how complex infrastructures project their\nvulnerabilities onto their external interfaces. The paper draws a parallel\nbetween a black hole's event horizon, which encodes all internal information,\nand the attack surface, which reflects the internal architecture's security\nposture. Additionally, the article outlines how this conceptual framework can\nguide cybersecurity practices, emphasizing strategies such as attack surface\nreduction, continuous scanning with tools like OWASP ZAP and Greenbone OpenVAS,\nand the implementation of Zero Trust Architecture. This analogy not only\nprovides a unique perspective on digital security but also underscores the\ncritical importance of boundary-level defenses in protecting vast internal\ninfrastructures."
    },
    {
        "date": "2025-07",
        "title": "Visual Contextual Attack: Jailbreaking MLLMs with Image-Driven Context Injection",
        "author": "Ziqi Miao, Yi Ding, Lijun Li, and Jing Shao",
        "link": "http://arxiv.org/abs/2507.02844v1",
        "abstract": "With the emergence of strong visual-language capabilities, multimodal large\nlanguage models (MLLMs) have demonstrated tremendous potential for real-world\napplications. However, the security vulnerabilities exhibited by the visual\nmodality pose significant challenges to deploying such models in open-world\nenvironments. Recent studies have successfully induced harmful responses from\ntarget MLLMs by encoding harmful textual semantics directly into visual inputs.\nHowever, in these approaches, the visual modality primarily serves as a trigger\nfor unsafe behavior, often exhibiting semantic ambiguity and lacking grounding\nin realistic scenarios. In this work, we define a novel setting: visual-centric\njailbreak, where visual information serves as a necessary component in\nconstructing a complete and realistic jailbreak context. Building on this\nsetting, we propose the VisCo (Visual Contextual) Attack. VisCo fabricates\ncontextual dialogue using four distinct visual-focused strategies, dynamically\ngenerating auxiliary images when necessary to construct a visual-centric\njailbreak scenario. To maximize attack effectiveness, it incorporates automatic\ntoxicity obfuscation and semantic refinement to produce a final attack prompt\nthat reliably triggers harmful responses from the target black-box MLLMs.\nSpecifically, VisCo achieves a toxicity score of 4.78 and an Attack Success\nRate (ASR) of 85% on MM-SafetyBench against GPT-4o, significantly outperforming\nthe baseline, which performs a toxicity score of 2.48 and an ASR of 22.2%. The\ncode is available at https://github.com/Dtc7w3PQ/Visco-Attack."
    },
    {
        "date": "2025-07",
        "title": "Meta SecAlign: A Secure Foundation LLM Against Prompt Injection Attacks",
        "author": "Sizhe Chen, Arman Zharmagambetov, David Wagner, and Chuan Guo",
        "link": "http://arxiv.org/abs/2507.02735v1",
        "abstract": "Prompt injection attacks pose a significant security threat to LLM-integrated\napplications. Model-level defenses have shown strong effectiveness, but are\ncurrently deployed into commercial-grade models in a closed-source manner. We\nbelieve open-source models are needed by the AI security community, where\nco-development of attacks and defenses through open research drives scientific\nprogress in mitigation against prompt injection attacks. To this end, we\ndevelop Meta SecAlign, the first open-source and open-weight LLM with built-in\nmodel-level defense that achieves commercial-grade model performance. We\nprovide complete details of our training recipe, which utilizes an improved\nversion of the SOTA SecAlign defense. Evaluations on 9 utility benchmarks and 7\nsecurity benchmarks show that Meta SecAlign, despite being trained on a generic\ninstruction-tuning dataset, confers security in unseen downstream tasks,\nincluding tool-calling and agentic web navigation, in addition general\ninstruction-following. Our best model -- Meta-SecAlign-70B -- achieves\nstate-of-the-art robustness against prompt injection attacks and comparable\nutility to closed-source commercial LLM with model-level defense."
    },
    {
        "date": "2025-07",
        "title": "Control at Stake: Evaluating the Security Landscape of LLM-Driven Email Agents",
        "author": "Jiangrong Wu, Yuhong Nan, Jianliang Wu, Zitong Yao, and Zibin Zheng",
        "link": "http://arxiv.org/abs/2507.02699v1",
        "abstract": "The increasing capabilities of LLMs have led to the rapid proliferation of\nLLM agent apps, where developers enhance LLMs with access to external resources\nto support complex task execution. Among these, LLM email agent apps represent\none of the widely used categories, as email remains a critical communication\nmedium for users. LLM email agents are capable of managing and responding to\nemail using LLM-driven reasoning and autonomously executing user instructions\nvia external email APIs (e.g., send email). However, despite their growing\ndeployment and utility, the security mechanism of LLM email agent apps remains\nunderexplored. Currently, there is no comprehensive study into the potential\nsecurity risk within these agent apps and their broader implications.\n  In this paper, we conduct the first in-depth and systematic security study of\nLLM email agents. We propose the Email Agent Hijacking (EAH) attack, which\noverrides the original prompts of the email agent via external email resources,\nallowing attackers to gain control of the email agent remotely and further\nperform specific attack scenarios without user awareness.\n  To facilitate the large-scale evaluation, we propose EAHawk, a pipeline to\nevaluate the EAH attack of LLM email agent apps. By EAHawk, we performed an\nempirical study spanning 14 representative LLM agent frameworks, 63 agent apps,\n12 LLMs, and 20 email services, which led to the generation of 1,404 real-world\nemail agent instances for evaluation. Experimental results indicate that all\n1,404 instances were successfully hijacked; on average, only 2.03 attack\nattempts are required to control an email agent instance. Even worse, for some\nLLMs, the average number of attempts needed to achieve full agent control drops\nto as few as 1.23."
    },
    {
        "date": "2025-07",
        "title": "Alleviating Attack Data Scarcity: SCANIA's Experience Towards Enhancing In-Vehicle Cyber Security Measures",
        "author": "Frida Sundfeldt, Bianca Widstam, Mahshid Helali Moghadam, Kuo-Yun Liang, and Anders Vesterberg",
        "link": "http://arxiv.org/abs/2507.02607v2",
        "abstract": "The digital evolution of connected vehicles and the subsequent security risks\nemphasize the critical need for implementing in-vehicle cyber security measures\nsuch as intrusion detection and response systems. The continuous advancement of\nattack scenarios further highlights the need for adaptive detection mechanisms\nthat can detect evolving, unknown, and complex threats. The effective use of\nML-driven techniques can help address this challenge. However, constraints on\nimplementing diverse attack scenarios on test vehicles due to safety, cost, and\nethical considerations result in a scarcity of data representing attack\nscenarios. This limitation necessitates alternative efficient and effective\nmethods for generating high-quality attack-representing data. This paper\npresents a context-aware attack data generator that generates attack inputs and\ncorresponding in-vehicle network log, i.e., controller area network (CAN) log,\nrepresenting various types of attack including denial of service (DoS), fuzzy,\nspoofing, suspension, and replay attacks. It utilizes parameterized attack\nmodels augmented with CAN message decoding and attack intensity adjustments to\nconfigure the attack scenarios with high similarity to real-world scenarios and\npromote variability. We evaluate the practicality of the generated\nattack-representing data within an intrusion detection system (IDS) case study,\nin which we develop and perform an empirical evaluation of two deep neural\nnetwork IDS models using the generated data. In addition to the efficiency and\nscalability of the approach, the performance results of IDS models, high\ndetection and classification capabilities, validate the consistency and\neffectiveness of the generated data as well. In this experience study, we also\nelaborate on the aspects influencing the fidelity of the data to real-world\nscenarios and provide insights into its application."
    },
    {
        "date": "2025-07",
        "title": "De-AntiFake: Rethinking the Protective Perturbations Against Voice Cloning Attacks",
        "author": "Wei Fan, Kejiang Chen, Chang Liu, Weiming Zhang, and Nenghai Yu",
        "link": "http://arxiv.org/abs/2507.02606v1",
        "abstract": "The rapid advancement of speech generation models has heightened privacy and\nsecurity concerns related to voice cloning (VC). Recent studies have\ninvestigated disrupting unauthorized voice cloning by introducing adversarial\nperturbations. However, determined attackers can mitigate these protective\nperturbations and successfully execute VC. In this study, we conduct the first\nsystematic evaluation of these protective perturbations against VC under\nrealistic threat models that include perturbation purification. Our findings\nreveal that while existing purification methods can neutralize a considerable\nportion of the protective perturbations, they still lead to distortions in the\nfeature space of VC models, which degrades the performance of VC. From this\nperspective, we propose a novel two-stage purification method: (1) Purify the\nperturbed speech; (2) Refine it using phoneme guidance to align it with the\nclean speech distribution. Experimental results demonstrate that our method\noutperforms state-of-the-art purification methods in disrupting VC defenses.\nOur study reveals the limitations of adversarial perturbation-based VC defenses\nand underscores the urgent need for more robust solutions to mitigate the\nsecurity and privacy risks posed by VC. The code and audio samples are\navailable at https://de-antifake.github.io."
    },
    {
        "date": "2025-07",
        "title": "IGDNet: Zero-Shot Robust Underexposed Image Enhancement via Illumination-Guided and Denoising",
        "author": "Hailong Yan, Junjian Huang, and Tingwen Huang",
        "link": "http://arxiv.org/abs/2507.02445v1",
        "abstract": "Current methods for restoring underexposed images typically rely on\nsupervised learning with paired underexposed and well-illuminated images.\nHowever, collecting such datasets is often impractical in real-world scenarios.\nMoreover, these methods can lead to over-enhancement, distorting\nwell-illuminated regions. To address these issues, we propose IGDNet, a\nZero-Shot enhancement method that operates solely on a single test image,\nwithout requiring guiding priors or training data. IGDNet exhibits strong\ngeneralization ability and effectively suppresses noise while restoring\nillumination. The framework comprises a decomposition module and a denoising\nmodule. The former separates the image into illumination and reflection\ncomponents via a dense connection network, while the latter enhances\nnon-uniformly illuminated regions using an illumination-guided pixel adaptive\ncorrection method. A noise pair is generated through downsampling and refined\niteratively to produce the final result. Extensive experiments on four public\ndatasets demonstrate that IGDNet significantly improves visual quality under\ncomplex lighting conditions. Quantitative results on metrics like PSNR\n(20.41dB) and SSIM (0.860dB) show that it outperforms 14 state-of-the-art\nunsupervised methods. The code will be released soon."
    },
    {
        "date": "2025-07",
        "title": "Toward a Robust and Generalizable Metamaterial Foundation Model",
        "author": "Namjung Kim, Dongseok Lee, Jongbin Yu, Sung Woong Cho, Dosung Lee, Yesol Park, and Youngjoon Hong",
        "link": "http://arxiv.org/abs/2507.02436v1",
        "abstract": "Advances in material functionalities drive innovations across various fields,\nwhere metamaterials-defined by structure rather than composition-are leading\nthe way. Despite the rise of artificial intelligence (AI)-driven design\nstrategies, their impact is limited by task-specific retraining, poor\nout-of-distribution(OOD) generalization, and the need for separate models for\nforward and inverse design. To address these limitations, we introduce the\nMetamaterial Foundation Model (MetaFO), a Bayesian transformer-based foundation\nmodel inspired by large language models. MetaFO learns the underlying mechanics\nof metamaterials, enabling probabilistic, zero-shot predictions across diverse,\nunseen combinations of material properties and structural responses. It also\nexcels in nonlinear inverse design, even under OOD conditions. By treating\nmetamaterials as an operator that maps material properties to structural\nresponses, MetaFO uncovers intricate structure-property relationships and\nsignificantly expands the design space. This scalable and generalizable\nframework marks a paradigm shift in AI-driven metamaterial discovery, paving\nthe way for next-generation innovations."
    },
    {
        "date": "2025-07",
        "title": "CyberRAG: An agentic RAG cyber attack classification and reporting tool",
        "author": "Francesco Blefari, Cristian Cosentino, Francesco Aurelio Pironti, Angelo Furfaro, and Fabrizio Marozzo",
        "link": "http://arxiv.org/abs/2507.02424v1",
        "abstract": "Intrusion Detection and Prevention Systems (IDS/IPS) in large enterprises can\ngenerate hundreds of thousands of alerts per hour, overwhelming security\nanalysts with logs that demand deep, rapidly evolving domain expertise.\nConventional machine-learning detectors trim the alert volume but still yield\nhigh false-positive rates, while standard single-pass Retrieval-Augmented\nGeneration (RAG) pipelines often retrieve irrelevant context and fail to\njustify their predictions. To overcome these shortcomings, we present CyberRAG,\na modular, agent-based RAG framework that delivers real-time classification,\nexplanation, and structured reporting for cyber-attacks. A central LLM agent\norchestrates (i) a pool of fine-tuned specialized classifiers, each tailored to\na distinct attack family; (ii) tool adapters for enrichment and alerting; and\n(iii) an iterative retrieval-and-reason loop that continuously queries a\ndomain-specific knowledge base until the evidence is both relevant and\nself-consistent. Unlike traditional RAG systems, CyberRAG embraces an agentic\ndesign that enables dynamic control flow and adaptive reasoning. This\nagent-centric architecture refines its threat labels and natural-language\njustifications autonomously, reducing false positives and enhancing\ninterpretability. The framework is fully extensible: new attack types can be\nsupported by simply adding a classifier without retraining the core agent.\nCyberRAG has been evaluated achieving over 94% accuracy per class and pushing\nfinal classification accuracy to 94.92% through semantic orchestration.\nGenerated explanations score up to 0.94 in BERTScore and 4.9/5 in GPT-4-based\nexpert evaluation. These results show that agentic, specialist-oriented RAG can\npair high detection accuracy with trustworthy, SOC-ready prose, offering a\npractical and scalable path toward semi-autonomous cyber-defence workflows."
    },
    {
        "date": "2025-07",
        "title": "Evaluating Language Models For Threat Detection in IoT Security Logs",
        "author": "Jorge J. Tejero-Fern\u00e1ndez, and Alfonso S\u00e1nchez-Maci\u00e1n",
        "link": "http://arxiv.org/abs/2507.02390v1",
        "abstract": "Log analysis is a relevant research field in cybersecurity as they can\nprovide a source of information for the detection of threats to networks and\nsystems. This paper presents a pipeline to use fine-tuned Large Language Models\n(LLMs) for anomaly detection and mitigation recommendation using IoT security\nlogs. Utilizing classical machine learning classifiers as a baseline, three\nopen-source LLMs are compared for binary and multiclass anomaly detection, with\nthree strategies: zero-shot, few-shot prompting and fine-tuning using an IoT\ndataset. LLMs give better results on multi-class attack classification than the\ncorresponding baseline models. By mapping detected threats to MITRE CAPEC,\ndefining a set of IoT-specific mitigation actions, and fine-tuning the models\nwith those actions, the models are able to provide a combined detection and\nrecommendation guidance."
    },
    {
        "date": "2025-07",
        "title": "A robust and versatile deep learning model for prediction of the arterial input function in dynamic small animal $\\left[^{18}\\text{F}\\right]$FDG PET imaging",
        "author": "Christian Salomonsen, Luigi Tommaso Luppino, Fredrik Aspheim, Kristoffer Wickstr\u00f8m, Elisabeth Wetzer, Michael Kampffmeyer, Rodrigo Berzaghi, Rune Sundset, Robert Jenssen, and Samuel Kuttner",
        "link": "http://arxiv.org/abs/2507.02367v1",
        "abstract": "Dynamic positron emission tomography (PET) and kinetic modeling are pivotal\nin advancing tracer development research in small animal studies. Accurate\nkinetic modeling requires precise input function estimation, traditionally\nachieved via arterial blood sampling. However, arterial cannulation in small\nanimals like mice, involves intricate, time-consuming, and terminal procedures,\nprecluding longitudinal studies. This work proposes a non-invasive, fully\nconvolutional deep learning-based approach (FC-DLIF) to predict input functions\ndirectly from PET imaging, potentially eliminating the need for blood sampling\nin dynamic small-animal PET. The proposed FC-DLIF model includes a spatial\nfeature extractor acting on the volumetric time frames of the PET sequence,\nextracting spatial features. These are subsequently further processed in a\ntemporal feature extractor that predicts the arterial input function. The\nproposed approach is trained and evaluated using images and arterial blood\ncurves from [$^{18}$F]FDG data using cross validation. Further, the model\napplicability is evaluated on imaging data and arterial blood curves collected\nusing two additional radiotracers ([$^{18}$F]FDOPA, and [$^{68}$Ga]PSMA). The\nmodel was further evaluated on data truncated and shifted in time, to simulate\nshorter, and shifted, PET scans. The proposed FC-DLIF model reliably predicts\nthe arterial input function with respect to mean squared error and correlation.\nFurthermore, the FC-DLIF model is able to predict the arterial input function\neven from truncated and shifted samples. The model fails to predict the AIF\nfrom samples collected using different radiotracers, as these are not\nrepresented in the training data. Our deep learning-based input function offers\na non-invasive and reliable alternative to arterial blood sampling, proving\nrobust and flexible to temporal shifts and different scan durations."
    },
    {
        "date": "2025-07",
        "title": "Rethinking Broken Object Level Authorization Attacks Under Zero Trust Principle",
        "author": "Anbin Wu, Zhiyong Feng, and Ruitao Feng",
        "link": "http://arxiv.org/abs/2507.02309v1",
        "abstract": "RESTful APIs facilitate data exchange between applications, but they also\nexpose sensitive resources to potential exploitation. Broken Object Level\nAuthorization (BOLA) is the top vulnerability in the OWASP API Security Top 10,\nexemplifies a critical access control flaw where attackers manipulate API\nparameters to gain unauthorized access. To address this, we propose BOLAZ, a\ndefense framework grounded in zero trust principles. BOLAZ analyzes the data\nflow of resource IDs, pinpointing BOLA attack injection points and determining\nthe associated authorization intervals to prevent horizontal privilege\nescalation. Our approach leverages static taint tracking to categorize APIs\ninto producers and consumers based on how they handle resource IDs. By mapping\nthe propagation paths of resource IDs, BOLAZ captures the context in which\nthese IDs are produced and consumed, allowing for precise identification of\nauthorization boundaries. Unlike defense methods based on common authorization\nmodels, BOLAZ is the first authorization-guided method that adapts defense\nrules based on the system's best-practice authorization logic. We validate\nBOLAZ through empirical research on 10 GitHub projects. The results demonstrate\nBOLAZ's effectiveness in defending against vulnerabilities collected from CVE\nand discovering 35 new BOLA vulnerabilities in the wild, demonstrating its\npracticality in real-world deployments."
    },
    {
        "date": "2025-07",
        "title": "A robust and adaptive MPC formulation for Gaussian process models",
        "author": "Mathieu Dubied, Amon Lahr, Melanie N. Zeilinger, and Johannes K\u00f6hler",
        "link": "http://arxiv.org/abs/2507.02098v1",
        "abstract": "In this paper, we present a robust and adaptive model predictive control\n(MPC) framework for uncertain nonlinear systems affected by bounded\ndisturbances and unmodeled nonlinearities. We use Gaussian Processes (GPs) to\nlearn the uncertain dynamics based on noisy measurements, including those\ncollected during system operation. As a key contribution, we derive robust\npredictions for GP models using contraction metrics, which are incorporated in\nthe MPC formulation. The proposed design guarantees recursive feasibility,\nrobust constraint satisfaction and convergence to a reference state, with high\nprobability. We provide a numerical example of a planar quadrotor subject to\ndifficult-to-model ground effects, which highlights significant improvements\nachieved through the proposed robust prediction method and through online\nlearning."
    },
    {
        "date": "2025-07",
        "title": "AMD: Adaptive Momentum and Decoupled Contrastive Learning Framework for Robust Long-Tail Trajectory Prediction",
        "author": "Bin Rao, Haicheng Liao, Yanchen Guan, Chengyue Wang, Bonan Wang, Jiaxun Zhang, and Zhenning Li",
        "link": "http://arxiv.org/abs/2507.01801v2",
        "abstract": "Accurately predicting the future trajectories of traffic agents is essential\nin autonomous driving. However, due to the inherent imbalance in trajectory\ndistributions, tail data in natural datasets often represents more complex and\nhazardous scenarios. Existing studies typically rely solely on a base model's\nprediction error, without considering the diversity and uncertainty of\nlong-tail trajectory patterns. We propose an adaptive momentum and decoupled\ncontrastive learning framework (AMD), which integrates unsupervised and\nsupervised contrastive learning strategies. By leveraging an improved momentum\ncontrast learning (MoCo-DT) and decoupled contrastive learning (DCL) module,\nour framework enhances the model's ability to recognize rare and complex\ntrajectories. Additionally, we design four types of trajectory random\naugmentation methods and introduce an online iterative clustering strategy,\nallowing the model to dynamically update pseudo-labels and better adapt to the\ndistributional shifts in long-tail data. We propose three different criteria to\ndefine long-tail trajectories and conduct extensive comparative experiments on\nthe nuScenes and ETH$/$UCY datasets. The results show that AMD not only\nachieves optimal performance in long-tail trajectory prediction but also\ndemonstrates outstanding overall prediction accuracy."
    },
    {
        "date": "2025-07",
        "title": "Robust brain age estimation from structural MRI with contrastive learning",
        "author": "Carlo Alberto Barbano, Benoit Dufumier, Edouard Duchesnay, Marco Grangetto, and Pietro Gori",
        "link": "http://arxiv.org/abs/2507.01794v1",
        "abstract": "Estimating brain age from structural MRI has emerged as a powerful tool for\ncharacterizing normative and pathological aging. In this work, we explore\ncontrastive learning as a scalable and robust alternative to supervised\napproaches for brain age estimation. We introduce a novel contrastive loss\nfunction, $\\mathcal{L}^{exp}$, and evaluate it across multiple public\nneuroimaging datasets comprising over 20,000 scans. Our experiments reveal four\nkey findings. First, scaling pre-training on diverse, multi-site data\nconsistently improves generalization performance, cutting external mean\nabsolute error (MAE) nearly in half. Second, $\\mathcal{L}^{exp}$ is robust to\nsite-related confounds, maintaining low scanner-predictability as training size\nincreases. Third, contrastive models reliably capture accelerated aging in\npatients with cognitive impairment and Alzheimer's disease, as shown through\nbrain age gap analysis, ROC curves, and longitudinal trends. Lastly, unlike\nsupervised baselines, $\\mathcal{L}^{exp}$ maintains a strong correlation\nbetween brain age accuracy and downstream diagnostic performance, supporting\nits potential as a foundation model for neuroimaging. These results position\ncontrastive learning as a promising direction for building generalizable and\nclinically meaningful brain representations."
    },
    {
        "date": "2025-07",
        "title": "Boosting Adversarial Transferability Against Defenses via Multi-Scale Transformation",
        "author": "Zihong Guo, Chen Wan, Yayin Zheng, Hailing Kuang, and Xiaohai Lu",
        "link": "http://arxiv.org/abs/2507.01791v1",
        "abstract": "The transferability of adversarial examples poses a significant security\nchallenge for deep neural networks, which can be attacked without knowing\nanything about them. In this paper, we propose a new Segmented Gaussian Pyramid\n(SGP) attack method to enhance the transferability, particularly against\ndefense models. Unlike existing methods that generally focus on single-scale\nimages, our approach employs Gaussian filtering and three types of downsampling\nto construct a series of multi-scale examples. Then, the gradients of the loss\nfunction with respect to each scale are computed, and their average is used to\ndetermine the adversarial perturbations. The proposed SGP can be considered an\ninput transformation with high extensibility that is easily integrated into\nmost existing adversarial attacks. Extensive experiments demonstrate that in\ncontrast to the state-of-the-art methods, SGP significantly enhances attack\nsuccess rates against black-box defense models, with average attack success\nrates increasing by 2.3% to 32.6%, based only on transferability."
    },
    {
        "date": "2025-07",
        "title": "Signals and Symptoms: ICS Attack Dataset From Railway Cyber Range",
        "author": "Anis Yusof, Yuancheng Liu, Niklaus Kang, Choon Meng Seah, Zhenkai Liang, and Ee-Chien Chang",
        "link": "http://arxiv.org/abs/2507.01768v1",
        "abstract": "The prevalence of cyberattacks on Industrial Control Systems (ICS) has\nhighlighted the necessity for robust security measures and incident response to\nprotect critical infrastructure. This is prominent when Operational Technology\n(OT) systems undergo digital transformation by integrating with Information\nTechnology (IT) systems to enhance operational efficiency, adaptability, and\nsafety. To support analysts in staying abreast of emerging attack patterns,\nthere is a need for ICS datasets that reflect indicators representative of\ncontemporary cyber threats. To address this, we conduct two ICS cyberattack\nsimulations to showcase the impact of trending ICS cyberattacks on a railway\ncyber range that resembles the railway infrastructure. The attack scenario is\ndesigned to blend trending attack trends with attack patterns observed from\nhistorical ICS incidents. The resulting evidence is collected as datasets,\nserving as an essential resource for cyberattack analysis. This captures key\nindicators that are relevant to the current threat landscape, augmenting the\neffectiveness of security systems and analysts to protect against ICS cyber\nthreats."
    },
    {
        "date": "2025-07",
        "title": "RobuSTereo: Robust Zero-Shot Stereo Matching under Adverse Weather",
        "author": "Yuran Wang, Yingping Liang, Yutao Hu, and Ying Fu",
        "link": "http://arxiv.org/abs/2507.01653v1",
        "abstract": "Learning-based stereo matching models struggle in adverse weather conditions\ndue to the scarcity of corresponding training data and the challenges in\nextracting discriminative features from degraded images. These limitations\nsignificantly hinder zero-shot generalization to out-of-distribution weather\nconditions. In this paper, we propose \\textbf{RobuSTereo}, a novel framework\nthat enhances the zero-shot generalization of stereo matching models under\nadverse weather by addressing both data scarcity and feature extraction\nchallenges. First, we introduce a diffusion-based simulation pipeline with a\nstereo consistency module, which generates high-quality stereo data tailored\nfor adverse conditions. By training stereo matching models on our synthetic\ndatasets, we reduce the domain gap between clean and degraded images,\nsignificantly improving the models' robustness to unseen weather conditions.\nThe stereo consistency module ensures structural alignment across synthesized\nimage pairs, preserving geometric integrity and enhancing depth estimation\naccuracy. Second, we design a robust feature encoder that combines a\nspecialized ConvNet with a denoising transformer to extract stable and reliable\nfeatures from degraded images. The ConvNet captures fine-grained local\nstructures, while the denoising transformer refines global representations,\neffectively mitigating the impact of noise, low visibility, and weather-induced\ndistortions. This enables more accurate disparity estimation even under\nchallenging visual conditions. Extensive experiments demonstrate that\n\\textbf{RobuSTereo} significantly improves the robustness and generalization of\nstereo matching models across diverse adverse weather scenarios."
    },
    {
        "date": "2025-07",
        "title": "SAILViT: Towards Robust and Generalizable Visual Backbones for MLLMs via Gradual Feature Refinement",
        "author": "Weijie Yin, Dingkang Yang, Hongyuan Dong, Zijian Kang, Jiacong Wang, Xiao Liang, Chao Feng, and Jiao Ran",
        "link": "http://arxiv.org/abs/2507.01643v1",
        "abstract": "Vision Transformers (ViTs) are essential as foundation backbones in\nestablishing the visual comprehension capabilities of Multimodal Large Language\nModels (MLLMs). Although most ViTs achieve impressive performance through\nimage-text pair-based contrastive learning or self-supervised mechanisms, they\nstruggle to engage in connector-based co-training directly with LLMs due to\npotential parameter initialization conflicts and modality semantic gaps. To\naddress the above challenges, this paper proposes SAILViT, a gradual feature\nlearning-enhanced ViT for facilitating MLLMs to break through performance\nbottlenecks in complex multimodal interactions. SAILViT achieves\ncoarse-to-fine-grained feature alignment and world knowledge infusion with\ngradual feature refinement, which better serves target training demands. We\nperform thorough empirical analyses to confirm the powerful robustness and\ngeneralizability of SAILViT across different dimensions, including parameter\nsizes, model architectures, training strategies, and data scales. Equipped with\nSAILViT, existing MLLMs show significant and consistent performance\nimprovements on the OpenCompass benchmark across extensive downstream tasks.\nSAILViT series models are released at\nhttps://huggingface.co/BytedanceDouyinContent."
    },
    {
        "date": "2025-07",
        "title": "Survivability of Backdoor Attacks on Unconstrained Face Recognition Systems",
        "author": "Quentin Le Roux, Yannick Teglia, Teddy Furon, Philippe Loubet-Moundi, and Eric Bourbao",
        "link": "http://arxiv.org/abs/2507.01607v1",
        "abstract": "The widespread use of deep learning face recognition raises several security\nconcerns. Although prior works point at existing vulnerabilities, DNN backdoor\nattacks against real-life, unconstrained systems dealing with images captured\nin the wild remain a blind spot of the literature. This paper conducts the\nfirst system-level study of backdoors in deep learning-based face recognition\nsystems. This paper yields four contributions by exploring the feasibility of\nDNN backdoors on these pipelines in a holistic fashion. We demonstrate for the\nfirst time two backdoor attacks on the face detection task: face generation and\nface landmark shift attacks. We then show that face feature extractors trained\nwith large margin losses also fall victim to backdoor attacks. Combining our\nmodels, we then show using 20 possible pipeline configurations and 15 attack\ncases that a single backdoor enables an attacker to bypass the entire function\nof a system. Finally, we provide stakeholders with several best practices and\ncountermeasures."
    },
    {
        "date": "2025-07",
        "title": "On the Effect of Ruleset Tuning and Data Imbalance on Explainable Network Security Alert Classifications: a Case-Study on DeepCASE",
        "author": "Koen T. W. Teuwen, Sam Baggen, Emmanuele Zambon, and Luca Allodi",
        "link": "http://arxiv.org/abs/2507.01571v1",
        "abstract": "Automation in Security Operations Centers (SOCs) plays a prominent role in\nalert classification and incident escalation. However, automated methods must\nbe robust in the presence of imbalanced input data, which can negatively affect\nperformance. Additionally, automated methods should make explainable decisions.\nIn this work, we evaluate the effect of label imbalance on the classification\nof network intrusion alerts. As our use-case we employ DeepCASE, the\nstate-of-the-art method for automated alert classification. We show that label\nimbalance impacts both classification performance and correctness of the\nclassification explanations offered by DeepCASE. We conclude tuning the\ndetection rules used in SOCs can significantly reduce imbalance and may benefit\nthe performance and explainability offered by alert post-processing methods\nsuch as DeepCASE. Therefore, our findings suggest that traditional methods to\nimprove the quality of input data can benefit automation."
    },
    {
        "date": "2025-07",
        "title": "SafePTR: Token-Level Jailbreak Defense in Multimodal LLMs via Prune-then-Restore Mechanism",
        "author": "Beitao Chen, Xinyu Lyu, Lianli Gao, Jingkuan Song, and Heng Tao Shen",
        "link": "http://arxiv.org/abs/2507.01513v1",
        "abstract": "By incorporating visual inputs, Multimodal Large Language Models (MLLMs)\nextend LLMs to support visual reasoning. However, this integration also\nintroduces new vulnerabilities, making MLLMs susceptible to multimodal\njailbreak attacks and hindering their safe deployment.Existing defense methods,\nincluding Image-to-Text Translation, Safe Prompting, and Multimodal Safety\nTuning, attempt to address this by aligning multimodal inputs with LLMs'\nbuilt-in safeguards.Yet, they fall short in uncovering root causes of\nmultimodal vulnerabilities, particularly how harmful multimodal tokens trigger\njailbreak in MLLMs? Consequently, they remain vulnerable to text-driven\nmultimodal jailbreaks, often exhibiting overdefensive behaviors and imposing\nheavy training overhead.To bridge this gap, we present an comprehensive\nanalysis of where, how and which harmful multimodal tokens bypass safeguards in\nMLLMs. Surprisingly, we find that less than 1% tokens in early-middle layers\nare responsible for inducing unsafe behaviors, highlighting the potential of\nprecisely removing a small subset of harmful tokens, without requiring safety\ntuning, can still effectively improve safety against jailbreaks. Motivated by\nthis, we propose Safe Prune-then-Restore (SafePTR), an training-free defense\nframework that selectively prunes harmful tokens at vulnerable layers while\nrestoring benign features at subsequent layers.Without incurring additional\ncomputational overhead, SafePTR significantly enhances the safety of MLLMs\nwhile preserving efficiency. Extensive evaluations across three MLLMs and five\nbenchmarks demonstrate SafePTR's state-of-the-art performance in mitigating\njailbreak risks without compromising utility."
    },
    {
        "date": "2025-07",
        "title": "How to Securely Shuffle? A survey about Secure Shufflers for privacy-preserving computations",
        "author": "Marc Damie, Florian Hahn, Andreas Peter, and Jan Ramon",
        "link": "http://arxiv.org/abs/2507.01487v1",
        "abstract": "Ishai et al. (FOCS'06) introduced secure shuffling as an efficient building\nblock for private data aggregation. Recently, the field of differential privacy\nhas revived interest in secure shufflers by highlighting the privacy\namplification they can provide in various computations. Although several works\nargue for the utility of secure shufflers, they often treat them as black\nboxes; overlooking the practical vulnerabilities and performance trade-offs of\nexisting implementations. This leaves a central question open: what makes a\ngood secure shuffler?\n  This survey addresses that question by identifying, categorizing, and\ncomparing 26 secure protocols that realize the necessary shuffling\nfunctionality. To enable a meaningful comparison, we adapt and unify existing\nsecurity definitions into a consistent set of properties. We also present an\noverview of privacy-preserving technologies that rely on secure shufflers,\noffer practical guidelines for selecting appropriate protocols, and outline\npromising directions for future work."
    },
    {
        "date": "2025-07",
        "title": "What Really Matters for Robust Multi-Sensor HD Map Construction?",
        "author": "Xiaoshuai Hao, Yuting Zhao, Yuheng Ji, Luanyuan Dai, Peng Hao, Dingzhe Li, Shuai Cheng, and Rong Yin",
        "link": "http://arxiv.org/abs/2507.01484v1",
        "abstract": "High-definition (HD) map construction methods are crucial for providing\nprecise and comprehensive static environmental information, which is essential\nfor autonomous driving systems. While Camera-LiDAR fusion techniques have shown\npromising results by integrating data from both modalities, existing approaches\nprimarily focus on improving model accuracy and often neglect the robustness of\nperception models, which is a critical aspect for real-world applications. In\nthis paper, we explore strategies to enhance the robustness of multi-modal\nfusion methods for HD map construction while maintaining high accuracy. We\npropose three key components: data augmentation, a novel multi-modal fusion\nmodule, and a modality dropout training strategy. These components are\nevaluated on a challenging dataset containing 10 days of NuScenes data. Our\nexperimental results demonstrate that our proposed methods significantly\nenhance the robustness of baseline methods. Furthermore, our approach achieves\nstate-of-the-art performance on the clean validation set of the NuScenes\ndataset. Our findings provide valuable insights for developing more robust and\nreliable HD map construction models, advancing their applicability in\nreal-world autonomous driving scenarios. Project website:\nhttps://robomap-123.github.io."
    },
    {
        "date": "2025-07",
        "title": "Rational Censorship Attack: Breaking Blockchain with a Blackboard",
        "author": "Michelle Yeo, and Haoqian Zhang",
        "link": "http://arxiv.org/abs/2507.01453v1",
        "abstract": "Censorship resilience is a fundamental assumption underlying the security of\nblockchain protocols. Additionally, the analysis of blockchain security from an\neconomic and game theoretic perspective has been growing in popularity in\nrecent years. In this work, we present a surprising rational censorship attack\non blockchain censorship resilience when we adopt the analysis of blockchain\nsecurity from a game theoretic lens and assume all users are rational. In our\nattack, a colluding group with sufficient voting power censors the remainder\nnodes such that the group alone can gain all the rewards from maintaining the\nblockchain. We show that if nodes are rational, coordinating this attack just\nrequires a public read and write blackboard and we formally model the attack\nusing a game theoretic framework. Furthermore, we note that to ensure the\nsuccess of the attack, nodes need to know the total true voting power held by\nthe colluding group. We prove that the strategy to join the rational censorship\nattack and also for nodes to honestly declare their power is a subgame perfect\nequilibrium in the corresponding extensive form game induced by our attack.\nFinally, we discuss the implications of the attack on blockchain users and\nprotocol designers as well as some potential countermeasures."
    },
    {
        "date": "2025-07",
        "title": "TurboReg: TurboClique for Robust and Efficient Point Cloud Registration",
        "author": "Shaocheng Yan, Pengcheng Shi, Zhenjun Zhao, Kaixin Wang, Kuang Cao, Ji Wu, and Jiayuan Li",
        "link": "http://arxiv.org/abs/2507.01439v2",
        "abstract": "Robust estimation is essential in correspondence-based Point Cloud\nRegistration (PCR). Existing methods using maximal clique search in\ncompatibility graphs achieve high recall but suffer from exponential time\ncomplexity, limiting their use in time-sensitive applications. To address this\nchallenge, we propose a fast and robust estimator, TurboReg, built upon a novel\nlightweight clique, TurboClique, and a highly parallelizable Pivot-Guided\nSearch (PGS) algorithm. First, we define the TurboClique as a 3-clique within a\nhighly-constrained compatibility graph. The lightweight nature of the 3-clique\nallows for efficient parallel searching, and the highly-constrained\ncompatibility graph ensures robust spatial consistency for stable\ntransformation estimation. Next, PGS selects matching pairs with high SC$^2$\nscores as pivots, effectively guiding the search toward TurboCliques with\nhigher inlier ratios. Moreover, the PGS algorithm has linear time complexity\nand is significantly more efficient than the maximal clique search with\nexponential time complexity. Extensive experiments show that TurboReg achieves\nstate-of-the-art performance across multiple real-world datasets, with\nsubstantial speed improvements. For example, on the 3DMatch+FCGF dataset,\nTurboReg (1K) operates $208.22\\times$ faster than 3DMAC while also achieving\nhigher recall. Our code is accessible at\n\\href{https://github.com/Laka-3DV/TurboReg}{\\texttt{TurboReg}}."
    },
    {
        "date": "2025-07",
        "title": "DiffMark: Diffusion-based Robust Watermark Against Deepfakes",
        "author": "Chen Sun, Haiyang Sun, Zhiqing Guo, Yunfeng Diao, Liejun Wang, Dan Ma, Gaobo Yang, and Keqin Li",
        "link": "http://arxiv.org/abs/2507.01428v1",
        "abstract": "Deepfakes pose significant security and privacy threats through malicious\nfacial manipulations. While robust watermarking can aid in authenticity\nverification and source tracking, existing methods often lack the sufficient\nrobustness against Deepfake manipulations. Diffusion models have demonstrated\nremarkable performance in image generation, enabling the seamless fusion of\nwatermark with image during generation. In this study, we propose a novel\nrobust watermarking framework based on diffusion model, called DiffMark. By\nmodifying the training and sampling scheme, we take the facial image and\nwatermark as conditions to guide the diffusion model to progressively denoise\nand generate corresponding watermarked image. In the construction of facial\ncondition, we weight the facial image by a timestep-dependent factor that\ngradually reduces the guidance intensity with the decrease of noise, thus\nbetter adapting to the sampling process of diffusion model. To achieve the\nfusion of watermark condition, we introduce a cross information fusion (CIF)\nmodule that leverages a learnable embedding table to adaptively extract\nwatermark features and integrates them with image features via cross-attention.\nTo enhance the robustness of the watermark against Deepfake manipulations, we\nintegrate a frozen autoencoder during training phase to simulate Deepfake\nmanipulations. Additionally, we introduce Deepfake-resistant guidance that\nemploys specific Deepfake model to adversarially guide the diffusion sampling\nprocess to generate more robust watermarked images. Experimental results\ndemonstrate the effectiveness of the proposed DiffMark on typical Deepfakes.\nOur code will be available at https://github.com/vpsg-research/DiffMark."
    },
    {
        "date": "2025-07",
        "title": "A Compact 16-bit S-box over Tower Field $\\F_{(((2^2)^2)^2)^2}$ with High Security",
        "author": "Bahram Rashidi, and Behrooz Khadem",
        "link": "http://arxiv.org/abs/2507.01423v1",
        "abstract": "This paper introduces a compact and secure 16-bit substitution box (S-box)\ndesigned over the composite field $\\F_{(((2^2)^2)^2)^2}$, optimized for both\nhardware efficiency and cryptographic robustness. The proposed S-box decomposes\noperations into subfields, leveraging a tower field architecture. This enables\nsignificant hardware reduction through optimized field inversion and a low-cost\naffine transformation. Security evaluations confirm resilience against linear,\ndifferential, algebraic and DPA attacks, validated via metrics including\nNonlinearity (32512), Differential Uniformity (4), Algebraic Degree (15),\nTransparency order (15.9875) and SNR (0.34e-08). The hardware results, in 65 nm\nCMOS technology, show the proposed 16-bit S-box has lower hardware resources\nconsumption and lower critical path delay (CPD) than those of other 16-bit\nS-boxes. By integrating high algebraic complexity with resource-efficient\nstructures, this work addresses the growing demand for scalable cryptographic\nprimitives in data-sensitive applications, demonstrating that larger S-boxes\ncan enhance security without proportional hardware costs. The results\nunderscore the viability of composite field-based architectures in balancing\nsecurity and efficiency for modern block ciphers."
    },
    {
        "date": "2025-07",
        "title": "3D Gaussian Splatting Driven Multi-View Robust Physical Adversarial Camouflage Generation",
        "author": "Tianrui Lou, Xiaojun Jia, Siyuan Liang, Jiawei Liang, Ming Zhang, Yanjun Xiao, and Xiaochun Cao",
        "link": "http://arxiv.org/abs/2507.01367v1",
        "abstract": "Physical adversarial attack methods expose the vulnerabilities of deep neural\nnetworks and pose a significant threat to safety-critical scenarios such as\nautonomous driving. Camouflage-based physical attack is a more promising\napproach compared to the patch-based attack, offering stronger adversarial\neffectiveness in complex physical environments. However, most prior work relies\non mesh priors of the target object and virtual environments constructed by\nsimulators, which are time-consuming to obtain and inevitably differ from the\nreal world. Moreover, due to the limitations of the backgrounds in training\nimages, previous methods often fail to produce multi-view robust adversarial\ncamouflage and tend to fall into sub-optimal solutions. Due to these reasons,\nprior work lacks adversarial effectiveness and robustness across diverse\nviewpoints and physical environments. We propose a physical attack framework\nbased on 3D Gaussian Splatting (3DGS), named PGA, which provides rapid and\nprecise reconstruction with few images, along with photo-realistic rendering\ncapabilities. Our framework further enhances cross-view robustness and\nadversarial effectiveness by preventing mutual and self-occlusion among\nGaussians and employing a min-max optimization approach that adjusts the\nimaging background of each viewpoint, helping the algorithm filter out\nnon-robust adversarial features. Extensive experiments validate the\neffectiveness and superiority of PGA. Our code is available\nat:https://github.com/TRLou/PGA."
    },
    {
        "date": "2025-07",
        "title": "ICLShield: Exploring and Mitigating In-Context Learning Backdoor Attacks",
        "author": "Zhiyao Ren, Siyuan Liang, Aishan Liu, and Dacheng Tao",
        "link": "http://arxiv.org/abs/2507.01321v1",
        "abstract": "In-context learning (ICL) has demonstrated remarkable success in large\nlanguage models (LLMs) due to its adaptability and parameter-free nature.\nHowever, it also introduces a critical vulnerability to backdoor attacks, where\nadversaries can manipulate LLM behaviors by simply poisoning a few ICL\ndemonstrations. In this paper, we propose, for the first time, the\ndual-learning hypothesis, which posits that LLMs simultaneously learn both the\ntask-relevant latent concepts and backdoor latent concepts within poisoned\ndemonstrations, jointly influencing the probability of model outputs. Through\ntheoretical analysis, we derive an upper bound for ICL backdoor effects,\nrevealing that the vulnerability is dominated by the concept preference ratio\nbetween the task and the backdoor. Motivated by these findings, we propose\nICLShield, a defense mechanism that dynamically adjusts the concept preference\nratio. Our method encourages LLMs to select clean demonstrations during the ICL\nphase by leveraging confidence and similarity scores, effectively mitigating\nsusceptibility to backdoor attacks. Extensive experiments across multiple LLMs\nand tasks demonstrate that our method achieves state-of-the-art defense\neffectiveness, significantly outperforming existing approaches (+26.02% on\naverage). Furthermore, our method exhibits exceptional adaptability and\ndefensive performance even for closed-source models (e.g., GPT-4)."
    },
    {
        "date": "2025-07",
        "title": "LANet: A Lane Boundaries-Aware Approach For Robust Trajectory Prediction",
        "author": "Muhammad Atta ur Rahman, Dooseop Choi, and KyoungWook Min",
        "link": "http://arxiv.org/abs/2507.01308v1",
        "abstract": "Accurate motion forecasting is critical for safe and efficient autonomous\ndriving, enabling vehicles to predict future trajectories and make informed\ndecisions in complex traffic scenarios. Most of the current designs of motion\nprediction models are based on the major representation of lane centerlines,\nwhich limits their capability to capture critical road environments and traffic\nrules and constraints. In this work, we propose an enhanced motion forecasting\nmodel informed by multiple vector map elements, including lane boundaries and\nroad edges, that facilitates a richer and more complete representation of\ndriving environments. An effective feature fusion strategy is developed to\nmerge information in different vector map components, where the model learns\nholistic information on road structures and their interactions with agents.\nSince encoding more information about the road environment increases memory\nusage and is computationally expensive, we developed an effective pruning\nmechanism that filters the most relevant map connections to the target agent,\nensuring computational efficiency while maintaining essential spatial and\nsemantic relationships for accurate trajectory prediction. Overcoming the\nlimitations of lane centerline-based models, our method provides a more\ninformative and efficient representation of the driving environment and\nadvances the state of the art for autonomous vehicle motion forecasting. We\nverify our approach with extensive experiments on the Argoverse 2 motion\nforecasting dataset, where our method maintains competitiveness on AV2 while\nachieving improved performance.\n  Index Terms-Autonomous driving, trajectory prediction, vector map elements,\nroad topology, connection pruning, Argoverse 2."
    },
    {
        "date": "2025-07",
        "title": "Robust Brain Tumor Segmentation with Incomplete MRI Modalities Using H\u00f6lder Divergence and Mutual Information-Enhanced Knowledge Transfer",
        "author": "Runze Cheng, Xihang Qiu, Ming Li, Ye Zhang, Chun Li, and Fei Yu",
        "link": "http://arxiv.org/abs/2507.01254v1",
        "abstract": "Multimodal MRI provides critical complementary information for accurate brain\ntumor segmentation. However, conventional methods struggle when certain\nmodalities are missing due to issues such as image quality, protocol\ninconsistencies, patient allergies, or financial constraints. To address this,\nwe propose a robust single-modality parallel processing framework that achieves\nhigh segmentation accuracy even with incomplete modalities. Leveraging Holder\ndivergence and mutual information, our model maintains modality-specific\nfeatures while dynamically adjusting network parameters based on the available\ninputs. By using these divergence- and information-based loss functions, the\nframework effectively quantifies discrepancies between predictions and\nground-truth labels, resulting in consistently accurate segmentation. Extensive\nevaluations on the BraTS 2018 and BraTS 2020 datasets demonstrate superior\nperformance over existing methods in handling missing modalities."
    },
    {
        "date": "2025-07",
        "title": "FreqCross: A Multi-Modal Frequency-Spatial Fusion Network for Robust Detection of Stable Diffusion 3.5 Generated Images",
        "author": "Guang Yang",
        "link": "http://arxiv.org/abs/2507.02995v2",
        "abstract": "The rapid advancement of diffusion models, particularly Stable Diffusion 3.5,\nhas enabled the generation of highly photorealistic synthetic images that pose\nsignificant challenges to existing detection methods. This paper presents\nFreqCross, a novel multi-modal fusion network that combines spatial RGB\nfeatures, frequency domain artifacts, and radial energy distribution patterns\nto achieve robust detection of AI-generated images. Our approach leverages a\nthree-branch architecture: (1) a ResNet-18 backbone for spatial feature\nextraction, (2) a lightweight CNN for processing 2D FFT magnitude spectra, and\n(3) a multi-layer perceptron for analyzing radial energy profiles. We introduce\na novel radial energy distribution analysis that captures characteristic\nfrequency artifacts inherent in diffusion-generated images, and fuse it with\nspatial and spectral cues via simple feature concatenation followed by a\ncompact classification head. Extensive experiments on a dataset of 10,000\npaired real (MS-COCO) and synthetic (Stable Diffusion 3.5) images demonstrate\nthat FreqCross achieves 97.8\\% accuracy, outperforming state-of-the-art\nbaselines by 5.2\\%. The frequency analysis further reveals that synthetic\nimages exhibit distinct spectral signatures in the 0.1--0.4 normalised\nfrequency range, providing theoretical foundation for our approach. Code and\npre-trained models are publicly available to facilitate reproducible research."
    },
    {
        "date": "2025-07",
        "title": "Enabling Robust, Real-Time Verification of Vision-Based Navigation through View Synthesis",
        "author": "Marius Neuhalfen, Jonathan Grzymisch, and Manuel Sanchez-Gestido",
        "link": "http://arxiv.org/abs/2507.02993v1",
        "abstract": "This work introduces VISY-REVE: a novel pipeline to validate image processing\nalgorithms for Vision-Based Navigation. Traditional validation methods such as\nsynthetic rendering or robotic testbed acquisition suffer from difficult setup\nand slow runtime. Instead, we propose augmenting image datasets in real-time\nwith synthesized views at novel poses. This approach creates continuous\ntrajectories from sparse, pre-existing datasets in open or closed-loop. In\naddition, we introduce a new distance metric between camera poses, the\nBoresight Deviation Distance, which is better suited for view synthesis than\nexisting metrics. Using it, a method for increasing the density of image\ndatasets is developed."
    },
    {
        "date": "2025-07",
        "title": "Evaluating Robustness of Monocular Depth Estimation with Procedural Scene Perturbations",
        "author": "Jack Nugent, Siyang Wu, Zeyu Ma, Beining Han, Meenal Parakh, Abhishek Joshi, Lingjie Mei, Alexander Raistrick, Xinyuan Li, and Jia Deng",
        "link": "http://arxiv.org/abs/2507.00981v2",
        "abstract": "Recent years have witnessed substantial progress on monocular depth\nestimation, particularly as measured by the success of large models on standard\nbenchmarks. However, performance on standard benchmarks does not offer a\ncomplete assessment, because most evaluate accuracy but not robustness. In this\nwork, we introduce PDE (Procedural Depth Evaluation), a new benchmark which\nenables systematic robustness evaluation. PDE uses procedural generation to\ncreate 3D scenes that test robustness to various controlled perturbations,\nincluding object, camera, material and lighting changes. Our analysis yields\ninteresting findings on what perturbations are challenging for state-of-the-art\ndepth models, which we hope will inform further research. Code and data are\navailable at https://github.com/princeton-vl/proc-depth-eval."
    },
    {
        "date": "2025-07",
        "title": "Reasoning as an Adaptive Defense for Safety",
        "author": "Taeyoun Kim, Fahim Tajwar, Aditi Raghunathan, and Aviral Kumar",
        "link": "http://arxiv.org/abs/2507.00971v1",
        "abstract": "Reasoning methods that adaptively allocate test-time compute have advanced\nLLM performance on easy to verify domains such as math and code. In this work,\nwe study how to utilize this approach to train models that exhibit a degree of\nrobustness to safety vulnerabilities, and show that doing so can provide\nbenefits. We build a recipe called $\\textit{TARS}$ (Training Adaptive Reasoners\nfor Safety), a reinforcement learning (RL) approach that trains models to\nreason about safety using chain-of-thought traces and a reward signal that\nbalances safety with task completion. To build TARS, we identify three critical\ndesign choices: (1) a \"lightweight\" warmstart SFT stage, (2) a mix of harmful,\nharmless, and ambiguous prompts to prevent shortcut behaviors such as too many\nrefusals, and (3) a reward function to prevent degeneration of reasoning\ncapabilities during training. Models trained with TARS exhibit adaptive\nbehaviors by spending more compute on ambiguous queries, leading to better\nsafety-refusal trade-offs. They also internally learn to better distinguish\nbetween safe and unsafe prompts and attain greater robustness to both white-box\n(e.g., GCG) and black-box attacks (e.g., PAIR). Overall, our work provides an\neffective, open recipe for training LLMs against jailbreaks and harmful\nrequests by reasoning per prompt."
    },
    {
        "date": "2025-07",
        "title": "SafeMap: Robust HD Map Construction from Incomplete Observations",
        "author": "Xiaoshuai Hao, Lingdong Kong, Rong Yin, Pengwei Wang, Jing Zhang, Yunfeng Diao, and Shu Zhao",
        "link": "http://arxiv.org/abs/2507.00861v1",
        "abstract": "Robust high-definition (HD) map construction is vital for autonomous driving,\nyet existing methods often struggle with incomplete multi-view camera data.\nThis paper presents SafeMap, a novel framework specifically designed to secure\naccuracy even when certain camera views are missing. SafeMap integrates two key\ncomponents: the Gaussian-based Perspective View Reconstruction (G-PVR) module\nand the Distillation-based Bird's-Eye-View (BEV) Correction (D-BEVC) module.\nG-PVR leverages prior knowledge of view importance to dynamically prioritize\nthe most informative regions based on the relationships among available camera\nviews. Furthermore, D-BEVC utilizes panoramic BEV features to correct the BEV\nrepresentations derived from incomplete observations. Together, these\ncomponents facilitate the end-to-end map reconstruction and robust HD map\ngeneration. SafeMap is easy to implement and integrates seamlessly into\nexisting systems, offering a plug-and-play solution for enhanced robustness.\nExperimental results demonstrate that SafeMap significantly outperforms\nprevious methods in both complete and incomplete scenarios, highlighting its\nsuperior performance and reliability."
    },
    {
        "date": "2025-07",
        "title": "Robust Component Detection for Flexible Manufacturing: A Deep Learning Approach to Tray-Free Object Recognition under Variable Lighting",
        "author": "Fatemeh Sadat Daneshmand",
        "link": "http://arxiv.org/abs/2507.00852v1",
        "abstract": "Flexible manufacturing systems in Industry 4.0 require robots capable of\nhandling objects in unstructured environments without rigid positioning\nconstraints. This paper presents a computer vision system that enables\nindustrial robots to detect and grasp pen components in arbitrary orientations\nwithout requiring structured trays, while maintaining robust performance under\nvarying lighting conditions. We implement and evaluate a Mask R-CNN-based\napproach on a complete pen manufacturing line at ZHAW, addressing three\ncritical challenges: object detection without positional constraints,\nrobustness to extreme lighting variations, and reliable performance with\ncost-effective cameras. Our system achieves 95% detection accuracy across\ndiverse lighting conditions while eliminating the need for structured component\nplacement, demonstrating a 30% reduction in setup time and significant\nimprovement in manufacturing flexibility. The approach is validated through\nextensive testing under four distinct lighting scenarios, showing practical\napplicability for real-world industrial deployment."
    },
    {
        "date": "2025-07",
        "title": "Stealtooth: Breaking Bluetooth Security Abusing Silent Automatic Pairing",
        "author": "Keiichiro Kimura, Hiroki Kuzuno, Yoshiaki Shiraishi, and Masakatu Morii",
        "link": "http://arxiv.org/abs/2507.00847v2",
        "abstract": "Bluetooth is a pervasive wireless communication technology used by billions\nof devices for short-range connectivity. The security of Bluetooth relies on\nthe pairing process, where devices establish shared long-term keys for secure\ncommunications. However, many commercial Bluetooth devices implement automatic\npairing functions to improve user convenience, creating a previously unexplored\nattack surface.\n  We present Stealtooth, a novel attack that abuses unknown vulnerabilities in\nthe automatic pairing functions in commercial Bluetooth devices to achieve\ncompletely silent device link key overwriting. The Stealtooth attack leverages\nthe fact that Bluetooth audio devices automatically transition to pairing mode\nunder specific conditions, enabling attackers to hijack pairing processes\nwithout user awareness or specialized tools. We also extend the attack into the\nMitM Stealtooth attack, combining automatic pairing abuse with power-saving\nmode techniques to enable man-in-the-middle attacks.\n  We evaluate the attacks against 10 commercial Bluetooth devices from major\nmanufacturers, demonstrating widespread vulnerabilities across diverse device\ntypes and manufacturers. Our practical implementation requires only commodity\nhardware and open-source software, highlighting the low barrier to entry for\nattackers.\n  We propose defenses both device and protocol levels, including enhanced user\nnotifications and standardized automatic pairing guidelines. Our findings\nreveal a critical tension between security and usability, showing that current\nautomatic pairing implementations create systematic vulnerabilities. We\nresponsibly disclosed our findings to affected vendors, with several already\nreleasing patches."
    },
    {
        "date": "2025-07",
        "title": "CAVALRY-V: A Large-Scale Generator Framework for Adversarial Attacks on Video MLLMs",
        "author": "Jiaming Zhang, Rui Hu, Qing Guo, and Wei Yang Bryan Lim",
        "link": "http://arxiv.org/abs/2507.00817v1",
        "abstract": "Video Multimodal Large Language Models (V-MLLMs) have shown impressive\ncapabilities in temporal reasoning and cross-modal understanding, yet their\nvulnerability to adversarial attacks remains underexplored due to unique\nchallenges: complex cross-modal reasoning mechanisms, temporal dependencies,\nand computational constraints. We present CAVALRY-V (Cross-modal\nLanguage-Vision Adversarial Yielding for Videos), a novel framework that\ndirectly targets the critical interface between visual perception and language\ngeneration in V-MLLMs. Our approach introduces two key innovations: (1) a\ndual-objective semantic-visual loss function that simultaneously disrupts the\nmodel's text generation logits and visual representations to undermine\ncross-modal integration, and (2) a computationally efficient two-stage\ngenerator framework that combines large-scale pre-training for cross-model\ntransferability with specialized fine-tuning for spatiotemporal coherence.\nEmpirical evaluation on comprehensive video understanding benchmarks\ndemonstrates that CAVALRY-V significantly outperforms existing attack methods,\nachieving 22.8% average improvement over the best baseline attacks on both\ncommercial systems (GPT-4.1, Gemini 2.0) and open-source models (QwenVL-2.5,\nInternVL-2.5, Llava-Video, Aria, MiniCPM-o-2.6). Our framework achieves\nflexibility through implicit temporal coherence modeling rather than explicit\nregularization, enabling significant performance improvements even on image\nunderstanding (34.4% average gain). This capability demonstrates CAVALRY-V's\npotential as a foundational approach for adversarial research across multimodal\nsystems."
    },
    {
        "date": "2025-07",
        "title": "A Robust Algorithm for Non-IID Machine Learning Problems with Convergence Analysis",
        "author": "Qing Xu, and Xiaohua Xuan",
        "link": "http://arxiv.org/abs/2507.00810v1",
        "abstract": "In this paper, we propose an improved numerical algorithm for solving minimax\nproblems based on nonsmooth optimization, quadratic programming and iterative\nprocess. We also provide a rigorous proof of convergence for our algorithm\nunder some mild assumptions, such as gradient continuity and boundedness. Such\nan algorithm can be widely applied in various fields such as robust\noptimization, imbalanced learning, etc."
    },
    {
        "date": "2025-07",
        "title": "Multi-Modal Graph Convolutional Network with Sinusoidal Encoding for Robust Human Action Segmentation",
        "author": "Hao Xing, Kai Zhe Boey, Yuankai Wu, Darius Burschka, and Gordon Cheng",
        "link": "http://arxiv.org/abs/2507.00752v1",
        "abstract": "Accurate temporal segmentation of human actions is critical for intelligent\nrobots in collaborative settings, where a precise understanding of sub-activity\nlabels and their temporal structure is essential. However, the inherent noise\nin both human pose estimation and object detection often leads to\nover-segmentation errors, disrupting the coherence of action sequences. To\naddress this, we propose a Multi-Modal Graph Convolutional Network (MMGCN) that\nintegrates low-frame-rate (e.g., 1 fps) visual data with high-frame-rate (e.g.,\n30 fps) motion data (skeleton and object detections) to mitigate fragmentation.\nOur framework introduces three key contributions. First, a sinusoidal encoding\nstrategy that maps 3D skeleton coordinates into a continuous sin-cos space to\nenhance spatial representation robustness. Second, a temporal graph fusion\nmodule that aligns multi-modal inputs with differing resolutions via\nhierarchical feature aggregation, Third, inspired by the smooth transitions\ninherent to human actions, we design SmoothLabelMix, a data augmentation\ntechnique that mixes input sequences and labels to generate synthetic training\nexamples with gradual action transitions, enhancing temporal consistency in\npredictions and reducing over-segmentation artifacts.\n  Extensive experiments on the Bimanual Actions Dataset, a public benchmark for\nhuman-object interaction understanding, demonstrate that our approach\noutperforms state-of-the-art methods, especially in action segmentation\naccuracy, achieving F1@10: 94.5% and F1@25: 92.8%."
    },
    {
        "date": "2025-07",
        "title": "Safe Low Bandwidth SPV: A Formal Treatment of Simplified Payment Verification Protocols and Security Bounds",
        "author": "Craig S Wright",
        "link": "http://arxiv.org/abs/2507.00740v1",
        "abstract": "This paper presents a complete formal specification, protocol description,\nand mathematical proof structure for Simplified Payment Verification (SPV) as\noriginally defined in the Bitcoin whitepaper \\cite{nakamoto2008}. In stark\ncontrast to the misrepresentations proliferated by popular implementations, we\nshow that SPV is not only secure under bounded adversarial assumptions but\nstrictly optimal for digital cash systems requiring scalable and verifiable\ntransaction inclusion. We reconstruct the SPV protocol from first principles,\ngrounding its verification model in symbolic automata, Merkle membership\nrelations, and chain-of-proof dominance predicates. Through rigorous\nprobabilistic and game-theoretic analysis, we derive the economic bounds within\nwhich the protocol operates securely and verify its liveness and safety\nproperties under partial connectivity, hostile relay networks, and adversarial\npropagation delay. Our specification further introduces low-bandwidth\noptimisations such as adaptive polling and compressed header synchronisation\nwhile preserving correctness. This document serves both as a blueprint for\nsecure SPV implementation and a rebuttal of common misconceptions surrounding\nnon-validating clients."
    },
    {
        "date": "2025-07",
        "title": "Cage-Based Deformation for Transferable and Undefendable Point Cloud Attack",
        "author": "Keke Tang, Ziyong Du, Weilong Peng, Xiaofei Wang, Peican Zhu, Ligang Liu, and Zhihong Tian",
        "link": "http://arxiv.org/abs/2507.00690v1",
        "abstract": "Adversarial attacks on point clouds often impose strict geometric constraints\nto preserve plausibility; however, such constraints inherently limit\ntransferability and undefendability. While deformation offers an alternative,\nexisting unstructured approaches may introduce unnatural distortions, making\nadversarial point clouds conspicuous and undermining their plausibility. In\nthis paper, we propose CageAttack, a cage-based deformation framework that\nproduces natural adversarial point clouds. It first constructs a cage around\nthe target object, providing a structured basis for smooth, natural-looking\ndeformation. Perturbations are then applied to the cage vertices, which\nseamlessly propagate to the point cloud, ensuring that the resulting\ndeformations remain intrinsic to the object and preserve plausibility.\nExtensive experiments on seven 3D deep neural network classifiers across three\ndatasets show that CageAttack achieves a superior balance among\ntransferability, undefendability, and plausibility, outperforming\nstate-of-the-art methods. Codes will be made public upon acceptance."
    },
    {
        "date": "2025-07",
        "title": "Diffusion Classifier Guidance for Non-robust Classifiers",
        "author": "Philipp Vaeth, Dibyanshu Kumar, Benjamin Paassen, and Magda Gregorov\u00e1",
        "link": "http://arxiv.org/abs/2507.00687v1",
        "abstract": "Classifier guidance is intended to steer a diffusion process such that a\ngiven classifier reliably recognizes the generated data point as a certain\nclass. However, most classifier guidance approaches are restricted to robust\nclassifiers, which were specifically trained on the noise of the diffusion\nforward process. We extend classifier guidance to work with general,\nnon-robust, classifiers that were trained without noise. We analyze the\nsensitivity of both non-robust and robust classifiers to noise of the diffusion\nprocess on the standard CelebA data set, the specialized SportBalls data set\nand the high-dimensional real-world CelebA-HQ data set. Our findings reveal\nthat non-robust classifiers exhibit significant accuracy degradation under\nnoisy conditions, leading to unstable guidance gradients. To mitigate these\nissues, we propose a method that utilizes one-step denoised image predictions\nand implements stabilization techniques inspired by stochastic optimization\nmethods, such as exponential moving averages. Experimental results demonstrate\nthat our approach improves the stability of classifier guidance while\nmaintaining sample diversity and visual quality. This work contributes to\nadvancing conditional sampling techniques in generative models, enabling a\nbroader range of classifiers to be used as guidance classifiers."
    },
    {
        "date": "2025-07",
        "title": "Integrating Network and Attack Graphs for Service-Centric Impact Analysis",
        "author": "Joni Herttuainen, Vesa Kuikka, and Kimmo K. Kaski",
        "link": "http://arxiv.org/abs/2507.00637v1",
        "abstract": "We present a novel methodology for modelling, visualising, and analysing\ncyber threats, attack paths, as well as their impact on user services in\nenterprise or infrastructure networks of digital devices and services they\nprovide. Using probabilistic methods to track the propagation of an attack\nthrough attack graphs, via the service or application layers, and on physical\ncommunication networks, our model enables us to analyse cyber attacks at\ndifferent levels of detail. Understanding the propagation of an attack within a\nservice among microservices and its spread between different services or\napplication servers could help detect and mitigate it early. We demonstrate\nthat this network-based influence spreading modelling approach enables the\nevaluation of diverse attack scenarios and the development of protection and\nmitigation measures, taking into account the criticality of services from the\nuser's perspective. This methodology could also aid security specialists and\nsystem administrators in making well-informed decisions regarding risk\nmitigation strategies."
    },
    {
        "date": "2025-07",
        "title": "The Secrets Must Not Flow: Scaling Security Verification to Large Codebases (extended version)",
        "author": "Linard Arquint, Samarth Kishor, Jason R. Koenig, Joey Dodds, Daniel Kroening, and Peter M\u00fcller",
        "link": "http://arxiv.org/abs/2507.00595v1",
        "abstract": "Existing program verifiers can prove advanced properties about security\nprotocol implementations, but are difficult to scale to large codebases because\nof the manual effort required. We develop a novel methodology called *Diodon*\nthat addresses this challenge by splitting the codebase into the protocol\nimplementation (the *Core*) and the remainder (the *Application*). This split\nallows us to apply powerful semi-automated verification techniques to the\nsecurity-critical Core, while fully-automatic static analyses scale the\nverification to the entire codebase by ensuring that the Application cannot\ninvalidate the security properties proved for the Core. The static analyses\nachieve that by proving *I/O independence*, i.e., that the I/O operations\nwithin the Application are independent of the Core's security-relevant data\n(such as keys), and that the Application meets the Core's requirements. We have\nproved Diodon sound by first showing that we can safely allow the Application\nto perform I/O independent of the security protocol, and second that manual\nverification and static analyses soundly compose. We evaluate Diodon on two\ncase studies: an implementation of the signed Diffie-Hellman key exchange and a\nlarge (100k+ LoC) production Go codebase implementing a key exchange protocol\nfor which we obtained secrecy and injective agreement guarantees by verifying a\nCore of about 1% of the code with the auto-active program verifier Gobra in\nless than three person months."
    },
    {
        "date": "2025-07",
        "title": "BadViM: Backdoor Attack against Vision Mamba",
        "author": "Yinghao Wu, and Liyan Zhang",
        "link": "http://arxiv.org/abs/2507.00577v1",
        "abstract": "Vision State Space Models (SSMs), particularly architectures like Vision\nMamba (ViM), have emerged as promising alternatives to Vision Transformers\n(ViTs). However, the security implications of this novel architecture,\nespecially their vulnerability to backdoor attacks, remain critically\nunderexplored. Backdoor attacks aim to embed hidden triggers into victim\nmodels, causing the model to misclassify inputs containing these triggers while\nmaintaining normal behavior on clean inputs. This paper investigates the\nsusceptibility of ViM to backdoor attacks by introducing BadViM, a novel\nbackdoor attack framework specifically designed for Vision Mamba. The proposed\nBadViM leverages a Resonant Frequency Trigger (RFT) that exploits the frequency\nsensitivity patterns of the victim model to create stealthy, distributed\ntriggers. To maximize attack efficacy, we propose a Hidden State Alignment loss\nthat strategically manipulates the internal representations of model by\naligning the hidden states of backdoor images with those of target classes.\nExtensive experimental results demonstrate that BadViM achieves superior attack\nsuccess rates while maintaining clean data accuracy. Meanwhile, BadViM exhibits\nremarkable resilience against common defensive measures, including PatchDrop,\nPatchShuffle and JPEG compression, which typically neutralize normal backdoor\nattacks."
    },
    {
        "date": "2025-07",
        "title": "Cyber Attacks Detection, Prevention, and Source Localization in Digital Substation Communication using Hybrid Statistical-Deep Learning",
        "author": "Nicola Cibin, Bas Mulder, Herman Carstens, Peter Palensky, and Alexandru \u015etefanov",
        "link": "http://arxiv.org/abs/2507.00522v1",
        "abstract": "The digital transformation of power systems is accelerating the adoption of\nIEC 61850 standard. However, its communication protocols, including Sampled\nValues (SV), lack built-in security features such as authentication and\nencryption, making them vulnerable to malicious packet injection. Such cyber\nattacks can delay fault clearance or trigger unintended circuit breaker\noperations. While most existing research focuses on detecting cyber attacks in\ndigital substations, intrusion prevention systems have been disregarded because\nof the risk of potential communication network disruptions. This paper proposes\na novel method using hybrid statistical-deep learning for the detection,\nprevention, and source localization of IEC 61850 SV injection attacks. The\nmethod uses exponentially modified Gaussian distributions to model\ncommunication network latency and long short-term memory and Elman recurrent\nneural network to detect anomalous variations in the estimated probability\ndistributions. It effectively discards malicious SV frames with minimal\nprocessing overhead and latency, maintains robustness against communication\nnetwork latency variation and time-synchronization issues, and guarantees a\nnear-zero false positive rate in non-attack scenarios. Comprehensive validation\nis conducted on three testbeds involving industrial-grade devices,\nhardware-in-the-loop simulations, virtualized intelligent electronic devices\nand merging units, and high-fidelity emulated communication networks. Results\ndemonstrate the method's suitability for practical deployment in IEC\n61850-compliant digital substations."
    },
    {
        "date": "2025-07",
        "title": "TeamCMU at Touch\u00e9: Adversarial Co-Evolution for Advertisement Integration and Detection in Conversational Search",
        "author": "To Eun Kim, Jo\u00e3o Coelho, Gbemileke Onilude, and Jai Singh",
        "link": "http://arxiv.org/abs/2507.00509v1",
        "abstract": "As conversational search engines increasingly adopt generation-based\nparadigms powered by Large Language Models (LLMs) and Retrieval-Augmented\nGeneration (RAG), the integration of advertisements into generated responses\npresents both commercial opportunities and challenges for user experience.\nUnlike traditional search, where advertisements are clearly delineated,\ngenerative systems blur the boundary between informational content and\npromotional material, raising concerns around transparency and trust. In this\nwork, we propose a modular pipeline for advertisement management in RAG-based\nconversational systems, consisting of an ad-rewriter for seamless ad\nintegration and a robust ad-classifier for detection. We leverage synthetic\ndata to train high-performing classifiers, which are then used to guide two\ncomplementary ad-integration strategies: supervised fine-tuning of the\nad-rewriter and a best-of-N sampling approach that selects the least detectable\nad-integrated response among multiple candidates. Our evaluation focuses on two\ncore questions: the effectiveness of ad classifiers in detecting diverse ad\nintegration strategies, and the training methods that best support coherent,\nminimally intrusive ad insertion. Experimental results show that our\nad-classifier, trained on synthetic advertisement data inspired by marketing\nstrategies and enhanced through curriculum learning, achieves robust detection\nperformance. Additionally, we demonstrate that classifier-guided optimization,\nthrough both fine-tuning and best-of-N sampling, significantly improves ad\nstealth, enabling more seamless integration. These findings contribute an\nadversarial co-evolution framework for developing more sophisticated ad-aware\ngenerative search systems and robust ad classifiers."
    },
    {
        "date": "2025-07",
        "title": "SCING:Towards More Efficient and Robust Person Re-Identification through Selective Cross-modal Prompt Tuning",
        "author": "Yunfei Xie, Yuxuan Cheng, Juncheng Wu, Haoyu Zhang, Yuyin Zhou, and Shoudong Han",
        "link": "http://arxiv.org/abs/2507.00506v1",
        "abstract": "Recent advancements in adapting vision-language pre-training models like CLIP\nfor person re-identification (ReID) tasks often rely on complex adapter design\nor modality-specific tuning while neglecting cross-modal interaction, leading\nto high computational costs or suboptimal alignment. To address these\nlimitations, we propose a simple yet effective framework named Selective\nCross-modal Prompt Tuning (SCING) that enhances cross-modal alignment and\nrobustness against real-world perturbations. Our method introduces two key\ninnovations: Firstly, we proposed Selective Visual Prompt Fusion (SVIP), a\nlightweight module that dynamically injects discriminative visual features into\ntext prompts via a cross-modal gating mechanism. Moreover, the proposed\nPerturbation-Driven Consistency Alignment (PDCA) is a dual-path training\nstrategy that enforces invariant feature alignment under random image\nperturbations by regularizing consistency between original and augmented\ncross-modal embeddings. Extensive experiments are conducted on several popular\nbenchmarks covering Market1501, DukeMTMC-ReID, Occluded-Duke, Occluded-REID,\nand P-DukeMTMC, which demonstrate the impressive performance of the proposed\nmethod. Notably, our framework eliminates heavy adapters while maintaining\nefficient inference, achieving an optimal trade-off between performance and\ncomputational overhead. The code will be released upon acceptance."
    },
    {
        "date": "2025-07",
        "title": "PNAct: Crafting Backdoor Attacks in Safe Reinforcement Learning",
        "author": "Weiran Guo, Guanjun Liu, Ziyuan Zhou, and Ling Wang",
        "link": "http://arxiv.org/abs/2507.00485v1",
        "abstract": "Reinforcement Learning (RL) is widely used in tasks where agents interact\nwith an environment to maximize rewards. Building on this foundation, Safe\nReinforcement Learning (Safe RL) incorporates a cost metric alongside the\nreward metric, ensuring that agents adhere to safety constraints during\ndecision-making. In this paper, we identify that Safe RL is vulnerable to\nbackdoor attacks, which can manipulate agents into performing unsafe actions.\nFirst, we introduce the relevant concepts and evaluation metrics for backdoor\nattacks in Safe RL. It is the first attack framework in the Safe RL field that\ninvolves both Positive and Negative Action sample (PNAct) is to implant\nbackdoors, where positive action samples provide reference actions and negative\naction samples indicate actions to be avoided. We theoretically point out the\nproperties of PNAct and design an attack algorithm. Finally, we conduct\nexperiments to evaluate the effectiveness of our proposed backdoor attack\nframework, evaluating it with the established metrics. This paper highlights\nthe potential risks associated with Safe RL and underscores the feasibility of\nsuch attacks. Our code and supplementary material are available at\nhttps://github.com/azure-123/PNAct."
    },
    {
        "date": "2025-07",
        "title": "Find a Scapegoat: Poisoning Membership Inference Attack and Defense to Federated Learning",
        "author": "Wenjin Mo, Zhiyuan Li, Minghong Fang, and Mingwei Fang",
        "link": "http://arxiv.org/abs/2507.00423v1",
        "abstract": "Federated learning (FL) allows multiple clients to collaboratively train a\nglobal machine learning model with coordination from a central server, without\nneeding to share their raw data. This approach is particularly appealing in the\nera of privacy regulations like the GDPR, leading many prominent companies to\nadopt it. However, FL's distributed nature makes it susceptible to poisoning\nattacks, where malicious clients, controlled by an attacker, send harmful data\nto compromise the model. Most existing poisoning attacks in FL aim to degrade\nthe model's integrity, such as reducing its accuracy, with limited attention to\nprivacy concerns from these attacks. In this study, we introduce FedPoisonMIA,\na novel poisoning membership inference attack targeting FL. FedPoisonMIA\ninvolves malicious clients crafting local model updates to infer membership\ninformation. Additionally, we propose a robust defense mechanism to mitigate\nthe impact of FedPoisonMIA attacks. Extensive experiments across various\ndatasets demonstrate the attack's effectiveness, while our defense approach\nreduces its impact to a degree."
    },
    {
        "date": "2025-06",
        "title": "Are AI-Generated Fixes Secure? Analyzing LLM and Agent Patches on SWE-bench",
        "author": "Amirali Sajadi, Kostadin Damevski, and Preetha Chatterjee",
        "link": "http://arxiv.org/abs/2507.02976v1",
        "abstract": "Large Language Models (LLMs) and their agentic frameworks are increasingly\nadopted to automate software development tasks such as issue resolution and\nprogram repair. While prior work has identified security risks in LLM-generated\ncode, most evaluations have focused on synthetic or isolated settings, leaving\nopen questions about the security of these systems in real-world development\ncontexts. In this study, we present the first large-scale security analysis of\nLLM-generated patches using 20,000+ issues from the SWE-bench dataset. We\nevaluate patches produced by a standalone LLM (Llama 3.3) and compare them to\ndeveloper-written patches. We also assess the security of patches generated by\nthree top-performing agentic frameworks (OpenHands, AutoCodeRover, HoneyComb)\non a subset of our data. Finally, we analyze a wide range of code, issue, and\nproject-level factors to understand the conditions under which LLMs and agents\nare most likely to generate insecure code. Our findings reveal that the\nstandalone LLM introduces nearly 9x more new vulnerabilities than developers,\nwith many of these exhibiting unique patterns not found in developers' code.\nAgentic workflows also generate a significant number of vulnerabilities,\nparticularly when granting LLMs more autonomy, potentially increasing the\nlikelihood of misinterpreting project context or task requirements. We find\nthat vulnerabilities are more likely to occur in LLM patches associated with a\nhigher number of files, more lines of generated code, and GitHub issues that\nlack specific code snippets or information about the expected code behavior and\nsteps to reproduce. These results suggest that contextual factors play a\ncritical role in the security of the generated code and point toward the need\nfor proactive risk assessment methods that account for both code and\nissue-level information to complement existing vulnerability detection tools."
    },
    {
        "date": "2025-06",
        "title": "SQUASH: A SWAP-Based Quantum Attack to Sabotage Hybrid Quantum Neural Networks",
        "author": "Rahul Kumar, Wenqi Wei, Ying Mao, Junaid Farooq, Ying Wang, and Juntao Chen",
        "link": "http://arxiv.org/abs/2506.24081v1",
        "abstract": "We propose a circuit-level attack, SQUASH, a SWAP-Based Quantum Attack to\nsabotage Hybrid Quantum Neural Networks (HQNNs) for classification tasks.\nSQUASH is executed by inserting SWAP gate(s) into the variational quantum\ncircuit of the victim HQNN. Unlike conventional noise-based or adversarial\ninput attacks, SQUASH directly manipulates the circuit structure, leading to\nqubit misalignment and disrupting quantum state evolution. This attack is\nhighly stealthy, as it does not require access to training data or introduce\ndetectable perturbations in input states. Our results demonstrate that SQUASH\nsignificantly degrades classification performance, with untargeted SWAP attacks\nreducing accuracy by up to 74.08\\% and targeted SWAP attacks reducing target\nclass accuracy by up to 79.78\\%. These findings reveal a critical vulnerability\nin HQNN implementations, underscoring the need for more resilient architectures\nagainst circuit-level adversarial interventions."
    },
    {
        "date": "2025-06",
        "title": "STACK: Adversarial Attacks on LLM Safeguard Pipelines",
        "author": "Ian R. McKenzie, Oskar J. Hollinsworth, Tom Tseng, Xander Davies, Stephen Casper, Aaron D. Tucker, Robert Kirk, and Adam Gleave",
        "link": "http://arxiv.org/abs/2506.24068v1",
        "abstract": "Frontier AI developers are relying on layers of safeguards to protect against\ncatastrophic misuse of AI systems. Anthropic guards their latest Claude 4 Opus\nmodel using one such defense pipeline, and other frontier developers including\nGoogle DeepMind and OpenAI pledge to soon deploy similar defenses. However, the\nsecurity of such pipelines is unclear, with limited prior work evaluating or\nattacking these pipelines. We address this gap by developing and red-teaming an\nopen-source defense pipeline. First, we find that a novel few-shot-prompted\ninput and output classifier outperforms state-of-the-art open-weight safeguard\nmodel ShieldGemma across three attacks and two datasets, reducing the attack\nsuccess rate (ASR) to 0% on the catastrophic misuse dataset ClearHarm. Second,\nwe introduce a STaged AttaCK (STACK) procedure that achieves 71% ASR on\nClearHarm in a black-box attack against the few-shot-prompted classifier\npipeline. Finally, we also evaluate STACK in a transfer setting, achieving 33%\nASR, providing initial evidence that it is feasible to design attacks with no\naccess to the target pipeline. We conclude by suggesting specific mitigations\nthat developers could use to thwart staged attacks."
    },
    {
        "date": "2025-06",
        "title": "Consensus-based optimization for closed-box adversarial attacks and a connection to evolution strategies",
        "author": "Tim Roith, Leon Bungert, and Philipp Wacker",
        "link": "http://arxiv.org/abs/2506.24048v1",
        "abstract": "Consensus-based optimization (CBO) has established itself as an efficient\ngradient-free optimization scheme, with attractive mathematical properties,\nsuch as mean-field convergence results for non-convex loss functions. In this\nwork, we study CBO in the context of closed-box adversarial attacks, which are\nimperceptible input perturbations that aim to fool a classifier, without\naccessing its gradient. Our contribution is to establish a connection between\nthe so-called consensus hopping as introduced by Riedl et al. and natural\nevolution strategies (NES) commonly applied in the context of adversarial\nattacks and to rigorously relate both methods to gradient-based optimization\nschemes. Beyond that, we provide a comprehensive experimental study that shows\nthat despite the conceptual similarities, CBO can outperform NES and other\nevolutionary strategies in certain scenarios."
    },
    {
        "date": "2025-06",
        "title": "Poisoning Attacks to Local Differential Privacy for Ranking Estimation",
        "author": "Pei Zhan, Peng Tang, Yangzhuo Li, Puwen Wei, and Shanqing Guo",
        "link": "http://arxiv.org/abs/2506.24033v1",
        "abstract": "Local differential privacy (LDP) involves users perturbing their inputs to\nprovide plausible deniability of their data. However, this also makes LDP\nvulnerable to poisoning attacks. In this paper, we first introduce novel\npoisoning attacks for ranking estimation. These attacks are intricate, as fake\nattackers do not merely adjust the frequency of target items. Instead, they\nleverage a limited number of fake users to precisely modify frequencies,\neffectively altering item rankings to maximize gains. To tackle this challenge,\nwe introduce the concepts of attack cost and optimal attack item (set), and\npropose corresponding strategies for kRR, OUE, and OLH protocols. For kRR, we\niteratively select optimal attack items and allocate suitable fake users. For\nOUE, we iteratively determine optimal attack item sets and consider the\nincremental changes in item frequencies across different sets. Regarding OLH,\nwe develop a harmonic cost function based on the pre-image of a hash to select\nthat supporting a larger number of effective attack items. Lastly, we present\nan attack strategy based on confidence levels to quantify the probability of a\nsuccessful attack and the number of attack iterations more precisely. We\ndemonstrate the effectiveness of our attacks through theoretical and empirical\nevidence, highlighting the necessity for defenses against these attacks. The\nsource code and data have been made available at\nhttps://github.com/LDP-user/LDP-Ranking.git."
    },
    {
        "date": "2025-06",
        "title": "A Scalable Approach for Safe and Robust Learning via Lipschitz-Constrained Networks",
        "author": "Zain ul Abdeen, Vassilis Kekatos, and Ming Jin",
        "link": "http://arxiv.org/abs/2506.23977v1",
        "abstract": "Certified robustness is a critical property for deploying neural networks\n(NN) in safety-critical applications. A principle approach to achieving such\nguarantees is to constrain the global Lipschitz constant of the network.\nHowever, accurate methods for Lipschitz-constrained training often suffer from\nnon-convex formulations and poor scalability due to reliance on global\nsemidefinite programs (SDPs). In this letter, we propose a convex training\nframework that enforces global Lipschitz constraints via semidefinite\nrelaxation. By reparameterizing the NN using loop transformation, we derive a\nconvex admissibility condition that enables tractable and certifiable training.\nWhile the resulting formulation guarantees robustness, its scalability is\nlimited by the size of global SDP. To overcome this, we develop a randomized\nsubspace linear matrix inequalities (RS-LMI) approach that decomposes the\nglobal constraints into sketched layerwise constraints projected onto\nlow-dimensional subspaces, yielding a smooth and memory-efficient training\nobjective. Empirical results on MNIST, CIFAR-10, and ImageNet demonstrate that\nthe proposed framework achieves competitive accuracy with significantly\nimproved Lipschitz bounds and runtime performance."
    },
    {
        "date": "2025-06",
        "title": "Toward Simple and Robust Contrastive Explanations for Image Classification by Leveraging Instance Similarity and Concept Relevance",
        "author": "Yuliia Kaidashova, Bettina Finzel, and Ute Schmid",
        "link": "http://arxiv.org/abs/2506.23975v1",
        "abstract": "Understanding why a classification model prefers one class over another for\nan input instance is the challenge of contrastive explanation. This work\nimplements concept-based contrastive explanations for image classification by\nleveraging the similarity of instance embeddings and relevance of\nhuman-understandable concepts used by a fine-tuned deep learning model. Our\napproach extracts concepts with their relevance score, computes contrasts for\nsimilar instances, and evaluates the resulting contrastive explanations based\non explanation complexity. Robustness is tested for different image\naugmentations. Two research questions are addressed: (1) whether explanation\ncomplexity varies across different relevance ranges, and (2) whether\nexplanation complexity remains consistent under image augmentations such as\nrotation and noise. The results confirm that for our experiments higher concept\nrelevance leads to shorter, less complex explanations, while lower relevance\nresults in longer, more diffuse explanations. Additionally, explanations show\nvarying degrees of robustness. The discussion of these findings offers insights\ninto the potential of building more interpretable and robust AI systems."
    },
    {
        "date": "2025-06",
        "title": "Learning robust parameter inference and density reconstruction in flyer plate impact experiments",
        "author": "Evan Bell, Daniel A. Serino, Ben S. Southworth, Trevor Wilcox, and Marc L. Klasky",
        "link": "http://arxiv.org/abs/2506.23914v1",
        "abstract": "Estimating physical parameters or material properties from experimental\nobservations is a common objective in many areas of physics and material\nscience. In many experiments, especially in shock physics, radiography is the\nprimary means of observing the system of interest. However, radiography does\nnot provide direct access to key state variables, such as density, which\nprevents the application of traditional parameter estimation approaches. Here\nwe focus on flyer plate impact experiments on porous materials, and resolving\nthe underlying parameterized equation of state (EoS) and crush porosity model\nparameters given radiographic observation(s). We use machine learning as a tool\nto demonstrate with high confidence that using only high impact velocity data\ndoes not provide sufficient information to accurately infer both EoS and crush\nmodel parameters, even with fully resolved density fields or a dynamic sequence\nof images. We thus propose an observable data set consisting of low and high\nimpact velocity experiments/simulations that capture different regimes of\ncompaction and shock propagation, and proceed to introduce a generative machine\nlearning approach which produces a posterior distribution of physical\nparameters directly from radiographs. We demonstrate the effectiveness of the\napproach in estimating parameters from simulated flyer plate impact\nexperiments, and show that the obtained estimates of EoS and crush model\nparameters can then be used in hydrodynamic simulations to obtain accurate and\nphysically admissible density reconstructions. Finally, we examine the\nrobustness of the approach to model mismatches, and find that the learned\napproach can provide useful parameter estimates in the presence of\nout-of-distribution radiographic noise and previously unseen physics, thereby\npromoting a potential breakthrough in estimating material properties from\nexperimental radiographic images."
    },
    {
        "date": "2025-06",
        "title": "Exploring Privacy and Security as Drivers for Environmental Sustainability in Cloud-Based Office Solutions",
        "author": "Jason Kayembe, Iness Ben Guirat, and Jan Tobias M\u00fchlberg",
        "link": "http://arxiv.org/abs/2506.23866v3",
        "abstract": "In this paper, we explore the intersection of privacy, security, and\nenvironmental sustainability in cloud-based office solutions, focusing on\nquantifying user- and network-side energy use and associated carbon emissions.\nWe hypothesise that privacy-focused services are typically more\nenergy-efficient than those funded through data collection and advertising. To\nevaluate this, we propose a framework that systematically measures\nenvironmental costs based on energy usage and network data traffic during\nwell-defined, automated usage scenarios. To test our hypothesis, we first\nanalyse how underlying architectures and business models, such as monetisation\nthrough personalised advertising, contribute to the environmental footprint of\nthese services. We then explore existing methodologies and tools for software\nenvironmental impact assessment. We apply our framework to three mainstream\nemail services selected to reflect different privacy policies, from\nad-supported tracking-intensive models to privacy-focused designs: Microsoft\nOutlook, Google Mail (Gmail), and Proton Mail. We extend this comparison to a\nself-hosted email solution, evaluated with and without end-to-end encryption.\nWe show that the self-hosted solution, even with 14% of device energy and 15%\nof emissions overheads from PGP encryption, remains the most energy-efficient,\nsaving up to 33% of emissions per session compared to Gmail. Among commercial\nproviders, Proton Mail is the most efficient, saving up to 0.1 gCO2 e per\nsession compared to Outlook, whose emissions can be further reduced by 2%\nthrough ad-blocking."
    },
    {
        "date": "2025-06",
        "title": "A Survey on Autonomy-Induced Security Risks in Large Model-Based Agents",
        "author": "Hang Su, Jun Luo, Chang Liu, Xiao Yang, Yichi Zhang, Yinpeng Dong, and Jun Zhu",
        "link": "http://arxiv.org/abs/2506.23844v1",
        "abstract": "Recent advances in large language models (LLMs) have catalyzed the rise of\nautonomous AI agents capable of perceiving, reasoning, and acting in dynamic,\nopen-ended environments. These large-model agents mark a paradigm shift from\nstatic inference systems to interactive, memory-augmented entities. While these\ncapabilities significantly expand the functional scope of AI, they also\nintroduce qualitatively novel security risks - such as memory poisoning, tool\nmisuse, reward hacking, and emergent misalignment - that extend beyond the\nthreat models of conventional systems or standalone LLMs. In this survey, we\nfirst examine the structural foundations and key capabilities that underpin\nincreasing levels of agent autonomy, including long-term memory retention,\nmodular tool use, recursive planning, and reflective reasoning. We then analyze\nthe corresponding security vulnerabilities across the agent stack, identifying\nfailure modes such as deferred decision hazards, irreversible tool chains, and\ndeceptive behaviors arising from internal state drift or value misalignment.\nThese risks are traced to architectural fragilities that emerge across\nperception, cognition, memory, and action modules. To address these challenges,\nwe systematically review recent defense strategies deployed at different\nautonomy layers, including input sanitization, memory lifecycle control,\nconstrained decision-making, structured tool invocation, and introspective\nreflection. We introduce the Reflective Risk-Aware Agent Architecture (R2A2), a\nunified cognitive framework grounded in Constrained Markov Decision Processes\n(CMDPs), which incorporates risk-aware world modeling, meta-policy adaptation,\nand joint reward-risk optimization to enable principled, proactive safety\nacross the agent's decision-making loop."
    },
    {
        "date": "2025-06",
        "title": "An ontological lens on attack trees: Toward adequacy and interoperability",
        "author": "\u00cdtalo Oliveira, Stefano M. Nicoletti, Gal Engelberg, Mattia Fumagalli, Dan Klein, and Giancarlo Guizzardi",
        "link": "http://arxiv.org/abs/2506.23841v1",
        "abstract": "Attack Trees (AT) are a popular formalism for security analysis. They are\nmeant to display an attacker's goal decomposed into attack steps needed to\nachieve it and compute certain security metrics (e.g., attack cost,\nprobability, and damage). ATs offer three important services: (a) conceptual\nmodeling capabilities for representing security risk management scenarios, (b)\na qualitative assessment to find root causes and minimal conditions of\nsuccessful attacks, and (c) quantitative analyses via security metrics\ncomputation under formal semantics, such as minimal time and cost among all\nattacks. Still, the AT language presents limitations due to its lack of\nontological foundations, thus compromising associated services. Via an\nontological analysis grounded in the Common Ontology of Value and Risk (COVER)\n-- a reference core ontology based on the Unified Foundational Ontology (UFO)\n-- we investigate the ontological adequacy of AT and reveal four significant\nshortcomings: (1) ambiguous syntactical terms that can be interpreted in\nvarious ways; (2) ontological deficit concerning crucial domain-specific\nconcepts; (3) lacking modeling guidance to construct ATs decomposing a goal;\n(4) lack of semantic interoperability, resulting in ad hoc stand-alone tools.\nWe also discuss existing incremental solutions and how our analysis paves the\nway for overcoming those issues through a broader approach to risk management\nmodeling."
    },
    {
        "date": "2025-06",
        "title": "Concept-based Adversarial Attack: a Probabilistic Perspective",
        "author": "Andi Zhang, Xuan Ding, Steven McDonagh, and Samuel Kaski",
        "link": "http://arxiv.org/abs/2507.02965v1",
        "abstract": "We propose a concept-based adversarial attack framework that extends beyond\nsingle-image perturbations by adopting a probabilistic perspective. Rather than\nmodifying a single image, our method operates on an entire concept --\nrepresented by a probabilistic generative model or a set of images -- to\ngenerate diverse adversarial examples. Preserving the concept is essential, as\nit ensures that the resulting adversarial images remain identifiable as\ninstances of the original underlying category or identity. By sampling from\nthis concept-based adversarial distribution, we generate images that maintain\nthe original concept but vary in pose, viewpoint, or background, thereby\nmisleading the classifier. Mathematically, this framework remains consistent\nwith traditional adversarial attacks in a principled manner. Our theoretical\nand empirical results demonstrate that concept-based adversarial attacks yield\nmore diverse adversarial examples and effectively preserve the underlying\nconcept, while achieving higher attack efficiency."
    },
    {
        "date": "2025-06",
        "title": "Less Data, More Security: Advancing Cybersecurity LLMs Specialization via Resource-Efficient Domain-Adaptive Continuous Pre-training with Minimal Tokens",
        "author": "Salahuddin Salahuddin, Ahmed Hussain, Jussi L\u00f6pp\u00f6nen, Toni Jutila, and Panos Papadimitratos",
        "link": "http://arxiv.org/abs/2507.02964v1",
        "abstract": "While Large Language Models (LLMs) demonstrate exceptional natural language\ncapabilities, general-purpose models lack specialized domain knowledge for\neffective cybersecurity analysis. In this work, we investigate Domain-Adaptive\nContinuous Pretraining (DAP) as a methodology for enhancing cybersecurity\nunderstanding in pretrained LLMs while preserving general language\ncapabilities. We systematically adapted three decoder-based architectures --\nLlama-3.1-8B, DeepSeek-R1-Distill-Qwen-14B, and Llama-3.3-70B-Instruct -- using\na curated 126-million-word cybersecurity corpus from standards, academic\nliterature, and various other sources. Our approach employed constrained\ntraining parameters and distributed FSDP training to balance domain\nspecialization with knowledge preservation. Evaluation across three\ncybersecurity benchmarks, namely, CTI-MCQ, CyberMetric, and SecEval,\ndemonstrates consistent improvements post-adaptation. The Llama-3.3-70B-Ins-DAP\nmodel achieved state-of-the-art accuracies of 0.718, 0.933, and 0.864,\nrespectively, outperforming specialized models, including Llama-Primus-Base.\nNotably, competitive performance was achieved using substantially smaller\ndatasets (118.8 million versus 2.77 billion tokens), demonstrating efficient\ndomain specialization viability. We establish that targeted continuous\npretraining enables effective cybersecurity domain adaptation with\ncomputational feasibility, providing foundations for specialized AI assistants\nin threat analysis, vulnerability assessment, and security documentation while\nchallenging prevailing assumptions about data requirements for LLM\nspecialization."
    },
    {
        "date": "2025-06",
        "title": "VR-YOLO: Enhancing PCB Defect Detection with Viewpoint Robustness Based on YOLO",
        "author": "Hengyi Zhu, Linye Wei, and He Li",
        "link": "http://arxiv.org/abs/2507.02963v1",
        "abstract": "The integration of large-scale circuits and systems emphasizes the importance\nof automated defect detection of electronic components. The YOLO image\ndetection model has been used to detect PCB defects and it has become a typical\nAI-assisted case of traditional industrial production. However, conventional\ndetection algorithms have stringent requirements for the angle, orientation,\nand clarity of target images. In this paper, we propose an enhanced PCB defect\ndetection algorithm, named VR-YOLO, based on the YOLOv8 model. This algorithm\naims to improve the model's generalization performance and enhance viewpoint\nrobustness in practical application scenarios. We first propose a diversified\nscene enhancement (DSE) method by expanding the PCB defect dataset by\nincorporating diverse scenarios and segmenting samples to improve target\ndiversity. A novel key object focus (KOF) scheme is then presented by\nconsidering angular loss and introducing an additional attention mechanism to\nenhance fine-grained learning of small target features. Experimental results\ndemonstrate that our improved PCB defect detection approach achieves a mean\naverage precision (mAP) of 98.9% for the original test images, and 94.7% for\nthe test images with viewpoint shifts (horizontal and vertical shear\ncoefficients of $\\pm 0.06$ and rotation angle of $\\pm 10$ degrees), showing\nsignificant improvements compared to the baseline YOLO model with negligible\nadditional computational cost."
    },
    {
        "date": "2025-06",
        "title": "Threadbox: Sandboxing for Modular Security",
        "author": "Maysara Alhindi, and Joseph Hallett",
        "link": "http://arxiv.org/abs/2506.23683v1",
        "abstract": "There are many sandboxing mechanisms provided by operating systems to limit\nwhat resources applications can access, however, sometimes the use of these\nmechanisms requires developers to refactor their code to fit the sandboxing\nmodel. In this work, we investigate what makes existing sandboxing mechanisms\nchallenging to apply to certain types of applications, and propose Threadbox, a\nsandboxing mechanism that enables having modular and independent sandboxes, and\ncan be applied to threads and sandbox specific functions. We present case\nstudies to illustrate the applicability of the idea and discuss its\nlimitations."
    },
    {
        "date": "2025-06",
        "title": "Not quite a piece of CHERI-cake: Are new digital security by design architectures usable?",
        "author": "Maysara Alhindi, and Joseph Hallett",
        "link": "http://arxiv.org/abs/2506.23682v1",
        "abstract": "A digital security-by-design computer architecture, like CHERI, lets you\nprogram without fear of buffer overflows or other memory safety errors, but\nCHERI also rewrites some of the assumptions about how C works and how\nfundamental types (such as pointers) are implemented in hardware. We conducted\na usability study to examine how developers react to the changes required by\nCHERI when porting software to run on it. We find that developers struggle with\nCHERI's display of warnings and errors and a lack of diverse documentation."
    },
    {
        "date": "2025-06",
        "title": "A Unified Framework for Stealthy Adversarial Generation via Latent Optimization and Transferability Enhancement",
        "author": "Gaozheng Pei, Ke Ma, Dongpeng Zhang, Chengzhi Sun, Qianqian Xu, and Qingming Huang",
        "link": "http://arxiv.org/abs/2506.23676v1",
        "abstract": "Due to their powerful image generation capabilities, diffusion-based\nadversarial example generation methods through image editing are rapidly\ngaining popularity. However, due to reliance on the discriminative capability\nof the diffusion model, these diffusion-based methods often struggle to\ngeneralize beyond conventional image classification tasks, such as in Deepfake\ndetection. Moreover, traditional strategies for enhancing adversarial example\ntransferability are challenging to adapt to these methods. To address these\nchallenges, we propose a unified framework that seamlessly incorporates\ntraditional transferability enhancement strategies into diffusion model-based\nadversarial example generation via image editing, enabling their application\nacross a wider range of downstream tasks. Our method won first place in the\n\"1st Adversarial Attacks on Deepfake Detectors: A Challenge in the Era of\nAI-Generated Media\" competition at ACM MM25, which validates the effectiveness\nof our approach."
    },
    {
        "date": "2025-06",
        "title": "On the Domain Robustness of Contrastive Vision-Language Models",
        "author": "Mario Koddenbrock, Rudolf Hoffmann, David Brodmann, and Erik Rodner",
        "link": "http://arxiv.org/abs/2506.23663v1",
        "abstract": "In real-world vision-language applications, practitioners increasingly rely\non large, pretrained foundation models rather than custom-built solutions,\ndespite limited transparency regarding their training data and processes. While\nthese models achieve impressive performance on general benchmarks, their\neffectiveness can decline notably under specialized domain shifts, such as\nunique imaging conditions or environmental variations. In this work, we\nintroduce Deepbench, a framework designed to assess domain-specific robustness\nof vision-language models (VLMs). Deepbench leverages a large language model\n(LLM) to generate realistic, context-aware image corruptions tailored to\nspecific deployment domains without requiring labeled data. We evaluate a range\nof contrastive vision-language architectures and architectural variants across\nsix real-world domains and observe substantial variability in robustness,\nhighlighting the need for targeted, domain-aware evaluation. Deepbench is\nreleased as open-source software to support further research into domain-aware\nrobustness assessment."
    },
    {
        "date": "2025-06",
        "title": "Privacy-Preserving Federated Learning Scheme with Mitigating Model Poisoning Attacks: Vulnerabilities and Countermeasures",
        "author": "Jiahui Wu, Fucai Luo, Tiecheng Sun, Haiyan Wang, and Weizhe Zhang",
        "link": "http://arxiv.org/abs/2506.23622v1",
        "abstract": "The privacy-preserving federated learning schemes based on the setting of two\nhonest-but-curious and non-colluding servers offer promising solutions in terms\nof security and efficiency. However, our investigation reveals that these\nschemes still suffer from privacy leakage when considering model poisoning\nattacks from malicious users. Specifically, we demonstrate that the\nprivacy-preserving computation process for defending against model poisoning\nattacks inadvertently leaks privacy to one of the honest-but-curious servers,\nenabling it to access users' gradients in plaintext. To address both privacy\nleakage and model poisoning attacks, we propose an enhanced privacy-preserving\nand Byzantine-robust federated learning (PBFL) scheme, comprising three\ncomponents: (1) a two-trapdoor fully homomorphic encryption (FHE) scheme to\nbolster users' privacy protection; (2) a novel secure normalization judgment\nmethod to preemptively thwart gradient poisoning; and (3) an innovative secure\ncosine similarity measurement method for detecting model poisoning attacks\nwithout compromising data privacy. Our scheme guarantees privacy preservation\nand resilience against model poisoning attacks, even in scenarios with\nheterogeneous, non-IID (Independently and Identically Distributed) datasets.\nTheoretical analyses substantiate the security and efficiency of our scheme,\nand extensive experiments corroborate the efficacy of our private attacks.\nFurthermore, the experimental results demonstrate that our scheme accelerates\ntraining speed while reducing communication overhead compared to the\nstate-of-the-art PBFL schemes."
    },
    {
        "date": "2025-06",
        "title": "PBCAT: Patch-based composite adversarial training against physically realizable attacks on object detection",
        "author": "Xiao Li, Yiming Zhu, Yifan Huang, Wei Zhang, Yingzhe He, Jie Shi, and Xiaolin Hu",
        "link": "http://arxiv.org/abs/2506.23581v2",
        "abstract": "Object detection plays a crucial role in many security-sensitive\napplications. However, several recent studies have shown that object detectors\ncan be easily fooled by physically realizable attacks, \\eg, adversarial patches\nand recent adversarial textures, which pose realistic and urgent threats.\nAdversarial Training (AT) has been recognized as the most effective defense\nagainst adversarial attacks. While AT has been extensively studied in the\n$l_\\infty$ attack settings on classification models, AT against physically\nrealizable attacks on object detectors has received limited exploration. Early\nattempts are only performed to defend against adversarial patches, leaving AT\nagainst a wider range of physically realizable attacks under-explored. In this\nwork, we consider defending against various physically realizable attacks with\na unified AT method. We propose PBCAT, a novel Patch-Based Composite\nAdversarial Training strategy. PBCAT optimizes the model by incorporating the\ncombination of small-area gradient-guided adversarial patches and imperceptible\nglobal adversarial perturbations covering the entire image. With these designs,\nPBCAT has the potential to defend against not only adversarial patches but also\nunseen physically realizable attacks such as adversarial textures. Extensive\nexperiments in multiple settings demonstrated that PBCAT significantly improved\nrobustness against various physically realizable attacks over state-of-the-art\ndefense methods. Notably, it improved the detection accuracy by 29.7\\% over\nprevious defense methods under one recent adversarial texture attack."
    },
    {
        "date": "2025-06",
        "title": "Evaluating Multi-Agent Defences Against Jailbreaking Attacks on Large Language Models",
        "author": "Maria Carolina Cornelia Wit, and Jun Pang",
        "link": "http://arxiv.org/abs/2506.23576v1",
        "abstract": "Recent advances in large language models (LLMs) have raised concerns about\njailbreaking attacks, i.e., prompts that bypass safety mechanisms. This paper\ninvestigates the use of multi-agent LLM systems as a defence against such\nattacks. We evaluate three jailbreaking strategies, including the original\nAutoDefense attack and two from Deepleaps: BetterDan and JB. Reproducing the\nAutoDefense framework, we compare single-agent setups with two- and three-agent\nconfigurations. Our results show that multi-agent systems enhance resistance to\njailbreaks, especially by reducing false negatives. However, its effectiveness\nvaries by attack type, and it introduces trade-offs such as increased false\npositives and computational overhead. These findings point to the limitations\nof current automated defences and suggest directions for improving alignment\nrobustness in future LLM systems."
    },
    {
        "date": "2025-06",
        "title": "A Joint Topology-Data Fusion Graph Network for Robust Traffic Speed Prediction with Data Anomalism",
        "author": "Ruiyuan Jiang, Dongyao Jia, Eng Gee Lim, Pengfei Fan, Yuli Zhang, and Shangbo Wang",
        "link": "http://arxiv.org/abs/2507.00085v1",
        "abstract": "Accurate traffic prediction is essential for Intelligent Transportation\nSystems (ITS), yet current methods struggle with the inherent complexity and\nnon-linearity of traffic dynamics, making it difficult to integrate spatial and\ntemporal characteristics. Furthermore, existing approaches use static\ntechniques to address non-stationary and anomalous historical data, which\nlimits adaptability and undermines data smoothing. To overcome these\nchallenges, we propose the Graph Fusion Enhanced Network (GFEN), an innovative\nframework for network-level traffic speed prediction. GFEN introduces a novel\ntopological spatiotemporal graph fusion technique that meticulously extracts\nand merges spatial and temporal correlations from both data distribution and\nnetwork topology using trainable methods, enabling the modeling of multi-scale\nspatiotemporal features. Additionally, GFEN employs a hybrid methodology\ncombining a k-th order difference-based mathematical framework with an\nattention-based deep learning structure to adaptively smooth historical\nobservations and dynamically mitigate data anomalies and non-stationarity.\nExtensive experiments demonstrate that GFEN surpasses state-of-the-art methods\nby approximately 6.3% in prediction accuracy and exhibits convergence rates\nnearly twice as fast as recent hybrid models, confirming its superior\nperformance and potential to significantly enhance traffic prediction system\nefficiency."
    },
    {
        "date": "2025-06",
        "title": "User Behavior Prediction as a Generic, Robust, Scalable, and Low-Cost Evaluation Strategy for Estimating Generalization in LLMs",
        "author": "Sougata Saha, and Monojit Choudhury",
        "link": "http://arxiv.org/abs/2507.05266v1",
        "abstract": "Measuring the generalization ability of Large Language Models (LLMs) is\nchallenging due to data contamination. As models grow and computation becomes\ncheaper, ensuring tasks and test cases are unseen during training phases will\nbecome nearly impossible. We argue that knowledge-retrieval and reasoning tasks\nare not ideal for measuring generalization, as LLMs are not trained for\nspecific tasks. Instead, we propose user behavior prediction, also a key aspect\nof personalization, as a theoretically sound, scalable, and robust alternative.\nWe introduce a novel framework for this approach and test it on movie and music\nrecommendation datasets for GPT-4o, GPT-4o-mini, and Llama-3.1-8B-Instruct.\nResults align with our framework's predictions, showing GPT-4o outperforms\nGPT-4o-mini and Llama, though all models have much room for improvement,\nespecially Llama."
    },
    {
        "date": "2025-06",
        "title": "A Large-Scale Evolvable Dataset for Model Context Protocol Ecosystem and Security Analysis",
        "author": "Zhiwei Lin, Bonan Ruan, Jiahao Liu, and Weibo Zhao",
        "link": "http://arxiv.org/abs/2506.23474v1",
        "abstract": "The Model Context Protocol (MCP) has recently emerged as a standardized\ninterface for connecting language models with external tools and data. As the\necosystem rapidly expands, the lack of a structured, comprehensive view of\nexisting MCP artifacts presents challenges for research. To bridge this gap, we\nintroduce MCPCorpus, a large-scale dataset containing around 14K MCP servers\nand 300 MCP clients. Each artifact is annotated with 20+ normalized attributes\ncapturing its identity, interface configuration, GitHub activity, and metadata.\nMCPCorpus provides a reproducible snapshot of the real-world MCP ecosystem,\nenabling studies of adoption trends, ecosystem health, and implementation\ndiversity. To keep pace with the rapid evolution of the MCP ecosystem, we\nprovide utility tools for automated data synchronization, normalization, and\ninspection. Furthermore, to support efficient exploration and exploitation, we\nrelease a lightweight web-based search interface. MCPCorpus is publicly\navailable at: https://github.com/Snakinya/MCPCorpus."
    },
    {
        "date": "2025-06",
        "title": "AdFair-CLIP: Adversarial Fair Contrastive Language-Image Pre-training for Chest X-rays",
        "author": "Chenlang Yi, Zizhan Xiong, Qi Qi, Xiyuan Wei, Girish Bathla, Ching-Long Lin, Bobak Jack Mortazavi, and Tianbao Yang",
        "link": "http://arxiv.org/abs/2506.23467v1",
        "abstract": "Contrastive Language-Image Pre-training (CLIP) models have demonstrated\nsuperior performance across various visual tasks including medical image\nclassification. However, fairness concerns, including demographic biases, have\nreceived limited attention for CLIP models. This oversight leads to critical\nissues, particularly those related to race and gender, resulting in disparities\nin diagnostic outcomes and reduced reliability for underrepresented groups. To\naddress these challenges, we introduce AdFair-CLIP, a novel framework employing\nadversarial feature intervention to suppress sensitive attributes, thereby\nmitigating spurious correlations and improving prediction fairness. We conduct\ncomprehensive experiments on chest X-ray (CXR) datasets, and show that\nAdFair-CLIP significantly enhances both fairness and diagnostic accuracy, while\nmaintaining robust generalization in zero-shot and few-shot scenarios. These\nresults establish new benchmarks for fairness-aware learning in CLIP-based\nmedical diagnostic models, particularly for CXR analysis."
    },
    {
        "date": "2025-06",
        "title": "State and Memory is All You Need for Robust and Reliable AI Agents",
        "author": "Matthew Muhoberac, Atharva Parikh, Nirvi Vakharia, Saniya Virani, Aco Radujevic, Savannah Wood, Meghav Verma, Dimitri Metaxotos, Jeyaraman Soundararajan, Thierry Masquelin, Alexander G. Godfrey, Sean Gardner, Dobrila Rudnicki, Sam Michael, and Gaurav Chopra",
        "link": "http://arxiv.org/abs/2507.00081v1",
        "abstract": "Large language models (LLMs) have enabled powerful advances in natural\nlanguage understanding and generation. Yet their application to complex,\nreal-world scientific workflows remain limited by challenges in memory,\nplanning, and tool integration. Here, we introduce SciBORG (Scientific Bespoke\nArtificial Intelligence Agents Optimized for Research Goals), a modular agentic\nframework that allows LLM-based agents to autonomously plan, reason, and\nachieve robust and reliable domain-specific task execution. Agents are\nconstructed dynamically from source code documentation and augmented with\nfinite-state automata (FSA) memory, enabling persistent state tracking and\ncontext-aware decision-making. This approach eliminates the need for manual\nprompt engineering and allows for robust, scalable deployment across diverse\napplications via maintaining context across extended workflows and to recover\nfrom tool or execution failures. We validate SciBORG through integration with\nboth physical and virtual hardware, such as microwave synthesizers for\nexecuting user-specified reactions, with context-aware decision making and\ndemonstrate its use in autonomous multi-step bioassay retrieval from the\nPubChem database utilizing multi-step planning, reasoning, agent-to-agent\ncommunication and coordination for execution of exploratory tasks. Systematic\nbenchmarking shows that SciBORG agents achieve reliable execution, adaptive\nplanning, and interpretable state transitions. Our results show that memory and\nstate awareness are critical enablers of agentic planning and reliability,\noffering a generalizable foundation for deploying AI agents in complex\nenvironments."
    },
    {
        "date": "2025-06",
        "title": "Endo-4DGX: Robust Endoscopic Scene Reconstruction and Illumination Correction with Gaussian Splatting",
        "author": "Yiming Huang, Long Bai, Beilei Cui, Yanheng Li, Tong Chen, Jie Wang, Jinlin Wu, Zhen Lei, Hongbin Liu, and Hongliang Ren",
        "link": "http://arxiv.org/abs/2506.23308v1",
        "abstract": "Accurate reconstruction of soft tissue is crucial for advancing automation in\nimage-guided robotic surgery. The recent 3D Gaussian Splatting (3DGS)\ntechniques and their variants, 4DGS, achieve high-quality renderings of dynamic\nsurgical scenes in real-time. However, 3D-GS-based methods still struggle in\nscenarios with varying illumination, such as low light and over-exposure.\nTraining 3D-GS in such extreme light conditions leads to severe optimization\nproblems and devastating rendering quality. To address these challenges, we\npresent Endo-4DGX, a novel reconstruction method with illumination-adaptive\nGaussian Splatting designed specifically for endoscopic scenes with uneven\nlighting. By incorporating illumination embeddings, our method effectively\nmodels view-dependent brightness variations. We introduce a region-aware\nenhancement module to model the sub-area lightness at the Gaussian level and a\nspatial-aware adjustment module to learn the view-consistent brightness\nadjustment. With the illumination adaptive design, Endo-4DGX achieves superior\nrendering performance under both low-light and over-exposure conditions while\nmaintaining geometric accuracy. Additionally, we employ an exposure control\nloss to restore the appearance from adverse exposure to the normal level for\nillumination-adaptive optimization. Experimental results demonstrate that\nEndo-4DGX significantly outperforms combinations of state-of-the-art\nreconstruction and restoration methods in challenging lighting environments,\nunderscoring its potential to advance robot-assisted surgical applications. Our\ncode is available at https://github.com/lastbasket/Endo-4DGX."
    },
    {
        "date": "2025-06",
        "title": "Securing AI Systems: A Guide to Known Attacks and Impacts",
        "author": "Naoto Kiribuchi, Kengo Zenitani, and Takayuki Semitsu",
        "link": "http://arxiv.org/abs/2506.23296v1",
        "abstract": "Embedded into information systems, artificial intelligence (AI) faces\nsecurity threats that exploit AI-specific vulnerabilities. This paper provides\nan accessible overview of adversarial attacks unique to predictive and\ngenerative AI systems. We identify eleven major attack types and explicitly\nlink attack techniques to their impacts -- including information leakage,\nsystem compromise, and resource exhaustion -- mapped to the confidentiality,\nintegrity, and availability (CIA) security triad. We aim to equip researchers,\ndevelopers, security practitioners, and policymakers, even those without\nspecialized AI security expertise, with foundational knowledge to recognize\nAI-specific risks and implement effective defenses, thereby enhancing the\noverall security posture of AI systems."
    },
    {
        "date": "2025-06",
        "title": "TVG-SLAM: Robust Gaussian Splatting SLAM with Tri-view Geometric Constraints",
        "author": "Zhen Tan, Xieyuanli Chen, Lei Feng, Yangbing Ge, Shuaifeng Zhi, Jiaxiong Liu, and Dewen Hu",
        "link": "http://arxiv.org/abs/2506.23207v1",
        "abstract": "Recent advances in 3D Gaussian Splatting (3DGS) have enabled RGB-only SLAM\nsystems to achieve high-fidelity scene representation. However, the heavy\nreliance of existing systems on photometric rendering loss for camera tracking\nundermines their robustness, especially in unbounded outdoor environments with\nsevere viewpoint and illumination changes. To address these challenges, we\npropose TVG-SLAM, a robust RGB-only 3DGS SLAM system that leverages a novel\ntri-view geometry paradigm to ensure consistent tracking and high-quality\nmapping. We introduce a dense tri-view matching module that aggregates reliable\npairwise correspondences into consistent tri-view matches, forming robust\ngeometric constraints across frames. For tracking, we propose Hybrid Geometric\nConstraints, which leverage tri-view matches to construct complementary\ngeometric cues alongside photometric loss, ensuring accurate and stable pose\nestimation even under drastic viewpoint shifts and lighting variations. For\nmapping, we propose a new probabilistic initialization strategy that encodes\ngeometric uncertainty from tri-view correspondences into newly initialized\nGaussians. Additionally, we design a Dynamic Attenuation of Rendering Trust\nmechanism to mitigate tracking drift caused by mapping latency. Experiments on\nmultiple public outdoor datasets show that our TVG-SLAM outperforms prior\nRGB-only 3DGS-based SLAM systems. Notably, in the most challenging dataset, our\nmethod improves tracking robustness, reducing the average Absolute Trajectory\nError (ATE) by 69.0\\% while achieving state-of-the-art rendering quality. The\nimplementation of our method will be released as open-source."
    },
    {
        "date": "2025-06",
        "title": "Trident: Detecting Face Forgeries with Adversarial Triplet Learning",
        "author": "Mustafa Hakan Kara, Aysegul Dundar, and U\u011fur G\u00fcd\u00fckbay",
        "link": "http://arxiv.org/abs/2506.23189v1",
        "abstract": "As face forgeries generated by deep neural networks become increasingly\nsophisticated, detecting face manipulations in digital media has posed a\nsignificant challenge, underscoring the importance of maintaining digital media\nintegrity and combating visual disinformation. Current detection models,\npredominantly based on supervised training with domain-specific data, often\nfalter against forgeries generated by unencountered techniques. In response to\nthis challenge, we introduce \\textit{Trident}, a face forgery detection\nframework that employs triplet learning with a Siamese network architecture for\nenhanced adaptability across diverse forgery methods. \\textit{Trident} is\ntrained on curated triplets to isolate nuanced differences of forgeries,\ncapturing fine-grained features that distinguish pristine samples from\nmanipulated ones while controlling for other variables. To further enhance\ngeneralizability, we incorporate domain-adversarial training with a forgery\ndiscriminator. This adversarial component guides our embedding model towards\nforgery-agnostic representations, improving its robustness to unseen\nmanipulations. In addition, we prevent gradient flow from the classifier head\nto the embedding model, avoiding overfitting induced by artifacts peculiar to\ncertain forgeries. Comprehensive evaluations across multiple benchmarks and\nablation studies demonstrate the effectiveness of our framework. We will\nrelease our code in a GitHub repository."
    },
    {
        "date": "2025-06",
        "title": "A Practical and Secure Byzantine Robust Aggregator",
        "author": "De Zhang Lee, Aashish Kolluri, Prateek Saxena, and Ee-Chien Chang",
        "link": "http://arxiv.org/abs/2506.23183v3",
        "abstract": "In machine learning security, one is often faced with the problem of removing\noutliers from a given set of high-dimensional vectors when computing their\naverage. For example, many variants of data poisoning attacks produce gradient\nvectors during training that are outliers in the distribution of clean\ngradients, which bias the computed average used to derive the ML model.\nFiltering them out before averaging serves as a generic defense strategy.\nByzantine robust aggregation is an algorithmic primitive which computes a\nrobust average of vectors, in the presence of an $\\epsilon$ fraction of vectors\nwhich may have been arbitrarily and adaptively corrupted, such that the\nresulting bias in the final average is provably bounded.\n  In this paper, we give the first robust aggregator that runs in quasi-linear\ntime in the size of input vectors and provably has near-optimal bias bounds.\nOur algorithm also does not assume any knowledge of the distribution of clean\nvectors, nor does it require pre-computing any filtering thresholds from it.\nThis makes it practical to use directly in standard neural network training\nprocedures. We empirically confirm its expected runtime efficiency and its\neffectiveness in nullifying 10 different ML poisoning attacks."
    },
    {
        "date": "2025-06",
        "title": "Mirror Descent Policy Optimisation for Robust Constrained Markov Decision Processes",
        "author": "David Bossens, and Atsushi Nitanda",
        "link": "http://arxiv.org/abs/2506.23165v1",
        "abstract": "Safety is an essential requirement for reinforcement learning systems. The\nnewly emerging framework of robust constrained Markov decision processes allows\nlearning policies that satisfy long-term constraints while providing guarantees\nunder epistemic uncertainty. This paper presents mirror descent policy\noptimisation for robust constrained Markov decision processes (RCMDPs), making\nuse of policy gradient techniques to optimise both the policy (as a maximiser)\nand the transition kernel (as an adversarial minimiser) on the Lagrangian\nrepresenting a constrained MDP. In the oracle-based RCMDP setting, we obtain an\n$\\mathcal{O}\\left(\\frac{1}{T}\\right)$ convergence rate for the squared distance\nas a Bregman divergence, and an $\\mathcal{O}\\left(e^{-T}\\right)$ convergence\nrate for entropy-regularised objectives. In the sample-based RCMDP setting, we\nobtain an $\\tilde{\\mathcal{O}}\\left(\\frac{1}{T^{1/3}}\\right)$ convergence rate.\nExperiments confirm the benefits of mirror descent policy optimisation in\nconstrained and unconstrained optimisation, and significant improvements are\nobserved in robustness tests when compared to baseline policy optimisation\nalgorithms."
    },
    {
        "date": "2025-06",
        "title": "CoreMark: Toward Robust and Universal Text Watermarking Technique",
        "author": "Jiale Meng, Yiming Li, Zheming Lu, Zewei He, Hao Luo, and Tianwei Zhang",
        "link": "http://arxiv.org/abs/2506.23066v1",
        "abstract": "Text watermarking schemes have gained considerable attention in recent years,\nyet still face critical challenges in achieving simultaneous robustness,\ngeneralizability, and imperceptibility. This paper introduces a new embedding\nparadigm,termed CORE, which comprises several consecutively aligned black pixel\nsegments. Its key innovation lies in its inherent noise resistance during\ntransmission and broad applicability across languages and fonts. Based on the\nCORE, we present a text watermarking framework named CoreMark. Specifically,\nCoreMark first dynamically extracts COREs from characters. Then, the characters\nwith stronger robustness are selected according to the lengths of COREs. By\nmodifying the thickness of the CORE, the hidden data is embedded into the\nselected characters without causing significant visual distortions. Moreover, a\ngeneral plug-and-play embedding strength modulator is proposed, which can\nadaptively enhance the robustness for small font sizes by adjusting the\nembedding strength according to the font size. Experimental evaluation\nindicates that CoreMark demonstrates outstanding generalizability across\nmultiple languages and fonts. Compared to existing methods, CoreMark achieves\nsignificant improvements in resisting screenshot, print-scan, and print camera\nattacks, while maintaining satisfactory imperceptibility."
    },
    {
        "date": "2025-06",
        "title": "Fragile, Robust, and Antifragile: A Perspective from Parameter Responses in Reinforcement Learning Under Stress",
        "author": "Zain ul Abdeen, and Ming Jin",
        "link": "http://arxiv.org/abs/2506.23036v1",
        "abstract": "This paper explores Reinforcement learning (RL) policy robustness by\nsystematically analyzing network parameters under internal and external\nstresses. Inspired by synaptic plasticity in neuroscience, synaptic filtering\nintroduces internal stress by selectively perturbing parameters, while\nadversarial attacks apply external stress through modified agent observations.\nThis dual approach enables the classification of parameters as fragile, robust,\nor antifragile, based on their influence on policy performance in clean and\nadversarial settings. Parameter scores are defined to quantify these\ncharacteristics, and the framework is validated on PPO-trained agents in Mujoco\ncontinuous control environments. The results highlight the presence of\nantifragile parameters that enhance policy performance under stress,\ndemonstrating the potential of targeted filtering techniques to improve RL\npolicy adaptability. These insights provide a foundation for future\nadvancements in the design of robust and antifragile RL systems."
    },
    {
        "date": "2025-06",
        "title": "Revisiting CroPA: A Reproducibility Study and Enhancements for Cross-Prompt Adversarial Transferability in Vision-Language Models",
        "author": "Atharv Mittal, Agam Pandey, Amritanshu Tiwari, Sukrit Jindal, and Swadesh Swain",
        "link": "http://arxiv.org/abs/2506.22982v1",
        "abstract": "Large Vision-Language Models (VLMs) have revolutionized computer vision,\nenabling tasks such as image classification, captioning, and visual question\nanswering. However, they remain highly vulnerable to adversarial attacks,\nparticularly in scenarios where both visual and textual modalities can be\nmanipulated. In this study, we conduct a comprehensive reproducibility study of\n\"An Image is Worth 1000 Lies: Adversarial Transferability Across Prompts on\nVision-Language Models\" validating the Cross-Prompt Attack (CroPA) and\nconfirming its superior cross-prompt transferability compared to existing\nbaselines. Beyond replication we propose several key improvements: (1) A novel\ninitialization strategy that significantly improves Attack Success Rate (ASR).\n(2) Investigate cross-image transferability by learning universal\nperturbations. (3) A novel loss function targeting vision encoder attention\nmechanisms to improve generalization. Our evaluation across prominent VLMs --\nincluding Flamingo, BLIP-2, and InstructBLIP as well as extended experiments on\nLLaVA validates the original results and demonstrates that our improvements\nconsistently boost adversarial effectiveness. Our work reinforces the\nimportance of studying adversarial vulnerabilities in VLMs and provides a more\nrobust framework for generating transferable adversarial examples, with\nsignificant implications for understanding the security of VLMs in real-world\napplications."
    },
    {
        "date": "2025-06",
        "title": "MPC in the Quantum Head (or: Superposition-Secure (Quantum) Zero-Knowledge)",
        "author": "Andrea Coladangelo, Ruta Jawale, Dakshita Khurana, Giulio Malavolta, and Hendrik Waldner",
        "link": "http://arxiv.org/abs/2506.22961v1",
        "abstract": "The MPC-in-the-head technique (Ishai et al., STOC 2007) is a celebrated\nmethod to build zero-knowledge protocols with desirable theoretical properties\nand high practical efficiency. This technique has generated a large body of\nresearch and has influenced the design of real-world post-quantum cryptographic\nsignatures. In this work, we present a generalization of the MPC-in-the-head\nparadigm to the quantum setting, where the MPC is running a quantum\ncomputation. As an application of our framework, we propose a new approach to\nbuild zero-knowledge protocols where security holds even against a verifier\nthat can obtain a superposition of transcripts. This notion was pioneered by\nDamgard et al., who built a zero-knowledge protocol for NP (in the common\nreference string model) secure against superposition attacks, by relying on\nperfectly hiding and unconditionally binding dual-mode commitments.\nUnfortunately, no such commitments are known from standard cryptographic\nassumptions. In this work we revisit this problem, and present two new\nthree-round protocols in the common reference string model: (i) A\nzero-knowledge argument for NP, whose security reduces to the standard learning\nwith errors (LWE) problem. (ii) A zero-knowledge argument for QMA from the same\nassumption."
    },
    {
        "date": "2025-06",
        "title": "Peccavi: Visual Paraphrase Attack Safe and Distortion Free Image Watermarking Technique for AI-Generated Images",
        "author": "Shreyas Dixit, Ashhar Aziz, Shashwat Bajpai, Vasu Sharma, Aman Chadha, Vinija Jain, and Amitava Das",
        "link": "http://arxiv.org/abs/2506.22960v1",
        "abstract": "A report by the European Union Law Enforcement Agency predicts that by 2026,\nup to 90 percent of online content could be synthetically generated, raising\nconcerns among policymakers, who cautioned that \"Generative AI could act as a\nforce multiplier for political disinformation. The combined effect of\ngenerative text, images, videos, and audio may surpass the influence of any\nsingle modality.\" In response, California's Bill AB 3211 mandates the\nwatermarking of AI-generated images, videos, and audio. However, concerns\nremain regarding the vulnerability of invisible watermarking techniques to\ntampering and the potential for malicious actors to bypass them entirely.\nGenerative AI-powered de-watermarking attacks, especially the newly introduced\nvisual paraphrase attack, have shown an ability to fully remove watermarks,\nresulting in a paraphrase of the original image. This paper introduces PECCAVI,\nthe first visual paraphrase attack-safe and distortion-free image watermarking\ntechnique. In visual paraphrase attacks, an image is altered while preserving\nits core semantic regions, termed Non-Melting Points (NMPs). PECCAVI\nstrategically embeds watermarks within these NMPs and employs multi-channel\nfrequency domain watermarking. It also incorporates noisy burnishing to counter\nreverse-engineering efforts aimed at locating NMPs to disrupt the embedded\nwatermark, thereby enhancing durability. PECCAVI is model-agnostic. All\nrelevant resources and codes will be open-sourced."
    },
    {
        "date": "2025-06",
        "title": "A Study on Semi-Supervised Detection of DDoS Attacks under Class Imbalance",
        "author": "Ehsan Hallaji, Vaishnavi Shanmugam, Roozbeh Razavi-Far, and Mehrdad Saif",
        "link": "http://arxiv.org/abs/2506.22949v1",
        "abstract": "One of the most difficult challenges in cybersecurity is eliminating\nDistributed Denial of Service (DDoS) attacks. Automating this task using\nartificial intelligence is a complex process due to the inherent class\nimbalance and lack of sufficient labeled samples of real-world datasets. This\nresearch investigates the use of Semi-Supervised Learning (SSL) techniques to\nimprove DDoS attack detection when data is imbalanced and partially labeled. In\nthis process, 13 state-of-the-art SSL algorithms are evaluated for detecting\nDDoS attacks in several scenarios. We evaluate their practical efficacy and\nshortcomings, including the extent to which they work in extreme environments.\nThe results will offer insight into designing intelligent Intrusion Detection\nSystems (IDSs) that are robust against class imbalance and handle partially\nlabeled data."
    },
    {
        "date": "2025-06",
        "title": "MagShield: Towards Better Robustness in Sparse Inertial Motion Capture Under Magnetic Disturbances",
        "author": "Yunzhe Shao, Xinyu Yi, Lu Yin, Shihui Guo, Junhai Yong, and Feng Xu",
        "link": "http://arxiv.org/abs/2506.22907v1",
        "abstract": "This paper proposes a novel method called MagShield, designed to address the\nissue of magnetic interference in sparse inertial motion capture (MoCap)\nsystems. Existing Inertial Measurement Unit (IMU) systems are prone to\norientation estimation errors in magnetically disturbed environments, limiting\ntheir practical application in real-world scenarios. To address this problem,\nMagShield employs a \"detect-then-correct\" strategy, first detecting magnetic\ndisturbances through multi-IMU joint analysis, and then correcting orientation\nerrors using human motion priors. MagShield can be integrated with most\nexisting sparse inertial MoCap systems, improving their performance in\nmagnetically disturbed environments. Experimental results demonstrate that\nMagShield significantly enhances the accuracy of motion capture under magnetic\ninterference and exhibits good compatibility across different sparse inertial\nMoCap systems."
    },
    {
        "date": "2025-06",
        "title": "CP-Guard: A Unified, Probability-Agnostic, and Adaptive Framework for Malicious Agent Detection and Defense in Multi-Agent Embodied Perception Systems",
        "author": "Senkang Hu, Yihang Tao, Guowen Xu, Xinyuan Qian, Yiqin Deng, Xianhao Chen, Sam Tak Wu Kwong, and Yuguang Fang",
        "link": "http://arxiv.org/abs/2506.22890v1",
        "abstract": "Collaborative Perception (CP) has been shown to be a promising technique for\nmulti-agent autonomous driving and multi-agent robotic systems, where multiple\nagents share their perception information to enhance the overall perception\nperformance and expand the perception range. However, in CP, an ego agent needs\nto receive messages from its collaborators, which makes it vulnerable to\nattacks from malicious agents. To address this critical issue, we propose a\nunified, probability-agnostic, and adaptive framework, namely, CP-Guard, which\nis a tailored defense mechanism for CP deployed by each agent to accurately\ndetect and eliminate malicious agents in its collaboration network. Our key\nidea is to enable CP to reach a consensus rather than a conflict against an ego\nagent's perception results. Based on this idea, we first develop a\nprobability-agnostic sample consensus (PASAC) method to effectively sample a\nsubset of the collaborators and verify the consensus without prior\nprobabilities of malicious agents. Furthermore, we define collaborative\nconsistency loss (CCLoss) for object detection task and bird's eye view (BEV)\nsegmentation task to capture the discrepancy between an ego agent and its\ncollaborators, which is used as a verification criterion for consensus. In\naddition, we propose online adaptive threshold via dual sliding windows to\ndynamically adjust the threshold for consensus verification and ensure the\nreliability of the systems in dynamic environments. Finally, we conduct\nextensive experiments and demonstrate the effectiveness of our framework. Code\nwill be released at https://github.com/CP-Security/CP-Guard"
    },
    {
        "date": "2025-06",
        "title": "Offline Reinforcement Learning for Mobility Robustness Optimization",
        "author": "Pegah Alizadeh, Anastasios Giovanidis, Pradeepa Ramachandra, Vasileios Koutsoukis, and Osama Arouk",
        "link": "http://arxiv.org/abs/2506.22793v1",
        "abstract": "In this work we revisit the Mobility Robustness Optimisation (MRO) algorithm\nand study the possibility of learning the optimal Cell Individual Offset tuning\nusing offline Reinforcement Learning. Such methods make use of collected\noffline datasets to learn the optimal policy, without further exploration. We\nadapt and apply a sequence-based method called Decision Transformers as well as\na value-based method called Conservative Q-Learning to learn the optimal policy\nfor the same target reward as the vanilla rule-based MRO. The same input\nfeatures related to failures, ping-pongs, and other handover issues are used.\nEvaluation for realistic New Radio networks with 3500 MHz carrier frequency on\na traffic mix including diverse user service types and a specific tunable\ncell-pair shows that offline-RL methods outperform rule-based MRO, offering up\nto 7% improvement. Furthermore, offline-RL can be trained for diverse objective\nfunctions using the same available dataset, thus offering operational\nflexibility compared to rule-based methods."
    },
    {
        "date": "2025-06",
        "title": "Smaller = Weaker? Benchmarking Robustness of Quantized LLMs in Code Generation",
        "author": "Sen Fang, Weiyuan Ding, Antonio Mastropaolo, and Bowen Xu",
        "link": "http://arxiv.org/abs/2506.22776v1",
        "abstract": "Quantization has emerged as a mainstream method for compressing Large\nLanguage Models (LLMs), reducing memory requirements and accelerating inference\nwithout architectural modifications. While existing research primarily focuses\non evaluating the effectiveness of quantized LLMs compared to their original\ncounterparts, the impact on robustness remains largely unexplored.In this\npaper, we present the first systematic investigation of how quantization\naffects the robustness of LLMs in code generation tasks. Through extensive\nexperiments across four prominent LLM families (LLaMA, DeepSeek, CodeGen, and\nStarCoder) with parameter scales ranging from 350M to 33B, we evaluate\nrobustness from dual perspectives: adversarial attacks on input prompts and\nnoise perturbations on model architecture. Our findings challenge conventional\nwisdom by demonstrating that quantized LLMs often exhibit superior robustness\ncompared to their full-precision counterparts, with 51.59% versus 42.86% of our\nadversarial experiments showing better resilience in quantized LLMs. Similarly,\nour noise perturbation experiments also confirm that LLMs after quantitation\ngenerally withstand higher levels of weight disturbances. These results suggest\nthat quantization not only reduces computational requirements but can actually\nenhance LLMs' reliability in code generation tasks, providing valuable insights\nfor developing more robust and efficient LLM deployment strategies."
    },
    {
        "date": "2025-06",
        "title": "VSRM: A Robust Mamba-Based Framework for Video Super-Resolution",
        "author": "Dinh Phu Tran, Dao Duy Hung, and Daeyoung Kim",
        "link": "http://arxiv.org/abs/2506.22762v1",
        "abstract": "Video super-resolution remains a major challenge in low-level vision tasks.\nTo date, CNN- and Transformer-based methods have delivered impressive results.\nHowever, CNNs are limited by local receptive fields, while Transformers\nstruggle with quadratic complexity, posing challenges for processing long\nsequences in VSR. Recently, Mamba has drawn attention for its long-sequence\nmodeling, linear complexity, and large receptive fields. In this work, we\npropose VSRM, a novel \\textbf{V}ideo \\textbf{S}uper-\\textbf{R}esolution\nframework that leverages the power of \\textbf{M}amba. VSRM introduces\nSpatial-to-Temporal Mamba and Temporal-to-Spatial Mamba blocks to extract\nlong-range spatio-temporal features and enhance receptive fields efficiently.\nTo better align adjacent frames, we propose Deformable Cross-Mamba Alignment\nmodule. This module utilizes a deformable cross-mamba mechanism to make the\ncompensation stage more dynamic and flexible, preventing feature distortions.\nFinally, we minimize the frequency domain gaps between reconstructed and\nground-truth frames by proposing a simple yet effective Frequency\nCharbonnier-like loss that better preserves high-frequency content and enhances\nvisual quality. Through extensive experiments, VSRM achieves state-of-the-art\nresults on diverse benchmarks, establishing itself as a solid foundation for\nfuture research."
    },
    {
        "date": "2025-06",
        "title": "Robust Tensor Completion via Gradient Tensor Nulclear L1-L2 Norm for Traffic Data Recovery",
        "author": "Hao Shu, Jicheng Li, Tianyv Lei, and Lijun Sun",
        "link": "http://arxiv.org/abs/2506.22732v1",
        "abstract": "In real-world scenarios, spatiotemporal traffic data frequently experiences\ndual degradation from missing values and noise caused by sensor malfunctions\nand communication failures. Therefore, effective data recovery methods are\nessential to ensure the reliability of downstream data-driven applications.\nwhile classical tensor completion methods have been widely adopted, they are\nincapable of modeling noise, making them unsuitable for complex scenarios\ninvolving simultaneous data missingness and noise interference. Existing Robust\nTensor Completion (RTC) approaches offer potential solutions by separately\nmodeling the actual tensor data and noise. However, their effectiveness is\noften constrained by the over-relaxation of convex rank surrogates and the\nsuboptimal utilization of local consistency, leading to inadequate model\naccuracy. To address these limitations, we first introduce the tensor L1-L2\nnorm, a novel non-convex tensor rank surrogate that functions as an effective\nlow-rank representation tool. Leveraging an advanced feature fusion strategy,\nwe further develop the gradient tensor L1-L2 norm by incorporating the tensor\nL1-L2 norm in the gradient domain. By integrating the gradient tensor nuclear\nL1-L2 norm into the RTC framework, we propose the Robust Tensor Completion via\nGradient Tensor Nuclear L1-L2 Norm (RTC-GTNLN) model, which not only fully\nexploits both global low-rankness and local consistency without trade-off\nparameter, but also effectively handles the dual degradation challenges of\nmissing data and noise in traffic data. Extensive experiments conducted on\nmultiple real-world traffic datasets demonstrate that the RTC-GTNLN model\nconsistently outperforms existing state-of-the-art methods in complex recovery\nscenarios involving simultaneous missing values and noise."
    },
    {
        "date": "2025-06",
        "title": "Kill Two Birds with One Stone! Trajectory enabled Unified Online Detection of Adversarial Examples and Backdoor Attacks",
        "author": "Anmin Fu, Fanyu Meng, Huaibing Peng, Hua Ma, Zhi Zhang, Yifeng Zheng, Willy Susilo, and Yansong Gao",
        "link": "http://arxiv.org/abs/2506.22722v1",
        "abstract": "The proposed UniGuard is the first unified online detection framework capable\nof simultaneously addressing adversarial examples and backdoor attacks.\nUniGuard builds upon two key insights: first, both AE and backdoor attacks have\nto compromise the inference phase, making it possible to tackle them\nsimultaneously during run-time via online detection. Second, an adversarial\ninput, whether a perturbed sample in AE attacks or a trigger-carrying sample in\nbackdoor attacks, exhibits distinctive trajectory signatures from a benign\nsample as it propagates through the layers of a DL model in forward inference.\nThe propagation trajectory of the adversarial sample must deviate from that of\nits benign counterpart; otherwise, the adversarial objective cannot be\nfulfilled. Detecting these trajectory signatures is inherently challenging due\nto their subtlety; UniGuard overcomes this by treating the propagation\ntrajectory as a time-series signal, leveraging LSTM and spectrum transformation\nto amplify differences between adversarial and benign trajectories that are\nsubtle in the time domain. UniGuard exceptional efficiency and effectiveness\nhave been extensively validated across various modalities (image, text, and\naudio) and tasks (classification and regression), ranging from diverse model\narchitectures against a wide range of AE attacks and backdoor attacks,\nincluding challenging partial backdoors and dynamic triggers. When compared to\nSOTA methods, including ContraNet (NDSS 22) specific for AE detection and TED\n(IEEE SP 24) specific for backdoor detection, UniGuard consistently\ndemonstrates superior performance, even when matched against each method's\nstrengths in addressing their respective threats-each SOTA fails to parts of\nattack strategies while UniGuard succeeds for all."
    },
    {
        "date": "2025-06",
        "title": "General Autonomous Cybersecurity Defense: Learning Robust Policies for Dynamic Topologies and Diverse Attackers",
        "author": "Arun Ramamurthy, and Neil Dhir",
        "link": "http://arxiv.org/abs/2506.22706v1",
        "abstract": "In the face of evolving cyber threats such as malware, ransomware and\nphishing, autonomous cybersecurity defense (ACD) systems have become essential\nfor real-time threat detection and response with optional human intervention.\nHowever, existing ACD systems rely on limiting assumptions, particularly the\nstationarity of the underlying network dynamics. In real-world scenarios,\nnetwork topologies can change due to actions taken by attackers or defenders,\nsystem failures, or time evolution of networks, leading to failures in the\nadaptive capabilities of current defense agents. Moreover, many agents are\ntrained on static environments, resulting in overfitting to specific\ntopologies, which hampers their ability to generalize to out-of-distribution\nnetwork topologies. This work addresses these challenges by exploring methods\nfor developing agents to learn generalizable policies across dynamic network\nenvironments -- general ACD (GACD)."
    },
    {
        "date": "2025-06",
        "title": "Temperature Matters: Enhancing Watermark Robustness Against Paraphrasing Attacks",
        "author": "Badr Youbi Idrissi, Monica Millunzi, Amelia Sorrenti, Lorenzo Baraldi, and Daryna Dementieva",
        "link": "http://arxiv.org/abs/2506.22623v1",
        "abstract": "In the present-day scenario, Large Language Models (LLMs) are establishing\ntheir presence as powerful instruments permeating various sectors of society.\nWhile their utility offers valuable support to individuals, there are multiple\nconcerns over potential misuse. Consequently, some academic endeavors have\nsought to introduce watermarking techniques, characterized by the inclusion of\nmarkers within machine-generated text, to facilitate algorithmic\nidentification. This research project is focused on the development of a novel\nmethodology for the detection of synthetic text, with the overarching goal of\nensuring the ethical application of LLMs in AI-driven text generation. The\ninvestigation commences with replicating findings from a previous baseline\nstudy, thereby underscoring its susceptibility to variations in the underlying\ngeneration model. Subsequently, we propose an innovative watermarking approach\nand subject it to rigorous evaluation, employing paraphrased generated text to\nasses its robustness. Experimental results highlight the robustness of our\nproposal compared to the~\\cite{aarson} watermarking method."
    },
    {
        "date": "2025-06",
        "title": "Are Fast Methods Stable in Adversarially Robust Transfer Learning?",
        "author": "Joshua C. Zhao, and Saurabh Bagchi",
        "link": "http://arxiv.org/abs/2506.22602v1",
        "abstract": "Transfer learning is often used to decrease the computational cost of model\ntraining, as fine-tuning a model allows a downstream task to leverage the\nfeatures learned from the pre-training dataset and quickly adapt them to a new\ntask. This is particularly useful for achieving adversarial robustness, as\nadversarially training models from scratch is very computationally expensive.\nHowever, high robustness in transfer learning still requires adversarial\ntraining during the fine-tuning phase, which requires up to an order of\nmagnitude more time than standard fine-tuning. In this work, we revisit the use\nof the fast gradient sign method (FGSM) in robust transfer learning to improve\nthe computational cost of adversarial fine-tuning. We surprisingly find that\nFGSM is much more stable in adversarial fine-tuning than when training from\nscratch. In particular, FGSM fine-tuning does not suffer from any issues with\ncatastrophic overfitting at standard perturbation budgets of $\\varepsilon=4$ or\n$\\varepsilon=8$. This stability is further enhanced with parameter-efficient\nfine-tuning methods, where FGSM remains stable even up to $\\varepsilon=32$ for\nlinear probing. We demonstrate how this stability translates into performance\nacross multiple datasets. Compared to fine-tuning with the more commonly used\nmethod of projected gradient descent (PGD), on average, FGSM only loses 0.39%\nand 1.39% test robustness for $\\varepsilon=4$ and $\\varepsilon=8$ while using\n$4\\times$ less training time. Surprisingly, FGSM may not only be a\nsignificantly more efficient alternative to PGD in adversarially robust\ntransfer learning but also a well-performing one."
    },
    {
        "date": "2025-06",
        "title": "MetaCipher: A General and Extensible Reinforcement Learning Framework for Obfuscation-Based Jailbreak Attacks on Black-Box LLMs",
        "author": "Boyuan Chen, Minghao Shao, Abdul Basit, Siddharth Garg, and Muhammad Shafique",
        "link": "http://arxiv.org/abs/2506.22557v1",
        "abstract": "The growing capabilities of large language models (LLMs) have exposed them to\nincreasingly sophisticated jailbreak attacks. Among these, obfuscation-based\nattacks -- which encrypt malicious content to evade detection -- remain highly\neffective. By leveraging the reasoning ability of advanced LLMs to interpret\nencrypted prompts, such attacks circumvent conventional defenses that rely on\nkeyword detection or context filtering. These methods are very difficult to\ndefend against, as existing safety mechanisms are not designed to interpret or\ndecode ciphered content. In this work, we propose \\textbf{MetaCipher}, a novel\nobfuscation-based jailbreak framework, along with a reinforcement\nlearning-based dynamic cipher selection mechanism that adaptively chooses\noptimal encryption strategies from a cipher pool. This approach enhances\njailbreak effectiveness and generalizability across diverse task types, victim\nLLMs, and safety guardrails. Our framework is modular and extensible by design,\nsupporting arbitrary cipher families and accommodating evolving adversarial\nstrategies. We complement our method with a large-scale empirical analysis of\ncipher performance across multiple victim LLMs. Within as few as 10 queries,\nMetaCipher achieves over 92\\% attack success rate (ASR) on most recent standard\nmalicious prompt benchmarks against state-of-the-art non-reasoning LLMs, and\nover 74\\% ASR against reasoning-capable LLMs, outperforming all existing\nobfuscation-based jailbreak methods. These results highlight the long-term\nrobustness and adaptability of our approach, making it more resilient than\nprior methods in the face of advancing safety measures."
    },
    {
        "date": "2025-06",
        "title": "ARMOR: Robust Reinforcement Learning-based Control for UAVs under Physical Attacks",
        "author": "Pritam Dash, Ethan Chan, Nathan P. Lawrence, and Karthik Pattabiraman",
        "link": "http://arxiv.org/abs/2506.22423v1",
        "abstract": "Unmanned Aerial Vehicles (UAVs) depend on onboard sensors for perception,\nnavigation, and control. However, these sensors are susceptible to physical\nattacks, such as GPS spoofing, that can corrupt state estimates and lead to\nunsafe behavior. While reinforcement learning (RL) offers adaptive control\ncapabilities, existing safe RL methods are ineffective against such attacks. We\npresent ARMOR (Adaptive Robust Manipulation-Optimized State Representations),\nan attack-resilient, model-free RL controller that enables robust UAV operation\nunder adversarial sensor manipulation. Instead of relying on raw sensor\nobservations, ARMOR learns a robust latent representation of the UAV's physical\nstate via a two-stage training framework. In the first stage, a teacher\nencoder, trained with privileged attack information, generates attack-aware\nlatent states for RL policy training. In the second stage, a student encoder is\ntrained via supervised learning to approximate the teacher's latent states\nusing only historical sensor data, enabling real-world deployment without\nprivileged information. Our experiments show that ARMOR outperforms\nconventional methods, ensuring UAV safety. Additionally, ARMOR improves\ngeneralization to unseen attacks and reduces training cost by eliminating the\nneed for iterative adversarial training."
    },
    {
        "date": "2025-06",
        "title": "Multi-View Contrastive Learning for Robust Domain Adaptation in Medical Time Series Analysis",
        "author": "YongKyung Oh, and Alex Bui",
        "link": "http://arxiv.org/abs/2506.22393v1",
        "abstract": "Adapting machine learning models to medical time series across different\ndomains remains a challenge due to complex temporal dependencies and dynamic\ndistribution shifts. Current approaches often focus on isolated feature\nrepresentations, limiting their ability to fully capture the intricate temporal\ndynamics necessary for robust domain adaptation. In this work, we propose a\nnovel framework leveraging multi-view contrastive learning to integrate\ntemporal patterns, derivative-based dynamics, and frequency-domain features.\nOur method employs independent encoders and a hierarchical fusion mechanism to\nlearn feature-invariant representations that are transferable across domains\nwhile preserving temporal coherence. Extensive experiments on diverse medical\ndatasets, including electroencephalogram (EEG), electrocardiogram (ECG), and\nelectromyography (EMG) demonstrate that our approach significantly outperforms\nstate-of-the-art methods in transfer learning tasks. By advancing the\nrobustness and generalizability of machine learning models, our framework\noffers a practical pathway for deploying reliable AI systems in diverse\nhealthcare settings."
    },
    {
        "date": "2025-06",
        "title": "Conversational LLMs Simplify Secure Clinical Data Access, Understanding, and Analysis",
        "author": "Rafi Al Attrach, Pedro Moreira, Rajna Fani, Renato Umeton, and Leo Anthony Celi",
        "link": "http://arxiv.org/abs/2507.01053v1",
        "abstract": "As ever-larger clinical datasets become available, they have the potential to\nunlock unprecedented opportunities for medical research. Foremost among them is\nMedical Information Mart for Intensive Care (MIMIC-IV), the world's largest\nopen-source EHR database. However, the inherent complexity of these datasets,\nparticularly the need for sophisticated querying skills and the need to\nunderstand the underlying clinical settings, often presents a significant\nbarrier to their effective use. M3 lowers the technical barrier to\nunderstanding and querying MIMIC-IV data. With a single command it retrieves\nMIMIC-IV from PhysioNet, launches a local SQLite instance (or hooks into the\nhosted BigQuery), and-via the Model Context Protocol (MCP)-lets researchers\nconverse with the database in plain English. Ask a clinical question in natural\nlanguage; M3 uses a language model to translate it into SQL, executes the query\nagainst the MIMIC-IV dataset, and returns structured results alongside the\nunderlying query for verifiability and reproducibility. Demonstrations show\nthat minutes of dialogue with M3 yield the kind of nuanced cohort analyses that\nonce demanded hours of handcrafted SQL and relied on understanding the\ncomplexities of clinical workflows. By simplifying access, M3 invites the\nbroader research community to mine clinical critical-care data and accelerates\nthe translation of raw records into actionable insight."
    },
    {
        "date": "2025-06",
        "title": "From Ground to Air: Noise Robustness in Vision Transformers and CNNs for Event-Based Vehicle Classification with Potential UAV Applications",
        "author": "Nouf Almesafri, Hector Figueiredo, and Miguel Arana-Catania",
        "link": "http://arxiv.org/abs/2506.22360v1",
        "abstract": "This study investigates the performance of the two most relevant computer\nvision deep learning architectures, Convolutional Neural Network and Vision\nTransformer, for event-based cameras. These cameras capture scene changes,\nunlike traditional frame-based cameras with capture static images, and are\nparticularly suited for dynamic environments such as UAVs and autonomous\nvehicles. The deep learning models studied in this work are ResNet34 and ViT\nB16, fine-tuned on the GEN1 event-based dataset. The research evaluates and\ncompares these models under both standard conditions and in the presence of\nsimulated noise. Initial evaluations on the clean GEN1 dataset reveal that\nResNet34 and ViT B16 achieve accuracies of 88% and 86%, respectively, with\nResNet34 showing a slight advantage in classification accuracy. However, the\nViT B16 model demonstrates notable robustness, particularly given its\npre-training on a smaller dataset. Although this study focuses on ground-based\nvehicle classification, the methodologies and findings hold significant promise\nfor adaptation to UAV contexts, including aerial object classification and\nevent-based vision systems for aviation-related tasks."
    },
    {
        "date": "2025-06",
        "title": "Robust quantum reservoir computers for forecasting chaotic dynamics: generalized synchronization and stability",
        "author": "Osama Ahmed, Felix Tennie, and Luca Magri",
        "link": "http://arxiv.org/abs/2506.22335v1",
        "abstract": "We show that recurrent quantum reservoir computers (QRCs) and their\nrecurrence-free architectures (RF-QRCs) are robust tools for learning and\nforecasting chaotic dynamics from time-series data. First, we formulate and\ninterpret quantum reservoir computers as coupled dynamical systems, where the\nreservoir acts as a response system driven by training data; in other words,\nquantum reservoir computers are generalized-synchronization (GS) systems.\nSecond, we show that quantum reservoir computers can learn chaotic dynamics and\ntheir invariant properties, such as Lyapunov spectra, attractor dimensions, and\ngeometric properties such as the covariant Lyapunov vectors. This analysis is\nenabled by deriving the Jacobian of the quantum reservoir update. Third, by\nleveraging tools from generalized synchronization, we provide a method for\ndesigning robust quantum reservoir computers. We propose the criterion\n$GS=ESP$: GS implies the echo state property (ESP), and vice versa. We\nanalytically show that RF-QRCs, by design, fulfill $GS=ESP$. Finally, we\nanalyze the effect of simulated noise. We find that dissipation from noise\nenhances the robustness of quantum reservoir computers. Numerical verifications\non systems of different dimensions support our conclusions. This work opens\nopportunities for designing robust quantum machines for chaotic time series\nforecasting on near-term quantum hardware."
    },
    {
        "date": "2025-06",
        "title": "Robust and Accurate Multi-view 2D/3D Image Registration with Differentiable X-ray Rendering and Dual Cross-view Constraints",
        "author": "Yuxin Cui, Rui Song, Yibin Li, Max Q. -H. Meng, and Zhe Min",
        "link": "http://arxiv.org/abs/2506.22191v1",
        "abstract": "Robust and accurate 2D/3D registration, which aligns preoperative models with\nintraoperative images of the same anatomy, is crucial for successful\ninterventional navigation. To mitigate the challenge of a limited field of view\nin single-image intraoperative scenarios, multi-view 2D/3D registration is\nrequired by leveraging multiple intraoperative images. In this paper, we\npropose a novel multi-view 2D/3D rigid registration approach comprising two\nstages. In the first stage, a combined loss function is designed, incorporating\nboth the differences between predicted and ground-truth poses and the\ndissimilarities (e.g., normalized cross-correlation) between simulated and\nobserved intraoperative images. More importantly, additional cross-view\ntraining loss terms are introduced for both pose and image losses to explicitly\nenforce cross-view constraints. In the second stage, test-time optimization is\nperformed to refine the estimated poses from the coarse stage. Our method\nexploits the mutual constraints of multi-view projection poses to enhance the\nrobustness of the registration process. The proposed framework achieves a mean\ntarget registration error (mTRE) of $0.79 \\pm 2.17$ mm on six specimens from\nthe DeepFluoro dataset, demonstrating superior performance compared to\nstate-of-the-art registration algorithms."
    },
    {
        "date": "2025-06",
        "title": "Towards Scalable and Robust White Matter Lesion Localization via Multimodal Deep Learning",
        "author": "Julia Machnio, Sebastian N\u00f8rgaard Llambias, Mads Nielsen, and Mostafa Mehdipour Ghazi",
        "link": "http://arxiv.org/abs/2506.22041v1",
        "abstract": "White matter hyperintensities (WMH) are radiological markers of small vessel\ndisease and neurodegeneration, whose accurate segmentation and spatial\nlocalization are crucial for diagnosis and monitoring. While multimodal MRI\noffers complementary contrasts for detecting and contextualizing WM lesions,\nexisting approaches often lack flexibility in handling missing modalities and\nfail to integrate anatomical localization efficiently. We propose a deep\nlearning framework for WM lesion segmentation and localization that operates\ndirectly in native space using single- and multi-modal MRI inputs. Our study\nevaluates four input configurations: FLAIR-only, T1-only, concatenated FLAIR\nand T1, and a modality-interchangeable setup. It further introduces a\nmulti-task model for jointly predicting lesion and anatomical region masks to\nestimate region-wise lesion burden. Experiments conducted on the MICCAI WMH\nSegmentation Challenge dataset demonstrate that multimodal input significantly\nimproves the segmentation performance, outperforming unimodal models. While the\nmodality-interchangeable setting trades accuracy for robustness, it enables\ninference in cases with missing modalities. Joint lesion-region segmentation\nusing multi-task learning was less effective than separate models, suggesting\nrepresentational conflict between tasks. Our findings highlight the utility of\nmultimodal fusion for accurate and robust WMH analysis, and the potential of\njoint modeling for integrated predictions."
    },
    {
        "date": "2025-06",
        "title": "Advancing Jailbreak Strategies: A Hybrid Approach to Exploiting LLM Vulnerabilities and Bypassing Modern Defenses",
        "author": "Mohamed Ahmed, Mohamed Abdelmouty, Mingyu Kim, Gunvanth Kandula, Alex Park, and James C. Davis",
        "link": "http://arxiv.org/abs/2506.21972v1",
        "abstract": "The advancement of Pre-Trained Language Models (PTLMs) and Large Language\nModels (LLMs) has led to their widespread adoption across diverse applications.\nDespite their success, these models remain vulnerable to attacks that exploit\ntheir inherent weaknesses to bypass safety measures. Two primary\ninference-phase threats are token-level and prompt-level jailbreaks.\nToken-level attacks embed adversarial sequences that transfer well to black-box\nmodels like GPT but leave detectable patterns and rely on gradient-based token\noptimization, whereas prompt-level attacks use semantically structured inputs\nto elicit harmful responses yet depend on iterative feedback that can be\nunreliable. To address the complementary limitations of these methods, we\npropose two hybrid approaches that integrate token- and prompt-level techniques\nto enhance jailbreak effectiveness across diverse PTLMs. GCG + PAIR and the\nnewly explored GCG + WordGame hybrids were evaluated across multiple Vicuna and\nLlama models. GCG + PAIR consistently raised attack-success rates over its\nconstituent techniques on undefended models; for instance, on Llama-3, its\nAttack Success Rate (ASR) reached 91.6%, a substantial increase from PAIR's\n58.4% baseline. Meanwhile, GCG + WordGame matched the raw performance of\nWordGame maintaining a high ASR of over 80% even under stricter evaluators like\nMistral-Sorry-Bench. Crucially, both hybrids retained transferability and\nreliably pierced advanced defenses such as Gradient Cuff and JBShield, which\nfully blocked single-mode attacks. These findings expose previously unreported\nvulnerabilities in current safety stacks, highlight trade-offs between raw\nsuccess and defensive robustness, and underscore the need for holistic\nsafeguards against adaptive adversaries."
    },
    {
        "date": "2025-06",
        "title": "A Large Language Model-Empowered Agent for Reliable and Robust Structural Analysis",
        "author": "Jiachen Liu, Ziheng Geng, Ran Cao, Lu Cheng, Paolo Bocchini, and Minghui Cheng",
        "link": "http://arxiv.org/abs/2507.02938v1",
        "abstract": "Large language models (LLMs) have exhibited remarkable capabilities across\ndiverse open-domain tasks, yet their application in specialized domains such as\ncivil engineering remains largely unexplored. This paper starts bridging this\ngap by evaluating and enhancing the reliability and robustness of LLMs in\nstructural analysis of beams. Reliability is assessed through the accuracy of\ncorrect outputs under repetitive runs of the same problems, whereas robustness\nis evaluated via the performance across varying load and boundary conditions. A\nbenchmark dataset, comprising eight beam analysis problems, is created to test\nthe Llama-3.3 70B Instruct model. Results show that, despite a qualitative\nunderstanding of structural mechanics, the LLM lacks the quantitative\nreliability and robustness for engineering applications. To address these\nlimitations, a shift is proposed that reframes the structural analysis as code\ngeneration tasks. Accordingly, an LLM-empowered agent is developed that (a)\nintegrates chain-of-thought and few-shot prompting to generate accurate\nOpeeSeesPy code, and (b) automatically executes the code to produce structural\nanalysis results. Experimental results demonstrate that the agent achieves\naccuracy exceeding 99.0% on the benchmark dataset, exhibiting reliable and\nrobust performance across diverse conditions. Ablation studies highlight the\ncomplete example and function usage examples as the primary contributors to the\nagent's enhanced performance."
    },
    {
        "date": "2025-06",
        "title": "On the Feasibility of Poisoning Text-to-Image AI Models via Adversarial Mislabeling",
        "author": "Stanley Wu, Ronik Bhaskar, Anna Yoo Jeong Ha, Shawn Shan, Haitao Zheng, and Ben Y. Zhao",
        "link": "http://arxiv.org/abs/2506.21874v1",
        "abstract": "Today's text-to-image generative models are trained on millions of images\nsourced from the Internet, each paired with a detailed caption produced by\nVision-Language Models (VLMs). This part of the training pipeline is critical\nfor supplying the models with large volumes of high-quality image-caption pairs\nduring training. However, recent work suggests that VLMs are vulnerable to\nstealthy adversarial attacks, where adversarial perturbations are added to\nimages to mislead the VLMs into producing incorrect captions.\n  In this paper, we explore the feasibility of adversarial mislabeling attacks\non VLMs as a mechanism to poisoning training pipelines for text-to-image\nmodels. Our experiments demonstrate that VLMs are highly vulnerable to\nadversarial perturbations, allowing attackers to produce benign-looking images\nthat are consistently miscaptioned by the VLM models. This has the effect of\ninjecting strong \"dirty-label\" poison samples into the training pipeline for\ntext-to-image models, successfully altering their behavior with a small number\nof poisoned samples. We find that while potential defenses can be effective,\nthey can be targeted and circumvented by adaptive attackers. This suggests a\ncat-and-mouse game that is likely to reduce the quality of training data and\nincrease the cost of text-to-image model development. Finally, we demonstrate\nthe real-world effectiveness of these attacks, achieving high attack success\n(over 73%) even in black-box scenarios against commercial VLMs (Google Vertex\nAI and Microsoft Azure)."
    },
    {
        "date": "2025-06",
        "title": "Adversarial Threats in Quantum Machine Learning: A Survey of Attacks and Defenses",
        "author": "Archisman Ghosh, Satwik Kundu, and Swaroop Ghosh",
        "link": "http://arxiv.org/abs/2506.21842v1",
        "abstract": "Quantum Machine Learning (QML) integrates quantum computing with classical\nmachine learning, primarily to solve classification, regression and generative\ntasks. However, its rapid development raises critical security challenges in\nthe Noisy Intermediate-Scale Quantum (NISQ) era. This chapter examines\nadversarial threats unique to QML systems, focusing on vulnerabilities in\ncloud-based deployments, hybrid architectures, and quantum generative models.\nKey attack vectors include model stealing via transpilation or output\nextraction, data poisoning through quantum-specific perturbations, reverse\nengineering of proprietary variational quantum circuits, and backdoor attacks.\nAdversaries exploit noise-prone quantum hardware and insufficiently secured\nQML-as-a-Service (QMLaaS) workflows to compromise model integrity, ownership,\nand functionality. Defense mechanisms leverage quantum properties to counter\nthese threats. Noise signatures from training hardware act as non-invasive\nwatermarks, while hardware-aware obfuscation techniques and ensemble strategies\ndisrupt cloning attempts. Emerging solutions also adapt classical adversarial\ntraining and differential privacy to quantum settings, addressing\nvulnerabilities in quantum neural networks and generative architectures.\nHowever, securing QML requires addressing open challenges such as balancing\nnoise levels for reliability and security, mitigating cross-platform attacks,\nand developing quantum-classical trust frameworks. This chapter summarizes\nrecent advances in attacks and defenses, offering a roadmap for researchers and\npractitioners to build robust, trustworthy QML systems resilient to evolving\nadversarial landscapes."
    },
    {
        "date": "2025-06",
        "title": "ProSAM: Enhancing the Robustness of SAM-based Visual Reference Segmentation with Probabilistic Prompts",
        "author": "Xiaoqi Wang, Clint Sebastian, Wenbin He, and Liu Ren",
        "link": "http://arxiv.org/abs/2506.21835v2",
        "abstract": "The recent advancements in large foundation models have driven the success of\nopen-set image segmentation, a task focused on segmenting objects beyond\npredefined categories. Among various prompt types (such as points, boxes,\ntexts, and visual references), visual reference segmentation stands out for its\nunique flexibility and strong zero-shot capabilities. Recently, several\nSAM-based methods have made notable progress in this task by automatically\ngenerating prompts to guide SAM. However, these methods often generate prompts\nat object boundaries due to suboptimal prompt encoder, which results in\ninstability and reduced robustness. In this work, we introduce ProSAM, a simple\nbut effective method to address the stability challenges we identified in\nexisting SAM-based visual reference segmentation approaches. By learning a\nvariational prompt encoder to predict multivariate prompt distributions, ProSAM\navoids generating prompts that lie in unstable regions, overcoming the\ninstability caused by less robust prompts. Our approach consistently surpasses\nstate-of-the-art methods on the Pascal-5$^i$ and COCO-20$^i$ datasets,\nproviding a more robust solution for visual reference segmentation."
    },
    {
        "date": "2025-06",
        "title": "Multi-task parallelism for robust pre-training of graph foundation models on multi-source, multi-fidelity atomistic modeling data",
        "author": "Massimiliano Lupo Pasini, Jong Youl Choi, Pei Zhang, Kshitij Mehta, Rylie Weaver, Ashwin M. Aji, Karl W. Schulz, Jorda Polo, and Prasanna Balaprakash",
        "link": "http://arxiv.org/abs/2506.21788v1",
        "abstract": "Graph foundation models using graph neural networks promise sustainable,\nefficient atomistic modeling. To tackle challenges of processing multi-source,\nmulti-fidelity data during pre-training, recent studies employ multi-task\nlearning, in which shared message passing layers initially process input\natomistic structures regardless of source, then route them to multiple decoding\nheads that predict data-specific outputs. This approach stabilizes pre-training\nand enhances a model's transferability to unexplored chemical regions.\nPreliminary results on approximately four million structures are encouraging,\nyet questions remain about generalizability to larger, more diverse datasets\nand scalability on supercomputers. We propose a multi-task parallelism method\nthat distributes each head across computing resources with GPU acceleration.\nImplemented in the open-source HydraGNN architecture, our method was trained on\nover 24 million structures from five datasets and tested on the Perlmutter,\nAurora, and Frontier supercomputers, demonstrating efficient scaling on all\nthree highly heterogeneous super-computing architectures."
    },
    {
        "date": "2025-06",
        "title": "A Survey on Model Extraction Attacks and Defenses for Large Language Models",
        "author": "Kaixiang Zhao, Lincan Li, Kaize Ding, Neil Zhenqiang Gong, Yue Zhao, and Yushun Dong",
        "link": "http://arxiv.org/abs/2506.22521v1",
        "abstract": "Model extraction attacks pose significant security threats to deployed\nlanguage models, potentially compromising intellectual property and user\nprivacy. This survey provides a comprehensive taxonomy of LLM-specific\nextraction attacks and defenses, categorizing attacks into functionality\nextraction, training data extraction, and prompt-targeted attacks. We analyze\nvarious attack methodologies including API-based knowledge distillation, direct\nquerying, parameter recovery, and prompt stealing techniques that exploit\ntransformer architectures. We then examine defense mechanisms organized into\nmodel protection, data privacy protection, and prompt-targeted strategies,\nevaluating their effectiveness across different deployment scenarios. We\npropose specialized metrics for evaluating both attack effectiveness and\ndefense performance, addressing the specific challenges of generative language\nmodels. Through our analysis, we identify critical limitations in current\napproaches and propose promising research directions, including integrated\nattack methodologies and adaptive defense mechanisms that balance security with\nmodel utility. This work serves NLP researchers, ML engineers, and security\nprofessionals seeking to protect language models in production environments."
    },
    {
        "date": "2025-06",
        "title": "Maximal Matching Matters: Preventing Representation Collapse for Robust Cross-Modal Retrieval",
        "author": "Hani Alomari, Anushka Sivakumar, Andrew Zhang, and Chris Thomas",
        "link": "http://arxiv.org/abs/2506.21538v1",
        "abstract": "Cross-modal image-text retrieval is challenging because of the diverse\npossible associations between content from different modalities. Traditional\nmethods learn a single-vector embedding to represent semantics of each sample,\nbut struggle to capture nuanced and diverse relationships that can exist across\nmodalities. Set-based approaches, which represent each sample with multiple\nembeddings, offer a promising alternative, as they can capture richer and more\ndiverse relationships. In this paper, we show that, despite their promise,\nthese set-based representations continue to face issues including sparse\nsupervision and set collapse, which limits their effectiveness. To address\nthese challenges, we propose Maximal Pair Assignment Similarity to optimize\none-to-one matching between embedding sets which preserve semantic diversity\nwithin the set. We also introduce two loss functions to further enhance the\nrepresentations: Global Discriminative Loss to enhance distinction among\nembeddings, and Intra-Set Divergence Loss to prevent collapse within each set.\nOur method achieves state-of-the-art performance on MS-COCO and Flickr30k\nwithout relying on external data."
    },
    {
        "date": "2025-06",
        "title": "TITAN: Query-Token based Domain Adaptive Adversarial Learning",
        "author": "Tajamul Ashraf, and Janibul Bashir",
        "link": "http://arxiv.org/abs/2506.21484v1",
        "abstract": "We focus on the source-free domain adaptive object detection (SF-DAOD)\nproblem when source data is unavailable during adaptation and the model must\nadapt to an unlabeled target domain. The majority of approaches for the problem\nemploy a self-supervised approach using a student-teacher (ST) framework where\npseudo-labels are generated via a source-pretrained model for further\nfine-tuning. We observe that the performance of a student model often degrades\ndrastically, due to the collapse of the teacher model, primarily caused by high\nnoise in pseudo-labels, resulting from domain bias, discrepancies, and a\nsignificant domain shift across domains. To obtain reliable pseudo-labels, we\npropose a Target-based Iterative Query-Token Adversarial Network (TITAN), which\nseparates the target images into two subsets: those similar to the source\n(easy) and those dissimilar (hard). We propose a strategy to estimate variance\nto partition the target domain. This approach leverages the insight that higher\ndetection variances correspond to higher recall and greater similarity to the\nsource domain. Also, we incorporate query-token-based adversarial modules into\na student-teacher baseline framework to reduce the domain gaps between two\nfeature representations. Experiments conducted on four natural imaging datasets\nand two challenging medical datasets have substantiated the superior\nperformance of TITAN compared to existing state-of-the-art (SOTA)\nmethodologies. We report an mAP improvement of +22.7, +22.2, +21.1, and +3.7\npercent over the current SOTA on C2F, C2B, S2C, and K2C benchmarks,\nrespectively."
    },
    {
        "date": "2025-06",
        "title": "HyperSORT: Self-Organising Robust Training with hyper-networks",
        "author": "Samuel Joutard, Marijn Stollenga, Marc Balle Sanchez, Mohammad Farid Azampour, and Raphael Prevost",
        "link": "http://arxiv.org/abs/2506.21430v1",
        "abstract": "Medical imaging datasets often contain heterogeneous biases ranging from\nerroneous labels to inconsistent labeling styles. Such biases can negatively\nimpact deep segmentation networks performance. Yet, the identification and\ncharacterization of such biases is a particularly tedious and challenging task.\nIn this paper, we introduce HyperSORT, a framework using a hyper-network\npredicting UNets' parameters from latent vectors representing both the image\nand annotation variability. The hyper-network parameters and the latent vector\ncollection corresponding to each data sample from the training set are jointly\nlearned. Hence, instead of optimizing a single neural network to fit a dataset,\nHyperSORT learns a complex distribution of UNet parameters where low density\nareas can capture noise-specific patterns while larger modes robustly segment\norgans in differentiated but meaningful manners. We validate our method on two\n3D abdominal CT public datasets: first a synthetically perturbed version of the\nAMOS dataset, and TotalSegmentator, a large scale dataset containing real\nunknown biases and errors. Our experiments show that HyperSORT creates a\nstructured mapping of the dataset allowing the identification of relevant\nsystematic biases and erroneous samples. Latent space clusters yield UNet\nparameters performing the segmentation task in accordance with the underlying\nlearned systematic bias. The code and our analysis of the TotalSegmentator\ndataset are made available: https://github.com/ImFusionGmbH/HyperSORT"
    },
    {
        "date": "2025-06",
        "title": "GANet-Seg: Adversarial Learning for Brain Tumor Segmentation with Hybrid Generative Models",
        "author": "Qifei Cui, and Xinyu Lu",
        "link": "http://arxiv.org/abs/2506.21245v1",
        "abstract": "This work introduces a novel framework for brain tumor segmentation\nleveraging pre-trained GANs and Unet architectures. By combining a global\nanomaly detection module with a refined mask generation network, the proposed\nmodel accurately identifies tumor-sensitive regions and iteratively enhances\nsegmentation precision using adversarial loss constraints. Multi-modal MRI data\nand synthetic image augmentation are employed to improve robustness and address\nthe challenge of limited annotated datasets. Experimental results on the BraTS\ndataset demonstrate the effectiveness of the approach, achieving high\nsensitivity and accuracy in both lesion-wise Dice and HD95 metrics than the\nbaseline. This scalable method minimizes the dependency on fully annotated\ndata, paving the way for practical real-world applications in clinical\nsettings."
    },
    {
        "date": "2025-06",
        "title": "Robust Deep Learning for Myocardial Scar Segmentation in Cardiac MRI with Noisy Labels",
        "author": "Aida Moafi, Danial Moafi, Evgeny M. Mirkes, Gerry P. McCann, Abbas S. Alatrany, Jayanth R. Arnold, and Mostafa Mehdipour Ghazi",
        "link": "http://arxiv.org/abs/2506.21151v1",
        "abstract": "The accurate segmentation of myocardial scars from cardiac MRI is essential\nfor clinical assessment and treatment planning. In this study, we propose a\nrobust deep-learning pipeline for fully automated myocardial scar detection and\nsegmentation by fine-tuning state-of-the-art models. The method explicitly\naddresses challenges of label noise from semi-automatic annotations, data\nheterogeneity, and class imbalance through the use of Kullback-Leibler loss and\nextensive data augmentation. We evaluate the model's performance on both acute\nand chronic cases and demonstrate its ability to produce accurate and smooth\nsegmentations despite noisy labels. In particular, our approach outperforms\nstate-of-the-art models like nnU-Net and shows strong generalizability in an\nout-of-distribution test set, highlighting its robustness across various\nimaging conditions and clinical tasks. These results establish a reliable\nfoundation for automated myocardial scar quantification and support the broader\nclinical adoption of deep learning in cardiac imaging."
    },
    {
        "date": "2025-06",
        "title": "Generative Adversarial Evasion and Out-of-Distribution Detection for UAV Cyber-Attacks",
        "author": "Deepak Kumar Panda, and Weisi Guo",
        "link": "http://arxiv.org/abs/2506.21142v1",
        "abstract": "The growing integration of UAVs into civilian airspace underscores the need\nfor resilient and intelligent intrusion detection systems (IDS), as traditional\nanomaly detection methods often fail to identify novel threats. A common\napproach treats unfamiliar attacks as out-of-distribution (OOD) samples;\nhowever, this leaves systems vulnerable when mitigation is inadequate.\nMoreover, conventional OOD detectors struggle to distinguish stealthy\nadversarial attacks from genuine OOD events. This paper introduces a\nconditional generative adversarial network (cGAN)-based framework for crafting\nstealthy adversarial attacks that evade IDS mechanisms. We first design a\nrobust multi-class IDS classifier trained on benign UAV telemetry and known\ncyber-attacks, including Denial of Service (DoS), false data injection (FDI),\nman-in-the-middle (MiTM), and replay attacks. Using this classifier, our cGAN\nperturbs known attacks to generate adversarial samples that misclassify as\nbenign while retaining statistical resemblance to OOD distributions. These\nadversarial samples are iteratively refined to achieve high stealth and success\nrates. To detect such perturbations, we implement a conditional variational\nautoencoder (CVAE), leveraging negative log-likelihood to separate adversarial\ninputs from authentic OOD samples. Comparative evaluation shows that CVAE-based\nregret scores significantly outperform traditional Mahalanobis distance-based\ndetectors in identifying stealthy adversarial threats. Our findings emphasize\nthe importance of advanced probabilistic modeling to strengthen IDS\ncapabilities against adaptive, generative-model-based cyber intrusions."
    },
    {
        "date": "2025-06",
        "title": "Curriculum-Guided Antifragile Reinforcement Learning for Secure UAV Deconfliction under Observation-Space Attacks",
        "author": "Deepak Kumar Panda, Adolfo Perrusquia, and Weisi Guo",
        "link": "http://arxiv.org/abs/2506.21129v1",
        "abstract": "Reinforcement learning (RL) policies deployed in safety-critical systems,\nsuch as unmanned aerial vehicle (UAV) navigation in dynamic airspace, are\nvulnerable to out-ofdistribution (OOD) adversarial attacks in the observation\nspace. These attacks induce distributional shifts that significantly degrade\nvalue estimation, leading to unsafe or suboptimal decision making rendering the\nexisting policy fragile. To address this vulnerability, we propose an\nantifragile RL framework designed to adapt against curriculum of incremental\nadversarial perturbations. The framework introduces a simulated attacker which\nincrementally increases the strength of observation-space perturbations which\nenables the RL agent to adapt and generalize across a wider range of OOD\nobservations and anticipate previously unseen attacks. We begin with a\ntheoretical characterization of fragility, formally defining catastrophic\nforgetting as a monotonic divergence in value function distributions with\nincreasing perturbation strength. Building on this, we define antifragility as\nthe boundedness of such value shifts and derive adaptation conditions under\nwhich forgetting is stabilized. Our method enforces these bounds through\niterative expert-guided critic alignment using Wasserstein distance\nminimization across incrementally perturbed observations. We empirically\nevaluate the approach in a UAV deconfliction scenario involving dynamic 3D\nobstacles. Results show that the antifragile policy consistently outperforms\nstandard and robust RL baselines when subjected to both projected gradient\ndescent (PGD) and GPS spoofing attacks, achieving up to 15% higher cumulative\nreward and over 30% fewer conflict events. These findings demonstrate the\npractical and theoretical viability of antifragile reinforcement learning for\nsecure and resilient decision-making in environments with evolving threat\nscenarios."
    },
    {
        "date": "2025-06",
        "title": "Robust Policy Switching for Antifragile Reinforcement Learning for UAV Deconfliction in Adversarial Environments",
        "author": "Deepak Kumar Panda, and Weisi Guo",
        "link": "http://arxiv.org/abs/2506.21127v1",
        "abstract": "The increasing automation of navigation for unmanned aerial vehicles (UAVs)\nhas exposed them to adversarial attacks that exploit vulnerabilities in\nreinforcement learning (RL) through sensor manipulation. Although existing\nrobust RL methods aim to mitigate such threats, their effectiveness has limited\ngeneralization to out-of-distribution shifts from the optimal value\ndistribution, as they are primarily designed to handle fixed perturbation. To\naddress this limitation, this paper introduces an antifragile RL framework that\nenhances adaptability to broader distributional shifts by incorporating a\nswitching mechanism based on discounted Thompson sampling (DTS). This mechanism\ndynamically selects among multiple robust policies to minimize adversarially\ninduced state-action-value distribution shifts. The proposed approach first\nderives a diverse ensemble of action robust policies by accounting for a range\nof perturbations in the policy space. These policies are then modeled as a\nmultiarmed bandit (MAB) problem, where DTS optimally selects policies in\nresponse to nonstationary Bernoulli rewards, effectively adapting to evolving\nadversarial strategies. Theoretical framework has also been provided where by\noptimizing the DTS to minimize the overall regrets due to distributional shift,\nresults in effective adaptation against unseen adversarial attacks thus\ninducing antifragility. Extensive numerical simulations validate the\neffectiveness of the proposed framework in complex navigation environments with\nmultiple dynamic three-dimensional obstacles and with stronger projected\ngradient descent (PGD) and spoofing attacks. Compared to conventional robust,\nnon-adaptive RL methods, the antifragile approach achieves superior\nperformance, demonstrating shorter navigation path lengths and a higher rate of\nconflict-free navigation trajectories compared to existing robust RL techniques"
    },
    {
        "date": "2025-06",
        "title": "Boosting Generative Adversarial Transferability with Self-supervised Vision Transformer Features",
        "author": "Shangbo Wu, Yu-an Tan, Ruinan Ma, Wencong Ma, Dehua Zhu, and Yuanzhang Li",
        "link": "http://arxiv.org/abs/2506.21046v1",
        "abstract": "The ability of deep neural networks (DNNs) come from extracting and\ninterpreting features from the data provided. By exploiting intermediate\nfeatures in DNNs instead of relying on hard labels, we craft adversarial\nperturbation that generalize more effectively, boosting black-box\ntransferability. These features ubiquitously come from supervised learning in\nprevious work. Inspired by the exceptional synergy between self-supervised\nlearning and the Transformer architecture, this paper explores whether\nexploiting self-supervised Vision Transformer (ViT) representations can improve\nadversarial transferability. We present dSVA -- a generative dual\nself-supervised ViT features attack, that exploits both global structural\nfeatures from contrastive learning (CL) and local textural features from masked\nimage modeling (MIM), the self-supervised learning paradigm duo for ViTs. We\ndesign a novel generative training framework that incorporates a generator to\ncreate black-box adversarial examples, and strategies to train the generator by\nexploiting joint features and the attention mechanism of self-supervised ViTs.\nOur findings show that CL and MIM enable ViTs to attend to distinct feature\ntendencies, which, when exploited in tandem, boast great adversarial\ngeneralizability. By disrupting dual deep features distilled by self-supervised\nViTs, we are rewarded with remarkable black-box transferability to models of\nvarious architectures that outperform state-of-the-arts. Code available at\nhttps://github.com/spencerwooo/dSVA."
    },
    {
        "date": "2025-06",
        "title": "HybridQ: Hybrid Classical-Quantum Generative Adversarial Network for Skin Disease Image Generation",
        "author": "Qingyue Jiao, Kangyu Zheng, Yiyu Shi, and Zhiding Liang",
        "link": "http://arxiv.org/abs/2506.21015v1",
        "abstract": "Machine learning-assisted diagnosis is gaining traction in skin disease\ndetection, but training effective models requires large amounts of high-quality\ndata. Skin disease datasets often suffer from class imbalance, privacy\nconcerns, and object bias, making data augmentation essential. While classical\ngenerative models are widely used, they demand extensive computational\nresources and lengthy training time. Quantum computing offers a promising\nalternative, but existing quantum-based image generation methods can only yield\ngrayscale low-quality images. Through a novel classical-quantum latent space\nfusion technique, our work overcomes this limitation and introduces the first\nclassical-quantum generative adversarial network (GAN) capable of generating\ncolor medical images. Our model outperforms classical deep convolutional GANs\nand existing hybrid classical-quantum GANs in both image generation quality and\nclassification performance boost when used as data augmentation. Moreover, the\nperformance boost is comparable with that achieved using state-of-the-art\nclassical generative models, yet with over 25 times fewer parameters and 10\ntimes fewer training epochs. Such results suggest a promising future for\nquantum image generation as quantum hardware advances. Finally, we demonstrate\nthe robust performance of our model on real IBM quantum machine with hardware\nnoise."
    },
    {
        "date": "2025-06",
        "title": "Style-Aligned Image Composition for Robust Detection of Abnormal Cells in Cytopathology",
        "author": "Qiuyi Qi, Xin Li, Ming Kong, Zikang Xu, Bingdi Chen, Qiang Zhu, and S Kevin Zhou",
        "link": "http://arxiv.org/abs/2506.21001v1",
        "abstract": "Challenges such as the lack of high-quality annotations, long-tailed data\ndistributions, and inconsistent staining styles pose significant obstacles to\ntraining neural networks to detect abnormal cells in cytopathology robustly.\nThis paper proposes a style-aligned image composition (SAIC) method that\ncomposes high-fidelity and style-preserved pathological images to enhance the\neffectiveness and robustness of detection models. Without additional training,\nSAIC first selects an appropriate candidate from the abnormal cell bank based\non attribute guidance. Then, it employs a high-frequency feature reconstruction\nto achieve a style-aligned and high-fidelity composition of abnormal cells and\npathological backgrounds. Finally, it introduces a large vision-language model\nto filter high-quality synthesis images. Experimental results demonstrate that\nincorporating SAIC-synthesized images effectively enhances the performance and\nrobustness of abnormal cell detection for tail categories and styles, thereby\nimproving overall detection performance. The comprehensive quality evaluation\nfurther confirms the generalizability and practicality of SAIC in clinical\napplication scenarios. Our code will be released at\nhttps://github.com/Joey-Qi/SAIC."
    },
    {
        "date": "2025-06",
        "title": "SPA: Towards More Stealth and Persistent Backdoor Attacks in Federated Learning",
        "author": "Chengcheng Zhu, Ye Li, Bosen Rao, Jiale Zhang, Yunlong Mao, and Sheng Zhong",
        "link": "http://arxiv.org/abs/2506.20931v1",
        "abstract": "Federated Learning (FL) has emerged as a leading paradigm for\nprivacy-preserving distributed machine learning, yet the distributed nature of\nFL introduces unique security challenges, notably the threat of backdoor\nattacks. Existing backdoor strategies predominantly rely on end-to-end label\nsupervision, which, despite their efficacy, often results in detectable feature\ndisentanglement and limited persistence. In this work, we propose a novel and\nstealthy backdoor attack framework, named SPA, which fundamentally departs from\ntraditional approaches by leveraging feature-space alignment rather than direct\ntrigger-label association. Specifically, SPA reduces representational distances\nbetween backdoor trigger features and target class features, enabling the\nglobal model to misclassify trigger-embedded inputs with high stealth and\npersistence. We further introduce an adaptive, adversarial trigger optimization\nmechanism, utilizing boundary-search in the feature space to enhance attack\nlongevity and effectiveness, even against defensive FL scenarios and non-IID\ndata distributions. Extensive experiments on various FL benchmarks demonstrate\nthat SPA consistently achieves high attack success rates with minimal impact on\nmodel utility, maintains robustness under challenging participation and data\nheterogeneity conditions, and exhibits persistent backdoor effects far\nexceeding those of conventional techniques. Our results call urgent attention\nto the evolving sophistication of backdoor threats in FL and emphasize the\npressing need for advanced, feature-level defense techniques."
    },
    {
        "date": "2025-06",
        "title": "Development of MR spectral analysis method robust against static magnetic field inhomogeneity",
        "author": "Shuki Maruyama, and Hidenori Takeshima",
        "link": "http://arxiv.org/abs/2506.20897v1",
        "abstract": "Purpose:To develop a method that enhances the accuracy of spectral analysis\nin the presence of static magnetic field B0 inhomogeneity. Methods:The authors\nproposed a new spectral analysis method utilizing a deep learning model trained\non modeled spectra that consistently represent the spectral variations induced\nby B0 inhomogeneity. These modeled spectra were generated from the B0 map and\nmetabolite ratios of the healthy human brain. The B0 map was divided into a\npatch size of subregions, and the separately estimated metabolites and baseline\ncomponents were averaged and then integrated. The quality of the modeled\nspectra was visually and quantitatively evaluated against the measured spectra.\nThe analysis models were trained using measured, simulated, and modeled\nspectra. The performance of the proposed method was assessed using mean squared\nerrors (MSEs) of metabolite ratios. The mean absolute percentage errors (MAPEs)\nof the metabolite ratios were also compared to LCModel when analyzing the\nphantom spectra acquired under two types of B0 inhomogeneity. Results:The\nmodeled spectra exhibited broadened and narrowed spectral peaks depending on\nthe B0 inhomogeneity and were quantitatively close to the measured spectra. The\nanalysis model trained using measured spectra with modeled spectra improved\nMSEs by 49.89% compared to that trained using measured spectra alone, and by\n26.66% compared to that trained using measured spectra with simulated spectra.\nThe performance improved as the number of modeled spectra increased from 0 to\n1,000. This model showed significantly lower MAPEs than LCModel under both\ntypes of B0 inhomogeneity. Conclusion:A new spectral analysis-trained deep\nlearning model using the modeled spectra was developed. The results suggest\nthat the proposed method has the potential to improve the accuracy of spectral\nanalysis by increasing the training samples of spectra."
    },
    {
        "date": "2025-06",
        "title": "Universal and Efficient Detection of Adversarial Data through Nonuniform Impact on Network Layers",
        "author": "Furkan Mumcu, and Yasin Yilmaz",
        "link": "http://arxiv.org/abs/2506.20816v1",
        "abstract": "Deep Neural Networks (DNNs) are notoriously vulnerable to adversarial input\ndesigns with limited noise budgets. While numerous successful attacks with\nsubtle modifications to original input have been proposed, defense techniques\nagainst these attacks are relatively understudied. Existing defense approaches\neither focus on improving DNN robustness by negating the effects of\nperturbations or use a secondary model to detect adversarial data. Although\nequally important, the attack detection approach, which is studied in this\nwork, provides a more practical defense compared to the robustness approach. We\nshow that the existing detection methods are either ineffective against the\nstate-of-the-art attack techniques or computationally inefficient for real-time\nprocessing. We propose a novel universal and efficient method to detect\nadversarial examples by analyzing the varying degrees of impact of attacks on\ndifferent DNN layers. {Our method trains a lightweight regression model that\npredicts deeper-layer features from early-layer features, and uses the\nprediction error to detect adversarial samples.} Through theoretical arguments\nand extensive experiments, we demonstrate that our detection method is highly\neffective, computationally efficient for real-time processing, compatible with\nany DNN architecture, and applicable across different domains, such as image,\nvideo, and audio."
    },
    {
        "date": "2025-06",
        "title": "Poster: Enhancing GNN Robustness for Network Intrusion Detection via Agent-based Analysis",
        "author": "Zhonghao Zhan, Huichi Zhou, and Hamed Haddadi",
        "link": "http://arxiv.org/abs/2506.20806v1",
        "abstract": "Graph Neural Networks (GNNs) show great promise for Network Intrusion\nDetection Systems (NIDS), particularly in IoT environments, but suffer\nperformance degradation due to distribution drift and lack robustness against\nrealistic adversarial attacks. Current robustness evaluations often rely on\nunrealistic synthetic perturbations and lack demonstrations on systematic\nanalysis of different kinds of adversarial attack, which encompass both\nblack-box and white-box scenarios. This work proposes a novel approach to\nenhance GNN robustness and generalization by employing Large Language Models\n(LLMs) in an agentic pipeline as simulated cybersecurity expert agents. These\nagents scrutinize graph structures derived from network flow data, identifying\nand potentially mitigating suspicious or adversarially perturbed elements\nbefore GNN processing. Our experiments, using a framework designed for\nrealistic evaluation and testing with a variety of adversarial attacks\nincluding a dataset collected from physical testbed experiments, demonstrate\nthat integrating LLM analysis can significantly improve the resilience of\nGNN-based NIDS against challenges, showcasing the potential of LLM agent as a\ncomplementary layer in intrusion detection architectures."
    },
    {
        "date": "2025-06",
        "title": "On the Impact of Sybil-based Attacks on Mobile Crowdsensing for Transportation",
        "author": "Alexander S\u00f6derh\u00e4ll, Zahra Alimadadi, and Panos Papadimitratos",
        "link": "http://arxiv.org/abs/2506.20585v1",
        "abstract": "Mobile Crowd-Sensing (MCS) enables users with personal mobile devices (PMDs)\nto gain information on their surroundings. Users collect and contribute data on\ndifferent phenomena using their PMD sensors, and the MCS system processes this\ndata to extract valuable information for end users. Navigation MCS-based\napplications (N-MCS) are prevalent and important for transportation: users\nshare their location and speed while driving and, in return, find efficient\nroutes to their destinations. However, N-MCS are currently vulnerable to\nmalicious contributors, often termed Sybils: submitting falsified data,\nseemingly from many devices that are not truly present on target roads, falsely\nreporting congestion when there is none, thus changing the road status the\nN-MCS infers. The attack effect is that the N-MCS returns suboptimal routes to\nusers, causing late arrival and, overall, deteriorating road traffic flow. We\ninvestigate exactly the impact of Sybil-based attacks on N-MCS: we design an\nN-MCS system that offers efficient routing on top of the vehicular simulator\nSUMO, using the InTAS road network as our scenario. We design experiments\nattacking an individual N-MCS user as well as a larger population of users,\nselecting the adversary targets based on graph-theoretical arguments. Our\nexperiments show that the resources required for a successful attack depend on\nthe location of the attack (i.e., the surrounding road network and traffic) and\nthe extent of Sybil contributed data for the targeted road(s). We demonstrate\nthat Sybil attacks can alter the route of N-MCS users, increasing average\ntravel time by 20% with Sybils 3% of the N-MCS user population."
    },
    {
        "date": "2025-06",
        "title": "Vulnerability Disclosure through Adaptive Black-Box Adversarial Attacks on NIDS",
        "author": "Sabrine Ennaji, Elhadj Benkhelifa, and Luigi V. Mancini",
        "link": "http://arxiv.org/abs/2506.20576v1",
        "abstract": "Adversarial attacks, wherein slight inputs are carefully crafted to mislead\nintelligent models, have attracted increasing attention. However, a critical\ngap persists between theoretical advancements and practical application,\nparticularly in structured data like network traffic, where interdependent\nfeatures complicate effective adversarial manipulations. Moreover, ambiguity in\ncurrent approaches restricts reproducibility and limits progress in this field.\nHence, existing defenses often fail to handle evolving adversarial attacks.\nThis paper proposes a novel approach for black-box adversarial attacks, that\naddresses these limitations. Unlike prior work, which often assumes system\naccess or relies on repeated probing, our method strictly respect black-box\nconstraints, reducing interaction to avoid detection and better reflect\nreal-world scenarios. We present an adaptive feature selection strategy using\nchange-point detection and causality analysis to identify and target sensitive\nfeatures to perturbations. This lightweight design ensures low computational\ncost and high deployability. Our comprehensive experiments show the attack's\neffectiveness in evading detection with minimal interaction, enhancing its\nadaptability and applicability in real-world scenarios. By advancing the\nunderstanding of adversarial attacks in network traffic, this work lays a\nfoundation for developing robust defenses."
    },
    {
        "date": "2025-06",
        "title": "LARP: Learner-Agnostic Robust Data Prefiltering",
        "author": "Kristian Minchev, Dimitar Iliev Dimitrov, and Nikola Konstantinov",
        "link": "http://arxiv.org/abs/2506.20573v2",
        "abstract": "The widespread availability of large public datasets is a key factor behind\nthe recent successes of statistical inference and machine learning methods.\nHowever, these datasets often contain some low-quality or contaminated data, to\nwhich many learning procedures are sensitive. Therefore, the question of\nwhether and how public datasets should be prefiltered to facilitate accurate\ndownstream learning arises. On a technical level this requires the construction\nof principled data prefiltering methods which are learner-agnostic robust, in\nthe sense of provably protecting a set of pre-specified downstream learners\nfrom corrupted data. In this work, we formalize the problem of Learner-Agnostic\nRobust data Prefiltering (LARP), which aims at finding prefiltering procedures\nthat minimize a worst-case loss over a pre-specified set of learners. We first\ninstantiate our framework in the context of scalar mean estimation with Huber\nestimators under the Huber data contamination model. We provide a hardness\nresult on a specific problem instance and analyze several natural prefiltering\nprocedures. Our theoretical results indicate that performing LARP on a\nheterogeneous set of learners leads to some loss in model performance compared\nto the alternative of prefiltering data for each learner/use-case individually.\nWe explore the resulting utility loss and its dependence on the problem\nparameters via extensive experiments on real-world image and tabular data,\nobserving statistically significant reduction in utility. Finally, we model the\ntrade-off between the utility drop and the cost of repeated (learner-specific)\nprefiltering within a game-theoretic framework and showcase benefits of LARP\nfor large datasets."
    },
    {
        "date": "2025-06",
        "title": "AdvMIM: Adversarial Masked Image Modeling for Semi-Supervised Medical Image Segmentation",
        "author": "Lei Zhu, Jun Zhou, Rick Siow Mong Goh, and Yong Liu",
        "link": "http://arxiv.org/abs/2506.20563v1",
        "abstract": "Vision Transformer has recently gained tremendous popularity in medical image\nsegmentation task due to its superior capability in capturing long-range\ndependencies. However, transformer requires a large amount of labeled data to\nbe effective, which hinders its applicability in annotation scarce\nsemi-supervised learning scenario where only limited labeled data is available.\nState-of-the-art semi-supervised learning methods propose combinatorial\nCNN-Transformer learning to cross teach a transformer with a convolutional\nneural network, which achieves promising results. However, it remains a\nchallenging task to effectively train the transformer with limited labeled\ndata. In this paper, we propose an adversarial masked image modeling method to\nfully unleash the potential of transformer for semi-supervised medical image\nsegmentation. The key challenge in semi-supervised learning with transformer\nlies in the lack of sufficient supervision signal. To this end, we propose to\nconstruct an auxiliary masked domain from original domain with masked image\nmodeling and train the transformer to predict the entire segmentation mask with\nmasked inputs to increase supervision signal. We leverage the original labels\nfrom labeled data and pseudo-labels from unlabeled data to learn the masked\ndomain. To further benefit the original domain from masked domain, we provide a\ntheoretical analysis of our method from a multi-domain learning perspective and\ndevise a novel adversarial training loss to reduce the domain gap between the\noriginal and masked domain, which boosts semi-supervised learning performance.\nWe also extend adversarial masked image modeling to CNN network. Extensive\nexperiments on three public medical image segmentation datasets demonstrate the\neffectiveness of our method, where our method outperforms existing methods\nsignificantly. Our code is publicly available at\nhttps://github.com/zlheui/AdvMIM."
    }
]