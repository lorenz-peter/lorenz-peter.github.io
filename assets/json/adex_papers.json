[
    {
        "date": "2025-10",
        "title": "StealthAttack: Robust 3D Gaussian Splatting Poisoning via Density-Guided Illusions",
        "author": "Bo-Hsu Ke, You-Zhe Xie, Yu-Lun Liu, and Wei-Chen Chiu",
        "link": "http://arxiv.org/abs/2510.02314v1",
        "abstract": "3D scene representation methods like Neural Radiance Fields (NeRF) and 3D\nGaussian Splatting (3DGS) have significantly advanced novel view synthesis. As\nthese methods become prevalent, addressing their vulnerabilities becomes\ncritical. We analyze 3DGS robustness against image-level poisoning attacks and\npropose a novel density-guided poisoning method. Our method strategically\ninjects Gaussian points into low-density regions identified via Kernel Density\nEstimation (KDE), embedding viewpoint-dependent illusory objects clearly\nvisible from poisoned views while minimally affecting innocent views.\nAdditionally, we introduce an adaptive noise strategy to disrupt multi-view\nconsistency, further enhancing attack effectiveness. We propose a KDE-based\nevaluation protocol to assess attack difficulty systematically, enabling\nobjective benchmarking for future research. Extensive experiments demonstrate\nour method's superior performance compared to state-of-the-art techniques.\nProject page: https://hentci.github.io/stealthattack/"
    },
    {
        "date": "2025-10",
        "title": "Robust Tangent Space Estimation via Laplacian Eigenvector Gradient Orthogonalization",
        "author": "Dhruv Kohli, Sawyer J. Robertson, Gal Mishne, and Alexander Cloninger",
        "link": "http://arxiv.org/abs/2510.02308v1",
        "abstract": "Estimating the tangent spaces of a data manifold is a fundamental problem in\ndata analysis. The standard approach, Local Principal Component Analysis\n(LPCA), struggles in high-noise settings due to a critical trade-off in\nchoosing the neighborhood size. Selecting an optimal size requires prior\nknowledge of the geometric and noise characteristics of the data that are often\nunavailable. In this paper, we propose a spectral method, Laplacian Eigenvector\nGradient Orthogonalization (LEGO), that utilizes the global structure of the\ndata to guide local tangent space estimation. Instead of relying solely on\nlocal neighborhoods, LEGO estimates the tangent space at each data point by\northogonalizing the gradients of low-frequency eigenvectors of the graph\nLaplacian. We provide two theoretical justifications of our method. First, a\ndifferential geometric analysis on a tubular neighborhood of a manifold shows\nthat gradients of the low-frequency Laplacian eigenfunctions of the tube align\nclosely with the manifold's tangent bundle, while an eigenfunction with high\ngradient in directions orthogonal to the manifold lie deeper in the spectrum.\nSecond, a random matrix theoretic analysis also demonstrates that low-frequency\neigenvectors are robust to sub-Gaussian noise. Through comprehensive\nexperiments, we demonstrate that LEGO yields tangent space estimates that are\nsignificantly more robust to noise than those from LPCA, resulting in marked\nimprovements in downstream tasks such as manifold learning, boundary detection,\nand local intrinsic dimension estimation."
    },
    {
        "date": "2025-10",
        "title": "Tree-based Dialogue Reinforced Policy Optimization for Red-Teaming Attacks",
        "author": "Ruohao Guo, Afshin Oroojlooy, Roshan Sridhar, Miguel Ballesteros, Alan Ritter, and Dan Roth",
        "link": "http://arxiv.org/abs/2510.02286v1",
        "abstract": "Despite recent rapid progress in AI safety, current large language models\nremain vulnerable to adversarial attacks in multi-turn interaction settings,\nwhere attackers strategically adapt their prompts across conversation turns and\npose a more critical yet realistic challenge. Existing approaches that discover\nsafety vulnerabilities either rely on manual red-teaming with human experts or\nemploy automated methods using pre-defined templates and human-curated attack\ndata, with most focusing on single-turn attacks. However, these methods did not\nexplore the vast space of possible multi-turn attacks, failing to consider\nnovel attack trajectories that emerge from complex dialogue dynamics and\nstrategic conversation planning. This gap is particularly critical given recent\nfindings that LLMs exhibit significantly higher vulnerability to multi-turn\nattacks compared to single-turn attacks. We propose DialTree-RPO, an on-policy\nreinforcement learning framework integrated with tree search that autonomously\ndiscovers diverse multi-turn attack strategies by treating the dialogue as a\nsequential decision-making problem, enabling systematic exploration without\nmanually curated data. Through extensive experiments, our approach not only\nachieves more than 25.9% higher ASR across 10 target models compared to\nprevious state-of-the-art approaches, but also effectively uncovers new attack\nstrategies by learning optimal dialogue policies that maximize attack success\nacross multiple turns."
    },
    {
        "date": "2025-10",
        "title": "How to Combat Reactive and Dynamic Jamming Attacks with Reinforcement Learning",
        "author": "Yalin E. Sagduyu, Tugba Erpek, Kemal Davaslioglu, and Sastry Kompella",
        "link": "http://arxiv.org/abs/2510.02265v1",
        "abstract": "This paper studies the problem of mitigating reactive jamming, where a jammer\nadopts a dynamic policy of selecting channels and sensing thresholds to detect\nand jam ongoing transmissions. The transmitter-receiver pair learns to avoid\njamming and optimize throughput over time (without prior knowledge of channel\nconditions or jamming strategies) by using reinforcement learning (RL) to adapt\ntransmit power, modulation, and channel selection. Q-learning is employed for\ndiscrete jamming-event states, while Deep Q-Networks (DQN) are employed for\ncontinuous states based on received power. Through different reward functions\nand action sets, the results show that RL can adapt rapidly to spectrum\ndynamics and sustain high rates as channels and jamming policies change over\ntime."
    },
    {
        "date": "2025-10",
        "title": "PUL-Inter-slice Defender: An Anomaly Detection Solution for Distributed Slice Mobility Attacks",
        "author": "Ricardo Misael Ayala Molina, Hyame Assem Alameddine, Makan Pourzandi, and Chadi Assi",
        "link": "http://arxiv.org/abs/2510.02236v1",
        "abstract": "Network Slices (NSs) are virtual networks operating over a shared physical\ninfrastructure, each designed to meet specific application requirements while\nmaintaining consistent Quality of Service (QoS). In Fifth Generation (5G)\nnetworks, User Equipment (UE) can connect to and seamlessly switch between\nmultiple NSs to access diverse services. However, this flexibility, known as\nInter-Slice Switching (ISS), introduces a potential vulnerability that can be\nexploited to launch Distributed Slice Mobility (DSM) attacks, a form of\nDistributed Denial of Service (DDoS) attack. To secure 5G networks and their\nNSs against DSM attacks, we present in this work, PUL-Inter-Slice Defender; an\nanomaly detection solution that leverages Positive Unlabeled Learning (PUL) and\nincorporates a combination of Long Short-Term Memory Autoencoders and K-Means\nclustering. PUL-Inter-Slice Defender leverages the Third Generation Partnership\nProject (3GPP) key performance indicators and performance measurement counters\nas features for its machine learning models to detect DSM attack variants while\nmaintaining robustness in the presence of contaminated training data. When\nevaluated on data collected from our 5G testbed based on the open-source\nfree5GC and UERANSIM, a UE/ Radio Access Network (RAN) simulator;\nPUL-Inter-Slice Defender achieved F1-scores exceeding 98.50% on training\ndatasets with 10% to 40% attack contamination, consistently outperforming its\ncounterpart Inter-Slice Defender and other PUL based solutions combining\nOne-Class Support Vector Machine (OCSVM) with Random Forest and XGBoost."
    },
    {
        "date": "2025-10",
        "title": "Authentication Security of PRF GNSS Ranging",
        "author": "Jason Anderson",
        "link": "http://arxiv.org/abs/2510.02196v1",
        "abstract": "This work derives the authentication security of pseudorandom function (PRF)\nGNSS ranging under multiple GNSS spoofing models, including the Security Code\nEstimation and Replay (SCER) spoofer. When GNSS ranging codes derive from a PRF\nutilizing a secret known only to the broadcaster, the spoofer cannot predict\nthe ranging code before broadcast. Therefore, PRF ranging can be used to\nestablish trust in the GNSS pseudoranges and the resulting receiver position,\nnavigation, and timing (PNT) solution. I apply the methods herein to Galileo's\nSignal Authentication Service (SAS) utilizing the encrypted Galileo E6-C signal\nto compute that, at most, 400 ms of Galileo E6-C data to assert 128-bit\nauthentication security under non-SCER models. For the SCER adversary, I\npredict the adversary's needed receiving radio equipment to break\nauthentication security. One can use this work to design a PRF GNSS ranging\nprotocol to meet useful authentication security requirements by computing the\nprobability of missed detection."
    },
    {
        "date": "2025-10",
        "title": "NoMod: A Non-modular Attack on Module Learning With Errors",
        "author": "Cristian Bassotto, Ermes Franch, Marina Kr\u010dek, and Stjepan Picek",
        "link": "http://arxiv.org/abs/2510.02162v1",
        "abstract": "The advent of quantum computing threatens classical public-key cryptography,\nmotivating NIST's adoption of post-quantum schemes such as those based on the\nModule Learning With Errors (Module-LWE) problem. We present NoMod ML-Attack, a\nhybrid white-box cryptanalytic method that circumvents the challenge of\nmodeling modular reduction by treating wrap-arounds as statistical corruption\nand casting secret recovery as robust linear estimation. Our approach combines\noptimized lattice preprocessing--including reduced-vector saving and algebraic\namplification--with robust estimators trained via Tukey's Biweight loss.\nExperiments show NoMod achieves full recovery of binary secrets for dimension\n$n = 350$, recovery of sparse binomial secrets for $n = 256$, and successful\nrecovery of sparse secrets in CRYSTALS-Kyber settings with parameters $(n, k) =\n(128, 3)$ and $(256, 2)$. We release our implementation in an anonymous\nrepository https://anonymous.4open.science/r/NoMod-3BD4."
    },
    {
        "date": "2025-10",
        "title": "Mirage Fools the Ear, Mute Hides the Truth: Precise Targeted Adversarial Attacks on Polyphonic Sound Event Detection Systems",
        "author": "Junjie Su, Weifei Jin, Yuxin Cao, Derui Wang, Kai Ye, and Jie Hao",
        "link": "http://arxiv.org/abs/2510.02158v1",
        "abstract": "Sound Event Detection (SED) systems are increasingly deployed in\nsafety-critical applications such as industrial monitoring and audio\nsurveillance. However, their robustness against adversarial attacks has not\nbeen well explored. Existing audio adversarial attacks targeting SED systems,\nwhich incorporate both detection and localization capabilities, often lack\neffectiveness due to SED's strong contextual dependencies or lack precision by\nfocusing solely on misclassifying the target region as the target event,\ninadvertently affecting non-target regions. To address these challenges, we\npropose the Mirage and Mute Attack (M2A) framework, which is designed for\ntargeted adversarial attacks on polyphonic SED systems. In our optimization\nprocess, we impose specific constraints on the non-target output, which we\nrefer to as preservation loss, ensuring that our attack does not alter the\nmodel outputs for non-target region, thus achieving precise attacks.\nFurthermore, we introduce a novel evaluation metric Editing Precison (EP) that\nbalances effectiveness and precision, enabling our method to simultaneously\nenhance both. Comprehensive experiments show that M2A achieves 94.56% and\n99.11% EP on two state-of-the-art SED models, demonstrating that the framework\nis sufficiently effective while significantly enhancing attack precision."
    },
    {
        "date": "2025-10",
        "title": "Adaptive Heterogeneous Mixtures of Normalising Flows for Robust Variational Inference",
        "author": "Benjamin Wiriyapong, Oktay Karaku\u015f, and Kirill Sidorov",
        "link": "http://arxiv.org/abs/2510.02056v1",
        "abstract": "Normalising-flow variational inference (VI) can approximate complex\nposteriors, yet single-flow models often behave inconsistently across\nqualitatively different distributions. We propose Adaptive Mixture Flow\nVariational Inference (AMF-VI), a heterogeneous mixture of complementary flows\n(MAF, RealNVP, RBIG) trained in two stages: (i) sequential expert training of\nindividual flows, and (ii) adaptive global weight estimation via\nlikelihood-driven updates, without per-sample gating or architectural changes.\nEvaluated on six canonical posterior families of banana, X-shape, two-moons,\nrings, a bimodal, and a five-mode mixture, AMF-VI achieves consistently lower\nnegative log-likelihood than each single-flow baseline and delivers stable\ngains in transport metrics (Wasserstein-2) and maximum mean discrepancy (MDD),\nindicating improved robustness across shapes and modalities. The procedure is\nefficient and architecture-agnostic, incurring minimal overhead relative to\nstandard flow training, and demonstrates that adaptive mixtures of diverse\nflows provide a reliable route to robust VI across diverse posterior families\nwhilst preserving each expert's inductive bias."
    },
    {
        "date": "2025-10",
        "title": "Lower Bounds on Adversarial Robustness for Multiclass Classification with General Loss Functions",
        "author": "Camilo Andr\u00e9s Garc\u00eda Trillos, and Nicol\u00e1s Garc\u00eda Trillos",
        "link": "http://arxiv.org/abs/2510.01969v1",
        "abstract": "We consider adversarially robust classification in a multiclass setting under\narbitrary loss functions and derive dual and barycentric reformulations of the\ncorresponding learner-agnostic robust risk minimization problem. We provide\nexplicit characterizations for important cases such as the cross-entropy loss,\nloss functions with a power form, and the quadratic loss, extending in this way\navailable results for the 0-1 loss. These reformulations enable efficient\ncomputation of sharp lower bounds for adversarial risks and facilitate the\ndesign of robust classifiers beyond the 0-1 loss setting. Our paper uncovers\ninteresting connections between adversarial robustness, $\\alpha$-fair packing\nproblems, and generalized barycenter problems for arbitrary positive measures\nwhere Kullback-Leibler and Tsallis entropies are used as penalties. Our\ntheoretical results are accompanied with illustrative numerical experiments\nwhere we obtain tighter lower bounds for adversarial risks with the\ncross-entropy loss function."
    },
    {
        "date": "2025-10",
        "title": "Are LLMs Better GNN Helpers? Rethinking Robust Graph Learning under Deficiencies with Iterative Refinement",
        "author": "Zhaoyan Wang, Zheng Gao, Arogya Kharel, and In-Young Ko",
        "link": "http://arxiv.org/abs/2510.01910v1",
        "abstract": "Graph Neural Networks (GNNs) are widely adopted in Web-related applications,\nserving as a core technique for learning from graph-structured data, such as\ntext-attributed graphs. Yet in real-world scenarios, such graphs exhibit\ndeficiencies that substantially undermine GNN performance. While prior\nGNN-based augmentation studies have explored robustness against individual\nimperfections, a systematic understanding of how graph-native and Large\nLanguage Models (LLMs) enhanced methods behave under compound deficiencies is\nstill missing. Specifically, there has been no comprehensive investigation\ncomparing conventional approaches and recent LLM-on-graph frameworks, leaving\ntheir merits unclear. To fill this gap, we conduct the first empirical study\nthat benchmarks these two lines of methods across diverse graph deficiencies,\nrevealing overlooked vulnerabilities and challenging the assumption that LLM\naugmentation is consistently superior. Building on empirical findings, we\npropose Robust Graph Learning via Retrieval-Augmented Contrastive Refinement\n(RoGRAD) framework. Unlike prior one-shot LLM-as-Enhancer designs, RoGRAD is\nthe first iterative paradigm that leverages Retrieval-Augmented Generation\n(RAG) to inject retrieval-grounded augmentations by supplying class-consistent,\ndiverse augmentations and enforcing discriminative representations through\niterative graph contrastive learning. It transforms LLM augmentation for graphs\nfrom static signal injection into dynamic refinement. Extensive experiments\ndemonstrate RoGRAD's superiority over both conventional GNN- and LLM-enhanced\nbaselines, achieving up to 82.43% average improvement."
    },
    {
        "date": "2025-10",
        "title": "REPAIR: Robust Editing via Progressive Adaptive Intervention and Reintegration",
        "author": "Yisu Wang, Ming Wang, Haoyuan Song, Wenjie Huang, Chaozheng Wang, Yi Xie, and Xuming Ran",
        "link": "http://arxiv.org/abs/2510.01879v1",
        "abstract": "Post-training for large language models (LLMs) is constrained by the high\ncost of acquiring new knowledge or correcting errors and by the unintended side\neffects that frequently arise from retraining. To address these issues, we\nintroduce REPAIR (Robust Editing via Progressive Adaptive Intervention and\nReintegration), a lifelong editing framework designed to support precise and\nlow-cost model updates while preserving non-target knowledge. REPAIR mitigates\nthe instability and conflicts of large-scale sequential edits through a\nclosed-loop feedback mechanism coupled with dynamic memory management.\nFurthermore, by incorporating frequent knowledge fusion and enforcing strong\nlocality guards, REPAIR effectively addresses the shortcomings of traditional\ndistribution-agnostic approaches that often overlook unintended ripple effects.\nOur experiments demonstrate that REPAIR boosts editing accuracy by 10%-30%\nacross multiple model families and significantly reduces knowledge forgetting.\nThis work introduces a robust framework for developing reliable, scalable, and\ncontinually evolving LLMs."
    },
    {
        "date": "2025-10",
        "title": "Secure Multi-Modal Data Fusion in Federated Digital Health Systems via MCP",
        "author": "Aueaphum Aueawatthanaphisut",
        "link": "http://arxiv.org/abs/2510.01780v1",
        "abstract": "Secure and interoperable integration of heterogeneous medical data remains a\ngrand challenge in digital health. Current federated learning (FL) frameworks\noffer privacy-preserving model training but lack standardized mechanisms to\norchestrate multi-modal data fusion across distributed and resource-constrained\nenvironments. This study introduces a novel framework that leverages the Model\nContext Protocol (MCP) as an interoperability layer for secure, cross-agent\ncommunication in multi-modal federated healthcare systems. The proposed\narchitecture unifies three pillars: (i) multi-modal feature alignment for\nclinical imaging, electronic medical records, and wearable IoT data; (ii)\nsecure aggregation with differential privacy to protect patient-sensitive\nupdates; and (iii) energy-aware scheduling to mitigate dropouts in mobile\nclients. By employing MCP as a schema-driven interface, the framework enables\nadaptive orchestration of AI agents and toolchains while ensuring compliance\nwith privacy regulations. Experimental evaluation on benchmark datasets and\npilot clinical cohorts demonstrates up to 9.8\\% improvement in diagnostic\naccuracy compared with baseline FL, a 54\\% reduction in client dropout rates,\nand clinically acceptable privacy--utility trade-offs. These results highlight\nMCP-enabled multi-modal fusion as a scalable and trustworthy pathway toward\nequitable, next-generation federated health infrastructures."
    },
    {
        "date": "2025-10",
        "title": "Unsupervised Dynamic Feature Selection for Robust Latent Spaces in Vision Tasks",
        "author": "Bruno Corcuera, Carlos Eiras-Franco, and Brais Cancela",
        "link": "http://arxiv.org/abs/2510.01758v1",
        "abstract": "Latent representations are critical for the performance and robustness of\nmachine learning models, as they encode the essential features of data in a\ncompact and informative manner. However, in vision tasks, these representations\nare often affected by noisy or irrelevant features, which can degrade the\nmodel's performance and generalization capabilities. This paper presents a\nnovel approach for enhancing latent representations using unsupervised Dynamic\nFeature Selection (DFS). For each instance, the proposed method identifies and\nremoves misleading or redundant information in images, ensuring that only the\nmost relevant features contribute to the latent space. By leveraging an\nunsupervised framework, our approach avoids reliance on labeled data, making it\nbroadly applicable across various domains and datasets. Experiments conducted\non image datasets demonstrate that models equipped with unsupervised DFS\nachieve significant improvements in generalization performance across various\ntasks, including clustering and image generation, while incurring a minimal\nincrease in the computational cost."
    },
    {
        "date": "2025-10",
        "title": "Finite-Time Bounds for Distributionally Robust TD Learning with Linear Function Approximation",
        "author": "Saptarshi Mandal, Yashaswini Murthy, and R. Srikant",
        "link": "http://arxiv.org/abs/2510.01721v1",
        "abstract": "Distributionally robust reinforcement learning (DRRL) focuses on designing\npolicies that achieve good performance under model uncertainties. In\nparticular, we are interested in maximizing the worst-case long-term discounted\nreward, where the data for RL comes from a nominal model while the deployed\nenvironment can deviate from the nominal model within a prescribed uncertainty\nset. Existing convergence guarantees for robust temporal-difference (TD)\nlearning for policy evaluation are limited to tabular MDPs or are dependent on\nrestrictive discount-factor assumptions when function approximation is used. We\npresent the first robust TD learning with linear function approximation, where\nrobustness is measured with respect to the total-variation distance and\nWasserstein-l distance uncertainty set. Additionally, our algorithm is both\nmodel-free and does not require generative access to the MDP. Our algorithm\ncombines a two-time-scale stochastic-approximation update with an outer-loop\ntarget-network update. We establish an $\\tilde{O}(1/\\epsilon^2)$ sample\ncomplexity to obtain an $\\epsilon$-accurate value estimate. Our results close a\nkey gap between the empirical success of robust RL algorithms and the\nnon-asymptotic guarantees enjoyed by their non-robust counterparts. The key\nideas in the paper also extend in a relatively straightforward fashion to\nrobust Q-learning with function approximation."
    },
    {
        "date": "2025-10",
        "title": "Towards Imperceptible Adversarial Defense: A Gradient-Driven Shield against Facial Manipulations",
        "author": "Yue Li, Linying Xue, Dongdong Lin, Qiushi Li, Hui Tian, and Hongxia Wang",
        "link": "http://arxiv.org/abs/2510.01699v1",
        "abstract": "With the flourishing prosperity of generative models, manipulated facial\nimages have become increasingly accessible, raising concerns regarding privacy\ninfringement and societal trust. In response, proactive defense strategies\nembed adversarial perturbations into facial images to counter deepfake\nmanipulation. However, existing methods often face a tradeoff between\nimperceptibility and defense effectiveness-strong perturbations may disrupt\nforgeries but degrade visual fidelity. Recent studies have attempted to address\nthis issue by introducing additional visual loss constraints, yet often\noverlook the underlying gradient conflicts among losses, ultimately weakening\ndefense performance. To bridge the gap, we propose a gradient-projection-based\nadversarial proactive defense (GRASP) method that effectively counters facial\ndeepfakes while minimizing perceptual degradation. GRASP is the first approach\nto successfully integrate both structural similarity loss and low-frequency\nloss to enhance perturbation imperceptibility. By analyzing gradient conflicts\nbetween defense effectiveness loss and visual quality losses, GRASP pioneers\nthe design of the gradient-projection mechanism to mitigate these conflicts,\nenabling balanced optimization that preserves image fidelity without\nsacrificing defensive performance. Extensive experiments validate the efficacy\nof GRASP, achieving a PSNR exceeding 40 dB, SSIM of 0.99, and a 100% defense\nsuccess rate against facial attribute manipulations, significantly\noutperforming existing approaches in visual quality."
    },
    {
        "date": "2025-10",
        "title": "Beyond Simple Fusion: Adaptive Gated Fusion for Robust Multimodal Sentiment Analysis",
        "author": "Han Wu, Yanming Sun, Yunhe Yang, and Derek F. Wong",
        "link": "http://arxiv.org/abs/2510.01677v1",
        "abstract": "Multimodal sentiment analysis (MSA) leverages information fusion from diverse\nmodalities (e.g., text, audio, visual) to enhance sentiment prediction.\nHowever, simple fusion techniques often fail to account for variations in\nmodality quality, such as those that are noisy, missing, or semantically\nconflicting. This oversight leads to suboptimal performance, especially in\ndiscerning subtle emotional nuances. To mitigate this limitation, we introduce\na simple yet efficient \\textbf{A}daptive \\textbf{G}ated \\textbf{F}usion\n\\textbf{N}etwork that adaptively adjusts feature weights via a dual gate fusion\nmechanism based on information entropy and modality importance. This mechanism\nmitigates the influence of noisy modalities and prioritizes informative cues\nfollowing unimodal encoding and cross-modal interaction. Experiments on\nCMU-MOSI and CMU-MOSEI show that AGFN significantly outperforms strong\nbaselines in accuracy, effectively discerning subtle emotions with robust\nperformance. Visualization analysis of feature representations demonstrates\nthat AGFN enhances generalization by learning from a broader feature\ndistribution, achieved by reducing the correlation between feature location and\nprediction error, thereby decreasing reliance on specific locations and\ncreating more robust multimodal feature representations."
    },
    {
        "date": "2025-10",
        "title": "Evaluating the Robustness of a Production Malware Detection System to Transferable Adversarial Attacks",
        "author": "Milad Nasr, Yanick Fratantonio, Luca Invernizzi, Ange Albertini, Loua Farah, Alex Petit-Bianco, Andreas Terzis, Kurt Thomas, Elie Bursztein, and Nicholas Carlini",
        "link": "http://arxiv.org/abs/2510.01676v1",
        "abstract": "As deep learning models become widely deployed as components within larger\nproduction systems, their individual shortcomings can create system-level\nvulnerabilities with real-world impact. This paper studies how adversarial\nattacks targeting an ML component can degrade or bypass an entire\nproduction-grade malware detection system, performing a case study analysis of\nGmail's pipeline where file-type identification relies on a ML model.\n  The malware detection pipeline in use by Gmail contains a machine learning\nmodel that routes each potential malware sample to a specialized malware\nclassifier to improve accuracy and performance. This model, called Magika, has\nbeen open sourced. By designing adversarial examples that fool Magika, we can\ncause the production malware service to incorrectly route malware to an\nunsuitable malware detector thereby increasing our chance of evading detection.\nSpecifically, by changing just 13 bytes of a malware sample, we can\nsuccessfully evade Magika in 90% of cases and thereby allow us to send malware\nfiles over Gmail. We then turn our attention to defenses, and develop an\napproach to mitigate the severity of these types of attacks. For our defended\nproduction model, a highly resourced adversary requires 50 bytes to achieve\njust a 20% attack success rate. We implement this defense, and, thanks to a\ncollaboration with Google engineers, it has already been deployed in production\nfor the Gmail classifier."
    },
    {
        "date": "2025-10",
        "title": "UniVerse: Unleashing the Scene Prior of Video Diffusion Models for Robust Radiance Field Reconstruction",
        "author": "Jin Cao, Hongrui Wu, Ziyong Feng, Hujun Bao, Xiaowei Zhou, and Sida Peng",
        "link": "http://arxiv.org/abs/2510.01669v1",
        "abstract": "This paper tackles the challenge of robust reconstruction, i.e., the task of\nreconstructing a 3D scene from a set of inconsistent multi-view images. Some\nrecent works have attempted to simultaneously remove image inconsistencies and\nperform reconstruction by integrating image degradation modeling into neural 3D\nscene representations.However, these methods rely heavily on dense observations\nfor robustly optimizing model parameters.To address this issue, we propose to\ndecouple robust reconstruction into two subtasks: restoration and\nreconstruction, which naturally simplifies the optimization process.To this\nend, we introduce UniVerse, a unified framework for robust reconstruction based\non a video diffusion model. Specifically, UniVerse first converts inconsistent\nimages into initial videos, then uses a specially designed video diffusion\nmodel to restore them into consistent images, and finally reconstructs the 3D\nscenes from these restored images.Compared with case-by-case per-view\ndegradation modeling, the diffusion model learns a general scene prior from\nlarge-scale data, making it applicable to diverse image\ninconsistencies.Extensive experiments on both synthetic and real-world datasets\ndemonstrate the strong generalization capability and superior performance of\nour method in robust reconstruction. Moreover, UniVerse can control the style\nof the reconstructed 3D scene. Project page:\nhttps://jin-cao-tma.github.io/UniVerse.github.io/"
    },
    {
        "date": "2025-10",
        "title": "SoK: Measuring What Matters for Closed-Loop Security Agents",
        "author": "Mudita Khurana, and Raunak Jain",
        "link": "http://arxiv.org/abs/2510.01654v1",
        "abstract": "Cybersecurity is a relentless arms race, with AI driven offensive systems\nevolving faster than traditional defenses can adapt. Research and tooling\nremain fragmented across isolated defensive functions, creating blind spots\nthat adversaries exploit. Autonomous agents capable of integrating, exploit\nconfirmation, remediation, and validation into a single closed loop offer\npromise, but the field lacks three essentials: a framework defining the agentic\ncapabilities of security systems across security life cycle, a principled\nmethod for evaluating closed loop agents, and a benchmark for measuring their\nperformance in practice. We introduce CLASP: the Closed-Loop Autonomous\nSecurity Performance framework which aligns the security lifecycle\n(reconnaissance, exploitation, root cause analysis, patch synthesis,\nvalidation) with core agentic capabilities (planning, tool use, memory,\nreasoning, reflection & perception) providing a common vocabulary and rubric\nfor assessing agentic capabilities in security tasks. By applying CLASP to 21\nrepresentative works, we map where systems demonstrate strengths, and where\ncapability gaps persist. We then define the Closed-Loop Capability (CLC) Score,\na composite metric quantifying both degree of loop closure and operational\neffectiveness, and outline the requirements for a closed loop benchmark.\nTogether, CLASP and the CLC Score, provide the vocabulary, diagnostics, and\nmeasurements needed to advance both function level performance and measure\nclosed loop security agents."
    },
    {
        "date": "2025-10",
        "title": "MPMAvatar: Learning 3D Gaussian Avatars with Accurate and Robust Physics-Based Dynamics",
        "author": "Changmin Lee, Jihyun Lee, and Tae-Kyun Kim",
        "link": "http://arxiv.org/abs/2510.01619v1",
        "abstract": "While there has been significant progress in the field of 3D avatar creation\nfrom visual observations, modeling physically plausible dynamics of humans with\nloose garments remains a challenging problem. Although a few existing works\naddress this problem by leveraging physical simulation, they suffer from\nlimited accuracy or robustness to novel animation inputs. In this work, we\npresent MPMAvatar, a framework for creating 3D human avatars from multi-view\nvideos that supports highly realistic, robust animation, as well as\nphotorealistic rendering from free viewpoints. For accurate and robust dynamics\nmodeling, our key idea is to use a Material Point Method-based simulator, which\nwe carefully tailor to model garments with complex deformations and contact\nwith the underlying body by incorporating an anisotropic constitutive model and\na novel collision handling algorithm. We combine this dynamics modeling scheme\nwith our canonical avatar that can be rendered using 3D Gaussian Splatting with\nquasi-shadowing, enabling high-fidelity rendering for physically realistic\nanimations. In our experiments, we demonstrate that MPMAvatar significantly\noutperforms the existing state-of-the-art physics-based avatar in terms of (1)\ndynamics modeling accuracy, (2) rendering accuracy, and (3) robustness and\nefficiency. Additionally, we present a novel application in which our avatar\ngeneralizes to unseen interactions in a zero-shot manner-which was not\nachievable with previous learning-based methods due to their limited simulation\ngeneralizability. Our project page is at:\nhttps://KAISTChangmin.github.io/MPMAvatar/"
    },
    {
        "date": "2025-10",
        "title": "Securing generative artificial intelligence with parallel magnetic tunnel junction true randomness",
        "author": "Youwei Bao, Shuhan Yang, and Hyunsoo Yang",
        "link": "http://arxiv.org/abs/2510.01598v1",
        "abstract": "Deterministic pseudo random number generators (PRNGs) used in generative\nartificial intelligence (GAI) models produce predictable patterns vulnerable to\nexploitation by attackers. Conventional defences against the vulnerabilities\noften come with significant energy and latency overhead. Here, we embed\nhardware-generated true random bits from spin-transfer torque magnetic tunnel\njunctions (STT-MTJs) to address the challenges. A highly parallel,\nFPGA-assisted prototype computing system delivers megabit-per-second true\nrandom numbers, passing NIST randomness tests after in-situ operations with\nminimal overhead. Integrating the hardware random bits into a generative\nadversarial network (GAN) trained on CIFAR-10 reduces insecure outputs by up to\n18.6 times compared to the low-quality random number generators (RNG) baseline.\nWith nanosecond switching speed, high energy efficiency, and established\nscalability, our STT-MTJ-based system holds the potential to scale beyond 106\nparallel cells, achieving gigabit-per-second throughput suitable for large\nlanguage model sampling. This advancement highlights spintronic RNGs as\npractical security components for next-generation GAI systems."
    },
    {
        "date": "2025-10",
        "title": "Enhancing Noise Robustness of Parkinson's Disease Telemonitoring via Contrastive Feature Augmentation",
        "author": "Ziming Tang, Chengbin Hou, Tianyu Zhang, Bangxu Tian, Jinbao Wang, and Hairong Lv",
        "link": "http://arxiv.org/abs/2510.01588v1",
        "abstract": "Parkinson's disease (PD) is one of the most common neurodegenerative\ndisorder. PD telemonitoring emerges as a novel assessment modality enabling\nself-administered at-home tests of Unified Parkinson's Disease Rating Scale\n(UPDRS) scores, enhancing accessibility for PD patients. However, three types\nof noise would occur during measurements: (1) patient-induced measurement\ninaccuracies, (2) environmental noise, and (3) data packet loss during\ntransmission, resulting in higher prediction errors. To address these\nchallenges, NoRo, a noise-robust UPDRS prediction framework is proposed. First,\nthe original speech features are grouped into ordered bins, based on the\ncontinuous values of a selected feature, to construct contrastive pairs.\nSecond, the contrastive pairs are employed to train a multilayer perceptron\nencoder for generating noise-robust features. Finally, these features are\nconcatenated with the original features as the augmented features, which are\nthen fed into the UPDRS prediction models. Notably, we further introduces a\nnovel evaluation approach with customizable noise injection module, and\nextensive experiments show that NoRo can successfully enhance the noise\nrobustness of UPDRS prediction across various downstream prediction models\nunder different noisy environments."
    },
    {
        "date": "2025-10",
        "title": "AdvEvo-MARL: Shaping Internalized Safety through Adversarial Co-Evolution in Multi-Agent Reinforcement Learning",
        "author": "Zhenyu Pan, Yiting Zhang, Zhuo Liu, Yolo Yunlong Tang, Zeliang Zhang, Haozheng Luo, Yuwei Han, Jianshu Zhang, Dennis Wu, Hong-Yu Chen, Haoran Lu, Haoyang Fang, Manling Li, Chenliang Xu, Philip S. Yu, and Han Liu",
        "link": "http://arxiv.org/abs/2510.01586v1",
        "abstract": "LLM-based multi-agent systems excel at planning, tool use, and role\ncoordination, but their openness and interaction complexity also expose them to\njailbreak, prompt-injection, and adversarial collaboration. Existing defenses\nfall into two lines: (i) self-verification that asks each agent to pre-filter\nunsafe instructions before execution, and (ii) external guard modules that\npolice behaviors. The former often underperforms because a standalone agent\nlacks sufficient capacity to detect cross-agent unsafe chains and\ndelegation-induced risks; the latter increases system overhead and creates a\nsingle-point-of-failure-once compromised, system-wide safety collapses, and\nadding more guards worsens cost and complexity. To solve these challenges, we\npropose AdvEvo-MARL, a co-evolutionary multi-agent reinforcement learning\nframework that internalizes safety into task agents. Rather than relying on\nexternal guards, AdvEvo-MARL jointly optimizes attackers (which synthesize\nevolving jailbreak prompts) and defenders (task agents trained to both\naccomplish their duties and resist attacks) in adversarial learning\nenvironments. To stabilize learning and foster cooperation, we introduce a\npublic baseline for advantage estimation: agents within the same functional\ngroup share a group-level mean-return baseline, enabling lower-variance updates\nand stronger intra-group coordination. Across representative attack scenarios,\nAdvEvo-MARL consistently keeps attack-success rate (ASR) below 20%, whereas\nbaselines reach up to 38.33%, while preserving-and sometimes improving-task\naccuracy (up to +3.67% on reasoning tasks). These results show that safety and\nutility can be jointly improved without relying on extra guard agents or added\nsystem overhead."
    },
    {
        "date": "2025-10",
        "title": "Robust Classification of Oral Cancer with Limited Training Data",
        "author": "Akshay Bhagwan Sonawane, Lena D. Swamikannan, and Lakshman Tamil",
        "link": "http://arxiv.org/abs/2510.01547v1",
        "abstract": "Oral cancer ranks among the most prevalent cancers globally, with a\nparticularly high mortality rate in regions lacking adequate healthcare access.\nEarly diagnosis is crucial for reducing mortality; however, challenges persist\ndue to limited oral health programs, inadequate infrastructure, and a shortage\nof healthcare practitioners. Conventional deep learning models, while\npromising, often rely on point estimates, leading to overconfidence and reduced\nreliability. Critically, these models require large datasets to mitigate\noverfitting and ensure generalizability, an unrealistic demand in settings with\nlimited training data. To address these issues, we propose a hybrid model that\ncombines a convolutional neural network (CNN) with Bayesian deep learning for\noral cancer classification using small training sets. This approach employs\nvariational inference to enhance reliability through uncertainty\nquantification. The model was trained on photographic color images captured by\nsmartphones and evaluated on three distinct test datasets. The proposed method\nachieved 94% accuracy on a test dataset with a distribution similar to that of\nthe training data, comparable to traditional CNN performance. Notably, for\nreal-world photographic image data, despite limitations and variations\ndiffering from the training dataset, the proposed model demonstrated superior\ngeneralizability, achieving 88% accuracy on diverse datasets compared to 72.94%\nfor traditional CNNs, even with a smaller dataset. Confidence analysis revealed\nthat the model exhibits low uncertainty (high confidence) for correctly\nclassified samples and high uncertainty (low confidence) for misclassified\nsamples. These results underscore the effectiveness of Bayesian inference in\ndata-scarce environments in enhancing early oral cancer diagnosis by improving\nmodel reliability and generalizability."
    },
    {
        "date": "2025-10",
        "title": "Information Seeking for Robust Decision Making under Partial Observability",
        "author": "Djengo Cyun-Jyun Fang, and Tsung-Wei Ke",
        "link": "http://arxiv.org/abs/2510.01531v1",
        "abstract": "Explicit information seeking is essential to human problem-solving in\npractical environments characterized by incomplete information and noisy\ndynamics. When the true environmental state is not directly observable, humans\nseek information to update their internal dynamics and inform future\ndecision-making. Although existing Large Language Model (LLM) planning agents\nhave addressed observational uncertainty, they often overlook discrepancies\nbetween their internal dynamics and the actual environment. We introduce\nInformation Seeking Decision Planner (InfoSeeker), an LLM decision-making\nframework that integrates task-oriented planning with information seeking to\nalign internal dynamics and make optimal decisions under uncertainty in both\nagent observations and environmental dynamics. InfoSeeker prompts an LLM to\nactively gather information by planning actions to validate its understanding,\ndetect environmental changes, or test hypotheses before generating or revising\ntask-oriented plans. To evaluate InfoSeeker, we introduce a novel benchmark\nsuite featuring partially observable environments with incomplete observations\nand uncertain dynamics. Experiments demonstrate that InfoSeeker achieves a 74%\nabsolute performance gain over prior methods without sacrificing sample\nefficiency. Moreover, InfoSeeker generalizes across LLMs and outperforms\nbaselines on established benchmarks such as robotic manipulation and web\nnavigation. These findings underscore the importance of tightly integrating\nplanning and information seeking for robust behavior in partially observable\nenvironments. The project page is available at https://infoseekerllm.github.io"
    },
    {
        "date": "2025-10",
        "title": "Understanding Adversarial Transfer: Why Representation-Space Attacks Fail Where Data-Space Attacks Succeed",
        "author": "Isha Gupta, Rylan Schaeffer, Joshua Kazdan, Ken Liu, and Sanmi Koyejo",
        "link": "http://arxiv.org/abs/2510.01494v1",
        "abstract": "The field of adversarial robustness has long established that adversarial\nexamples can successfully transfer between image classifiers and that text\njailbreaks can successfully transfer between language models (LMs). However, a\npair of recent studies reported being unable to successfully transfer image\njailbreaks between vision-language models (VLMs). To explain this striking\ndifference, we propose a fundamental distinction regarding the transferability\nof attacks against machine learning models: attacks in the input data-space can\ntransfer, whereas attacks in model representation space do not, at least not\nwithout geometric alignment of representations. We then provide theoretical and\nempirical evidence of this hypothesis in four different settings. First, we\nmathematically prove this distinction in a simple setting where two networks\ncompute the same input-output map but via different representations. Second, we\nconstruct representation-space attacks against image classifiers that are as\nsuccessful as well-known data-space attacks, but fail to transfer. Third, we\nconstruct representation-space attacks against LMs that successfully jailbreak\nthe attacked models but again fail to transfer. Fourth, we construct data-space\nattacks against VLMs that successfully transfer to new VLMs, and we show that\nrepresentation space attacks \\emph{can} transfer when VLMs' latent geometries\nare sufficiently aligned in post-projector space. Our work reveals that\nadversarial transfer is not an inherent property of all attacks but contingent\non their operational domain - the shared data-space versus models' unique\nrepresentation spaces - a critical insight for building more robust models."
    },
    {
        "date": "2025-10",
        "title": "Securing IoT Devices in Smart Cities: A Review of Proposed Solutions",
        "author": "Andr\u00e9s F. Betancur-L\u00f3pez",
        "link": "http://arxiv.org/abs/2510.01445v1",
        "abstract": "Privacy and security in Smart Cities remain at constant risk due to the\nvulnerabilities introduced by Internet of Things (IoT) devices. The limited\ncomputational resources of these devices make them especially susceptible to\nattacks, while their widespread adoption increases the potential impact of\nsecurity breaches. This article presents a review of security proposals aimed\nat protecting IoT devices in Smart City environments. The review was conducted\nby analyzing recent literature on device-level security, with particular\nemphasis on lightweight cryptography, physically unclonable functions (PUFs),\nand blockchain-based solutions. Findings highlight both the strengths and\nlimitations of current approaches, as well as the need for more practical,\nscalable, and resource-efficient mechanisms to ensure user privacy and data\nprotection in IoT ecosystems."
    },
    {
        "date": "2025-10",
        "title": "E-FuzzEdge: Optimizing Embedded Device Security with Scalable In-Place Fuzzing",
        "author": "Davide Rusconi, Osama Yousef, Mirco Picca, Flavio Toffalini, and Andrea Lanzi",
        "link": "http://arxiv.org/abs/2510.01393v1",
        "abstract": "In this paper we show E-FuzzEdge, a novel fuzzing architecture targeted\ntowards improving the throughput of fuzzing campaigns in contexts where\nscalability is unavailable. E-FuzzEdge addresses the inefficiencies of\nhardware-in-the-loop fuzzing for microcontrollers by optimizing execution\nspeed. We evaluated our system against state-of-the-art benchmarks,\ndemonstrating significant performance improvements. A key advantage of\nE-FuzzEdgearchitecture is its compatibility with other embedded fuzzing\ntechniques that perform on device testing instead of firmware emulation. This\nmeans that the broader embedded fuzzing community can integrate E-FuzzEdge into\ntheir workflows to enhance overall testing efficiency."
    },
    {
        "date": "2025-10",
        "title": "Breaking the Code: Security Assessment of AI Code Agents Through Systematic Jailbreaking Attacks",
        "author": "Shoumik Saha, Jifan Chen, Sam Mayers, Sanjay Krishna Gouda, Zijian Wang, and Varun Kumar",
        "link": "http://arxiv.org/abs/2510.01359v1",
        "abstract": "Code-capable large language model (LLM) agents are increasingly embedded into\nsoftware engineering workflows where they can read, write, and execute code,\nraising the stakes of safety-bypass (\"jailbreak\") attacks beyond text-only\nsettings. Prior evaluations emphasize refusal or harmful-text detection,\nleaving open whether agents actually compile and run malicious programs. We\npresent JAWS-BENCH (Jailbreaks Across WorkSpaces), a benchmark spanning three\nescalating workspace regimes that mirror attacker capability: empty (JAWS-0),\nsingle-file (JAWS-1), and multi-file (JAWS-M). We pair this with a\nhierarchical, executable-aware Judge Framework that tests (i) compliance, (ii)\nattack success, (iii) syntactic correctness, and (iv) runtime executability,\nmoving beyond refusal to measure deployable harm. Using seven LLMs from five\nfamilies as backends, we find that under prompt-only conditions in JAWS-0, code\nagents accept 61% of attacks on average; 58% are harmful, 52% parse, and 27%\nrun end-to-end. Moving to single-file regime in JAWS-1 drives compliance to ~\n100% for capable models and yields a mean ASR (Attack Success Rate) ~ 71%; the\nmulti-file regime (JAWS-M) raises mean ASR to ~ 75%, with 32% instantly\ndeployable attack code. Across models, wrapping an LLM in an agent\nsubstantially increases vulnerability -- ASR raises by 1.6x -- because initial\nrefusals are frequently overturned during later planning/tool-use steps.\nCategory-level analyses identify which attack classes are most vulnerable and\nmost readily deployable, while others exhibit large execution gaps. These\nfindings motivate execution-aware defenses, code-contextual safety filters, and\nmechanisms that preserve refusal decisions throughout the agent's multi-step\nreasoning and tool use."
    },
    {
        "date": "2025-10",
        "title": "Integrated Security Mechanisms for Weight Protection in Memristive Crossbar Arrays",
        "author": "Muhammad Faheemur Rahman, and Wayne Burleson",
        "link": "http://arxiv.org/abs/2510.01350v1",
        "abstract": "Memristive crossbar arrays enable in-memory computing by performing parallel\nanalog computations directly within memory, making them well-suited for machine\nlearning, neural networks, and neuromorphic systems. However, despite their\nadvantages, non-volatile memristors are vulnerable to security threats (such as\nadversarial extraction of stored weights when the hardware is compromised.\nProtecting these weights is essential since they represent valuable\nintellectual property resulting from lengthy and costly training processes\nusing large, often proprietary, datasets. As a solution we propose two security\nmechanisms: Keyed Permutor and Watermark Protection Columns; where both\nsafeguard critical weights and establish verifiable ownership (even in cases of\ndata leakage). Our approach integrates efficiently with existing memristive\ncrossbar architectures without significant design modifications. Simulations\nacross 45nm, 22nm, and 7nm CMOS nodes, using a realistic interconnect model and\na large RF dataset, show that both mechanisms offer robust protection with\nunder 10% overhead in area, delay and power. We also present initial\nexperiments employing the widely known MNIST dataset; further highlighting the\nfeasibility of securing memristive in-memory computing systems with minimal\nperformance trade-offs."
    },
    {
        "date": "2025-10",
        "title": "How Does the Pretraining Distribution Shape In-Context Learning? Task Selection, Generalization, and Robustness",
        "author": "Wa\u00efss Azizian, and Ali Hasan",
        "link": "http://arxiv.org/abs/2510.01163v1",
        "abstract": "The emergence of in-context learning (ICL) in large language models (LLMs)\nremains poorly understood despite its consistent effectiveness, enabling models\nto adapt to new tasks from only a handful of examples. To clarify and improve\nthese capabilities, we characterize how the statistical properties of the\npretraining distribution (e.g., tail behavior, coverage) shape ICL on numerical\ntasks. We develop a theoretical framework that unifies task selection and\ngeneralization, extending and sharpening earlier results, and show how\ndistributional properties govern sample efficiency, task retrieval, and\nrobustness. To this end, we generalize Bayesian posterior consistency and\nconcentration results to heavy-tailed priors and dependent sequences, better\nreflecting the structure of LLM pretraining data. We then empirically study how\nICL performance varies with the pretraining distribution on challenging tasks\nsuch as stochastic differential equations and stochastic processes with memory.\nTogether, these findings suggest that controlling key statistical properties of\nthe pretraining distribution is essential for building ICL-capable and reliable\nLLMs."
    },
    {
        "date": "2025-10",
        "title": "Multi-Marginal Flow Matching with Adversarially Learnt Interpolants",
        "author": "Oskar Kviman, Kirill Tamogashev, Nicola Branchini, V\u00edctor Elvira, Jens Lagergren, and Nikolay Malkin",
        "link": "http://arxiv.org/abs/2510.01159v1",
        "abstract": "Learning the dynamics of a process given sampled observations at several time\npoints is an important but difficult task in many scientific applications. When\nno ground-truth trajectories are available, but one has only snapshots of data\ntaken at discrete time steps, the problem of modelling the dynamics, and thus\ninferring the underlying trajectories, can be solved by multi-marginal\ngeneralisations of flow matching algorithms. This paper proposes a novel flow\nmatching method that overcomes the limitations of existing multi-marginal\ntrajectory inference algorithms. Our proposed method, ALI-CFM, uses a\nGAN-inspired adversarial loss to fit neurally parametrised interpolant curves\nbetween source and target points such that the marginal distributions at\nintermediate time points are close to the observed distributions. The resulting\ninterpolants are smooth trajectories that, as we show, are unique under mild\nassumptions. These interpolants are subsequently marginalised by a flow\nmatching algorithm, yielding a trained vector field for the underlying\ndynamics. We showcase the versatility and scalability of our method by\noutperforming the existing baselines on spatial transcriptomics and cell\ntracking datasets, while performing on par with them on single-cell trajectory\nprediction.\n  Code: https://github.com/mmacosha/adversarially-learned-interpolants."
    },
    {
        "date": "2025-10",
        "title": "Backdoor Attacks Against Speech Language Models",
        "author": "Alexandrine Fortier, Thomas Thebaud, Jes\u00fas Villalba, Najim Dehak, and Patrick Cardinal",
        "link": "http://arxiv.org/abs/2510.01157v1",
        "abstract": "Large Language Models (LLMs) and their multimodal extensions are becoming\nincreasingly popular. One common approach to enable multimodality is to cascade\ndomain-specific encoders with an LLM, making the resulting model inherit\nvulnerabilities from all of its components. In this work, we present the first\nsystematic study of audio backdoor attacks against speech language models. We\ndemonstrate its effectiveness across four speech encoders and three datasets,\ncovering four tasks: automatic speech recognition (ASR), speech emotion\nrecognition, and gender and age prediction. The attack consistently achieves\nhigh success rates, ranging from 90.76% to 99.41%. To better understand how\nbackdoors propagate, we conduct a component-wise analysis to identify the most\nvulnerable stages of the pipeline. Finally, we propose a fine-tuning-based\ndefense that mitigates the threat of poisoned pretrained encoders."
    },
    {
        "date": "2025-10",
        "title": "Safety Instincts: LLMs Learn to Trust Their Internal Compass for Self-Defense",
        "author": "Guobin Shen, Dongcheng Zhao, Haibo Tong, Jindong Li, Feifei Zhao, and Yi Zeng",
        "link": "http://arxiv.org/abs/2510.01088v1",
        "abstract": "Ensuring Large Language Model (LLM) safety remains challenging due to the\nabsence of universal standards and reliable content validators, making it\ndifficult to obtain effective training signals. We discover that aligned models\nalready possess robust internal safety beliefs: they consistently produce\nhigh-confidence refusals to harmful requests while exhibiting high entropy when\ngenerating potentially dangerous content. This entropy gap reveals an untapped\nsignal--models intrinsically \"know\" when to refuse. We introduce Safety\nInstincts Reinforcement Learning (SIRL), which transforms this internal\nconfidence into a self-generated reward signal, eliminating dependence on\nexternal validators or human annotations. SIRL teaches models to trust their\nsafety instincts by reinforcing low-entropy refusal behaviors. Evaluated on\nLlama and Qwen models, SIRL maintains 89%+ Defense Success Rates (DSRs) against\n20+ jailbreak methods, from static prompts to adaptive attacks. Using only\n15,000 unlabeled prompts, SIRL surpasses resource-intensive supervised methods\nwhile preserving performance on mathematics, coding, and conversation\nbenchmarks. Our work demonstrates that effective alignment can emerge from\nwithin, paving the way for more autonomous and robust AI safety mechanisms that\nscale without extensive human oversight."
    },
    {
        "date": "2025-10",
        "title": "Activation-Deactivation: A General Framework for Robust Post-hoc Explainable AI",
        "author": "Akchunya Chanchal, David A. Kelly, and Hana Chockler",
        "link": "http://arxiv.org/abs/2510.01038v1",
        "abstract": "Black-box explainability methods are popular tools for explaining the\ndecisions of image classifiers. A major drawback of these tools is their\nreliance on mutants obtained by occluding parts of the input, leading to\nout-of-distribution images. This raises doubts about the quality of the\nexplanations. Moreover, choosing an appropriate occlusion value often requires\ndomain knowledge. In this paper we introduce a novel forward-pass paradigm\nActivation-Deactivation (AD), which removes the effects of occluded input\nfeatures from the model's decision-making by switching off the parts of the\nmodel that correspond to the occlusions. We introduce ConvAD, a drop-in\nmechanism that can be easily added to any trained Convolutional Neural Network\n(CNN), and which implements the AD paradigm. This leads to more robust\nexplanations without any additional training or fine-tuning. We prove that the\nConvAD mechanism does not change the decision-making process of the network. We\nprovide experimental evaluation across several datasets and model\narchitectures. We compare the quality of AD-explanations with explanations\nachieved using a set of masking values, using the proxies of robustness, size,\nand confidence drop-off. We observe a consistent improvement in robustness of\nAD explanations (up to 62.5%) compared to explanations obtained with\nocclusions, demonstrating that ConvAD extracts more robust explanations without\nthe need for domain knowledge."
    },
    {
        "date": "2025-10",
        "title": "Secure and reversible face anonymization with diffusion models",
        "author": "Pol Labarbarie, Vincent Itier, and William Puech",
        "link": "http://arxiv.org/abs/2510.01031v1",
        "abstract": "Face images processed by computer vision algorithms contain sensitive\npersonal information that malicious actors can capture without consent. These\nprivacy and security risks highlight the need for effective face anonymization\nmethods. Current methods struggle to propose a good trade-off between a secure\nscheme with high-quality image generation and reversibility for later person\nauthentication. Diffusion-based approaches produce high-quality anonymized\nimages but lack the secret key mechanism to ensure that only authorized parties\ncan reverse the process. In this paper, we introduce, to our knowledge, the\nfirst secure, high-quality reversible anonymization method based on a diffusion\nmodel. We propose to combine the secret key with the latent faces\nrepresentation of the diffusion model. To preserve identity-irrelevant\nfeatures, generation is constrained by a facial mask, maintaining high-quality\nimages. By using a deterministic forward and backward diffusion process, our\napproach enforces that the original face can be recovered with the correct\nsecret key. We also show that the proposed method produces anonymized faces\nthat are less visually similar to the original faces, compared to other\nprevious work."
    },
    {
        "date": "2025-10",
        "title": "Towards Adversarial Training under Hyperspectral Images",
        "author": "Weihua Zhang, Chengze Jiang, Jie Gui, and Lu Dong",
        "link": "http://arxiv.org/abs/2510.01014v1",
        "abstract": "Recent studies have revealed that hyperspectral classification models based\non deep learning are highly vulnerable to adversarial attacks, which pose\nsignificant security risks. Although several approaches have attempted to\nenhance adversarial robustness by modifying network architectures, these\nmethods often rely on customized designs that limit scalability and fail to\ndefend effectively against strong attacks. To address these challenges, we\nintroduce adversarial training to the hyperspectral domain, which is widely\nregarded as one of the most effective defenses against adversarial attacks.\nThrough extensive empirical analyses, we demonstrate that while adversarial\ntraining does enhance robustness across various models and datasets,\nhyperspectral data introduces unique challenges not seen in RGB images.\nSpecifically, we find that adversarial noise and the non-smooth nature of\nadversarial examples can distort or eliminate important spectral semantic\ninformation. To mitigate this issue, we employ data augmentation techniques and\npropose a novel hyperspectral adversarial training method, termed AT-RA. By\nincreasing the diversity of spectral information and ensuring spatial\nsmoothness, AT-RA preserves and corrects spectral semantics in hyperspectral\nimages. Experimental results show that AT-RA improves adversarial robustness by\n21.34% against AutoAttack and 18.78% against PGD-50 while boosting benign\naccuracy by 2.68%."
    },
    {
        "date": "2025-10",
        "title": "Adaptive Federated Few-Shot Rare-Disease Diagnosis with Energy-Aware Secure Aggregation",
        "author": "Aueaphum Aueawatthanaphisut",
        "link": "http://arxiv.org/abs/2510.00976v1",
        "abstract": "Rare-disease diagnosis remains one of the most pressing challenges in digital\nhealth, hindered by extreme data scarcity, privacy concerns, and the limited\nresources of edge devices. This paper proposes the Adaptive Federated Few-Shot\nRare-Disease Diagnosis (AFFR) framework, which integrates three pillars: (i)\nfew-shot federated optimization with meta-learning to generalize from limited\npatient samples, (ii) energy-aware client scheduling to mitigate device\ndropouts and ensure balanced participation, and (iii) secure aggregation with\ncalibrated differential privacy to safeguard sensitive model updates. Unlike\nprior work that addresses these aspects in isolation, AFFR unifies them into a\nmodular pipeline deployable on real-world clinical networks. Experimental\nevaluation on simulated rare-disease detection datasets demonstrates up to 10%\nimprovement in accuracy compared with baseline FL, while reducing client\ndropouts by over 50% without degrading convergence. Furthermore,\nprivacy-utility trade-offs remain within clinically acceptable bounds. These\nfindings highlight AFFR as a practical pathway for equitable and trustworthy\nfederated diagnosis of rare conditions."
    },
    {
        "date": "2025-10",
        "title": "On Discovering Algorithms for Adversarial Imitation Learning",
        "author": "Shashank Reddy Chirra, Jayden Teoh, Praveen Paruchuri, and Pradeep Varakantham",
        "link": "http://arxiv.org/abs/2510.00922v1",
        "abstract": "Adversarial Imitation Learning (AIL) methods, while effective in settings\nwith limited expert demonstrations, are often considered unstable. These\napproaches typically decompose into two components: Density Ratio (DR)\nestimation $\\frac{\\rho_E}{\\rho_{\\pi}}$, where a discriminator estimates the\nrelative occupancy of state-action pairs under the policy versus the expert;\nand Reward Assignment (RA), where this ratio is transformed into a reward\nsignal used to train the policy. While significant research has focused on\nimproving density estimation, the role of reward assignment in influencing\ntraining dynamics and final policy performance has been largely overlooked. RA\nfunctions in AIL are typically derived from divergence minimization objectives,\nrelying heavily on human design and ingenuity. In this work, we take a\ndifferent approach: we investigate the discovery of data-driven RA functions,\ni.e, based directly on the performance of the resulting imitation policy. To\nthis end, we leverage an LLM-guided evolutionary framework that efficiently\nexplores the space of RA functions, yielding \\emph{Discovered Adversarial\nImitation Learning} (DAIL), the first meta-learnt AIL algorithm. Remarkably,\nDAIL generalises across unseen environments and policy optimization algorithms,\noutperforming the current state-of-the-art of \\emph{human-designed} baselines.\nFinally, we analyse why DAIL leads to more stable training, offering novel\ninsights into the role of RA functions in the stability of AIL. Code is\npublicly available: https://github.com/shshnkreddy/DAIL."
    },
    {
        "date": "2025-10",
        "title": "NSARM: Next-Scale Autoregressive Modeling for Robust Real-World Image Super-Resolution",
        "author": "Xiangtao Kong, Rongyuan Wu, Shuaizheng Liu, Lingchen Sun, and Lei Zhang",
        "link": "http://arxiv.org/abs/2510.00820v1",
        "abstract": "Most recent real-world image super-resolution (Real-ISR) methods employ\npre-trained text-to-image (T2I) diffusion models to synthesize the high-quality\nimage either from random Gaussian noise, which yields realistic results but is\nslow due to iterative denoising, or directly from the input low-quality image,\nwhich is efficient but at the price of lower output quality. These approaches\ntrain ControlNet or LoRA modules while keeping the pre-trained model fixed,\nwhich often introduces over-enhanced artifacts and hallucinations, suffering\nfrom the robustness to inputs of varying degradations. Recent visual\nautoregressive (AR) models, such as pre-trained Infinity, can provide strong\nT2I generation capabilities while offering superior efficiency by using the\nbitwise next-scale prediction strategy. Building upon next-scale prediction, we\nintroduce a robust Real-ISR framework, namely Next-Scale Autoregressive\nModeling (NSARM). Specifically, we train NSARM in two stages: a transformation\nnetwork is first trained to map the input low-quality image to preliminary\nscales, followed by an end-to-end full-model fine-tuning. Such a comprehensive\nfine-tuning enhances the robustness of NSARM in Real-ISR tasks without\ncompromising its generative capability. Extensive quantitative and qualitative\nevaluations demonstrate that as a pure AR model, NSARM achieves superior visual\nresults over existing Real-ISR methods while maintaining a fast inference\nspeed. Most importantly, it demonstrates much higher robustness to the quality\nof input images, showing stronger generalization performance. Project page:\nhttps://github.com/Xiangtaokong/NSARM"
    },
    {
        "date": "2025-10",
        "title": "Fast, Secure, and High-Capacity Image Watermarking with Autoencoded Text Vectors",
        "author": "Gautier Evennou, Vivien Chappelier, and Ewa Kijak",
        "link": "http://arxiv.org/abs/2510.00799v1",
        "abstract": "Most image watermarking systems focus on robustness, capacity, and\nimperceptibility while treating the embedded payload as meaningless bits. This\nbit-centric view imposes a hard ceiling on capacity and prevents watermarks\nfrom carrying useful information. We propose LatentSeal, which reframes\nwatermarking as semantic communication: a lightweight text autoencoder maps\nfull-sentence messages into a compact 256-dimensional unit-norm latent vector,\nwhich is robustly embedded by a finetuned watermark model and secured through a\nsecret, invertible rotation. The resulting system hides full-sentence messages,\ndecodes in real time, and survives valuemetric and geometric attacks. It\nsurpasses prior state of the art in BLEU-4 and Exact Match on several\nbenchmarks, while breaking through the long-standing 256-bit payload ceiling.\nIt also introduces a statistically calibrated score that yields a ROC AUC score\nof 0.97-0.99, and practical operating points for deployment. By shifting from\nbit payloads to semantic latent vectors, LatentSeal enables watermarking that\nis not only robust and high-capacity, but also secure and interpretable,\nproviding a concrete path toward provenance, tamper explanation, and\ntrustworthy AI governance. Models, training and inference code, and data splits\nwill be available upon publication."
    },
    {
        "date": "2025-10",
        "title": "MetaLogic: Robustness Evaluation of Text-to-Image Models via Logically Equivalent Prompts",
        "author": "Yifan Shen, Yangyang Shu, Hye-young Paik, and Yulei Sui",
        "link": "http://arxiv.org/abs/2510.00796v1",
        "abstract": "Recent advances in text-to-image (T2I) models, especially diffusion-based\narchitectures, have significantly improved the visual quality of generated\nimages. However, these models continue to struggle with a critical limitation:\nmaintaining semantic consistency when input prompts undergo minor linguistic\nvariations. Despite being logically equivalent, such prompt pairs often yield\nmisaligned or semantically inconsistent images, exposing a lack of robustness\nin reasoning and generalisation. To address this, we propose MetaLogic, a novel\nevaluation framework that detects T2I misalignment without relying on ground\ntruth images. MetaLogic leverages metamorphic testing, generating image pairs\nfrom prompts that differ grammatically but are semantically identical. By\ndirectly comparing these image pairs, the framework identifies inconsistencies\nthat signal failures in preserving the intended meaning, effectively diagnosing\nrobustness issues in the model's logic understanding. Unlike existing\nevaluation methods that compare a generated image to a single prompt, MetaLogic\nevaluates semantic equivalence between paired images, offering a scalable,\nground-truth-free approach to identifying alignment failures. It categorises\nthese alignment errors (e.g., entity omission, duplication, positional\nmisalignment) and surfaces counterexamples that can be used for model debugging\nand refinement. We evaluate MetaLogic across multiple state-of-the-art T2I\nmodels and reveal consistent robustness failures across a range of logical\nconstructs. We find that even the SOTA text-to-image models like Flux.dev and\nDALLE-3 demonstrate a 59 percent and 71 percent misalignment rate,\nrespectively. Our results show that MetaLogic is not only efficient and\nscalable, but also effective in uncovering fine-grained logical inconsistencies\nthat are overlooked by existing evaluation metrics."
    },
    {
        "date": "2025-10",
        "title": "DIA: The Adversarial Exposure of Deterministic Inversion in Diffusion Models",
        "author": "Seunghoo Hong, Geonho Son, Juhun Lee, and Simon S. Woo",
        "link": "http://arxiv.org/abs/2510.00778v1",
        "abstract": "Diffusion models have shown to be strong representation learners, showcasing\nstate-of-the-art performance across multiple domains. Aside from accelerated\nsampling, DDIM also enables the inversion of real images back to their latent\ncodes. A direct inheriting application of this inversion operation is real\nimage editing, where the inversion yields latent trajectories to be utilized\nduring the synthesis of the edited image. Unfortunately, this practical tool\nhas enabled malicious users to freely synthesize misinformative or deepfake\ncontents with greater ease, which promotes the spread of unethical and abusive,\nas well as privacy-, and copyright-infringing contents. While defensive\nalgorithms such as AdvDM and Photoguard have been shown to disrupt the\ndiffusion process on these images, the misalignment between their objectives\nand the iterative denoising trajectory at test time results in weak disruptive\nperformance.In this work, we present the DDIM Inversion Attack (DIA) that\nattacks the integrated DDIM trajectory path. Our results support the effective\ndisruption, surpassing previous defensive methods across various editing\nmethods. We believe that our frameworks and results can provide practical\ndefense methods against the malicious use of AI for both the industry and the\nresearch community. Our code is available here:\nhttps://anonymous.4open.science/r/DIA-13419/."
    },
    {
        "date": "2025-10",
        "title": "ZQBA: Zero Query Black-box Adversarial Attack",
        "author": "Joana C. Costa, Tiago Roxo, Hugo Proen\u00e7a, and Pedro R. M. In\u00e1cio",
        "link": "http://arxiv.org/abs/2510.00769v1",
        "abstract": "Current black-box adversarial attacks either require multiple queries or\ndiffusion models to produce adversarial samples that can impair the target\nmodel performance. However, these methods require training a surrogate loss or\ndiffusion models to produce adversarial samples, which limits their\napplicability in real-world settings. Thus, we propose a Zero Query Black-box\nAdversarial (ZQBA) attack that exploits the representations of Deep Neural\nNetworks (DNNs) to fool other networks. Instead of requiring thousands of\nqueries to produce deceiving adversarial samples, we use the feature maps\nobtained from a DNN and add them to clean images to impair the classification\nof a target model. The results suggest that ZQBA can transfer the adversarial\nsamples to different models and across various datasets, namely CIFAR and Tiny\nImageNet. The experiments also show that ZQBA is more effective than\nstate-of-the-art black-box attacks with a single query, while maintaining the\nimperceptibility of perturbations, evaluated both quantitatively (SSIM) and\nqualitatively, emphasizing the vulnerabilities of employing DNNs in real-world\ncontexts. All the source code is available at\nhttps://github.com/Joana-Cabral/ZQBA."
    },
    {
        "date": "2025-10",
        "title": "Downgrade to Upgrade: Optimizer Simplification Enhances Robustness in LLM Unlearning",
        "author": "Yicheng Lang, Yihua Zhang, Chongyu Fan, Changsheng Wang, Jinghan Jia, and Sijia Liu",
        "link": "http://arxiv.org/abs/2510.00761v2",
        "abstract": "Large language model (LLM) unlearning aims to surgically remove the influence\nof undesired data or knowledge from an existing model while preserving its\nutility on unrelated tasks. This paradigm has shown promise in addressing\nprivacy and safety concerns. However, recent findings reveal that unlearning\neffects are often fragile: post-unlearning manipulations such as weight\nquantization or fine-tuning can quickly neutralize the intended forgetting.\nPrior efforts to improve robustness primarily reformulate unlearning objectives\nby explicitly assuming the role of vulnerability sources. In this work, we take\na different perspective by investigating the role of the optimizer, independent\nof unlearning objectives and formulations, in shaping unlearning robustness. We\nshow that the 'grade' of the optimizer, defined by the level of information it\nexploits, ranging from zeroth-order (gradient-free) to first-order\n(gradient-based) to second-order (Hessian-based), is tightly linked to the\nresilience of unlearning. Surprisingly, we find that downgrading the optimizer,\nsuch as using zeroth-order methods or compressed-gradient variants (e.g.,\ngradient sign-based optimizers), often leads to stronger robustness. While\nthese optimizers produce noisier and less precise updates, they encourage\nconvergence to harder-to-disturb basins in the loss landscape, thereby\nresisting post-training perturbations. By connecting zeroth-order methods with\nrandomized smoothing, we further highlight their natural advantage for robust\nunlearning. Motivated by these insights, we propose a hybrid optimizer that\ncombines first-order and zeroth-order updates, preserving unlearning efficacy\nwhile enhancing robustness. Extensive experiments on the MUSE and WMDP\nbenchmarks, across multiple LLM unlearning algorithms, validate that our\napproach achieves more resilient forgetting without sacrificing unlearning\nquality."
    },
    {
        "date": "2025-10",
        "title": "Erased, But Not Forgotten: Erased Rectified Flow Transformers Still Remain Unsafe Under Concept Attack",
        "author": "Nanxiang Jiang, Zhaoxin Fan, Enhan Kang, Daiheng Gao, Yun Zhou, Yanxia Chang, Zheng Zhu, Yeying Jin, and Wenjun Wu",
        "link": "http://arxiv.org/abs/2510.00635v1",
        "abstract": "Recent advances in text-to-image (T2I) diffusion models have enabled\nimpressive generative capabilities, but they also raise significant safety\nconcerns due to the potential to produce harmful or undesirable content. While\nconcept erasure has been explored as a mitigation strategy, most existing\napproaches and corresponding attack evaluations are tailored to Stable\nDiffusion (SD) and exhibit limited effectiveness when transferred to\nnext-generation rectified flow transformers such as Flux. In this work, we\npresent ReFlux, the first concept attack method specifically designed to assess\nthe robustness of concept erasure in the latest rectified flow-based T2I\nframework. Our approach is motivated by the observation that existing concept\nerasure techniques, when applied to Flux, fundamentally rely on a phenomenon\nknown as attention localization. Building on this insight, we propose a simple\nyet effective attack strategy that specifically targets this property. At its\ncore, a reverse-attention optimization strategy is introduced to effectively\nreactivate suppressed signals while stabilizing attention. This is further\nreinforced by a velocity-guided dynamic that enhances the robustness of concept\nreactivation by steering the flow matching process, and a\nconsistency-preserving objective that maintains the global layout and preserves\nunrelated content. Extensive experiments consistently demonstrate the\neffectiveness and efficiency of the proposed attack method, establishing a\nreliable benchmark for evaluating the robustness of concept erasure strategies\nin rectified flow transformers."
    },
    {
        "date": "2025-10",
        "title": "Robust Context-Aware Object Recognition",
        "author": "Klara Janouskova, Cristian Gavrus, and Jiri Matas",
        "link": "http://arxiv.org/abs/2510.00618v1",
        "abstract": "In visual recognition, both the object of interest (referred to as\nforeground, FG, for simplicity) and its surrounding context (background, BG)\nplay an important role. However, standard supervised learning often leads to\nunintended over-reliance on the BG, known as shortcut learning of spurious\ncorrelations, limiting model robustness in real-world deployment settings. In\nthe literature, the problem is mainly addressed by suppressing the BG,\nsacrificing context information for improved generalization.\n  We propose RCOR -- Robust Context-Aware Object Recognition -- the first\napproach that jointly achieves robustness and context-awareness without\ncompromising either. RCOR treats localization as an integral part of\nrecognition to decouple object-centric and context-aware modelling, followed by\na robust, non-parametric fusion. It improves the performance of both supervised\nmodels and VLM on datasets with both in-domain and out-of-domain BG, even\nwithout fine-tuning. The results confirm that localization before recognition\nis now possible even in complex scenes as in ImageNet-1k."
    },
    {
        "date": "2025-10",
        "title": "Designing Ambiguity Sets for Distributionally Robust Optimization Using Structural Causal Optimal Transport",
        "author": "Ahmad-Reza Ehyaei, Golnoosh Farnadi, and Samira Samadi",
        "link": "http://arxiv.org/abs/2510.00599v1",
        "abstract": "Distributionally robust optimization tackles out-of-sample issues like\noverfitting and distribution shifts by adopting an adversarial approach over a\nrange of possible data distributions, known as the ambiguity set. To balance\nconservatism and accuracy, these sets must include realistic probability\ndistributions by leveraging information from the nominal distribution. Assuming\nthat nominal distributions arise from a structural causal model with a directed\nacyclic graph $\\mathcal{G}$ and structural equations, previous methods such as\nadapted and $\\mathcal{G}$-causal optimal transport have only utilized causal\ngraph information in designing ambiguity sets. In this work, we propose\nincorporating structural equations, which include causal graph information, to\nenhance ambiguity sets, resulting in more realistic distributions. We introduce\nstructural causal optimal transport and its associated ambiguity set,\ndemonstrating their advantages and connections to previous methods. A key\nbenefit of our approach is a relaxed version, where a regularization term\nreplaces the complex causal constraints, enabling an efficient algorithm via\ndifference-of-convex programming to solve structural causal optimal transport.\nWe also show that when structural information is absent and must be estimated,\nour approach remains effective and provides finite sample guarantees. Lastly,\nwe address the radius of ambiguity sets, illustrating how our method overcomes\nthe curse of dimensionality in optimal transport problems, achieving faster\nshrinkage with dimension-free order."
    },
    {
        "date": "2025-10",
        "title": "Private Online Learning against an Adaptive Adversary: Realizable and Agnostic Settings",
        "author": "Bo Li, Wei Wang, and Peng Ye",
        "link": "http://arxiv.org/abs/2510.00574v1",
        "abstract": "We revisit the problem of private online learning, in which a learner\nreceives a sequence of $T$ data points and has to respond at each time-step a\nhypothesis. It is required that the entire stream of output hypotheses should\nsatisfy differential privacy. Prior work of Golowich and Livni [2021]\nestablished that every concept class $\\mathcal{H}$ with finite Littlestone\ndimension $d$ is privately online learnable in the realizable setting. In\nparticular, they proposed an algorithm that achieves an $O_{d}(\\log T)$ mistake\nbound against an oblivious adversary. However, their approach yields a\nsuboptimal $\\tilde{O}_{d}(\\sqrt{T})$ bound against an adaptive adversary. In\nthis work, we present a new algorithm with a mistake bound of $O_{d}(\\log T)$\nagainst an adaptive adversary, closing this gap. We further investigate the\nproblem in the agnostic setting, which is more general than the realizable\nsetting as it does not impose any assumptions on the data. We give an algorithm\nthat obtains a sublinear regret of $\\tilde{O}_d(\\sqrt{T})$ for generic\nLittlestone classes, demonstrating that they are also privately online\nlearnable in the agnostic setting."
    },
    {
        "date": "2025-10",
        "title": "Memory-Augmented Log Analysis with Phi-4-mini: Enhancing Threat Detection in Structured Security Logs",
        "author": "Anbi Guo, and Mahfuza Farooque",
        "link": "http://arxiv.org/abs/2510.00529v1",
        "abstract": "Structured security logs are critical for detecting advanced persistent\nthreats (APTs). Large language models (LLMs) struggle in this domain due to\nlimited context and domain mismatch. We propose \\textbf{DM-RAG}, a dual-memory\nretrieval-augmented generation framework for structured log analysis. It\nintegrates a short-term memory buffer for recent summaries and a long-term\nFAISS-indexed memory for historical patterns. An instruction-tuned Phi-4-mini\nprocesses the combined context and outputs structured predictions. Bayesian\nfusion promotes reliable persistence into memory. On the UNSW-NB15 dataset,\nDM-RAG achieves 53.64% accuracy and 98.70% recall, surpassing fine-tuned and\nRAG baselines in recall. The architecture is lightweight, interpretable, and\nscalable, enabling real-time threat monitoring without extra corpora or heavy\ntuning."
    },
    {
        "date": "2025-10",
        "title": "Understanding Sensitivity of Differential Attention through the Lens of Adversarial Robustness",
        "author": "Tsubasa Takahashi, Shojiro Yamabe, Futa Waseda, and Kento Sasaki",
        "link": "http://arxiv.org/abs/2510.00517v1",
        "abstract": "Differential Attention (DA) has been proposed as a refinement to standard\nattention, suppressing redundant or noisy context through a subtractive\nstructure and thereby reducing contextual hallucination. While this design\nsharpens task-relevant focus, we show that it also introduces a structural\nfragility under adversarial perturbations. Our theoretical analysis identifies\nnegative gradient alignment-a configuration encouraged by DA's subtraction-as\nthe key driver of sensitivity amplification, leading to increased gradient\nnorms and elevated local Lipschitz constants. We empirically validate this\nFragile Principle through systematic experiments on ViT/DiffViT and evaluations\nof pretrained CLIP/DiffCLIP, spanning five datasets in total. These results\ndemonstrate higher attack success rates, frequent gradient opposition, and\nstronger local sensitivity compared to standard attention. Furthermore,\ndepth-dependent experiments reveal a robustness crossover: stacking DA layers\nattenuates small perturbations via depth-dependent noise cancellation, though\nthis protection fades under larger attack budgets. Overall, our findings\nuncover a fundamental trade-off: DA improves discriminative focus on clean\ninputs but increases adversarial vulnerability, underscoring the need to\njointly design for selectivity and robustness in future attention mechanisms."
    },
    {
        "date": "2025-10",
        "title": "On the Adversarial Robustness of Learning-based Conformal Novelty Detection",
        "author": "Daofu Zhang, Mehrdad Pournaderi, Hanne M. Clifford, Yu Xiang, and Pramod K. Varshney",
        "link": "http://arxiv.org/abs/2510.00463v1",
        "abstract": "This paper studies the adversarial robustness of conformal novelty detection.\nIn particular, we focus on AdaDetect, a powerful learning-based framework for\nnovelty detection with finite-sample false discovery rate (FDR) control. While\nAdaDetect provides rigorous statistical guarantees under benign conditions, its\nbehavior under adversarial perturbations remains unexplored. We first formulate\nan oracle attack setting that quantifies the worst-case degradation of FDR,\nderiving an upper bound that characterizes the statistical cost of attacks.\nThis idealized formulation directly motivates a practical and effective attack\nscheme that only requires query access to AdaDetect's output labels. Coupling\nthese formulations with two popular and complementary black-box adversarial\nalgorithms, we systematically evaluate the vulnerability of AdaDetect on\nsynthetic and real-world datasets. Our results show that adversarial\nperturbations can significantly increase the FDR while maintaining high\ndetection power, exposing fundamental limitations of current error-controlled\nnovelty detection methods and motivating the development of more robust\nalternatives."
    },
    {
        "date": "2025-10",
        "title": "Robust Spatiotemporally Contiguous Anomaly Detection Using Tensor Decomposition",
        "author": "Rachita Mondal, Mert Indibi, Tapabrata Maiti, and Selin Aviyente",
        "link": "http://arxiv.org/abs/2510.00460v1",
        "abstract": "Anomaly detection in spatiotemporal data is a challenging problem encountered\nin a variety of applications, including video surveillance, medical imaging\ndata, and urban traffic monitoring. Existing anomaly detection methods focus\nmainly on point anomalies and cannot deal with temporal and spatial\ndependencies that arise in spatio-temporal data. Tensor-based anomaly detection\nmethods have been proposed to address this problem. Although existing methods\ncan capture dependencies across different modes, they are primarily supervised\nand do not account for the specific structure of anomalies. Moreover, these\nmethods focus mainly on extracting anomalous features without providing any\nstatistical confidence. In this paper, we introduce an unsupervised\ntensor-based anomaly detection method that simultaneously considers the sparse\nand spatiotemporally smooth nature of anomalies. The anomaly detection problem\nis formulated as a regularized robust low-rank + sparse tensor decomposition\nwhere the total variation of the tensor with respect to the underlying spatial\nand temporal graphs quantifies the spatiotemporal smoothness of the anomalies.\nOnce the anomalous features are extracted, we introduce a statistical anomaly\nscoring framework that accounts for local spatio-temporal dependencies. The\nproposed framework is evaluated on both synthetic and real data."
    },
    {
        "date": "2025-10",
        "title": "A Call to Action for a Secure-by-Design Generative AI Paradigm",
        "author": "Dalal Alharthi, and Ivan Roberto Kawaminami Garcia",
        "link": "http://arxiv.org/abs/2510.00451v1",
        "abstract": "Large language models have gained widespread prominence, yet their\nvulnerability to prompt injection and other adversarial attacks remains a\ncritical concern. This paper argues for a security-by-design AI paradigm that\nproactively mitigates LLM vulnerabilities while enhancing performance. To\nachieve this, we introduce PromptShield, an ontology-driven framework that\nensures deterministic and secure prompt interactions. It standardizes user\ninputs through semantic validation, eliminating ambiguity and mitigating\nadversarial manipulation. To assess PromptShield's security and performance\ncapabilities, we conducted an experiment on an agent-based system to analyze\ncloud logs within Amazon Web Services (AWS), containing 493 distinct events\nrelated to malicious activities and anomalies. By simulating prompt injection\nattacks and assessing the impact of deploying PromptShield, our results\ndemonstrate a significant improvement in model security and performance,\nachieving precision, recall, and F1 scores of approximately 94%. Notably, the\nontology-based framework not only mitigates adversarial threats but also\nenhances the overall performance and reliability of the system. Furthermore,\nPromptShield's modular and adaptable design ensures its applicability beyond\ncloud security, making it a robust solution for safeguarding generative AI\napplications across various domains. By laying the groundwork for AI safety\nstandards and informing future policy development, this work stimulates a\ncrucial dialogue on the pivotal role of deterministic prompt engineering and\nontology-based validation in ensuring the safe and responsible deployment of\nLLMs in high-stakes environments."
    },
    {
        "date": "2025-10",
        "title": "EgoTraj-Bench: Towards Robust Trajectory Prediction Under Ego-view Noisy Observations",
        "author": "Jiayi Liu, Jiaming Zhou, Ke Ye, Kun-Yu Lin, Allan Wang, and Junwei Liang",
        "link": "http://arxiv.org/abs/2510.00405v1",
        "abstract": "Reliable trajectory prediction from an ego-centric perspective is crucial for\nrobotic navigation in human-centric environments. However, existing methods\ntypically assume idealized observation histories, failing to account for the\nperceptual artifacts inherent in first-person vision, such as occlusions, ID\nswitches, and tracking drift. This discrepancy between training assumptions and\ndeployment reality severely limits model robustness. To bridge this gap, we\nintroduce EgoTraj-Bench, the first real-world benchmark that grounds noisy,\nfirst-person visual histories in clean, bird's-eye-view future trajectories,\nenabling robust learning under realistic perceptual constraints. Building on\nthis benchmark, we propose BiFlow, a dual-stream flow matching model that\nconcurrently denoises historical observations and forecasts future motion by\nleveraging a shared latent representation. To better model agent intent, BiFlow\nincorporates our EgoAnchor mechanism, which conditions the prediction decoder\non distilled historical features via feature modulation. Extensive experiments\nshow that BiFlow achieves state-of-the-art performance, reducing minADE and\nminFDE by 10-15% on average and demonstrating superior robustness. We\nanticipate that our benchmark and model will provide a critical foundation for\ndeveloping trajectory forecasting systems truly resilient to the challenges of\nreal-world, ego-centric perception."
    },
    {
        "date": "2025-09",
        "title": "DiSA-IQL: Offline Reinforcement Learning for Robust Soft Robot Control under Distribution Shifts",
        "author": "Linjin He, Xinda Qi, Dong Chen, Zhaojian Li, and Xiaobo Tan",
        "link": "http://arxiv.org/abs/2510.00358v1",
        "abstract": "Soft snake robots offer remarkable flexibility and adaptability in complex\nenvironments, yet their control remains challenging due to highly nonlinear\ndynamics. Existing model-based and bio-inspired controllers rely on simplified\nassumptions that limit performance. Deep reinforcement learning (DRL) has\nrecently emerged as a promising alternative, but online training is often\nimpractical because of costly and potentially damaging real-world interactions.\nOffline RL provides a safer option by leveraging pre-collected datasets, but it\nsuffers from distribution shift, which degrades generalization to unseen\nscenarios. To overcome this challenge, we propose DiSA-IQL\n(Distribution-Shift-Aware Implicit Q-Learning), an extension of IQL that\nincorporates robustness modulation by penalizing unreliable state-action pairs\nto mitigate distribution shift. We evaluate DiSA-IQL on goal-reaching tasks\nacross two settings: in-distribution and out-of-distribution evaluation.\nSimulation results show that DiSA-IQL consistently outperforms baseline models,\nincluding Behavior Cloning (BC), Conservative Q-Learning (CQL), and vanilla\nIQL, achieving higher success rates, smoother trajectories, and improved\nrobustness. The codes are open-sourced to support reproducibility and to\nfacilitate further research in offline RL for soft robot control."
    },
    {
        "date": "2025-09",
        "title": "Security and Privacy Analysis of Tile's Location Tracking Protocol",
        "author": "Akshaya Kumar, Anna Raymaker, and Michael Specter",
        "link": "http://arxiv.org/abs/2510.00350v1",
        "abstract": "We conduct the first comprehensive security analysis of Tile, the second most\npopular crowd-sourced location-tracking service behind Apple's AirTags. We\nidentify several exploitable vulnerabilities and design flaws, disproving many\nof the platform's claimed security and privacy guarantees: Tile's servers can\npersistently learn the location of all users and tags, unprivileged adversaries\ncan track users through Bluetooth advertisements emitted by Tile's devices, and\nTile's anti-theft mode is easily subverted.\n  Despite its wide deployment -- millions of users, devices, and purpose-built\nhardware tags -- Tile provides no formal description of its protocol or threat\nmodel. Worse, Tile intentionally weakens its antistalking features to support\nan antitheft use-case and relies on a novel \"accountability\" mechanism to\npunish those abusing the system to stalk victims.\n  We examine Tile's accountability mechanism, a unique feature of independent\ninterest; no other provider attempts to guarantee accountability. While an\nideal accountability mechanism may disincentivize abuse in crowd-sourced\nlocation tracking protocols, we show that Tile's implementation is subvertible\nand introduces new exploitable vulnerabilities. We conclude with a discussion\non the need for new, formal definitions of accountability in this setting."
    },
    {
        "date": "2025-09",
        "title": "When Hallucination Costs Millions: Benchmarking AI Agents in High-Stakes Adversarial Financial Markets",
        "author": "Zeshi Dai, Zimo Peng, Zerui Cheng, and Ryan Yihe Li",
        "link": "http://arxiv.org/abs/2510.00332v1",
        "abstract": "We present CAIA, a benchmark exposing a critical blind spot in AI evaluation:\nthe inability of state-of-the-art models to operate in adversarial, high-stakes\nenvironments where misinformation is weaponized and errors are irreversible.\nWhile existing benchmarks measure task completion in controlled settings,\nreal-world deployment demands resilience against active deception. Using crypto\nmarkets as a testbed where $30 billion was lost to exploits in 2024, we\nevaluate 17 models on 178 time-anchored tasks requiring agents to distinguish\ntruth from manipulation, navigate fragmented information landscapes, and make\nirreversible financial decisions under adversarial pressure.\n  Our results reveal a fundamental capability gap: without tools, even frontier\nmodels achieve only 28% accuracy on tasks junior analysts routinely handle.\nTool augmentation improves performance but plateaus at 67.4% versus 80% human\nbaseline, despite unlimited access to professional resources. Most critically,\nwe uncover a systematic tool selection catastrophe: models preferentially\nchoose unreliable web search over authoritative data, falling for SEO-optimized\nmisinformation and social media manipulation. This behavior persists even when\ncorrect answers are directly accessible through specialized tools, suggesting\nfoundational limitations rather than knowledge gaps. We also find that Pass@k\nmetrics mask dangerous trial-and-error behavior for autonomous deployment.\n  The implications extend beyond crypto to any domain with active adversaries,\ne.g. cybersecurity, content moderation, etc. We release CAIA with contamination\ncontrols and continuous updates, establishing adversarial robustness as a\nnecessary condition for trustworthy AI autonomy. The benchmark reveals that\ncurrent models, despite impressive reasoning scores, remain fundamentally\nunprepared for environments where intelligence must survive active opposition."
    },
    {
        "date": "2025-09",
        "title": "Robust Federated Inference",
        "author": "Akash Dhasade, Sadegh Farhadkhani, Rachid Guerraoui, Nirupam Gupta, Maxime Jacovella, Anne-Marie Kermarrec, and Rafael Pinot",
        "link": "http://arxiv.org/abs/2510.00310v1",
        "abstract": "Federated inference, in the form of one-shot federated learning, edge\nensembles, or federated ensembles, has emerged as an attractive solution to\ncombine predictions from multiple models. This paradigm enables each model to\nremain local and proprietary while a central server queries them and aggregates\npredictions. Yet, the robustness of federated inference has been largely\nneglected, leaving them vulnerable to even simple attacks. To address this\ncritical gap, we formalize the problem of robust federated inference and\nprovide the first robustness analysis of this class of methods. Our analysis of\naveraging-based aggregators shows that the error of the aggregator is small\neither when the dissimilarity between honest responses is small or the margin\nbetween the two most probable classes is large. Moving beyond linear averaging,\nwe show that problem of robust federated inference with non-linear aggregators\ncan be cast as an adversarial machine learning problem. We then introduce an\nadvanced technique using the DeepSet aggregation model, proposing a novel\ncomposition of adversarial training and test-time robust aggregation to\nrobustify non-linear aggregators. Our composition yields significant\nimprovements, surpassing existing robust aggregation methods by 4.7 - 22.2% in\naccuracy points across diverse benchmarks."
    },
    {
        "date": "2025-09",
        "title": "PrunedLoRA: Robust Gradient-Based structured pruning for Low-rank Adaptation in Fine-tuning",
        "author": "Xin Yu, Cong Xie, Ziyu Zhao, Tiantian Fan, Lingzhou Xue, and Zhi Zhang",
        "link": "http://arxiv.org/abs/2510.00192v1",
        "abstract": "Low-rank adaptation (LoRA) has become a widely used paradigm for\nparameter-efficient fine-tuning of large language models, yet its\nrepresentational capacity often lags behind full fine-tuning. Within the\ncontext of LoRA, a key open question is how to obtain expressive low-rank\nadapters from over-parameterized spaces. We propose \\textit{PrunedLoRA}, a new\nframework that leverages structured pruning to obtain highly representative\nlow-rank adapters from an over-parameterized initialization. Unlike prior\napproaches that impose a fixed low-rank budget, PrunedLoRA dynamically prunes\nless important components during fine-tuning and prevents their reactivation,\nenabling flexible and adaptive rank allocation. For structured pruning, by\nminimizing the pruning error for overall loss, we provide fine-grained pruning\nand recovery updates in a gradient-based pruning strategy with grounded\ninterpretation. We provide the first theoretical analysis of the robustness of\nstructured pruning and provably show that under the impact of weight\nperturbation, gradient-based pruning is more robust than activation-based\npruning with respect to overall loss. Empirically, PrunedLoRA consistently\noutperforms LoRA and its variants across supervised fine-tuning tasks in\nmathematical reasoning, code generation, and natural language understanding,\nand it also demonstrates advantages over existing structured pruning methods\nacross diverse sparsity levels."
    },
    {
        "date": "2025-09",
        "title": "Noisy-Pair Robust Representation Alignment for Positive-Unlabeled Learning",
        "author": "Hengwei Zhao, Zhengzhong Tu, Zhuo Zheng, Wei Wang, Junjue Wang, Rusty Feagin, and Wenzhe Jiao",
        "link": "http://arxiv.org/abs/2510.01278v1",
        "abstract": "Positive-Unlabeled (PU) learning aims to train a binary classifier (positive\nvs. negative) where only limited positive data and abundant unlabeled data are\navailable. While widely applicable, state-of-the-art PU learning methods\nsubstantially underperform their supervised counterparts on complex datasets,\nespecially without auxiliary negatives or pre-estimated parameters (e.g., a\n14.26% gap on CIFAR-100 dataset). We identify the primary bottleneck as the\nchallenge of learning discriminative representations under unreliable\nsupervision. To tackle this challenge, we propose NcPU, a non-contrastive PU\nlearning framework that requires no auxiliary information. NcPU combines a\nnoisy-pair robust supervised non-contrastive loss (NoiSNCL), which aligns\nintra-class representations despite unreliable supervision, with a phantom\nlabel disambiguation (PLD) scheme that supplies conservative negative\nsupervision via regret-based label updates. Theoretically, NoiSNCL and PLD can\niteratively benefit each other from the perspective of the\nExpectation-Maximization framework. Empirically, extensive experiments\ndemonstrate that: (1) NoiSNCL enables simple PU methods to achieve competitive\nperformance; and (2) NcPU achieves substantial improvements over\nstate-of-the-art PU methods across diverse datasets, including challenging\ndatasets on post-disaster building damage mapping, highlighting its promise for\nreal-world applications. Code: Code will be open-sourced after review."
    },
    {
        "date": "2025-09",
        "title": "Optimizing What Matters: AUC-Driven Learning for Robust Neural Retrieval",
        "author": "Nima Sheikholeslami, Erfan Hosseini, Patrice Bechard, Srivatsava Daruru, and Sai Rajeswar",
        "link": "http://arxiv.org/abs/2510.00137v1",
        "abstract": "Dual-encoder retrievers depend on the principle that relevant documents\nshould score higher than irrelevant ones for a given query. Yet the dominant\nNoise Contrastive Estimation (NCE) objective, which underpins Contrastive Loss,\noptimizes a softened ranking surrogate that we rigorously prove is\nfundamentally oblivious to score separation quality and unrelated to AUC. This\nmismatch leads to poor calibration and suboptimal performance in downstream\ntasks like retrieval-augmented generation (RAG). To address this fundamental\nlimitation, we introduce the MW loss, a new training objective that maximizes\nthe Mann-Whitney U statistic, which is mathematically equivalent to the Area\nunder the ROC Curve (AUC). MW loss encourages each positive-negative pair to be\ncorrectly ranked by minimizing binary cross entropy over score differences. We\nprovide theoretical guarantees that MW loss directly upper-bounds the AoC,\nbetter aligning optimization with retrieval goals. We further promote ROC\ncurves and AUC as natural threshold free diagnostics for evaluating retriever\ncalibration and ranking quality. Empirically, retrievers trained with MW loss\nconsistently outperform contrastive counterparts in AUC and standard retrieval\nmetrics. Our experiments show that MW loss is an empirically superior\nalternative to Contrastive Loss, yielding better-calibrated and more\ndiscriminative retrievers for high-stakes applications like RAG."
    },
    {
        "date": "2025-09",
        "title": "Are Robust LLM Fingerprints Adversarially Robust?",
        "author": "Anshul Nasery, Edoardo Contente, Alkin Kaz, Pramod Viswanath, and Sewoong Oh",
        "link": "http://arxiv.org/abs/2509.26598v1",
        "abstract": "Model fingerprinting has emerged as a promising paradigm for claiming model\nownership. However, robustness evaluations of these schemes have mostly focused\non benign perturbations such as incremental fine-tuning, model merging, and\nprompting. Lack of systematic investigations into {\\em adversarial robustness}\nagainst a malicious model host leaves current systems vulnerable. To bridge\nthis gap, we first define a concrete, practical threat model against model\nfingerprinting. We then take a critical look at existing model fingerprinting\nschemes to identify their fundamental vulnerabilities. Based on these, we\ndevelop adaptive adversarial attacks tailored for each vulnerability, and\ndemonstrate that these can bypass model authentication completely for ten\nrecently proposed fingerprinting schemes while maintaining high utility of the\nmodel for the end users. Our work encourages fingerprint designers to adopt\nadversarial robustness by design. We end with recommendations for future\nfingerprinting methods."
    },
    {
        "date": "2025-09",
        "title": "Machine-Learning Driven Load Shedding to Mitigate Instability Attacks in Power Grids",
        "author": "Justin Tackett, Benjamin Francis, Luis Garcia, David Grimsman, and Sean Warnick",
        "link": "http://arxiv.org/abs/2509.26532v1",
        "abstract": "Every year critical infrastructure becomes more complex and we grow to rely\non it more and more. With this reliance, it becomes an attractive target for\ncyberattacks from sophisticated actors, with one of the most attractive targets\nbeing the power grid. One class of attacks, instability attacks, is a newer\ntype of attack that has relatively few protections developed. We present a cost\neffective, data-driven approach to training a supervised machine learning model\nto retrofit load shedding decision systems in power grids with the capacity to\ndefend against instability attacks. We show a proof of concept on the IEEE 14\nBus System using the Achilles Heel Technologies Power Grid Analyzer, and show\nthrough an implementation of modified Prony analysis (MPA) that MPA is a viable\nmethod for detecting instability attacks and triggering defense mechanisms."
    },
    {
        "date": "2025-09",
        "title": "Explainable and Resilient ML-Based Physical-Layer Attack Detectors",
        "author": "Aleksandra Knapi\u0144ska, and Marija Furdek",
        "link": "http://arxiv.org/abs/2509.26530v1",
        "abstract": "Detection of emerging attacks on network infrastructure is a critical aspect\nof security management. To meet the growing scale and complexity of modern\nthreats, machine learning (ML) techniques offer valuable tools for automating\nthe detection of malicious activities. However, as these techniques become more\ncomplex, their internal operations grow increasingly opaque. In this context,\nwe address the need for explainable physical-layer attack detection methods.\nFirst, we analyze the inner workings of various classifiers trained to alert\nabout physical layer intrusions, examining how the influence of different\nmonitored parameters varies depending on the type of attack being detected.\nThis analysis not only improves the interpretability of the models but also\nsuggests ways to enhance their design for increased speed. In the second part,\nwe evaluate the detectors' resilience to malicious parameter noising. The\nresults highlight a key trade-off between model speed and resilience. This work\nserves as a design guideline for developing fast and robust detectors trained\non available network monitoring data."
    },
    {
        "date": "2025-09",
        "title": "DEPTHOR++: Robust Depth Enhancement from a Real-World Lightweight dToF and RGB Guidance",
        "author": "Jijun Xiang, Longliang Liu, Xuan Zhu, Xianqi Wang, Min Lin, and Xin Yang",
        "link": "http://arxiv.org/abs/2509.26498v1",
        "abstract": "Depth enhancement, which converts raw dToF signals into dense depth maps\nusing RGB guidance, is crucial for improving depth perception in high-precision\ntasks such as 3D reconstruction and SLAM. However, existing methods often\nassume ideal dToF inputs and perfect dToF-RGB alignment, overlooking\ncalibration errors and anomalies, thus limiting real-world applicability. This\nwork systematically analyzes the noise characteristics of real-world\nlightweight dToF sensors and proposes a practical and novel depth completion\nframework, DEPTHOR++, which enhances robustness to noisy dToF inputs from three\nkey aspects. First, we introduce a simulation method based on synthetic\ndatasets to generate realistic training samples for robust model training.\nSecond, we propose a learnable-parameter-free anomaly detection mechanism to\nidentify and remove erroneous dToF measurements, preventing misleading\npropagation during completion. Third, we design a depth completion network\ntailored to noisy dToF inputs, which integrates RGB images and pre-trained\nmonocular depth estimation priors to improve depth recovery in challenging\nregions. On the ZJU-L5 dataset and real-world samples, our training strategy\nsignificantly boosts existing depth completion models, with our model achieving\nstate-of-the-art performance, improving RMSE and Rel by 22% and 11% on average.\nOn the Mirror3D-NYU dataset, by incorporating the anomaly detection method, our\nmodel improves upon the previous SOTA by 37% in mirror regions. On the Hammer\ndataset, using simulated low-cost dToF data from RealSense L515, our method\nsurpasses the L515 measurements with an average gain of 22%, demonstrating its\npotential to enable low-cost sensors to outperform higher-end devices.\nQualitative results across diverse real-world datasets further validate the\neffectiveness and generalizability of our approach."
    },
    {
        "date": "2025-09",
        "title": "STaR-Attack: A Spatio-Temporal and Narrative Reasoning Attack Framework for Unified Multimodal Understanding and Generation Models",
        "author": "Shaoxiong Guo, Tianyi Du, Lijun Li, Yuyao Wu, Jie Li, and Jing Shao",
        "link": "http://arxiv.org/abs/2509.26473v1",
        "abstract": "Unified Multimodal understanding and generation Models (UMMs) have\ndemonstrated remarkable capabilities in both understanding and generation\ntasks. However, we identify a vulnerability arising from the\ngeneration-understanding coupling in UMMs. The attackers can use the generative\nfunction to craft an information-rich adversarial image and then leverage the\nunderstanding function to absorb it in a single pass, which we call Cross-Modal\nGenerative Injection (CMGI). Current attack methods on malicious instructions\nare often limited to a single modality while also relying on prompt rewriting\nwith semantic drift, leaving the unique vulnerabilities of UMMs unexplored. We\npropose STaR-Attack, the first multi-turn jailbreak attack framework that\nexploits unique safety weaknesses of UMMs without semantic drift. Specifically,\nour method defines a malicious event that is strongly correlated with the\ntarget query within a spatio-temporal context. Using the three-act narrative\ntheory, STaR-Attack generates the pre-event and the post-event scenes while\nconcealing the malicious event as the hidden climax. When executing the attack\nstrategy, the opening two rounds exploit the UMM's generative ability to\nproduce images for these scenes. Subsequently, an image-based question guessing\nand answering game is introduced by exploiting the understanding capability.\nSTaR-Attack embeds the original malicious question among benign candidates,\nforcing the model to select and answer the most relevant one given the\nnarrative context. Extensive experiments show that STaR-Attack consistently\nsurpasses prior approaches, achieving up to 93.06% ASR on Gemini-2.0-Flash and\nsurpasses the strongest prior baseline, FlipAttack. Our work uncovers a\ncritical yet underdeveloped vulnerability and highlights the need for safety\nalignments in UMMs."
    },
    {
        "date": "2025-09",
        "title": "SoK: Systematic analysis of adversarial threats against deep learning approaches for autonomous anomaly detection systems in SDN-IoT networks",
        "author": "Tharindu Lakshan Yasarathna, and Nhien-An Le-Khac",
        "link": "http://arxiv.org/abs/2509.26350v1",
        "abstract": "Integrating SDN and the IoT enhances network control and flexibility.\nDL-based AAD systems improve security by enabling real-time threat detection in\nSDN-IoT networks. However, these systems remain vulnerable to adversarial\nattacks that manipulate input data or exploit model weaknesses, significantly\ndegrading detection accuracy. Existing research lacks a systematic analysis of\nadversarial vulnerabilities specific to DL-based AAD systems in SDN-IoT\nenvironments. This SoK study introduces a structured adversarial threat model\nand a comprehensive taxonomy of attacks, categorising them into data, model,\nand hybrid-level threats. Unlike previous studies, we systematically evaluate\nwhite, black, and grey-box attack strategies across popular benchmark datasets.\nOur findings reveal that adversarial attacks can reduce detection accuracy by\nup to 48.4%, with Membership Inference causing the most significant drop. C&W\nand DeepFool achieve high evasion success rates. However, adversarial training\nenhances robustness, and its high computational overhead limits the real-time\ndeployment of SDN-IoT applications. We propose adaptive countermeasures,\nincluding real-time adversarial mitigation, enhanced retraining mechanisms, and\nexplainable AI-driven security frameworks. By integrating structured threat\nmodels, this study offers a more comprehensive approach to attack\ncategorisation, impact assessment, and defence evaluation than previous\nresearch. Our work highlights critical vulnerabilities in existing DL-based AAD\nmodels and provides practical recommendations for improving resilience,\ninterpretability, and computational efficiency. This study serves as a\nfoundational reference for researchers and practitioners seeking to enhance\nDL-based AAD security in SDN-IoT networks, offering a systematic adversarial\nthreat model and conceptual defence evaluation based on prior empirical\nstudies."
    },
    {
        "date": "2025-09",
        "title": "SafeBehavior: Simulating Human-Like Multistage Reasoning to Mitigate Jailbreak Attacks in Large Language Models",
        "author": "Qinjian Zhao, Jiaqi Wang, Zhiqiang Gao, Zhihao Dou, Belal Abuhaija, and Kaizhu Huang",
        "link": "http://arxiv.org/abs/2509.26345v1",
        "abstract": "Large Language Models (LLMs) have achieved impressive performance across\ndiverse natural language processing tasks, but their growing power also\namplifies potential risks such as jailbreak attacks that circumvent built-in\nsafety mechanisms. Existing defenses including input paraphrasing, multi step\nevaluation, and safety expert models often suffer from high computational\ncosts, limited generalization, or rigid workflows that fail to detect subtle\nmalicious intent embedded in complex contexts. Inspired by cognitive science\nfindings on human decision making, we propose SafeBehavior, a novel\nhierarchical jailbreak defense mechanism that simulates the adaptive multistage\nreasoning process of humans. SafeBehavior decomposes safety evaluation into\nthree stages: intention inference to detect obvious input risks, self\nintrospection to assess generated responses and assign confidence based\njudgments, and self revision to adaptively rewrite uncertain outputs while\npreserving user intent and enforcing safety constraints. We extensively\nevaluate SafeBehavior against five representative jailbreak attack types\nincluding optimization based, contextual manipulation, and prompt based attacks\nand compare it with seven state of the art defense baselines. Experimental\nresults show that SafeBehavior significantly improves robustness and\nadaptability across diverse threat scenarios, offering an efficient and human\ninspired approach to safeguarding LLMs against jailbreak attempts."
    },
    {
        "date": "2025-09",
        "title": "Wasserstein Distributionally Robust Optimization Through the Lens of Structural Causal Models and Individual Fairness",
        "author": "Ahmad-Reza Ehyaei, Golnoosh Farnadi, and Samira Samadi",
        "link": "http://arxiv.org/abs/2509.26275v1",
        "abstract": "In recent years, Wasserstein Distributionally Robust Optimization (DRO) has\ngarnered substantial interest for its efficacy in data-driven decision-making\nunder distributional uncertainty. However, limited research has explored the\napplication of DRO to address individual fairness concerns, particularly when\nconsidering causal structures and sensitive attributes in learning problems. To\naddress this gap, we first formulate the DRO problem from causality and\nindividual fairness perspectives. We then present the DRO dual formulation as\nan efficient tool to convert the DRO problem into a more tractable and\ncomputationally efficient form. Next, we characterize the closed form of the\napproximate worst-case loss quantity as a regularizer, eliminating the max-step\nin the min-max DRO problem. We further estimate the regularizer in more general\ncases and explore the relationship between DRO and classical robust\noptimization. Finally, by removing the assumption of a known structural causal\nmodel, we provide finite sample error bounds when designing DRO with empirical\ndistributions and estimated causal structures to ensure efficiency and robust\nlearning."
    },
    {
        "date": "2025-09",
        "title": "Stealthy Yet Effective: Distribution-Preserving Backdoor Attacks on Graph Classification",
        "author": "Xiaobao Wang, Ruoxiao Sun, Yujun Zhang, Bingdao Feng, Dongxiao He, Luzhi Wang, and Di Jin",
        "link": "http://arxiv.org/abs/2509.26032v1",
        "abstract": "Graph Neural Networks (GNNs) have demonstrated strong performance across\ntasks such as node classification, link prediction, and graph classification,\nbut remain vulnerable to backdoor attacks that implant imperceptible triggers\nduring training to control predictions. While node-level attacks exploit local\nmessage passing, graph-level attacks face the harder challenge of manipulating\nglobal representations while maintaining stealth. We identify two main sources\nof anomaly in existing graph classification backdoor methods: structural\ndeviation from rare subgraph triggers and semantic deviation caused by label\nflipping, both of which make poisoned graphs easily detectable by anomaly\ndetection models. To address this, we propose DPSBA, a clean-label backdoor\nframework that learns in-distribution triggers via adversarial training guided\nby anomaly-aware discriminators. DPSBA effectively suppresses both structural\nand semantic anomalies, achieving high attack success while significantly\nimproving stealth. Extensive experiments on real-world datasets validate that\nDPSBA achieves a superior balance between effectiveness and detectability\ncompared to state-of-the-art baselines."
    },
    {
        "date": "2025-09",
        "title": "Reconcile Certified Robustness and Accuracy for DNN-based Smoothed Majority Vote Classifier",
        "author": "Gaojie Jin, Xinping Yi, and Xiaowei Huang",
        "link": "http://arxiv.org/abs/2509.25979v1",
        "abstract": "Within the PAC-Bayesian framework, the Gibbs classifier (defined on a\nposterior $Q$) and the corresponding $Q$-weighted majority vote classifier are\ncommonly used to analyze the generalization performance. However, there exists\na notable lack in theoretical research exploring the certified robustness of\nmajority vote classifier and its interplay with generalization. In this study,\nwe develop a generalization error bound that possesses a certified robust\nradius for the smoothed majority vote classifier (i.e., the $Q$-weighted\nmajority vote classifier with smoothed inputs); In other words, the\ngeneralization bound holds under any data perturbation within the certified\nrobust radius. As a byproduct, we find that the underpinnings of both the\ngeneralization bound and the certified robust radius draw, in part, upon weight\nspectral norm, which thereby inspires the adoption of spectral regularization\nin smooth training to boost certified robustness. Utilizing the\ndimension-independent property of spherical Gaussian inputs in smooth training,\nwe propose a novel and inexpensive spectral regularizer to enhance the smoothed\nmajority vote classifier. In addition to the theoretical contribution, a set of\nempirical results is provided to substantiate the effectiveness of our proposed\nmethod."
    },
    {
        "date": "2025-09",
        "title": "Scalable and Robust LLM Unlearning by Correcting Responses with Retrieved Exclusions",
        "author": "Junbeom Kim, Kyuyoung Kim, Jihoon Tack, Dongha Lim, and Jinwoo Shin",
        "link": "http://arxiv.org/abs/2509.25973v1",
        "abstract": "Language models trained on web-scale corpora risk memorizing and exposing\nsensitive information, prompting the need for effective machine unlearning.\nPrior methods mainly focus on input queries to suppress sensitive outputs, yet\nthis often fails to eliminate the underlying knowledge and limits scalability.\nTo address this, we propose Corrective Unlearning with Retrieved Exclusions\n(CURE), a novel unlearning framework that verifies model outputs for leakage\nand revises them into safe responses. Specifically, CURE employs a lightweight\ncorrector that is applied to the original model to verify whether outputs\ncontain target knowledge and to rewrite them if any leakage is detected. To\nefficiently handle large-scale unlearning requests, CURE retrieves unlearning\ntargets that are relevant to the initial response and provides them as\nin-context references to the corrector for detection and conditional revision.\nBy leveraging this retrieval augmentation, the corrector can adapt to new\nunlearning requests without additional training. Extensive evaluations\ndemonstrate that CURE substantially reduces information leakage, even from\nindirect queries where prior works fall short, while maintaining response\nquality and general utility. Moreover, it demonstrates robustness under\ncontinual unlearning scenarios, making it practical for real-world\napplications."
    },
    {
        "date": "2025-09",
        "title": "The Impact of Scaling Training Data on Adversarial Robustness",
        "author": "Marco Zimmerli, Andreas Plesner, Till Aczel, and Roger Wattenhofer",
        "link": "http://arxiv.org/abs/2509.25927v1",
        "abstract": "Deep neural networks remain vulnerable to adversarial examples despite\nadvances in architectures and training paradigms. We investigate how training\ndata characteristics affect adversarial robustness across 36 state-of-the-art\nvision models spanning supervised, self-supervised, and contrastive learning\napproaches, trained on datasets from 1.2M to 22B images. Models were evaluated\nunder six black-box attack categories: random perturbations, two types of\ngeometric masks, COCO object manipulations, ImageNet-C corruptions, and\nImageNet-R style shifts. Robustness follows a logarithmic scaling law with both\ndata volume and model size: a tenfold increase in data reduces attack success\nrate (ASR) on average by ~3.2%, whereas a tenfold increase in model size\nreduces ASR on average by ~13.4%. Notably, some self-supervised models trained\non curated datasets, such as DINOv2, outperform others trained on much larger\nbut less curated datasets, challenging the assumption that scale alone drives\nrobustness. Adversarial fine-tuning of ResNet50s improves generalization across\nstructural variations but not across color distributions. Human evaluation\nreveals persistent gaps between human and machine vision. These results show\nthat while scaling improves robustness, data quality, architecture, and\ntraining objectives play a more decisive role than raw scale in achieving\nbroad-spectrum adversarial resilience."
    },
    {
        "date": "2025-09",
        "title": "ASGuard: Activation-Scaling Guard to Mitigate Targeted Jailbreaking Attack",
        "author": "Yein Park, Jungwoo Park, and Jaewoo Kang",
        "link": "http://arxiv.org/abs/2509.25843v1",
        "abstract": "Large language models (LLMs), despite being safety-aligned, exhibit brittle\nrefusal behaviors that can be circumvented by simple linguistic changes. As\ntense jailbreaking demonstrates that models refusing harmful requests often\ncomply when rephrased in past tense, a critical generalization gap is revealed\nin current alignment methods whose underlying mechanisms are poorly understood.\nIn this work, we introduce Activation-Scaling Guard (ASGuard), an insightful,\nmechanistically-informed framework that surgically mitigates this specific\nvulnerability. For the first step, we use circuit analysis to identify the\nspecific attention heads causally linked to the targeted jailbreaking, the\ntense-changing attack. Second, we train a precise, channel-wise scaling vector\nto recalibrate the activation of tense vulnerable heads. Lastly, we apply it\ninto a \"preventative fine-tuning\", forcing the model to learn a more robust\nrefusal mechanism. Across three LLMs, ASGuard effectively reduces the attack\nsuccess rate of targeted jailbreaking while preserving general capabilities and\nminimizing over refusal, achieving a Pareto-optimal balance between safety and\nutility. Our findings underscore how adversarial suffixes suppress the\npropagation of the refusal-mediating direction, based on mechanistic analysis.\nFurthermore, our work showcases how a deep understanding of model internals can\nbe leveraged to develop practical, efficient, and targeted methods for\nadjusting model behavior, charting a course for more reliable and interpretable\nAI safety."
    },
    {
        "date": "2025-09",
        "title": "Enhancing Certifiable Semantic Robustness via Robust Pruning of Deep Neural Networks",
        "author": "Hanjiang Hu, Bowei Li, Ziwei Wang, Tianhao Wei, Casidhe Hutchison, Eric Sample, and Changliu Liu",
        "link": "http://arxiv.org/abs/2510.00083v1",
        "abstract": "Deep neural networks have been widely adopted in many vision and robotics\napplications with visual inputs. It is essential to verify its robustness\nagainst semantic transformation perturbations, such as brightness and contrast.\nHowever, current certified training and robustness certification methods face\nthe challenge of over-parameterization, which hinders the tightness and\nscalability due to the over-complicated neural networks. To this end, we first\nanalyze stability and variance of layers and neurons against input\nperturbation, showing that certifiable robustness can be indicated by a\nfundamental Unbiased and Smooth Neuron metric (USN). Based on USN, we introduce\na novel neural network pruning method that removes neurons with low USN and\nretains those with high USN, thereby preserving model expressiveness without\nover-parameterization. To further enhance this pruning process, we propose a\nnew Wasserstein distance loss to ensure that pruned neurons are more\nconcentrated across layers. We validate our approach through extensive\nexperiments on the challenging robust keypoint detection task, which involves\nrealistic brightness and contrast perturbations, demonstrating that our method\nachieves superior robustness certification performance and efficiency compared\nto baselines."
    },
    {
        "date": "2025-09",
        "title": "PUREVQ-GAN: Defending Data Poisoning Attacks through Vector-Quantized Bottlenecks",
        "author": "Alexander Branch, Omead Pooladzandi, Radin Khosraviani, Sunay Gajanan Bhat, Jeffrey Jiang, and Gregory Pottie",
        "link": "http://arxiv.org/abs/2509.25792v1",
        "abstract": "We introduce PureVQ-GAN, a defense against data poisoning that forces\nbackdoor triggers through a discrete bottleneck using Vector-Quantized VAE with\nGAN discriminator. By quantizing poisoned images through a learned codebook,\nPureVQ-GAN destroys fine-grained trigger patterns while preserving semantic\ncontent. A GAN discriminator ensures outputs match the natural image\ndistribution, preventing reconstruction of out-of-distribution perturbations.\nOn CIFAR-10, PureVQ-GAN achieves 0% poison success rate (PSR) against Gradient\nMatching and Bullseye Polytope attacks, and 1.64% against Narcissus while\nmaintaining 91-95% clean accuracy. Unlike diffusion-based defenses requiring\nhundreds of iterative refinement steps, PureVQ-GAN is over 50x faster, making\nit practical for real training pipelines."
    },
    {
        "date": "2025-09",
        "title": "Lightweight and Robust Federated Data Valuation",
        "author": "Guojun Tang, Jiayu Zhou, Mohammad Mamun, and Steve Drew",
        "link": "http://arxiv.org/abs/2509.25560v1",
        "abstract": "Federated learning (FL) faces persistent robustness challenges due to non-IID\ndata distributions and adversarial client behavior. A promising mitigation\nstrategy is contribution evaluation, which enables adaptive aggregation by\nquantifying each client's utility to the global model. However,\nstate-of-the-art Shapley-value-based approaches incur high computational\noverhead due to repeated model reweighting and inference, which limits their\nscalability. We propose FedIF, a novel FL aggregation framework that leverages\ntrajectory-based influence estimation to efficiently compute client\ncontributions. FedIF adapts decentralized FL by introducing normalized and\nsmoothed influence scores computed from lightweight gradient operations on\nclient updates and a public validation set. Theoretical analysis demonstrates\nthat FedIF yields a tighter bound on one-step global loss change under noisy\nconditions. Extensive experiments on CIFAR-10 and Fashion-MNIST show that FedIF\nachieves robustness comparable to or exceeding SV-based methods in the presence\nof label noise, gradient noise, and adversarial samples, while reducing\naggregation overhead by up to 450x. Ablation studies confirm the effectiveness\nof FedIF's design choices, including local weight normalization and influence\nsmoothing. Our results establish FedIF as a practical, theoretically grounded,\nand scalable alternative to Shapley-value-based approaches for efficient and\nrobust FL in real-world deployments."
    },
    {
        "date": "2025-09",
        "title": "Robust Visual Localization in Compute-Constrained Environments by Salient Edge Rendering and Weighted Hamming Similarity",
        "author": "Tu-Hoa Pham, Philip Bailey, Daniel Posada, Georgios Georgakis, Jorge Enriquez, Surya Suresh, Marco Dolci, and Philip Twu",
        "link": "http://arxiv.org/abs/2509.25520v1",
        "abstract": "We consider the problem of vision-based 6-DoF object pose estimation in the\ncontext of the notional Mars Sample Return campaign, in which a robotic arm\nwould need to localize multiple objects of interest for low-clearance pickup\nand insertion, under severely constrained hardware. We propose a novel\nlocalization algorithm leveraging a custom renderer together with a new\ntemplate matching metric tailored to the edge domain to achieve robust pose\nestimation using only low-fidelity, textureless 3D models as inputs. Extensive\nevaluations on synthetic datasets as well as from physical testbeds on Earth\nand in situ Mars imagery shows that our method consistently beats the state of\nthe art in compute and memory-constrained localization, both in terms of\nrobustness and accuracy, in turn enabling new possibilities for cheap and\nreliable localization on general-purpose hardware."
    },
    {
        "date": "2025-09",
        "title": "Environmental Rate Manipulation Attacks on Power Grid Security",
        "author": "Yonatan Gizachew Achamyeleh, Yang Xiang, Yun-Ping Hsiao, Yasamin Moghaddas, and Mohammad Abdullah Al Faruque",
        "link": "http://arxiv.org/abs/2509.25476v1",
        "abstract": "The growing complexity of global supply chains has made hardware Trojans a\nsignificant threat in sensor-based power electronics. Traditional Trojan\ndesigns depend on digital triggers or fixed threshold conditions that can be\ndetected during standard testing. In contrast, we introduce Environmental Rate\nManipulation (ERM), a novel Trojan triggering mechanism that activates by\nmonitoring the rate of change in environmental parameters rather than their\nabsolute values. This approach allows the Trojan to remain inactive under\nnormal conditions and evade redundancy and sensor-fusion defenses. We implement\na compact 14~$\\mu$m$^2$ circuit that measures capacitor charging rates in\nstandard sensor front-ends and disrupts inverter pulse-width modulation PWM\nsignals when a rapid change is induced. Experiments on a commercial Texas\nInstruments solar inverter demonstrate that ERM can trigger catastrophic driver\nchip failure. Furthermore, ETAP simulations indicate that a single compromised\n100~kW inverter may initiate cascading grid instabilities. The attack's\nsignificance extends beyond individual sensors to entire classes of\nenvironmental sensing systems common in power electronics, demonstrating\nfundamental challenges for hardware security."
    },
    {
        "date": "2025-09",
        "title": "Balancing Compliance and Privacy in Offline CBDC Transactions Using a Secure Element-based System",
        "author": "Panagiotis Michalopoulos, Anthony Mack, Cameron Clark, Linus Chen, Johannes Sedlmeir, and Andreas Veneris",
        "link": "http://arxiv.org/abs/2509.25469v1",
        "abstract": "Blockchain technology has spawned a vast ecosystem of digital currencies with\nCentral Bank Digital Currencies (CBDCs) -- digital forms of fiat currency --\nbeing one of them. An important feature of digital currencies is facilitating\ntransactions without network connectivity, which can enhance the scalability of\ncryptocurrencies and the privacy of CBDC users. However, in the case of CBDCs,\nthis characteristic also introduces new regulatory challenges, particularly\nwhen it comes to applying established Anti-Money Laundering and Countering the\nFinancing of Terrorism (AML/CFT) frameworks. This paper introduces a prototype\nfor offline digital currency payments, equally applicable to cryptocurrencies\nand CBDCs, that leverages Secure Elements and digital credentials to address\nthe tension of offline payment support with regulatory compliance. Performance\nevaluation results suggest that the prototype can be flexibly adapted to\ndifferent regulatory environments, with a transaction latency comparable to\nreal-life commercial payment systems. Furthermore, we conceptualize how the\nintegration of Zero-Knowledge Proofs into our design could accommodate various\ntiers of enhanced privacy protection."
    },
    {
        "date": "2025-09",
        "title": "Managing Differentiated Secure Connectivity using Intents",
        "author": "Loay Abdelrazek, and Filippo Rebecchi",
        "link": "http://arxiv.org/abs/2509.25462v1",
        "abstract": "Mobile networks in the 5G and 6G era require to rethink how to manage\nsecurity due to the introduction of new services, use cases, each with its own\nsecurity requirements, while simultaneously expanding the threat landscape.\nAlthough automation has emerged as a key enabler to address complexity in\nnetworks, existing approaches lack the expressiveness to define and enforce\ncomplex, goal-driven, and measurable security requirements. In this paper, we\npropose the concept of differentiated security levels and leveraging intents as\na management framework. We discuss the requirements and enablers to extend the\ncurrently defined intent-based management frameworks to pave the path for\nintent-based security management in mobile networks. Our approach formalizes\nboth functional and non-functional security requirements and demonstrates how\nthese can be expressed and modeled using an extended TM Forum (TMF) intent\nsecurity ontology. We further discuss the required standardization steps to\nachieve intent-based security management. Our work aims at advance security\nautomation, improve adaptability, and strengthen the resilience and security\nposture of the next-generation mobile networks."
    },
    {
        "date": "2025-09",
        "title": "Beyond Noisy-TVs: Noise-Robust Exploration Via Learning Progress Monitoring",
        "author": "Zhibo Hou, Zhiyu An, and Wan Du",
        "link": "http://arxiv.org/abs/2509.25438v1",
        "abstract": "When there exists an unlearnable source of randomness (noisy-TV) in the\nenvironment, a naively intrinsic reward driven exploring agent gets stuck at\nthat source of randomness and fails at exploration. Intrinsic reward based on\nuncertainty estimation or distribution similarity, while eventually escapes\nnoisy-TVs as time unfolds, suffers from poor sample efficiency and high\ncomputational cost. Inspired by recent findings from neuroscience that humans\nmonitor their improvements during exploration, we propose a novel method for\nintrinsically-motivated exploration, named Learning Progress Monitoring (LPM).\nDuring exploration, LPM rewards model improvements instead of prediction error\nor novelty, effectively rewards the agent for observing learnable transitions\nrather than the unlearnable transitions. We introduce a dual-network design\nthat uses an error model to predict the expected prediction error of the\ndynamics model in its previous iteration, and use the difference between the\nmodel errors of the current iteration and previous iteration to guide\nexploration. We theoretically show that the intrinsic reward of LPM is\nzero-equivariant and a monotone indicator of Information Gain (IG), and that\nthe error model is necessary to achieve monotonicity correspondence with IG. We\nempirically compared LPM against state-of-the-art baselines in noisy\nenvironments based on MNIST, 3D maze with 160x120 RGB inputs, and Atari.\nResults show that LPM's intrinsic reward converges faster, explores more states\nin the maze experiment, and achieves higher extrinsic reward in Atari. This\nconceptually simple approach marks a shift-of-paradigm of noise-robust\nexploration. For code to reproduce our experiments, see\nhttps://github.com/Akuna23Matata/LPM_exploration"
    },
    {
        "date": "2025-09",
        "title": "Fast Energy-Theft Attack on Frequency-Varying Wireless Power without Additional Sensors",
        "author": "Hui Wang, Nima Tashakor, Xiaoyang Tian, Hans D. Schotten, and Stefan M. Goetz",
        "link": "http://arxiv.org/abs/2509.25394v1",
        "abstract": "With the popularity of wireless charging, energy access protection and\ncybersecurity are gaining importance, especially in public places. Currently,\nthe most common energy encryption method uses frequency and associated\nimpedance variation. However, we have proven that this method is not reliable,\nsince a hacker can detect the changing frequency and adjust the compensation.\nHowever, the previously presented system needed time to follow the updated\nfrequency, while encryption systems may vary the frequency faster to avoid\nenergy theft. Furthermore, the previous system required an additional sensor\ncoil. To solve these problems, we optimized the attack and the associated\nsystem, which can intrude and steal energy within 0.2 ms. The key is the\nelimination of the time-consuming maximum receiver current regulation. Also, we\nuse the main receiving coil rather than any additional sensor antenna to detect\nthe magnetic field. Thus, the new hardware is even simpler. A simulation model\nand experimental results demonstrate the fast response speed of the attack on\nencrypted wireless power and steal 65% of the power. Overall, the applicability\nof the attack is highly improved and leaves less room for hardening the\nencryption. The results demonstrate that energy access protection needs to be\ngiven great attention."
    },
    {
        "date": "2025-09",
        "title": "UniAPL: A Unified Adversarial Preference Learning Framework for Instruct-Following",
        "author": "FaQiang Qian, WeiKun Zhang, Ziliang Wang, Kang An, Xuhui Zheng, Liangjian Wen, Mengya Gao, Yong Dai, and Yichao Wu",
        "link": "http://arxiv.org/abs/2509.25148v1",
        "abstract": "Shaping powerful LLMs to be beneficial and safe is central to AI alignment.\nWe argue that post-training alignment is fundamentally a unified Preference\nLearning problem, involving two modalities: demonstrated preferences (e.g.,\nSupervised Fine-Tuning, SFT) and comparative preferences (e.g., Reinforcement\nLearning, RL).The standard sequential pipeline-SFT followed by RL-is flawed due\nto a critical distributional mismatch: SFT uses static expert data, but as the\npolicy evolves, its generation distribution drifts, making SFT knowledge\nbrittle. Subsequent RL then explores without direct access to the rich,\nground-truth knowledge in expert demonstrations, leading to inefficient,\nungrounded updates. This separation prevents mutual regularization between data\nsources. To address this, we reframe alignment as a constrained optimization\nproblem and propose Unified Adversarial Preference Learning (UniAPL),a novel\nframework that dynamically aligns the policy's distribution with the expert's.\nUniAPL implements a single-stage unified training objective, jointly learning\nfrom mixed batches of SFT and preference data. In every gradient step, dense\nexpert demonstrations directly ground and regularize online exploration,\ninherently resolving distributional mismatch and maximizing data synergy.We\nevaluate UniAPL on instruction-following tasks using Qwen3-235B-Instruct-2507\nas the teacher. Our models match or exceed strong GRPO baselines: +5.77% on\nQwen3-0.6B (matching a 32B model) and +3.75% on Qwen3-4B,even outperforming the\nteacher. Analyses of response length and log-probability distributions confirm\nthat UniAPL outputs closely mimic expert demonstrations, achieving both\nstronger performance and better behavioral alignment."
    },
    {
        "date": "2025-09",
        "title": "Learning in an Echo Chamber: Online Learning with Replay Adversary",
        "author": "Daniil Dmitriev, Harald Eskelund Franck, Carolin Heinzler, and Amartya Sanyal",
        "link": "http://arxiv.org/abs/2509.25135v1",
        "abstract": "As machine learning systems increasingly train on self-annotated data, they\nrisk reinforcing errors and becoming echo chambers of their own beliefs. We\nmodel this phenomenon by introducing a learning-theoretic framework: Online\nLearning in the Replay Setting. In round $t$, the learner outputs a hypothesis\n$\\hat{h}_t$; the adversary then reveals either the true label $f^\\ast(x_t)$ or\na replayed label $\\hat{h}_i(x_t)$ from an earlier round $i < t$. A mistake is\ncounted only when the true label is shown, yet classical algorithms such as the\nSOA or the halving algorithm are easily misled by the replayed errors.\n  We introduce the Extended Threshold dimension, $\\mathrm{ExThD}(\\mathcal{H})$,\nand prove matching upper and lower bounds that make\n$\\mathrm{ExThD}(\\mathcal{H})$ the exact measure of learnability in this model.\nA closure-based learner makes at most $\\mathrm{ExThD}(\\mathcal{H})$ mistakes\nagainst any adaptive adversary, and no algorithm can perform better. For\nstochastic adversaries, we prove a similar bound for every intersection-closed\nclass. The replay setting is provably harder than the classical mistake bound\nsetting: some classes have constant Littlestone dimension but arbitrarily large\n$\\mathrm{ExThD}(\\mathcal{H})$. Proper learning exhibits an even sharper\nseparation: a class is properly learnable under replay if and only if it is\n(almost) intersection-closed. Otherwise, every proper learner suffers\n$\\Omega(T)$ errors, whereas our improper algorithm still achieves the\n$\\mathrm{ExThD}(\\mathcal{H})$ bound. These results give the first tight\nanalysis of learning against replay adversaries, based on new results for\nclosure-type algorithms."
    },
    {
        "date": "2025-09",
        "title": "MANI-Pure: Magnitude-Adaptive Noise Injection for Adversarial Purification",
        "author": "Xiaoyi Huang, Junwei Wu, Kejia Zhang, Carl Yang, and Zhiming Luo",
        "link": "http://arxiv.org/abs/2509.25082v1",
        "abstract": "Adversarial purification with diffusion models has emerged as a promising\ndefense strategy, but existing methods typically rely on uniform noise\ninjection, which indiscriminately perturbs all frequencies, corrupting semantic\nstructures and undermining robustness. Our empirical study reveals that\nadversarial perturbations are not uniformly distributed: they are predominantly\nconcentrated in high-frequency regions, with heterogeneous magnitude intensity\npatterns that vary across frequencies and attack types. Motivated by this\nobservation, we introduce MANI-Pure, a magnitude-adaptive purification\nframework that leverages the magnitude spectrum of inputs to guide the\npurification process. Instead of injecting homogeneous noise, MANI-Pure\nadaptively applies heterogeneous, frequency-targeted noise, effectively\nsuppressing adversarial perturbations in fragile high-frequency, low-magnitude\nbands while preserving semantically critical low-frequency content. Extensive\nexperiments on CIFAR-10 and ImageNet-1K validate the effectiveness of\nMANI-Pure. It narrows the clean accuracy gap to within 0.59 of the original\nclassifier, while boosting robust accuracy by 2.15, and achieves the top-1\nrobust accuracy on the RobustBench leaderboard, surpassing the previous\nstate-of-the-art method."
    },
    {
        "date": "2025-09",
        "title": "Fast Real-Time Pipeline for Robust Arm Gesture Recognition",
        "author": "Mil\u00e1n Zsolt Bagladi, L\u00e1szl\u00f3 Guly\u00e1s, and Gerg\u0151 Szalay",
        "link": "http://arxiv.org/abs/2509.25042v1",
        "abstract": "This paper presents a real-time pipeline for dynamic arm gesture recognition\nbased on OpenPose keypoint estimation, keypoint normalization, and a recurrent\nneural network classifier. The 1 x 1 normalization scheme and two feature\nrepresentations (coordinate- and angle-based) are presented for the pipeline.\nIn addition, an efficient method to improve robustness against camera angle\nvariations is also introduced by using artificially rotated training data.\nExperiments on a custom traffic-control gesture dataset demonstrate high\naccuracy across varying viewing angles and speeds. Finally, an approach to\ncalculate the speed of the arm signal (if necessary) is also presented."
    },
    {
        "date": "2025-09",
        "title": "SDPose: Exploiting Diffusion Priors for Out-of-Domain and Robust Pose Estimation",
        "author": "Shuang Liang, Jing He, Chuanmeizhi Wang, Lejun Liao, Guo Zhang, Yingcong Chen, and Yuan Yuan",
        "link": "http://arxiv.org/abs/2509.24980v1",
        "abstract": "Pre-trained diffusion models provide rich multi-scale latent features and are\nemerging as powerful vision backbones. While recent works such as\nMarigold~\\citep{ke2024repurposing} and Lotus~\\citep{he2024lotus} adapt\ndiffusion priors for dense prediction with strong cross-domain generalization,\ntheir potential for structured outputs (e.g., human pose estimation) remains\nunderexplored. In this paper, we propose \\textbf{SDPose}, a fine-tuning\nframework built upon Stable Diffusion to fully exploit pre-trained diffusion\npriors for human pose estimation. First, rather than modifying cross-attention\nmodules or introducing learnable embeddings, we directly predict keypoint\nheatmaps in the SD U-Net's image latent space to preserve the original\ngenerative priors. Second, we map these latent features into keypoint heatmaps\nthrough a lightweight convolutional pose head, which avoids disrupting the\npre-trained backbone. Finally, to prevent overfitting and enhance\nout-of-distribution robustness, we incorporate an auxiliary RGB reconstruction\nbranch that preserves domain-transferable generative semantics. To evaluate\nrobustness under domain shift, we further construct \\textbf{COCO-OOD}, a\nstyle-transferred variant of COCO with preserved annotations. With just\none-fifth of the training schedule used by Sapiens on COCO, SDPose attains\nparity with Sapiens-1B/2B on the COCO validation set and establishes a new\nstate of the art on the cross-domain benchmarks HumanArt and COCO-OOD.\nFurthermore, we showcase SDPose as a zero-shot pose annotator for downstream\ncontrollable generation tasks, including ControlNet-based image synthesis and\nvideo generation, where it delivers qualitatively superior pose guidance."
    },
    {
        "date": "2025-09",
        "title": "Secret Leader Election in Ethereum PoS: An Empirical Security Analysis of Whisk and Homomorphic Sortition under DoS on the Leader and Censorship Attacks",
        "author": "Tereza Burianov\u00e1, Martin Pere\u0161\u00edni, and Ivan Homoliak",
        "link": "http://arxiv.org/abs/2509.24955v1",
        "abstract": "Proposer anonymity in Proof-of-Stake (PoS) blockchains is a critical concern\ndue to the risk of targeted attacks such as malicious denial-of-service (DoS)\nand censorship attacks. While several Secret Single Leader Election (SSLE)\nmechanisms have been proposed to address these threats, their practical impact\nand trade-offs remain insufficiently explored. In this work, we present a\nunified experimental framework for evaluating SSLE mechanisms under adversarial\nconditions, grounded in a simplified yet representative model of Ethereum's PoS\nconsensus layer. The framework includes configurable adversaries capable of\nlaunching targeted DoS and censorship attacks, including coordinated strategies\nthat simultaneously compromise groups of validators. We simulate and compare\nkey protection mechanisms - Whisk, and homomorphic sortition. To the best of\nour knowledge, this is the first comparative study to examine adversarial DoS\nscenarios involving multiple attackers under diverse protection mechanisms. Our\nresults show that while both designs offer strong protection against targeted\nDoS attacks on the leader, neither defends effectively against coordinated\nattacks on validator groups. Moreover, Whisk simplifies a DoS attack by\nnarrowing the target set from all validators to a smaller list of known\ncandidates. Homomorphic sortition, despite its theoretical strength, remains\nimpractical due to the complexity of cryptographic operations over large\nvalidator sets."
    },
    {
        "date": "2025-09",
        "title": "VAGUEGAN: Stealthy Poisoning and Backdoor Attacks on Image Generative Pipelines",
        "author": "Mostafa Mohaimen Akand Faisal, and Rabeya Amin Jhuma",
        "link": "http://arxiv.org/abs/2509.24891v1",
        "abstract": "Generative models such as GANs and diffusion models are widely used to\nsynthesize photorealistic images and to support downstream creative and editing\ntasks. While adversarial attacks on discriminative models are well studied,\nattacks targeting generative pipelines where small, stealthy perturbations in\ninputs lead to controlled changes in outputs are less explored. This study\nintroduces VagueGAN, an attack pipeline combining a modular perturbation\nnetwork PoisonerNet with a Generator Discriminator pair to craft stealthy\ntriggers that cause targeted changes in generated images. Attack efficacy is\nevaluated using a custom proxy metric, while stealth is analyzed through\nperceptual and frequency domain measures. The transferability of the method to\na modern diffusion based pipeline is further examined through ControlNet guided\nediting. Interestingly, the experiments show that poisoned outputs can display\nhigher visual quality compared to clean counterparts, challenging the\nassumption that poisoning necessarily reduces fidelity. Unlike conventional\npixel level perturbations, latent space poisoning in GANs and diffusion\npipelines can retain or even enhance output aesthetics, exposing a blind spot\nin pixel level defenses. Moreover, carefully optimized perturbations can\nproduce consistent, stealthy effects on generator outputs while remaining\nvisually inconspicuous, raising concerns for the integrity of image generation\npipelines."
    },
    {
        "date": "2025-09",
        "title": "Vision At Night: Exploring Biologically Inspired Preprocessing For Improved Robustness Via Color And Contrast Transformations",
        "author": "Lorena Stracke, Lia Nimmermann, Shashank Agnihotri, Margret Keuper, and Volker Blanz",
        "link": "http://arxiv.org/abs/2509.24863v1",
        "abstract": "Inspired by the human visual system's mechanisms for contrast enhancement and\ncolor-opponency, we explore biologically motivated input preprocessing for\nrobust semantic segmentation. By applying Difference-of-Gaussians (DoG)\nfiltering to RGB, grayscale, and opponent-color channels, we enhance local\ncontrast without modifying model architecture or training. Evaluations on\nCityscapes, ACDC, and Dark Zurich show that such preprocessing maintains\nin-distribution performance while improving robustness to adverse conditions\nlike night, fog, and snow. As this processing is model-agnostic and\nlightweight, it holds potential for integration into imaging pipelines,\nenabling imaging systems to deliver task-ready, robust inputs for downstream\nvision models in safety-critical environments."
    },
    {
        "date": "2025-09",
        "title": "Fidelity-Aware Data Composition for Robust Robot Generalization",
        "author": "Zizhao Tong, Di Chen, Sicheng Hu, Hongwei Fan, Liliang Chen, Guanghui Ren, Hao Tang, Hao Dong, and Ling Shao",
        "link": "http://arxiv.org/abs/2509.24797v1",
        "abstract": "Generalist robot policies trained on large-scale, visually homogeneous\ndatasets can be susceptible to shortcut learning, which impairs their\nout-of-distribution (OOD) generalization. While generative data augmentation is\na common approach to introduce diversity, it presents a subtle challenge: data\ncomposition. Naively mixing real and synthetic data can corrupt the learning\nsignal, as this process often prioritizes visual diversity at the expense of\ninformation fidelity. This paper suggests that robust generalization depends on\nprincipled, fidelity-aware data composition. We introduce Coherent Information\nFidelity Tuning (CIFT), a framework that treats data composition as an\noptimization problem. CIFT uses a practical proxy for Information Fidelity\nbased on the feature-space geometry of a dataset. This enables the\nidentification of a phase transition, termed the Decoherence Point, where\ntraining stability degrades. The framework includes a generative engine,\nMulti-View Video Augmentation (MVAug), to synthesize a causally disentangled\ndata spectrum for this tuning process. Applying CIFT to policy architectures\nsuch as $\\pi_0$ and Diffusion Policy improves OOD success rates by over 54\\%.\nThese results indicate that fidelity-aware composition, beyond data synthesis\nalone, is an important component for developing robust, general-purpose robots."
    },
    {
        "date": "2025-09",
        "title": "Robust Policy Expansion for Offline-to-Online RL under Diverse Data Corruption",
        "author": "Longxiang He, Deheng Ye, Junbo Tan, Xueqian Wang, and Li Shen",
        "link": "http://arxiv.org/abs/2509.24748v1",
        "abstract": "Pretraining a policy on offline data followed by fine-tuning through online\ninteractions, known as Offline-to-Online Reinforcement Learning (O2O RL), has\nemerged as a promising paradigm for real-world RL deployment. However, both\noffline datasets and online interactions in practical environments are often\nnoisy or even maliciously corrupted, severely degrading the performance of O2O\nRL. Existing works primarily focus on mitigating the conservatism of offline\npolicies via online exploration, while the robustness of O2O RL under data\ncorruption, including states, actions, rewards, and dynamics, is still\nunexplored. In this work, we observe that data corruption induces heavy-tailed\nbehavior in the policy, thereby substantially degrading the efficiency of\nonline exploration. To address this issue, we incorporate Inverse Probability\nWeighted (IPW) into the online exploration policy to alleviate\nheavy-tailedness, and propose a novel, simple yet effective method termed\n$\\textbf{RPEX}$: $\\textbf{R}$obust $\\textbf{P}$olicy $\\textbf{EX}$pansion.\nExtensive experimental results on D4RL datasets demonstrate that RPEX achieves\nSOTA O2O performance across a wide range of data corruption scenarios. Code is\navailable at\n$\\href{https://github.com/felix-thu/RPEX}{https://github.com/felix-thu/RPEX}$."
    },
    {
        "date": "2025-09",
        "title": "Circuit-Aware Reward Training: A Mechanistic Framework for Longtail Robustness in RLHF",
        "author": "Jing Liu",
        "link": "http://arxiv.org/abs/2509.24713v1",
        "abstract": "Reinforcement Learning from Human Feedback (RLHF) reward models exhibit\nsystematic failures on longtail distributions, leading to reward hacking and\nmisalignment. We propose a mechanistic interpretability framework that\nidentifies specialized neural circuits responsible for rare-event processing in\nreward models. Drawing from recent advances showing distributed specialization\nfor rare tokens in language models\\citep{liu2025no, liu2025emergent}, we\nhypothesize that reward models also develop functionally distinct circuits for\nlongtail scenarios. Our theoretical framework establishes formal connections\nbetween circuit specialization, reward generalization bounds, and longtail\nperformance. We introduce \\textbf{Circuit-Aware Reward Training (CART)}, which\nuses circuit analysis to guide data augmentation, regularization, and ensemble\nstrategies. This approach provides both theoretical insights into reward model\nfailures and practical interventions for improving longtail robustness."
    },
    {
        "date": "2025-09",
        "title": "Community detection robustness of graph neural networks",
        "author": "Jaidev Goel, Pablo Moriano, Ramakrishnan Kannan, and Yulia R. Gel",
        "link": "http://arxiv.org/abs/2509.24662v1",
        "abstract": "Graph neural networks (GNNs) are increasingly widely used for community\ndetection in attributed networks. They combine structural topology with node\nattributes through message passing and pooling. However, their robustness or\nlack of thereof with respect to different perturbations and targeted attacks in\nconjunction with community detection tasks is not well understood. To shed\nlight into latent mechanisms behind GNN sensitivity on community detection\ntasks, we conduct a systematic computational evaluation of six widely adopted\nGNN architectures: GCN, GAT, Graph-SAGE, DiffPool, MinCUT, and DMoN. The\nanalysis covers three perturbation categories: node attribute manipulations,\nedge topology distortions, and adversarial attacks. We use element-centric\nsimilarity as the evaluation metric on synthetic benchmarks and real-world\ncitation networks. Our findings indicate that supervised GNNs tend to achieve\nhigher baseline accuracy, while unsupervised methods, particularly DMoN,\nmaintain stronger resilience under targeted and adversarial perturbations.\nFurthermore, robustness appears to be strongly influenced by community\nstrength, with well-defined communities reducing performance loss. Across all\nmodels, node attribute perturbations associated with targeted edge deletions\nand shift in attribute distributions tend to cause the largest degradation in\ncommunity recovery. These findings highlight important trade-offs between\naccuracy and robustness in GNN-based community detection and offer new insights\ninto selecting architectures resilient to noise and adversarial attacks."
    },
    {
        "date": "2025-09",
        "title": "TokenSwap: Backdoor Attack on the Compositional Understanding of Large Vision-Language Models",
        "author": "Zhifang Zhang, Qiqi Tao, Jiaqi Lv, Na Zhao, Lei Feng, and Joey Tianyi Zhou",
        "link": "http://arxiv.org/abs/2509.24566v1",
        "abstract": "Large vision-language models (LVLMs) have achieved impressive performance\nacross a wide range of vision-language tasks, while they remain vulnerable to\nbackdoor attacks. Existing backdoor attacks on LVLMs aim to force the victim\nmodel to generate a predefined target pattern, which is either inserted into or\nreplaces the original content. We find that these fixed-pattern attacks are\nrelatively easy to detect, because the attacked LVLM tends to memorize such\nfrequent patterns in the training dataset, thereby exhibiting overconfidence on\nthese targets given poisoned inputs. To address these limitations, we introduce\nTokenSwap, a more evasive and stealthy backdoor attack that focuses on the\ncompositional understanding capabilities of LVLMs. Instead of enforcing a fixed\ntargeted content, TokenSwap subtly disrupts the understanding of object\nrelationships in text. Specifically, it causes the backdoored model to generate\noutputs that mention the correct objects in the image but misrepresent their\nrelationships (i.e., bags-of-words behavior). During training, TokenSwap\ninjects a visual trigger into selected samples and simultaneously swaps the\ngrammatical roles of key tokens in the corresponding textual answers. However,\nthe poisoned samples exhibit only subtle differences from the original ones,\nmaking it challenging for the model to learn the backdoor behavior. To address\nthis, TokenSwap employs an adaptive token-weighted loss that explicitly\nemphasizes the learning of swapped tokens, such that the visual triggers and\nbags-of-words behavior are associated. Extensive experiments demonstrate that\nTokenSwap achieves high attack success rates while maintaining superior\nevasiveness and stealthiness across multiple benchmarks and various LVLM\narchitectures."
    },
    {
        "date": "2025-09",
        "title": "LEAF: A Robust Expert-Based Framework for Few-Shot Continual Event Detection",
        "author": "Bao-Ngoc Dao, Quang Nguyen, Luyen Ngo Dinh, Minh Le, and Linh Ngo Van",
        "link": "http://arxiv.org/abs/2509.24547v1",
        "abstract": "Few-shot Continual Event Detection (FCED) poses the dual challenges of\nlearning from limited data and mitigating catastrophic forgetting across\nsequential tasks. Existing approaches often suffer from severe forgetting due\nto the full fine-tuning of a shared base model, which leads to knowledge\ninterference between tasks. Moreover, they frequently rely on data augmentation\nstrategies that can introduce unnatural or semantically distorted inputs. To\naddress these limitations, we propose LEAF, a novel and robust expert-based\nframework for FCED. LEAF integrates a specialized mixture of experts\narchitecture into the base model, where each expert is parameterized with\nlow-rank adaptation (LoRA) matrices. A semantic-aware expert selection\nmechanism dynamically routes instances to the most relevant experts, enabling\nexpert specialization and reducing knowledge interference. To improve\ngeneralization in limited-data settings, LEAF incorporates a contrastive\nlearning objective guided by label descriptions, which capture high-level\nsemantic information about event types. Furthermore, to prevent overfitting on\nthe memory buffer, our framework employs a knowledge distillation strategy that\ntransfers knowledge from previous models to the current one. Extensive\nexperiments on multiple FCED benchmarks demonstrate that LEAF consistently\nachieves state-of-the-art performance."
    },
    {
        "date": "2025-09",
        "title": "Robust Multimodal Semantic Segmentation with Balanced Modality Contributions",
        "author": "Jiaqi Tan, Xu Zheng, Fangyu Li, and Yang Liu",
        "link": "http://arxiv.org/abs/2509.24505v1",
        "abstract": "Multimodal semantic segmentation enhances model robustness by exploiting\ncross-modal complementarities. However, existing methods often suffer from\nimbalanced modal dependencies, where overall performance degrades significantly\nonce a dominant modality deteriorates in real-world scenarios. Thus, modality\nbalance has become acritical challenge for practical multimodal segmentation.\nTo address this issue, we propose EQUISeg, a multimodal segmentation framework\nthat balances modality contributions through equal encoding of modalities.\nBuilt upon a four-stage Cross-modal Transformer Block(CMTB), EQUISeg enables\nefficient multimodal fusion and hierarchical selection. Furthermore, we design\na Self-guided Module(SGM) that mitigates modality imbalance by introducing a\nmutual guidance mechanism, enabling each modality to adaptively adjust its\ncontribution and enhance robustness under degraded conditions. Extensive\nexperiments on multiple datasets demonstrate that EQUISeg achieves significant\nperformance gains and effectively alleviates the adverse effects of modality\nimbalance in segmentation tasks."
    },
    {
        "date": "2025-09",
        "title": "Distributionally Robust Federated Learning with Outlier Resilience",
        "author": "Zifan Wang, Xinlei Yi, Xenia Konti, Michael M. Zavlanos, and Karl H. Johansson",
        "link": "http://arxiv.org/abs/2509.24462v1",
        "abstract": "Federated learning (FL) enables collaborative model training without direct\ndata sharing, but its performance can degrade significantly in the presence of\ndata distribution perturbations. Distributionally robust optimization (DRO)\nprovides a principled framework for handling this by optimizing performance\nagainst the worst-case distributions within a prescribed ambiguity set.\nHowever, existing DRO-based FL methods often overlook the detrimental impact of\noutliers in local datasets, which can disproportionately bias the learned\nmodels. In this work, we study distributionally robust federated learning with\nexplicit outlier resilience. We introduce a novel ambiguity set based on the\nunbalanced Wasserstein distance, which jointly captures geometric\ndistributional shifts and incorporates a non-geometric Kullback--Leibler\npenalization to mitigate the influence of outliers. This formulation naturally\nleads to a challenging min--max--max optimization problem. To enable\ndecentralized training, we reformulate the problem as a tractable Lagrangian\npenalty optimization, which admits robustness certificates. Building on this\nreformulation, we propose the distributionally outlier-robust federated\nlearning algorithm and establish its convergence guarantees. Extensive\nexperiments on both synthetic and real-world datasets demonstrate the\neffectiveness of our approach."
    },
    {
        "date": "2025-09",
        "title": "DRIFT: Divergent Response in Filtered Transformations for Robust Adversarial Defense",
        "author": "Amira Guesmi, and Muhammad Shafique",
        "link": "http://arxiv.org/abs/2509.24359v1",
        "abstract": "Deep neural networks remain highly vulnerable to adversarial examples, and\nmost defenses collapse once gradients can be reliably estimated. We identify\n\\emph{gradient consensus} -- the tendency of randomized transformations to\nyield aligned gradients -- as a key driver of adversarial transferability.\nAttackers exploit this consensus to construct perturbations that remain\neffective across transformations. We introduce \\textbf{DRIFT} (Divergent\nResponse in Filtered Transformations), a stochastic ensemble of lightweight,\nlearnable filters trained to actively disrupt gradient consensus. Unlike prior\nrandomized defenses that rely on gradient masking, DRIFT enforces\n\\emph{gradient dissonance} by maximizing divergence in Jacobian- and\nlogit-space responses while preserving natural predictions. Our contributions\nare threefold: (i) we formalize gradient consensus and provide a theoretical\nanalysis linking consensus to transferability; (ii) we propose a\nconsensus-divergence training strategy combining prediction consistency,\nJacobian separation, logit-space separation, and adversarial robustness; and\n(iii) we show that DRIFT achieves substantial robustness gains on ImageNet\nacross CNNs and Vision Transformers, outperforming state-of-the-art\npreprocessing, adversarial training, and diffusion-based defenses under\nadaptive white-box, transfer-based, and gradient-free attacks. DRIFT delivers\nthese improvements with negligible runtime and memory cost, establishing\ngradient divergence as a practical and generalizable principle for adversarial\ndefense."
    },
    {
        "date": "2025-09",
        "title": "OMeGa: Joint Optimization of Explicit Meshes and Gaussian Splats for Robust Scene-Level Surface Reconstruction",
        "author": "Yuhang Cao, Haojun Yan, and Danya Yao",
        "link": "http://arxiv.org/abs/2509.24308v1",
        "abstract": "Neural rendering with Gaussian splatting has advanced novel view synthesis,\nand most methods reconstruct surfaces via post-hoc mesh extraction. However,\nexisting methods suffer from two limitations: (i) inaccurate geometry in\ntexture-less indoor regions, and (ii) the decoupling of mesh extraction from\noptimization, thereby missing the opportunity to leverage mesh geometry to\nguide splat optimization. In this paper, we present OMeGa, an end-to-end\nframework that jointly optimizes an explicit triangle mesh and 2D Gaussian\nsplats via a flexible binding strategy, where spatial attributes of Gaussian\nSplats are expressed in the mesh frame and texture attributes are retained on\nsplats. To further improve reconstruction accuracy, we integrate mesh\nconstraints and monocular normal supervision into the optimization, thereby\nregularizing geometry learning. In addition, we propose a heuristic, iterative\nmesh-refinement strategy that splits high-error faces and prunes unreliable\nones to further improve the detail and accuracy of the reconstructed mesh.\nOMeGa achieves state-of-the-art performance on challenging indoor\nreconstruction benchmarks, reducing Chamfer-$L_1$ by 47.3\\% over the 2DGS\nbaseline while maintaining competitive novel-view rendering quality. The\nexperimental results demonstrate that OMeGa effectively addresses prior\nlimitations in indoor texture-less reconstruction."
    },
    {
        "date": "2025-09",
        "title": "Robust Partial 3D Point Cloud Registration via Confidence Estimation under Global Context",
        "author": "Yongqiang Wang, Weigang Li, Wenping Liu, Zhe Xu, and Zhiqiang Tian",
        "link": "http://arxiv.org/abs/2509.24275v1",
        "abstract": "Partial point cloud registration is essential for autonomous perception and\n3D scene understanding, yet it remains challenging owing to structural\nambiguity, partial visibility, and noise. We address these issues by proposing\nConfidence Estimation under Global Context (CEGC), a unified, confidence-driven\nframework for robust partial 3D registration. CEGC enables accurate alignment\nin complex scenes by jointly modeling overlap confidence and correspondence\nreliability within a shared global context. Specifically, the hybrid overlap\nconfidence estimation module integrates semantic descriptors and geometric\nsimilarity to detect overlapping regions and suppress outliers early. The\ncontext-aware matching strategy smitigates ambiguity by employing global\nattention to assign soft confidence scores to correspondences, improving\nrobustness. These scores guide a differentiable weighted singular value\ndecomposition solver to compute precise transformations. This tightly coupled\npipeline adaptively down-weights uncertain regions and emphasizes contextually\nreliable matches. Experiments on ModelNet40, ScanObjectNN, and 7Scenes 3D\nvision datasets demonstrate that CEGC outperforms state-of-the-art methods in\naccuracy, robustness, and generalization. Overall, CEGC offers an interpretable\nand scalable solution to partial point cloud registration under challenging\nconditions."
    },
    {
        "date": "2025-09",
        "title": "Adversarial Reinforcement Learning Framework for ESP Cheater Simulation",
        "author": "Inkyu Park, Jeong-Gwan Lee, Taehwan Kwon, Juheon Choi, Seungku Kim, Junsu Kim, and Kimin Lee",
        "link": "http://arxiv.org/abs/2509.24274v1",
        "abstract": "Extra-Sensory Perception (ESP) cheats, which reveal hidden in-game\ninformation such as enemy locations, are difficult to detect because their\neffects are not directly observable in player behavior. The lack of observable\nevidence makes it difficult to collect reliably labeled data, which is\nessential for training effective anti-cheat systems. Furthermore, cheaters\noften adapt their behavior by limiting or disguising their cheat usage, which\nfurther complicates detection and detector development. To address these\nchallenges, we propose a simulation framework for controlled modeling of ESP\ncheaters, non-cheaters, and trajectory-based detectors. We model cheaters and\nnon-cheaters as reinforcement learning agents with different levels of\nobservability, while detectors classify their behavioral trajectories. Next, we\nformulate the interaction between the cheater and the detector as an\nadversarial game, allowing both players to co-adapt over time. To reflect\nrealistic cheater strategies, we introduce a structured cheater model that\ndynamically switches between cheating and non-cheating behaviors based on\ndetection risk. Experiments demonstrate that our framework successfully\nsimulates adaptive cheater behaviors that strategically balance reward\noptimization and detection evasion. This work provides a controllable and\nextensible platform for studying adaptive cheating behaviors and developing\neffective cheat detectors."
    },
    {
        "date": "2025-09",
        "title": "Skeleton-based Robust Registration Framework for Corrupted 3D Point Clouds",
        "author": "Yongqiang Wang, Weigang Li, Wenping Liu, Zhiqiang Tian, and Jinling Li",
        "link": "http://arxiv.org/abs/2509.24273v1",
        "abstract": "Point cloud registration is fundamental in 3D vision applications, including\nautonomous driving, robotics, and medical imaging, where precise alignment of\nmultiple point clouds is essential for accurate environment reconstruction.\nHowever, real-world point clouds are often affected by sensor limitations,\nenvironmental noise, and preprocessing errors, making registration challenging\ndue to density distortions, noise contamination, and geometric deformations.\nExisting registration methods rely on direct point matching or surface feature\nextraction, which are highly susceptible to these corruptions and lead to\nreduced alignment accuracy. To address these challenges, a skeleton-based\nrobust registration framework is presented, which introduces a\ncorruption-resilient skeletal representation to improve registration robustness\nand accuracy. The framework integrates skeletal structures into the\nregistration process and combines the transformations obtained from both the\ncorrupted point cloud alignment and its skeleton alignment to achieve optimal\nregistration. In addition, a distribution distance loss function is designed to\nenforce the consistency between the source and target skeletons, which\nsignificantly improves the registration performance. This framework ensures\nthat the alignment considers both the original local geometric features and the\nglobal stability of the skeleton structure, resulting in robust and accurate\nregistration results. Experimental evaluations on diverse corrupted datasets\ndemonstrate that SRRF consistently outperforms state-of-the-art registration\nmethods across various corruption scenarios, including density distortions,\nnoise contamination, and geometric deformations. The results confirm the\nrobustness of SRRF in handling corrupted point clouds, making it a potential\napproach for 3D perception tasks in real-world scenarios."
    },
    {
        "date": "2025-09",
        "title": "When MCP Servers Attack: Taxonomy, Feasibility, and Mitigation",
        "author": "Weibo Zhao, Jiahao Liu, Bonan Ruan, Shaofei Li, and Zhenkai Liang",
        "link": "http://arxiv.org/abs/2509.24272v1",
        "abstract": "Model Context Protocol (MCP) servers enable AI applications to connect to\nexternal systems in a plug-and-play manner, but their rapid proliferation also\nintroduces severe security risks. Unlike mature software ecosystems with\nrigorous vetting, MCP servers still lack standardized review mechanisms, giving\nadversaries opportunities to distribute malicious implementations. Despite this\npressing risk, the security implications of MCP servers remain underexplored.\nTo address this gap, we present the first systematic study that treats MCP\nservers as active threat actors and decomposes them into core components to\nexamine how adversarial developers can implant malicious intent. Specifically,\nwe investigate three research questions: (i) what types of attacks malicious\nMCP servers can launch, (ii) how vulnerable MCP hosts and Large Language Models\n(LLMs) are to these attacks, and (iii) how feasible it is to carry out MCP\nserver attacks in practice. Our study proposes a component-based taxonomy\ncomprising twelve attack categories. For each category, we develop\nProof-of-Concept (PoC) servers and demonstrate their effectiveness across\ndiverse real-world host-LLM settings. We further show that attackers can\ngenerate large numbers of malicious servers at virtually no cost. We then test\nstate-of-the-art scanners on the generated servers and found that existing\ndetection approaches are insufficient. These findings highlight that malicious\nMCP servers are easy to implement, difficult to detect with current tools, and\ncapable of causing concrete damage to AI agent systems. Addressing this threat\nrequires coordinated efforts among protocol designers, host developers, LLM\nproviders, and end users to build a more secure and resilient MCP ecosystem."
    },
    {
        "date": "2025-09",
        "title": "AdvChain: Adversarial Chain-of-Thought Tuning for Robust Safety Alignment of Large Reasoning Models",
        "author": "Zihao Zhu, Xinyu Wu, Gehan Hu, Siwei Lyu, Ke Xu, and Baoyuan Wu",
        "link": "http://arxiv.org/abs/2509.24269v1",
        "abstract": "Large Reasoning Models (LRMs) have demonstrated remarkable capabilities in\ncomplex problem-solving through Chain-of-Thought (CoT) reasoning. However, the\nmulti-step nature of CoT introduces new safety challenges that extend beyond\nconventional language model alignment. We identify a failure mode in current\nsafety CoT tuning methods: the \\textit{snowball effect}, where minor reasoning\ndeviations progressively amplify throughout the thought process, leading to\neither harmful compliance or excessive refusal. This effect stems from models\nbeing trained to imitate perfect reasoning scripts without learning to\nself-correct. To address this limitation, we propose AdvChain, an alignment\nparadigm that teaches models dynamic self-correction through adversarial CoT\ntuning. Our method involves constructing a dataset containing\nTemptation-Correction and Hesitation-Correction samples, where models learn to\nrecover from harmful reasoning drifts and unnecessary cautions. Extensive\nexperiments show that AdvChain significantly enhances robustness against\njailbreak attacks and CoT hijacking while substantially reducing over-refusal\non benign prompts, achieving a superior safety-utility balance without\ncompromising reasoning capabilities. Our work establishes a new direction for\nbuilding more robust and reliable reasoning models."
    },
    {
        "date": "2025-09",
        "title": "PROFusion: Robust and Accurate Dense Reconstruction via Camera Pose Regression and Optimization",
        "author": "Siyan Dong, Zijun Wang, Lulu Cai, Yi Ma, and Yanchao Yang",
        "link": "http://arxiv.org/abs/2509.24236v1",
        "abstract": "Real-time dense scene reconstruction during unstable camera motions is\ncrucial for robotics, yet current RGB-D SLAM systems fail when cameras\nexperience large viewpoint changes, fast motions, or sudden shaking. Classical\noptimization-based methods deliver high accuracy but fail with poor\ninitialization during large motions, while learning-based approaches provide\nrobustness but lack sufficient accuracy for dense reconstruction. We address\nthis challenge through a combination of learning-based initialization with\noptimization-based refinement. Our method employs a camera pose regression\nnetwork to predict metric-aware relative poses from consecutive RGB-D frames,\nwhich serve as reliable starting points for a randomized optimization algorithm\nthat further aligns depth images with the scene geometry. Extensive experiments\ndemonstrate promising results: our approach outperforms the best competitor on\nchallenging benchmarks, while maintaining comparable accuracy on stable motion\nsequences. The system operates in real-time, showcasing that combining simple\nand principled techniques can achieve both robustness for unstable motions and\naccuracy for dense reconstruction. Project page:\nhttps://github.com/siyandong/PROFusion."
    },
    {
        "date": "2025-09",
        "title": "MAESTRO : Adaptive Sparse Attention and Robust Learning for Multimodal Dynamic Time Series",
        "author": "Payal Mohapatra, Yueyuan Sui, Akash Pandey, Stephen Xia, and Qi Zhu",
        "link": "http://arxiv.org/abs/2509.25278v1",
        "abstract": "From clinical healthcare to daily living, continuous sensor monitoring across\nmultiple modalities has shown great promise for real-world intelligent\ndecision-making but also faces various challenges. In this work, we introduce\nMAESTRO, a novel framework that overcomes key limitations of existing\nmultimodal learning approaches: (1) reliance on a single primary modality for\nalignment, (2) pairwise modeling of modalities, and (3) assumption of complete\nmodality observations. These limitations hinder the applicability of these\napproaches in real-world multimodal time-series settings, where primary\nmodality priors are often unclear, the number of modalities can be large\n(making pairwise modeling impractical), and sensor failures often result in\narbitrary missing observations. At its core, MAESTRO facilitates dynamic intra-\nand cross-modal interactions based on task relevance, and leverages symbolic\ntokenization and adaptive attention budgeting to construct long multimodal\nsequences, which are processed via sparse cross-modal attention. The resulting\ncross-modal tokens are routed through a sparse Mixture-of-Experts (MoE)\nmechanism, enabling black-box specialization under varying modality\ncombinations. We evaluate MAESTRO against 10 baselines on four diverse datasets\nspanning three applications, and observe average relative improvements of 4%\nand 8% over the best existing multimodal and multivariate approaches,\nrespectively, under complete observations. Under partial observations -- with\nup to 40% of missing modalities -- MAESTRO achieves an average 9% improvement.\nFurther analysis also demonstrates the robustness and efficiency of MAESTRO's\nsparse, modality-aware design for learning from dynamic time series."
    },
    {
        "date": "2025-09",
        "title": "Latent Collective Preference Optimization: A General Framework for Robust LLM Alignment",
        "author": "Xiaoyang Cao, Zelai Xu, Mo Guang, Kaiwen Long, Michiel A. Bakker, Yu Wang, and Chao Yu",
        "link": "http://arxiv.org/abs/2509.24159v2",
        "abstract": "Standard human preference-based alignment methods, such as Reinforcement\nLearning from Human Feedback (RLHF), are a cornerstone technology for aligning\nLarge Language Models (LLMs) with human values. However, these methods are all\nunderpinned by a critical, yet flawed assumption: human preferences are\nhomogeneous (representing a single, unified preference) and the collected data\nis noiseless (free from error). In reality, neither is true since human\npreference is pluralistic and annotators can make mistakes. This creates a\ndiscrepancy between the recorded data and the ground-truth preferences, which\ncan misguide the model and degrade its performance. To address this challenge,\nwe introduce Latent Collective Preference Optimization (LCPO). LCPO leverages\nan Expectation-Maximization (EM) algorithm to learn the latent collective\nconsensus from noisy data. It operates by inferring the correctness of each\npreference label and using this probability as an adaptive weight to\nre-calibrate each data point's contribution to the training loss, thereby\nmitigating noise. We generalize this approach by establishing a theoretical\nlink between arbitrary preference losses and their corresponding probabilistic\nmodels, elevating LCPO from a specific algorithm to a general framework for\nrobust preference alignment. Theoretically, we prove that under the condition\nof a perfectly calibrated model, LCPO is guaranteed to converge to the true\nnoise level of the dataset. Our experiments demonstrate LCPO's effectiveness as\na general framework, consistently enhancing four state-of-the-art alignment\nalgorithms (DPO, IPO, SimPO, and CPO). When applied to Mistral and Llama 3\nmodels, the LCPO-enhanced methods achieve substantial win rate gains on\nAlpacaEval 2 and Arena-Hard, with improvements of up to 7.0% on both\nbenchmarks."
    },
    {
        "date": "2025-09",
        "title": "Generalist Scanner Meets Specialist Locator: A Synergistic Coarse-to-Fine Framework for Robust GUI Grounding",
        "author": "Zhecheng Li, Guoxian Song, Yiwei Wang, Zhen Xiong, Junsong Yuan, and Yujun Cai",
        "link": "http://arxiv.org/abs/2509.24133v1",
        "abstract": "Grounding natural language queries in graphical user interfaces (GUIs)\npresents a challenging task that requires models to comprehend diverse UI\nelements across various applications and systems, while also accurately\npredicting the spatial coordinates for the intended operation. To tackle this\nproblem, we propose GMS: Generalist Scanner Meets Specialist Locator, a\nsynergistic coarse-to-fine framework that effectively improves GUI grounding\nperformance. GMS leverages the complementary strengths of general\nvision-language models (VLMs) and small, task-specific GUI grounding models by\nassigning them distinct roles within the framework. Specifically, the general\nVLM acts as a 'Scanner' to identify potential regions of interest, while the\nfine-tuned grounding model serves as a 'Locator' that outputs precise\ncoordinates within these regions. This design is inspired by how humans perform\nGUI grounding, where the eyes scan the interface and the brain focuses on\ninterpretation and localization. Our whole framework consists of five stages\nand incorporates hierarchical search with cross-modal communication to achieve\npromising prediction results. Experimental results on the ScreenSpot-Pro\ndataset show that while the 'Scanner' and 'Locator' models achieve only $2.0\\%$\nand $3.7\\%$ accuracy respectively when used independently, their integration\nwithin GMS framework yields an overall accuracy of $35.7\\%$, representing a $10\n\\times$ improvement. Additionally, GMS significantly outperforms other strong\nbaselines under various settings, demonstrating its robustness and potential\nfor general-purpose GUI grounding."
    },
    {
        "date": "2025-09",
        "title": "RPG360: Robust 360 Depth Estimation with Perspective Foundation Models and Graph Optimization",
        "author": "Dongki Jung, Jaehoon Choi, Yonghan Lee, and Dinesh Manocha",
        "link": "http://arxiv.org/abs/2509.23991v1",
        "abstract": "The increasing use of 360 images across various domains has emphasized the\nneed for robust depth estimation techniques tailored for omnidirectional\nimages. However, obtaining large-scale labeled datasets for 360 depth\nestimation remains a significant challenge. In this paper, we propose RPG360, a\ntraining-free robust 360 monocular depth estimation method that leverages\nperspective foundation models and graph optimization. Our approach converts 360\nimages into six-face cubemap representations, where a perspective foundation\nmodel is employed to estimate depth and surface normals. To address depth scale\ninconsistencies across different faces of the cubemap, we introduce a novel\ndepth scale alignment technique using graph-based optimization, which\nparameterizes the predicted depth and normal maps while incorporating an\nadditional per-face scale parameter. This optimization ensures depth scale\nconsistency across the six-face cubemap while preserving 3D structural\nintegrity. Furthermore, as foundation models exhibit inherent robustness in\nzero-shot settings, our method achieves superior performance across diverse\ndatasets, including Matterport3D, Stanford2D3D, and 360Loc. We also demonstrate\nthe versatility of our depth estimation approach by validating its benefits in\ndownstream tasks such as feature matching 3.2 ~ 5.4% and Structure from Motion\n0.2 ~ 9.7% in AUC@5."
    },
    {
        "date": "2025-09",
        "title": "Evaluating the Robustness of Chinchilla Compute-Optimal Scaling",
        "author": "Rylan Schaeffer, Noam Levi, Andreas Kirsch, Theo Guenais, Brando Miranda, Elyas Obbad, and Sanmi Koyejo",
        "link": "http://arxiv.org/abs/2509.23963v1",
        "abstract": "Hoffman et al (2022)'s Chinchilla paper introduced the principle of\ncompute-optimal scaling, laying a foundation for future scaling of language\nmodels. In the years since, however, valid concerns about Chinchilla have been\nraised: wide confidence intervals, discrepancies between its three approaches,\nand incongruities with other scaling laws. This raises a critical question for\nthe field: Can practitioners still rely on Chinchilla's prescriptions? Our work\ndemonstrates the answer is yes. We begin by uncovering that the model\nparameters central to Chinchilla's analyses were ambiguous: three\ninterpretations are possible, with relative differences between different\ninterpretations of model parameters as high as 15.2%. We find that, perhaps\nsurprisingly, which model parameters are used for the analyses do not\nmeaningfully affect key results: the scaling law estimates and the\ncompute-optimal tokens-to-parameter ratio. Indeed, under one interpretation,\nthe tokens-to-parameter ratio becomes more constant with the target compute\nbudget. We then ask how distorted the Chinchilla model parameters could have\nbeen without meaningfully affecting the key results. By deliberately perturbing\nmodel parameters in four structured ways, we find that key Chinchilla results\nare most sensitive to additive or systematic errors, which can alter the\notherwise flat trend of the optimal tokens-to-parameter ratio, but overall,\nChinchilla's key results withstand sizable perturbations. Altogether, our\nfindings offer the field renewed confidence in Chinchilla as a durable guide\nfor scaling language models."
    },
    {
        "date": "2025-09",
        "title": "Learning-Based Testing for Deep Learning: Enhancing Model Robustness with Adversarial Input Prioritization",
        "author": "Sheikh Md Mushfiqur Rahman, and Nasir Eisty",
        "link": "http://arxiv.org/abs/2509.23961v1",
        "abstract": "Context: Deep Neural Networks (DNNs) are increasingly deployed in critical\napplications, where resilience against adversarial inputs is paramount.\nHowever, whether coverage-based or confidence-based, existing test\nprioritization methods often fail to efficiently identify the most\nfault-revealing inputs, limiting their practical effectiveness. Aims: This\nproject aims to enhance fault detection and model robustness in DNNs by\nintegrating Learning-Based Testing (LBT) with hypothesis and mutation testing\nto efficiently prioritize adversarial test cases. Methods: Our method selects a\nsubset of adversarial inputs with a high likelihood of exposing model faults,\nwithout relying on architecture-specific characteristics or formal\nverification, making it adaptable across diverse DNNs. Results: Our results\ndemonstrate that the proposed LBT method consistently surpasses baseline\napproaches in prioritizing fault-revealing inputs and accelerating fault\ndetection. By efficiently organizing test permutations, it uncovers all\npotential faults significantly faster across various datasets, model\narchitectures, and adversarial attack techniques. Conclusion: Beyond improving\nfault detection, our method preserves input diversity and provides effective\nguidance for model retraining, further enhancing robustness. These advantages\nestablish our approach as a powerful and practical solution for adversarial\ntest prioritization in real-world DNN applications."
    },
    {
        "date": "2025-09",
        "title": "Easy Turn: Integrating Acoustic and Linguistic Modalities for Robust Turn-Taking in Full-Duplex Spoken Dialogue Systems",
        "author": "Guojian Li, Chengyou Wang, Hongfei Xue, Shuiyuan Wang, Dehui Gao, Zihan Zhang, Yuke Lin, Wenjie Li, Longshuai Xiao, Zhonghua Fu, and Lei Xie",
        "link": "http://arxiv.org/abs/2509.23938v1",
        "abstract": "Full-duplex interaction is crucial for natural human-machine communication,\nyet remains challenging as it requires robust turn-taking detection to decide\nwhen the system should speak, listen, or remain silent. Existing solutions\neither rely on dedicated turn-taking models, most of which are not\nopen-sourced. The few available ones are limited by their large parameter size\nor by supporting only a single modality, such as acoustic or linguistic.\nAlternatively, some approaches finetune LLM backbones to enable full-duplex\ncapability, but this requires large amounts of full-duplex data, which remain\nscarce in open-source form. To address these issues, we propose Easy Turn, an\nopen-source, modular turn-taking detection model that integrates acoustic and\nlinguistic bimodal information to predict four dialogue turn states: complete,\nincomplete, backchannel, and wait, accompanied by the release of Easy Turn\ntrainset, a 1,145-hour speech dataset designed for training turn-taking\ndetection models. Compared to existing open-source models like TEN Turn\nDetection and Smart Turn V2, our model achieves state-of-the-art turn-taking\ndetection accuracy on our open-source Easy Turn testset. The data and model\nwill be made publicly available on GitHub."
    },
    {
        "date": "2025-09",
        "title": "Bridging the Task Gap: Multi-Task Adversarial Transferability in CLIP and Its Derivatives",
        "author": "Kuanrong Liu, Siyuan Liang, Cheng Qian, Ming Zhang, and Xiaochun Cao",
        "link": "http://arxiv.org/abs/2509.23917v1",
        "abstract": "As a general-purpose vision-language pretraining model, CLIP demonstrates\nstrong generalization ability in image-text alignment tasks and has been widely\nadopted in downstream applications such as image classification and image-text\nretrieval. However, it struggles with fine-grained tasks such as object\ndetection and semantic segmentation. While many variants aim to improve CLIP on\nthese tasks, its robustness to adversarial perturbations remains underexplored.\nUnderstanding how adversarial examples transfer across tasks is key to\nassessing CLIP's generalization limits and security risks. In this work, we\nconduct a systematic empirical analysis of the cross-task transfer behavior of\nCLIP-based models on image-text retrieval, object detection, and semantic\nsegmentation under adversarial perturbations. We find that adversarial examples\ngenerated from fine-grained tasks (e.g., object detection and semantic\nsegmentation) often exhibit stronger transfer potential than those from\ncoarse-grained tasks, enabling more effective attacks against the original CLIP\nmodel. Motivated by this observation, we propose a novel framework, Multi-Task\nAdversarial CLIP (MT-AdvCLIP), which introduces a task-aware feature\naggregation loss and generates perturbations with enhanced cross-task\ngeneralization capability. This design strengthens the attack effectiveness of\nfine-grained task models on the shared CLIP backbone. Experimental results on\nmultiple public datasets show that MT-AdvCLIP significantly improves the\nadversarial transfer success rate (The average attack success rate across\nmultiple tasks is improved by over 39%.) against various CLIP-derived models,\nwithout increasing the perturbation budget. This study reveals the transfer\nmechanism of adversarial examples in multi-task CLIP models, offering new\ninsights into multi-task robustness evaluation and adversarial example design."
    },
    {
        "date": "2025-09",
        "title": "Adversarial Versus Federated: An Adversarial Learning based Multi-Modality Cross-Domain Federated Medical Segmentation",
        "author": "You Zhou, Lijiang Chen, Shuchang Lyu, Guangxia Cui, Wenpei Bai, Zheng Zhou, Meng Li, Guangliang Cheng, Huiyu Zhou, and Qi Zhao",
        "link": "http://arxiv.org/abs/2509.23907v1",
        "abstract": "Federated learning enables collaborative training of machine learning models\namong different clients while ensuring data privacy, emerging as the mainstream\nfor breaking data silos in the healthcare domain. However, the imbalance of\nmedical resources, data corruption or improper data preservation may lead to a\nsituation where different clients possess medical images of different modality.\nThis heterogeneity poses a significant challenge for cross-domain medical image\nsegmentation within the federated learning framework. To address this\nchallenge, we propose a new Federated Domain Adaptation (FedDA) segmentation\ntraining framework. Specifically, we propose a feature-level adversarial\nlearning among clients by aligning feature maps across clients through\nembedding an adversarial training mechanism. This design can enhance the\nmodel's generalization on multiple domains and alleviate the negative impact\nfrom domain-shift. Comprehensive experiments on three medical image datasets\ndemonstrate that our proposed FedDA substantially achieves cross-domain\nfederated aggregation, endowing single modality client with cross-modality\nprocessing capabilities, and consistently delivers robust performance compared\nto state-of-the-art federated aggregation algorithms in objective and\nsubjective assessment. Our code are available at\nhttps://github.com/GGbond-study/FedDA."
    },
    {
        "date": "2025-09",
        "title": "PCRI: Measuring Context Robustness in Multimodal Models for Enterprise Applications",
        "author": "Hitesh Laxmichand Patel, Amit Agarwal, Srikant Panda, Hansa Meghwani, Karan Dua, Paul Li, Tao Sheng, Sujith Ravi, and Dan Roth",
        "link": "http://arxiv.org/abs/2509.23879v1",
        "abstract": "The reliability of Multimodal Large Language Models (MLLMs) in real-world\nsettings is often undermined by sensitivity to irrelevant or distracting visual\ncontext, an aspect not captured by existing evaluation metrics. We introduce\nthe \\textbf{Patch Context Robustness Index (PCRI)}, the first systematic and\ninterpretable score for quantifying MLLM robustness to variations in visual\ncontext granularity, measuring performance changes between localized image\npatches and full-image input.\n  Applying PCRI to 19 state-of-the-art MLLMs across 15 vision-language\nbenchmarks, we find that most leading models remain brittle to background\nnoise, with only a few, such as InternVL2-26B and Qwen2VL-72B, demonstrating\nconsistent robustness across tasks. PCRI analysis also highlights how different\nmodel architectures handle and integrate visual context, offering actionable\ndiagnostic insight for both researchers and practitioners.\n  PCRI enables rigorous comparison of context robustness, supporting principled\nmodel selection and guiding the development of future architectures and\ntraining strategies for robust, real-world deployment."
    },
    {
        "date": "2025-09",
        "title": "Taught Well Learned Ill: Towards Distillation-conditional Backdoor Attack",
        "author": "Yukun Chen, Boheng Li, Yu Yuan, Leyi Qi, Yiming Li, Tianwei Zhang, Zhan Qin, and Kui Ren",
        "link": "http://arxiv.org/abs/2509.23871v1",
        "abstract": "Knowledge distillation (KD) is a vital technique for deploying deep neural\nnetworks (DNNs) on resource-constrained devices by transferring knowledge from\nlarge teacher models to lightweight student models. While teacher models from\nthird-party platforms may undergo security verification (\\eg, backdoor\ndetection), we uncover a novel and critical threat: distillation-conditional\nbackdoor attacks (DCBAs). DCBA injects dormant and undetectable backdoors into\nteacher models, which become activated in student models via the KD process,\neven with clean distillation datasets. While the direct extension of existing\nmethods is ineffective for DCBA, we implement this attack by formulating it as\na bilevel optimization problem and proposing a simple yet effective method\n(\\ie, SCAR). Specifically, the inner optimization simulates the KD process by\noptimizing a surrogate student model, while the outer optimization leverages\noutputs from this surrogate to optimize the teacher model for implanting the\nconditional backdoor. Our SCAR addresses this complex optimization utilizing an\nimplicit differentiation algorithm with a pre-optimized trigger injection\nfunction. Extensive experiments across diverse datasets, model architectures,\nand KD techniques validate the effectiveness of our SCAR and its resistance\nagainst existing backdoor detection, highlighting a significant yet previously\noverlooked vulnerability in the KD process. Our code is available at\nhttps://github.com/WhitolfChen/SCAR."
    },
    {
        "date": "2025-09",
        "title": "FairViT-GAN: A Hybrid Vision Transformer with Adversarial Debiasing for Fair and Explainable Facial Beauty Prediction",
        "author": "Djamel Eddine Boukhari",
        "link": "http://arxiv.org/abs/2509.23859v1",
        "abstract": "Facial Beauty Prediction (FBP) has made significant strides with the\napplication of deep learning, yet state-of-the-art models often exhibit\ncritical limitations, including architectural constraints, inherent demographic\nbiases, and a lack of transparency. Existing methods, primarily based on\nConvolutional Neural Networks (CNNs), excel at capturing local texture but\nstruggle with global facial harmony, while Vision Transformers (ViTs)\neffectively model long-range dependencies but can miss fine-grained details.\nFurthermore, models trained on benchmark datasets can inadvertently learn and\nperpetuate societal biases related to protected attributes like ethnicity. To\naddress these interconnected challenges, we propose \\textbf{FairViT-GAN}, a\nnovel hybrid framework that synergistically integrates a CNN branch for local\nfeature extraction and a ViT branch for global context modeling. More\nsignificantly, we introduce an adversarial debiasing mechanism where the\nfeature extractor is explicitly trained to produce representations that are\ninvariant to protected attributes, thereby actively mitigating algorithmic\nbias. Our framework's transparency is enhanced by visualizing the distinct\nfocus of each architectural branch. Extensive experiments on the SCUT-FBP5500\nbenchmark demonstrate that FairViT-GAN not only sets a new state-of-the-art in\npredictive accuracy, achieving a Pearson Correlation of \\textbf{0.9230} and\nreducing RMSE to \\textbf{0.2650}, but also excels in fairness. Our analysis\nreveals a remarkable \\textbf{82.9\\% reduction in the performance gap} between\nethnic subgroups, with the adversary's classification accuracy dropping to\nnear-random chance (52.1\\%). We believe FairViT-GAN provides a robust,\ntransparent, and significantly fairer blueprint for developing responsible AI\nsystems for subjective visual assessment."
    },
    {
        "date": "2025-09",
        "title": "Adversarial Diffusion for Robust Reinforcement Learning",
        "author": "Daniele Foffano, Alessio Russo, and Alexandre Proutiere",
        "link": "http://arxiv.org/abs/2509.23846v1",
        "abstract": "Robustness to modeling errors and uncertainties remains a central challenge\nin reinforcement learning (RL). In this work, we address this challenge by\nleveraging diffusion models to train robust RL policies. Diffusion models have\nrecently gained popularity in model-based RL due to their ability to generate\nfull trajectories \"all at once\", mitigating the compounding errors typical of\nstep-by-step transition models. Moreover, they can be conditioned to sample\nfrom specific distributions, making them highly flexible. We leverage\nconditional sampling to learn policies that are robust to uncertainty in\nenvironment dynamics. Building on the established connection between\nConditional Value at Risk (CVaR) optimization and robust RL, we introduce\nAdversarial Diffusion for Robust Reinforcement Learning (AD-RRL). AD-RRL guides\nthe diffusion process to generate worst-case trajectories during training,\neffectively optimizing the CVaR of the cumulative return. Empirical results\nacross standard benchmarks show that AD-RRL achieves superior robustness and\nperformance compared to existing robust RL methods."
    },
    {
        "date": "2025-09",
        "title": "Influence-Guided Concolic Testing of Transformer Robustness",
        "author": "Chih-Duo Hong, Yu Wang, Yao-Chen Chang, and Fang Yu",
        "link": "http://arxiv.org/abs/2509.23806v1",
        "abstract": "Concolic testing for deep neural networks alternates concrete execution with\nconstraint solving to search for inputs that flip decisions. We present an\n{influence-guided} concolic tester for Transformer classifiers that ranks path\npredicates by SHAP-based estimates of their impact on the model output. To\nenable SMT solving on modern architectures, we prototype a solver-compatible,\npure-Python semantics for multi-head self-attention and introduce practical\nscheduling heuristics that temper constraint growth on deeper models. In a\nwhite-box study on compact Transformers under small $L_0$ budgets, influence\nguidance finds label-flip inputs more efficiently than a FIFO baseline and\nmaintains steady progress on deeper networks. Aggregating successful attack\ninstances with a SHAP-based critical decision path analysis reveals recurring,\ncompact decision logic shared across attacks. These observations suggest that\n(i) influence signals provide a useful search bias for symbolic exploration,\nand (ii) solver-friendly attention semantics paired with lightweight scheduling\nmake concolic testing feasible for contemporary Transformer models, offering\npotential utility for debugging and model auditing."
    },
    {
        "date": "2025-09",
        "title": "GroupCoOp: Group-robust Fine-tuning via Group Prompt Learning",
        "author": "Nayeong Kim, Seong Joon Oh, and Suha Kwak",
        "link": "http://arxiv.org/abs/2509.23781v1",
        "abstract": "Parameter-efficient fine-tuning (PEFT) of vision-language models (VLMs)\nexcels in various vision tasks thanks to the rich knowledge and generalization\nability of VLMs. However, recent studies revealed that such fine-tuned VLMs are\nvulnerable to spurious correlations stemming from the subgroup imbalance in the\nfine-tuning datasets. To resolve this issue, we propose Group Context\nOptimization (GroupCoOp), a simple and effective debiased fine-tuning algorithm\nthat enhances the group robustness of fine-tuned VLMs. Its key idea is to\nemploy group-specific text prompts as group representatives serving as multiple\nclassifiers for their target class. The rich semantic knowledge of the text\nencoder of VLM enables the discovery of effective group prompts even for groups\nwith a small number of training samples. Leveraging the group prompts for each\nclass addresses the issues caused by the group-imbalanced training set, such as\nthe neglect of minority groups and the scattered distribution of each class in\nthe embedding space. GroupCoOp achieved the best results on five benchmarks\nacross five CLIP architectures and occasionally outperformed prior methods that\nfine-tune the entire network, despite training only 0.016\\% of the network's\nparameters."
    },
    {
        "date": "2025-09",
        "title": "Accuracy-Robustness Trade Off via Spiking Neural Network Gradient Sparsity Trail",
        "author": "Nhan T. Luu",
        "link": "http://arxiv.org/abs/2509.23762v1",
        "abstract": "Spiking Neural Networks (SNNs) have attracted growing interest in both\ncomputational neuroscience and artificial intelligence, primarily due to their\ninherent energy efficiency and compact memory footprint. However, achieving\nadversarial robustness in SNNs, particularly for vision-related tasks, remains\na nascent and underexplored challenge. Recent studies have proposed leveraging\nsparse gradients as a form of regularization to enhance robustness against\nadversarial perturbations. In this work, we present a surprising finding: under\nspecific architectural configurations, SNNs exhibit natural gradient sparsity\nand can achieve state-of-the-art adversarial defense performance without the\nneed for any explicit regularization. Further analysis reveals a trade-off\nbetween robustness and generalization: while sparse gradients contribute to\nimproved adversarial resilience, they can impair the model's ability to\ngeneralize; conversely, denser gradients support better generalization but\nincrease vulnerability to attacks."
    },
    {
        "date": "2025-09",
        "title": "Merge Now, Regret Later: The Hidden Cost of Model Merging is Adversarial Transferability",
        "author": "Ankit Gangwal, and Aaryan Ajay Sharma",
        "link": "http://arxiv.org/abs/2509.23689v1",
        "abstract": "Model Merging (MM) has emerged as a promising alternative to multi-task\nlearning, where multiple fine-tuned models are combined, without access to\ntasks' training data, into a single model that maintains performance across\ntasks. Recent works have explored the impact of MM on adversarial attacks,\nparticularly backdoor attacks. However, none of them have sufficiently explored\nits impact on transfer attacks using adversarial examples, i.e., a black-box\nadversarial attack where examples generated for a surrogate model successfully\nmislead a target model.\n  In this work, we study the effect of MM on the transferability of adversarial\nexamples. We perform comprehensive evaluations and statistical analysis\nconsisting of 8 MM methods, 7 datasets, and 6 attack methods, sweeping over 336\ndistinct attack settings. Through it, we first challenge the prevailing notion\nof MM conferring free adversarial robustness, and show MM cannot reliably\ndefend against transfer attacks, with over 95% relative transfer attack success\nrate. Moreover, we reveal 3 key insights for machine-learning practitioners\nregarding MM and transferability for a robust system design: (1) stronger MM\nmethods increase vulnerability to transfer attacks; (2) mitigating\nrepresentation bias increases vulnerability to transfer attacks; and (3) weight\naveraging, despite being the weakest MM method, is the most vulnerable MM\nmethod to transfer attacks. Finally, we analyze the underlying reasons for this\nincreased vulnerability, and provide potential solutions to the problem. Our\nfindings offer critical insights for designing more secure systems employing\nMM."
    },
    {
        "date": "2025-09",
        "title": "Joint Hybrid Beamforming and Artificial Noise Design for Secure Multi-UAV ISAC Networks",
        "author": "Runze Dong, Buhong Wang, Cunqian Feng, Jiang Weng, Chen Han, and Jiwei Tian",
        "link": "http://arxiv.org/abs/2509.23687v1",
        "abstract": "Integrated sensing and communication (ISAC) emerges as a key enabler for\nnext-generation applications such as smart cities and autonomous systems. Its\nintegration with unmanned aerial vehicles (UAVs) unlocks new potentials for\nreliable communication and precise sensing in dynamic aerial environments.\nHowever, existing research predominantly treats UAVs as aerial base stations,\noverlooking their role as ISAC users, and fails to leverage large-scale antenna\narrays at terrestrial base stations to enhance security and spectral\nefficiency. This paper propose a secure and spectral efficient ISAC framework\nfor multi-UAV networks, and a two-stage optimization approach is developed to\njointly design hybrid beamforming (HBF), artificial noise (AN) injection, and\nUAV trajectories. Aiming at maximizing the sum secrecy rate, the first stage\nemploys Proximal Policy Optimization (PPO) to optimize digital beamformers and\ntrajectories, and the second stage decomposes the digital solution into analog\nand digital components via low-complexity matrix factorization. Simulation\nresults demonstrate the effectiveness of the proposed framework compared to\nbenchmark schemes."
    },
    {
        "date": "2025-09",
        "title": "Color-Pair Guided Robust Zero-Shot 6D Pose Estimation and Tracking of Cluttered Objects on Edge Devices",
        "author": "Xingjian Yang, and Ashis G. Banerjee",
        "link": "http://arxiv.org/abs/2509.23647v1",
        "abstract": "Robust 6D pose estimation of novel objects under challenging illumination\nremains a significant challenge, often requiring a trade-off between accurate\ninitial pose estimation and efficient real-time tracking. We present a unified\nframework explicitly designed for efficient execution on edge devices, which\nsynergizes a robust initial estimation module with a fast motion-based tracker.\nThe key to our approach is a shared, lighting-invariant color-pair feature\nrepresentation that forms a consistent foundation for both stages. For initial\nestimation, this feature facilitates robust registration between the live RGB-D\nview and the object's 3D mesh. For tracking, the same feature logic validates\ntemporal correspondences, enabling a lightweight model to reliably regress the\nobject's motion. Extensive experiments on benchmark datasets demonstrate that\nour integrated approach is both effective and robust, providing competitive\npose estimation accuracy while maintaining high-fidelity tracking even through\nabrupt pose changes."
    },
    {
        "date": "2025-09",
        "title": "DRIK: Distribution-Robust Inductive Kriging without Information Leakage",
        "author": "Chen Yang, Changhao Zhao, Chen Wang, and Jiansheng Fan",
        "link": "http://arxiv.org/abs/2509.23631v1",
        "abstract": "Inductive kriging supports high-resolution spatio-temporal estimation with\nsparse sensor networks, but conventional training-evaluation setups often\nsuffer from information leakage and poor out-of-distribution (OOD)\ngeneralization. We find that the common 2x2 spatio-temporal split allows test\ndata to influence model selection through early stopping, obscuring the true\nOOD characteristics of inductive kriging. To address this issue, we propose a\n3x3 partition that cleanly separates training, validation, and test sets,\neliminating leakage and better reflecting real-world applications. Building on\nthis redefined setting, we introduce DRIK, a Distribution-Robust Inductive\nKriging approach designed with the intrinsic properties of inductive kriging in\nmind to explicitly enhance OOD generalization, employing a three-tier strategy\nat the node, edge, and subgraph levels. DRIK perturbs node coordinates to\ncapture continuous spatial relationships, drops edges to reduce ambiguity in\ninformation flow and increase topological diversity, and adds pseudo-labeled\nsubgraphs to strengthen domain generalization. Experiments on six diverse\nspatio-temporal datasets show that DRIK consistently outperforms existing\nmethods, achieving up to 12.48% lower MAE while maintaining strong scalability."
    },
    {
        "date": "2025-09",
        "title": "Generalizable Speech Deepfake Detection via Information Bottleneck Enhanced Adversarial Alignment",
        "author": "Pu Huang, Shouguang Wang, Siya Yao, and Mengchu Zhou",
        "link": "http://arxiv.org/abs/2509.23618v1",
        "abstract": "Neural speech synthesis techniques have enabled highly realistic speech\ndeepfakes, posing major security risks. Speech deepfake detection is\nchallenging due to distribution shifts across spoofing methods and variability\nin speakers, channels, and recording conditions. We explore learning shared\ndiscriminative features as a path to robust detection and propose Information\nBottleneck enhanced Confidence-Aware Adversarial Network (IB-CAAN).\nConfidence-guided adversarial alignment adaptively suppresses attack-specific\nartifacts without erasing discriminative cues, while the information bottleneck\nremoves nuisance variability to preserve transferable features. Experiments on\nASVspoof 2019/2021, ASVspoof 5, and In-the-Wild demonstrate that IB-CAAN\nconsistently outperforms baseline and achieves state-of-the-art performance on\nmany benchmarks."
    },
    {
        "date": "2025-09",
        "title": "StolenLoRA: Exploring LoRA Extraction Attacks via Synthetic Data",
        "author": "Yixu Wang, Yan Teng, Yingchun Wang, and Xingjun Ma",
        "link": "http://arxiv.org/abs/2509.23594v1",
        "abstract": "Parameter-Efficient Fine-Tuning (PEFT) methods like LoRA have transformed\nvision model adaptation, enabling the rapid deployment of customized models.\nHowever, the compactness of LoRA adaptations introduces new safety concerns,\nparticularly their vulnerability to model extraction attacks. This paper\nintroduces a new focus of model extraction attacks named LoRA extraction that\nextracts LoRA-adaptive models based on a public pre-trained model. We then\npropose a novel extraction method called StolenLoRA which trains a substitute\nmodel to extract the functionality of a LoRA-adapted model using synthetic\ndata. StolenLoRA leverages a Large Language Model to craft effective prompts\nfor data generation, and it incorporates a Disagreement-based Semi-supervised\nLearning (DSL) strategy to maximize information gain from limited queries. Our\nexperiments demonstrate the effectiveness of StolenLoRA, achieving up to a\n96.60% attack success rate with only 10k queries, even in cross-backbone\nscenarios where the attacker and victim models utilize different pre-trained\nbackbones. These findings reveal the specific vulnerability of LoRA-adapted\nmodels to this type of extraction and underscore the urgent need for robust\ndefense mechanisms tailored to PEFT methods. We also explore a preliminary\ndefense strategy based on diversified LoRA deployments, highlighting its\npotential to mitigate such attacks."
    },
    {
        "date": "2025-09",
        "title": "RobuQ: Pushing DiTs to W1.58A2 via Robust Activation Quantization",
        "author": "Kaicheng Yang, Xun Zhang, Haotong Qin, Yucheng Lin, Kaisen Yang, Xianglong Yan, and Yulun Zhang",
        "link": "http://arxiv.org/abs/2509.23582v1",
        "abstract": "Diffusion Transformers (DiTs) have recently emerged as a powerful backbone\nfor image generation, demonstrating superior scalability and performance over\nU-Net architectures. However, their practical deployment is hindered by\nsubstantial computational and memory costs. While Quantization-Aware Training\n(QAT) has shown promise for U-Nets, its application to DiTs faces unique\nchallenges, primarily due to the sensitivity and distributional complexity of\nactivations. In this work, we identify activation quantization as the primary\nbottleneck for pushing DiTs to extremely low-bit settings. To address this, we\npropose a systematic QAT framework for DiTs, named RobuQ. We start by\nestablishing a strong ternary weight (W1.58A4) DiT baseline. Building upon\nthis, we propose RobustQuantizer to achieve robust activation quantization. Our\ntheoretical analyses show that the Hadamard transform can convert unknown\nper-token distributions into per-token normal distributions, providing a strong\nfoundation for this method. Furthermore, we propose AMPN, the first\nActivation-only Mixed-Precision Network pipeline for DiTs. This method applies\nternary weights across the entire network while allocating different activation\nprecisions to each layer to eliminate information bottlenecks. Through\nextensive experiments on unconditional and conditional image generation, our\nRobuQ framework achieves state-of-the-art performance for DiT quantization in\nsub-4-bit quantization configuration. To the best of our knowledge, RobuQ is\nthe first achieving stable and competitive image generation on large datasets\nlike ImageNet-1K with activations quantized to average 2 bits. The code and\nmodels will be available at https://github.com/racoonykc/RobuQ ."
    },
    {
        "date": "2025-09",
        "title": "Improving constraint-based discovery with robust propagation and reliable LLM priors",
        "author": "Ruiqi Lyu, Alistair Turcan, Martin Jinye Zhang, and Bryan Wilder",
        "link": "http://arxiv.org/abs/2509.23570v1",
        "abstract": "Learning causal structure from observational data is central to scientific\nmodeling and decision-making. Constraint-based methods aim to recover\nconditional independence (CI) relations in a causal directed acyclic graph\n(DAG). Classical approaches such as PC and subsequent methods orient\nv-structures first and then propagate edge directions from these seeds,\nassuming perfect CI tests and exhaustive search of separating subsets --\nassumptions often violated in practice, leading to cascading errors in the\nfinal graph. Recent work has explored using large language models (LLMs) as\nexperts, prompting sets of nodes for edge directions, and could augment edge\norientation when assumptions are not met. However, such methods implicitly\nassume perfect experts, which is unrealistic for hallucination-prone LLMs. We\npropose MosaCD, a causal discovery method that propagates edges from a\nhigh-confidence set of seeds derived from both CI tests and LLM annotations. To\nfilter hallucinations, we introduce shuffled queries that exploit LLMs'\npositional bias, retaining only high-confidence seeds. We then apply a novel\nconfidence-down propagation strategy that orients the most reliable edges\nfirst, and can be integrated with any skeleton-based discovery method. Across\nmultiple real-world graphs, MosaCD achieves higher accuracy in final graph\nconstruction than existing constraint-based methods, largely due to the\nimproved reliability of initial seeds and robust propagation strategies."
    },
    {
        "date": "2025-09",
        "title": "ReliabilityRAG: Effective and Provably Robust Defense for RAG-based Web-Search",
        "author": "Zeyu Shen, Basileal Imana, Tong Wu, Chong Xiang, Prateek Mittal, and Aleksandra Korolova",
        "link": "http://arxiv.org/abs/2509.23519v1",
        "abstract": "Retrieval-Augmented Generation (RAG) enhances Large Language Models by\ngrounding their outputs in external documents. These systems, however, remain\nvulnerable to attacks on the retrieval corpus, such as prompt injection.\nRAG-based search systems (e.g., Google's Search AI Overview) present an\ninteresting setting for studying and protecting against such threats, as\ndefense algorithms can benefit from built-in reliability signals -- like\ndocument ranking -- and represent a non-LLM challenge for the adversary due to\ndecades of work to thwart SEO.\n  Motivated by, but not limited to, this scenario, this work introduces\nReliabilityRAG, a framework for adversarial robustness that explicitly\nleverages reliability information of retrieved documents.\n  Our first contribution adopts a graph-theoretic perspective to identify a\n\"consistent majority\" among retrieved documents to filter out malicious ones.\nWe introduce a novel algorithm based on finding a Maximum Independent Set (MIS)\non a document graph where edges encode contradiction. Our MIS variant\nexplicitly prioritizes higher-reliability documents and provides provable\nrobustness guarantees against bounded adversarial corruption under natural\nassumptions. Recognizing the computational cost of exact MIS for large\nretrieval sets, our second contribution is a scalable weighted sample and\naggregate framework. It explicitly utilizes reliability information, preserving\nsome robustness guarantees while efficiently handling many documents.\n  We present empirical results showing ReliabilityRAG provides superior\nrobustness against adversarial attacks compared to prior methods, maintains\nhigh benign accuracy, and excels in long-form generation tasks where prior\nrobustness-focused methods struggled. Our work is a significant step towards\nmore effective, provably robust defenses against retrieved corpus corruption in\nRAG."
    },
    {
        "date": "2025-09",
        "title": "Robust Multi-Modal Face Anti-Spoofing with Domain Adaptation: Tackling Missing Modalities, Noisy Pseudo-Labels, and Model Degradation",
        "author": "Ming-Tsung Hsu, Fang-Yu Hsu, Yi-Ting Lin, Kai-Heng Chien, Jun-Ren Chen, Cheng-Hsiang Su, Yi-Chen Ou, Chiou-Ting Hsu, and Pei-Kai Huang",
        "link": "http://arxiv.org/abs/2509.23475v1",
        "abstract": "Recent multi-modal face anti-spoofing (FAS) methods have investigated the\npotential of leveraging multiple modalities to distinguish live and spoof\nfaces. However, pre-adapted multi-modal FAS models often fail to detect unseen\nattacks from new target domains. Although a more realistic domain adaptation\n(DA) scenario has been proposed for single-modal FAS to learn specific spoof\nattacks during inference, DA remains unexplored in multi-modal FAS methods. In\nthis paper, we propose a novel framework, MFAS-DANet, to address three major\nchallenges in multi-modal FAS under the DA scenario: missing modalities, noisy\npseudo labels, and model degradation. First, to tackle the issue of missing\nmodalities, we propose extracting complementary features from other modalities\nto substitute missing modality features or enhance existing ones. Next, to\nreduce the impact of noisy pseudo labels during model adaptation, we propose\nderiving reliable pseudo labels by leveraging prediction uncertainty across\ndifferent modalities. Finally, to prevent model degradation, we design an\nadaptive mechanism that decreases the loss weight during unstable adaptations\nand increasing it during stable ones. Extensive experiments demonstrate the\neffectiveness and state-of-the-art performance of our proposed MFAS-DANet."
    },
    {
        "date": "2025-09",
        "title": "3DPCNet: Pose Canonicalization for Robust Viewpoint-Invariant 3D Kinematic Analysis from Monocular RGB cameras",
        "author": "Tharindu Ekanayake, Constantino \u00c1lvarez Casado, and Miguel Bordallo L\u00f3pez",
        "link": "http://arxiv.org/abs/2509.23455v1",
        "abstract": "Monocular 3D pose estimators produce camera-centered skeletons, creating\nview-dependent kinematic signals that complicate comparative analysis in\napplications such as health and sports science. We present 3DPCNet, a compact,\nestimator-agnostic module that operates directly on 3D joint coordinates to\nrectify any input pose into a consistent, body-centered canonical frame. Its\nhybrid encoder fuses local skeletal features from a graph convolutional network\nwith global context from a transformer via a gated cross-attention mechanism.\nFrom this representation, the model predicts a continuous 6D rotation that is\nmapped to an $SO(3)$ matrix to align the pose. We train the model in a\nself-supervised manner on the MM-Fi dataset using synthetically rotated poses,\nguided by a composite loss ensuring both accurate rotation and pose\nreconstruction. On the MM-Fi benchmark, 3DPCNet reduces the mean rotation error\nfrom over 20$^{\\circ}$ to 3.4$^{\\circ}$ and the Mean Per Joint Position Error\nfrom ~64 mm to 47 mm compared to a geometric baseline. Qualitative evaluations\non the TotalCapture dataset further demonstrate that our method produces\nacceleration signals from video that show strong visual correspondence to\nground-truth IMU sensor data, confirming that our module removes viewpoint\nvariability to enable physically plausible motion analysis."
    },
    {
        "date": "2025-09",
        "title": "AudioFuse: Unified Spectral-Temporal Learning via a Hybrid ViT-1D CNN Architecture for Robust Phonocardiogram Classification",
        "author": "Md. Saiful Bari Siddiqui, and Utsab Saha",
        "link": "http://arxiv.org/abs/2509.23454v1",
        "abstract": "Biomedical audio signals, such as phonocardiograms (PCG), are inherently\nrhythmic and contain diagnostic information in both their spectral (tonal) and\ntemporal domains. Standard 2D spectrograms provide rich spectral features but\ncompromise the phase information and temporal precision of the 1D waveform. We\npropose AudioFuse, an architecture that simultaneously learns from both\ncomplementary representations to classify PCGs. To mitigate the overfitting\nrisk common in fusion models, we integrate a custom, wide-and-shallow Vision\nTransformer (ViT) for spectrograms with a shallow 1D CNN for raw waveforms. On\nthe PhysioNet 2016 dataset, AudioFuse achieves a state-of-the-art competitive\nROC-AUC of 0.8608 when trained from scratch, outperforming its spectrogram\n(0.8066) and waveform (0.8223) baselines. Moreover, it demonstrates superior\nrobustness to domain shift on the challenging PASCAL dataset, maintaining an\nROC-AUC of 0.7181 while the spectrogram baseline collapses (0.4873). Fusing\ncomplementary representations thus provides a strong inductive bias, enabling\nthe creation of efficient, generalizable classifiers without requiring\nlarge-scale pre-training."
    },
    {
        "date": "2025-09",
        "title": "Flow Matching for Robust Simulation-Based Inference under Model Misspecification",
        "author": "Pierre-Louis Ruhlmann, Pedro L. C. Rodrigues, Michael Arbel, and Florence Forbes",
        "link": "http://arxiv.org/abs/2509.23385v2",
        "abstract": "Simulation-based inference (SBI) is transforming experimental sciences by\nenabling parameter estimation in complex non-linear models from simulated data.\nA persistent challenge, however, is model misspecification: simulators are only\napproximations of reality, and mismatches between simulated and real data can\nyield biased or overconfident posteriors. We address this issue by introducing\nFlow Matching Corrected Posterior Estimation (FMCPE), a framework that\nleverages the flow matching paradigm to refine simulation-trained posterior\nestimators using a small set of real calibration samples. Our approach proceeds\nin two stages: first, a posterior approximator is trained on abundant simulated\ndata; second, flow matching transports its predictions toward the true\nposterior supported by real observations, without requiring explicit knowledge\nof the misspecification. This design enables FMCPE to combine the scalability\nof SBI with robustness to distributional shift. Across synthetic benchmarks and\nreal-world datasets, we show that our proposal consistently mitigates the\neffects of misspecification, delivering improved inference accuracy and\nuncertainty calibration compared to standard SBI baselines, while remaining\ncomputationally efficient."
    },
    {
        "date": "2025-09",
        "title": "Dual-Space Smoothness for Robust and Balanced LLM Unlearning",
        "author": "Han Yan, Zheyuan Liu, and Meng Jiang",
        "link": "http://arxiv.org/abs/2509.23362v1",
        "abstract": "With the rapid advancement of large language models, Machine Unlearning has\nemerged to address growing concerns around user privacy, copyright\ninfringement, and overall safety. Yet state-of-the-art (SOTA) unlearning\nmethods often suffer from catastrophic forgetting and metric imbalance, for\nexample by over-optimizing one objective (e.g., unlearning effectiveness,\nutility preservation, or privacy protection) at the expense of others. In\naddition, small perturbations in the representation or parameter space can be\nexploited by relearn and jailbreak attacks. To address these challenges, we\npropose PRISM, a unified framework that enforces dual-space smoothness in\nrepresentation and parameter spaces to improve robustness and balance\nunlearning metrics. PRISM consists of two smoothness optimization stages: (i) a\nrepresentation space stage that employs a robustly trained probe to defend\nagainst jailbreak attacks, and (ii) a parameter-space stage that decouples\nretain-forget gradient conflicts, reduces imbalance, and smooths the parameter\nspace to mitigate relearning attacks. Extensive experiments on WMDP and MUSE,\nacross conversational-dialogue and continuous-text settings, show that PRISM\noutperforms SOTA baselines under multiple attacks while achieving a better\nbalance among key metrics."
    },
    {
        "date": "2025-09",
        "title": "Robust Fine-Tuning from Non-Robust Pretrained Models: Mitigating Suboptimal Transfer With Adversarial Scheduling",
        "author": "Jonas Ngnaw\u00e9, Maxime Heuillet, Sabyasachi Sahoo, Yann Pequignot, Ola Ahmad, Audrey Durand, Fr\u00e9d\u00e9ric Precioso, and Christian Gagn\u00e9",
        "link": "http://arxiv.org/abs/2509.23325v1",
        "abstract": "Fine-tuning pretrained models is a standard and effective workflow in modern\nmachine learning. However, robust fine-tuning (RFT), which aims to\nsimultaneously achieve adaptation to a downstream task and robustness to\nadversarial examples, remains challenging. Despite the abundance of non-robust\npretrained models in open-source repositories, their potential for RFT is less\nunderstood. We address this knowledge gap by systematically examining RFT from\nsuch non-robust models. Our experiments reveal that fine-tuning non-robust\nmodels with a robust objective, even under small perturbations, can lead to\npoor performance, a phenomenon that we dub \\emph{suboptimal transfer}. In\nchallenging scenarios (eg, difficult tasks, high perturbation), the resulting\nperformance can be so low that it may be considered a transfer failure. We find\nthat fine-tuning using a robust objective impedes task adaptation at the\nbeginning of training and eventually prevents optimal transfer. However, we\npropose a novel heuristic, \\emph{Epsilon-Scheduling}, a schedule over\nperturbation strength used during training that promotes optimal transfer.\nAdditionally, we introduce \\emph{expected robustness}, a metric that captures\nperformance across a range of perturbations, providing a more comprehensive\nevaluation of the accuracy-robustness trade-off for diverse models at test\ntime. Extensive experiments on a wide range of configurations (six pretrained\nmodels and five datasets) show that \\emph{Epsilon-Scheduling} successfully\nprevents \\emph{suboptimal transfer} and consistently improves expected\nrobustness."
    },
    {
        "date": "2025-09",
        "title": "ICS-SimLab: A Containerized Approach for Simulating Industrial Control Systems for Cyber Security Research",
        "author": "Jaxson Brown, Duc-Son Pham, Sie-Teng Soh, Foad Motalebi, Sivaraman Eswaran, and Mahathir Almashor",
        "link": "http://arxiv.org/abs/2509.23305v1",
        "abstract": "Industrial Control Systems (ICSs) are complex interconnected systems used to\nmanage process control within industrial environments, such as chemical\nprocessing plants and water treatment facilities. As the modern industrial\nenvironment moves towards Internet-facing services, ICSs face an increased risk\nof attacks that necessitates ICS-specific Intrusion Detection Systems (IDS).\nThe development of such IDS relies significantly on a simulated testbed as it\nis unrealistic and sometimes hazardous to utilize an operational control\nsystem. Whilst some testbeds have been proposed, they often use a limited\nselection of virtual ICS simulations to test and verify cyber security\nsolutions. There is a lack of investigation done on developing systems that can\nefficiently simulate multiple ICS architectures. Currently, the trend within\nresearch involves developing security solutions on just one ICS simulation,\nwhich can result in bias to its specific architecture. We present ICS-SimLab,\nan end-to-end software suite that utilizes Docker containerization technology\nto create a highly configurable ICS simulation environment. This software\nframework enables researchers to rapidly build and customize different ICS\nenvironments, facilitating the development of security solutions across\ndifferent systems that adhere to the Purdue Enterprise Reference Architecture.\nTo demonstrate its capability, we present three virtual ICS simulations: a\nsolar panel smart grid, a water bottle filling facility, and a system of\nintelligent electronic devices. Furthermore, we run cyber-attacks on these\nsimulations and construct a dataset of recorded malicious and benign network\ntraffic to be used for IDS development."
    },
    {
        "date": "2025-09",
        "title": "NanoFlux: Adversarial Dual-LLM Evaluation and Distillation For Multi-Domain Reasoning",
        "author": "Raviteja Anantha, Soheil Hor, Teodor Nicola Antoniu, and Layne C. Price",
        "link": "http://arxiv.org/abs/2509.23252v1",
        "abstract": "We present NanoFlux, a novel adversarial framework for generating targeted\ntraining data to improve LLM reasoning, where adversarially-generated datasets\ncontaining fewer than 200 examples outperform conventional fine-tuning\napproaches. The framework employs a competitive dynamic between models\nalternating as Attacker and Defender, supervised by a tool-augmented Judge,\nsynthesizing multi-step questions with explanatory annotations that target\nspecific reasoning capabilities. Fine-tuning a 4B-parameter model on\nNanoFlux-generated data yields performance gains across diverse domains\ncompared to full-benchmark fine-tuning: +5.9% on mathematical reasoning\n(GSMHard), +3.6% on scientific reasoning (GenomeBench), and +16.6% on medical\nreasoning (MultiMedQA), while reducing computational requirements by 3-14x.\nAblation studies reveal a non-monotonic relationship between dataset\ncharacteristics and model performance, uncovering domain-specific optimal\npoints for question complexity and reasoning quality. NanoFlux automates\ntraining data generation through embedding-based novelty filtering,\ntool-augmented evaluation, and multi-hop reasoning, suggesting that future\nmodel improvements may lie in the intelligent synthesis of small, precisely\ntargeted training datasets."
    },
    {
        "date": "2025-09",
        "title": "$p$-less Sampling: A Robust Hyperparameter-Free Approach for LLM Decoding",
        "author": "Runyan Tan, Shuang Wu, and Phillip Howard",
        "link": "http://arxiv.org/abs/2509.23234v2",
        "abstract": "Obtaining high-quality outputs from Large Language Models (LLMs) often\ndepends upon the choice of a sampling-based decoding strategy to\nprobabilistically choose the next token at each generation step. While a\nvariety of such sampling methods have been proposed, their performance can be\nsensitive to the selection of hyperparameters which may require different\nsettings depending upon the generation task and temperature configuration. In\nthis work, we introduce $p$-less sampling: an information-theoretic approach to\nsampling which dynamically sets a truncation threshold at each decoding step\nbased on the entire token probability distribution. Unlike existing methods,\n$p$-less sampling has no hyperparameters and consistently produces high-quality\noutputs as temperature increases. We provide theoretical perspectives on\n$p$-less sampling to ground our proposed method and conduct experiments to\nempirically validate its effectiveness across a range of math, logical\nreasoning, and creative writing tasks. Our results demonstrate how $p$-less\nsampling consistently outperforms existing sampling approaches while exhibiting\nmuch less degradation in text quality at higher temperature values. We further\nshow how $p$-less achieves greater inference-time efficiency than alternative\nmethods through lower average token sampling times and shorter generation\nlengths, without sacrificing accuracy. Finally, we provide analyses to\nhighlight the benefits of $p$-less through qualitative examples, case studies,\nand diversity assessments."
    },
    {
        "date": "2025-09",
        "title": "Real-World Transferable Adversarial Attack on Face-Recognition Systems",
        "author": "Andrey Kaznacheev, Matvey Mikhalchuk, Andrey Kuznetsov, Aleksandr Petiushko, and Anton Razzhigaev",
        "link": "http://arxiv.org/abs/2509.23198v1",
        "abstract": "Adversarial attacks on face recognition (FR) systems pose a significant\nsecurity threat, yet most are confined to the digital domain or require\nwhite-box access. We introduce GaP (Gaussian Patch), a novel method to generate\na universal, physically transferable adversarial patch under a strict black-box\nsetting. Our approach uses a query-efficient, zero-order greedy algorithm to\niteratively construct a symmetric, grayscale pattern for the forehead. The\npatch is optimized by successively adding Gaussian blobs, guided only by the\ncosine similarity scores from a surrogate FR model to maximally degrade\nidentity recognition. We demonstrate that with approximately 10,000 queries to\na black-box ArcFace model, the resulting GaP achieves a high attack success\nrate in both digital and real-world physical tests. Critically, the attack\nshows strong transferability, successfully deceiving an entirely unseen FaceNet\nmodel. Our work highlights a practical and severe vulnerability, proving that\nrobust, transferable attacks can be crafted with limited knowledge of the\ntarget system."
    },
    {
        "date": "2025-09",
        "title": "CoSIFL: Collaborative Secure and Incentivized Federated Learning with Differential Privacy",
        "author": "Zhanhong Xie, Meifan Zhang, and Lihua Yin",
        "link": "http://arxiv.org/abs/2509.23190v1",
        "abstract": "Federated learning (FL) has emerged as a promising paradigm for collaborative\nmodel training while preserving data locality. However, it still faces\nchallenges from malicious or compromised clients, as well as difficulties in\nincentivizing participants to contribute high-quality data under strict privacy\nrequirements. Motivated by these considerations, we propose CoSIFL, a novel\nframework that integrates proactive alarming for robust security and local\ndifferential privacy (LDP) for inference attacks, together with a\nStackelberg-based incentive scheme to encourage client participation and data\nsharing. Specifically, CoSIFL uses an active alarming mechanism and robust\naggregation to defend against Byzantine and inference attacks, while a Tullock\ncontest-inspired incentive module rewards honest clients for both data\ncontributions and reliable alarm triggers. We formulate the interplay between\nthe server and clients as a two-stage game: in the first stage, the server\ndetermines total rewards, selects participants, and fixes global iteration\nsettings, whereas in the second stage, each client decides its mini-batch size,\nprivacy noise scale, and alerting strategy. We prove that the server-client\ngame admits a unique equilibrium, and analyze how clients' multi-dimensional\nattributes - such as non-IID degrees and privacy budgets - jointly affect\nsystem efficiency. Experimental results on standard benchmarks demonstrate that\nCoSIFL outperforms state-of-the-art solutions in improving model robustness and\nreducing total server costs, highlighting the effectiveness of our integrated\ndesign."
    },
    {
        "date": "2025-09",
        "title": "Confidence-Calibrating Regularization for Robust Brain MRI Segmentation Under Domain Shift",
        "author": "Behraj Khan, and Tahir Qasim Syed",
        "link": "http://arxiv.org/abs/2509.23176v1",
        "abstract": "The Segment Anything Model (SAM) exhibits strong zero-shot performance on\nnatural images but suffers from domain shift and overconfidence when applied to\nmedical volumes. We propose \\textbf{CalSAM}, a lightweight adaptation framework\nthat (i) reduces encoder sensitivity to domain shift via a \\emph{Feature Fisher\nInformation Penalty} (FIP) computed on 3D feature maps and (ii) penalizes\noverconfident voxel-wise errors through a \\emph{Confidence Misalignment\nPenalty} (CMP). The combined loss, \\(\\mathcal{L}_{\\mathrm{CalSAM}}\\) fine-tunes\nonly the mask decoder while keeping SAM's encoders frozen. On cross-center and\nscanner-shift evaluations, CalSAM substantially improves accuracy and\ncalibration: e.g., on the BraTS scanner split (Siemens$\\to$GE) CalSAM shows a\n$+7.4\\%$ relative improvement in $\\mathrm{DSC}$ (80.1\\% vs.\\ 74.6\\%), a\n$-26.9\\%$ reduction in $\\mathrm{HD95}$ (4.6 mm vs.\\ 6.3 mm), and a $-39.5\\%$\nreduction in $\\mathrm{ECE}$ (5.2\\% vs.\\ 8.6\\%). On ATLAS-C (motion\ncorruptions), CalSAM achieves a $+5.3\\%$ relative improvement in $\\mathrm{DSC}$\n(75.9\\%) and a $-32.6\\%$ reduction in $\\mathrm{ECE}$ (5.8\\%). Ablations show\nFIP and CMP contribute complementary gains ($p<0.01$), and the Fisher penalty\nincurs a modest $\\sim$15\\% training-time overhead. CalSAM therefore delivers\nimproved domain generalization and better-calibrated uncertainty estimates for\nbrain MRI segmentation, while retaining the computational benefits of freezing\nSAM's encoder."
    },
    {
        "date": "2025-09",
        "title": "Virus Infection Attack on LLMs: Your Poisoning Can Spread \"VIA\" Synthetic Data",
        "author": "Zi Liang, Qingqing Ye, Xuan Liu, Yanyun Wang, Jianliang Xu, and Haibo Hu",
        "link": "http://arxiv.org/abs/2509.23041v1",
        "abstract": "Synthetic data refers to artificial samples generated by models. While it has\nbeen validated to significantly enhance the performance of large language\nmodels (LLMs) during training and has been widely adopted in LLM development,\npotential security risks it may introduce remain uninvestigated. This paper\nsystematically evaluates the resilience of synthetic-data-integrated training\nparadigm for LLMs against mainstream poisoning and backdoor attacks. We reveal\nthat such a paradigm exhibits strong resistance to existing attacks, primarily\nthanks to the different distribution patterns between poisoning data and\nqueries used to generate synthetic samples. To enhance the effectiveness of\nthese attacks and further investigate the security risks introduced by\nsynthetic data, we introduce a novel and universal attack framework, namely,\nVirus Infection Attack (VIA), which enables the propagation of current attacks\nthrough synthetic data even under purely clean queries. Inspired by the\nprinciples of virus design in cybersecurity, VIA conceals the poisoning payload\nwithin a protective \"shell\" and strategically searches for optimal hijacking\npoints in benign samples to maximize the likelihood of generating malicious\ncontent. Extensive experiments on both data poisoning and backdoor attacks show\nthat VIA significantly increases the presence of poisoning content in synthetic\ndata and correspondingly raises the attack success rate (ASR) on downstream\nmodels to levels comparable to those observed in the poisoned upstream models."
    },
    {
        "date": "2025-09",
        "title": "GuardNet: Graph-Attention Filtering for Jailbreak Defense in Large Language Models",
        "author": "Javad Forough, Mohammad Maheri, and Hamed Haddadi",
        "link": "http://arxiv.org/abs/2509.23037v1",
        "abstract": "Large Language Models (LLMs) are increasingly susceptible to jailbreak\nattacks, which are adversarial prompts that bypass alignment constraints and\ninduce unauthorized or harmful behaviors. These vulnerabilities undermine the\nsafety, reliability, and trustworthiness of LLM outputs, posing critical risks\nin domains such as healthcare, finance, and legal compliance. In this paper, we\npropose GuardNet, a hierarchical filtering framework that detects and filters\njailbreak prompts prior to inference. GuardNet constructs structured graphs\nthat combine sequential links, syntactic dependencies, and attention-derived\ntoken relations to capture both linguistic structure and contextual patterns\nindicative of jailbreak behavior. It then applies graph neural networks at two\nlevels: (i) a prompt-level filter that detects global adversarial prompts, and\n(ii) a token-level filter that pinpoints fine-grained adversarial spans.\nExtensive experiments across three datasets and multiple attack settings show\nthat GuardNet substantially outperforms prior defenses. It raises prompt-level\nF$_1$ scores from 66.4\\% to 99.8\\% on LLM-Fuzzer, and from 67-79\\% to over 94\\%\non PLeak datasets. At the token level, GuardNet improves F$_1$ from 48-75\\% to\n74-91\\%, with IoU gains up to +28\\%. Despite its structural complexity,\nGuardNet maintains acceptable latency and generalizes well in cross-domain\nevaluations, making it a practical and robust defense against jailbreak threats\nin real-world LLM deployments."
    },
    {
        "date": "2025-09",
        "title": "Desensitizing for Improving Corruption Robustness in Point Cloud Classification through Adversarial Training",
        "author": "Zhiqiang Tian, Weigang Li, Chunhua Deng, Junwei Hu, Yongqiang Wang, and Wenping Liu",
        "link": "http://arxiv.org/abs/2509.23010v1",
        "abstract": "Due to scene complexity, sensor inaccuracies, and processing imprecision,\npoint cloud corruption is inevitable. Over-reliance on input features is the\nroot cause of DNN vulnerabilities. It remains unclear whether this issue exists\nin 3D tasks involving point clouds and whether reducing dependence on these\nfeatures can enhance the model's robustness to corrupted point clouds. This\nstudy attempts to answer these questions. Specifically, we quantified the\nsensitivity of the DNN to point cloud features using Shapley values and found\nthat models trained using traditional methods exhibited high sensitivity values\nfor certain features. Furthermore, under an equal pruning ratio, prioritizing\nthe pruning of highly sensitive features causes more severe damage to model\nperformance than random pruning. We propose `Desensitized Adversarial Training'\n(DesenAT), generating adversarial samples using feature desensitization and\nconducting training within a self-distillation framework, which aims to\nalleviate DNN's over-reliance on point clouds features by smoothing\nsensitivity. First, data points with high contribution components are\neliminated, and spatial transformation is used to simulate corruption scenes,\ngenerate adversarial samples, and conduct adversarial training on the model.\nNext, to compensate for information loss in adversarial samples, we use the\nself-distillation method to transfer knowledge from clean samples to\nadversarial samples, and perform adversarial training in a distillation\nmanner.Extensive experiments on ModelNet-C and PointCloud-C demonstrate show\nthat the propose method can effectively improve the robustness of the model\nwithout reducing the performance of clean data sets. This code is publicly\navailable at\n\\href{https://github.com/JerkyT/DesenAT/tree/master}{https://github.com/JerkyT/DesenAT}."
    },
    {
        "date": "2025-09",
        "title": "Creative Adversarial Testing (CAT): A Novel Framework for Evaluating Goal-Oriented Agentic AI Systems",
        "author": "Hassen Dhrif",
        "link": "http://arxiv.org/abs/2509.23006v1",
        "abstract": "Agentic AI represents a paradigm shift in enhancing the capabilities of\ngenerative AI models. While these systems demonstrate immense potential and\npower, current evaluation techniques primarily focus on assessing their\nefficacy in identifying appropriate agents, tools, and parameters. However, a\ncritical gap exists in evaluating the alignment between an Agentic AI system's\ntasks and its overarching goals. This paper introduces the Creative Adversarial\nTesting (CAT) framework, a novel approach designed to capture and analyze the\ncomplex relationship between Agentic AI tasks and the system's intended\nobjectives.\n  We validate the CAT framework through extensive simulation using synthetic\ninteraction data modeled after Alexa+ audio services, a sophisticated Agentic\nAI system that shapes the user experience for millions of users globally. This\nsynthetic data approach enables comprehensive testing of edge cases and failure\nmodes while protecting user privacy. Our results demonstrate that the CAT\nframework provides unprecedented insights into goal-task alignment, enabling\nmore effective optimization and development of Agentic AI systems."
    },
    {
        "date": "2025-09",
        "title": "Blockchain-Based Secure Online Voting Platform Ensuring Voter Anonymity, Integrity, and End-to-End Verifiability",
        "author": "Yousef Tahboub, Anthony Revilla, Jaydon Lynch, and Greg Floyd",
        "link": "http://arxiv.org/abs/2509.22965v2",
        "abstract": "Casting a ballot from a phone or laptop sounds appealing, but only if voters\ncan be confident their choice remains secret and results cannot be altered in\nthe dark. This paper proposes a hybrid blockchain-based voting model that\nstores encrypted votes on a private blockchain maintained by election\norganizers and neutral observers, while periodically anchoring hashes of these\nvotes onto a public blockchain as a tamper-evident seal. The system issues\nvoters one-time blind-signed tokens to protect anonymity, and provides receipts\nso they can confirm their vote was counted. We implemented a live prototype\nusing common web technologies (Next.js, React, Firebase) to demonstrate\nend-to-end functionality, accessibility, and cost efficiency. Our contributions\ninclude developing a working demo, a complete election workflow, a hybrid\nblockchain design, and a user-friendly interface that balances privacy,\nsecurity, transparency, and practicality. This research highlights the\nfeasibility of secure, verifiable, and scalable online voting for organizations\nranging from small groups to larger institutions."
    },
    {
        "date": "2025-09",
        "title": "Doubly-Robust LLM-as-a-Judge: Externally Valid Estimation with Imperfect Personas",
        "author": "Luke Guerdan, Justin Whitehouse, Kimberly Truong, Kenneth Holstein, and Zhiwei Steven Wu",
        "link": "http://arxiv.org/abs/2509.22957v1",
        "abstract": "As Generative AI (GenAI) systems see growing adoption, a key concern involves\nthe external validity of evaluations, or the extent to which they generalize\nfrom lab-based to real-world deployment conditions. Threats to the external\nvalidity of GenAI evaluations arise when the source sample of human raters and\nsystem outputs used to obtain a system quality estimate differs from the target\ndistribution at deployment time. In this work, we propose a doubly-robust\nestimation framework designed to address this evaluation sampling bias. Key to\nour approach is the use of \"persona\" ratings produced by prompting an LLM\nevaluator (i.e., an LLM-as-a-judge) to behave as a human rater with specific\nsociodemographic characteristics. Our doubly-robust framework combines these\ninformative yet imperfect persona ratings with human ratings obtained under\nevaluation sampling bias to produce statistically valid system quality\nestimates. In particular, we show that our approach yields valid system quality\nestimates when either (i) a model trained to predict human ratings using\npersona ratings and source data observed under sampling bias, or (ii) a\nreweighting model that corrects for sampling bias is of sufficient quality. We\nvalidate our framework theoretically and via a novel Persona Simulation\nFramework (PSF) designed to systematically manipulate persona quality and the\ndegree of evaluation sampling bias present in source data. Our work provides a\nprincipled foundation for combining imperfect persona ratings with human\nratings observed under sampling bias to obtain valid system quality estimates."
    },
    {
        "date": "2025-09",
        "title": "AntiFLipper: A Secure and Efficient Defense Against Label-Flipping Attacks in Federated Learning",
        "author": "Aashnan Rahman, Abid Hasan, Sherajul Arifin, Faisal Haque Bappy, Tahrim Hossain, Tariqul Islam, Abu Raihan Mostofa Kamal, and Md. Azam Hossain",
        "link": "http://arxiv.org/abs/2509.22873v2",
        "abstract": "Federated learning (FL) enables privacy-preserving model training by keeping\ndata decentralized. However, it remains vulnerable to label-flipping attacks,\nwhere malicious clients manipulate labels to poison the global model. Despite\ntheir simplicity, these attacks can severely degrade model performance, and\ndefending against them remains challenging. We introduce AntiFLipper, a novel\nand computationally efficient defense against multi-class label-flipping\nattacks in FL. Unlike existing methods that ensure security at the cost of high\ncomputational overhead, AntiFLipper employs a novel client-side detection\nstrategy, significantly reducing the central server's burden during\naggregation. Comprehensive empirical evaluations across multiple datasets under\ndifferent distributions demonstrate that AntiFLipper achieves accuracy\ncomparable to state-of-the-art defenses while requiring substantially fewer\ncomputational resources in server side. By balancing security and efficiency,\nAntiFLipper addresses a critical gap in existing defenses, making it\nparticularly suitable for resource-constrained FL deployments where both model\nintegrity and operational efficiency are essential."
    },
    {
        "date": "2025-09",
        "title": "Observation-Free Attacks on Online Learning to Rank",
        "author": "Sameep Chattopadhyay, Nikhil Karamchandani, and Sharayu Mohair",
        "link": "http://arxiv.org/abs/2509.22855v1",
        "abstract": "Online learning to rank (OLTR) plays a critical role in information retrieval\nand machine learning systems, with a wide range of applications in search\nengines and content recommenders. However, despite their extensive adoption,\nthe susceptibility of OLTR algorithms to coordinated adversarial attacks\nremains poorly understood. In this work, we present a novel framework for\nattacking some of the widely used OLTR algorithms. Our framework is designed to\npromote a set of target items so that they appear in the list of top-K\nrecommendations for T - o(T) rounds, while simultaneously inducing linear\nregret in the learning algorithm. We propose two novel attack strategies:\nCascadeOFA for CascadeUCB1 and PBMOFA for PBM-UCB . We provide theoretical\nguarantees showing that both strategies require only O(log T) manipulations to\nsucceed. Additionally, we supplement our theoretical analysis with empirical\nresults on real-world data."
    },
    {
        "date": "2025-09",
        "title": "Boundary on the Table: Efficient Black-Box Decision-Based Attacks for Structured Data",
        "author": "Roie Kazoom, Yuval Ratzabi, Etamar Rothstein, and Ofer Hadar",
        "link": "http://arxiv.org/abs/2509.22850v1",
        "abstract": "Adversarial robustness in structured data remains an underexplored frontier\ncompared to vision and language domains. In this work, we introduce a novel\nblack-box, decision-based adversarial attack tailored for tabular data. Our\napproach combines gradient-free direction estimation with an iterative boundary\nsearch, enabling efficient navigation of discrete and continuous feature spaces\nunder minimal oracle access. Extensive experiments demonstrate that our method\nsuccessfully compromises nearly the entire test set across diverse models,\nranging from classical machine learning classifiers to large language model\n(LLM)-based pipelines. Remarkably, the attack achieves success rates\nconsistently above 90%, while requiring only a small number of queries per\ninstance. These results highlight the critical vulnerability of tabular models\nto adversarial perturbations, underscoring the urgent need for stronger\ndefenses in real-world decision-making systems."
    },
    {
        "date": "2025-09",
        "title": "Seeing Isn't Believing: Context-Aware Adversarial Patch Synthesis via Conditional GAN",
        "author": "Roie Kazoom, Alon Goldberg, Hodaya Cohen, and Ofer Hadar",
        "link": "http://arxiv.org/abs/2509.22836v1",
        "abstract": "Adversarial patch attacks pose a severe threat to deep neural networks, yet\nmost existing approaches rely on unrealistic white-box assumptions, untargeted\nobjectives, or produce visually conspicuous patches that limit real-world\napplicability. In this work, we introduce a novel framework for fully\ncontrollable adversarial patch generation, where the attacker can freely choose\nboth the input image x and the target class y target, thereby dictating the\nexact misclassification outcome. Our method combines a generative U-Net design\nwith Grad-CAM-guided patch placement, enabling semantic-aware localization that\nmaximizes attack effectiveness while preserving visual realism. Extensive\nexperiments across convolutional networks (DenseNet-121, ResNet-50) and vision\ntransformers (ViT-B/16, Swin-B/16, among others) demonstrate that our approach\nachieves state-of-the-art performance across all settings, with attack success\nrates (ASR) and target-class success (TCS) consistently exceeding 99%.\n  Importantly, we show that our method not only outperforms prior white-box\nattacks and untargeted baselines, but also surpasses existing non-realistic\napproaches that produce detectable artifacts. By simultaneously ensuring\nrealism, targeted control, and black-box applicability-the three most\nchallenging dimensions of patch-based attacks-our framework establishes a new\nbenchmark for adversarial robustness research, bridging the gap between\ntheoretical attack strength and practical stealthiness."
    },
    {
        "date": "2025-09",
        "title": "Model Context Protocol for Vision Systems: Audit, Security, and Protocol Extensions",
        "author": "Aditi Tiwari, Akshit Bhalla, and Darshan Prasad",
        "link": "http://arxiv.org/abs/2509.22814v1",
        "abstract": "The Model Context Protocol (MCP) defines a schema bound execution model for\nagent-tool interaction, enabling modular computer vision workflows without\nretraining. To our knowledge, this is the first protocol level, deployment\nscale audit of MCP in vision systems, identifying systemic weaknesses in schema\nsemantics, interoperability, and runtime coordination. We analyze 91 publicly\nregistered vision centric MCP servers, annotated along nine dimensions of\ncompositional fidelity, and develop an executable benchmark with validators to\ndetect and categorize protocol violations. The audit reveals high prevalence of\nschema format divergence, missing runtime schema validation, undeclared\ncoordinate conventions, and reliance on untracked bridging scripts. Validator\nbased testing quantifies these failures, with schema format checks flagging\nmisalignments in 78.0 percent of systems, coordinate convention checks\ndetecting spatial reference errors in 24.6 percent, and memory scope checks\nissuing an average of 33.8 warnings per 100 executions. Security probes show\nthat dynamic and multi agent workflows exhibit elevated risks of privilege\nescalation and untyped tool connections. The proposed benchmark and validator\nsuite, implemented in a controlled testbed and to be released on GitHub,\nestablishes a reproducible framework for measuring and improving the\nreliability and security of compositional vision workflows."
    },
    {
        "date": "2025-09",
        "title": "What Do They Fix? LLM-Aided Categorization of Security Patches for Critical Memory Bugs",
        "author": "Xingyu Li, Juefei Pu, Yifan Wu, Xiaochen Zou, Shitong Zhu, Xiaochen Zou, Shitong Zhu, Qiushi Wu, Zheng Zhang, Joshua Hsu, Yue Dong, Zhiyun Qian, Kangjie Lu, Trent Jaeger, Michael De Lucia, and Srikanth V. Krishnamurthy",
        "link": "http://arxiv.org/abs/2509.22796v1",
        "abstract": "Open-source software projects are foundational to modern software ecosystems,\nwith the Linux kernel standing out as a critical exemplar due to its ubiquity\nand complexity. Although security patches are continuously integrated into the\nLinux mainline kernel, downstream maintainers often delay their adoption,\ncreating windows of vulnerability. A key reason for this lag is the difficulty\nin identifying security-critical patches, particularly those addressing\nexploitable vulnerabilities such as out-of-bounds (OOB) accesses and\nuse-after-free (UAF) bugs. This challenge is exacerbated by intentionally\nsilent bug fixes, incomplete or missing CVE assignments, delays in CVE\nissuance, and recent changes to the CVE assignment criteria for the Linux\nkernel. While fine-grained patch classification approaches exist, they exhibit\nlimitations in both coverage and accuracy. In this work, we identify previously\nunexplored opportunities to significantly improve fine-grained patch\nclassification. Specifically, by leveraging cues from commit titles/messages\nand diffs alongside appropriate code context, we develop DUALLM, a dual-method\npipeline that integrates two approaches based on a Large Language Model (LLM)\nand a fine-tuned small language model. DUALLM achieves 87.4% accuracy and an\nF1-score of 0.875, significantly outperforming prior solutions. Notably, DUALLM\nsuccessfully identified 111 of 5,140 recent Linux kernel patches as addressing\nOOB or UAF vulnerabilities, with 90 true positives confirmed by manual\nverification (many do not have clear indications in patch descriptions).\nMoreover, we constructed proof-of-concepts for two identified bugs (one UAF and\none OOB), including one developed to conduct a previously unknown control-flow\nhijack as further evidence of the correctness of the classification."
    },
    {
        "date": "2025-09",
        "title": "Your RAG is Unfair: Exposing Fairness Vulnerabilities in Retrieval-Augmented Generation via Backdoor Attacks",
        "author": "Gaurav Bagwe, Saket S. Chaturvedi, Xiaolong Ma, Xiaoyong Yuan, Kuang-Ching Wang, and Lan Zhang",
        "link": "http://arxiv.org/abs/2509.22486v1",
        "abstract": "Retrieval-augmented generation (RAG) enhances factual grounding by\nintegrating retrieval mechanisms with generative models but introduces new\nattack surfaces, particularly through backdoor attacks. While prior research\nhas largely focused on disinformation threats, fairness vulnerabilities remain\nunderexplored. Unlike conventional backdoors that rely on direct\ntrigger-to-target mappings, fairness-driven attacks exploit the interaction\nbetween retrieval and generation models, manipulating semantic relationships\nbetween target groups and social biases to establish a persistent and covert\ninfluence on content generation.\n  This paper introduces BiasRAG, a systematic framework that exposes fairness\nvulnerabilities in RAG through a two-phase backdoor attack. During the\npre-training phase, the query encoder is compromised to align the target group\nwith the intended social bias, ensuring long-term persistence. In the\npost-deployment phase, adversarial documents are injected into knowledge bases\nto reinforce the backdoor, subtly influencing retrieved content while remaining\nundetectable under standard fairness evaluations. Together, BiasRAG ensures\nprecise target alignment over sensitive attributes, stealthy execution, and\nresilience. Empirical evaluations demonstrate that BiasRAG achieves high attack\nsuccess rates while preserving contextual relevance and utility, establishing a\npersistent and evolving threat to fairness in RAG."
    },
    {
        "date": "2025-09",
        "title": "B\u00e9zier Meets Diffusion: Robust Generation Across Domains for Medical Image Segmentation",
        "author": "Chen Li, Meilong Xu, Xiaoling Hu, Weimin Lyu, and Chao Chen",
        "link": "http://arxiv.org/abs/2509.22476v1",
        "abstract": "Training robust learning algorithms across different medical imaging\nmodalities is challenging due to the large domain gap. Unsupervised domain\nadaptation (UDA) mitigates this problem by using annotated images from the\nsource domain and unlabeled images from the target domain to train the deep\nmodels. Existing approaches often rely on GAN-based style transfer, but these\nmethods struggle to capture cross-domain mappings in regions with high\nvariability. In this paper, we propose a unified framework, B\\'ezier Meets\nDiffusion, for cross-domain image generation. First, we introduce a\nB\\'ezier-curve-based style transfer strategy that effectively reduces the\ndomain gap between source and target domains. The transferred source images\nenable the training of a more robust segmentation model across domains.\nThereafter, using pseudo-labels generated by this segmentation model on the\ntarget domain, we train a conditional diffusion model (CDM) to synthesize\nhigh-quality, labeled target-domain images. To mitigate the impact of noisy\npseudo-labels, we further develop an uncertainty-guided score matching method\nthat improves the robustness of CDM training. Extensive experiments on public\ndatasets demonstrate that our approach generates realistic labeled images,\nsignificantly augmenting the target domain and improving segmentation\nperformance."
    },
    {
        "date": "2025-09",
        "title": "On Robustness of Vision-Language-Action Model against Multi-Modal Perturbations",
        "author": "Jianing Guo, Zhenhong Wu, Chang Tu, Yiyao Ma, Xiangqi Kong, Zhiqian Liu, Jiaming Ji, Shuning Zhang, Yuanpei Chen, Kai Chen, Xianglong Liu, Qi Dou, Yaodong Yang, Huijie Zhao, Weifeng Lv, and Simin Li",
        "link": "http://arxiv.org/abs/2510.00037v1",
        "abstract": "In Vision-Language-Action (VLA) models, robustness to real-world\nperturbations is critical for deployment. Existing methods target simple visual\ndisturbances, overlooking the broader multi-modal perturbations that arise in\nactions, instructions, environments, and observations. Here, we first evaluate\nthe robustness of mainstream VLAs under 17 perturbations across four\nmodalities. We find (1) actions as the most fragile modality, (2) Existing\nvisual-robust VLA do not gain robustness in other modality, and (3) pi0\ndemonstrates superior robustness with a diffusion-based action head. To build\nmulti-modal robust VLAs, we propose RobustVLA against perturbations in VLA\ninputs and outputs. For output robustness, we perform offline robust\noptimization against worst-case action noise that maximizes mismatch in flow\nmatching objective. This can be seen as adversarial training, label smoothing,\nand outlier penalization. For input robustness, we enforce consistent actions\nacross input variations that preserve task semantics. To account for multiple\nperturbations, we formulate robustness as a multi-armed bandit problem and\napply an upper confidence bound algorithm to automatically identify the most\nharmful noise. Experiments on LIBERO demonstrate our RobustVLA delivers\nabsolute gains over baselines of 12.6% on the pi0 backbone and 10.4% on the\nOpenVLA backbone across all 17 perturbations, achieving 50.6x faster inference\nthan existing visual-robust VLAs, and a 10.4% gain under mixed perturbations.\nOur RobustVLA is particularly effective on real-world FR5 robot with limited\ndemonstrations, showing absolute gains by 65.6% under perturbations of four\nmodalities."
    },
    {
        "date": "2025-09",
        "title": "Text Adversarial Attacks with Dynamic Outputs",
        "author": "Wenqiang Wang, Siyuan Liang, Xiao Yan, and Xiaochun Cao",
        "link": "http://arxiv.org/abs/2509.22393v1",
        "abstract": "Text adversarial attack methods are typically designed for static scenarios\nwith fixed numbers of output labels and a predefined label space, relying on\nextensive querying of the victim model (query-based attacks) or the surrogate\nmodel (transfer-based attacks). To address this gap, we introduce the Textual\nDynamic Outputs Attack (TDOA) method, which employs a clustering-based\nsurrogate model training approach to convert the dynamic-output scenario into a\nstatic single-output scenario. To improve attack effectiveness, we propose the\nfarthest-label targeted attack strategy, which selects adversarial vectors that\ndeviate most from the model's coarse-grained labels, thereby maximizing\ndisruption. We extensively evaluate TDOA on four datasets and eight victim\nmodels (e.g., ChatGPT-4o, ChatGPT-4.1), showing its effectiveness in crafting\nadversarial examples and its strong potential to compromise large language\nmodels with limited access. With a single query per text, TDOA achieves a\nmaximum attack success rate of 50.81\\%. Additionally, we find that TDOA also\nachieves state-of-the-art performance in conventional static output scenarios,\nreaching a maximum ASR of 82.68\\%. Meanwhile, by conceptualizing translation\ntasks as classification problems with unbounded output spaces, we extend the\nTDOA framework to generative settings, surpassing prior results by up to 0.64\nRDBLEU and 0.62 RDchrF."
    },
    {
        "date": "2025-09",
        "title": "Erase or Hide? Suppressing Spurious Unlearning Neurons for Robust Unlearning",
        "author": "Nakyeong Yang, Dong-Kyum Kim, Jea Kwon, Minsung Kim, Kyomin Jung, and Meeyoung Cha",
        "link": "http://arxiv.org/abs/2509.22263v1",
        "abstract": "Large language models trained on web-scale data can memorize private or\nsensitive knowledge, raising significant privacy risks. Although some\nunlearning methods mitigate these risks, they remain vulnerable to \"relearning\"\nduring subsequent training, allowing a substantial portion of forgotten\nknowledge to resurface. In this paper, we show that widely used unlearning\nmethods cause shallow alignment: instead of faithfully erasing target\nknowledge, they generate spurious unlearning neurons that amplify negative\ninfluence to hide it. To overcome this limitation, we introduce Ssiuu, a new\nclass of unlearning methods that employs attribution-guided regularization to\nprevent spurious negative influence and faithfully remove target knowledge.\nExperimental results confirm that our method reliably erases target knowledge\nand outperforms strong baselines across two practical retraining scenarios: (1)\nadversarial injection of private data, and (2) benign attack using an\ninstruction-following benchmark. Our findings highlight the necessity of robust\nand faithful unlearning methods for safe deployment of language models."
    },
    {
        "date": "2025-09",
        "title": "Secure and Efficient Access Control for Computer-Use Agents via Context Space",
        "author": "Haochen Gong, Chenxiao Li, Rui Chang, and Wenbo Shen",
        "link": "http://arxiv.org/abs/2509.22256v1",
        "abstract": "Large language model (LLM)-based computer-use agents represent a convergence\nof AI and OS capabilities, enabling natural language to control system- and\napplication-level functions. However, due to LLMs' inherent uncertainty issues,\ngranting agents control over computers poses significant security risks. When\nagent actions deviate from user intentions, they can cause irreversible\nconsequences. Existing mitigation approaches, such as user confirmation and\nLLM-based dynamic action validation, still suffer from limitations in\nusability, security, and performance. To address these challenges, we propose\nCSAgent, a system-level, static policy-based access control framework for\ncomputer-use agents. To bridge the gap between static policy and dynamic\ncontext and user intent, CSAgent introduces intent- and context-aware policies,\nand provides an automated toolchain to assist developers in constructing and\nrefining them. CSAgent enforces these policies through an optimized OS service,\nensuring that agent actions can only be executed under specific user intents\nand contexts. CSAgent supports protecting agents that control computers through\ndiverse interfaces, including API, CLI, and GUI. We implement and evaluate\nCSAgent, which successfully defends against more than 99.36% of attacks while\nintroducing only 6.83% performance overhead."
    },
    {
        "date": "2025-09",
        "title": "COMPASS: Robust Feature Conformal Prediction for Medical Segmentation Metrics",
        "author": "Matt Y. Cheung, Ashok Veeraraghavan, and Guha Balakrishnan",
        "link": "http://arxiv.org/abs/2509.22240v1",
        "abstract": "In clinical applications, the utility of segmentation models is often based\non the accuracy of derived downstream metrics such as organ size, rather than\nby the pixel-level accuracy of the segmentation masks themselves. Thus,\nuncertainty quantification for such metrics is crucial for decision-making.\nConformal prediction (CP) is a popular framework to derive such principled\nuncertainty guarantees, but applying CP naively to the final scalar metric is\ninefficient because it treats the complex, non-linear segmentation-to-metric\npipeline as a black box. We introduce COMPASS, a practical framework that\ngenerates efficient, metric-based CP intervals for image segmentation models by\nleveraging the inductive biases of their underlying deep neural networks.\nCOMPASS performs calibration directly in the model's representation space by\nperturbing intermediate features along low-dimensional subspaces maximally\nsensitive to the target metric. We prove that COMPASS achieves valid marginal\ncoverage under exchangeability and nestedness assumptions. Empirically, we\ndemonstrate that COMPASS produces significantly tighter intervals than\ntraditional CP baselines on four medical image segmentation tasks for area\nestimation of skin lesions and anatomical structures. Furthermore, we show that\nleveraging learned internal features to estimate importance weights allows\nCOMPASS to also recover target coverage under covariate shifts. COMPASS paves\nthe way for practical, metric-based uncertainty quantification for medical\nimage segmentation."
    },
    {
        "date": "2025-09",
        "title": "Learn, Check, Test -- Security Testing Using Automata Learning and Model Checking",
        "author": "Stefan Marksteiner, Mikael Sj\u00f6din, and Marjan Sirjani",
        "link": "http://arxiv.org/abs/2509.22215v1",
        "abstract": "Cyber-physical systems are part of industrial systems and critical\ninfrastructure. Therefore, they should be examined in a comprehensive manner to\nverify their correctness and security. At the same time, the complexity of such\nsystems demands such examinations to be systematic and, if possible, automated\nfor efficiency and accuracy. A method that can be useful in this context is\nmodel checking. However, this requires a model that faithfully represents the\nbehavior of the examined system. Obtaining such a model is not trivial, as many\nof these systems can be examined only in black box settings due to, e.g., long\nsupply chains or secrecy. We therefore utilize active black box learning\ntechniques to infer behavioral models in the form of Mealy machines of such\nsystems and translate them into a form that can be evaluated using a model\nchecker. To this end, we will investigate a cyber-physical systems as a black\nbox using its external communication interface. We first annotate the model\nwith propositions by mapping context information from the respective protocol\nto the model using Context-based Proposition Maps (CPMs). We gain annotated\nMealy machines that resemble Kripke structures. We then formally define a\ntemplate, to transfer the structures model checker-compatible format. We\nfurther define generic security properties based on basic security\nrequirements. Due to the used CPMs, we can instantiate these properties with a\nmeaningful context to check a specific protocol, which makes the approach\nflexible and scalable. The gained model can be easily altered to introduce\nnon-deterministic behavior (like timeouts) or faults and examined if the\nproperties still. Lastly, we demonstrate the versatility of the approach by\nproviding case studies of different communication protocols (NFC and UDS),\nchecked with the same tool chain and the same security properties."
    },
    {
        "date": "2025-09",
        "title": "Red Teaming Quantum-Resistant Cryptographic Standards: A Penetration Testing Framework Integrating AI and Quantum Security",
        "author": "Petar Radanliev",
        "link": "http://arxiv.org/abs/2509.22757v1",
        "abstract": "This study presents a structured approach to evaluating vulnerabilities\nwithin quantum cryptographic protocols, focusing on the BB84 quantum key\ndistribution method and National Institute of Standards and Technology (NIST)\napproved quantum-resistant algorithms. By integrating AI-driven red teaming,\nautomated penetration testing, and real-time anomaly detection, the research\ndevelops a framework for assessing and mitigating security risks in quantum\nnetworks. The findings demonstrate that AI can be effectively used to simulate\nadversarial attacks, probe weaknesses in cryptographic implementations, and\nrefine security mechanisms through iterative feedback. The use of automated\nexploit simulations and protocol fuzzing provides a scalable means of\nidentifying latent vulnerabilities, while adversarial machine learning\ntechniques highlight novel attack surfaces within AI-enhanced cryptographic\nprocesses. This study offers a comprehensive methodology for strengthening\nquantum security and provides a foundation for integrating AI-driven\ncybersecurity practices into the evolving quantum landscape."
    },
    {
        "date": "2025-09",
        "title": "Collusion-Driven Impersonation Attack on Channel-Resistant RF Fingerprinting",
        "author": "Zhou Xu, Guyue Li, Zhe Peng, and Aiqun Hu",
        "link": "http://arxiv.org/abs/2509.22154v1",
        "abstract": "Radio frequency fingerprint (RFF) is a promising device identification\ntechnology, with recent research shifting from robustness to security due to\ngrowing concerns over vulnerabilities. To date, while the security of RFF\nagainst basic spoofing such as MAC address tampering has been validated, its\nresilience to advanced mimicry remains unknown. To address this gap, we propose\na collusion-driven impersonation attack that achieves RF-level mimicry,\nsuccessfully breaking RFF identification systems across diverse environments.\nSpecifically, the attacker synchronizes with a colluding receiver to match the\ncentralized logarithmic power spectrum (CLPS) of the legitimate transmitter;\nonce the colluder deems the CLPS identical, the victim receiver will also\naccept the forged fingerprint, completing RF-level spoofing. Given that the\ndistribution of CLPS features is relatively concentrated and has a clear\nunderlying structure, we design a spoofed signal generation network that\nintegrates a variational autoencoder (VAE) with a multi-objective loss function\nto enhance the similarity and deceptive capability of the generated samples. We\ncarry out extensive simulations, validating cross-channel attacks in\nenvironments that incorporate standard channel variations including additive\nwhite Gaussian noise (AWGN), multipath fading, and Doppler shift. The results\nindicate that the proposed attack scheme essentially maintains a success rate\nof over 95% under different channel conditions, revealing the effectiveness of\nthis attack."
    },
    {
        "date": "2025-09",
        "title": "Joint graph entropy knowledge distillation for point cloud classification and robustness against corruptions",
        "author": "Zhiqiang Tian, Weigang Li, Junwei Hu, and Chunhua Deng",
        "link": "http://arxiv.org/abs/2509.22150v1",
        "abstract": "Classification tasks in 3D point clouds often assume that class events\n\\replaced{are }{follow }independent and identically distributed (IID), although\nthis assumption destroys the correlation between classes. This \\replaced{study\n}{paper }proposes a classification strategy, \\textbf{J}oint \\textbf{G}raph\n\\textbf{E}ntropy \\textbf{K}nowledge \\textbf{D}istillation (JGEKD), suitable for\nnon-independent and identically distributed 3D point cloud data,\n\\replaced{which }{the strategy } achieves knowledge transfer of class\ncorrelations through knowledge distillation by constructing a loss function\nbased on joint graph entropy. First\\deleted{ly}, we employ joint graphs to\ncapture add{the }hidden relationships between classes\\replaced{ and}{,}\nimplement knowledge distillation to train our model by calculating the entropy\nof add{add }graph.\\replaced{ Subsequently}{ Then}, to handle 3D point clouds\n\\deleted{that is }invariant to spatial transformations, we construct\n\\replaced{S}{s}iamese structures and develop two frameworks, self-knowledge\ndistillation and teacher-knowledge distillation, to facilitate information\ntransfer between different transformation forms of the same data. \\replaced{In\naddition}{ Additionally}, we use the above framework to achieve knowledge\ntransfer between point clouds and their corrupted forms, and increase the\nrobustness against corruption of model. Extensive experiments on ScanObject,\nModelNet40, ScanntV2\\_cls and ModelNet-C demonstrate that the proposed strategy\ncan achieve competitive results."
    },
    {
        "date": "2025-09",
        "title": "Countering adversarial evasion in regression analysis",
        "author": "David Benfield, Phan Tu Vuong, and Alain Zemkoho",
        "link": "http://arxiv.org/abs/2509.22113v1",
        "abstract": "Adversarial machine learning challenges the assumption that the underlying\ndistribution remains consistent throughout the training and implementation of a\nprediction model. In particular, adversarial evasion considers scenarios where\nadversaries adapt their data to influence particular outcomes from established\nprediction models, such scenarios arise in applications such as spam email\nfiltering, malware detection and fake-image generation, where security methods\nmust be actively updated to keep up with the ever-improving generation of\nmalicious data. Game theoretic models have been shown to be effective at\nmodelling these scenarios and hence training resilient predictors against such\nadversaries. Recent advancements in the use of pessimistic bilevel optimsiation\nwhich remove assumptions about the convexity and uniqueness of the adversary's\noptimal strategy have proved to be particularly effective at mitigating threats\nto classifiers due to its ability to capture the antagonistic nature of the\nadversary. However, this formulation has not yet been adapted to regression\nscenarios. This article serves to propose a pessimistic bilevel optimisation\nprogram for regression scenarios which makes no assumptions on the convexity or\nuniqueness of the adversary's solutions."
    },
    {
        "date": "2025-09",
        "title": "Concept activation vectors: a unifying view and adversarial attacks",
        "author": "Ekkehard Schnoor, Malik Tiomoko, Jawher Said, Alex Jung, and Wojciech Samek",
        "link": "http://arxiv.org/abs/2509.22755v1",
        "abstract": "Concept Activation Vectors (CAVs) are a tool from explainable AI, offering a\npromising approach for understanding how human-understandable concepts are\nencoded in a model's latent spaces. They are computed from hidden-layer\nactivations of inputs belonging either to a concept class or to non-concept\nexamples. Adopting a probabilistic perspective, the distribution of the\n(non-)concept inputs induces a distribution over the CAV, making it a random\nvector in the latent space. This enables us to derive mean and covariance for\ndifferent types of CAVs, leading to a unified theoretical view. This\nprobabilistic perspective also reveals a potential vulnerability: CAVs can\nstrongly depend on the rather arbitrary non-concept distribution, a factor\nlargely overlooked in prior work. We illustrate this with a simple yet\neffective adversarial attack, underscoring the need for a more systematic\nstudy."
    },
    {
        "date": "2025-09",
        "title": "SecureAgentBench: Benchmarking Secure Code Generation under Realistic Vulnerability Scenarios",
        "author": "Junkai Chen, Huihui Huang, Yunbo Lyu, Junwen An, Jieke Shi, Chengran Yang, Ting Zhang, Haoye Tian, Yikun Li, Zhenhao Li, Xin Zhou, Xing Hu, and David Lo",
        "link": "http://arxiv.org/abs/2509.22097v1",
        "abstract": "Large language model (LLM) powered code agents are rapidly transforming\nsoftware engineering by automating tasks such as testing, debugging, and\nrepairing, yet the security risks of their generated code have become a\ncritical concern. Existing benchmarks have offered valuable insights but remain\ninsufficient: they often overlook the genuine context in which vulnerabilities\nwere introduced or adopt narrow evaluation protocols that fail to capture\neither functional correctness or newly introduced vulnerabilities. We therefore\nintroduce SecureAgentBench, a benchmark of 105 coding tasks designed to\nrigorously evaluate code agents' capabilities in secure code generation. Each\ntask includes (i) realistic task settings that require multi-file edits in\nlarge repositories, (ii) aligned contexts based on real-world open-source\nvulnerabilities with precisely identified introduction points, and (iii)\ncomprehensive evaluation that combines functionality testing, vulnerability\nchecking through proof-of-concept exploits, and detection of newly introduced\nvulnerabilities using static analysis. We evaluate three representative agents\n(SWE-agent, OpenHands, and Aider) with three state-of-the-art LLMs (Claude 3.7\nSonnet, GPT-4.1, and DeepSeek-V3.1). Results show that (i) current agents\nstruggle to produce secure code, as even the best-performing one, SWE-agent\nsupported by DeepSeek-V3.1, achieves merely 15.2% correct-and-secure solutions,\n(ii) some agents produce functionally correct code but still introduce\nvulnerabilities, including new ones not previously recorded, and (iii) adding\nexplicit security instructions for agents does not significantly improve secure\ncoding, underscoring the need for further research. These findings establish\nSecureAgentBench as a rigorous benchmark for secure code generation and a step\ntoward more reliable software development with LLMs."
    },
    {
        "date": "2025-09",
        "title": "Non-Linear Trajectory Modeling for Multi-Step Gradient Inversion Attacks in Federated Learning",
        "author": "Li Xia, Zheng Liu, Sili Huang, Wei Tang, and Xuan Liu",
        "link": "http://arxiv.org/abs/2509.22082v1",
        "abstract": "Federated Learning (FL) preserves privacy by keeping raw data local, yet\nGradient Inversion Attacks (GIAs) pose significant threats. In FedAVG\nmulti-step scenarios, attackers observe only aggregated gradients, making data\nreconstruction challenging. Existing surrogate model methods like SME assume\nlinear parameter trajectories, but we demonstrate this severely underestimates\nSGD's nonlinear complexity, fundamentally limiting attack effectiveness. We\npropose Non-Linear Surrogate Model Extension (NL-SME), the first method to\nintroduce nonlinear parametric trajectory modeling for GIAs. Our approach\nreplaces linear interpolation with learnable quadratic B\\'ezier curves that\ncapture SGD's curved characteristics through control points, combined with\nregularization and dvec scaling mechanisms for enhanced expressiveness.\nExtensive experiments on CIFAR-100 and FEMNIST datasets show NL-SME\nsignificantly outperforms baselines across all metrics, achieving\norder-of-magnitude improvements in cosine similarity loss while maintaining\ncomputational efficiency.This work exposes heightened privacy vulnerabilities\nin FL's multi-step update paradigm and offers novel perspectives for developing\nrobust defense strategies."
    },
    {
        "date": "2025-09",
        "title": "SpecXNet: A Dual-Domain Convolutional Network for Robust Deepfake Detection",
        "author": "Inzamamul Alam, Md Tanvir Islam, and Simon S. Woo",
        "link": "http://arxiv.org/abs/2509.22070v1",
        "abstract": "The increasing realism of content generated by GANs and diffusion models has\nmade deepfake detection significantly more challenging. Existing approaches\noften focus solely on spatial or frequency-domain features, limiting their\ngeneralization to unseen manipulations. We propose the Spectral\nCross-Attentional Network (SpecXNet), a dual-domain architecture for robust\ndeepfake detection. The core \\textbf{Dual-Domain Feature Coupler (DDFC)}\ndecomposes features into a local spatial branch for capturing texture-level\nanomalies and a global spectral branch that employs Fast Fourier Transform to\nmodel periodic inconsistencies. This dual-domain formulation allows SpecXNet to\njointly exploit localized detail and global structural coherence, which are\ncritical for distinguishing authentic from manipulated images. We also\nintroduce the \\textbf{Dual Fourier Attention (DFA)} module, which dynamically\nfuses spatial and spectral features in a content-aware manner. Built atop a\nmodified XceptionNet backbone, we embed the DDFC and DFA modules within a\nseparable convolution block. Extensive experiments on multiple deepfake\nbenchmarks show that SpecXNet achieves state-of-the-art accuracy, particularly\nunder cross-dataset and unseen manipulation scenarios, while maintaining\nreal-time feasibility. Our results highlight the effectiveness of unified\nspatial-spectral learning for robust and generalizable deepfake detection. To\nensure reproducibility, we released the full code on\n\\href{https://github.com/inzamamulDU/SpecXNet}{\\textcolor{blue}{\\textbf{GitHub}}}."
    },
    {
        "date": "2025-09",
        "title": "Decoding Deception: Understanding Automatic Speech Recognition Vulnerabilities in Evasion and Poisoning Attacks",
        "author": "Aravindhan G, Yuvaraj Govindarajulu, and Parin Shah",
        "link": "http://arxiv.org/abs/2509.22060v1",
        "abstract": "Recent studies have demonstrated the vulnerability of Automatic Speech\nRecognition systems to adversarial examples, which can deceive these systems\ninto misinterpreting input speech commands. While previous research has\nprimarily focused on white-box attacks with constrained optimizations, and\ntransferability based black-box attacks against commercial Automatic Speech\nRecognition devices, this paper explores cost efficient white-box attack and\nnon transferability black-box adversarial attacks on Automatic Speech\nRecognition systems, drawing insights from approaches such as Fast Gradient\nSign Method and Zeroth-Order Optimization. Further, the novelty of the paper\nincludes how poisoning attack can degrade the performances of state-of-the-art\nmodels leading to misinterpretation of audio signals. Through experimentation\nand analysis, we illustrate how hybrid models can generate subtle yet impactful\nadversarial examples with very little perturbation having Signal Noise Ratio of\n35dB that can be generated within a minute. These vulnerabilities of\nstate-of-the-art open source model have practical security implications, and\nemphasize the need for adversarial security."
    },
    {
        "date": "2025-09",
        "title": "\"Your AI, My Shell\": Demystifying Prompt Injection Attacks on Agentic AI Coding Editors",
        "author": "Yue Liu, Yanjie Zhao, Yunbo Lyu, Ting Zhang, Haoyu Wang, and David Lo",
        "link": "http://arxiv.org/abs/2509.22040v1",
        "abstract": "Agentic AI coding editors driven by large language models have recently\nbecome more popular due to their ability to improve developer productivity\nduring software development. Modern editors such as Cursor are designed not\njust for code completion, but also with more system privileges for complex\ncoding tasks (e.g., run commands in the terminal, access development\nenvironments, and interact with external systems). While this brings us closer\nto the \"fully automated programming\" dream, it also raises new security\nconcerns. In this study, we present the first empirical analysis of prompt\ninjection attacks targeting these high-privilege agentic AI coding editors. We\nshow how attackers can remotely exploit these systems by poisoning external\ndevelopment resources with malicious instructions, effectively hijacking AI\nagents to run malicious commands, turning \"your AI\" into \"attacker's shell\". To\nperform this analysis, we implement AIShellJack, an automated testing framework\nfor assessing prompt injection vulnerabilities in agentic AI coding editors.\nAIShellJack contains 314 unique attack payloads that cover 70 techniques from\nthe MITRE ATT&CK framework. Using AIShellJack, we conduct a large-scale\nevaluation on GitHub Copilot and Cursor, and our evaluation results show that\nattack success rates can reach as high as 84% for executing malicious commands.\nMoreover, these attacks are proven effective across a wide range of objectives,\nranging from initial access and system discovery to credential theft and data\nexfiltration."
    },
    {
        "date": "2025-09",
        "title": "Active Attacks: Red-teaming LLMs via Adaptive Environments",
        "author": "Taeyoung Yun, Pierre-Luc St-Charles, Jinkyoo Park, Yoshua Bengio, and Minsu Kim",
        "link": "http://arxiv.org/abs/2509.21947v1",
        "abstract": "We address the challenge of generating diverse attack prompts for large\nlanguage models (LLMs) that elicit harmful behaviors (e.g., insults, sexual\ncontent) and are used for safety fine-tuning. Rather than relying on manual\nprompt engineering, attacker LLMs can be trained with reinforcement learning\n(RL) to automatically generate such prompts using only a toxicity classifier as\na reward. However, capturing a wide range of harmful behaviors is a significant\nchallenge that requires explicit diversity objectives. Existing\ndiversity-seeking RL methods often collapse to limited modes: once high-reward\nprompts are found, exploration of new regions is discouraged. Inspired by the\nactive learning paradigm that encourages adaptive exploration, we introduce\n\\textit{Active Attacks}, a novel RL-based red-teaming algorithm that adapts its\nattacks as the victim evolves. By periodically safety fine-tuning the victim\nLLM with collected attack prompts, rewards in exploited regions diminish, which\nforces the attacker to seek unexplored vulnerabilities. This process naturally\ninduces an easy-to-hard exploration curriculum, where the attacker progresses\nbeyond easy modes toward increasingly difficult ones. As a result, Active\nAttacks uncovers a wide range of local attack modes step by step, and their\ncombination achieves wide coverage of the multi-mode distribution. Active\nAttacks, a simple plug-and-play module that seamlessly integrates into existing\nRL objectives, unexpectedly outperformed prior RL-based methods -- including\nGFlowNets, PPO, and REINFORCE -- by improving cross-attack success rates\nagainst GFlowNets, the previous state-of-the-art, from 0.07% to 31.28% (a\nrelative gain greater than $400\\ \\times$) with only a 6% increase in\ncomputation. Our code is publicly available\n\\href{https://github.com/dbsxodud-11/active_attacks}{here}."
    },
    {
        "date": "2025-09",
        "title": "DyRo-MCTS: A Robust Monte Carlo Tree Search Approach to Dynamic Job Shop Scheduling",
        "author": "Ruiqi Chen, Yi Mei, Fangfang Zhang, and Mengjie Zhang",
        "link": "http://arxiv.org/abs/2509.21902v1",
        "abstract": "Dynamic job shop scheduling, a fundamental combinatorial optimisation problem\nin various industrial sectors, poses substantial challenges for effective\nscheduling due to frequent disruptions caused by the arrival of new jobs.\nState-of-the-art methods employ machine learning to learn scheduling policies\noffline, enabling rapid responses to dynamic events. However, these offline\npolicies are often imperfect, necessitating the use of planning techniques such\nas Monte Carlo Tree Search (MCTS) to improve performance at online decision\ntime. The unpredictability of new job arrivals complicates online planning, as\ndecisions based on incomplete problem information are vulnerable to\ndisturbances. To address this issue, we propose the Dynamic Robust MCTS\n(DyRo-MCTS) approach, which integrates action robustness estimation into MCTS.\nDyRo-MCTS guides the production environment toward states that not only yield\ngood scheduling outcomes but are also easily adaptable to future job arrivals.\nExtensive experiments show that DyRo-MCTS significantly improves the\nperformance of offline-learned policies with negligible additional online\nplanning time. Moreover, DyRo-MCTS consistently outperforms vanilla MCTS across\nvarious scheduling scenarios. Further analysis reveals that its ability to make\nrobust scheduling decisions leads to long-term, sustainable performance gains\nunder disturbances."
    },
    {
        "date": "2025-09",
        "title": "Zubov-Net: Adaptive Stability for Neural ODEs Reconciling Accuracy with Robustness",
        "author": "Chaoyang Luo, Yan Zou, and Nanjing Huang",
        "link": "http://arxiv.org/abs/2509.21879v1",
        "abstract": "Despite neural ordinary differential equations (Neural ODEs) exhibiting\nintrinsic robustness under input perturbations due to their dynamical systems\nnature, recent approaches often involve imposing Lyapunov-based stability\nconditions to provide formal robustness guarantees. However, a fundamental\nchallenge remains: the tension between robustness and accuracy, primarily\nstemming from the difficulty in imposing appropriate stability conditions. To\naddress this, we propose an adaptive stable learning framework named Zubov-Net,\nwhich innovatively reformulates Zubov's equation into a consistency\ncharacterization between regions of attraction (RoAs) and prescribed RoAs\n(PRoAs). Building on this consistency, we introduce a new paradigm for actively\ncontrolling the geometry of RoAs by directly optimizing PRoAs to reconcile\naccuracy and robustness. Our approach is realized through tripartite losses\n(consistency, classification, and separation losses) and a parallel boundary\nsampling algorithm that co-optimizes the Neural ODE and the Lyapunov function.\nTo enhance the discriminativity of Lyapunov functions, we design an\ninput-attention-based convex neural network via a softmax attention mechanism\nthat focuses on equilibrium-relevant features and also serves as weight\nnormalization to maintain training stability in deep architectures.\nTheoretically, we prove that minimizing the tripartite loss guarantees\nconsistent alignment of PRoAs-RoAs, trajectory stability, and non-overlapping\nPRoAs. Moreover, we establish stochastic convex separability with tighter\nprobability bounds and fewer dimensionality requirements to justify the convex\ndesign in Lyapunov functions. Experimentally, Zubov-Net maintains high\nclassification accuracy while significantly improving robustness against\nvarious stochastic noises and adversarial attacks."
    },
    {
        "date": "2025-09",
        "title": "SBFA: Single Sneaky Bit Flip Attack to Break Large Language Models",
        "author": "Jingkai Guo, Chaitali Chakrabarti, and Deliang Fan",
        "link": "http://arxiv.org/abs/2509.21843v1",
        "abstract": "Model integrity of Large language models (LLMs) has become a pressing\nsecurity concern with their massive online deployment. Prior Bit-Flip Attacks\n(BFAs) -- a class of popular AI weight memory fault-injection techniques -- can\nseverely compromise Deep Neural Networks (DNNs): as few as tens of bit flips\ncan degrade accuracy toward random guessing. Recent studies extend BFAs to LLMs\nand reveal that, despite the intuition of better robustness from modularity and\nredundancy, only a handful of adversarial bit flips can also cause LLMs'\ncatastrophic accuracy degradation. However, existing BFA methods typically\nfocus on either integer or floating-point models separately, limiting attack\nflexibility. Moreover, in floating-point models, random bit flips often cause\nperturbed parameters to extreme values (e.g., flipping in exponent bit), making\nit not stealthy and leading to numerical runtime error (e.g., invalid tensor\nvalues (NaN/Inf)). In this work, for the first time, we propose SBFA (Sneaky\nBit-Flip Attack), which collapses LLM performance with only one single bit flip\nwhile keeping perturbed values within benign layer-wise weight distribution. It\nis achieved through iterative searching and ranking through our defined\nparameter sensitivity metric, ImpactScore, which combines gradient sensitivity\nand perturbation range constrained by the benign layer-wise weight\ndistribution. A novel lightweight SKIP searching algorithm is also proposed to\ngreatly reduce searching complexity, which leads to successful SBFA searching\ntaking only tens of minutes for SOTA LLMs. Across Qwen, LLaMA, and Gemma\nmodels, with only one single bit flip, SBFA successfully degrades accuracy to\nbelow random levels on MMLU and SST-2 in both BF16 and INT8 data formats.\nRemarkably, flipping a single bit out of billions of parameters reveals a\nsevere security concern of SOTA LLM models."
    },
    {
        "date": "2025-09",
        "title": "A Framework for Scalable Heterogeneous Multi-Agent Adversarial Reinforcement Learning in IsaacLab",
        "author": "Isaac Peterson, Christopher Allred, Jacob Morrey, and Mario Harper",
        "link": "http://arxiv.org/abs/2510.01264v1",
        "abstract": "Multi-Agent Reinforcement Learning (MARL) is central to robotic systems\ncooperating in dynamic environments. While prior work has focused on these\ncollaborative settings, adversarial interactions are equally critical for\nreal-world applications such as pursuit-evasion, security, and competitive\nmanipulation. In this work, we extend the IsaacLab framework to support\nscalable training of adversarial policies in high-fidelity physics simulations.\nWe introduce a suite of adversarial MARL environments featuring heterogeneous\nagents with asymmetric goals and capabilities. Our platform integrates a\ncompetitive variant of Heterogeneous Agent Reinforcement Learning with Proximal\nPolicy Optimization (HAPPO), enabling efficient training and evaluation under\nadversarial dynamics. Experiments across several benchmark scenarios\ndemonstrate the framework's ability to model and train robust policies for\nmorphologically diverse multi-agent competition while maintaining high\nthroughput and simulation realism. Code and benchmarks are available at:\nhttps://github.com/DIRECTLab/IsaacLab-HARL ."
    },
    {
        "date": "2025-09",
        "title": "Benchmarking MLLM-based Web Understanding: Reasoning, Robustness and Safety",
        "author": "Junliang Liu, Jingyu Xiao, Wenxin Tang, Wenxuan Wang, Zhixian Wang, Minrui Zhang, and Shuanghe Yu",
        "link": "http://arxiv.org/abs/2509.21782v1",
        "abstract": "Multimodal large language models (MLLMs) are increasingly positioned as AI\ncollaborators for building complex web-related applications like GUI agents and\nfront-end code generation. However, existing benchmarks largely emphasize\nvisual perception or UI code generation, showing insufficient evaluation on the\nreasoning, robustness and safety capability required for end-to-end web\napplications. To bridge the gap, we introduce a comprehensive web understanding\nbenchmark, named WebRSSBench, that jointly evaluates Reasoning, Robustness, and\nSafety across eight tasks, such as position relationship reasoning, color\nrobustness, and safety critical detection, etc. The benchmark is constructed\nfrom 729 websites and contains 3799 question answer pairs that probe multi-step\ninference over page structure, text, widgets, and safety-critical interactions.\nTo ensure reliable measurement, we adopt standardized prompts, deterministic\nevaluation scripts, and multi-stage quality control combining automatic checks\nwith targeted human verification. We evaluate 12 MLLMs on WebRSSBench. The\nresults reveal significant gaps, models still struggle with compositional and\ncross-element reasoning over realistic layouts, show limited robustness when\nfacing perturbations in user interfaces and content such as layout\nrearrangements or visual style shifts, and are rather conservative in\nrecognizing and avoiding safety critical or irreversible actions. Our code is\navailable at https://github.com/jinliang-byte/webssrbench."
    },
    {
        "date": "2025-09",
        "title": "Learning What To Hear: Boosting Sound-Source Association For Robust Audiovisual Instance Segmentation",
        "author": "Jinbae Seo, Hyeongjun Kwon, Kwonyoung Kim, Jiyoung Lee, and Kwanghoon Sohn",
        "link": "http://arxiv.org/abs/2509.22740v1",
        "abstract": "Audiovisual instance segmentation (AVIS) requires accurately localizing and\ntracking sounding objects throughout video sequences. Existing methods suffer\nfrom visual bias stemming from two fundamental issues: uniform additive fusion\nprevents queries from specializing to different sound sources, while\nvisual-only training objectives allow queries to converge to arbitrary salient\nobjects. We propose Audio-Centric Query Generation using cross-attention,\nenabling each query to selectively attend to distinct sound sources and carry\nsound-specific priors into visual decoding. Additionally, we introduce\nSound-Aware Ordinal Counting (SAOC) loss that explicitly supervises sounding\nobject numbers through ordinal regression with monotonic consistency\nconstraints, preventing visual-only convergence during training. Experiments on\nAVISeg benchmark demonstrate consistent improvements: +1.64 mAP, +0.6 HOTA, and\n+2.06 FSLA, validating that query specialization and explicit counting\nsupervision are crucial for accurate audiovisual instance segmentation."
    },
    {
        "date": "2025-09",
        "title": "TRiCo: Triadic Game-Theoretic Co-Training for Robust Semi-Supervised Learning",
        "author": "Hongyang He, Xinyuan Song, Yangfan He, Zeyu Zhang, Yanshu Li, Haochen You, Lifan Sun, and Wenqiao Zhang",
        "link": "http://arxiv.org/abs/2509.21526v1",
        "abstract": "We introduce TRiCo, a novel triadic game-theoretic co-training framework that\nrethinks the structure of semi-supervised learning by incorporating a teacher,\ntwo students, and an adversarial generator into a unified training paradigm.\nUnlike existing co-training or teacher-student approaches, TRiCo formulates SSL\nas a structured interaction among three roles: (i) two student classifiers\ntrained on frozen, complementary representations, (ii) a meta-learned teacher\nthat adaptively regulates pseudo-label selection and loss balancing via\nvalidation-based feedback, and (iii) a non-parametric generator that perturbs\nembeddings to uncover decision boundary weaknesses. Pseudo-labels are selected\nbased on mutual information rather than confidence, providing a more robust\nmeasure of epistemic uncertainty. This triadic interaction is formalized as a\nStackelberg game, where the teacher leads strategy optimization and students\nfollow under adversarial perturbations. By addressing key limitations in\nexisting SSL frameworks, such as static view interactions, unreliable\npseudo-labels, and lack of hard sample modeling, TRiCo provides a principled\nand generalizable solution. Extensive experiments on CIFAR-10, SVHN, STL-10,\nand ImageNet demonstrate that TRiCo consistently achieves state-of-the-art\nperformance in low-label regimes, while remaining architecture-agnostic and\ncompatible with frozen vision backbones."
    },
    {
        "date": "2025-09",
        "title": "Contrastive Mutual Information Learning: Toward Robust Representations without Positive-Pair Augmentations",
        "author": "Micha Livne",
        "link": "http://arxiv.org/abs/2509.21511v1",
        "abstract": "Learning representations that transfer well to diverse downstream tasks\nremains a central challenge in representation learning. Existing paradigms --\ncontrastive learning, self-supervised masking, and denoising auto-encoders --\nbalance this challenge with different trade-offs. We introduce the {contrastive\nMutual Information Machine} (cMIM), a probabilistic framework that extends the\nMutual Information Machine (MIM) with a contrastive objective. While MIM\nmaximizes mutual information between inputs and latents and promotes clustering\nof codes, it falls short on discriminative tasks. cMIM addresses this gap by\nimposing global discriminative structure while retaining MIM's generative\nfidelity. Our contributions are threefold. First, we propose cMIM, a\ncontrastive extension of MIM that removes the need for positive data\naugmentation and is substantially less sensitive to batch size than InfoNCE.\nSecond, we introduce {informative embeddings}, a general technique for\nextracting enriched features from encoder-decoder models that boosts\ndiscriminative performance without additional training and applies broadly\nbeyond MIM. Third, we provide empirical evidence across vision and molecular\nbenchmarks showing that cMIM outperforms MIM and InfoNCE on classification and\nregression tasks while preserving competitive reconstruction quality. These\nresults position cMIM as a unified framework for representation learning,\nadvancing the goal of models that serve both discriminative and generative\napplications effectively."
    },
    {
        "date": "2025-09",
        "title": "Functional Encryption in Secure Neural Network Training: Data Leakage and Practical Mitigations",
        "author": "Alexandru Ioni\u0163\u0103, and Andreea Ioni\u0163\u0103",
        "link": "http://arxiv.org/abs/2509.21497v1",
        "abstract": "With the increased interest in artificial intelligence, Machine Learning as a\nService provides the infrastructure in the Cloud for easy training, testing,\nand deploying models. However, these systems have a major privacy issue:\nuploading sensitive data to the Cloud, especially during training. Therefore,\nachieving secure Neural Network training has been on many researchers' minds\nlately. More and more solutions for this problem are built around a main\npillar: Functional Encryption (FE). Although these approaches are very\ninteresting and offer a new perspective on ML training over encrypted data,\nsome vulnerabilities do not seem to be taken into consideration. In our paper,\nwe present an attack on neural networks that uses FE for secure training over\nencrypted data. Our approach uses linear programming to reconstruct the\noriginal input, unveiling the previous security promises. To address the\nattack, we propose two solutions for secure training and inference that involve\nthe client during the computation phase. One approach ensures security without\nrelying on encryption, while the other uses function-hiding inner-product\ntechniques."
    },
    {
        "date": "2025-09",
        "title": "No Prior, No Leakage: Revisiting Reconstruction Attacks in Trained Neural Networks",
        "author": "Yehonatan Refael, Guy Smorodinsky, Ofir Lindenbaum, and Itay Safran",
        "link": "http://arxiv.org/abs/2509.21296v1",
        "abstract": "The memorization of training data by neural networks raises pressing concerns\nfor privacy and security. Recent work has shown that, under certain conditions,\nportions of the training set can be reconstructed directly from model\nparameters. Some of these methods exploit implicit bias toward margin\nmaximization, suggesting that properties often regarded as beneficial for\ngeneralization may actually compromise privacy. Yet despite striking empirical\ndemonstrations, the reliability of these attacks remains poorly understood and\nlacks a solid theoretical foundation. In this work, we take a complementary\nperspective: rather than designing stronger attacks, we analyze the inherent\nweaknesses and limitations of existing reconstruction methods and identify\nconditions under which they fail. We rigorously prove that, without\nincorporating prior knowledge about the data, there exist infinitely many\nalternative solutions that may lie arbitrarily far from the true training set,\nrendering reconstruction fundamentally unreliable. Empirically, we further\ndemonstrate that exact duplication of training examples occurs only by chance.\nOur results refine the theoretical understanding of when training set leakage\nis possible and offer new insights into mitigating reconstruction attacks.\nRemarkably, we demonstrate that networks trained more extensively, and\ntherefore satisfying implicit bias conditions more strongly -- are, in fact,\nless susceptible to reconstruction attacks, reconciling privacy with the need\nfor strong generalization in this setting."
    },
    {
        "date": "2025-09",
        "title": "Optimal Robust Recourse with $L^p$-Bounded Model Change",
        "author": "Phone Kyaw, Kshitij Kayastha, and Shahin Jabbari",
        "link": "http://arxiv.org/abs/2509.21293v1",
        "abstract": "Recourse provides individuals who received undesirable labels (e.g., denied a\nloan) from algorithmic decision-making systems with a minimum-cost improvement\nsuggestion to achieve the desired outcome. However, in practice, models often\nget updated to reflect changes in the data distribution or environment,\ninvalidating the recourse recommendations (i.e., following the recourse will\nnot lead to the desirable outcome). The robust recourse literature addresses\nthis issue by providing a framework for computing recourses whose validity is\nresilient to slight changes in the model. However, since the optimization\nproblem of computing robust recourse is non-convex (even for linear models),\nmost of the current approaches do not have any theoretical guarantee on the\noptimality of the recourse. Recent work by Kayastha et. al. provides the first\nprovably optimal algorithm for robust recourse with respect to generalized\nlinear models when the model changes are measured using the $L^{\\infty}$ norm.\nHowever, using the $L^{\\infty}$ norm can lead to recourse solutions with a high\nprice. To address this shortcoming, we consider more constrained model changes\ndefined by the $L^p$ norm, where $p\\geq 1$ but $p\\neq \\infty$, and provide a\nnew algorithm that provably computes the optimal robust recourse for\ngeneralized linear models. Empirically, for both linear and non-linear models,\nwe demonstrate that our algorithm achieves a significantly lower price of\nrecourse (up to several orders of magnitude) compared to prior work and also\nexhibits a better trade-off between the implementation cost of recourse and its\nvalidity. Our empirical analysis also illustrates that our approach provides\nmore sparse recourses compared to prior work and remains resilient to\npost-processing approaches that guarantee feasibility."
    },
    {
        "date": "2025-09",
        "title": "Every Subtlety Counts: Fine-grained Person Independence Micro-Action Recognition via Distributionally Robust Optimization",
        "author": "Feng-Qi Cui, Jinyang Huang, Anyang Tong, Ziyu Jia, Jie Zhang, Zhi Liu, Dan Guo, Jianwei Lu, and Meng Wang",
        "link": "http://arxiv.org/abs/2509.21261v2",
        "abstract": "Micro-action Recognition is vital for psychological assessment and\nhuman-computer interaction. However, existing methods often fail in real-world\nscenarios because inter-person variability causes the same action to manifest\ndifferently, hindering robust generalization. To address this, we propose the\nPerson Independence Universal Micro-action Recognition Framework, which\nintegrates Distributionally Robust Optimization principles to learn\nperson-agnostic representations. Our framework contains two plug-and-play\ncomponents operating at the feature and loss levels. At the feature level, the\nTemporal-Frequency Alignment Module normalizes person-specific motion\ncharacteristics with a dual-branch design: the temporal branch applies\nWasserstein-regularized alignment to stabilize dynamic trajectories, while the\nfrequency branch introduces variance-guided perturbations to enhance robustness\nagainst person-specific spectral differences. A consistency-driven fusion\nmechanism integrates both branches. At the loss level, the Group-Invariant\nRegularized Loss partitions samples into pseudo-groups to simulate unseen\nperson-specific distributions. By up-weighting boundary cases and regularizing\nsubgroup variance, it forces the model to generalize beyond easy or frequent\nsamples, thus enhancing robustness to difficult variations. Experiments on the\nlarge-scale MA-52 dataset demonstrate that our framework outperforms existing\nmethods in both accuracy and robustness, achieving stable generalization under\nfine-grained conditions."
    },
    {
        "date": "2025-09",
        "title": "TABLET: A Large-Scale Dataset for Robust Visual Table Understanding",
        "author": "I\u00f1igo Alonso, Imanol Miranda, Eneko Agirre, and Mirella Lapata",
        "link": "http://arxiv.org/abs/2509.21205v1",
        "abstract": "While table understanding increasingly relies on pixel-only settings where\ntables are processed as visual representations, current benchmarks\npredominantly use synthetic renderings that lack the complexity and visual\ndiversity of real-world tables. Additionally, existing visual table\nunderstanding (VTU) datasets offer fixed examples with single visualizations\nand pre-defined instructions, providing no access to underlying serialized data\nfor reformulation. We introduce TABLET, a large-scale VTU dataset with 4\nmillion examples across 20 tasks, grounded in 2 million unique tables where 88%\npreserve original visualizations. Each example includes paired image-HTML\nrepresentations, comprehensive metadata, and provenance information linking\nback to the source datasets. Fine-tuning vision-language models like\nQwen2.5-VL-7B on TABLET improves performance on seen and unseen VTU tasks while\nincreasing robustness on real-world table visualizations. By preserving\noriginal visualizations and maintaining example traceability in a unified\nlarge-scale collection, TABLET establishes a foundation for robust training and\nextensible evaluation of future VTU models."
    },
    {
        "date": "2025-09",
        "title": "Emerging Paradigms for Securing Federated Learning Systems",
        "author": "Amr Akmal Abouelmagd, and Amr Hilal",
        "link": "http://arxiv.org/abs/2509.21147v1",
        "abstract": "Federated Learning (FL) facilitates collaborative model training while\nkeeping raw data decentralized, making it a conduit for leveraging the power of\nIoT devices while maintaining privacy of the locally collected data. However,\nexisting privacy- preserving techniques present notable hurdles. Methods such\nas Multi-Party Computation (MPC), Homomorphic Encryption (HE), and Differential\nPrivacy (DP) often incur high compu- tational costs and suffer from limited\nscalability. This survey examines emerging approaches that hold promise for\nenhancing both privacy and efficiency in FL, including Trusted Execution\nEnvironments (TEEs), Physical Unclonable Functions (PUFs), Quantum Computing\n(QC), Chaos-Based Encryption (CBE), Neuromorphic Computing (NC), and Swarm\nIntelligence (SI). For each paradigm, we assess its relevance to the FL\npipeline, outlining its strengths, limitations, and practical considerations.\nWe conclude by highlighting open challenges and prospective research avenues,\noffering a detailed roadmap for advancing secure and scalable FL systems."
    },
    {
        "date": "2025-09",
        "title": "Bidirectional Intention Inference Enhances LLMs' Defense Against Multi-Turn Jailbreak Attacks",
        "author": "Haibo Tong, Dongcheng Zhao, Guobin Shen, Xiang He, Dachuan Lin, Feifei Zhao, and Yi Zeng",
        "link": "http://arxiv.org/abs/2509.22732v1",
        "abstract": "The remarkable capabilities of Large Language Models (LLMs) have raised\nsignificant safety concerns, particularly regarding \"jailbreak\" attacks that\nexploit adversarial prompts to bypass safety alignment mechanisms. Existing\ndefense research primarily focuses on single-turn attacks, whereas multi-turn\njailbreak attacks progressively break through safeguards through by concealing\nmalicious intent and tactical manipulation, ultimately rendering conventional\nsingle-turn defenses ineffective. To address this critical challenge, we\npropose the Bidirectional Intention Inference Defense (BIID). The method\nintegrates forward request-based intention inference with backward\nresponse-based intention retrospection, establishing a bidirectional synergy\nmechanism to detect risks concealed within seemingly benign inputs, thereby\nconstructing a more robust guardrails that effectively prevents harmful content\ngeneration. The proposed method undergoes systematic evaluation compared with a\nno-defense baseline and seven representative defense methods across three LLMs\nand two safety benchmarks under 10 different attack methods. Experimental\nresults demonstrate that the proposed method significantly reduces the Attack\nSuccess Rate (ASR) across both single-turn and multi-turn jailbreak attempts,\noutperforming all existing baseline methods while effectively maintaining\npractical utility. Notably, comparative experiments across three multi-turn\nsafety datasets further validate the proposed model's significant advantages\nover other defense approaches."
    },
    {
        "date": "2025-09",
        "title": "Sparse Representations Improve Adversarial Robustness of Neural Network Classifiers",
        "author": "Killian Steunou, Sigurd Saue, and Th\u00e9o Druilhe",
        "link": "http://arxiv.org/abs/2509.21130v1",
        "abstract": "Deep neural networks perform remarkably well on image classification tasks\nbut remain vulnerable to carefully crafted adversarial perturbations. This work\nrevisits linear dimensionality reduction as a simple, data-adapted defense. We\nempirically compare standard Principal Component Analysis (PCA) with its sparse\nvariant (SPCA) as front-end feature extractors for downstream classifiers, and\nwe complement these experiments with a theoretical analysis. On the theory\nside, we derive exact robustness certificates for linear heads applied to SPCA\nfeatures: for both $\\ell_\\infty$ and $\\ell_2$ threat models (binary and\nmulticlass), the certified radius grows as the dual norms of $W^\\top u$ shrink,\nwhere $W$ is the projection and $u$ the head weights. We further show that for\ngeneral (non-linear) heads, sparsity reduces operator-norm bounds through a\nLipschitz composition argument, predicting lower input sensitivity.\nEmpirically, with a small non-linear network after the projection, SPCA\nconsistently degrades more gracefully than PCA under strong white-box and\nblack-box attacks while maintaining competitive clean accuracy. Taken together,\nthe theory identifies the mechanism (sparser projections reduce adversarial\nleverage) and the experiments verify that this benefit persists beyond the\nlinear setting. Our code is available at\nhttps://github.com/killian31/SPCARobustness."
    },
    {
        "date": "2025-09",
        "title": "EvoMail: Self-Evolving Cognitive Agents for Adaptive Spam and Phishing Email Defense",
        "author": "Wei Huang, De-Tian Chu, Lin-Yuan Bai, Wei Kang, Hai-Tao Zhang, Bo Li, Zhi-Mo Han, Jing Ge, and Hai-Feng Lin",
        "link": "http://arxiv.org/abs/2509.21129v1",
        "abstract": "Modern email spam and phishing attacks have evolved far beyond keyword\nblacklists or simple heuristics. Adversaries now craft multi-modal campaigns\nthat combine natural-language text with obfuscated URLs, forged headers, and\nmalicious attachments, adapting their strategies within days to bypass filters.\nTraditional spam detection systems, which rely on static rules or\nsingle-modality models, struggle to integrate heterogeneous signals or to\ncontinuously adapt, leading to rapid performance degradation.\n  We propose EvoMail, a self-evolving cognitive agent framework for robust\ndetection of spam and phishing. EvoMail first constructs a unified\nheterogeneous email graph that fuses textual content, metadata (headers,\nsenders, domains), and embedded resources (URLs, attachments). A Cognitive\nGraph Neural Network enhanced by a Large Language Model (LLM) performs\ncontext-aware reasoning across these sources to identify coordinated spam\ncampaigns. Most critically, EvoMail engages in an adversarial self-evolution\nloop: a ''red-team'' agent generates novel evasion tactics -- such as character\nobfuscation or AI-generated phishing text -- while the ''blue-team'' detector\nlearns from failures, compresses experiences into a memory module, and reuses\nthem for future reasoning.\n  Extensive experiments on real-world datasets (Enron-Spam, Ling-Spam,\nSpamAssassin, and TREC) and synthetic adversarial variants demonstrate that\nEvoMail consistently outperforms state-of-the-art baselines in detection\naccuracy, adaptability to evolving spam tactics, and interpretability of\nreasoning traces. These results highlight EvoMail's potential as a resilient\nand explainable defense framework against next-generation spam and phishing\nthreats."
    },
    {
        "date": "2025-09",
        "title": "Are Modern Speech Enhancement Systems Vulnerable to Adversarial Attacks?",
        "author": "Rostislav Makarov, Lea Sch\u00f6nherr, and Timo Gerkmann",
        "link": "http://arxiv.org/abs/2509.21087v1",
        "abstract": "Machine learning approaches for speech enhancement are becoming increasingly\nexpressive, enabling ever more powerful modifications of input signals. In this\npaper, we demonstrate that this expressiveness introduces a vulnerability:\nadvanced speech enhancement models can be susceptible to adversarial attacks.\nSpecifically, we show that adversarial noise, carefully crafted and\npsychoacoustically masked by the original input, can be injected such that the\nenhanced speech output conveys an entirely different semantic meaning. We\nexperimentally verify that contemporary predictive speech enhancement models\ncan indeed be manipulated in this way. Furthermore, we highlight that diffusion\nmodels with stochastic samplers exhibit inherent robustness to such adversarial\nattacks by design."
    },
    {
        "date": "2025-09",
        "title": "Vision Transformers: the threat of realistic adversarial patches",
        "author": "Kasper Cools, Clara Maathuis, Alexander M. van Oers, Claudia S. H\u00fcbner, Nikos Deligiannis, Marijke Vandewal, and Geert De Cubber",
        "link": "http://arxiv.org/abs/2509.21084v1",
        "abstract": "The increasing reliance on machine learning systems has made their security a\ncritical concern. Evasion attacks enable adversaries to manipulate the\ndecision-making processes of AI systems, potentially causing security breaches\nor misclassification of targets. Vision Transformers (ViTs) have gained\nsignificant traction in modern machine learning due to increased 1) performance\ncompared to Convolutional Neural Networks (CNNs) and 2) robustness against\nadversarial perturbations. However, ViTs remain vulnerable to evasion attacks,\nparticularly to adversarial patches, unique patterns designed to manipulate AI\nclassification systems. These vulnerabilities are investigated by designing\nrealistic adversarial patches to cause misclassification in person vs.\nnon-person classification tasks using the Creases Transformation (CT)\ntechnique, which adds subtle geometric distortions similar to those occurring\nnaturally when wearing clothing. This study investigates the transferability of\nadversarial attack techniques used in CNNs when applied to ViT classification\nmodels. Experimental evaluation across four fine-tuned ViT models on a binary\nperson classification task reveals significant vulnerability variations: attack\nsuccess rates ranged from 40.04% (google/vit-base-patch16-224-in21k) to 99.97%\n(facebook/dino-vitb16), with google/vit-base-patch16-224 achieving 66.40% and\nfacebook/dinov3-vitb16 reaching 65.17%. These results confirm the\ncross-architectural transferability of adversarial patches from CNNs to ViTs,\nwith pre-training dataset scale and methodology strongly influencing model\nresilience to adversarial attacks."
    },
    {
        "date": "2025-09",
        "title": "PMark: Towards Robust and Distortion-free Semantic-level Watermarking with Channel Constraints",
        "author": "Jiahao Huo, Shuliang Liu, Bin Wang, Junyan Zhang, Yibo Yan, Aiwei Liu, Xuming Hu, and Mingxun Zhou",
        "link": "http://arxiv.org/abs/2509.21057v1",
        "abstract": "Semantic-level watermarking (SWM) for large language models (LLMs) enhances\nwatermarking robustness against text modifications and paraphrasing attacks by\ntreating the sentence as the fundamental unit. However, existing methods still\nlack strong theoretical guarantees of robustness, and reject-sampling-based\ngeneration often introduces significant distribution distortions compared with\nunwatermarked outputs. In this work, we introduce a new theoretical framework\non SWM through the concept of proxy functions (PFs) $\\unicode{x2013}$ functions\nthat map sentences to scalar values. Building on this framework, we propose\nPMark, a simple yet powerful SWM method that estimates the PF median for the\nnext sentence dynamically through sampling while enforcing multiple PF\nconstraints (which we call channels) to strengthen watermark evidence. Equipped\nwith solid theoretical guarantees, PMark achieves the desired distortion-free\nproperty and improves the robustness against paraphrasing-style attacks. We\nalso provide an empirically optimized version that further removes the\nrequirement for dynamical median estimation for better sampling efficiency.\nExperimental results show that PMark consistently outperforms existing SWM\nbaselines in both text quality and robustness, offering a more effective\nparadigm for detecting machine-generated text. Our code will be released at\n[this URL](https://github.com/PMark-repo/PMark)."
    },
    {
        "date": "2025-09",
        "title": "FORCE: Transferable Visual Jailbreaking Attacks via Feature Over-Reliance CorrEction",
        "author": "Runqi Lin, Alasdair Paren, Suqin Yuan, Muyang Li, Philip Torr, Adel Bibi, and Tongliang Liu",
        "link": "http://arxiv.org/abs/2509.21029v2",
        "abstract": "The integration of new modalities enhances the capabilities of multimodal\nlarge language models (MLLMs) but also introduces additional vulnerabilities.\nIn particular, simple visual jailbreaking attacks can manipulate open-source\nMLLMs more readily than sophisticated textual attacks. However, these\nunderdeveloped attacks exhibit extremely limited cross-model transferability,\nfailing to reliably identify vulnerabilities in closed-source MLLMs. In this\nwork, we analyse the loss landscape of these jailbreaking attacks and find that\nthe generated attacks tend to reside in high-sharpness regions, whose\neffectiveness is highly sensitive to even minor parameter changes during\ntransfer. To further explain the high-sharpness localisations, we analyse their\nfeature representations in both the intermediate layers and the spectral\ndomain, revealing an improper reliance on narrow layer representations and\nsemantically poor frequency components. Building on this, we propose a Feature\nOver-Reliance CorrEction (FORCE) method, which guides the attack to explore\nbroader feasible regions across layer features and rescales the influence of\nfrequency features according to their semantic content. By eliminating\nnon-generalizable reliance on both layer and spectral features, our method\ndiscovers flattened feasible regions for visual jailbreaking attacks, thereby\nimproving cross-model transferability. Extensive experiments demonstrate that\nour approach effectively facilitates visual red-teaming evaluations against\nclosed-source MLLMs."
    },
    {
        "date": "2025-09",
        "title": "Toward Robust and Efficient ML-Based GPU Caching for Modern Inference",
        "author": "Peng Chen, Jiaji Zhang, Hailiang Zhao, Yirong Zhang, Jiahong Yu, Xueyan Tang, Yixuan Wang, Hao Li, Jianping Zou, Gang Xiong, Kingsum Chow, Shuibing He, and Shuiguang Deng",
        "link": "http://arxiv.org/abs/2509.20979v1",
        "abstract": "In modern GPU inference, cache efficiency remains a major bottleneck. In\nrecommendation models, embedding hit rates largely determine throughput, while\nin large language models, KV-cache misses substantially increase\ntime-to-first-token (TTFT). Heuristic policies such as \\textsc{LRU} often\nstruggle under structured access patterns. Learning-based approaches are\npromising, but in practice face two major limitations: they degrade sharply\nwhen predictions are inaccurate, or they gain little even with accurate\npredictions due to conservative designs. Some also incur high overhead, further\nlimiting practicality.\n  We present \\textsc{LCR}, a practical framework for learning-based GPU caching\nthat delivers performance gains while ensuring robustness and efficiency. Its\ncore algorithm, \\textsc{LARU}, enhances \\textsc{LRU} with machine-learned\npredictions and dynamically adapts to prediction accuracy through online error\nestimation. When predictions are accurate, \\textsc{LARU} achieves near-optimal\nperformance. With inaccurate predictions, it degrades gracefully to\nnear-\\textsc{LRU} performance. With \\textsc{LCR}, we bridge the gap between\nempirical progress and theoretical advances in learning-based caching.\n  Experiments show that \\textsc{LCR} delivers consistent gains under realistic\nconditions. In DLRM and LLM scenarios, it improves throughput by up to 24.2\\%\nand reduces P99 TTFT by up to 28.3\\%, outperforming widely used inference\nsystems. Even under poor predictions, its performance remains stable,\ndemonstrating practical robustness."
    },
    {
        "date": "2025-09",
        "title": "Unlocking Noise-Resistant Vision: Key Architectural Secrets for Robust Models",
        "author": "Bum Jun Kim, Makoto Kawano, Yusuke Iwasawa, and Yutaka Matsuo",
        "link": "http://arxiv.org/abs/2509.20939v1",
        "abstract": "While the robustness of vision models is often measured, their dependence on\nspecific architectural design choices is rarely dissected. We investigate why\ncertain vision architectures are inherently more robust to additive Gaussian\nnoise and convert these empirical insights into simple, actionable design\nrules. Specifically, we performed extensive evaluations on 1,174 pretrained\nvision models, empirically identifying four consistent design patterns for\nimproved robustness against Gaussian noise: larger stem kernels, smaller input\nresolutions, average pooling, and supervised vision transformers (ViTs) rather\nthan CLIP ViTs, which yield up to 506 rank improvements and 21.6\\%p accuracy\ngains. We then develop a theoretical analysis that explains these findings,\nconverting observed correlations into causal mechanisms. First, we prove that\nlow-pass stem kernels attenuate noise with a gain that decreases quadratically\nwith kernel size and that anti-aliased downsampling reduces noise energy\nroughly in proportion to the square of the downsampling factor. Second, we\ndemonstrate that average pooling is unbiased and suppresses noise in proportion\nto the pooling window area, whereas max pooling incurs a positive bias that\ngrows slowly with window size and yields a relatively higher mean-squared error\nand greater worst-case sensitivity. Third, we reveal and explain the\nvulnerability of CLIP ViTs via a pixel-space Lipschitz bound: The smaller\nnormalization standard deviations used in CLIP preprocessing amplify worst-case\nsensitivity by up to 1.91 times relative to the Inception-style preprocessing\ncommon in supervised ViTs. Our results collectively disentangle robustness into\ninterpretable modules, provide a theory that explains the observed trends, and\nbuild practical, plug-and-play guidelines for designing vision models more\nrobust against Gaussian noise."
    }
]