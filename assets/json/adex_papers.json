[
    {
        "date": "2025-09",
        "title": "Adversarially-Refined VQ-GAN with Dense Motion Tokenization for Spatio-Temporal Heatmaps",
        "author": "Gabriel Maldonado, Narges Rashvand, Armin Danesh Pazho, Ghazal Alinezhad Noghre, Vinit Katariya, and Hamed Tabkhi",
        "link": "http://arxiv.org/abs/2509.19252v1",
        "abstract": "Continuous human motion understanding remains a core challenge in computer\nvision due to its high dimensionality and inherent redundancy. Efficient\ncompression and representation are crucial for analyzing complex motion\ndynamics. In this work, we introduce an adversarially-refined VQ-GAN framework\nwith dense motion tokenization for compressing spatio-temporal heatmaps while\npreserving the fine-grained traces of human motion. Our approach combines dense\nmotion tokenization with adversarial refinement, which eliminates\nreconstruction artifacts like motion smearing and temporal misalignment\nobserved in non-adversarial baselines. Our experiments on the CMU Panoptic\ndataset provide conclusive evidence of our method's superiority, outperforming\nthe dVAE baseline by 9.31% SSIM and reducing temporal instability by 37.1%.\nFurthermore, our dense tokenization strategy enables a novel analysis of motion\ncomplexity, revealing that 2D motion can be optimally represented with a\ncompact 128-token vocabulary, while 3D motion's complexity demands a much\nlarger 1024-token codebook for faithful reconstruction. These results establish\npractical deployment feasibility across diverse motion analysis applications.\nThe code base for this work is available at\nhttps://github.com/TeCSAR-UNCC/Pose-Quantization."
    },
    {
        "date": "2025-09",
        "title": "Stability and Generalization of Adversarial Diffusion Training",
        "author": "Hesam Hosseini, Ying Cao, and Ali H. Sayed",
        "link": "http://arxiv.org/abs/2509.19234v1",
        "abstract": "Algorithmic stability is an established tool for analyzing generalization.\nWhile adversarial training enhances model robustness, it often suffers from\nrobust overfitting and an enlarged generalization gap. Although recent work has\nestablished the convergence of adversarial training in decentralized networks,\nits generalization properties remain unexplored. This work presents a\nstability-based generalization analysis of adversarial training under the\ndiffusion strategy for convex losses. We derive a bound showing that the\ngeneralization error grows with both the adversarial perturbation strength and\nthe number of training steps, a finding consistent with single-agent case but\nnovel for decentralized settings. Numerical experiments on logistic regression\nvalidate these theoretical predictions."
    },
    {
        "date": "2025-09",
        "title": "FedFusion: Federated Learning with Diversity- and Cluster-Aware Encoders for Robust Adaptation under Label Scarcity",
        "author": "Ferdinand Kahenga, Antoine Bagula, Patrick Sello, and Sajal K. Das",
        "link": "http://arxiv.org/abs/2509.19220v1",
        "abstract": "Federated learning in practice must contend with heterogeneous feature\nspaces, severe non-IID data, and scarce labels across clients. We present\nFedFusion, a federated transfer-learning framework that unifies domain\nadaptation and frugal labelling with diversity-/cluster-aware encoders (DivEn,\nDivEn-mix, DivEn-c). Labelled teacher clients guide learner clients via\nconfidence-filtered pseudo-labels and domain-adaptive transfer, while clients\nmaintain personalised encoders tailored to local data. To preserve global\ncoherence under heterogeneity, FedFusion employs similarity-weighted classifier\ncoupling (with optional cluster-wise averaging), mitigating dominance by\ndata-rich sites and improving minority-client performance. The frugal-labelling\npipeline combines self-/semi-supervised pretext training with selective\nfine-tuning, reducing annotation demands without sharing raw data. Across\ntabular and imaging benchmarks under IID, non-IID, and label-scarce regimes,\nFedFusion consistently outperforms state-of-the-art baselines in accuracy,\nrobustness, and fairness while maintaining comparable communication and\ncomputation budgets. These results show that harmonising personalisation,\ndomain adaptation, and label efficiency is an effective recipe for robust\nfederated learning under real-world constraints."
    },
    {
        "date": "2025-09",
        "title": "A Validation Strategy for Deep Learning Models: Evaluating and Enhancing Robustness",
        "author": "Abdul-Rauf Nuhu, Parham Kebria, Vahid Hemmati, Benjamin Lartey, Mahmoud Nabil Mahmoud, Abdollah Homaifar, and Edward Tunstel",
        "link": "http://arxiv.org/abs/2509.19197v1",
        "abstract": "Data-driven models, especially deep learning classifiers often demonstrate\ngreat success on clean datasets. Yet, they remain vulnerable to common data\ndistortions such as adversarial and common corruption perturbations. These\nperturbations can significantly degrade performance, thereby challenging the\noverall reliability of the models. Traditional robustness validation typically\nrelies on perturbed test datasets to assess and improve model performance. In\nour framework, however, we propose a validation approach that extracts \"weak\nrobust\" samples directly from the training dataset via local robustness\nanalysis. These samples, being the most susceptible to perturbations, serve as\nan early and sensitive indicator of the model's vulnerabilities. By evaluating\nmodels on these challenging training instances, we gain a more nuanced\nunderstanding of its robustness, which informs targeted performance\nenhancement. We demonstrate the effectiveness of our approach on models trained\nwith CIFAR-10, CIFAR-100, and ImageNet, highlighting how robustness validation\nguided by weak robust samples can drive meaningful improvements in model\nreliability under adversarial and common corruption scenarios."
    },
    {
        "date": "2025-09",
        "title": "RoSe: Robust Self-supervised Stereo Matching under Adverse Weather Conditions",
        "author": "Yun Wang, Junjie Hu, Junhui Hou, Chenghao Zhang, Renwei Yang, and Dapeng Oliver Wu",
        "link": "http://arxiv.org/abs/2509.19165v1",
        "abstract": "Recent self-supervised stereo matching methods have made significant\nprogress, but their performance significantly degrades under adverse weather\nconditions such as night, rain, and fog. We identify two primary weaknesses\ncontributing to this performance degradation. First, adverse weather introduces\nnoise and reduces visibility, making CNN-based feature extractors struggle with\ndegraded regions like reflective and textureless areas. Second, these degraded\nregions can disrupt accurate pixel correspondences, leading to ineffective\nsupervision based on the photometric consistency assumption. To address these\nchallenges, we propose injecting robust priors derived from the visual\nfoundation model into the CNN-based feature extractor to improve feature\nrepresentation under adverse weather conditions. We then introduce scene\ncorrespondence priors to construct robust supervisory signals rather than\nrelying solely on the photometric consistency assumption. Specifically, we\ncreate synthetic stereo datasets with realistic weather degradations. These\ndatasets feature clear and adverse image pairs that maintain the same semantic\ncontext and disparity, preserving the scene correspondence property. With this\nknowledge, we propose a robust self-supervised training paradigm, consisting of\ntwo key steps: robust self-supervised scene correspondence learning and adverse\nweather distillation. Both steps aim to align underlying scene results from\nclean and adverse image pairs, thus improving model disparity estimation under\nadverse weather effects. Extensive experiments demonstrate the effectiveness\nand versatility of our proposed solution, which outperforms existing\nstate-of-the-art self-supervised methods. Codes are available at\n\\textcolor{blue}{https://github.com/cocowy1/RoSe-Robust-Self-supervised-Stereo-Matching-under-Adverse-Weather-Conditions}."
    },
    {
        "date": "2025-09",
        "title": "DRO-REBEL: Distributionally Robust Relative-Reward Regression for Fast and Efficient LLM Alignment",
        "author": "Sharan Sahu, and Martin T. Wells",
        "link": "http://arxiv.org/abs/2509.19104v1",
        "abstract": "Reinforcement learning with human feedback (RLHF) has become crucial for\naligning Large Language Models (LLMs) with human intent. However, existing\noffline RLHF approaches suffer from overoptimization, where models overfit to\nreward misspecification and drift from preferred behaviors observed during\ntraining. We introduce DRO-REBEL, a unified family of robust REBEL updates with\ntype-$p$ Wasserstein, KL, and $\\chi^2$ ambiguity sets. Using Fenchel duality,\neach update reduces to a simple relative-reward regression, preserving\nscalability and avoiding PPO-style clipping or auxiliary value networks. Under\nstandard linear-reward and log-linear policy classes with a data-coverage\ncondition, we establish $O(n^{-1/4})$ estimation bounds with tighter constants\nthan prior DRO-DPO approaches, and recover the minimax-optimal $O(n^{-1/2})$\nrate via a localized Rademacher complexity analysis. The same analysis closes\nthe gap for Wasserstein-DPO and KL-DPO, showing both also attain optimal\nparametric rates. We derive practical SGD algorithms for all three divergences:\ngradient regularization (Wasserstein), importance weighting (KL), and a fast\n1-D dual solve ($\\chi^2$). Experiments on Emotion Alignment, the large-scale\nArmoRM multi-objective benchmark, and HH-Alignment demonstrate strong\nworst-case robustness across unseen preference mixtures, model sizes, and data\nscales, with $\\chi^2$-REBEL showing consistently strong empirical performance.\nA controlled radius--coverage study validates a no-free-lunch trade-off: radii\nshrinking faster than empirical divergence concentration rates achieve\nminimax-optimal parametric rates but forfeit coverage, while\ncoverage-guaranteeing radii incur $O(n^{-1/4})$ rates."
    },
    {
        "date": "2025-09",
        "title": "Algorithms for Adversarially Robust Deep Learning",
        "author": "Alexander Robey",
        "link": "http://arxiv.org/abs/2509.19100v1",
        "abstract": "Given the widespread use of deep learning models in safety-critical\napplications, ensuring that the decisions of such models are robust against\nadversarial exploitation is of fundamental importance. In this thesis, we\ndiscuss recent progress toward designing algorithms that exhibit desirable\nrobustness properties. First, we discuss the problem of adversarial examples in\ncomputer vision, for which we introduce new technical results, training\nparadigms, and certification algorithms. Next, we consider the problem of\ndomain generalization, wherein the task is to train neural networks to\ngeneralize from a family of training distributions to unseen test\ndistributions. We present new algorithms that achieve state-of-the-art\ngeneralization in medical imaging, molecular identification, and image\nclassification. Finally, we study the setting of jailbreaking large language\nmodels (LLMs), wherein an adversarial user attempts to design prompts that\nelicit objectionable content from an LLM. We propose new attacks and defenses,\nwhich represent the frontier of progress toward designing robust language-based\nagents."
    },
    {
        "date": "2025-09",
        "title": "Latent Danger Zone: Distilling Unified Attention for Cross-Architecture Black-box Attacks",
        "author": "Yang Li, Chenyu Wang, Tingrui Wang, Yongwei Wang, Haonan Li, Zhunga Liu, and Quan Pan",
        "link": "http://arxiv.org/abs/2509.19044v1",
        "abstract": "Black-box adversarial attacks remain challenging due to limited access to\nmodel internals. Existing methods often depend on specific network\narchitectures or require numerous queries, resulting in limited\ncross-architecture transferability and high query costs. To address these\nlimitations, we propose JAD, a latent diffusion model framework for black-box\nadversarial attacks. JAD generates adversarial examples by leveraging a latent\ndiffusion model guided by attention maps distilled from both a convolutional\nneural network (CNN) and a Vision Transformer (ViT) models. By focusing on\nimage regions that are commonly sensitive across architectures, this approach\ncrafts adversarial perturbations that transfer effectively between different\nmodel types. This joint attention distillation strategy enables JAD to be\narchitecture-agnostic, achieving superior attack generalization across diverse\nmodels. Moreover, the generative nature of the diffusion framework yields high\nadversarial sample generation efficiency by reducing reliance on iterative\nqueries. Experiments demonstrate that JAD offers improved attack\ngeneralization, generation efficiency, and cross-architecture transferability\ncompared to existing methods, providing a promising and effective paradigm for\nblack-box adversarial attacks."
    },
    {
        "date": "2025-09",
        "title": "Towards Robust LiDAR Localization: Deep Learning-based Uncertainty Estimation",
        "author": "Minoo Dolatabadi, Fardin Ayar, Ehsan Javanmardi, Manabu Tsukada, and Mahdi Javanmardi",
        "link": "http://arxiv.org/abs/2509.18954v1",
        "abstract": "LiDAR-based localization and SLAM often rely on iterative matching\nalgorithms, particularly the Iterative Closest Point (ICP) algorithm, to align\nsensor data with pre-existing maps or previous scans. However, ICP is prone to\nerrors in featureless environments and dynamic scenes, leading to inaccurate\npose estimation. Accurately predicting the uncertainty associated with ICP is\ncrucial for robust state estimation but remains challenging, as existing\napproaches often rely on handcrafted models or simplified assumptions.\nMoreover, a few deep learning-based methods for localizability estimation\neither depend on a pre-built map, which may not always be available, or provide\na binary classification of localizable versus non-localizable, which fails to\nproperly model uncertainty. In this work, we propose a data-driven framework\nthat leverages deep learning to estimate the registration error covariance of\nICP before matching, even in the absence of a reference map. By associating\neach LiDAR scan with a reliable 6-DoF error covariance estimate, our method\nenables seamless integration of ICP within Kalman filtering, enhancing\nlocalization accuracy and robustness. Extensive experiments on the KITTI\ndataset demonstrate the effectiveness of our approach, showing that it\naccurately predicts covariance and, when applied to localization using a\npre-built map or SLAM, reduces localization errors and improves robustness."
    },
    {
        "date": "2025-09",
        "title": "Eva-VLA: Evaluating Vision-Language-Action Models' Robustness Under Real-World Physical Variations",
        "author": "Hanqing Liu, Jiahuan Long, Junqi Wu, Jiacheng Hou, Huili Tang, Tingsong Jiang, Weien Zhou, and Wen Yao",
        "link": "http://arxiv.org/abs/2509.18953v1",
        "abstract": "Vision-Language-Action (VLA) models have emerged as promising solutions for\nrobotic manipulation, yet their robustness to real-world physical variations\nremains critically underexplored. To bridge this gap, we propose Eva-VLA, the\nfirst unified framework that systematically evaluates the robustness of VLA\nmodels by transforming discrete physical variations into continuous\noptimization problems. However, comprehensively assessing VLA robustness\npresents two key challenges: (1) how to systematically characterize diverse\nphysical variations encountered in real-world deployments while maintaining\nevaluation reproducibility, and (2) how to discover worst-case scenarios\nwithout prohibitive real-world data collection costs efficiently. To address\nthe first challenge, we decompose real-world variations into three critical\ndomains: object 3D transformations that affect spatial reasoning, illumination\nvariations that challenge visual perception, and adversarial patches that\ndisrupt scene understanding. For the second challenge, we introduce a\ncontinuous black-box optimization framework that transforms discrete physical\nvariations into parameter optimization, enabling systematic exploration of\nworst-case scenarios. Extensive experiments on state-of-the-art OpenVLA models\nacross multiple benchmarks reveal alarming vulnerabilities: all variation types\ntrigger failure rates exceeding 60%, with object transformations causing up to\n97.8% failure in long-horizon tasks. Our findings expose critical gaps between\ncontrolled laboratory success and unpredictable deployment readiness, while the\nEva-VLA framework provides a practical pathway for hardening VLA-based robotic\nmanipulation models against real-world deployment challenges."
    },
    {
        "date": "2025-09",
        "title": "Generic Adversarial Smart Contract Detection with Semantics and Uncertainty-Aware LLM",
        "author": "Yating Liu, Xing Su, Hao Wu, Sijin Li, Yuxi Cheng, Fengyuan Xu, and Sheng Zhong",
        "link": "http://arxiv.org/abs/2509.18934v1",
        "abstract": "Adversarial smart contracts, mostly on EVM-compatible chains like Ethereum\nand BSC, are deployed as EVM bytecode to exploit vulnerable smart contracts\ntypically for financial gains. Detecting such malicious contracts at the time\nof deployment is an important proactive strategy preventing loss from victim\ncontracts. It offers a better cost-benefit than detecting vulnerabilities on\ndiverse potential victims. However, existing works are not generic with limited\ndetection types and effectiveness due to imbalanced samples, while the emerging\nLLM technologies, which show its potentials in generalization, have two key\nproblems impeding its application in this task: hard digestion of compiled-code\ninputs, especially those with task-specific logic, and hard assessment of LLMs'\ncertainty in their binary answers, i.e., yes-or-no answers. Therefore, we\npropose a generic adversarial smart contracts detection framework FinDet, which\nleverages LLMs with two enhancements addressing above two problems. FinDet\ntakes as input only the EVM-bytecode contracts and identifies adversarial ones\namong them with high balanced accuracy. The first enhancement extracts concise\nsemantic intentions and high-level behavioral logic from the low-level bytecode\ninputs, unleashing the LLM reasoning capability restricted by the task input.\nThe second enhancement probes and measures the LLM uncertainty to its\nmulti-round answering to the same query, improving the LLM answering robustness\nfor binary classifications required by the task output. Our comprehensive\nevaluation shows that FinDet achieves a BAC of 0.9223 and a TPR of 0.8950,\nsignificantly outperforming existing baselines. It remains robust under\nchallenging conditions including unseen attack patterns, low-data settings, and\nfeature obfuscation. FinDet detects all 5 public and 20+ unreported adversarial\ncontracts in a 10-day real-world test, confirmed manually."
    },
    {
        "date": "2025-09",
        "title": "Frequency-Domain Decomposition and Recomposition for Robust Audio-Visual Segmentation",
        "author": "Yunzhe Shen, Kai Peng, Leiye Liu, Wei Ji, Jingjing Li, Miao Zhang, Yongri Piao, and Huchuan Lu",
        "link": "http://arxiv.org/abs/2509.18912v1",
        "abstract": "Audio-visual segmentation (AVS) plays a critical role in multimodal machine\nlearning by effectively integrating audio and visual cues to precisely segment\nobjects or regions within visual scenes. Recent AVS methods have demonstrated\nsignificant improvements. However, they overlook the inherent frequency-domain\ncontradictions between audio and visual modalities--the pervasively interfering\nnoise in audio high-frequency signals vs. the structurally rich details in\nvisual high-frequency signals. Ignoring these differences can result in\nsuboptimal performance. In this paper, we rethink the AVS task from a deeper\nperspective by reformulating AVS task as a frequency-domain decomposition and\nrecomposition problem. To this end, we introduce a novel Frequency-Aware\nAudio-Visual Segmentation (FAVS) framework consisting of two key modules:\nFrequency-Domain Enhanced Decomposer (FDED) module and Synergistic Cross-Modal\nConsistency (SCMC) module. FDED module employs a residual-based iterative\nfrequency decomposition to discriminate modality-specific semantics and\nstructural features, and SCMC module leverages a mixture-of-experts\narchitecture to reinforce semantic consistency and modality-specific feature\npreservation through dynamic expert routing. Extensive experiments demonstrate\nthat our FAVS framework achieves state-of-the-art performance on three\nbenchmark datasets, and abundant qualitative visualizations further verify the\neffectiveness of the proposed FDED and SCMC modules. The code will be released\nas open source upon acceptance of the paper."
    },
    {
        "date": "2025-09",
        "title": "Enhancing the Effectiveness and Durability of Backdoor Attacks in Federated Learning through Maximizing Task Distinction",
        "author": "Zhaoxin Wang, Handing Wang, Cong Tian, and Yaochu Jin",
        "link": "http://arxiv.org/abs/2509.18904v1",
        "abstract": "Federated learning allows multiple participants to collaboratively train a\ncentral model without sharing their private data. However, this distributed\nnature also exposes new attack surfaces. In particular, backdoor attacks allow\nattackers to implant malicious behaviors into the global model while\nmaintaining high accuracy on benign inputs. Existing attacks usually rely on\nfixed patterns or adversarial perturbations as triggers, which tightly couple\nthe main and backdoor tasks. This coupling makes them vulnerable to dilution by\nhonest updates and limits their persistence under federated defenses. In this\nwork, we propose an approach to decouple the backdoor task from the main task\nby dynamically optimizing the backdoor trigger within a min-max framework. The\ninner layer maximizes the performance gap between poisoned and benign samples,\nensuring that the contributions of benign users have minimal impact on the\nbackdoor. The outer process injects the adaptive triggers into the local model.\nWe evaluate our method on both computer vision and natural language tasks, and\ncompare it with six backdoor attack methods under six defense algorithms.\nExperimental results show that our method achieves good attack performance and\ncan be easily integrated into existing backdoor attack techniques."
    },
    {
        "date": "2025-09",
        "title": "DeblurSplat: SfM-free 3D Gaussian Splatting with Event Camera for Robust Deblurring",
        "author": "Pengteng Li, Yunfan Lu, Pinhao Song, Weiyu Guo, Huizai Yao, F. Richard Yu, and Hui Xiong",
        "link": "http://arxiv.org/abs/2509.18898v1",
        "abstract": "In this paper, we propose the first Structure-from-Motion (SfM)-free\ndeblurring 3D Gaussian Splatting method via event camera, dubbed DeblurSplat.\nWe address the motion-deblurring problem in two ways. First, we leverage the\npretrained capability of the dense stereo module (DUSt3R) to directly obtain\naccurate initial point clouds from blurred images. Without calculating camera\nposes as an intermediate result, we avoid the cumulative errors transfer from\ninaccurate camera poses to the initial point clouds' positions. Second, we\nintroduce the event stream into the deblur pipeline for its high sensitivity to\ndynamic change. By decoding the latent sharp images from the event stream and\nblurred images, we can provide a fine-grained supervision signal for scene\nreconstruction optimization. Extensive experiments across a range of scenes\ndemonstrate that DeblurSplat not only excels in generating high-fidelity novel\nviews but also achieves significant rendering efficiency compared to the SOTAs\nin deblur 3D-GS."
    },
    {
        "date": "2025-09",
        "title": "Attack for Defense: Adversarial Agents for Point Prompt Optimization Empowering Segment Anything Model",
        "author": "Xueyu Liu, Xiaoyi Zhang, Guangze Shi, Meilin Liu, Yexin Lai, Yongfei Wu, and Mingqiang Wei",
        "link": "http://arxiv.org/abs/2509.18891v1",
        "abstract": "Prompt quality plays a critical role in the performance of the Segment\nAnything Model (SAM), yet existing approaches often rely on heuristic or\nmanually crafted prompts, limiting scalability and generalization. In this\npaper, we propose Point Prompt Defender, an adversarial reinforcement learning\nframework that adopts an attack-for-defense paradigm to automatically optimize\npoint prompts. We construct a task-agnostic point prompt environment by\nrepresenting image patches as nodes in a dual-space graph, where edges encode\nboth physical and semantic distances. Within this environment, an attacker\nagent learns to activate a subset of prompts that maximally degrade SAM's\nsegmentation performance, while a defender agent learns to suppress these\ndisruptive prompts and restore accuracy. Both agents are trained using Deep\nQ-Networks with a reward signal based on segmentation quality variation. During\ninference, only the defender is deployed to refine arbitrary coarse prompt\nsets, enabling enhanced SAM segmentation performance across diverse tasks\nwithout retraining. Extensive experiments show that Point Prompt Defender\neffectively improves SAM's robustness and generalization, establishing a\nflexible, interpretable, and plug-and-play framework for prompt-based\nsegmentation."
    },
    {
        "date": "2025-09",
        "title": "R-CONV++: Uncovering Privacy Vulnerabilities through Analytical Gradient Inversion Attacks",
        "author": "Tamer Ahmed Eltaras, Qutaibah Malluhi, Alessandro Savino, Stefano Di Carlo, and Adnan Qayyum",
        "link": "http://arxiv.org/abs/2509.18871v1",
        "abstract": "Federated learning has emerged as a prominent privacy-preserving technique\nfor leveraging large-scale distributed datasets by sharing gradients instead of\nraw data. However, recent studies indicate that private training data can still\nbe exposed through gradient inversion attacks. While earlier analytical methods\nhave demonstrated success in reconstructing input data from fully connected\nlayers, their effectiveness significantly diminishes when applied to\nconvolutional layers, high-dimensional inputs, and scenarios involving multiple\ntraining examples. This paper extends our previous work \\cite{eltaras2024r} and\nproposes three advanced algorithms to broaden the applicability of gradient\ninversion attacks. The first algorithm presents a novel data leakage method\nthat efficiently exploits convolutional layer gradients, demonstrating that\neven with non-fully invertible activation functions, such as ReLU, training\nsamples can be analytically reconstructed directly from gradients without the\nneed to reconstruct intermediate layer outputs. Building on this foundation,\nthe second algorithm extends this analytical approach to support\nhigh-dimensional input data, substantially enhancing its utility across complex\nreal-world datasets. The third algorithm introduces an innovative analytical\nmethod for reconstructing mini-batches, addressing a critical gap in current\nresearch that predominantly focuses on reconstructing only a single training\nexample. Unlike previous studies that focused mainly on the weight constraints\nof convolutional layers, our approach emphasizes the pivotal role of gradient\nconstraints, revealing that successful attacks can be executed with fewer than\n5\\% of the constraints previously deemed necessary in certain layers."
    },
    {
        "date": "2025-09",
        "title": "Security Evaluation of Android apps in budget African Mobile Devices",
        "author": "Alioune Diallo, Anta Diop, Abdoul Kader Kabore, Jordan Samhi, Aleksandr Pilgun, Tegawend\u00e9 F. Bissyande, and Jacque Klein",
        "link": "http://arxiv.org/abs/2509.18800v1",
        "abstract": "Android's open-source nature facilitates widespread smartphone accessibility,\nparticularly in price-sensitive markets. System and vendor applications that\ncome pre-installed on budget Android devices frequently operate with elevated\nprivileges, yet they receive limited independent examination. To address this\ngap, we developed a framework that extracts APKs from physical devices and\napplies static analysis to identify privacy and security issues in embedded\nsoftware. Our study examined 1,544 APKs collected from seven African\nsmartphones. The analysis revealed that 145 applications (9%) disclose\nsensitive data, 249 (16%) expose critical components without sufficient\nsafeguards, and many present additional risks: 226 execute privileged or\ndangerous commands, 79 interact with SMS messages (read, send, or delete), and\n33 perform silent installation operations. We also uncovered a vendor-supplied\npackage that appears to transmit device identifiers and location details to an\nexternal third party. These results demonstrate that pre-installed applications\non widely distributed low-cost devices represent a significant and\nunderexplored threat to user security and privacy."
    },
    {
        "date": "2025-09",
        "title": "Detection of security smells in IaC scripts through semantics-aware code and language processing",
        "author": "Aicha War, Adnan A. Rawass, Abdoul K. Kabore, Jordan Samhi, Jacques Klein, and Tegawende F. Bissyande",
        "link": "http://arxiv.org/abs/2509.18790v1",
        "abstract": "Infrastructure as Code (IaC) automates the provisioning and management of IT\ninfrastructure through scripts and tools, streamlining software deployment.\nPrior studies have shown that IaC scripts often contain recurring security\nmisconfigurations, and several detection and mitigation approaches have been\nproposed. Most of these rely on static analysis, using statistical code\nrepresentations or Machine Learning (ML) classifiers to distinguish insecure\nconfigurations from safe code.\n  In this work, we introduce a novel approach that enhances static analysis\nwith semantic understanding by jointly leveraging natural language and code\nrepresentations. Our method builds on two complementary ML models: CodeBERT, to\ncapture semantics across code and text, and LongFormer, to represent long IaC\nscripts without losing contextual information. We evaluate our approach on\nmisconfiguration datasets from two widely used IaC tools, Ansible and Puppet.\nTo validate its effectiveness, we conduct two ablation studies (removing code\ntext from the natural language input and truncating scripts to reduce context)\nand compare against four large language models (LLMs) and prior work. Results\nshow that semantic enrichment substantially improves detection, raising\nprecision and recall from 0.46 and 0.79 to 0.92 and 0.88 on Ansible, and from\n0.55 and 0.97 to 0.87 and 0.75 on Puppet, respectively."
    },
    {
        "date": "2025-09",
        "title": "Security smells in infrastructure as code: a taxonomy update beyond the seven sins",
        "author": "Aicha War, Serge L. B. Nikiema, Jordan Samhi, Jacques Klein, and Tegawende F. Bissyande",
        "link": "http://arxiv.org/abs/2509.18761v1",
        "abstract": "Infrastructure as Code (IaC) has become essential for modern software\nmanagement, yet security flaws in IaC scripts can have severe consequences, as\nexemplified by the recurring exploits of Cloud Web Services. Prior work has\nrecognized the need to build a precise taxonomy of security smells in IaC\nscripts as a first step towards developing approaches to improve IaC security.\nThis first effort led to the unveiling of seven sins, limited by the focus on a\nsingle IaC tool as well as by the extensive, and potentially biased, manual\neffort that was required. We propose, in our work, to revisit this taxonomy:\nfirst, we extend the study of IaC security smells to a more diverse dataset\nwith scripts associated with seven popular IaC tools, including Terraform,\nAnsible, Chef, Puppet, Pulumi, Saltstack, and Vagrant; second, we bring in some\nautomation for the analysis by relying on an LLM. While we leverage LLMs for\ninitial pattern processing, all taxonomic decisions underwent systematic human\nvalidation and reconciliation with established security standards. Our study\nyields a comprehensive taxonomy of 62 security smell categories, significantly\nexpanding beyond the previously known seven. We demonstrate actionability by\nimplementing new security checking rules within linters for seven popular IaC\ntools, often achieving 1.00 precision score. Our evolution study of security\nsmells in GitHub projects reveals that these issues persist for extended\nperiods, likely due to inadequate detection and mitigation tools. This work\nprovides IaC practitioners with insights for addressing common security smells\nand systematically adopting DevSecOps practices to build safer infrastructure\ncode."
    },
    {
        "date": "2025-09",
        "title": "TriFusion-AE: Language-Guided Depth and LiDAR Fusion for Robust Point Cloud Processing",
        "author": "Susmit Neogi",
        "link": "http://arxiv.org/abs/2509.18743v1",
        "abstract": "LiDAR-based perception is central to autonomous driving and robotics, yet raw\npoint clouds remain highly vulnerable to noise, occlusion, and adversarial\ncorruptions. Autoencoders offer a natural framework for denoising and\nreconstruction, but their performance degrades under challenging real-world\nconditions. In this work, we propose TriFusion-AE, a multimodal cross-attention\nautoencoder that integrates textual priors, monocular depth maps from\nmulti-view images, and LiDAR point clouds to improve robustness. By aligning\nsemantic cues from text, geometric (depth) features from images, and spatial\nstructure from LiDAR, TriFusion-AE learns representations that are resilient to\nstochastic noise and adversarial perturbations. Interestingly, while showing\nlimited gains under mild perturbations, our model achieves significantly more\nrobust reconstruction under strong adversarial attacks and heavy noise, where\nCNN-based autoencoders collapse. We evaluate on the nuScenes-mini dataset to\nreflect realistic low-data deployment scenarios. Our multimodal fusion\nframework is designed to be model-agnostic, enabling seamless integration with\nany CNN-based point cloud autoencoder for joint representation learning."
    },
    {
        "date": "2025-09",
        "title": "MER-Inspector: Assessing model extraction risks from an attack-agnostic perspective",
        "author": "Xinwei Zhang, Haibo Hu, Qingqing Ye, Li Bai, and Huadi Zheng",
        "link": "http://arxiv.org/abs/2509.18578v1",
        "abstract": "Information leakage issues in machine learning-based Web applications have\nattracted increasing attention. While the risk of data privacy leakage has been\nrigorously analyzed, the theory of model function leakage, known as Model\nExtraction Attacks (MEAs), has not been well studied. In this paper, we are the\nfirst to understand MEAs theoretically from an attack-agnostic perspective and\nto propose analytical metrics for evaluating model extraction risks. By using\nthe Neural Tangent Kernel (NTK) theory, we formulate the linearized MEA as a\nregularized kernel classification problem and then derive the fidelity gap and\ngeneralization error bounds of the attack performance. Based on these\ntheoretical analyses, we propose a new theoretical metric called Model Recovery\nComplexity (MRC), which measures the distance of weight changes between the\nvictim and surrogate models to quantify risk. Additionally, we find that victim\nmodel accuracy, which shows a strong positive correlation with model extraction\nrisk, can serve as an empirical metric. By integrating these two metrics, we\npropose a framework, namely Model Extraction Risk Inspector (MER-Inspector), to\ncompare the extraction risks of models under different model architectures by\nutilizing relative metric values. We conduct extensive experiments on 16 model\narchitectures and 5 datasets. The experimental results demonstrate that the\nproposed metrics have a high correlation with model extraction risks, and\nMER-Inspector can accurately compare the extraction risks of any two models\nwith up to 89.58%."
    },
    {
        "date": "2025-09",
        "title": "Examining I2P Resilience: Effect of Centrality-based Attack",
        "author": "Kemi Akanbi, Sunkanmi Oluwadare, Jess Kropczynski, and Jacques Bou Abdo",
        "link": "http://arxiv.org/abs/2509.18572v1",
        "abstract": "This study examines the robustness of I2P, a well-regarded anonymous and\ndecentralized peer-to-peer network designed to ensure anonymity,\nconfidentiality, and circumvention of censorship. Unlike its more widely\nresearched counterpart, TOR, I2P's resilience has received less scholarly\nattention. Employing network analysis, this research evaluates I2P's\nsusceptibility to adversarial percolation. By utilizing the degree centrality\nas a measure of nodes' influence in the network, the finding suggests the\nnetwork is vulnerable to targeted disruptions. Before percolation, the network\nexhibited a density of 0.01065443 and an average path length of 6.842194. At\nthe end of the percolation process, the density decreased by approximately 10%,\nand the average path length increased by 33%, indicating a decline in\nefficiency and connectivity. These results highlight that even decentralized\nnetworks, such as I2P, exhibit structural fragility under targeted attacks,\nemphasizing the need for improved design strategies to enhance resilience\nagainst adversarial disruptions."
    },
    {
        "date": "2025-09",
        "title": "SEGA: A Transferable Signed Ensemble Gaussian Black-Box Attack against No-Reference Image Quality Assessment Models",
        "author": "Yujia Liu, Dingquan Li, and Tiejun Huang",
        "link": "http://arxiv.org/abs/2509.18546v1",
        "abstract": "No-Reference Image Quality Assessment (NR-IQA) models play an important role\nin various real-world applications. Recently, adversarial attacks against\nNR-IQA models have attracted increasing attention, as they provide valuable\ninsights for revealing model vulnerabilities and guiding robust system design.\nSome effective attacks have been proposed against NR-IQA models in white-box\nsettings, where the attacker has full access to the target model. However,\nthese attacks often suffer from poor transferability to unknown target models\nin more realistic black-box scenarios, where the target model is inaccessible.\nThis work makes the first attempt to address the challenge of low\ntransferability in attacking NR-IQA models by proposing a transferable Signed\nEnsemble Gaussian black-box Attack (SEGA). The main idea is to approximate the\ngradient of the target model by applying Gaussian smoothing to source models\nand ensembling their smoothed gradients. To ensure the imperceptibility of\nadversarial perturbations, SEGA further removes inappropriate perturbations\nusing a specially designed perturbation filter mask. Experimental results on\nthe CLIVE dataset demonstrate the superior transferability of SEGA, validating\nits effectiveness in enabling successful transfer-based black-box attacks\nagainst NR-IQA models."
    },
    {
        "date": "2025-09",
        "title": "GluMind: Multimodal Parallel Attention and Knowledge Retention for Robust Cross-Population Blood Glucose Forecasting",
        "author": "Ebrahim Farahmand, Reza Rahimi Azghan, Nooshin Taheri Chatrudi, Velarie Yaa Ansu-Baidoo, Eric Kim, Gautham Krishna Gudur, Mohit Malu, Owen Krueger, Edison Thomaz, Giulia Pedrielli, Pavan Turaga, and Hassan Ghasemzadeh",
        "link": "http://arxiv.org/abs/2509.18457v1",
        "abstract": "This paper proposes GluMind, a transformer-based multimodal framework\ndesigned for continual and long-term blood glucose forecasting. GluMind devises\ntwo attention mechanisms, including cross-attention and multi-scale attention,\nwhich operate in parallel and deliver accurate predictive performance.\nCross-attention effectively integrates blood glucose data with other\nphysiological and behavioral signals such as activity, stress, and heart rate,\naddressing challenges associated with varying sampling rates and their adverse\nimpacts on robust prediction. Moreover, the multi-scale attention mechanism\ncaptures long-range temporal dependencies. To mitigate catastrophic forgetting,\nGluMind incorporates a knowledge retention technique into the transformer-based\nforecasting model. The knowledge retention module not only enhances the model's\nability to retain prior knowledge but also boosts its overall forecasting\nperformance. We evaluate GluMind on the recently released AIREADI dataset,\nwhich contains behavioral and physiological data collected from healthy people,\nindividuals with prediabetes, and those with type 2 diabetes. We examine the\nperformance stability and adaptability of GluMind in learning continuously as\nnew patient cohorts are introduced. Experimental results show that GluMind\nconsistently outperforms other state-of-the-art forecasting models, achieving\napproximately 15% and 9% improvements in root mean squared error (RMSE) and\nmean absolute error (MAE), respectively."
    },
    {
        "date": "2025-09",
        "title": "VoxGuard: Evaluating User and Attribute Privacy in Speech via Membership Inference Attacks",
        "author": "Efthymios Tsaprazlis, Thanathai Lertpetchpun, Tiantian Feng, Sai Praneeth Karimireddy, and Shrikanth Narayanan",
        "link": "http://arxiv.org/abs/2509.18413v1",
        "abstract": "Voice anonymization aims to conceal speaker identity and attributes while\npreserving intelligibility, but current evaluations rely almost exclusively on\nEqual Error Rate (EER) that obscures whether adversaries can mount\nhigh-precision attacks. We argue that privacy should instead be evaluated in\nthe low false-positive rate (FPR) regime, where even a small number of\nsuccessful identifications constitutes a meaningful breach. To this end, we\nintroduce VoxGuard, a framework grounded in differential privacy and membership\ninference that formalizes two complementary notions: User Privacy, preventing\nspeaker re-identification, and Attribute Privacy, protecting sensitive traits\nsuch as gender and accent. Across synthetic and real datasets, we find that\ninformed adversaries, especially those using fine-tuned models and\nmax-similarity scoring, achieve orders-of-magnitude stronger attacks at low-FPR\ndespite similar EER. For attributes, we show that simple transparent attacks\nrecover gender and accent with near-perfect accuracy even after anonymization.\nOur results demonstrate that EER substantially underestimates leakage,\nhighlighting the need for low-FPR evaluation, and recommend VoxGuard as a\nbenchmark for evaluating privacy leakage."
    },
    {
        "date": "2025-09",
        "title": "SoK: A Beginner-Friendly Introduction to Fault Injection Attacks",
        "author": "Christopher Simon Liu, Fan Wang, Patrick Gould, and Carter Yagemann",
        "link": "http://arxiv.org/abs/2509.18341v1",
        "abstract": "Fault Injection is the study of observing how systems behave under unusual\nstress, environmental or otherwise. In practice, fault injection involves\ntesting the limits of computer systems and finding novel ways to potentially\nbreak cyber-physical security.\n  The contributions of this paper are three-fold. First, we provide a\nbeginner-friendly introduction to this research topic and an in-depth taxonomy\nof fault injection techniques. Second, we highlight the current\nstate-of-the-art and provide a cost-benefit analysis of each attack method.\nThird, for those interested in doing fault injection research, we provide a\nreplication analysis of an existing vulnerability detection tool and identify a\nresearch focus for future work."
    },
    {
        "date": "2025-09",
        "title": "SEQR: Secure and Efficient QR-based LoRA Routing",
        "author": "William Fleshman, and Benjamin Van Durme",
        "link": "http://arxiv.org/abs/2509.18093v1",
        "abstract": "Low-Rank Adaptation (LoRA) has become a standard technique for\nparameter-efficient fine-tuning of large language models, enabling large\nlibraries of LoRAs, each for a specific task or domain. Efficiently selecting\nthe correct LoRA adapter for a given input remains a challenge, particularly in\nsecure environments where supervised training of routers may raise privacy\nconcerns. Motivated by previous approaches, we formalize the goal of\nunsupervised LoRA routing in terms of activation norm maximization, providing a\ntheoretical framework for analysis. We demonstrate the discriminative power of\nactivation norms and introduce SEQR, an unsupervised LoRA routing algorithm\ndesigned to maximize efficiency while providing strict routing guarantees. SEQR\nprovably identifies the norm-maximizing adapter with significantly greater\nefficiency, making it a highly scalable and effective solution for dynamic LoRA\ncomposition. We validate our results through experiments that demonstrate\nimproved multi-task performance and efficiency."
    },
    {
        "date": "2025-09",
        "title": "Hybrid Reputation Aggregation: A Robust Defense Mechanism for Adversarial Federated Learning in 5G and Edge Network Environments",
        "author": "Saeid Sheikhi, Panos Kostakos, and Lauri Loven",
        "link": "http://arxiv.org/abs/2509.18044v1",
        "abstract": "Federated Learning (FL) in 5G and edge network environments face severe\nsecurity threats from adversarial clients. Malicious participants can perform\nlabel flipping, inject backdoor triggers, or launch Sybil attacks to corrupt\nthe global model. This paper introduces Hybrid Reputation Aggregation (HRA), a\nnovel robust aggregation mechanism designed to defend against diverse\nadversarial behaviors in FL without prior knowledge of the attack type. HRA\ncombines geometric anomaly detection with momentum-based reputation tracking of\nclients. In each round, it detects outlier model updates via distance-based\ngeometric analysis while continuously updating a trust score for each client\nbased on historical behavior. This hybrid approach enables adaptive filtering\nof suspicious updates and long-term penalization of unreliable clients,\ncountering attacks ranging from backdoor insertions to random noise Byzantine\nfailures. We evaluate HRA on a large-scale proprietary 5G network dataset (3M+\nrecords) and the widely used NF-CSE-CIC-IDS2018 benchmark under diverse\nadversarial attack scenarios. Experimental results reveal that HRA achieves\nrobust global model accuracy of up to 98.66% on the 5G dataset and 96.60% on\nNF-CSE-CIC-IDS2018, outperforming state-of-the-art aggregators such as Krum,\nTrimmed Mean, and Bulyan by significant margins. Our ablation studies further\ndemonstrate that the full hybrid system achieves 98.66% accuracy, while the\nanomaly-only and reputation-only variants drop to 84.77% and 78.52%,\nrespectively, validating the synergistic value of our dual-mechanism approach.\nThis demonstrates HRA's enhanced resilience and robustness in 5G/edge federated\nlearning deployments, even under significant adversarial conditions."
    },
    {
        "date": "2025-09",
        "title": "Detection of Misreporting Attacks on Software-Defined Immersive Environments",
        "author": "Sourya Saha, Md Nurul Absur, Shima Yousefi, and Saptarshi Debroy",
        "link": "http://arxiv.org/abs/2509.18040v1",
        "abstract": "The ability to centrally control network infrastructure using a programmable\nmiddleware has made Software-Defined Networking (SDN) ideal for emerging\napplications, such as immersive environments. However, such flexibility\nintroduces new vulnerabilities, such as switch misreporting led load imbalance,\nwhich in turn make such immersive environment vulnerable to severe quality\ndegradation. In this paper, we present a hybrid machine learning (ML)-based\nnetwork anomaly detection framework that identifies such stealthy misreporting\nby capturing temporal inconsistencies in switch-reported loads, and thereby\ncounter potentially catastrophic quality degradation of hosted immersive\napplication. The detection system combines unsupervised anomaly scoring with\nsupervised classification to robustly distinguish malicious behavior. Data\ncollected from a realistic testbed deployment under both benign and adversarial\nconditions is used to train and evaluate the model. Experimental results show\nthat the framework achieves high recall in detecting misreporting behavior,\nmaking it effective for early and reliable detection in SDN environments."
    },
    {
        "date": "2025-09",
        "title": "Robust, Online, and Adaptive Decentralized Gaussian Processes",
        "author": "Fernando Llorente, Daniel Waxman, Sanket Jantre, Nathan M. Urban, and Susan E. Minkoff",
        "link": "http://arxiv.org/abs/2509.18011v1",
        "abstract": "Gaussian processes (GPs) offer a flexible, uncertainty-aware framework for\nmodeling complex signals, but scale cubically with data, assume static targets,\nand are brittle to outliers, limiting their applicability in large-scale\nproblems with dynamic and noisy environments. Recent work introduced\ndecentralized random Fourier feature Gaussian processes (DRFGP), an online and\ndistributed algorithm that casts GPs in an information-filter form, enabling\nexact sequential inference and fully distributed computation without reliance\non a fusion center. In this paper, we extend DRFGP along two key directions:\nfirst, by introducing a robust-filtering update that downweights the impact of\natypical observations; and second, by incorporating a dynamic adaptation\nmechanism that adapts to time-varying functions. The resulting algorithm\nretains the recursive information-filter structure while enhancing stability\nand accuracy. We demonstrate its effectiveness on a large-scale Earth system\napplication, underscoring its potential for in-situ modeling."
    },
    {
        "date": "2025-09",
        "title": "Budgeted Adversarial Attack against Graph-Based Anomaly Detection in Sensor Networks",
        "author": "Sanju Xaviar, and Omid Ardakanian",
        "link": "http://arxiv.org/abs/2509.17987v1",
        "abstract": "Graph Neural Networks (GNNs) have emerged as powerful models for anomaly\ndetection in sensor networks, particularly when analyzing multivariate time\nseries. In this work, we introduce BETA, a novel grey-box evasion attack\ntargeting such GNN-based detectors, where the attacker is constrained to\nperturb sensor readings from a limited set of nodes, excluding the target\nsensor, with the goal of either suppressing a true anomaly or triggering a\nfalse alarm at the target node. BETA identifies the sensors most influential to\nthe target node's classification and injects carefully crafted adversarial\nperturbations into their features, all while maintaining stealth and respecting\nthe attacker's budget. Experiments on three real-world sensor network datasets\nshow that BETA reduces the detection accuracy of state-of-the-art GNN-based\ndetectors by 30.62 to 39.16% on average, and significantly outperforms baseline\nattack strategies, while operating within realistic constraints."
    },
    {
        "date": "2025-09",
        "title": "The Reverse File System: Towards open cost-effective secure WORM storage devices for logging",
        "author": "Gorka Guardiola M\u00fazquiz, Juan Gonz\u00e1lez-G\u00f3mez, and Enrique Soriano-Salvador",
        "link": "http://arxiv.org/abs/2509.17969v1",
        "abstract": "Write Once Read Many (WORM) properties for storage devices are desirable to\nensure data immutability for applications such as secure logging, regulatory\ncompliance, archival storage, and other types of backup systems. WORM devices\nguarantee that data, once written, cannot be altered or deleted. However,\nimplementing secure and compatible WORM storage remains a challenge.\nTraditional solutions often rely on specialized hardware, which is either\ncostly, closed, or inaccessible to the general public. Distributed approaches,\nwhile promising, introduce additional risks such as denial-of-service\nvulnerabilities and operational complexity. We introduce Socarrat, a novel,\ncost-effective, and local WORM storage solution that leverages a simple\nexternal USB device (specifically, a single-board computer running Linux with\nUSB On-The-Go support). The resulting device can be connected via USB,\nappearing as an ordinary external disk formatted with an ext4 or exFAT file\nsystem, without requiring any specialized software or drivers. By isolating the\nWORM enforcement mechanism in a dedicated USB hardware module, Socarrat\nsignificantly reduces the attack surface and ensures that even privileged\nattackers cannot modify or erase stored data. In addition to the WORM capacity,\nthe system is designed to be tamper-evident, becoming resilient against\nadvanced attacks. This work describes a novel approach, the Reverse File\nSystem, based on inferring the file system operations occurring at higher\nlayers in the host computer where Socarrat is mounted. The paper also describes\nthe current Socarrat prototype, implemented in Go and available as free/libre\nsoftware. Finally, it provides a complete evaluation of the logging performance\non different single-board computers."
    },
    {
        "date": "2025-09",
        "title": "SmaRT: Style-Modulated Robust Test-Time Adaptation for Cross-Domain Brain Tumor Segmentation in MRI",
        "author": "Yuanhan Wang, Yifei Chen, Shuo Jiang, Wenjing Yu, Mingxuan Liu, Beining Wu, Jinying Zong, Feiwei Qin, Changmiao Wang, and Qiyuan Tian",
        "link": "http://arxiv.org/abs/2509.17925v1",
        "abstract": "Reliable brain tumor segmentation in MRI is indispensable for treatment\nplanning and outcome monitoring, yet models trained on curated benchmarks often\nfail under domain shifts arising from scanner and protocol variability as well\nas population heterogeneity. Such gaps are especially severe in low-resource\nand pediatric cohorts, where conventional test-time or source-free adaptation\nstrategies often suffer from instability and structural inconsistency. We\npropose SmaRT, a style-modulated robust test-time adaptation framework that\nenables source-free cross-domain generalization. SmaRT integrates style-aware\naugmentation to mitigate appearance discrepancies, a dual-branch momentum\nstrategy for stable pseudo-label refinement, and structural priors enforcing\nconsistency, integrity, and connectivity. This synergy ensures both adaptation\nstability and anatomical fidelity under extreme domain shifts. Extensive\nevaluations on sub-Saharan Africa and pediatric glioma datasets show that SmaRT\nconsistently outperforms state-of-the-art methods, with notable gains in Dice\naccuracy and boundary precision. Overall, SmaRT bridges the gap between\nalgorithmic advances and equitable clinical applicability, supporting robust\ndeployment of MRI-based neuro-oncology tools in diverse clinical environments.\nOur source code is available at https://github.com/baiyou1234/SmaRT."
    },
    {
        "date": "2025-09",
        "title": "Lipschitz-Based Robustness Certification for Recurrent Neural Networks via Convex Relaxation",
        "author": "Paul Hamelbeck, and Johannes Schiffer",
        "link": "http://arxiv.org/abs/2509.17898v1",
        "abstract": "Robustness certification against bounded input noise or adversarial\nperturbations is increasingly important for deployment recurrent neural\nnetworks (RNNs) in safety-critical control applications. To address this\nchallenge, we present RNN-SDP, a relaxation based method that models the RNN's\nlayer interactions as a convex problem and computes a certified upper bound on\nthe Lipschitz constant via semidefinite programming (SDP). We also explore an\nextension that incorporates known input constraints to further tighten the\nresulting Lipschitz bounds. RNN-SDP is evaluated on a synthetic multi-tank\nsystem, with upper bounds compared to empirical estimates. While incorporating\ninput constraints yields only modest improvements, the general method produces\nreasonably tight and certifiable bounds, even as sequence length increases. The\nresults also underscore the often underestimated impact of initialization\nerrors, an important consideration for applications where models are frequently\nre-initialized, such as model predictive control (MPC)."
    },
    {
        "date": "2025-09",
        "title": "Quickest Change Detection in Continuous-Time in Presence of a Covert Adversary",
        "author": "Amir Reza Ramtin, Philippe Nain, and Don Towsley",
        "link": "http://arxiv.org/abs/2509.17778v1",
        "abstract": "We investigate the problem of covert quickest change detection in a\ncontinuous-time setting, where a Brownian motion experiences a drift change at\nan unknown time. Unlike classical formulations, we consider a covert adversary\nwho adjusts the post-change drift $\\mu = \\mu(\\gamma)$ as a function of the\nfalse alarm constraint parameter $\\gamma$, with the goal of remaining\nundetected for as long as possible. Leveraging the exact expressions for the\naverage detection delay (ADD) and average time to false alarm (AT2FA) known for\nthe continuous-time CuSum procedure, we rigorously analyze how the asymptotic\nbehavior of ADD evolves as $\\mu(\\gamma) \\to 0$ with increasing $\\gamma$. Our\nresults reveal that classical detection delay characterizations no longer hold\nin this regime. We derive sharp asymptotic expressions for the ADD under\nvarious convergence rates of $\\mu(\\gamma)$, identify precise conditions for\nmaintaining covertness, and characterize the total damage inflicted by the\nadversary. We show that the adversary achieves maximal damage when the drift\nscales as $\\mu(\\gamma) = \\Theta(1/\\sqrt{\\gamma})$, marking a fundamental\ntrade-off between stealth and impact in continuous-time detection systems."
    },
    {
        "date": "2025-09",
        "title": "I2VWM: Robust Watermarking for Image to Video Generation",
        "author": "Guanjie Wang, Zehua Ma, Han Fang, and Weiming Zhang",
        "link": "http://arxiv.org/abs/2509.17773v1",
        "abstract": "The rapid progress of image-guided video generation (I2V) has raised concerns\nabout its potential misuse in misinformation and fraud, underscoring the urgent\nneed for effective digital watermarking. While existing watermarking methods\ndemonstrate robustness within a single modality, they fail to trace source\nimages in I2V settings. To address this gap, we introduce the concept of Robust\nDiffusion Distance, which measures the temporal persistence of watermark\nsignals in generated videos. Building on this, we propose I2VWM, a cross-modal\nwatermarking framework designed to enhance watermark robustness across time.\nI2VWM leverages a video-simulation noise layer during training and employs an\noptical-flow-based alignment module during inference. Experiments on both\nopen-source and commercial I2V models demonstrate that I2VWM significantly\nimproves robustness while maintaining imperceptibility, establishing a new\nparadigm for cross-modal watermarking in the era of generative video.\n\\href{https://github.com/MrCrims/I2VWM-Robust-Watermarking-for-Image-to-Video-Generation}{Code\nReleased.}"
    },
    {
        "date": "2025-09",
        "title": "A State-Update Prompting Strategy for Efficient and Robust Multi-turn Dialogue",
        "author": "Ziyi Liu",
        "link": "http://arxiv.org/abs/2509.17766v1",
        "abstract": "Large Language Models (LLMs) struggle with information forgetting and\ninefficiency in long-horizon, multi-turn dialogues. To address this, we propose\na training-free prompt engineering method, the State-Update Multi-turn Dialogue\nStrategy. It utilizes \"State Reconstruction\" and \"History Remind\" mechanisms to\neffectively manage dialogue history. Our strategy shows strong performance\nacross multiple multi-hop QA datasets. For instance, on the HotpotQA dataset,\nit improves the core information filtering score by 32.6%, leading to a 14.1%\nincrease in the downstream QA score, while also reducing inference time by\n73.1% and token consumption by 59.4%. Ablation studies confirm the pivotal\nroles of both components. Our work offers an effective solution for optimizing\nLLMs in long-range interactions, providing new insights for developing more\nrobust Agents."
    },
    {
        "date": "2025-09",
        "title": "An Empirical Study on the Robustness of YOLO Models for Underwater Object Detection",
        "author": "Edwine Nabahirwa, Wei Song, Minghua Zhang, and Shufan Chen",
        "link": "http://arxiv.org/abs/2509.17561v1",
        "abstract": "Underwater object detection (UOD) remains a critical challenge in computer\nvision due to underwater distortions which degrade low-level features and\ncompromise the reliability of even state-of-the-art detectors. While YOLO\nmodels have become the backbone of real-time object detection, little work has\nsystematically examined their robustness under these uniquely challenging\nconditions. This raises a critical question: Are YOLO models genuinely robust\nwhen operating under the chaotic and unpredictable conditions of underwater\nenvironments? In this study, we present one of the first comprehensive\nevaluations of recent YOLO variants (YOLOv8-YOLOv12) across six simulated\nunderwater environments. Using a unified dataset of 10,000 annotated images\nfrom DUO and Roboflow100, we not only benchmark model robustness but also\nanalyze how distortions affect key low-level features such as texture, edges,\nand color. Our findings show that (1) YOLOv12 delivers the strongest overall\nperformance but is highly vulnerable to noise, and (2) noise disrupts edge and\ntexture features, explaining the poor detection performance in noisy images.\nClass imbalance is a persistent challenge in UOD. Experiments revealed that (3)\nimage counts and instance frequency primarily drive detection performance,\nwhile object appearance exerts only a secondary influence. Finally, we\nevaluated lightweight training-aware strategies: noise-aware sample injection,\nwhich improves robustness in both noisy and real-world conditions, and\nfine-tuning with advanced enhancement, which boosts accuracy in enhanced\ndomains but slightly lowers performance in original data, demonstrating strong\npotential for domain adaptation, respectively. Together, these insights provide\npractical guidance for building resilient and cost-efficient UOD systems."
    },
    {
        "date": "2025-09",
        "title": "Distributionally Robust Safety Verification of Neural Networks via Worst-Case CVaR",
        "author": "Masako Kishida",
        "link": "http://arxiv.org/abs/2509.17413v1",
        "abstract": "Ensuring the safety of neural networks under input uncertainty is a\nfundamental challenge in safety-critical applications. This paper builds on and\nexpands Fazlyab's quadratic-constraint (QC) and semidefinite-programming (SDP)\nframework for neural network verification to a distributionally robust and\ntail-risk-aware setting by integrating worst-case Conditional Value-at-Risk\n(WC-CVaR) over a moment-based ambiguity set with fixed mean and covariance. The\nresulting conditions remain SDP-checkable and explicitly account for tail risk.\nThis integration broadens input-uncertainty geometry-covering ellipsoids,\npolytopes, and hyperplanes-and extends applicability to safety-critical domains\nwhere tail-event severity matters. Applications to closed-loop reachability of\ncontrol systems and classification are demonstrated through numerical\nexperiments, illustrating how the risk level $\\varepsilon$ trades conservatism\nfor tolerance to tail events-while preserving the computational structure of\nprior QC/SDP methods for neural network verification and robustness analysis."
    },
    {
        "date": "2025-09",
        "title": "Robust Mixture Models for Algorithmic Fairness Under Latent Heterogeneity",
        "author": "Siqi Li, Molei Liu, Ziye Tian, Chuan Hong, and Nan Liu",
        "link": "http://arxiv.org/abs/2509.17411v1",
        "abstract": "Standard machine learning models optimized for average performance often fail\non minority subgroups and lack robustness to distribution shifts. This\nchallenge worsens when subgroups are latent and affected by complex\ninteractions among continuous and discrete features. We introduce ROME (RObust\nMixture Ensemble), a framework that learns latent group structure from data\nwhile optimizing for worst-group performance. ROME employs two approaches: an\nExpectation-Maximization algorithm for linear models and a neural\nMixture-of-Experts for nonlinear settings. Through simulations and experiments\non real-world datasets, we demonstrate that ROME significantly improves\nalgorithmic fairness compared to standard methods while maintaining competitive\naverage performance. Importantly, our method requires no predefined group\nlabels, making it practical when sources of disparities are unknown or\nevolving."
    },
    {
        "date": "2025-09",
        "title": "Robust Anomaly Detection Under Normality Distribution Shift in Dynamic Graphs",
        "author": "Xiaoyang Xu, Xiaofeng Lin, Koh Takeuchi, Kyohei Atarashi, and Hisashi Kashima",
        "link": "http://arxiv.org/abs/2509.17400v1",
        "abstract": "Anomaly detection in dynamic graphs is a critical task with broad real-world\napplications, including social networks, e-commerce, and cybersecurity. Most\nexisting methods assume that normal patterns remain stable over time; however,\nthis assumption often fails in practice due to the phenomenon we refer to as\nnormality distribution shift (NDS), where normal behaviors evolve over time.\nIgnoring NDS can lead models to misclassify shifted normal instances as\nanomalies, degrading detection performance. To tackle this issue, we propose\nWhENDS, a novel unsupervised anomaly detection method that aligns normal edge\nembeddings across time by estimating distributional statistics and applying\nwhitening transformations. Extensive experiments on four widely-used dynamic\ngraph datasets show that WhENDS consistently outperforms nine strong baselines,\nachieving state-of-the-art results and underscoring the importance of\naddressing NDS in dynamic graph anomaly detection."
    },
    {
        "date": "2025-09",
        "title": "SilentStriker:Toward Stealthy Bit-Flip Attacks on Large Language Models",
        "author": "Haotian Xu, Qingsong Peng, Jie Shi, Huadi Zheng, Yu Li, and Cheng Zhuo",
        "link": "http://arxiv.org/abs/2509.17371v2",
        "abstract": "The rapid adoption of large language models (LLMs) in critical domains has\nspurred extensive research into their security issues. While input manipulation\nattacks (e.g., prompt injection) have been well studied, Bit-Flip Attacks\n(BFAs) -- which exploit hardware vulnerabilities to corrupt model parameters\nand cause severe performance degradation -- have received far less attention.\nExisting BFA methods suffer from key limitations: they fail to balance\nperformance degradation and output naturalness, making them prone to discovery.\nIn this paper, we introduce SilentStriker, the first stealthy bit-flip attack\nagainst LLMs that effectively degrades task performance while maintaining\noutput naturalness. Our core contribution lies in addressing the challenge of\ndesigning effective loss functions for LLMs with variable output length and the\nvast output space. Unlike prior approaches that rely on output perplexity for\nattack loss formulation, which inevitably degrade output naturalness, we\nreformulate the attack objective by leveraging key output tokens as targets for\nsuppression, enabling effective joint optimization of attack effectiveness and\nstealthiness. Additionally, we employ an iterative, progressive search strategy\nto maximize attack efficacy. Experiments show that SilentStriker significantly\noutperforms existing baselines, achieving successful attacks without\ncompromising the naturalness of generated text."
    },
    {
        "date": "2025-09",
        "title": "GraphWeave: Interpretable and Robust Graph Generation via Random Walk Trajectories",
        "author": "Rahul Nandakumar, and Deepayan Chakrabarti",
        "link": "http://arxiv.org/abs/2509.17291v1",
        "abstract": "Given a set of graphs from some unknown family, we want to generate new\ngraphs from that family. Recent methods use diffusion on either graph\nembeddings or the discrete space of nodes and edges. However, simple changes to\nembeddings (say, adding noise) can mean uninterpretable changes in the graph.\nIn discrete-space diffusion, each step may add or remove many nodes/edges. It\nis hard to predict what graph patterns we will observe after many diffusion\nsteps. Our proposed method, called GraphWeave, takes a different approach. We\nseparate pattern generation and graph construction. To find patterns in the\ntraining graphs, we see how they transform vectors during random walks. We then\ngenerate new graphs in two steps. First, we generate realistic random walk\n\"trajectories\" which match the learned patterns. Then, we find the optimal\ngraph that fits these trajectories. The optimization infers all edges jointly,\nwhich improves robustness to errors. On four simulated and five real-world\nbenchmark datasets, GraphWeave outperforms existing methods. The most\nsignificant differences are on large-scale graph structures such as PageRank,\ncuts, communities, degree distributions, and flows. GraphWeave is also 10x\nfaster than its closest competitor. Finally, GraphWeave is simple, needing only\na transformer and standard optimizers."
    },
    {
        "date": "2025-09",
        "title": "Unaligned Incentives: Pricing Attacks Against Blockchain Rollups",
        "author": "Stefanos Chaliasos, Conner Swann, Sina Pilehchiha, Nicolas Mohnblatt, Benjamin Livshits, and Assimakis Kattis",
        "link": "http://arxiv.org/abs/2509.17126v1",
        "abstract": "Rollups have become the de facto scalability solution for Ethereum, securing\nmore than $55B in assets. They achieve scale by executing transactions on a\nLayer 2 ledger, while periodically posting data and finalizing state on the\nLayer 1, either optimistically or via validity proofs. Their fees must\nsimultaneously reflect the pricing of three resources: L2 costs (e.g.,\nexecution), L1 DA, and underlying L1 gas costs for batch settlement and proof\nverification. In this work, we identify critical mis-pricings in existing\nrollup transaction fee mechanisms (TFMs) that allow for two powerful attacks.\nFirstly, an adversary can saturate the L2's DA batch capacity with\ncompute-light data-heavy transactions, forcing low-gas transaction batches that\nenable both L2 DoS attacks, and finality-delay attacks. Secondly, by crafting\nprover killer transactions that maximize proving cycles relative to the gas\ncharges, an adversary can effectively stall proof generation, delaying finality\nby hours and inflicting prover-side economic losses to the rollup at a minimal\ncost.\n  We analyze the above attack vectors across the major Ethereum rollups,\nquantifying adversarial costs and protocol losses. We find that the first\nattack enables periodic DoS on rollups, lasting up to 30 minutes, at a cost\nbelow 2 ETH for most rollups. Moreover, we identify three rollups that are\nexposed to indefinite DoS at a cost of approximately 0.8 to 2.7 ETH per hour.\nThe attack can be further modified to increase finalization delays by a factor\nof about 1.45x to 2.73x, compared to direct L1 blob-stuffing, depending on the\nrollup's parameters. Furthermore, we find that the prover killer attack induces\na finalization latency increase of about 94x. Finally, we propose comprehensive\nmitigations to prevent these attacks and suggest how some practical uses of\nmulti-dimensional rollup TFMs can rectify the identified mis-pricing attacks."
    },
    {
        "date": "2025-09",
        "title": "Uncertainty-Supervised Interpretable and Robust Evidential Segmentation",
        "author": "Yuzhu Li, An Sui, Fuping Wu, and Xiahai Zhuang",
        "link": "http://arxiv.org/abs/2509.17098v1",
        "abstract": "Uncertainty estimation has been widely studied in medical image segmentation\nas a tool to provide reliability, particularly in deep learning approaches.\nHowever, previous methods generally lack effective supervision in uncertainty\nestimation, leading to low interpretability and robustness of the predictions.\nIn this work, we propose a self-supervised approach to guide the learning of\nuncertainty. Specifically, we introduce three principles about the\nrelationships between the uncertainty and the image gradients around boundaries\nand noise. Based on these principles, two uncertainty supervision losses are\ndesigned. These losses enhance the alignment between model predictions and\nhuman interpretation. Accordingly, we introduce novel quantitative metrics for\nevaluating the interpretability and robustness of uncertainty. Experimental\nresults demonstrate that compared to state-of-the-art approaches, the proposed\nmethod can achieve competitive segmentation performance and superior results in\nout-of-distribution (OOD) scenarios while significantly improving the\ninterpretability and robustness of uncertainty estimation. Code is available\nvia https://github.com/suiannaius/SURE."
    },
    {
        "date": "2025-09",
        "title": "Static Security Vulnerability Scanning of Proprietary and Open-Source Software: An Adaptable Process with Variants and Results",
        "author": "James J. Cusick",
        "link": "http://arxiv.org/abs/2509.16985v1",
        "abstract": "Software vulnerabilities remain a significant risk factor in achieving\nsecurity objectives within software development organizations. This is\nespecially true where either proprietary or open-source software (OSS) is\nincluded in the technological environment. In this paper an end-to-end process\nwith supporting methods and tools is presented. This industry proven generic\nprocess allows for the custom instantiation, configuration, and execution of\nroutinized code scanning for software vulnerabilities and their prioritized\nremediation. A select set of tools are described for this key DevSecOps\nfunction and placed into an iterative process. Examples of both industrial\nproprietary applications and open-source applications are provided including\nspecific vulnerability instances and a discussion of their treatment. The\nbenefits of each selected tool are considered, and alternative tools are also\nintroduced. Application of this method in a comprehensive SDLC model is also\nreviewed along with prospective enhancements from automation and the\napplication of advanced technologies including AI. Adoption of this method can\nbe achieved with minimal adjustments and with maximum flexibility for results\nin reducing source code vulnerabilities, reducing supply chain risk, and\nimproving the security profile of new or legacy solutions."
    },
    {
        "date": "2025-09",
        "title": "Temporal Logic-Based Multi-Vehicle Backdoor Attacks against Offline RL Agents in End-to-end Autonomous Driving",
        "author": "Xuan Chen, Shiwei Feng, Zikang Xiong, Shengwei An, Yunshu Mao, Lu Yan, Guanhong Tao, Wenbo Guo, and Xiangyu Zhang",
        "link": "http://arxiv.org/abs/2509.16950v1",
        "abstract": "Assessing the safety of autonomous driving (AD) systems against security\nthreats, particularly backdoor attacks, is a stepping stone for real-world\ndeployment. However, existing works mainly focus on pixel-level triggers that\nare impractical to deploy in the real world. We address this gap by introducing\na novel backdoor attack against the end-to-end AD systems that leverage one or\nmore other vehicles' trajectories as triggers. To generate precise trigger\ntrajectories, we first use temporal logic (TL) specifications to define the\nbehaviors of attacker vehicles. Configurable behavior models are then used to\ngenerate these trajectories, which are quantitatively evaluated and iteratively\nrefined based on the TL specifications. We further develop a negative training\nstrategy by incorporating patch trajectories that are similar to triggers but\nare designated not to activate the backdoor. It enhances the stealthiness of\nthe attack and refines the system's responses to trigger scenarios. Through\nextensive experiments on 5 offline reinforcement learning (RL) driving agents\nwith 6 trigger patterns and target action combinations, we demonstrate the\nflexibility and effectiveness of our proposed attack, showing the\nunder-exploration of existing end-to-end AD systems' vulnerabilities to such\ntrajectory-based backdoor attacks."
    },
    {
        "date": "2025-09",
        "title": "Security Vulnerabilities in Software Supply Chain for Autonomous Vehicles",
        "author": "Md Wasiul Haque, Md Erfan, Sagar Dasgupta, Md Rayhanur Rahman, and Mizanur Rahman",
        "link": "http://arxiv.org/abs/2509.16899v1",
        "abstract": "The interest in autonomous vehicles (AVs) for critical missions, including\ntransportation, rescue, surveillance, reconnaissance, and mapping, is growing\nrapidly due to their significant safety and mobility benefits. AVs consist of\ncomplex software systems that leverage artificial intelligence (AI), sensor\nfusion algorithms, and real-time data processing. Additionally, AVs are\nbecoming increasingly reliant on open-source software supply chains, such as\nopen-source packages, third-party software components, AI models, and\nthird-party datasets. Software security best practices in the automotive sector\nare often an afterthought for developers. Thus, significant cybersecurity risks\nexist in the software supply chain of AVs, particularly when secure software\ndevelopment practices are not rigorously implemented. For example, Upstream's\n2024 Automotive Cybersecurity Report states that 49.5% of cyberattacks in the\nautomotive sector are related to exploiting security vulnerabilities in\nsoftware systems. In this chapter, we analyze security vulnerabilities in\nopen-source software components in AVs. We utilize static analyzers on popular\nopen-source AV software, such as Autoware, Apollo, and openpilot. Specifically,\nthis chapter covers: (1) prevalent software security vulnerabilities of AVs;\nand (2) a comparison of static analyzer outputs for different open-source AV\nrepositories. The goal is to inform researchers, practitioners, and\npolicymakers about the existing security flaws in the commonplace open-source\nsoftware ecosystem in the AV domain. The findings would emphasize the necessity\nof security best practices earlier in the software development lifecycle to\nreduce cybersecurity risks, thereby ensuring system reliability, safeguarding\nuser data, and maintaining public trust in an increasingly automated world."
    },
    {
        "date": "2025-09",
        "title": "SOLAR: Switchable Output Layer for Accuracy and Robustness in Once-for-All Training",
        "author": "Shaharyar Ahmed Khan Tareen, Lei Fan, Xiaojing Yuan, Qin Lin, and Bin Hu",
        "link": "http://arxiv.org/abs/2509.16833v1",
        "abstract": "Once-for-All (OFA) training enables a single super-net to generate multiple\nsub-nets tailored to diverse deployment scenarios, supporting flexible\ntrade-offs among accuracy, robustness, and model-size without retraining.\nHowever, as the number of supported sub-nets increases, excessive parameter\nsharing in the backbone limits representational capacity, leading to degraded\ncalibration and reduced overall performance. To address this, we propose SOLAR\n(Switchable Output Layer for Accuracy and Robustness in Once-for-All Training),\na simple yet effective technique that assigns each sub-net a separate\nclassification head. By decoupling the logit learning process across sub-nets,\nthe Switchable Output Layer (SOL) reduces representational interference and\nimproves optimization, without altering the shared backbone. We evaluate SOLAR\non five datasets (SVHN, CIFAR-10, STL-10, CIFAR-100, and TinyImageNet) using\nfour super-net backbones (ResNet-34, WideResNet-16-8, WideResNet-40-2, and\nMobileNetV2) for two OFA training frameworks (OATS and SNNs). Experiments show\nthat SOLAR outperforms the baseline methods: compared to OATS, it improves\naccuracy of sub-nets up to 1.26 %, 4.71 %, 1.67 %, and 1.76 %, and robustness\nup to 9.01 %, 7.71 %, 2.72 %, and 1.26 % on SVHN, CIFAR-10, STL-10, and\nCIFAR-100, respectively. Compared to SNNs, it improves TinyImageNet accuracy by\nup to 2.93 %, 2.34 %, and 1.35 % using ResNet-34, WideResNet-16-8, and\nMobileNetV2 backbones (with 8 sub-nets), respectively."
    },
    {
        "date": "2025-09",
        "title": "IPF-RDA: An Information-Preserving Framework for Robust Data Augmentation",
        "author": "Suorong Yang, Hongchao Yang, Suhan Guo, Furao Shen, and Jian Zhao",
        "link": "http://arxiv.org/abs/2509.16678v1",
        "abstract": "Data augmentation is widely utilized as an effective technique to enhance the\ngeneralization performance of deep models. However, data augmentation may\ninevitably introduce distribution shifts and noises, which significantly\nconstrain the potential and deteriorate the performance of deep networks. To\nthis end, we propose a novel information-preserving framework, namely IPF-RDA,\nto enhance the robustness of data augmentations in this paper. IPF-RDA combines\nthe proposal of (i) a new class-discriminative information estimation algorithm\nthat identifies the points most vulnerable to data augmentation operations and\ncorresponding importance scores; And (ii) a new information-preserving scheme\nthat preserves the critical information in the augmented samples and ensures\nthe diversity of augmented data adaptively. We divide data augmentation methods\ninto three categories according to the operation types and integrate these\napproaches into our framework accordingly. After being integrated into our\nframework, the robustness of data augmentation methods can be enhanced and\ntheir full potential can be unleashed. Extensive experiments demonstrate that\nalthough being simple, IPF-RDA consistently improves the performance of\nnumerous commonly used state-of-the-art data augmentation methods with popular\ndeep models on a variety of datasets, including CIFAR-10, CIFAR-100,\nTiny-ImageNet, CUHK03, Market1501, Oxford Flower, and MNIST, where its\nperformance and scalability are stressed. The implementation is available at\nhttps://github.com/Jackbrocp/IPF-RDA."
    },
    {
        "date": "2025-09",
        "title": "Segment-to-Act: Label-Noise-Robust Action-Prompted Video Segmentation Towards Embodied Intelligence",
        "author": "Wenxin Li, Kunyu Peng, Di Wen, Ruiping Liu, Mengfei Duan, Kai Luo, and Kailun Yang",
        "link": "http://arxiv.org/abs/2509.16677v1",
        "abstract": "Embodied intelligence relies on accurately segmenting objects actively\ninvolved in interactions. Action-based video object segmentation addresses this\nby linking segmentation with action semantics, but it depends on large-scale\nannotations and prompts that are costly, inconsistent, and prone to multimodal\nnoise such as imprecise masks and referential ambiguity. To date, this\nchallenge remains unexplored. In this work, we take the first step by studying\naction-based video object segmentation under label noise, focusing on two\nsources: textual prompt noise (category flips and within-category noun\nsubstitutions) and mask annotation noise (perturbed object boundaries to mimic\nimprecise supervision). Our contributions are threefold. First, we introduce\ntwo types of label noises for the action-based video object segmentation task.\nSecond, we build up the first action-based video object segmentation under a\nlabel noise benchmark ActiSeg-NL and adapt six label-noise learning strategies\nto this setting, and establish protocols for evaluating them under textual,\nboundary, and mixed noise. Third, we provide a comprehensive analysis linking\nnoise types to failure modes and robustness gains, and we introduce a Parallel\nMask Head Mechanism (PMHM) to address mask annotation noise. Qualitative\nevaluations further reveal characteristic failure modes, including boundary\nleakage and mislocalization under boundary perturbations, as well as occasional\nidentity substitutions under textual flips. Our comparative analysis reveals\nthat different learning strategies exhibit distinct robustness profiles,\ngoverned by a foreground-background trade-off where some achieve balanced\nperformance while others prioritize foreground accuracy at the cost of\nbackground precision. The established benchmark and source code will be made\npublicly available at https://github.com/mylwx/ActiSeg-NL."
    },
    {
        "date": "2025-09",
        "title": "ADVEDM:Fine-grained Adversarial Attack against VLM-based Embodied Agents",
        "author": "Yichen Wang, Hangtao Zhang, Hewen Pan, Ziqi Zhou, Xianlong Wang, Peijin Guo, Lulu Xue, Shengshan Hu, Minghui Li, and Leo Yu Zhang",
        "link": "http://arxiv.org/abs/2509.16645v1",
        "abstract": "Vision-Language Models (VLMs), with their strong reasoning and planning\ncapabilities, are widely used in embodied decision-making (EDM) tasks in\nembodied agents, such as autonomous driving and robotic manipulation. Recent\nresearch has increasingly explored adversarial attacks on VLMs to reveal their\nvulnerabilities. However, these attacks either rely on overly strong\nassumptions, requiring full knowledge of the victim VLM, which is impractical\nfor attacking VLM-based agents, or exhibit limited effectiveness. The latter\nstems from disrupting most semantic information in the image, which leads to a\nmisalignment between the perception and the task context defined by system\nprompts. This inconsistency interrupts the VLM's reasoning process, resulting\nin invalid outputs that fail to affect interactions in the physical world. To\nthis end, we propose a fine-grained adversarial attack framework, ADVEDM, which\nmodifies the VLM's perception of only a few key objects while preserving the\nsemantics of the remaining regions. This attack effectively reduces conflicts\nwith the task context, making VLMs output valid but incorrect decisions and\naffecting the actions of agents, thus posing a more substantial safety threat\nin the physical world. We design two variants of based on this framework,\nADVEDM-R and ADVEDM-A, which respectively remove the semantics of a specific\nobject from the image and add the semantics of a new object into the image. The\nexperimental results in both general scenarios and EDM tasks demonstrate\nfine-grained control and excellent attack performance."
    },
    {
        "date": "2025-09",
        "title": "Reproducing a Security Risk Assessment Using Computer Aided Design",
        "author": "Avi Shaked",
        "link": "http://arxiv.org/abs/2509.16593v1",
        "abstract": "Security risk assessment is essential in establishing the trustworthiness and\nreliability of modern systems. While various security risk assessment\napproaches exist, prevalent applications are \"pen and paper\" implementations\nthat -- even if performed digitally using computers -- remain prone to\nauthoring mistakes and inconsistencies. Computer-aided design approaches can\ntransform security risk assessments into more rigorous and sustainable efforts.\nThis is of value to both industrial practitioners and researchers, who practice\nsecurity risk assessments to reflect on systems' designs and to contribute to\nthe discipline's state-of-the-art. In this article, we report the application\nof a model-based security design tool to reproduce a previously reported\nsecurity assessment. The main contributions are: 1) an independent attempt to\nreproduce a refereed article describing a real security risk assessment of a\nsystem; 2) comparison of a new computer-aided application with a previous\nnon-computer-aided application, based on a published, real-world case study; 3)\na showcase for the potential advantages -- for both practitioners and\nresearchers -- of using computer-aided design approaches to analyze reports and\nto assess systems."
    },
    {
        "date": "2025-09",
        "title": "SCAN: Self-Denoising Monte Carlo Annotation for Robust Process Reward Learning",
        "author": "Yuyang Ding, Xinyu Shi, Juntao Li, Xiaobo Liang, Zhaopeng Tu, and Min Zhang",
        "link": "http://arxiv.org/abs/2509.16548v1",
        "abstract": "Process reward models (PRMs) offer fine-grained, step-level evaluations that\nfacilitate deeper reasoning processes in large language models (LLMs), proving\neffective in complex tasks like mathematical reasoning. However, developing\nPRMs is challenging due to the high cost and limited scalability of\nhuman-annotated data. Synthetic data from Monte Carlo (MC) estimation is a\npromising alternative but suffers from a high noise ratio, which can cause\noverfitting and hinder large-scale training. In this work, we conduct a\npreliminary study on the noise distribution in synthetic data from MC\nestimation, identifying that annotation models tend to both underestimate and\noverestimate step correctness due to limitations in their annotation\ncapabilities. Building on these insights, we propose Self-Denoising Monte Carlo\nAnnotation (SCAN), an efficient data synthesis and noise-tolerant learning\nframework. Our key findings indicate that: (1) Even lightweight models (e.g.,\n1.5B parameters) can produce high-quality annotations through a self-denoising\nstrategy, enabling PRMs to achieve superior performance with only 6% the\ninference cost required by vanilla MC estimation. (2) With our robust learning\nstrategy, PRMs can effectively learn from this weak supervision, achieving a\n39.2 F1 score improvement (from 19.9 to 59.1) in ProcessBench. Despite using\nonly a compact synthetic dataset, our models surpass strong baselines,\nincluding those trained on large-scale human-annotated datasets such as\nPRM800K. Furthermore, performance continues to improve as we scale up the\nsynthetic data, highlighting the potential of SCAN for scalable,\ncost-efficient, and robust PRM training."
    },
    {
        "date": "2025-09",
        "title": "Train to Defend: First Defense Against Cryptanalytic Neural Network Parameter Extraction Attacks",
        "author": "Ashley Kurian, and Aydin Aysu",
        "link": "http://arxiv.org/abs/2509.16546v1",
        "abstract": "Neural networks are valuable intellectual property due to the significant\ncomputational cost, expert labor, and proprietary data involved in their\ndevelopment. Consequently, protecting their parameters is critical not only for\nmaintaining a competitive advantage but also for enhancing the model's security\nand privacy. Prior works have demonstrated the growing capability of\ncryptanalytic attacks to scale to deeper models. In this paper, we present the\nfirst defense mechanism against cryptanalytic parameter extraction attacks. Our\nkey insight is to eliminate the neuron uniqueness necessary for these attacks\nto succeed. We achieve this by a novel, extraction-aware training method.\nSpecifically, we augment the standard loss function with an additional\nregularization term that minimizes the distance between neuron weights within a\nlayer. Therefore, the proposed defense has zero area-delay overhead during\ninference. We evaluate the effectiveness of our approach in mitigating\nextraction attacks while analyzing the model accuracy across different\narchitectures and datasets. When re-trained with the same model architecture,\nthe results show that our defense incurs a marginal accuracy change of less\nthan 1% with the modified loss function. Moreover, we present a theoretical\nframework to quantify the success probability of the attack. When tested\ncomprehensively with prior attack settings, our defense demonstrated empirical\nsuccess for sustained periods of extraction, whereas unprotected networks are\nextracted between 14 minutes to 4 hours."
    },
    {
        "date": "2025-09",
        "title": "Overfitting in Adaptive Robust Optimization",
        "author": "Karl Zhu, and Dimitris Bertsimas",
        "link": "http://arxiv.org/abs/2509.16451v1",
        "abstract": "Adaptive robust optimization (ARO) extends static robust optimization by\nallowing decisions to depend on the realized uncertainty - weakly dominating\nstatic solutions within the modeled uncertainty set. However, ARO makes\nprevious constraints that were independent of uncertainty now dependent, making\nit vulnerable to additional infeasibilities when realizations fall outside the\nuncertainty set. This phenomenon of adaptive policies being brittle is\nanalogous to overfitting in machine learning. To mitigate against this, we\npropose assigning constraint-specific uncertainty set sizes, with harder\nconstraints given stronger probabilistic guarantees. Interpreted through the\noverfitting lens, this acts as regularization: tighter guarantees shrink\nadaptive coefficients to ensure stability, while looser ones preserve useful\nflexibility. This view motivates a principled approach to designing uncertainty\nsets that balances robustness and adaptivity."
    },
    {
        "date": "2025-09",
        "title": "B5GRoam: A Zero Trust Framework for Secure and Efficient On-Chain B5G Roaming",
        "author": "Mohamed Abdessamed Rezazi, Mouhamed Amine Bouchiha, Ahmed Mounsf Rafik Bendada, and Yacine Ghamri-Doudane",
        "link": "http://arxiv.org/abs/2509.16390v1",
        "abstract": "Roaming settlement in 5G and beyond networks demands secure, efficient, and\ntrustworthy mechanisms for billing reconciliation between mobile operators.\nWhile blockchain promises decentralization and auditability, existing solutions\nsuffer from critical limitations-namely, data privacy risks, assumptions of\nmutual trust, and scalability bottlenecks. To address these challenges, we\npresent B5GRoam, a novel on-chain and zero-trust framework for secure,\nprivacy-preserving, and scalable roaming settlements. B5GRoam introduces a\ncryptographically verifiable call detail record (CDR) submission protocol,\nenabling smart contracts to authenticate usage claims without exposing\nsensitive data. To preserve privacy, we integrate non-interactive\nzero-knowledge proofs (zkSNARKs) that allow on-chain verification of roaming\nactivity without revealing user or network details. To meet the high-throughput\ndemands of 5G environments, B5GRoam leverages Layer 2 zk-Rollups, significantly\nreducing gas costs while maintaining the security guarantees of Layer 1.\nExperimental results demonstrate a throughput of over 7,200 tx/s with strong\nprivacy and substantial cost savings. By eliminating intermediaries and\nenhancing verifiability, B5GRoam offers a practical and secure foundation for\ndecentralized roaming in future mobile networks."
    },
    {
        "date": "2025-09",
        "title": "Secure Confidential Business Information When Sharing Machine Learning Models",
        "author": "Yunfan Yang, Jiarong Xu, Hongzhe Zhang, and Xiao Fang",
        "link": "http://arxiv.org/abs/2509.16352v1",
        "abstract": "Model-sharing offers significant business value by enabling firms with\nwell-established Machine Learning (ML) models to monetize and share their\nmodels with others who lack the resources to develop ML models from scratch.\nHowever, concerns over data confidentiality remain a significant barrier to\nmodel-sharing adoption, as Confidential Property Inference (CPI) attacks can\nexploit shared ML models to uncover confidential properties of the model\nprovider's private model training data. Existing defenses often assume that CPI\nattacks are non-adaptive to the specific ML model they are targeting. This\nassumption overlooks a key characteristic of real-world adversaries: their\nresponsiveness, i.e., adversaries' ability to dynamically adjust their attack\nmodels based on the information of the target and its defenses. To overcome\nthis limitation, we propose a novel defense method that explicitly accounts for\nthe responsive nature of real-world adversaries via two methodological\ninnovations: a novel Responsive CPI attack and an attack-defense arms race\nframework. The former emulates the responsive behaviors of adversaries in the\nreal world, and the latter iteratively enhances both the target and attack\nmodels, ultimately producing a secure ML model that is robust against\nresponsive CPI attacks. Furthermore, we propose and integrate a novel\napproximate strategy into our defense, which addresses a critical computational\nbottleneck of defense methods and improves defense efficiency. Through\nextensive empirical evaluations across various realistic model-sharing\nscenarios, we demonstrate that our method outperforms existing defenses by more\neffectively defending against CPI attacks, preserving ML model utility, and\nreducing computational overhead."
    },
    {
        "date": "2025-09",
        "title": "Agentic Reasoning for Robust Vision Systems via Increased Test-Time Compute",
        "author": "Chung-En, Yu, Brian Jalaian, and Nathaniel D. Bastian",
        "link": "http://arxiv.org/abs/2509.16343v1",
        "abstract": "Developing trustworthy intelligent vision systems for high-stakes domains,\n\\emph{e.g.}, remote sensing and medical diagnosis, demands broad robustness\nwithout costly retraining. We propose \\textbf{Visual Reasoning Agent (VRA)}, a\ntraining-free, agentic reasoning framework that wraps off-the-shelf\nvision-language models \\emph{and} pure vision systems in a\n\\emph{Think--Critique--Act} loop. While VRA incurs significant additional\ntest-time computation, it achieves up to 40\\% absolute accuracy gains on\nchallenging visual reasoning benchmarks. Future work will optimize query\nrouting and early stopping to reduce inference overhead while preserving\nreliability in vision tasks."
    },
    {
        "date": "2025-09",
        "title": "Quantum Generative Adversarial Autoencoders: Learning latent representations for quantum data generation",
        "author": "Naipunnya Raj, Rajiv Sangle, Avinash Singh, and Krishna Kumar Sabapathy",
        "link": "http://arxiv.org/abs/2509.16186v1",
        "abstract": "In this work, we introduce the Quantum Generative Adversarial Autoencoder\n(QGAA), a quantum model for generation of quantum data. The QGAA consists of\ntwo components: (a) Quantum Autoencoder (QAE) to compress quantum states, and\n(b) Quantum Generative Adversarial Network (QGAN) to learn the latent space of\nthe trained QAE. This approach imparts the QAE with generative capabilities.\nThe utility of QGAA is demonstrated in two representative scenarios: (a)\ngeneration of pure entangled states, and (b) generation of parameterized\nmolecular ground states for H$_2$ and LiH. The average errors in the energies\nestimated by the trained QGAA are 0.02 Ha for H$_2$ and 0.06 Ha for LiH in\nsimulations upto 6 qubits. These results illustrate the potential of QGAA for\nquantum state generation, quantum chemistry, and near-term quantum machine\nlearning applications."
    },
    {
        "date": "2025-09",
        "title": "Robust Vision-Language Models via Tensor Decomposition: A Defense Against Adversarial Attacks",
        "author": "Het Patel, Muzammil Allie, Qian Zhang, Jia Chen, and Evangelos E. Papalexakis",
        "link": "http://arxiv.org/abs/2509.16163v1",
        "abstract": "Vision language models (VLMs) excel in multimodal understanding but are prone\nto adversarial attacks. Existing defenses often demand costly retraining or\nsignificant architecture changes. We introduce a lightweight defense using\ntensor decomposition suitable for any pre-trained VLM, requiring no retraining.\nBy decomposing and reconstructing vision encoder representations, it filters\nadversarial noise while preserving meaning. Experiments with CLIP on COCO and\nFlickr30K show improved robustness. On Flickr30K, it restores 12.3\\%\nperformance lost to attacks, raising Recall@1 accuracy from 7.5\\% to 19.8\\%. On\nCOCO, it recovers 8.1\\% performance, improving accuracy from 3.8\\% to 11.9\\%.\nAnalysis shows Tensor Train decomposition with low rank (8-32) and low residual\nstrength ($\\alpha=0.1-0.2$) is optimal. This method is a practical,\nplug-and-play solution with minimal overhead for existing VLMs."
    },
    {
        "date": "2025-09",
        "title": "Automated Cyber Defense with Generalizable Graph-based Reinforcement Learning Agents",
        "author": "Isaiah J. King, Benjamin Bowman, and H. Howie Huang",
        "link": "http://arxiv.org/abs/2509.16151v1",
        "abstract": "Deep reinforcement learning (RL) is emerging as a viable strategy for\nautomated cyber defense (ACD). The traditional RL approach represents networks\nas a list of computers in various states of safety or threat. Unfortunately,\nthese models are forced to overfit to specific network topologies, rendering\nthem ineffective when faced with even small environmental perturbations. In\nthis work, we frame ACD as a two-player context-based partially observable\nMarkov decision problem with observations represented as attributed graphs.\nThis approach allows our agents to reason through the lens of relational\ninductive bias. Agents learn how to reason about hosts interacting with other\nsystem entities in a more general manner, and their actions are understood as\nedits to the graph representing the environment. By introducing this bias, we\nwill show that our agents can better reason about the states of networks and\nzero-shot adapt to new ones. We show that this approach outperforms the\nstate-of-the-art by a wide margin, and makes our agents capable of defending\nnever-before-seen networks against a wide range of adversaries in a variety of\ncomplex, and multi-agent environments."
    },
    {
        "date": "2025-09",
        "title": "PRISM: Probabilistic and Robust Inverse Solver with Measurement-Conditioned Diffusion Prior for Blind Inverse Problems",
        "author": "Yuanyun Hu, Evan Bell, Guijin Wang, and Yu Sun",
        "link": "http://arxiv.org/abs/2509.16106v1",
        "abstract": "Diffusion models are now commonly used to solve inverse problems in\ncomputational imaging. However, most diffusion-based inverse solvers require\ncomplete knowledge of the forward operator to be used. In this work, we\nintroduce a novel probabilistic and robust inverse solver with\nmeasurement-conditioned diffusion prior (PRISM) to effectively address blind\ninverse problems. PRISM offers a technical advancement over current methods by\nincorporating a powerful measurement-conditioned diffusion model into a\ntheoretically principled posterior sampling scheme. Experiments on blind image\ndeblurring validate the effectiveness of the proposed method, demonstrating the\nsuperior performance of PRISM over state-of-the-art baselines in both image and\nblur kernel recovery."
    },
    {
        "date": "2025-09",
        "title": "Robust LLM Training Infrastructure at ByteDance",
        "author": "Borui Wan, Gaohong Liu, Zuquan Song, Jun Wang, Yun Zhang, Guangming Sheng, Shuguang Wang, Houmin Wei, Chenyuan Wang, Weiqiang Lou, Xi Yang, Mofan Zhang, Kaihua Jiang, Cheng Ren, Xiaoyun Zhi, Menghan Yu, Zhe Nan, Zhuolin Zheng, Baoquan Zhong, Qinlong Wang, Huan Yu, Jinxin Chi, Wang Zhang, Yuhan Li, Zixian Du, Sida Zhao, Yongqiang Zhang, Jingzhe Tang, Zherui Liu, Chuan Wu, Yanghua Peng, Haibin Lin, Wencong Xiao, Xin Liu, and Liang Xiang",
        "link": "http://arxiv.org/abs/2509.16293v1",
        "abstract": "The training scale of large language models (LLMs) has reached tens of\nthousands of GPUs and is still continuously expanding, enabling faster learning\nof larger models. Accompanying the expansion of the resource scale is the\nprevalence of failures (CUDA error, NaN values, job hang, etc.), which poses\nsignificant challenges to training stability. Any large-scale LLM training\ninfrastructure should strive for minimal training interruption, efficient fault\ndiagnosis, and effective failure tolerance to enable highly efficient\ncontinuous training. This paper presents ByteRobust, a large-scale GPU\ninfrastructure management system tailored for robust and stable training of\nLLMs. It exploits the uniqueness of LLM training process and gives top\npriorities to detecting and recovering failures in a routine manner. Leveraging\nparallelisms and characteristics of LLM training, ByteRobust enables\nhigh-capacity fault tolerance, prompt fault demarcation, and localization with\nan effective data-driven approach, comprehensively ensuring continuous and\nefficient training of LLM tasks. ByteRobust is deployed on a production GPU\nplatform with over 200,000 GPUs and achieves 97% ETTR for a three-month\ntraining job on 9,600 GPUs."
    },
    {
        "date": "2025-09",
        "title": "GLip: A Global-Local Integrated Progressive Framework for Robust Visual Speech Recognition",
        "author": "Tianyue Wang, Shuang Yang, Shiguang Shan, and Xilin Chen",
        "link": "http://arxiv.org/abs/2509.16031v1",
        "abstract": "Visual speech recognition (VSR), also known as lip reading, is the task of\nrecognizing speech from silent video. Despite significant advancements in VSR\nover recent decades, most existing methods pay limited attention to real-world\nvisual challenges such as illumination variations, occlusions, blurring, and\npose changes. To address these challenges, we propose GLip, a Global-Local\nIntegrated Progressive framework designed for robust VSR. GLip is built upon\ntwo key insights: (i) learning an initial \\textit{coarse} alignment between\nvisual features across varying conditions and corresponding speech content\nfacilitates the subsequent learning of \\textit{precise} visual-to-speech\nmappings in challenging environments; (ii) under adverse conditions, certain\nlocal regions (e.g., non-occluded areas) often exhibit more discriminative cues\nfor lip reading than global features. To this end, GLip introduces a dual-path\nfeature extraction architecture that integrates both global and local features\nwithin a two-stage progressive learning framework. In the first stage, the\nmodel learns to align both global and local visual features with corresponding\nacoustic speech units using easily accessible audio-visual data, establishing a\ncoarse yet semantically robust foundation. In the second stage, we introduce a\nContextual Enhancement Module (CEM) to dynamically integrate local features\nwith relevant global context across both spatial and temporal dimensions,\nrefining the coarse representations into precise visual-speech mappings. Our\nframework uniquely exploits discriminative local regions through a progressive\nlearning strategy, demonstrating enhanced robustness against various visual\nchallenges and consistently outperforming existing methods on the LRS2 and LRS3\nbenchmarks. We further validate its effectiveness on a newly introduced\nchallenging Mandarin dataset."
    },
    {
        "date": "2025-09",
        "title": "Towards Robust Visual Continual Learning with Multi-Prototype Supervision",
        "author": "Xiwei Liu, Yulong Li, Yichen Li, Xinlin Zhuang, Haolin Yang, Huifa Li, and Imran Razzak",
        "link": "http://arxiv.org/abs/2509.16011v1",
        "abstract": "Language-guided supervision, which utilizes a frozen semantic target from a\nPretrained Language Model (PLM), has emerged as a promising paradigm for visual\nContinual Learning (CL). However, relying on a single target introduces two\ncritical limitations: 1) semantic ambiguity, where a polysemous category name\nresults in conflicting visual representations, and 2) intra-class visual\ndiversity, where a single prototype fails to capture the rich variety of visual\nappearances within a class. To this end, we propose MuproCL, a novel framework\nthat replaces the single target with multiple, context-aware prototypes.\nSpecifically, we employ a lightweight LLM agent to perform category\ndisambiguation and visual-modal expansion to generate a robust set of semantic\nprototypes. A LogSumExp aggregation mechanism allows the vision model to\nadaptively align with the most relevant prototype for a given image. Extensive\nexperiments across various CL baselines demonstrate that MuproCL consistently\nenhances performance and robustness, establishing a more effective path for\nlanguage-guided continual learning."
    },
    {
        "date": "2025-09",
        "title": "Adversarial Graph Fusion for Incomplete Multi-view Semi-supervised Learning with Tensorial Imputation",
        "author": "Zhangqi Jiang, Tingjin Luo, Xu Yang, and Xinyan Liang",
        "link": "http://arxiv.org/abs/2509.15955v1",
        "abstract": "View missing remains a significant challenge in graph-based multi-view\nsemi-supervised learning, hindering their real-world applications. To address\nthis issue, traditional methods introduce a missing indicator matrix and focus\non mining partial structure among existing samples in each view for label\npropagation (LP). However, we argue that these disregarded missing samples\nsometimes induce discontinuous local structures, i.e., sub-clusters, breaking\nthe fundamental smoothness assumption in LP. Consequently, such a Sub-Cluster\nProblem (SCP) would distort graph fusion and degrade classification\nperformance. To alleviate SCP, we propose a novel incomplete multi-view\nsemi-supervised learning method, termed AGF-TI. Firstly, we design an\nadversarial graph fusion scheme to learn a robust consensus graph against the\ndistorted local structure through a min-max framework. By stacking all\nsimilarity matrices into a tensor, we further recover the incomplete structure\nfrom the high-order consistency information based on the low-rank tensor\nlearning. Additionally, the anchor-based strategy is incorporated to reduce the\ncomputational complexity. An efficient alternative optimization algorithm\ncombining a reduced gradient descent method is developed to solve the\nformulated objective, with theoretical convergence. Extensive experimental\nresults on various datasets validate the superiority of our proposed AGF-TI as\ncompared to state-of-the-art methods. Code is available at\nhttps://github.com/ZhangqiJiang07/AGF_TI."
    },
    {
        "date": "2025-09",
        "title": "An Adversarial Robust Behavior Sequence Anomaly Detection Approach Based on Critical Behavior Unit Learning",
        "author": "Dongyang Zhan, Kai Tan, Lin Ye, Xiangzhan Yu, Hongli Zhang, and Zheng He",
        "link": "http://arxiv.org/abs/2509.15756v1",
        "abstract": "Sequential deep learning models (e.g., RNN and LSTM) can learn the sequence\nfeatures of software behaviors, such as API or syscall sequences. However,\nrecent studies have shown that these deep learning-based approaches are\nvulnerable to adversarial samples. Attackers can use adversarial samples to\nchange the sequential characteristics of behavior sequences and mislead malware\nclassifiers. In this paper, an adversarial robustness anomaly detection method\nbased on the analysis of behavior units is proposed to overcome this problem.\nWe extract related behaviors that usually perform a behavior intention as a\nbehavior unit, which contains the representative semantic information of local\nbehaviors and can be used to improve the robustness of behavior analysis. By\nlearning the overall semantics of each behavior unit and the contextual\nrelationships among behavior units based on a multilevel deep learning model,\nour approach can mitigate perturbation attacks that target local and\nlarge-scale behaviors. In addition, our approach can be applied to both\nlow-level and high-level behavior logs (e.g., API and syscall logs). The\nexperimental results show that our approach outperforms all the compared\nmethods, which indicates that our approach has better performance against\nobfuscation attacks."
    },
    {
        "date": "2025-09",
        "title": "Flying Drones to Locate Cyber-Attackers in LoRaWAN Metropolitan Networks",
        "author": "Matteo Repetto, Enrico Cambiaso, Fabio Patrone, and Sandro Zappatore",
        "link": "http://arxiv.org/abs/2509.15725v1",
        "abstract": "Today, many critical services and industrial systems rely on wireless\nnetworks for interaction with the IoT, hence becoming vulnerable to a broad\nnumber of cyber-threats. While detecting this kind of attacks is not difficult\nwith common cyber-security tools, and even trivial for jamming, finding their\norigin and identifying culprits is almost impossible today, yet indispensable\nto stop them, especially when attacks are generated with portable or self-made\ndevices that continuously move around. To address this open challenge, the\nFOLLOWME project investigates the feasibility of using UAV to locate and even\nchase attackers during illicit usage of the radio spectrum. The main objective\nis to develop a cyber-physical security framework that integrates network\ntelemetry with wireless localization. The former triggers alarms in case of\nanomalies or known attack patterns and provides a coarse-grained indication of\nthe physical area (i.e., the position of affected access gateways), whereas the\nlatter systematically scans such area to identify the exact location of the\nattacker. The project will specifically address long-range metropolitan area\nnetworks and focus on the LoRaWAN protocol, which is the typical scenario for\nSmart City services."
    },
    {
        "date": "2025-09",
        "title": "Inference Attacks on Encrypted Online Voting via Traffic Analysis",
        "author": "Anastasiia Belousova, Francesco Marchiori, and Mauro Conti",
        "link": "http://arxiv.org/abs/2509.15694v1",
        "abstract": "Online voting enables individuals to participate in elections remotely,\noffering greater efficiency and accessibility in both governmental and\norganizational settings. As this method gains popularity, ensuring the security\nof online voting systems becomes increasingly vital, as the systems supporting\nit must satisfy a demanding set of security requirements. Most research in this\narea emphasizes the design and verification of cryptographic protocols to\nprotect voter integrity and system confidentiality. However, other vectors,\nsuch as network traffic analysis, remain relatively understudied, even though\nthey may pose significant threats to voter privacy and the overall\ntrustworthiness of the system.\n  In this paper, we examine how adversaries can exploit metadata from encrypted\nnetwork traffic to uncover sensitive information during online voting. Our\nanalysis reveals that, even without accessing the encrypted content, it is\npossible to infer critical voter actions, such as whether a person votes, the\nexact moment a ballot is submitted, and whether the ballot is valid or spoiled.\nWe test these attacks with both rule-based techniques and machine learning\nmethods. We evaluate our attacks on two widely used online voting platforms,\none proprietary and one partially open source, achieving classification\naccuracy as high as 99.5%. These results expose a significant privacy\nvulnerability that threatens key properties of secure elections, including\nvoter secrecy and protection against coercion or vote-buying. We explore\nmitigations to our attacks, demonstrating that countermeasures such as payload\npadding and timestamp equalization can substantially limit their effectiveness."
    },
    {
        "date": "2025-09",
        "title": "Future-Proofing Cloud Security Against Quantum Attacks: Risk, Transition, and Mitigation Strategies",
        "author": "Yaser Baseri, Abdelhakim Hafid, and Arash Habibi Lashkari",
        "link": "http://arxiv.org/abs/2509.15653v2",
        "abstract": "Quantum Computing (QC) introduces a transformative threat to digital\nsecurity, with the potential to compromise widely deployed classical\ncryptographic systems. This survey offers a comprehensive and systematic\nexamination of quantumsafe security for Cloud Computing (CC), focusing on the\nvulnerabilities, transition strategies, and mitigation mechanisms required to\nsecure cloud infrastructures in the quantum era. We evaluated the landscape of\nquantum threats across the entire CC stack, demonstrating how quantum\nalgorithms can undermine classical encryption and compromise cloud security at\nmultiple architectural layers. Using a structured risk assessment methodology\nbased on the STRIDE model, we evaluate quantum-induced attack vectors and their\nimpact on cloud environments. To address these challenges, we propose a layered\nsecurity framework that integrates hybrid cryptographic transition strategies,\ncryptographic agility, and proactive risk mitigation. We analyze the\npreparation and implementation approaches of the major Cloud Service Providers\n(CSPs), including AWS, Azure and GCP, synthesizing platform-specific\ninitiatives toward Post-Quantum Cryptography (PQC). Furthermore, we provide a\ndetailed evaluation of standardized PQC algorithms, exploring their resilience\nto side-channel and active attacks within cloud-native deployments. This survey\nserves as a strategic reference for cloud architects, policymakers, and\nresearchers, offering actionable insights for navigating the complex transition\nto quantum-resilient cloud systems. We conclude by identifying six key future\nresearch directions: standardization and interoperability, performance and\nscalability, implementation security, integration with emerging technologies,\nsystemic preparedness, and crypto-agile migration frameworks."
    },
    {
        "date": "2025-09",
        "title": "Cuckoo Attack: Stealthy and Persistent Attacks Against AI-IDE",
        "author": "Xinpeng Liu, Junming Liu, Peiyu Liu, Han Zheng, Qinying Wang, Mathias Payer, Shouling Ji, and Wenhai Wang",
        "link": "http://arxiv.org/abs/2509.15572v1",
        "abstract": "Modern AI-powered Integrated Development Environments (AI-IDEs) are\nincreasingly defined by an Agent-centric architecture, where an LLM-powered\nAgent is deeply integrated to autonomously execute complex tasks. This tight\nintegration, however, also introduces a new and critical attack surface.\nAttackers can exploit these components by injecting malicious instructions into\nuntrusted external sources, effectively hijacking the Agent to perform harmful\noperations beyond the user's intention or awareness. This emerging threat has\nquickly attracted research attention, leading to various proposed attack\nvectors, such as hijacking Model Context Protocol (MCP) Servers to access\nprivate data. However, most existing approaches lack stealth and persistence,\nlimiting their practical impact.\n  We propose the Cuckoo Attack, a novel attack that achieves stealthy and\npersistent command execution by embedding malicious payloads into configuration\nfiles. These files, commonly used in AI-IDEs, execute system commands during\nroutine operations, without displaying execution details to the user. Once\nconfigured, such files are rarely revisited unless an obvious runtime error\noccurs, creating a blind spot for attackers to exploit. We formalize our attack\nparadigm into two stages, including initial infection and persistence. Based on\nthese stages, we analyze the practicality of the attack execution process and\nidentify the relevant exploitation techniques. Furthermore, we analyze the\nimpact of Cuckoo Attack, which can not only invade the developer's local\ncomputer but also achieve supply chain attacks through the spread of\nconfiguration files. We contribute seven actionable checkpoints for vendors to\nevaluate their product security. The critical need for these checks is\ndemonstrated by our end-to-end Proof of Concept, which validated the proposed\nattack across nine mainstream Agent and AI-IDE pairs."
    },
    {
        "date": "2025-09",
        "title": "Adversarially Robust Assembly Language Model for Packed Executables Detection",
        "author": "Shijia Li, Jiang Ming, Lanqing Liu, Longwei Yang, Ni Zhang, and Chunfu Jia",
        "link": "http://arxiv.org/abs/2509.15499v1",
        "abstract": "Detecting packed executables is a critical component of large-scale malware\nanalysis and antivirus engine workflows, as it identifies samples that warrant\ncomputationally intensive dynamic unpacking to reveal concealed malicious\nbehavior. Traditionally, packer detection techniques have relied on empirical\nfeatures, such as high entropy or specific binary patterns. However, these\nempirical, feature-based methods are increasingly vulnerable to evasion by\nadversarial samples or unknown packers (e.g., low-entropy packers).\nFurthermore, the dependence on expert-crafted features poses challenges in\nsustaining and evolving these methods over time.\n  In this paper, we examine the limitations of existing packer detection\nmethods and propose Pack-ALM, a novel deep-learning-based approach for\ndetecting packed executables. Inspired by the linguistic concept of\ndistinguishing between real and pseudo words, we reformulate packer detection\nas a task of differentiating between legitimate and \"pseudo\" instructions. To\nachieve this, we preprocess native data and packed data into \"pseudo\"\ninstructions and design a pre-trained assembly language model that recognizes\nfeatures indicative of packed data. We evaluate Pack-ALM against leading\nindustrial packer detection tools and state-of-the-art assembly language\nmodels. Extensive experiments on over 37,000 samples demonstrate that Pack-ALM\neffectively identifies packed binaries, including samples created with\nadversarial or previously unseen packing techniques. Moreover, Pack-ALM\noutperforms traditional entropy-based methods and advanced assembly language\nmodels in both detection accuracy and adversarial robustness."
    },
    {
        "date": "2025-09",
        "title": "Explainable AI-Enhanced Supervisory Control for Robust Multi-Agent Robotic Systems",
        "author": "Reza Pirayeshshirazinezhad, and Nima Fathi",
        "link": "http://arxiv.org/abs/2509.15491v1",
        "abstract": "We present an explainable AI-enhanced supervisory control framework for\nmulti-agent robotics that combines (i) a timed-automata supervisor for safe,\nauditable mode switching, (ii) robust continuous control (Lyapunov-based\ncontroller for large-angle maneuver; sliding-mode controller (SMC) with\nboundary layers for precision and disturbance rejection), and (iii) an\nexplainable predictor that maps mission context to gains and expected\nperformance (energy, error). Monte Carlo-driven optimization provides the\ntraining data, enabling transparent real-time trade-offs.\n  We validated the approach in two contrasting domains, spacecraft formation\nflying and autonomous underwater vehicles (AUVs). Despite different\nenvironments (gravity/actuator bias vs. hydrodynamic drag/currents), both share\nuncertain six degrees of freedom (6-DOF) rigid-body dynamics, relative motion,\nand tight tracking needs, making them representative of general robotic\nsystems. In the space mission, the supervisory logic selects parameters that\nmeet mission criteria. In AUV leader-follower tests, the same SMC structure\nmaintains a fixed offset under stochastic currents with bounded steady error.\nIn spacecraft validation, the SMC controller achieved submillimeter alignment\nwith 21.7% lower tracking error and 81.4% lower energy consumption compared to\nProportional-Derivative PD controller baselines. At the same time, in AUV\ntests, SMC maintained bounded errors under stochastic currents. These results\nhighlight both the portability and the interpretability of the approach for\nsafety-critical, resource-constrained multi-agent robotics."
    },
    {
        "date": "2025-09",
        "title": "CAGE: Continuity-Aware edGE Network Unlocks Robust Floorplan Reconstruction",
        "author": "Yiyi Liu, Chunyang Liu, Weiqin Jiao, Bojian Wu, Fashuai Li, and Biao Xiong",
        "link": "http://arxiv.org/abs/2509.15459v1",
        "abstract": "We present \\textbf{CAGE} (\\textit{Continuity-Aware edGE}) network, a\n\\textcolor{red}{robust} framework for reconstructing vector floorplans directly\nfrom point-cloud density maps. Traditional corner-based polygon representations\nare highly sensitive to noise and incomplete observations, often resulting in\nfragmented or implausible layouts. Recent line grouping methods leverage\nstructural cues to improve robustness but still struggle to recover fine\ngeometric details. To address these limitations, we propose a \\textit{native}\nedge-centric formulation, modeling each wall segment as a directed,\ngeometrically continuous edge. This representation enables inference of\ncoherent floorplan structures, ensuring watertight, topologically valid room\nboundaries while improving robustness and reducing artifacts. Towards this\ndesign, we develop a dual-query transformer decoder that integrates perturbed\nand latent queries within a denoising framework, which not only stabilizes\noptimization but also accelerates convergence. Extensive experiments on\nStructured3D and SceneCAD show that \\textbf{CAGE} achieves state-of-the-art\nperformance, with F1 scores of 99.1\\% (rooms), 91.7\\% (corners), and 89.3\\%\n(angles). The method also demonstrates strong cross-dataset generalization,\nunderscoring the efficacy of our architectural innovations. Code and pretrained\nmodels will be released upon acceptance."
    },
    {
        "date": "2025-09",
        "title": "Impact of Phonetics on Speaker Identity in Adversarial Voice Attack",
        "author": "Daniyal Kabir Dar, Qiben Yan, Li Xiao, and Arun Ross",
        "link": "http://arxiv.org/abs/2509.15437v1",
        "abstract": "Adversarial perturbations in speech pose a serious threat to automatic speech\nrecognition (ASR) and speaker verification by introducing subtle waveform\nmodifications that remain imperceptible to humans but can significantly alter\nsystem outputs. While targeted attacks on end-to-end ASR models have been\nwidely studied, the phonetic basis of these perturbations and their effect on\nspeaker identity remain underexplored. In this work, we analyze adversarial\naudio at the phonetic level and show that perturbations exploit systematic\nconfusions such as vowel centralization and consonant substitutions. These\ndistortions not only mislead transcription but also degrade phonetic cues\ncritical for speaker verification, leading to identity drift. Using DeepSpeech\nas our ASR target, we generate targeted adversarial examples and evaluate their\nimpact on speaker embeddings across genuine and impostor samples. Results\nacross 16 phonetically diverse target phrases demonstrate that adversarial\naudio induces both transcription errors and identity drift, highlighting the\nneed for phonetic-aware defenses to ensure the robustness of ASR and speaker\nrecognition systems."
    },
    {
        "date": "2025-09",
        "title": "ORCA: Agentic Reasoning For Hallucination and Adversarial Robustness in Vision-Language Models",
        "author": "Chung-En Johnny Yu, Hsuan-Chih, Chen, Brian Jalaian, and Nathaniel D. Bastian",
        "link": "http://arxiv.org/abs/2509.15435v1",
        "abstract": "Large Vision-Language Models (LVLMs) exhibit strong multimodal capabilities\nbut remain vulnerable to hallucinations from intrinsic errors and adversarial\nattacks from external exploitations, limiting their reliability in real-world\napplications. We present ORCA, an agentic reasoning framework that improves the\nfactual accuracy and adversarial robustness of pretrained LVLMs through\ntest-time structured inference reasoning with a suite of small vision models\n(less than 3B parameters). ORCA operates via an Observe--Reason--Critique--Act\nloop, querying multiple visual tools with evidential questions, validating\ncross-model inconsistencies, and refining predictions iteratively without\naccess to model internals or retraining. ORCA also stores intermediate\nreasoning traces, which supports auditable decision-making. Though designed\nprimarily to mitigate object-level hallucinations, ORCA also exhibits emergent\nadversarial robustness without requiring adversarial training or defense\nmechanisms. We evaluate ORCA across three settings: (1) clean images on\nhallucination benchmarks, (2) adversarially perturbed images without defense,\nand (3) adversarially perturbed images with defense applied. On the POPE\nhallucination benchmark, ORCA improves standalone LVLM performance by +3.64\\%\nto +40.67\\% across different subsets. Under adversarial perturbations on POPE,\nORCA achieves an average accuracy gain of +20.11\\% across LVLMs. When combined\nwith defense techniques on adversarially perturbed AMBER images, ORCA further\nimproves standalone LVLM performance, with gains ranging from +1.20\\% to\n+48.00\\% across evaluation metrics. These results demonstrate that ORCA offers\na promising path toward building more reliable and robust multimodal systems."
    },
    {
        "date": "2025-09",
        "title": "LLM-Driven SAST-Genius: A Hybrid Static Analysis Framework for Comprehensive and Actionable Security",
        "author": "Vaibhav Agrawal, and Kiarash Ahi",
        "link": "http://arxiv.org/abs/2509.15433v2",
        "abstract": "This report examines the synergy between Large Language Models (LLMs) and\nStatic Application Security Testing (SAST) to improve vulnerability discovery.\nTraditional SAST tools, while effective for proactive security, are limited by\nhigh false-positive rates and a lack of contextual understanding. Conversely,\nLLMs excel at code analysis and pattern recognition but can be prone to\ninconsistencies and hallucinations. By integrating these two technologies, a\nmore intelligent and efficient system is created. This combination moves beyond\nmere vulnerability detection optimization, transforming security into a deeply\nintegrated, contextual process that provides tangible benefits like improved\ntriage, dynamic bug descriptions, bug validation via exploit generation and\nenhanced analysis of complex codebases. The result is a more effective security\napproach that leverages the strengths of both technologies while mitigating\ntheir weaknesses. SAST-Genius reduced false positives by about 91 % (225 to 20)\ncompared to Semgrep alone."
    },
    {
        "date": "2025-09",
        "title": "NeuroRAD-FM: A Foundation Model for Neuro-Oncology with Distributionally Robust Training",
        "author": "Moinak Bhattacharya, Angelica P. Kurtz, Fabio M. Iwamoto, Prateek Prasanna, and Gagandeep Singh",
        "link": "http://arxiv.org/abs/2509.15416v1",
        "abstract": "Neuro-oncology poses unique challenges for machine learning due to\nheterogeneous data and tumor complexity, limiting the ability of foundation\nmodels (FMs) to generalize across cohorts. Existing FMs also perform poorly in\npredicting uncommon molecular markers, which are essential for treatment\nresponse and risk stratification. To address these gaps, we developed a\nneuro-oncology specific FM with a distributionally robust loss function,\nenabling accurate estimation of tumor phenotypes while maintaining\ncross-institution generalization. We pretrained self-supervised backbones\n(BYOL, DINO, MAE, MoCo) on multi-institutional brain tumor MRI and applied\ndistributionally robust optimization (DRO) to mitigate site and class\nimbalance. Downstream tasks included molecular classification of common markers\n(MGMT, IDH1, 1p/19q, EGFR), uncommon alterations (ATRX, TP53, CDKN2A/2B, TERT),\ncontinuous markers (Ki-67, TP53), and overall survival prediction in IDH1\nwild-type glioblastoma at UCSF, UPenn, and CUIMC. Our method improved molecular\nprediction and reduced site-specific embedding differences. At CUIMC, mean\nbalanced accuracy rose from 0.744 to 0.785 and AUC from 0.656 to 0.676, with\nthe largest gains for underrepresented endpoints (CDKN2A/2B accuracy 0.86 to\n0.92, AUC 0.73 to 0.92; ATRX AUC 0.69 to 0.82; Ki-67 accuracy 0.60 to 0.69).\nFor survival, c-index improved at all sites: CUIMC 0.592 to 0.597, UPenn 0.647\nto 0.672, UCSF 0.600 to 0.627. Grad-CAM highlighted tumor and peri-tumoral\nregions, confirming interpretability. Overall, coupling FMs with DRO yields\nmore site-invariant representations, improves prediction of common and uncommon\nmarkers, and enhances survival discrimination, underscoring the need for\nprospective validation and integration of longitudinal and interventional\nsignals to advance precision neuro-oncology."
    },
    {
        "date": "2025-09",
        "title": "Adversarial generalization of unfolding (model-based) networks",
        "author": "Vicky Kouni",
        "link": "http://arxiv.org/abs/2509.15370v1",
        "abstract": "Unfolding networks are interpretable networks emerging from iterative\nalgorithms, incorporate prior knowledge of data structure, and are designed to\nsolve inverse problems like compressed sensing, which deals with recovering\ndata from noisy, missing observations. Compressed sensing finds applications in\ncritical domains, from medical imaging to cryptography, where adversarial\nrobustness is crucial to prevent catastrophic failures. However, a solid\ntheoretical understanding of the performance of unfolding networks in the\npresence of adversarial attacks is still in its infancy. In this paper, we\nstudy the adversarial generalization of unfolding networks when perturbed with\n$l_2$-norm constrained attacks, generated by the fast gradient sign method.\nParticularly, we choose a family of state-of-the-art overaparameterized\nunfolding networks and deploy a new framework to estimate their adversarial\nRademacher complexity. Given this estimate, we provide adversarial\ngeneralization error bounds for the networks under study, which are tight with\nrespect to the attack level. To our knowledge, this is the first theoretical\nanalysis on the adversarial generalization of unfolding networks. We further\npresent a series of experiments on real-world data, with results corroborating\nour derived theory, consistently for all data. Finally, we observe that the\nfamily's overparameterization can be exploited to promote adversarial\nrobustness, shedding light on how to efficiently robustify neural networks."
    },
    {
        "date": "2025-09",
        "title": "AIP: Subverting Retrieval-Augmented Generation via Adversarial Instructional Prompt",
        "author": "Saket S. Chaturvedi, Gaurav Bagwe, Lan Zhang, and Xiaoyong Yuan",
        "link": "http://arxiv.org/abs/2509.15159v1",
        "abstract": "Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by\nretrieving relevant documents from external sources to improve factual accuracy\nand verifiability. However, this reliance introduces new attack surfaces within\nthe retrieval pipeline, beyond the LLM itself. While prior RAG attacks have\nexposed such vulnerabilities, they largely rely on manipulating user queries,\nwhich is often infeasible in practice due to fixed or protected user inputs.\nThis narrow focus overlooks a more realistic and stealthy vector: instructional\nprompts, which are widely reused, publicly shared, and rarely audited. Their\nimplicit trust makes them a compelling target for adversaries to manipulate RAG\nbehavior covertly.\n  We introduce a novel attack for Adversarial Instructional Prompt (AIP) that\nexploits adversarial instructional prompts to manipulate RAG outputs by subtly\naltering retrieval behavior. By shifting the attack surface to the\ninstructional prompts, AIP reveals how trusted yet seemingly benign interface\ncomponents can be weaponized to degrade system integrity. The attack is crafted\nto achieve three goals: (1) naturalness, to evade user detection; (2) utility,\nto encourage use of prompts; and (3) robustness, to remain effective across\ndiverse query variations. We propose a diverse query generation strategy that\nsimulates realistic linguistic variation in user queries, enabling the\ndiscovery of prompts that generalize across paraphrases and rephrasings.\nBuilding on this, a genetic algorithm-based joint optimization is developed to\nevolve adversarial prompts by balancing attack success, clean-task utility, and\nstealthiness. Experimental results show that AIP achieves up to 95.23% ASR\nwhile preserving benign functionality. These findings uncover a critical and\npreviously overlooked vulnerability in RAG systems, emphasizing the need to\nreassess the shared instructional prompts."
    },
    {
        "date": "2025-09",
        "title": "Benefits of Online Tilted Empirical Risk Minimization: A Case Study of Outlier Detection and Robust Regression",
        "author": "Yigit E. Yildirim, Samet Demir, and Zafer Dogan",
        "link": "http://arxiv.org/abs/2509.15141v1",
        "abstract": "Empirical Risk Minimization (ERM) is a foundational framework for supervised\nlearning but primarily optimizes average-case performance, often neglecting\nfairness and robustness considerations. Tilted Empirical Risk Minimization\n(TERM) extends ERM by introducing an exponential tilt hyperparameter $t$ to\nbalance average-case accuracy with worst-case fairness and robustness. However,\nin online or streaming settings where data arrive one sample at a time, the\nclassical TERM objective degenerates to standard ERM, losing tilt sensitivity.\nWe address this limitation by proposing an online TERM formulation that removes\nthe logarithm from the classical objective, preserving tilt effects without\nadditional computational or memory overhead. This formulation enables a\ncontinuous trade-off controlled by $t$, smoothly interpolating between ERM ($t\n\\to 0$), fairness emphasis ($t > 0$), and robustness to outliers ($t < 0$). We\nempirically validate online TERM on two representative streaming tasks: robust\nlinear regression with adversarial outliers and minority-class detection in\nbinary classification. Our results demonstrate that negative tilting\neffectively suppresses outlier influence, while positive tilting improves\nrecall with minimal impact on precision, all at per-sample computational cost\nequivalent to ERM. Online TERM thus recovers the full robustness-fairness\nspectrum of classical TERM in an efficient single-sample learning regime."
    },
    {
        "date": "2025-09",
        "title": "M4Diffuser: Multi-View Diffusion Policy with Manipulability-Aware Control for Robust Mobile Manipulation",
        "author": "Ju Dong, Lei Zhang, Liding Zhang, Yao Ling, Yu Fu, Kaixin Bai, Zolt\u00e1n-Csaba M\u00e1rton, Zhenshan Bing, Zhaopeng Chen, Alois Christian Knoll, and Jianwei Zhang",
        "link": "http://arxiv.org/abs/2509.14980v1",
        "abstract": "Mobile manipulation requires the coordinated control of a mobile base and a\nrobotic arm while simultaneously perceiving both global scene context and\nfine-grained object details. Existing single-view approaches often fail in\nunstructured environments due to limited fields of view, exploration, and\ngeneralization abilities. Moreover, classical controllers, although stable,\nstruggle with efficiency and manipulability near singularities. To address\nthese challenges, we propose M4Diffuser, a hybrid framework that integrates a\nMulti-View Diffusion Policy with a novel Reduced and Manipulability-aware QP\n(ReM-QP) controller for mobile manipulation. The diffusion policy leverages\nproprioceptive states and complementary camera perspectives with both\nclose-range object details and global scene context to generate task-relevant\nend-effector goals in the world frame. These high-level goals are then executed\nby the ReM-QP controller, which eliminates slack variables for computational\nefficiency and incorporates manipulability-aware preferences for robustness\nnear singularities. Comprehensive experiments in simulation and real-world\nenvironments show that M4Diffuser achieves 7 to 56 percent higher success rates\nand reduces collisions by 3 to 31 percent over baselines. Our approach\ndemonstrates robust performance for smooth whole-body coordination, and strong\ngeneralization to unseen tasks, paving the way for reliable mobile manipulation\nin unstructured environments. Details of the demo and supplemental material are\navailable on our project website https://sites.google.com/view/m4diffuser."
    },
    {
        "date": "2025-09",
        "title": "Discrete optimal transport is a strong audio adversarial attack",
        "author": "Anton Selitskiy, Akib Shahriyar, and Jishnuraj Prakasan",
        "link": "http://arxiv.org/abs/2509.14959v1",
        "abstract": "In this paper, we show that discrete optimal transport (DOT) is an effective\nblack-box adversarial attack against modern audio anti-spoofing countermeasures\n(CMs). Our attack operates as a post-processing, distribution-alignment step:\nframe-level WavLM embeddings of generated speech are aligned to an unpaired\nbona fide pool via entropic OT and a top-$k$ barycentric projection, then\ndecoded with a neural vocoder. Evaluated on ASVspoof2019 and ASVspoof5 with\nAASIST baselines, DOT yields consistently high equal error rate (EER) across\ndatasets and remains competitive after CM fine-tuning, outperforming several\nconventional attacks in cross-dataset transfer. Ablation analysis highlights\nthe practical impact of vocoder overlap. Results indicate that\ndistribution-level alignment is a powerful and stable attack surface for\ndeployed CMs."
    },
    {
        "date": "2025-09",
        "title": "Sentinel Agents for Secure and Trustworthy Agentic AI in Multi-Agent Systems",
        "author": "Diego Gosmar, and Deborah A. Dahl",
        "link": "http://arxiv.org/abs/2509.14956v1",
        "abstract": "This paper proposes a novel architectural framework aimed at enhancing\nsecurity and reliability in multi-agent systems (MAS). A central component of\nthis framework is a network of Sentinel Agents, functioning as a distributed\nsecurity layer that integrates techniques such as semantic analysis via large\nlanguage models (LLMs), behavioral analytics, retrieval-augmented verification,\nand cross-agent anomaly detection. Such agents can potentially oversee\ninter-agent communications, identify potential threats, enforce privacy and\naccess controls, and maintain comprehensive audit records. Complementary to the\nidea of Sentinel Agents is the use of a Coordinator Agent. The Coordinator\nAgent supervises policy implementation, and manages agent participation. In\naddition, the Coordinator also ingests alerts from Sentinel Agents. Based on\nthese alerts, it can adapt policies, isolate or quarantine misbehaving agents,\nand contain threats to maintain the integrity of the MAS ecosystem. This\ndual-layered security approach, combining the continuous monitoring of Sentinel\nAgents with the governance functions of Coordinator Agents, supports dynamic\nand adaptive defense mechanisms against a range of threats, including prompt\ninjection, collusive agent behavior, hallucinations generated by LLMs, privacy\nbreaches, and coordinated multi-agent attacks. In addition to the architectural\ndesign, we present a simulation study where 162 synthetic attacks of different\nfamilies (prompt injection, hallucination, and data exfiltration) were injected\ninto a multi-agent conversational environment. The Sentinel Agents successfully\ndetected the attack attempts, confirming the practical feasibility of the\nproposed monitoring approach. The framework also offers enhanced system\nobservability, supports regulatory compliance, and enables policy evolution\nover time."
    },
    {
        "date": "2025-09",
        "title": "Robust Barycenters of Persistence Diagrams",
        "author": "Keanu Sisouk, Eloi Tanguy, Julie Delon, and Julien Tierny",
        "link": "http://arxiv.org/abs/2509.14904v1",
        "abstract": "This short paper presents a general approach for computing robust Wasserstein\nbarycenters of persistence diagrams. The classical method consists in computing\nassignment arithmetic means after finding the optimal transport plans between\nthe barycenter and the persistence diagrams. However, this procedure only works\nfor the transportation cost related to the $q$-Wasserstein distance $W_q$ when\n$q=2$. We adapt an alternative fixed-point method to compute a barycenter\ndiagram for generic transportation costs ($q > 1$), in particular those robust\nto outliers, $q \\in (1,2)$. We show the utility of our work in two\napplications: \\emph{(i)} the clustering of persistence diagrams on their metric\nspace and \\emph{(ii)} the dictionary encoding of persistence diagrams. In both\nscenarios, we demonstrate the added robustness to outliers provided by our\ngeneralized framework. Our Python implementation is available at this address:\nhttps://github.com/Keanu-Sisouk/RobustBarycenter ."
    },
    {
        "date": "2025-09",
        "title": "Learning Graph from Smooth Signals under Partial Observation: A Robustness Analysis",
        "author": "Hoang-Son Nguyen, and Hoi-To Wai",
        "link": "http://arxiv.org/abs/2509.14887v1",
        "abstract": "Learning the graph underlying a networked system from nodal signals is\ncrucial to downstream tasks in graph signal processing and machine learning.\nThe presence of hidden nodes whose signals are not observable might corrupt the\nestimated graph. While existing works proposed various robustifications of\nvanilla graph learning objectives by explicitly accounting for the presence of\nthese hidden nodes, a robustness analysis of \"naive\", hidden-node agnostic\napproaches is still underexplored. This work demonstrates that vanilla graph\ntopology learning methods are implicitly robust to partial observations of\nlow-pass filtered graph signals. We achieve this theoretical result through\nextending the restricted isometry property (RIP) to the Dirichlet energy\nfunction used in graph learning objectives. We show that smoothness-based graph\nlearning formulation (e.g., the GL-SigRep method) on partial observations can\nrecover the ground truth graph topology corresponding to the observed nodes.\nSynthetic and real data experiments corroborate our findings."
    },
    {
        "date": "2025-09",
        "title": "Enhancing Retrieval Augmentation via Adversarial Collaboration",
        "author": "Letian Zhang, Guanghao Meng, Xudong Ren, Yiming Wang, and Shu-Tao Xia",
        "link": "http://arxiv.org/abs/2509.14750v1",
        "abstract": "Retrieval-augmented Generation (RAG) is a prevalent approach for\ndomain-specific LLMs, yet it is often plagued by \"Retrieval Hallucinations\"--a\nphenomenon where fine-tuned models fail to recognize and act upon poor-quality\nretrieved documents, thus undermining performance. To address this, we propose\nthe Adversarial Collaboration RAG (AC-RAG) framework. AC-RAG employs two\nheterogeneous agents: a generalist Detector that identifies knowledge gaps, and\na domain-specialized Resolver that provides precise solutions. Guided by a\nmoderator, these agents engage in an adversarial collaboration, where the\nDetector's persistent questioning challenges the Resolver's expertise. This\ndynamic process allows for iterative problem dissection and refined knowledge\nretrieval. Extensive experiments show that AC-RAG significantly improves\nretrieval accuracy and outperforms state-of-the-art RAG methods across various\nvertical domains."
    },
    {
        "date": "2025-09",
        "title": "Security Analysis of Web Applications Based on Gruyere",
        "author": "Yonghao Ni, Zhongwen Li, and Xiaoqi Li",
        "link": "http://arxiv.org/abs/2509.14706v1",
        "abstract": "With the rapid development of Internet technologies, web systems have become\nessential infrastructures for modern information exchange and business\noperations. However, alongside their expansion, numerous security\nvulnerabilities have emerged, making web security a critical research focus\nwithin the broader field of cybersecurity. These issues are closely related to\ndata protection, privacy preservation, and business continuity, and systematic\nresearch on web security is crucial for mitigating malicious attacks and\nenhancing the reliability and robustness of network systems. This paper first\nreviews the OWASP Top 10, summarizing the types, causes, and impacts of common\nweb vulnerabilities, and illustrates their exploitation mechanisms through\nrepresentative cases. Building upon this, the Gruyere platform is adopted as an\nexperimental subject for analyzing known vulnerabilities. The study presents\ndetailed reproduction steps for specific vulnerabilities, proposes\ncomprehensive remediation strategies, and further compares Gruyere's\nvulnerabilities with contemporary real-world cases. The findings suggest that,\nalthough Gruyere's vulnerabilities are relatively outdated, their underlying\nprinciples remain highly relevant for explaining a wide range of modern\nsecurity flaws. Overall, this research demonstrates that web system security\nanalysis based on Gruyere not only deepens the understanding of vulnerability\nmechanisms but also provides practical support for technological innovation and\nsecurity defense."
    },
    {
        "date": "2025-09",
        "title": "Threat Modeling for Enhancing Security of IoT Audio Classification Devices under a Secure Protocols Framework",
        "author": "Sergio Benlloch-Lopez, Miquel Viel-Vazquez, Javier Naranjo-Alcazar, Jordi Grau-Haro, and Pedro Zuccarello",
        "link": "http://arxiv.org/abs/2509.14657v2",
        "abstract": "The rapid proliferation of IoT nodes equipped with microphones and capable of\nperforming on-device audio classification exposes highly sensitive data while\noperating under tight resource constraints. To protect against this, we present\na defence-in-depth architecture comprising a security protocol that treats the\nedge device, cellular network and cloud backend as three separate trust\ndomains, linked by TPM-based remote attestation and mutually authenticated TLS\n1.3. A STRIDE-driven threat model and attack-tree analysis guide the design. At\nstartup, each boot stage is measured into TPM PCRs. The node can only decrypt\nits LUKS-sealed partitions after the cloud has verified a TPM quote and\nreleased a one-time unlock key. This ensures that rogue or tampered devices\nremain inert. Data in transit is protected by TLS 1.3 and hybridised with Kyber\nand Dilithium to provide post-quantum resilience. Meanwhile, end-to-end\nencryption and integrity hashes safeguard extracted audio features. Signed,\nrollback-protected AI models and tamper-responsive sensors harden firmware and\nhardware. Data at rest follows a 3-2-1 strategy comprising a solid-state drive\nsealed with LUKS, an offline cold archive encrypted with a hybrid post-quantum\ncipher and an encrypted cloud replica. Finally, we set out a plan for\nevaluating the physical and logical security of the proposed protocol."
    },
    {
        "date": "2025-09",
        "title": "Adversarial Distilled Retrieval-Augmented Guarding Model for Online Malicious Intent Detection",
        "author": "Yihao Guo, Haocheng Bian, Liutong Zhou, Ze Wang, Zhaoyi Zhang, Francois Kawala, Milan Dean, Ian Fischer, Yuantao Peng, Noyan Tokgozoglu, Ivan Barrientos, Riyaaz Shaik, Rachel Li, Chandru Venkataraman, Reza Shifteh Far, Moses Pawar, Venkat Sundaranatha, Michael Xu, and Frank Chu",
        "link": "http://arxiv.org/abs/2509.14622v1",
        "abstract": "With the deployment of Large Language Models (LLMs) in interactive\napplications, online malicious intent detection has become increasingly\ncritical. However, existing approaches fall short of handling diverse and\ncomplex user queries in real time. To address these challenges, we introduce\nADRAG (Adversarial Distilled Retrieval-Augmented Guard), a two-stage framework\nfor robust and efficient online malicious intent detection. In the training\nstage, a high-capacity teacher model is trained on adversarially perturbed,\nretrieval-augmented inputs to learn robust decision boundaries over diverse and\ncomplex user queries. In the inference stage, a distillation scheduler\ntransfers the teacher's knowledge into a compact student model, with a\ncontinually updated knowledge base collected online. At deployment, the compact\nstudent model leverages top-K similar safety exemplars retrieved from the\nonline-updated knowledge base to enable both online and real-time malicious\nquery detection. Evaluations across ten safety benchmarks demonstrate that\nADRAG, with a 149M-parameter model, achieves 98.5% of WildGuard-7B's\nperformance, surpasses GPT-4 by 3.3% and Llama-Guard-3-8B by 9.5% on\nout-of-distribution detection, while simultaneously delivering up to 5.6x lower\nlatency at 300 queries per second (QPS) in real-time applications."
    },
    {
        "date": "2025-09",
        "title": "Threats and Security Strategies for IoMT Infusion Pumps",
        "author": "Ramazan Yener, Muhammad Hassan, and Masooda Bashir",
        "link": "http://arxiv.org/abs/2509.14604v1",
        "abstract": "The integration of the Internet of Medical Things (IoMT) into healthcare\nsystems has transformed patient care by enabling real-time monitoring, enhanced\ndiagnostics, and enhanced operational efficiency. However, this increased\nconnectivity has also expanded the attack surface for cybercriminals, raising\nsignificant cybersecurity and privacy concerns. This study focuses on the\ncybersecurity vulnerabilities of IoMT infusion pumps, which are critical\ndevices in modern healthcare. Through a targeted literature review of the past\nfive years, we analyzed seven current studies from a pool of 132 papers to\nidentify security vulnerabilities. Our findings indicate that infusion pumps\nface vulnerabilities such as device-level flaws, authentication and access\ncontrol issues, network and communication weaknesses, data security and privacy\nrisks, and operational or organizational challenges that can expose them to\nlateral attacks within healthcare networks. Our analysis synthesizes findings\nfrom seven recent studies to clarify how and why infusion pumps remain\nvulnerable in each of these areas. By categorizing the security gaps, we\nhighlight critical risk patterns and their implications. This work underscores\nthe scope of the issue and provides a structured understanding that is valuable\nfor healthcare IT professionals and device manufacturers. Ultimately, the\nfindings can inform the development of targeted, proactive security strategies\nto better safeguard infusion pumps and protect patient well-being."
    },
    {
        "date": "2025-09",
        "title": "What Gets Measured Gets Managed: Mitigating Supply Chain Attacks with a Link Integrity Management System",
        "author": "Johnny So, Michael Ferdman, and Nick Nikiforakis",
        "link": "http://arxiv.org/abs/2509.14583v1",
        "abstract": "The web continues to grow, but dependency-monitoring tools and standards for\nresource integrity lag behind. Currently, there exists no robust method to\nverify the integrity of web resources, much less in a generalizable yet\nperformant manner, and supply chains remain one of the most targeted parts of\nthe attack surface of web applications.\n  In this paper, we present the design of LiMS, a transparent system to\nbootstrap link integrity guarantees in web browsing sessions with minimal\noverhead. At its core, LiMS uses a set of customizable integrity policies to\ndeclare the (un)expected properties of resources, verifies these policies, and\nenforces them for website visitors. We discuss how basic integrity policies can\nserve as building blocks for a comprehensive set of integrity policies, while\nproviding guarantees that would be sufficient to defend against recent supply\nchain attacks detailed by security industry reports. Finally, we evaluate our\nopen-sourced prototype by simulating deployments on a representative sample of\n450 domains that are diverse in ranking and category. We find that our proposal\noffers the ability to bootstrap marked security improvements with an overall\noverhead of hundreds of milliseconds on initial page loads, and negligible\noverhead on reloads, regardless of network speeds. In addition, from examining\narchived data for the sample sites, we find that several of the proposed policy\nbuilding blocks suit their dependency usage patterns, and would incur minimal\nadministrative overhead."
    },
    {
        "date": "2025-09",
        "title": "VisMoDAl: Visual Analytics for Evaluating and Improving Corruption Robustness of Vision-Language Models",
        "author": "Huanchen Wang, Wencheng Zhang, Zhiqiang Wang, Zhicong Lu, and Yuxin Ma",
        "link": "http://arxiv.org/abs/2509.14571v1",
        "abstract": "Vision-language (VL) models have shown transformative potential across\nvarious critical domains due to their capability to comprehend multi-modal\ninformation. However, their performance frequently degrades under distribution\nshifts, making it crucial to assess and improve robustness against real-world\ndata corruption encountered in practical applications. While advancements in VL\nbenchmark datasets and data augmentation (DA) have contributed to robustness\nevaluation and improvement, there remain challenges due to a lack of in-depth\ncomprehension of model behavior as well as the need for expertise and iterative\nefforts to explore data patterns. Given the achievement of visualization in\nexplaining complex models and exploring large-scale data, understanding the\nimpact of various data corruption on VL models aligns naturally with a visual\nanalytics approach. To address these challenges, we introduce VisMoDAl, a\nvisual analytics framework designed to evaluate VL model robustness against\nvarious corruption types and identify underperformed samples to guide the\ndevelopment of effective DA strategies. Grounded in the literature review and\nexpert discussions, VisMoDAl supports multi-level analysis, ranging from\nexamining performance under specific corruptions to task-driven inspection of\nmodel behavior and corresponding data slice. Unlike conventional works,\nVisMoDAl enables users to reason about the effects of corruption on VL models,\nfacilitating both model behavior understanding and DA strategy formulation. The\nutility of our system is demonstrated through case studies and quantitative\nevaluations focused on corruption robustness in the image captioning task."
    },
    {
        "date": "2025-09",
        "title": "RLBind: Adversarial-Invariant Cross-Modal Alignment for Unified Robust Embeddings",
        "author": "Yuhong Lu",
        "link": "http://arxiv.org/abs/2509.14383v1",
        "abstract": "Unified multi-modal encoders that bind vision, audio, and other sensors into\na shared embedding space are attractive building blocks for robot perception\nand decision-making. However, on-robot deployment exposes the vision branch to\nadversarial and natural corruptions, making robustness a prerequisite for\nsafety. Prior defenses typically align clean and adversarial features within\nCLIP-style encoders and overlook broader cross-modal correspondence, yielding\nmodest gains and often degrading zero-shot transfer. We introduce RLBind, a\ntwo-stage adversarial-invariant cross-modal alignment framework for robust\nunified embeddings. Stage 1 performs unsupervised fine-tuning on\nclean-adversarial pairs to harden the visual encoder. Stage 2 leverages\ncross-modal correspondence by minimizing the discrepancy between\nclean/adversarial features and a text anchor, while enforcing class-wise\ndistributional alignment across modalities. Extensive experiments on Image,\nAudio, Thermal, and Video data show that RLBind consistently outperforms the\nLanguageBind backbone and standard fine-tuning baselines in both clean accuracy\nand norm-bounded adversarial robustness. By improving resilience without\nsacrificing generalization, RLBind provides a practical path toward safer\nmulti-sensor perception stacks for embodied robots in navigation, manipulation,\nand other autonomy settings."
    },
    {
        "date": "2025-09",
        "title": "Defending Diffusion Models Against Membership Inference Attacks via Higher-Order Langevin Dynamics",
        "author": "Benjamin Sterling, Yousef El-Laham, and M\u00f3nica F. Bugallo",
        "link": "http://arxiv.org/abs/2509.14225v1",
        "abstract": "Recent advances in generative artificial intelligence applications have\nraised new data security concerns. This paper focuses on defending diffusion\nmodels against membership inference attacks. This type of attack occurs when\nthe attacker can determine if a certain data point was used to train the model.\nAlthough diffusion models are intrinsically more resistant to membership\ninference attacks than other generative models, they are still susceptible. The\ndefense proposed here utilizes critically-damped higher-order Langevin\ndynamics, which introduces several auxiliary variables and a joint diffusion\nprocess along these variables. The idea is that the presence of auxiliary\nvariables mixes external randomness that helps to corrupt sensitive input data\nearlier on in the diffusion process. This concept is theoretically investigated\nand validated on a toy dataset and a speech dataset using the Area Under the\nReceiver Operating Characteristic (AUROC) curves and the FID metric."
    },
    {
        "date": "2025-09",
        "title": "Bellman Optimality of Average-Reward Robust Markov Decision Processes with a Constant Gain",
        "author": "Shengbo Wang, and Nian Si",
        "link": "http://arxiv.org/abs/2509.14203v1",
        "abstract": "Learning and optimal control under robust Markov decision processes (MDPs)\nhave received increasing attention, yet most existing theory, algorithms, and\napplications focus on finite-horizon or discounted models. The average-reward\nformulation, while natural in many operations research and management contexts,\nremains underexplored. This is primarily because the dynamic programming\nfoundations are technically challenging and only partially understood, with\nseveral fundamental questions remaining open. This paper steps toward a general\nframework for average-reward robust MDPs by analyzing the constant-gain\nsetting. We study the average-reward robust control problem with possible\ninformation asymmetries between the controller and an S-rectangular adversary.\nOur analysis centers on the constant-gain robust Bellman equation, examining\nboth the existence of solutions and their relationship to the optimal average\nreward. Specifically, we identify when solutions to the robust Bellman equation\ncharacterize the optimal average reward and stationary policies, and we provide\nsufficient conditions ensuring solutions' existence. These findings expand the\ndynamic programming theory for average-reward robust MDPs and lay a foundation\nfor robust dynamic decision making under long-run average criteria in\noperational environments."
    },
    {
        "date": "2025-09",
        "title": "TGPO: Tree-Guided Preference Optimization for Robust Web Agent Reinforcement Learning",
        "author": "Ziyuan Chen, Zhenghui Zhao, Zhangye Han, Miancan Liu, Xianhang Ye, Yiqing Li, Hongbo Min, Jinkui Ren, Xiantao Zhang, and Guitao Cao",
        "link": "http://arxiv.org/abs/2509.14172v2",
        "abstract": "With the rapid advancement of large language models and vision-language\nmodels, employing large models as Web Agents has become essential for automated\nweb interaction. However, training Web Agents with reinforcement learning faces\ncritical challenges including credit assignment misallocation, prohibitively\nhigh annotation costs, and reward sparsity. To address these issues, we propose\nTree-Guided Preference Optimization (TGPO), an offline reinforcement learning\nframework that proposes a tree-structured trajectory representation merging\nsemantically identical states across trajectories to eliminate label conflicts.\nOur framework incorporates a Process Reward Model that automatically generates\nfine-grained rewards through subgoal progress, redundancy detection, and action\nverification. Additionally, a dynamic weighting mechanism prioritizes\nhigh-impact decision points during training. Experiments on Online-Mind2Web and\nour self-constructed C-WebShop datasets demonstrate that TGPO significantly\noutperforms existing methods, achieving higher success rates with fewer\nredundant steps."
    },
    {
        "date": "2025-09",
        "title": "Cybersecurity AI: Humanoid Robots as Attack Vectors",
        "author": "V\u00edctor Mayoral-Vilches, Andreas Makris, and Kevin Finisterre",
        "link": "http://arxiv.org/abs/2509.14139v3",
        "abstract": "We present a systematic security assessment of the Unitree G1 humanoid\nshowing it operates simultaneously as a covert surveillance node and can be\npurposed as an active cyber operations platform. Initial access can be achieved\nby exploiting the BLE provisioning protocol which contains a critical command\ninjection vulnerability allowing root access via malformed Wi-Fi credentials,\nexploitable using hardcoded AES keys shared across all units. Partial reverse\nengineering of Unitree's proprietary FMX encryption reveal a static\nBlowfish-ECB layer and a predictable LCG mask-enabled inspection of the\nsystem's otherwise sophisticated security architecture, the most mature we have\nobserved in commercial robotics. Two empirical case studies expose the critical\nrisk of this humanoid robot: (a) the robot functions as a trojan horse,\ncontinuously exfiltrating multi-modal sensor and service-state telemetry to\n43.175.228.18:17883 and 43.175.229.18:17883 every 300 seconds without operator\nnotice, creating violations of GDPR Articles 6 and 13; (b) a resident\nCybersecurity AI (CAI) agent can pivot from reconnaissance to offensive\npreparation against any target, such as the manufacturer's cloud control plane,\ndemonstrating escalation from passive monitoring to active counter-operations.\nThese findings argue for adaptive CAI-powered defenses as humanoids move into\ncritical infrastructure, contributing the empirical evidence needed to shape\nfuture security standards for physical-cyber convergence systems."
    },
    {
        "date": "2025-09",
        "title": "Deceptive Beauty: Evaluating the Impact of Beauty Filters on Deepfake and Morphing Attack Detection",
        "author": "Sara Concas, Simone Maurizio La Cava, Andrea Panzino, Ester Masala, Giulia Orr\u00f9, and Gian Luca Marcialis",
        "link": "http://arxiv.org/abs/2509.14120v1",
        "abstract": "Digital beautification through social media filters has become increasingly\npopular, raising concerns about the reliability of facial images and videos and\nthe effectiveness of automated face analysis. This issue is particularly\ncritical for digital manipulation detectors, systems aiming at distinguishing\nbetween genuine and manipulated data, especially in cases involving deepfakes\nand morphing attacks designed to deceive humans and automated facial\nrecognition. This study examines whether beauty filters impact the performance\nof deepfake and morphing attack detectors. We perform a comprehensive analysis,\nevaluating multiple state-of-the-art detectors on benchmark datasets before and\nafter applying various smoothing filters. Our findings reveal performance\ndegradation, highlighting vulnerabilities introduced by facial enhancements and\nunderscoring the need for robust detection models resilient to such\nalterations."
    },
    {
        "date": "2025-09",
        "title": "Performance Optimization of YOLO-FEDER FusionNet for Robust Drone Detection in Visually Complex Environments",
        "author": "Tamara R. Lenhard, Andreas Weinmann, and Tobias Koch",
        "link": "http://arxiv.org/abs/2509.14012v1",
        "abstract": "Drone detection in visually complex environments remains challenging due to\nbackground clutter, small object scale, and camouflage effects. While generic\nobject detectors like YOLO exhibit strong performance in low-texture scenes,\ntheir effectiveness degrades in cluttered environments with low\nobject-background separability. To address these limitations, this work\npresents an enhanced iteration of YOLO-FEDER FusionNet -- a detection framework\nthat integrates generic object detection with camouflage object detection\ntechniques. Building upon the original architecture, the proposed iteration\nintroduces systematic advancements in training data composition, feature fusion\nstrategies, and backbone design. Specifically, the training process leverages\nlarge-scale, photo-realistic synthetic data, complemented by a small set of\nreal-world samples, to enhance robustness under visually complex conditions.\nThe contribution of intermediate multi-scale FEDER features is systematically\nevaluated, and detection performance is comprehensively benchmarked across\nmultiple YOLO-based backbone configurations. Empirical results indicate that\nintegrating intermediate FEDER features, in combination with backbone upgrades,\ncontributes to notable performance improvements. In the most promising\nconfiguration -- YOLO-FEDER FusionNet with a YOLOv8l backbone and FEDER\nfeatures derived from the DWD module -- these enhancements lead to a FNR\nreduction of up to 39.1 percentage points and a mAP increase of up to 62.8\npercentage points at an IoU threshold of 0.5, compared to the initial baseline."
    },
    {
        "date": "2025-09",
        "title": "Differential Privacy in Federated Learning: Mitigating Inference Attacks with Randomized Response",
        "author": "Ozer Ozturk, Busra Buyuktanir, Gozde Karatas Baydogmus, and Kazim Yildiz",
        "link": "http://arxiv.org/abs/2509.13987v1",
        "abstract": "Machine learning models used for distributed architectures consisting of\nservers and clients require large amounts of data to achieve high accuracy.\nData obtained from clients are collected on a central server for model\ntraining. However, storing data on a central server raises concerns about\nsecurity and privacy. To address this issue, a federated learning architecture\nhas been proposed. In federated learning, each client trains a local model\nusing its own data. The trained models are periodically transmitted to the\ncentral server. The server then combines the received models using federated\naggregation algorithms to obtain a global model. This global model is\ndistributed back to the clients, and the process continues in a cyclical\nmanner. Although preventing data from leaving the clients enhances security,\ncertain concerns still remain. Attackers can perform inference attacks on the\nobtained models to approximate the training dataset, potentially causing data\nleakage. In this study, differential privacy was applied to address the\naforementioned security vulnerability, and a performance analysis was\nconducted. The Data-Unaware Classification Based on Association (duCBA)\nalgorithm was used as the federated aggregation method. Differential privacy\nwas implemented on the data using the Randomized Response technique, and the\ntrade-off between security and performance was examined under different epsilon\nvalues. As the epsilon value decreased, the model accuracy declined, and class\nprediction imbalances were observed. This indicates that higher levels of\nprivacy do not always lead to practical outcomes and that the balance between\nsecurity and performance must be carefully considered."
    },
    {
        "date": "2025-09",
        "title": "FedIA: A Plug-and-Play Importance-Aware Gradient Pruning Aggregation Method for Domain-Robust Federated Graph Learning on Node Classification",
        "author": "Zhanting Zhou, KaHou Tam, Zeqin Wu, Pengzhao Sun, Jinbo Wang, and Fengli Zhang",
        "link": "http://arxiv.org/abs/2509.18171v1",
        "abstract": "Federated Graph Learning (FGL) under domain skew -- as observed on platforms\nsuch as \\emph{Twitch Gamers} and multilingual \\emph{Wikipedia} networks --\ndrives client models toward incompatible representations, rendering naive\naggregation both unstable and ineffective. We find that the culprit is not the\nweighting scheme but the \\emph{noisy gradient signal}: empirical analysis of\nbaseline methods suggests that a vast majority of gradient dimensions can be\ndominated by domain-specific variance. We therefore shift focus from\n\"aggregation-first\" to a \\emph{projection-first} strategy that denoises client\nupdates \\emph{before} they are combined. The proposed FedIA framework realises\nthis \\underline{I}mportance-\\underline{A}ware idea through a two-stage,\nplug-and-play pipeline: (i) a server-side top-$\\rho$ mask keeps only the most\ninformative about 5% of coordinates, and (ii) a lightweight\ninfluence-regularised momentum weight suppresses outlier clients. FedIA adds\n\\emph{no extra uplink traffic and only negligible server memory}, making it\nreadily deployable. On both homogeneous (Twitch Gamers) and heterogeneous\n(Wikipedia) graphs, it yields smoother, more stable convergence and higher\nfinal accuracy than nine strong baselines. A convergence sketch further shows\nthat dynamic projection maintains the optimal\n$\\mathcal{O}(\\sigma^{2}/\\sqrt{T})$ rate."
    },
    {
        "date": "2025-09",
        "title": "Towards Robust Defense against Customization via Protective Perturbation Resistant to Diffusion-based Purification",
        "author": "Wenkui Yang, Jie Cao, Junxian Duan, and Ran He",
        "link": "http://arxiv.org/abs/2509.13922v2",
        "abstract": "Diffusion models like Stable Diffusion have become prominent in visual\nsynthesis tasks due to their powerful customization capabilities, which also\nintroduce significant security risks, including deepfakes and copyright\ninfringement. In response, a class of methods known as protective perturbation\nemerged, which mitigates image misuse by injecting imperceptible adversarial\nnoise. However, purification can remove protective perturbations, thereby\nexposing images again to the risk of malicious forgery. In this work, we\nformalize the anti-purification task, highlighting challenges that hinder\nexisting approaches, and propose a simple diagnostic protective perturbation\nnamed AntiPure. AntiPure exposes vulnerabilities of purification within the\n\"purification-customization\" workflow, owing to two guidance mechanisms: 1)\nPatch-wise Frequency Guidance, which reduces the model's influence over\nhigh-frequency components in the purified image, and 2) Erroneous Timestep\nGuidance, which disrupts the model's denoising strategy across different\ntimesteps. With additional guidance, AntiPure embeds imperceptible\nperturbations that persist under representative purification settings,\nachieving effective post-customization distortion. Experiments show that, as a\nstress test for purification, AntiPure achieves minimal perceptual discrepancy\nand maximal distortion, outperforming other protective perturbation methods\nwithin the purification-customization workflow."
    },
    {
        "date": "2025-09",
        "title": "A Survey and Evaluation Framework for Secure DNS Resolution",
        "author": "Ali Sadeghi Jahromi, AbdelRahman Abdou, and Paul C. van Oorschot",
        "link": "http://arxiv.org/abs/2509.13797v1",
        "abstract": "Since security was not among the original design goals of the Domain Name\nSystem (herein called Vanilla DNS), many secure DNS schemes have been proposed\nto enhance the security and privacy of the DNS resolution process. Some\nproposed schemes aim to replace the existing DNS infrastructure entirely, but\nnone have succeeded in doing so. In parallel, numerous schemes focus on\nimproving DNS security without modifying its fundamental two-stage structure.\nThese efforts highlight the feasibility of addressing DNS security as two\ndistinct but compatible stages. We survey DNS resolution process attacks and\nthreats and develop a comprehensive threat model and attack taxonomy for their\nsystematic categorization. This analysis results in the formulation of 14\ndesirable security, privacy, and availability properties to mitigate the\nidentified threats. Using these properties, we develop an objective evaluation\nframework and apply it to comparatively analyze 12 secure DNS schemes surveyed\nin this work that aim to augment the properties of the DNS resolution process.\nOur evaluation reveals that no single scheme provides ideal protection across\nthe entire resolution path. Instead, the schemes tend to address a subset of\nproperties specific to individual stages. Since these schemes targeting\ndifferent stages of DNS resolution are complementary and can operate together,\ncombining compatible schemes offers a practical and effective approach to\nachieving comprehensive security in the DNS resolution process."
    },
    {
        "date": "2025-09",
        "title": "Bridging the Synthetic-Real Gap: Supervised Domain Adaptation for Robust Spacecraft 6-DoF Pose Estimation",
        "author": "Inder Pal Singh, Nidhal Eddine Chenni, Abd El Rahman Shabayek, Arunkumar Rathinam, and Djamila Aouada",
        "link": "http://arxiv.org/abs/2509.13792v1",
        "abstract": "Spacecraft Pose Estimation (SPE) is a fundamental capability for autonomous\nspace operations such as rendezvous, docking, and in-orbit servicing. Hybrid\npipelines that combine object detection, keypoint regression, and\nPerspective-n-Point (PnP) solvers have recently achieved strong results on\nsynthetic datasets, yet their performance deteriorates sharply on real or\nlab-generated imagery due to the persistent synthetic-to-real domain gap.\nExisting unsupervised domain adaptation approaches aim to mitigate this issue\nbut often underperform when a modest number of labeled target samples are\navailable. In this work, we propose the first Supervised Domain Adaptation\n(SDA) framework tailored for SPE keypoint regression. Building on the Learning\nInvariant Representation and Risk (LIRR) paradigm, our method jointly optimizes\ndomain-invariant representations and task-specific risk using both labeled\nsynthetic and limited labeled real data, thereby reducing generalization error\nunder domain shift. Extensive experiments on the SPEED+ benchmark demonstrate\nthat our approach consistently outperforms source-only, fine-tuning, and oracle\nbaselines. Notably, with only 5% labeled target data, our method matches or\nsurpasses oracle performance trained on larger fractions of labeled data. The\nframework is lightweight, backbone-agnostic, and computationally efficient,\noffering a practical pathway toward robust and deployable spacecraft pose\nestimation in real-world space environments."
    },
    {
        "date": "2025-09",
        "title": "GitHub's Copilot Code Review: Can AI Spot Security Flaws Before You Commit?",
        "author": "Amena Amro, and Manar H. Alalfi",
        "link": "http://arxiv.org/abs/2509.13650v1",
        "abstract": "As software development practices increasingly adopt AI-powered tools,\nensuring that such tools can support secure coding has become critical. This\nstudy evaluates the effectiveness of GitHub Copilot's recently introduced code\nreview feature in detecting security vulnerabilities. Using a curated set of\nlabeled vulnerable code samples drawn from diverse open-source projects\nspanning multiple programming languages and application domains, we\nsystematically assessed Copilot's ability to identify and provide feedback on\ncommon security flaws. Contrary to expectations, our results reveal that\nCopilot's code review frequently fails to detect critical vulnerabilities such\nas SQL injection, cross-site scripting (XSS), and insecure deserialization.\nInstead, its feedback primarily addresses low-severity issues, such as coding\nstyle and typographical errors. These findings expose a significant gap between\nthe perceived capabilities of AI-assisted code review and its actual\neffectiveness in supporting secure development practices. Our results highlight\nthe continued necessity of dedicated security tools and manual code audits to\nensure robust software security."
    },
    {
        "date": "2025-09",
        "title": "Secure UAV-assisted Federated Learning: A Digital Twin-Driven Approach with Zero-Knowledge Proofs",
        "author": "Md Bokhtiar Al Zami, Md Raihan Uddin, and Dinh C. Nguyen",
        "link": "http://arxiv.org/abs/2509.13634v1",
        "abstract": "Federated learning (FL) has gained popularity as a privacy-preserving method\nof training machine learning models on decentralized networks. However to\nensure reliable operation of UAV-assisted FL systems, issues like as excessive\nenergy consumption, communication inefficiencies, and security vulnerabilities\nmust be solved. This paper proposes an innovative framework that integrates\nDigital Twin (DT) technology and Zero-Knowledge Federated Learning (zkFed) to\ntackle these challenges. UAVs act as mobile base stations, allowing scattered\ndevices to train FL models locally and upload model updates for aggregation. By\nincorporating DT technology, our approach enables real-time system monitoring\nand predictive maintenance, improving UAV network efficiency. Additionally,\nZero-Knowledge Proofs (ZKPs) strengthen security by allowing model verification\nwithout exposing sensitive data. To optimize energy efficiency and resource\nmanagement, we introduce a dynamic allocation strategy that adjusts UAV flight\npaths, transmission power, and processing rates based on network conditions.\nUsing block coordinate descent and convex optimization techniques, our method\nsignificantly reduces system energy consumption by up to 29.6% compared to\nconventional FL approaches. Simulation results demonstrate improved learning\nperformance, security, and scalability, positioning this framework as a\npromising solution for next-generation UAV-based intelligent networks."
    },
    {
        "date": "2025-09",
        "title": "SAMIR, an efficient registration framework via robust feature learning from SAM",
        "author": "Yue He, Min Liu, Qinghao Liu, Jiazheng Wang, Yaonan Wang, Hang Zhang, and Xiang Chen",
        "link": "http://arxiv.org/abs/2509.13629v1",
        "abstract": "Image registration is a fundamental task in medical image analysis.\nDeformations are often closely related to the morphological characteristics of\ntissues, making accurate feature extraction crucial. Recent weakly supervised\nmethods improve registration by incorporating anatomical priors such as\nsegmentation masks or landmarks, either as inputs or in the loss function.\nHowever, such weak labels are often not readily available, limiting their\npractical use. Motivated by the strong representation learning ability of\nvisual foundation models, this paper introduces SAMIR, an efficient medical\nimage registration framework that utilizes the Segment Anything Model (SAM) to\nenhance feature extraction. SAM is pretrained on large-scale natural image\ndatasets and can learn robust, general-purpose visual representations. Rather\nthan using raw input images, we design a task-specific adaptation pipeline\nusing SAM's image encoder to extract structure-aware feature embeddings,\nenabling more accurate modeling of anatomical consistency and deformation\npatterns. We further design a lightweight 3D head to refine features within the\nembedding space, adapting to local deformations in medical images.\nAdditionally, we introduce a Hierarchical Feature Consistency Loss to guide\ncoarse-to-fine feature matching and improve anatomical alignment. Extensive\nexperiments demonstrate that SAMIR significantly outperforms state-of-the-art\nmethods on benchmark datasets for both intra-subject cardiac image registration\nand inter-subject abdomen CT image registration, achieving performance\nimprovements of 2.68% on ACDC and 6.44% on the abdomen dataset. The source code\nwill be publicly available on GitHub following the acceptance of this paper."
    },
    {
        "date": "2025-09",
        "title": "Secure, Scalable and Privacy Aware Data Strategy in Cloud",
        "author": "Vijay Kumar Butte, and Sujata Butte",
        "link": "http://arxiv.org/abs/2509.13627v1",
        "abstract": "The enterprises today are faced with the tough challenge of processing,\nstoring large amounts of data in a secure, scalable manner and enabling\ndecision makers to make quick, informed data driven decisions. This paper\naddresses this challenge and develops an effective enterprise data strategy in\nthe cloud. Various components of an effective data strategy are discussed and\narchitectures addressing security, scalability and privacy aspects are\nprovided."
    },
    {
        "date": "2025-09",
        "title": "Agentic JWT: A Secure Delegation Protocol for Autonomous AI Agents",
        "author": "Abhishek Goswami",
        "link": "http://arxiv.org/abs/2509.13597v1",
        "abstract": "Autonomous LLM agents can issue thousands of API calls per hour without human\noversight. OAuth 2.0 assumes deterministic clients, but in agentic settings\nstochastic reasoning, prompt injection, or multi-agent orchestration can\nsilently expand privileges.\n  We introduce Agentic JWT (A-JWT), a dual-faceted intent token that binds each\nagent's action to verifiable user intent and, optionally, to a specific\nworkflow step. A-JWT carries an agent's identity as a one-way checksum hash\nderived from its prompt, tools and configuration, and a chained delegation\nassertion to prove which downstream agent may execute a given task, and\nper-agent proof-of-possession keys to prevent replay and in-process\nimpersonation. We define a new authorization mechanism and add a lightweight\nclient shim library that self-verifies code at run time, mints intent tokens,\ntracks workflow steps and derives keys, thus enabling secure agent identity and\nseparation even within a single process.\n  We illustrate a comprehensive threat model for agentic applications,\nimplement a Python proof-of-concept and show functional blocking of\nscope-violating requests, replay, impersonation, and prompt-injection pathways\nwith sub-millisecond overhead on commodity hardware. The design aligns with\nongoing OAuth agent discussions and offers a drop-in path toward zero-trust\nguarantees for agentic applications. A comprehensive performance and security\nevaluation with experimental results will appear in our forthcoming journal\npublication"
    },
    {
        "date": "2025-09",
        "title": "GuardianPWA: Enhancing Security Throughout the Progressive Web App Installation Lifecycle",
        "author": "Mengxiao Wang, and Guofei Gu",
        "link": "http://arxiv.org/abs/2509.13561v1",
        "abstract": "Progressive Web App (PWA) installation is critical for integrating web and\nmobile app functionalities, offering a seamless user experience. However,\nensuring the security of the PWA installation lifecycle is essential for\nmaintaining user trust and privacy. This paper introduces the GUARDIANPWA\nframework, a comprehensive approach to analyzing the PWA installation mechanism\nbased on the CIA security principles (Confidentiality, Integrity, and\nAvailability) and identifying areas where browser vendors fail to comply with\nthese principles. Our study revealed 203 instances of non-compliance with\nsecurity principles, highlighting how these irregularities in the PWA\ninstallation lifecycle can lead to potential violations of user privacy. For\ninstance, in Firefox, PWAs installed in private mode incorrectly appear in\nnormal mode, risking user confidentiality. Additionally, 29,465 PWAs are at\nrisk because Samsung Internet does not display origins when PWAs navigate to\nthird-party websites, undermining integrity. These findings were reported to\nbrowser vendors, leading to Firefox acknowledging four issues, resolving one,\nand planning to resolve two others. GUARDIANPWA supports developers by\nanalyzing PWA manifest files for syntactic and semantic correctness, offering\nactionable recommendations, and helping to create PWAs that align with security\nbest practices. By using GUARDIANPWA, developers and users can address critical\nsecurity gaps and enhance compliance with CIA principles throughout the PWA\ninstallation lifecycle."
    },
    {
        "date": "2025-09",
        "title": "AQUA-LLM: Evaluating Accuracy, Quantization, and Adversarial Robustness Trade-offs in LLMs for Cybersecurity Question Answering",
        "author": "Onat Gungor, Roshan Sood, Harold Wang, and Tajana Rosing",
        "link": "http://arxiv.org/abs/2509.13514v1",
        "abstract": "Large Language Models (LLMs) have recently demonstrated strong potential for\ncybersecurity question answering (QA), supporting decision-making in real-time\nthreat detection and response workflows. However, their substantial\ncomputational demands pose significant challenges for deployment on\nresource-constrained edge devices. Quantization, a widely adopted model\ncompression technique, can alleviate these constraints. Nevertheless,\nquantization may degrade model accuracy and increase susceptibility to\nadversarial attacks. Fine-tuning offers a potential means to mitigate these\nlimitations, but its effectiveness when combined with quantization remains\ninsufficiently explored. Hence, it is essential to understand the trade-offs\namong accuracy, efficiency, and robustness. We propose AQUA-LLM, an evaluation\nframework designed to benchmark several state-of-the-art small LLMs under four\ndistinct configurations: base, quantized-only, fine-tuned, and fine-tuned\ncombined with quantization, specifically for cybersecurity QA. Our results\ndemonstrate that quantization alone yields the lowest accuracy and robustness\ndespite improving efficiency. In contrast, combining quantization with\nfine-tuning enhances both LLM robustness and predictive performance, achieving\nan optimal balance of accuracy, robustness, and efficiency. These findings\nhighlight the critical need for quantization-aware, robustness-preserving\nfine-tuning methodologies to enable the robust and efficient deployment of LLMs\nfor cybersecurity QA."
    },
    {
        "date": "2025-09",
        "title": "Adversarial Appearance Learning in Augmented Cityscapes for Pedestrian Recognition in Autonomous Driving",
        "author": "Artem Savkin, Thomas Lapotre, Kevin Strauss, Uzair Akbar, and Federico Tombari",
        "link": "http://arxiv.org/abs/2509.13507v1",
        "abstract": "In the autonomous driving area synthetic data is crucial for cover specific\ntraffic scenarios which autonomous vehicle must handle. This data commonly\nintroduces domain gap between synthetic and real domains. In this paper we\ndeploy data augmentation to generate custom traffic scenarios with VRUs in\norder to improve pedestrian recognition. We provide a pipeline for augmentation\nof the Cityscapes dataset with virtual pedestrians. In order to improve\naugmentation realism of the pipeline we reveal a novel generative network\narchitecture for adversarial learning of the data-set lighting conditions. We\nalso evaluate our approach on the tasks of semantic and instance segmentation."
    },
    {
        "date": "2025-09",
        "title": "Semantic-Enhanced Cross-Modal Place Recognition for Robust Robot Localization",
        "author": "Yujia Lin, and Nicholas Evans",
        "link": "http://arxiv.org/abs/2509.13474v1",
        "abstract": "Ensuring accurate localization of robots in environments without GPS\ncapability is a challenging task. Visual Place Recognition (VPR) techniques can\npotentially achieve this goal, but existing RGB-based methods are sensitive to\nchanges in illumination, weather, and other seasonal changes. Existing\ncross-modal localization methods leverage the geometric properties of RGB\nimages and 3D LiDAR maps to reduce the sensitivity issues highlighted above.\nCurrently, state-of-the-art methods struggle in complex scenes, fine-grained or\nhigh-resolution matching, and situations where changes can occur in viewpoint.\nIn this work, we introduce a framework we call Semantic-Enhanced Cross-Modal\nPlace Recognition (SCM-PR) that combines high-level semantics utilizing RGB\nimages for robust localization in LiDAR maps. Our proposed method introduces: a\nVMamba backbone for feature extraction of RGB images; a Semantic-Aware Feature\nFusion (SAFF) module for using both place descriptors and segmentation masks;\nLiDAR descriptors that incorporate both semantics and geometry; and a\ncross-modal semantic attention mechanism in NetVLAD to improve matching.\nIncorporating the semantic information also was instrumental in designing a\nMulti-View Semantic-Geometric Matching and a Semantic Consistency Loss, both in\na contrastive learning framework. Our experimental work on the KITTI and\nKITTI-360 datasets show that SCM-PR achieves state-of-the-art performance\ncompared to other cross-modal place recognition methods."
    },
    {
        "date": "2025-09",
        "title": "A Multi-Agent LLM Defense Pipeline Against Prompt Injection Attacks",
        "author": "S M Asif Hossain, Ruksat Khan Shayoni, Mohd Ruhul Ameen, Akif Islam, M. F. Mridha, and Jungpil Shin",
        "link": "http://arxiv.org/abs/2509.14285v1",
        "abstract": "Prompt injection attacks represent a major vulnerability in Large Language\nModel (LLM) deployments, where malicious instructions embedded in user inputs\ncan override system prompts and induce unintended behaviors. This paper\npresents a novel multi-agent defense framework that employs specialized LLM\nagents in coordinated pipelines to detect and neutralize prompt injection\nattacks in real-time. We evaluate our approach using two distinct\narchitectures: a sequential chain-of-agents pipeline and a hierarchical\ncoordinator-based system. Our comprehensive evaluation on 55 unique prompt\ninjection attacks, grouped into 8 categories and totaling 400 attack instances\nacross two LLM platforms (ChatGLM and Llama2), demonstrates significant\nsecurity improvements. Without defense mechanisms, baseline Attack Success\nRates (ASR) reached 30% for ChatGLM and 20% for Llama2. Our multi-agent\npipeline achieved 100% mitigation, reducing ASR to 0% across all tested\nscenarios. The framework demonstrates robustness across multiple attack\ncategories including direct overrides, code execution attempts, data\nexfiltration, and obfuscation techniques, while maintaining system\nfunctionality for legitimate queries."
    },
    {
        "date": "2025-09",
        "title": "LIGHT-HIDS: A Lightweight and Effective Machine Learning-Based Framework for Robust Host Intrusion Detection",
        "author": "Onat Gungor, Ishaan Kale, Jiasheng Zhou, and Tajana Rosing",
        "link": "http://arxiv.org/abs/2509.13464v1",
        "abstract": "The expansion of edge computing has increased the attack surface, creating an\nurgent need for robust, real-time machine learning (ML)-based host intrusion\ndetection systems (HIDS) that balance accuracy and efficiency. In such\nsettings, inference latency poses a critical security risk, as delays may\nprovide exploitable opportunities for attackers. However, many state-of-the-art\nML-based HIDS solutions rely on computationally intensive architectures with\nhigh inference costs, limiting their practical deployment. This paper proposes\nLIGHT-HIDS, a lightweight machine learning framework that combines a compressed\nneural network feature extractor trained via Deep Support Vector Data\nDescription (DeepSVDD) with an efficient novelty detection model. This hybrid\napproach enables the learning of compact, meaningful representations of normal\nsystem call behavior for accurate anomaly detection. Experimental results on\nmultiple datasets demonstrate that LIGHT-HIDS consistently enhances detection\naccuracy while reducing inference time by up to 75x compared to\nstate-of-the-art methods. These findings highlight its effectiveness and\nscalability as a machine learning-based solution for real-time host intrusion\ndetection."
    },
    {
        "date": "2025-09",
        "title": "Defining Security in Quantum Key Distribution",
        "author": "Carla Ferradini, Martin Sandfuchs, Ramona Wolf, and Renato Renner",
        "link": "http://arxiv.org/abs/2509.13405v1",
        "abstract": "The security of quantum key distribution (QKD) is quantified by a parameter\n$\\varepsilon>0$, which -- under well-defined physical assumptions -- can be\nbounded explicitly. This contrasts with computationally secure schemes, where\nsecurity claims are only asymptotic (i.e., under standard complexity\nassumptions, one only knows that $\\varepsilon \\to 0$ as the key size grows, but\nhas no explicit bound). Here we explain the definition and interpretation of\n$\\varepsilon$-security. Adopting an axiomatic approach, we show that\n$\\varepsilon$ can be understood as the maximum probability of a security\nfailure. Finally, we review and address several criticisms of this definition\nthat have appeared in the literature."
    },
    {
        "date": "2025-09",
        "title": "JANUS: A Dual-Constraint Generative Framework for Stealthy Node Injection Attacks",
        "author": "Jiahao Zhang, Xiaobing Pei, Zhaokun Zhong, Wenqiang Hao, and Zhenghao Tang",
        "link": "http://arxiv.org/abs/2509.13266v1",
        "abstract": "Graph Neural Networks (GNNs) have demonstrated remarkable performance across\nvarious applications, yet they are vulnerable to sophisticated adversarial\nattacks, particularly node injection attacks. The success of such attacks\nheavily relies on their stealthiness, the ability to blend in with the original\ngraph and evade detection. However, existing methods often achieve stealthiness\nby relying on indirect proxy metrics, lacking consideration for the fundamental\ncharacteristics of the injected content, or focusing only on imitating local\nstructures, which leads to the problem of local myopia. To overcome these\nlimitations, we propose a dual-constraint stealthy node injection framework,\ncalled Joint Alignment of Nodal and Universal Structures (JANUS). At the local\nlevel, we introduce a local feature manifold alignment strategy to achieve\ngeometric consistency in the feature space. At the global level, we incorporate\nstructured latent variables and maximize the mutual information with the\ngenerated structures, ensuring the injected structures are consistent with the\nsemantic patterns of the original graph. We model the injection attack as a\nsequential decision process, which is optimized by a reinforcement learning\nagent. Experiments on multiple standard datasets demonstrate that the JANUS\nframework significantly outperforms existing methods in terms of both attack\neffectiveness and stealthiness."
    },
    {
        "date": "2025-09",
        "title": "On the Out-of-Distribution Backdoor Attack for Federated Learning",
        "author": "Jiahao Xu, Zikai Zhang, and Rui Hu",
        "link": "http://arxiv.org/abs/2509.13219v1",
        "abstract": "Traditional backdoor attacks in federated learning (FL) operate within\nconstrained attack scenarios, as they depend on visible triggers and require\nphysical modifications to the target object, which limits their practicality.\nTo address this limitation, we introduce a novel backdoor attack prototype for\nFL called the out-of-distribution (OOD) backdoor attack ($\\mathtt{OBA}$), which\nuses OOD data as both poisoned samples and triggers simultaneously. Our\napproach significantly broadens the scope of backdoor attack scenarios in FL.\nTo improve the stealthiness of $\\mathtt{OBA}$, we propose $\\mathtt{SoDa}$,\nwhich regularizes both the magnitude and direction of malicious local models\nduring local training, aligning them closely with their benign versions to\nevade detection. Empirical results demonstrate that $\\mathtt{OBA}$ effectively\ncircumvents state-of-the-art defenses while maintaining high accuracy on the\nmain task.\n  To address this security vulnerability in the FL system, we introduce\n$\\mathtt{BNGuard}$, a new server-side defense method tailored against\n$\\mathtt{SoDa}$. $\\mathtt{BNGuard}$ leverages the observation that OOD data\ncauses significant deviations in the running statistics of batch normalization\nlayers. This allows $\\mathtt{BNGuard}$ to identify malicious model updates and\nexclude them from aggregation, thereby enhancing the backdoor robustness of FL.\nExtensive experiments across various settings show the effectiveness of\n$\\mathtt{BNGuard}$ on defending against $\\mathtt{SoDa}$. The code is available\nat https://github.com/JiiahaoXU/SoDa-BNGuard."
    },
    {
        "date": "2025-09",
        "title": "FOSSIL: Regret-minimizing weighting for robust learning under imbalance and small data",
        "author": "J. Cha, J. Lee, J. Cho, and J. Shin",
        "link": "http://arxiv.org/abs/2509.13218v1",
        "abstract": "Imbalanced and small data regimes are pervasive in domains such as rare\ndisease imaging, genomics, and disaster response, where labeled samples are\nscarce and naive augmentation often introduces artifacts. Existing solutions\nsuch as oversampling, focal loss, or meta-weighting address isolated aspects of\nthis challenge but remain fragile or complex. We introduce FOSSIL (Flexible\nOptimization via Sample Sensitive Importance Learning), a unified weighting\nframework that seamlessly integrates class imbalance correction,\ndifficulty-aware curricula, augmentation penalties, and warmup dynamics into a\nsingle interpretable formula. Unlike prior heuristics, the proposed framework\nprovides regret-based theoretical guarantees and achieves consistent empirical\ngains over ERM, curriculum, and meta-weighting baselines on synthetic and\nreal-world datasets, while requiring no architectural changes."
    },
    {
        "date": "2025-09",
        "title": "Resisting Quantum Key Distribution Attacks Using Quantum Machine Learning",
        "author": "Ali Al-kuwari, Noureldin Mohamed, Saif Al-kuwari, Ahmed Farouk, and Bikash K. Behera",
        "link": "http://arxiv.org/abs/2509.14282v1",
        "abstract": "The emergence of quantum computing poses significant risks to the security of\nmodern communication networks as it breaks today's public-key cryptographic\nalgorithms. Quantum Key Distribution (QKD) offers a promising solution by\nharnessing the principles of quantum mechanics to establish secure keys.\nHowever, practical QKD implementations remain vulnerable to hardware\nimperfections and advanced attacks such as Photon Number Splitting and\nTrojan-Horse attacks. In this work, we investigate the potential of using\nquantum machine learning (QML) to detect popular QKD attacks. In particular, we\npropose a Hybrid Quantum Long Short-Term Memory (QLSTM) model to improve the\ndetection of common QKD attacks. By combining quantum-enhanced learning with\nclassical deep learning, the model captures complex temporal patterns in QKD\ndata, improving detection accuracy. To evaluate the proposed model, we\nintroduce a realistic QKD dataset simulating normal QKD operations along with\nseven attack scenarios, Intercept-and-Resend, Photon-Number Splitting (PNS),\nTrojan-Horse attacks Random Number Generator (RNG), Detector Blinding,\nWavelength-dependent Trojan Horse, and Combined attacks. The dataset includes\nquantum security metrics such as Quantum Bit Error Rate (QBER), measurement\nentropy, signal and decoy loss rates, and time-based metrics, ensuring an\naccurate representation of real-world conditions. Our results demonstrate\npromising performance of the quantum machine learning approach compared to\ntraditional classical machine learning models, highlighting the potential of\nhybrid techniques to enhance the security of future quantum communication\nnetworks. The proposed Hybrid QLSTM model achieved an accuracy of 93.7.0\\%\nafter 50 training epochs, outperforming classical deep learning models such as\nLSTM, and CNN."
    },
    {
        "date": "2025-09",
        "title": "Digital Sovereignty Control Framework for Military AI-based Cyber Security",
        "author": "Clara Maathuis, and Kasper Cools",
        "link": "http://arxiv.org/abs/2509.13072v1",
        "abstract": "In today's evolving threat landscape, ensuring digital sovereignty has become\nmandatory for military organizations, especially given their increased\ndevelopment and investment in AI-driven cyber security solutions. To this end,\na multi-angled framework is proposed in this article in order to define and\nassess digital sovereign control of data and AI-based models for military cyber\nsecurity. This framework focuses on aspects such as context, autonomy,\nstakeholder involvement, and mitigation of risks in this domain. Grounded on\nthe concepts of digital sovereignty and data sovereignty, the framework aims to\nprotect sensitive defence assets against threats such as unauthorized access,\nransomware, and supply-chain attacks. This approach reflects the multifaceted\nnature of digital sovereignty by preserving operational autonomy, assuring\nsecurity and safety, securing privacy, and fostering ethical compliance of both\nmilitary systems and decision-makers. At the same time, the framework addresses\ninteroperability challenges among allied forces, strategic and legal\nconsiderations, and the integration of emerging technologies by considering a\nmultidisciplinary approach that enhances the resilience and preservation of\ncontrol over (critical) digital assets. This is done by adopting a design\noriented research where systematic literature review is merged with critical\nthinking and analysis of field incidents in order to assure the effectivity and\nrealism of the framework proposed."
    },
    {
        "date": "2025-09",
        "title": "TFANet: Three-Stage Image-Text Feature Alignment Network for Robust Referring Image Segmentation",
        "author": "Qianqi Lu, Yuxiang Xie, Jing Zhang, Shiwei Zou, Yan Chen, and Xidao Luan",
        "link": "http://arxiv.org/abs/2509.13070v1",
        "abstract": "Referring Image Segmentation (RIS) is a task that segments image regions\nbased on language expressions, requiring fine-grained alignment between two\nmodalities. However, existing methods often struggle with multimodal\nmisalignment and language semantic loss, especially in complex scenes\ncontaining multiple visually similar objects, where uniquely described targets\nare frequently mislocalized or incompletely segmented. To tackle these\nchallenges, this paper proposes TFANet, a Three-stage Image-Text Feature\nAlignment Network that systematically enhances multimodal alignment through a\nhierarchical framework comprising three stages: Knowledge Plus Stage (KPS),\nKnowledge Fusion Stage (KFS), and Knowledge Intensification Stage (KIS). In the\nfirst stage, we design the Multiscale Linear Cross-Attention Module (MLAM),\nwhich facilitates bidirectional semantic exchange between visual features and\ntextual representations across multiple scales. This establishes rich and\nefficient alignment between image regions and different granularities of\nlinguistic descriptions. Subsequently, the KFS further strengthens feature\nalignment through the Cross-modal Feature Scanning Module (CFSM), which applies\nmultimodal selective scanning to capture long-range dependencies and construct\na unified multimodal representation. This is essential for modeling long-range\ncross-modal dependencies and enhancing alignment accuracy in complex scenes.\nFinally, in the KIS, we propose the Word-level Linguistic Feature-guided\nSemantic Deepening Module (WFDM) to compensate for semantic degradation\nintroduced in earlier stages."
    },
    {
        "date": "2025-09",
        "title": "MIA-EPT: Membership Inference Attack via Error Prediction for Tabular Data",
        "author": "Eyal German, Daniel Samira, Yuval Elovici, and Asaf Shabtai",
        "link": "http://arxiv.org/abs/2509.13046v1",
        "abstract": "Synthetic data generation plays an important role in enabling data sharing,\nparticularly in sensitive domains like healthcare and finance. Recent advances\nin diffusion models have made it possible to generate realistic, high-quality\ntabular data, but they may also memorize training records and leak sensitive\ninformation. Membership inference attacks (MIAs) exploit this vulnerability by\ndetermining whether a record was used in training. While MIAs have been studied\nin images and text, their use against tabular diffusion models remains\nunderexplored despite the unique risks of structured attributes and limited\nrecord diversity. In this paper, we introduce MIAEPT, Membership Inference\nAttack via Error Prediction for Tabular Data, a novel black-box attack\nspecifically designed to target tabular diffusion models. MIA-EPT constructs\nerrorbased feature vectors by masking and reconstructing attributes of target\nrecords, disclosing membership signals based on how well these attributes are\npredicted. MIA-EPT operates without access to the internal components of the\ngenerative model, relying only on its synthetic data output, and was shown to\ngeneralize across multiple state-of-the-art diffusion models. We validate\nMIA-EPT on three diffusion-based synthesizers, achieving AUC-ROC scores of up\nto 0.599 and TPR@10% FPR values of 22.0% in our internal tests. Under the MIDST\n2025 competition conditions, MIA-EPT achieved second place in the Black-box\nMulti-Table track (TPR@10% FPR = 20.0%). These results demonstrate that our\nmethod can uncover substantial membership leakage in synthetic tabular data,\nchallenging the assumption that synthetic data is inherently\nprivacy-preserving. Our code is publicly available at\nhttps://github.com/eyalgerman/MIA-EPT."
    },
    {
        "date": "2025-09",
        "title": "BAPFL: Exploring Backdoor Attacks Against Prototype-based Federated Learning",
        "author": "Honghong Zeng, Jiong Lou, Zhe Wang, Hefeng Zhou, Chentao Wu, Wei Zhao, and Jie Li",
        "link": "http://arxiv.org/abs/2509.12964v1",
        "abstract": "Prototype-based federated learning (PFL) has emerged as a promising paradigm\nto address data heterogeneity problems in federated learning, as it leverages\nmean feature vectors as prototypes to enhance model generalization. However,\nits robustness against backdoor attacks remains largely unexplored. In this\npaper, we identify that PFL is inherently resistant to existing backdoor\nattacks due to its unique prototype learning mechanism and local data\nheterogeneity. To further explore the security of PFL, we propose BAPFL, the\nfirst backdoor attack method specifically designed for PFL frameworks. BAPFL\nintegrates a prototype poisoning strategy with a trigger optimization\nmechanism. The prototype poisoning strategy manipulates the trajectories of\nglobal prototypes to mislead the prototype training of benign clients, pushing\ntheir local prototypes of clean samples away from the prototypes of\ntrigger-embedded samples. Meanwhile, the trigger optimization mechanism learns\na unique and stealthy trigger for each potential target label, and guides the\nprototypes of trigger-embedded samples to align closely with the global\nprototype of the target label. Experimental results across multiple datasets\nand PFL variants demonstrate that BAPFL achieves a 35\\%-75\\% improvement in\nattack success rate compared to traditional backdoor attacks, while preserving\nmain task accuracy. These results highlight the effectiveness, stealthiness,\nand adaptability of BAPFL in PFL."
    },
    {
        "date": "2025-09",
        "title": "Towards Robust Agentic CUDA Kernel Benchmarking, Verification, and Optimization",
        "author": "Robert Tjarko Lange, Qi Sun, Aaditya Prasad, Maxence Faldor, Yujin Tang, and David Ha",
        "link": "http://arxiv.org/abs/2509.14279v1",
        "abstract": "Recent advances in large language models (LLMs) demonstrate their\neffectiveness in scaling test-time compute for software engineering tasks.\nHowever, these approaches often focus on high-level solutions, with limited\nattention to optimizing low-level CUDA kernel implementations. Additionally,\nexisting kernel generation benchmarks suffer from exploitable loopholes and\ninsufficient diversity in testing conditions, hindering true generalization\nassessment. To address these limitations, we introduce robust-kbench, a new\nbenchmark for rigorous evaluation of kernel performance and correctness across\nvaried scenarios. Furthermore, we present a comprehensive agentic framework\nthat automates CUDA kernel discovery, verification, and optimization. This\npipeline enables frontier LLMs to translate torch code to CUDA kernels and\niteratively improve their runtime within our robust evaluation setting. Our\nsequential workflow first translates PyTorch code into equivalent CUDA kernels.\nIt then optimizes their runtime using a novel evolutionary meta-generation\nprocedure tailored to the CUDA ecosystem, guided by LLM-based verifiers for\ncorrectness and efficient filtering. Evaluated on robust-kbench, our approach\nproduces CUDA kernels outperforming torch implementations for practical\napplications, including forward and backward passes. It can fuse operations and\ndeploy various runtime optimization strategies. The verifier workflow\naccurately classifies incorrect kernels, enhancing hardware verification\nefficiency."
    },
    {
        "date": "2025-09",
        "title": "Sy-FAR: Symmetry-based Fair Adversarial Robustness",
        "author": "Haneen Najjar, Eyal Ronen, and Mahmood Sharif",
        "link": "http://arxiv.org/abs/2509.12939v1",
        "abstract": "Security-critical machine-learning (ML) systems, such as face-recognition\nsystems, are susceptible to adversarial examples, including real-world\nphysically realizable attacks. Various means to boost ML's adversarial\nrobustness have been proposed; however, they typically induce unfair\nrobustness: It is often easier to attack from certain classes or groups than\nfrom others. Several techniques have been developed to improve adversarial\nrobustness while seeking perfect fairness between classes. Yet, prior work has\nfocused on settings where security and fairness are less critical. Our insight\nis that achieving perfect parity in realistic fairness-critical tasks, such as\nface recognition, is often infeasible -- some classes may be highly similar,\nleading to more misclassifications between them. Instead, we suggest that\nseeking symmetry -- i.e., attacks from class $i$ to $j$ would be as successful\nas from $j$ to $i$ -- is more tractable. Intuitively, symmetry is a desirable\nbecause class resemblance is a symmetric relation in most domains.\nAdditionally, as we prove theoretically, symmetry between individuals induces\nsymmetry between any set of sub-groups, in contrast to other fairness notions\nwhere group-fairness is often elusive. We develop Sy-FAR, a technique to\nencourage symmetry while also optimizing adversarial robustness and extensively\nevaluate it using five datasets, with three model architectures, including\nagainst targeted and untargeted realistic attacks. The results show Sy-FAR\nsignificantly improves fair adversarial robustness compared to state-of-the-art\nmethods. Moreover, we find that Sy-FAR is faster and more consistent across\nruns. Notably, Sy-FAR also ameliorates another type of unfairness we discover\nin this work -- target classes that adversarial examples are likely to be\nclassified into become significantly less vulnerable after inducing symmetry."
    },
    {
        "date": "2025-09",
        "title": "A Graph-Based Approach to Alert Contextualisation in Security Operations Centres",
        "author": "Magnus Wiik Eckhoff, Peter Marius Flydal, Siem Peters, Martin Eian, Jonas Halvorsen, Vasileios Mavroeidis, and Gudmund Grov",
        "link": "http://arxiv.org/abs/2509.12923v2",
        "abstract": "Interpreting the massive volume of security alerts is a significant challenge\nin Security Operations Centres (SOCs). Effective contextualisation is\nimportant, enabling quick distinction between genuine threats and benign\nactivity to prioritise what needs further analysis. This paper proposes a\ngraph-based approach to enhance alert contextualisation in a SOC by aggregating\nalerts into graph-based alert groups, where nodes represent alerts and edges\ndenote relationships within defined time-windows. By grouping related alerts,\nwe enable analysis at a higher abstraction level, capturing attack steps more\neffectively than individual alerts. Furthermore, to show that our format is\nwell suited for downstream machine learning methods, we employ Graph Matching\nNetworks (GMNs) to correlate incoming alert groups with historical incidents,\nproviding analysts with additional insights."
    },
    {
        "date": "2025-09",
        "title": "T-SiamTPN: Temporal Siamese Transformer Pyramid Networks for Robust and Efficient UAV Tracking",
        "author": "Hojat Ardi, Amir Jahanshahi, and Ali Diba",
        "link": "http://arxiv.org/abs/2509.12913v1",
        "abstract": "Aerial object tracking remains a challenging task due to scale variations,\ndynamic backgrounds, clutter, and frequent occlusions. While most existing\ntrackers emphasize spatial cues, they often overlook temporal dependencies,\nresulting in limited robustness in long-term tracking and under occlusion.\nFurthermore, correlation-based Siamese trackers are inherently constrained by\nthe linear nature of correlation operations, making them ineffective against\ncomplex, non-linear appearance changes. To address these limitations, we\nintroduce T-SiamTPN, a temporal-aware Siamese tracking framework that extends\nthe SiamTPN architecture with explicit temporal modeling. Our approach\nincorporates temporal feature fusion and attention-based interactions,\nstrengthening temporal consistency and enabling richer feature representations.\nThese enhancements yield significant improvements over the baseline and achieve\nperformance competitive with state-of-the-art trackers. Crucially, despite the\nadded temporal modules, T-SiamTPN preserves computational efficiency. Deployed\non the resource-constrained Jetson Nano, the tracker runs in real time at 7.1\nFPS, demonstrating its suitability for real-world embedded applications without\nnotable runtime overhead. Experimental results highlight substantial gains:\ncompared to the baseline, T-SiamTPN improves success rate by 13.7% and\nprecision by 14.7%. These findings underscore the importance of temporal\nmodeling in Siamese tracking frameworks and establish T-SiamTPN as a strong and\nefficient solution for aerial object tracking. Code is available at:\nhttps://github.com/to/be/released"
    },
    {
        "date": "2025-09",
        "title": "MEGAN: Mixture of Experts for Robust Uncertainty Estimation in Endoscopy Videos",
        "author": "Damola Agbelese, Krishna Chaitanya, Pushpak Pati, Chaitanya Parmar, Pooya Mobadersany, Shreyas Fadnavis, Lindsey Surace, Shadi Yarandi, Louis R. Ghanem, Molly Lucas, Tommaso Mansi, Oana Gabriela Cula, Pablo F. Damasceno, and Kristopher Standish",
        "link": "http://arxiv.org/abs/2509.12772v1",
        "abstract": "Reliable uncertainty quantification (UQ) is essential in medical AI.\nEvidential Deep Learning (EDL) offers a computationally efficient way to\nquantify model uncertainty alongside predictions, unlike traditional methods\nsuch as Monte Carlo (MC) Dropout and Deep Ensembles (DE). However, all these\nmethods often rely on a single expert's annotations as ground truth for model\ntraining, overlooking the inter-rater variability in healthcare. To address\nthis issue, we propose MEGAN, a Multi-Expert Gating Network that aggregates\nuncertainty estimates and predictions from multiple AI experts via EDL models\ntrained with diverse ground truths and modeling strategies. MEGAN's gating\nnetwork optimally combines predictions and uncertainties from each EDL model,\nenhancing overall prediction confidence and calibration. We extensively\nbenchmark MEGAN on endoscopy videos for Ulcerative colitis (UC) disease\nseverity estimation, assessed by visual labeling of Mayo Endoscopic Subscore\n(MES), where inter-rater variability is prevalent. In large-scale prospective\nUC clinical trial, MEGAN achieved a 3.5% improvement in F1-score and a 30.5%\nreduction in Expected Calibration Error (ECE) compared to existing methods.\nFurthermore, MEGAN facilitated uncertainty-guided sample stratification,\nreducing the annotation burden and potentially increasing efficiency and\nconsistency in UC trials."
    },
    {
        "date": "2025-09",
        "title": "Defense-to-Attack: Bypassing Weak Defenses Enables Stronger Jailbreaks in Vision-Language Models",
        "author": "Yunhan Zhao, Xiang Zheng, and Xingjun Ma",
        "link": "http://arxiv.org/abs/2509.12724v1",
        "abstract": "Despite their superb capabilities, Vision-Language Models (VLMs) have been\nshown to be vulnerable to jailbreak attacks. While recent jailbreaks have\nachieved notable progress, their effectiveness and efficiency can still be\nimproved. In this work, we reveal an interesting phenomenon: incorporating weak\ndefense into the attack pipeline can significantly enhance both the\neffectiveness and the efficiency of jailbreaks on VLMs. Building on this\ninsight, we propose Defense2Attack, a novel jailbreak method that bypasses the\nsafety guardrails of VLMs by leveraging defensive patterns to guide jailbreak\nprompt design. Specifically, Defense2Attack consists of three key components:\n(1) a visual optimizer that embeds universal adversarial perturbations with\naffirmative and encouraging semantics; (2) a textual optimizer that refines the\ninput using a defense-styled prompt; and (3) a red-team suffix generator that\nenhances the jailbreak through reinforcement fine-tuning. We empirically\nevaluate our method on four VLMs and four safety benchmarks. The results\ndemonstrate that Defense2Attack achieves superior jailbreak performance in a\nsingle attempt, outperforming state-of-the-art attack methods that often\nrequire multiple tries. Our work offers a new perspective on jailbreaking VLMs."
    },
    {
        "date": "2025-09",
        "title": "A Systematic Evaluation of Parameter-Efficient Fine-Tuning Methods for the Security of Code LLMs",
        "author": "Kiho Lee, Jungkon Kim, Doowon Kim, and Hyoungshick Kim",
        "link": "http://arxiv.org/abs/2509.12649v1",
        "abstract": "Code-generating Large Language Models (LLMs) significantly accelerate\nsoftware development. However, their frequent generation of insecure code\npresents serious risks. We present a comprehensive evaluation of seven\nparameter-efficient fine-tuning (PEFT) techniques, demonstrating substantial\ngains in secure code generation without compromising functionality. Our\nresearch identifies prompt-tuning as the most effective PEFT method, achieving\nan 80.86% Overall-Secure-Rate on CodeGen2 16B, a 13.5-point improvement over\nthe 67.28% baseline. Optimizing decoding strategies through sampling\ntemperature further elevated security to 87.65%. This equates to a reduction of\napproximately 203,700 vulnerable code snippets per million generated. Moreover,\nprompt and prefix tuning increase robustness against poisoning attacks in our\nTrojanPuzzle evaluation, with strong performance against CWE-79 and CWE-502\nattack vectors. Our findings generalize across Python and Java, confirming\nprompt-tuning's consistent effectiveness. This study provides essential\ninsights and practical guidance for building more resilient software systems\nwith LLMs."
    },
    {
        "date": "2025-09",
        "title": "CIARD: Cyclic Iterative Adversarial Robustness Distillation",
        "author": "Liming Lu, Shuchao Pang, Xu Zheng, Xiang Gu, Anan Du, Yunhuai Liu, and Yongbin Zhou",
        "link": "http://arxiv.org/abs/2509.12633v1",
        "abstract": "Adversarial robustness distillation (ARD) aims to transfer both performance\nand robustness from teacher model to lightweight student model, enabling\nresilient performance on resource-constrained scenarios. Though existing ARD\napproaches enhance student model's robustness, the inevitable by-product leads\nto the degraded performance on clean examples. We summarize the causes of this\nproblem inherent in existing methods with dual-teacher framework as: 1. The\ndivergent optimization objectives of dual-teacher models, i.e., the clean and\nrobust teachers, impede effective knowledge transfer to the student model, and\n2. The iteratively generated adversarial examples during training lead to\nperformance deterioration of the robust teacher model. To address these\nchallenges, we propose a novel Cyclic Iterative ARD (CIARD) method with two key\ninnovations: a. A multi-teacher framework with contrastive push-loss alignment\nto resolve conflicts in dual-teacher optimization objectives, and b. Continuous\nadversarial retraining to maintain dynamic teacher robustness against\nperformance degradation from the varying adversarial examples. Extensive\nexperiments on CIFAR-10, CIFAR-100, and Tiny-ImageNet demonstrate that CIARD\nachieves remarkable performance with an average 3.53 improvement in adversarial\ndefense rates across various attack scenarios and a 5.87 increase in clean\nsample accuracy, establishing a new benchmark for balancing model robustness\nand generalization. Our code is available at https://github.com/eminentgu/CIARD"
    },
    {
        "date": "2025-09",
        "title": "DisorientLiDAR: Physical Attacks on LiDAR-based Localization",
        "author": "Yizhen Lao, Yu Zhang, Ziting Wang, Chengbo Wang, Yifei Xue, and Wanpeng Shao",
        "link": "http://arxiv.org/abs/2509.12595v1",
        "abstract": "Deep learning models have been shown to be susceptible to adversarial attacks\nwith visually imperceptible perturbations. Even this poses a serious security\nchallenge for the localization of self-driving cars, there has been very little\nexploration of attack on it, as most of adversarial attacks have been applied\nto 3D perception. In this work, we propose a novel adversarial attack framework\ncalled DisorientLiDAR targeting LiDAR-based localization. By\nreverse-engineering localization models (e.g., feature extraction networks),\nadversaries can identify critical keypoints and strategically remove them,\nthereby disrupting LiDAR-based localization. Our proposal is first evaluated on\nthree state-of-the-art point-cloud registration models (HRegNet, D3Feat, and\nGeoTransformer) using the KITTI dataset. Experimental results demonstrate that\nremoving regions containing Top-K keypoints significantly degrades their\nregistration accuracy. We further validate the attack's impact on the Autoware\nautonomous driving platform, where hiding merely a few critical regions induces\nnoticeable localization drift. Finally, we extended our attacks to the physical\nworld by hiding critical regions with near-infrared absorptive materials,\nthereby successfully replicate the attack effects observed in KITTI data. This\nstep has been closer toward the realistic physical-world attack that\ndemonstrate the veracity and generality of our proposal."
    },
    {
        "date": "2025-09",
        "title": "Secure and Efficient Out-of-band Call Metadata Transmission",
        "author": "David Adei, Varun Madathil, Nithin Shyam S., and Bradley Reaves",
        "link": "http://arxiv.org/abs/2509.12582v1",
        "abstract": "The STIR/SHAKEN (S/S) attestation Framework mandated by the United States,\nCanada, and France to combat pervasive telephone abuse has not achieved its\ngoals, partly because legacy non-VoIP infrastructure could not participate. The\nindustry solution to extend S/S broadcasts sensitive metadata of every non-VoIP\ncall in plaintext to every third party required to facilitate the system. It\nhas no mechanism to determine whether a provider's request for call data is\nappropriate, nor can it ensure that every copy of that call data is unavailable\nafter its specified expiration. It threatens subscriber privacy and provider\nconfidentiality.\n  In this paper, we present Sidecar, a distributed, privacy-preserving system\nwith tunable decentralization that securely extends S/S across all telephone\nnetwork technologies. We introduce the notion of secure out-of-band signaling\nfor telephony and formalize its system and security requirements. We then\ndesign novel, scalable protocols that realize these requirements and prove\ntheir security within the Universal Composability framework. Finally, we\ndemonstrate Sidecar's efficiency with our open-sourced reference\nimplementation. Compared to the current solution, Sidecar 1) protects the\nconfidentiality of subscriber identity and provider trade secrets, 2)\nguarantees record expiration as long as a single node handling a record is\nhonest, 3) reduces resource requirements while providing virtually identical\ncall-setup times and equivalent or better uptimes, and 4) enables secure\npay-per-use billing and integrates mechanisms to mitigate and detect\nmisbehavior. Moreover, Sidecar can be extended to provide the same security\nguarantees for arbitrary call metadata. Not only is Sidecar a superior\napproach, it is also a transformative tool to retrofit fragmented global\ntelephony and enable future improvements, such as stronger call authentication\nand Branded Calling."
    },
    {
        "date": "2025-09",
        "title": "Selective Risk Certification for LLM Outputs via Information-Lift Statistics: PAC-Bayes, Robustness, and Skeleton Design",
        "author": "Sanjeda Akter, Ibne Farabi Shihab, and Anuj Sharma",
        "link": "http://arxiv.org/abs/2509.12527v1",
        "abstract": "Large language models often produce plausible but incorrect outputs. Existing\nheuristics such as HallBayes lack formal guarantees. We develop the first\ncomprehensive theory of \\emph{information-lift certificates} under selective\nclassification. Our contributions are: (i) a PAC-Bayes \\emph{sub-gamma}\nanalysis extending beyond standard Bernstein bounds; (ii) explicit skeleton\nsensitivity theorems quantifying robustness to misspecification; (iii)\nfailure-mode guarantees under assumption violations; and (iv) a principled\nvariational method for skeleton construction. Across six datasets and multiple\nmodel families, we validate assumptions empirically, reduce abstention by\n12--15\\% at the same risk, and maintain runtime overhead below 20\\% (further\nreduced via batching)."
    },
    {
        "date": "2025-09",
        "title": "Evaluating Robustness of Vision-Language Models Under Noisy Conditions",
        "author": "Purushoth, and Alireza",
        "link": "http://arxiv.org/abs/2509.12492v1",
        "abstract": "Vision-Language Models (VLMs) have attained exceptional success across\nmultimodal tasks such as image captioning and visual question answering.\nHowever, their robustness under noisy conditions remains unfamiliar. In this\nstudy, we present a comprehensive evaluation framework to evaluate the\nperformance of several state-of-the-art VLMs under controlled perturbations,\nincluding lighting variation, motion blur, and compression artifacts. We used\nboth lexical-based metrics (BLEU, METEOR, ROUGE, CIDEr) and neural-based\nsimilarity measures using sentence embeddings to quantify semantic alignment.\nOur experiments span diverse datasets, revealing key insights: (1)\ndescriptiveness of ground-truth captions significantly influences model\nperformance; (2) larger models like LLaVA excel in semantic understanding but\ndo not universally outperform smaller models; and (3) certain noise types, such\nas JPEG compression and motion blur, dramatically degrade performance across\nmodels. Our findings highlight the nuanced trade-offs between model size,\ndataset characteristics, and noise resilience, offering a standardized\nbenchmark for future robust multimodal learning."
    },
    {
        "date": "2025-09",
        "title": "Redefining Website Fingerprinting Attacks With Multiagent LLMs",
        "author": "Chuxu Song, Dheekshith Dev Manohar Mekala, Hao Wang, and Richard Martin",
        "link": "http://arxiv.org/abs/2509.12462v1",
        "abstract": "Website Fingerprinting (WFP) uses deep learning models to classify encrypted\nnetwork traffic to infer visited websites. While historically effective, prior\nmethods fail to generalize to modern web environments. Single-page applications\n(SPAs) eliminate the paradigm of websites as sets of discrete pages,\nundermining page-based classification, and traffic from scripted browsers lacks\nthe behavioral richness seen in real user sessions. Our study reveals that\nusers exhibit highly diverse behaviors even on the same website, producing\ntraffic patterns that vary significantly across individuals. This behavioral\nentropy makes WFP a harder problem than previously assumed and highlights the\nneed for larger, more diverse, and representative datasets to achieve robust\nperformance. To address this, we propose a new paradigm: we drop\nsession-boundaries in favor of contiguous traffic segments and develop a\nscalable data generation pipeline using large language models (LLM) agents.\nThese multi-agent systems coordinate decision-making and browser interaction to\nsimulate realistic, persona-driven browsing behavior at 3--5x lower cost than\nhuman collection. We evaluate nine state-of-the-art WFP models on traffic from\n20 modern websites browsed by 30 real users, and compare training performance\nacross human, scripted, and LLM-generated datasets. All models achieve under\n10\\% accuracy when trained on scripted traffic and tested on human data. In\ncontrast, LLM-generated traffic boosts accuracy into the 80\\% range,\ndemonstrating strong generalization to real-world traces. Our findings indicate\nthat for modern WFP, model performance is increasingly bottlenecked by data\nquality, and that scalable, semantically grounded synthetic traffic is\nessential for capturing the complexity of real user behavior."
    },
    {
        "date": "2025-09",
        "title": "Amulet: a Python Library for Assessing Interactions Among ML Defenses and Risks",
        "author": "Asim Waheed, Vasisht Duddu, Rui Zhang, Sebastian Szyller, and N. Asokan",
        "link": "http://arxiv.org/abs/2509.12386v1",
        "abstract": "ML models are susceptible to risks to security, privacy, and fairness.\nSeveral defenses are designed to protect against their intended risks, but can\ninadvertently affect susceptibility to other unrelated risks, known as\nunintended interactions. Several jurisdictions are preparing ML regulatory\nframeworks that require ML practitioners to assess the susceptibility of ML\nmodels to different risks. A library for valuating unintended interactions that\ncan be used by (a) practitioners to evaluate unintended interactions at scale\nprior to model deployment and (b) researchers to design defenses which do not\nsuffer from an unintended increase in unrelated risks. Ideally, such a library\nshould be i) comprehensive by including representative attacks, defenses and\nmetrics for different risks, ii) extensible to new modules due to its modular\ndesign, iii) consistent with a user-friendly API template for inputs and\noutputs, iv) applicable to evaluate previously unexplored unintended\ninteractions. We present AMULET, a Python library that covers risks to\nsecurity, privacy, and fairness, which satisfies all these requirements. AMULET\ncan be used to evaluate unexplored unintended interactions, compare\neffectiveness between defenses or attacks, and include new attacks and\ndefenses."
    },
    {
        "date": "2025-09",
        "title": "Early Approaches to Adversarial Fine-Tuning for Prompt Injection Defense: A 2022 Study of GPT-3 and Contemporary Models",
        "author": "Gustavo Sandoval, Denys Fenchenko, and Junyao Chen",
        "link": "http://arxiv.org/abs/2509.14271v1",
        "abstract": "This paper documents early research conducted in 2022 on defending against\nprompt injection attacks in large language models, providing historical context\nfor the evolution of this critical security domain. This research focuses on\ntwo adversarial attacks against Large Language Models (LLMs): prompt injection\nand goal hijacking. We examine how to construct these attacks, test them on\nvarious LLMs, and compare their effectiveness. We propose and evaluate a novel\ndefense technique called Adversarial Fine-Tuning. Our results show that,\nwithout this defense, the attacks succeeded 31\\% of the time on GPT-3 series\nmodels. When using our Adversarial Fine-Tuning approach, attack success rates\nwere reduced to near zero for smaller GPT-3 variants (Ada, Babbage, Curie),\nthough we note that subsequent research has revealed limitations of\nfine-tuning-based defenses. We also find that more flexible models exhibit\ngreater vulnerability to these attacks. Consequently, large models such as\nGPT-3 Davinci are more vulnerable than smaller models like GPT-2. While the\nspecific models tested are now superseded, the core methodology and empirical\nfindings contributed to the foundation of modern prompt injection defense\nresearch, including instruction hierarchy systems and constitutional AI\napproaches."
    },
    {
        "date": "2025-09",
        "title": "Enhancing Smart Farming Through Federated Learning: A Secure, Scalable, and Efficient Approach for AI-Driven Agriculture",
        "author": "Ritesh Janga, and Rushit Dave",
        "link": "http://arxiv.org/abs/2509.12363v1",
        "abstract": "The agricultural sector is undergoing a transformation with the integration\nof advanced technologies, particularly in data-driven decision-making. This\nwork proposes a federated learning framework for smart farming, aiming to\ndevelop a scalable, efficient, and secure solution for crop disease detection\ntailored to the environmental and operational conditions of Minnesota farms. By\nmaintaining sensitive farm data locally and enabling collaborative model\nupdates, our proposed framework seeks to achieve high accuracy in crop disease\nclassification without compromising data privacy. We outline a methodology\ninvolving data collection from Minnesota farms, application of local deep\nlearning algorithms, transfer learning, and a central aggregation server for\nmodel refinement, aiming to achieve improved accuracy in disease detection,\ngood generalization across agricultural scenarios, lower costs in communication\nand training time, and earlier identification and intervention against diseases\nin future implementations. We outline a methodology and anticipated outcomes,\nsetting the stage for empirical validation in subsequent studies. This work\ncomes in a context where more and more demand for data-driven interpretations\nin agriculture has to be weighed with concerns about privacy from farms that\nare hesitant to share their operational data. This will be important to provide\na secure and efficient disease detection method that can finally revolutionize\nsmart farming systems and solve local agricultural problems with data\nconfidentiality. In doing so, this paper bridges the gap between advanced\nmachine learning techniques and the practical, privacy-sensitive needs of\nfarmers in Minnesota and beyond, leveraging the benefits of federated learning."
    },
    {
        "date": "2025-09",
        "title": "From Autoencoders to CycleGAN: Robust Unpaired Face Manipulation via Adversarial Learning",
        "author": "Collin Guo",
        "link": "http://arxiv.org/abs/2509.12176v1",
        "abstract": "Human face synthesis and manipulation are increasingly important in\nentertainment and AI, with a growing demand for highly realistic,\nidentity-preserving images even when only unpaired, unaligned datasets are\navailable. We study unpaired face manipulation via adversarial learning, moving\nfrom autoencoder baselines to a robust, guided CycleGAN framework. While\nautoencoders capture coarse identity, they often miss fine details. Our\napproach integrates spectral normalization for stable training, identity- and\nperceptual-guided losses to preserve subject identity and high-level structure,\nand landmark-weighted cycle constraints to maintain facial geometry across pose\nand illumination changes. Experiments show that our adversarial trained\nCycleGAN improves realism (FID), perceptual quality (LPIPS), and identity\npreservation (ID-Sim) over autoencoders, with competitive cycle-reconstruction\nSSIM and practical inference times, which achieved high quality without paired\ndatasets and approaching pix2pix on curated paired subsets. These results\ndemonstrate that guided, spectrally normalized CycleGANs provide a practical\npath from autoencoders to robust unpaired face manipulation."
    },
    {
        "date": "2025-09",
        "title": "Robust Fetal Pose Estimation across Gestational Ages via Cross-Population Augmentation",
        "author": "Sebastian Diaz, Benjamin Billot, Neel Dey, Molin Zhang, Esra Abaci Turk, P. Ellen Grant, Polina Golland, and Elfar Adalsteinsson",
        "link": "http://arxiv.org/abs/2509.12062v1",
        "abstract": "Fetal motion is a critical indicator of neurological development and\nintrauterine health, yet its quantification remains challenging, particularly\nat earlier gestational ages (GA). Current methods track fetal motion by\npredicting the location of annotated landmarks on 3D echo planar imaging (EPI)\ntime-series, primarily in third-trimester fetuses. The predicted landmarks\nenable simplification of the fetal body for downstream analysis. While these\nmethods perform well within their training age distribution, they consistently\nfail to generalize to early GAs due to significant anatomical changes in both\nmother and fetus across gestation, as well as the difficulty of obtaining\nannotated early GA EPI data. In this work, we develop a cross-population data\naugmentation framework that enables pose estimation models to robustly\ngeneralize to younger GA clinical cohorts using only annotated images from\nolder GA cohorts. Specifically, we introduce a fetal-specific augmentation\nstrategy that simulates the distinct intrauterine environment and fetal\npositioning of early GAs. Our experiments find that cross-population\naugmentation yields reduced variability and significant improvements across\nboth older GA and challenging early GA cases. By enabling more reliable pose\nestimation across gestation, our work potentially facilitates early clinical\ndetection and intervention in challenging 4D fetal imaging settings. Code is\navailable at https://github.com/sebodiaz/cross-population-pose."
    },
    {
        "date": "2025-09",
        "title": "RAM++: Robust Representation Learning via Adaptive Mask for All-in-One Image Restoration",
        "author": "Zilong Zhang, Chujie Qin, Chunle Guo, Yong Zhang, Chao Xue, Ming-Ming Cheng, and Chongyi Li",
        "link": "http://arxiv.org/abs/2509.12039v1",
        "abstract": "This work presents Robust Representation Learning via Adaptive Mask (RAM++),\na two-stage framework for all-in-one image restoration. RAM++ integrates\nhigh-level semantic understanding with low-level texture generation to achieve\ncontent-oriented robust restoration. It addresses the limitations of existing\ndegradation-oriented methods in extreme scenarios (e.g., degradations strongly\ncoupled with image structures). RAM++ also mitigates common challenges such as\nunbalanced performance across tasks, overfitting to seen degradations, and weak\ngeneralization to unseen ones through three key designs: 1) Adaptive\nSemantic-Aware Mask (AdaSAM): a pretraining strategy that applies pixel-level\nmasks to semantically rich and textured regions. This design enables the\nnetwork to learn both generative priors and image content priors from various\ndegradations. 2) Mask Attribute Conductance (MAC): a selective fine-tuning\nstrategy that adjusts the layers with higher contributions to bridge the\nintegrity gap between masked pretraining and full-image fine-tuning while\nretaining learned priors. 3) Robust Feature Regularization (RFR): a strategy\nthat leverages DINOv2's semantically consistent and degradation-invariant\nrepresentations, together with efficient feature fusion, to achieve faithful\nand semantically coherent restoration. With these designs, RAM++ achieves\nrobust, well-balanced, and state-of-the-art performance across seen, unseen,\nextreme, and mixed degradations. Our code and model will be released at\nhttps://github.com/DragonisCV/RAM"
    },
    {
        "date": "2025-09",
        "title": "Robust Concept Erasure in Diffusion Models: A Theoretical Perspective on Security and Robustness",
        "author": "Zixuan Fu, Yan Ren, Finn Carter, Chenyue Wen, Le Ku, Daheng Yu, Emily Davis, and Bo Zhang",
        "link": "http://arxiv.org/abs/2509.12024v1",
        "abstract": "Diffusion models have achieved unprecedented success in image generation but\npose increasing risks in terms of privacy, fairness, and security. A growing\ndemand exists to \\emph{erase} sensitive or harmful concepts (e.g., NSFW\ncontent, private individuals, artistic styles) from these models while\npreserving their overall generative capabilities. We introduce \\textbf{SCORE}\n(Secure and Concept-Oriented Robust Erasure), a novel framework for robust\nconcept removal in diffusion models. SCORE formulates concept erasure as an\n\\emph{adversarial independence} problem, theoretically guaranteeing that the\nmodel's outputs become statistically independent of the erased concept. Unlike\nprior heuristic methods, SCORE minimizes the mutual information between a\ntarget concept and generated outputs, yielding provable erasure guarantees. We\nprovide formal proofs establishing convergence properties and derive upper\nbounds on residual concept leakage. Empirically, we evaluate SCORE on Stable\nDiffusion and FLUX across four challenging benchmarks: object erasure, NSFW\nremoval, celebrity face suppression, and artistic style unlearning. SCORE\nconsistently outperforms state-of-the-art methods including EraseAnything, ANT,\nMACE, ESD, and UCE, achieving up to \\textbf{12.5\\%} higher erasure efficacy\nwhile maintaining comparable or superior image quality. By integrating\nadversarial optimization, trajectory consistency, and saliency-driven\nfine-tuning, SCORE sets a new standard for secure and robust concept erasure in\ndiffusion models."
    },
    {
        "date": "2025-09",
        "title": "Time-Constrained Intelligent Adversaries for Automation Vulnerability Testing: A Multi-Robot Patrol Case Study",
        "author": "James C. Ward, Alex Bott, Connor York, and Edmund R. Hunt",
        "link": "http://arxiv.org/abs/2509.11971v1",
        "abstract": "Simulating hostile attacks of physical autonomous systems can be a useful\ntool to examine their robustness to attack and inform vulnerability-aware\ndesign. In this work, we examine this through the lens of multi-robot patrol,\nby presenting a machine learning-based adversary model that observes robot\npatrol behavior in order to attempt to gain undetected access to a secure\nenvironment within a limited time duration. Such a model allows for evaluation\nof a patrol system against a realistic potential adversary, offering insight\ninto future patrol strategy design. We show that our new model outperforms\nexisting baselines, thus providing a more stringent test, and examine its\nperformance against multiple leading decentralized multi-robot patrol\nstrategies."
    },
    {
        "date": "2025-09",
        "title": "NeuroGaze-Distill: Brain-informed Distillation and Depression-Inspired Geometric Priors for Robust Facial Emotion Recognition",
        "author": "Zilin Li, Weiwei Xu, Xuanqi Zhao, and Yiran Zhu",
        "link": "http://arxiv.org/abs/2509.11916v1",
        "abstract": "Facial emotion recognition (FER) models trained only on pixels often fail to\ngeneralize across datasets because facial appearance is an indirect and biased\nproxy for underlying affect. We present NeuroGaze-Distill, a cross-modal\ndistillation framework that transfers brain-informed priors into an image-only\nFER student via static Valence/Arousal (V/A) prototypes and a\ndepression-inspired geometric prior (D-Geo). A teacher trained on EEG\ntopographic maps from DREAMER (with MAHNOB-HCI as unlabeled support) produces a\nconsolidated 5x5 V/A prototype grid that is frozen and reused; no EEG-face\npairing and no non-visual signals at deployment are required. The student\n(ResNet-18/50) is trained on FERPlus with conventional CE/KD and two\nlightweight regularizers: (i) Proto-KD (cosine) aligns student features to the\nstatic prototypes; (ii) D-Geo softly shapes the embedding geometry in line with\naffective findings often reported in depression research (e.g., anhedonia-like\ncontraction in high-valence regions). We evaluate both within-domain (FERPlus\nvalidation) and cross-dataset protocols (AffectNet-mini; optional CK+),\nreporting standard 8-way scores alongside present-only Macro-F1 and balanced\naccuracy to fairly handle label-set mismatch. Ablations attribute consistent\ngains to prototypes and D-Geo, and favor 5x5 over denser grids for stability.\nThe method is simple, deployable, and improves robustness without architectural\ncomplexity."
    },
    {
        "date": "2025-09",
        "title": "PREDICT-GBM: Platform for Robust Evaluation and Development of Individualized Computational Tumor Models in Glioblastoma",
        "author": "L. Zimmer, J. Weidner, M. Balcerak, F. Kofler, I. Ezhov, B. Menze, and B. Wiestler",
        "link": "http://arxiv.org/abs/2509.13360v1",
        "abstract": "Glioblastoma is the most prevalent primary brain malignancy, distinguished by\nits highly invasive behavior and exceptionally high rates of recurrence.\nConventional radiation therapy, which employs uniform treatment margins, fails\nto account for patient-specific anatomical and biological factors that\ncritically influence tumor cell migration. To address this limitation, numerous\ncomputational models of glioblastoma growth have been developed, enabling\ngeneration of tumor cell distribution maps extending beyond radiographically\nvisible regions and thus informing more precise treatment strategies. However,\ndespite encouraging preliminary findings, the clinical adoption of these growth\nmodels remains limited. To bridge this translational gap and accelerate both\nmodel development and clinical validation, we introduce PREDICT-GBM, a\ncomprehensive integrated pipeline and dataset for modeling and evaluation. This\nplatform enables systematic benchmarking of state-of-the-art tumor growth\nmodels using an expert-curated clinical dataset comprising 255 subjects with\ncomplete tumor segmentations and tissue characterization maps. Our analysis\ndemonstrates that personalized radiation treatment plans derived from tumor\ngrowth predictions achieved superior recurrence coverage compared to\nconventional uniform margin approaches for two of the evaluated models. This\nwork establishes a robust platform for advancing and systematically evaluating\ncutting-edge tumor growth modeling approaches, with the ultimate goal of\nfacilitating clinical translation and improving patient outcomes."
    },
    {
        "date": "2025-09",
        "title": "Efficient Byzantine-Robust Privacy-Preserving Federated Learning via Dimension Compression",
        "author": "Xian Qin, Xue Yang, and Xiaohu Tang",
        "link": "http://arxiv.org/abs/2509.11870v1",
        "abstract": "Federated Learning (FL) allows collaborative model training across\ndistributed clients without sharing raw data, thus preserving privacy. However,\nthe system remains vulnerable to privacy leakage from gradient updates and\nByzantine attacks from malicious clients. Existing solutions face a critical\ntrade-off among privacy preservation, Byzantine robustness, and computational\nefficiency. We propose a novel scheme that effectively balances these competing\nobjectives by integrating homomorphic encryption with dimension compression\nbased on the Johnson-Lindenstrauss transformation. Our approach employs a\ndual-server architecture that enables secure Byzantine defense in the\nciphertext domain while dramatically reducing computational overhead through\ngradient compression. The dimension compression technique preserves the\ngeometric relationships necessary for Byzantine defence while reducing\ncomputation complexity from $O(dn)$ to $O(kn)$ cryptographic operations, where\n$k \\ll d$. Extensive experiments across diverse datasets demonstrate that our\napproach maintains model accuracy comparable to non-private FL while\neffectively defending against Byzantine clients comprising up to $40\\%$ of the\nnetwork."
    },
    {
        "date": "2025-09",
        "title": "Tenma: Robust Cross-Embodiment Robot Manipulation with Diffusion Transformer",
        "author": "Travis Davies, Yiqi Huang, Yunxin Liu, Xiang Chen, Huxian Liu, and Luhui Hu",
        "link": "http://arxiv.org/abs/2509.11865v1",
        "abstract": "Scaling Transformer policies and diffusion models has advanced robotic\nmanipulation, yet combining these techniques in lightweight, cross-embodiment\nlearning settings remains challenging. We study design choices that most affect\nstability and performance for diffusion-transformer policies trained on\nheterogeneous, multimodal robot data, and introduce Tenma, a lightweight\ndiffusion-transformer for bi-manual arm control. Tenma integrates multiview\nRGB, proprioception, and language via a cross-embodiment normalizer that maps\ndisparate state/action spaces into a shared latent space; a Joint State-Time\nencoder for temporally aligned observation learning with inference speed\nboosts; and a diffusion action decoder optimized for training stability and\nlearning capacity. Across benchmarks and under matched compute, Tenma achieves\nan average success rate of 88.95% in-distribution and maintains strong\nperformance under object and scene shifts, substantially exceeding baseline\npolicies whose best in-distribution average is 18.12%. Despite using moderate\ndata scale, Tenma delivers robust manipulation and generalization, indicating\nthe great potential for multimodal and cross-embodiment learning strategies for\nfurther augmenting the capacity of transformer-based imitation learning\npolicies."
    },
    {
        "date": "2025-09",
        "title": "NeuroStrike: Neuron-Level Attacks on Aligned LLMs",
        "author": "Lichao Wu, Sasha Behrouzi, Mohamadreza Rostami, Maximilian Thang, Stjepan Picek, and Ahmad-Reza Sadeghi",
        "link": "http://arxiv.org/abs/2509.11864v1",
        "abstract": "Safety alignment is critical for the ethical deployment of large language\nmodels (LLMs), guiding them to avoid generating harmful or unethical content.\nCurrent alignment techniques, such as supervised fine-tuning and reinforcement\nlearning from human feedback, remain fragile and can be bypassed by carefully\ncrafted adversarial prompts. Unfortunately, such attacks rely on trial and\nerror, lack generalizability across models, and are constrained by scalability\nand reliability.\n  This paper presents NeuroStrike, a novel and generalizable attack framework\nthat exploits a fundamental vulnerability introduced by alignment techniques:\nthe reliance on sparse, specialized safety neurons responsible for detecting\nand suppressing harmful inputs. We apply NeuroStrike to both white-box and\nblack-box settings: In the white-box setting, NeuroStrike identifies safety\nneurons through feedforward activation analysis and prunes them during\ninference to disable safety mechanisms. In the black-box setting, we propose\nthe first LLM profiling attack, which leverages safety neuron transferability\nby training adversarial prompt generators on open-weight surrogate models and\nthen deploying them against black-box and proprietary targets. We evaluate\nNeuroStrike on over 20 open-weight LLMs from major LLM developers. By removing\nless than 0.6% of neurons in targeted layers, NeuroStrike achieves an average\nattack success rate (ASR) of 76.9% using only vanilla malicious prompts.\nMoreover, Neurostrike generalizes to four multimodal LLMs with 100% ASR on\nunsafe image inputs. Safety neurons transfer effectively across architectures,\nraising ASR to 78.5% on 11 fine-tuned models and 77.7% on five distilled\nmodels. The black-box LLM profiling attack achieves an average ASR of 63.7%\nacross five black-box models, including the Google Gemini family."
    },
    {
        "date": "2025-09",
        "title": "Probabilistic Robustness Analysis in High Dimensional Space: Application to Semantic Segmentation Network",
        "author": "Navid Hashemi, Samuel Sasaki, Diego Manzanas Lopez, Ipek Oguz, Meiyi Ma, and Taylor T. Johnson",
        "link": "http://arxiv.org/abs/2509.11838v1",
        "abstract": "Semantic segmentation networks (SSNs) play a critical role in domains such as\nmedical imaging, autonomous driving, and environmental monitoring, where safety\nhinges on reliable model behavior under uncertainty. Yet, existing\nprobabilistic verification approaches struggle to scale with the complexity and\ndimensionality of modern segmentation tasks, often yielding guarantees that are\ntoo conservative to be practical. We introduce a probabilistic verification\nframework that is both architecture-agnostic and scalable to high-dimensional\noutputs. Our approach combines sampling-based reachability analysis with\nconformal inference (CI) to deliver provable guarantees while avoiding the\nexcessive conservatism of prior methods. To counteract CI's limitations in\nhigh-dimensional settings, we propose novel strategies that reduce conservatism\nwithout compromising rigor. Empirical evaluation on large-scale segmentation\nmodels across CamVid, OCTA-500, Lung Segmentation, and Cityscapes demonstrates\nthat our method provides reliable safety guarantees while substantially\ntightening bounds compared to SOTA. We also provide a toolbox implementing this\ntechnique, available on Github."
    },
    {
        "date": "2025-09",
        "title": "A Practical Adversarial Attack against Sequence-based Deep Learning Malware Classifiers",
        "author": "Kai Tan, Dongyang Zhan, Lin Ye, Hongli Zhang, and Binxing Fang",
        "link": "http://arxiv.org/abs/2509.11836v1",
        "abstract": "Sequence-based deep learning models (e.g., RNNs), can detect malware by\nanalyzing its behavioral sequences. Meanwhile, these models are susceptible to\nadversarial attacks. Attackers can create adversarial samples that alter the\nsequence characteristics of behavior sequences to deceive malware classifiers.\nThe existing methods for generating adversarial samples typically involve\ndeleting or replacing crucial behaviors in the original data sequences, or\ninserting benign behaviors that may violate the behavior constraints. However,\nthese methods that directly manipulate sequences make adversarial samples\ndifficult to implement or apply in practice. In this paper, we propose an\nadversarial attack approach based on Deep Q-Network and a heuristic\nbacktracking search strategy, which can generate perturbation sequences that\nsatisfy practical conditions for successful attacks. Subsequently, we utilize a\nnovel transformation approach that maps modifications back to the source code,\nthereby avoiding the need to directly modify the behavior log sequences. We\nconduct an evaluation of our approach, and the results confirm its\neffectiveness in generating adversarial samples from real-world malware\nbehavior sequences, which have a high success rate in evading anomaly detection\nmodels. Furthermore, our approach is practical and can generate adversarial\nsamples while maintaining the functionality of the modified software."
    },
    {
        "date": "2025-09",
        "title": "Collapse of Irrelevant Representations (CIR) Ensures Robust and Non-Disruptive LLM Unlearning",
        "author": "Filip Sondej, and Yushi Yang",
        "link": "http://arxiv.org/abs/2509.11816v1",
        "abstract": "Current unlearning techniques and safety training consistently fail to remove\ndangerous knowledge from language models. We analyze the root causes and\npropose a highly selective technique which unlearns robustly and without\ndisrupting general performance.\n  We perform PCA on activations and module output gradients to identify\nsubspaces containing common representations, and collapse them before\ncalculating unlearning updates. This way we avoid unlearning general\nrepresentations, and only target those specific to the unlearned facts.\n  When unlearning WMDP dataset facts from Llama-3.1-8B, we drop post-attack\naccuracy 80x more than our best baseline (Circuit Breakers) on biohazardous\nfacts and 30x more on cyberhazardous facts. Despite this, we disrupt general\nperformance 30x less (only 0.1% WikiText loss increase), while requiring less\nthan 3 GPU-seconds per fact."
    },
    {
        "date": "2025-09",
        "title": "DetectAnyLLM: Towards Generalizable and Robust Detection of Machine-Generated Text Across Domains and Models",
        "author": "Jiachen Fu, Chun-Le Guo, and Chongyi Li",
        "link": "http://arxiv.org/abs/2509.14268v1",
        "abstract": "The rapid advancement of large language models (LLMs) has drawn urgent\nattention to the task of machine-generated text detection (MGTD). However,\nexisting approaches struggle in complex real-world scenarios: zero-shot\ndetectors rely heavily on scoring model's output distribution while\ntraining-based detectors are often constrained by overfitting to the training\ndata, limiting generalization. We found that the performance bottleneck of\ntraining-based detectors stems from the misalignment between training objective\nand task needs. To address this, we propose Direct Discrepancy Learning (DDL),\na novel optimization strategy that directly optimizes the detector with\ntask-oriented knowledge. DDL enables the detector to better capture the core\nsemantics of the detection task, thereby enhancing both robustness and\ngeneralization. Built upon this, we introduce DetectAnyLLM, a unified detection\nframework that achieves state-of-the-art MGTD performance across diverse LLMs.\nTo ensure a reliable evaluation, we construct MIRAGE, the most diverse\nmulti-task MGTD benchmark. MIRAGE samples human-written texts from 10 corpora\nacross 5 text-domains, which are then re-generated or revised using 17\ncutting-edge LLMs, covering a wide spectrum of proprietary models and textual\nstyles. Extensive experiments on MIRAGE reveal the limitations of existing\nmethods in complex environment. In contrast, DetectAnyLLM consistently\noutperforms them, achieving over a 70% performance improvement under the same\ntraining data and base scoring model, underscoring the effectiveness of our\nDDL. Project page: {https://fjc2005.github.io/detectanyllm}."
    },
    {
        "date": "2025-09",
        "title": "Removal Attack and Defense on AI-generated Content Latent-based Watermarking",
        "author": "De Zhang Lee, Han Fang, Hanyi Wang, and Ee-Chien Chang",
        "link": "http://arxiv.org/abs/2509.11745v2",
        "abstract": "Digital watermarks can be embedded into AI-generated content (AIGC) by\ninitializing the generation process with starting points sampled from a secret\ndistribution. When combined with pseudorandom error-correcting codes, such\nwatermarked outputs can remain indistinguishable from unwatermarked objects,\nwhile maintaining robustness under whitenoise. In this paper, we go beyond\nindistinguishability and investigate security under removal attacks. We\ndemonstrate that indistinguishability alone does not necessarily guarantee\nresistance to adversarial removal. Specifically, we propose a novel attack that\nexploits boundary information leaked by the locations of watermarked objects.\nThis attack significantly reduces the distortion required to remove watermarks\n-- by up to a factor of $15 \\times$ compared to a baseline whitenoise attack\nunder certain settings. To mitigate such attacks, we introduce a defense\nmechanism that applies a secret transformation to hide the boundary, and prove\nthat the secret transformation effectively rendering any attacker's\nperturbations equivalent to those of a naive whitenoise adversary. Our\nempirical evaluations, conducted on multiple versions of Stable Diffusion,\nvalidate the effectiveness of both the attack and the proposed defense,\nhighlighting the importance of addressing boundary leakage in latent-based\nwatermarking schemes."
    },
    {
        "date": "2025-09",
        "title": "DRAG: Data Reconstruction Attack using Guided Diffusion",
        "author": "Wa-Kin Lei, Jun-Cheng Chen, and Shang-Tse Chen",
        "link": "http://arxiv.org/abs/2509.11724v1",
        "abstract": "With the rise of large foundation models, split inference (SI) has emerged as\na popular computational paradigm for deploying models across lightweight edge\ndevices and cloud servers, addressing data privacy and computational cost\nconcerns. However, most existing data reconstruction attacks have focused on\nsmaller CNN classification models, leaving the privacy risks of foundation\nmodels in SI settings largely unexplored. To address this gap, we propose a\nnovel data reconstruction attack based on guided diffusion, which leverages the\nrich prior knowledge embedded in a latent diffusion model (LDM) pre-trained on\na large-scale dataset. Our method performs iterative reconstruction on the\nLDM's learned image prior, effectively generating high-fidelity images\nresembling the original data from their intermediate representations (IR).\nExtensive experiments demonstrate that our approach significantly outperforms\nstate-of-the-art methods, both qualitatively and quantitatively, in\nreconstructing data from deep-layer IRs of the vision foundation model. The\nresults highlight the urgent need for more robust privacy protection mechanisms\nfor large models in SI scenarios. Code is available at:\nhttps://github.com/ntuaislab/DRAG."
    },
    {
        "date": "2025-09",
        "title": "A Holistic Approach to E-Commerce Innovation: Redefining Security and User Experience",
        "author": "Mohammad Olid Ali Akash, and Priyangana Saha",
        "link": "http://arxiv.org/abs/2509.11712v1",
        "abstract": "In the modern, fast-moving world of e-commerce, many Android apps face\nchallenges in providing a simple and secure shopping experience. Many of these\napps, often enough, have complicated designs that prevent users from finding\nwhat they want quickly, thus frustrating them and wasting their precious time.\nAnother major issue is that of security; with the limitation of payment options\nand weak authentication mechanisms, users' sensitive information can be\ncompromised. This research presents a new e-commerce platform that responds to\nthe above challenges with an intuitive interface and strong security measures.\nThe platform makes online shopping easy with well-organized categories of\nproducts and a fast, efficient checkout process. It also gives priority to\nsecurity by incorporating features such as Google authentication and\nSSL-secured payment gateways to protect user data and ensure secure\ntransactions. This paper discusses how a focus on user-friendliness, security,\nand personalization steps up the game for e-commerce platforms, providing\nworkable frameworks that match modern user needs and expectations. The findings\nshow the e-commerce user experience can be remodelled by the platform, hence\nopening ways for future developments in that respect."
    },
    {
        "date": "2025-09",
        "title": "An Unsupervised Learning Approach For A Reliable Profiling Of Cyber Threat Actors Reported Globally Based On Complete Contextual Information Of Cyber Attacks",
        "author": "Sawera Shahid, Umara Noor, and Zahid Rashid",
        "link": "http://arxiv.org/abs/2509.11683v1",
        "abstract": "Cyber attacks are rapidly increasing with the advancement of technology and\nthere is no protection for our information. To prevent future cyberattacks it\nis critical to promptly recognize cyberattacks and establish strong defense\nmechanisms against them. To respond to cybersecurity threats immediately, it is\nessential to examine the attackers skills, knowledge, and behaviors with the\ngoal of evaluating their impact on the system and comprehending the traits\nassociated with these attacks. Creating a profile of cyber threat actors based\non their traits or patterns of behavior can help to create effective defenses\nagainst cyberattacks in advance. In the current literature, multiple supervised\nmachine learning based approaches considered a smaller number of features for\nattacker profiling that are reported in textual cyber threat incident documents\nalthough these profiles have been developed based on the security experts own\nperception, we cannot rely on them. Supervised machine learning approaches\nstrictly depend upon the structure data set. This usually leads to a two step\nprocess where we first have to establish a structured data set before we can\nanalyze it and then employ it to construct defense mechanisms, which takes\ntime. In this paper, an unsupervised efficient agglomerative hierarchal\nclustering technique is proposed for profiling cybercriminal groups based on\ntheir comprehensive contextual threat information in order to address the\naforementioned issues. The main objective of this report is to identify the\nrelationship between cyber threat actors based on their common features,\naggregate them, and also profile cyber criminal groups."
    },
    {
        "date": "2025-09",
        "title": "Secure Human Oversight of AI: Exploring the Attack Surface of Human Oversight",
        "author": "Jonas C. Ditz, Veronika Lazar, Elmar Lichtme\u00df, Carola Plesch, Matthias Heck, Kevin Baum, and Markus Langer",
        "link": "http://arxiv.org/abs/2509.12290v1",
        "abstract": "Human oversight of AI is promoted as a safeguard against risks such as\ninaccurate outputs, system malfunctions, or violations of fundamental rights,\nand is mandated in regulation like the European AI Act. Yet debates on human\noversight have largely focused on its effectiveness, while overlooking a\ncritical dimension: the security of human oversight. We argue that human\noversight creates a new attack surface within the safety, security, and\naccountability architecture of AI operations. Drawing on cybersecurity\nperspectives, we analyze attack vectors that threaten the requirements of\neffective human oversight, thereby undermining the safety of AI operations.\nSuch attacks may target the AI system, its communication with oversight\npersonnel, or the personnel themselves. We then outline hardening strategies to\nmitigate these risks. Our contributions are: (1) introducing a security\nperspective on human oversight, and (2) providing an overview of attack vectors\nand hardening strategies to enable secure human oversight of AI."
    },
    {
        "date": "2025-09",
        "title": "Cyber Attack Mitigation Framework for Denial of Service (DoS) Attacks in Fog Computing",
        "author": "Fizza Khurshid, Umara Noor, and Zahid Rashid",
        "link": "http://arxiv.org/abs/2509.11668v1",
        "abstract": "Innovative solutions to cyber security issues are shaped by the ever-changing\nlandscape of cyber threats. Automating the mitigation of these threats can be\nachieved through a new methodology that addresses the domain of mitigation\nautomation, which is often overlooked. This literature overview emphasizes the\nlack of scholarly work focusing specifically on automated cyber threat\nmitigation, particularly in addressing challenges beyond detection. The\nproposed methodology comprise of the development of an automatic cyber threat\nmitigation framework tailored for Distributed Denial-of-Service (DDoS) attacks.\nThis framework adopts a multi-layer security approach, utilizing smart devices\nat the device layer, and leveraging fog network and cloud computing layers for\ndeeper understanding and technological adaptability. Initially, firewall\nrule-based packet inspection is conducted on simulated attack traffic to filter\nout DoS packets, forwarding legitimate packets to the fog. The methodology\nemphasizes the integration of fog detection through statistical and behavioral\nanalysis, specification-based detection, and deep packet inspection, resulting\nin a comprehensive cyber protection system. Furthermore, cloud-level inspection\nis performed to confirm and mitigate attacks using firewalls, enhancing\nstrategic defense and increasing robustness against cyber threats. These\nenhancements enhance understanding of the research framework's practical\nimplementation and assessment strategies, substantiating its importance in\naddressing current cyber security challenges and shaping future automation\nmitigation approaches."
    },
    {
        "date": "2025-09",
        "title": "Task-Agnostic Learnable Weighted-Knowledge Base Scheme for Robust Semantic Communications",
        "author": "Shiyao Jiang, Jian Jiao, Xingjian Zhang, Ye Wang, Dusit Niyato, and Qinyu Zhang",
        "link": "http://arxiv.org/abs/2509.11636v1",
        "abstract": "With the emergence of diverse and massive data in the upcoming\nsixth-generation (6G) networks, the task-agnostic semantic communication system\nis regarded to provide robust intelligent services. In this paper, we propose a\ntask-agnostic learnable weighted-knowledge base semantic communication (TALSC)\nframework for robust image transmission to address the real-world heterogeneous\ndata bias in KB, including label flipping noise and class imbalance. The TALSC\nframework incorporates a sample confidence module (SCM) as meta-learner and the\nsemantic coding networks as learners. The learners are updated based on the\nempirical knowledge provided by the learnable weighted-KB (LW-KB). Meanwhile,\nthe meta-learner evaluates the significance of samples according to the task\nloss feedback, and adjusts the update strategy of learners to enhance the\nrobustness in semantic recovery for unknown tasks. To strike a balance between\nSCM parameters and precision of significance evaluation, we design an SCM-grid\nextension (SCM-GE) approach by embedding the Kolmogorov-Arnold networks (KAN)\nwithin SCM, which leverages the concept of spline refinement in KAN and enables\nscalable SCM with customizable granularity without retraining. Simulations\ndemonstrate that the TALSC framework effectively mitigates the effects of\nflipping noise and class imbalance in task-agnostic image semantic\ncommunication, achieving at least 12% higher semantic recovery accuracy (SRA)\nand multi-scale structural similarity (MS-SSIM) compared to state-of-the-art\nmethods."
    },
    {
        "date": "2025-09",
        "title": "Reasoned Safety Alignment: Ensuring Jailbreak Defense via Answer-Then-Check",
        "author": "Chentao Cao, Xiaojun Xu, Bo Han, and Hang Li",
        "link": "http://arxiv.org/abs/2509.11629v1",
        "abstract": "As large language models (LLMs) continue to advance in capabilities, ensuring\ntheir safety against jailbreak attacks remains a critical challenge. In this\npaper, we introduce a novel safety alignment approach called Answer-Then-Check,\nwhich enhances LLM robustness against malicious prompts by applying thinking\nability to mitigate jailbreaking problems before producing a final answer to\nthe user. Our method enables models to directly answer the question in their\nthought and then critically evaluate its safety before deciding whether to\nprovide it. To implement this approach, we construct the Reasoned Safety\nAlignment (ReSA) dataset, comprising 80K examples that teach models to reason\nthrough direct responses and then analyze their safety. Experimental results\ndemonstrate that our approach achieves the Pareto frontier with superior safety\ncapability while decreasing over-refusal rates on over-refusal benchmarks.\nNotably, the model fine-tuned with ReSA maintains general reasoning\ncapabilities on benchmarks like MMLU, MATH500, and HumanEval. Besides, our\nmethod equips models with the ability to perform safe completion. Unlike\npost-hoc methods that can only reject harmful queries, our model can provide\nhelpful and safe alternative responses for sensitive topics (e.g., self-harm).\nFurthermore, we discover that training on a small subset of just 500 examples\ncan achieve comparable performance to using the full dataset, suggesting that\nsafety alignment may require less data than previously assumed."
    },
    {
        "date": "2025-09",
        "title": "Cyber Threat Hunting: Non-Parametric Mining of Attack Patterns from Cyber Threat Intelligence for Precise Threats Attribution",
        "author": "Rimsha Kanwal, Umara Noor, Zafar Iqbal, and Zahid Rashid",
        "link": "http://arxiv.org/abs/2509.11615v1",
        "abstract": "With the ever-changing landscape of cyber threats, identifying their origin\nhas become paramount, surpassing the simple task of attack classification.\nCyber threat attribution gives security analysts the insights they need to\ndevice effective threat mitigation strategies. Such strategies empower\nenterprises to proactively detect and defend against future cyber-attacks.\nHowever, existing approaches exhibit limitations in accurately identifying\nthreat actors, leading to low precision and a significant occurrence of false\npositives. Machine learning offers the potential to automate certain aspects of\ncyber threat attribution. The distributed nature of information regarding cyber\nthreat actors and their intricate attack methodologies has hindered substantial\nprogress in this domain. Cybersecurity analysts deal with an ever-expanding\ncollection of cyber threat intelligence documents. While these documents hold\nvaluable insights, their sheer volume challenges efficient organization and\nretrieval of pertinent information. To assist the cybersecurity analyst\nactivities, we propose a machine learning based approach featuring visually\ninteractive analytics tool named the Cyber-Attack Pattern Explorer (CAPE),\ndesigned to facilitate efficient information discovery by employing interactive\nvisualization and mining techniques. In the proposed system, a non-parametric\nmining technique is proposed to create a dataset for identifying the attack\npatterns within cyber threat intelligence documents. These attack patterns\nalign semantically with commonly employed themes ensuring ease of\ninterpretation. The extracted dataset is used for training of proposed machine\nlearning algorithms that enables the attribution of cyber threats with\nrespective to the actors."
    },
    {
        "date": "2025-09",
        "title": "E-ROBOT: a dimension-free method for robust statistics and machine learning via Schr\u00f6dinger bridge",
        "author": "Davide La Vecchia, and Hang Liu",
        "link": "http://arxiv.org/abs/2509.11532v1",
        "abstract": "We propose the Entropic-regularized Robust Optimal Transport (E-ROBOT)\nframework, a novel method that combines the robustness of ROBOT with the\ncomputational and statistical benefits of entropic regularization. We show\nthat, rooted in the Schr\\\"{o}dinger bridge problem theory, E-ROBOT defines the\nrobust Sinkhorn divergence $\\overline{W}_{\\varepsilon,\\lambda}$, where the\nparameter $\\lambda$ controls robustness and $\\varepsilon$ governs the\nregularization strength. Letting $n\\in \\mathbb{N}$ denote the sample size, a\ncentral theoretical contribution is establishing that the sample complexity of\n$\\overline{W}_{\\varepsilon,\\lambda}$ is $\\mathcal{O}(n^{-1/2})$, thereby\navoiding the curse of dimensionality that plagues standard ROBOT. This\ndimension-free property unlocks the use of $\\overline{W}_{\\varepsilon,\\lambda}$\nas a loss function in large-dimensional statistical and machine learning tasks.\nWith this regard, we demonstrate its utility through four applications:\ngoodness-of-fit testing; computation of barycenters for corrupted 2D and 3D\nshapes; definition of gradient flows; and image colour transfer. From the\ncomputation standpoint, a perk of our novel method is that it can be easily\nimplemented by modifying existing (\\texttt{Python}) routines. From the\ntheoretical standpoint, our work opens the door to many research directions in\nstatistics and machine learning: we discuss some of them."
    },
    {
        "date": "2025-09",
        "title": "DARD: Dice Adversarial Robustness Distillation against Adversarial Attacks",
        "author": "Jing Zou, Shungeng Zhang, Meikang Qiu, and Chong Li",
        "link": "http://arxiv.org/abs/2509.11525v1",
        "abstract": "Deep learning models are vulnerable to adversarial examples, posing critical\nsecurity challenges in real-world applications. While Adversarial Training (AT\n) is a widely adopted defense mechanism to enhance robustness, it often incurs\na trade-off by degrading performance on unperturbed, natural data. Recent\nefforts have highlighted that larger models exhibit enhanced robustness over\ntheir smaller counterparts. In this paper, we empirically demonstrate that such\nrobustness can be systematically distilled from large teacher models into\ncompact student models. To achieve better performance, we introduce Dice\nAdversarial Robustness Distillation (DARD), a novel method designed to transfer\nrobustness through a tailored knowledge distillation paradigm. Additionally, we\npropose Dice Projected Gradient Descent (DPGD), an adversarial example\ngeneralization method optimized for effective attack. Our extensive experiments\ndemonstrate that the DARD approach consistently outperforms adversarially\ntrained networks with the same architecture, achieving superior robustness and\nstandard accuracy."
    },
    {
        "date": "2025-09",
        "title": "MultiMAE for Brain MRIs: Robustness to Missing Inputs Using Multi-Modal Masked Autoencoder",
        "author": "Ayhan Can Erdur, Christian Beischl, Daniel Scholz, Jiazhen Pan, Benedikt Wiestler, Daniel Rueckert, and Jan C Peeken",
        "link": "http://arxiv.org/abs/2509.11442v1",
        "abstract": "Missing input sequences are common in medical imaging data, posing a\nchallenge for deep learning models reliant on complete input data. In this\nwork, inspired by MultiMAE [2], we develop a masked autoencoder (MAE) paradigm\nfor multi-modal, multi-task learning in 3D medical imaging with brain MRIs. Our\nmethod treats each MRI sequence as a separate input modality, leveraging a\nlate-fusion-style transformer encoder to integrate multi-sequence information\n(multi-modal) and individual decoder streams for each modality for multi-task\nreconstruction. This pretraining strategy guides the model to learn rich\nrepresentations per modality while also equipping it to handle missing inputs\nthrough cross-sequence reasoning. The result is a flexible and generalizable\nencoder for brain MRIs that infers missing sequences from available inputs and\ncan be adapted to various downstream applications. We demonstrate the\nperformance and robustness of our method against an MAE-ViT baseline in\ndownstream segmentation and classification tasks, showing absolute improvement\nof $10.1$ overall Dice score and $0.46$ MCC over the baselines with missing\ninput sequences. Our experiments demonstrate the strength of this pretraining\nstrategy. The implementation is made available."
    },
    {
        "date": "2025-09",
        "title": "Securing AI Agents: Implementing Role-Based Access Control for Industrial Applications",
        "author": "Aadil Gani Ganie",
        "link": "http://arxiv.org/abs/2509.11431v1",
        "abstract": "The emergence of Large Language Models (LLMs) has significantly advanced\nsolutions across various domains, from political science to software\ndevelopment. However, these models are constrained by their training data,\nwhich is static and limited to information available up to a specific date.\nAdditionally, their generalized nature often necessitates fine-tuning --\nwhether for classification or instructional purposes -- to effectively perform\nspecific downstream tasks. AI agents, leveraging LLMs as their core, mitigate\nsome of these limitations by accessing external tools and real-time data,\nenabling applications such as live weather reporting and data analysis. In\nindustrial settings, AI agents are transforming operations by enhancing\ndecision-making, predictive maintenance, and process optimization. For example,\nin manufacturing, AI agents enable near-autonomous systems that boost\nproductivity and support real-time decision-making. Despite these advancements,\nAI agents remain vulnerable to security threats, including prompt injection\nattacks, which pose significant risks to their integrity and reliability. To\naddress these challenges, this paper proposes a framework for integrating\nRole-Based Access Control (RBAC) into AI agents, providing a robust security\nguardrail. This framework aims to support the effective and scalable deployment\nof AI agents, with a focus on on-premises implementations."
    },
    {
        "date": "2025-09",
        "title": "Pulse-to-Circuit Characterization of Stealthy Crosstalk Attack on Multi-Tenant Superconducting Quantum Hardware",
        "author": "Syed Emad Uddin Shubha, and Tasnuva Farheen",
        "link": "http://arxiv.org/abs/2509.11407v1",
        "abstract": "Hardware crosstalk in multi-tenant superconducting quantum computers\nconstitutes a significant security threat, enabling adversaries to inject\ntargeted errors across tenant boundaries. We present the first end-to-end\nframework for mapping physical pulse-level attacks to interpretable logical\nerror channels, integrating density-matrix simulation, quantum process\ntomography (QPT), and a novel isometry-based circuit extraction method. Our\npipeline reconstructs the complete induced error channel and fits an effective\nlogical circuit model, revealing a fundamentally asymmetric attack mechanism:\none adversarial qubit acts as a driver to set the induced logical rotation,\nwhile a second, the catalyst, refines the attack's coherence. Demonstrated on a\nlinear three-qubit system, our approach shows that such attacks can\nsignificantly disrupt diverse quantum protocols, sometimes reducing accuracy to\nrandom guessing, while remaining effective and stealthy even under realistic\nhardware parameter variations. We further propose a protocol-level detection\nstrategy based on observable attack signatures, showing that stealthy attacks\ncan be exposed through targeted monitoring and providing a foundation for\nfuture defense-in-depth in quantum cloud platforms."
    },
    {
        "date": "2025-09",
        "title": "Some Robustness Properties of Label Cleaning",
        "author": "Chen Cheng, and John Duchi",
        "link": "http://arxiv.org/abs/2509.11379v1",
        "abstract": "We demonstrate that learning procedures that rely on aggregated labels, e.g.,\nlabel information distilled from noisy responses, enjoy robustness properties\nimpossible without data cleaning. This robustness appears in several ways. In\nthe context of risk consistency -- when one takes the standard approach in\nmachine learning of minimizing a surrogate (typically convex) loss in place of\na desired task loss (such as the zero-one mis-classification error) --\nprocedures using label aggregation obtain stronger consistency guarantees than\nthose even possible using raw labels. And while classical statistical scenarios\nof fitting perfectly-specified models suggest that incorporating all possible\ninformation -- modeling uncertainty in labels -- is statistically efficient,\nconsistency fails for ``standard'' approaches as soon as a loss to be minimized\nis even slightly mis-specified. Yet procedures leveraging aggregated\ninformation still converge to optimal classifiers, highlighting how\nincorporating a fuller view of the data analysis pipeline, from collection to\nmodel-fitting to prediction time, can yield a more robust methodology by\nrefining noisy signals."
    },
    {
        "date": "2025-09",
        "title": "Promoting Shape Bias in CNNs: Frequency-Based and Contrastive Regularization for Corruption Robustness",
        "author": "Robin Narsingh Ranabhat, Longwei Wang, Amit Kumar Patel, and KC santosh",
        "link": "http://arxiv.org/abs/2509.11355v1",
        "abstract": "Convolutional Neural Networks (CNNs) excel at image classification but remain\nvulnerable to common corruptions that humans handle with ease. A key reason for\nthis fragility is their reliance on local texture cues rather than global\nobject shapes -- a stark contrast to human perception. To address this, we\npropose two complementary regularization strategies designed to encourage\nshape-biased representations and enhance robustness. The first introduces an\nauxiliary loss that enforces feature consistency between original and\nlow-frequency filtered inputs, discouraging dependence on high-frequency\ntextures. The second incorporates supervised contrastive learning to structure\nthe feature space around class-consistent, shape-relevant representations.\nEvaluated on the CIFAR-10-C benchmark, both methods improve corruption\nrobustness without degrading clean accuracy. Our results suggest that\nloss-level regularization can effectively steer CNNs toward more shape-aware,\nresilient representations."
    },
    {
        "date": "2025-09",
        "title": "On the Escaping Efficiency of Distributed Adversarial Training Algorithms",
        "author": "Ying Cao, Kun Yuan, and Ali H. Sayed",
        "link": "http://arxiv.org/abs/2509.11337v1",
        "abstract": "Adversarial training has been widely studied in recent years due to its role\nin improving model robustness against adversarial attacks. This paper focuses\non comparing different distributed adversarial training algorithms--including\ncentralized and decentralized strategies--within multi-agent learning\nenvironments. Previous studies have highlighted the importance of model\nflatness in determining robustness. To this end, we develop a general\ntheoretical framework to study the escaping efficiency of these algorithms from\nlocal minima, which is closely related to the flatness of the resulting models.\nWe show that when the perturbation bound is sufficiently small (i.e., when the\nattack strength is relatively mild) and a large batch size is used,\ndecentralized adversarial training algorithms--including consensus and\ndiffusion--are guaranteed to escape faster from local minima than the\ncentralized strategy, thereby favoring flatter minima. However, as the\nperturbation bound increases, this trend may no longer hold. In the simulation\nresults, we illustrate our theoretical findings and systematically compare the\nperformance of models obtained through decentralized and centralized\nadversarial training algorithms. The results highlight the potential of\ndecentralized strategies to enhance the robustness of models in distributed\nsettings."
    },
    {
        "date": "2025-09",
        "title": "SelectMix: Enhancing Label Noise Robustness through Targeted Sample Mixing",
        "author": "Qiuhao Liu, Ling Li, Yao Lu, Qi Xuan, Zhaowei Zhu, and Jiaheng Wei",
        "link": "http://arxiv.org/abs/2509.11265v1",
        "abstract": "Deep neural networks tend to memorize noisy labels, severely degrading their\ngeneralization performance. Although Mixup has demonstrated effectiveness in\nimproving generalization and robustness, existing Mixup-based methods typically\nperform indiscriminate mixing without principled guidance on sample selection\nand mixing strategy, inadvertently propagating noisy supervision. To overcome\nthese limitations, we propose SelectMix, a confidence-guided mixing framework\nexplicitly tailored for noisy labels. SelectMix first identifies potentially\nnoisy or ambiguous samples through confidence based mismatch analysis using\nK-fold cross-validation, then selectively blends identified uncertain samples\nwith confidently predicted peers from their potential classes. Furthermore,\nSelectMix employs soft labels derived from all classes involved in the mixing\nprocess, ensuring the labels accurately represent the composition of the mixed\nsamples, thus aligning supervision signals closely with the actual mixed\ninputs. Through extensive theoretical analysis and empirical evaluations on\nmultiple synthetic (MNIST, Fashion-MNIST, CIFAR-10, CIFAR-100) and real-world\nbenchmark datasets (CIFAR-N, MNIST and Clothing1M), we demonstrate that\nSelectMix consistently outperforms strong baseline methods, validating its\neffectiveness and robustness in learning with noisy labels."
    },
    {
        "date": "2025-09",
        "title": "Realistic Environmental Injection Attacks on GUI Agents",
        "author": "Yitong Zhang, Ximo Li, Liyi Cai, and Jia Li",
        "link": "http://arxiv.org/abs/2509.11250v1",
        "abstract": "GUI agents built on LVLMs are increasingly used to interact with websites.\nHowever, their exposure to open-world content makes them vulnerable to\nEnvironmental Injection Attacks (EIAs) that hijack agent behavior via webpage\nelements. Many recent studies assume the attacker to be a regular user who can\nonly upload a single trigger image, which is more realistic than earlier\nassumptions of website-level administrative control. However, these works still\nfall short of realism: (1) the trigger's position and surrounding context\nremain largely fixed between training and testing, failing to capture the\ndynamic nature of real webpages and (2) the trigger often occupies an\nunrealistically large area, whereas real-world images are typically small. To\nbetter reflect real-world scenarios, we introduce a more realistic threat model\nwhere the attacker is a regular user and the trigger image is small and\nembedded within a dynamically changing environment. As a result, existing\nattacks prove largely ineffective under this threat model.\n  To better expose the vulnerabilities of GUI agents, we propose Chameleon, an\nattack framework with two main novelties. The first is LLM-Driven Environment\nSimulation, which automatically generates diverse and high-fidelity webpage\nsimulations. The second is Attention Black Hole, which transforms attention\nweights into explicit supervisory signals that guide the agent's focus toward\nthe trigger region. We evaluate Chameleon on 6 realistic websites and 4\nrepresentative LVLM-powered GUI agents, where it significantly outperforms\nexisting methods. Ablation studies confirm that both novelties are critical to\nperformance. Our findings reveal underexplored vulnerabilities in modern GUI\nagents and establish a robust foundation for future research on defense in\nopen-world GUI agent systems. The code is publicly available at\nhttps://github.com/zhangyitonggg/attack2gui."
    },
    {
        "date": "2025-09",
        "title": "Exploring and Exploiting the Resource Isolation Attack Surface of WebAssembly Containers",
        "author": "Zhaofeng Yu, Dongyang Zhan, Lin Ye, Haining Yu, Hongli Zhang, and Zhihong Tian",
        "link": "http://arxiv.org/abs/2509.11242v1",
        "abstract": "Recently, the WebAssembly (or Wasm) technology has been rapidly evolving,\nwith many runtimes actively under development, providing cross-platform secure\nsandboxes for Wasm modules to run as portable containers. Compared with Docker,\nwhich isolates applications at the operating system level, Wasm runtimes\nprovide more security mechanisms, such as linear memory, type checking, and\nprotected call stacks. Although Wasm is designed with security in mind and\nconsidered to be a more secure container runtime, various security challenges\nhave arisen, and researchers have focused on the security of Wasm runtimes,\nsuch as discovering vulnerabilities or proposing new security mechanisms to\nachieve robust isolation. However, we have observed that the resource isolation\nis not well protected by the current Wasm runtimes, and attackers can exhaust\nthe host's resources to interfere with the execution of other container\ninstances by exploiting the WASI/WASIX interfaces. And the attack surface has\nnot been well explored and measured. In this paper, we explore the resource\nisolation attack surface of Wasm runtimes systematically by proposing several\nstatic Wasm runtime analysis approaches. Based on the analysis results, we\npropose several exploitation strategies to break the resource isolation of Wasm\nruntimes. The experimental results show that malicious Wasm instances can not\nonly consume large amounts of system resources on their own but also introduce\nhigh workloads into other components of the underlying operating system,\nleading to a substantial performance degradation of the whole system. In\naddition, the mitigation approaches have also been discussed."
    },
    {
        "date": "2025-09",
        "title": "ANROT-HELANet: Adverserially and Naturally Robust Attention-Based Aggregation Network via The Hellinger Distance for Few-Shot Classification",
        "author": "Gao Yu Lee, Tanmoy Dam, Md Meftahul Ferdaus, Daniel Puiu Poenar, and Vu N. Duong",
        "link": "http://arxiv.org/abs/2509.11220v1",
        "abstract": "Few-Shot Learning (FSL), which involves learning to generalize using only a\nfew data samples, has demonstrated promising and superior performances to\nordinary CNN methods. While Bayesian based estimation approaches using\nKullback-Leibler (KL) divergence have shown improvements, they remain\nvulnerable to adversarial attacks and natural noises. We introduce\nANROT-HELANet, an Adversarially and Naturally RObusT Hellinger Aggregation\nNetwork that significantly advances the state-of-the-art in FSL robustness and\nperformance. Our approach implements an adversarially and naturally robust\nHellinger distance-based feature class aggregation scheme, demonstrating\nresilience to adversarial perturbations up to $\\epsilon=0.30$ and Gaussian\nnoise up to $\\sigma=0.30$. The network achieves substantial improvements across\nbenchmark datasets, including gains of 1.20\\% and 1.40\\% for 1-shot and 5-shot\nscenarios on miniImageNet respectively. We introduce a novel Hellinger\nSimilarity contrastive loss function that generalizes cosine similarity\ncontrastive loss for variational few-shot inference scenarios. Our approach\nalso achieves superior image reconstruction quality with a FID score of 2.75,\noutperforming traditional VAE (3.43) and WAE (3.38) approaches. Extensive\nexperiments conducted on four few-shot benchmarked datasets verify that\nANROT-HELANet's combination of Hellinger distance-based feature aggregation,\nattention mechanisms, and our novel loss function establishes new\nstate-of-the-art performance while maintaining robustness against both\nadversarial and natural perturbations. Our code repository will be available at\nhttps://github.com/GreedYLearner1146/ANROT-HELANet/tree/main."
    },
    {
        "date": "2025-09",
        "title": "DMLDroid: Deep Multimodal Fusion Framework for Android Malware Detection with Resilience to Code Obfuscation and Adversarial Perturbations",
        "author": "Doan Minh Trung, Tien Duc Anh Hao, Luong Hoang Minh, Nghi Hoang Khoa, Nguyen Tan Cam, Van-Hau Pham, and Phan The Duy",
        "link": "http://arxiv.org/abs/2509.11187v1",
        "abstract": "In recent years, learning-based Android malware detection has seen\nsignificant advancements, with detectors generally falling into three\ncategories: string-based, image-based, and graph-based approaches. While these\nmethods have shown strong detection performance, they often struggle to sustain\nrobustness in real-world settings, particularly when facing code obfuscation\nand adversarial examples (AEs). Deep multimodal learning has emerged as a\npromising solution, leveraging the strengths of multiple feature types to\nenhance robustness and generalization. However, a systematic investigation of\nmultimodal fusion for both accuracy and resilience remains underexplored. In\nthis study, we propose DMLDroid, an Android malware detection based on\nmultimodal fusion that leverages three different representations of malware\nfeatures, including permissions & intents (tabular-based), DEX file\nrepresentations (image-based), and API calls (graph-derived sequence-based). We\nconduct exhaustive experiments independently on each feature, as well as in\ncombination, using different fusion strategies. Experimental results on the\nCICMalDroid 2020 dataset demonstrate that our multimodal approach with the\ndynamic weighted fusion mechanism achieves high performance, reaching 97.98%\naccuracy and 98.67% F1-score on original malware detection. Notably, the\nproposed method maintains strong robustness, sustaining over 98% accuracy and\n98% F1-score under both obfuscation and adversarial attack scenarios. Our\nfindings highlight the benefits of multimodal fusion in improving both\ndetection accuracy and robustness against evolving Android malware threats."
    },
    {
        "date": "2025-09",
        "title": "UDFS: Lightweight Representation-Driven Robust Network Traffic Classification",
        "author": "Youquan Xian, Xueying Zeng, Mei Huang, Aoxiang Zhou, Xiaoyu Cui, Peng Liu, and Lei Cui",
        "link": "http://arxiv.org/abs/2509.11157v1",
        "abstract": "In recent years, sequence features such as packet length have received\nconsiderable attention due to their central role in encrypted traffic analysis.\nExisting sequence modeling approaches can be broadly categorized into\nflow-level and trace-level methods: the former suffer from high feature\nredundancy, limiting their discriminative power, whereas the latter preserve\ncomplete information but incur substantial computational and storage overhead.\nTo address these limitations, we propose the \\textbf{U}p-\\textbf{D}own\n\\textbf{F}low \\textbf{S}equence (\\textbf{UDFS}) representation, which\ncompresses an entire trace into a two-dimensional sequence and characterizes\neach flow by the aggregate of its upstream and downstream traffic, reducing\ncomplexity while maintaining high discriminability. Furthermore, to address the\nchallenge of class-specific discriminability differences, we propose an\nadaptive threshold mechanism that dynamically adjusts training weights and\nrejection boundaries, enhancing the model's classification performance.\nExperimental results demonstrate that the proposed method achieves superior\nclassification performance and robustness on both coarse-grained and\nfine-grained datasets, as well as under concept drift and open-world scenarios.\nCode and Dataset are available at https://github.com/kid1999/UDFS."
    },
    {
        "date": "2025-09",
        "title": "RoVerFly: Robust and Versatile Learning-based Control of Quadrotor Across Payload Configurations",
        "author": "Mintae Kim, Jiaze Cai, and Koushil Sreenath",
        "link": "http://arxiv.org/abs/2509.11149v1",
        "abstract": "Designing robust controllers for precise, arbitrary trajectory tracking with\nquadrotors is challenging due to nonlinear dynamics and underactuation, and\nbecomes harder with flexible cable-suspended payloads that introduce extra\ndegrees of freedom and hybridness. Classical model-based methods offer\nstability guarantees but require extensive tuning and often do not adapt when\nthe configuration changes, such as when a payload is added or removed, or when\nthe payload mass or cable length varies. We present RoVerFly, a unified\nlearning-based control framework in which a reinforcement learning (RL) policy\nserves as a robust and versatile tracking controller for standard quadrotors\nand for cable-suspended payload systems across a range of configurations.\nTrained with task and domain randomization, the controller is resilient to\ndisturbances and varying dynamics. It achieves strong zero-shot generalization\nacross payload settings, including no payload as well as varying mass and cable\nlength, without controller switching or re-tuning, while retaining the\ninterpretability and structure of a feedback tracking controller. Code and\nsupplementary materials are available at\nhttps://github.com/mintaeshkim/roverfly"
    },
    {
        "date": "2025-09",
        "title": "SoK: How Sensor Attacks Disrupt Autonomous Vehicles: An End-to-end Analysis, Challenges, and Missed Threats",
        "author": "Qingzhao Zhang, Shaocheng Luo, Z. Morley Mao, Miroslav Pajic, and Michael K. Reiter",
        "link": "http://arxiv.org/abs/2509.11120v3",
        "abstract": "Autonomous vehicles, including self-driving cars, robotic ground vehicles,\nand drones, rely on complex sensor pipelines to ensure safe and reliable\noperation. However, these safety-critical systems remain vulnerable to\nadversarial sensor attacks that can compromise their performance and mission\nsuccess. While extensive research has demonstrated various sensor attack\ntechniques, critical gaps remain in understanding their feasibility in\nreal-world, end-to-end systems. This gap largely stems from the lack of a\nsystematic perspective on how sensor errors propagate through interconnected\nmodules in autonomous systems when autonomous vehicles interact with the\nphysical world.\n  To bridge this gap, we present a comprehensive survey of autonomous vehicle\nsensor attacks across platforms, sensor modalities, and attack methods. Central\nto our analysis is the System Error Propagation Graph (SEPG), a structured\ndemonstration tool that illustrates how sensor attacks propagate through system\npipelines, exposing the conditions and dependencies that determine attack\nfeasibility. With the aid of SEPG, our study distills seven key findings that\nhighlight the feasibility challenges of sensor attacks and uncovers eleven\npreviously overlooked attack vectors exploiting inter-module interactions,\nseveral of which we validate through proof-of-concept experiments.\nAdditionally, we demonstrate how large language models (LLMs) can automate\naspects of SEPG construction and cross-validate expert analysis, showcasing the\npromise of AI-assisted security evaluation."
    },
    {
        "date": "2025-09",
        "title": "Membership Inference Attacks on Recommender System: A Survey",
        "author": "Jiajie He, Yuechun Gu, Keke Chen, and Xintong Chen",
        "link": "http://arxiv.org/abs/2509.11080v1",
        "abstract": "Recommender systems (RecSys) have been widely applied to various\napplications, including E-commerce, finance, healthcare, social media and have\nbecome increasingly influential in shaping user behavior and decision-making,\nhighlighting their growing impact in various domains. However, recent studies\nhave shown that RecSys are vulnerable to membership inference attacks (MIAs),\nwhich aim to infer whether user interaction record was used to train a target\nmodel or not. MIAs on RecSys models can directly lead to a privacy breach. For\nexample, via identifying the fact that a purchase record that has been used to\ntrain a RecSys associated with a specific user, an attacker can infer that\nuser's special quirks. In recent years, MIAs have been shown to be effective on\nother ML tasks, e.g., classification models and natural language processing.\nHowever, traditional MIAs are ill-suited for RecSys due to the unseen posterior\nprobability. Although MIAs on RecSys form a newly emerging and rapidly growing\nresearch area, there has been no systematic survey on this topic yet. In this\narticle, we conduct the first comprehensive survey on RecSys MIAs. This survey\noffers a comprehensive review of the latest advancements in RecSys MIAs,\nexploring the design principles, challenges, attack and defense associated with\nthis emerging field. We provide a unified taxonomy that categorizes different\nRecSys MIAs based on their characterizations and discuss their pros and cons.\nBased on the limitations and gaps identified in this survey, we point out\nseveral promising future research directions to inspire the researchers who\nwish to follow this area. This survey not only serves as a reference for the\nresearch community but also provides a clear description for researchers\noutside this research domain."
    },
    {
        "date": "2025-09",
        "title": "Large Language Models for Security Operations Centers: A Comprehensive Survey",
        "author": "Ali Habibzadeh, Farid Feyzi, and Reza Ebrahimi Atani",
        "link": "http://arxiv.org/abs/2509.10858v2",
        "abstract": "Large Language Models (LLMs) have emerged as powerful tools capable of\nunderstanding and generating human-like text, offering transformative potential\nacross diverse domains. The Security Operations Center (SOC), responsible for\nsafeguarding digital infrastructure, represents one of these domains. SOCs\nserve as the frontline of defense in cybersecurity, tasked with continuous\nmonitoring, detection, and response to incidents. However, SOCs face persistent\nchallenges such as high alert volumes, limited resources, high demand for\nexperts with advanced knowledge, delayed response times, and difficulties in\nleveraging threat intelligence effectively. In this context, LLMs can offer\npromising solutions by automating log analysis, streamlining triage, improving\ndetection accuracy, and providing the required knowledge in less time. This\nsurvey systematically explores the integration of generative AI and more\nspecifically LLMs into SOC workflow, providing a structured perspective on its\ncapabilities, challenges, and future directions. We believe that this survey\noffers researchers and SOC managers a broad overview of the current state of\nLLM integration within academic study. To the best of our knowledge, this is\nthe first comprehensive study to examine LLM applications in SOCs in details."
    },
    {
        "date": "2025-09",
        "title": "ORQ: Complex Analytics on Private Data with Strong Security Guarantees",
        "author": "Eli Baum, Sam Buxbaum, Nitin Mathai, Muhammad Faisal, Vasiliki Kalavri, Mayank Varia, and John Liagouris",
        "link": "http://arxiv.org/abs/2509.10793v1",
        "abstract": "We present ORQ, a system that enables collaborative analysis of large private\ndatasets using cryptographically secure multi-party computation (MPC). ORQ\nprotects data against semi-honest or malicious parties and can efficiently\nevaluate relational queries with multi-way joins and aggregations that have\nbeen considered notoriously expensive under MPC. To do so, ORQ eliminates the\nquadratic cost of secure joins by leveraging the fact that, in practice, the\nstructure of many real queries allows us to join records and apply the\naggregations \"on the fly\" while keeping the result size bounded. On the system\nside, ORQ contributes generic oblivious operators, a data-parallel vectorized\nquery engine, a communication layer that amortizes MPC network costs, and a\ndataflow API for expressing relational analytics -- all built from the ground\nup.\n  We evaluate ORQ in LAN and WAN deployments on a diverse set of workloads,\nincluding complex queries with multiple joins and custom aggregations. When\ncompared to state-of-the-art solutions, ORQ significantly reduces MPC execution\ntimes and can process one order of magnitude larger datasets. For our most\nchallenging workload, the full TPC-H benchmark, we report results entirely\nunder MPC with Scale Factor 10 -- a scale that had previously been achieved\nonly with information leakage or the use of trusted third parties."
    },
    {
        "date": "2025-09",
        "title": "GoldenTransformer: A Modular Fault Injection Framework for Transformer Robustness Research",
        "author": "Luke Howard",
        "link": "http://arxiv.org/abs/2509.10790v1",
        "abstract": "Transformers have become the foundation for a wide range of\nstate--of--the--art models across natural language processing, computer vision,\nand other machine learning domains. Despite their widespread deployment, the\nrobustness of these models under fault conditions remains underexplored. We\npresent GoldenTransformer, a modular and extensible fault injection framework\ndesigned to evaluate the resiliency of Large Language Models to induced\nhardware faults. GoldenTransformer offers a unified Python-based platform for\ninjecting diverse classes of faults--such as weight corruption, activation\ninjections, and attention--level disruptions--into pretrained\ntransformer--based models. Inspired by the GoldenEye simulator for DNNs, our\nframework focuses on the unique challenges of working with large transformer\narchitectures, including considerations such as structural complexity, latent\ndependencies, and nonuniform layer definitions. GoldenTransformer is built atop\nPyTorch and HuggingFace Transformers, and it supports experiment\nreproducibility, metric logging, and visualization out of the box. We detail\nthe technical design and use of GoldenTransformer and demonstrate through\nseveral example experiments on classification and generation tasks. By enabling\ncontrolled injection of faults at multiple logical and structural points in a\ntransformer, GoldenTransformer offers researchers and practitioners a valuable\ntool for model robustness analysis and for guiding dependable system design in\nreal-world LLM applications."
    },
    {
        "date": "2025-09",
        "title": "Five Minutes of DDoS Brings down Tor: DDoS Attacks on the Tor Directory Protocol and Mitigations",
        "author": "Zhongtang Luo, Jianting Zhang, Akshat Neerati, and Aniket Kate",
        "link": "http://arxiv.org/abs/2509.10755v1",
        "abstract": "The Tor network offers network anonymity to its users by routing their\ntraffic through a sequence of relays. A group of nine directory authorities\nmaintains information about all available relay nodes using a distributed\ndirectory protocol. We observe that the current protocol makes a steep\nsynchrony assumption, which makes it vulnerable to natural as well as\nadversarial non-synchronous communication scenarios over the Internet. In this\npaper, we show that it is possible to cause a failure in the Tor directory\nprotocol by targeting a majority of the authorities for only five minutes using\na well-executed distributed denial-of-service (DDoS) attack. We demonstrate\nthis attack in a controlled environment and show that it is cost-effective for\nas little as \\$53.28 per month to disrupt the protocol and to effectively bring\ndown the entire Tor network. To mitigate this problem, we consider the popular\npartial synchrony assumption for the Tor directory protocol that ensures that\nthe protocol security is hampered even when the network delays are large and\nunknown. We design a new Tor directory protocol that leverages any standard\npartial-synchronous consensus protocol to solve this problem, while also\nproving its security. We have implemented a prototype in Rust, demonstrating\ncomparable performance to the current protocol while resisting similar attacks."
    },
    {
        "date": "2025-09",
        "title": "Security theory for data flow and access control: From partial orders to lattices and back, a half-century trip",
        "author": "Luigi Logrippo",
        "link": "http://arxiv.org/abs/2509.10727v1",
        "abstract": "The multi level Bell La Padula model for secure data access and data flow\ncontrol, formulated in the 1970s, was based on the theory of partial orders.\nSince then, another model, based on lattice theory, has prevailed. We present\nreasons why the partial order model is more appropriate. We also show, by\nexample, how non lattice data flow networks can be easily implemented by using\nAttribute-based access control (ABAC)."
    },
    {
        "date": "2025-09",
        "title": "Safety and Security Analysis of Large Language Models: Risk Profile and Harm Potential",
        "author": "Charankumar Akiri, Harrison Simpson, Kshitiz Aryal, Aarav Khanna, and Maanak Gupta",
        "link": "http://arxiv.org/abs/2509.10655v1",
        "abstract": "While the widespread deployment of Large Language Models (LLMs) holds great\npotential for society, their vulnerabilities to adversarial manipulation and\nexploitation can pose serious safety, security, and ethical risks. As new\nthreats continue to emerge, it becomes critically necessary to assess the\nlandscape of LLMs' safety and security against evolving adversarial prompt\ntechniques. To understand the behavior of LLMs, this research provides an\nempirical analysis and risk profile of nine prominent LLMs, Claude Opus 4,\nDeepSeek V3 (both open-source and online), Gemini 2.5 Flash, GPT-4o, Grok 3,\nLlama 4 Scout, Mistral 7B, and Qwen 3 1.7B, against 24 different security and\nsafety categories. These LLMs are evaluated on their ability to produce harmful\nresponses for adversarially crafted prompts (dataset has been made public) for\na broad range of safety and security topics, such as promotion of violent\ncriminal behavior, promotion of non-violent criminal activity, societal harms\nrelated to safety, illegal sexual content, dangerous code generation, and\ncybersecurity threats beyond code. Our study introduces the Risk Severity Index\n(RSI), an agile and scalable evaluation score, to quantify and compare the\nsecurity posture and creating a risk profile of LLMs. As the LLM development\nlandscape progresses, the RSI is intended to be a valuable metric for comparing\nthe risks of LLMs across evolving threats. This research finds widespread\nvulnerabilities in the safety filters of the LLMs tested and highlights the\nurgent need for stronger alignment, responsible deployment practices, and model\ngovernance, particularly for open-access and rapidly iterated models."
    },
    {
        "date": "2025-09",
        "title": "Immunizing Images from Text to Image Editing via Adversarial Cross-Attention",
        "author": "Matteo Trippodo, Federico Becattini, and Lorenzo Seidenari",
        "link": "http://arxiv.org/abs/2509.10359v1",
        "abstract": "Recent advances in text-based image editing have enabled fine-grained\nmanipulation of visual content guided by natural language. However, such\nmethods are susceptible to adversarial attacks. In this work, we propose a\nnovel attack that targets the visual component of editing methods. We introduce\nAttention Attack, which disrupts the cross-attention between a textual prompt\nand the visual representation of the image by using an automatically generated\ncaption of the source image as a proxy for the edit prompt. This breaks the\nalignment between the contents of the image and their textual description,\nwithout requiring knowledge of the editing method or the editing prompt.\nReflecting on the reliability of existing metrics for immunization success, we\npropose two novel evaluation strategies: Caption Similarity, which quantifies\nsemantic consistency between original and adversarial edits, and semantic\nIntersection over Union (IoU), which measures spatial layout disruption via\nsegmentation masks. Experiments conducted on the TEDBench++ benchmark\ndemonstrate that our attack significantly degrades editing performance while\nremaining imperceptible."
    },
    {
        "date": "2025-09",
        "title": "SME-TEAM: Leveraging Trust and Ethics for Secure and Responsible Use of AI and LLMs in SMEs",
        "author": "Iqbal H. Sarker, Helge Janicke, Ahmad Mohsin, and Leandros Maglaras",
        "link": "http://arxiv.org/abs/2509.10594v1",
        "abstract": "Artificial Intelligence (AI) and Large Language Models (LLMs) are reshaping\ntoday's business practices, however, their adoption within small and\nmedium-sized enterprises (SMEs) raises significant technical, ethical and trust\nissues. This paper proposes a structured, multi-phased framework designed to\nembed trust and ethical principles throughout the AI lifecycle for their secure\nand responsible use in SMEs. Structured around four pillars, i.e., Data,\nAlgorithms, Human oversight, and Model Architecture, the framework bridges\ntheoretical ethical principles with operational practice, enhancing AI\ncapabilities in diverse SME applications. Ultimately, this paper offers a\nstructured roadmap for responsible AI adoption, framing trust and ethics as a\ncatalyst for resilience, competitiveness, and sustainable innovation in SMEs."
    },
    {
        "date": "2025-09",
        "title": "Innovating Augmented Reality Security: Recent E2E Encryption Approaches",
        "author": "Hamish Alsop, Leandros Maglaras, Helge Janicke, Iqbal H. Sarker, and Mohamed Amine Ferrag",
        "link": "http://arxiv.org/abs/2509.10313v1",
        "abstract": "End-to-end encryption (E2EE) has emerged as a fundamental element of modern\ndigital communication, protecting data from unauthorized access during\ntransmission. By design, E2EE ensures that only the intended recipient can\ndecrypt the information, making it inaccessible even to service providers. Yet,\nthis powerful safeguard of individual privacy and digital trust also introduces\na paradox: it can simultaneously prevent law enforcement efforts by hiding\npotential malicious activities. This paper examines the dual role of E2EE, its\ncritical importance to privacy, the challenges it"
    },
    {
        "date": "2025-09",
        "title": "Adversarial robustness through Lipschitz-Guided Stochastic Depth in Neural Networks",
        "author": "Laith Nayal, Mahmoud Mousatat, and Bader Rasheed",
        "link": "http://arxiv.org/abs/2509.10298v1",
        "abstract": "Deep neural networks and Vision Transformers achieve state-of-the-art\nperformance in computer vision but are highly vulnerable to adversarial\nperturbations. Standard defenses often incur high computational cost or lack\nformal guarantees. We propose a Lipschitz-guided stochastic depth (DropPath)\nmethod, where drop probabilities increase with depth to control the effective\nLipschitz constant of the network. This approach regularizes deeper layers,\nimproving robustness while preserving clean accuracy and reducing computation.\nExperiments on CIFAR-10 with ViT-Tiny show that our custom depth-dependent\nschedule maintains near-baseline clean accuracy, enhances robustness under\nFGSM, PGD-20, and AutoAttack, and significantly reduces FLOPs compared to\nbaseline and linear DropPath schedules."
    },
    {
        "date": "2025-09",
        "title": "Proof of AutoML: SDN based Secure Energy Trading with Blockchain in Disaster Case",
        "author": "Salih Toprak, and Muge Erel-Ozcevik",
        "link": "http://arxiv.org/abs/2509.10291v1",
        "abstract": "In disaster scenarios where conventional energy infrastructure is\ncompromised, secure and traceable energy trading between solar-powered\nhouseholds and mobile charging units becomes a necessity. To ensure the\nintegrity of such transactions over a blockchain network, robust and\nunpredictable nonce generation is vital. This study proposes an SDN-enabled\narchitecture where machine learning regressors are leveraged not for their\naccuracy, but for their potential to generate randomized values suitable as\nnonce candidates. Therefore, it is newly called Proof of AutoML. Here, SDN\nallows flexible control over data flows and energy routing policies even in\nfragmented or degraded networks, ensuring adaptive response during emergencies.\nUsing a 9000-sample dataset, we evaluate five AutoML-selected regression models\n- Gradient Boosting, LightGBM, Random Forest, Extra Trees, and K-Nearest\nNeighbors - not by their prediction accuracy, but by their ability to produce\ndiverse and non-deterministic outputs across shuffled data inputs. Randomness\nanalysis reveals that Random Forest and Extra Trees regressors exhibit complete\ndependency on randomness, whereas Gradient Boosting, K-Nearest Neighbors and\nLightGBM show strong but slightly lower randomness scores (97.6%, 98.8% and\n99.9%, respectively). These findings highlight that certain machine learning\nmodels, particularly tree-based ensembles, may serve as effective and\nlightweight nonce generators within blockchain-secured, SDN-based energy\ntrading infrastructures resilient to disaster conditions."
    },
    {
        "date": "2025-09",
        "title": "Robustness and Diagnostic Performance of Super-Resolution Fetal Brain MRI",
        "author": "Ema Masterl, Tina Vipotnik Vesnaver, and \u017diga \u0160piclin",
        "link": "http://arxiv.org/abs/2509.10257v1",
        "abstract": "Fetal brain MRI relies on rapid multi-view 2D slice acquisitions to reduce\nmotion artifacts caused by fetal movement. However, these stacks are typically\nlow resolution, may suffer from motion corruption, and do not adequately\ncapture 3D anatomy. Super-resolution reconstruction (SRR) methods aim to\naddress these limitations by combining slice-to-volume registration and\nsuper-resolution techniques to generate high-resolution (HR) 3D volumes. While\nseveral SRR methods have been proposed, their comparative performance -\nparticularly in pathological cases - and their influence on downstream\nvolumetric analysis and diagnostic tasks remain underexplored. In this study,\nwe applied three state-of-the-art SRR method - NiftyMIC, SVRTK, and NeSVoR - to\n140 fetal brain MRI scans, including both healthy controls (HC) and\npathological cases (PC) with ventriculomegaly (VM). Each HR reconstruction was\nsegmented using the BoUNTi algorithm to extract volumes of nine principal brain\nstructures. We evaluated visual quality, SRR success rates, volumetric\nmeasurement agreement, and diagnostic classification performance. NeSVoR\ndemonstrated the highest and most consistent reconstruction success rate (>90%)\nacross both HC and PC groups. Although significant differences in volumetric\nestimates were observed between SRR methods, classification performance for VM\nwas not affected by the choice of SRR method. These findings highlight NeSVoR's\nrobustness and the resilience of diagnostic performance despite SRR-induced\nvolumetric variability."
    },
    {
        "date": "2025-09",
        "title": "Prompt Injection Attacks on LLM Generated Reviews of Scientific Publications",
        "author": "Janis Keuper",
        "link": "http://arxiv.org/abs/2509.10248v2",
        "abstract": "The ongoing intense discussion on rising LLM usage in the scientific\npeer-review process has recently been mingled by reports of authors using\nhidden prompt injections to manipulate review scores. Since the existence of\nsuch \"attacks\" - although seen by some commentators as \"self-defense\" - would\nhave a great impact on the further debate, this paper investigates the\npracticability and technical success of the described manipulations. Our\nsystematic evaluation uses 1k reviews of 2024 ICLR papers generated by a wide\nrange of LLMs shows two distinct results: I) very simple prompt injections are\nindeed highly effective, reaching up to 100% acceptance scores. II) LLM reviews\nare generally biased toward acceptance (>95% in many models). Both results have\ngreat impact on the ongoing discussions on LLM usage in peer-review."
    },
    {
        "date": "2025-09",
        "title": "Online Robust Planning under Model Uncertainty: A Sample-Based Approach",
        "author": "Tamir Shazman, Idan Lev-Yehudi, Ron Benchetit, and Vadim Indelman",
        "link": "http://arxiv.org/abs/2509.10162v2",
        "abstract": "Online planning in Markov Decision Processes (MDPs) enables agents to make\nsequential decisions by simulating future trajectories from the current state,\nmaking it well-suited for large-scale or dynamic environments. Sample-based\nmethods such as Sparse Sampling and Monte Carlo Tree Search (MCTS) are widely\nadopted for their ability to approximate optimal actions using a generative\nmodel. However, in practical settings, the generative model is often learned\nfrom limited data, introducing approximation errors that can degrade\nperformance or lead to unsafe behaviors. To address these challenges, Robust\nMDPs (RMDPs) offer a principled framework for planning under model uncertainty,\nyet existing approaches are typically computationally intensive and not suited\nfor real-time use. In this work, we introduce Robust Sparse Sampling (RSS), the\nfirst online planning algorithm for RMDPs with finite-sample theoretical\nperformance guarantees. Unlike Sparse Sampling, which estimates the nominal\nvalue function, RSS computes a robust value function by leveraging the\nefficiency and theoretical properties of Sample Average Approximation (SAA),\nenabling tractable robust policy computation in online settings. RSS is\napplicable to infinite or continuous state spaces, and its sample and\ncomputational complexities are independent of the state space size. We provide\ntheoretical performance guarantees and empirically show that RSS outperforms\nstandard Sparse Sampling in environments with uncertain dynamics."
    },
    {
        "date": "2025-09",
        "title": "Securing LLM-Generated Embedded Firmware through AI Agent-Driven Validation and Patching",
        "author": "Seyed Moein Abtahi, and Akramul Azim",
        "link": "http://arxiv.org/abs/2509.09970v1",
        "abstract": "Large Language Models (LLMs) show promise in generating firmware for embedded\nsystems, but often introduce security flaws and fail to meet real-time\nperformance constraints. This paper proposes a three-phase methodology that\ncombines LLM-based firmware generation with automated security validation and\niterative refinement in a virtualized environment. Using structured prompts,\nmodels like GPT-4 generate firmware for networking and control tasks, deployed\non FreeRTOS via QEMU. These implementations are tested using fuzzing, static\nanalysis, and runtime monitoring to detect vulnerabilities such as buffer\noverflows (CWE-120), race conditions (CWE-362), and denial-of-service threats\n(CWE-400). Specialized AI agents for Threat Detection, Performance\nOptimization, and Compliance Verification collaborate to improve detection and\nremediation. Identified issues are categorized using CWE, then used to prompt\ntargeted LLM-generated patches in an iterative loop. Experiments show a 92.4\\%\nVulnerability Remediation Rate (37.3\\% improvement), 95.8\\% Threat Model\nCompliance, and 0.87 Security Coverage Index. Real-time metrics include 8.6ms\nworst-case execution time and 195{\\mu}s jitter. This process enhances firmware\nsecurity and performance while contributing an open-source dataset for future\nresearch."
    },
    {
        "date": "2025-09",
        "title": "Online 3D Multi-Camera Perception through Robust 2D Tracking and Depth-based Late Aggregation",
        "author": "Vu-Minh Le, Thao-Anh Tran, Duc Huy Do, Xuan Canh Do, Huong Ninh, and Hai Tran",
        "link": "http://arxiv.org/abs/2509.09946v1",
        "abstract": "Multi-Target Multi-Camera Tracking (MTMC) is an essential computer vision\ntask for automating large-scale surveillance. With camera calibration and depth\ninformation, the targets in the scene can be projected into 3D space, offering\nunparalleled levels of automatic perception of a 3D environment. However,\ntracking in the 3D space requires replacing all 2D tracking components from the\nground up, which may be infeasible for existing MTMC systems. In this paper, we\npresent an approach for extending any online 2D multi-camera tracking system\ninto 3D space by utilizing depth information to reconstruct a target in\npoint-cloud space, and recovering its 3D box through clustering and yaw\nrefinement following tracking. We also introduced an enhanced online data\nassociation mechanism that leverages the target's local ID consistency to\nassign global IDs across frames. The proposed framework is evaluated on the\n2025 AI City Challenge's 3D MTMC dataset, achieving 3rd place on the\nleaderboard."
    },
    {
        "date": "2025-09",
        "title": "SmartCoder-R1: Towards Secure and Explainable Smart Contract Generation with Security-Aware Group Relative Policy Optimization",
        "author": "Lei Yu, Jingyuan Zhang, Xin Wang, Jiajia Ma, Li Yang, and Fengjun Zhang",
        "link": "http://arxiv.org/abs/2509.09942v1",
        "abstract": "Smart contracts automate the management of high-value assets, where\nvulnerabilities can lead to catastrophic financial losses. This challenge is\namplified in Large Language Models (LLMs) by two interconnected failures: they\noperate as unauditable \"black boxes\" lacking a transparent reasoning process,\nand consequently, generate code riddled with critical security vulnerabilities.\nTo address both issues, we propose SmartCoder-R1 (based on Qwen2.5-Coder-7B), a\nnovel framework for secure and explainable smart contract generation. It begins\nwith Continual Pre-training (CPT) to specialize the model. We then apply Long\nChain-of-Thought Supervised Fine-Tuning (L-CoT SFT) on 7,998 expert-validated\nreasoning-and-code samples to train the model to emulate human security\nanalysis. Finally, to directly mitigate vulnerabilities, we employ\nSecurity-Aware Group Relative Policy Optimization (S-GRPO), a reinforcement\nlearning phase that refines the generation policy by optimizing a weighted\nreward signal for compilation success, security compliance, and format\ncorrectness. Evaluated against 17 baselines on a benchmark of 756 real-world\nfunctions, SmartCoder-R1 establishes a new state of the art, achieving top\nperformance across five key metrics: a ComPass of 87.70%, a VulRate of 8.60%, a\nSafeAval of 80.16%, a FuncRate of 53.84%, and a FullRate of 50.53%. This\nFullRate marks a 45.79% relative improvement over the strongest baseline,\nDeepSeek-R1. Crucially, its generated reasoning also excels in human\nevaluations, achieving high-quality ratings for Functionality (82.7%), Security\n(85.3%), and Clarity (90.7%)."
    },
    {
        "date": "2025-09",
        "title": "NISQ Security and Complexity via Simple Classical Reasoning",
        "author": "Alexandru Cojocaru, Juan Garay, Qipeng Liu, and Fang Song",
        "link": "http://arxiv.org/abs/2509.09900v1",
        "abstract": "We give novel lifting theorems for security games in the quantum random\noracle model (QROM) in Noisy Intermediate-Scale Quantum (NISQ) settings such as\nthe hybrid query model, the noisy oracle and the bounded-depth models. We\nprovide, for the first time, a hybrid lifting theorem for hybrid algorithms\nthat can perform both quantum and classical queries, as well as a lifting\ntheorem for quantum algorithms with access to noisy oracles or bounded quantum\ndepth.\n  At the core of our results lies a novel measure-and-reprogram framework,\ncalled hybrid coherent measure-and-reprogramming, tailored specifically for\nhybrid algorithms. Equipped with the lifting theorem, we are able to prove\ndirectly NISQ security and complexity results by calculating a single\ncombinatorial quantity, relying solely on classical reasoning.\n  As applications, we derive the first direct product theorems in the average\ncase, in the hybrid setting-i.e., an enabling tool to determine the hybrid\nhardness of solving multi-instance security games. This allows us to derive in\na straightforward manner the NISQ hardness of various security games, such as\n(i) the non-uniform hardness of salted games, (ii) the hardness of specific\ncryptographic tasks such as the multiple instance version of one-wayness and\ncollision-resistance, and (iii) uniform or non-uniform hardness of many other\ngames."
    }
]