[
    {
        "date": "2025-08",
        "title": "LongSplat: Robust Unposed 3D Gaussian Splatting for Casual Long Videos",
        "author": "Chin-Yang Lin, Cheng Sun, Fu-En Yang, Min-Hung Chen, Yen-Yu Lin, and Yu-Lun Liu",
        "link": "http://arxiv.org/abs/2508.14041v1",
        "abstract": "LongSplat addresses critical challenges in novel view synthesis (NVS) from\ncasually captured long videos characterized by irregular camera motion, unknown\ncamera poses, and expansive scenes. Current methods often suffer from pose\ndrift, inaccurate geometry initialization, and severe memory limitations. To\naddress these issues, we introduce LongSplat, a robust unposed 3D Gaussian\nSplatting framework featuring: (1) Incremental Joint Optimization that\nconcurrently optimizes camera poses and 3D Gaussians to avoid local minima and\nensure global consistency; (2) a robust Pose Estimation Module leveraging\nlearned 3D priors; and (3) an efficient Octree Anchor Formation mechanism that\nconverts dense point clouds into anchors based on spatial density. Extensive\nexperiments on challenging benchmarks demonstrate that LongSplat achieves\nstate-of-the-art results, substantially improving rendering quality, pose\naccuracy, and computational efficiency compared to prior approaches. Project\npage: https://linjohnss.github.io/longsplat/"
    },
    {
        "date": "2025-08",
        "title": "ASDFormer: A Transformer with Mixtures of Pooling-Classifier Experts for Robust Autism Diagnosis and Biomarker Discovery",
        "author": "Mohammad Izadi, and Mehran Safayani",
        "link": "http://arxiv.org/abs/2508.14005v1",
        "abstract": "Autism Spectrum Disorder (ASD) is a complex neurodevelopmental condition\nmarked by disruptions in brain connectivity. Functional MRI (fMRI) offers a\nnon-invasive window into large-scale neural dynamics by measuring\nblood-oxygen-level-dependent (BOLD) signals across the brain. These signals can\nbe modeled as interactions among Regions of Interest (ROIs), which are grouped\ninto functional communities based on their underlying roles in brain function.\nEmerging evidence suggests that connectivity patterns within and between these\ncommunities are particularly sensitive to ASD-related alterations. Effectively\ncapturing these patterns and identifying interactions that deviate from typical\ndevelopment is essential for improving ASD diagnosis and enabling biomarker\ndiscovery. In this work, we introduce ASDFormer, a Transformer-based\narchitecture that incorporates a Mixture of Pooling-Classifier Experts (MoE) to\ncapture neural signatures associated with ASD. By integrating multiple\nspecialized expert branches with attention mechanisms, ASDFormer adaptively\nemphasizes different brain regions and connectivity patterns relevant to\nautism. This enables both improved classification performance and more\ninterpretable identification of disorder-related biomarkers. Applied to the\nABIDE dataset, ASDFormer achieves state-of-the-art diagnostic accuracy and\nreveals robust insights into functional connectivity disruptions linked to ASD,\nhighlighting its potential as a tool for biomarker discovery."
    },
    {
        "date": "2025-08",
        "title": "FedUP: Efficient Pruning-based Federated Unlearning for Model Poisoning Attacks",
        "author": "Nicol\u00f2 Romandini, Cristian Borcea, Rebecca Montanari, and Luca Foschini",
        "link": "http://arxiv.org/abs/2508.13853v1",
        "abstract": "Federated Learning (FL) can be vulnerable to attacks, such as model\npoisoning, where adversaries send malicious local weights to compromise the\nglobal model. Federated Unlearning (FU) is emerging as a solution to address\nsuch vulnerabilities by selectively removing the influence of detected\nmalicious contributors on the global model without complete retraining.\nHowever, unlike typical FU scenarios where clients are trusted and cooperative,\napplying FU with malicious and possibly colluding clients is challenging\nbecause their collaboration in unlearning their data cannot be assumed. This\nwork presents FedUP, a lightweight FU algorithm designed to efficiently\nmitigate malicious clients' influence by pruning specific connections within\nthe attacked model. Our approach achieves efficiency by relying only on\nclients' weights from the last training round before unlearning to identify\nwhich connections to inhibit. Isolating malicious influence is non-trivial due\nto overlapping updates from benign and malicious clients. FedUP addresses this\nby carefully selecting and zeroing the highest magnitude weights that diverge\nthe most between the latest updates from benign and malicious clients while\npreserving benign information. FedUP is evaluated under a strong adversarial\nthreat model, where up to 50%-1 of the clients could be malicious and have full\nknowledge of the aggregation process. We demonstrate the effectiveness,\nrobustness, and efficiency of our solution through experiments across IID and\nNon-IID data, under label-flipping and backdoor attacks, and by comparing it\nwith state-of-the-art (SOTA) FU solutions. In all scenarios, FedUP reduces\nmalicious influence, lowering accuracy on malicious data to match that of a\nmodel retrained from scratch while preserving performance on benign data. FedUP\nachieves effective unlearning while consistently being faster and saving\nstorage compared to the SOTA."
    },
    {
        "date": "2025-08",
        "title": "Timestep-Compressed Attack on Spiking Neural Networks through Timestep-Level Backpropagation",
        "author": "Donghwa Kang, Doohyun Kim, Sang-Ki Ko, Jinkyu Lee, Hyeongboo Baek, and Brent ByungHoon Kang",
        "link": "http://arxiv.org/abs/2508.13812v1",
        "abstract": "State-of-the-art (SOTA) gradient-based adversarial attacks on spiking neural\nnetworks (SNNs), which largely rely on extending FGSM and PGD frameworks, face\na critical limitation: substantial attack latency from multi-timestep\nprocessing, rendering them infeasible for practical real-time applications.\nThis inefficiency stems from their design as direct extensions of ANN\nparadigms, which fail to exploit key SNN properties. In this paper, we propose\nthe timestep-compressed attack (TCA), a novel framework that significantly\nreduces attack latency. TCA introduces two components founded on key insights\ninto SNN behavior. First, timestep-level backpropagation (TLBP) is based on our\nfinding that global temporal information in backpropagation to generate\nperturbations is not critical for an attack's success, enabling per-timestep\nevaluation for early stopping. Second, adversarial membrane potential reuse\n(A-MPR) is motivated by the observation that initial timesteps are\ninefficiently spent accumulating membrane potential, a warm-up phase that can\nbe pre-calculated and reused. Our experiments on VGG-11 and ResNet-17 with the\nCIFAR-10/100 and CIFAR10-DVS datasets show that TCA significantly reduces the\nrequired attack latency by up to 56.6% and 57.1% compared to SOTA methods in\nwhite-box and black-box settings, respectively, while maintaining a comparable\nattack success rate."
    },
    {
        "date": "2025-08",
        "title": "NodeShield: Runtime Enforcement of Security-Enhanced SBOMs for Node.js",
        "author": "Eric Cornelissen, and Musard Balliu",
        "link": "http://arxiv.org/abs/2508.13750v1",
        "abstract": "The software supply chain is an increasingly common attack vector for\nmalicious actors. The Node.js ecosystem has been subject to a wide array of\nattacks, likely due to its size and prevalence. To counter such attacks, the\nresearch community and practitioners have proposed a range of static and\ndynamic mechanisms, including process- and language-level sandboxing,\npermission systems, and taint tracking. Drawing on valuable insight from these\nworks, this paper studies a runtime protection mechanism for (the supply chain\nof) Node.js applications with the ambitious goals of compatibility, automation,\nminimal overhead, and policy conciseness.\n  Specifically, we design, implement and evaluate NodeShield, a protection\nmechanism for Node.js that enforces an application's dependency hierarchy and\ncontrols access to system resources at runtime. We leverage the up-and-coming\nSBOM standard as the source of truth for the dependency hierarchy of the\napplication, thus preventing components from stealthily abusing undeclared\ncomponents. We propose to enhance the SBOM with a notion of capabilities that\nrepresents a set of related system resources a component may access. Our\nproposed SBOM extension, the Capability Bill of Materials or CBOM, records the\nrequired capabilities of each component, providing valuable insight into the\npotential privileged behavior. NodeShield enforces the SBOM and CBOM at runtime\nvia code outlining (as opposed to inlining) with no modifications to the\noriginal code or Node.js runtime, thus preventing unexpected, potentially\nmalicious behavior. Our evaluation shows that NodeShield can prevent over 98%\nout of 67 known supply chain attacks while incurring minimal overhead on\nservers at less than 1ms per request. We achieve this while maintaining broad\ncompatibility with vanilla Node.js and a concise policy language that consists\nof at most 7 entries per dependency."
    },
    {
        "date": "2025-08",
        "title": "Enhancing Targeted Adversarial Attacks on Large Vision-Language Models through Intermediate Projector Guidance",
        "author": "Yiming Cao, Yanjie Li, Kaisheng Liang, Yuni Lai, and Bin Xiao",
        "link": "http://arxiv.org/abs/2508.13739v1",
        "abstract": "Targeted adversarial attacks are essential for proactively identifying\nsecurity flaws in Vision-Language Models before real-world deployment. However,\ncurrent methods perturb images to maximize global similarity with the target\ntext or reference image at the encoder level, collapsing rich visual semantics\ninto a single global vector. This limits attack granularity, hindering\nfine-grained manipulations such as modifying a car while preserving its\nbackground. Furthermore, these methods largely overlook the projector module, a\ncritical semantic bridge between the visual encoder and the language model in\nVLMs, thereby failing to disrupt the full vision-language alignment pipeline\nwithin VLMs and limiting attack effectiveness. To address these issues, we\npropose the Intermediate Projector Guided Attack (IPGA), the first method to\nattack using the intermediate stage of the projector module, specifically the\nwidely adopted Q-Former, which transforms global image embeddings into\nfine-grained visual features. This enables more precise control over\nadversarial perturbations by operating on semantically meaningful visual tokens\nrather than a single global representation. Specifically, IPGA leverages the\nQ-Former pretrained solely on the first vision-language alignment stage,\nwithout LLM fine-tuning, which improves both attack effectiveness and\ntransferability across diverse VLMs. Furthermore, we propose Residual Query\nAlignment (RQA) to preserve unrelated visual content, thereby yielding more\ncontrolled and precise adversarial manipulations. Extensive experiments show\nthat our attack method consistently outperforms existing methods in both\nstandard global image captioning tasks and fine-grained visual\nquestion-answering tasks in black-box environment. Additionally, IPGA\nsuccessfully transfers to multiple commercial VLMs, including Google Gemini and\nOpenAI GPT."
    },
    {
        "date": "2025-08",
        "title": "On the Security and Privacy of Federated Learning: A Survey with Attacks, Defenses, Frameworks, Applications, and Future Directions",
        "author": "Daniel M. Jimenez-Gutierrez, Yelizaveta Falkouskaya, Jose L. Hernandez-Ramos, Aris Anagnostopoulos, Ioannis Chatzigiannakis, and Andrea Vitaletti",
        "link": "http://arxiv.org/abs/2508.13730v1",
        "abstract": "Federated Learning (FL) is an emerging distributed machine learning paradigm\nenabling multiple clients to train a global model collaboratively without\nsharing their raw data. While FL enhances data privacy by design, it remains\nvulnerable to various security and privacy threats. This survey provides a\ncomprehensive overview of more than 200 papers regarding the state-of-the-art\nattacks and defense mechanisms developed to address these challenges,\ncategorizing them into security-enhancing and privacy-preserving techniques.\nSecurity-enhancing methods aim to improve FL robustness against malicious\nbehaviors such as byzantine attacks, poisoning, and Sybil attacks. At the same\ntime, privacy-preserving techniques focus on protecting sensitive data through\ncryptographic approaches, differential privacy, and secure aggregation. We\ncritically analyze the strengths and limitations of existing methods, highlight\nthe trade-offs between privacy, security, and model performance, and discuss\nthe implications of non-IID data distributions on the effectiveness of these\ndefenses. Furthermore, we identify open research challenges and future\ndirections, including the need for scalable, adaptive, and energy-efficient\nsolutions operating in dynamic and heterogeneous FL environments. Our survey\naims to guide researchers and practitioners in developing robust and\nprivacy-preserving FL systems, fostering advancements safeguarding\ncollaborative learning frameworks' integrity and confidentiality."
    },
    {
        "date": "2025-08",
        "title": "Heavy-tailed Linear Bandits: Adversarial Robustness, Best-of-both-worlds, and Beyond",
        "author": "Canzhe Zhao, Shinji Ito, and Shuai Li",
        "link": "http://arxiv.org/abs/2508.13679v1",
        "abstract": "Heavy-tailed bandits have been extensively studied since the seminal work of\n\\citet{Bubeck2012BanditsWH}. In particular, heavy-tailed linear bandits,\nenabling efficient learning with both a large number of arms and heavy-tailed\nnoises, have recently attracted significant attention\n\\citep{ShaoYKL18,XueWWZ20,ZhongHYW21,Wang2025heavy,tajdini2025improved}.\nHowever, prior studies focus almost exclusively on stochastic regimes, with few\nexceptions limited to the special case of heavy-tailed multi-armed bandits\n(MABs) \\citep{Huang0H22,ChengZ024,Chen2024uniINF}.\n  In this work, we propose a general framework for adversarial heavy-tailed\nbandit problems, which performs follow-the-regularized-leader (FTRL) over the\nloss estimates shifted by a bonus function. Via a delicate setup of the bonus\nfunction, we devise the first FTRL-type best-of-both-worlds (BOBW) algorithm\nfor heavy-tailed MABs, which does not require the truncated non-negativity\nassumption and achieves an $\\widetilde{O}(T^{\\frac{1}{\\varepsilon}})$\nworst-case regret in the adversarial regime as well as an $\\widetilde{O}(\\log\nT)$ gap-dependent regret in the stochastic regime. We then extend our framework\nto the linear case, proposing the first algorithm for adversarial heavy-tailed\nlinear bandits with finite arm sets. This algorithm achieves an\n$\\widetilde{O}(d^{\\frac{1}{2}}T^{\\frac{1}{\\varepsilon}})$ regret, matching the\nbest-known worst-case regret bound in stochastic regimes. Moreover, we propose\na general data-dependent learning rate, termed \\textit{heavy-tailed noise aware\nstability-penalty matching} (HT-SPM). We prove that HT-SPM guarantees BOBW\nregret bounds for general heavy-tailed bandit problems once certain conditions\nare satisfied. By using HT-SPM and, in particular, a variance-reduced linear\nloss estimator, we obtain the first BOBW result for heavy-tailed linear\nbandits."
    },
    {
        "date": "2025-08",
        "title": "V2P: From Background Suppression to Center Peaking for Robust GUI Grounding Task",
        "author": "Jikai Chen, Long Chen, Dong Wang, Leilei Gan, Chenyi Zhuang, and Jinjie Gu",
        "link": "http://arxiv.org/abs/2508.13634v1",
        "abstract": "Precise localization of GUI elements is crucial for the development of GUI\nagents. Traditional methods rely on bounding box or center-point regression,\nneglecting spatial interaction uncertainty and visual-semantic hierarchies.\nRecent methods incorporate attention mechanisms but still face two key issues:\n(1) ignoring processing background regions causes attention drift from the\ndesired area, and (2) uniform labeling fails to distinguish between center and\nedges of the target UI element, leading to click imprecision. Inspired by how\nhumans visually process and interact with GUI elements, we propose the\nValley-to-Peak (V2P) method to address these issues. To mitigate background\ndistractions, V2P introduces a suppression attention mechanism that minimizes\nthe model's focus on irrelevant regions to highlight the intended region. For\nthe issue of center-edge distinction, V2P applies a Fitts' Law-inspired\napproach by modeling GUI interactions as 2D Gaussian heatmaps where the weight\ngradually decreases from the center towards the edges. The weight distribution\nfollows a Gaussian function, with the variance determined by the target's size.\nConsequently, V2P effectively isolates the target area and teaches the model to\nconcentrate on the most essential point of the UI element. The model trained by\nV2P achieves the performance with 92.3% and 50.5% on two benchmarks\nScreenSpot-v2 and ScreenSpot-Pro. Ablations further confirm each component's\ncontribution, highlighting V2P's generalizability for precise GUI grounding\ntasks."
    },
    {
        "date": "2025-08",
        "title": "DDoS Attacks in Cloud Computing: Detection and Prevention",
        "author": "Zain Ahmad, Musab Ahmad, and Bilal Ahmad",
        "link": "http://arxiv.org/abs/2508.13522v1",
        "abstract": "DDoS attacks are one of the most prevalent and harmful cybersecurity threats\nfaced by organizations and individuals today. In recent years, the complexity\nand frequency of DDoS attacks have increased significantly, making it\nchallenging to detect and mitigate them effectively. The study analyzes various\ntypes of DDoS attacks, including volumetric, protocol, and application layer\nattacks, and discusses the characteristics, impact, and potential targets of\neach type. It also examines the existing techniques used for DDoS attack\ndetection, such as packet filtering, intrusion detection systems, and machine\nlearning-based approaches, and their strengths and limitations. Moreover, the\nstudy explores the prevention techniques employed to mitigate DDoS attacks,\nsuch as firewalls, rate limiting , CPP and ELD mechanism. It evaluates the\neffectiveness of each approach and its suitability for different types of\nattacks and environments. In conclusion, this study provides a comprehensive\noverview of the different types of DDoS attacks, their detection, and\nprevention techniques. It aims to provide insights and guidelines for\norganizations and individuals to enhance their cybersecurity posture and\nprotect against DDoS attacks."
    },
    {
        "date": "2025-08",
        "title": "Optimizing Scalar Selection in Elliptic Curve Cryptography Using Differential Evolution for Enhanced Security",
        "author": "Takreem Haider",
        "link": "http://arxiv.org/abs/2508.13520v1",
        "abstract": "Elliptic Curve Cryptography (ECC) is a fundamental component of modern\npublic-key cryptosystems that enable efficient and secure digital signatures,\nkey exchanges, and encryption. Its core operation, scalar multiplication,\ndenoted as $k \\cdot P$, where $P$ is a base point and $k$ is a private scalar,\nrelies heavily on the secrecy and unpredictability of $k$. Conventionally, $k$\nis selected using user input or pseudorandom number generators. However, in\nresource-constrained environments with weak entropy sources, these approaches\nmay yield low-entropy or biased scalars, increasing susceptibility to\nside-channel and key recovery attacks. To mitigate these vulnerabilities, we\nintroduce an optimization-driven scalar generation method that explicitly\nmaximizes bit-level entropy. Our approach uses differential evolution (DE), a\npopulation-based metaheuristic algorithm, to search for scalars whose binary\nrepresentations exhibit maximal entropy, defined by an even and statistically\nuniform distribution of ones and zeros. This reformulation of scalar selection\nas an entropy-optimization problem enhances resistance to entropy-based\ncryptanalytic techniques and improves overall unpredictability. Experimental\nresults demonstrate that DE-optimized scalars achieve entropy significantly\nhigher than conventionally generated scalars. The proposed method can be\nintegrated into existing ECC-based protocols, offering a deterministic, tunable\nalternative to traditional randomness, ideal for applications in blockchain,\nsecure messaging, IoT, and other resource-constrained environments."
    },
    {
        "date": "2025-08",
        "title": "ROVER: Robust Loop Closure Verification with Trajectory Prior in Repetitive Environments",
        "author": "Jingwen Yu, Jiayi Yang, Anjun Hu, Jiankun Wang, Ping Tan, and Hong Zhang",
        "link": "http://arxiv.org/abs/2508.13488v1",
        "abstract": "Loop closure detection is important for simultaneous localization and mapping\n(SLAM), which associates current observations with historical keyframes,\nachieving drift correction and global relocalization. However, a falsely\ndetected loop can be fatal, and this is especially difficult in repetitive\nenvironments where appearance-based features fail due to the high similarity.\nTherefore, verification of a loop closure is a critical step in avoiding false\npositive detections. Existing works in loop closure verification predominantly\nfocus on learning invariant appearance features, neglecting the prior knowledge\nof the robot's spatial-temporal motion cue, i.e., trajectory. In this letter,\nwe propose ROVER, a loop closure verification method that leverages the\nhistorical trajectory as a prior constraint to reject false loops in\nchallenging repetitive environments. For each loop candidate, it is first used\nto estimate the robot trajectory with pose-graph optimization. This trajectory\nis then submitted to a scoring scheme that assesses its compliance with the\ntrajectory without the loop, which we refer to as the trajectory prior, to\ndetermine if the loop candidate should be accepted. Benchmark comparisons and\nreal-world experiments demonstrate the effectiveness of the proposed method.\nFurthermore, we integrate ROVER into state-of-the-art SLAM systems to verify\nits robustness and efficiency. Our source code and self-collected dataset are\navailable at https://github.com/jarvisyjw/ROVER."
    },
    {
        "date": "2025-08",
        "title": "Enhancing Robustness of Implicit Neural Representations Against Weight Perturbations",
        "author": "Wenyong Zhou, Yuxin Cheng, Zhengwu Liu, Taiqiang Wu, Chen Zhang, and Ngai Wong",
        "link": "http://arxiv.org/abs/2508.13481v1",
        "abstract": "Implicit Neural Representations (INRs) encode discrete signals in a\ncontinuous manner using neural networks, demonstrating significant value across\nvarious multimedia applications. However, the vulnerability of INRs presents a\ncritical challenge for their real-world deployments, as the network weights\nmight be subjected to unavoidable perturbations. In this work, we investigate\nthe robustness of INRs for the first time and find that even minor\nperturbations can lead to substantial performance degradation in the quality of\nsignal reconstruction. To mitigate this issue, we formulate the robustness\nproblem in INRs by minimizing the difference between loss with and without\nweight perturbations. Furthermore, we derive a novel robust loss function to\nregulate the gradient of the reconstruction loss with respect to weights,\nthereby enhancing the robustness. Extensive experiments on reconstruction tasks\nacross multiple modalities demonstrate that our method achieves up to a 7.5~dB\nimprovement in peak signal-to-noise ratio (PSNR) values compared to original\nINRs under noisy conditions."
    },
    {
        "date": "2025-08",
        "title": "When Secure Aggregation Falls Short: Achieving Long-Term Privacy in Asynchronous Federated Learning for LEO Satellite Networks",
        "author": "Mohamed Elmahallawy, and Tie Luo",
        "link": "http://arxiv.org/abs/2508.13425v1",
        "abstract": "Secure aggregation is a common technique in federated learning (FL) for\nprotecting data privacy from both curious internal entities (clients or server)\nand external adversaries (eavesdroppers). However, in dynamic and\nresource-constrained environments such as low Earth orbit (LEO) satellite\nnetworks, traditional secure aggregation methods fall short in two aspects: (1)\nthey assume continuous client availability while LEO satellite visibility is\nintermittent and irregular; (2) they consider privacy in each communication\nround but have overlooked the possible privacy leakage through multiple rounds.\nTo address these limitations, we propose LTP-FLEO, an asynchronous FL framework\nthat preserves long-term privacy (LTP) for LEO satellite networks. LTP-FLEO\nintroduces (i) privacy-aware satellite partitioning, which groups satellites\nbased on their predictable visibility to the server and enforces joint\nparticipation; (ii) model age balancing, which mitigates the adverse impact of\nstale model updates; and (iii) fair global aggregation, which treats satellites\nof different visibility durations in an equitable manner. Theoretical analysis\nand empirical validation demonstrate that LTP-FLEO effectively safeguards both\nmodel and data privacy across multi-round training, promotes fairness in line\nwith satellite contributions, accelerates global convergence, and achieves\ncompetitive model accuracy."
    },
    {
        "date": "2025-08",
        "title": "DAASH: A Meta-Attack Framework for Synthesizing Effective and Stealthy Adversarial Examples",
        "author": "Abdullah Al Nomaan Nafi, Habibur Rahaman, Zafaryab Haider, Tanzim Mahfuz, Fnu Suya, Swarup Bhunia, and Prabuddha Chakraborty",
        "link": "http://arxiv.org/abs/2508.13309v1",
        "abstract": "Numerous techniques have been proposed for generating adversarial examples in\nwhite-box settings under strict Lp-norm constraints. However, such norm-bounded\nexamples often fail to align well with human perception, and only recently have\na few methods begun specifically exploring perceptually aligned adversarial\nexamples. Moreover, it remains unclear whether insights from Lp-constrained\nattacks can be effectively leveraged to improve perceptual efficacy. In this\npaper, we introduce DAASH, a fully differentiable meta-attack framework that\ngenerates effective and perceptually aligned adversarial examples by\nstrategically composing existing Lp-based attack methods. DAASH operates in a\nmulti-stage fashion: at each stage, it aggregates candidate adversarial\nexamples from multiple base attacks using learned, adaptive weights and\npropagates the result to the next stage. A novel meta-loss function guides this\nprocess by jointly minimizing misclassification loss and perceptual distortion,\nenabling the framework to dynamically modulate the contribution of each base\nattack throughout the stages. We evaluate DAASH on adversarially trained models\nacross CIFAR-10, CIFAR-100, and ImageNet. Despite relying solely on\nLp-constrained based methods, DAASH significantly outperforms state-of-the-art\nperceptual attacks such as AdvAD -- achieving higher attack success rates\n(e.g., 20.63\\% improvement) and superior visual quality, as measured by SSIM,\nLPIPS, and FID (improvements $\\approx$ of 11, 0.015, and 5.7, respectively).\nFurthermore, DAASH generalizes well to unseen defenses, making it a practical\nand strong baseline for evaluating robustness without requiring handcrafted\nadaptive attacks for each new defense."
    },
    {
        "date": "2025-08",
        "title": "CLoE: Curriculum Learning on Endoscopic Images for Robust MES Classification",
        "author": "Zeynep Ozdemir, Hacer Yalim Keles, and Omer Ozgur Tanriover",
        "link": "http://arxiv.org/abs/2508.13280v1",
        "abstract": "Estimating disease severity from endoscopic images is essential in assessing\nulcerative colitis, where the Mayo Endoscopic Subscore (MES) is widely used to\ngrade inflammation. However, MES classification remains challenging due to\nlabel noise from inter-observer variability and the ordinal nature of the\nscore, which standard models often ignore. We propose CLoE, a curriculum\nlearning framework that accounts for both label reliability and ordinal\nstructure. Image quality, estimated via a lightweight model trained on Boston\nBowel Preparation Scale (BBPS) labels, is used as a proxy for annotation\nconfidence to order samples from easy (clean) to hard (noisy). This curriculum\nis further combined with ResizeMix augmentation to improve robustness.\nExperiments on the LIMUC and HyperKvasir datasets, using both CNNs and\nTransformers, show that CLoE consistently improves performance over strong\nsupervised and self-supervised baselines. For instance, ConvNeXt-Tiny reaches\n82.5\\% accuracy and a QWK of 0.894 on LIMUC with low computational cost. These\nresults highlight the potential of difficulty-aware training strategies for\nimproving ordinal classification under label uncertainty. Code will be released\nat https://github.com/zeynepozdemir/CLoE."
    },
    {
        "date": "2025-08",
        "title": "The Application of Transformer-Based Models for Predicting Consequences of Cyber Attacks",
        "author": "Bipin Chhetri, and Akbar Siami Namin",
        "link": "http://arxiv.org/abs/2508.13030v1",
        "abstract": "Cyberattacks are increasing, and securing against such threats is costing\nindustries billions of dollars annually. Threat Modeling, that is,\ncomprehending the consequences of these attacks, can provide critical support\nto cybersecurity professionals, enabling them to take timely action and\nallocate resources that could be used elsewhere. Cybersecurity is heavily\ndependent on threat modeling, as it assists security experts in assessing and\nmitigating risks related to identifying vulnerabilities and threats. Recently,\nthere has been a pressing need for automated methods to assess attack\ndescriptions and forecast the future consequences of the increasing complexity\nof cyberattacks. This study examines how Natural Language Processing (NLP) and\ndeep learning can be applied to analyze the potential impact of cyberattacks by\nleveraging textual descriptions from the MITRE Common Weakness Enumeration\n(CWE) database. We emphasize classifying attack consequences into five\nprincipal categories: Availability, Access Control, Confidentiality, Integrity,\nand Other. This paper investigates the use of Bidirectional Encoder\nRepresentations from Transformers (BERT) in combination with Hierarchical\nAttention Networks (HANs) for Multi-label classification, evaluating their\nperformance in comparison with conventional CNN and LSTM-based models.\nExperimental findings show that BERT achieves an overall accuracy of $0.972$,\nfar higher than conventional deep learning models in multi-label\nclassification. HAN outperforms baseline forms of CNN and LSTM-based models on\nspecific cybersecurity labels. However, BERT consistently achieves better\nprecision and recall, making it more suitable for predicting the consequences\nof a cyberattack."
    },
    {
        "date": "2025-08",
        "title": "Design and Analysis of Robust Adaptive Filtering with the Hyperbolic Tangent Exponential Kernel M-Estimator Function for Active Noise Control",
        "author": "Iam Kim de S. Hermont, Andre R. Flores, and Rodrigo C. de Lamare",
        "link": "http://arxiv.org/abs/2508.13018v1",
        "abstract": "In this work, we propose a robust adaptive filtering approach for active\nnoise control applications in the presence of impulsive noise. In particular,\nwe develop the filtered-x hyperbolic tangent exponential generalized Kernel\nM-estimate function (FXHEKM) robust adaptive algorithm. A statistical analysis\nof the proposed FXHEKM algorithm is carried out along with a study of its\ncomputational cost. {In order to evaluate the proposed FXHEKM algorithm, the\nmean-square error (MSE) and the average noise reduction (ANR) performance\nmetrics have been adopted.} Numerical results show the efficiency of the\nproposed FXHEKM algorithm to cancel the presence of the additive spurious\nsignals, such as \\textbf{$\\alpha$}-stable noises against competing algorithms."
    },
    {
        "date": "2025-08",
        "title": "Fed-DPRoC:Communication-Efficient Differentially Private and Robust Federated Learning",
        "author": "Yue Xia, Tayyebeh Jahani-Nezhad, and Rawad Bitar",
        "link": "http://arxiv.org/abs/2508.12978v1",
        "abstract": "We propose Fed-DPRoC, a novel federated learning framework that\nsimultaneously ensures differential privacy (DP), Byzantine robustness, and\ncommunication efficiency. We introduce the concept of robust-compatible\ncompression, which enables users to compress DP-protected updates while\nmaintaining the robustness of the aggregation rule. We instantiate our\nframework as RobAJoL, combining the Johnson-Lindenstrauss (JL) transform for\ncompression with robust averaging for robust aggregation. We theoretically\nprove the compatibility of JL transform with robust averaging and show that\nRobAJoL preserves robustness guarantees, ensures DP, and reduces communication\ncost. Experiments on CIFAR-10 and Fashion MNIST validate our theoretical claims\nand demonstrate that RobAJoL outperforms existing methods in terms of\nrobustness and utility under different Byzantine attacks."
    },
    {
        "date": "2025-08",
        "title": "Prescriptive Zero Trust- Assessing the impact of zero trust on cyber attack prevention",
        "author": "Samuel Aiello",
        "link": "http://arxiv.org/abs/2508.12953v1",
        "abstract": "Increasingly sophisticated and varied cyber threats necessitate ever\nimproving enterprise security postures. For many organizations today, those\npostures have a foundation in the Zero Trust Architecture. This strategy sees\ntrust as something an enterprise must not give lightly or assume too broadly.\nUnderstanding the ZTA and its numerous controls centered around the idea of not\ntrusting anything inside or outside the network without verification, will\nallow organizations to comprehend and leverage this increasingly common\nparadigm. The ZTA, unlike many other regulatory frameworks, is not tightly\ndefined. The research assesses the likelihood of quantifiable guidelines that\nmeasure cybersecurity maturity for an enterprise organization in relation to\nZTA implementation. This is a new, data driven methodology for quantifying\ncyber resilience enabled by the adoption of Zero Trust principles to\npragmatically address the critical need of organizations. It also looks at the\npractical aspects ZTA has on capabilities in deterring cyberattacks on a\nnetwork. The outcomes of this research define a prescriptive set of key\ntechnical controls across identity verification, microsegmentation, data\nencryption, analytics, and orchestration that characterize the comprehensive\nZTA deployment. By evaluating the depth of integration for each control\ncomponent and aligning to industry best practices, the study's results help\nassess an organization's ZTA maturity level on a scale from Initial to\nOptimized adoption. The research's resultant four tier model demarcates phases\nfor an organization on its security transformation journey, with each tier\nadding to the capability of the last."
    },
    {
        "date": "2025-08",
        "title": "SecFSM: Knowledge Graph-Guided Verilog Code Generation for Secure Finite State Machines in Systems-on-Chip",
        "author": "Ziteng Hu, Yingjie Xia, Xiyuan Chen, and Li Kuang",
        "link": "http://arxiv.org/abs/2508.12910v1",
        "abstract": "Finite State Machines (FSMs) play a critical role in implementing control\nlogic for Systems-on-Chip (SoC). Traditionally, FSMs are implemented by\nhardware engineers through Verilog coding, which is often tedious and\ntime-consuming. Recently, with the remarkable progress of Large Language Models\n(LLMs) in code generation, LLMs have been increasingly explored for automating\nVerilog code generation. However, LLM-generated Verilog code often suffers from\nsecurity vulnerabilities, which is particularly concerning for\nsecurity-sensitive FSM implementations. To address this issue, we propose\nSecFSM, a novel method that leverages a security-oriented knowledge graph to\nguide LLMs in generating more secure Verilog code. Specifically, we first\nconstruct a FSM Security Knowledge Graph (FSKG) as an external aid to LLMs.\nSubsequently, we analyze users' requirements to identify vulnerabilities and\nget a list of vulnerabilities in the requirements. Then, we retrieve knowledge\nfrom FSKG based on the vulnerabilities list. Finally, we construct security\nprompts based on the security knowledge for Verilog code generation. To\nevaluate SecFSM, we build a dedicated dataset collected from academic datasets,\nartificial datasets, papers, and industrial cases. Extensive experiments\ndemonstrate that SecFSM outperforms state-of-the-art baselines. In particular,\non a benchmark of 25 security test cases evaluated by DeepSeek-R1, SecFSM\nachieves an outstanding pass rate of 21/25."
    },
    {
        "date": "2025-08",
        "title": "A Unified Cortical Circuit Model with Divisive Normalization and Self-Excitation for Robust Representation and Memory Maintenance",
        "author": "Jie Su, Weiwei Wang, Zhaotian Gu, Dahui Wang, and Tianyi Qian",
        "link": "http://arxiv.org/abs/2508.12702v1",
        "abstract": "Robust information representation and its persistent maintenance are\nfundamental for higher cognitive functions. Existing models employ distinct\nneural mechanisms to separately address noise-resistant processing or\ninformation maintenance, yet a unified framework integrating both operations\nremains elusive -- a critical gap in understanding cortical computation. Here,\nwe introduce a recurrent neural circuit that combines divisive normalization\nwith self-excitation to achieve both robust encoding and stable retention of\nnormalized inputs. Mathematical analysis shows that, for suitable parameter\nregimes, the system forms a continuous attractor with two key properties: (1)\ninput-proportional stabilization during stimulus presentation; and (2)\nself-sustained memory states persisting after stimulus offset. We demonstrate\nthe model's versatility in two canonical tasks: (a) noise-robust encoding in a\nrandom-dot kinematogram (RDK) paradigm; and (b) approximate Bayesian belief\nupdating in a probabilistic Wisconsin Card Sorting Test (pWCST). This work\nestablishes a unified mathematical framework that bridges noise suppression,\nworking memory, and approximate Bayesian inference within a single cortical\nmicrocircuit, offering fresh insights into the brain's canonical computation\nand guiding the design of biologically plausible artificial neural\narchitectures."
    },
    {
        "date": "2025-08",
        "title": "Robust Federated Learning under Adversarial Attacks via Loss-Based Client Clustering",
        "author": "Emmanouil Kritharakis, Dusan Jakovetic, Antonios Makris, and Konstantinos Tserpes",
        "link": "http://arxiv.org/abs/2508.12672v2",
        "abstract": "Federated Learning (FL) enables collaborative model training across multiple\nclients without sharing private data. We consider FL scenarios wherein FL\nclients are subject to adversarial (Byzantine) attacks, while the FL server is\ntrusted (honest) and has a trustworthy side dataset. This may correspond to,\ne.g., cases where the server possesses trusted data prior to federation, or to\nthe presence of a trusted client that temporarily assumes the server role. Our\napproach requires only two honest participants, i.e., the server and one\nclient, to function effectively, without prior knowledge of the number of\nmalicious clients. Theoretical analysis demonstrates bounded optimality gaps\neven under strong Byzantine attacks. Experimental results show that our\nalgorithm significantly outperforms standard and robust FL baselines such as\nMean, Trimmed Mean, Median, Krum, and Multi-Krum under various attack\nstrategies including label flipping, sign flipping, and Gaussian noise addition\nacross MNIST, FMNIST, and CIFAR-10 benchmarks using the Flower framework."
    },
    {
        "date": "2025-08",
        "title": "Quantifying Loss Aversion in Cyber Adversaries via LLM Analysis",
        "author": "Soham Hans, Nikolos Gurney, Stacy Marsella, and Sofia Hirschmann",
        "link": "http://arxiv.org/abs/2508.13240v1",
        "abstract": "Understanding and quantifying human cognitive biases from empirical data has\nlong posed a formidable challenge, particularly in cybersecurity, where\ndefending against unknown adversaries is paramount. Traditional cyber defense\nstrategies have largely focused on fortification, while some approaches attempt\nto anticipate attacker strategies by mapping them to cognitive vulnerabilities,\nyet they fall short in dynamically interpreting attacks in progress. In\nrecognition of this gap, IARPA's ReSCIND program seeks to infer, defend\nagainst, and even exploit attacker cognitive traits. In this paper, we present\na novel methodology that leverages large language models (LLMs) to extract\nquantifiable insights into the cognitive bias of loss aversion from hacker\nbehavior. Our data are collected from an experiment in which hackers were\nrecruited to attack a controlled demonstration network. We process the hacker\ngenerated notes using LLMs using it to segment the various actions and\ncorrelate the actions to predefined persistence mechanisms used by hackers. By\ncorrelating the implementation of these mechanisms with various operational\ntriggers, our analysis provides new insights into how loss aversion manifests\nin hacker decision-making. The results demonstrate that LLMs can effectively\ndissect and interpret nuanced behavioral patterns, thereby offering a\ntransformative approach to enhancing cyber defense strategies through\nreal-time, behavior-based analysis."
    },
    {
        "date": "2025-08",
        "title": "How can we trust opaque systems? Criteria for robust explanations in XAI",
        "author": "Florian J. Boge, and Annika Schuster",
        "link": "http://arxiv.org/abs/2508.12623v1",
        "abstract": "Deep learning (DL) algorithms are becoming ubiquitous in everyday life and in\nscientific research. However, the price we pay for their impressively accurate\npredictions is significant: their inner workings are notoriously opaque - it is\nunknown to laypeople and researchers alike what features of the data a DL\nsystem focuses on and how it ultimately succeeds in predicting correct outputs.\nA necessary criterion for trustworthy explanations is that they should reflect\nthe relevant processes the algorithms' predictions are based on. The field of\neXplainable Artificial Intelligence (XAI) presents promising methods to create\nsuch explanations. But recent reviews about their performance offer reasons for\nskepticism. As we will argue, a good criterion for trustworthiness is\nexplanatory robustness: different XAI methods produce the same explanations in\ncomparable contexts. However, in some instances, all methods may give the same,\nbut still wrong, explanation. We therefore argue that in addition to\nexplanatory robustness (ER), a prior requirement of explanation method\nrobustness (EMR) has to be fulfilled by every XAI method. Conversely, the\nrobustness of an individual method is in itself insufficient for\ntrustworthiness. In what follows, we develop and formalize criteria for ER as\nwell as EMR, providing a framework for explaining and establishing trust in DL\nalgorithms. We also highlight interesting application cases and outline\ndirections for future work."
    },
    {
        "date": "2025-08",
        "title": "Reducing False Positives with Active Behavioral Analysis for Cloud Security",
        "author": "Dikshant, and Verma",
        "link": "http://arxiv.org/abs/2508.12584v1",
        "abstract": "Rule-based cloud security posture management (CSPM) solutions are known to\nproduce a lot of false positives based on the limited contextual understanding\nand dependence on static heuristics testing. This paper introduces a\nvalidation-driven methodology that integrates active behavioral testing in\ncloud security posture management solution(s) to evaluate the exploitability of\npolicy violations in real time. The proposed system employs lightweight and\nautomated probes, built from open-source tools, validation scripts, and\npenetration testing test cases, to simulate adversarial attacks on\nmisconfigured or vulnerable cloud assets without any impact to the cloud\nservices or environment. For instance, cloud services may be flagged as\npublicly exposed and vulnerable despite being protected by access control\nlayers, or secure policies, resulting in non-actionable alerts that consumes\nanalysts time during manual validation. Through controlled experimentation in a\nreproducible AWS setup, we evaluated the reduction in false positive rates\nacross various misconfiguration and vulnerable alerts. Our findings indicate an\naverage reduction of 93\\% in false positives. Furthermore, the framework\ndemonstrates low latency performance. These results demonstrate a scalable\nmethod to improve detection accuracy and analyst productivity in large cloud\nenvironments. While our evaluation focuses on AWS, the architecture is modular\nand extensible to multi-cloud setups."
    },
    {
        "date": "2025-08",
        "title": "DEFENDCLI: {Command-Line} Driven Attack Provenance Examination",
        "author": "Peilun Wu, Nan Sun, Nour Moustafa, Youyang Qu, and Ming Ding",
        "link": "http://arxiv.org/abs/2508.12553v1",
        "abstract": "Endpoint Detection and Response (EDR) solutions embrace the method of attack\nprovenance graph to discover unknown threats through system event correlation.\nHowever, this method still faces some unsolved problems in the fields of\ninteroperability, reliability, flexibility, and practicability to deliver\nactionable results. Our research highlights the limitations of current\nsolutions in detecting obfuscation, correlating attacks, identifying\nlow-frequency events, and ensuring robust context awareness in relation to\ncommand-line activities. To address these challenges, we introduce DEFENDCLI,\nan innovative system leveraging provenance graphs that, for the first time,\ndelves into command-line-level detection. By offering finer detection\ngranularity, it addresses a gap in modern EDR systems that has been overlooked\nin previous research. Our solution improves the precision of the information\nrepresentation by evaluating differentiation across three levels: unusual\nsystem process calls, suspicious command-line executions, and infrequent\nexternal network connections. This multi-level approach enables EDR systems to\nbe more reliable in complex and dynamic environments. Our evaluation\ndemonstrates that DEFENDCLI improves precision by approximately 1.6x compared\nto the state-of-the-art methods on the DARPA Engagement Series attack datasets.\nExtensive real-time industrial testing across various attack scenarios further\nvalidates its practical effectiveness. The results indicate that DEFENDCLI not\nonly detects previously unknown attack instances, which are missed by other\nmodern commercial solutions, but also achieves a 2.3x improvement in precision\nover the state-of-the-art research work."
    },
    {
        "date": "2025-08",
        "title": "Systematic Analysis of MCP Security",
        "author": "Yongjian Guo, Puzhuo Liu, Wanlun Ma, Zehang Deng, Xiaogang Zhu, Peng Di, Xi Xiao, and Sheng Wen",
        "link": "http://arxiv.org/abs/2508.12538v1",
        "abstract": "The Model Context Protocol (MCP) has emerged as a universal standard that\nenables AI agents to seamlessly connect with external tools, significantly\nenhancing their functionality. However, while MCP brings notable benefits, it\nalso introduces significant vulnerabilities, such as Tool Poisoning Attacks\n(TPA), where hidden malicious instructions exploit the sycophancy of large\nlanguage models (LLMs) to manipulate agent behavior. Despite these risks,\ncurrent academic research on MCP security remains limited, with most studies\nfocusing on narrow or qualitative analyses that fail to capture the diversity\nof real-world threats. To address this gap, we present the MCP Attack Library\n(MCPLIB), which categorizes and implements 31 distinct attack methods under\nfour key classifications: direct tool injection, indirect tool injection,\nmalicious user attacks, and LLM inherent attack. We further conduct a\nquantitative analysis of the efficacy of each attack. Our experiments reveal\nkey insights into MCP vulnerabilities, including agents' blind reliance on tool\ndescriptions, sensitivity to file-based attacks, chain attacks exploiting\nshared context, and difficulty distinguishing external data from executable\ncommands. These insights, validated through attack experiments, underscore the\nurgency for robust defense strategies and informed MCP design. Our\ncontributions include 1) constructing a comprehensive MCP attack taxonomy, 2)\nintroducing a unified attack framework MCPLIB, and 3) conducting empirical\nvulnerability analysis to enhance MCP security mechanisms. This work provides a\nfoundational framework, supporting the secure evolution of MCP ecosystems."
    },
    {
        "date": "2025-08",
        "title": "A Robust Cross-Domain IDS using BiGRU-LSTM-Attention for Medical and Industrial IoT Security",
        "author": "Afrah Gueriani, Hamza Kheddar, Ahmed Cherif Mazari, and Mohamed Chahine Ghanem",
        "link": "http://arxiv.org/abs/2508.12470v1",
        "abstract": "The increased Internet of Medical Things IoMT and the Industrial Internet of\nThings IIoT interconnectivity has introduced complex cybersecurity challenges,\nexposing sensitive data, patient safety, and industrial operations to advanced\ncyber threats. To mitigate these risks, this paper introduces a novel\ntransformer-based intrusion detection system IDS, termed BiGAT-ID a hybrid\nmodel that combines bidirectional gated recurrent units BiGRU, long short-term\nmemory LSTM networks, and multi-head attention MHA. The proposed architecture\nis designed to effectively capture bidirectional temporal dependencies, model\nsequential patterns, and enhance contextual feature representation. Extensive\nexperiments on two benchmark datasets, CICIoMT2024 medical IoT and EdgeIIoTset\nindustrial IoT demonstrate the model's cross-domain robustness, achieving\ndetection accuracies of 99.13 percent and 99.34 percent, respectively.\nAdditionally, the model exhibits exceptional runtime efficiency, with inference\ntimes as low as 0.0002 seconds per instance in IoMT and 0.0001 seconds in IIoT\nscenarios. Coupled with a low false positive rate, BiGAT-ID proves to be a\nreliable and efficient IDS for deployment in real-world heterogeneous IoT\nenvironments"
    },
    {
        "date": "2025-08",
        "title": "Adversarial Attacks on VQA-NLE: Exposing and Alleviating Inconsistencies in Visual Question Answering Explanations",
        "author": "Yahsin Yeh, Yilun Wu, Bokai Ruan, and Honghan Shuai",
        "link": "http://arxiv.org/abs/2508.12430v1",
        "abstract": "Natural language explanations in visual question answering (VQA-NLE) aim to\nmake black-box models more transparent by elucidating their decision-making\nprocesses. However, we find that existing VQA-NLE systems can produce\ninconsistent explanations and reach conclusions without genuinely understanding\nthe underlying context, exposing weaknesses in either their inference pipeline\nor explanation-generation mechanism. To highlight these vulnerabilities, we not\nonly leverage an existing adversarial strategy to perturb questions but also\npropose a novel strategy that minimally alters images to induce contradictory\nor spurious outputs. We further introduce a mitigation method that leverages\nexternal knowledge to alleviate these inconsistencies, thereby bolstering model\nrobustness. Extensive evaluations on two standard benchmarks and two widely\nused VQA-NLE models underscore the effectiveness of our attacks and the\npotential of knowledge-based defenses, ultimately revealing pressing security\nand reliability concerns in current VQA-NLE systems."
    },
    {
        "date": "2025-08",
        "title": "ViT-EnsembleAttack: Augmenting Ensemble Models for Stronger Adversarial Transferability in Vision Transformers",
        "author": "Hanwen Cao, Haobo Lu, Xiaosen Wang, and Kun He",
        "link": "http://arxiv.org/abs/2508.12384v1",
        "abstract": "Ensemble-based attacks have been proven to be effective in enhancing\nadversarial transferability by aggregating the outputs of models with various\narchitectures. However, existing research primarily focuses on refining\nensemble weights or optimizing the ensemble path, overlooking the exploration\nof ensemble models to enhance the transferability of adversarial attacks. To\naddress this gap, we propose applying adversarial augmentation to the surrogate\nmodels, aiming to boost overall generalization of ensemble models and reduce\nthe risk of adversarial overfitting. Meanwhile, observing that ensemble Vision\nTransformers (ViTs) gain less attention, we propose ViT-EnsembleAttack based on\nthe idea of model adversarial augmentation, the first ensemble-based attack\nmethod tailored for ViTs to the best of our knowledge. Our approach generates\naugmented models for each surrogate ViT using three strategies: Multi-head\ndropping, Attention score scaling, and MLP feature mixing, with the associated\nparameters optimized by Bayesian optimization. These adversarially augmented\nmodels are ensembled to generate adversarial examples. Furthermore, we\nintroduce Automatic Reweighting and Step Size Enlargement modules to boost\ntransferability. Extensive experiments demonstrate that ViT-EnsembleAttack\nsignificantly enhances the adversarial transferability of ensemble-based\nattacks on ViTs, outperforming existing methods by a substantial margin. Code\nis available at https://github.com/Trustworthy-AI-Group/TransferAttack."
    },
    {
        "date": "2025-08",
        "title": "MCPSecBench: A Systematic Security Benchmark and Playground for Testing Model Context Protocols",
        "author": "Yixuan Yang, Daoyuan Wu, and Yufan Chen",
        "link": "http://arxiv.org/abs/2508.13220v1",
        "abstract": "Large Language Models (LLMs) are increasingly integrated into real-world\napplications via the Model Context Protocol (MCP), a universal, open standard\nfor connecting AI agents with data sources and external tools. While MCP\nenhances the capabilities of LLM-based agents, it also introduces new security\nrisks and expands their attack surfaces. In this paper, we present the first\nsystematic taxonomy of MCP security, identifying 17 attack types across 4\nprimary attack surfaces. We introduce MCPSecBench, a comprehensive security\nbenchmark and playground that integrates prompt datasets, MCP servers, MCP\nclients, and attack scripts to evaluate these attacks across three major MCP\nproviders. Our benchmark is modular and extensible, allowing researchers to\nincorporate custom implementations of clients, servers, and transport protocols\nfor systematic security assessment. Experimental results show that over 85% of\nthe identified attacks successfully compromise at least one platform, with core\nvulnerabilities universally affecting Claude, OpenAI, and Cursor, while\nprompt-based and tool-centric attacks exhibit considerable variability across\ndifferent hosts and models. Overall, MCPSecBench standardizes the evaluation of\nMCP security and enables rigorous testing across all MCP layers."
    },
    {
        "date": "2025-08",
        "title": "Adjustable AprilTags For Identity Secured Tasks",
        "author": "Hao Li",
        "link": "http://arxiv.org/abs/2508.12304v1",
        "abstract": "Special tags such as AprilTags that facilitate image processing and pattern\nrecognition are useful in practical applications. In close and private\nenvironments, identity security is unlikely to be an issue because all involved\nAprilTags can be completely regulated. However, in open and public\nenvironments, identity security is no longer an issue that can be neglected. To\nhandle potential harm caused by adversarial attacks, this note advocates\nutilization of adjustable AprilTags instead of fixed ones."
    },
    {
        "date": "2025-08",
        "title": "HuBERT-VIC: Improving Noise-Robust Automatic Speech Recognition of Speech Foundation Model via Variance-Invariance-Covariance Regularization",
        "author": "Hyebin Ahn, Kangwook Jang, and Hoirin Kim",
        "link": "http://arxiv.org/abs/2508.12292v1",
        "abstract": "Noise robustness in speech foundation models (SFMs) has been a critical\nchallenge, as most models are primarily trained on clean data and experience\nperformance degradation when the models are exposed to noisy speech. To address\nthis issue, we propose HuBERT-VIC, a noise-robust SFM with variance,\nin-variance, and covariance regularization (VICReg) objectives. These\nobjectives adjust the statistics of noisy speech representations, enabling the\nmodel to capture diverse acoustic characteristics and improving the\ngeneralization ability across different types of noise. When applied to HuBERT,\nour model shows relative performance improvements of 23.3% on LibriSpeech\ntest-clean and 13.2% on test-other, compared to the baseline model pre-trained\non noisy speech."
    },
    {
        "date": "2025-08",
        "title": "WXSOD: A Benchmark for Robust Salient Object Detection in Adverse Weather Conditions",
        "author": "Quan Chen, Xiong Yang, Rongfeng Lu, Qianyu Zhang, Yu Liu, Xiaofei Zhou, and Bolun Zheng",
        "link": "http://arxiv.org/abs/2508.12250v1",
        "abstract": "Salient object detection (SOD) in complex environments remains a challenging\nresearch topic. Most existing methods perform well in natural scenes with\nnegligible noise, and tend to leverage multi-modal information (e.g., depth and\ninfrared) to enhance accuracy. However, few studies are concerned with the\ndamage of weather noise on SOD performance due to the lack of dataset with\npixel-wise annotations. To bridge this gap, this paper introduces a novel\nWeather-eXtended Salient Object Detection (WXSOD) dataset. It consists of\n14,945 RGB images with diverse weather noise, along with the corresponding\nground truth annotations and weather labels. To verify algorithm\ngeneralization, WXSOD contains two test sets, i.e., a synthesized test set and\na real test set. The former is generated by adding weather noise to clean\nimages, while the latter contains real-world weather noise. Based on WXSOD, we\npropose an efficient baseline, termed Weather-aware Feature Aggregation Network\n(WFANet), which adopts a fully supervised two-branch architecture.\nSpecifically, the weather prediction branch mines weather-related deep\nfeatures, while the saliency detection branch fuses semantic features extracted\nfrom the backbone with weather features for SOD. Comprehensive comparisons\nagainst 17 SOD methods shows that our WFANet achieves superior performance on\nWXSOD. The code and benchmark results will be made publicly available at\nhttps://github.com/C-water/WXSOD"
    },
    {
        "date": "2025-08",
        "title": "CAN Networks Security in Smart Grids Communication Technologies",
        "author": "Ayman W. Baharia, Khaled T. Naga, Hesham S. Abdelfattah, Shady A. Maged, and Sherif A. Hammad",
        "link": "http://arxiv.org/abs/2508.12181v1",
        "abstract": "The rapid evolution of smart grids requires effective communication protocols\nto transfer data reliably and securely. Controller Area Network (CAN) is one of\nthe most recognized protocols that offer reliable data transmission in smart\ngrids due to its robustness, real-time capabilities, and relatively low initial\ncost of its required hardware. However, as a smart city becomes more\ninterconnected, it also becomes more vulnerable to cyber-attacks. As there are\nmany mechanisms to secure the CAN nodes from attacks, most of those mechanisms\nhave computational overhead, resulting in more delay in the network. We\nimplemented a solution that requires almost no overhead to any CAN node\nconnected to the network. It depends on a single node responsible for securing\nthe CAN network. This approach seeks to augment network security while reducing\nsecurity mechanisms overhead to all CAN network nodes. The methodology and\ncomprehensive test results will be presented in detail during a subsequent\ndiscussion. The used software for development is Code Composer Studio, and the\nused microcontroller evaluation boards (EVB) are TM4C 1294."
    },
    {
        "date": "2025-08",
        "title": "Invitation Is All You Need! Promptware Attacks Against LLM-Powered Assistants in Production Are Practical and Dangerous",
        "author": "Ben Nassi, Stav Cohen, and Or Yair",
        "link": "http://arxiv.org/abs/2508.12175v1",
        "abstract": "The growing integration of LLMs into applications has introduced new security\nrisks, notably known as Promptware - maliciously engineered prompts designed to\nmanipulate LLMs to compromise the CIA triad of these applications. While prior\nresearch warned about a potential shift in the threat landscape for LLM-powered\napplications, the risk posed by Promptware is frequently perceived as low. In\nthis paper, we investigate the risk Promptware poses to users of Gemini-powered\nassistants (web application, mobile application, and Google Assistant). We\npropose a novel Threat Analysis and Risk Assessment (TARA) framework to assess\nPromptware risks for end users. Our analysis focuses on a new variant of\nPromptware called Targeted Promptware Attacks, which leverage indirect prompt\ninjection via common user interactions such as emails, calendar invitations,\nand shared documents. We demonstrate 14 attack scenarios applied against\nGemini-powered assistants across five identified threat classes: Short-term\nContext Poisoning, Permanent Memory Poisoning, Tool Misuse, Automatic Agent\nInvocation, and Automatic App Invocation. These attacks highlight both digital\nand physical consequences, including spamming, phishing, disinformation\ncampaigns, data exfiltration, unapproved user video streaming, and control of\nhome automation devices. We reveal Promptware's potential for on-device lateral\nmovement, escaping the boundaries of the LLM-powered application, to trigger\nmalicious actions using a device's applications. Our TARA reveals that 73% of\nthe analyzed threats pose High-Critical risk to end users. We discuss\nmitigations and reassess the risk (in response to deployed mitigations) and\nshow that the risk could be reduced significantly to Very Low-Medium. We\ndisclosed our findings to Google, which deployed dedicated mitigations."
    },
    {
        "date": "2025-08",
        "title": "Attack Graph Generation on HPC Clusters",
        "author": "Ming Li, and John Hale",
        "link": "http://arxiv.org/abs/2508.12161v1",
        "abstract": "Attack graphs (AGs) are graphical tools to analyze the security of computer\nnetworks. By connecting the exploitation of individual vulnerabilities, AGs\nexpose possible multi-step attacks against target networks, allowing system\nadministrators to take preventive measures to enhance their network's security.\nAs powerful analytical tools, however, AGs are both time- and memory-consuming\nto be generated. As the numbers of network assets, interconnections between\ndevices, as well as vulnerabilities increase, the size and volume of the\nresulting AGs grow at a much higher rate, leading to the well-known state-space\nexplosion. In this paper, we propose the use of high performance computing\n(HPC) clusters to implement AG generators. We evaluate the performance through\nexperiments and provide insights into how cluster environments can help resolve\nthe issues of slow speed and high memory demands in AG generation in a balanced\nway."
    },
    {
        "date": "2025-08",
        "title": "TriQDef: Disrupting Semantic and Gradient Alignment to Prevent Adversarial Patch Transferability in Quantized Neural Networks",
        "author": "Amira Guesmi, Bassem Ouni, and Muhammad Shafique",
        "link": "http://arxiv.org/abs/2508.12132v1",
        "abstract": "Quantized Neural Networks (QNNs) are increasingly deployed in edge and\nresource-constrained environments due to their efficiency in computation and\nmemory usage. While shown to distort the gradient landscape and weaken\nconventional pixel-level attacks, it provides limited robustness against\npatch-based adversarial attacks-localized, high-saliency perturbations that\nremain surprisingly transferable across bit-widths. Existing defenses either\noverfit to fixed quantization settings or fail to address this cross-bit\ngeneralization vulnerability. We introduce \\textbf{TriQDef}, a tri-level\nquantization-aware defense framework designed to disrupt the transferability of\npatch-based adversarial attacks across QNNs. TriQDef consists of: (1) a Feature\nDisalignment Penalty (FDP) that enforces semantic inconsistency by penalizing\nperceptual similarity in intermediate representations; (2) a Gradient\nPerceptual Dissonance Penalty (GPDP) that explicitly misaligns input gradients\nacross bit-widths by minimizing structural and directional agreement via Edge\nIoU and HOG Cosine metrics; and (3) a Joint Quantization-Aware Training\nProtocol that unifies these penalties within a shared-weight training scheme\nacross multiple quantization levels. Extensive experiments on CIFAR-10 and\nImageNet demonstrate that TriQDef reduces Attack Success Rates (ASR) by over\n40\\% on unseen patch and quantization combinations, while preserving high clean\naccuracy. Our findings underscore the importance of disrupting both semantic\nand perceptual gradient alignment to mitigate patch transferability in QNNs."
    },
    {
        "date": "2025-08",
        "title": "Ethereum Crypto Wallets under Address Poisoning: How Usable and Secure Are They?",
        "author": "Shixuan Guan, and Kai Li",
        "link": "http://arxiv.org/abs/2508.12107v1",
        "abstract": "Blockchain address poisoning is an emerging phishing attack that crafts\n\"similar-looking\" transfer records in the victim's transaction history, which\naims to deceive victims and lure them into mistakenly transferring funds to the\nattacker. Recent works have shown that millions of Ethereum users were targeted\nand lost over 100 million US dollars.\n  Ethereum crypto wallets, serving users in browsing transaction history and\ninitiating transactions to transfer funds, play a central role in deploying\ncountermeasures to mitigate the address poisoning attack. However, whether they\nhave done so remains an open question. To fill the research void, in this\npaper, we design experiments to simulate address poisoning attacks and\nsystematically evaluate the usability and security of 53 popular Ethereum\ncrypto wallets. Our evaluation shows that there exist communication failures\nbetween 12 wallets and their transaction activity provider, which renders them\nunable to download the users' transaction history. Besides, our evaluation also\nshows that 16 wallets pose a high risk to their users due to displaying fake\ntoken phishing transfers. Moreover, our further analysis suggests that most\nwallets rely on transaction activity providers to filter out phishing\ntransfers. However, their phishing detection capability varies. Finally, we\nfound that only three wallets throw an explicit warning message when users\nattempt to transfer to the phishing address, implying a significant gap within\nthe broader Ethereum crypto wallet community in protecting users from address\npoisoning attacks.\n  Overall, our work shows that more efforts are needed by the Ethereum crypto\nwallet developer community to achieve the highest usability and security\nstandard. Our bug reports have been acknowledged by the developer community,\nwho are currently developing mitigation solutions."
    },
    {
        "date": "2025-08",
        "title": "Robust Data Fusion via Subsampling",
        "author": "Jing Wang, HaiYing Wang, and Kun Chen",
        "link": "http://arxiv.org/abs/2508.12048v1",
        "abstract": "Data fusion and transfer learning are rapidly growing fields that enhance\nmodel performance for a target population by leveraging other related data\nsources or tasks. The challenges lie in the various potential heterogeneities\nbetween the target and external data, as well as various practical concerns\nthat prevent a na\\\"ive data integration. We consider a realistic scenario where\nthe target data is limited in size while the external data is large but\ncontaminated with outliers; such data contamination, along with other\ncomputational and operational constraints, necessitates proper selection or\nsubsampling of the external data for transfer learning. To our\nknowledge,transfer learning and subsampling under data contamination have not\nbeen thoroughly investigated. We address this gap by studying various transfer\nlearning methods with subsamples of the external data, accounting for outliers\ndeviating from the underlying true model due to arbitrary mean shifts. Two\nsubsampling strategies are investigated: one aimed at reducing biases and the\nother at minimizing variances. Approaches to combine these strategies are also\nintroduced to enhance the performance of the estimators. We provide\nnon-asymptotic error bounds for the transfer learning estimators, clarifying\nthe roles of sample sizes, signal strength, sampling rates, magnitude of\noutliers, and tail behaviors of model error distributions, among other factors.\nExtensive simulations show the superior performance of the proposed methods.\nAdditionally, we apply our methods to analyze the risk of hard landings in A380\nairplanes by utilizing data from other airplane types,demonstrating that robust\ntransfer learning can improve estimation efficiency for relatively rare\nairplane types with the help of data from other types of airplanes."
    },
    {
        "date": "2025-08",
        "title": "DynamicPose: Real-time and Robust 6D Object Pose Tracking for Fast-Moving Cameras and Objects",
        "author": "Tingbang Liang, Yixin Zeng, Jiatong Xie, and Boyu Zhou",
        "link": "http://arxiv.org/abs/2508.11950v1",
        "abstract": "We present DynamicPose, a retraining-free 6D pose tracking framework that\nimproves tracking robustness in fast-moving camera and object scenarios.\nPrevious work is mainly applicable to static or quasi-static scenes, and its\nperformance significantly deteriorates when both the object and the camera move\nrapidly. To overcome these challenges, we propose three synergistic components:\n(1) A visual-inertial odometry compensates for the shift in the Region of\nInterest (ROI) caused by camera motion; (2) A depth-informed 2D tracker\ncorrects ROI deviations caused by large object translation; (3) A VIO-guided\nKalman filter predicts object rotation, generates multiple candidate poses, and\nthen obtains the final pose by hierarchical refinement. The 6D pose tracking\nresults guide subsequent 2D tracking and Kalman filter updates, forming a\nclosed-loop system that ensures accurate pose initialization and precise pose\ntracking. Simulation and real-world experiments demonstrate the effectiveness\nof our method, achieving real-time and robust 6D pose tracking for fast-moving\ncameras and objects."
    },
    {
        "date": "2025-08",
        "title": "Extending Straight-Through Estimation for Robust Neural Networks on Analog CIM Hardware",
        "author": "Yuannuo Feng, Wenyong Zhou, Yuexi Lyu, Yixiang Zhang, Zhengwu Liu, Ngai Wong, and Wang Kang",
        "link": "http://arxiv.org/abs/2508.11940v1",
        "abstract": "Analog Compute-In-Memory (CIM) architectures promise significant energy\nefficiency gains for neural network inference, but suffer from complex\nhardware-induced noise that poses major challenges for deployment. While\nnoise-aware training methods have been proposed to address this issue, they\ntypically rely on idealized and differentiable noise models that fail to\ncapture the full complexity of analog CIM hardware variations. Motivated by the\nStraight-Through Estimator (STE) framework in quantization, we decouple forward\nnoise simulation from backward gradient computation, enabling noise-aware\ntraining with more accurate but computationally intractable noise modeling in\nanalog CIM systems. We provide theoretical analysis demonstrating that our\napproach preserves essential gradient directional information while maintaining\ncomputational tractability and optimization stability. Extensive experiments\nshow that our extended STE framework achieves up to 5.3% accuracy improvement\non image classification, 0.72 perplexity reduction on text generation,\n2.2$\\times$ speedup in training time, and 37.9% lower peak memory usage\ncompared to standard noise-aware training methods."
    },
    {
        "date": "2025-08",
        "title": "HPD: Hybrid Projection Decomposition for Robust State Space Models on Analog CIM Hardware",
        "author": "Yuannuo Feng, Wenyong Zhou, Yuexi Lyu, Hanjie Liu, Zhengwu Liu, Ngai Wong, and Wang Kang",
        "link": "http://arxiv.org/abs/2508.11935v1",
        "abstract": "State Space Models (SSMs) are efficient alternatives to traditional sequence\nmodels, excelling at processing long sequences with lower computational\ncomplexity. Their reliance on matrix multiplications makes them ideal for\ncompute-in-memory (CIM) architectures, which improve energy efficiency by\ncomputing within memory arrays. However, device non-idealities in CIM introduce\nweight perturbations that can degrade inference accuracy. In this paper, we\nsystematically analyze the robustness of SSMs under noisy conditions,\nidentifying that the final block and output projection layers are more\nsusceptible to perturbations compared to other components. Building on these\ninsights, we propose HPD, a Hybrid Projection Decomposition strategy for the\nlast output projection layer. We replace the original weight matrix with the\nmultiplication of U and {\\Sigma} in its SVD to ensure compatibility with\nexisting hardware architectures, while offloading V> to digital hardware for\nprecise and robust correction. Comprehensive tests on Mamba models show that\nour method reduces perplexity by up to 99.57% under various noise conditions\ncompared to baseline models, with accuracy gains of up to 96.67% on the PIQA\nbenchmark for commonsense reasoning."
    },
    {
        "date": "2025-08",
        "title": "An Improved Algorithm for Adversarial Linear Contextual Bandits via Reduction",
        "author": "Tim van Erven, Jack Mayo, Julia Olkhovskaya, and Chen-Yu Wei",
        "link": "http://arxiv.org/abs/2508.11931v1",
        "abstract": "We present an efficient algorithm for linear contextual bandits with\nadversarial losses and stochastic action sets. Our approach reduces this\nsetting to misspecification-robust adversarial linear bandits with fixed action\nsets. Without knowledge of the context distribution or access to a context\nsimulator, the algorithm achieves $\\tilde{O}(\\min\\{d^2\\sqrt{T}, \\sqrt{d^3T\\log\nK}\\})$ regret and runs in $\\text{poly}(d,C,T)$ time, where $d$ is the feature\ndimension, $C$ is an upper bound on the number of linear constraints defining\nthe action set in each round, $K$ is an upper bound on the number of actions in\neach round, and $T$ is number of rounds. This resolves the open question by Liu\net al. (2023) on whether one can obtain $\\text{poly}(d)\\sqrt{T}$ regret in\npolynomial time independent of the number of actions. For the important class\nof combinatorial bandits with adversarial losses and stochastic action sets\nwhere the action sets can be described by a polynomial number of linear\nconstraints, our algorithm is the first to achieve $\\text{poly}(d)\\sqrt{T}$\nregret in polynomial time, while no prior algorithm achieves even $o(T)$ regret\nin polynomial time to our knowledge. When a simulator is available, the regret\nbound can be improved to $\\tilde{O}(d\\sqrt{L^\\star})$, where $L^\\star$ is the\ncumulative loss of the best policy."
    },
    {
        "date": "2025-08",
        "title": "Deciphering the Interplay between Attack and Protection Complexity in Privacy-Preserving Federated Learning",
        "author": "Xiaojin Zhang, Mingcong Xu, Yiming Li, Wei Chen, and Qiang Yang",
        "link": "http://arxiv.org/abs/2508.11907v1",
        "abstract": "Federated learning (FL) offers a promising paradigm for collaborative model\ntraining while preserving data privacy. However, its susceptibility to gradient\ninversion attacks poses a significant challenge, necessitating robust privacy\nprotection mechanisms. This paper introduces a novel theoretical framework to\ndecipher the intricate interplay between attack and protection complexities in\nprivacy-preserving FL. We formally define \"Attack Complexity\" as the minimum\ncomputational and data resources an adversary requires to reconstruct private\ndata below a given error threshold, and \"Protection Complexity\" as the expected\ndistortion introduced by privacy mechanisms. Leveraging Maximum Bayesian\nPrivacy (MBP), we derive tight theoretical bounds for protection complexity,\ndemonstrating its scaling with model dimensionality and privacy budget.\nFurthermore, we establish comprehensive bounds for attack complexity, revealing\nits dependence on privacy leakage, gradient distortion, model dimension, and\nthe chosen privacy level. Our findings quantitatively illuminate the\nfundamental trade-offs between privacy guarantees, system utility, and the\neffort required for both attacking and defending. This framework provides\ncritical insights for designing more secure and efficient federated learning\nsystems."
    },
    {
        "date": "2025-08",
        "title": "ComplicitSplat: Downstream Models are Vulnerable to Blackbox Attacks by 3D Gaussian Splat Camouflages",
        "author": "Matthew Hull, Haoyang Yang, Pratham Mehta, Mansi Phute, Aeree Cho, Haorang Wang, Matthew Lau, Wenke Lee, Wilian Lunardi, Martin Andreoni, and Polo Chau",
        "link": "http://arxiv.org/abs/2508.11854v1",
        "abstract": "As 3D Gaussian Splatting (3DGS) gains rapid adoption in safety-critical tasks\nfor efficient novel-view synthesis from static images, how might an adversary\ntamper images to cause harm? We introduce ComplicitSplat, the first attack that\nexploits standard 3DGS shading methods to create viewpoint-specific camouflage\n- colors and textures that change with viewing angle - to embed adversarial\ncontent in scene objects that are visible only from specific viewpoints and\nwithout requiring access to model architecture or weights. Our extensive\nexperiments show that ComplicitSplat generalizes to successfully attack a\nvariety of popular detector - both single-stage, multi-stage, and\ntransformer-based models on both real-world capture of physical objects and\nsynthetic scenes. To our knowledge, this is the first black-box attack on\ndownstream object detectors using 3DGS, exposing a novel safety risk for\napplications like autonomous navigation and other mission-critical robotic\nsystems."
    },
    {
        "date": "2025-08",
        "title": "Adversarial Robustness in Distributed Quantum Machine Learning",
        "author": "Pouya Kananian, and Hans-Arno Jacobsen",
        "link": "http://arxiv.org/abs/2508.11848v1",
        "abstract": "Studying adversarial robustness of quantum machine learning (QML) models is\nessential in order to understand their potential advantages over classical\nmodels and build trustworthy systems. Distributing QML models allows leveraging\nmultiple quantum processors to overcome the limitations of individual devices\nand build scalable systems. However, this distribution can affect their\nadversarial robustness, potentially making them more vulnerable to new attacks.\nKey paradigms in distributed QML include federated learning, which, similar to\nclassical models, involves training a shared model on local data and sending\nonly the model updates, as well as circuit distribution methods inherent to\nquantum computing, such as circuit cutting and teleportation-based techniques.\nThese quantum-specific methods enable the distributed execution of quantum\ncircuits across multiple devices. This work reviews the differences between\nthese distribution methods, summarizes existing approaches on the adversarial\nrobustness of QML models when distributed using each paradigm, and discusses\nopen questions in this area."
    },
    {
        "date": "2025-08",
        "title": "Securing Sideways: Thwarting Lateral Movement by Implementing Active Directory Tiering",
        "author": "Tyler Schroder, and Sohee Kim Park",
        "link": "http://arxiv.org/abs/2508.11812v1",
        "abstract": "The advancement of computing equipment and the advances in services over the\nInternet has allowed corporations, higher education, and many other\norganizations to pursue the shared computing network environment. A requirement\nfor shared computing environments is a centralized identity system to\nauthenticate and authorize user access. An organization's digital identity\nplane is a prime target for cyber threat actors. When compromised, identities\ncan be exploited to steal credentials, create unauthorized accounts, and\nmanipulate permissions-enabling attackers to gain control of the network and\nundermine its confidentiality, availability, and integrity. Cybercrime losses\nreached a record of 16.6 B in the United States in 2024. For organizations\nusing Microsoft software, Active Directory is the on-premises identity system\nof choice. In this article, we examine the challenge of security compromises in\nActive Directory (AD) environments and present effective strategies to prevent\ncredential theft and limit lateral movement by threat actors. Our proposed\napproaches aim to confine the movement of compromised credentials, preventing\nsignificant privilege escalation and theft. We argue that through our\nillustration of real-world scenarios, tiering can halt lateral movement and\nadvanced cyber-attacks, thus reducing ransom escalation. Our work bridges a gap\nin existing literature by combining technical guidelines with theoretical\narguments in support of tiering, positioning it as a vital component of modern\ncybersecurity strategy even though it cannot function in isolation. As the\nhardware advances and the cloud sourced services along with AI is advancing\nwith unprecedented speed, we think it is important for security experts and the\nbusiness to work together and start designing and developing software and\nframeworks to classify devices automatically and accurately within the tiered\nstructure."
    },
    {
        "date": "2025-08",
        "title": "Assessing User Privacy Leakage in Synthetic Packet Traces: An Attack-Grounded Approach",
        "author": "Minhao Jin, Hongyu He, and Maria Apostolaki",
        "link": "http://arxiv.org/abs/2508.11742v1",
        "abstract": "Current synthetic traffic generators (SynNetGens) promise privacy but lack\ncomprehensive guarantees or empirical validation, even as their fidelity\nsteadily improves. We introduce the first attack-grounded benchmark for\nassessing the privacy of SynNetGens directly from the traffic they produce. We\nframe privacy as membership inference at the traffic-source level--a realistic\nand actionable threat for data holders. To this end, we present TraceBleed, the\nfirst attack that exploits behavioral fingerprints across flows using\ncontrastive learning and temporal chunking, outperforming prior membership\ninference baselines by 172%. Our large-scale study across GAN-, diffusion-, and\nGPT-based SynNetGens uncovers critical insights: (i) SynNetGens leak user-level\ninformation; (ii) differential privacy either fails to stop these attacks or\nseverely degrades fidelity; and (iii) sharing more synthetic data amplifies\nleakage by 59% on average. Finally, we introduce TracePatch, the first\nSynNetGen-agnostic defense that combines adversarial ML with SMT constraints to\nmitigate leakage while preserving fidelity."
    },
    {
        "date": "2025-08",
        "title": "Pushing the Limits of Frequency Analysis in Leakage Abuse Attacks",
        "author": "Nathaniel Moyer, Charalampos Papamanthou, and Evgenios Kornaropoulos",
        "link": "http://arxiv.org/abs/2508.11563v1",
        "abstract": "Searchable encryption (SE) is the most scalable cryptographic primitive for\nsearching on encrypted data. Typical SE constructions often allow\naccess-pattern leakage, revealing which encrypted records are retrieved in the\nserver's responses. All the known generic cryptanalyses assume either that the\nqueries are issued uniformly at random or that the attacker observes the\nsearch-pattern leakage. It remains unclear what can be reconstructed when using\nonly the access-pattern leakage and knowledge of the query distribution. In\nthis work, we focus on the cryptanalytic technique of frequency analysis in the\ncontext of leakage-abuse attacks on schemes that support encrypted range\nqueries. Frequency analysis matches the frequency of retrieval of an encrypted\nrecord with a plaintext value based on its probability of retrieval that\nfollows from the knowledge of the query distribution. We generalize this\nunderexplored cryptanalytic technique and introduce a generic attack framework\ncalled Leakage-Abuse via Matching (LAMA) that works even on high-dimensional\nencrypted data. We identify a parameterization of LAMA that brings frequency\nanalysis to its limit -- that is, we prove that there is no additional\nfrequency matching that an attacker can perform to refine the result.\nFurthermore, we show that our results hold for any class of convex queries, and\nnot just axis-aligned rectangles, which is the assumption in all other attacks\non range schemes. Using these results, we identify query distributions that\nmake frequency analysis challenging for the attacker and, thus, can act as a\nmitigation mechanism. Finally, we implement and benchmark LAMA and reconstruct,\nfor the first time, plaintext data from encrypted range queries spanning up to\nfour dimensions."
    },
    {
        "date": "2025-08",
        "title": "RMSL: Weakly-Supervised Insider Threat Detection with Robust Multi-sphere Learning",
        "author": "Yang Wang, Yaxin Zhao, Xinyu Jiao, Sihan Xu, Xiangrui Cai, Ying Zhang, and Xiaojie Yuan",
        "link": "http://arxiv.org/abs/2508.11472v1",
        "abstract": "Insider threat detection aims to identify malicious user behavior by\nanalyzing logs that record user interactions. Due to the lack of fine-grained\nbehavior-level annotations, detecting specific behavior-level anomalies within\nuser behavior sequences is challenging. Unsupervised methods face high false\npositive rates and miss rates due to the inherent ambiguity between normal and\nanomalous behaviors. In this work, we instead introduce weak labels of behavior\nsequences, which have lower annotation costs, i.e., the training labels\n(anomalous or normal) are at sequence-level instead of behavior-level, to\nenhance the detection capability for behavior-level anomalies by learning\ndiscriminative features. To achieve this, we propose a novel framework called\nRobust Multi-sphere Learning (RMSL). RMSL uses multiple hyper-spheres to\nrepresent the normal patterns of behaviors. Initially, a one-class classifier\nis constructed as a good anomaly-supervision-free starting point. Building on\nthis, using multiple instance learning and adaptive behavior-level\nself-training debiasing based on model prediction confidence, the framework\nfurther refines hyper-spheres and feature representations using weak\nsequence-level labels. This approach enhances the model's ability to\ndistinguish between normal and anomalous behaviors. Extensive experiments\ndemonstrate that RMSL significantly improves the performance of behavior-level\ninsider threat detection."
    },
    {
        "date": "2025-08",
        "title": "Robust Convolution Neural ODEs via Contractivity-promoting regularization",
        "author": "Muhammad Zakwan, Liang Xu, and Giancarlo Ferrari-Trecate",
        "link": "http://arxiv.org/abs/2508.11432v1",
        "abstract": "Neural networks can be fragile to input noise and adversarial attacks.\n  In this work, we consider Convolutional Neural Ordinary Differential\nEquations (NODEs), a family of continuous-depth neural networks represented by\ndynamical systems, and propose to use contraction theory to improve their\nrobustness.\n  For a contractive dynamical system two trajectories starting from different\ninitial conditions converge to each other exponentially fast.\n  Contractive Convolutional NODEs can enjoy increased robustness as slight\nperturbations of the features do not cause a significant change in the output.\n  Contractivity can be induced during training by using a regularization term\ninvolving the Jacobian of the system dynamics.\n  To reduce the computational burden, we show that it can also be promoted\nusing carefully selected weight regularization terms for a class of NODEs with\nslope-restricted activation functions.\n  The performance of the proposed regularizers is illustrated through benchmark\nimage classification tasks on MNIST and FashionMNIST datasets, where images are\ncorrupted by different kinds of noise and attacks."
    },
    {
        "date": "2025-08",
        "title": "When Punctuation Matters: A Large-Scale Comparison of Prompt Robustness Methods for LLMs",
        "author": "Mikhail Seleznyov, Mikhail Chaichuk, Gleb Ershov, Alexander Panchenko, Elena Tutubalina, and Oleg Somov",
        "link": "http://arxiv.org/abs/2508.11383v1",
        "abstract": "Large Language Models (LLMs) are highly sensitive to subtle, non-semantic\nvariations in prompt phrasing and formatting. In this work, we present the\nfirst systematic evaluation of 5 methods for improving prompt robustness within\na unified experimental framework. We benchmark these techniques on 8 models\nfrom Llama, Qwen and Gemma families across 52 tasks from Natural Instructions\ndataset. Our evaluation covers robustness methods from both fine-tuned and\nin-context learning paradigms, and tests their generalization against multiple\ntypes of distribution shifts. Finally, we extend our analysis to GPT-4.1 and\nDeepSeek V3 to assess frontier models' current robustness to format\nperturbations. Our findings offer actionable insights into the relative\neffectiveness of these robustness methods, enabling practitioners to make\ninformed decisions when aiming for stable and reliable LLM performance in\nreal-world applications. Code:\nhttps://github.com/AIRI-Institute/when-punctuation-matters."
    },
    {
        "date": "2025-08",
        "title": "Semantically Guided Adversarial Testing of Vision Models Using Language Models",
        "author": "Katarzyna Filus, and Jorge M. Cruz-Duarte",
        "link": "http://arxiv.org/abs/2508.11341v1",
        "abstract": "In targeted adversarial attacks on vision models, the selection of the target\nlabel is a critical yet often overlooked determinant of attack success. This\ntarget label corresponds to the class that the attacker aims to force the model\nto predict. Now, existing strategies typically rely on randomness, model\npredictions, or static semantic resources, limiting interpretability,\nreproducibility, or flexibility. This paper then proposes a semantics-guided\nframework for adversarial target selection using the cross-modal knowledge\ntransfer from pretrained language and vision-language models. We evaluate\nseveral state-of-the-art models (BERT, TinyLLAMA, and CLIP) as similarity\nsources to select the most and least semantically related labels with respect\nto the ground truth, forming best- and worst-case adversarial scenarios. Our\nexperiments on three vision models and five attack methods reveal that these\nmodels consistently render practical adversarial targets and surpass static\nlexical databases, such as WordNet, particularly for distant class\nrelationships. We also observe that static testing of target labels offers a\npreliminary assessment of the effectiveness of similarity sources, \\textit{a\npriori} testing. Our results corroborate the suitability of pretrained models\nfor constructing interpretable, standardized, and scalable adversarial\nbenchmarks across architectures and datasets."
    },
    {
        "date": "2025-08",
        "title": "Salty Seagull: A VSAT Honeynet to Follow the Bread Crumb of Attacks in Ship Networks",
        "author": "Georgios Michail Makrakis, Jeroen Pijpker, Remco Hassing, Rob Loves, and Stephen McCombie",
        "link": "http://arxiv.org/abs/2508.11325v1",
        "abstract": "Cyber threats against the maritime industry have increased notably in recent\nyears, highlighting the need for innovative cybersecurity approaches. Ships, as\ncritical assets, possess highly specialized and interconnected network\ninfrastructures, where their legacy systems and operational constraints further\nexacerbate their vulnerability to cyberattacks. To better understand this\nevolving threat landscape, we propose the use of cyber-deception techniques and\nin particular honeynets, as a means to gather valuable insights into ongoing\nattack campaigns targeting the maritime sector.\n  In this paper we present Salty Seagull, a honeynet conceived to simulate a\nVSAT system for ships. This environment mimics the operations of a functional\nVSAT system onboard and, at the same time, enables a user to interact with it\nthrough a Web dashboard and a CLI environment. Furthermore, based on existing\nvulnerabilities, we purposefully integrate them into our system to increase\nattacker engagement. We exposed our honeynet for 30 days to the Internet to\nassess its capability and measured the received interaction. Results show that\nwhile numerous generic attacks have been attempted, only one curious attacker\nwith knowledge of the nature of the system and its vulnerabilities managed to\naccess it, without however exploring its full potential."
    },
    {
        "date": "2025-08",
        "title": "Delving into Dynamic Scene Cue-Consistency for Robust 3D Multi-Object Tracking",
        "author": "Haonan Zhang, Xinyao Wang, Boxi Wu, Tu Zheng, Wang Yunhua, and Zheng Yang",
        "link": "http://arxiv.org/abs/2508.11323v1",
        "abstract": "3D multi-object tracking is a critical and challenging task in the field of\nautonomous driving. A common paradigm relies on modeling individual object\nmotion, e.g., Kalman filters, to predict trajectories. While effective in\nsimple scenarios, this approach often struggles in crowded environments or with\ninaccurate detections, as it overlooks the rich geometric relationships between\nobjects. This highlights the need to leverage spatial cues. However, existing\ngeometry-aware methods can be susceptible to interference from irrelevant\nobjects, leading to ambiguous features and incorrect associations. To address\nthis, we propose focusing on cue-consistency: identifying and matching stable\nspatial patterns over time. We introduce the Dynamic Scene Cue-Consistency\nTracker (DSC-Track) to implement this principle. Firstly, we design a unified\nspatiotemporal encoder using Point Pair Features (PPF) to learn discriminative\ntrajectory embeddings while suppressing interference. Secondly, our\ncue-consistency transformer module explicitly aligns consistent feature\nrepresentations between historical tracks and current detections. Finally, a\ndynamic update mechanism preserves salient spatiotemporal information for\nstable online tracking. Extensive experiments on the nuScenes and Waymo Open\nDatasets validate the effectiveness and robustness of our approach. On the\nnuScenes benchmark, for instance, our method achieves state-of-the-art\nperformance, reaching 73.2% and 70.3% AMOTA on the validation and test sets,\nrespectively."
    },
    {
        "date": "2025-08",
        "title": "Boosting the Robustness-Accuracy Trade-off of SNNs by Robust Temporal Self-Ensemble",
        "author": "Jihang Wang, Dongcheng Zhao, Ruolin Chen, Qian Zhang, and Yi Zeng",
        "link": "http://arxiv.org/abs/2508.11279v1",
        "abstract": "Spiking Neural Networks (SNNs) offer a promising direction for\nenergy-efficient and brain-inspired computing, yet their vulnerability to\nadversarial perturbations remains poorly understood. In this work, we revisit\nthe adversarial robustness of SNNs through the lens of temporal ensembling,\ntreating the network as a collection of evolving sub-networks across discrete\ntimesteps. This formulation uncovers two critical but underexplored\nchallenges-the fragility of individual temporal sub-networks and the tendency\nfor adversarial vulnerabilities to transfer across time. To overcome these\nlimitations, we propose Robust Temporal self-Ensemble (RTE), a training\nframework that improves the robustness of each sub-network while reducing the\ntemporal transferability of adversarial perturbations. RTE integrates both\nobjectives into a unified loss and employs a stochastic sampling strategy for\nefficient optimization. Extensive experiments across multiple benchmarks\ndemonstrate that RTE consistently outperforms existing training methods in\nrobust-accuracy trade-off. Additional analyses reveal that RTE reshapes the\ninternal robustness landscape of SNNs, leading to more resilient and temporally\ndiversified decision boundaries. Our study highlights the importance of\ntemporal structure in adversarial learning and offers a principled foundation\nfor building robust spiking models."
    },
    {
        "date": "2025-08",
        "title": "Enhancing Interactive Voting-Based Map Matching: Improving Efficiency and Robustness for Heterogeneous GPS Trajectories",
        "author": "William Alemanni, Arianna Burzacchi, Davide Colombi, and Elena Giarratano",
        "link": "http://arxiv.org/abs/2508.11235v1",
        "abstract": "This paper presents an enhanced version of the Interactive Voting-Based Map\nMatching algorithm, designed to efficiently process trajectories with varying\nsampling rates. The main aim is to reconstruct GPS trajectories with high\naccuracy, independent of input data quality. Building upon the original\nalgorithm, developed exclusively for aligning GPS signals to road networks, we\nextend its capabilities by integrating trajectory imputation. Our improvements\nalso include the implementation of a distance-bounded interactive voting\nstrategy to reduce computational complexity, as well as modifications to\naddress missing data in the road network. Furthermore, we incorporate a\ncustom-built asset derived from OpenStreetMap, enabling this approach to be\nsmoothly applied in any geographic region covered by OpenStreetMap's road\nnetwork. These advancements preserve the core strengths of the original\nalgorithm while significantly extending its applicability to diverse real-world\nscenarios."
    },
    {
        "date": "2025-08",
        "title": "CHARM3R: Towards Unseen Camera Height Robust Monocular 3D Detector",
        "author": "Abhinav Kumar, Yuliang Guo, Zhihao Zhang, Xinyu Huang, Liu Ren, and Xiaoming Liu",
        "link": "http://arxiv.org/abs/2508.11185v1",
        "abstract": "Monocular 3D object detectors, while effective on data from one ego camera\nheight, struggle with unseen or out-of-distribution camera heights. Existing\nmethods often rely on Plucker embeddings, image transformations or data\naugmentation. This paper takes a step towards this understudied problem by\nfirst investigating the impact of camera height variations on state-of-the-art\n(SoTA) Mono3D models. With a systematic analysis on the extended CARLA dataset\nwith multiple camera heights, we observe that depth estimation is a primary\nfactor influencing performance under height variations. We mathematically prove\nand also empirically observe consistent negative and positive trends in mean\ndepth error of regressed and ground-based depth models, respectively, under\ncamera height changes. To mitigate this, we propose Camera Height Robust\nMonocular 3D Detector (CHARM3R), which averages both depth estimates within the\nmodel. CHARM3R improves generalization to unseen camera heights by more than\n$45\\%$, achieving SoTA performance on the CARLA dataset. Codes and Models at\nhttps://github.com/abhi1kumar/CHARM3R"
    },
    {
        "date": "2025-08",
        "title": "SHLIME: Foiling adversarial attacks fooling SHAP and LIME",
        "author": "Sam Chauhan, Estelle Duguet, Karthik Ramakrishnan, Hugh Van Deventer, Jack Kruger, and Ranjan Subbaraman",
        "link": "http://arxiv.org/abs/2508.11053v1",
        "abstract": "Post hoc explanation methods, such as LIME and SHAP, provide interpretable\ninsights into black-box classifiers and are increasingly used to assess model\nbiases and generalizability. However, these methods are vulnerable to\nadversarial manipulation, potentially concealing harmful biases. Building on\nthe work of Slack et al. (2020), we investigate the susceptibility of LIME and\nSHAP to biased models and evaluate strategies for improving robustness. We\nfirst replicate the original COMPAS experiment to validate prior findings and\nestablish a baseline. We then introduce a modular testing framework enabling\nsystematic evaluation of augmented and ensemble explanation approaches across\nclassifiers of varying performance. Using this framework, we assess multiple\nLIME/SHAP ensemble configurations on out-of-distribution models, comparing\ntheir resistance to bias concealment against the original methods. Our results\nidentify configurations that substantially improve bias detection, highlighting\ntheir potential for enhancing transparency in the deployment of high-stakes\nmachine learning systems."
    },
    {
        "date": "2025-08",
        "title": "MCP-Guard: A Defense Framework for Model Context Protocol Integrity in Large Language Model Applications",
        "author": "Wenpeng Xing, Zhonghao Qi, Yupeng Qin, Yilin Li, Caini Chang, Jiahui Yu, Changting Lin, Zhenzhen Xie, and Meng Han",
        "link": "http://arxiv.org/abs/2508.10991v1",
        "abstract": "The integration of Large Language Models (LLMs) with external tools via\nprotocols such as the Model Context Protocol (MCP) introduces critical security\nvulnerabilities, including prompt injection, data exfiltration, and other\nthreats. To counter these challenges, we propose MCP-Guard, a robust, layered\ndefense architecture designed for LLM--tool interactions. MCP-Guard employs a\nthree-stage detection pipeline that balances efficiency with accuracy: it\nprogresses from lightweight static scanning for overt threats and a deep neural\ndetector for semantic attacks, to our fine-tuned E5-based model achieves\n(96.01) accuracy in identifying adversarial prompts. Finally, a lightweight LLM\narbitrator synthesizes these signals to deliver the final decision while\nminimizing false positives. To facilitate rigorous training and evaluation, we\nalso introduce MCP-AttackBench, a comprehensive benchmark of over 70,000\nsamples. Sourced from public datasets and augmented by GPT-4, MCP-AttackBench\nsimulates diverse, real-world attack vectors in the MCP format, providing a\nfoundation for future research into securing LLM-tool ecosystems."
    },
    {
        "date": "2025-08",
        "title": "Beyond conventional vision: RGB-event fusion for robust object detection in dynamic traffic scenarios",
        "author": "Zhanwen Liu, Yujing Sun, Yang Wang, Nan Yang, Shengbo Eben Li, and Xiangmo Zhao",
        "link": "http://arxiv.org/abs/2508.10704v1",
        "abstract": "The dynamic range limitation of conventional RGB cameras reduces global\ncontrast and causes loss of high-frequency details such as textures and edges\nin complex traffic environments (e.g., nighttime driving, tunnels), hindering\ndiscriminative feature extraction and degrading frame-based object detection.\nTo address this, we integrate a bio-inspired event camera with an RGB camera to\nprovide high dynamic range information and propose a motion cue fusion network\n(MCFNet), which achieves optimal spatiotemporal alignment and adaptive\ncross-modal feature fusion under challenging lighting. Specifically, an event\ncorrection module (ECM) temporally aligns asynchronous event streams with image\nframes via optical-flow-based warping, jointly optimized with the detection\nnetwork to learn task-aware event representations. The event dynamic upsampling\nmodule (EDUM) enhances spatial resolution of event frames to match image\nstructures, ensuring precise spatiotemporal alignment. The cross-modal mamba\nfusion module (CMM) uses adaptive feature fusion with a novel interlaced\nscanning mechanism, effectively integrating complementary information for\nrobust detection. Experiments conducted on the DSEC-Det and PKU-DAVIS-SOD\ndatasets demonstrate that MCFNet significantly outperforms existing methods in\nvarious poor lighting and fast moving traffic scenarios. Notably, on the\nDSEC-Det dataset, MCFNet achieves a remarkable improvement, surpassing the best\nexisting methods by 7.4% in mAP50 and 1.7% in mAP metrics, respectively. The\ncode is available at https://github.com/Charm11492/MCFNet."
    },
    {
        "date": "2025-08",
        "title": "Physics-Informed Joint Multi-TE Super-Resolution with Implicit Neural Representation for Robust Fetal T2 Mapping",
        "author": "Busra Bulut, Maik Dannecker, Thomas Sanchez, Sara Neves Silva, Vladyslav Zalevskyi, Steven Jia, Jean-Baptiste Ledoux, Guillaume Auzias, Fran\u00e7ois Rousseau, Jana Hutter, Daniel Rueckert, and Meritxell Bach Cuadra",
        "link": "http://arxiv.org/abs/2508.10680v1",
        "abstract": "T2 mapping in fetal brain MRI has the potential to improve characterization\nof the developing brain, especially at mid-field (0.55T), where T2 decay is\nslower. However, this is challenging as fetal MRI acquisition relies on\nmultiple motion-corrupted stacks of thick slices, requiring slice-to-volume\nreconstruction (SVR) to estimate a high-resolution (HR) 3D volume. Currently,\nT2 mapping involves repeated acquisitions of these stacks at each echo time\n(TE), leading to long scan times and high sensitivity to motion. We tackle this\nchallenge with a method that jointly reconstructs data across TEs, addressing\nsevere motion. Our approach combines implicit neural representations with a\nphysics-informed regularization that models T2 decay, enabling information\nsharing across TEs while preserving anatomical and quantitative T2 fidelity. We\ndemonstrate state-of-the-art performance on simulated fetal brain and in vivo\nadult datasets with fetal-like motion. We also present the first in vivo fetal\nT2 mapping results at 0.55T. Our study shows potential for reducing the number\nof stacks per TE in T2 mapping by leveraging anatomical redundancy."
    },
    {
        "date": "2025-08",
        "title": "MirGuard: Towards a Robust Provenance-based Intrusion Detection System Against Graph Manipulation Attacks",
        "author": "Anyuan Sang, Lu Zhou, Li Yang, Junbo Jia, Huipeng Yang, Pengbin Feng, and Jianfeng Ma",
        "link": "http://arxiv.org/abs/2508.10639v1",
        "abstract": "Learning-based Provenance-based Intrusion Detection Systems (PIDSes) have\nbecome essential tools for anomaly detection in host systems due to their\nability to capture rich contextual and structural information, as well as their\npotential to detect unknown attacks. However, recent studies have shown that\nthese systems are vulnerable to graph manipulation attacks, where attackers\nmanipulate the graph structure to evade detection. While some previous\napproaches have discussed this type of attack, none have fully addressed it\nwith a robust detection solution, limiting the practical applicability of\nPIDSes.\n  To address this challenge, we propose MirGuard, a robust anomaly detection\nframework that combines logic-aware multi-view augmentation with contrastive\nrepresentation learning. Rather than applying arbitrary structural\nperturbations, MirGuard introduces Logic-Aware Noise Injection (LNI) to\ngenerate semantically valid graph views, ensuring that all augmentations\npreserve the underlying causal semantics of the provenance data. These views\nare then used in a Logic-Preserving Contrastive Learning framework, which\nencourages the model to learn representations that are invariant to benign\ntransformations but sensitive to adversarial inconsistencies. Comprehensive\nevaluations on multiple provenance datasets demonstrate that MirGuard\nsignificantly outperforms state-of-the-art detectors in robustness against\nvarious graph manipulation attacks without sacrificing detection performance\nand efficiency. Our work represents the first targeted study to enhance PIDS\nagainst such adversarial threats, providing a robust and effective solution to\nmodern cybersecurity challenges."
    },
    {
        "date": "2025-08",
        "title": "A Transformer-Based Approach for DDoS Attack Detection in IoT Networks",
        "author": "Sandipan Dey, Payal Santosh Kate, Vatsala Upadhyay, and Abhishek Vaish",
        "link": "http://arxiv.org/abs/2508.10636v1",
        "abstract": "DDoS attacks have become a major threat to the security of IoT devices and\ncan cause severe damage to the network infrastructure. IoT devices suffer from\nthe inherent problem of resource constraints and are therefore susceptible to\nsuch resource-exhausting attacks. Traditional methods for detecting DDoS\nattacks are not efficient enough to cope with the dynamic nature of IoT\nnetworks, as well as the scalability of the attacks, diversity of protocols,\nhigh volume of traffic, and variability in device behavior, and variability of\nprotocols like MQTT, CoAP, making it hard to implement security across all the\nprotocols. In this paper, we propose a novel approach, i.e., the use of\nTransformer models, which have shown remarkable performance in natural language\nprocessing tasks, for detecting DDoS attacks on IoT devices. The proposed model\nextracts features from network traffic data and processes them using a\nself-attention mechanism. Experiments conducted on a real-world dataset\ndemonstrate that the proposed approach outperforms traditional machine learning\ntechniques, which can be validated by comparing both approaches' accuracy,\nprecision, recall, and F1-score. The results of this study show that the\nTransformer models can be an effective solution for detecting DDoS attacks on\nIoT devices and have the potential to be deployed in real-world IoT\nenvironments."
    },
    {
        "date": "2025-08",
        "title": "Towards Powerful and Practical Patch Attacks for 2D Object Detection in Autonomous Driving",
        "author": "Yuxin Cao, Yedi Zhang, Wentao He, Yifan Liao, Yan Xiao, Chang Li, Zhiyong Huang, and Jin Song Dong",
        "link": "http://arxiv.org/abs/2508.10600v1",
        "abstract": "Learning-based autonomous driving systems remain critically vulnerable to\nadversarial patches, posing serious safety and security risks in their\nreal-world deployment. Black-box attacks, notable for their high attack success\nrate without model knowledge, are especially concerning, with their\ntransferability extensively studied to reduce computational costs compared to\nquery-based attacks. Previous transferability-based black-box attacks typically\nadopt mean Average Precision (mAP) as the evaluation metric and design training\nloss accordingly. However, due to the presence of multiple detected bounding\nboxes and the relatively lenient Intersection over Union (IoU) thresholds, the\nattack effectiveness of these approaches is often overestimated, resulting in\nreduced success rates in practical attacking scenarios. Furthermore, patches\ntrained on low-resolution data often fail to maintain effectiveness on\nhigh-resolution images, limiting their transferability to autonomous driving\ndatasets. To fill this gap, we propose P$^3$A, a Powerful and Practical Patch\nAttack framework for 2D object detection in autonomous driving, specifically\noptimized for high-resolution datasets. First, we introduce a novel metric,\nPractical Attack Success Rate (PASR), to more accurately quantify attack\neffectiveness with greater relevance for pedestrian safety. Second, we present\na tailored Localization-Confidence Suppression Loss (LCSL) to improve attack\ntransferability under PASR. Finally, to maintain the transferability for\nhigh-resolution datasets, we further incorporate the Probabilistic\nScale-Preserving Padding (PSPP) into the patch attack pipeline as a data\npreprocessing step. Extensive experiments show that P$^3$A outperforms\nstate-of-the-art attacks on unseen models and unseen high-resolution datasets,\nboth under the proposed practical IoU-based evaluation metric and the previous\nmAP-based metrics."
    },
    {
        "date": "2025-08",
        "title": "Oops!... They Stole it Again: Attacks on Split Learning",
        "author": "Tanveer Khan, and Antonis Michalas",
        "link": "http://arxiv.org/abs/2508.10598v1",
        "abstract": "Split Learning (SL) is a collaborative learning approach that improves\nprivacy by keeping data on the client-side while sharing only the intermediate\noutput with a server. However, the distributed nature of SL introduces new\nsecurity challenges, necessitating a comprehensive exploration of potential\nattacks. This paper systematically reviews various attacks on SL, classifying\nthem based on factors such as the attacker's role, the type of privacy risks,\nwhen data leaks occur, and where vulnerabilities exist. We also analyze\nexisting defense methods, including cryptographic methods, data modification\napproaches, distributed techniques, and hybrid solutions. Our findings reveal\nsecurity gaps, highlighting the effectiveness and limitations of existing\ndefenses. By identifying open challenges and future directions, this work\nprovides valuable information to improve SL privacy issues and guide further\nresearch."
    },
    {
        "date": "2025-08",
        "title": "Self-Supervised Temporal Super-Resolution of Energy Data using Generative Adversarial Transformer",
        "author": "Xuanhao Mu, G\u00f6khan Demirel, Yuzhe Zhang, Jianlei Liu, Thorsten Schlachter, and Veit Hagenmeyer",
        "link": "http://arxiv.org/abs/2508.10587v1",
        "abstract": "To bridge the temporal granularity gap in energy network design and operation\nbased on Energy System Models, resampling of time series is required. While\nconventional upsampling methods are computationally efficient, they often\nresult in significant information loss or increased noise. Advanced models such\nas time series generation models, Super-Resolution models and imputation models\nshow potential, but also face fundamental challenges. The goal of time series\ngenerative models is to learn the distribution of the original data to generate\nhigh-resolution series with similar statistical characteristics. This is not\nentirely consistent with the definition of upsampling. Time series\nSuper-Resolution models or imputation models can degrade the accuracy of\nupsampling because the input low-resolution time series are sparse and may have\ninsufficient context. Moreover, such models usually rely on supervised learning\nparadigms. This presents a fundamental application paradox: their training\nrequires the high-resolution time series that is intrinsically absent in\nupsampling application scenarios. To address the mentioned upsampling issue,\nthis paper introduces a new method utilizing Generative Adversarial\nTransformers (GATs), which can be trained without access to any ground-truth\nhigh-resolution data. Compared with conventional interpolation methods, the\nintroduced method can reduce the root mean square error (RMSE) of upsampling\ntasks by 9%, and the accuracy of a model predictive control (MPC) application\nscenario is improved by 13%."
    },
    {
        "date": "2025-08",
        "title": "Contrastive ECOC: Learning Output Codes for Adversarial Defense",
        "author": "Che-Yu Chou, and Hung-Hsuan Chen",
        "link": "http://arxiv.org/abs/2508.10491v1",
        "abstract": "Although one-hot encoding is commonly used for multiclass classification, it\nis not always the most effective encoding mechanism. Error Correcting Output\nCodes (ECOC) address multiclass classification by mapping each class to a\nunique codeword used as a label. Traditional ECOC methods rely on manually\ndesigned or randomly generated codebooks, which are labor-intensive and may\nyield suboptimal, dataset-agnostic results. This paper introduces three models\nfor automated codebook learning based on contrastive learning, allowing\ncodebooks to be learned directly and adaptively from data. Across four\ndatasets, our proposed models demonstrate superior robustness to adversarial\nattacks compared to two baselines. The source is available at\nhttps://github.com/YuChou20/Automated-Codebook-Learning-with-Error-Correcting-Output-Code-Technique."
    },
    {
        "date": "2025-08",
        "title": "Yet Another Mirage of Breaking MIRAGE: Debunking Occupancy-based Side-Channel Attacks on Fully Associative Randomized Caches",
        "author": "Chris Cao, and Gururaj Saileshwar",
        "link": "http://arxiv.org/abs/2508.10431v1",
        "abstract": "Recent work presented at USENIX Security 2025 claims that occupancy-based\nattacks can recover AES keys from the MIRAGE randomized cache. In this paper,\nwe examine these claims and find that they arise from fundamental modeling\nflaws. Most critically, the authors' simulation of MIRAGE uses a constant seed\nto initialize the random number generator used for global evictions in MIRAGE,\ncausing every AES encryption they trace to evict the same deterministic\nsequence of cache lines. This artificially creates a highly repeatable timing\npattern that is not representative of a realistic implementation of MIRAGE,\nwhere eviction sequences vary randomly between encryptions. When we instead\nrandomize the eviction seed for each run, reflecting realistic operation, the\ncorrelation between AES T-table accesses and attacker runtimes disappears, and\nthe attack fails. These findings show that the reported leakage is an artifact\nof incorrect modeling, and not an actual vulnerability in MIRAGE."
    },
    {
        "date": "2025-08",
        "title": "HiRef: Leveraging Hierarchical Ontology and Network Refinement for Robust Medication Recommendation",
        "author": "Yan Ting Chok, Soyon Park, Seungheun Baek, Hajung Kim, Junhyun Lee, and Jaewoo Kang",
        "link": "http://arxiv.org/abs/2508.10425v1",
        "abstract": "Medication recommendation is a crucial task for assisting physicians in\nmaking timely decisions from longitudinal patient medical records. However,\nreal-world EHR data present significant challenges due to the presence of\nrarely observed medical entities and incomplete records that may not fully\ncapture the clinical ground truth. While data-driven models trained on\nlongitudinal Electronic Health Records often achieve strong empirical\nperformance, they struggle to generalize under missing or novel conditions,\nlargely due to their reliance on observed co-occurrence patterns. To address\nthese issues, we propose Hierarchical Ontology and Network Refinement for\nRobust Medication Recommendation (HiRef), a unified framework that combines two\ncomplementary structures: (i) the hierarchical semantics encoded in curated\nmedical ontologies, and (ii) refined co-occurrence patterns derived from\nreal-world EHRs. We embed ontology entities in hyperbolic space, which\nnaturally captures tree-like relationships and enables knowledge transfer\nthrough shared ancestors, thereby improving generalizability to unseen codes.\nTo further improve robustness, we introduce a prior-guided sparse\nregularization scheme that refines the EHR co-occurrence graph by suppressing\nspurious edges while preserving clinically meaningful associations. Our model\nachieves strong performance on EHR benchmarks (MIMIC-III and MIMIC-IV) and\nmaintains high accuracy under simulated unseen-code settings. Extensive\nexperiments with comprehensive ablation studies demonstrate HiRef's resilience\nto unseen medical codes, supported by in-depth analyses of the learned\nsparsified graph structure and medical code embeddings."
    },
    {
        "date": "2025-08",
        "title": "Enhancing GraphQL Security by Detecting Malicious Queries Using Large Language Models, Sentence Transformers, and Convolutional Neural Networks",
        "author": "Irash Perera, Hiranya Abeyrathne, Sanjeewa Malalgoda, and Arshardh Ifthikar",
        "link": "http://arxiv.org/abs/2508.11711v1",
        "abstract": "GraphQL's flexibility, while beneficial for efficient data fetching,\nintroduces unique security vulnerabilities that traditional API security\nmechanisms often fail to address. Malicious GraphQL queries can exploit the\nlanguage's dynamic nature, leading to denial-of-service attacks, data\nexfiltration through injection, and other exploits. Existing solutions, such as\nstatic analysis, rate limiting, and general-purpose Web Application Firewalls,\noffer limited protection against sophisticated, context-aware attacks. This\npaper presents a novel, AI-driven approach for real-time detection of malicious\nGraphQL queries. Our method combines static analysis with machine learning\ntechniques, including Large Language Models (LLMs) for dynamic schema-based\nconfiguration, Sentence Transformers (SBERT and Doc2Vec) for contextual\nembedding of query payloads, and Convolutional Neural Networks (CNNs), Random\nForests, and Multilayer Perceptrons for classification. We detail the system\narchitecture, implementation strategies optimized for production environments\n(including ONNX Runtime optimization and parallel processing), and evaluate the\nperformance of our detection models and the overall system under load. Results\ndemonstrate high accuracy in detecting various threats, including SQL\ninjection, OS command injection, and XSS exploits, alongside effective\nmitigation of DoS and SSRF attempts. This research contributes a robust and\nadaptable solution for enhancing GraphQL API security."
    },
    {
        "date": "2025-08",
        "title": "Layer-Wise Perturbations via Sparse Autoencoders for Adversarial Text Generation",
        "author": "Huizhen Shu, Xuying Li, Qirui Wang, Yuji Kosuga, Mengqiu Tian, and Zhuo Li",
        "link": "http://arxiv.org/abs/2508.10404v1",
        "abstract": "With the rapid proliferation of Natural Language Processing (NLP), especially\nLarge Language Models (LLMs), generating adversarial examples to jailbreak LLMs\nremains a key challenge for understanding model vulnerabilities and improving\nrobustness. In this context, we propose a new black-box attack method that\nleverages the interpretability of large models. We introduce the Sparse Feature\nPerturbation Framework (SFPF), a novel approach for adversarial text generation\nthat utilizes sparse autoencoders to identify and manipulate critical features\nin text. After using the SAE model to reconstruct hidden layer representations,\nwe perform feature clustering on the successfully attacked texts to identify\nfeatures with higher activations. These highly activated features are then\nperturbed to generate new adversarial texts. This selective perturbation\npreserves the malicious intent while amplifying safety signals, thereby\nincreasing their potential to evade existing defenses. Our method enables a new\nred-teaming strategy that balances adversarial effectiveness with safety\nalignment. Experimental results demonstrate that adversarial texts generated by\nSFPF can bypass state-of-the-art defense mechanisms, revealing persistent\nvulnerabilities in current NLP systems.However, the method's effectiveness\nvaries across prompts and layers, and its generalizability to other\narchitectures and larger models remains to be validated."
    },
    {
        "date": "2025-08",
        "title": "Unlocking Robust Semantic Segmentation Performance via Label-only Elastic Deformations against Implicit Label Noise",
        "author": "Yechan Kim, Dongho Yoon, Younkwan Lee, Unse Fatima, Hong Kook Kim, Songjae Lee, Sanga Park, Jeong Ho Park, Seonjong Kang, and Moongu Jeon",
        "link": "http://arxiv.org/abs/2508.10383v1",
        "abstract": "While previous studies on image segmentation focus on handling severe (or\nexplicit) label noise, real-world datasets also exhibit subtle (or implicit)\nlabel imperfections. These arise from inherent challenges, such as ambiguous\nobject boundaries and annotator variability. Although not explicitly present,\nsuch mild and latent noise can still impair model performance. Typical data\naugmentation methods, which apply identical transformations to the image and\nits label, risk amplifying these subtle imperfections and limiting the model's\ngeneralization capacity. In this paper, we introduce NSegment+, a novel\naugmentation framework that decouples image and label transformations to\naddress such realistic noise for semantic segmentation. By introducing\ncontrolled elastic deformations only to segmentation labels while preserving\nthe original images, our method encourages models to focus on learning robust\nrepresentations of object structures despite minor label inconsistencies.\nExtensive experiments demonstrate that NSegment+ consistently improves\nperformance, achieving mIoU gains of up to +2.29, +2.38, +1.75, and +3.39 in\naverage on Vaihingen, LoveDA, Cityscapes, and PASCAL VOC, respectively-even\nwithout bells and whistles, highlighting the importance of addressing implicit\nlabel noise. These gains can be further amplified when combined with other\ntraining tricks, including CutMix and Label Smoothing."
    },
    {
        "date": "2025-08",
        "title": "A Hierarchical IDS for Zero-Day Attack Detection in Internet of Medical Things Networks",
        "author": "Md Ashraf Uddin, Nam H. Chu, and Reza Rafeh",
        "link": "http://arxiv.org/abs/2508.10346v1",
        "abstract": "The Internet of Medical Things (IoMT) is driving a healthcare revolution but\nremains vulnerable to cyberattacks such as denial of service, ransomware, data\nhijacking, and spoofing. These networks comprise resource constrained,\nheterogeneous devices (e.g., wearable sensors, smart pills, implantables),\nmaking traditional centralized Intrusion Detection Systems (IDSs) unsuitable\ndue to response delays, privacy risks, and added vulnerabilities. Centralized\nIDSs require all sensors to transmit data to a central server, causing delays\nor network disruptions in dense environments. Running IDSs locally on IoMT\ndevices is often infeasible due to limited computation, and even lightweight\nIDS components remain at risk if updated models are delayed leaving them\nexposed to zero-day attacks that threaten patient health and data security. We\npropose a multi level IoMT IDS framework capable of detecting zero day attacks\nand distinguishing between known and unknown threats. The first layer (near\nEdge) filters traffic at a coarse level (attack or not) using meta-learning or\nOne Class Classification (OCC) with the usfAD algorithm. Subsequent layers (far\nEdge, Cloud) identify attack type and novelty. Experiments on the CICIoMT2024\ndataset show 99.77 percentage accuracy and 97.8 percentage F1-score. The first\nlayer detects zero-day attacks with high accuracy without needing new datasets,\nensuring strong applicability in IoMT environments. Additionally, the\nmeta-learning approach achieves high."
    },
    {
        "date": "2025-08",
        "title": "A Vision-Language Pre-training Model-Guided Approach for Mitigating Backdoor Attacks in Federated Learning",
        "author": "Keke Gai, Dongjue Wang, Jing Yu, Liehuang Zhu, and Qi Wu",
        "link": "http://arxiv.org/abs/2508.10315v1",
        "abstract": "Existing backdoor defense methods in Federated Learning (FL) rely on the\nassumption of homogeneous client data distributions or the availability of a\nclean serve dataset, which limits the practicality and effectiveness. Defending\nagainst backdoor attacks under heterogeneous client data distributions while\npreserving model performance remains a significant challenge. In this paper, we\npropose a FL backdoor defense framework named CLIP-Fed, which leverages the\nzero-shot learning capabilities of vision-language pre-training models. By\nintegrating both pre-aggregation and post-aggregation defense strategies,\nCLIP-Fed overcomes the limitations of Non-IID imposed on defense effectiveness.\nTo address privacy concerns and enhance the coverage of the dataset against\ndiverse triggers, we construct and augment the server dataset using the\nmultimodal large language model and frequency analysis without any client\nsamples. To address class prototype deviations caused by backdoor samples and\neliminate the correlation between trigger patterns and target labels, CLIP-Fed\naligns the knowledge of the global model and CLIP on the augmented dataset\nusing prototype contrastive loss and Kullback-Leibler divergence. Extensive\nexperiments on representative datasets validate the effectiveness of CLIP-Fed.\nCompared to state-of-the-art methods, CLIP-Fed achieves an average reduction in\nASR, i.e., 2.03\\% on CIFAR-10 and 1.35\\% on CIFAR-10-LT, while improving\naverage MA by 7.92\\% and 0.48\\%, respectively."
    },
    {
        "date": "2025-08",
        "title": "Pose-Robust Calibration Strategy for Point-of-Gaze Estimation on Mobile Phones",
        "author": "Yujie Zhao, Jiabei Zeng, and Shiguang Shan",
        "link": "http://arxiv.org/abs/2508.10268v1",
        "abstract": "Although appearance-based point-of-gaze (PoG) estimation has improved, the\nestimators still struggle to generalize across individuals due to personal\ndifferences. Therefore, person-specific calibration is required for accurate\nPoG estimation. However, calibrated PoG estimators are often sensitive to head\npose variations. To address this, we investigate the key factors influencing\ncalibrated estimators and explore pose-robust calibration strategies.\nSpecifically, we first construct a benchmark, MobilePoG, which includes facial\nimages from 32 individuals focusing on designated points under either fixed or\ncontinuously changing head poses. Using this benchmark, we systematically\nanalyze how the diversity of calibration points and head poses influences\nestimation accuracy. Our experiments show that introducing a wider range of\nhead poses during calibration improves the estimator's ability to handle pose\nvariation. Building on this insight, we propose a dynamic calibration strategy\nin which users fixate on calibration points while moving their phones. This\nstrategy naturally introduces head pose variation during a user-friendly and\nefficient calibration process, ultimately producing a better calibrated PoG\nestimator that is less sensitive to head pose variations than those using\nconventional calibration strategies. Codes and datasets are available at our\nproject page."
    },
    {
        "date": "2025-08",
        "title": "DINOMotion: advanced robust tissue motion tracking with DINOv2 in 2D-Cine MRI-guided radiotherapy",
        "author": "Soorena Salari, Catherine Spino, Laurie-Anne Pharand, Fabienne Lathuiliere, Hassan Rivaz, Silvain Beriault, and Yiming Xiao",
        "link": "http://arxiv.org/abs/2508.10260v1",
        "abstract": "Accurate tissue motion tracking is critical to ensure treatment outcome and\nsafety in 2D-Cine MRI-guided radiotherapy. This is typically achieved by\nregistration of sequential images, but existing methods often face challenges\nwith large misalignments and lack of interpretability. In this paper, we\nintroduce DINOMotion, a novel deep learning framework based on DINOv2 with\nLow-Rank Adaptation (LoRA) layers for robust, efficient, and interpretable\nmotion tracking. DINOMotion automatically detects corresponding landmarks to\nderive optimal image registration, enhancing interpretability by providing\nexplicit visual correspondences between sequential images. The integration of\nLoRA layers reduces trainable parameters, improving training efficiency, while\nDINOv2's powerful feature representations offer robustness against large\nmisalignments. Unlike iterative optimization-based methods, DINOMotion directly\ncomputes image registration at test time. Our experiments on volunteer and\npatient datasets demonstrate its effectiveness in estimating both linear and\nnonlinear transformations, achieving Dice scores of 92.07% for the kidney,\n90.90% for the liver, and 95.23% for the lung, with corresponding Hausdorff\ndistances of 5.47 mm, 8.31 mm, and 6.72 mm, respectively. DINOMotion processes\neach scan in approximately 30ms and consistently outperforms state-of-the-art\nmethods, particularly in handling large misalignments. These results highlight\nits potential as a robust and interpretable solution for real-time motion\ntracking in 2D-Cine MRI-guided radiotherapy."
    },
    {
        "date": "2025-08",
        "title": "Pruning and Malicious Injection: A Retraining-Free Backdoor Attack on Transformer Models",
        "author": "Taibiao Zhao, Mingxuan Sun, Hao Wang, Xiaobing Chen, and Xiangwei Zhou",
        "link": "http://arxiv.org/abs/2508.10243v1",
        "abstract": "Transformer models have demonstrated exceptional performance and have become\nindispensable in computer vision (CV) and natural language processing (NLP)\ntasks. However, recent studies reveal that transformers are susceptible to\nbackdoor attacks. Prior backdoor attack methods typically rely on retraining\nwith clean data or altering the model architecture, both of which can be\nresource-intensive and intrusive. In this paper, we propose Head-wise Pruning\nand Malicious Injection (HPMI), a novel retraining-free backdoor attack on\ntransformers that does not alter the model's architecture. Our approach\nrequires only a small subset of the original data and basic knowledge of the\nmodel architecture, eliminating the need for retraining the target transformer.\nTechnically, HPMI works by pruning the least important head and injecting a\npre-trained malicious head to establish the backdoor. We provide a rigorous\ntheoretical justification demonstrating that the implanted backdoor resists\ndetection and removal by state-of-the-art defense techniques, under reasonable\nassumptions. Experimental evaluations across multiple datasets further validate\nthe effectiveness of HPMI, showing that it 1) incurs negligible clean accuracy\nloss, 2) achieves at least 99.55% attack success rate, and 3) bypasses four\nadvanced defense mechanisms. Additionally, relative to state-of-the-art\nretraining-dependent attacks, HPMI achieves greater concealment and robustness\nagainst diverse defense strategies, while maintaining minimal impact on clean\naccuracy."
    },
    {
        "date": "2025-08",
        "title": "Detecting Untargeted Attacks and Mitigating Unreliable Updates in Federated Learning for Underground Mining Operations",
        "author": "Md Sazedur Rahman, Mohamed Elmahallawy, Sanjay Madria, and Samuel Frimpong",
        "link": "http://arxiv.org/abs/2508.10212v1",
        "abstract": "Underground mining operations rely on distributed sensor networks to collect\ncritical data daily, including mine temperature, toxic gas concentrations, and\nminer movements for hazard detection and operational decision-making. However,\ntransmitting raw sensor data to a central server for training deep learning\nmodels introduces significant privacy risks, potentially exposing sensitive\nmine-specific information. Federated Learning (FL) offers a transformative\nsolution by enabling collaborative model training while ensuring that raw data\nremains localized at each mine. Despite its advantages, FL in underground\nmining faces key challenges: (i) An attacker may compromise a mine's local\nmodel by employing techniques such as sign-flipping attacks or additive noise,\nleading to erroneous predictions; (ii) Low-quality (yet potentially valuable)\ndata, caused by poor lighting conditions or sensor inaccuracies in mines may\ndegrade the FL training process. In response, this paper proposes MineDetect, a\ndefense FL framework that detects and isolates the attacked models while\nmitigating the impact of mines with low-quality data. MineDetect introduces two\nkey innovations: (i) Detecting attacked models (maliciously manipulated) by\ndeveloping a history-aware mechanism that leverages local and global averages\nof gradient updates; (ii) Identifying and eliminating adversarial influences\nfrom unreliable models (generated by clients with poor data quality) on the FL\ntraining process. Comprehensive simulations across diverse datasets demonstrate\nthat MineDetect outperforms existing methods in both robustness and accuracy,\neven in challenging non-IID data scenarios. Its ability to counter adversarial\ninfluences while maintaining lower computational efficiency makes it a vital\nadvancement for improving safety and operational effectiveness in underground\nmining."
    },
    {
        "date": "2025-08",
        "title": "Empowering Morphing Attack Detection using Interpretable Image-Text Foundation Model",
        "author": "Sushrut Patwardhan, Raghavendra Ramachandra, and Sushma Venkatesh",
        "link": "http://arxiv.org/abs/2508.10110v1",
        "abstract": "Morphing attack detection has become an essential component of face\nrecognition systems for ensuring a reliable verification scenario. In this\npaper, we present a multimodal learning approach that can provide a textual\ndescription of morphing attack detection. We first show that zero-shot\nevaluation of the proposed framework using Contrastive Language-Image\nPretraining (CLIP) can yield not only generalizable morphing attack detection,\nbut also predict the most relevant text snippet. We present an extensive\nanalysis of ten different textual prompts that include both short and long\ntextual prompts. These prompts are engineered by considering the human\nunderstandable textual snippet. Extensive experiments were performed on a face\nmorphing dataset that was developed using a publicly available face biometric\ndataset. We present an evaluation of SOTA pre-trained neural networks together\nwith the proposed framework in the zero-shot evaluation of five different\nmorphing generation techniques that are captured in three different mediums."
    },
    {
        "date": "2025-08",
        "title": "Amazon Nova AI Challenge -- Trusted AI: Advancing secure, AI-assisted software development",
        "author": "Sattvik Sahai, Prasoon Goyal, Michael Johnston, Anna Gottardi, Yao Lu, Lucy Hu, Luke Dai, Shaohua Liu, Samyuth Sagi, Hangjie Shi, Desheng Zhang, Lavina Vaz, Leslie Ball, Maureen Murray, Rahul Gupta, and Shankar Ananthakrishna",
        "link": "http://arxiv.org/abs/2508.10108v1",
        "abstract": "AI systems for software development are rapidly gaining prominence, yet\nsignificant challenges remain in ensuring their safety. To address this, Amazon\nlaunched the Trusted AI track of the Amazon Nova AI Challenge, a global\ncompetition among 10 university teams to drive advances in secure AI. In the\nchallenge, five teams focus on developing automated red teaming bots, while the\nother five create safe AI assistants. This challenge provides teams with a\nunique platform to evaluate automated red-teaming and safety alignment methods\nthrough head-to-head adversarial tournaments where red teams have multi-turn\nconversations with the competing AI coding assistants to test their safety\nalignment. Along with this, the challenge provides teams with a feed of high\nquality annotated data to fuel iterative improvement. Throughout the challenge,\nteams developed state-of-the-art techniques, introducing novel approaches in\nreasoning-based safety alignment, robust model guardrails, multi-turn\njail-breaking, and efficient probing of large language models (LLMs). To\nsupport these efforts, the Amazon Nova AI Challenge team made substantial\nscientific and engineering investments, including building a custom baseline\ncoding specialist model for the challenge from scratch, developing a tournament\norchestration service, and creating an evaluation harness. This paper outlines\nthe advancements made by university teams and the Amazon Nova AI Challenge team\nin addressing the safety challenges of AI for software development,\nhighlighting this collaborative effort to raise the bar for AI safety."
    },
    {
        "date": "2025-08",
        "title": "IPG: Incremental Patch Generation for Generalized Adversarial Patch Training",
        "author": "Wonho Lee, Hyunsik Na, Jisu Lee, and Daeseon Choi",
        "link": "http://arxiv.org/abs/2508.10946v1",
        "abstract": "The advent of adversarial patches poses a significant challenge to the\nrobustness of AI models, particularly in the domain of computer vision tasks\nsuch as object detection. In contradistinction to traditional adversarial\nexamples, these patches target specific regions of an image, resulting in the\nmalfunction of AI models. This paper proposes Incremental Patch Generation\n(IPG), a method that generates adversarial patches up to 11.1 times more\nefficiently than existing approaches while maintaining comparable attack\nperformance. The efficacy of IPG is demonstrated by experiments and ablation\nstudies including YOLO's feature distribution visualization and adversarial\ntraining results, which show that it produces well-generalized patches that\neffectively cover a broader range of model vulnerabilities. Furthermore,\nIPG-generated datasets can serve as a robust knowledge foundation for\nconstructing a robust model, enabling structured representation, advanced\nreasoning, and proactive defenses in AI security ecosystems. The findings of\nthis study suggest that IPG has considerable potential for future utilization\nnot only in adversarial patch defense but also in real-world applications such\nas autonomous vehicles, security systems, and medical imaging, where AI models\nmust remain resilient to adversarial attacks in dynamic and high-stakes\nenvironments."
    },
    {
        "date": "2025-08",
        "title": "Dynamic Multi-Agent System with Stable Maneuvering for Robust GAIA Problem Solving by AWorld",
        "author": "Zhitian Xie, Qintong Wu, Chengyue Yu, Chenyi Zhuang, and Jinjie Gu",
        "link": "http://arxiv.org/abs/2508.09889v2",
        "abstract": "The rapid advancement of large language models (LLMs) has empowered\nintelligent agents to leverage diverse external tools for solving complex\nreal-world problems. However, as agents increasingly depend on multiple tools,\nthey encounter new challenges: extended contexts from disparate sources and\nnoisy or irrelevant tool outputs can undermine system reliability and accuracy.\nThese challenges underscore the necessity for enhanced stability in agent-based\nsystems. To address this, we introduce dynamic supervision and maneuvering\nmechanisms, constructing a robust and dynamic Multi-Agent System (MAS)\narchitecture within the AWorld framework. In our approach, the Execution Agent\ninvokes the Guard Agent at critical steps to verify and correct the reasoning\nprocess, effectively reducing errors arising from noise and bolstering\nproblem-solving robustness. Extensive experiments on the GAIA test dataset\nreveal that our dynamic maneuvering mechanism significantly improves both the\neffectiveness and stability of solutions, outperforming single-agent system\n(SAS) and standard tool-augmented systems. As a result, our dynamic MAS system\nachieved first place among open-source projects on the prestigious GAIA\nleaderboard. These findings highlight the practical value of collaborative\nagent roles in developing more reliable and trustworthy intelligent systems."
    },
    {
        "date": "2025-08",
        "title": "On the Generalization Limits of Quantum Generative Adversarial Networks with Pure State Generators",
        "author": "Jasmin Frkatovic, Akash Malemath, Ivan Kankeu, Yannick Werner, Matthias Tsch\u00f6pe, Vitor Fortes Rey, Sungho Suh, Paul Lukowicz, Nikolaos Palaiodimopoulos, and Maximilian Kiefer-Emmanouilidis",
        "link": "http://arxiv.org/abs/2508.09844v1",
        "abstract": "We investigate the capabilities of Quantum Generative Adversarial Networks\n(QGANs) in image generations tasks. Our analysis centers on fully quantum\nimplementations of both the generator and discriminator. Through extensive\nnumerical testing of current main architectures, we find that QGANs struggle to\ngeneralize across datasets, converging on merely the average representation of\nthe training data. When the output of the generator is a pure-state, we\nanalytically derive a lower bound for the discriminator quality given by the\nfidelity between the pure-state output of the generator and the target data\ndistribution, thereby providing a theoretical explanation for the limitations\nobserved in current models. Our findings reveal fundamental challenges in the\ngeneralization capabilities of existing quantum generative models. While our\nanalysis focuses on QGANs, the results carry broader implications for the\nperformance of related quantum generative models."
    },
    {
        "date": "2025-08",
        "title": "Robustness analysis of Deep Sky Objects detection models on HPC",
        "author": "Olivier Parisot, and Diogo Ramalho Fernandes",
        "link": "http://arxiv.org/abs/2508.09831v1",
        "abstract": "Astronomical surveys and the growing involvement of amateur astronomers are\nproducing more sky images than ever before, and this calls for automated\nprocessing methods that are accurate and robust. Detecting Deep Sky Objects --\nsuch as galaxies, nebulae, and star clusters -- remains challenging because of\ntheir faint signals and complex backgrounds. Advances in Computer Vision and\nDeep Learning now make it possible to improve and automate this process. In\nthis paper, we present the training and comparison of different detection\nmodels (YOLO, RET-DETR) on smart telescope images, using High-Performance\nComputing (HPC) to parallelise computations, in particular for robustness\ntesting."
    },
    {
        "date": "2025-08",
        "title": "Extending the OWASP Multi-Agentic System Threat Modeling Guide: Insights from Multi-Agent Security Research",
        "author": "Klaudia Krawiecka, and Christian Schroeder de Witt",
        "link": "http://arxiv.org/abs/2508.09815v1",
        "abstract": "We propose an extension to the OWASP Multi-Agentic System (MAS) Threat\nModeling Guide, translating recent anticipatory research in multi-agent\nsecurity (MASEC) into practical guidance for addressing challenges unique to\nlarge language model (LLM)-driven multi-agent architectures. Although OWASP's\nexisting taxonomy covers many attack vectors, our analysis identifies gaps in\nmodeling failures, including, but not limited to: reasoning collapse across\nplanner-executor chains, metric overfitting, unsafe delegation escalation,\nemergent covert coordination, and heterogeneous multi-agent exploits. We\nintroduce additional threat classes and scenarios grounded in practical MAS\ndeployments, highlighting risks from benign goal drift, cross-agent\nhallucination propagation, affective prompt framing, and multi-agent backdoors.\nWe also outline evaluation strategies, including robustness testing,\ncoordination assessment, safety enforcement, and emergent behavior monitoring,\nto ensure complete coverage. This work complements the framework of OWASP by\nexpanding its applicability to increasingly complex, autonomous, and adaptive\nmulti-agent systems, with the goal of improving security posture and resilience\nin real world deployments."
    },
    {
        "date": "2025-08",
        "title": "Improving the Speaker Anonymization Evaluation's Robustness to Target Speakers with Adversarial Learning",
        "author": "Carlos Franzreb, Arnab Das, Tim Polzehl, and Sebastian M\u00f6ller",
        "link": "http://arxiv.org/abs/2508.09803v1",
        "abstract": "The current privacy evaluation for speaker anonymization often overestimates\nprivacy when a same-gender target selection algorithm (TSA) is used, although\nthis TSA leaks the speaker's gender and should hence be more vulnerable. We\nhypothesize that this occurs because the evaluation does not account for the\nfact that anonymized speech contains information from both the source and\ntarget speakers. To address this, we propose to add a target classifier that\nmeasures the influence of target speaker information in the evaluation, which\ncan also be removed with adversarial learning. Experiments demonstrate that\nthis approach is effective for multiple anonymizers, particularly when using a\nsame-gender TSA, leading to a more reliable assessment."
    },
    {
        "date": "2025-08",
        "title": "Perfect message authentication codes are robust to small deviations from uniform key distributions",
        "author": "Boris Ryabko",
        "link": "http://arxiv.org/abs/2508.09783v1",
        "abstract": "We investigate the impact of (possible) deviations of the probability\ndistribution of key values from a uniform distribution for the\ninformation-theoretic strong, or perfect, message authentication code. We found\na simple expression for the decrease in security as a function of the\nstatistical distance between the real key probability distribution and the\nuniform one. In a sense, a perfect message authentication code is robust to\nsmall deviations from a uniform key distribution."
    },
    {
        "date": "2025-08",
        "title": "Succinct Oblivious Tensor Evaluation and Applications: Adaptively-Secure Laconic Function Evaluation and Trapdoor Hashing for All Circuits",
        "author": "Damiano Abram, Giulio Malavolta, and Lawrence Roy",
        "link": "http://arxiv.org/abs/2508.09673v1",
        "abstract": "We propose the notion of succinct oblivious tensor evaluation (OTE), where\ntwo parties compute an additive secret sharing of a tensor product of two\nvectors $\\mathbf{x} \\otimes \\mathbf{y}$, exchanging two simultaneous messages.\nCrucially, the size of both messages and of the CRS is independent of the\ndimension of $\\mathbf{x}$.\n  We present a construction of OTE with optimal complexity from the standard\nlearning with errors (LWE) problem. Then we show how this new technical tool\nenables a host of cryptographic primitives, all with security reducible to LWE,\nsuch as:\n  * Adaptively secure laconic function evaluation for depth-$D$ functions\n$f:\\{0, 1\\}^m\\rightarrow\\{0, 1\\}^\\ell$ with communication $m+\\ell+D\\cdot\n\\mathrm{poly}(\\lambda)$.\n  * A trapdoor hash function for all functions.\n  * An (optimally) succinct homomorphic secret sharing for all functions.\n  * A rate-$1/2$ laconic oblivious transfer for batch messages, which is best\npossible.\n  In particular, we obtain the first laconic function evaluation scheme that is\nadaptively secure from the standard LWE assumption, improving upon Quach, Wee,\nand Wichs (FOCS 2018).\n  As a key technical ingredient, we introduce a new notion of \\emph{adaptive\nlattice encodings}, which may be of independent interest."
    },
    {
        "date": "2025-08",
        "title": "Noise-adapted Neural Operator for Robust Non-Line-of-Sight Imaging",
        "author": "Lianfang Wang, Kuilin Qin, Xueying Liu, Huibin Chang, Yong Wang, and Yuping Duan",
        "link": "http://arxiv.org/abs/2508.09655v1",
        "abstract": "Computational imaging, especially non-line-of-sight (NLOS) imaging, the\nextraction of information from obscured or hidden scenes is achieved through\nthe utilization of indirect light signals resulting from multiple reflections\nor scattering. The inherently weak nature of these signals, coupled with their\nsusceptibility to noise, necessitates the integration of physical processes to\nensure accurate reconstruction. This paper presents a parameterized inverse\nproblem framework tailored for large-scale linear problems in 3D imaging\nreconstruction. Initially, a noise estimation module is employed to adaptively\nassess the noise levels present in transient data. Subsequently, a\nparameterized neural operator is developed to approximate the inverse mapping,\nfacilitating end-to-end rapid image reconstruction. Our 3D image reconstruction\nframework, grounded in operator learning, is constructed through deep algorithm\nunfolding, which not only provides commendable model interpretability but also\nenables dynamic adaptation to varying noise levels in the acquired data,\nthereby ensuring consistently robust and accurate reconstruction outcomes.\nFurthermore, we introduce a novel method for the fusion of global and local\nspatiotemporal data features. By integrating structural and detailed\ninformation, this method significantly enhances both accuracy and robustness.\nComprehensive numerical experiments conducted on both simulated and real\ndatasets substantiate the efficacy of the proposed method. It demonstrates\nremarkable performance with fast scanning data and sparse illumination point\ndata, offering a viable solution for NLOS imaging in complex scenarios."
    },
    {
        "date": "2025-08",
        "title": "TOTNet: Occlusion-Aware Temporal Tracking for Robust Ball Detection in Sports Videos",
        "author": "Hao Xu, Arbind Agrahari Baniya, Sam Wells, Mohamed Reda Bouadjenek, Richard Dazely, and Sunil Aryal",
        "link": "http://arxiv.org/abs/2508.09650v1",
        "abstract": "Robust ball tracking under occlusion remains a key challenge in sports video\nanalysis, affecting tasks like event detection and officiating. We present\nTOTNet, a Temporal Occlusion Tracking Network that leverages 3D convolutions,\nvisibility-weighted loss, and occlusion augmentation to improve performance\nunder partial and full occlusions. Developed in collaboration with Paralympics\nAustralia, TOTNet is designed for real-world sports analytics. We introduce\nTTA, a new occlusion-rich table tennis dataset collected from\nprofessional-level Paralympic matches, comprising 9,159 samples with 1,996\nocclusion cases. Evaluated on four datasets across tennis, badminton, and table\ntennis, TOTNet significantly outperforms prior state-of-the-art methods,\nreducing RMSE from 37.30 to 7.19 and improving accuracy on fully occluded\nframes from 0.63 to 0.80. These results demonstrate TOTNets effectiveness for\noffline sports analytics in fast-paced scenarios. Code and data\naccess:\\href{https://github.com/AugustRushG/TOTNet}{AugustRushG/TOTNet}."
    },
    {
        "date": "2025-08",
        "title": "Dual Recursive Feedback on Generation and Appearance Latents for Pose-Robust Text-to-Image Diffusion",
        "author": "Jiwon Kim, Pureum Kim, SeonHwa Kim, Soobin Park, Eunju Cha, and Kyong Hwan Jin",
        "link": "http://arxiv.org/abs/2508.09575v1",
        "abstract": "Recent advancements in controllable text-to-image (T2I) diffusion models,\nsuch as Ctrl-X and FreeControl, have demonstrated robust spatial and appearance\ncontrol without requiring auxiliary module training. However, these models\noften struggle to accurately preserve spatial structures and fail to capture\nfine-grained conditions related to object poses and scene layouts. To address\nthese challenges, we propose a training-free Dual Recursive Feedback (DRF)\nsystem that properly reflects control conditions in controllable T2I models.\nThe proposed DRF consists of appearance feedback and generation feedback that\nrecursively refines the intermediate latents to better reflect the given\nappearance information and the user's intent. This dual-update mechanism guides\nlatent representations toward reliable manifolds, effectively integrating\nstructural and appearance attributes. Our approach enables fine-grained\ngeneration even between class-invariant structure-appearance fusion, such as\ntransferring human motion onto a tiger's form. Extensive experiments\ndemonstrate the efficacy of our method in producing high-quality, semantically\ncoherent, and structurally consistent image generations. Our source code is\navailable at https://github.com/jwonkm/DRF."
    },
    {
        "date": "2025-08",
        "title": "Causal Graph Profiling via Structural Divergence for Robust Anomaly Detection in Cyber-Physical Systems",
        "author": "Arun Vignesh Malarkkan, Haoyue Bai, Dongjie Wang, and Yanjie Fu",
        "link": "http://arxiv.org/abs/2508.09504v1",
        "abstract": "With the growing complexity of cyberattacks targeting critical\ninfrastructures such as water treatment networks, there is a pressing need for\nrobust anomaly detection strategies that account for both system\nvulnerabilities and evolving attack patterns. Traditional methods --\nstatistical, density-based, and graph-based models struggle with distribution\nshifts and class imbalance in multivariate time series, often leading to high\nfalse positive rates. To address these challenges, we propose CGAD, a Causal\nGraph-based Anomaly Detection framework designed for reliable cyberattack\ndetection in public infrastructure systems. CGAD follows a two-phase supervised\nframework -- causal profiling and anomaly scoring. First, it learns causal\ninvariant graph structures representing the system's behavior under \"Normal\"\nand \"Attack\" states using Dynamic Bayesian Networks. Second, it employs\nstructural divergence to detect anomalies via causal graph comparison by\nevaluating topological deviations in causal graphs over time. By leveraging\ncausal structures, CGAD achieves superior adaptability and accuracy in\nnon-stationary and imbalanced time series environments compared to conventional\nmachine learning approaches. By uncovering causal structures beneath volatile\nsensor data, our framework not only detects cyberattacks with markedly higher\nprecision but also redefines robustness in anomaly detection, proving\nresilience where traditional models falter under imbalance and drift. Our\nframework achieves substantial gains in F1 and ROC-AUC scores over\nbest-performing baselines across four industrial datasets, demonstrating robust\ndetection of delayed and structurally complex anomalies."
    },
    {
        "date": "2025-08",
        "title": "Event-driven Robust Fitting on Neuromorphic Hardware",
        "author": "Tam Ngoc-Bang Nguyen, Anh-Dzung Doan, Zhipeng Cai, and Tat-Jun Chin",
        "link": "http://arxiv.org/abs/2508.09466v1",
        "abstract": "Robust fitting of geometric models is a fundamental task in many computer\nvision pipelines. Numerous innovations have been produced on the topic, from\nimproving the efficiency and accuracy of random sampling heuristics to\ngenerating novel theoretical insights that underpin new approaches with\nmathematical guarantees. However, one aspect of robust fitting that has\nreceived little attention is energy efficiency. This performance metric has\nbecome critical as high energy consumption is a growing concern for AI\nadoption. In this paper, we explore energy-efficient robust fitting via the\nneuromorphic computing paradigm. Specifically, we designed a novel spiking\nneural network for robust fitting on real neuromorphic hardware, the Intel\nLoihi 2. Enabling this are novel event-driven formulations of model estimation\nthat allow robust fitting to be implemented in the unique architecture of Loihi\n2, and algorithmic strategies to alleviate the current limited precision and\ninstruction set of the hardware. Results show that our neuromorphic robust\nfitting consumes only a fraction (15%) of the energy required to run the\nestablished robust fitting algorithm on a standard CPU to equivalent accuracy."
    },
    {
        "date": "2025-08",
        "title": "IAG: Input-aware Backdoor Attack on VLMs for Visual Grounding",
        "author": "Junxian Li, Beining Xu, and Di Zhang",
        "link": "http://arxiv.org/abs/2508.09456v1",
        "abstract": "Vision-language models (VLMs) have shown significant advancements in tasks\nsuch as visual grounding, where they localize specific objects in images based\non natural language queries and images. However, security issues in visual\ngrounding tasks for VLMs remain underexplored, especially in the context of\nbackdoor attacks. In this paper, we introduce a novel input-aware backdoor\nattack method, IAG, designed to manipulate the grounding behavior of VLMs. This\nattack forces the model to ground a specific target object in the input image,\nregardless of the user's query. We propose an adaptive trigger generator that\nembeds the semantic information of the attack target's description into the\noriginal image using a text-conditional U-Net, thereby overcoming the\nopen-vocabulary attack challenge. To ensure the attack's stealthiness, we\nutilize a reconstruction loss to minimize visual discrepancies between poisoned\nand clean images. Additionally, we introduce a unified method for generating\nattack data. IAG is evaluated theoretically and empirically, demonstrating its\nfeasibility and effectiveness. Notably, our ASR@0.5 on InternVL-2.5-8B reaches\nover 65\\% on various testing sets. IAG also shows promising potential on\nmanipulating Ferret-7B and LlaVA-1.5-7B with very little accuracy decrease on\nclean samples. Extensive specific experiments, such as ablation study and\npotential defense, also indicate the robustness and transferability of our\nattack."
    },
    {
        "date": "2025-08",
        "title": "Security Analysis of ChatGPT: Threats and Privacy Risks",
        "author": "Yushan Xiang, Zhongwen Li, and Xiaoqi Li",
        "link": "http://arxiv.org/abs/2508.09426v1",
        "abstract": "As artificial intelligence technology continues to advance, chatbots are\nbecoming increasingly powerful. Among them, ChatGPT, launched by OpenAI, has\ngarnered widespread attention globally due to its powerful natural language\nprocessing capabilities based on the GPT model, which enables it to engage in\nnatural conversations with users, understand various forms of linguistic\nexpressions, and generate useful information and suggestions. However, as its\napplication scope expands, user demand grows, and malicious attacks related to\nit become increasingly frequent, the security threats and privacy risks faced\nby ChatGPT are gradually coming to the forefront. In this paper, the security\nof ChatGPT is mainly studied from two aspects, security threats and privacy\nrisks. The article systematically analyzes various types of vulnerabilities\ninvolved in the above two types of problems and their causes. Briefly, we\ndiscuss the controversies that ChatGPT may cause at the ethical and moral\nlevels. In addition, this paper reproduces several network attack and defense\ntest scenarios by simulating the attacker's perspective and methodology.\nSimultaneously, it explores the feasibility of using ChatGPT for security\nvulnerability detection and security tool generation from the defender's\nperspective."
    },
    {
        "date": "2025-08",
        "title": "Can AI Keep a Secret? Contextual Integrity Verification: A Provable Security Architecture for LLMs",
        "author": "Aayush Gupta",
        "link": "http://arxiv.org/abs/2508.09288v2",
        "abstract": "Large language models (LLMs) remain acutely vulnerable to prompt injection\nand related jailbreak attacks; heuristic guardrails (rules, filters, LLM\njudges) are routinely bypassed. We present Contextual Integrity Verification\n(CIV), an inference-time security architecture that attaches cryptographically\nsigned provenance labels to every token and enforces a source-trust lattice\ninside the transformer via a pre-softmax hard attention mask (with optional\nFFN/residual gating). CIV provides deterministic, per-token non-interference\nguarantees on frozen models: lower-trust tokens cannot influence higher-trust\nrepresentations. On benchmarks derived from recent taxonomies of\nprompt-injection vectors (Elite-Attack + SoK-246), CIV attains 0% attack\nsuccess rate under the stated threat model while preserving 93.1% token-level\nsimilarity and showing no degradation in model perplexity on benign tasks; we\nnote a latency overhead attributable to a non-optimized data path. Because CIV\nis a lightweight patch -- no fine-tuning required -- we demonstrate drop-in\nprotection for Llama-3-8B and Mistral-7B. We release a reference\nimplementation, an automated certification harness, and the Elite-Attack corpus\nto support reproducible research."
    },
    {
        "date": "2025-08",
        "title": "Constrained Black-Box Attacks Against Multi-Agent Reinforcement Learning",
        "author": "Amine Andam, Jamal Bentahar, and Mustapha Hedabou",
        "link": "http://arxiv.org/abs/2508.09275v1",
        "abstract": "Collaborative multi-agent reinforcement learning (c-MARL) has rapidly\nevolved, offering state-of-the-art algorithms for real-world applications,\nincluding sensitive domains. However, a key challenge to its widespread\nadoption is the lack of a thorough investigation into its vulnerabilities to\nadversarial attacks. Existing work predominantly focuses on training-time\nattacks or unrealistic scenarios, such as access to policy weights or the\nability to train surrogate policies. In this paper, we investigate new\nvulnerabilities under more realistic and constrained conditions, assuming an\nadversary can only collect and perturb the observations of deployed agents. We\nalso consider scenarios where the adversary has no access at all. We propose\nsimple yet highly effective algorithms for generating adversarial perturbations\ndesigned to misalign how victim agents perceive their environment. Our approach\nis empirically validated on three benchmarks and 22 environments, demonstrating\nits effectiveness across diverse algorithms and environments. Furthermore, we\nshow that our algorithm is sample-efficient, requiring only 1,000 samples\ncompared to the millions needed by previous methods."
    },
    {
        "date": "2025-08",
        "title": "Deep Learning Models for Robust Facial Liveness Detection",
        "author": "Oleksandr Kuznetsov, Emanuele Frontoni, Luca Romeo, Riccardo Rosati, Andrea Maranesi, and Alessandro Muscatello",
        "link": "http://arxiv.org/abs/2508.09094v1",
        "abstract": "In the rapidly evolving landscape of digital security, biometric\nauthentication systems, particularly facial recognition, have emerged as\nintegral components of various security protocols. However, the reliability of\nthese systems is compromised by sophisticated spoofing attacks, where imposters\ngain unauthorized access by falsifying biometric traits. Current literature\nreveals a concerning gap: existing liveness detection methodologies - designed\nto counteract these breaches - fall short against advanced spoofing tactics\nemploying deepfakes and other artificial intelligence-driven manipulations.\nThis study introduces a robust solution through novel deep learning models\naddressing the deficiencies in contemporary anti-spoofing techniques. By\ninnovatively integrating texture analysis and reflective properties associated\nwith genuine human traits, our models distinguish authentic presence from\nreplicas with remarkable precision. Extensive evaluations were conducted across\nfive diverse datasets, encompassing a wide range of attack vectors and\nenvironmental conditions. Results demonstrate substantial advancement over\nexisting systems, with our best model (AttackNet V2.2) achieving 99.9% average\naccuracy when trained on combined data. Moreover, our research unveils critical\ninsights into the behavioral patterns of impostor attacks, contributing to a\nmore nuanced understanding of their evolving nature. The implications are\nprofound: our models do not merely fortify the authentication processes but\nalso instill confidence in biometric systems across various sectors reliant on\nsecure access."
    },
    {
        "date": "2025-08",
        "title": "NetMoniAI: An Agentic AI Framework for Network Security & Monitoring",
        "author": "Pallavi Zambare, Venkata Nikhil Thanikella, Nikhil Padmanabh Kottur, Sree Akhil Akula, and Ying Liu",
        "link": "http://arxiv.org/abs/2508.10052v1",
        "abstract": "In this paper, we present NetMoniAI, an agentic AI framework for automatic\nnetwork monitoring and security that integrates decentralized analysis with\nlightweight centralized coordination. The framework consists of two layers:\nautonomous micro-agents at each node perform local traffic analysis and anomaly\ndetection. A central controller then aggregates insights across nodes to detect\ncoordinated attacks and maintain system-wide situational awareness. We\nevaluated NetMoniAI on a local micro-testbed and through NS-3 simulations.\nResults confirm that the two-tier agentic-AI design scales under resource\nconstraints, reduces redundancy, and improves response time without\ncompromising accuracy. To facilitate broader adoption and reproducibility, the\ncomplete framework is available as open source. This enables researchers and\npractitioners to replicate, validate, and extend it across diverse network\nenvironments and threat scenarios. Github link:\nhttps://github.com/pzambare3/NetMoniAI"
    },
    {
        "date": "2025-08",
        "title": "Attacks and Defenses Against LLM Fingerprinting",
        "author": "Kevin Kurian, Ethan Holland, and Sean Oesch",
        "link": "http://arxiv.org/abs/2508.09021v1",
        "abstract": "As large language models are increasingly deployed in sensitive environments,\nfingerprinting attacks pose significant privacy and security risks. We present\na study of LLM fingerprinting from both offensive and defensive perspectives.\nOur attack methodology uses reinforcement learning to automatically optimize\nquery selection, achieving better fingerprinting accuracy with only 3 queries\ncompared to randomly selecting 3 queries from the same pool. Our defensive\napproach employs semantic-preserving output filtering through a secondary LLM\nto obfuscate model identity while maintaining semantic integrity. The defensive\nmethod reduces fingerprinting accuracy across tested models while preserving\noutput quality. These contributions show the potential to improve\nfingerprinting tools capabilities while providing practical mitigation\nstrategies against fingerprinting attacks."
    },
    {
        "date": "2025-08",
        "title": "Fre-CW: Targeted Attack on Time Series Forecasting using Frequency Domain Loss",
        "author": "Naifu Feng, Lixing Chen, Junhua Tang, Hua Ding, Jianhua Li, and Yang Bai",
        "link": "http://arxiv.org/abs/2508.08955v1",
        "abstract": "Transformer-based models have made significant progress in time series\nforecasting. However, a key limitation of deep learning models is their\nsusceptibility to adversarial attacks, which has not been studied enough in the\ncontext of time series prediction. In contrast to areas such as computer\nvision, where adversarial robustness has been extensively studied, frequency\ndomain features of time series data play an important role in the prediction\ntask but have not been sufficiently explored in terms of adversarial attacks.\nThis paper proposes a time series prediction attack algorithm based on\nfrequency domain loss. Specifically, we adapt an attack method originally\ndesigned for classification tasks to the prediction field and optimize the\nadversarial samples using both time-domain and frequency-domain losses. To the\nbest of our knowledge, there is no relevant research on using frequency\ninformation for time-series adversarial attacks. Our experimental results show\nthat these current time series prediction models are vulnerable to adversarial\nattacks, and our approach achieves excellent performance on major time series\nforecasting datasets."
    },
    {
        "date": "2025-08",
        "title": "Load-Altering Attacks Against Power Grids: A Case Study Using the GB-36 Bus System Open Dataset",
        "author": "Syed Irtiza Maksud, and Subhash Lakshminarayana",
        "link": "http://arxiv.org/abs/2508.08945v1",
        "abstract": "The growing digitalization and the rapid adoption of high-powered\nInternet-of-Things (IoT)-enabled devices (e.g., EV charging stations) have\nincreased the vulnerability of power grids to cyber threats. In particular, the\nso-called Load Altering Attacks (LAAs) can trigger rapid frequency fluctuations\nand potentially destabilize the power grid. This paper aims to bridge the gap\nbetween academic research and practical application by using open-source\ndatasets released by grid operators. It investigates various LAA scenarios on a\nreal-world transmission network, namely the Great Britain (GB)-36 Zone model\nreleased by the UK's National Electricity System Operator (NESO). It evaluates\nthe threshold of LAA severity that the grid can tolerate before triggering\ncascading effects. Additionally, it explores how Battery Energy Storage Systems\n(BESS) based fast frequency response services can mitigate or prevent such\nimpacts. Simulations are conducted using DIgSILENT PowerFactory to ensure\nrealistic system representation. The analysis provides several useful insights\nto grid operators on the LAA impact, such as the influence of the relative\nlocations of BESS and LAA, as well as how delays in attack execution can\ninfluence the overall system response."
    },
    {
        "date": "2025-08",
        "title": "MADPromptS: Unlocking Zero-Shot Morphing Attack Detection with Multiple Prompt Aggregation",
        "author": "Eduarda Caldeira, Fadi Boutros, and Naser Damer",
        "link": "http://arxiv.org/abs/2508.08939v1",
        "abstract": "Face Morphing Attack Detection (MAD) is a critical challenge in face\nrecognition security, where attackers can fool systems by interpolating the\nidentity information of two or more individuals into a single face image,\nresulting in samples that can be verified as belonging to multiple identities\nby face recognition systems. While multimodal foundation models (FMs) like CLIP\noffer strong zero-shot capabilities by jointly modeling images and text, most\nprior works on FMs for biometric recognition have relied on fine-tuning for\nspecific downstream tasks, neglecting their potential for direct, generalizable\ndeployment. This work explores a pure zero-shot approach to MAD by leveraging\nCLIP without any additional training or fine-tuning, focusing instead on the\ndesign and aggregation of multiple textual prompts per class. By aggregating\nthe embeddings of diverse prompts, we better align the model's internal\nrepresentations with the MAD task, capturing richer and more varied cues\nindicative of bona-fide or attack samples. Our results show that prompt\naggregation substantially improves zero-shot detection performance,\ndemonstrating the effectiveness of exploiting foundation models' built-in\nmultimodal knowledge through efficient prompt engineering."
    },
    {
        "date": "2025-08",
        "title": "EGGCodec: A Robust Neural Encodec Framework for EGG Reconstruction and F0 Extraction",
        "author": "Rui Feng, Yuang Chen, Yu Hu, Jun Du, and Jiahong Yuan",
        "link": "http://arxiv.org/abs/2508.08924v1",
        "abstract": "This letter introduces EGGCodec, a robust neural Encodec framework engineered\nfor electroglottography (EGG) signal reconstruction and F0 extraction. We\npropose a multi-scale frequency-domain loss function to capture the nuanced\nrelationship between original and reconstructed EGG signals, complemented by a\ntime-domain correlation loss to improve generalization and accuracy. Unlike\nconventional Encodec models that extract F0 directly from features, EGGCodec\nleverages reconstructed EGG signals, which more closely correspond to F0. By\nremoving the conventional GAN discriminator, we streamline EGGCodec's training\nprocess without compromising efficiency, incurring only negligible performance\ndegradation. Trained on a widely used EGG-inclusive dataset, extensive\nevaluations demonstrate that EGGCodec outperforms state-of-the-art F0\nextraction schemes, reducing mean absolute error (MAE) from 14.14 Hz to 13.69\nHz, and improving voicing decision error (VDE) by 38.2\\%. Moreover, extensive\nablation experiments validate the contribution of each component of EGGCodec."
    },
    {
        "date": "2025-08",
        "title": "Exploring Cross-Stage Adversarial Transferability in Class-Incremental Continual Learning",
        "author": "Jungwoo Kim, and Jong-Seok Lee",
        "link": "http://arxiv.org/abs/2508.08920v1",
        "abstract": "Class-incremental continual learning addresses catastrophic forgetting by\nenabling classification models to preserve knowledge of previously learned\nclasses while acquiring new ones. However, the vulnerability of the models\nagainst adversarial attacks during this process has not been investigated\nsufficiently. In this paper, we present the first exploration of vulnerability\nto stage-transferred attacks, i.e., an adversarial example generated using the\nmodel in an earlier stage is used to attack the model in a later stage. Our\nfindings reveal that continual learning methods are highly susceptible to these\nattacks, raising a serious security issue. We explain this phenomenon through\nmodel similarity between stages and gradual robustness degradation.\nAdditionally, we find that existing adversarial training-based defense methods\nare not sufficiently effective to stage-transferred attacks. Codes are\navailable at https://github.com/mcml-official/CSAT."
    },
    {
        "date": "2025-08",
        "title": "A Robust Epipolar-Domain Regularization Algorithm for Light Field Depth Estimation",
        "author": "Noor Islam S. Mohammad",
        "link": "http://arxiv.org/abs/2508.08900v1",
        "abstract": "Robust depth estimation in light field imaging remains a critical challenge\nfor pattern recognition applications such as augmented reality, biomedical\nimaging, and scene reconstruction. While existing approaches often rely heavily\non deep convolutional neural networks, they tend to incur high computational\ncosts and struggle in noisy real-world environments. This paper proposes a\nnovel lightweight depth estimation pipeline that integrates light field-based\ndisparity information with a directed random walk refinement algorithm. Unlike\ntraditional CNN-based methods, our approach enhances depth map consistency\nwithout requiring extensive training or large-scale datasets. The proposed\nmethod was evaluated on the 4D Light Field Benchmark dataset and a diverse set\nof real-world images. Experimental results indicate that while performance\nslightly declines under uncontrolled conditions, the algorithm consistently\nmaintains low computational complexity and competitive accuracy compared to\nstate-of-the-art deep learning models. These findings highlight the potential\nof our method as a robust and efficient alternative for depth estimation and\nsegmentation in light field imaging. The work provides insights into practical\nalgorithm design for light field-based pattern recognition and opens new\ndirections for integrating probabilistic graph models with depth sensing\nframeworks."
    },
    {
        "date": "2025-08",
        "title": "Flow Battery Manifold Design with Heterogeneous Inputs Through Generative Adversarial Neural Networks",
        "author": "Eric Seng, Hugh O'Connor, Adam Boyce, Josh J. Bailey, and Anton van Beek",
        "link": "http://arxiv.org/abs/2508.08863v1",
        "abstract": "Generative machine learning has emerged as a powerful tool for design\nrepresentation and exploration. However, its application is often constrained\nby the need for large datasets of existing designs and the lack of\ninterpretability about what features drive optimality. To address these\nchallenges, we introduce a systematic framework for constructing training\ndatasets tailored to generative models and demonstrate how these models can be\nleveraged for interpretable design. The novelty of this work is twofold: (i) we\npresent a systematic framework for generating archetypes with internally\nhomogeneous but mutually heterogeneous inputs that can be used to generate a\ntraining dataset, and (ii) we show how integrating generative models with\nBayesian optimization can enhance the interpretability of the latent space of\nadmissible designs. These findings are validated by using the framework to\ndesign a flow battery manifold, demonstrating that it effectively captures the\nspace of feasible designs, including novel configurations while enabling\nefficient exploration. This work broadens the applicability of generative\nmachine-learning models in system designs by enhancing quality and reliability."
    },
    {
        "date": "2025-08",
        "title": "An Investigation of Robustness of LLMs in Mathematical Reasoning: Benchmarking with Mathematically-Equivalent Transformation of Advanced Mathematical Problems",
        "author": "Yuren Hao, Xiang Wan, and Chengxiang Zhai",
        "link": "http://arxiv.org/abs/2508.08833v1",
        "abstract": "In this paper, we introduce a systematic framework beyond conventional method\nto assess LLMs' mathematical-reasoning robustness by stress-testing them on\nadvanced math problems that are mathematically equivalent but with linguistic\nand parametric variation. These transformations allow us to measure the\nsensitivity of LLMs to non-mathematical perturbations, thereby enabling a more\naccurate evaluation of their mathematical reasoning capabilities. Using this\nnew evaluation methodology, we created PutnamGAP, a new benchmark dataset with\nmultiple mathematically-equivalent variations of competition-level math\nproblems. With the new dataset, we evaluate multiple families of representative\nLLMs and examine their robustness. Across 18 commercial and open-source models\nwe observe sharp performance degradation on the variants. OpenAI's flagship\nreasoning model, O3, scores 49 % on the originals but drops by 4 percentage\npoints on surface variants, and by 10.5 percentage points on core-step-based\nvariants, while smaller models fare far worse. Overall, the results show that\nthe proposed new evaluation methodology is effective for deepening our\nunderstanding of the robustness of LLMs and generating new insights for further\nimproving their mathematical reasoning capabilities."
    },
    {
        "date": "2025-08",
        "title": "Generative Modeling for Robust Deep Reinforcement Learning on the Traveling Salesman Problem",
        "author": "Michael Li, Eric Bae, Christopher Haberland, and Natasha Jaques",
        "link": "http://arxiv.org/abs/2508.08718v1",
        "abstract": "The Traveling Salesman Problem (TSP) is a classic NP-hard combinatorial\noptimization task with numerous practical applications. Classic heuristic\nsolvers can attain near-optimal performance for small problem instances, but\nbecome computationally intractable for larger problems. Real-world logistics\nproblems such as dynamically re-routing last-mile deliveries demand a solver\nwith fast inference time, which has led researchers to investigate specialized\nneural network solvers. However, neural networks struggle to generalize beyond\nthe synthetic data they were trained on. In particular, we show that there\nexist TSP distributions that are realistic in practice, which also consistently\nlead to poor worst-case performance for existing neural approaches. To address\nthis issue of distribution robustness, we present Combinatorial Optimization\nwith Generative Sampling (COGS), where training data is sampled from a\ngenerative TSP model. We show that COGS provides better data coverage and\ninterpolation in the space of TSP training distributions. We also present\nTSPLib50, a dataset of realistically distributed TSP samples, which tests\nreal-world generalization ability without conflating this issue with instance\nsize. We evaluate our method on various synthetic datasets as well as TSPLib50,\nand compare to state-of-the-art neural baselines. We demonstrate that COGS\nimproves distribution robustness, with most performance gains coming from\nworst-case scenarios."
    },
    {
        "date": "2025-08",
        "title": "Evasive Ransomware Attacks Using Low-level Behavioral Adversarial Examples",
        "author": "Manabu Hirano, and Ryotaro Kobayashi",
        "link": "http://arxiv.org/abs/2508.08656v1",
        "abstract": "Protecting state-of-the-art AI-based cybersecurity defense systems from cyber\nattacks is crucial. Attackers create adversarial examples by adding small\nchanges (i.e., perturbations) to the attack features to evade or fool the deep\nlearning model. This paper introduces the concept of low-level behavioral\nadversarial examples and its threat model of evasive ransomware. We formulate\nthe method and the threat model to generate the optimal source code of evasive\nmalware. We then examine the method using the leaked source code of Conti\nransomware with the micro-behavior control function. The micro-behavior control\nfunction is our test component to simulate changing source code in ransomware;\nransomware's behavior can be changed by specifying the number of threads, file\nencryption ratio, and delay after file encryption at the boot time. We\nevaluated how much an attacker can control the behavioral features of\nransomware using the micro-behavior control function to decrease the detection\nrate of a ransomware detector."
    },
    {
        "date": "2025-08",
        "title": "AME: Aligned Manifold Entropy for Robust Vision-Language Distillation",
        "author": "Guiming Cao, and Yuming Ou",
        "link": "http://arxiv.org/abs/2508.08644v1",
        "abstract": "Knowledge distillation is a long-established technique for knowledge\ntransfer, and has regained attention in the context of the recent emergence of\nlarge vision-language models (VLMs). However, vision-language knowledge\ndistillation often requires sufficient training data to achieve robust\ngeneralization on amples with ambiguous or boundary-adjacent representations,\nwhich are associated with high predictive uncertainty. Critically, collecting\nsuch large-scale, task-specific data for training is often impractical in\nreal-world scenarios. To address this major challenge arising from the\nentanglement of uncertainty and cross-modal feature representation, we propose\nAligned Manifold Entropy for Robust Vision-Language Distillation (AME), aiming\nto achieve robust generalization under real-world conditions. AME applies\nentropy minimization over a reconfigured shared manifold, where multi-modal\ndata (i.e., image and text) are bridged through a pair of projection functions,\nconducive to structural compression for cross-modal feature representations.\nThis enables robust knowledge distillation under low-data regimes, while\nrequiring no architectural modifications to the backbone. As a result, it can\nserve as a plug-and-play module compatible with a wide range of vision-language\ndistillation frameworks. Notably, our theoretical analysis reveals that\nintegrating knowledge distillation with entropy minimization over the shared\nmanifold leads to a tighter generalization error bound. Extensive experiments\nacross diverse distillation architectures and training settings demonstrate\nthat AME consistently facilitates robust knowledge distillation, resulting in\nsuperior generalization performance across a wide spectrum of downstream tasks."
    },
    {
        "date": "2025-08",
        "title": "Securing Educational LLMs: A Generalised Taxonomy of Attacks on LLMs and DREAD Risk Assessment",
        "author": "Farzana Zahid, Anjalika Sewwandi, Lee Brandon, Vimal Kumar, and Roopak Sinha",
        "link": "http://arxiv.org/abs/2508.08629v1",
        "abstract": "Due to perceptions of efficiency and significant productivity gains, various\norganisations, including in education, are adopting Large Language Models\n(LLMs) into their workflows. Educator-facing, learner-facing, and\ninstitution-facing LLMs, collectively, Educational Large Language Models\n(eLLMs), complement and enhance the effectiveness of teaching, learning, and\nacademic operations. However, their integration into an educational setting\nraises significant cybersecurity concerns. A comprehensive landscape of\ncontemporary attacks on LLMs and their impact on the educational environment is\nmissing. This study presents a generalised taxonomy of fifty attacks on LLMs,\nwhich are categorized as attacks targeting either models or their\ninfrastructure. The severity of these attacks is evaluated in the educational\nsector using the DREAD risk assessment framework. Our risk assessment indicates\nthat token smuggling, adversarial prompts, direct injection, and multi-step\njailbreak are critical attacks on eLLMs. The proposed taxonomy, its application\nin the educational environment, and our risk assessment will help academic and\nindustrial practitioners to build resilient solutions that protect learners and\ninstitutions."
    },
    {
        "date": "2025-08",
        "title": "AI Security Map: Holistic Organization of AI Security Technologies and Impacts on Stakeholders",
        "author": "Hiroya Kato, Kentaro Kita, Kento Hasegawa, and Seira Hidano",
        "link": "http://arxiv.org/abs/2508.08583v1",
        "abstract": "As the social implementation of AI has been steadily progressing, research\nand development related to AI security has also been increasing. However,\nexisting studies have been limited to organizing related techniques, attacks,\ndefenses, and risks in terms of specific domains or AI elements. Thus, it\nextremely difficult to understand the relationships among them and how negative\nimpacts on stakeholders are brought about. In this paper, we argue that the\nknowledge, technologies, and social impacts related to AI security should be\nholistically organized to help understand relationships among them. To this\nend, we first develop an AI security map that holistically organizes\ninterrelationships among elements related to AI security as well as negative\nimpacts on information systems and stakeholders. This map consists of the two\naspects, namely the information system aspect (ISA) and the external influence\naspect (EIA). The elements that AI should fulfill within information systems\nare classified under the ISA. The EIA includes elements that affect\nstakeholders as a result of AI being attacked or misused. For each element,\ncorresponding negative impacts are identified. By referring to the AI security\nmap, one can understand the potential negative impacts, along with their causes\nand countermeasures. Additionally, our map helps clarify how the negative\nimpacts on AI-based systems relate to those on stakeholders. We show some\nfindings newly obtained by referring to our map. We also provide several\nrecommendations and open problems to guide future AI security communities."
    },
    {
        "date": "2025-08",
        "title": "Multi-Target Backdoor Attacks Against Speaker Recognition",
        "author": "Alexandrine Fortier, Sonal Joshi, Thomas Thebaud, Jesus Villalba Lopez, Najim Dehak, and Patrick Cardinal",
        "link": "http://arxiv.org/abs/2508.08559v2",
        "abstract": "In this work, we propose a multi-target backdoor attack against speaker\nidentification using position-independent clicking sounds as triggers. Unlike\nprevious single-target approaches, our method targets up to 50 speakers\nsimultaneously, achieving success rates of up to 95.04%. To simulate more\nrealistic attack conditions, we vary the signal-to-noise ratio between speech\nand trigger, demonstrating a trade-off between stealth and effectiveness. We\nfurther extend the attack to the speaker verification task by selecting the\nmost similar training speaker - based on cosine similarity - as a proxy target.\nThe attack is most effective when target and enrolled speaker pairs are highly\nsimilar, reaching success rates of up to 90% in such cases."
    },
    {
        "date": "2025-08",
        "title": "Securing Agentic AI: Threat Modeling and Risk Analysis for Network Monitoring Agentic AI System",
        "author": "Pallavi Zambare, Venkata Nikhil Thanikella, and Ying Liu",
        "link": "http://arxiv.org/abs/2508.10043v1",
        "abstract": "When combining Large Language Models (LLMs) with autonomous agents, used in\nnetwork monitoring and decision-making systems, this will create serious\nsecurity issues. In this research, the MAESTRO framework consisting of the\nseven layers threat modeling architecture in the system was used to expose,\nevaluate, and eliminate vulnerabilities of agentic AI. The prototype agent\nsystem was constructed and implemented, using Python, LangChain, and telemetry\nin WebSockets, and deployed with inference, memory, parameter tuning, and\nanomaly detection modules. Two practical threat cases were confirmed as\nfollows: (i) resource denial of service by traffic replay denial-of-service,\nand (ii) memory poisoning by tampering with the historical log file maintained\nby the agent. These situations resulted in measurable levels of performance\ndegradation, i.e. telemetry updates were delayed, and computational loads were\nincreased, as a result of poor system adaptations. It was suggested to use a\nmultilayered defense-in-depth approach with memory isolation, validation of\nplanners and anomaly response systems in real-time. These findings verify that\nMAESTRO is viable in operational threat mapping, prospective risk scoring, and\nthe basis of the resilient system design. The authors bring attention to the\nimportance of the enforcement of memory integrity, paying attention to the\nadaptation logic monitoring, and cross-layer communication protection that\nguarantee the agentic AI reliability in adversarial settings."
    },
    {
        "date": "2025-08",
        "title": "FIDELIS: Blockchain-Enabled Protection Against Poisoning Attacks in Federated Learning",
        "author": "Jane Carney, Kushal Upreti, Gaby G. Dagher, and Tim Andersen",
        "link": "http://arxiv.org/abs/2508.10042v1",
        "abstract": "Federated learning enhances traditional deep learning by enabling the joint\ntraining of a model with the use of IoT device's private data. It ensures\nprivacy for clients, but is susceptible to data poisoning attacks during\ntraining that degrade model performance and integrity. Current poisoning\ndetection methods in federated learning lack a standardized detection method or\ntake significant liberties with trust. In this paper, we present \\Sys, a novel\nblockchain-enabled poison detection framework in federated learning. The\nframework decentralizes the role of the global server across participating\nclients. We introduce a judge model used to detect data poisoning in model\nupdates. The judge model is produced by each client and verified to reach\nconsensus on a single judge model. We implement our solution to show \\Sys is\nrobust against data poisoning attacks and the creation of our judge model is\nscalable."
    },
    {
        "date": "2025-08",
        "title": "Adaptive Learning for IRS-Assisted Wireless Networks: Securing Opportunistic Communications Against Byzantine Eavesdroppers",
        "author": "Amirhossein Taherpour, Abbas Taherpour, and Tamer Khattab",
        "link": "http://arxiv.org/abs/2508.08206v1",
        "abstract": "We propose a joint learning framework for Byzantine-resilient spectrum\nsensing and secure intelligent reflecting surface (IRS)--assisted opportunistic\naccess under channel state information (CSI) uncertainty. The sensing stage\nperforms logit-domain Bayesian updates with trimmed aggregation and\nattention-weighted consensus, and the base station (BS) fuses network beliefs\nwith a conservative minimum rule, preserving detection accuracy under a bounded\nnumber of Byzantine users. Conditioned on the sensing outcome, we pose downlink\ndesign as sum mean-squared error (MSE) minimization under transmit-power and\nsignal-leakage constraints and jointly optimize the BS precoder, IRS phase\nshifts, and user equalizers. With partial (or known) CSI, we develop an\naugmented-Lagrangian alternating algorithm with projected updates and provide\nprovable sublinear convergence, with accelerated rates under mild local\ncurvature. With unknown CSI, we perform constrained Bayesian optimization (BO)\nin a geometry-aware low-dimensional latent space using Gaussian process (GP)\nsurrogates; we prove regret bounds for a constrained upper confidence bound\n(UCB) variant of the BO module, and demonstrate strong empirical performance of\nthe implemented procedure. Simulations across diverse network conditions show\nhigher detection probability at fixed false-alarm rate under adversarial\nattacks, large reductions in sum MSE for honest users, strong suppression of\neavesdropper signal power, and fast convergence. The framework offers a\npractical path to secure opportunistic communication that adapts to CSI\navailability while coherently coordinating sensing and transmission through\njoint learning."
    },
    {
        "date": "2025-08",
        "title": "Pindrop it! Audio and Visual Deepfake Countermeasures for Robust Detection and Fine Grained-Localization",
        "author": "Nicholas Klein, Hemlata Tak, James Fullwood, Krishna Regmi, Leonidas Spinoulas, Ganesh Sivaraman, Tianxiang Chen, and Elie Khoury",
        "link": "http://arxiv.org/abs/2508.08141v1",
        "abstract": "The field of visual and audio generation is burgeoning with new\nstate-of-the-art methods. This rapid proliferation of new techniques\nunderscores the need for robust solutions for detecting synthetic content in\nvideos. In particular, when fine-grained alterations via localized\nmanipulations are performed in visual, audio, or both domains, these subtle\nmodifications add challenges to the detection algorithms. This paper presents\nsolutions for the problems of deepfake video classification and localization.\nThe methods were submitted to the ACM 1M Deepfakes Detection Challenge,\nachieving the best performance in the temporal localization task and a top four\nranking in the classification task for the TestA split of the evaluation\ndataset."
    },
    {
        "date": "2025-08",
        "title": "BlindGuard: Safeguarding LLM-based Multi-Agent Systems under Unknown Attacks",
        "author": "Rui Miao, Yixin Liu, Yili Wang, Xu Shen, Yue Tan, Yiwei Dai, Shirui Pan, and Xin Wang",
        "link": "http://arxiv.org/abs/2508.08127v1",
        "abstract": "The security of LLM-based multi-agent systems (MAS) is critically threatened\nby propagation vulnerability, where malicious agents can distort collective\ndecision-making through inter-agent message interactions. While existing\nsupervised defense methods demonstrate promising performance, they may be\nimpractical in real-world scenarios due to their heavy reliance on labeled\nmalicious agents to train a supervised malicious detection model. To enable\npractical and generalizable MAS defenses, in this paper, we propose BlindGuard,\nan unsupervised defense method that learns without requiring any\nattack-specific labels or prior knowledge of malicious behaviors. To this end,\nwe establish a hierarchical agent encoder to capture individual, neighborhood,\nand global interaction patterns of each agent, providing a comprehensive\nunderstanding for malicious agent detection. Meanwhile, we design a\ncorruption-guided detector that consists of directional noise injection and\ncontrastive learning, allowing effective detection model training solely on\nnormal agent behaviors. Extensive experiments show that BlindGuard effectively\ndetects diverse attack types (i.e., prompt injection, memory poisoning, and\ntool attack) across MAS with various communication patterns while maintaining\nsuperior generalizability compared to supervised baselines. The code is\navailable at: https://github.com/MR9812/BlindGuard."
    },
    {
        "date": "2025-08",
        "title": "IPBA: Imperceptible Perturbation Backdoor Attack in Federated Self-Supervised Learning",
        "author": "Jiayao Wang, Yang Song, Zhendong Zhao, Jiale Zhang, Qilin Wu, Junwu Zhu, and Dongfang Zhao",
        "link": "http://arxiv.org/abs/2508.08031v1",
        "abstract": "Federated self-supervised learning (FSSL) combines the advantages of\ndecentralized modeling and unlabeled representation learning, serving as a\ncutting-edge paradigm with strong potential for scalability and privacy\npreservation. Although FSSL has garnered increasing attention, research\nindicates that it remains vulnerable to backdoor attacks. Existing methods\ngenerally rely on visually obvious triggers, which makes it difficult to meet\nthe requirements for stealth and practicality in real-world deployment. In this\npaper, we propose an imperceptible and effective backdoor attack method against\nFSSL, called IPBA. Our empirical study reveals that existing imperceptible\ntriggers face a series of challenges in FSSL, particularly limited\ntransferability, feature entanglement with augmented samples, and\nout-of-distribution properties. These issues collectively undermine the\neffectiveness and stealthiness of traditional backdoor attacks in FSSL. To\novercome these challenges, IPBA decouples the feature distributions of backdoor\nand augmented samples, and introduces Sliced-Wasserstein distance to mitigate\nthe out-of-distribution properties of backdoor samples, thereby optimizing the\ntrigger generation process. Our experimental results on several FSSL scenarios\nand datasets show that IPBA significantly outperforms existing backdoor attack\nmethods in performance and exhibits strong robustness under various defense\nmechanisms."
    },
    {
        "date": "2025-08",
        "title": "Robust Anomaly Detection in O-RAN: Leveraging LLMs against Data Manipulation Attacks",
        "author": "Thusitha Dayaratne, Ngoc Duy Pham, Viet Vo, Shangqi Lai, Sharif Abuadbba, Hajime Suzuki, Xingliang Yuan, and Carsten Rudolph",
        "link": "http://arxiv.org/abs/2508.08029v1",
        "abstract": "The introduction of 5G and the Open Radio Access Network (O-RAN) architecture\nhas enabled more flexible and intelligent network deployments. However, the\nincreased complexity and openness of these architectures also introduce novel\nsecurity challenges, such as data manipulation attacks on the semi-standardised\nShared Data Layer (SDL) within the O-RAN platform through malicious xApps. In\nparticular, malicious xApps can exploit this vulnerability by introducing\nsubtle Unicode-wise alterations (hypoglyphs) into the data that are being used\nby traditional machine learning (ML)-based anomaly detection methods. These\nUnicode-wise manipulations can potentially bypass detection and cause failures\nin anomaly detection systems based on traditional ML, such as AutoEncoders,\nwhich are unable to process hypoglyphed data without crashing. We investigate\nthe use of Large Language Models (LLMs) for anomaly detection within the O-RAN\narchitecture to address this challenge. We demonstrate that LLM-based xApps\nmaintain robust operational performance and are capable of processing\nmanipulated messages without crashing. While initial detection accuracy\nrequires further improvements, our results highlight the robustness of LLMs to\nadversarial attacks such as hypoglyphs in input data. There is potential to use\ntheir adaptability through prompt engineering to further improve the accuracy,\nalthough this requires further research. Additionally, we show that LLMs\nachieve low detection latency (under 0.07 seconds), making them suitable for\nNear-Real-Time (Near-RT) RIC deployments."
    },
    {
        "date": "2025-08",
        "title": "TrackOR: Towards Personalized Intelligent Operating Rooms Through Robust Tracking",
        "author": "Tony Danjun Wang, Christian Heiliger, Nassir Navab, and Lennart Bastian",
        "link": "http://arxiv.org/abs/2508.07968v1",
        "abstract": "Providing intelligent support to surgical teams is a key frontier in\nautomated surgical scene understanding, with the long-term goal of improving\npatient outcomes. Developing personalized intelligence for all staff members\nrequires maintaining a consistent state of who is located where for long\nsurgical procedures, which still poses numerous computational challenges. We\npropose TrackOR, a framework for tackling long-term multi-person tracking and\nre-identification in the operating room. TrackOR uses 3D geometric signatures\nto achieve state-of-the-art online tracking performance (+11% Association\nAccuracy over the strongest baseline), while also enabling an effective offline\nrecovery process to create analysis-ready trajectories. Our work shows that by\nleveraging 3D geometric information, persistent identity tracking becomes\nattainable, enabling a critical shift towards the more granular, staff-centric\nanalyses required for personalized intelligent systems in the operating room.\nThis new capability opens up various applications, including our proposed\ntemporal pathway imprints that translate raw tracking data into actionable\ninsights for improving team efficiency and safety and ultimately providing\npersonalized support."
    },
    {
        "date": "2025-08",
        "title": "VOIDFace: A Privacy-Preserving Multi-Network Face Recognition With Enhanced Security",
        "author": "Ajnas Muhammed, Iurri Medvedev, and Nuno Gon\u00e7alves",
        "link": "http://arxiv.org/abs/2508.07960v1",
        "abstract": "Advancement of machine learning techniques, combined with the availability of\nlarge-scale datasets, has significantly improved the accuracy and efficiency of\nfacial recognition. Modern facial recognition systems are trained using large\nface datasets collected from diverse individuals or public repositories.\nHowever, for training, these datasets are often replicated and stored in\nmultiple workstations, resulting in data replication, which complicates\ndatabase management and oversight. Currently, once a user submits their face\nfor dataset preparation, they lose control over how their data is used, raising\nsignificant privacy and ethical concerns. This paper introduces VOIDFace, a\nnovel framework for facial recognition systems that addresses two major issues.\nFirst, it eliminates the need of data replication and improves data control to\nsecurely store training face data by using visual secret sharing. Second, it\nproposes a patch-based multi-training network that uses this novel training\ndata storage mechanism to develop a robust, privacy-preserving facial\nrecognition system. By integrating these advancements, VOIDFace aims to improve\nthe privacy, security, and efficiency of facial recognition training, while\nensuring greater control over sensitive personal face data. VOIDFace also\nenables users to exercise their Right-To-Be-Forgotten property to control their\npersonal data. Experimental evaluations on the VGGFace2 dataset show that\nVOIDFace provides Right-To-Be-Forgotten, improved data control, security, and\nprivacy while maintaining competitive facial recognition performance. Code is\navailable at: https://github.com/ajnasmuhammed89/VOIDFace"
    },
    {
        "date": "2025-08",
        "title": "Boosting Active Defense Persistence: A Two-Stage Defense Framework Combining Interruption and Poisoning Against Deepfake",
        "author": "Hongrui Zheng, Yuezun Li, Liejun Wang, Yunfeng Diao, and Zhiqing Guo",
        "link": "http://arxiv.org/abs/2508.07795v2",
        "abstract": "Active defense strategies have been developed to counter the threat of\ndeepfake technology. However, a primary challenge is their lack of persistence,\nas their effectiveness is often short-lived. Attackers can bypass these\ndefenses by simply collecting protected samples and retraining their models.\nThis means that static defenses inevitably fail when attackers retrain their\nmodels, which severely limits practical use. We argue that an effective defense\nnot only distorts forged content but also blocks the model's ability to adapt,\nwhich occurs when attackers retrain their models on protected images. To\nachieve this, we propose an innovative Two-Stage Defense Framework (TSDF).\nBenefiting from the intensity separation mechanism designed in this paper, the\nframework uses dual-function adversarial perturbations to perform two roles.\nFirst, it can directly distort the forged results. Second, it acts as a\npoisoning vehicle that disrupts the data preparation process essential for an\nattacker's retraining pipeline. By poisoning the data source, TSDF aims to\nprevent the attacker's model from adapting to the defensive perturbations, thus\nensuring the defense remains effective long-term. Comprehensive experiments\nshow that the performance of traditional interruption methods degrades sharply\nwhen it is subjected to adversarial retraining. However, our framework shows a\nstrong dual defense capability, which can improve the persistence of active\ndefense. Our code will be available at https://github.com/vpsg-research/TSDF."
    },
    {
        "date": "2025-08",
        "title": "Best-Effort Policies for Robust Markov Decision Processes",
        "author": "Alessandro Abate, Thom Badings, Giuseppe De Giacomo, and Francesco Fabiano",
        "link": "http://arxiv.org/abs/2508.07790v1",
        "abstract": "We study the common generalization of Markov decision processes (MDPs) with\nsets of transition probabilities, known as robust MDPs (RMDPs). A standard goal\nin RMDPs is to compute a policy that maximizes the expected return under an\nadversarial choice of the transition probabilities. If the uncertainty in the\nprobabilities is independent between the states, known as s-rectangularity,\nsuch optimal robust policies can be computed efficiently using robust value\niteration. However, there might still be multiple optimal robust policies,\nwhich, while equivalent with respect to the worst-case, reflect different\nexpected returns under non-adversarial choices of the transition probabilities.\nHence, we propose a refined policy selection criterion for RMDPs, drawing\ninspiration from the notions of dominance and best-effort in game theory.\nInstead of seeking a policy that only maximizes the worst-case expected return,\nwe additionally require the policy to achieve a maximal expected return under\ndifferent (i.e., not fully adversarial) transition probabilities. We call such\na policy an optimal robust best-effort (ORBE) policy. We prove that ORBE\npolicies always exist, characterize their structure, and present an algorithm\nto compute them with a small overhead compared to standard robust value\niteration. ORBE policies offer a principled tie-breaker among optimal robust\npolicies. Numerical experiments show the feasibility of our approach."
    },
    {
        "date": "2025-08",
        "title": "Robust Reinforcement Learning over Wireless Networks with Homomorphic State Representations",
        "author": "Pietro Talli, Federico Mason, Federico Chiariotti, and Andrea Zanella",
        "link": "http://arxiv.org/abs/2508.07722v1",
        "abstract": "In this work, we address the problem of training Reinforcement Learning (RL)\nagents over communication networks. The RL paradigm requires the agent to\ninstantaneously perceive the state evolution to infer the effects of its\nactions on the environment. This is impossible if the agent receives state\nupdates over lossy or delayed wireless systems and thus operates with partial\nand intermittent information. In recent years, numerous frameworks have been\nproposed to manage RL with imperfect feedback; however, they often offer\nspecific solutions with a substantial computational burden. To address these\nlimits, we propose a novel architecture, named Homomorphic Robust Remote\nReinforcement Learning (HR3L), that enables the training of remote RL agents\nexchanging observations across a non-ideal wireless channel. HR3L considers two\nunits: the transmitter, which encodes meaningful representations of the\nenvironment, and the receiver, which decodes these messages and performs\nactions to maximize a reward signal. Importantly, HR3L does not require the\nexchange of gradient information across the wireless channel, allowing for\nquicker training and a lower communication overhead than state-of-the-art\nsolutions. Experimental results demonstrate that HR3L significantly outperforms\nbaseline methods in terms of sample efficiency and adapts to different\ncommunication scenarios, including packet losses, delayed transmissions, and\ncapacity limitations."
    },
    {
        "date": "2025-08",
        "title": "AD-AVSR: Asymmetric Dual-stream Enhancement for Robust Audio-Visual Speech Recognition",
        "author": "Junxiao Xue, Xiaozhen Liu, Xuecheng Wu, Xinyi Yin, Danlei Huang, and Fei Yu",
        "link": "http://arxiv.org/abs/2508.07608v1",
        "abstract": "Audio-visual speech recognition (AVSR) combines audio-visual modalities to\nimprove speech recognition, especially in noisy environments. However, most\nexisting methods deploy the unidirectional enhancement or symmetric fusion\nmanner, which limits their capability to capture heterogeneous and\ncomplementary correlations of audio-visual data-especially under asymmetric\ninformation conditions. To tackle these gaps, we introduce a new AVSR framework\ntermed AD-AVSR based on bidirectional modality enhancement. Specifically, we\nfirst introduce the audio dual-stream encoding strategy to enrich audio\nrepresentations from multiple perspectives and intentionally establish\nasymmetry to support subsequent cross-modal interactions. The enhancement\nprocess involves two key components, Audio-aware Visual Refinement Module for\nenhanced visual representations under audio guidance, and Cross-modal Noise\nSuppression Masking Module which refines audio representations using visual\ncues, collaboratively leading to the closed-loop and bidirectional information\nflow. To further enhance correlation robustness, we adopt a threshold-based\nselection mechanism to filter out irrelevant or weakly correlated audio-visual\npairs. Extensive experimental results on the LRS2 and LRS3 datasets indicate\nthat our AD-AVSR consistently surpasses SOTA methods in both performance and\nnoise robustness, highlighting the effectiveness of our model design."
    },
    {
        "date": "2025-08",
        "title": "Quantum-Enhanced Generative Adversarial Networks: Comparative Analysis of Classical and Hybrid Quantum-Classical Generative Adversarial Networks",
        "author": "Kun Ming Goh",
        "link": "http://arxiv.org/abs/2508.09209v2",
        "abstract": "Generative adversarial networks (GANs) have emerged as a powerful paradigm\nfor producing high-fidelity data samples, yet their performance is constrained\nby the quality of latent representations, typically sampled from classical\nnoise distributions. This study investigates hybrid quantum-classical GANs\n(HQCGANs) in which a quantum generator, implemented via parameterised quantum\ncircuits, produces latent vectors for a classical discriminator. We evaluate a\nclassical GAN alongside three HQCGAN variants with 3, 5, and 7 qubits, using\nQiskit's AerSimulator with realistic noise models to emulate near-term quantum\ndevices. The binary MNIST dataset (digits 0 and 1) is used to align with the\nlow-dimensional latent spaces imposed by current quantum hardware. Models are\ntrained for 150 epochs and assessed with Frechet Inception Distance (FID) and\nKernel Inception Distance (KID). Results show that while the classical GAN\nachieved the best scores, the 7-qubit HQCGAN produced competitive performance,\nnarrowing the gap in later epochs, whereas the 3-qubit model exhibited earlier\nconvergence limitations. Efficiency analysis indicates only moderate training\ntime increases despite quantum sampling overhead. These findings validate the\nfeasibility of noisy quantum circuits as latent priors in GAN architectures,\nhighlighting their potential to enhance generative modelling within the\nconstraints of the noisy intermediate-scale quantum (NISQ) era."
    },
    {
        "date": "2025-08",
        "title": "ForensicsSAM: Toward Robust and Unified Image Forgery Detection and Localization Resisting to Adversarial Attack",
        "author": "Rongxuan Peng, Shunquan Tan, Chenqi Kong, Anwei Luo, Alex C. Kot, and Jiwu Huang",
        "link": "http://arxiv.org/abs/2508.07402v2",
        "abstract": "Parameter-efficient fine-tuning (PEFT) has emerged as a popular strategy for\nadapting large vision foundation models, such as the Segment Anything Model\n(SAM) and LLaVA, to downstream tasks like image forgery detection and\nlocalization (IFDL). However, existing PEFT-based approaches overlook their\nvulnerability to adversarial attacks. In this paper, we show that highly\ntransferable adversarial images can be crafted solely via the upstream model,\nwithout accessing the downstream model or training data, significantly\ndegrading the IFDL performance. To address this, we propose ForensicsSAM, a\nunified IFDL framework with built-in adversarial robustness. Our design is\nguided by three key ideas: (1) To compensate for the lack of forgery-relevant\nknowledge in the frozen image encoder, we inject forgery experts into each\ntransformer block to enhance its ability to capture forgery artifacts. These\nforgery experts are always activated and shared across any input images. (2) To\ndetect adversarial images, we design an light-weight adversary detector that\nlearns to capture structured, task-specific artifact in RGB domain, enabling\nreliable discrimination across various attack methods. (3) To resist\nadversarial attacks, we inject adversary experts into the global attention\nlayers and MLP modules to progressively correct feature shifts induced by\nadversarial noise. These adversary experts are adaptively activated by the\nadversary detector, thereby avoiding unnecessary interference with clean\nimages. Extensive experiments across multiple benchmarks demonstrate that\nForensicsSAM achieves superior resistance to various adversarial attack\nmethods, while also delivering state-of-the-art performance in image-level\nforgery detection and pixel-level forgery localization. The resource is\navailable at https://github.com/siriusPRX/ForensicsSAM."
    },
    {
        "date": "2025-08",
        "title": "Multi-task Adversarial Attacks against Black-box Model with Few-shot Queries",
        "author": "Wenqiang Wang, Yan Xiao, Hao Lin, Yangshijie Zhang, and Xiaochun Cao",
        "link": "http://arxiv.org/abs/2508.10039v1",
        "abstract": "Current multi-task adversarial text attacks rely on abundant access to shared\ninternal features and numerous queries, often limited to a single task type. As\na result, these attacks are less effective against practical scenarios\ninvolving black-box feedback APIs, limited queries, or multiple task types. To\nbridge this gap, we propose \\textbf{C}luster and \\textbf{E}nsemble\n\\textbf{M}ulti-task Text Adversarial \\textbf{A}ttack (\\textbf{CEMA}), an\neffective black-box attack that exploits the transferability of adversarial\ntexts across different tasks. CEMA simplifies complex multi-task scenarios by\nusing a \\textit{deep-level substitute model} trained in a\n\\textit{plug-and-play} manner for text classification, enabling attacks without\nmimicking the victim model. This approach requires only a few queries for\ntraining, converting multi-task attacks into classification attacks and\nallowing attacks across various tasks.\n  CEMA generates multiple adversarial candidates using different text\nclassification methods and selects the one that most effectively attacks\nsubstitute models.\n  In experiments involving multi-task models with two, three, or six\ntasks--spanning classification, translation, summarization, and text-to-image\ngeneration--CEMA demonstrates significant attack success with as few as 100\nqueries. Furthermore, CEMA can target commercial APIs (e.g., Baidu and Google\nTranslate), large language models (e.g., ChatGPT 4o), and image-generation\nmodels (e.g., Stable Diffusion V2), showcasing its versatility and\neffectiveness in real-world applications."
    },
    {
        "date": "2025-08",
        "title": "ObfusQAte: A Proposed Framework to Evaluate LLM Robustness on Obfuscated Factual Question Answering",
        "author": "Shubhra Ghosh, Abhilekh Borah, Aditya Kumar Guru, and Kripabandhu Ghosh",
        "link": "http://arxiv.org/abs/2508.07321v1",
        "abstract": "The rapid proliferation of Large Language Models (LLMs) has significantly\ncontributed to the development of equitable AI systems capable of factual\nquestion-answering (QA). However, no known study tests the LLMs' robustness\nwhen presented with obfuscated versions of questions. To systematically\nevaluate these limitations, we propose a novel technique, ObfusQAte and,\nleveraging the same, introduce ObfusQA, a comprehensive, first of its kind,\nframework with multi-tiered obfuscation levels designed to examine LLM\ncapabilities across three distinct dimensions: (i) Named-Entity Indirection,\n(ii) Distractor Indirection, and (iii) Contextual Overload. By capturing these\nfine-grained distinctions in language, ObfusQA provides a comprehensive\nbenchmark for evaluating LLM robustness and adaptability. Our study observes\nthat LLMs exhibit a tendency to fail or generate hallucinated responses when\nconfronted with these increasingly nuanced variations. To foster research in\nthis direction, we make ObfusQAte publicly available."
    },
    {
        "date": "2025-08",
        "title": "Fading the Digital Ink: A Universal Black-Box Attack Framework for 3DGS Watermarking Systems",
        "author": "Qingyuan Zeng, Shu Jiang, Jiajing Lin, Zhenzhong Wang, Kay Chen Tan, and Min Jiang",
        "link": "http://arxiv.org/abs/2508.07263v1",
        "abstract": "With the rise of 3D Gaussian Splatting (3DGS), a variety of digital\nwatermarking techniques, embedding either 1D bitstreams or 2D images, are used\nfor copyright protection. However, the robustness of these watermarking\ntechniques against potential attacks remains underexplored. This paper\nintroduces the first universal black-box attack framework, the Group-based\nMulti-objective Evolutionary Attack (GMEA), designed to challenge these\nwatermarking systems. We formulate the attack as a large-scale multi-objective\noptimization problem, balancing watermark removal with visual quality. In a\nblack-box setting, we introduce an indirect objective function that blinds the\nwatermark detector by minimizing the standard deviation of features extracted\nby a convolutional network, thus rendering the feature maps uninformative. To\nmanage the vast search space of 3DGS models, we employ a group-based\noptimization strategy to partition the model into multiple, independent\nsub-optimization problems. Experiments demonstrate that our framework\neffectively removes both 1D and 2D watermarks from mainstream 3DGS watermarking\nmethods while maintaining high visual fidelity. This work reveals critical\nvulnerabilities in existing 3DGS copyright protection schemes and calls for the\ndevelopment of more robust watermarking systems."
    },
    {
        "date": "2025-08",
        "title": "Certifiably robust malware detectors by design",
        "author": "Pierre-Francois Gimenez, Sarath Sivaprasad, and Mario Fritz",
        "link": "http://arxiv.org/abs/2508.10038v1",
        "abstract": "Malware analysis involves analyzing suspicious software to detect malicious\npayloads. Static malware analysis, which does not require software execution,\nrelies increasingly on machine learning techniques to achieve scalability.\nAlthough such techniques obtain very high detection accuracy, they can be\neasily evaded with adversarial examples where a few modifications of the sample\ncan dupe the detector without modifying the behavior of the software. Unlike\nother domains, such as computer vision, creating an adversarial example of\nmalware without altering its functionality requires specific transformations.\nWe propose a new model architecture for certifiably robust malware detection by\ndesign. In addition, we show that every robust detector can be decomposed into\na specific structure, which can be applied to learn empirically robust malware\ndetectors, even on fragile features. Our framework ERDALT is based on this\nstructure. We compare and validate these approaches with machine-learning-based\nmalware detection methods, allowing for robust detection with limited reduction\nof detection performance."
    },
    {
        "date": "2025-08",
        "title": "Civil Servants as Builders: Enabling Non-IT Staff to Develop Secure Python and R Tools",
        "author": "Prashant Sharma",
        "link": "http://arxiv.org/abs/2508.07203v1",
        "abstract": "Current digital government literature focuses on professional in-house IT\nteams, specialized digital service teams, vendor-developed systems, or\nproprietary low-code/no-code tools. Almost no scholarship addresses a growing\nmiddle ground: technically skilled civil servants outside formal IT roles who\ncan write real code but lack a sanctioned, secure path to deploy their work.\nThis paper introduces a limits-aware, open-source and replicable platform that\nenables such public servants to develop, peer review, and deploy small-scale,\ndomain-specific applications within government networks via a sandboxed,\nauditable workflow. By combining Jupyter Notebooks, preapproved open-source\nlibraries, and lightweight governance, the platform works within institutional\nconstraints such as procurement rules and IT security policies while avoiding\nvendor lock-in. Unlike low/no-code approaches, it preserves and enhances civil\nservants' programming skills, keeping them technically competitive with their\nprivate-sector peers. This contribution fills a critical gap, offering a\nreplicable model for public-sector skill retention, resilience, and bottom-up\ndigital transformation."
    },
    {
        "date": "2025-08",
        "title": "A Real-Time, Self-Tuning Moderator Framework for Adversarial Prompt Detection",
        "author": "Ivan Zhang",
        "link": "http://arxiv.org/abs/2508.07139v1",
        "abstract": "Ensuring LLM alignment is critical to information security as AI models\nbecome increasingly widespread and integrated in society. Unfortunately, many\ndefenses against adversarial attacks and jailbreaking on LLMs cannot adapt\nquickly to new attacks, degrade model responses to benign prompts, or introduce\nsignificant barriers to scalable implementation. To mitigate these challenges,\nwe introduce a real-time, self-tuning (RTST) moderator framework to defend\nagainst adversarial attacks while maintaining a lightweight training footprint.\nWe empirically evaluate its effectiveness using Google's Gemini models against\nmodern, effective jailbreaks. Our results demonstrate the advantages of an\nadaptive, minimally intrusive framework for jailbreak defense over traditional\nfine-tuning or classifier models."
    },
    {
        "date": "2025-08",
        "title": "Sensory robustness through top-down feedback and neural stochasticity in recurrent vision models",
        "author": "Antonino Greco, Marco D'Alessandro, Karl J. Friston, Giovanni Pezzulo, and Markus Siegel",
        "link": "http://arxiv.org/abs/2508.07115v1",
        "abstract": "Biological systems leverage top-down feedback for visual processing, yet most\nartificial vision models succeed in image classification using purely\nfeedforward or recurrent architectures, calling into question the functional\nsignificance of descending cortical pathways. Here, we trained convolutional\nrecurrent neural networks (ConvRNN) on image classification in the presence or\nabsence of top-down feedback projections to elucidate the specific\ncomputational contributions of those feedback pathways. We found that ConvRNNs\nwith top-down feedback exhibited remarkable speed-accuracy trade-off and\nrobustness to noise perturbations and adversarial attacks, but only when they\nwere trained with stochastic neural variability, simulated by randomly\nsilencing single units via dropout. By performing detailed analyses to identify\nthe reasons for such benefits, we observed that feedback information\nsubstantially shaped the representational geometry of the post-integration\nlayer, combining the bottom-up and top-down streams, and this effect was\namplified by dropout. Moreover, feedback signals coupled with dropout optimally\nconstrained network activity onto a low-dimensional manifold and encoded object\ninformation more efficiently in out-of-distribution regimes, with top-down\ninformation stabilizing the representational dynamics at the population level.\nTogether, these findings uncover a dual mechanism for resilient sensory coding.\nOn the one hand, neural stochasticity prevents unit-level co-adaptation albeit\nat the cost of more chaotic dynamics. On the other hand, top-down feedback\nharnesses high-level information to stabilize network activity on compact\nlow-dimensional manifolds."
    },
    {
        "date": "2025-08",
        "title": "ScamDetect: Towards a Robust, Agnostic Framework to Uncover Threats in Smart Contracts",
        "author": "Pasquale De Rosa, Pascal Felber, and Valerio Schiavoni",
        "link": "http://arxiv.org/abs/2508.07094v2",
        "abstract": "Smart contracts have transformed decentralized finance by enabling\nprogrammable, trustless transactions. However, their widespread adoption and\ngrowing financial significance have attracted persistent and sophisticated\nthreats, such as phishing campaigns and contract-level exploits. Traditional\ntransaction-based threat detection methods often expose sensitive user data and\ninteractions, raising privacy and security concerns. In response, static\nbytecode analysis has emerged as a proactive mitigation strategy, identifying\nmalicious contracts before they execute harmful actions. Building on this\napproach, we introduced PhishingHook, the first machine-learning-based\nframework for detecting phishing activities in smart contracts via static\nbytecode and opcode analysis, achieving approximately 90% detection accuracy.\nNevertheless, two pressing challenges remain: (1) the increasing use of\nsophisticated bytecode obfuscation techniques designed to evade static\nanalysis, and (2) the heterogeneity of blockchain environments requiring\nplatform-agnostic solutions. This paper presents a vision for ScamDetect (Smart\nContract Agnostic Malware Detector), a robust, modular, and platform-agnostic\nframework for smart contract malware detection. Over the next 2.5 years,\nScamDetect will evolve in two stages: first, by tackling obfuscated Ethereum\nVirtual Machine (EVM) bytecode through graph neural network (GNN) analysis of\ncontrol flow graphs (CFGs), leveraging GNNs' ability to capture complex\nstructural patterns beyond opcode sequences; and second, by generalizing\ndetection capabilities to emerging runtimes such as WASM. ScamDetect aims to\nenable proactive, scalable security for the future of decentralized ecosystems."
    },
    {
        "date": "2025-08",
        "title": "Neural Network-Based Detection and Multi-Class Classification of FDI Attacks in Smart Grid Home Energy Systems",
        "author": "Varsha Sen, and Biswash Basnet",
        "link": "http://arxiv.org/abs/2508.10035v1",
        "abstract": "False Data Injection Attacks (FDIAs) pose a significant threat to smart grid\ninfrastructures, particularly Home Area Networks (HANs), where real-time\nmonitoring and control are highly adopted. Owing to the comparatively less\nstringent security controls and widespread availability of HANs, attackers view\nthem as an attractive entry point to manipulate aggregated demand patterns,\nwhich can ultimately propagate and disrupt broader grid operations. These\nattacks undermine the integrity of smart meter data, enabling malicious actors\nto manipulate consumption values without activating conventional alarms,\nthereby creating serious vulnerabilities across both residential and\nutility-scale infrastructures. This paper presents a machine learning-based\nframework for both the detection and classification of FDIAs using residential\nenergy data. A real-time detection is provided by the lightweight Artificial\nNeural Network (ANN), which works by using the most vital features of energy\nconsumption, cost, and time context. For the classification of different attack\ntypes, a Bidirectional LSTM is trained to recognize normal, trapezoidal, and\nsigmoid attack shapes through learning sequential dependencies in the data. A\nsynthetic time-series dataset was generated to emulate realistic household\nbehaviour. Experimental results demonstrate that the proposed models are\neffective in identifying and classifying FDIAs, offering a scalable solution\nfor enhancing grid resilience at the edge. This work contributes toward\nbuilding intelligent, data-driven defence mechanisms that strengthen smart grid\ncybersecurity from residential endpoints."
    },
    {
        "date": "2025-08",
        "title": "Membership Inference Attacks with False Discovery Rate Control",
        "author": "Chenxu Zhao, Wei Qian, Aobo Chen, and Mengdi Huai",
        "link": "http://arxiv.org/abs/2508.07066v1",
        "abstract": "Recent studies have shown that deep learning models are vulnerable to\nmembership inference attacks (MIAs), which aim to infer whether a data record\nwas used to train a target model or not. To analyze and study these\nvulnerabilities, various MIA methods have been proposed. Despite the\nsignificance and popularity of MIAs, existing works on MIAs are limited in\nproviding guarantees on the false discovery rate (FDR), which refers to the\nexpected proportion of false discoveries among the identified positive\ndiscoveries. However, it is very challenging to ensure the false discovery rate\nguarantees, because the underlying distribution is usually unknown, and the\nestimated non-member probabilities often exhibit interdependence. To tackle the\nabove challenges, in this paper, we design a novel membership inference attack\nmethod, which can provide the guarantees on the false discovery rate.\nAdditionally, we show that our method can also provide the marginal probability\nguarantee on labeling true non-member data as member data. Notably, our method\ncan work as a wrapper that can be seamlessly integrated with existing MIA\nmethods in a post-hoc manner, while also providing the FDR control. We perform\nthe theoretical analysis for our method. Extensive experiments in various\nsettings (e.g., the black-box setting and the lifelong learning setting) are\nalso conducted to verify the desirable performance of our method."
    },
    {
        "date": "2025-08",
        "title": "SPARE: Securing Progressive Web Applications Against Unauthorized Replications",
        "author": "Sajib Talukder, Nur Imtiazul Haque, and Khandakar Ashrafi Akbar",
        "link": "http://arxiv.org/abs/2508.07053v1",
        "abstract": "WebView applications are widely used in mobile applications to display web\ncontent directly within the app, enhancing user engagement by eliminating the\nneed to open an external browser and providing a seamless experience.\nProgressive Web Applications (PWAs) further improve usability by combining the\naccessibility of web apps with the speed, offline capabilities, and\nresponsiveness of native applications. However, malicious developers can\nexploit this technology by duplicating PWA web links to create counterfeit\nnative apps, monetizing through user diversion. This unethical practice poses\nsignificant risks to users and the original application developers,\nunderscoring the need for robust security measures to prevent unauthorized\nreplication. Considering the one-way communication of Trusted Web Activity (a\nmethod for integrating web content into Android applications) and PWAs, we\npropose a query parameter-based practical security solution to defend against\nor mitigate such attacks. We analyze the vulnerabilities of our proposed\nsecurity solution to assess its effectiveness and introduce advanced measures\nto address any identified weaknesses, presenting a comprehensive defense\nframework. As part of our work, we developed a prototype web application that\nsecures PWAs from replication by embedding a combination of Unix timestamps and\ndevice identifiers into the query parameters. We evaluate the effectiveness of\nthis defense strategy by simulating an advanced attack scenario. Additionally,\nwe created a realistic dataset reflecting mobile app user behavior, modeled\nusing a Zipfian distribution, to validate our framework."
    },
    {
        "date": "2025-08",
        "title": "TADoc: Robust Time-Aware Document Image Dewarping",
        "author": "Fangmin Zhao, Weichao Zeng, Zhenhang Li, Dongbao Yang, and Yu Zhou",
        "link": "http://arxiv.org/abs/2508.06988v1",
        "abstract": "Flattening curved, wrinkled, and rotated document images captured by portable\nphotographing devices, termed document image dewarping, has become an\nincreasingly important task with the rise of digital economy and online\nworking. Although many methods have been proposed recently, they often struggle\nto achieve satisfactory results when confronted with intricate document\nstructures and higher degrees of deformation in real-world scenarios. Our main\ninsight is that, unlike other document restoration tasks (e.g., deblurring),\ndewarping in real physical scenes is a progressive motion rather than a\none-step transformation. Based on this, we have undertaken two key initiatives.\nFirstly, we reformulate this task, modeling it for the first time as a dynamic\nprocess that encompasses a series of intermediate states. Secondly, we design a\nlightweight framework called TADoc (Time-Aware Document Dewarping Network) to\naddress the geometric distortion of document images. In addition, due to the\ninadequacy of OCR metrics for document images containing sparse text, the\ncomprehensiveness of evaluation is insufficient. To address this shortcoming,\nwe propose a new metric -- DLS (Document Layout Similarity) -- to evaluate the\neffectiveness of document dewarping in downstream tasks. Extensive experiments\nand in-depth evaluations have been conducted and the results indicate that our\nmodel possesses strong robustness, achieving superiority on several benchmarks\nwith different document types and degrees of distortion."
    },
    {
        "date": "2025-08",
        "title": "Adversarial Video Promotion Against Text-to-Video Retrieval",
        "author": "Qiwei Tian, Chenhao Lin, Zhengyu Zhao, Qian Li, Shuai Liu, and Chao Shen",
        "link": "http://arxiv.org/abs/2508.06964v2",
        "abstract": "Thanks to the development of cross-modal models, text-to-video retrieval\n(T2VR) is advancing rapidly, but its robustness remains largely unexamined.\nExisting attacks against T2VR are designed to push videos away from queries,\ni.e., suppressing the ranks of videos, while the attacks that pull videos\ntowards selected queries, i.e., promoting the ranks of videos, remain largely\nunexplored. These attacks can be more impactful as attackers may gain more\nviews/clicks for financial benefits and widespread (mis)information. To this\nend, we pioneer the first attack against T2VR to promote videos adversarially,\ndubbed the Video Promotion attack (ViPro). We further propose Modal Refinement\n(MoRe) to capture the finer-grained, intricate interaction between visual and\ntextual modalities to enhance black-box transferability. Comprehensive\nexperiments cover 2 existing baselines, 3 leading T2VR models, 3 prevailing\ndatasets with over 10k videos, evaluated under 3 scenarios. All experiments are\nconducted in a multi-target setting to reflect realistic scenarios where\nattackers seek to promote the video regarding multiple queries simultaneously.\nWe also evaluated our attacks for defences and imperceptibility. Overall, ViPro\nsurpasses other baselines by over $30/10/4\\%$ for white/grey/black-box settings\non average. Our work highlights an overlooked vulnerability, provides a\nqualitative analysis on the upper/lower bound of our attacks, and offers\ninsights into potential counterplays. Code will be publicly available at\nhttps://github.com/michaeltian108/ViPro."
    },
    {
        "date": "2025-08",
        "title": "When Prompt Engineering Meets Software Engineering: CNL-P as Natural and Robust \"APIs'' for Human-AI Interaction",
        "author": "Zhenchang Xing, Yang Liu, Zhuo Cheng, Qing Huang, Dehai Zhao, Daniel Sun, and Chenhua Liu",
        "link": "http://arxiv.org/abs/2508.06942v1",
        "abstract": "With the growing capabilities of large language models (LLMs), they are\nincreasingly applied in areas like intelligent customer service, code\ngeneration, and knowledge management. Natural language (NL) prompts act as the\n``APIs'' for human-LLM interaction. To improve prompt quality, best practices\nfor prompt engineering (PE) have been developed, including writing guidelines\nand templates. Building on this, we propose Controlled NL for Prompt (CNL-P),\nwhich not only incorporates PE best practices but also draws on key principles\nfrom software engineering (SE). CNL-P introduces precise grammar structures and\nstrict semantic norms, further eliminating NL's ambiguity, allowing for a\ndeclarative but structured and accurate expression of user intent. This helps\nLLMs better interpret and execute the prompts, leading to more consistent and\nhigher-quality outputs. We also introduce an NL2CNL-P conversion tool based on\nLLMs, enabling users to write prompts in NL, which are then transformed into\nCNL-P format, thus lowering the learning curve of CNL-P. In particular, we\ndevelop a linting tool that checks CNL-P prompts for syntactic and semantic\naccuracy, applying static analysis techniques to NL for the first time.\nExtensive experiments demonstrate that CNL-P enhances the quality of LLM\nresponses through the novel and organic synergy of PE and SE. We believe that\nCNL-P can bridge the gap between emerging PE and traditional SE, laying the\nfoundation for a new programming paradigm centered around NL."
    },
    {
        "date": "2025-08",
        "title": "Model-Agnostic Sentiment Distribution Stability Analysis for Robust LLM-Generated Texts Detection",
        "author": "Siyuan Li, Xi Lin, Guangyan Li, Zehao Liu, Aodu Wulianghai, Li Ding, Jun Wu, and Jianhua Li",
        "link": "http://arxiv.org/abs/2508.06913v1",
        "abstract": "The rapid advancement of large language models (LLMs) has resulted in\nincreasingly sophisticated AI-generated content, posing significant challenges\nin distinguishing LLM-generated text from human-written language. Existing\ndetection methods, primarily based on lexical heuristics or fine-tuned\nclassifiers, often suffer from limited generalizability and are vulnerable to\nparaphrasing, adversarial perturbations, and cross-domain shifts. In this work,\nwe propose SentiDetect, a model-agnostic framework for detecting LLM-generated\ntext by analyzing the divergence in sentiment distribution stability. Our\nmethod is motivated by the empirical observation that LLM outputs tend to\nexhibit emotionally consistent patterns, whereas human-written texts display\ngreater emotional variability. To capture this phenomenon, we define two\ncomplementary metrics: sentiment distribution consistency and sentiment\ndistribution preservation, which quantify stability under sentiment-altering\nand semantic-preserving transformations. We evaluate SentiDetect on five\ndiverse datasets and a range of advanced LLMs,including Gemini-1.5-Pro,\nClaude-3, GPT-4-0613, and LLaMa-3.3. Experimental results demonstrate its\nsuperiority over state-of-the-art baselines, with over 16% and 11% F1 score\nimprovements on Gemini-1.5-Pro and GPT-4-0613, respectively. Moreover,\nSentiDetect also shows greater robustness to paraphrasing, adversarial attacks,\nand text length variations, outperforming existing detectors in challenging\nscenarios."
    },
    {
        "date": "2025-08",
        "title": "Towards Effective Prompt Stealing Attack against Text-to-Image Diffusion Models",
        "author": "Shiqian Zhao, Chong Wang, Yiming Li, Yihao Huang, Wenjie Qu, Siew-Kei Lam, Yi Xie, Kangjie Chen, Jie Zhang, and Tianwei Zhang",
        "link": "http://arxiv.org/abs/2508.06837v1",
        "abstract": "Text-to-Image (T2I) models, represented by DALL$\\cdot$E and Midjourney, have\ngained huge popularity for creating realistic images. The quality of these\nimages relies on the carefully engineered prompts, which have become valuable\nintellectual property. While skilled prompters showcase their AI-generated art\non markets to attract buyers, this business incidentally exposes them to\n\\textit{prompt stealing attacks}. Existing state-of-the-art attack techniques\nreconstruct the prompts from a fixed set of modifiers (i.e., style\ndescriptions) with model-specific training, which exhibit restricted\nadaptability and effectiveness to diverse showcases (i.e., target images) and\ndiffusion models.\n  To alleviate these limitations, we propose Prometheus, a training-free,\nproxy-in-the-loop, search-based prompt-stealing attack, which reverse-engineers\nthe valuable prompts of the showcases by interacting with a local proxy model.\nIt consists of three innovative designs. First, we introduce dynamic modifiers,\nas a supplement to static modifiers used in prior works. These dynamic\nmodifiers provide more details specific to the showcases, and we exploit NLP\nanalysis to generate them on the fly. Second, we design a contextual matching\nalgorithm to sort both dynamic and static modifiers. This offline process helps\nreduce the search space of the subsequent step. Third, we interact with a local\nproxy model to invert the prompts with a greedy search algorithm. Based on the\nfeedback guidance, we refine the prompt to achieve higher fidelity. The\nevaluation results show that Prometheus successfully extracts prompts from\npopular platforms like PromptBase and AIFrog against diverse victim models,\nincluding Midjourney, Leonardo.ai, and DALL$\\cdot$E, with an ASR improvement of\n25.0\\%. We also validate that Prometheus is resistant to extensive potential\ndefenses, further highlighting its severity in practice."
    },
    {
        "date": "2025-08",
        "title": "Hardness-Aware Dynamic Curriculum Learning for Robust Multimodal Emotion Recognition with Missing Modalities",
        "author": "Rui Liu, Haolin Zuo, Zheng Lian, Hongyu Yuan, and Qi Fan",
        "link": "http://arxiv.org/abs/2508.06800v2",
        "abstract": "Missing modalities have recently emerged as a critical research direction in\nmultimodal emotion recognition (MER). Conventional approaches typically address\nthis issue through missing modality reconstruction. However, these methods fail\nto account for variations in reconstruction difficulty across different\nsamples, consequently limiting the model's ability to handle hard samples\neffectively. To overcome this limitation, we propose a novel Hardness-Aware\nDynamic Curriculum Learning framework, termed HARDY-MER. Our framework operates\nin two key stages: first, it estimates the hardness level of each sample, and\nsecond, it strategically emphasizes hard samples during training to enhance\nmodel performance on these challenging instances. Specifically, we first\nintroduce a Multi-view Hardness Evaluation mechanism that quantifies\nreconstruction difficulty by considering both Direct Hardness (modality\nreconstruction errors) and Indirect Hardness (cross-modal mutual information).\nMeanwhile, we introduce a Retrieval-based Dynamic Curriculum Learning strategy\nthat dynamically adjusts the training curriculum by retrieving samples with\nsimilar semantic information and balancing the learning focus between easy and\nhard instances. Extensive experiments on benchmark datasets demonstrate that\nHARDY-MER consistently outperforms existing methods in missing-modality\nscenarios. Our code will be made publicly available at\nhttps://github.com/HARDY-MER/HARDY-MER."
    },
    {
        "date": "2025-08",
        "title": "Label Inference Attacks against Federated Unlearning",
        "author": "Wei Wang, Xiangyun Tang, Yajie Wang, Yijing Lin, Tao Zhang, Meng Shen, Dusit Niyato, and Liehuang Zhu",
        "link": "http://arxiv.org/abs/2508.06789v1",
        "abstract": "Federated Unlearning (FU) has emerged as a promising solution to respond to\nthe right to be forgotten of clients, by allowing clients to erase their data\nfrom global models without compromising model performance. Unfortunately,\nresearchers find that the parameter variations of models induced by FU expose\nclients' data information, enabling attackers to infer the label of unlearning\ndata, while label inference attacks against FU remain unexplored. In this\npaper, we introduce and analyze a new privacy threat against FU and propose a\nnovel label inference attack, ULIA, which can infer unlearning data labels\nacross three FU levels. To address the unique challenges of inferring labels\nvia the models variations, we design a gradient-label mapping mechanism in ULIA\nthat establishes a relationship between gradient variations and unlearning\nlabels, enabling inferring labels on accumulated model variations. We evaluate\nULIA on both IID and non-IID settings. Experimental results show that in the\nIID setting, ULIA achieves a 100% Attack Success Rate (ASR) under both\nclass-level and client-level unlearning. Even when only 1% of a user's local\ndata is forgotten, ULIA still attains an ASR ranging from 93% to 62.3%."
    },
    {
        "date": "2025-08",
        "title": "Learning Causal Structure Distributions for Robust Planning",
        "author": "Alejandro Murillo-Gonzalez, Junhong Xu, and Lantao Liu",
        "link": "http://arxiv.org/abs/2508.06742v1",
        "abstract": "Structural causal models describe how the components of a robotic system\ninteract. They provide both structural and functional information about the\nrelationships that are present in the system. The structural information\noutlines the variables among which there is interaction. The functional\ninformation describes how such interactions work, via equations or learned\nmodels. In this paper we find that learning the functional relationships while\naccounting for the uncertainty about the structural information leads to more\nrobust dynamics models which improves downstream planning, while using\nsignificantly lower computational resources. This in contrast with common\nmodel-learning methods that ignore the causal structure and fail to leverage\nthe sparsity of interactions in robotic systems. We achieve this by estimating\na causal structure distribution that is used to sample causal graphs that\ninform the latent-space representations in an encoder-multidecoder\nprobabilistic model. We show that our model can be used to learn the dynamics\nof a robot, which together with a sampling-based planner can be used to perform\nnew tasks in novel environments, provided an objective function for the new\nrequirement is available. We validate our method using manipulators and mobile\nrobots in both simulation and the real-world. Additionally, we validate the\nlearned dynamics' adaptability and increased robustness to corrupted inputs and\nchanges in the environment, which is highly desirable in challenging real-world\nrobotics scenarios. Video: https://youtu.be/X6k5t7OOnNc."
    },
    {
        "date": "2025-08",
        "title": "Towards Robust Red-Green Watermarking for Autoregressive Image Generators",
        "author": "Denis Lukovnikov, Andreas M\u00fcller, Erwin Quiring, and Asja Fischer",
        "link": "http://arxiv.org/abs/2508.06656v1",
        "abstract": "In-generation watermarking for detecting and attributing generated content\nhas recently been explored for latent diffusion models (LDMs), demonstrating\nhigh robustness. However, the use of in-generation watermarks in autoregressive\n(AR) image models has not been explored yet. AR models generate images by\nautoregressively predicting a sequence of visual tokens that are then decoded\ninto pixels using a vector-quantized decoder. Inspired by red-green watermarks\nfor large language models, we examine token-level watermarking schemes that\nbias the next-token prediction based on prior tokens. We find that a direct\ntransfer of these schemes works in principle, but the detectability of the\nwatermarks decreases considerably under common image perturbations. As a\nremedy, we propose two novel watermarking methods that rely on visual token\nclustering to assign similar tokens to the same set. Firstly, we investigate a\ntraining-free approach that relies on a cluster lookup table, and secondly, we\nfinetune VAE encoders to predict token clusters directly from perturbed images.\nOverall, our experiments show that cluster-level watermarks improve robustness\nagainst perturbations and regeneration attacks while preserving image quality.\nCluster classification further boosts watermark detectability, outperforming a\nset of baselines. Moreover, our methods offer fast verification runtime,\ncomparable to lightweight post-hoc watermarking methods."
    },
    {
        "date": "2025-08",
        "title": "Rethinking Key-frame-based Micro-expression Recognition: A Robust and Accurate Framework Against Key-frame Errors",
        "author": "Zheyuan Zhang, Weihao Tang, and Hong Chen",
        "link": "http://arxiv.org/abs/2508.06640v1",
        "abstract": "Micro-expression recognition (MER) is a highly challenging task in affective\ncomputing. With the reduced-sized micro-expression (ME) input that contains key\ninformation based on key-frame indexes, key-frame-based methods have\nsignificantly improved the performance of MER. However, most of these methods\nfocus on improving the performance with relatively accurate key-frame indexes,\nwhile ignoring the difficulty of obtaining accurate key-frame indexes and the\nobjective existence of key-frame index errors, which impedes them from moving\ntowards practical applications. In this paper, we propose CausalNet, a novel\nframework to achieve robust MER facing key-frame index errors while maintaining\naccurate recognition. To enhance robustness, CausalNet takes the representation\nof the entire ME sequence as the input. To address the information redundancy\nbrought by the complete ME range input and maintain accurate recognition,\nfirst, the Causal Motion Position Learning Module (CMPLM) is proposed to help\nthe model locate the muscle movement areas related to Action Units (AUs),\nthereby reducing the attention to other redundant areas. Second, the Causal\nAttention Block (CAB) is proposed to deeply learn the causal relationships\nbetween the muscle contraction and relaxation movements in MEs. Empirical\nexperiments have demonstrated that on popular ME benchmarks, the CausalNet has\nachieved robust MER under different levels of key-frame index noise. Meanwhile,\nit has surpassed state-of-the-art (SOTA) methods on several standard MER\nbenchmarks when using the provided annotated key-frames. Code is available at\nhttps://github.com/tony19980810/CausalNet."
    },
    {
        "date": "2025-08",
        "title": "TRUST: Leveraging Text Robustness for Unsupervised Domain Adaptation",
        "author": "Mattia Litrico, Mario Valerio Giuffrida, Sebastiano Battiato, and Devis Tuia",
        "link": "http://arxiv.org/abs/2508.06452v1",
        "abstract": "Recent unsupervised domain adaptation (UDA) methods have shown great success\nin addressing classical domain shifts (e.g., synthetic-to-real), but they still\nsuffer under complex shifts (e.g. geographical shift), where both the\nbackground and object appearances differ significantly across domains. Prior\nworks showed that the language modality can help in the adaptation process,\nexhibiting more robustness to such complex shifts. In this paper, we introduce\nTRUST, a novel UDA approach that exploits the robustness of the language\nmodality to guide the adaptation of a vision model. TRUST generates\npseudo-labels for target samples from their captions and introduces a novel\nuncertainty estimation strategy that uses normalised CLIP similarity scores to\nestimate the uncertainty of the generated pseudo-labels. Such estimated\nuncertainty is then used to reweight the classification loss, mitigating the\nadverse effects of wrong pseudo-labels obtained from low-quality captions. To\nfurther increase the robustness of the vision model, we propose a multimodal\nsoft-contrastive learning loss that aligns the vision and language feature\nspaces, by leveraging captions to guide the contrastive training of the vision\nmodel on target images. In our contrastive loss, each pair of images acts as\nboth a positive and a negative pair and their feature representations are\nattracted and repulsed with a strength proportional to the similarity of their\ncaptions. This solution avoids the need for hardly determining positive and\nnegative pairs, which is critical in the UDA setting. Our approach outperforms\nprevious methods, setting the new state-of-the-art on classical (DomainNet) and\ncomplex (GeoNet) domain shifts. The code will be available upon acceptance."
    },
    {
        "date": "2025-08",
        "title": "Learning to Detect Unknown Jailbreak Attacks in Large Vision-Language Models: A Unified and Accurate Approach",
        "author": "Shuang Liang, Zhihao Xu, Jialing Tao, Hui Xue, and Xiting Wang",
        "link": "http://arxiv.org/abs/2508.09201v1",
        "abstract": "Despite extensive alignment efforts, Large Vision-Language Models (LVLMs)\nremain vulnerable to jailbreak attacks, posing serious safety risks. Although\nrecent detection works have shifted to internal representations due to their\nrich cross-modal information, most methods rely on heuristic rules rather than\nprincipled objectives, resulting in suboptimal performance. To address these\nlimitations, we propose Learning to Detect (LoD), a novel unsupervised\nframework that formulates jailbreak detection as anomaly detection. LoD\nintroduces two key components: Multi-modal Safety Concept Activation Vectors\n(MSCAV), which capture layer-wise safety-related representations across\nmodalities, and the Safety Pattern Auto-Encoder, which models the distribution\nof MSCAV derived from safe inputs and detects anomalies via reconstruction\nerrors. By training the auto-encoder (AE) solely on safe samples without attack\nlabels, LoD naturally identifies jailbreak inputs as distributional anomalies,\nenabling accurate and unified detection of jailbreak attacks. Comprehensive\nexperiments on three different LVLMs and five benchmarks demonstrate that LoD\nachieves state-of-the-art performance, with an average AUROC of 0.9951 and an\nimprovement of up to 38.89% in the minimum AUROC over the strongest baselines."
    },
    {
        "date": "2025-08",
        "title": "Robust Target Speaker Diarization and Separation via Augmented Speaker Embedding Sampling",
        "author": "Md Asif Jalal, Luca Remaggi, Vasileios Moschopoulos, Thanasis Kotsiopoulos, Vandana Rajan, Karthikeyan Saravanan, Anastasis Drosou, Junho Heo, Hyuk Oh, and Seokyeong Jeong",
        "link": "http://arxiv.org/abs/2508.06393v1",
        "abstract": "Traditional speech separation and speaker diarization approaches rely on\nprior knowledge of target speakers or a predetermined number of participants in\naudio signals. To address these limitations, recent advances focus on\ndeveloping enrollment-free methods capable of identifying targets without\nexplicit speaker labeling. This work introduces a new approach to train\nsimultaneous speech separation and diarization using automatic identification\nof target speaker embeddings, within mixtures. Our proposed model employs a\ndual-stage training pipeline designed to learn robust speaker representation\nfeatures that are resilient to background noise interference. Furthermore, we\npresent an overlapping spectral loss function specifically tailored for\nenhancing diarization accuracy during overlapped speech frames. Experimental\nresults show significant performance gains compared to the current SOTA\nbaseline, achieving 71% relative improvement in DER and 69% in cpWER."
    },
    {
        "date": "2025-08",
        "title": "FVGen: Accelerating Novel-View Synthesis with Adversarial Video Diffusion Distillation",
        "author": "Wenbin Teng, Gonglin Chen, Haiwei Chen, and Yajie Zhao",
        "link": "http://arxiv.org/abs/2508.06392v1",
        "abstract": "Recent progress in 3D reconstruction has enabled realistic 3D models from\ndense image captures, yet challenges persist with sparse views, often leading\nto artifacts in unseen areas. Recent works leverage Video Diffusion Models\n(VDMs) to generate dense observations, filling the gaps when only sparse views\nare available for 3D reconstruction tasks. A significant limitation of these\nmethods is their slow sampling speed when using VDMs. In this paper, we present\nFVGen, a novel framework that addresses this challenge by enabling fast novel\nview synthesis using VDMs in as few as four sampling steps. We propose a novel\nvideo diffusion model distillation method that distills a multi-step denoising\nteacher model into a few-step denoising student model using Generative\nAdversarial Networks (GANs) and softened reverse KL-divergence minimization.\nExtensive experiments on real-world datasets show that, compared to previous\nworks, our framework generates the same number of novel views with similar (or\neven better) visual quality while reducing sampling time by more than 90%.\nFVGen significantly improves time efficiency for downstream reconstruction\ntasks, particularly when working with sparse input views (more than 2) where\npre-trained VDMs need to be run multiple times to achieve better spatial\ncoverage."
    },
    {
        "date": "2025-08",
        "title": "Introducing Fractional Classification Loss for Robust Learning with Noisy Labels",
        "author": "Mert Can Kurucu, Tufan Kumbasar, \u0130brahim Eksin, and M\u00fcjde G\u00fczelkaya",
        "link": "http://arxiv.org/abs/2508.06346v1",
        "abstract": "Robust loss functions are crucial for training deep neural networks in the\npresence of label noise, yet existing approaches require extensive,\ndataset-specific hyperparameter tuning. In this work, we introduce Fractional\nClassification Loss (FCL), an adaptive robust loss that automatically\ncalibrates its robustness to label noise during training. Built within the\nactive-passive loss framework, FCL employs the fractional derivative of the\nCross-Entropy (CE) loss as its active component and the Mean Absolute Error\n(MAE) as its passive loss component. With this formulation, we demonstrate that\nthe fractional derivative order $\\mu$ spans a family of loss functions that\ninterpolate between MAE-like robustness and CE-like fast convergence.\nFurthermore, we integrate $\\mu$ into the gradient-based optimization as a\nlearnable parameter and automatically adjust it to optimize the trade-off\nbetween robustness and convergence speed. We reveal that FCL's unique property\nestablishes a critical trade-off that enables the stable learning of $\\mu$:\nlower log penalties on difficult or mislabeled examples improve robustness but\nimpose higher penalties on easy or clean data, reducing model confidence in\nthem. Consequently, FCL can dynamically reshape its loss landscape to achieve\neffective classification performance under label noise. Extensive experiments\non benchmark datasets show that FCL achieves state-of-the-art results without\nthe need for manual hyperparameter tuning."
    },
    {
        "date": "2025-08",
        "title": "Unsupervised Partner Design Enables Robust Ad-hoc Teamwork",
        "author": "Constantin Ruhdorfer, Matteo Bortoletto, Victor Oei, Anna Penzkofer, and Andreas Bulling",
        "link": "http://arxiv.org/abs/2508.06336v1",
        "abstract": "We introduce Unsupervised Partner Design (UPD) - a population-free,\nmulti-agent reinforcement learning framework for robust ad-hoc teamwork that\nadaptively generates training partners without requiring pretrained partners or\nmanual parameter tuning. UPD constructs diverse partners by stochastically\nmixing an ego agent's policy with biased random behaviours and scores them\nusing a variance-based learnability metric that prioritises partners near the\nego agent's current learning frontier. We show that UPD can be integrated with\nunsupervised environment design, resulting in the first method enabling fully\nunsupervised curricula over both level and partner distributions in a\ncooperative setting. Through extensive evaluations on Overcooked-AI and the\nOvercooked Generalisation Challenge, we demonstrate that this dynamic partner\ncurriculum is highly effective: UPD consistently outperforms both\npopulation-based and population-free baselines as well as ablations. In a user\nstudy, we further show that UPD achieves higher returns than all baselines and\nwas perceived as significantly more adaptive, more human-like, a better\ncollaborator, and less frustrating."
    },
    {
        "date": "2025-08",
        "title": "LLM Robustness Leaderboard v1 --Technical report",
        "author": "Pierre Peign\u00e9 - Lefebvre, Quentin Feuillade-Montixi, Tom David, and Nicolas Miailhe",
        "link": "http://arxiv.org/abs/2508.06296v2",
        "abstract": "This technical report accompanies the LLM robustness leaderboard published by\nPRISM Eval for the Paris AI Action Summit. We introduce PRISM Eval Behavior\nElicitation Tool (BET), an AI system performing automated red-teaming through\nDynamic Adversarial Optimization that achieves 100% Attack Success Rate (ASR)\nagainst 37 of 41 state-of-the-art LLMs. Beyond binary success metrics, we\npropose a fine-grained robustness metric estimating the average number of\nattempts required to elicit harmful behaviors, revealing that attack difficulty\nvaries by over 300-fold across models despite universal vulnerability. We\nintroduce primitive-level vulnerability analysis to identify which jailbreaking\ntechniques are most effective for specific hazard categories. Our collaborative\nevaluation with trusted third parties from the AI Safety Network demonstrates\npractical pathways for distributed robustness assessment across the community."
    },
    {
        "date": "2025-08",
        "title": "In-Training Defenses against Emergent Misalignment in Language Models",
        "author": "David Kacz\u00e9r, Magnus J\u00f8rgenv\u00e5g, Clemens Vetter, Lucie Flek, and Florian Mai",
        "link": "http://arxiv.org/abs/2508.06249v1",
        "abstract": "Fine-tuning lets practitioners repurpose aligned large language models (LLMs)\nfor new domains, yet recent work reveals emergent misalignment (EMA): Even a\nsmall, domain-specific fine-tune can induce harmful behaviors far outside the\ntarget domain. Even in the case where model weights are hidden behind a\nfine-tuning API, this gives attackers inadvertent access to a broadly\nmisaligned model in a way that can be hard to detect from the fine-tuning data\nalone. We present the first systematic study of in-training safeguards against\nEMA that are practical for providers who expose fine-tuning via an API. We\ninvestigate four training regularization interventions: (i) KL-divergence\nregularization toward a safe reference model, (ii) $\\ell_2$ distance in feature\nspace, (iii) projecting onto a safe subspace (SafeLoRA), and (iv) interleaving\nof a small amount of safe training examples from a general instruct-tuning\ndataset. We first evaluate the methods' emergent misalignment effect across\nfour malicious, EMA-inducing tasks. Second, we assess the methods' impacts on\nbenign tasks. We conclude with a discussion of open questions in emergent\nmisalignment research."
    },
    {
        "date": "2025-08",
        "title": "Membership Inference Attack with Partial Features",
        "author": "Xurun Wang, Guangrui Liu, Xinjie Li, Haoyu He, Lin Yao, and Weizhe Zhang",
        "link": "http://arxiv.org/abs/2508.06244v1",
        "abstract": "Machine learning models have been shown to be susceptible to membership\ninference attack, which can be used to determine whether a given sample appears\nin the training data. Existing membership inference methods commonly assume\nthat the adversary has full access to the features of the target sample. This\nassumption, however, does not hold in many real-world scenarios where only\npartial features information is available, thereby limiting the applicability\nof these methods. In this work, we study an inference scenario where the\nadversary observes only partial features of each sample and aims to infer\nwhether this observed subset was present in the training set of the target\nmodel. We define this problem as Partial Feature Membership Inference (PFMI).\nTo address this problem, we propose MRAD (Memory-guided Reconstruction and\nAnomaly Detection), a two-stage attack framework. In the first stage, MRAD\noptimizes the unknown feature values to minimize the loss of the sample. In the\nsecond stage, it measures the deviation between the reconstructed sample and\nthe training distribution using anomaly detection. Empirical results\ndemonstrate that MRAD is effective across a range of datasets, and maintains\ncompatibility with various off-the-shelf anomaly detection techniques. For\nexample, on STL-10, our attack achieves an AUC of around 0.6 even with 40% of\nthe missing features."
    },
    {
        "date": "2025-08",
        "title": "SLIP: Soft Label Mechanism and Key-Extraction-Guided CoT-based Defense Against Instruction Backdoor in APIs",
        "author": "Zhengxian Wu, Juan Wen, Wanli Peng, Haowei Chang, Yinghan Zhou, and Yiming Xue",
        "link": "http://arxiv.org/abs/2508.06153v1",
        "abstract": "With the development of customized large language model (LLM) agents, a new\nthreat of black-box backdoor attacks has emerged, where malicious instructions\nare injected into hidden system prompts. These attacks easily bypass existing\ndefenses that rely on white-box access, posing a serious security challenge. To\naddress this, we propose SLIP, a Soft Label mechanism and key-extraction-guided\nCoT-based defense against Instruction backdoors in APIs. SLIP is designed based\non two key insights. First, to counteract the model's oversensitivity to\ntriggers, we propose a Key-extraction-guided Chain-of-Thought (KCoT). Instead\nof only considering the single trigger or the input sentence, KCoT prompts the\nagent to extract task-relevant key phrases. Second, to guide the LLM toward\ncorrect answers, our proposed Soft Label Mechanism (SLM) prompts the agent to\nquantify the semantic correlation between key phrases and candidate answers.\nCrucially, to mitigate the influence of residual triggers or misleading content\nin phrases extracted by KCoT, which typically causes anomalous scores, SLM\nexcludes anomalous scores deviating significantly from the mean and\nsubsequently averages the remaining scores to derive a more reliable semantic\nrepresentation. Extensive experiments on classification and question-answer\n(QA) tasks demonstrate that SLIP is highly effective, reducing the average\nattack success rate (ASR) from 90.2% to 25.13% while maintaining high accuracy\non clean data and outperforming state-of-the-art defenses. Our code are\navailable in\nhttps://github.com/CAU-ISS-Lab/Backdoor-Attack-Defense-LLMs/tree/main/SLIP."
    },
    {
        "date": "2025-08",
        "title": "Study of Robust Features in Formulating Guidance for Heuristic Algorithms for Solving the Vehicle Routing Problem",
        "author": "Bachtiar Herdianto, Romain Billot, Flavien Lucas, and Marc Sevaux",
        "link": "http://arxiv.org/abs/2508.06129v1",
        "abstract": "The Vehicle Routing Problem (VRP) is a complex optimization problem with\nnumerous real-world applications, mostly solved using metaheuristic algorithms\ndue to its $\\mathcal{NP}$-Hard nature. Traditionally, these metaheuristics rely\non human-crafted designs developed through empirical studies. However, recent\nresearch shows that machine learning methods can be used the structural\ncharacteristics of solutions in combinatorial optimization, thereby aiding in\ndesigning more efficient algorithms, particularly for solving VRP. Building on\nthis advancement, this study extends the previous research by conducting a\nsensitivity analysis using multiple classifier models that are capable of\npredicting the quality of VRP solutions. Hence, by leveraging explainable AI,\nthis research is able to extend the understanding of how these models make\ndecisions. Finally, our findings indicate that while feature importance varies,\ncertain features consistently emerge as strong predictors. Furthermore, we\npropose a unified framework able of ranking feature impact across different\nscenarios to illustrate this finding. These insights highlight the potential of\nfeature importance analysis as a foundation for developing a guidance mechanism\nof metaheuristic algorithms for solving the VRP."
    },
    {
        "date": "2025-08",
        "title": "SAM Encoder Breach by Adversarial Simplicial Complex Triggers Downstream Model Failures",
        "author": "Yi Qin, Rui Wang, Tao Huang, Tong Xiao, and Liping Jing",
        "link": "http://arxiv.org/abs/2508.06127v1",
        "abstract": "While the Segment Anything Model (SAM) transforms interactive segmentation\nwith zero-shot abilities, its inherent vulnerabilities present a single-point\nrisk, potentially leading to the failure of numerous downstream applications.\nProactively evaluating these transferable vulnerabilities is thus imperative.\nPrior adversarial attacks on SAM often present limited transferability due to\ninsufficient exploration of common weakness across domains. To address this, we\npropose Vertex-Refining Simplicial Complex Attack (VeSCA), a novel method that\nleverages only the encoder of SAM for generating transferable adversarial\nexamples. Specifically, it achieves this by explicitly characterizing the\nshared vulnerable regions between SAM and downstream models through a\nparametric simplicial complex. Our goal is to identify such complexes within\nadversarially potent regions by iterative vertex-wise refinement. A lightweight\ndomain re-adaptation strategy is introduced to bridge domain divergence using\nminimal reference data during the initialization of simplicial complex.\nUltimately, VeSCA generates consistently transferable adversarial examples\nthrough random simplicial complex sampling. Extensive experiments demonstrate\nthat VeSCA achieves performance improved by 12.7% compared to state-of-the-art\nmethods across three downstream model categories across five domain-specific\ndatasets. Our findings further highlight the downstream model risks posed by\nSAM's vulnerabilities and emphasize the urgency of developing more robust\nfoundation models."
    },
    {
        "date": "2025-08",
        "title": "ProvX: Generating Counterfactual-Driven Attack Explanations for Provenance-Based Detection",
        "author": "Weiheng Wu, Wei Qiao, Teng Li, Yebo Feng, Zhuo Ma, Jianfeng Ma, and Yang Liu",
        "link": "http://arxiv.org/abs/2508.06073v1",
        "abstract": "Provenance graph-based intrusion detection systems are deployed on hosts to\ndefend against increasingly severe Advanced Persistent Threat. Using Graph\nNeural Networks to detect these threats has become a research focus and has\ndemonstrated exceptional performance. However, the widespread adoption of\nGNN-based security models is limited by their inherent black-box nature, as\nthey fail to provide security analysts with any verifiable explanations for\nmodel predictions or any evidence regarding the model's judgment in relation to\nreal-world attacks. To address this challenge, we propose ProvX, an effective\nexplanation framework for exlaining GNN-based security models on provenance\ngraphs. ProvX introduces counterfactual explanation logic, seeking the minimal\nstructural subset within a graph predicted as malicious that, when perturbed,\ncan subvert the model's original prediction. We innovatively transform the\ndiscrete search problem of finding this critical subgraph into a continuous\noptimization task guided by a dual objective of prediction flipping and\ndistance minimization. Furthermore, a Staged Solidification strategy is\nincorporated to enhance the precision and stability of the explanations. We\nconducted extensive evaluations of ProvX on authoritative datasets. The\nexperimental results demonstrate that ProvX can locate critical graph\nstructures that are highly relevant to real-world attacks and achieves an\naverage explanation necessity of 51.59\\%, with these metrics outperforming\ncurrent SOTA explainers. Furthermore, we explore and provide a preliminary\nvalidation of a closed-loop Detection-Explanation-Feedback enhancement\nframework, demonstrating through experiments that the explanation results from\nProvX can guide model optimization, effectively enhancing its robustness\nagainst adversarial attacks."
    },
    {
        "date": "2025-08",
        "title": "A Game-Theoretic Foundation for Bitcoin's Price: A Security-Utility Equilibrium",
        "author": "Liang Chen",
        "link": "http://arxiv.org/abs/2508.06071v2",
        "abstract": "This paper introduces a structural game-theoretic model to value\ndecentralized digital assets like Bitcoin. Instead of relying on speculative\nbeliefs, it frames the asset's price within a Rational-Expectations\nSecurity-Utility Nash Equilibrium (RESUNE). This equilibrium is a fixed point\nwhere the market-clearing price dictates the hash rate through a free-entry\nmining model, which in turn endogenously sets the network's security. The\nsecurity, defined as one minus the probability of a 51% attack, is determined\nvia a global games model of attacker coordination, providing a unique and\ncontinuous security function. We prove the existence of a RESUNE and offer\nconditions for its uniqueness and stability. The model predicts that the\nstabilizing direct effect of price on demand must outweigh the potentially\ndestabilizing feedback from price to security. The framework generates testable\npredictions, such as a protocol halving causing a contraction in both hash rate\nand price. A structural Vector Autoregression (VAR) model is proposed to test\nthis mechanism. The model decomposes Bitcoin's value into transactional\nutility, security, and speculative components and explains the observed\nunidirectional causality from price to hash rate."
    },
    {
        "date": "2025-08",
        "title": "Fact2Fiction: Targeted Poisoning Attack to Agentic Fact-checking System",
        "author": "Haorui He, Yupeng Li, Bin Benjamin Zhu, Dacheng Wen, Reynold Cheng, and Francis C. M. Lau",
        "link": "http://arxiv.org/abs/2508.06059v1",
        "abstract": "State-of-the-art fact-checking systems combat misinformation at scale by\nemploying autonomous LLM-based agents to decompose complex claims into smaller\nsub-claims, verify each sub-claim individually, and aggregate the partial\nresults to produce verdicts with justifications (explanatory rationales for the\nverdicts). The security of these systems is crucial, as compromised\nfact-checkers, which tend to be easily underexplored, can amplify\nmisinformation. This work introduces Fact2Fiction, the first poisoning attack\nframework targeting such agentic fact-checking systems. Fact2Fiction mirrors\nthe decomposition strategy and exploits system-generated justifications to\ncraft tailored malicious evidences that compromise sub-claim verification.\nExtensive experiments demonstrate that Fact2Fiction achieves 8.9\\%--21.2\\%\nhigher attack success rates than state-of-the-art attacks across various\npoisoning budgets. Fact2Fiction exposes security weaknesses in current\nfact-checking systems and highlights the need for defensive countermeasures."
    },
    {
        "date": "2025-08",
        "title": "REFS: Robust EEG feature selection with missing multi-dimensional annotation for emotion recognition",
        "author": "Xueyuan Xu, Wenjia Dong, Fulin Wei, and Li Zhuo",
        "link": "http://arxiv.org/abs/2508.05933v1",
        "abstract": "The affective brain-computer interface is a crucial technology for affective\ninteraction and emotional intelligence, emerging as a significant area of\nresearch in the human-computer interaction. Compared to single-type features,\nmulti-type EEG features provide a multi-level representation for analyzing\nmulti-dimensional emotions. However, the high dimensionality of multi-type EEG\nfeatures, combined with the relatively small number of high-quality EEG\nsamples, poses challenges such as classifier overfitting and suboptimal\nreal-time performance in multi-dimensional emotion recognition. Moreover,\npractical applications of affective brain-computer interface frequently\nencounters partial absence of multi-dimensional emotional labels due to the\nopen nature of the acquisition environment, and ambiguity and variability in\nindividual emotion perception. To address these challenges, this study proposes\na novel EEG feature selection method for missing multi-dimensional emotion\nrecognition. The method leverages adaptive orthogonal non-negative matrix\nfactorization to reconstruct the multi-dimensional emotional label space\nthrough second-order and higher-order correlations, which could reduce the\nnegative impact of missing values and outliers on label reconstruction.\nSimultaneously, it employs least squares regression with graph-based manifold\nlearning regularization and global feature redundancy minimization\nregularization to enable EEG feature subset selection despite missing\ninformation, ultimately achieving robust EEG-based multi-dimensional emotion\nrecognition. Simulation experiments on three widely used multi-dimensional\nemotional datasets, DREAMER, DEAP and HDED, reveal that the proposed method\noutperforms thirteen advanced feature selection methods in terms of robustness\nfor EEG emotional feature selection."
    },
    {
        "date": "2025-08",
        "title": "Robust Image Stitching with Optimal Plane",
        "author": "Lang Nie, Yuan Mei, Kang Liao, Yunqiu Xu, Chunyu Lin, and Bin Xiao",
        "link": "http://arxiv.org/abs/2508.05903v1",
        "abstract": "We present \\textit{RopStitch}, an unsupervised deep image stitching framework\nwith both robustness and naturalness. To ensure the robustness of\n\\textit{RopStitch}, we propose to incorporate the universal prior of content\nperception into the image stitching model by a dual-branch architecture. It\nseparately captures coarse and fine features and integrates them to achieve\nhighly generalizable performance across diverse unseen real-world scenes.\nConcretely, the dual-branch model consists of a pretrained branch to capture\nsemantically invariant representations and a learnable branch to extract\nfine-grained discriminative features, which are then merged into a whole by a\ncontrollable factor at the correlation level. Besides, considering that content\nalignment and structural preservation are often contradictory to each other, we\npropose a concept of virtual optimal planes to relieve this conflict. To this\nend, we model this problem as a process of estimating homography decomposition\ncoefficients, and design an iterative coefficient predictor and minimal\nsemantic distortion constraint to identify the optimal plane. This scheme is\nfinally incorporated into \\textit{RopStitch} by warping both views onto the\noptimal plane bidirectionally. Extensive experiments across various datasets\ndemonstrate that \\textit{RopStitch} significantly outperforms existing methods,\nparticularly in scene robustness and content naturalness. The code is available\nat {\\color{red}https://github.com/MmelodYy/RopStitch}."
    },
    {
        "date": "2025-08",
        "title": "Secure and Scalable Blockchain Voting: A Comparative Framework and the Role of Large Language Models",
        "author": "Kiana Kiashemshaki, Elvis Nnaemeka Chukwuani, Mohammad Jalili Torkamani, and Negin Mahmoudi",
        "link": "http://arxiv.org/abs/2508.05865v1",
        "abstract": "Blockchain technology offers a promising foundation for modernizing E-Voting\nsystems by enhancing transparency, decentralization, and security. Yet,\nreal-world adoption remains limited due to persistent challenges such as\nscalability constraints, high computational demands, and complex privacy\nrequirements. This paper presents a comparative framework for analyzing\nblockchain-based E-Voting architectures, consensus mechanisms, and\ncryptographic protocols. We examine the limitations of prevalent models like\nProof of Work, Proof of Stake, and Delegated Proof of Stake, and propose\noptimization strategies that include hybrid consensus, lightweight\ncryptography, and decentralized identity management. Additionally, we explore\nthe novel role of Large Language Models (LLMs) in smart contract generation,\nanomaly detection, and user interaction. Our findings offer a foundation for\ndesigning secure, scalable, and intelligent blockchain-based E-Voting systems\nsuitable for national-scale deployment. This work lays the groundwork for\nbuilding an end-to-end blockchain E-Voting prototype enhanced by LLM-guided\nsmart contract generation and validation, supported by a systematic framework\nand simulation-based analysis."
    },
    {
        "date": "2025-08",
        "title": "A Neurosymbolic Framework for Interpretable Cognitive Attack Detection in Augmented Reality",
        "author": "Rongqian Chen, Allison Andreyev, Yanming Xiu, Mahdi Imani, Bin Li, Maria Gorlatova, Gang Tan, and Tian Lan",
        "link": "http://arxiv.org/abs/2508.09185v2",
        "abstract": "Augmented Reality (AR) enriches perception by overlaying virtual elements on\nthe physical world. Due to its growing popularity, cognitive attacks that alter\nAR content to manipulate users' semantic perception have received increasing\nattention. Existing detection methods often focus on visual changes, which are\nrestricted to pixel- or image-level processing and lack semantic reasoning\ncapabilities, or they rely on pre-trained vision-language models (VLMs), which\nfunction as black-box approaches with limited interpretability. In this paper,\nwe present CADAR, a novel neurosymbolic approach for cognitive attack detection\nin AR. It fuses multimodal vision-language inputs using neural VLMs to obtain a\nsymbolic perception-graph representation, incorporating prior knowledge,\nsalience weighting, and temporal correlations. The model then enables\nparticle-filter based statistical reasoning -- a sequential Monte Carlo method\n-- to detect cognitive attacks. Thus, CADAR inherits the adaptability of\npre-trained VLM and the interpretability and reasoning rigor of particle\nfiltering. Experiments on an extended AR cognitive attack dataset show accuracy\nimprovements of up to 10.7% over strong baselines on challenging AR attack\nscenarios, underscoring the promise of neurosymbolic methods for effective and\ninterpretable cognitive attack detection."
    },
    {
        "date": "2025-08",
        "title": "FS-IQA: Certified Feature Smoothing for Robust Image Quality Assessment",
        "author": "Ekaterina Shumitskaya, Dmitriy Vatolin, and Anastasia Antsiferova",
        "link": "http://arxiv.org/abs/2508.05516v1",
        "abstract": "We propose a novel certified defense method for Image Quality Assessment\n(IQA) models based on randomized smoothing with noise applied in the feature\nspace rather than the input space. Unlike prior approaches that inject Gaussian\nnoise directly into input images, often degrading visual quality, our method\npreserves image fidelity while providing robustness guarantees. To formally\nconnect noise levels in the feature space with corresponding input-space\nperturbations, we analyze the maximum singular value of the backbone network's\nJacobian. Our approach supports both full-reference (FR) and no-reference (NR)\nIQA models without requiring any architectural modifications, suitable for\nvarious scenarios. It is also computationally efficient, requiring a single\nbackbone forward pass per image. Compared to previous methods, it reduces\ninference time by 99.5% without certification and by 20.6% when certification\nis applied. We validate our method with extensive experiments on two benchmark\ndatasets, involving six widely-used FR and NR IQA models and comparisons\nagainst five state-of-the-art certified defenses. Our results demonstrate\nconsistent improvements in correlation with subjective quality scores by up to\n30.9%."
    },
    {
        "date": "2025-08",
        "title": "Keep It Real: Challenges in Attacking Compression-Based Adversarial Purification",
        "author": "Samuel R\u00e4ber, Till Aczel, Andreas Plesner, and Roger Wattenhofer",
        "link": "http://arxiv.org/abs/2508.05489v1",
        "abstract": "Previous work has suggested that preprocessing images through lossy\ncompression can defend against adversarial perturbations, but comprehensive\nattack evaluations have been lacking. In this paper, we construct strong\nwhite-box and adaptive attacks against various compression models and identify\na critical challenge for attackers: high realism in reconstructed images\nsignificantly increases attack difficulty. Through rigorous evaluation across\nmultiple attack scenarios, we demonstrate that compression models capable of\nproducing realistic, high-fidelity reconstructions are substantially more\nresistant to our attacks. In contrast, low-realism compression models can be\nbroken. Our analysis reveals that this is not due to gradient masking. Rather,\nrealistic reconstructions maintaining distributional alignment with natural\nimages seem to offer inherent robustness. This work highlights a significant\nobstacle for future adversarial attacks and suggests that developing more\neffective techniques to overcome realism represents an essential challenge for\ncomprehensive security evaluation."
    },
    {
        "date": "2025-08",
        "title": "Task complexity shapes internal representations and robustness in neural networks",
        "author": "Robert Jankowski, Filippo Radicchi, M. \u00c1ngeles Serrano, Mari\u00e1n Bogu\u00f1\u00e1, and Santo Fortunato",
        "link": "http://arxiv.org/abs/2508.05463v1",
        "abstract": "Neural networks excel across a wide range of tasks, yet remain black boxes.\nIn particular, how their internal representations are shaped by the complexity\nof the input data and the problems they solve remains obscure. In this work, we\nintroduce a suite of five data-agnostic probes-pruning, binarization, noise\ninjection, sign flipping, and bipartite network randomization-to quantify how\ntask difficulty influences the topology and robustness of representations in\nmultilayer perceptrons (MLPs). MLPs are represented as signed, weighted\nbipartite graphs from a network science perspective. We contrast easy and hard\nclassification tasks on the MNIST and Fashion-MNIST datasets. We show that\nbinarizing weights in hard-task models collapses accuracy to chance, whereas\neasy-task models remain robust. We also find that pruning low-magnitude edges\nin binarized hard-task models reveals a sharp phase-transition in performance.\nMoreover, moderate noise injection can enhance accuracy, resembling a\nstochastic-resonance effect linked to optimal sign flips of small-magnitude\nweights. Finally, preserving only the sign structure-instead of precise weight\nmagnitudes-through bipartite network randomizations suffices to maintain high\naccuracy. These phenomena define a model- and modality-agnostic measure of task\ncomplexity: the performance gap between full-precision and binarized or\nshuffled neural network performance. Our findings highlight the crucial role of\nsigned bipartite topology in learned representations and suggest practical\nstrategies for model compression and interpretability that align with task\ncomplexity."
    },
    {
        "date": "2025-08",
        "title": "Physical Adversarial Camouflage through Gradient Calibration and Regularization",
        "author": "Jiawei Liang, Siyuan Liang, Jianjie Huang, Chenxi Si, Ming Zhang, and Xiaochun Cao",
        "link": "http://arxiv.org/abs/2508.05414v1",
        "abstract": "The advancement of deep object detectors has greatly affected safety-critical\nfields like autonomous driving. However, physical adversarial camouflage poses\na significant security risk by altering object textures to deceive detectors.\nExisting techniques struggle with variable physical environments, facing two\nmain challenges: 1) inconsistent sampling point densities across distances\nhinder the gradient optimization from ensuring local continuity, and 2)\nupdating texture gradients from multiple angles causes conflicts, reducing\noptimization stability and attack effectiveness. To address these issues, we\npropose a novel adversarial camouflage framework based on gradient\noptimization. First, we introduce a gradient calibration strategy, which\nensures consistent gradient updates across distances by propagating gradients\nfrom sparsely to unsampled texture points. Additionally, we develop a gradient\ndecorrelation method, which prioritizes and orthogonalizes gradients based on\nloss values, enhancing stability and effectiveness in multi-angle optimization\nby eliminating redundant or conflicting updates. Extensive experimental results\non various detection models, angles and distances show that our method\nsignificantly exceeds the state of the art, with an average increase in attack\nsuccess rate (ASR) of 13.46% across distances and 11.03% across angles.\nFurthermore, empirical evaluation in real-world scenarios highlights the need\nfor more robust system design."
    },
    {
        "date": "2025-08",
        "title": "NT-ML: Backdoor Defense via Non-target Label Training and Mutual Learning",
        "author": "Wenjie Huo, and Katinka Wolter",
        "link": "http://arxiv.org/abs/2508.05404v1",
        "abstract": "Recent studies have shown that deep neural networks (DNNs) are vulnerable to\nbackdoor attacks, where a designed trigger is injected into the dataset,\ncausing erroneous predictions when activated. In this paper, we propose a novel\ndefense mechanism, Non-target label Training and Mutual Learning (NT-ML), which\ncan successfully restore the poisoned model under advanced backdoor attacks. NT\naims to reduce the harm of poisoned data by retraining the model with the\noutputs of the standard training. At this stage, a teacher model with high\naccuracy on clean data and a student model with higher confidence in correct\nprediction on poisoned data are obtained. Then, the teacher and student can\nlearn the strengths from each other through ML to obtain a purified student\nmodel. Extensive experiments show that NT-ML can effectively defend against 6\nbackdoor attacks with a small number of clean samples, and outperforms 5\nstate-of-the-art backdoor defenses."
    },
    {
        "date": "2025-08",
        "title": "Secure and practical Quantum Digital Signatures",
        "author": "Federico Grasselli, Gaetano Russo, and Massimiliano Proietti",
        "link": "http://arxiv.org/abs/2508.05355v1",
        "abstract": "Digital signatures represent a crucial cryptographic asset that must be\nprotected against quantum adversaries. Quantum Digital Signatures (QDS) can\noffer solutions that are information-theoretically (IT) secure and thus immune\nto quantum attacks. In this work, we analyze three existing practical QDS\nprotocols based on preshared secure keys (e.g., established with quantum key\ndistribution) and universal hashing families. For each protocol, we make\namendments to close potential loopholes and prove their IT security while\naccounting for the failure of IT-secure authenticated communication. We then\nnumerically optimize the protocol parameters to improve efficiency in terms of\npreshared bit consumption and signature length, allowing us to identify the\nmost efficient protocol."
    },
    {
        "date": "2025-08",
        "title": "CoCAViT: Compact Vision Transformer with Robust Global Coordination",
        "author": "Xuyang Wang, Lingjuan Miao, and Zhiqiang Zhou",
        "link": "http://arxiv.org/abs/2508.05307v1",
        "abstract": "In recent years, large-scale visual backbones have demonstrated remarkable\ncapabilities in learning general-purpose features from images via extensive\npre-training. Concurrently, many efficient architectures have emerged that have\nperformance comparable to that of larger models on in-domain benchmarks.\nHowever, we observe that for smaller models, the performance drop on\nout-of-distribution (OOD) data is disproportionately larger, indicating a\ndeficiency in the generalization performance of existing efficient models. To\naddress this, we identify key architectural bottlenecks and inappropriate\ndesign choices that contribute to this issue, retaining robustness for smaller\nmodels. To restore the global field of pure window attention, we further\nintroduce a Coordinator-patch Cross Attention (CoCA) mechanism, featuring\ndynamic, domain-aware global tokens that enhance local-global feature modeling\nand adaptively capture robust patterns across domains with minimal\ncomputational overhead. Integrating these advancements, we present CoCAViT, a\nnovel visual backbone designed for robust real-time visual representation.\nExtensive experiments empirically validate our design. At a resolution of\n224*224, CoCAViT-28M achieves 84.0% top-1 accuracy on ImageNet-1K, with\nsignificant gains on multiple OOD benchmarks, compared to competing models. It\nalso attains 52.2 mAP on COCO object detection and 51.3 mIOU on ADE20K semantic\nsegmentation, while maintaining low latency."
    },
    {
        "date": "2025-08",
        "title": "Robust Tracking with Particle Filtering for Fluorescent Cardiac Imaging",
        "author": "Suresh Guttikonda, Maximilian Neidhart, Johanna Sprenger, Johannes Petersen, Christian Detter, and Alexander Schlaefer",
        "link": "http://arxiv.org/abs/2508.05262v1",
        "abstract": "Intraoperative fluorescent cardiac imaging enables quality control following\ncoronary bypass grafting surgery. We can estimate local quantitative\nindicators, such as cardiac perfusion, by tracking local feature points.\nHowever, heart motion and significant fluctuations in image characteristics\ncaused by vessel structural enrichment limit traditional tracking methods. We\npropose a particle filtering tracker based on cyclicconsistency checks to\nrobustly track particles sampled to follow target landmarks. Our method tracks\n117 targets simultaneously at 25.4 fps, allowing real-time estimates during\ninterventions. It achieves a tracking error of (5.00 +/- 0.22 px) and\noutperforms other deep learning trackers (22.3 +/- 1.1 px) and conventional\ntrackers (58.1 +/- 27.1 px)."
    },
    {
        "date": "2025-08",
        "title": "Navigating the Trade-off: A Synthesis of Defensive Strategies for Zero-Shot Adversarial Robustness in Vision-Language Models",
        "author": "Zane Xu, and Jason Sun",
        "link": "http://arxiv.org/abs/2508.05237v1",
        "abstract": "This report synthesizes eight seminal papers on the zero-shot adversarial\nrobustness of vision-language models (VLMs) like CLIP. A central challenge in\nthis domain is the inherent trade-off between enhancing adversarial robustness\nand preserving the model's zero-shot generalization capabilities. We analyze\ntwo primary defense paradigms: Adversarial Fine-Tuning (AFT), which modifies\nmodel parameters, and Training-Free/Test-Time Defenses, which preserve them. We\ntrace the evolution from alignment-preserving methods (TeCoA) to embedding\nspace re-engineering (LAAT, TIMA), and from input heuristics (AOM, TTC) to\nlatent-space purification (CLIPure). Finally, we identify key challenges and\nfuture directions including hybrid defense strategies and adversarial\npre-training."
    },
    {
        "date": "2025-08",
        "title": "PhysPatch: A Physically Realizable and Transferable Adversarial Patch Attack for Multimodal Large Language Models-based Autonomous Driving Systems",
        "author": "Qi Guo, Xiaojun Jia, Shanmin Pang, Simeng Qin, Lin Wang, Ju Jia, Yang Liu, and Qing Guo",
        "link": "http://arxiv.org/abs/2508.05167v1",
        "abstract": "Multimodal Large Language Models (MLLMs) are becoming integral to autonomous\ndriving (AD) systems due to their strong vision-language reasoning\ncapabilities. However, MLLMs are vulnerable to adversarial attacks,\nparticularly adversarial patch attacks, which can pose serious threats in\nreal-world scenarios. Existing patch-based attack methods are primarily\ndesigned for object detection models and perform poorly when transferred to\nMLLM-based systems due to the latter's complex architectures and reasoning\nabilities. To address these limitations, we propose PhysPatch, a physically\nrealizable and transferable adversarial patch framework tailored for MLLM-based\nAD systems. PhysPatch jointly optimizes patch location, shape, and content to\nenhance attack effectiveness and real-world applicability. It introduces a\nsemantic-based mask initialization strategy for realistic placement, an\nSVD-based local alignment loss with patch-guided crop-resize to improve\ntransferability, and a potential field-based mask refinement method. Extensive\nexperiments across open-source, commercial, and reasoning-capable MLLMs\ndemonstrate that PhysPatch significantly outperforms prior methods in steering\nMLLM-based AD systems toward target-aligned perception and planning outputs.\nMoreover, PhysPatch consistently places adversarial patches in physically\nfeasible regions of AD scenes, ensuring strong real-world applicability and\ndeployability."
    },
    {
        "date": "2025-08",
        "title": "G-UBS: Towards Robust Understanding of Implicit Feedback via Group-Aware User Behavior Simulation",
        "author": "Boyu Chen, Siran Chen, Zhengrong Yue, Kainan Yan, Chenyun Yu, Beibei Kong, Cheng Lei, Chengxiang Zhuo, Zang Li, and Yali Wang",
        "link": "http://arxiv.org/abs/2508.05709v1",
        "abstract": "User feedback is critical for refining recommendation systems, yet explicit\nfeedback (e.g., likes or dislikes) remains scarce in practice. As a more\nfeasible alternative, inferring user preferences from massive implicit feedback\nhas shown great potential (e.g., a user quickly skipping a recommended video\nusually indicates disinterest). Unfortunately, implicit feedback is often\nnoisy: a user might skip a video due to accidental clicks or other reasons,\nrather than disliking it. Such noise can easily misjudge user interests,\nthereby undermining recommendation performance. To address this issue, we\npropose a novel Group-aware User Behavior Simulation (G-UBS) paradigm, which\nleverages contextual guidance from relevant user groups, enabling robust and\nin-depth interpretation of implicit feedback for individual users.\nSpecifically, G-UBS operates via two key agents. First, the User Group Manager\n(UGM) effectively clusters users to generate group profiles utilizing a\n``summarize-cluster-reflect\" workflow based on LLMs. Second, the User Feedback\nModeler (UFM) employs an innovative group-aware reinforcement learning\napproach, where each user is guided by the associated group profiles during the\nreinforcement learning process, allowing UFM to robustly and deeply examine the\nreasons behind implicit feedback. To assess our G-UBS paradigm, we have\nconstructed a Video Recommendation benchmark with Implicit Feedback (IF-VR). To\nthe best of our knowledge, this is the first multi-modal benchmark for implicit\nfeedback evaluation in video recommendation, encompassing 15k users, 25k\nvideos, and 933k interaction records with implicit feedback. Extensive\nexperiments on IF-VR demonstrate that G-UBS significantly outperforms\nmainstream LLMs and MLLMs, with a 4.0% higher proportion of videos achieving a\nplay rate > 30% and 14.9% higher reasoning accuracy on IF-VR."
    },
    {
        "date": "2025-08",
        "title": "FLUX-Makeup: High-Fidelity, Identity-Consistent, and Robust Makeup Transfer via Diffusion Transformer",
        "author": "Jian Zhu, Shanyuan Liu, Liuzhuozheng Li, Yue Gong, He Wang, Bo Cheng, Yuhang Ma, Liebucha Wu, Xiaoyu Wu, Dawei Leng, Yuhui Yin, and Yang Xu",
        "link": "http://arxiv.org/abs/2508.05069v1",
        "abstract": "Makeup transfer aims to apply the makeup style from a reference face to a\ntarget face and has been increasingly adopted in practical applications.\nExisting GAN-based approaches typically rely on carefully designed loss\nfunctions to balance transfer quality and facial identity consistency, while\ndiffusion-based methods often depend on additional face-control modules or\nalgorithms to preserve identity. However, these auxiliary components tend to\nintroduce extra errors, leading to suboptimal transfer results. To overcome\nthese limitations, we propose FLUX-Makeup, a high-fidelity,\nidentity-consistent, and robust makeup transfer framework that eliminates the\nneed for any auxiliary face-control components. Instead, our method directly\nleverages source-reference image pairs to achieve superior transfer\nperformance. Specifically, we build our framework upon FLUX-Kontext, using the\nsource image as its native conditional input. Furthermore, we introduce\nRefLoRAInjector, a lightweight makeup feature injector that decouples the\nreference pathway from the backbone, enabling efficient and comprehensive\nextraction of makeup-related information. In parallel, we design a robust and\nscalable data generation pipeline to provide more accurate supervision during\ntraining. The paired makeup datasets produced by this pipeline significantly\nsurpass the quality of all existing datasets. Extensive experiments demonstrate\nthat FLUX-Makeup achieves state-of-the-art performance, exhibiting strong\nrobustness across diverse scenarios."
    },
    {
        "date": "2025-08",
        "title": "Automatic Image Colorization with Convolutional Neural Networks and Generative Adversarial Networks",
        "author": "Changyuan Qiu, Hangrui Cao, Qihan Ren, Ruiyu Li, and Yuqing Qiu",
        "link": "http://arxiv.org/abs/2508.05068v2",
        "abstract": "Image colorization, the task of adding colors to grayscale images, has been\nthe focus of significant research efforts in computer vision in recent years\nfor its various application areas such as color restoration and automatic\nanimation colorization [15, 1]. The colorization problem is challenging as it\nis highly ill-posed with two out of three image dimensions lost, resulting in\nlarge degrees of freedom. However, semantics of the scene as well as the\nsurface texture could provide important cues for colors: the sky is typically\nblue, the clouds are typically white and the grass is typically green, and\nthere are huge amounts of training data available for learning such priors\nsince any colored image could serve as a training data point [20].\n  Colorization is initially formulated as a regression task[5], which ignores\nthe multi-modal nature of color prediction. In this project, we explore\nautomatic image colorization via classification and adversarial learning. We\nwill build our models on prior works, apply modifications for our specific\nscenario and make comparisons."
    },
    {
        "date": "2025-08",
        "title": "System Security Framework for 5G Advanced /6G IoT Integrated Terrestrial Network-Non-Terrestrial Network (TN-NTN) with AI-Enabled Cloud Security",
        "author": "Sasa Maric, Rasil Baidar, Robert Abbas, and Sam Reisenfeld",
        "link": "http://arxiv.org/abs/2508.05707v1",
        "abstract": "The integration of Terrestrial Networks (TN) and Non-Terrestrial Networks\n(NTN), including 5G Advanced/6G and the Internet of Things (IoT) technologies,\nusing Low Earth Orbit (LEO) satellites, high-altitude platforms (HAPS), and\nUnmanned Aerial Vehicles (UAVs), is redefining the landscape of global\nconnectivity. This paper introduces a new system-level security framework for\n5G Advanced/6G IoT-integrated TN-NTN architectures with AI-native-enabled cloud\nsecurity. Due to the heterogeneity, scale, and distributed nature of these\nnetworks, new security challenges have emerged. Leveraging AI-native cloud\nplatforms offers powerful capabilities for real-time threat detection, security\nautomation, and intelligent policy enforcement. The NTN satellite access\nfunction enhances security for discontinuous coverage via satellite\nconnections. In addition, this paper explores the security risks associated\nwith integrated 5G Advanced/6G IoT TN-NTN systems, including full network\nsegmentation, network slicing, and the cloudification of the RAN and core. We\npresent a comprehensive AI-enabled cloud security framework and conclude with\nproposals for implementing AI-powered, satellite-based NTN within future 5G\nAdvanced/6G IoT networks. Our approach emphasizes zero-trust principles,\nfederated learning, secure orchestration, a layered security framework, and\nresilience against adversarial threats."
    },
    {
        "date": "2025-08",
        "title": "AdvDINO: Domain-Adversarial Self-Supervised Representation Learning for Spatial Proteomics",
        "author": "Stella Su, Marc Harary, Scott J. Rodig, and William Lotter",
        "link": "http://arxiv.org/abs/2508.04955v1",
        "abstract": "Self-supervised learning (SSL) has emerged as a powerful approach for\nlearning visual representations without manual annotations. However, the\nrobustness of standard SSL methods to domain shift -- systematic differences\nacross data sources -- remains uncertain, posing an especially critical\nchallenge in biomedical imaging where batch effects can obscure true biological\nsignals. We present AdvDINO, a domain-adversarial self-supervised learning\nframework that integrates a gradient reversal layer into the DINOv2\narchitecture to promote domain-invariant feature learning. Applied to a\nreal-world cohort of six-channel multiplex immunofluorescence (mIF) whole slide\nimages from non-small cell lung cancer patients, AdvDINO mitigates\nslide-specific biases to learn more robust and biologically meaningful\nrepresentations than non-adversarial baselines. Across $>5.46$ million mIF\nimage tiles, the model uncovers phenotype clusters with distinct proteomic\nprofiles and prognostic significance, and improves survival prediction in\nattention-based multiple instance learning. While demonstrated on mIF data,\nAdvDINO is broadly applicable to other imaging domains -- including radiology,\nremote sensing, and autonomous driving -- where domain shift and limited\nannotated data hinder model generalization and interpretability."
    },
    {
        "date": "2025-08",
        "title": "Towards Robust Evaluation of Visual Activity Recognition: Resolving Verb Ambiguity with Sense Clustering",
        "author": "Louie Hong Yao, Nicholas Jarvis, and Tianyu Jiang",
        "link": "http://arxiv.org/abs/2508.04945v1",
        "abstract": "Evaluating visual activity recognition systems is challenging due to inherent\nambiguities in verb semantics and image interpretation. When describing actions\nin images, synonymous verbs can refer to the same event (e.g., brushing vs.\ngrooming), while different perspectives can lead to equally valid but distinct\nverb choices (e.g., piloting vs. operating). Standard exact-match evaluation,\nwhich relies on a single gold answer, fails to capture these ambiguities,\nresulting in an incomplete assessment of model performance. To address this, we\npropose a vision-language clustering framework that constructs verb sense\nclusters, providing a more robust evaluation. Our analysis of the imSitu\ndataset shows that each image maps to an average of 2.8 sense clusters, with\neach cluster representing a distinct perspective of the image. We evaluate\nmultiple activity recognition models and compare our cluster-based evaluation\nwith standard evaluation methods. Additionally, our human alignment analysis\nsuggests that the cluster-based evaluation better aligns with human judgements,\noffering a more nuanced assessment of model performance."
    },
    {
        "date": "2025-08",
        "title": "Adversarial Attacks and Defenses on Graph-aware Large Language Models (LLMs)",
        "author": "Iyiola E. Olatunji, Franziska Boenisch, Jing Xu, and Adam Dziedzic",
        "link": "http://arxiv.org/abs/2508.04894v1",
        "abstract": "Large Language Models (LLMs) are increasingly integrated with\ngraph-structured data for tasks like node classification, a domain\ntraditionally dominated by Graph Neural Networks (GNNs). While this integration\nleverages rich relational information to improve task performance, their\nrobustness against adversarial attacks remains unexplored. We take the first\nstep to explore the vulnerabilities of graph-aware LLMs by leveraging existing\nadversarial attack methods tailored for graph-based models, including those for\npoisoning (training-time attacks) and evasion (test-time attacks), on two\nrepresentative models, LLAGA (Chen et al. 2024) and GRAPHPROMPTER (Liu et al.\n2024). Additionally, we discover a new attack surface for LLAGA where an\nattacker can inject malicious nodes as placeholders into the node sequence\ntemplate to severely degrade its performance. Our systematic analysis reveals\nthat certain design choices in graph encoding can enhance attack success, with\nspecific findings that: (1) the node sequence template in LLAGA increases its\nvulnerability; (2) the GNN encoder used in GRAPHPROMPTER demonstrates greater\nrobustness; and (3) both approaches remain susceptible to imperceptible feature\nperturbation attacks. Finally, we propose an end-to-end defense framework\nGALGUARD, that combines an LLM-based feature correction module to mitigate\nfeature-level perturbations and adapted GNN defenses to protect against\nstructural attacks."
    },
    {
        "date": "2025-08",
        "title": "A Robust Pipeline for Differentially Private Federated Learning on Imbalanced Clinical Data using SMOTETomek and FedProx",
        "author": "Rodrigo Tertulino",
        "link": "http://arxiv.org/abs/2508.10017v1",
        "abstract": "Federated Learning (FL) presents a groundbreaking approach for collaborative\nhealth research, allowing model training on decentralized data while\nsafeguarding patient privacy. FL offers formal security guarantees when\ncombined with Differential Privacy (DP). The integration of these technologies,\nhowever, introduces a significant trade-off between privacy and clinical\nutility, a challenge further complicated by the severe class imbalance often\npresent in medical datasets. The research presented herein addresses these\ninterconnected issues through a systematic, multi-stage analysis. An FL\nframework was implemented for cardiovascular risk prediction, where initial\nexperiments showed that standard methods struggled with imbalanced data,\nresulting in a recall of zero. To overcome such a limitation, we first\nintegrated the hybrid Synthetic Minority Over-sampling Technique with Tomek\nLinks (SMOTETomek) at the client level, successfully developing a clinically\nuseful model. Subsequently, the framework was optimized for non-IID data using\na tuned FedProx algorithm. Our final results reveal a clear, non-linear\ntrade-off between the privacy budget (epsilon) and model recall, with the\noptimized FedProx consistently out-performing standard FedAvg. An optimal\noperational region was identified on the privacy-utility frontier, where strong\nprivacy guarantees (with epsilon 9.0) can be achieved while maintaining high\nclinical utility (recall greater than 77%). Ultimately, our study provides a\npractical methodological blueprint for creating effective, secure, and accurate\ndiagnostic tools that can be applied to real-world, heterogeneous healthcare\ndata."
    },
    {
        "date": "2025-08",
        "title": "Multi-Stage Knowledge-Distilled VGAE and GAT for Robust Controller-Area-Network Intrusion Detection",
        "author": "Robert Frenken, Sidra Ghayour Bhatti, Hanqin Zhang, and Qadeer Ahmed",
        "link": "http://arxiv.org/abs/2508.04845v1",
        "abstract": "The Controller Area Network (CAN) protocol is a standard for in-vehicle\ncommunication but remains susceptible to cyber-attacks due to its lack of\nbuilt-in security. This paper presents a multi-stage intrusion detection\nframework leveraging unsupervised anomaly detection and supervised graph\nlearning tailored for automotive CAN traffic. Our architecture combines a\nVariational Graph Autoencoder (VGAE) for structural anomaly detection with a\nKnowledge-Distilled Graph Attention Network (KD-GAT) for robust attack\nclassification. CAN bus activity is encoded as graph sequences to model\ntemporal and relational dependencies. The pipeline applies VGAE-based selective\nundersampling to address class imbalance, followed by GAT classification with\noptional score-level fusion. The compact student GAT achieves 96% parameter\nreduction compared to the teacher model while maintaining strong predictive\nperformance. Experiments on six public CAN intrusion datasets--Car-Hacking,\nCar-Survival, and can-train-and-test--demonstrate competitive accuracy and\nefficiency, with average improvements of 16.2% in F1-score over existing\nmethods, particularly excelling on highly imbalanced datasets with up to 55%\nF1-score improvements."
    },
    {
        "date": "2025-08",
        "title": "Attack Pattern Mining to Discover Hidden Threats to Industrial Control Systems",
        "author": "Muhammad Azmi Umer, Chuadhry Mujeeb Ahmed, Aditya Mathur, and Muhammad Taha Jilani",
        "link": "http://arxiv.org/abs/2508.04561v1",
        "abstract": "This work focuses on validation of attack pattern mining in the context of\nIndustrial Control System (ICS) security. A comprehensive security assessment\nof an ICS requires generating a large and variety of attack patterns. For this\npurpose we have proposed a data driven technique to generate attack patterns\nfor an ICS. The proposed technique has been used to generate over 100,000\nattack patterns from data gathered from an operational water treatment plant.\nIn this work we present a detailed case study to validate the attack patterns."
    },
    {
        "date": "2025-08",
        "title": "Learning Robust Intervention Representations with Delta Embeddings",
        "author": "Panagiotis Alimisis, and Christos Diou",
        "link": "http://arxiv.org/abs/2508.04492v1",
        "abstract": "Causal representation learning has attracted significant research interest\nduring the past few years, as a means for improving model generalization and\nrobustness. Causal representations of interventional image pairs, have the\nproperty that only variables corresponding to scene elements affected by the\nintervention / action are changed between the start state and the end state.\nWhile most work in this area has focused on identifying and representing the\nvariables of the scene under a causal model, fewer efforts have focused on\nrepresentations of the interventions themselves. In this work, we show that an\neffective strategy for improving out of distribution (OOD) robustness is to\nfocus on the representation of interventions in the latent space. Specifically,\nwe propose that an intervention can be represented by a Causal Delta Embedding\nthat is invariant to the visual scene and sparse in terms of the causal\nvariables it affects. Leveraging this insight, we propose a framework that is\ncapable of learning causal representations from image pairs, without any\nadditional supervision. Experiments in the Causal Triplet challenge demonstrate\nthat Causal Delta Embeddings are highly effective in OOD settings,\nsignificantly exceeding baseline performance in both synthetic and real-world\nbenchmarks."
    },
    {
        "date": "2025-08",
        "title": "Emotion Detection Using Conditional Generative Adversarial Networks (cGAN): A Deep Learning Approach",
        "author": "Anushka Srivastava",
        "link": "http://arxiv.org/abs/2508.04481v1",
        "abstract": "This paper presents a deep learning-based approach to emotion detection using\nConditional Generative Adversarial Networks (cGANs). Unlike traditional\nunimodal techniques that rely on a single data type, we explore a multimodal\nframework integrating text, audio, and facial expressions. The proposed cGAN\narchitecture is trained to generate synthetic emotion-rich data and improve\nclassification accuracy across multiple modalities. Our experimental results\ndemonstrate significant improvements in emotion recognition performance\ncompared to baseline models. This work highlights the potential of cGANs in\nenhancing human-computer interaction systems by enabling more nuanced emotional\nunderstanding."
    },
    {
        "date": "2025-08",
        "title": "PKSS-Align: Robust Point Cloud Registration on Pre-Kendall Shape Space",
        "author": "Chenlei Lv, and Hui Huang",
        "link": "http://arxiv.org/abs/2508.04286v1",
        "abstract": "Point cloud registration is a classical topic in the field of 3D Vision and\nComputer Graphics. Generally, the implementation of registration is typically\nsensitive to similarity transformations (translation, scaling, and rotation),\nnoisy points, and incomplete geometric structures. Especially, the non-uniform\nscales and defective parts of point clouds increase probability of struck local\noptima in registration task. In this paper, we propose a robust point cloud\nregistration PKSS-Align that can handle various influences, including\nsimilarity transformations, non-uniform densities, random noisy points, and\ndefective parts. The proposed method measures shape feature-based similarity\nbetween point clouds on the Pre-Kendall shape space (PKSS),\n\\textcolor{black}{which is a shape measurement-based scheme and doesn't require\npoint-to-point or point-to-plane metric.} The employed measurement can be\nregarded as the manifold metric that is robust to various representations in\nthe Euclidean coordinate system. Benefited from the measurement, the\ntransformation matrix can be directly generated for point clouds with mentioned\ninfluences at the same time. The proposed method does not require data training\nand complex feature encoding. Based on a simple parallel acceleration, it can\nachieve significant improvement for efficiency and feasibility in practice.\nExperiments demonstrate that our method outperforms the relevant\nstate-of-the-art methods."
    },
    {
        "date": "2025-08",
        "title": "Per-element Secure Aggregation against Data Reconstruction Attacks in Federated Learning",
        "author": "Takumi Suimon, Yuki Koizumi, Junji Takemasa, and Toru Hasegawa",
        "link": "http://arxiv.org/abs/2508.04285v2",
        "abstract": "Federated learning (FL) enables collaborative model training without sharing\nraw data, but individual model updates may still leak sensitive information.\nSecure aggregation (SecAgg) mitigates this risk by allowing the server to\naccess only the sum of client updates, thereby concealing individual\ncontributions. However, a significant vulnerability has recently attracted\nincreasing attention: when model updates are sparse vectors, a non-zero value\ncontributed by a single client at a given index can be directly revealed in the\naggregate, enabling precise data reconstruction attacks. In this paper, we\npropose a novel enhancement to SecAgg that reveals aggregated values only at\nindices with at least $t$ non-zero contributions. Our mechanism introduces a\nper-element masking strategy to prevent the exposure of under-contributed\nelements, while maintaining modularity and compatibility with many existing\nSecAgg implementations by relying solely on cryptographic primitives already\nemployed in a typical setup. We integrate this mechanism into Flamingo, a\nlow-round SecAgg protocol, to provide a robust defense against such attacks.\nOur analysis and experimental results indicate that the additional\ncomputational and communication overhead introduced by our mechanism remains\nwithin an acceptable range, supporting the practicality of our approach."
    },
    {
        "date": "2025-08",
        "title": "A Few Words Can Distort Graphs: Knowledge Poisoning Attacks on Graph-based Retrieval-Augmented Generation of Large Language Models",
        "author": "Jiayi Wen, Tianxin Chen, Zhirun Zheng, and Cheng Huang",
        "link": "http://arxiv.org/abs/2508.04276v2",
        "abstract": "Graph-based Retrieval-Augmented Generation (GraphRAG) has recently emerged as\na promising paradigm for enhancing large language models (LLMs) by converting\nraw text into structured knowledge graphs, improving both accuracy and\nexplainability. However, GraphRAG relies on LLMs to extract knowledge from raw\ntext during graph construction, and this process can be maliciously manipulated\nto implant misleading information. Targeting this attack surface, we propose\ntwo knowledge poisoning attacks (KPAs) and demonstrate that modifying only a\nfew words in the source text can significantly change the constructed graph,\npoison the GraphRAG, and severely mislead downstream reasoning. The first\nattack, named Targeted KPA (TKPA), utilizes graph-theoretic analysis to locate\nvulnerable nodes in the generated graphs and rewrites the corresponding\nnarratives with LLMs, achieving precise control over specific\nquestion-answering (QA) outcomes with a success rate of 93.1\\%, while keeping\nthe poisoned text fluent and natural. The second attack, named Universal KPA\n(UKPA), exploits linguistic cues such as pronouns and dependency relations to\ndisrupt the structural integrity of the generated graph by altering globally\ninfluential words. With fewer than 0.05\\% of full text modified, the QA\naccuracy collapses from 95\\% to 50\\%. Furthermore, experiments show that\nstate-of-the-art defense methods fail to detect these attacks, highlighting\nthat securing GraphRAG pipelines against knowledge poisoning remains largely\nunexplored."
    },
    {
        "date": "2025-08",
        "title": "SelectiveShield: Lightweight Hybrid Defense Against Gradient Leakage in Federated Learning",
        "author": "Borui Li, Li Yan, and Jianmin Liu",
        "link": "http://arxiv.org/abs/2508.04265v1",
        "abstract": "Federated Learning (FL) enables collaborative model training on decentralized\ndata but remains vulnerable to gradient leakage attacks that can reconstruct\nsensitive user information. Existing defense mechanisms, such as differential\nprivacy (DP) and homomorphic encryption (HE), often introduce a trade-off\nbetween privacy, model utility, and system overhead, a challenge that is\nexacerbated in heterogeneous environments with non-IID data and varying client\ncapabilities. To address these limitations, we propose SelectiveShield, a\nlightweight hybrid defense framework that adaptively integrates selective\nhomomorphic encryption and differential privacy. SelectiveShield leverages\nFisher information to quantify parameter sensitivity, allowing clients to\nidentify critical parameters locally. Through a collaborative negotiation\nprotocol, clients agree on a shared set of the most sensitive parameters for\nprotection via homomorphic encryption. Parameters that are uniquely important\nto individual clients are retained locally, fostering personalization, while\nnon-critical parameters are protected with adaptive differential privacy noise.\nExtensive experiments demonstrate that SelectiveShield maintains strong model\nutility while significantly mitigating gradient leakage risks, offering a\npractical and scalable defense mechanism for real-world federated learning\ndeployments."
    },
    {
        "date": "2025-08",
        "title": "Boosting Adversarial Transferability via Residual Perturbation Attack",
        "author": "Jinjia Peng, Zeze Tao, Huibing Wang, Meng Wang, and Yang Wang",
        "link": "http://arxiv.org/abs/2508.05689v1",
        "abstract": "Deep neural networks are susceptible to adversarial examples while suffering\nfrom incorrect predictions via imperceptible perturbations. Transfer-based\nattacks create adversarial examples for surrogate models and transfer these\nexamples to target models under black-box scenarios. Recent studies reveal that\nadversarial examples in flat loss landscapes exhibit superior transferability\nto alleviate overfitting on surrogate models. However, the prior arts overlook\nthe influence of perturbation directions, resulting in limited transferability.\nIn this paper, we propose a novel attack method, named Residual Perturbation\nAttack (ResPA), relying on the residual gradient as the perturbation direction\nto guide the adversarial examples toward the flat regions of the loss function.\nSpecifically, ResPA conducts an exponential moving average on the input\ngradients to obtain the first moment as the reference gradient, which\nencompasses the direction of historical gradients. Instead of heavily relying\non the local flatness that stems from the current gradients as the perturbation\ndirection, ResPA further considers the residual between the current gradient\nand the reference gradient to capture the changes in the global perturbation\ndirection. The experimental results demonstrate the better transferability of\nResPA than the existing typical transfer-based attack methods, while the\ntransferability can be further improved by combining ResPA with the current\ninput transformation methods. The code is available at\nhttps://github.com/ZezeTao/ResPA."
    },
    {
        "date": "2025-08",
        "title": "From Learning to Unlearning: Biomedical Security Protection in Multimodal Large Language Models",
        "author": "Dunyuan Xu, Xikai Yang, Yaoqian Li, Jinpeng Li, and Pheng-Ann Heng",
        "link": "http://arxiv.org/abs/2508.04192v1",
        "abstract": "The security of biomedical Multimodal Large Language Models (MLLMs) has\nattracted increasing attention. However, training samples easily contain\nprivate information and incorrect knowledge that are difficult to detect,\npotentially leading to privacy leakage or erroneous outputs after deployment.\nAn intuitive idea is to reprocess the training set to remove unwanted content\nand retrain the model from scratch. Yet, this is impractical due to significant\ncomputational costs, especially for large language models. Machine unlearning\nhas emerged as a solution to this problem, which avoids complete retraining by\nselectively removing undesired knowledge derived from harmful samples while\npreserving required capabilities on normal cases. However, there exist no\navailable datasets to evaluate the unlearning quality for security protection\nin biomedical MLLMs. To bridge this gap, we propose the first benchmark\nMultimodal Large Language Model Unlearning for BioMedicine (MLLMU-Med) built\nupon our novel data generation pipeline that effectively integrates synthetic\nprivate data and factual errors into the training set. Our benchmark targets\ntwo key scenarios: 1) Privacy protection, where patient private information is\nmistakenly included in the training set, causing models to unintentionally\nrespond with private data during inference; and 2) Incorrectness removal, where\nwrong knowledge derived from unreliable sources is embedded into the dataset,\nleading to unsafe model responses. Moreover, we propose a novel Unlearning\nEfficiency Score that directly reflects the overall unlearning performance\nacross different subsets. We evaluate five unlearning approaches on MLLMU-Med\nand find that these methods show limited effectiveness in removing harmful\nknowledge from biomedical MLLMs, indicating significant room for improvement.\nThis work establishes a new pathway for further research in this promising\nfield."
    }
]