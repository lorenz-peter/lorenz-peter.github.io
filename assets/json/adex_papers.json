[
    {
        "date": "2025-08",
        "title": "FS-IQA: Certified Feature Smoothing for Robust Image Quality Assessment",
        "author": "Ekaterina Shumitskaya, Dmitriy Vatolin, and Anastasia Antsiferova",
        "link": "http://arxiv.org/abs/2508.05516v1",
        "abstract": "We propose a novel certified defense method for Image Quality Assessment\n(IQA) models based on randomized smoothing with noise applied in the feature\nspace rather than the input space. Unlike prior approaches that inject Gaussian\nnoise directly into input images, often degrading visual quality, our method\npreserves image fidelity while providing robustness guarantees. To formally\nconnect noise levels in the feature space with corresponding input-space\nperturbations, we analyze the maximum singular value of the backbone network's\nJacobian. Our approach supports both full-reference (FR) and no-reference (NR)\nIQA models without requiring any architectural modifications, suitable for\nvarious scenarios. It is also computationally efficient, requiring a single\nbackbone forward pass per image. Compared to previous methods, it reduces\ninference time by 99.5% without certification and by 20.6% when certification\nis applied. We validate our method with extensive experiments on two benchmark\ndatasets, involving six widely-used FR and NR IQA models and comparisons\nagainst five state-of-the-art certified defenses. Our results demonstrate\nconsistent improvements in correlation with subjective quality scores by up to\n30.9%."
    },
    {
        "date": "2025-08",
        "title": "Keep It Real: Challenges in Attacking Compression-Based Adversarial Purification",
        "author": "Samuel R\u00e4ber, Till Aczel, Andreas Plesner, and Roger Wattenhofer",
        "link": "http://arxiv.org/abs/2508.05489v1",
        "abstract": "Previous work has suggested that preprocessing images through lossy\ncompression can defend against adversarial perturbations, but comprehensive\nattack evaluations have been lacking. In this paper, we construct strong\nwhite-box and adaptive attacks against various compression models and identify\na critical challenge for attackers: high realism in reconstructed images\nsignificantly increases attack difficulty. Through rigorous evaluation across\nmultiple attack scenarios, we demonstrate that compression models capable of\nproducing realistic, high-fidelity reconstructions are substantially more\nresistant to our attacks. In contrast, low-realism compression models can be\nbroken. Our analysis reveals that this is not due to gradient masking. Rather,\nrealistic reconstructions maintaining distributional alignment with natural\nimages seem to offer inherent robustness. This work highlights a significant\nobstacle for future adversarial attacks and suggests that developing more\neffective techniques to overcome realism represents an essential challenge for\ncomprehensive security evaluation."
    },
    {
        "date": "2025-08",
        "title": "Task complexity shapes internal representations and robustness in neural networks",
        "author": "Robert Jankowski, Filippo Radicchi, M. \u00c1ngeles Serrano, Mari\u00e1n Bogu\u00f1\u00e1, and Santo Fortunato",
        "link": "http://arxiv.org/abs/2508.05463v1",
        "abstract": "Neural networks excel across a wide range of tasks, yet remain black boxes.\nIn particular, how their internal representations are shaped by the complexity\nof the input data and the problems they solve remains obscure. In this work, we\nintroduce a suite of five data-agnostic probes-pruning, binarization, noise\ninjection, sign flipping, and bipartite network randomization-to quantify how\ntask difficulty influences the topology and robustness of representations in\nmultilayer perceptrons (MLPs). MLPs are represented as signed, weighted\nbipartite graphs from a network science perspective. We contrast easy and hard\nclassification tasks on the MNIST and Fashion-MNIST datasets. We show that\nbinarizing weights in hard-task models collapses accuracy to chance, whereas\neasy-task models remain robust. We also find that pruning low-magnitude edges\nin binarized hard-task models reveals a sharp phase-transition in performance.\nMoreover, moderate noise injection can enhance accuracy, resembling a\nstochastic-resonance effect linked to optimal sign flips of small-magnitude\nweights. Finally, preserving only the sign structure-instead of precise weight\nmagnitudes-through bipartite network randomizations suffices to maintain high\naccuracy. These phenomena define a model- and modality-agnostic measure of task\ncomplexity: the performance gap between full-precision and binarized or\nshuffled neural network performance. Our findings highlight the crucial role of\nsigned bipartite topology in learned representations and suggest practical\nstrategies for model compression and interpretability that align with task\ncomplexity."
    },
    {
        "date": "2025-08",
        "title": "Physical Adversarial Camouflage through Gradient Calibration and Regularization",
        "author": "Jiawei Liang, Siyuan Liang, Jianjie Huang, Chenxi Si, Ming Zhang, and Xiaochun Cao",
        "link": "http://arxiv.org/abs/2508.05414v1",
        "abstract": "The advancement of deep object detectors has greatly affected safety-critical\nfields like autonomous driving. However, physical adversarial camouflage poses\na significant security risk by altering object textures to deceive detectors.\nExisting techniques struggle with variable physical environments, facing two\nmain challenges: 1) inconsistent sampling point densities across distances\nhinder the gradient optimization from ensuring local continuity, and 2)\nupdating texture gradients from multiple angles causes conflicts, reducing\noptimization stability and attack effectiveness. To address these issues, we\npropose a novel adversarial camouflage framework based on gradient\noptimization. First, we introduce a gradient calibration strategy, which\nensures consistent gradient updates across distances by propagating gradients\nfrom sparsely to unsampled texture points. Additionally, we develop a gradient\ndecorrelation method, which prioritizes and orthogonalizes gradients based on\nloss values, enhancing stability and effectiveness in multi-angle optimization\nby eliminating redundant or conflicting updates. Extensive experimental results\non various detection models, angles and distances show that our method\nsignificantly exceeds the state of the art, with an average increase in attack\nsuccess rate (ASR) of 13.46% across distances and 11.03% across angles.\nFurthermore, empirical evaluation in real-world scenarios highlights the need\nfor more robust system design."
    },
    {
        "date": "2025-08",
        "title": "NT-ML: Backdoor Defense via Non-target Label Training and Mutual Learning",
        "author": "Wenjie Huo, and Katinka Wolter",
        "link": "http://arxiv.org/abs/2508.05404v1",
        "abstract": "Recent studies have shown that deep neural networks (DNNs) are vulnerable to\nbackdoor attacks, where a designed trigger is injected into the dataset,\ncausing erroneous predictions when activated. In this paper, we propose a novel\ndefense mechanism, Non-target label Training and Mutual Learning (NT-ML), which\ncan successfully restore the poisoned model under advanced backdoor attacks. NT\naims to reduce the harm of poisoned data by retraining the model with the\noutputs of the standard training. At this stage, a teacher model with high\naccuracy on clean data and a student model with higher confidence in correct\nprediction on poisoned data are obtained. Then, the teacher and student can\nlearn the strengths from each other through ML to obtain a purified student\nmodel. Extensive experiments show that NT-ML can effectively defend against 6\nbackdoor attacks with a small number of clean samples, and outperforms 5\nstate-of-the-art backdoor defenses."
    },
    {
        "date": "2025-08",
        "title": "Secure and practical Quantum Digital Signatures",
        "author": "Federico Grasselli, Gaetano Russo, and Massimiliano Proietti",
        "link": "http://arxiv.org/abs/2508.05355v1",
        "abstract": "Digital signatures represent a crucial cryptographic asset that must be\nprotected against quantum adversaries. Quantum Digital Signatures (QDS) can\noffer solutions that are information-theoretically (IT) secure and thus immune\nto quantum attacks. In this work, we analyze three existing practical QDS\nprotocols based on preshared secure keys (e.g., established with quantum key\ndistribution) and universal hashing families. For each protocol, we make\namendments to close potential loopholes and prove their IT security while\naccounting for the failure of IT-secure authenticated communication. We then\nnumerically optimize the protocol parameters to improve efficiency in terms of\npreshared bit consumption and signature length, allowing us to identify the\nmost efficient protocol."
    },
    {
        "date": "2025-08",
        "title": "CoCAViT: Compact Vision Transformer with Robust Global Coordination",
        "author": "Xuyang Wang, Lingjuan Miao, and Zhiqiang Zhou",
        "link": "http://arxiv.org/abs/2508.05307v1",
        "abstract": "In recent years, large-scale visual backbones have demonstrated remarkable\ncapabilities in learning general-purpose features from images via extensive\npre-training. Concurrently, many efficient architectures have emerged that have\nperformance comparable to that of larger models on in-domain benchmarks.\nHowever, we observe that for smaller models, the performance drop on\nout-of-distribution (OOD) data is disproportionately larger, indicating a\ndeficiency in the generalization performance of existing efficient models. To\naddress this, we identify key architectural bottlenecks and inappropriate\ndesign choices that contribute to this issue, retaining robustness for smaller\nmodels. To restore the global field of pure window attention, we further\nintroduce a Coordinator-patch Cross Attention (CoCA) mechanism, featuring\ndynamic, domain-aware global tokens that enhance local-global feature modeling\nand adaptively capture robust patterns across domains with minimal\ncomputational overhead. Integrating these advancements, we present CoCAViT, a\nnovel visual backbone designed for robust real-time visual representation.\nExtensive experiments empirically validate our design. At a resolution of\n224*224, CoCAViT-28M achieves 84.0% top-1 accuracy on ImageNet-1K, with\nsignificant gains on multiple OOD benchmarks, compared to competing models. It\nalso attains 52.2 mAP on COCO object detection and 51.3 mIOU on ADE20K semantic\nsegmentation, while maintaining low latency."
    },
    {
        "date": "2025-08",
        "title": "Robust Tracking with Particle Filtering for Fluorescent Cardiac Imaging",
        "author": "Suresh Guttikonda, Maximilian Neidhart, Johanna Sprenger, Johannes Petersen, Christian Detter, and Alexander Schlaefer",
        "link": "http://arxiv.org/abs/2508.05262v1",
        "abstract": "Intraoperative fluorescent cardiac imaging enables quality control following\ncoronary bypass grafting surgery. We can estimate local quantitative\nindicators, such as cardiac perfusion, by tracking local feature points.\nHowever, heart motion and significant fluctuations in image characteristics\ncaused by vessel structural enrichment limit traditional tracking methods. We\npropose a particle filtering tracker based on cyclicconsistency checks to\nrobustly track particles sampled to follow target landmarks. Our method tracks\n117 targets simultaneously at 25.4 fps, allowing real-time estimates during\ninterventions. It achieves a tracking error of (5.00 +/- 0.22 px) and\noutperforms other deep learning trackers (22.3 +/- 1.1 px) and conventional\ntrackers (58.1 +/- 27.1 px)."
    },
    {
        "date": "2025-08",
        "title": "Navigating the Trade-off: A Synthesis of Defensive Strategies for Zero-Shot Adversarial Robustness in Vision-Language Models",
        "author": "Zane Xu, and Jason Sun",
        "link": "http://arxiv.org/abs/2508.05237v1",
        "abstract": "This report synthesizes eight seminal papers on the zero-shot adversarial\nrobustness of vision-language models (VLMs) like CLIP. A central challenge in\nthis domain is the inherent trade-off between enhancing adversarial robustness\nand preserving the model's zero-shot generalization capabilities. We analyze\ntwo primary defense paradigms: Adversarial Fine-Tuning (AFT), which modifies\nmodel parameters, and Training-Free/Test-Time Defenses, which preserve them. We\ntrace the evolution from alignment-preserving methods (TeCoA) to embedding\nspace re-engineering (LAAT, TIMA), and from input heuristics (AOM, TTC) to\nlatent-space purification (CLIPure). Finally, we identify key challenges and\nfuture directions including hybrid defense strategies and adversarial\npre-training."
    },
    {
        "date": "2025-08",
        "title": "PhysPatch: A Physically Realizable and Transferable Adversarial Patch Attack for Multimodal Large Language Models-based Autonomous Driving Systems",
        "author": "Qi Guo, Xiaojun Jia, Shanmin Pang, Simeng Qin, Lin Wang, Ju Jia, Yang Liu, and Qing Guo",
        "link": "http://arxiv.org/abs/2508.05167v1",
        "abstract": "Multimodal Large Language Models (MLLMs) are becoming integral to autonomous\ndriving (AD) systems due to their strong vision-language reasoning\ncapabilities. However, MLLMs are vulnerable to adversarial attacks,\nparticularly adversarial patch attacks, which can pose serious threats in\nreal-world scenarios. Existing patch-based attack methods are primarily\ndesigned for object detection models and perform poorly when transferred to\nMLLM-based systems due to the latter's complex architectures and reasoning\nabilities. To address these limitations, we propose PhysPatch, a physically\nrealizable and transferable adversarial patch framework tailored for MLLM-based\nAD systems. PhysPatch jointly optimizes patch location, shape, and content to\nenhance attack effectiveness and real-world applicability. It introduces a\nsemantic-based mask initialization strategy for realistic placement, an\nSVD-based local alignment loss with patch-guided crop-resize to improve\ntransferability, and a potential field-based mask refinement method. Extensive\nexperiments across open-source, commercial, and reasoning-capable MLLMs\ndemonstrate that PhysPatch significantly outperforms prior methods in steering\nMLLM-based AD systems toward target-aligned perception and planning outputs.\nMoreover, PhysPatch consistently places adversarial patches in physically\nfeasible regions of AD scenes, ensuring strong real-world applicability and\ndeployability."
    },
    {
        "date": "2025-08",
        "title": "FLUX-Makeup: High-Fidelity, Identity-Consistent, and Robust Makeup Transfer via Diffusion Transformer",
        "author": "Jian Zhu, Shanyuan Liu, Liuzhuozheng Li, Yue Gong, He Wang, Bo Cheng, Yuhang Ma, Liebucha Wu, Xiaoyu Wu, Dawei Leng, Yuhui Yin, and Yang Xu",
        "link": "http://arxiv.org/abs/2508.05069v1",
        "abstract": "Makeup transfer aims to apply the makeup style from a reference face to a\ntarget face and has been increasingly adopted in practical applications.\nExisting GAN-based approaches typically rely on carefully designed loss\nfunctions to balance transfer quality and facial identity consistency, while\ndiffusion-based methods often depend on additional face-control modules or\nalgorithms to preserve identity. However, these auxiliary components tend to\nintroduce extra errors, leading to suboptimal transfer results. To overcome\nthese limitations, we propose FLUX-Makeup, a high-fidelity,\nidentity-consistent, and robust makeup transfer framework that eliminates the\nneed for any auxiliary face-control components. Instead, our method directly\nleverages source-reference image pairs to achieve superior transfer\nperformance. Specifically, we build our framework upon FLUX-Kontext, using the\nsource image as its native conditional input. Furthermore, we introduce\nRefLoRAInjector, a lightweight makeup feature injector that decouples the\nreference pathway from the backbone, enabling efficient and comprehensive\nextraction of makeup-related information. In parallel, we design a robust and\nscalable data generation pipeline to provide more accurate supervision during\ntraining. The paired makeup datasets produced by this pipeline significantly\nsurpass the quality of all existing datasets. Extensive experiments demonstrate\nthat FLUX-Makeup achieves state-of-the-art performance, exhibiting strong\nrobustness across diverse scenarios."
    },
    {
        "date": "2025-08",
        "title": "Automatic Image Colorization with Convolutional Neural Networks and Generative Adversarial Networks",
        "author": "Ruiyu Li, Changyuan Qiu, Hangrui Cao, Qihan Ren, and Yuqing Qiu",
        "link": "http://arxiv.org/abs/2508.05068v1",
        "abstract": "Image colorization, the task of adding colors to grayscale images, has been\nthe focus of significant research efforts in computer vision in recent years\nfor its various application areas such as color restoration and automatic\nanimation colorization [15, 1]. The colorization problem is challenging as it\nis highly ill-posed with two out of three image dimensions lost, resulting in\nlarge degrees of freedom. However, semantics of the scene as well as the\nsurface texture could provide important cues for colors: the sky is typically\nblue, the clouds are typically white and the grass is typically green, and\nthere are huge amounts of training data available for learning such priors\nsince any colored image could serve as a training data point [20].\n  Colorization is initially formulated as a regression task[5], which ignores\nthe multi-modal nature of color prediction. In this project, we explore\nautomatic image colorization via classification and adversarial learning. We\nwill build our models on prior works, apply modifications for our specific\nscenario and make comparisons."
    },
    {
        "date": "2025-08",
        "title": "AdvDINO: Domain-Adversarial Self-Supervised Representation Learning for Spatial Proteomics",
        "author": "Stella Su, Marc Harary, Scott J. Rodig, and William Lotter",
        "link": "http://arxiv.org/abs/2508.04955v1",
        "abstract": "Self-supervised learning (SSL) has emerged as a powerful approach for\nlearning visual representations without manual annotations. However, the\nrobustness of standard SSL methods to domain shift -- systematic differences\nacross data sources -- remains uncertain, posing an especially critical\nchallenge in biomedical imaging where batch effects can obscure true biological\nsignals. We present AdvDINO, a domain-adversarial self-supervised learning\nframework that integrates a gradient reversal layer into the DINOv2\narchitecture to promote domain-invariant feature learning. Applied to a\nreal-world cohort of six-channel multiplex immunofluorescence (mIF) whole slide\nimages from non-small cell lung cancer patients, AdvDINO mitigates\nslide-specific biases to learn more robust and biologically meaningful\nrepresentations than non-adversarial baselines. Across $>5.46$ million mIF\nimage tiles, the model uncovers phenotype clusters with distinct proteomic\nprofiles and prognostic significance, and improves survival prediction in\nattention-based multiple instance learning. While demonstrated on mIF data,\nAdvDINO is broadly applicable to other imaging domains -- including radiology,\nremote sensing, and autonomous driving -- where domain shift and limited\nannotated data hinder model generalization and interpretability."
    },
    {
        "date": "2025-08",
        "title": "Towards Robust Evaluation of Visual Activity Recognition: Resolving Verb Ambiguity with Sense Clustering",
        "author": "Louie Hong Yao, Nicholas Jarvis, and Tianyu Jiang",
        "link": "http://arxiv.org/abs/2508.04945v1",
        "abstract": "Evaluating visual activity recognition systems is challenging due to inherent\nambiguities in verb semantics and image interpretation. When describing actions\nin images, synonymous verbs can refer to the same event (e.g., brushing vs.\ngrooming), while different perspectives can lead to equally valid but distinct\nverb choices (e.g., piloting vs. operating). Standard exact-match evaluation,\nwhich relies on a single gold answer, fails to capture these ambiguities,\nresulting in an incomplete assessment of model performance. To address this, we\npropose a vision-language clustering framework that constructs verb sense\nclusters, providing a more robust evaluation. Our analysis of the imSitu\ndataset shows that each image maps to an average of 2.8 sense clusters, with\neach cluster representing a distinct perspective of the image. We evaluate\nmultiple activity recognition models and compare our cluster-based evaluation\nwith standard evaluation methods. Additionally, our human alignment analysis\nsuggests that the cluster-based evaluation better aligns with human judgements,\noffering a more nuanced assessment of model performance."
    },
    {
        "date": "2025-08",
        "title": "Adversarial Attacks and Defenses on Graph-aware Large Language Models (LLMs)",
        "author": "Iyiola E. Olatunji, Franziska Boenisch, Jing Xu, and Adam Dziedzic",
        "link": "http://arxiv.org/abs/2508.04894v1",
        "abstract": "Large Language Models (LLMs) are increasingly integrated with\ngraph-structured data for tasks like node classification, a domain\ntraditionally dominated by Graph Neural Networks (GNNs). While this integration\nleverages rich relational information to improve task performance, their\nrobustness against adversarial attacks remains unexplored. We take the first\nstep to explore the vulnerabilities of graph-aware LLMs by leveraging existing\nadversarial attack methods tailored for graph-based models, including those for\npoisoning (training-time attacks) and evasion (test-time attacks), on two\nrepresentative models, LLAGA (Chen et al. 2024) and GRAPHPROMPTER (Liu et al.\n2024). Additionally, we discover a new attack surface for LLAGA where an\nattacker can inject malicious nodes as placeholders into the node sequence\ntemplate to severely degrade its performance. Our systematic analysis reveals\nthat certain design choices in graph encoding can enhance attack success, with\nspecific findings that: (1) the node sequence template in LLAGA increases its\nvulnerability; (2) the GNN encoder used in GRAPHPROMPTER demonstrates greater\nrobustness; and (3) both approaches remain susceptible to imperceptible feature\nperturbation attacks. Finally, we propose an end-to-end defense framework\nGALGUARD, that combines an LLM-based feature correction module to mitigate\nfeature-level perturbations and adapted GNN defenses to protect against\nstructural attacks."
    },
    {
        "date": "2025-08",
        "title": "Multi-Stage Knowledge-Distilled VGAE and GAT for Robust Controller-Area-Network Intrusion Detection",
        "author": "Robert Frenken, Sidra Ghayour Bhatti, Hanqin Zhang, and Qadeer Ahmed",
        "link": "http://arxiv.org/abs/2508.04845v1",
        "abstract": "The Controller Area Network (CAN) protocol is a standard for in-vehicle\ncommunication but remains susceptible to cyber-attacks due to its lack of\nbuilt-in security. This paper presents a multi-stage intrusion detection\nframework leveraging unsupervised anomaly detection and supervised graph\nlearning tailored for automotive CAN traffic. Our architecture combines a\nVariational Graph Autoencoder (VGAE) for structural anomaly detection with a\nKnowledge-Distilled Graph Attention Network (KD-GAT) for robust attack\nclassification. CAN bus activity is encoded as graph sequences to model\ntemporal and relational dependencies. The pipeline applies VGAE-based selective\nundersampling to address class imbalance, followed by GAT classification with\noptional score-level fusion. The compact student GAT achieves 96% parameter\nreduction compared to the teacher model while maintaining strong predictive\nperformance. Experiments on six public CAN intrusion datasets--Car-Hacking,\nCar-Survival, and can-train-and-test--demonstrate competitive accuracy and\nefficiency, with average improvements of 16.2% in F1-score over existing\nmethods, particularly excelling on highly imbalanced datasets with up to 55%\nF1-score improvements."
    },
    {
        "date": "2025-08",
        "title": "Attack Pattern Mining to Discover Hidden Threats to Industrial Control Systems",
        "author": "Muhammad Azmi Umer, Chuadhry Mujeeb Ahmed, Aditya Mathur, and Muhammad Taha Jilani",
        "link": "http://arxiv.org/abs/2508.04561v1",
        "abstract": "This work focuses on validation of attack pattern mining in the context of\nIndustrial Control System (ICS) security. A comprehensive security assessment\nof an ICS requires generating a large and variety of attack patterns. For this\npurpose we have proposed a data driven technique to generate attack patterns\nfor an ICS. The proposed technique has been used to generate over 100,000\nattack patterns from data gathered from an operational water treatment plant.\nIn this work we present a detailed case study to validate the attack patterns."
    },
    {
        "date": "2025-08",
        "title": "Learning Robust Intervention Representations with Delta Embeddings",
        "author": "Panagiotis Alimisis, and Christos Diou",
        "link": "http://arxiv.org/abs/2508.04492v1",
        "abstract": "Causal representation learning has attracted significant research interest\nduring the past few years, as a means for improving model generalization and\nrobustness. Causal representations of interventional image pairs, have the\nproperty that only variables corresponding to scene elements affected by the\nintervention / action are changed between the start state and the end state.\nWhile most work in this area has focused on identifying and representing the\nvariables of the scene under a causal model, fewer efforts have focused on\nrepresentations of the interventions themselves. In this work, we show that an\neffective strategy for improving out of distribution (OOD) robustness is to\nfocus on the representation of interventions in the latent space. Specifically,\nwe propose that an intervention can be represented by a Causal Delta Embedding\nthat is invariant to the visual scene and sparse in terms of the causal\nvariables it affects. Leveraging this insight, we propose a framework that is\ncapable of learning causal representations from image pairs, without any\nadditional supervision. Experiments in the Causal Triplet challenge demonstrate\nthat Causal Delta Embeddings are highly effective in OOD settings,\nsignificantly exceeding baseline performance in both synthetic and real-world\nbenchmarks."
    },
    {
        "date": "2025-08",
        "title": "Emotion Detection Using Conditional Generative Adversarial Networks (cGAN): A Deep Learning Approach",
        "author": "Anushka Srivastava",
        "link": "http://arxiv.org/abs/2508.04481v1",
        "abstract": "This paper presents a deep learning-based approach to emotion detection using\nConditional Generative Adversarial Networks (cGANs). Unlike traditional\nunimodal techniques that rely on a single data type, we explore a multimodal\nframework integrating text, audio, and facial expressions. The proposed cGAN\narchitecture is trained to generate synthetic emotion-rich data and improve\nclassification accuracy across multiple modalities. Our experimental results\ndemonstrate significant improvements in emotion recognition performance\ncompared to baseline models. This work highlights the potential of cGANs in\nenhancing human-computer interaction systems by enabling more nuanced emotional\nunderstanding."
    },
    {
        "date": "2025-08",
        "title": "PKSS-Align: Robust Point Cloud Registration on Pre-Kendall Shape Space",
        "author": "Chenlei Lv, and Hui Huang",
        "link": "http://arxiv.org/abs/2508.04286v1",
        "abstract": "Point cloud registration is a classical topic in the field of 3D Vision and\nComputer Graphics. Generally, the implementation of registration is typically\nsensitive to similarity transformations (translation, scaling, and rotation),\nnoisy points, and incomplete geometric structures. Especially, the non-uniform\nscales and defective parts of point clouds increase probability of struck local\noptima in registration task. In this paper, we propose a robust point cloud\nregistration PKSS-Align that can handle various influences, including\nsimilarity transformations, non-uniform densities, random noisy points, and\ndefective parts. The proposed method measures shape feature-based similarity\nbetween point clouds on the Pre-Kendall shape space (PKSS),\n\\textcolor{black}{which is a shape measurement-based scheme and doesn't require\npoint-to-point or point-to-plane metric.} The employed measurement can be\nregarded as the manifold metric that is robust to various representations in\nthe Euclidean coordinate system. Benefited from the measurement, the\ntransformation matrix can be directly generated for point clouds with mentioned\ninfluences at the same time. The proposed method does not require data training\nand complex feature encoding. Based on a simple parallel acceleration, it can\nachieve significant improvement for efficiency and feasibility in practice.\nExperiments demonstrate that our method outperforms the relevant\nstate-of-the-art methods."
    },
    {
        "date": "2025-08",
        "title": "Per-element Secure Aggregation against Data Reconstruction Attacks in Federated Learning",
        "author": "Takumi Suimon, Yuki Koizumi, Junji Takemasa, and Toru Hasegawa",
        "link": "http://arxiv.org/abs/2508.04285v1",
        "abstract": "Federated learning (FL) enables collaborative model training without sharing\nraw data, but individual model updates may still leak sensitive information.\nSecure aggregation (SecAgg) mitigates this risk by allowing the server to\naccess only the sum of client updates, thereby concealing individual\ncontributions. However, a significant vulnerability has recently attracted\nincreasing attention: when model updates are sparse vectors, a non-zero value\ncontributed by a single client at a given index can be directly revealed in the\naggregate, enabling precise data reconstruction attacks. In this paper, we\npropose a novel enhancement to SecAgg that reveals aggregated values only at\nindices with at least $t$ non-zero contributions. Our mechanism introduces a\nper-element masking strategy to prevent the exposure of under-contributed\nelements, while maintaining modularity and compatibility with many existing\nSecAgg implementations by relying solely on cryptographic primitives already\nemployed in a typical setup. We integrate this mechanism into Flamingo, a\nlow-round SecAgg protocol, to provide a robust defense against such attacks.\nOur analysis and experimental results indicate that the additional\ncomputational and communication overhead introduced by our mechanism remains\nwithin an acceptable range, supporting the practicality of our approach."
    },
    {
        "date": "2025-08",
        "title": "A Few Words Can Distort Graphs: Knowledge Poisoning Attacks on Graph-based Retrieval-Augmented Generation of Large Language Models",
        "author": "Jiayi Wen, Tianxin Chen, Zhirun Zheng, and Cheng Huang",
        "link": "http://arxiv.org/abs/2508.04276v1",
        "abstract": "Graph-based Retrieval-Augmented Generation (GraphRAG) has recently emerged as\na promising paradigm for enhancing large language models (LLMs) by converting\nraw text into structured knowledge graphs, improving both accuracy and\nexplainability. However, GraphRAG relies on LLMs to extract knowledge from raw\ntext during graph construction, and this process can be maliciously manipulated\nto implant misleading information. Targeting this attack surface, we propose\ntwo knowledge poisoning attacks (KPAs) and demonstrate that modifying only a\nfew words in the source text can significantly change the constructed graph,\npoison the GraphRAG, and severely mislead downstream reasoning. The first\nattack, named Targeted KPA (TKPA), utilizes graph-theoretic analysis to locate\nvulnerable nodes in the generated graphs and rewrites the corresponding\nnarratives with LLMs, achieving precise control over specific\nquestion-answering (QA) outcomes with a success rate of 93.1\\%, while keeping\nthe poisoned text fluent and natural. The second attack, named Universal KPA\n(UKPA), exploits linguistic cues such as pronouns and dependency relations to\ndisrupt the structural integrity of the generated graph by altering globally\ninfluential words. With fewer than 0.05\\% of full text modified, the QA\naccuracy collapses from 95\\% to 50\\%. Furthermore, experiments show that\nstate-of-the-art defense methods fail to detect these attacks, highlighting\nthat securing GraphRAG pipelines against knowledge poisoning remains largely\nunexplored."
    },
    {
        "date": "2025-08",
        "title": "SelectiveShield: Lightweight Hybrid Defense Against Gradient Leakage in Federated Learning",
        "author": "Borui Li, Li Yan, and Jianmin Liu",
        "link": "http://arxiv.org/abs/2508.04265v1",
        "abstract": "Federated Learning (FL) enables collaborative model training on decentralized\ndata but remains vulnerable to gradient leakage attacks that can reconstruct\nsensitive user information. Existing defense mechanisms, such as differential\nprivacy (DP) and homomorphic encryption (HE), often introduce a trade-off\nbetween privacy, model utility, and system overhead, a challenge that is\nexacerbated in heterogeneous environments with non-IID data and varying client\ncapabilities. To address these limitations, we propose SelectiveShield, a\nlightweight hybrid defense framework that adaptively integrates selective\nhomomorphic encryption and differential privacy. SelectiveShield leverages\nFisher information to quantify parameter sensitivity, allowing clients to\nidentify critical parameters locally. Through a collaborative negotiation\nprotocol, clients agree on a shared set of the most sensitive parameters for\nprotection via homomorphic encryption. Parameters that are uniquely important\nto individual clients are retained locally, fostering personalization, while\nnon-critical parameters are protected with adaptive differential privacy noise.\nExtensive experiments demonstrate that SelectiveShield maintains strong model\nutility while significantly mitigating gradient leakage risks, offering a\npractical and scalable defense mechanism for real-world federated learning\ndeployments."
    },
    {
        "date": "2025-08",
        "title": "From Learning to Unlearning: Biomedical Security Protection in Multimodal Large Language Models",
        "author": "Dunyuan Xu, Xikai Yang, Yaoqian Li, Jinpeng Li, and Pheng-Ann Heng",
        "link": "http://arxiv.org/abs/2508.04192v1",
        "abstract": "The security of biomedical Multimodal Large Language Models (MLLMs) has\nattracted increasing attention. However, training samples easily contain\nprivate information and incorrect knowledge that are difficult to detect,\npotentially leading to privacy leakage or erroneous outputs after deployment.\nAn intuitive idea is to reprocess the training set to remove unwanted content\nand retrain the model from scratch. Yet, this is impractical due to significant\ncomputational costs, especially for large language models. Machine unlearning\nhas emerged as a solution to this problem, which avoids complete retraining by\nselectively removing undesired knowledge derived from harmful samples while\npreserving required capabilities on normal cases. However, there exist no\navailable datasets to evaluate the unlearning quality for security protection\nin biomedical MLLMs. To bridge this gap, we propose the first benchmark\nMultimodal Large Language Model Unlearning for BioMedicine (MLLMU-Med) built\nupon our novel data generation pipeline that effectively integrates synthetic\nprivate data and factual errors into the training set. Our benchmark targets\ntwo key scenarios: 1) Privacy protection, where patient private information is\nmistakenly included in the training set, causing models to unintentionally\nrespond with private data during inference; and 2) Incorrectness removal, where\nwrong knowledge derived from unreliable sources is embedded into the dataset,\nleading to unsafe model responses. Moreover, we propose a novel Unlearning\nEfficiency Score that directly reflects the overall unlearning performance\nacross different subsets. We evaluate five unlearning approaches on MLLMU-Med\nand find that these methods show limited effectiveness in removing harmful\nknowledge from biomedical MLLMs, indicating significant room for improvement.\nThis work establishes a new pathway for further research in this promising\nfield."
    },
    {
        "date": "2025-08",
        "title": "RPCANet++: Deep Interpretable Robust PCA for Sparse Object Segmentation",
        "author": "Fengyi Wu, Yimian Dai, Tianfang Zhang, Yixuan Ding, Jian Yang, Ming-Ming Cheng, and Zhenming Peng",
        "link": "http://arxiv.org/abs/2508.04190v1",
        "abstract": "Robust principal component analysis (RPCA) decomposes an observation matrix\ninto low-rank background and sparse object components. This capability has\nenabled its application in tasks ranging from image restoration to\nsegmentation. However, traditional RPCA models suffer from computational\nburdens caused by matrix operations, reliance on finely tuned hyperparameters,\nand rigid priors that limit adaptability in dynamic scenarios. To solve these\nlimitations, we propose RPCANet++, a sparse object segmentation framework that\nfuses the interpretability of RPCA with efficient deep architectures. Our\napproach unfolds a relaxed RPCA model into a structured network comprising a\nBackground Approximation Module (BAM), an Object Extraction Module (OEM), and\nan Image Restoration Module (IRM). To mitigate inter-stage transmission loss in\nthe BAM, we introduce a Memory-Augmented Module (MAM) to enhance background\nfeature preservation, while a Deep Contrast Prior Module (DCPM) leverages\nsaliency cues to expedite object extraction. Extensive experiments on diverse\ndatasets demonstrate that RPCANet++ achieves state-of-the-art performance under\nvarious imaging scenarios. We further improve interpretability via visual and\nnumerical low-rankness and sparsity measurements. By combining the theoretical\nstrengths of RPCA with the efficiency of deep networks, our approach sets a new\nbaseline for reliable and interpretable sparse object segmentation. Codes are\navailable at our Project Webpage https://fengyiwu98.github.io/rpcanetx."
    },
    {
        "date": "2025-08",
        "title": "BadTime: An Effective Backdoor Attack on Multivariate Long-Term Time Series Forecasting",
        "author": "Kunlan Xiang, Haomiao Yang, Meng Hao, Haoxin Wang, Shaofeng Li, and Wenbo Jiang",
        "link": "http://arxiv.org/abs/2508.04189v1",
        "abstract": "Multivariate Long-Term Time Series Forecasting (MLTSF) models are\nincreasingly deployed in critical domains such as climate, finance, and\ntransportation. Although a variety of powerful MLTSF models have been proposed\nto improve predictive performance, the robustness of MLTSF models against\nmalicious backdoor attacks remains entirely unexplored, which is crucial to\nensuring their reliable and trustworthy deployment. To address this gap, we\nconduct an in-depth study on backdoor attacks against MLTSF models and propose\nthe first effective attack method named BadTime. BadTime executes a backdoor\nattack by poisoning training data and customizing the backdoor training\nprocess. During data poisoning, BadTime proposes a contrast-guided strategy to\nselect the most suitable training samples for poisoning, then employs a graph\nattention network to identify influential variables for trigger injection.\nSubsequently, BadTime further localizes optimal positions for trigger injection\nbased on lag analysis and proposes a puzzle-like trigger structure that\ndistributes the trigger across multiple poisoned variables to jointly steer the\nprediction of the target variable. During backdoor training, BadTime\nalternately optimizes the model and triggers via proposed tailored optimization\nobjectives. Extensive experiments show that BadTime significantly outperforms\nstate-of-the-art (SOTA) backdoor attacks on time series forecasting by reducing\nMAE by over 50% on target variables and boosting stealthiness by more than 3\ntimes."
    },
    {
        "date": "2025-08",
        "title": "Secure Development of a Hooking-Based Deception Framework Against Keylogging Techniques",
        "author": "Md Sajidul Islam Sajid, Shihab Ahmed, and Ryan Sosnoski",
        "link": "http://arxiv.org/abs/2508.04178v1",
        "abstract": "Keyloggers remain a serious threat in modern cybersecurity, silently\ncapturing user keystrokes to steal credentials and sensitive information.\nTraditional defenses focus mainly on detection and removal, which can halt\nmalicious activity but do little to engage or mislead adversaries. In this\npaper, we present a deception framework that leverages API hooking to intercept\ninput-related API calls invoked by keyloggers at runtime and inject realistic\ndecoy keystrokes. A core challenge, however, lies in the increasing adoption of\nanti-hooking techniques by advanced keyloggers. Anti-hooking strategies allow\nmalware to bypass or detect instrumentation. To counter this, we introduce a\nhardened hooking layer that detects tampering and rapidly reinstates disrupted\nhooks, ensuring continuity of deception. We evaluate our framework against a\ncustom-built \"super keylogger\" incorporating multiple evasion strategies, as\nwell as 50 real-world malware samples spanning ten prominent keylogger\nfamilies. Experimental results demonstrate that our system successfully resists\nsophisticated bypass attempts, maintains operational stealth, and reliably\ndeceives attackers by feeding them decoys. The system operates with negligible\nperformance overhead and no observable impact on user experience. Our findings\nshow that resilient, runtime deception can play a practical and robust role in\nconfronting advanced threats."
    },
    {
        "date": "2025-08",
        "title": "Evaluating Selective Encryption Against Gradient Inversion Attacks",
        "author": "Jiajun Gu, Yuhang Yao, Shuaiqi Wang, and Carlee Joe-Wong",
        "link": "http://arxiv.org/abs/2508.04155v1",
        "abstract": "Gradient inversion attacks pose significant privacy threats to distributed\ntraining frameworks such as federated learning, enabling malicious parties to\nreconstruct sensitive local training data from gradient communications between\nclients and an aggregation server during the aggregation process. While\ntraditional encryption-based defenses, such as homomorphic encryption, offer\nstrong privacy guarantees without compromising model utility, they often incur\nprohibitive computational overheads. To mitigate this, selective encryption has\nemerged as a promising approach, encrypting only a subset of gradient data\nbased on the data's significance under a certain metric. However, there have\nbeen few systematic studies on how to specify this metric in practice. This\npaper systematically evaluates selective encryption methods with different\nsignificance metrics against state-of-the-art attacks. Our findings demonstrate\nthe feasibility of selective encryption in reducing computational overhead\nwhile maintaining resilience against attacks. We propose a distance-based\nsignificance analysis framework that provides theoretical foundations for\nselecting critical gradient elements for encryption. Through extensive\nexperiments on different model architectures (LeNet, CNN, BERT, GPT-2) and\nattack types, we identify gradient magnitude as a generally effective metric\nfor protection against optimization-based gradient inversions. However, we also\nobserve that no single selective encryption strategy is universally optimal\nacross all attack scenarios, and we provide guidelines for choosing appropriate\nstrategies for different model architectures and privacy requirements."
    },
    {
        "date": "2025-08",
        "title": "Model Inversion Attacks on Vision-Language Models: Do They Leak What They Learn?",
        "author": "Ngoc-Bao Nguyen, Sy-Tuyen Ho, Koh Jun Hao, and Ngai-Man Cheung",
        "link": "http://arxiv.org/abs/2508.04097v1",
        "abstract": "Model inversion (MI) attacks pose significant privacy risks by reconstructing\nprivate training data from trained neural networks. While prior works have\nfocused on conventional unimodal DNNs, the vulnerability of vision-language\nmodels (VLMs) remains underexplored. In this paper, we conduct the first study\nto understand VLMs' vulnerability in leaking private visual training data. To\ntailored for VLMs' token-based generative nature, we propose a suite of novel\ntoken-based and sequence-based model inversion strategies. Particularly, we\npropose Token-based Model Inversion (TMI), Convergent Token-based Model\nInversion (TMI-C), Sequence-based Model Inversion (SMI), and Sequence-based\nModel Inversion with Adaptive Token Weighting (SMI-AW). Through extensive\nexperiments and user study on three state-of-the-art VLMs and multiple\ndatasets, we demonstrate, for the first time, that VLMs are susceptible to\ntraining data leakage. The experiments show that our proposed sequence-based\nmethods, particularly SMI-AW combined with a logit-maximization loss based on\nvocabulary representation, can achieve competitive reconstruction and\noutperform token-based methods in attack accuracy and visual similarity.\nImportantly, human evaluation of the reconstructed images yields an attack\naccuracy of 75.31\\%, underscoring the severity of model inversion threats in\nVLMs. Notably we also demonstrate inversion attacks on the publicly released\nVLMs. Our study reveals the privacy vulnerability of VLMs as they become\nincreasingly popular across many applications such as healthcare and finance."
    },
    {
        "date": "2025-08",
        "title": "Adversarial Fair Multi-View Clustering",
        "author": "Mudi Jiang, Jiahui Zhou, Lianyu Hu, Xinying Liu, Zengyou He, and Zhikui Chen",
        "link": "http://arxiv.org/abs/2508.04071v1",
        "abstract": "Cluster analysis is a fundamental problem in data mining and machine\nlearning. In recent years, multi-view clustering has attracted increasing\nattention due to its ability to integrate complementary information from\nmultiple views. However, existing methods primarily focus on clustering\nperformance, while fairness-a critical concern in human-centered\napplications-has been largely overlooked. Although recent studies have explored\ngroup fairness in multi-view clustering, most methods impose explicit\nregularization on cluster assignments, relying on the alignment between\nsensitive attributes and the underlying cluster structure. However, this\nassumption often fails in practice and can degrade clustering performance. In\nthis paper, we propose an adversarial fair multi-view clustering (AFMVC)\nframework that integrates fairness learning into the representation learning\nprocess. Specifically, our method employs adversarial training to fundamentally\nremove sensitive attribute information from learned features, ensuring that the\nresulting cluster assignments are unaffected by it. Furthermore, we\ntheoretically prove that aligning view-specific clustering assignments with a\nfairness-invariant consensus distribution via KL divergence preserves\nclustering consistency without significantly compromising fairness, thereby\nproviding additional theoretical guarantees for our framework. Extensive\nexperiments on data sets with fairness constraints demonstrate that AFMVC\nachieves superior fairness and competitive clustering performance compared to\nexisting multi-view clustering and fairness-aware clustering methods."
    },
    {
        "date": "2025-08",
        "title": "FLAT: Latent-Driven Arbitrary-Target Backdoor Attacks in Federated Learning",
        "author": "Tuan Nguyen, Khoa D Doan, and Kok-Seng Wong",
        "link": "http://arxiv.org/abs/2508.04064v1",
        "abstract": "Federated learning (FL) is vulnerable to backdoor attacks, yet most existing\nmethods are limited by fixed-pattern or single-target triggers, making them\ninflexible and easier to detect. We propose FLAT (FL Arbitrary-Target Attack),\na novel backdoor attack that leverages a latent-driven conditional autoencoder\nto generate diverse, target-specific triggers as needed. By introducing a\nlatent code, FLAT enables the creation of visually adaptive and highly variable\ntriggers, allowing attackers to select arbitrary targets without retraining and\nto evade conventional detection mechanisms. Our approach unifies attack\nsuccess, stealth, and diversity within a single framework, introducing a new\nlevel of flexibility and sophistication to backdoor attacks in FL. Extensive\nexperiments show that FLAT achieves high attack success and remains robust\nagainst advanced FL defenses. These results highlight the urgent need for new\ndefense strategies to address latent-driven, multi-target backdoor threats in\nfederated settings."
    },
    {
        "date": "2025-08",
        "title": "Reputation-based partition scheme for IoT security",
        "author": "Zhikui Chen, Muhammad Zeeshan Haider, Naiwen Luo, Shuo Yu, Xu Yuan, Yaochen Zhang, and Tayyaba Noreen",
        "link": "http://arxiv.org/abs/2508.03981v1",
        "abstract": "With the popularity of smart terminals, such as the Internet of Things,\ncrowdsensing is an emerging data aggregation paradigm, which plays a pivotal\nrole in data-driven applications. There are some key issues in the development\nof crowdsensing such as platform security and privacy protection. As the\ncrowdsensing is usually managed by a centralized platform, centralized\nmanagement will bring various security vulnerabilities and scalability issues.\nTo solve these issues, an effective reputation-based partition scheme (RSPC) is\nproposed in this article. The partition scheme calculates the optimal partition\nsize by combining the node reputation value and divides the node into several\ndisjoint partitions according to the node reputation value. By selecting the\nappropriate partition size, RSPC provides a mechanism to ensure that each\npartition is valid, as long as themaximum permissible threshold for the failed\nnode is observed. At the same time, the RSPC reorganizes the network\nperiodically to avoid partition attacks. In addition, for cross-partition\ntransactions, this paper innovatively proposes a four-stage confirmation\nprotocol to ensure the efficient and safe completion of cross-partition\ntransactions. Finally, experiments show that RSPC improves scalability, low\nlatency, and high throughput for crowdsensing."
    },
    {
        "date": "2025-08",
        "title": "Simulating Cyberattacks through a Breach Attack Simulation (BAS) Platform empowered by Security Chaos Engineering (SCE)",
        "author": "Arturo S\u00e1nchez-Matas, Pablo Escribano Ruiz, Daniel D\u00edaz-L\u00f3pez, Angel Luis Perales G\u00f3mez, Pantaleone Nespoli, and Gregorio Mart\u00ednez P\u00e9rez",
        "link": "http://arxiv.org/abs/2508.03882v1",
        "abstract": "In today digital landscape, organizations face constantly evolving cyber\nthreats, making it essential to discover slippery attack vectors through novel\ntechniques like Security Chaos Engineering (SCE), which allows teams to test\ndefenses and identify vulnerabilities effectively. This paper proposes to\nintegrate SCE into Breach Attack Simulation (BAS) platforms, leveraging\nadversary profiles and abilities from existing threat intelligence databases.\nThis innovative proposal for cyberattack simulation employs a structured\narchitecture composed of three layers: SCE Orchestrator, Connector, and BAS\nlayers. Utilizing MITRE Caldera in the BAS layer, our proposal executes\nautomated attack sequences, creating inferred attack trees from adversary\nprofiles. Our proposal evaluation illustrates how integrating SCE with BAS can\nenhance the effectiveness of attack simulations beyond traditional scenarios,\nand be a useful component of a cyber defense strategy."
    },
    {
        "date": "2025-08",
        "title": "Evaluating Software Supply Chain Security in Research Software",
        "author": "Richard Hegewald, and Rebecca Beyer",
        "link": "http://arxiv.org/abs/2508.03856v1",
        "abstract": "The security of research software is essential for ensuring the integrity and\nreproducibility of scientific results. However, research software security is\nstill largely unexplored. Due to its dependence on open source components and\ndistributed development practices, research software is particularly vulnerable\nto supply chain attacks. This study analyses 3,248 high-quality, largely\npeer-reviewed research software repositories using the OpenSSF Scorecard. We\nfind a generally weak security posture with an average score of 3.5/10.\nImportant practices, such as signed releases and branch protection, are rarely\nimplemented. Finally, we present actionable, low-effort recommendations that\ncan help research teams improve software security and mitigate potential\nthreats to scientific integrity."
    },
    {
        "date": "2025-08",
        "title": "CompassVerifier: A Unified and Robust Verifier for LLMs Evaluation and Outcome Reward",
        "author": "Shudong Liu, Hongwei Liu, Junnan Liu, Linchen Xiao, Songyang Gao, Chengqi Lyu, Yuzhe Gu, Wenwei Zhang, Derek F. Wong, Songyang Zhang, and Kai Chen",
        "link": "http://arxiv.org/abs/2508.03686v1",
        "abstract": "Answer verification is crucial not only for evaluating large language models\n(LLMs) by matching their unstructured outputs against standard answers, but\nalso serves as the reward model to guide LLM optimization. Most evaluation\nframeworks rely on regularized matching or employ general LLMs for answer\nverification, which demands extensive, repetitive customization for regex rules\nor evaluation prompts. Two fundamental limitations persist in current\nmethodologies: 1) the absence of comprehensive benchmarks that systematically\nevaluate verification capabilities across different LLMs; and 2) the nascent\nstage of verifier development, where existing approaches lack both the\nrobustness to handle complex edge cases and the generalizability across\ndifferent domains. In this work, we develop CompassVerifier, an accurate and\nrobust lightweight verifier model for evaluation and outcome reward. It\ndemonstrates multi-domain competency spanning math, knowledge, and diverse\nreasoning tasks, with the capability to process various answer types, including\nmulti-subproblems, formulas, and sequence answers, while effectively\nidentifying abnormal/invalid responses. We introduce VerifierBench benchmark\ncomprising model outputs collected from multiple data sources, augmented\nthrough manual analysis of metaerror patterns to enhance CompassVerifier. We\nanticipate that CompassVerifier and VerifierBench will facilitate answer\nverification, evaluation protocols, and reinforcement learning research. Code\nand dataset are available at https://github.com/open-compass/CompassVerifier."
    },
    {
        "date": "2025-08",
        "title": "VITA: Variational Pretraining of Transformers for Climate-Robust Crop Yield Forecasting",
        "author": "Adib Hasan, Mardavij Roozbehani, and Munther Dahleh",
        "link": "http://arxiv.org/abs/2508.03589v1",
        "abstract": "Accurate crop yield forecasting is essential for global food security.\nHowever, current AI models systematically underperform when yields deviate from\nhistorical trends. This issue arises from key data challenges, including a\nmajor asymmetry between rich pretraining weather datasets and the limited data\navailable for fine-tuning. We introduce VITA (Variational Inference Transformer\nfor Asymmetric data), a variational pretraining framework that addresses this\nasymmetry. Instead of relying on input reconstruction, VITA uses detailed\nweather variables as proxy targets during pretraining and learns to predict\nrich atmospheric states through self-supervised feature masking. This allows\nthe model to be fine-tuned using only basic weather statistics during\ndeployment. Applied to 763 counties in the U.S. Corn Belt, VITA achieves\nstate-of-the-art performance in predicting corn and soybean yields across all\nevaluation scenarios. While it consistently delivers superior performance under\nnormal conditions, its advantages are particularly pronounced during extreme\nweather years, with statistically significant improvements (paired t-test, $p\n\\approx 0.01$). Importantly, VITA outperforms prior frameworks like GNN-RNN\nusing less data, making it more practical for real-world use--particularly in\ndata-scarce regions. This work highlights how domain-aware AI design can\novercome data limitations and support resilient agricultural forecasting in a\nchanging climate."
    },
    {
        "date": "2025-08",
        "title": "Heterogeneity-Oblivious Robust Federated Learning",
        "author": "Weiyao Zhang, Jinyang Li, Qi Song, Miao Wang, Chungang Lin, Haitong Luo, Xuying Meng, and Yujun Zhang",
        "link": "http://arxiv.org/abs/2508.03579v2",
        "abstract": "Federated Learning (FL) remains highly vulnerable to poisoning attacks,\nespecially under real-world hyper-heterogeneity, where clients differ\nsignificantly in data distributions, communication capabilities, and model\narchitectures. Such heterogeneity not only undermines the effectiveness of\naggregation strategies but also makes attacks more difficult to detect.\nFurthermore, high-dimensional models expand the attack surface. To address\nthese challenges, we propose Horus, a heterogeneity-oblivious robust FL\nframework centered on low-rank adaptations (LoRAs). Rather than aggregating\nfull model parameters, Horus inserts LoRAs into empirically stable layers and\naggregates only LoRAs to reduce the attack uncover a key empirical observation\nthat the input projection (LoRA-A) is markedly more stable than the output\nprojection (LoRA-B) under heterogeneity and poisoning. Leveraging this, we\ndesign a Heterogeneity-Oblivious Poisoning Score using the features from LoRA-A\nto filter poisoned clients. For the remaining benign clients, we propose\nprojection-aware aggregation mechanism to preserve collaborative signals while\nsuppressing drifts, which reweights client updates by consistency with the\nglobal directions. Extensive experiments across diverse datasets, model\narchitectures, and attacks demonstrate that Horus consistently outperforms\nstate-of-the-art baselines in both robustness and accuracy."
    },
    {
        "date": "2025-08",
        "title": "Probing and Enhancing the Robustness of GNN-based QEC Decoders with Reinforcement Learning",
        "author": "Ryota Ikeda",
        "link": "http://arxiv.org/abs/2508.03783v2",
        "abstract": "Graph Neural Networks (GNNs) have emerged as a powerful, data-driven approach\nfor Quantum Error Correction (QEC) decoding, capable of learning complex noise\ncharacteristics directly from syndrome data. However, the robustness of these\ndecoders against subtle, adversarial perturbations remains a critical open\nquestion. This work introduces a novel framework to systematically probe the\nvulnerabilities of a GNN decoder using a reinforcement learning (RL) agent. The\nRL agent is trained as an adversary with the goal of finding minimal syndrome\nmodifications that cause the decoder to misclassify. We apply this framework to\na Graph Attention Network (GAT) decoder trained on experimental surface code\ndata from Google Quantum AI. Our results show that the RL agent can\nsuccessfully identify specific, critical vulnerabilities, achieving a high\nattack success rate with a minimal number of bit flips. Furthermore, we\ndemonstrate that the decoder's robustness can be significantly enhanced through\nadversarial training, where the model is retrained on the adversarial examples\ngenerated by the RL agent. This iterative process of automated vulnerability\ndiscovery and targeted retraining presents a promising methodology for\ndeveloping more reliable and robust neural network decoders for fault-tolerant\nquantum computing."
    },
    {
        "date": "2025-08",
        "title": "UPLME: Uncertainty-Aware Probabilistic Language Modelling for Robust Empathy Regression",
        "author": "Md Rakibul Hasan, Md Zakir Hossain, Aneesh Krishna, Shafin Rahman, and Tom Gedeon",
        "link": "http://arxiv.org/abs/2508.03520v1",
        "abstract": "Supervised learning for empathy regression is challenged by noisy\nself-reported empathy scores. While many algorithms have been proposed for\nlearning with noisy labels in textual classification problems, the regression\ncounterpart is relatively under-explored. We propose UPLME, an\nuncertainty-aware probabilistic language modelling framework to capture label\nnoise in the regression setting of empathy detection. UPLME includes a\nprobabilistic language model that predicts both empathy score and\nheteroscedastic uncertainty and is trained using Bayesian concepts with\nvariational model ensembling. We further introduce two novel loss components:\none penalises degenerate Uncertainty Quantification (UQ), and another enforces\nthe similarity between the input pairs on which we predict empathy. UPLME\nprovides state-of-the-art performance (Pearson Correlation Coefficient:\n$0.558\\rightarrow0.580$ and $0.629\\rightarrow0.634$) in terms of the\nperformance reported in the literature in two public benchmarks, having label\nnoise. Through synthetic label noise injection, we show that UPLME is effective\nin separating noisy and clean samples based on the predicted uncertainty. UPLME\nfurther outperform (Calibration error: $0.571\\rightarrow0.376$) a recent\nvariational model ensembling-based UQ method designed for regression problems."
    },
    {
        "date": "2025-08",
        "title": "AVPDN: Learning Motion-Robust and Scale-Adaptive Representations for Video-Based Polyp Detection",
        "author": "Zilin Chen, and Shengnan Lu",
        "link": "http://arxiv.org/abs/2508.03458v1",
        "abstract": "Accurate detection of polyps is of critical importance for the early and\nintermediate stages of colorectal cancer diagnosis. Compared to static images,\ndynamic colonoscopy videos provide more comprehensive visual information, which\ncan facilitate the development of effective treatment plans. However, unlike\nfixed-camera recordings, colonoscopy videos often exhibit rapid camera\nmovement, introducing substantial background noise that disrupts the structural\nintegrity of the scene and increases the risk of false positives. To address\nthese challenges, we propose the Adaptive Video Polyp Detection Network\n(AVPDN), a robust framework for multi-scale polyp detection in colonoscopy\nvideos. AVPDN incorporates two key components: the Adaptive Feature Interaction\nand Augmentation (AFIA) module and the Scale-Aware Context Integration (SACI)\nmodule. The AFIA module adopts a triple-branch architecture to enhance feature\nrepresentation. It employs dense self-attention for global context modeling,\nsparse self-attention to mitigate the influence of low query-key similarity in\nfeature aggregation, and channel shuffle operations to facilitate inter-branch\ninformation exchange. In parallel, the SACI module is designed to strengthen\nmulti-scale feature integration. It utilizes dilated convolutions with varying\nreceptive fields to capture contextual information at multiple spatial scales,\nthereby improving the model's denoising capability. Experiments conducted on\nseveral challenging public benchmarks demonstrate the effectiveness and\ngeneralization ability of the proposed method, achieving competitive\nperformance in video-based polyp detection tasks."
    },
    {
        "date": "2025-08",
        "title": "Are Inherently Interpretable Models More Robust? A Study In Music Emotion Recognition",
        "author": "Katharina Hoedt, Arthur Flexer, and Gerhard Widmer",
        "link": "http://arxiv.org/abs/2508.03780v1",
        "abstract": "One of the desired key properties of deep learning models is the ability to\ngeneralise to unseen samples. When provided with new samples that are\n(perceptually) similar to one or more training samples, deep learning models\nare expected to produce correspondingly similar outputs. Models that succeed in\npredicting similar outputs for similar inputs are often called robust. Deep\nlearning models, on the other hand, have been shown to be highly vulnerable to\nminor (adversarial) perturbations of the input, which manage to drastically\nchange a model's output and simultaneously expose its reliance on spurious\ncorrelations. In this work, we investigate whether inherently interpretable\ndeep models, i.e., deep models that were designed to focus more on meaningful\nand interpretable features, are more robust to irrelevant perturbations in the\ndata, compared to their black-box counterparts. We test our hypothesis by\ncomparing the robustness of an interpretable and a black-box music emotion\nrecognition (MER) model when challenged with adversarial examples. Furthermore,\nwe include an adversarially trained model, which is optimised to be more\nrobust, in the comparison. Our results indicate that inherently more\ninterpretable models can indeed be more robust than their black-box\ncounterparts, and achieve similar levels of robustness as adversarially trained\nmodels, at lower computational cost."
    },
    {
        "date": "2025-08",
        "title": "Smart Car Privacy: Survey of Attacks and Privacy Issues",
        "author": "Akshay Madhav Deshmukh",
        "link": "http://arxiv.org/abs/2508.03413v1",
        "abstract": "Automobiles are becoming increasingly important in our day to day life.\nModern automobiles are highly computerized and hence potentially vulnerable to\nattack. Providing many wireless connectivity for vehicles enables a bridge\nbetween vehicles and their external environments. Such a connected vehicle\nsolution is expected to be the next frontier for automotive revolution and the\nkey to the evolution to next generation intelligent transportation systems.\nVehicular Ad hoc Networks (VANETs) are emerging mobile ad hoc network\ntechnologies incorporating mobile routing protocols for inter-vehicle data\ncommunications to support intelligent transportation systems. Thus security and\nprivacy are the major concerns in VANETs due to the mobility of the vehicles.\nThus designing security mechanisms to remove adversaries from the network\nremarkably important in VANETs.\n  This paper provides an overview of various vehicular network architectures.\nThe evolution of security in modern vehicles. Various security and privacy\nattacks in VANETs with their defending mechanisms with examples and classify\nthese mechanisms. It also provides an overview of various privacy implication\nthat a vehicular network possess."
    },
    {
        "date": "2025-08",
        "title": "DepthGait: Multi-Scale Cross-Level Feature Fusion of RGB-Derived Depth and Silhouette Sequences for Robust Gait Recognition",
        "author": "Xinzhu Li, Juepeng Zheng, Yikun Chen, Xudong Mao, Guanghui Yue, Wei Zhou, Chenlei Lv, Ruomei Wang, Fan Zhou, and Baoquan Zhao",
        "link": "http://arxiv.org/abs/2508.03397v1",
        "abstract": "Robust gait recognition requires highly discriminative representations, which\nare closely tied to input modalities. While binary silhouettes and skeletons\nhave dominated recent literature, these 2D representations fall short of\ncapturing sufficient cues that can be exploited to handle viewpoint variations,\nand capture finer and meaningful details of gait. In this paper, we introduce a\nnovel framework, termed DepthGait, that incorporates RGB-derived depth maps and\nsilhouettes for enhanced gait recognition. Specifically, apart from the 2D\nsilhouette representation of the human body, the proposed pipeline explicitly\nestimates depth maps from a given RGB image sequence and uses them as a new\nmodality to capture discriminative features inherent in human locomotion. In\naddition, a novel multi-scale and cross-level fusion scheme has also been\ndeveloped to bridge the modality gap between depth maps and silhouettes.\nExtensive experiments on standard benchmarks demonstrate that the proposed\nDepthGait achieves state-of-the-art performance compared to peer methods and\nattains an impressive mean rank-1 accuracy on the challenging datasets."
    },
    {
        "date": "2025-08",
        "title": "Hide and Seek with LLMs: An Adversarial Game for Sneaky Error Generation and Self-Improving Diagnosis",
        "author": "Rui Zou, Mengqi Wei, Yutao Zhu, Jirong Wen, Xin Zhao, and Jing Chen",
        "link": "http://arxiv.org/abs/2508.03396v1",
        "abstract": "Large Language Models (LLMs) excel in reasoning and generation across\ndomains, but still struggle with identifying and diagnosing complex errors.\nThis stems mainly from training objectives that prioritize correct answers,\nlimiting exposure to and learning from errors. While recent studies have begun\nto address this by introducing error signals, most rely on shallow, static\nerrors, restricting improvement in deep diagnostic ability. To overcome this,\nwe propose Hide and Seek Game (HSG), a dynamic adversarial framework for error\ngeneration and diagnosis, and evaluate it on mathematical problem-solving. HSG\ninvolves two adversarial roles: Sneaky, which \"hides\" by generating subtle,\ndeceptive reasoning errors, and Diagnosis, which \"seeks\" to accurately detect\nthem. Through adversarial co-evolution, both error stealth and diagnostic\nprecision are enhanced. Experiments on several math reasoning tasks show that\nHSG significantly boosts error diagnosis, achieving 16.8\\%--31.4\\% higher\naccuracy than baselines like GPT-4o. We also release a challenging dataset of\ndeceptive errors and diagnostic annotations as a benchmark for future research."
    },
    {
        "date": "2025-08",
        "title": "When Good Sounds Go Adversarial: Jailbreaking Audio-Language Models with Benign Inputs",
        "author": "Bodam Kim, Hiskias Dingeto, Taeyoun Kwon, Dasol Choi, DongGeon Lee, Haon Park, JaeHoon Lee, and Jongho Shin",
        "link": "http://arxiv.org/abs/2508.03365v1",
        "abstract": "As large language models become increasingly integrated into daily life,\naudio has emerged as a key interface for human-AI interaction. However, this\nconvenience also introduces new vulnerabilities, making audio a potential\nattack surface for adversaries. Our research introduces WhisperInject, a\ntwo-stage adversarial audio attack framework that can manipulate\nstate-of-the-art audio language models to generate harmful content. Our method\nuses imperceptible perturbations in audio inputs that remain benign to human\nlisteners. The first stage uses a novel reward-based optimization method,\nReinforcement Learning with Projected Gradient Descent (RL-PGD), to guide the\ntarget model to circumvent its own safety protocols and generate harmful native\nresponses. This native harmful response then serves as the target for Stage 2,\nPayload Injection, where we use Projected Gradient Descent (PGD) to optimize\nsubtle perturbations that are embedded into benign audio carriers, such as\nweather queries or greeting messages. Validated under the rigorous\nStrongREJECT, LlamaGuard, as well as Human Evaluation safety evaluation\nframework, our experiments demonstrate a success rate exceeding 86% across\nQwen2.5-Omni-3B, Qwen2.5-Omni-7B, and Phi-4-Multimodal. Our work demonstrates a\nnew class of practical, audio-native threats, moving beyond theoretical\nexploits to reveal a feasible and covert method for manipulating AI behavior."
    },
    {
        "date": "2025-08",
        "title": "BDFirewall: Towards Effective and Expeditiously Black-Box Backdoor Defense in MLaaS",
        "author": "Ye Li, Chengcheng Zhu, Yanchao Zhao, and Jiale Zhang",
        "link": "http://arxiv.org/abs/2508.03307v1",
        "abstract": "In this paper, we endeavor to address the challenges of backdoor attacks\ncountermeasures in black-box scenarios, thereby fortifying the security of\ninference under MLaaS. We first categorize backdoor triggers from a new\nperspective, i.e., their impact on the patched area, and divide them into:\nhigh-visibility triggers (HVT), semi-visibility triggers (SVT), and\nlow-visibility triggers (LVT). Based on this classification, we propose a\nprogressive defense framework, BDFirewall, that removes these triggers from the\nmost conspicuous to the most subtle, without requiring model access. First, for\nHVTs, which create the most significant local semantic distortions, we identify\nand eliminate them by detecting these salient differences. We then restore the\npatched area to mitigate the adverse impact of such removal process. The\nlocalized purification designed for HVTs is, however, ineffective against SVTs,\nwhich globally perturb benign features. We therefore model an SVT-poisoned\ninput as a mixture of a trigger and benign features, where we unconventionally\ntreat the benign features as \"noise\". This formulation allows us to reconstruct\nSVTs by applying a denoising process that removes these benign \"noise\"\nfeatures. The SVT-free input is then obtained by subtracting the reconstructed\ntrigger. Finally, to neutralize the nearly imperceptible but fragile LVTs, we\nintroduce lightweight noise to disrupt the trigger pattern and then apply DDPM\nto restore any collateral impact on clean features. Comprehensive experiments\ndemonstrate that our method outperforms state-of-the-art defenses. Compared\nwith baselines, BDFirewall reduces the Attack Success Rate (ASR) by an average\nof 33.25%, improving poisoned sample accuracy (PA) by 29.64%, and achieving up\nto a 111x speedup in inference time. Code will be made publicly available upon\nacceptance."
    },
    {
        "date": "2025-08",
        "title": "Robust Single-Stage Fully Sparse 3D Object Detection via Detachable Latent Diffusion",
        "author": "Wentao Qu, Guofeng Mei, Jing Wang, Yujiao Wu, Xiaoshui Huang, and Liang Xiao",
        "link": "http://arxiv.org/abs/2508.03252v1",
        "abstract": "Denoising Diffusion Probabilistic Models (DDPMs) have shown success in robust\n3D object detection tasks. Existing methods often rely on the score matching\nfrom 3D boxes or pre-trained diffusion priors. However, they typically require\nmulti-step iterations in inference, which limits efficiency. To address this,\nwe propose a \\textbf{R}obust single-stage fully \\textbf{S}parse 3D object\n\\textbf{D}etection \\textbf{Net}work with a Detachable Latent Framework (DLF) of\nDDPMs, named RSDNet. Specifically, RSDNet learns the denoising process in\nlatent feature spaces through lightweight denoising networks like multi-level\ndenoising autoencoders (DAEs). This enables RSDNet to effectively understand\nscene distributions under multi-level perturbations, achieving robust and\nreliable detection. Meanwhile, we reformulate the noising and denoising\nmechanisms of DDPMs, enabling DLF to construct multi-type and multi-level noise\nsamples and targets, enhancing RSDNet robustness to multiple perturbations.\nFurthermore, a semantic-geometric conditional guidance is introduced to\nperceive the object boundaries and shapes, alleviating the center feature\nmissing problem in sparse representations, enabling RSDNet to perform in a\nfully sparse detection pipeline. Moreover, the detachable denoising network\ndesign of DLF enables RSDNet to perform single-step detection in inference,\nfurther enhancing detection efficiency. Extensive experiments on public\nbenchmarks show that RSDNet can outperform existing methods, achieving\nstate-of-the-art detection."
    },
    {
        "date": "2025-08",
        "title": "BadBlocks: Low-Cost and Stealthy Backdoor Attacks Tailored for Text-to-Image Diffusion Models",
        "author": "Yu Pan, Jiahao Chen, Lin Wang, Bingrong Dai, and Yi Du",
        "link": "http://arxiv.org/abs/2508.03221v1",
        "abstract": "In recent years,Diffusion models have achieved remarkable progress in the\nfield of image generation.However,recent studies have shown that diffusion\nmodels are susceptible to backdoor attacks,in which attackers can manipulate\nthe output by injecting covert triggers such as specific visual patterns or\ntextual phrases into the training dataset.Fortunately,with the continuous\nadvancement of defense techniques,defenders have become increasingly capable of\nidentifying and mitigating most backdoor attacks using visual inspection and\nneural network-based detection methods.However,in this paper,we identify a\nnovel type of backdoor threat that is more lightweight and covert than existing\napproaches,which we name BadBlocks,requires only about 30\\% of the\ncomputational resources and 20\\% GPU time typically needed by previous backdoor\nattacks,yet it successfully injects backdoors and evades the most advanced\ndefense frameworks.BadBlocks enables attackers to selectively contaminate\nspecific blocks within the UNet architecture of diffusion models while\nmaintaining normal functionality in the remaining components.Experimental\nresults demonstrate that BadBlocks achieves a high attack success rate (ASR)\nand low perceptual quality loss (as measured by FID Score),even under extremely\nconstrained computational resources and GPU time.Moreover,BadBlocks is able to\nbypass existing defense frameworks,especially the attention-based backdoor\ndetection method, highlighting it as a novel and noteworthy threat.Ablation\nstudies further demonstrate that effective backdoor injection does not require\nfine-tuning the entire network and highlight the pivotal role of certain neural\nnetwork layers in backdoor mapping.Overall,BadBlocks significantly reduces the\nbarrier to conducting backdoor attacks in all aspects.It enables attackers to\ninject backdoors into large-scale diffusion models even using consumer-grade\nGPUs."
    },
    {
        "date": "2025-08",
        "title": "The Power of Many: Synergistic Unification of Diverse Augmentations for Efficient Adversarial Robustness",
        "author": "Wang Yu-Hang, Shiwei Li, Jianxiang Liao, Li Bohan, Jian Liu, and Wenfei Yin",
        "link": "http://arxiv.org/abs/2508.03213v1",
        "abstract": "Adversarial perturbations pose a significant threat to deep learning models.\nAdversarial Training (AT), the predominant defense method, faces challenges of\nhigh computational costs and a degradation in standard performance. While data\naugmentation offers an alternative path, existing techniques either yield\nlimited robustness gains or incur substantial training overhead. Therefore,\ndeveloping a defense mechanism that is both highly efficient and strongly\nrobust is of paramount importance.In this work, we first conduct a systematic\nanalysis of existing augmentation techniques, revealing that the synergy among\ndiverse strategies -- rather than any single method -- is crucial for enhancing\nrobustness. Based on this insight, we propose the Universal Adversarial\nAugmenter (UAA) framework, which is characterized by its plug-and-play nature\nand training efficiency. UAA decouples the expensive perturbation generation\nprocess from model training by pre-computing a universal transformation\noffline, which is then used to efficiently generate unique adversarial\nperturbations for each sample during training.Extensive experiments conducted\non multiple benchmarks validate the effectiveness of UAA. The results\ndemonstrate that UAA establishes a new state-of-the-art (SOTA) for\ndata-augmentation-based adversarial defense strategies , without requiring the\nonline generation of adversarial examples during training. This framework\nprovides a practical and efficient pathway for building robust models,Our code\nis available in the supplementary materials."
    },
    {
        "date": "2025-08",
        "title": "GeoShield: Safeguarding Geolocation Privacy from Vision-Language Models via Adversarial Perturbations",
        "author": "Xinwei Liu, Xiaojun Jia, Yuan Xun, Simeng Qin, and Xiaochun Cao",
        "link": "http://arxiv.org/abs/2508.03209v1",
        "abstract": "Vision-Language Models (VLMs) such as GPT-4o now demonstrate a remarkable\nability to infer users' locations from public shared images, posing a\nsubstantial risk to geoprivacy. Although adversarial perturbations offer a\npotential defense, current methods are ill-suited for this scenario: they often\nperform poorly on high-resolution images and low perturbation budgets, and may\nintroduce irrelevant semantic content. To address these limitations, we propose\nGeoShield, a novel adversarial framework designed for robust geoprivacy\nprotection in real-world scenarios. GeoShield comprises three key modules: a\nfeature disentanglement module that separates geographical and non-geographical\ninformation, an exposure element identification module that pinpoints\ngeo-revealing regions within an image, and a scale-adaptive enhancement module\nthat jointly optimizes perturbations at both global and local levels to ensure\neffectiveness across resolutions. Extensive experiments on challenging\nbenchmarks show that GeoShield consistently surpasses prior methods in\nblack-box settings, achieving strong privacy protection with minimal impact on\nvisual or semantic quality. To our knowledge, this work is the first to explore\nadversarial perturbations for defending against geolocation inference by\nadvanced VLMs, providing a practical and effective solution to escalating\nprivacy concerns."
    },
    {
        "date": "2025-08",
        "title": "Attack the Messages, Not the Agents: A Multi-round Adaptive Stealthy Tampering Framework for LLM-MAS",
        "author": "Bingyu Yan, Ziyi Zhou, Xiaoming Zhang, Chaozhuo Li, Ruilin Zeng, Yirui Qi, Tianbo Wang, and Litian Zhang",
        "link": "http://arxiv.org/abs/2508.03125v1",
        "abstract": "Large language model-based multi-agent systems (LLM-MAS) effectively\naccomplish complex and dynamic tasks through inter-agent communication, but\nthis reliance introduces substantial safety vulnerabilities. Existing attack\nmethods targeting LLM-MAS either compromise agent internals or rely on direct\nand overt persuasion, which limit their effectiveness, adaptability, and\nstealthiness. In this paper, we propose MAST, a Multi-round Adaptive Stealthy\nTampering framework designed to exploit communication vulnerabilities within\nthe system. MAST integrates Monte Carlo Tree Search with Direct Preference\nOptimization to train an attack policy model that adaptively generates\neffective multi-round tampering strategies. Furthermore, to preserve\nstealthiness, we impose dual semantic and embedding similarity constraints\nduring the tampering process. Comprehensive experiments across diverse tasks,\ncommunication architectures, and LLMs demonstrate that MAST consistently\nachieves high attack success rates while significantly enhancing stealthiness\ncompared to baselines. These findings highlight the effectiveness,\nstealthiness, and adaptability of MAST, underscoring the need for robust\ncommunication safeguards in LLM-MAS."
    },
    {
        "date": "2025-08",
        "title": "Pseudo-label Induced Subspace Representation Learning for Robust Out-of-Distribution Detection",
        "author": "Tarhib Al Azad, Faizul Rakib Sayem, and Shahana Ibrahim",
        "link": "http://arxiv.org/abs/2508.03108v1",
        "abstract": "Out-of-distribution (OOD) detection lies at the heart of robust artificial\nintelligence (AI), aiming to identify samples from novel distributions beyond\nthe training set. Recent approaches have exploited feature representations as\ndistinguishing signatures for OOD detection. However, most existing methods\nrely on restrictive assumptions on the feature space that limit the\nseparability between in-distribution (ID) and OOD samples. In this work, we\npropose a novel OOD detection framework based on a pseudo-label-induced\nsubspace representation, that works under more relaxed and natural assumptions\ncompared to existing feature-based techniques. In addition, we introduce a\nsimple yet effective learning criterion that integrates a cross-entropy-based\nID classification loss with a subspace distance-based regularization loss to\nenhance ID-OOD separability. Extensive experiments validate the effectiveness\nof our framework."
    },
    {
        "date": "2025-08",
        "title": "VRPO: Rethinking Value Modeling for Robust RL Training under Noisy Supervision",
        "author": "Dingwei Zhu, Shihan Dou, Zhiheng Xi, Senjie Jin, Guoqiang Zhang, Jiazheng Zhang, Junjie Ye, Mingxu Chai, Enyu Zhou, Ming Zhang, Caishuang Huang, Yunke Zhang, Yuran Wang, and Tao Gui",
        "link": "http://arxiv.org/abs/2508.03058v1",
        "abstract": "Reinforcement Learning from Human Feedback (RLHF) often suffers from noisy or\nimperfect reward supervision in real-world settings, which undermines policy\nstability and generalization. Such noise may cause models to lose attention on\nkey words during advantage estimation. While prior work focuses on reward\ndenoising or filtering poor data, it often overlooks the critical role of the\nvalue model in policy optimization. In this work, we show that a strong value\nmodel is essential for mitigating noise by absorbing unstable signals and\nenabling more reliable advantage estimation. We propose VRPO, a value-centric\nframework for robust PPO training under noisy supervision. VRPO combines two\ncore designs: (1) an auxiliary loss guided by entropy and perplexity from a\nfrozen language model, and (2) a variational information bottleneck. These\nmechanisms enhance the value model's ability to filter out noise and capture\nkey words from the context during advantage estimation, transforming it from a\npassive predictor into an active regulator of noise. Experiments on math\nreasoning, science QA, and multi-turn dialogue, under both rule-based and\nmodel-based noisy rewards, show that VRPO consistently outperforms PPO and GRPO\nbaselines. Our findings underscore the often-overlooked importance of the value\nmodel in RLHF and offer a principled and practical approach to robust policy\noptimization in noisy real-world environments."
    },
    {
        "date": "2025-08",
        "title": "Beyond Surface-Level Detection: Towards Cognitive-Driven Defense Against Jailbreak Attacks via Meta-Operations Reasoning",
        "author": "Rui Pu, Chaozhuo Li, Rui Ha, Litian Zhang, Lirong Qiu, and Xi Zhang",
        "link": "http://arxiv.org/abs/2508.03054v1",
        "abstract": "Defending large language models (LLMs) against jailbreak attacks is essential\nfor their safe and reliable deployment. Existing defenses often rely on shallow\npattern matching, which struggles to generalize to novel and unseen attack\nstrategies. To address this challenge, we propose the Cognitive-Driven Defense\n(CDD) framework, which targets the underlying structure of jailbreak prompts by\napplying meta-operations, defined as basic manipulations that conceal harmful\nintent.CDD emulates human cognitive reasoning through a structured reasoning\nchain. It begins with a global perception of the prompt and follows with a\nlocalized analysis to uncover hidden manipulations. By applying supervised\nfine-tuning on this structured chain, the model learns to identify and reason\nabout known manipulation patterns. To enhance generalization to unseen threats,\nan entropy-guided reinforcement learning algorithm (EG-GRPO) is introduced to\nencourage exploration of new types and variants of meta-operations. Experiments\ndemonstrate that CDD can achieve state-of-the-art defense performance and\nexhibit strong generalization to unseen jailbreak attacks."
    },
    {
        "date": "2025-08",
        "title": "Aerobatic maneuvers in insect-scale flapping-wing aerial robots via deep-learned robust tube model predictive control",
        "author": "Yi-Hsuan Hsiao, Andrea Tagliabue, Owen Matteson, Suhan Kim, Tong Zhao, Jonathan P. How, and YuFeng Chen",
        "link": "http://arxiv.org/abs/2508.03043v1",
        "abstract": "Aerial insects exhibit highly agile maneuvers such as sharp braking,\nsaccades, and body flips under disturbance. In contrast, insect-scale aerial\nrobots are limited to tracking non-aggressive trajectories with small body\nacceleration. This performance gap is contributed by a combination of low robot\ninertia, fast dynamics, uncertainty in flapping-wing aerodynamics, and high\nsusceptibility to environmental disturbance. Executing highly dynamic maneuvers\nrequires the generation of aggressive flight trajectories that push against the\nhardware limit and a high-rate feedback controller that accounts for model and\nenvironmental uncertainty. Here, through designing a deep-learned robust tube\nmodel predictive controller, we showcase insect-like flight agility and\nrobustness in a 750-millgram flapping-wing robot. Our model predictive\ncontroller can track aggressive flight trajectories under disturbance. To\nachieve a high feedback rate in a compute-constrained real-time system, we\ndesign imitation learning methods to train a two-layer, fully connected neural\nnetwork, which resembles insect flight control architecture consisting of\ncentral nervous system and motor neurons. Our robot demonstrates insect-like\nsaccade movements with lateral speed and acceleration of 197 centimeters per\nsecond and 11.7 meters per second square, representing 447$\\%$ and 255$\\%$\nimprovement over prior results. The robot can also perform saccade maneuvers\nunder 160 centimeters per second wind disturbance and large command-to-force\nmapping errors. Furthermore, it performs 10 consecutive body flips in 11\nseconds - the most challenging maneuver among sub-gram flyers. These results\nrepresent a milestone in achieving insect-scale flight agility and inspire\nfuture investigations on sensing and compute autonomy."
    },
    {
        "date": "2025-08",
        "title": "Provably Near-Optimal Distributionally Robust Reinforcement Learning in Online Settings",
        "author": "Debamita Ghosh, George K. Atia, and Yue Wang",
        "link": "http://arxiv.org/abs/2508.03768v1",
        "abstract": "Reinforcement learning (RL) faces significant challenges in real-world\ndeployments due to the sim-to-real gap, where policies trained in simulators\noften underperform in practice due to mismatches between training and\ndeployment conditions. Distributionally robust RL addresses this issue by\noptimizing worst-case performance over an uncertainty set of environments and\nproviding an optimized lower bound on deployment performance. However, existing\nstudies typically assume access to either a generative model or offline\ndatasets with broad coverage of the deployment environment -- assumptions that\nlimit their practicality in unknown environments without prior knowledge. In\nthis work, we study the more realistic and challenging setting of online\ndistributionally robust RL, where the agent interacts only with a single\nunknown training environment while aiming to optimize its worst-case\nperformance. We focus on general $f$-divergence-based uncertainty sets,\nincluding Chi-Square and KL divergence balls, and propose a computationally\nefficient algorithm with sublinear regret guarantees under minimal assumptions.\nFurthermore, we establish a minimax lower bound on regret of online learning,\ndemonstrating the near-optimality of our approach. Extensive experiments across\ndiverse environments further confirm the robustness and efficiency of our\nalgorithm, validating our theoretical findings."
    },
    {
        "date": "2025-08",
        "title": "A Robust and Efficient Pipeline for Enterprise-Level Large-Scale Entity Resolution",
        "author": "Sandeepa Kannangara, Arman Abrahamyan, Daniel Elias, Thomas Kilby, Nadav Dar, Luiz Pizzato, Anna Leontjeva, and Dan Jermyn",
        "link": "http://arxiv.org/abs/2508.03767v1",
        "abstract": "Entity resolution (ER) remains a significant challenge in data management,\nespecially when dealing with large datasets. This paper introduces MERAI\n(Massive Entity Resolution using AI), a robust and efficient pipeline designed\nto address record deduplication and linkage issues in high-volume datasets at\nan enterprise level. The pipeline's resilience and accuracy have been validated\nthrough various large-scale record deduplication and linkage projects. To\nevaluate MERAI's performance, we compared it with two well-known entity\nresolution libraries, Dedupe and Splink. While Dedupe failed to scale beyond 2\nmillion records due to memory constraints, MERAI successfully processed\ndatasets of up to 15.7 million records and produced accurate results across all\nexperiments. Experimental data demonstrates that MERAI outperforms both\nbaseline systems in terms of matching accuracy, with consistently higher F1\nscores in both deduplication and record linkage tasks. MERAI offers a scalable\nand reliable solution for enterprise-level large-scale entity resolution,\nensuring data integrity and consistency in real-world applications."
    },
    {
        "date": "2025-08",
        "title": "VCNet: Recreating High-Level Visual Cortex Principles for Robust Artificial Vision",
        "author": "Brennen A. Hill, Zhang Xinyu, and Timothy Putra Prasetio",
        "link": "http://arxiv.org/abs/2508.02995v1",
        "abstract": "Despite their success in image classification, modern convolutional neural\nnetworks (CNNs) exhibit fundamental limitations, including data inefficiency,\npoor out-of-distribution generalization, and vulnerability to adversarial\nperturbations. The primate visual system, in contrast, demonstrates superior\nefficiency and robustness, suggesting that its architectural principles may\noffer a blueprint for more capable artificial vision systems. This paper\nintroduces Visual Cortex Network (VCNet), a novel neural network architecture\nwhose design is informed by the macro-scale organization of the primate visual\ncortex. VCNet emulates key biological mechanisms, including hierarchical\nprocessing across distinct cortical areas, dual-stream information segregation,\nand top-down predictive feedback. We evaluate VCNet on two specialized\nbenchmarks: the Spots-10 animal pattern dataset and a light field image\nclassification task. Our results show that VCNet achieves a classification\naccuracy of 92.1\\% on Spots-10 and 74.4\\% on the light field dataset,\nsurpassing contemporary models of comparable size. This work demonstrates that\nintegrating neuroscientific principles into network design can lead to more\nefficient and robust models, providing a promising direction for addressing\nlong-standing challenges in machine learning."
    },
    {
        "date": "2025-08",
        "title": "Adversarial Attention Perturbations for Large Object Detection Transformers",
        "author": "Zachary Yahn, Selim Furkan Tekin, Fatih Ilhan, Sihao Hu, Tiansheng Huang, Yichang Xu, Margaret Loper, and Ling Liu",
        "link": "http://arxiv.org/abs/2508.02987v1",
        "abstract": "Adversarial perturbations are useful tools for exposing vulnerabilities in\nneural networks. Existing adversarial perturbation methods for object detection\nare either limited to attacking CNN-based detectors or weak against\ntransformer-based detectors. This paper presents an Attention-Focused Offensive\nGradient (AFOG) attack against object detection transformers. By design, AFOG\nis neural-architecture agnostic and effective for attacking both large\ntransformer-based object detectors and conventional CNN-based detectors with a\nunified adversarial attention framework. This paper makes three original\ncontributions. First, AFOG utilizes a learnable attention mechanism that\nfocuses perturbations on vulnerable image regions in multi-box detection tasks,\nincreasing performance over non-attention baselines by up to 30.6%. Second,\nAFOG's attack loss is formulated by integrating two types of feature loss\nthrough learnable attention updates with iterative injection of adversarial\nperturbations. Finally, AFOG is an efficient and stealthy adversarial\nperturbation method. It probes the weak spots of detection transformers by\nadding strategically generated and visually imperceptible perturbations which\ncan cause well-trained object detection models to fail. Extensive experiments\nconducted with twelve large detection transformers on COCO demonstrate the\nefficacy of AFOG. Our empirical results also show that AFOG outperforms\nexisting attacks on transformer-based and CNN-based object detectors by up to\n83% with superior speed and imperceptibility. Code is available at\nhttps://github.com/zacharyyahn/AFOG."
    },
    {
        "date": "2025-08",
        "title": "Towards Robust Image Denoising with Scale Equivariance",
        "author": "Dawei Zhang, and Xiaojie Guo",
        "link": "http://arxiv.org/abs/2508.02967v1",
        "abstract": "Despite notable advances in image denoising, existing models often struggle\nto generalize beyond in-distribution noise patterns, particularly when\nconfronted with out-of-distribution (OOD) conditions characterized by spatially\nvariant noise. This generalization gap remains a fundamental yet underexplored\nchallenge. In this work, we investigate \\emph{scale equivariance} as a core\ninductive bias for improving OOD robustness. We argue that incorporating\nscale-equivariant structures enables models to better adapt from training on\nspatially uniform noise to inference on spatially non-uniform degradations.\nBuilding on this insight, we propose a robust blind denoising framework\nequipped with two key components: a Heterogeneous Normalization Module (HNM)\nand an Interactive Gating Module (IGM). HNM stabilizes feature distributions\nand dynamically corrects features under varying noise intensities, while IGM\nfacilitates effective information modulation via gated interactions between\nsignal and feature paths. Extensive evaluations demonstrate that our model\nconsistently outperforms state-of-the-art methods on both synthetic and\nreal-world benchmarks, especially under spatially heterogeneous noise. Code\nwill be made publicly available."
    },
    {
        "date": "2025-08",
        "title": "Injecting Measurement Information Yields a Fast and Noise-Robust Diffusion-Based Inverse Problem Solver",
        "author": "Jonathan Patsenker, Henry Li, Myeongseob Ko, Ruoxi Jia, and Yuval Kluger",
        "link": "http://arxiv.org/abs/2508.02964v1",
        "abstract": "Diffusion models have been firmly established as principled zero-shot solvers\nfor linear and nonlinear inverse problems, owing to their powerful image prior\nand iterative sampling algorithm. These approaches often rely on Tweedie's\nformula, which relates the diffusion variate $\\mathbf{x}_t$ to the posterior\nmean $\\mathbb{E} [\\mathbf{x}_0 | \\mathbf{x}_t]$, in order to guide the\ndiffusion trajectory with an estimate of the final denoised sample\n$\\mathbf{x}_0$. However, this does not consider information from the\nmeasurement $\\mathbf{y}$, which must then be integrated downstream. In this\nwork, we propose to estimate the conditional posterior mean $\\mathbb{E}\n[\\mathbf{x}_0 | \\mathbf{x}_t, \\mathbf{y}]$, which can be formulated as the\nsolution to a lightweight, single-parameter maximum likelihood estimation\nproblem. The resulting prediction can be integrated into any standard sampler,\nresulting in a fast and memory-efficient inverse solver. Our optimizer is\namenable to a noise-aware likelihood-based stopping criteria that is robust to\nmeasurement noise in $\\mathbf{y}$. We demonstrate comparable or improved\nperformance against a wide selection of contemporary inverse solvers across\nmultiple datasets and tasks."
    },
    {
        "date": "2025-08",
        "title": "AMD-Mamba: A Phenotype-Aware Multi-Modal Framework for Robust AMD Prognosis",
        "author": "Puzhen Wu, Mingquan Lin, Qingyu Chen, Emily Y. Chew, Zhiyong Lu, Yifan Peng, and Hexin Dong",
        "link": "http://arxiv.org/abs/2508.02957v1",
        "abstract": "Age-related macular degeneration (AMD) is a leading cause of irreversible\nvision loss, making effective prognosis crucial for timely intervention. In\nthis work, we propose AMD-Mamba, a novel multi-modal framework for AMD\nprognosis, and further develop a new AMD biomarker. This framework integrates\ncolor fundus images with genetic variants and socio-demographic variables. At\nits core, AMD-Mamba introduces an innovative metric learning strategy that\nleverages AMD severity scale score as prior knowledge. This strategy allows the\nmodel to learn richer feature representations by aligning learned features with\nclinical phenotypes, thereby improving the capability of conventional prognosis\nmethods in capturing disease progression patterns. In addition, unlike existing\nmodels that use traditional CNN backbones and focus primarily on local\ninformation, such as the presence of drusen, AMD-Mamba applies Vision Mamba and\nsimultaneously fuses local and long-range global information, such as vascular\nchanges. Furthermore, we enhance prediction performance through multi-scale\nfusion, combining image information with clinical variables at different\nresolutions. We evaluate AMD-Mamba on the AREDS dataset, which includes 45,818\ncolor fundus photographs, 52 genetic variants, and 3 socio-demographic\nvariables from 2,741 subjects. Our experimental results demonstrate that our\nproposed biomarker is one of the most significant biomarkers for the\nprogression of AMD. Notably, combining this biomarker with other existing\nvariables yields promising improvements in detecting high-risk AMD patients at\nearly stages. These findings highlight the potential of our multi-modal\nframework to facilitate more precise and proactive management of AMD."
    },
    {
        "date": "2025-08",
        "title": "Online Robust Multi-Agent Reinforcement Learning under Model Uncertainties",
        "author": "Zain Ulabedeen Farhat, Debamita Ghosh, George K. Atia, and Yue Wang",
        "link": "http://arxiv.org/abs/2508.02948v1",
        "abstract": "Well-trained multi-agent systems can fail when deployed in real-world\nenvironments due to model mismatches between the training and deployment\nenvironments, caused by environment uncertainties including noise or\nadversarial attacks. Distributionally Robust Markov Games (DRMGs) enhance\nsystem resilience by optimizing for worst-case performance over a defined set\nof environmental uncertainties. However, current methods are limited by their\ndependence on simulators or large offline datasets, which are often\nunavailable. This paper pioneers the study of online learning in DRMGs, where\nagents learn directly from environmental interactions without prior data. We\nintroduce the {\\it Robust Optimistic Nash Value Iteration (RONAVI)} algorithm\nand provide the first provable guarantees for this setting. Our theoretical\nanalysis demonstrates that the algorithm achieves low regret and efficiently\nfinds the optimal robust policy for uncertainty sets measured by Total\nVariation divergence and Kullback-Leibler divergence. These results establish a\nnew, practical path toward developing truly robust multi-agent systems."
    },
    {
        "date": "2025-08",
        "title": "RDDPM: Robust Denoising Diffusion Probabilistic Model for Unsupervised Anomaly Segmentation",
        "author": "Mehrdad Moradi, and Kamran Paynabar",
        "link": "http://arxiv.org/abs/2508.02903v1",
        "abstract": "Recent advancements in diffusion models have demonstrated significant success\nin unsupervised anomaly segmentation. For anomaly segmentation, these models\nare first trained on normal data; then, an anomalous image is noised to an\nintermediate step, and the normal image is reconstructed through backward\ndiffusion. Unlike traditional statistical methods, diffusion models do not rely\non specific assumptions about the data or target anomalies, making them\nversatile for use across different domains. However, diffusion models typically\nassume access to normal data for training, limiting their applicability in\nrealistic settings. In this paper, we propose novel robust denoising diffusion\nmodels for scenarios where only contaminated (i.e., a mix of normal and\nanomalous) unlabeled data is available. By casting maximum likelihood\nestimation of the data as a nonlinear regression problem, we reinterpret the\ndenoising diffusion probabilistic model through a regression lens. Using robust\nregression, we derive a robust version of denoising diffusion probabilistic\nmodels. Our novel framework offers flexibility in constructing various robust\ndiffusion models. Our experiments show that our approach outperforms current\nstate of the art diffusion models, for unsupervised anomaly segmentation when\nonly contaminated data is available. Our method outperforms existing\ndiffusion-based approaches, achieving up to 8.08\\% higher AUROC and 10.37\\%\nhigher AUPRC on MVTec datasets. The implementation code is available at:\nhttps://github.com/mehrdadmoradi124/RDDPM"
    },
    {
        "date": "2025-08",
        "title": "Optimizing Preventive and Reactive Defense Resource Allocation with Uncertain Sensor Signals",
        "author": "Faezeh Shojaeighadikolaei, Shouhuai Xu, and Keith Paarporn",
        "link": "http://arxiv.org/abs/2508.02881v2",
        "abstract": "Cyber attacks continue to be a cause of concern despite advances in cyber\ndefense techniques. Although cyber attacks cannot be fully prevented, standard\ndecision-making frameworks typically focus on how to prevent them from\nsucceeding, without considering the cost of cleaning up the damages incurred by\nsuccessful attacks. This motivates us to investigate a new resource allocation\nproblem formulated in this paper: The defender must decide how to split its\ninvestment between preventive defenses, which aim to harden nodes from attacks,\nand reactive defenses, which aim to quickly clean up the compromised nodes.\nThis encounters a challenge imposed by the uncertainty associated with the\nobservation, or sensor signal, whether a node is truly compromised or not; this\nuncertainty is real because attack detectors are not perfect. We investigate\nhow the quality of sensor signals impacts the defender's strategic investment\nin the two types of defense, and ultimately the level of security that can be\nachieved. In particular, we show that the optimal investment in preventive\nresources increases, and thus reactive resource investment decreases, with\nhigher sensor quality. We also show that the defender's performance\nimprovement, relative to a baseline of no sensors employed, is maximal when the\nattacker can only achieve low attack success probabilities."
    },
    {
        "date": "2025-08",
        "title": "Beyond Least Squares: Robust Regression Transformer (R2T)",
        "author": "Roman Gutierrez, Tony Kai Tang, and Isabel Gutierrez",
        "link": "http://arxiv.org/abs/2508.02874v1",
        "abstract": "Robust regression techniques rely on least-squares optimization, which works\nwell for Gaussian noise but fails in the presence of asymmetric structured\nnoise. We propose a hybrid neural-symbolic architecture where a transformer\nencoder processes numerical sequences, a compression NN predicts symbolic\nparameters, and a fixed symbolic equation reconstructs the original sequence.\nUsing synthetic data, the training objective is to recover the original\nsequence after adding asymmetric structured noise, effectively learning a\nsymbolic fit guided by neural parameter estimation. Our model achieves a median\nregression MSE of 6e-6 to 3.5e-5 on synthetic wearable data, which is a 10-300\ntimes improvement when compared with ordinary least squares fit and robust\nregression techniques such as Huber loss or SoftL1."
    },
    {
        "date": "2025-08",
        "title": "Secure mmWave Beamforming with Proactive-ISAC Defense Against Beam-Stealing Attacks",
        "author": "Seyed Bagher Hashemi Natanzi, Hossein Mohammadi, Bo Tang, and Vuk Marojevic",
        "link": "http://arxiv.org/abs/2508.02856v1",
        "abstract": "Millimeter-wave (mmWave) communication systems face increasing susceptibility\nto advanced beam-stealing attacks, posing a significant physical layer security\nthreat. This paper introduces a novel framework employing an advanced Deep\nReinforcement Learning (DRL) agent for proactive and adaptive defense against\nthese sophisticated attacks. A key innovation is leveraging Integrated Sensing\nand Communications (ISAC) capabilities for active, intelligent threat\nassessment. The DRL agent, built on a Proximal Policy Optimization (PPO)\nalgorithm, dynamically controls ISAC probing actions to investigate suspicious\nactivities. We introduce an intensive curriculum learning strategy that\nguarantees the agent experiences successful detection during training to\novercome the complex exploration challenges inherent to such a\nsecurity-critical task. Consequently, the agent learns a robust and adaptive\npolicy that intelligently balances security and communication performance.\nNumerical results demonstrate that our framework achieves a mean attacker\ndetection rate of 92.8% while maintaining an average user SINR of over 13 dB."
    },
    {
        "date": "2025-08",
        "title": "Defending Against Knowledge Poisoning Attacks During Retrieval-Augmented Generation",
        "author": "Kennedy Edemacu, Vinay M. Shashidhar, Micheal Tuape, Dan Abudu, Beakcheol Jang, and Jong Wook Kim",
        "link": "http://arxiv.org/abs/2508.02835v1",
        "abstract": "Retrieval-Augmented Generation (RAG) has emerged as a powerful approach to\nboost the capabilities of large language models (LLMs) by incorporating\nexternal, up-to-date knowledge sources. However, this introduces a potential\nvulnerability to knowledge poisoning attacks, where attackers can compromise\nthe knowledge source to mislead the generation model. One such attack is the\nPoisonedRAG in which the injected adversarial texts steer the model to generate\nan attacker-chosen response to a target question. In this work, we propose\nnovel defense methods, FilterRAG and ML-FilterRAG, to mitigate the PoisonedRAG\nattack. First, we propose a new property to uncover distinct properties to\ndifferentiate between adversarial and clean texts in the knowledge data source.\nNext, we employ this property to filter out adversarial texts from clean ones\nin the design of our proposed approaches. Evaluation of these methods using\nbenchmark datasets demonstrate their effectiveness, with performances close to\nthose of the original RAG systems."
    },
    {
        "date": "2025-08",
        "title": "Uncertainty Sets for Distributionally Robust Bandits Using Structural Equation Models",
        "author": "Katherine Avery, Chinmay Pendse, and David Jensen",
        "link": "http://arxiv.org/abs/2508.02812v1",
        "abstract": "Distributionally robust evaluation estimates the worst-case expected return\nover an uncertainty set of possible covariate and reward distributions, and\ndistributionally robust learning finds a policy that maximizes that worst-case\nreturn across that uncertainty set. Unfortunately, current methods for\ndistributionally robust evaluation and learning create overly conservative\nevaluations and policies. In this work, we propose a practical bandit\nevaluation and learning algorithm that tailors the uncertainty set to specific\nproblems using mathematical programs constrained by structural equation models.\nFurther, we show how conditional independence testing can be used to detect\nshifted variables for modeling. We find that the structural equation model\n(SEM) approach gives more accurate evaluations and learns lower-variance\npolicies than traditional approaches, particularly for large shifts. Further,\nthe SEM approach learns an optimal policy, assuming the model is sufficiently\nwell-specified."
    },
    {
        "date": "2025-08",
        "title": "Real-World Evaluation of Protocol-Compliant Denial-of-Service Attacks on C-V2X-based Forward Collision Warning Systems",
        "author": "Jean Michel Tine, Mohammed Aldeen, Abyad Enan, M Sabbir Salek, Long Cheng, and Mashrur Chowdhury",
        "link": "http://arxiv.org/abs/2508.02805v1",
        "abstract": "Cellular Vehicle-to-Everything (C-V2X) technology enables low-latency,\nreliable communications essential for safety applications such as a Forward\nCollision Warning (FCW) system. C-V2X deployments operate under strict protocol\ncompliance with the 3rd Generation Partnership Project (3GPP) and the Society\nof Automotive Engineers Standard (SAE) J2735 specifications to ensure\ninteroperability. This paper presents a real-world testbed evaluation of\nprotocol-compliant Denial-of-Service (DoS) attacks using User Datagram Protocol\n(UDP) flooding and oversized Basic Safety Message (BSM) attacks that 7 exploit\ntransport- and application-layer vulnerabilities in C-V2X. The attacks\npresented in this study transmit valid messages over standard PC5 sidelinks,\nfully adhering to 3GPP and SAE J2735 specifications, but at abnormally high\nrates and with oversized payloads that overload the receiver resources without\nbreaching any protocol rules such as IEEE 1609. Using a real-world connected\nvehicle 11 testbed with commercially available On-Board Units (OBUs), we\ndemonstrate that high-rate UDP flooding and oversized payload of BSM flooding\ncan severely degrade FCW performance. Results show that UDP flooding alone\nreduces packet delivery ratio by up to 87% and increases latency to over 400ms,\nwhile oversized BSM floods overload receiver processing resources, delaying or\ncompletely suppressing FCW alerts. When UDP and BSM attacks are executed\nsimultaneously, they cause near-total communication failure, preventing FCW\nwarnings entirely. These findings reveal that protocol-compliant communications\ndo not necessarily guarantee safe or reliable operation of C-V2X-based safety\napplications."
    },
    {
        "date": "2025-08",
        "title": "CSI Obfuscation: Single-Antenna Transmitters Can Not Hide from Adversarial Multi-Antenna Radio Localization Systems",
        "author": "Phillip Stephan, Florian Euchner, and Stephan ten Brink",
        "link": "http://arxiv.org/abs/2508.02553v1",
        "abstract": "The ability of modern telecommunication systems to locate users and objects\nin the radio environment raises justified privacy concerns. To prevent\nunauthorized localization, single-antenna transmitters can obfuscate the signal\nby convolving it with a randomized sequence prior to transmission, which alters\nthe channel state information (CSI) estimated at the receiver. However, this\nstrategy is only effective against CSI-based localization systems deploying\nsingle-antenna receivers. Inspired by the concept of blind multichannel\nidentification, we propose a simple CSI recovery method for multi-antenna\nreceivers to extract channel features that ensure reliable user localization\nregardless of the transmitted signal. We comparatively evaluate the impact of\nsignal obfuscation and the proposed recovery method on the localization\nperformance of CSI fingerprinting, channel charting, and classical\ntriangulation using real-world channel measurements. This work aims to\ndemonstrate the necessity for further efforts to protect the location privacy\nof users from adversarial radio-based localization systems."
    },
    {
        "date": "2025-08",
        "title": "Multi-class Image Anomaly Detection for Practical Applications: Requirements and Robust Solutions",
        "author": "Jaehyuk Heo, and Pilsung Kang",
        "link": "http://arxiv.org/abs/2508.02477v1",
        "abstract": "Recent advances in image anomaly detection have extended unsupervised\nlearning-based models from single-class settings to multi-class frameworks,\naiming to improve efficiency in training time and model storage. When a single\nmodel is trained to handle multiple classes, it often underperforms compared to\nclass-specific models in terms of per-class detection accuracy. Accordingly,\nprevious studies have primarily focused on narrowing this performance gap.\nHowever, the way class information is used, or not used, remains a relatively\nunderstudied factor that could influence how detection thresholds are defined\nin multi-class image anomaly detection. These thresholds, whether\nclass-specific or class-agnostic, significantly affect detection outcomes. In\nthis study, we identify and formalize the requirements that a multi-class image\nanomaly detection model must satisfy under different conditions, depending on\nwhether class labels are available during training and evaluation. We then\nre-examine existing methods under these criteria. To meet these challenges, we\npropose Hierarchical Coreset (HierCore), a novel framework designed to satisfy\nall defined requirements. HierCore operates effectively even without class\nlabels, leveraging a hierarchical memory bank to estimate class-wise decision\ncriteria for anomaly detection. We empirically validate the applicability and\nrobustness of existing methods and HierCore under four distinct scenarios,\ndetermined by the presence or absence of class labels in the training and\nevaluation phases. The experimental results demonstrate that HierCore\nconsistently meets all requirements and maintains strong, stable performance\nacross all settings, highlighting its practical potential for real-world\nmulti-class anomaly detection tasks."
    },
    {
        "date": "2025-08",
        "title": "Correspondence-Free Fast and Robust Spherical Point Pattern Registration",
        "author": "Anik Sarker, and Alan T. Asbeck",
        "link": "http://arxiv.org/abs/2508.02339v1",
        "abstract": "Existing methods for rotation estimation between two spherical\n($\\mathbb{S}^2$) patterns typically rely on spherical cross-correlation\nmaximization between two spherical function. However, these approaches exhibit\ncomputational complexities greater than cubic $O(n^3)$ with respect to rotation\nspace discretization and lack extensive evaluation under significant outlier\ncontamination. To this end, we propose a rotation estimation algorithm between\ntwo spherical patterns with linear time complexity $O(n)$. Unlike existing\nspherical-function-based methods, we explicitly represent spherical patterns as\ndiscrete 3D point sets on the unit sphere, reformulating rotation estimation as\na spherical point-set alignment (i.e., Wahba problem for 3D unit vectors).\nGiven the geometric nature of our formulation, our spherical pattern alignment\nalgorithm naturally aligns with the Wahba problem framework for 3D unit\nvectors. Specifically, we introduce three novel algorithms: (1) SPMC (Spherical\nPattern Matching by Correlation), (2) FRS (Fast Rotation Search), and (3) a\nhybrid approach (SPMC+FRS) that combines the advantages of the previous two\nmethods. Our experiments demonstrate that in the $\\mathbb{S}^2$ domain and in\ncorrespondence-free settings, our algorithms are over 10x faster and over 10x\nmore accurate than current state-of-the-art methods for the Wahba problem with\noutliers. We validate our approach through extensive simulations on a new\ndataset of spherical patterns, the ``Robust Vector Alignment Dataset.\n\"Furthermore, we adapt our methods to two real-world tasks: (i) Point Cloud\nRegistration (PCR) and (ii) rotation estimation for spherical images."
    },
    {
        "date": "2025-08",
        "title": "A Survey on Data Security in Large Language Models",
        "author": "Kang Chen, Xiuze Zhou, Yuanguo Lin, Jinhe Su, Yuanhui Yu, Li Shen, and Fan Lin",
        "link": "http://arxiv.org/abs/2508.02312v1",
        "abstract": "Large Language Models (LLMs), now a foundation in advancing natural language\nprocessing, power applications such as text generation, machine translation,\nand conversational systems. Despite their transformative potential, these\nmodels inherently rely on massive amounts of training data, often collected\nfrom diverse and uncurated sources, which exposes them to serious data security\nrisks. Harmful or malicious data can compromise model behavior, leading to\nissues such as toxic output, hallucinations, and vulnerabilities to threats\nsuch as prompt injection or data poisoning. As LLMs continue to be integrated\ninto critical real-world systems, understanding and addressing these\ndata-centric security risks is imperative to safeguard user trust and system\nreliability. This survey offers a comprehensive overview of the main data\nsecurity risks facing LLMs and reviews current defense strategies, including\nadversarial training, RLHF, and data augmentation. Additionally, we categorize\nand analyze relevant datasets used for assessing robustness and security across\ndifferent domains, providing guidance for future research. Finally, we\nhighlight key research directions that focus on secure model updates,\nexplainability-driven defenses, and effective governance frameworks, aiming to\npromote the safe and responsible development of LLM technology. This work aims\nto inform researchers, practitioners, and policymakers, driving progress toward\ndata security in LLMs."
    },
    {
        "date": "2025-08",
        "title": "Pigeon-SL: Robust Split Learning Framework for Edge Intelligence under Malicious Clients",
        "author": "Sangjun Park, Tony Q. S. Quek, and Hyowoon Seo",
        "link": "http://arxiv.org/abs/2508.02235v1",
        "abstract": "Recent advances in split learning (SL) have established it as a promising\nframework for privacy-preserving, communication-efficient distributed learning\nat the network edge. However, SL's sequential update process is vulnerable to\neven a single malicious client, which can significantly degrade model accuracy.\nTo address this, we introduce Pigeon-SL, a novel scheme grounded in the\npigeonhole principle that guarantees at least one entirely honest cluster among\nM clients, even when up to N of them are adversarial. In each global round, the\naccess point partitions the clients into N+1 clusters, trains each cluster\nindependently via vanilla SL, and evaluates their validation losses on a shared\ndataset. Only the cluster with the lowest loss advances, thereby isolating and\ndiscarding malicious updates. We further enhance training and communication\nefficiency with Pigeon-SL+, which repeats training on the selected cluster to\nmatch the update throughput of standard SL. We validate the robustness and\neffectiveness of our approach under three representative attack models -- label\nflipping, activation and gradient manipulation -- demonstrating significant\nimprovements in accuracy and resilience over baseline SL methods in future\nintelligent wireless networks."
    },
    {
        "date": "2025-08",
        "title": "Failure Cases Are Better Learned But Boundary Says Sorry: Facilitating Smooth Perception Change for Accuracy-Robustness Trade-Off in Adversarial Training",
        "author": "Yanyun Wang, and Li Liu",
        "link": "http://arxiv.org/abs/2508.02186v1",
        "abstract": "Adversarial Training (AT) is one of the most effective methods to train\nrobust Deep Neural Networks (DNNs). However, AT creates an inherent trade-off\nbetween clean accuracy and adversarial robustness, which is commonly attributed\nto the more complicated decision boundary caused by the insufficient learning\nof hard adversarial samples. In this work, we reveal a counterintuitive fact\nfor the first time: From the perspective of perception consistency, hard\nadversarial samples that can still attack the robust model after AT are already\nlearned better than those successfully defended. Thus, different from previous\nviews, we argue that it is rather the over-sufficient learning of hard\nadversarial samples that degrades the decision boundary and contributes to the\ntrade-off problem. Specifically, the excessive pursuit of perception\nconsistency would force the model to view the perturbations as noise and ignore\nthe information within them, which should have been utilized to induce a\nsmoother perception transition towards the decision boundary to support its\nestablishment to an appropriate location. In response, we define a new AT\nobjective named Robust Perception, encouraging the model perception to change\nsmoothly with input perturbations, based on which we propose a novel Robust\nPerception Adversarial Training (RPAT) method, effectively mitigating the\ncurrent accuracy-robustness trade-off. Experiments on CIFAR-10, CIFAR-100, and\nTiny-ImageNet with ResNet-18, PreActResNet-18, and WideResNet-34-10 demonstrate\nthe effectiveness of our method beyond four common baselines and 12\nstate-of-the-art (SOTA) works. The code is available at\nhttps://github.com/FlaAI/RPAT."
    },
    {
        "date": "2025-08",
        "title": "Robust Detection of Planted Subgraphs in Semi-Random Models",
        "author": "Dor Elimelech, and Wasim Huleihel",
        "link": "http://arxiv.org/abs/2508.02158v1",
        "abstract": "Detection of planted subgraphs in Erd\\\"os-R\\'enyi random graphs has been\nextensively studied, leading to a rich body of results characterizing both\nstatistical and computational thresholds. However, most prior work assumes a\npurely random generative model, making the resulting algorithms potentially\nfragile in the face of real-world perturbations. In this work, we initiate the\nstudy of semi-random models for the planted subgraph detection problem, wherein\nan adversary is allowed to remove edges outside the planted subgraph before the\ngraph is revealed to the statistician. Crucially, the statistician remains\nunaware of which edges have been removed, introducing fundamental challenges to\nthe inference task. We establish fundamental statistical limits for detection\nunder this semi-random model, revealing a sharp dichotomy. Specifically, for\nplanted subgraphs with strongly sub-logarithmic maximum density detection\nbecomes information-theoretically impossible in the presence of an adversary,\ndespite being possible in the classical random model. In stark contrast, for\nsubgraphs with super-logarithmic density, the statistical limits remain\nessentially unchanged; we prove that the optimal (albeit computationally\nintractable) likelihood ratio test remains robust. Beyond these statistical\nboundaries, we design a new computationally efficient and robust detection\nalgorithm, and provide rigorous statistical guarantees for its performance. Our\nresults establish the first robust framework for planted subgraph detection and\nopen new directions in the study of semi-random models,\ncomputational-statistical trade-offs, and robustness in graph inference\nproblems."
    },
    {
        "date": "2025-08",
        "title": "Large-Scale Model Enabled Semantic Communication Based on Robust Knowledge Distillation",
        "author": "Kuiyuan DIng, Caili Guo, Yang Yang, Zhongtian Du, and Walid Saad",
        "link": "http://arxiv.org/abs/2508.02148v1",
        "abstract": "Large-scale models (LSMs) can be an effective framework for semantic\nrepresentation and understanding, thereby providing a suitable tool for\ndesigning semantic communication (SC) systems. However, their direct deployment\nis often hindered by high computational complexity and resource requirements.\nIn this paper, a novel robust knowledge distillation based semantic\ncommunication (RKD-SC) framework is proposed to enable efficient and\n\\textcolor{black}{channel-noise-robust} LSM-powered SC. The framework addresses\ntwo key challenges: determining optimal compact model architectures and\neffectively transferring knowledge while maintaining robustness against channel\nnoise. First, a knowledge distillation-based lightweight differentiable\narchitecture search (KDL-DARTS) algorithm is proposed. This algorithm\nintegrates knowledge distillation loss and a complexity penalty into the neural\narchitecture search process to identify high-performance, lightweight semantic\nencoder architectures. Second, a novel two-stage robust knowledge distillation\n(RKD) algorithm is developed to transfer semantic capabilities from an LSM\n(teacher) to a compact encoder (student) and subsequently enhance system\nrobustness. To further improve resilience to channel impairments, a\nchannel-aware transformer (CAT) block is introduced as the channel codec,\ntrained under diverse channel conditions with variable-length outputs.\nExtensive simulations on image classification tasks demonstrate that the RKD-SC\nframework significantly reduces model parameters while preserving a high degree\nof the teacher model's performance and exhibiting superior robustness compared\nto existing methods."
    },
    {
        "date": "2025-08",
        "title": "The Dark Side of Upgrades: Uncovering Security Risks in Smart Contract Upgrades",
        "author": "Dingding Wang, Jianting He, Siwei Wu, Yajin Zhou, Lei Wu, and Cong Wang",
        "link": "http://arxiv.org/abs/2508.02145v1",
        "abstract": "Smart contract upgrades are increasingly common due to their flexibility in\nmodifying deployed contracts, such as fixing bugs or adding new\nfunctionalities. Meanwhile, upgrades compromise the immutability of contracts,\nintroducing significant security concerns. While existing research has explored\nthe security impacts of contract upgrades, these studies are limited in\ncollection of upgrade behaviors and identification of insecurities.\n  To address these limitations, we conduct a comprehensive study on the\ninsecurities of upgrade behaviors. First, we build a dataset containing 83,085\nupgraded contracts and 20,902 upgrade chains. To our knowledge, this is the\nfirst large-scale dataset about upgrade behaviors, revealing their diversity\nand exposing gaps in public disclosure. Next, we develop a taxonomy of\ninsecurities based on 37 real-world security incidents, categorizing eight\ntypes of upgrade risks and providing the first complete view of upgrade-related\ninsecurities. Finally, we survey public awareness of these risks and existing\nmitigations. Our findings show that four types of security risks are overlooked\nby the public and lack mitigation measures. We detect these upgrade risks\nthrough a preliminary study, identifying 31,407 related issues - a finding that\nraises significant concerns."
    },
    {
        "date": "2025-08",
        "title": "TrackletGait: A Robust Framework for Gait Recognition in the Wild",
        "author": "Shaoxiong Zhang, Jinkai Zheng, Shangdong Zhu, and Chenggang Yan",
        "link": "http://arxiv.org/abs/2508.02143v1",
        "abstract": "Gait recognition aims to identify individuals based on their body shape and\nwalking patterns. Though much progress has been achieved driven by deep\nlearning, gait recognition in real-world surveillance scenarios remains quite\nchallenging to current methods. Conventional approaches, which rely on periodic\ngait cycles and controlled environments, struggle with the non-periodic and\noccluded silhouette sequences encountered in the wild. In this paper, we\npropose a novel framework, TrackletGait, designed to address these challenges\nin the wild. We propose Random Tracklet Sampling, a generalization of existing\nsampling methods, which strikes a balance between robustness and representation\nin capturing diverse walking patterns. Next, we introduce Haar Wavelet-based\nDownsampling to preserve information during spatial downsampling. Finally, we\npresent a Hardness Exclusion Triplet Loss, designed to exclude low-quality\nsilhouettes by discarding hard triplet samples. TrackletGait achieves\nstate-of-the-art results, with 77.8 and 80.4 rank-1 accuracy on the Gait3D and\nGREW datasets, respectively, while using only 10.3M backbone parameters.\nExtensive experiments are also conducted to further investigate the factors\naffecting gait recognition in the wild."
    },
    {
        "date": "2025-08",
        "title": "SUAD: Solid-Channel Ultrasound Injection Attack and Defense to Voice Assistants",
        "author": "Chao Liu, Zhezheng Zhu, Hao Chen, Zhe Chen, Kaiwen Guo, Penghao Wang, and Jun Luo",
        "link": "http://arxiv.org/abs/2508.02116v1",
        "abstract": "As a versatile AI application, voice assistants (VAs) have become\nincreasingly popular, but are vulnerable to security threats. Attackers have\nproposed various inaudible attacks, but are limited by cost, distance, or LoS.\nTherefore, we propose \\name~Attack, a long-range, cross-barrier, and\ninterference-free inaudible voice attack via solid channels. We begin by\nthoroughly analyzing the dispersion effect in solid channels, revealing its\nunique impact on signal propagation. To avoid distortions in voice commands, we\ndesign a modular command generation model that parameterizes attack distance,\nvictim audio, and medium dispersion features to adapt to variations in the\nsolid-channel state. Additionally, we propose SUAD Defense, a universal defense\nthat uses ultrasonic perturbation signals to block inaudible voice attacks\n(IVAs) without impacting normal speech. Since the attack can occur at arbitrary\nfrequencies and times, we propose a training method that randomizes both time\nand frequency to generate perturbation signals that break ultrasonic commands.\nNotably, the perturbation signal is modulated to an inaudible frequency without\naffecting the functionality of voice commands for VAs. Experiments on six\nsmartphones have shown that SUAD Attack achieves activation success rates above\n89.8% and SUAD Defense blocks IVAs with success rates exceeding 98%."
    },
    {
        "date": "2025-08",
        "title": "Coward: Toward Practical Proactive Federated Backdoor Defense via Collision-based Watermark",
        "author": "Wenjie Li, Siying Gu, Yiming Li, Kangjie Chen, Zhili Chen, Tianwei Zhang, Shu-Tao Xia, and Dacheng Tao",
        "link": "http://arxiv.org/abs/2508.02115v1",
        "abstract": "Backdoor detection is currently the mainstream defense against backdoor\nattacks in federated learning (FL), where malicious clients upload poisoned\nupdates that compromise the global model and undermine the reliability of FL\ndeployments. Existing backdoor detection techniques fall into two categories,\nincluding passive and proactive ones, depending on whether the server\nproactively modifies the global model. However, both have inherent limitations\nin practice: passive defenses are vulnerable to common non-i.i.d. data\ndistributions and random participation of FL clients, whereas current proactive\ndefenses suffer inevitable out-of-distribution (OOD) bias because they rely on\nbackdoor co-existence effects. To address these issues, we introduce a new\nproactive defense, dubbed Coward, inspired by our discovery of multi-backdoor\ncollision effects, in which consecutively planted, distinct backdoors\nsignificantly suppress earlier ones. In general, we detect attackers by\nevaluating whether the server-injected, conflicting global watermark is erased\nduring local training rather than retained. Our method preserves the advantages\nof proactive defenses in handling data heterogeneity (\\ie, non-i.i.d. data)\nwhile mitigating the adverse impact of OOD bias through a revised detection\nmechanism. Extensive experiments on benchmark datasets confirm the\neffectiveness of Coward and its resilience to potential adaptive attacks. The\ncode for our method would be available at\nhttps://github.com/still2009/cowardFL."
    },
    {
        "date": "2025-08",
        "title": "Attractive Metadata Attack: Inducing LLM Agents to Invoke Malicious Tools",
        "author": "Kanghua Mo, Li Hu, Yucheng Long, and Zhihao Li",
        "link": "http://arxiv.org/abs/2508.02110v1",
        "abstract": "Large language model (LLM) agents have demonstrated remarkable capabilities\nin complex reasoning and decision-making by leveraging external tools. However,\nthis tool-centric paradigm introduces a previously underexplored attack\nsurface: adversaries can manipulate tool metadata -- such as names,\ndescriptions, and parameter schemas -- to influence agent behavior. We identify\nthis as a new and stealthy threat surface that allows malicious tools to be\npreferentially selected by LLM agents, without requiring prompt injection or\naccess to model internals. To demonstrate and exploit this vulnerability, we\npropose the Attractive Metadata Attack (AMA), a black-box in-context learning\nframework that generates highly attractive but syntactically and semantically\nvalid tool metadata through iterative optimization. Our attack integrates\nseamlessly into standard tool ecosystems and requires no modification to the\nagent's execution framework. Extensive experiments across ten realistic,\nsimulated tool-use scenarios and a range of popular LLM agents demonstrate\nconsistently high attack success rates (81\\%-95\\%) and significant privacy\nleakage, with negligible impact on primary task execution. Moreover, the attack\nremains effective even under prompt-level defenses and structured\ntool-selection protocols such as the Model Context Protocol, revealing systemic\nvulnerabilities in current agent architectures. These findings reveal that\nmetadata manipulation constitutes a potent and stealthy attack surface,\nhighlighting the need for execution-level security mechanisms that go beyond\nprompt-level defenses."
    },
    {
        "date": "2025-08",
        "title": "FPEdit: Robust LLM Fingerprinting through Localized Knowledge Editing",
        "author": "Shida Wang, Chaohu Liu, Yubo Wang, and Linli Xu",
        "link": "http://arxiv.org/abs/2508.02092v1",
        "abstract": "Large language models represent significant investments in computation, data,\nand engineering expertise, making them extraordinarily valuable intellectual\nassets. Nevertheless, these AI assets remain vulnerable to unauthorized\nredistribution and commercial exploitation through fine-tuning or black-box\ndeployment. Current fingerprinting approaches face a fundamental trade-off:\nintrinsic methods require full parameter access, while backdoor-based\ntechniques employ statistically anomalous triggers easily detected and filtered\nby adversaries. To address these limitations, we introduce FPEdit, a novel\nknowledge-editing framework that injects semantically coherent natural language\nfingerprints by modifying a sparse subset of model weights. This ensures\nstealthy and precise ownership encoding without degrading the core\nfunctionality. Extensive experiments show that FPEdit achieves $95$-$100\\%$\nfingerprint retention under both full-parameter fine-tuning and\nparameter-efficient adaptation, while preserving performance on 24 downstream\nbenchmarks. Moreover, FPEdit remains robust under quantization, pruning, and\nstochastic decoding, and can embed 10 fingerprint pairs into LLaMA2-7B in under\n10 minutes using less than 32 GB of GPU memory, a $70\\%$ reduction in resource\nrequirements compared to existing techniques. These advances establish FPEdit\nas the first fingerprinting approach to simultaneously achieve robustness\nagainst adaptation, resistance to detection, and preservation of model utility,\nproviding a minimally invasive solution for reliable provenance verification of\nlarge language models in adversarial deployment scenarios."
    },
    {
        "date": "2025-08",
        "title": "GPU in the Blind Spot: Overlooked Security Risks in Transportation",
        "author": "Sefatun-Noor Puspa, and Mashrur Chowdhury",
        "link": "http://arxiv.org/abs/2508.01995v1",
        "abstract": "Graphics processing units (GPUs) are becoming an essential part of the\nintelligent transportation system (ITS) for enabling video-based and artificial\nintelligence (AI) based applications. GPUs provide high-throughput and\nenergy-efficient computing for tasks like sensor fusion and roadside video\nanalytics. However, these GPUs are one of the most unmonitored components in\nterms of security. This makes them vulnerable to cyber and hardware attacks,\nincluding unauthorized crypto mining. This paper highlights GPU security as a\ncritical blind spot in transportation cybersecurity. To support this concern,\nit also presents a case study showing the impact of stealthy unauthorized\ncrypto miners on critical AI workloads, along with a detection strategy. We\nused a YOLOv8-based video processing pipeline running on an RTX 2060 GPU for\nthe case study. A multi-streaming application was executed while a T-Rex crypto\nminer ran in the background. We monitored how the miner degraded GPU\nperformance by reducing the frame rate and increasing power consumption, which\ncould be a serious concern for GPUs operating in autonomous vehicles or\nbattery-powered edge devices. We observed measurable impacts using GPU\ntelemetry (nvidia-smi) and Nsight Compute profiling, where frame rate dropped\nby 50 percent, and power usage increased by up to 90%. To detect, we trained\nlightweight classifiers using extracted telemetry features. All models achieved\nhigh accuracy, precision, recall, and F1-score. This paper raises urgent\nawareness about GPU observability gaps in ITS and offers a replicable framework\nfor detecting GPU misuse through on-device telemetry."
    },
    {
        "date": "2025-08",
        "title": "Controllable and Stealthy Shilling Attacks via Dispersive Latent Diffusion",
        "author": "Shutong Qiao, Wei Yuan, Junliang Yu, Tong Chen, Quoc Viet Hung Nguyen, and Hongzhi Yin",
        "link": "http://arxiv.org/abs/2508.01987v1",
        "abstract": "Recommender systems (RSs) are now fundamental to various online platforms,\nbut their dependence on user-contributed data leaves them vulnerable to\nshilling attacks that can manipulate item rankings by injecting fake users.\nAlthough widely studied, most existing attack models fail to meet two critical\nobjectives simultaneously: achieving strong adversarial promotion of target\nitems while maintaining realistic behavior to evade detection. As a result, the\ntrue severity of shilling threats that manage to reconcile the two objectives\nremains underappreciated. To expose this overlooked vulnerability, we present\nDLDA, a diffusion-based attack framework that can generate highly effective yet\nindistinguishable fake users by enabling fine-grained control over target\npromotion. Specifically, DLDA operates in a pre-aligned collaborative embedding\nspace, where it employs a conditional latent diffusion process to iteratively\nsynthesize fake user profiles with precise target item control. To evade\ndetection, DLDA introduces a dispersive regularization mechanism that promotes\nvariability and realism in generated behavioral patterns. Extensive experiments\non three real-world datasets and five popular RS models demonstrate that,\ncompared to prior attacks, DLDA consistently achieves stronger item promotion\nwhile remaining harder to detect. These results highlight that modern RSs are\nmore vulnerable than previously recognized, underscoring the urgent need for\nmore robust defenses."
    },
    {
        "date": "2025-08",
        "title": "Generative AI-Empowered Secure Communications in Space-Air-Ground Integrated Networks: A Survey and Tutorial",
        "author": "Chenbo Hu, Ruichen Zhang, Bo Li, Xu Jiang, Nan Zhao, Marco Di Renzo, Dusit Niyato, Arumugam Nallanathan, and George K. Karagiannidis",
        "link": "http://arxiv.org/abs/2508.01983v1",
        "abstract": "Space-air-ground integrated networks (SAGINs) face unprecedented security\nchallenges due to their inherent characteristics, such as multidimensional\nheterogeneity and dynamic topologies. These characteristics fundamentally\nundermine conventional security methods and traditional artificial intelligence\n(AI)-driven solutions. Generative AI (GAI) is a transformative approach that\ncan safeguard SAGIN security by synthesizing data, understanding semantics, and\nmaking autonomous decisions. This survey fills existing review gaps by\nexamining GAI-empowered secure communications across SAGINs. First, we\nintroduce secured SAGINs and highlight GAI's advantages over traditional AI for\nsecurity defenses. Then, we explain how GAI mitigates failures of authenticity,\nbreaches of confidentiality, tampering of integrity, and disruptions of\navailability across the physical, data link, and network layers of SAGINs.\nThree step-by-step tutorials discuss how to apply GAI to solve specific\nproblems using concrete methods, emphasizing its generative paradigm beyond\ntraditional AI. Finally, we outline open issues and future research directions,\nincluding lightweight deployment, adversarial robustness, and cross-domain\ngovernance, to provide major insights into GAI's role in shaping\nnext-generation SAGIN security."
    },
    {
        "date": "2025-08",
        "title": "Proactive Disentangled Modeling of Trigger-Object Pairings for Backdoor Defense",
        "author": "Kyle Stein, Andrew A. Mahyari, Guillermo Francia III, and Eman El-Sheikh",
        "link": "http://arxiv.org/abs/2508.01932v1",
        "abstract": "Deep neural networks (DNNs) and generative AI (GenAI) are increasingly\nvulnerable to backdoor attacks, where adversaries embed triggers into inputs to\ncause models to misclassify or misinterpret target labels. Beyond traditional\nsingle-trigger scenarios, attackers may inject multiple triggers across various\nobject classes, forming unseen backdoor-object configurations that evade\nstandard detection pipelines. In this paper, we introduce DBOM (Disentangled\nBackdoor-Object Modeling), a proactive framework that leverages structured\ndisentanglement to identify and neutralize both seen and unseen backdoor\nthreats at the dataset level. Specifically, DBOM factorizes input image\nrepresentations by modeling triggers and objects as independent primitives in\nthe embedding space through the use of Vision-Language Models (VLMs). By\nleveraging the frozen, pre-trained encoders of VLMs, our approach decomposes\nthe latent representations into distinct components through a learnable visual\nprompt repository and prompt prefix tuning, ensuring that the relationships\nbetween triggers and objects are explicitly captured. To separate trigger and\nobject representations in the visual prompt repository, we introduce the\ntrigger-object separation and diversity losses that aids in disentangling\ntrigger and object visual features. Next, by aligning image features with\nfeature decomposition and fusion, as well as learned contextual prompt tokens\nin a shared multimodal space, DBOM enables zero-shot generalization to novel\ntrigger-object pairings that were unseen during training, thereby offering\ndeeper insights into adversarial attack patterns. Experimental results on\nCIFAR-10 and GTSRB demonstrate that DBOM robustly detects poisoned images prior\nto downstream training, significantly enhancing the security of DNN training\npipelines."
    },
    {
        "date": "2025-08",
        "title": "From Binary to Continuous: Stochastic Re-Weighting for Robust Graph Explanation",
        "author": "Zhuomin Chen, Jingchao Ni, Hojat Allah Salehi, Xu Zheng, and Dongsheng Luo",
        "link": "http://arxiv.org/abs/2508.01925v1",
        "abstract": "Graph Neural Networks (GNNs) have achieved remarkable performance in a wide\nrange of graph-related learning tasks. However, explaining their predictions\nremains a challenging problem, especially due to the mismatch between the\ngraphs used during training and those encountered during explanation. Most\nexisting methods optimize soft edge masks on weighted graphs to highlight\nimportant substructures, but these graphs differ from the unweighted graphs on\nwhich GNNs are trained. This distributional shift leads to unreliable gradients\nand degraded explanation quality, especially when generating small, sparse\nsubgraphs. To address this issue, we propose a novel iterative explanation\nframework which improves explanation robustness by aligning the model's\ntraining data distribution with the weighted graph distribution appeared during\nexplanation. Our method alternates between two phases: explanation subgraph\nidentification and model adaptation. It begins with a relatively large\nexplanation subgraph where soft mask optimization is reliable. Based on this\nsubgraph, we assign importance-aware edge weights to explanatory and\nnon-explanatory edges, and retrain the GNN on these weighted graphs. This\nprocess is repeated with progressively smaller subgraphs, forming an iterative\nrefinement procedure. We evaluate our method on multiple benchmark datasets\nusing different GNN backbones and explanation methods. Experimental results\nshow that our method consistently improves explanation quality and can be\nflexibly integrated with different architectures."
    },
    {
        "date": "2025-08",
        "title": "Complete Evasion, Zero Modification: PDF Attacks on AI Text Detection",
        "author": "Aldan Creo",
        "link": "http://arxiv.org/abs/2508.01887v1",
        "abstract": "AI-generated text detectors have become essential tools for maintaining\ncontent authenticity, yet their robustness against evasion attacks remains\nquestionable. We present PDFuzz, a novel attack that exploits the discrepancy\nbetween visual text layout and extraction order in PDF documents. Our method\npreserves exact textual content while manipulating character positioning to\nscramble extraction sequences. We evaluate this approach against the ArguGPT\ndetector using a dataset of human and AI-generated text. Our results\ndemonstrate complete evasion: detector performance drops from (93.6 $\\pm$ 1.4)\n% accuracy and 0.938 $\\pm$ 0.014 F1 score to random-level performance ((50.4\n$\\pm$ 3.2) % accuracy, 0.0 F1 score) while maintaining perfect visual fidelity.\nOur work reveals a vulnerability in current detection systems that is inherent\nto PDF document structures and underscores the need for implementing sturdy\nsafeguards against such attacks. We make our code publicly available at\nhttps://github.com/ACMCMC/PDFuzz."
    },
    {
        "date": "2025-08",
        "title": "Beyond Vulnerabilities: A Survey of Adversarial Attacks as Both Threats and Defenses in Computer Vision Systems",
        "author": "Zhongliang Guo, Yifei Qian, Yanli Li, Weiye Li, Chun Tong Lei, Shuai Zhao, Lei Fang, Ognjen Arandjelovi\u0107, and Chun Pong Lau",
        "link": "http://arxiv.org/abs/2508.01845v1",
        "abstract": "Adversarial attacks against computer vision systems have emerged as a\ncritical research area that challenges the fundamental assumptions about neural\nnetwork robustness and security. This comprehensive survey examines the\nevolving landscape of adversarial techniques, revealing their dual nature as\nboth sophisticated security threats and valuable defensive tools. We provide a\nsystematic analysis of adversarial attack methodologies across three primary\ndomains: pixel-space attacks, physically realizable attacks, and latent-space\nattacks. Our investigation traces the technical evolution from early\ngradient-based methods such as FGSM and PGD to sophisticated optimization\ntechniques incorporating momentum, adaptive step sizes, and advanced\ntransferability mechanisms. We examine how physically realizable attacks have\nsuccessfully bridged the gap between digital vulnerabilities and real-world\nthreats through adversarial patches, 3D textures, and dynamic optical\nperturbations. Additionally, we explore the emergence of latent-space attacks\nthat leverage semantic structure in internal representations to create more\ntransferable and meaningful adversarial examples. Beyond traditional offensive\napplications, we investigate the constructive use of adversarial techniques for\nvulnerability assessment in biometric authentication systems and protection\nagainst malicious generative models. Our analysis reveals critical research\ngaps, particularly in neural style transfer protection and computational\nefficiency requirements. This survey contributes a comprehensive taxonomy,\nevolution analysis, and identification of future research directions, aiming to\nadvance understanding of adversarial vulnerabilities and inform the development\nof more robust and trustworthy computer vision systems."
    },
    {
        "date": "2025-08",
        "title": "AGENTICT$^2$S:Robust Text-to-SPARQL via Agentic Collaborative Reasoning over Heterogeneous Knowledge Graphs for the Circular Economy",
        "author": "Yang Zhao, Chengxiao Dai, Wei Zhuo, Tan Chuan Fu, Yue Xiu, Dusit Niyato, Jonathan Z. Low, Eugene Ho Hong Zhuang, and Daren Zong Loong Tan",
        "link": "http://arxiv.org/abs/2508.01815v1",
        "abstract": "Question answering over heterogeneous knowledge graphs (KGQA) involves\nreasoning across diverse schemas, incomplete alignments, and distributed data\nsources. Existing text-to-SPARQL approaches rely on large-scale domain-specific\nfine-tuning or operate within single-graph settings, limiting their\ngeneralizability in low-resource domains and their ability to handle queries\nspanning multiple graphs. These challenges are particularly relevant in domains\nsuch as the circular economy, where information about classifications,\nprocesses, and emissions is distributed across independently curated knowledge\ngraphs (KGs). We present AgenticT$^2$S, a modular framework that decomposes\nKGQA into subtasks managed by specialized agents responsible for retrieval,\nquery generation, and verification. A scheduler assigns subgoals to different\ngraphs using weak-to-strong alignment strategies. A two-stage verifier detects\nstructurally invalid and semantically underspecified queries through symbolic\nvalidation and counterfactual consistency checks. Experiments on real-world\ncircular economy KGs demonstrate that AgenticT$^2$S improves execution accuracy\nby 17.3% and triple level F$_1$ by 25.4% over the best baseline, while reducing\nthe average prompt length by 46.4%. These results demonstrate the benefits of\nagent-based schema-aware reasoning for scalable KGQA and support\ndecision-making in sustainability domains through robust cross-graph reasoning."
    },
    {
        "date": "2025-08",
        "title": "Simulated Ensemble Attack: Transferring Jailbreaks Across Fine-tuned Vision-Language Models",
        "author": "Ruofan Wang, Xin Wang, Yang Yao, Xuan Tong, and Xingjun Ma",
        "link": "http://arxiv.org/abs/2508.01741v1",
        "abstract": "Fine-tuning open-source Vision-Language Models (VLMs) creates a critical yet\nunderexplored attack surface: vulnerabilities in the base VLM could be retained\nin fine-tuned variants, rendering them susceptible to transferable jailbreak\nattacks. To demonstrate this risk, we introduce the Simulated Ensemble Attack\n(SEA), a novel grey-box jailbreak method in which the adversary has full access\nto the base VLM but no knowledge of the fine-tuned target's weights or training\nconfiguration. To improve jailbreak transferability across fine-tuned VLMs, SEA\ncombines two key techniques: Fine-tuning Trajectory Simulation (FTS) and\nTargeted Prompt Guidance (TPG). FTS generates transferable adversarial images\nby simulating the vision encoder's parameter shifts, while TPG is a textual\nstrategy that steers the language decoder toward adversarially optimized\noutputs. Experiments on the Qwen2-VL family (2B and 7B) demonstrate that SEA\nachieves high transfer attack success rates exceeding 86.5% and toxicity rates\nnear 49.5% across diverse fine-tuned variants, even those specifically\nfine-tuned to improve safety behaviors. Notably, while direct PGD-based image\njailbreaks rarely transfer across fine-tuned VLMs, SEA reliably exploits\ninherited vulnerabilities from the base model, significantly enhancing\ntransferability. These findings highlight an urgent need to safeguard\nfine-tuned proprietary VLMs against transferable vulnerabilities inherited from\nopen-source foundations, motivating the development of holistic defenses across\nthe entire model lifecycle."
    },
    {
        "date": "2025-08",
        "title": "Tracking the Unstable: Appearance-Guided Motion Modeling for Robust Multi-Object Tracking in UAV-Captured Videos",
        "author": "Jianbo Ma, Hui Luo, Qi Chen, Yuankai Qi, Yumei Sun, Amin Beheshti, Jianlin Zhang, and Ming-Hsuan Yang",
        "link": "http://arxiv.org/abs/2508.01730v1",
        "abstract": "Multi-object tracking (MOT) aims to track multiple objects while maintaining\nconsistent identities across frames of a given video. In unmanned aerial\nvehicle (UAV) recorded videos, frequent viewpoint changes and complex\nUAV-ground relative motion dynamics pose significant challenges, which often\nlead to unstable affinity measurement and ambiguous association. Existing\nmethods typically model motion and appearance cues separately, overlooking\ntheir spatio-temporal interplay and resulting in suboptimal tracking\nperformance. In this work, we propose AMOT, which jointly exploits appearance\nand motion cues through two key components: an Appearance-Motion Consistency\n(AMC) matrix and a Motion-aware Track Continuation (MTC) module. Specifically,\nthe AMC matrix computes bi-directional spatial consistency under the guidance\nof appearance features, enabling more reliable and context-aware identity\nassociation. The MTC module complements AMC by reactivating unmatched tracks\nthrough appearance-guided predictions that align with Kalman-based predictions,\nthereby reducing broken trajectories caused by missed detections. Extensive\nexperiments on three UAV benchmarks, including VisDrone2019, UAVDT, and\nVT-MOT-UAV, demonstrate that our AMOT outperforms current state-of-the-art\nmethods and generalizes well in a plug-and-play and training-free manner."
    },
    {
        "date": "2025-08",
        "title": "Imbalance-Robust and Sampling-Efficient Continuous Conditional GANs via Adaptive Vicinity and Auxiliary Regularization",
        "author": "Xin Ding, Yun Chen, Yongwei Wang, Kao Zhang, Sen Zhang, Peibei Cao, and Xiangxue Wang",
        "link": "http://arxiv.org/abs/2508.01725v2",
        "abstract": "Recent advances in conditional generative modeling have introduced Continuous\nconditional Generative Adversarial Network (CcGAN) and Continuous Conditional\nDiffusion Model (CCDM) for estimating high-dimensional data distributions\nconditioned on scalar, continuous regression labels (e.g., angles, ages, or\ntemperatures). However, these approaches face fundamental limitations: CcGAN\nsuffers from data imbalance due to fixed-size vicinity constraints, while CCDM\nrequires computationally expensive iterative sampling. We present CcGAN-AVAR,\nan enhanced CcGAN framework that addresses both challenges: (1) leveraging the\nGAN framework's native one-step generation to overcome CCDMs' sampling\nbottleneck (achieving 300x-2000x faster inference), while (2) two novel\ncomponents specifically target data imbalance - an adaptive vicinity mechanism\nthat dynamically adjusts vicinity's size, and a multi-task discriminator that\nconstructs two regularization terms (through auxiliary regression and density\nratio estimation) to significantly improve generator training. Extensive\nexperiments on four benchmark datasets (64x64 to 192x192 resolution) across\neight challenging imbalanced settings demonstrate that CcGAN-AVAR achieves\nstate-of-the-art generation quality while maintaining sampling efficiency."
    },
    {
        "date": "2025-08",
        "title": "A Provably Secure Network Protocol for Private Communication with Analysis and Tracing Resistance",
        "author": "Chao Ge, Wei Yuan, Ge Chen, Yanbin Pan, and Yuan Shen",
        "link": "http://arxiv.org/abs/2508.01714v1",
        "abstract": "Anonymous communication networks have emerged as crucial tools for\nobfuscating communication pathways and concealing user identities. However,\ntheir practical deployments face significant challenges, including\nsusceptibility to artificial intelligence (AI)-powered metadata analysis,\ndifficulties in decentralized architectures, and the absence of provable\nsecurity guarantees. To address these issues, this paper proposes a novel\ndecentralized anonymous routing protocol with resistance to tracing and traffic\nanalysis. The protocol eliminates dependencies on the threshold model and\ntrusted third-party setups, ensuring indistinguishable identity privacy even in\nhighly adversarial environments. Different from traditional empirical security\nanalysis of anonymous networks, this paper rigorously proves indistinguishable\nidentity privacy for users even in extremely adversarial environments.\nFurthermore, simulations confirm its practical feasibility, demonstrating both\nsecurity and efficiency. By achieving information sharing with privacy\npreservation, the proposed protocol offers a provably secure solution for\nprivacy-preserving communication in digital environments."
    },
    {
        "date": "2025-08",
        "title": "Benchmarking Adversarial Patch Selection and Location",
        "author": "Shai Kimhi, Avi Mendlson, and Moshe Kimhi",
        "link": "http://arxiv.org/abs/2508.01676v1",
        "abstract": "Adversarial patch attacks threaten the reliability of modern vision models.\nWe present PatchMap, the first spatially exhaustive benchmark of patch\nplacement, built by evaluating over 1.5e8 forward passes on ImageNet validation\nimages. PatchMap reveals systematic hot-spots where small patches (as little as\n2% of the image) induce confident misclassifications and large drops in model\nconfidence. To demonstrate its utility, we propose a simple segmentation guided\nplacement heuristic that leverages off the shelf masks to identify vulnerable\nregions without any gradient queries. Across five architectures-including\nadversarially trained ResNet50, our method boosts attack success rates by 8 to\n13 percentage points compared to random or fixed placements. We publicly\nrelease PatchMap and the code implementation. The full PatchMap bench (6.5B\npredictions, multiple backbones) will be released soon to further accelerate\nresearch on location-aware defenses and adaptive attacks."
    },
    {
        "date": "2025-08",
        "title": "Semantic Encryption: Secure and Effective Interaction with Cloud-based Large Language Models via Semantic Transformation",
        "author": "Dong Chen, Tong Yang, Feipeng Zhai, Pengpeng Ouyang, Qidong Liu, Yafei Li, Chong Fu, and Mingliang Xu",
        "link": "http://arxiv.org/abs/2508.01638v1",
        "abstract": "The increasing adoption of Cloud-based Large Language Models (CLLMs) has\nraised significant concerns regarding data privacy during user interactions.\nWhile existing approaches primarily focus on encrypting sensitive information,\nthey often overlook the logical structure of user inputs. This oversight can\nlead to reduced data utility and degraded performance of CLLMs. To address\nthese limitations and enable secure yet effective interactions, we propose\nSemantic Encryption (SE)-a plug-and-play framework designed to preserve both\nprivacy and utility. SE consists of two key components: Semantic Encoding and\nSemantic Decoding. In the encoding phase, a lightweight local model transforms\nthe original user input into an alternative semantic context that maintains the\noriginal intent and logical structure while obfuscating sensitive information.\nThis transformed input is then processed by the CLLM, which generates a\nresponse based on the transformed semantic context. To maintain a seamless user\nexperience, the decoding phase will reconstruct the CLLM's response back into\nthe original semantic context by referencing the locally stored user input.\nExtensive experimental evaluations demonstrate that SE effectively protects\ndata privacy without compromising data utility or user experience, offering a\npractical solution for secure interaction with CLLMs. Particularly, the\nproposed SE demonstrates a significant improvement over the state-of-the-art\nInferDPT, surpassing it across various evaluated metrics and datasets."
    },
    {
        "date": "2025-08",
        "title": "Practical, Generalizable and Robust Backdoor Attacks on Text-to-Image Diffusion Models",
        "author": "Haoran Dai, Jiawen Wang, Ruo Yang, Manali Sharma, Zhonghao Liao, Yuan Hong, and Binghui Wang",
        "link": "http://arxiv.org/abs/2508.01605v1",
        "abstract": "Text-to-image diffusion models (T2I DMs) have achieved remarkable success in\ngenerating high-quality and diverse images from text prompts, yet recent\nstudies have revealed their vulnerability to backdoor attacks. Existing attack\nmethods suffer from critical limitations: 1) they rely on unnatural adversarial\nprompts that lack human readability and require massive poisoned data; 2) their\neffectiveness is typically restricted to specific models, lacking\ngeneralizability; and 3) they can be mitigated by recent backdoor defenses.\n  To overcome these challenges, we propose a novel backdoor attack framework\nthat achieves three key properties: 1) \\emph{Practicality}: Our attack requires\nonly a few stealthy backdoor samples to generate arbitrary attacker-chosen\ntarget images, as well as ensuring high-quality image generation in benign\nscenarios. 2) \\emph{Generalizability:} The attack is applicable across multiple\nT2I DMs without requiring model-specific redesign. 3) \\emph{Robustness:} The\nattack remains effective against existing backdoor defenses and adaptive\ndefenses. Our extensive experimental results on multiple T2I DMs demonstrate\nthat with only 10 carefully crafted backdoored samples, our attack method\nachieves $>$90\\% attack success rate with negligible degradation in benign\nimage generation quality. We also conduct human evaluation to validate our\nattack effectiveness. Furthermore, recent backdoor detection and mitigation\nmethods, as well as adaptive defense tailored to our attack are not\nsufficiently effective, highlighting the pressing need for more robust defense\nmechanisms against the proposed attack."
    },
    {
        "date": "2025-08",
        "title": "BeDKD: Backdoor Defense based on Dynamic Knowledge Distillation and Directional Mapping Modulator",
        "author": "Zhengxian Wu, Juan Wen, Wanli Peng, Yinghan Zhou, Changtong dou, and Yiming Xue",
        "link": "http://arxiv.org/abs/2508.01595v1",
        "abstract": "Although existing backdoor defenses have gained success in mitigating\nbackdoor attacks, they still face substantial challenges. In particular, most\nof them rely on large amounts of clean data to weaken the backdoor mapping but\ngenerally struggle with residual trigger effects, resulting in persistently\nhigh attack success rates (ASR). Therefore, in this paper, we propose a novel\nBackdoor defense method based on Directional mapping module and adversarial\nKnowledge Distillation (BeDKD), which balances the trade-off between defense\neffectiveness and model performance using a small amount of clean and poisoned\ndata. We first introduce a directional mapping module to identify poisoned\ndata, which destroys clean mapping while keeping backdoor mapping on a small\nset of flipped clean data. Then, the adversarial knowledge distillation is\ndesigned to reinforce clean mapping and suppress backdoor mapping through a\ncycle iteration mechanism between trust and punish distillations using clean\nand identified poisoned data. We conduct experiments to mitigate mainstream\nattacks on three datasets, and experimental results demonstrate that BeDKD\nsurpasses the state-of-the-art defenses and reduces the ASR by 98% without\nsignificantly reducing the CACC. Our code are available in\nhttps://github.com/CAU-ISS-Lab/Backdoor-Attack-Defense-LLMs/tree/main/BeDKD."
    },
    {
        "date": "2025-08",
        "title": "Are All Prompt Components Value-Neutral? Understanding the Heterogeneous Adversarial Robustness of Dissected Prompt in Large Language Models",
        "author": "Yujia Zheng, Tianhao Li, Haotian Huang, Tianyu Zeng, Jingyu Lu, Chuangxin Chu, Yuekai Huang, Ziyou Jiang, Qian Xiong, Yuyao Ge, and Mingyang Li",
        "link": "http://arxiv.org/abs/2508.01554v1",
        "abstract": "Prompt-based adversarial attacks have become an effective means to assess the\nrobustness of large language models (LLMs). However, existing approaches often\ntreat prompts as monolithic text, overlooking their structural\nheterogeneity-different prompt components contribute unequally to adversarial\nrobustness. Prior works like PromptRobust assume prompts are value-neutral, but\nour analysis reveals that complex, domain-specific prompts with rich structures\nhave components with differing vulnerabilities. To address this gap, we\nintroduce PromptAnatomy, an automated framework that dissects prompts into\nfunctional components and generates diverse, interpretable adversarial examples\nby selectively perturbing each component using our proposed method, ComPerturb.\nTo ensure linguistic plausibility and mitigate distribution shifts, we further\nincorporate a perplexity (PPL)-based filtering mechanism. As a complementary\nresource, we annotate four public instruction-tuning datasets using the\nPromptAnatomy framework, verified through human review. Extensive experiments\nacross these datasets and five advanced LLMs demonstrate that ComPerturb\nachieves state-of-the-art attack success rates. Ablation studies validate the\ncomplementary benefits of prompt dissection and PPL filtering. Our results\nunderscore the importance of prompt structure awareness and controlled\nperturbation for reliable adversarial robustness evaluation in LLMs. Code and\ndata are available at https://github.com/Yujiaaaaa/PACP."
    },
    {
        "date": "2025-08",
        "title": "Leveraging Machine Learning for Botnet Attack Detection in Edge-Computing Assisted IoT Networks",
        "author": "Dulana Rupanetti, and Naima Kaabouch",
        "link": "http://arxiv.org/abs/2508.01542v1",
        "abstract": "The increase of IoT devices, driven by advancements in hardware technologies,\nhas led to widespread deployment in large-scale networks that process massive\namounts of data daily. However, the reliance on Edge Computing to manage these\ndevices has introduced significant security vulnerabilities, as attackers can\ncompromise entire networks by targeting a single IoT device. In light of\nescalating cybersecurity threats, particularly botnet attacks, this paper\ninvestigates the application of machine learning techniques to enhance security\nin Edge-Computing-Assisted IoT environments. Specifically, it presents a\ncomparative analysis of Random Forest, XGBoost, and LightGBM -- three advanced\nensemble learning algorithms -- to address the dynamic and complex nature of\nbotnet threats. Utilizing a widely recognized IoT network traffic dataset\ncomprising benign and malicious instances, the models were trained, tested, and\nevaluated for their accuracy in detecting and classifying botnet activities.\nFurthermore, the study explores the feasibility of deploying these models in\nresource-constrained edge and IoT devices, demonstrating their practical\napplicability in real-world scenarios. The results highlight the potential of\nmachine learning to fortify IoT networks against emerging cybersecurity\nchallenges."
    },
    {
        "date": "2025-08",
        "title": "VWAttacker: A Systematic Security Testing Framework for Voice over WiFi User Equipments",
        "author": "Imtiaz Karim, Hyunwoo Lee, Hassan Asghar, Kazi Samin Mubasshir, Seulgi Han, Mashroor Hasan Bhuiyan, and Elisa Bertino",
        "link": "http://arxiv.org/abs/2508.01469v1",
        "abstract": "We present VWAttacker, the first systematic testing framework for analyzing\nthe security of Voice over WiFi (VoWiFi) User Equipment (UE) implementations.\nVWAttacker includes a complete VoWiFi network testbed that communicates with\nCommercial-Off-The-Shelf (COTS) UEs based on a simple interface to test the\nbehavior of diverse VoWiFi UE implementations; uses property-guided adversarial\ntesting to uncover security issues in different UEs systematically. To reduce\nmanual effort in extracting and testing properties, we introduce an LLM-based,\nsemi-automatic, and scalable approach for property extraction and testcase (TC)\ngeneration. These TCs are systematically mutated by two domain-specific\ntransformations. Furthermore, we introduce two deterministic oracles to detect\nproperty violations automatically. Coupled with these techniques, VWAttacker\nextracts 63 properties from 11 specifications, evaluates 1,116 testcases, and\ndetects 13 issues in 21 UEs. The issues range from enforcing a DH shared secret\nto 0 to supporting weak algorithms. These issues result in attacks that expose\nthe victim UE's identity or establish weak channels, thus severely hampering\nthe security of cellular networks. We responsibly disclose the findings to all\nthe related vendors. At the time of writing, one of the vulnerabilities has\nbeen acknowledged by MediaTek with high severity."
    },
    {
        "date": "2025-08",
        "title": "Capturing More: Learning Multi-Domain Representations for Robust Online Handwriting Verification",
        "author": "Peirong Zhang, Kai Ding, and Lianwen Jin",
        "link": "http://arxiv.org/abs/2508.01427v1",
        "abstract": "In this paper, we propose SPECTRUM, a temporal-frequency synergistic model\nthat unlocks the untapped potential of multi-domain representation learning for\nonline handwriting verification (OHV). SPECTRUM comprises three core\ncomponents: (1) a multi-scale interactor that finely combines temporal and\nfrequency features through dual-modal sequence interaction and multi-scale\naggregation, (2) a self-gated fusion module that dynamically integrates global\ntemporal and frequency features via self-driven balancing. These two components\nwork synergistically to achieve micro-to-macro spectral-temporal integration.\n(3) A multi-domain distance-based verifier then utilizes both temporal and\nfrequency representations to improve discrimination between genuine and forged\nhandwriting, surpassing conventional temporal-only approaches. Extensive\nexperiments demonstrate SPECTRUM's superior performance over existing OHV\nmethods, underscoring the effectiveness of temporal-frequency multi-domain\nlearning. Furthermore, we reveal that incorporating multiple handwritten\nbiometrics fundamentally enhances the discriminative power of handwriting\nrepresentations and facilitates verification. These findings not only validate\nthe efficacy of multi-domain learning in OHV but also pave the way for future\nresearch in multi-domain approaches across both feature and biometric domains.\nCode is publicly available at https://github.com/NiceRingNode/SPECTRUM."
    },
    {
        "date": "2025-08",
        "title": "AI-Driven Cybersecurity Threat Detection: Building Resilient Defense Systems Using Predictive Analytics",
        "author": "Biswajit Chandra Das, M Saif Sartaz, Syed Ali Reza, Arat Hossain, Md Nasiruddin, Kanchon Kumar Bishnu, Kazi Sharmin Sultana, Sadia Sharmeen Shatyi, MD Azam Khan, and Joynal Abed",
        "link": "http://arxiv.org/abs/2508.01422v1",
        "abstract": "This study examines how Artificial Intelligence can aid in identifying and\nmitigating cyber threats in the U.S. across four key areas: intrusion\ndetection, malware classification, phishing detection, and insider threat\nanalysis. Each of these problems has its quirks, meaning there needs to be\ndifferent approaches to each, so we matched the models to the shape of the\nproblem. For intrusion detection, catching things like unauthorized access, we\ntested unsupervised anomaly detection methods. Isolation forests and deep\nautoencoders both gave us useful signals by picking up odd patterns in network\ntraffic. When it came to malware detection, we leaned on ensemble models like\nRandom Forest and XGBoost, trained on features pulled from files and traffic\nlogs. Phishing was more straightforward. We fed standard classifiers (logistic\nregression, Random Forest, XGBoost) a mix of email and web-based features.\nThese models handled the task surprisingly well. Phishing turned out to be the\neasiest problem to crack, at least with the data we had. There was a different\nstory. We utilized an LSTM autoencoder to identify behavioral anomalies in user\nactivity logs. It caught every suspicious behavior but flagged a lot of\nharmless ones too. That kind of model makes sense when the cost of missing a\nthreat is high and you are willing to sift through some noise. What we saw\nacross the board is that performance was not about stacking the most complex\nmodel. What mattered was how well the models structure matched the way the data\nbehaved. When signals were strong and obvious, simple models worked fine. But\nfor messier, more subtle threats, we needed something more adaptive, sequence\nmodels and anomaly detectors, though they brought their trade offs. The\ntakeaway here is clear in cybersecurity, context drives the solution."
    },
    {
        "date": "2025-08",
        "title": "BlockA2A: Towards Secure and Verifiable Agent-to-Agent Interoperability",
        "author": "Zhenhua Zou, Zhuotao Liu, Lepeng Zhao, and Qiuyang Zhan",
        "link": "http://arxiv.org/abs/2508.01332v2",
        "abstract": "The rapid adoption of agentic AI, powered by large language models (LLMs), is\ntransforming enterprise ecosystems with autonomous agents that execute complex\nworkflows. Yet we observe several key security vulnerabilities in LLM-driven\nmulti-agent systems (MASes): fragmented identity frameworks, insecure\ncommunication channels, and inadequate defenses against Byzantine agents or\nadversarial prompts. In this paper, we present the first systematic analysis of\nthese emerging multi-agent risks and explain why the legacy security strategies\ncannot effectively address these risks. Afterwards, we propose BlockA2A, the\nfirst unified multi-agent trust framework that enables secure and verifiable\nand agent-to-agent interoperability. At a high level, BlockA2A adopts\ndecentralized identifiers (DIDs) to enable fine-grained cross-domain agent\nauthentication, blockchain-anchored ledgers to enable immutable auditability,\nand smart contracts to dynamically enforce context-aware access control\npolicies. BlockA2A eliminates centralized trust bottlenecks, ensures message\nauthenticity and execution integrity, and guarantees accountability across\nagent interactions. Furthermore, we propose a Defense Orchestration Engine\n(DOE) that actively neutralizes attacks through real-time mechanisms, including\nByzantine agent flagging, reactive execution halting, and instant permission\nrevocation. Empirical evaluations demonstrate BlockA2A's effectiveness in\nneutralizing prompt-based, communication-based, behavioral and systemic MAS\nattacks. We formalize its integration into existing MAS and showcase a\npractical implementation for Google's A2A protocol. Experiments confirm that\nBlockA2A and DOE operate with sub-second overhead, enabling scalable deployment\nin production LLM-based MAS environments."
    },
    {
        "date": "2025-08",
        "title": "Blockchain security based on cryptography: a review",
        "author": "Wenwen Zhou, Dongyang Lyu, and Xiaoqi Li",
        "link": "http://arxiv.org/abs/2508.01280v1",
        "abstract": "As an emerging service framework built by combining cryptography, P2P\nnetwork, consensus mechanism and innovative contract technology, blockchain has\nbeen widely used in digital finance, data sharing, message traceability and\nelectronic evidence preservation because of its decentralised, non-tamperable\nand transaction traceability. However, with the complex and changeable\napplication scenarios of blockchain technology and the continuous enhancement\nof blockchain attack technology, the security of the blockchain system has been\nseriously threatened, dramatically affecting the development and application of\nblockchain technology. This paper aims to analyse the attacks on blockchain\nfrom the perspective of cryptography. Firstly, from the cryptography technology\nin the blockchain, the principle of hash functions, digital signatures, and\nother technologies, as well as their role in the blockchain, are introduced.\nThen, based on the six-layer architecture of the blockchain, the attacks on the\ndata layer, the network layer, the consensus layer, the contract layer, the\nincentive layer and the application layer are analysed, and the methods to\nmitigate or resist the attacks are proposed. Secondly, the attack principles of\n51% attack, Double-Spending attack, Reentrancy attack, Replay attack, Sybil\nattack and Timestamp Tampering attack were analysed, and the mitigation or\ndefence solutions for these six attacks were designed. Finally, the core\nproblems to be solved in blockchain technology are summarised, and the future\ndevelopment of blockchain security technology is projected."
    },
    {
        "date": "2025-08",
        "title": "Defending Against Beta Poisoning Attacks in Machine Learning Models",
        "author": "Nilufer Gulciftci, and M. Emre Gursoy",
        "link": "http://arxiv.org/abs/2508.01276v1",
        "abstract": "Poisoning attacks, in which an attacker adversarially manipulates the\ntraining dataset of a machine learning (ML) model, pose a significant threat to\nML security. Beta Poisoning is a recently proposed poisoning attack that\ndisrupts model accuracy by making the training dataset linearly nonseparable.\nIn this paper, we propose four defense strategies against Beta Poisoning\nattacks: kNN Proximity-Based Defense (KPB), Neighborhood Class Comparison\n(NCC), Clustering-Based Defense (CBD), and Mean Distance Threshold (MDT). The\ndefenses are based on our observations regarding the characteristics of\npoisoning samples generated by Beta Poisoning, e.g., poisoning samples have\nclose proximity to one another, and they are centered near the mean of the\ntarget class. Experimental evaluations using MNIST and CIFAR-10 datasets\ndemonstrate that KPB and MDT can achieve perfect accuracy and F1 scores, while\nCBD and NCC also provide strong defensive capabilities. Furthermore, by\nanalyzing performance across varying parameters, we offer practical insights\nregarding defenses' behaviors under varying conditions."
    },
    {
        "date": "2025-08",
        "title": "Win-k: Improved Membership Inference Attacks on Small Language Models",
        "author": "Roya Arkhmammadova, Hosein Madadi Tamar, and M. Emre Gursoy",
        "link": "http://arxiv.org/abs/2508.01268v1",
        "abstract": "Small language models (SLMs) are increasingly valued for their efficiency and\ndeployability in resource-constrained environments, making them useful for\non-device, privacy-sensitive, and edge computing applications. On the other\nhand, membership inference attacks (MIAs), which aim to determine whether a\ngiven sample was used in a model's training, are an important threat with\nserious privacy and intellectual property implications. In this paper, we study\nMIAs on SLMs. Although MIAs were shown to be effective on large language models\n(LLMs), they are relatively less studied on emerging SLMs, and furthermore,\ntheir effectiveness decreases as models get smaller. Motivated by this finding,\nwe propose a new MIA called win-k, which builds on top of a state-of-the-art\nattack (min-k). We experimentally evaluate win-k by comparing it with five\nexisting MIAs using three datasets and eight SLMs. Results show that win-k\noutperforms existing MIAs in terms of AUROC, TPR @ 1% FPR, and FPR @ 99% TPR\nmetrics, especially on smaller models."
    },
    {
        "date": "2025-08",
        "title": "Enhancing Diffusion-based Dataset Distillation via Adversary-Guided Curriculum Sampling",
        "author": "Lexiao Zou, Gongwei Chen, Yanda Chen, and Miao Zhang",
        "link": "http://arxiv.org/abs/2508.01264v1",
        "abstract": "Dataset distillation aims to encapsulate the rich information contained in\ndataset into a compact distilled dataset but it faces performance degradation\nas the image-per-class (IPC) setting or image resolution grows larger. Recent\nadvancements demonstrate that integrating diffusion generative models can\neffectively facilitate the compression of large-scale datasets while\nmaintaining efficiency due to their superiority in matching data distribution\nand summarizing representative patterns. However, images sampled from diffusion\nmodels are always blamed for lack of diversity which may lead to information\nredundancy when multiple independent sampled images are aggregated as a\ndistilled dataset. To address this issue, we propose Adversary-guided\nCurriculum Sampling (ACS), which partitions the distilled dataset into multiple\ncurricula. For generating each curriculum, ACS guides diffusion sampling\nprocess by an adversarial loss to challenge a discriminator trained on sampled\nimages, thus mitigating information overlap between curricula and fostering a\nmore diverse distilled dataset. Additionally, as the discriminator evolves with\nthe progression of curricula, ACS generates images from simpler to more\ncomplex, ensuring efficient and systematic coverage of target data\ninformational spectrum. Extensive experiments demonstrate the effectiveness of\nACS, which achieves substantial improvements of 4.1\\% on Imagewoof and 2.1\\% on\nImageNet-1k over the state-of-the-art."
    },
    {
        "date": "2025-08",
        "title": "DisTaC: Conditioning Task Vectors via Distillation for Robust Model Merging",
        "author": "Kotaro Yoshida, Yuji Naraki, Takafumi Horie, Ryotaro Shimizu, and Hiroki Naganuma",
        "link": "http://arxiv.org/abs/2508.01148v1",
        "abstract": "Model merging has emerged as an efficient and flexible paradigm for\nmulti-task learning, with numerous methods being proposed in recent years.\nHowever, these state-of-the-art techniques are typically evaluated on benchmark\nsuites that are highly favorable to model merging, and their robustness in more\nrealistic settings remains largely unexplored. In this work, we first\ninvestigate the vulnerabilities of model-merging methods and pinpoint the\nsource-model characteristics that critically underlie them. Specifically, we\nidentify two factors that are particularly harmful to the merging process: (1)\ndisparities in task vector norms, and (2) the low confidence of the source\nmodels. To address this issue, we propose DisTaC (Distillation for Task vector\nConditioning), a novel method that pre-conditions these problematic task\nvectors before the merge. DisTaC leverages knowledge distillation to adjust a\ntask vector's norm and increase source-model confidence while preserving its\nessential task-specific knowledge. Our extensive experiments demonstrate that\nby pre-conditioning task vectors with DisTaC, state-of-the-art merging\ntechniques can successfully integrate models exhibiting the harmful traits --\nwhere they would otherwise fail -- achieving significant performance gains."
    },
    {
        "date": "2025-08",
        "title": "Beyond Algorithmic Proofs: Towards Implementation-Level Provable Security",
        "author": "Jiahui Shang, Luning Zhang, and Zhongxiang Zheng",
        "link": "http://arxiv.org/abs/2508.01144v1",
        "abstract": "While traditional cryptographic research focuses on algorithm-level provable\nsecurity, many real-world attacks exploit weaknesses in system implementations,\nsuch as memory mismanagement, poor entropy sources, and insecure key\nlifecycles. Existing approaches address these risks in isolation but lack a\nunified, verifiable framework for modeling implementation-layer security. In\nthis work, we propose Implementation-Level Provable Security, a new paradigm\nthat defines security in terms of structurally verifiable resilience against\nreal-world attack surfaces during deployment. To demonstrate its feasibility,\nwe present SEER (Secure and Efficient Encryption-based Erasure via Ransomware),\na file destruction system that repurposes and reinforces the encryption core of\nBabuk ransomware. SEER incorporates key erasure, entropy validation, and\nexecution consistency checks to ensure a well-constrained, auditable attack\nsurface. Our evaluation shows that SEER achieves strong irrecoverability\nguarantees while maintaining practical performance. This work demonstrates a\nshift from abstract theoretical models toward practically verifiable\nimplementation-layer security."
    },
    {
        "date": "2025-08",
        "title": "TensoMeta-VQC: A Tensor-Train-Guided Meta-Learning Framework for Robust and Scalable Variational Quantum Computing",
        "author": "Jun Qi, Chao-Han Yang, Pin-Yu Chen, and Min-Hsiu Hsieh",
        "link": "http://arxiv.org/abs/2508.01116v1",
        "abstract": "Variational Quantum Computing (VQC) faces fundamental barriers in\nscalability, primarily due to barren plateaus and quantum noise sensitivity. To\naddress these challenges, we introduce TensoMeta-VQC, a novel tensor-train\n(TT)-guided meta-learning framework designed to improve the robustness and\nscalability of VQC significantly. Our framework fully delegates the generation\nof quantum circuit parameters to a classical TT network, effectively decoupling\noptimization from quantum hardware. This innovative parameterization mitigates\ngradient vanishing, enhances noise resilience through structured low-rank\nrepresentations, and facilitates efficient gradient propagation. Based on\nNeural Tangent Kernel and statistical learning theory, our rigorous theoretical\nanalyses establish strong guarantees on approximation capability, optimization\nstability, and generalization performance. Extensive empirical results across\nquantum dot classification, Max-Cut optimization, and molecular quantum\nsimulation tasks demonstrate that TensoMeta-VQC consistently achieves superior\nperformance and robust noise tolerance, establishing it as a principled pathway\ntoward practical and scalable VQC on near-term quantum devices."
    },
    {
        "date": "2025-08",
        "title": "AdVAR-DNN: Adversarial Misclassification Attack on Collaborative DNN Inference",
        "author": "Shima Yousefi, Motahare Mounesan, and Saptarshi Debroy",
        "link": "http://arxiv.org/abs/2508.01107v1",
        "abstract": "In recent years, Deep Neural Networks (DNNs) have become increasingly\nintegral to IoT-based environments, enabling realtime visual computing.\nHowever, the limited computational capacity of these devices has motivated the\nadoption of collaborative DNN inference, where the IoT device offloads part of\nthe inference-related computation to a remote server. Such offloading often\nrequires dynamic DNN partitioning information to be exchanged among the\nparticipants over an unsecured network or via relays/hops, leading to novel\nprivacy vulnerabilities. In this paper, we propose AdVAR-DNN, an adversarial\nvariational autoencoder (VAE)-based misclassification attack, leveraging\nclassifiers to detect model information and a VAE to generate untraceable\nmanipulated samples, specifically designed to compromise the collaborative\ninference process. AdVAR-DNN attack uses the sensitive information exchange\nvulnerability of collaborative DNN inference and is black-box in nature in\nterms of having no prior knowledge about the DNN model and how it is\npartitioned. Our evaluation using the most popular object classification DNNs\non the CIFAR-100 dataset demonstrates the effectiveness of AdVAR-DNN in terms\nof high attack success rate with little to no probability of detection."
    },
    {
        "date": "2025-08",
        "title": "AURA: A Hybrid Spatiotemporal-Chromatic Framework for Robust, Real-Time Detection of Industrial Smoke Emissions",
        "author": "Mikhail Bychkov, Matey Yordanov, and Andrei Kuchma",
        "link": "http://arxiv.org/abs/2508.01095v2",
        "abstract": "This paper introduces AURA, a novel hybrid spatiotemporal-chromatic framework\ndesigned for robust, real-time detection and classification of industrial smoke\nemissions. The framework addresses critical limitations of current monitoring\nsystems, which often lack the specificity to distinguish smoke types and\nstruggle with environmental variability. AURA leverages both the dynamic\nmovement patterns and the distinct color characteristics of industrial smoke to\nprovide enhanced accuracy and reduced false positives. This framework aims to\nsignificantly improve environmental compliance, operational safety, and public\nhealth outcomes by enabling precise, automated monitoring of industrial\nemissions."
    },
    {
        "date": "2025-08",
        "title": "COSTARR: Consolidated Open Set Technique with Attenuation for Robust Recognition",
        "author": "Ryan Rabinowitz, Steve Cruz, Walter Scheirer, and Terrance E. Boult",
        "link": "http://arxiv.org/abs/2508.01087v1",
        "abstract": "Handling novelty remains a key challenge in visual recognition systems.\nExisting open-set recognition (OSR) methods rely on the familiarity hypothesis,\ndetecting novelty by the absence of familiar features. We propose a novel\nattenuation hypothesis: small weights learned during training attenuate\nfeatures and serve a dual role-differentiating known classes while discarding\ninformation useful for distinguishing known from unknown classes. To leverage\nthis overlooked information, we present COSTARR, a novel approach that combines\nboth the requirement of familiar features and the lack of unfamiliar ones. We\nprovide a probabilistic interpretation of the COSTARR score, linking it to the\nlikelihood of correct classification and belonging in a known class. To\ndetermine the individual contributions of the pre- and post-attenuated features\nto COSTARR's performance, we conduct ablation studies that show both\npre-attenuated deep features and the underutilized post-attenuated Hadamard\nproduct features are essential for improving OSR. Also, we evaluate COSTARR in\na large-scale setting using ImageNet2012-1K as known data and NINCO,\niNaturalist, OpenImage-O, and other datasets as unknowns, across multiple\nmodern pre-trained architectures (ViTs, ConvNeXts, and ResNet). The experiments\ndemonstrate that COSTARR generalizes effectively across various architectures\nand significantly outperforms prior state-of-the-art methods by incorporating\npreviously discarded attenuation information, advancing open-set recognition\ncapabilities."
    },
    {
        "date": "2025-08",
        "title": "An Unconditionally Secure Encryption Scheme for IoBT Networks",
        "author": "Mohammad Moltafet, Hamid R. Sadjadpour, and Zouheir Rezki",
        "link": "http://arxiv.org/abs/2508.01085v1",
        "abstract": "We consider an Internet of Battlefield Things (IoBT) system consisting of\nmultiple devices that want to securely communicate with each other during a\nmission in the presence of an adversary with unbounded computational power. The\nadversary has complete access to listen/read the ciphertext without tampering\nwith the communication line. We provide an unconditionally secure encryption\nscheme to exchange messages among devices in the system. The main idea behind\nthe scheme is to provide secret keys to exchange messages using a random binary\nmatrix that is securely shared among all the devices, and pair-wise random\nsecret keys established between each pair of devices attempting to communicate\nbefore the mission. The scheme is implemented by using finite group modular\naddition. We show that the scheme is absolutely semantically secure, i.e., the\nscheme guarantees that an adversary with unbounded computational power cannot\nget even one bit of information about a message, except for an exponentially\nsmall probability in a security parameter. Besides that, we show that even if\nthe random binary matrix is revealed to the adversary, the provided scheme is\ncomputationally secure against the key recovery attack."
    },
    {
        "date": "2025-08",
        "title": "Provably Secure Retrieval-Augmented Generation",
        "author": "Pengcheng Zhou, Yinglun Feng, and Zhongliang Yang",
        "link": "http://arxiv.org/abs/2508.01084v1",
        "abstract": "Although Retrieval-Augmented Generation (RAG) systems have been widely\napplied, the privacy and security risks they face, such as data leakage and\ndata poisoning, have not been systematically addressed yet. Existing defense\nstrategies primarily rely on heuristic filtering or enhancing retriever\nrobustness, which suffer from limited interpretability, lack of formal security\nguarantees, and vulnerability to adaptive attacks. To address these challenges,\nthis paper proposes the first provably secure framework for RAG systems(SAG).\nOur framework employs a pre-storage full-encryption scheme to ensure dual\nprotection of both retrieved content and vector embeddings, guaranteeing that\nonly authorized entities can access the data. Through formal security proofs,\nwe rigorously verify the scheme's confidentiality and integrity under a\ncomputational security model. Extensive experiments across multiple benchmark\ndatasets demonstrate that our framework effectively resists a range of\nstate-of-the-art attacks. This work establishes a theoretical foundation and\npractical paradigm for verifiably secure RAG systems, advancing AI-powered\nservices toward formally guaranteed security."
    },
    {
        "date": "2025-08",
        "title": "CP-FREEZER: Latency Attacks against Vehicular Cooperative Perception",
        "author": "Chenyi Wang, Ruoyu Song, Raymond Muller, Jean-Philippe Monteuuis, Z. Berkay Celik, Jonathan Petit, Ryan Gerdes, and Ming Li",
        "link": "http://arxiv.org/abs/2508.01062v1",
        "abstract": "Cooperative perception (CP) enhances situational awareness of connected and\nautonomous vehicles by exchanging and combining messages from multiple agents.\nWhile prior work has explored adversarial integrity attacks that degrade\nperceptual accuracy, little is known about CP's robustness against attacks on\ntimeliness (or availability), a safety-critical requirement for autonomous\ndriving. In this paper, we present CP-FREEZER, the first latency attack that\nmaximizes the computation delay of CP algorithms by injecting adversarial\nperturbation via V2V messages. Our attack resolves several unique challenges,\nincluding the non-differentiability of point cloud preprocessing, asynchronous\nknowledge of the victim's input due to transmission delays, and uses a novel\nloss function that effectively maximizes the execution time of the CP pipeline.\nExtensive experiments show that CP-FREEZER increases end-to-end CP latency by\nover $90\\times$, pushing per-frame processing time beyond 3 seconds with a 100%\nsuccess rate on our real-world vehicle testbed. Our findings reveal a critical\nthreat to the availability of CP systems, highlighting the urgent need for\nrobust defenses."
    },
    {
        "date": "2025-08",
        "title": "MMBERT: Scaled Mixture-of-Experts Multimodal BERT for Robust Chinese Hate Speech Detection under Cloaking Perturbations",
        "author": "Qiyao Xue, Yuchen Dou, Ryan Shi, Xiang Lorraine Li, and Wei Gao",
        "link": "http://arxiv.org/abs/2508.00760v1",
        "abstract": "Hate speech detection on Chinese social networks presents distinct\nchallenges, particularly due to the widespread use of cloaking techniques\ndesigned to evade conventional text-based detection systems. Although large\nlanguage models (LLMs) have recently improved hate speech detection\ncapabilities, the majority of existing work has concentrated on English\ndatasets, with limited attention given to multimodal strategies in the Chinese\ncontext. In this study, we propose MMBERT, a novel BERT-based multimodal\nframework that integrates textual, speech, and visual modalities through a\nMixture-of-Experts (MoE) architecture. To address the instability associated\nwith directly integrating MoE into BERT-based models, we develop a progressive\nthree-stage training paradigm. MMBERT incorporates modality-specific experts, a\nshared self-attention mechanism, and a router-based expert allocation strategy\nto enhance robustness against adversarial perturbations. Empirical results in\nseveral Chinese hate speech datasets show that MMBERT significantly surpasses\nfine-tuned BERT-based encoder models, fine-tuned LLMs, and LLMs utilizing\nin-context learning approaches."
    },
    {
        "date": "2025-08",
        "title": "Efficient Solution and Learning of Robust Factored MDPs",
        "author": "Yannik Schnitzer, Alessandro Abate, and David Parker",
        "link": "http://arxiv.org/abs/2508.00707v1",
        "abstract": "Robust Markov decision processes (r-MDPs) extend MDPs by explicitly modelling\nepistemic uncertainty about transition dynamics. Learning r-MDPs from\ninteractions with an unknown environment enables the synthesis of robust\npolicies with provable (PAC) guarantees on performance, but this can require a\nlarge number of sample interactions. We propose novel methods for solving and\nlearning r-MDPs based on factored state-space representations that leverage the\nindependence between model uncertainty across system components. Although\npolicy synthesis for factored r-MDPs leads to hard, non-convex optimisation\nproblems, we show how to reformulate these into tractable linear programs.\nBuilding on these, we also propose methods to learn factored model\nrepresentations directly. Our experimental results show that exploiting\nfactored structure can yield dimensional gains in sample efficiency, producing\nmore effective robust policies with tighter performance guarantees than\nstate-of-the-art methods."
    },
    {
        "date": "2025-08",
        "title": "Wind Power Scenario Generation based on the Generalized Dynamic Factor Model and Generative Adversarial Network",
        "author": "Young-ho Cho, Hao Zhu, Duehee Lee, and Ross Baldick",
        "link": "http://arxiv.org/abs/2508.00692v1",
        "abstract": "For conducting resource adequacy studies, we synthesize multiple long-term\nwind power scenarios of distributed wind farms simultaneously by using the\nspatio-temporal features: spatial and temporal correlation, waveforms, marginal\nand ramp rates distributions of waveform, power spectral densities, and\nstatistical characteristics. Generating the spatial correlation in scenarios\nrequires the design of common factors for neighboring wind farms and\nantithetical factors for distant wind farms. The generalized dynamic factor\nmodel (GDFM) can extract the common factors through cross spectral density\nanalysis, but it cannot closely imitate waveforms. The GAN can synthesize\nplausible samples representing the temporal correlation by verifying samples\nthrough a fake sample discriminator. To combine the advantages of GDFM and GAN,\nwe use the GAN to provide a filter that extracts dynamic factors with temporal\ninformation from the observation data, and we then apply this filter in the\nGDFM to represent both spatial and frequency correlations of plausible\nwaveforms. Numerical tests on the combination of GDFM and GAN have demonstrated\nperformance improvements over competing alternatives in synthesizing wind power\nscenarios from Australia, better realizing plausible statistical\ncharacteristics of actual wind power compared to alternatives such as the GDFM\nwith a filter synthesized from distributions of actual dynamic filters and the\nGAN with direct synthesis without dynamic factors."
    },
    {
        "date": "2025-08",
        "title": "VAULT: Vigilant Adversarial Updates via LLM-Driven Retrieval-Augmented Generation for NLI",
        "author": "Roie Kazoom, Ofir Cohen, Rami Puzis, Asaf Shabtai, and Ofer Hadar",
        "link": "http://arxiv.org/abs/2508.00965v1",
        "abstract": "We introduce VAULT, a fully automated adversarial RAG pipeline that\nsystematically uncovers and remedies weaknesses in NLI models through three\nstages: retrieval, adversarial generation, and iterative retraining. First, we\nperform balanced few-shot retrieval by embedding premises with both semantic\n(BGE) and lexical (BM25) similarity. Next, we assemble these contexts into LLM\nprompts to generate adversarial hypotheses, which are then validated by an LLM\nensemble for label fidelity. Finally, the validated adversarial examples are\ninjected back into the training set at increasing mixing ratios, progressively\nfortifying a zero-shot RoBERTa-base model.On standard benchmarks, VAULT\nelevates RoBERTa-base accuracy from 88.48% to 92.60% on SNLI +4.12%, from\n75.04% to 80.95% on ANLI +5.91%, and from 54.67% to 71.99% on MultiNLI +17.32%.\nIt also consistently outperforms prior in-context adversarial methods by up to\n2.0% across datasets. By automating high-quality adversarial data curation at\nscale, VAULT enables rapid, human-independent robustness improvements in NLI\ninference tasks."
    },
    {
        "date": "2025-08",
        "title": "Revisiting Adversarial Patch Defenses on Object Detectors: Unified Evaluation, Large-Scale Dataset, and New Insights",
        "author": "Junhao Zheng, Jiahao Sun, Chenhao Lin, Zhengyu Zhao, Chen Ma, Chong Zhang, Cong Wang, Qian Wang, and Chao Shen",
        "link": "http://arxiv.org/abs/2508.00649v2",
        "abstract": "Developing reliable defenses against patch attacks on object detectors has\nattracted increasing interest. However, we identify that existing defense\nevaluations lack a unified and comprehensive framework, resulting in\ninconsistent and incomplete assessments of current methods. To address this\nissue, we revisit 11 representative defenses and present the first patch\ndefense benchmark, involving 2 attack goals, 13 patch attacks, 11 object\ndetectors, and 4 diverse metrics. This leads to the large-scale adversarial\npatch dataset with 94 types of patches and 94,000 images. Our comprehensive\nanalyses reveal new insights: (1) The difficulty in defending against\nnaturalistic patches lies in the data distribution, rather than the commonly\nbelieved high frequencies. Our new dataset with diverse patch distributions can\nbe used to improve existing defenses by 15.09% AP@0.5. (2) The average\nprecision of the attacked object, rather than the commonly pursued patch\ndetection accuracy, shows high consistency with defense performance. (3)\nAdaptive attacks can substantially bypass existing defenses, and defenses with\ncomplex/stochastic models or universal patch properties are relatively robust.\nWe hope that our analyses will serve as guidance on properly evaluating patch\nattacks/defenses and advancing their design. Code and dataset are available at\nhttps://github.com/Gandolfczjh/APDE, where we will keep integrating new\nattacks/defenses."
    },
    {
        "date": "2025-08",
        "title": "Reinforcement Learning for Decision-Level Interception Prioritization in Drone Swarm Defense",
        "author": "Alessandro Palmas",
        "link": "http://arxiv.org/abs/2508.00641v1",
        "abstract": "The growing threat of low-cost kamikaze drone swarms poses a critical\nchallenge to modern defense systems demanding rapid and strategic\ndecision-making to prioritize interceptions across multiple effectors and\nhigh-value target zones. In this work, we present a case study demonstrating\nthe practical advantages of reinforcement learning in addressing this\nchallenge. We introduce a high-fidelity simulation environment that captures\nrealistic operational constraints, within which a decision-level reinforcement\nlearning agent learns to coordinate multiple effectors for optimal interception\nprioritization. Operating in a discrete action space, the agent selects which\ndrone to engage per effector based on observed state features such as\npositions, classes, and effector status. We evaluate the learned policy against\na handcrafted rule-based baseline across hundreds of simulated attack\nscenarios. The reinforcement learning based policy consistently achieves lower\naverage damage and higher defensive efficiency in protecting critical zones.\nThis case study highlights the potential of reinforcement learning as a\nstrategic layer within defense architectures, enhancing resilience without\ndisplacing existing control systems. All code and simulation assets are\npublicly released for full reproducibility, and a video demonstration\nillustrates the policy's qualitative behavior."
    },
    {
        "date": "2025-08",
        "title": "Cyber-Physical Co-Simulation of Load Frequency Control under Load-Altering Attacks",
        "author": "Micha\u0142 Forystek, Andrew D. Syrmakesis, Alkistis Kontou, Panos Kotsampopoulos, Nikos D. Hatziargyriou, and Charalambos Konstantinou",
        "link": "http://arxiv.org/abs/2508.00637v1",
        "abstract": "Integrating Information and Communications Technology (ICT) devices into the\npower grid brings many benefits. However, it also exposes the grid to new\npotential cyber threats. Many control and protection mechanisms, such as Load\nFrequency Control (LFC), responsible for maintaining nominal frequency during\nload fluctuations and Under Frequency Load Shedding (UFLS) disconnecting\nportion of the load during an emergency, are dependent on information exchange\nthrough the communication network. The recently emerging Load Altering Attacks\n(LAAs) utilize a botnet of high-wattage devices to introduce load fluctuation.\nIn their dynamic form (DLAAs), they manipulate the load in response to live\ngrid frequency measurements for increased efficiency, posing a notable threat\nto grid stability. Recognizing the importance of communication networks in\npower grid cyber security research, this paper presents an open-source\nco-simulation environment that models the power grid with the corresponding\ncommunication network, implementing grid protective mechanisms. This setup\nallows the comprehensive analysis of the attacks in concrete LFC and UFLS\nscenarios."
    },
    {
        "date": "2025-08",
        "title": "FedGuard: A Diverse-Byzantine-Robust Mechanism for Federated Learning with Major Malicious Clients",
        "author": "Haocheng Jiang, Hua Shen, Jixin Zhang, Willy Susilo, and Mingwu Zhang",
        "link": "http://arxiv.org/abs/2508.00636v1",
        "abstract": "Federated learning is a distributed training framework vulnerable to\nByzantine attacks, particularly when over 50% of clients are malicious or when\ndatasets are highly non-independent and identically distributed (non-IID).\nAdditionally, most existing defense mechanisms are designed for specific attack\ntypes (e.g., gradient similarity-based schemes can only defend against outlier\nmodel poisoning), limiting their effectiveness. In response, we propose\nFedGuard, a novel federated learning mechanism. FedGuard cleverly addresses the\naforementioned issues by leveraging the high sensitivity of membership\ninference to model bias. By requiring clients to include an additional\nmini-batch of server-specified data in their training, FedGuard can identify\nand exclude poisoned models, as their confidence in the mini-batch will drop\nsignificantly. Our comprehensive evaluation unequivocally shows that, under\nthree highly non-IID datasets, with 90% of clients being Byzantine and seven\ndifferent types of Byzantine attacks occurring in each round, FedGuard\nsignificantly outperforms existing robust federated learning schemes in\nmitigating various types of Byzantine attacks."
    },
    {
        "date": "2025-08",
        "title": "Backdoor Attacks on Deep Learning Face Detection",
        "author": "Quentin Le Roux, Yannick Teglia, Teddy Furon, and Philippe Loubet-Moundi",
        "link": "http://arxiv.org/abs/2508.00620v1",
        "abstract": "Face Recognition Systems that operate in unconstrained environments capture\nimages under varying conditions,such as inconsistent lighting, or diverse face\nposes. These challenges require including a Face Detection module that\nregresses bounding boxes and landmark coordinates for proper Face Alignment.\nThis paper shows the effectiveness of Object Generation Attacks on Face\nDetection, dubbed Face Generation Attacks, and demonstrates for the first time\na Landmark Shift Attack that backdoors the coordinate regression task performed\nby face detectors. We then offer mitigations against these vulnerabilities."
    },
    {
        "date": "2025-08",
        "title": "DACTYL: Diverse Adversarial Corpus of Texts Yielded from Large Language Models",
        "author": "Shantanu Thorat, and Andrew Caines",
        "link": "http://arxiv.org/abs/2508.00619v1",
        "abstract": "Existing AIG (AI-generated) text detectors struggle in real-world settings\ndespite succeeding in internal testing, suggesting that they may not be robust\nenough. We rigorously examine the machine-learning procedure to build these\ndetectors to address this. Most current AIG text detection datasets focus on\nzero-shot generations, but little work has been done on few-shot or one-shot\ngenerations, where LLMs are given human texts as an example. In response, we\nintroduce the Diverse Adversarial Corpus of Texts Yielded from Language models\n(DACTYL), a challenging AIG text detection dataset focusing on\none-shot/few-shot generations. We also include texts from domain-specific\ncontinued-pre-trained (CPT) language models, where we fully train all\nparameters using a memory-efficient optimization approach. Many existing AIG\ntext detectors struggle significantly on our dataset, indicating a potential\nvulnerability to one-shot/few-shot and CPT-generated texts. We also train our\nown classifiers using two approaches: standard binary cross-entropy (BCE)\noptimization and a more recent approach, deep X-risk optimization (DXO). While\nBCE-trained classifiers marginally outperform DXO classifiers on the DACTYL\ntest set, the latter excels on out-of-distribution (OOD) texts. In our mock\ndeployment scenario in student essay detection with an OOD student essay\ndataset, the best DXO classifier outscored the best BCE-trained classifier by\n50.56 macro-F1 score points at the lowest false positive rates for both. Our\nresults indicate that DXO classifiers generalize better without overfitting to\nthe test set. Our experiments highlight several areas of improvement for AIG\ntext detectors."
    },
    {
        "date": "2025-08",
        "title": "LeakSealer: A Semisupervised Defense for LLMs Against Prompt Injection and Leakage Attacks",
        "author": "Francesco Panebianco, Stefano Bonfanti, Francesco Trov\u00f2, and Michele Carminati",
        "link": "http://arxiv.org/abs/2508.00602v1",
        "abstract": "The generalization capabilities of Large Language Models (LLMs) have led to\ntheir widespread deployment across various applications. However, this\nincreased adoption has introduced several security threats, notably in the\nforms of jailbreaking and data leakage attacks. Additionally, Retrieval\nAugmented Generation (RAG), while enhancing context-awareness in LLM responses,\nhas inadvertently introduced vulnerabilities that can result in the leakage of\nsensitive information. Our contributions are twofold. First, we introduce a\nmethodology to analyze historical interaction data from an LLM system, enabling\nthe generation of usage maps categorized by topics (including adversarial\ninteractions). This approach further provides forensic insights for tracking\nthe evolution of jailbreaking attack patterns. Second, we propose LeakSealer, a\nmodel-agnostic framework that combines static analysis for forensic insights\nwith dynamic defenses in a Human-In-The-Loop (HITL) pipeline. This technique\nidentifies topic groups and detects anomalous patterns, allowing for proactive\ndefense mechanisms. We empirically evaluate LeakSealer under two scenarios: (1)\njailbreak attempts, employing a public benchmark dataset, and (2) PII leakage,\nsupported by a curated dataset of labeled LLM interactions. In the static\nsetting, LeakSealer achieves the highest precision and recall on the ToxicChat\ndataset when identifying prompt injection. In the dynamic setting, PII leakage\ndetection achieves an AUPRC of $0.97$, significantly outperforming baselines\nsuch as Llama Guard."
    },
    {
        "date": "2025-08",
        "title": "DPoser-X: Diffusion Model as Robust 3D Whole-body Human Pose Prior",
        "author": "Junzhe Lu, Jing Lin, Hongkun Dou, Ailing Zeng, Yue Deng, Xian Liu, Zhongang Cai, Lei Yang, Yulun Zhang, Haoqian Wang, and Ziwei Liu",
        "link": "http://arxiv.org/abs/2508.00599v2",
        "abstract": "We present DPoser-X, a diffusion-based prior model for 3D whole-body human\nposes. Building a versatile and robust full-body human pose prior remains\nchallenging due to the inherent complexity of articulated human poses and the\nscarcity of high-quality whole-body pose datasets. To address these\nlimitations, we introduce a Diffusion model as body Pose prior (DPoser) and\nextend it to DPoser-X for expressive whole-body human pose modeling. Our\napproach unifies various pose-centric tasks as inverse problems, solving them\nthrough variational diffusion sampling. To enhance performance on downstream\napplications, we introduce a novel truncated timestep scheduling method\nspecifically designed for pose data characteristics. We also propose a masked\ntraining mechanism that effectively combines whole-body and part-specific\ndatasets, enabling our model to capture interdependencies between body parts\nwhile avoiding overfitting to specific actions. Extensive experiments\ndemonstrate DPoser-X's robustness and versatility across multiple benchmarks\nfor body, hand, face, and full-body pose modeling. Our model consistently\noutperforms state-of-the-art alternatives, establishing a new benchmark for\nwhole-body human pose prior modeling."
    },
    {
        "date": "2025-08",
        "title": "Information-Theoretic Decentralized Secure Aggregation with Collusion Resilience",
        "author": "Xiang Zhang, Zhou Li, Shuangyang Li, Kai Wan, Derrick Wing Kwan Ng, and Giuseppe Caire",
        "link": "http://arxiv.org/abs/2508.00596v1",
        "abstract": "In decentralized federated learning (FL), multiple clients collaboratively\nlearn a shared machine learning (ML) model by leveraging their privately held\ndatasets distributed across the network, through interactive exchange of the\nintermediate model updates. To ensure data security, cryptographic techniques\nare commonly employed to protect model updates during aggregation. Despite\ngrowing interest in secure aggregation, existing works predominantly focus on\nprotocol design and computational guarantees, with limited understanding of the\nfundamental information-theoretic limits of such systems. Moreover, optimal\nbounds on communication and key usage remain unknown in decentralized settings,\nwhere no central aggregator is available. Motivated by these gaps, we study the\nproblem of decentralized secure aggregation (DSA) from an information-theoretic\nperspective. Specifically, we consider a network of $K$ fully-connected users,\neach holding a private input -- an abstraction of local training data -- who\naim to securely compute the sum of all inputs. The security constraint requires\nthat no user learns anything beyond the input sum, even when colluding with up\nto $T$ other users. We characterize the optimal rate region, which specifies\nthe minimum achievable communication and secret key rates for DSA. In\nparticular, we show that to securely compute one symbol of the desired input\nsum, each user must (i) transmit at least one symbol to others, (ii) hold at\nleast one symbol of secret key, and (iii) all users must collectively hold no\nfewer than $K - 1$ independent key symbols. Our results establish the\nfundamental performance limits of DSA, providing insights for the design of\nprovably secure and communication-efficient protocols in distributed learning\nsystems."
    },
    {
        "date": "2025-08",
        "title": "Activation-Guided Local Editing for Jailbreaking Attacks",
        "author": "Jiecong Wang, Haoran Li, Hao Peng, Ziqian Zeng, Zihao Wang, Haohua Du, and Zhengtao Yu",
        "link": "http://arxiv.org/abs/2508.00555v1",
        "abstract": "Jailbreaking is an essential adversarial technique for red-teaming these\nmodels to uncover and patch security flaws. However, existing jailbreak methods\nface significant drawbacks. Token-level jailbreak attacks often produce\nincoherent or unreadable inputs and exhibit poor transferability, while\nprompt-level attacks lack scalability and rely heavily on manual effort and\nhuman ingenuity. We propose a concise and effective two-stage framework that\ncombines the advantages of these approaches. The first stage performs a\nscenario-based generation of context and rephrases the original malicious query\nto obscure its harmful intent. The second stage then utilizes information from\nthe model's hidden states to guide fine-grained edits, effectively steering the\nmodel's internal representation of the input from a malicious toward a benign\none. Extensive experiments demonstrate that this method achieves\nstate-of-the-art Attack Success Rate, with gains of up to 37.74% over the\nstrongest baseline, and exhibits excellent transferability to black-box models.\nOur analysis further demonstrates that AGILE maintains substantial\neffectiveness against prominent defense mechanisms, highlighting the\nlimitations of current safeguards and providing valuable insights for future\ndefense development. Our code is available at\nhttps://github.com/yunsaijc/AGILE."
    },
    {
        "date": "2025-08",
        "title": "DBLP: Noise Bridge Consistency Distillation For Efficient And Reliable Adversarial Purification",
        "author": "Chihan Huang, Belal Alsinglawi, and Islam Al-qudah",
        "link": "http://arxiv.org/abs/2508.00552v1",
        "abstract": "Recent advances in deep neural networks (DNNs) have led to remarkable success\nacross a wide range of tasks. However, their susceptibility to adversarial\nperturbations remains a critical vulnerability. Existing diffusion-based\nadversarial purification methods often require intensive iterative denoising,\nseverely limiting their practical deployment. In this paper, we propose\nDiffusion Bridge Distillation for Purification (DBLP), a novel and efficient\ndiffusion-based framework for adversarial purification. Central to our approach\nis a new objective, noise bridge distillation, which constructs a principled\nalignment between the adversarial noise distribution and the clean data\ndistribution within a latent consistency model (LCM). To further enhance\nsemantic fidelity, we introduce adaptive semantic enhancement, which fuses\nmulti-scale pyramid edge maps as conditioning input to guide the purification\nprocess. Extensive experiments across multiple datasets demonstrate that DBLP\nachieves state-of-the-art (SOTA) robust accuracy, superior image quality, and\naround 0.2s inference time, marking a significant step toward real-time\nadversarial purification."
    },
    {
        "date": "2025-08",
        "title": "CyGATE: Game-Theoretic Cyber Attack-Defense Engine for Patch Strategy Optimization",
        "author": "Yuning Jiang, Nay Oo, Qiaoran Meng, Lu Lin, Dusit Niyato, Zehui Xiong, Hoon Wei Lim, and Biplab Sikdar",
        "link": "http://arxiv.org/abs/2508.00478v1",
        "abstract": "Modern cyber attacks unfold through multiple stages, requiring defenders to\ndynamically prioritize mitigations under uncertainty. While game-theoretic\nmodels capture attacker-defender interactions, existing approaches often rely\non static assumptions and lack integration with real-time threat intelligence,\nlimiting their adaptability. This paper presents CyGATE, a game-theoretic\nframework modeling attacker-defender interactions, using large language models\n(LLMs) with retrieval-augmented generation (RAG) to enhance tactic selection\nand patch prioritization. Applied to a two-agent scenario, CyGATE frames cyber\nconflicts as a partially observable stochastic game (POSG) across Cyber Kill\nChain stages. Both agents use belief states to navigate uncertainty, with the\nattacker adapting tactics and the defender re-prioritizing patches based on\nevolving risks and observed adversary behavior. The framework's flexible\narchitecture enables extension to multi-agent scenarios involving coordinated\nattackers, collaborative defenders, or complex enterprise environments with\nmultiple stakeholders. Evaluated in a dynamic patch scheduling scenario, CyGATE\neffectively prioritizes high-risk vulnerabilities, enhancing adaptability\nthrough dynamic threat integration, strategic foresight by anticipating\nattacker moves under uncertainty, and efficiency by optimizing resource use."
    },
    {
        "date": "2025-08",
        "title": "Occlusion-robust Stylization for Drawing-based 3D Animation",
        "author": "Sunjae Yoon, Gwanhyeong Koo, Younghwan Lee, Ji Woo Hong, and Chang D. Yoo",
        "link": "http://arxiv.org/abs/2508.00398v1",
        "abstract": "3D animation aims to generate a 3D animated video from an input image and a\ntarget 3D motion sequence. Recent advances in image-to-3D models enable the\ncreation of animations directly from user-hand drawings. Distinguished from\nconventional 3D animation, drawing-based 3D animation is crucial to preserve\nartist's unique style properties, such as rough contours and distinct stroke\npatterns. However, recent methods still exhibit quality deterioration in style\nproperties, especially under occlusions caused by overlapping body parts,\nleading to contour flickering and stroke blurring. This occurs due to a\n`stylization pose gap' between training and inference in stylization networks\ndesigned to preserve drawing styles in drawing-based 3D animation systems. The\nstylization pose gap denotes that input target poses used to train the\nstylization network are always in occlusion-free poses, while target poses\nencountered in an inference include diverse occlusions under dynamic motions.\nTo this end, we propose Occlusion-robust Stylization Framework (OSF) for\ndrawing-based 3D animation. We found that while employing object's edge can be\neffective input prior for guiding stylization, it becomes notably inaccurate\nwhen occlusions occur at inference. Thus, our proposed OSF provides\nocclusion-robust edge guidance for stylization network using optical flow,\nensuring a consistent stylization even under occlusions. Furthermore, OSF\noperates in a single run instead of the previous two-stage method, achieving\n2.4x faster inference and 2.1x less memory."
    },
    {
        "date": "2025-08",
        "title": "Preliminary Investigation into Uncertainty-Aware Attack Stage Classification",
        "author": "Alessandro Gaudenzi, Lorenzo Nodari, Lance Kaplan, Alessandra Russo, Murat Sensoy, and Federico Cerutti",
        "link": "http://arxiv.org/abs/2508.00368v1",
        "abstract": "Advanced Persistent Threats (APTs) represent a significant challenge in\ncybersecurity due to their prolonged, multi-stage nature and the sophistication\nof their operators. Traditional detection systems typically focus on\nidentifying malicious activity in binary terms (benign or malicious) without\naccounting for the progression of an attack. However, effective response\nstrategies depend on accurate inference of the attack's current stage, as\ncountermeasures must be tailored to whether an adversary is in the early\nreconnaissance phase or actively conducting exploitation or exfiltration. This\nwork addresses the problem of attack stage inference under uncertainty, with a\nfocus on robustness to out-of-distribution (OOD) inputs. We propose a\nclassification approach based on Evidential Deep Learning (EDL), which models\npredictive uncertainty by outputting parameters of a Dirichlet distribution\nover possible stages. This allows the system not only to predict the most\nlikely stage of an attack but also to indicate when it is uncertain or the\ninput lies outside the training distribution. Preliminary experiments in a\nsimulated environment demonstrate that the proposed model can accurately infer\nthe stage of an attack with calibrated confidence while effectively detecting\nOOD inputs, which may indicate changes in the attackers' tactics. These results\nsupport the feasibility of deploying uncertainty-aware models for staged threat\ndetection in dynamic and adversarial environments."
    },
    {
        "date": "2025-08",
        "title": "ranDecepter: Real-time Identification and Deterrence of Ransomware Attacks",
        "author": "Md Sajidul Islam Sajid, Jinpeng Wei, and Ehab Al-Shaer",
        "link": "http://arxiv.org/abs/2508.00293v3",
        "abstract": "Ransomware (RW) presents a significant and widespread threat in the digital\nlandscape, necessitating effective countermeasures. Active cyber deception is a\npromising strategy to thwart RW and limiting its propagation by misleading it\nwith false information and revealing its true behaviors. Furthermore, RW often\nacts as a communication conduit between attackers and defenders, allowing\ndeception to return false data to attackers and deplete their resources. This\npaper introduces ranDecepter, a novel approach that combines active cyber\ndeception with real-time analysis to enhance defenses against RW attacks. The\nranDecepter identifies RW in real-time and isolates it within a deceptive\nenvironment, autonomously identifying critical elements in the RW code to\ncreate a loop mechanism. By repeatedly restarting the malware and transmitting\ncounterfeit encryption information and secret keys to the attacker, it forces\nthe attacker to store these fabricated details for each victim, thereby\ndepleting their resources. Our comprehensive evaluation of ranDecepter,\nconducted using 1,134 real-world malware samples and twelve benign\napplications, demonstrates a remarkable 100% accuracy in RW identification,\nwith no false positives and minimal impact on response times. Furthermore,\nwithin 24-hours, ranDecepter generates up to 9,223K entries in the attacker's\ndatabase using 50 agents, showcasing its potential to undermine attacker\nresources."
    },
    {
        "date": "2025-08",
        "title": "Towards Robust Semantic Correspondence: A Benchmark and Insights",
        "author": "Wenyue Chong",
        "link": "http://arxiv.org/abs/2508.00272v1",
        "abstract": "Semantic correspondence aims to identify semantically meaningful\nrelationships between different images and is a fundamental challenge in\ncomputer vision. It forms the foundation for numerous tasks such as 3D\nreconstruction, object tracking, and image editing. With the progress of\nlarge-scale vision models, semantic correspondence has achieved remarkable\nperformance in controlled and high-quality conditions. However, the robustness\nof semantic correspondence in challenging scenarios is much less investigated.\nIn this work, we establish a novel benchmark for evaluating semantic\ncorrespondence in adverse conditions. The benchmark dataset comprises 14\ndistinct challenging scenarios that reflect commonly encountered imaging\nissues, including geometric distortion, image blurring, digital artifacts, and\nenvironmental occlusion. Through extensive evaluations, we provide several key\ninsights into the robustness of semantic correspondence approaches: (1) All\nexisting methods suffer from noticeable performance drops under adverse\nconditions; (2) Using large-scale vision models can enhance overall robustness,\nbut fine-tuning on these models leads to a decline in relative robustness; (3)\nThe DINO model outperforms the Stable Diffusion in relative robustness, and\ntheir fusion achieves better absolute robustness; Moreover, We evaluate common\nrobustness enhancement strategies for semantic correspondence and find that\ngeneral data augmentations are ineffective, highlighting the need for\ntask-specific designs. These results are consistent across both our dataset and\nreal-world benchmarks."
    },
    {
        "date": "2025-08",
        "title": "Large AI Model-Enabled Secure Communications in Low-Altitude Wireless Networks: Concepts, Perspectives and Case Study",
        "author": "Chuang Zhang, Geng Sun, Jiacheng Wang, Yijing Lin, Weijie Yuan, Sinem Coleri, Dusit Niyato, and Tony Q. S. Quek",
        "link": "http://arxiv.org/abs/2508.00256v1",
        "abstract": "Low-altitude wireless networks (LAWNs) have the potential to revolutionize\ncommunications by supporting a range of applications, including urban parcel\ndelivery, aerial inspections and air taxis. However, compared with traditional\nwireless networks, LAWNs face unique security challenges due to low-altitude\noperations, frequent mobility and reliance on unlicensed spectrum, making it\nmore vulnerable to some malicious attacks. In this paper, we investigate some\nlarge artificial intelligence model (LAM)-enabled solutions for secure\ncommunications in LAWNs. Specifically, we first explore the amplified security\nrisks and important limitations of traditional AI methods in LAWNs. Then, we\nintroduce the basic concepts of LAMs and delve into the role of LAMs in\naddressing these challenges. To demonstrate the practical benefits of LAMs for\nsecure communications in LAWNs, we propose a novel LAM-based optimization\nframework that leverages large language models (LLMs) to generate enhanced\nstate features on top of handcrafted representations, and to design intrinsic\nrewards accordingly, thereby improving reinforcement learning performance for\nsecure communication tasks. Through a typical case study, simulation results\nvalidate the effectiveness of the proposed framework. Finally, we outline\nfuture directions for integrating LAMs into secure LAWN applications."
    },
    {
        "date": "2025-07",
        "title": "Robust Classification under Noisy Labels: A Geometry-Aware Reliability Framework for Foundation Models",
        "author": "Ecem Bozkurt, and Antonio Ortega",
        "link": "http://arxiv.org/abs/2508.00202v1",
        "abstract": "Foundation models (FMs) pretrained on large datasets have become fundamental\nfor various downstream machine learning tasks, in particular in scenarios where\nobtaining perfectly labeled data is prohibitively expensive. In this paper, we\nassume an FM has to be fine-tuned with noisy data and present a two-stage\nframework to ensure robust classification in the presence of label noise\nwithout model retraining. Recent work has shown that simple k-nearest neighbor\n(kNN) approaches using an embedding derived from an FM can achieve good\nperformance even in the presence of severe label noise. Our work is motivated\nby the fact that these methods make use of local geometry. In this paper,\nfollowing a similar two-stage procedure, reliability estimation followed by\nreliability-weighted inference, we show that improved performance can be\nachieved by introducing geometry information. For a given instance, our\nproposed inference uses a local neighborhood of training data, obtained using\nthe non-negative kernel (NNK) neighborhood construction. We propose several\nmethods for reliability estimation that can rely less on distance and local\nneighborhood as the label noise increases. Our evaluation on CIFAR-10 and\nDermaMNIST shows that our methods improve robustness across various noise\nconditions, surpassing standard K-NN approaches and recent\nadaptive-neighborhood baselines."
    },
    {
        "date": "2025-07",
        "title": "DiSC-Med: Diffusion-based Semantic Communications for Robust Medical Image Transmission",
        "author": "Fupei Guo, Hao Zheng, Xiang Zhang, Li Chen, Yue Wang, and Songyang Zhang",
        "link": "http://arxiv.org/abs/2508.00172v1",
        "abstract": "The rapid development of artificial intelligence has driven smart health with\nnext-generation wireless communication technologies, stimulating exciting\napplications in remote diagnosis and intervention. To enable a timely and\neffective response for remote healthcare, efficient transmission of medical\ndata through noisy channels with limited bandwidth emerges as a critical\nchallenge. In this work, we propose a novel diffusion-based semantic\ncommunication framework, namely DiSC-Med, for the medical image transmission,\nwhere medical-enhanced compression and denoising blocks are developed for\nbandwidth efficiency and robustness, respectively. Unlike conventional\npixel-wise communication framework, our proposed DiSC-Med is able to capture\nthe key semantic information and achieve superior reconstruction performance\nwith ultra-high bandwidth efficiency against noisy channels. Extensive\nexperiments on real-world medical datasets validate the effectiveness of our\nframework, demonstrating its potential for robust and efficient telehealth\napplications."
    },
    {
        "date": "2025-07",
        "title": "Robust 3D Object Detection using Probabilistic Point Clouds from Single-Photon LiDARs",
        "author": "Bhavya Goyal, Felipe Gutierrez-Barragan, Wei Lin, Andreas Velten, Yin Li, and Mohit Gupta",
        "link": "http://arxiv.org/abs/2508.00169v1",
        "abstract": "LiDAR-based 3D sensors provide point clouds, a canonical 3D representation\nused in various scene understanding tasks. Modern LiDARs face key challenges in\nseveral real-world scenarios, such as long-distance or low-albedo objects,\nproducing sparse or erroneous point clouds. These errors, which are rooted in\nthe noisy raw LiDAR measurements, get propagated to downstream perception\nmodels, resulting in potentially severe loss of accuracy. This is because\nconventional 3D processing pipelines do not retain any uncertainty information\nfrom the raw measurements when constructing point clouds.\n  We propose Probabilistic Point Clouds (PPC), a novel 3D scene representation\nwhere each point is augmented with a probability attribute that encapsulates\nthe measurement uncertainty (or confidence) in the raw data. We further\nintroduce inference approaches that leverage PPC for robust 3D object\ndetection; these methods are versatile and can be used as computationally\nlightweight drop-in modules in 3D inference pipelines. We demonstrate, via both\nsimulations and real captures, that PPC-based 3D inference methods outperform\nseveral baselines using LiDAR as well as camera-LiDAR fusion models, across\nchallenging indoor and outdoor scenarios involving small, distant, and\nlow-albedo objects, as well as strong ambient light.\n  Our project webpage is at https://bhavyagoyal.github.io/ppc ."
    },
    {
        "date": "2025-07",
        "title": "Hyperproperty-Constrained Secure Reinforcement Learning",
        "author": "Ernest Bonnah, Luan Viet Nguyen, and Khaza Anuarul Hoque",
        "link": "http://arxiv.org/abs/2508.00106v1",
        "abstract": "Hyperproperties for Time Window Temporal Logic (HyperTWTL) is a\ndomain-specific formal specification language known for its effectiveness in\ncompactly representing security, opacity, and concurrency properties for\nrobotics applications. This paper focuses on HyperTWTL-constrained secure\nreinforcement learning (SecRL). Although temporal logic-constrained safe\nreinforcement learning (SRL) is an evolving research problem with several\nexisting literature, there is a significant research gap in exploring\nsecurity-aware reinforcement learning (RL) using hyperproperties. Given the\ndynamics of an agent as a Markov Decision Process (MDP) and opacity/security\nconstraints formalized as HyperTWTL, we propose an approach for learning\nsecurity-aware optimal policies using dynamic Boltzmann softmax RL while\nsatisfying the HyperTWTL constraints. The effectiveness and scalability of our\nproposed approach are demonstrated using a pick-up and delivery robotic mission\ncase study. We also compare our results with two other baseline RL algorithms,\nshowing that our proposed method outperforms them."
    },
    {
        "date": "2025-07",
        "title": "Riemannian Optimization for Distance Geometry: A Study of Convergence, Robustness, and Incoherence",
        "author": "Chandler Smith, HanQin Cai, and Abiy Tasissa",
        "link": "http://arxiv.org/abs/2508.00091v1",
        "abstract": "The problem of recovering a configuration of points from partial pairwise\ndistances, referred to as the Euclidean Distance Geometry (EDG) problem, arises\nin a broad range of applications, including sensor network localization,\nmolecular conformation, and manifold learning. In this paper, we propose a\nRiemannian optimization framework for solving the EDG problem by formulating it\nas a low-rank matrix completion task over the space of positive semi-definite\nGram matrices. The available distance measurements are encoded as expansion\ncoefficients in a non-orthogonal basis, and optimization over the Gram matrix\nimplicitly enforces geometric consistency through the triangle inequality, a\nstructure inherited from classical multidimensional scaling. Under a Bernoulli\nsampling model for observed distances, we prove that Riemannian gradient\ndescent on the manifold of rank-$r$ matrices locally converges linearly with\nhigh probability when the sampling probability satisfies $p \\geq\n\\mathcal{O}(\\nu^2 r^2 \\log(n)/n)$, where $\\nu$ is an EDG-specific incoherence\nparameter. Furthermore, we provide an initialization candidate using a one-step\nhard thresholding procedure that yields convergence, provided the sampling\nprobability satisfies $p \\geq \\mathcal{O}(\\nu r^{3/2} \\log^{3/4}(n)/n^{1/4})$.\nA key technical contribution of this work is the analysis of a symmetric linear\noperator arising from a dual basis expansion in the non-orthogonal basis, which\nrequires a novel application of the Hanson--Wright inequality to establish an\noptimal restricted isometry property in the presence of coupled terms.\nEmpirical evaluations on synthetic data demonstrate that our algorithm achieves\ncompetitive performance relative to state-of-the-art methods. Moreover, we\npropose a novel notion of matrix incoherence tailored to the EDG setting and\nprovide robustness guarantees for our method."
    },
    {
        "date": "2025-07",
        "title": "DiffuMatch: Category-Agnostic Spectral Diffusion Priors for Robust Non-rigid Shape Matching",
        "author": "Emery Pierson, Lei Li, Angela Dai, and Maks Ovsjanikov",
        "link": "http://arxiv.org/abs/2507.23715v1",
        "abstract": "Deep functional maps have recently emerged as a powerful tool for solving\nnon-rigid shape correspondence tasks. Methods that use this approach combine\nthe power and flexibility of the functional map framework, with data-driven\nlearning for improved accuracy and generality. However, most existing methods\nin this area restrict the learning aspect only to the feature functions and\nstill rely on axiomatic modeling for formulating the training loss or for\nfunctional map regularization inside the networks. This limits both the\naccuracy and the applicability of the resulting approaches only to scenarios\nwhere assumptions of the axiomatic models hold. In this work, we show, for the\nfirst time, that both in-network regularization and functional map training can\nbe replaced with data-driven methods. For this, we first train a generative\nmodel of functional maps in the spectral domain using score-based generative\nmodeling, built from a large collection of high-quality maps. We then exploit\nthe resulting model to promote the structural properties of ground truth\nfunctional maps on new shape collections. Remarkably, we demonstrate that the\nlearned models are category-agnostic, and can fully replace commonly used\nstrategies such as enforcing Laplacian commutativity or orthogonality of\nfunctional maps. Our key technical contribution is a novel distillation\nstrategy from diffusion models in the spectral domain. Experiments demonstrate\nthat our learned regularization leads to better results than axiomatic\napproaches for zero-shot non-rigid shape matching. Our code is available at:\nhttps://github.com/daidedou/diffumatch/"
    },
    {
        "date": "2025-07",
        "title": "OptiGradTrust: Byzantine-Robust Federated Learning with Multi-Feature Gradient Analysis and Reinforcement Learning-Based Trust Weighting",
        "author": "Mohammad Karami, Fatemeh Ghassemi, Hamed Kebriaei, and Hamid Azadegan",
        "link": "http://arxiv.org/abs/2507.23638v1",
        "abstract": "Federated Learning (FL) enables collaborative model training across\ndistributed medical institutions while preserving patient privacy, but remains\nvulnerable to Byzantine attacks and statistical heterogeneity. We present\nOptiGradTrust, a comprehensive defense framework that evaluates gradient\nupdates through a novel six-dimensional fingerprint including VAE\nreconstruction error, cosine similarity metrics, $L_2$ norm, sign-consistency\nratio, and Monte Carlo Shapley value, which drive a hybrid RL-attention module\nfor adaptive trust scoring. To address convergence challenges under data\nheterogeneity, we develop FedBN-Prox (FedBN-P), combining Federated Batch\nNormalization with proximal regularization for optimal accuracy-convergence\ntrade-offs. Extensive evaluation across MNIST, CIFAR-10, and Alzheimer's MRI\ndatasets under various Byzantine attack scenarios demonstrates significant\nimprovements over state-of-the-art defenses, achieving up to +1.6 percentage\npoints over FLGuard under non-IID conditions while maintaining robust\nperformance against diverse attack patterns through our adaptive learning\napproach."
    },
    {
        "date": "2025-07",
        "title": "Improved Robustness and Functional Localization in Topographic CNNs Through Weight Similarity",
        "author": "Nhut Truong, and Uri Hasson",
        "link": "http://arxiv.org/abs/2508.00043v1",
        "abstract": "Topographic neural networks are computational models that can simulate the\nspatial and functional organization of the brain. Topographic constraints in\nneural networks can be implemented in multiple ways, with potentially different\nimpacts on the representations learned by the network. The impact of such\ndifferent implementations has not been systematically examined. To this end,\nhere we compare topographic convolutional neural networks trained with two\nspatial constraints: Weight Similarity (WS), which pushes neighboring units to\ndevelop similar incoming weights, and Activation Similarity (AS), which\nenforces similarity in unit activations. We evaluate the resulting models on\nclassification accuracy, robustness to weight perturbations and input\ndegradation, and the spatial organization of learned representations. Compared\nto both AS and standard CNNs, WS provided three main advantages: i) improved\nrobustness to noise, also showing higher accuracy under weight corruption; ii)\ngreater input sensitivity, reflected in higher activation variance; and iii)\nstronger functional localization, with units showing similar activations\npositioned at closer distances. In addition, WS produced differences in\norientation tuning, symmetry sensitivity, and eccentricity profiles of units,\nindicating an influence of this spatial constraint on the representational\ngeometry of the network. Our findings suggest that during end-to-end training,\nWS constraints produce more robust representations than AS or non-topographic\nCNNs. These findings also suggest that weight-based spatial constraints can\nshape feature learning and functional organization in biophysical inspired\nmodels."
    },
    {
        "date": "2025-07",
        "title": "Role-Aware Language Models for Secure and Contextualized Access Control in Organizations",
        "author": "Saeed Almheiri, Yerulan Kongrat, Adrian Santosh, Ruslan Tasmukhanov, Josemaria Vera, Muhammad Dehan Al Kautsar, and Fajri Koto",
        "link": "http://arxiv.org/abs/2507.23465v1",
        "abstract": "As large language models (LLMs) are increasingly deployed in enterprise\nsettings, controlling model behavior based on user roles becomes an essential\nrequirement. Existing safety methods typically assume uniform access and focus\non preventing harmful or toxic outputs, without addressing role-specific access\nconstraints. In this work, we investigate whether LLMs can be fine-tuned to\ngenerate responses that reflect the access privileges associated with different\norganizational roles. We explore three modeling strategies: a BERT-based\nclassifier, an LLM-based classifier, and role-conditioned generation. To\nevaluate these approaches, we construct two complementary datasets. The first\nis adapted from existing instruction-tuning corpora through clustering and role\nlabeling, while the second is synthetically generated to reflect realistic,\nrole-sensitive enterprise scenarios. We assess model performance across varying\norganizational structures and analyze robustness to prompt injection, role\nmismatch, and jailbreak attempts."
    },
    {
        "date": "2025-07",
        "title": "Counterfactual Evaluation for Blind Attack Detection in LLM-based Evaluation Systems",
        "author": "Lijia Liu, Takumi Kondo, Kyohei Atarashi, Koh Takeuchi, Jiyi Li, Shigeru Saito, and Hisashi Kashima",
        "link": "http://arxiv.org/abs/2507.23453v1",
        "abstract": "This paper investigates defenses for LLM-based evaluation systems against\nprompt injection. We formalize a class of threats called blind attacks, where a\ncandidate answer is crafted independently of the true answer to deceive the\nevaluator. To counter such attacks, we propose a framework that augments\nStandard Evaluation (SE) with Counterfactual Evaluation (CFE), which\nre-evaluates the submission against a deliberately false ground-truth answer.\nAn attack is detected if the system validates an answer under both standard and\ncounterfactual conditions. Experiments show that while standard evaluation is\nhighly vulnerable, our SE+CFE framework significantly improves security by\nboosting attack detection with minimal performance trade-offs."
    },
    {
        "date": "2025-07",
        "title": "Scalable and Precise Patch Robustness Certification for Deep Learning Models with Top-k Predictions",
        "author": "Qilin Zhou, Haipeng Wang, Zhengyuan Wei, and W. K. Chan",
        "link": "http://arxiv.org/abs/2507.23335v1",
        "abstract": "Patch robustness certification is an emerging verification approach for\ndefending against adversarial patch attacks with provable guarantees for deep\nlearning systems. Certified recovery techniques guarantee the prediction of the\nsole true label of a certified sample. However, existing techniques, if\napplicable to top-k predictions, commonly conduct pairwise comparisons on those\nvotes between labels, failing to certify the sole true label within the top k\nprediction labels precisely due to the inflation on the number of votes\ncontrolled by the attacker (i.e., attack budget); yet enumerating all\ncombinations of vote allocation suffers from the combinatorial explosion\nproblem. We propose CostCert, a novel, scalable, and precise voting-based\ncertified recovery defender. CostCert verifies the true label of a sample\nwithin the top k predictions without pairwise comparisons and combinatorial\nexplosion through a novel design: whether the attack budget on the sample is\ninfeasible to cover the smallest total additional votes on top of the votes\nuncontrollable by the attacker to exclude the true labels from the top k\nprediction labels. Experiments show that CostCert significantly outperforms the\ncurrent state-of-the-art defender PatchGuard, such as retaining up to 57.3% in\ncertified accuracy when the patch size is 96, whereas PatchGuard has already\ndropped to zero."
    },
    {
        "date": "2025-07",
        "title": "PriorFusion: Unified Integration of Priors for Robust Road Perception in Autonomous Driving",
        "author": "Xuewei Tang, Mengmeng Yang, Tuopu Wen, Peijin Jia, Le Cui, Mingshang Luo, Kehua Sheng, Bo Zhang, Diange Yang, and Kun Jiang",
        "link": "http://arxiv.org/abs/2507.23309v2",
        "abstract": "With the growing interest in autonomous driving, there is an increasing\ndemand for accurate and reliable road perception technologies. In complex\nenvironments without high-definition map support, autonomous vehicles must\nindependently interpret their surroundings to ensure safe and robust\ndecision-making. However, these scenarios pose significant challenges due to\nthe large number, complex geometries, and frequent occlusions of road elements.\nA key limitation of existing approaches lies in their insufficient exploitation\nof the structured priors inherently present in road elements, resulting in\nirregular, inaccurate predictions. To address this, we propose PriorFusion, a\nunified framework that effectively integrates semantic, geometric, and\ngenerative priors to enhance road element perception. We introduce an\ninstance-aware attention mechanism guided by shape-prior features, then\nconstruct a data-driven shape template space that encodes low-dimensional\nrepresentations of road elements, enabling clustering to generate anchor points\nas reference priors. We design a diffusion-based framework that leverages these\nprior anchors to generate accurate and complete predictions. Experiments on\nlarge-scale autonomous driving datasets demonstrate that our method\nsignificantly improves perception accuracy, particularly under challenging\nconditions. Visualization results further confirm that our approach produces\nmore accurate, regular, and coherent predictions of road elements."
    },
    {
        "date": "2025-07",
        "title": "Adversarial-Guided Diffusion for Multimodal LLM Attacks",
        "author": "Chengwei Xia, Fan Ma, Ruijie Quan, Kun Zhan, and Yi Yang",
        "link": "http://arxiv.org/abs/2507.23202v1",
        "abstract": "This paper addresses the challenge of generating adversarial image using a\ndiffusion model to deceive multimodal large language models (MLLMs) into\ngenerating the targeted responses, while avoiding significant distortion of the\nclean image. To address the above challenges, we propose an adversarial-guided\ndiffusion (AGD) approach for adversarial attack MLLMs. We introduce\nadversarial-guided noise to ensure attack efficacy. A key observation in our\ndesign is that, unlike most traditional adversarial attacks which embed\nhigh-frequency perturbations directly into the clean image, AGD injects target\nsemantics into the noise component of the reverse diffusion. Since the added\nnoise in a diffusion model spans the entire frequency spectrum, the adversarial\nsignal embedded within it also inherits this full-spectrum property.\nImportantly, during reverse diffusion, the adversarial image is formed as a\nlinear combination of the clean image and the noise. Thus, when applying\ndefenses such as a simple low-pass filtering, which act independently on each\ncomponent, the adversarial image within the noise component is less likely to\nbe suppressed, as it is not confined to the high-frequency band. This makes AGD\ninherently robust to variety defenses. Extensive experiments demonstrate that\nour AGD outperforms state-of-the-art methods in attack performance as well as\nin model robustness to some defenses."
    },
    {
        "date": "2025-07",
        "title": "A Novel Dataset for Flood Detection Robust to Seasonal Changes in Satellite Imagery",
        "author": "Youngsun Jang, Dongyoun Kim, Chulwoo Pack, and Kwanghee Won",
        "link": "http://arxiv.org/abs/2507.23193v1",
        "abstract": "This study introduces a novel dataset for segmenting flooded areas in\nsatellite images. After reviewing 77 existing benchmarks utilizing satellite\nimagery, we identified a shortage of suitable datasets for this specific task.\nTo fill this gap, we collected satellite imagery of the 2019 Midwestern USA\nfloods from Planet Explorer by Planet Labs (Image \\c{opyright} 2024 Planet Labs\nPBC). The dataset consists of 10 satellite images per location, each containing\nboth flooded and non-flooded areas. We selected ten locations from each of the\nfive states: Iowa, Kansas, Montana, Nebraska, and South Dakota. The dataset\nensures uniform resolution and resizing during data processing. For evaluating\nsemantic segmentation performance, we tested state-of-the-art models in\ncomputer vision and remote sensing on our dataset. Additionally, we conducted\nan ablation study varying window sizes to capture temporal characteristics.\nOverall, the models demonstrated modest results, suggesting a requirement for\nfuture multimodal and temporal learning strategies. The dataset will be\npublicly available on\n<https://github.com/youngsunjang/SDSU_MidWest_Flood_2019>."
    },
    {
        "date": "2025-07",
        "title": "Evaluating and Improving the Robustness of Speech Command Recognition Models to Noise and Distribution Shifts",
        "author": "Ana\u00efs Baranger, and Lucas Maison",
        "link": "http://arxiv.org/abs/2507.23128v1",
        "abstract": "Although prior work in computer vision has shown strong correlations between\nin-distribution (ID) and out-of-distribution (OOD) accuracies, such\nrelationships remain underexplored in audio-based models. In this study, we\ninvestigate how training conditions and input features affect the robustness\nand generalization abilities of spoken keyword classifiers under OOD\nconditions. We benchmark several neural architectures across a variety of\nevaluation sets. To quantify the impact of noise on generalization, we make use\nof two metrics: Fairness (F), which measures overall accuracy gains compared to\na baseline model, and Robustness (R), which assesses the convergence between ID\nand OOD performance. Our results suggest that noise-aware training improves\nrobustness in some configurations. These findings shed new light on the\nbenefits and limitations of noise-based augmentation for generalization in\nspeech models."
    },
    {
        "date": "2025-07",
        "title": "Robust and Efficient 3D Gaussian Splatting for Urban Scene Reconstruction",
        "author": "Zhensheng Yuan, Haozhi Huang, Zhen Xiong, Di Wang, and Guanghua Yang",
        "link": "http://arxiv.org/abs/2507.23006v1",
        "abstract": "We present a framework that enables fast reconstruction and real-time\nrendering of urban-scale scenes while maintaining robustness against appearance\nvariations across multi-view captures. Our approach begins with scene\npartitioning for parallel training, employing a visibility-based image\nselection strategy to optimize training efficiency. A controllable\nlevel-of-detail (LOD) strategy explicitly regulates Gaussian density under a\nuser-defined budget, enabling efficient training and rendering while\nmaintaining high visual fidelity. The appearance transformation module\nmitigates the negative effects of appearance inconsistencies across images\nwhile enabling flexible adjustments. Additionally, we utilize enhancement\nmodules, such as depth regularization, scale regularization, and antialiasing,\nto improve reconstruction fidelity. Experimental results demonstrate that our\nmethod effectively reconstructs urban-scale scenes and outperforms previous\napproaches in both efficiency and quality. The source code is available at:\nhttps://yzslab.github.io/REUrbanGS."
    },
    {
        "date": "2025-07",
        "title": "RLVMR: Reinforcement Learning with Verifiable Meta-Reasoning Rewards for Robust Long-Horizon Agents",
        "author": "Zijing Zhang, Ziyang Chen, Mingxiao Li, Zhaopeng Tu, and Xiaolong Li",
        "link": "http://arxiv.org/abs/2507.22844v1",
        "abstract": "The development of autonomous agents for complex, long-horizon tasks is a\ncentral goal in AI. However, dominant training paradigms face a critical\nlimitation: reinforcement learning (RL) methods that optimize solely for final\ntask success often reinforce flawed or inefficient reasoning paths, a problem\nwe term inefficient exploration. This leads to agents that are brittle and fail\nto generalize, as they learn to find solutions without learning how to reason\ncoherently. To address this, we introduce RLVMR, a novel framework that\nintegrates dense, process-level supervision into end-to-end RL by rewarding\nverifiable, meta-reasoning behaviors. RLVMR equips an agent to explicitly tag\nits cognitive steps, such as planning, exploration, and reflection, and\nprovides programmatic, rule-based rewards for actions that contribute to\neffective problem-solving. These process-centric rewards are combined with the\nfinal outcome signal and optimized using a critic-free policy gradient method.\nOn the challenging ALFWorld and ScienceWorld benchmarks, RLVMR achieves new\nstate-of-the-art results, with our 7B model reaching an 83.6% success rate on\nthe most difficult unseen task split. Our analysis confirms these gains stem\nfrom improved reasoning quality, including significant reductions in redundant\nactions and enhanced error recovery, leading to more robust, efficient, and\ninterpretable agents."
    },
    {
        "date": "2025-07",
        "title": "CapRecover: A Cross-Modality Feature Inversion Attack Framework on Vision Language Models",
        "author": "Kedong Xiu, and Sai Qian Zhang",
        "link": "http://arxiv.org/abs/2507.22828v2",
        "abstract": "As Vision-Language Models (VLMs) are increasingly deployed in split-DNN\nconfigurations--with visual encoders (e.g., ResNet, ViT) operating on user\ndevices and sending intermediate features to the cloud--there is a growing\nprivacy risk from semantic information leakage. Existing approaches to\nreconstructing images from these intermediate features often result in blurry,\nsemantically ambiguous images. To directly address semantic leakage, we propose\nCapRecover, a cross-modality inversion framework that recovers high-level\nsemantic content, such as labels or captions, directly from intermediate\nfeatures without image reconstruction.\n  We evaluate CapRecover on multiple datasets and victim models, demonstrating\nstrong performance in semantic recovery. Specifically, CapRecover achieves up\nto 92.71% Top-1 label accuracy on CIFAR-10 and generates fluent captions from\nResNet50 features on COCO2017 with ROUGE-L scores up to 0.52. Our analysis\nfurther reveals that deeper convolutional layers encode significantly more\nsemantic information compared to shallow layers. To mitigate semantic leakage,\nwe introduce a simple yet effective protection method: adding random noise to\nintermediate features at each layer and removing the noise in the next layer.\nExperimental results show that this approach prevents semantic leakage without\nadditional training costs."
    },
    {
        "date": "2025-07",
        "title": "DoS Attacks and Defense Technologies in Blockchain Systems: A Hierarchical Analysis",
        "author": "Chunyi Zhang, Fengjiao Dou, and Xiaoqi Li",
        "link": "http://arxiv.org/abs/2507.22611v1",
        "abstract": "Blockchain technology is widely used in various fields due to its ability to\nprovide decentralization and trustless security. This is a fundamental\nunderstanding held by many advocates, but it is misunderstood, leading\nparticipants to fail to recognize the limitations of the security that\nblockchain can provide. Among all current network attacks, Denial of Service\n(DoS) attacks pose significant threats due to their ease of execution and\ndestructive potential. This paper, based on the blockchain architecture\nhierarchy, categorizes and organizes existing DoS attacks, with a focus on\nexplaining the principles and methods of contract layer and consensus layer DoS\nattacks. Furthermore, this paper comprehensively analyzes and compares commonly\nused detection methods and defense technologies, which will contribute to\nstrengthening the security and stability of blockchain systems and promoting\nfurther innovation and application of blockchain systems."
    },
    {
        "date": "2025-07",
        "title": "Robust Deepfake Detection for Electronic Know Your Customer Systems Using Registered Images",
        "author": "Takuma Amada, Kazuya Kakizaki, Taiki Miyagawa, Akinori F. Ebihara, Kaede Shiohara, and Toshihiko Yamasaki",
        "link": "http://arxiv.org/abs/2507.22601v1",
        "abstract": "In this paper, we present a deepfake detection algorithm specifically\ndesigned for electronic Know Your Customer (eKYC) systems. To ensure the\nreliability of eKYC systems against deepfake attacks, it is essential to\ndevelop a robust deepfake detector capable of identifying both face swapping\nand face reenactment, while also being robust to image degradation. We address\nthese challenges through three key contributions: (1)~Our approach evaluates\nthe video's authenticity by detecting temporal inconsistencies in identity\nvectors extracted by face recognition models, leading to comprehensive\ndetection of both face swapping and face reenactment. (2)~In addition to\nprocessing video input, the algorithm utilizes a registered image (assumed to\nbe genuine) to calculate identity discrepancies between the input video and the\nregistered image, significantly improving detection accuracy. (3)~We find that\nemploying a face feature extractor trained on a larger dataset enhances both\ndetection performance and robustness against image degradation. Our\nexperimental results show that our proposed method accurately detects both face\nswapping and face reenactment comprehensively and is robust against various\nforms of unseen image degradation. Our source code is publicly available\nhttps://github.com/TaikiMiyagawa/DeepfakeDetection4eKYC."
    },
    {
        "date": "2025-07",
        "title": "Robust Adverse Weather Removal via Spectral-based Spatial Grouping",
        "author": "Yuhwan Jeong, Yunseo Yang, Youngho Yoon, and Kuk-Jin Yoon",
        "link": "http://arxiv.org/abs/2507.22498v2",
        "abstract": "Adverse weather conditions cause diverse and complex degradation patterns,\ndriving the development of All-in-One (AiO) models. However, recent AiO\nsolutions still struggle to capture diverse degradations, since global\nfiltering methods like direct operations on the frequency domain fail to handle\nhighly variable and localized distortions. To address these issue, we propose\nSpectral-based Spatial Grouping Transformer (SSGformer), a novel approach that\nleverages spectral decomposition and group-wise attention for multi-weather\nimage restoration. SSGformer decomposes images into high-frequency edge\nfeatures using conventional edge detection and low-frequency information via\nSingular Value Decomposition. We utilize multi-head linear attention to\neffectively model the relationship between these features. The fused features\nare integrated with the input to generate a grouping-mask that clusters regions\nbased on the spatial similarity and image texture. To fully leverage this mask,\nwe introduce a group-wise attention mechanism, enabling robust adverse weather\nremoval and ensuring consistent performance across diverse weather conditions.\nWe also propose a Spatial Grouping Transformer Block that uses both channel\nattention and spatial attention, effectively balancing feature-wise\nrelationships and spatial dependencies. Extensive experiments show the\nsuperiority of our approach, validating its effectiveness in handling the\nvaried and intricate adverse weather degradations."
    },
    {
        "date": "2025-07",
        "title": "RANA: Robust Active Learning for Noisy Network Alignment",
        "author": "Yixuan Nan, Xixun Lin, Yanmin Shang, Zhuofan Li, Can Zhao, and Yanan Cao",
        "link": "http://arxiv.org/abs/2507.22434v1",
        "abstract": "Network alignment has attracted widespread attention in various fields.\nHowever, most existing works mainly focus on the problem of label sparsity,\nwhile overlooking the issue of noise in network alignment, which can\nsubstantially undermine model performance. Such noise mainly includes\nstructural noise from noisy edges and labeling noise caused by human-induced\nand process-driven errors. To address these problems, we propose RANA, a Robust\nActive learning framework for noisy Network Alignment. RANA effectively tackles\nboth structure noise and label noise while addressing the sparsity of anchor\nlink annotations, which can improve the robustness of network alignment models.\nSpecifically, RANA introduces the proposed Noise-aware Selection Module and the\nLabel Denoising Module to address structural noise and labeling noise,\nrespectively. In the first module, we design a noise-aware maximization\nobjective to select node pairs, incorporating a cleanliness score to address\nstructural noise. In the second module, we propose a novel multi-source fusion\ndenoising strategy that leverages model and twin node pairs labeling to provide\nmore accurate labels for node pairs. Empirical results on three real-world\ndatasets demonstrate that RANA outperforms state-of-the-art active\nlearning-based methods in alignment accuracy. Our code is available at\nhttps://github.com/YXNan0110/RANA."
    },
    {
        "date": "2025-07",
        "title": "Theoretical Analysis of Relative Errors in Gradient Computations for Adversarial Attacks with CE Loss",
        "author": "Yunrui Yu, Hang Su, Cheng-zhong Xu, Zhizhong Su, and Jun Zhu",
        "link": "http://arxiv.org/abs/2507.22428v1",
        "abstract": "Gradient-based adversarial attacks using the Cross-Entropy (CE) loss often\nsuffer from overestimation due to relative errors in gradient computation\ninduced by floating-point arithmetic. This paper provides a rigorous\ntheoretical analysis of these errors, conducting the first comprehensive study\nof floating-point computation errors in gradient-based attacks across four\ndistinct scenarios: (i) unsuccessful untargeted attacks, (ii) successful\nuntargeted attacks, (iii) unsuccessful targeted attacks, and (iv) successful\ntargeted attacks. We establish theoretical foundations characterizing the\nbehavior of relative numerical errors under different attack conditions,\nrevealing previously unknown patterns in gradient computation instability, and\nidentify floating-point underflow and rounding as key contributors. Building on\nthis insight, we propose the Theoretical MIFPE (T-MIFPE) loss function, which\nincorporates an optimal scaling factor $T = t^*$ to minimize the impact of\nfloating-point errors, thereby enhancing the accuracy of gradient computation\nin adversarial attacks. Extensive experiments on the MNIST, CIFAR-10, and\nCIFAR-100 datasets demonstrate that T-MIFPE outperforms existing loss\nfunctions, including CE, C\\&W, DLR, and MIFPE, in terms of attack potency and\nrobustness evaluation accuracy."
    },
    {
        "date": "2025-07",
        "title": "On the Reliability of Vision-Language Models Under Adversarial Frequency-Domain Perturbations",
        "author": "Jordan Vice, Naveed Akhtar, Yansong Gao, Richard Hartley, and Ajmal Mian",
        "link": "http://arxiv.org/abs/2507.22398v1",
        "abstract": "Vision-Language Models (VLMs) are increasingly used as perceptual modules for\nvisual content reasoning, including through captioning and DeepFake detection.\nIn this work, we expose a critical vulnerability of VLMs when exposed to\nsubtle, structured perturbations in the frequency domain. Specifically, we\nhighlight how these feature transformations undermine authenticity/DeepFake\ndetection and automated image captioning tasks. We design targeted image\ntransformations, operating in the frequency domain to systematically adjust VLM\noutputs when exposed to frequency-perturbed real and synthetic images. We\ndemonstrate that the perturbation injection method generalizes across five\nstate-of-the-art VLMs which includes different-parameter Qwen2/2.5 and BLIP\nmodels. Experimenting across ten real and generated image datasets reveals that\nVLM judgments are sensitive to frequency-based cues and may not wholly align\nwith semantic content. Crucially, we show that visually-imperceptible spatial\nfrequency transformations expose the fragility of VLMs deployed for automated\nimage captioning and authenticity detection tasks. Our findings under\nrealistic, black-box constraints challenge the reliability of VLMs,\nunderscoring the need for robust multimodal perception systems."
    },
    {
        "date": "2025-07",
        "title": "Robust Filtering and Learning in State-Space Models: Skewness and Heavy Tails Via Asymmetric Laplace Distribution",
        "author": "Yifan Yu, Shengjie Xiu, and Daniel P. Palomar",
        "link": "http://arxiv.org/abs/2507.22343v1",
        "abstract": "State-space models are pivotal for dynamic system analysis but often struggle\nwith outlier data that deviates from Gaussian distributions, frequently\nexhibiting skewness and heavy tails. This paper introduces a robust extension\nutilizing the asymmetric Laplace distribution, specifically tailored to capture\nthese complex characteristics. We propose an efficient variational Bayes\nalgorithm and a novel single-loop parameter estimation strategy, significantly\nenhancing the efficiency of the filtering, smoothing, and parameter estimation\nprocesses. Our comprehensive experiments demonstrate that our methods provide\nconsistently robust performance across various noise settings without the need\nfor manual hyperparameter adjustments. In stark contrast, existing models\ngenerally rely on specific noise conditions and necessitate extensive manual\ntuning. Moreover, our approach uses far fewer computational resources, thereby\nvalidating the model's effectiveness and underscoring its potential for\npractical applications in fields such as robust control and financial modeling."
    },
    {
        "date": "2025-07",
        "title": "SleepWalk: Exploiting Context Switching and Residual Power for Physical Side-Channel Attacks",
        "author": "Sahan Sanjaya, Aruna Jayasena, and Prabhat Mishra",
        "link": "http://arxiv.org/abs/2507.22306v1",
        "abstract": "Context switching is utilized by operating systems to change the execution\ncontext between application programs. It involves saving and restoring the\nstates of multiple registers and performing a pipeline flush to remove any\npre-fetched instructions, leading to a higher instantaneous power consumption\ncompared to typical program execution. In this paper, we introduce a physical\npower side-channel leakage source that exploits the power spike observed during\na context switch, triggered by the inbuilt sleep function of the system kernel.\nWe observed that this power spike directly correlates with both the power\nconsumption during context switching and the residual power consumption of the\npreviously executed program. Notably, the persistence of residual power\nsignatures from previous workloads extends the scope of this side-channel\nbeyond extracting the data in registers during the context switch. Unlike\ntraditional approaches that require analyzing full power traces, applying\ncomplex preprocessing, or relying on external synchronization triggers, this\nnovel technique leverages only the amplitude of a single power spike,\nsignificantly simplifying the attack. We developed a power model to illustrate\nthe feasibility of mounting end-to-end side-channel attacks using the\nsleep-induced power spikes. Experimental evaluation demonstrates that our\nframework can successfully perform cryptographic key recovery for both AES and\nSIKE implementations on Broadcom BCM2711."
    },
    {
        "date": "2025-07",
        "title": "CS-SHRED: Enhancing SHRED for Robust Recovery of Spatiotemporal Dynamics",
        "author": "Romulo B. da Silva, Diego Passos, C\u00e1ssio M. Oishi, and J. Nathan Kutz",
        "link": "http://arxiv.org/abs/2507.22303v2",
        "abstract": "We present CS-SHRED, a novel deep learning architecture that integrates\nCompressed Sensing (CS) into a Shallow Recurrent Decoder (SHRED) to reconstruct\nspatiotemporal dynamics from incomplete, compressed, or corrupted data. Our\napproach introduces two key innovations. First, by incorporating CS techniques\ninto the SHRED architecture, our method leverages a batch-based forward\nframework with $\\ell_1$ regularization to robustly recover signals even in\nscenarios with sparse sensor placements, noisy measurements, and incomplete\nsensor acquisitions. Second, an adaptive loss function dynamically combines\nMean Squared Error (MSE) and Mean Absolute Error (MAE) terms with a piecewise\nSignal-to-Noise Ratio (SNR) regularization, which suppresses noise and outliers\nin low-SNR regions while preserving fine-scale features in high-SNR regions.\n  We validate CS-SHRED on challenging problems including viscoelastic fluid\nflows, maximum specific humidity fields, sea surface temperature distributions,\nand rotating turbulent flows. Compared to the traditional SHRED approach,\nCS-SHRED achieves significantly higher reconstruction fidelity -- as\ndemonstrated by improved SSIM and PSNR values, lower normalized errors, and\nenhanced LPIPS scores-thereby providing superior preservation of small-scale\nstructures and increased robustness against noise and outliers.\n  Our results underscore the advantages of the jointly trained CS and SHRED\ndesign architecture which includes an LSTM sequence model for characterizing\nthe temporal evolution with a shallow decoder network (SDN) for modeling the\nhigh-dimensional state space. The SNR-guided adaptive loss function for the\nspatiotemporal data recovery establishes CS-SHRED as a promising tool for a\nwide range of applications in environmental, climatic, and scientific data\nanalyses."
    },
    {
        "date": "2025-07",
        "title": "Programmable Data Planes for Network Security",
        "author": "Gursimran Singh, H. B. Acharya, and Minseok Kwon",
        "link": "http://arxiv.org/abs/2507.22165v1",
        "abstract": "The emergence of programmable data planes, and particularly switches\nsupporting the P4 language, has transformed network security by enabling\ncustomized, line-rate packet processing. These switches, originally intended\nfor flexible forwarding, now play a broader role: detecting and mitigating\nattacks such as DDoS and spoofing, enforcing next-generation firewall policies,\nand even supporting in-network cryptography and machine learning. These\ncapabilities are made possible by techniques such as recirculate-and-truncate\nand lookup-table precomputation, which work around architectural constraints\nlike limited memory and restricted instruction sets. In this paper, we\nsystematize recent advances in security applications built on programmable\nswitches, with an emphasis on the capabilities, challenges, and architectural\nworkarounds. We highlight the non-obvious design techniques that make complex\nin-network security functions feasible despite the constraints of the hardware\nplatform, and also comment on remaining issues and emerging research\ndirections."
    },
    {
        "date": "2025-07",
        "title": "Tiny Noise-Robust Voice Activity Detector for Voice Assistants",
        "author": "Hamed Jafarzadeh Asl, Mahsa Ghazvini Nejad, Amin Edraki, Masoud Asgharian, and Vahid Partovi Nia",
        "link": "http://arxiv.org/abs/2507.22157v1",
        "abstract": "Voice Activity Detection (VAD) in the presence of background noise remains a\nchallenging problem in speech processing. Accurate VAD is essential in\nautomatic speech recognition, voice-to-text, conversational agents, etc, where\nnoise can severely degrade the performance. A modern application includes the\nvoice assistant, specially mounted on Artificial Intelligence of Things (AIoT)\ndevices such as cell phones, smart glasses, earbuds, etc, where the voice\nsignal includes background noise. Therefore, VAD modules must remain\nlight-weight due to their practical on-device limitation. The existing models\noften struggle with low signal-to-noise ratios across diverse acoustic\nenvironments. A simple VAD often detects human voice in a clean environment,\nbut struggles to detect the human voice in noisy conditions. We propose a\nnoise-robust VAD that comprises a light-weight VAD, with data pre-processing\nand post-processing added modules to handle the background noise. This approach\nsignificantly enhances the VAD accuracy in noisy environments and requires\nneither a larger model, nor fine-tuning. Experimental results demonstrate that\nour approach achieves a notable improvement compared to baselines, particularly\nin environments with high background noise interference. This modified VAD\nadditionally improving clean speech detection."
    },
    {
        "date": "2025-07",
        "title": "Secure Tug-of-War (SecTOW): Iterative Defense-Attack Training with Reinforcement Learning for Multimodal Model Security",
        "author": "Muzhi Dai, Shixuan Liu, Zhiyuan Zhao, Junyu Gao, Hao Sun, and Xuelong Li",
        "link": "http://arxiv.org/abs/2507.22037v1",
        "abstract": "The rapid advancement of multimodal large language models (MLLMs) has led to\nbreakthroughs in various applications, yet their security remains a critical\nchallenge. One pressing issue involves unsafe image-query pairs--jailbreak\ninputs specifically designed to bypass security constraints and elicit\nunintended responses from MLLMs. Compared to general multimodal data, such\nunsafe inputs are relatively sparse, which limits the diversity and richness of\ntraining samples available for developing robust defense models. Meanwhile,\nexisting guardrail-type methods rely on external modules to enforce security\nconstraints but fail to address intrinsic vulnerabilities within MLLMs.\nTraditional supervised fine-tuning (SFT), on the other hand, often over-refuses\nharmless inputs, compromising general performance. Given these challenges, we\npropose Secure Tug-of-War (SecTOW), an innovative iterative defense-attack\ntraining method to enhance the security of MLLMs. SecTOW consists of two\nmodules: a defender and an auxiliary attacker, both trained iteratively using\nreinforcement learning (GRPO). During the iterative process, the attacker\nidentifies security vulnerabilities in the defense model and expands jailbreak\ndata. The expanded data are then used to train the defender, enabling it to\naddress identified security vulnerabilities. We also design reward mechanisms\nused for GRPO to simplify the use of response labels, reducing dependence on\ncomplex generative labels and enabling the efficient use of synthetic data.\nAdditionally, a quality monitoring mechanism is used to mitigate the defender's\nover-refusal of harmless inputs and ensure the diversity of the jailbreak data\ngenerated by the attacker. Experimental results on safety-specific and general\nbenchmarks demonstrate that SecTOW significantly improves security while\npreserving general performance."
    },
    {
        "date": "2025-07",
        "title": "Teach Me to Trick: Exploring Adversarial Transferability via Knowledge Distillation",
        "author": "Siddhartha Pradhan, Shikshya Shiwakoti, and Neha Bathuri",
        "link": "http://arxiv.org/abs/2507.21992v1",
        "abstract": "We investigate whether knowledge distillation (KD) from multiple\nheterogeneous teacher models can enhance the generation of transferable\nadversarial examples. A lightweight student model is trained using two KD\nstrategies: curriculum-based switching and joint optimization, with ResNet50\nand DenseNet-161 as teachers. The trained student is then used to generate\nadversarial examples using FG, FGS, and PGD attacks, which are evaluated\nagainst a black-box target model (GoogLeNet). Our results show that student\nmodels distilled from multiple teachers achieve attack success rates comparable\nto ensemble-based baselines, while reducing adversarial example generation time\nby up to a factor of six. An ablation study further reveals that lower\ntemperature settings and the inclusion of hard-label supervision significantly\nenhance transferability. These findings suggest that KD can serve not only as a\nmodel compression technique but also as a powerful tool for improving the\nefficiency and effectiveness of black-box adversarial attacks."
    },
    {
        "date": "2025-07",
        "title": "ZIUM: Zero-Shot Intent-Aware Adversarial Attack on Unlearned Models",
        "author": "Hyun Jun Yook, Ga San Jhun, Jae Hyun Cho, Min Jeon, Donghyun Kim, Tae Hyung Kim, and Youn Kyu Lee",
        "link": "http://arxiv.org/abs/2507.21985v1",
        "abstract": "Machine unlearning (MU) removes specific data points or concepts from deep\nlearning models to enhance privacy and prevent sensitive content generation.\nAdversarial prompts can exploit unlearned models to generate content containing\nremoved concepts, posing a significant security risk. However, existing\nadversarial attack methods still face challenges in generating content that\naligns with an attacker's intent while incurring high computational costs to\nidentify successful prompts. To address these challenges, we propose ZIUM, a\nZero-shot Intent-aware adversarial attack on Unlearned Models, which enables\nthe flexible customization of target attack images to reflect an attacker's\nintent. Additionally, ZIUM supports zero-shot adversarial attacks without\nrequiring further optimization for previously attacked unlearned concepts. The\nevaluation across various MU scenarios demonstrated ZIUM's effectiveness in\nsuccessfully customizing content based on user-intent prompts while achieving a\nsuperior attack success rate compared to existing methods. Moreover, its\nzero-shot adversarial attack significantly reduces the attack time for\npreviously attacked unlearned concepts."
    },
    {
        "date": "2025-07",
        "title": "EIFNet: Leveraging Event-Image Fusion for Robust Semantic Segmentation",
        "author": "Zhijiang Li, and Haoran He",
        "link": "http://arxiv.org/abs/2507.21971v1",
        "abstract": "Event-based semantic segmentation explores the potential of event cameras,\nwhich offer high dynamic range and fine temporal resolution, to achieve robust\nscene understanding in challenging environments. Despite these advantages, the\ntask remains difficult due to two main challenges: extracting reliable features\nfrom sparse and noisy event streams, and effectively fusing them with dense,\nsemantically rich image data that differ in structure and representation. To\naddress these issues, we propose EIFNet, a multi-modal fusion network that\ncombines the strengths of both event and frame-based inputs. The network\nincludes an Adaptive Event Feature Refinement Module (AEFRM), which improves\nevent representations through multi-scale activity modeling and spatial\nattention. In addition, we introduce a Modality-Adaptive Recalibration Module\n(MARM) and a Multi-Head Attention Gated Fusion Module (MGFM), which align and\nintegrate features across modalities using attention mechanisms and gated\nfusion strategies. Experiments on DDD17-Semantic and DSEC-Semantic datasets\nshow that EIFNet achieves state-of-the-art performance, demonstrating its\neffectiveness in event-based semantic segmentation."
    },
    {
        "date": "2025-07",
        "title": "Low-Cost Test-Time Adaptation for Robust Video Editing",
        "author": "Jianhui Wang, Yinda Chen, Yangfan He, Xinyuan Song, Yi Xin, Dapeng Zhang, Zhongwei Wan, Bin Li, and Rongchao Zhang",
        "link": "http://arxiv.org/abs/2507.21858v1",
        "abstract": "Video editing is a critical component of content creation that transforms raw\nfootage into coherent works aligned with specific visual and narrative\nobjectives. Existing approaches face two major challenges: temporal\ninconsistencies due to failure in capturing complex motion patterns, and\noverfitting to simple prompts arising from limitations in UNet backbone\narchitectures. While learning-based methods can enhance editing quality, they\ntypically demand substantial computational resources and are constrained by the\nscarcity of high-quality annotated data. In this paper, we present Vid-TTA, a\nlightweight test-time adaptation framework that personalizes optimization for\neach test video during inference through self-supervised auxiliary tasks. Our\napproach incorporates a motion-aware frame reconstruction mechanism that\nidentifies and preserves crucial movement regions, alongside a prompt\nperturbation and reconstruction strategy that strengthens model robustness to\ndiverse textual descriptions. These innovations are orchestrated by a\nmeta-learning driven dynamic loss balancing mechanism that adaptively adjusts\nthe optimization process based on video characteristics. Extensive experiments\ndemonstrate that Vid-TTA significantly improves video temporal consistency and\nmitigates prompt overfitting while maintaining low computational overhead,\noffering a plug-and-play performance boost for existing video editing models."
    },
    {
        "date": "2025-07",
        "title": "Anyone Can Jailbreak: Prompt-Based Attacks on LLMs and T2Is",
        "author": "Ahmed B Mustafa, Zihan Ye, Yang Lu, Michael P Pound, and Shreyank N Gowda",
        "link": "http://arxiv.org/abs/2507.21820v1",
        "abstract": "Despite significant advancements in alignment and content moderation, large\nlanguage models (LLMs) and text-to-image (T2I) systems remain vulnerable to\nprompt-based attacks known as jailbreaks. Unlike traditional adversarial\nexamples requiring expert knowledge, many of today's jailbreaks are low-effort,\nhigh-impact crafted by everyday users with nothing more than cleverly worded\nprompts. This paper presents a systems-style investigation into how non-experts\nreliably circumvent safety mechanisms through techniques such as multi-turn\nnarrative escalation, lexical camouflage, implication chaining, fictional\nimpersonation, and subtle semantic edits. We propose a unified taxonomy of\nprompt-level jailbreak strategies spanning both text-output and T2I models,\ngrounded in empirical case studies across popular APIs. Our analysis reveals\nthat every stage of the moderation pipeline, from input filtering to output\nvalidation, can be bypassed with accessible strategies. We conclude by\nhighlighting the urgent need for context-aware defenses that reflect the ease\nwith which these jailbreaks can be reproduced in real-world settings."
    },
    {
        "date": "2025-07",
        "title": "Learning Kinetic Monte Carlo stochastic dynamics with Deep Generative Adversarial Networks",
        "author": "Daniele Lanzoni, Olivier Pierre-Louis, Roberto Bergamaschini, and Francesco Montalenti",
        "link": "http://arxiv.org/abs/2507.21763v1",
        "abstract": "We show that Generative Adversarial Networks (GANs) may be fruitfully\nexploited to learn stochastic dynamics, surrogating traditional models while\ncapturing thermal fluctuations. Specifically, we showcase the application to a\ntwo-dimensional, many-particle system, focusing on surface-step fluctuations\nand on the related time-dependent roughness. After the construction of a\ndataset based on Kinetic Monte Carlo simulations, a conditional GAN is trained\nto propagate stochastically the state of the system in time, allowing the\ngeneration of new sequences with a reduced computational cost. Modifications\nwith respect to standard GANs, which facilitate convergence and increase\naccuracy, are discussed. The trained network is demonstrated to quantitatively\nreproduce equilibrium and kinetic properties, including scaling laws, with\ndeviations of a few percent from the exact value. Extrapolation limits and\nfuture perspectives are critically discussed."
    },
    {
        "date": "2025-07",
        "title": "Adversarial Reconstruction Feedback for Robust Fine-grained Generalization",
        "author": "Shijie Wang, Jian Shi, and Haojie Li",
        "link": "http://arxiv.org/abs/2507.21742v1",
        "abstract": "Existing fine-grained image retrieval (FGIR) methods predominantly rely on\nsupervision from predefined categories to learn discriminative representations\nfor retrieving fine-grained objects. However, they inadvertently introduce\ncategory-specific semantics into the retrieval representation, creating\nsemantic dependencies on predefined classes that critically hinder\ngeneralization to unseen categories. To tackle this, we propose AdvRF, a novel\nadversarial reconstruction feedback framework aimed at learning\ncategory-agnostic discrepancy representations. Specifically, AdvRF reformulates\nFGIR as a visual discrepancy reconstruction task via synergizing category-aware\ndiscrepancy localization from retrieval models with category-agnostic feature\nlearning from reconstruction models. The reconstruction model exposes residual\ndiscrepancies overlooked by the retrieval model, forcing it to improve\nlocalization accuracy, while the refined signals from the retrieval model guide\nthe reconstruction model to improve its reconstruction ability. Consequently,\nthe retrieval model localizes visual differences, while the reconstruction\nmodel encodes these differences into category-agnostic representations. This\nrepresentation is then transferred to the retrieval model through knowledge\ndistillation for efficient deployment. Quantitative and qualitative evaluations\ndemonstrate that our AdvRF achieves impressive performance on both widely-used\nfine-grained and coarse-grained datasets."
    },
    {
        "date": "2025-07",
        "title": "Zero-Shot Machine Unlearning with Proxy Adversarial Data Generation",
        "author": "Huiqiang Chen, Tianqing Zhu, Xin Yu, and Wanlei Zhou",
        "link": "http://arxiv.org/abs/2507.21738v1",
        "abstract": "Machine unlearning aims to remove the influence of specific samples from a\ntrained model. A key challenge in this process is over-unlearning, where the\nmodel's performance on the remaining data significantly drops due to the change\nin the model's parameters. Existing unlearning algorithms depend on the\nremaining data to prevent this issue. As such, these methods are inapplicable\nin a more practical scenario, where only the unlearning samples are available\n(i.e., zero-shot unlearning). This paper presents a novel framework, ZS-PAG, to\nfill this gap. Our approach offers three key innovations: (1) we approximate\nthe inaccessible remaining data by generating adversarial samples; (2)\nleveraging the generated samples, we pinpoint a specific subspace to perform\nthe unlearning process, therefore preventing over-unlearning in the challenging\nzero-shot scenario; and (3) we consider the influence of the unlearning process\non the remaining samples and design an influence-based pseudo-labeling\nstrategy. As a result, our method further improves the model's performance\nafter unlearning. The proposed method holds a theoretical guarantee, and\nexperiments on various benchmarks validate the effectiveness and superiority of\nour proposed method over several baselines."
    },
    {
        "date": "2025-07",
        "title": "Can We End the Cat-and-Mouse Game? Simulating Self-Evolving Phishing Attacks with LLMs and Genetic Algorithms",
        "author": "Seiji Sato, Tetsushi Ohki, and Masakatsu Nishigaki",
        "link": "http://arxiv.org/abs/2507.21538v1",
        "abstract": "Anticipating emerging attack methodologies is crucial for proactive\ncybersecurity. Recent advances in Large Language Models (LLMs) have enabled the\nautomated generation of phishing messages and accelerated research into\npotential attack techniques. However, predicting future threats remains\nchallenging due to reliance on existing training data. To address this\nlimitation, we propose a novel framework that integrates LLM-based phishing\nattack simulations with a genetic algorithm in a psychological context,\nenabling phishing strategies to evolve dynamically through adversarial\ninteractions with simulated victims. Through simulations using Llama 3.1, we\ndemonstrate that (1) self-evolving phishing strategies employ increasingly\nsophisticated psychological manipulation techniques, surpassing naive\nLLM-generated attacks, (2) variations in a victim's prior knowledge\nsignificantly influence the evolution of attack strategies, and (3) adversarial\ninteractions between evolving attacks and adaptive defenses create a\ncat-and-mouse dynamic, revealing an inherent asymmetry in cybersecurity --\nattackers continuously refine their methods, whereas defenders struggle to\ncomprehensively counter all evolving threats. Our approach provides a scalable,\ncost-effective method for analyzing the evolution of phishing strategies and\ndefenses, offering insights into future social engineering threats and\nunderscoring the necessity of proactive cybersecurity measures."
    },
    {
        "date": "2025-07",
        "title": "Model Predictive Adversarial Imitation Learning for Planning from Observation",
        "author": "Tyler Han, Yanda Bao, Bhaumik Mehta, Gabriel Guo, Anubhav Vishwakarma, Emily Kang, Sanghun Jung, Rosario Scalise, Jason Zhou, Bryan Xu, and Byron Boots",
        "link": "http://arxiv.org/abs/2507.21533v1",
        "abstract": "Human demonstration data is often ambiguous and incomplete, motivating\nimitation learning approaches that also exhibit reliable planning behavior. A\ncommon paradigm to perform planning-from-demonstration involves learning a\nreward function via Inverse Reinforcement Learning (IRL) then deploying this\nreward via Model Predictive Control (MPC). Towards unifying these methods, we\nderive a replacement of the policy in IRL with a planning-based agent. With\nconnections to Adversarial Imitation Learning, this formulation enables\nend-to-end interactive learning of planners from observation-only\ndemonstrations. In addition to benefits in interpretability, complexity, and\nsafety, we study and observe significant improvements on sample efficiency,\nout-of-distribution generalization, and robustness. The study includes\nevaluations in both simulated control benchmarks and real-world navigation\nexperiments using few-to-single observation-only demonstrations."
    },
    {
        "date": "2025-07",
        "title": "NCCR: to Evaluate the Robustness of Neural Networks and Adversarial Examples",
        "author": "Shi Pu, Fu Song, and Wenjie Wang",
        "link": "http://arxiv.org/abs/2507.21483v2",
        "abstract": "Neural networks have received a lot of attention recently, and related\nsecurity issues have come with it. Many studies have shown that neural networks\nare vulnerable to adversarial examples that have been artificially perturbed\nwith modification, which is too small to be distinguishable by human\nperception. Different attacks and defenses have been proposed to solve these\nproblems, but there is little research on evaluating the robustness of neural\nnetworks and their inputs. In this work, we propose a metric called the neuron\ncover change rate (NCCR) to measure the ability of deep learning models to\nresist attacks and the stability of adversarial examples. NCCR monitors\nalterations in the output of specifically chosen neurons when the input is\nperturbed, and networks with a smaller degree of variation are considered to be\nmore robust. The results of the experiment on image recognition and the speaker\nrecognition model show that our metrics can provide a good assessment of the\nrobustness of neural networks or their inputs. It can also be used to detect\nwhether an input is adversarial or not, as adversarial examples are always less\nrobust."
    },
    {
        "date": "2025-07",
        "title": "Cascading and Proxy Membership Inference Attacks",
        "author": "Yuntao Du, Jiacheng Li, Yuetian Chen, Kaiyuan Zhang, Zhizhen Yuan, Hanshen Xiao, Bruno Ribeiro, and Ninghui Li",
        "link": "http://arxiv.org/abs/2507.21412v1",
        "abstract": "A Membership Inference Attack (MIA) assesses how much a trained machine\nlearning model reveals about its training data by determining whether specific\nquery instances were included in the dataset. We classify existing MIAs into\nadaptive or non-adaptive, depending on whether the adversary is allowed to\ntrain shadow models on membership queries. In the adaptive setting, where the\nadversary can train shadow models after accessing query instances, we highlight\nthe importance of exploiting membership dependencies between instances and\npropose an attack-agnostic framework called Cascading Membership Inference\nAttack (CMIA), which incorporates membership dependencies via conditional\nshadow training to boost membership inference performance.\n  In the non-adaptive setting, where the adversary is restricted to training\nshadow models before obtaining membership queries, we introduce Proxy\nMembership Inference Attack (PMIA). PMIA employs a proxy selection strategy\nthat identifies samples with similar behaviors to the query instance and uses\ntheir behaviors in shadow models to perform a membership posterior odds test\nfor membership inference. We provide theoretical analyses for both attacks, and\nextensive experimental results demonstrate that CMIA and PMIA substantially\noutperform existing MIAs in both settings, particularly in the low\nfalse-positive regime, which is crucial for evaluating privacy risks."
    },
    {
        "date": "2025-07",
        "title": "Radio Adversarial Attacks on EMG-based Gesture Recognition Networks",
        "author": "Hongyi Xie",
        "link": "http://arxiv.org/abs/2507.21387v1",
        "abstract": "Surface electromyography (EMG) enables non-invasive human-computer\ninteraction in rehabilitation, prosthetics, and virtual reality. While deep\nlearning models achieve over 97% classification accuracy, their vulnerability\nto adversarial attacks remains largely unexplored in the physical domain. We\npresent ERa Attack, the first radio frequency (RF) adversarial method targeting\nEMG devices through intentional electromagnetic interference (IEMI). Using\nlow-power software-defined radio transmitters, attackers inject optimized RF\nperturbations to mislead downstream models. Our approach bridges digital and\nphysical domains: we generate adversarial perturbations using Projected\nGradient Descent, extract 50-150 Hz components via inverse STFT, and employ\nsynchronization-free strategies (constant spectrum noise or narrowband\nmodulation). Perturbations, constrained to 1-10% of signal amplitude, are\namplitude-modulated onto 433 MHz carriers. Experiments on the Myo Dataset (7\ngestures, 350 samples) demonstrate significant impact: at 1 meter and 0 dBm\ntransmission power, classification accuracy drops from 97.8% to 58.3%, with\n41.7% misclassification rate and 25.6% targeted attack success rate. Attack\neffectiveness decreases exponentially with distance, recovering to 85% accuracy\nat 3 meters. Increasing power to 10 dBm reduces accuracy by an additional 15%\nat 1 meter. This work pioneers RF-based adversarial attacks on EMG recognition\nsystems, revealing critical vulnerabilities in safety-critical applications. We\nquantify attack effectiveness across different perturbation modes and\ndistances, and propose defenses including hardware shielding, spectrum\nmonitoring, and adversarial training. Our findings inform the design of robust\nEMG systems against electromagnetic threats."
    },
    {
        "date": "2025-07",
        "title": "Fairness and Robustness of CLIP-Based Models for Chest X-rays",
        "author": "Th\u00e9o Sourget, David Restrepo, C\u00e9line Hudelot, Enzo Ferrante, Stergios Christodoulidis, and Maria Vakalopoulou",
        "link": "http://arxiv.org/abs/2507.21291v1",
        "abstract": "Motivated by the strong performance of CLIP-based models in natural\nimage-text domains, recent efforts have adapted these architectures to medical\ntasks, particularly in radiology, where large paired datasets of images and\nreports, such as chest X-rays, are available. While these models have shown\nencouraging results in terms of accuracy and discriminative performance, their\nfairness and robustness in the different clinical tasks remain largely\nunderexplored. In this study, we extensively evaluate six widely used\nCLIP-based models on chest X-ray classification using three publicly available\ndatasets: MIMIC-CXR, NIH-CXR14, and NEATX. We assess the models fairness across\nsix conditions and patient subgroups based on age, sex, and race. Additionally,\nwe assess the robustness to shortcut learning by evaluating performance on\npneumothorax cases with and without chest drains. Our results indicate\nperformance gaps between patients of different ages, but more equitable results\nfor the other attributes. Moreover, all models exhibit lower performance on\nimages without chest drains, suggesting reliance on spurious correlations. We\nfurther complement the performance analysis with a study of the embeddings\ngenerated by the models. While the sensitive attributes could be classified\nfrom the embeddings, we do not see such patterns using PCA, showing the\nlimitations of these visualisation techniques when assessing models. Our code\nis available at https://github.com/TheoSourget/clip_cxr_fairness"
    },
    {
        "date": "2025-07",
        "title": "Structured Relevance Assessment for Robust Retrieval-Augmented Language Models",
        "author": "Aryan Raj, Astitva Veer Garg, and Anitha D",
        "link": "http://arxiv.org/abs/2507.21287v1",
        "abstract": "Retrieval-Augmented Language Models (RALMs) face significant challenges in\nreducing factual errors, particularly in document relevance evaluation and\nknowledge integration. We introduce a framework for structured relevance\nassessment that enhances RALM robustness through improved document evaluation,\nbalanced intrinsic and external knowledge integration, and effective handling\nof unanswerable queries. Our approach employs a multi-dimensional scoring\nsystem that considers both semantic matching and source reliability, utilizing\nembedding-based relevance scoring and synthetic training data with\nmixed-quality documents. We implement specialized benchmarking on niche topics,\na knowledge integration mechanism, and an \"unknown\" response protocol for\nqueries with insufficient knowledge coverage. Preliminary evaluations\ndemonstrate significant reductions in hallucination rates and improved\ntransparency in reasoning processes. Our framework advances the development of\nmore reliable question-answering systems capable of operating effectively in\ndynamic environments with variable data quality. While challenges persist in\naccurately distinguishing credible information and balancing system latency\nwith thoroughness, this work represents a meaningful step toward enhancing RALM\nreliability."
    },
    {
        "date": "2025-07",
        "title": "Development and analysis of a secured VoIP system for surveillance activities",
        "author": "M. Matsive Ali",
        "link": "http://arxiv.org/abs/2507.21038v2",
        "abstract": "Since the 1990s, the telephone has been the primary mode of communication.\nHowever, Voice over Internet Protocol (VoIP), which is a highly straightforward\nand affordable form of data transfer, is now becoming an important part of\ndaily communication. VoIP is the technology that makes it possible to send\nspeech and multimedia data packets across either a public or private IP\nnetwork. However, a cyberattack known as a man-in-the-middle attack poses a\nserious concern in transferring data through any network. Therefore, the\nauthors have designed a system that sends voice over the internet within the\nrange of a router using encrypted data transfer. An embedded system comprising\nan electret microphone, Embedded C, Particle Photon microcontroller, and\nInternet of Things (IoT) technology is developed. Due to its compact size, this\ntype of device may be incorporated into automobiles, surveillance systems, or\ncovert listening tools. The VoIP system gathers sound signals using the MAX9814\nmicrophone, while the Particle Photon microcontroller securely transmits the\ndata. Devices with access can download data from the VoIP systems Transmission\nControl Protocol (TCP) server. The accessed device stores the audio locally and\nuploads the corresponding data to Google Drive. This VoIP system provides a\nsecure method of communication while conserving the integrity of the original\nsignal."
    },
    {
        "date": "2025-07",
        "title": "Improving Adversarial Robustness Through Adaptive Learning-Driven Multi-Teacher Knowledge Distillation",
        "author": "Hayat Ullah, Syed Muhammad Talha Zaidi, and Arslan Munir",
        "link": "http://arxiv.org/abs/2507.20996v1",
        "abstract": "Convolutional neural networks (CNNs) excel in computer vision but are\nsusceptible to adversarial attacks, crafted perturbations designed to mislead\npredictions. Despite advances in adversarial training, a gap persists between\nmodel accuracy and robustness. To mitigate this issue, in this paper, we\npresent a multi-teacher adversarial robustness distillation using an adaptive\nlearning strategy. Specifically, our proposed method first trained multiple\nclones of a baseline CNN model using an adversarial training strategy on a pool\nof perturbed data acquired through different adversarial attacks. Once trained,\nthese adversarially trained models are used as teacher models to supervise the\nlearning of a student model on clean data using multi-teacher knowledge\ndistillation. To ensure an effective robustness distillation, we design an\nadaptive learning strategy that controls the knowledge contribution of each\nmodel by assigning weights as per their prediction precision. Distilling\nknowledge from adversarially pre-trained teacher models not only enhances the\nlearning capabilities of the student model but also empowers it with the\ncapacity to withstand different adversarial attacks, despite having no exposure\nto adversarial data. To verify our claims, we extensively evaluated our\nproposed method on MNIST-Digits and Fashion-MNIST datasets across diverse\nexperimental settings. The obtained results exhibit the efficacy of our\nmulti-teacher adversarial distillation and adaptive learning strategy,\nenhancing CNNs' adversarial robustness against various adversarial attacks."
    },
    {
        "date": "2025-07",
        "title": "Security Tensors as a Cross-Modal Bridge: Extending Text-Aligned Safety to Vision in LVLM",
        "author": "Shen Li, Liuyi Yao, Wujia Niu, Lan Zhang, and Yaliang Li",
        "link": "http://arxiv.org/abs/2507.20994v1",
        "abstract": "Large visual-language models (LVLMs) integrate aligned large language models\n(LLMs) with visual modules to process multimodal inputs. However, the safety\nmechanisms developed for text-based LLMs do not naturally extend to visual\nmodalities, leaving LVLMs vulnerable to harmful image inputs. To address this\ncross-modal safety gap, we introduce security tensors - trainable input vectors\napplied during inference through either the textual or visual modality. These\ntensors transfer textual safety alignment to visual processing without\nmodifying the model's parameters. They are optimized using a curated dataset\ncontaining (i) malicious image-text pairs requiring rejection, (ii) contrastive\nbenign pairs with text structurally similar to malicious queries, with the\npurpose of being contrastive examples to guide visual reliance, and (iii)\ngeneral benign samples preserving model functionality. Experimental results\ndemonstrate that both textual and visual security tensors significantly enhance\nLVLMs' ability to reject diverse harmful visual inputs while maintaining\nnear-identical performance on benign tasks. Further internal analysis towards\nhidden-layer representations reveals that security tensors successfully\nactivate the language module's textual \"safety layers\" in visual inputs,\nthereby effectively extending text-based safety to the visual modality."
    },
    {
        "date": "2025-07",
        "title": "Testbed and Software Architecture for Enhancing Security in Industrial Private 5G Networks",
        "author": "Song Son Ha, Florian Foerster, Thomas Robert Doebbert, Tim Kittel, Dominik Merli, and Gerd Scholl",
        "link": "http://arxiv.org/abs/2507.20873v1",
        "abstract": "In the era of Industry 4.0, the growing need for secure and efficient\ncommunication systems has driven the development of fifth-generation (5G)\nnetworks characterized by extremely low latency, massive device connectivity\nand high data transfer speeds. However, the deployment of 5G networks presents\nsignificant security challenges, requiring advanced and robust solutions to\ncounter increasingly sophisticated cyber threats. This paper proposes a testbed\nand software architecture to strengthen the security of Private 5G Networks,\nparticularly in industrial communication environments."
    },
    {
        "date": "2025-07",
        "title": "Not Only Grey Matter: OmniBrain for Robust Multimodal Classification of Alzheimer's Disease",
        "author": "Ahmed Sharshar, Yasser Ashraf, Tameem Bakr, Salma Hassan, Hosam Elgendy, Mohammad Yaqub, and Mohsen Guizani",
        "link": "http://arxiv.org/abs/2507.20872v1",
        "abstract": "Alzheimer's disease affects over 55 million people worldwide and is projected\nto more than double by 2050, necessitating rapid, accurate, and scalable\ndiagnostics. However, existing approaches are limited because they cannot\nachieve clinically acceptable accuracy, generalization across datasets,\nrobustness to missing modalities, and explainability all at the same time. This\ninability to satisfy all these requirements simultaneously undermines their\nreliability in clinical settings. We propose OmniBrain, a multimodal framework\nthat integrates brain MRI, radiomics, gene expression, and clinical data using\na unified model with cross-attention and modality dropout. OmniBrain achieves\n$92.2 \\pm 2.4\\%$accuracy on the ANMerge dataset and generalizes to the MRI-only\nADNI dataset with $70.4 \\pm 2.7\\%$ accuracy, outperforming unimodal and prior\nmultimodal approaches. Explainability analyses highlight neuropathologically\nrelevant brain regions and genes, enhancing clinical trust. OmniBrain offers a\nrobust, interpretable, and practical solution for real-world Alzheimer's\ndiagnosis."
    },
    {
        "date": "2025-07",
        "title": "An Open-source Implementation and Security Analysis of Triad's TEE Trusted Time Protocol",
        "author": "Matthieu Bettinger, Sonia Ben Mokhtar, and Anthony Simonet-Boulogne",
        "link": "http://arxiv.org/abs/2507.20851v1",
        "abstract": "The logic of many protocols relies on time measurements. However, in Trusted\nExecution Environments (TEEs) like Intel SGX, the time source is outside the\nTrusted Computing Base: a malicious system hosting the TEE can manipulate that\nTEE's notion of time, e.g., jumping in time or affecting the perceived time\nspeed. Previous work like Triad propose protocols for TEEs to maintain a\ntrustworthy time source. However, in this paper, based on a public\nimplementation of Triad that we contribute, we empirically showcase\nvulnerabilities to this protocol. For example, an attacker controlling the\noperating system, and consequently the scheduling algorithm, may arbitrarily\nmanipulate their local TEE's clock speed. What is worse, in case of faster\nmalicious clock speeds, an attacker on a single compromised machine may\npropagate the attack to honest machines participating in Triad's Trusted Time\nprotocol, causing them to skip to timestamps arbitrarily far in the future.\nThen, infected honest machines propagate time-skips themselves to other honest\nmachines interacting with them. We discuss protocol changes to Triad for higher\nresilience against such attacks."
    },
    {
        "date": "2025-07",
        "title": "Enhancing Jailbreak Attacks on LLMs via Persona Prompts",
        "author": "Zheng Zhang, Peilin Zhao, Deheng Ye, and Hao Wang",
        "link": "http://arxiv.org/abs/2507.22171v1",
        "abstract": "Jailbreak attacks aim to exploit large language models (LLMs) by inducing\nthem to generate harmful content, thereby revealing their vulnerabilities.\nUnderstanding and addressing these attacks is crucial for advancing the field\nof LLM safety. Previous jailbreak approaches have mainly focused on direct\nmanipulations of harmful intent, with limited attention to the impact of\npersona prompts. In this study, we systematically explore the efficacy of\npersona prompts in compromising LLM defenses. We propose a genetic\nalgorithm-based method that automatically crafts persona prompts to bypass\nLLM's safety mechanisms. Our experiments reveal that: (1) our evolved persona\nprompts reduce refusal rates by 50-70% across multiple LLMs, and (2) these\nprompts demonstrate synergistic effects when combined with existing attack\nmethods, increasing success rates by 10-20%. Our code and data are available at\nhttps://github.com/CjangCjengh/Generic_Persona."
    },
    {
        "date": "2025-07",
        "title": "Multi-Masked Querying Network for Robust Emotion Recognition from Incomplete Multi-Modal Physiological Signals",
        "author": "Geng-Xin Xu, Xiang Zuo, and Ye Li",
        "link": "http://arxiv.org/abs/2507.20737v1",
        "abstract": "Emotion recognition from physiological data is crucial for mental health\nassessment, yet it faces two significant challenges: incomplete multi-modal\nsignals and interference from body movements and artifacts. This paper presents\na novel Multi-Masked Querying Network (MMQ-Net) to address these issues by\nintegrating multiple querying mechanisms into a unified framework.\nSpecifically, it uses modality queries to reconstruct missing data from\nincomplete signals, category queries to focus on emotional state features, and\ninterference queries to separate relevant information from noise. Extensive\nexperiment results demonstrate the superior emotion recognition performance of\nMMQ-Net compared to existing approaches, particularly under high levels of data\nincompleteness."
    },
    {
        "date": "2025-07",
        "title": "Exposing the Illusion of Fairness: Auditing Vulnerabilities to Distributional Manipulation Attacks",
        "author": "Valentin Lafargue, Adriana Laurindo Monteiro, Emmanuelle Claeys, Laurent Risser, and Jean-Michel Loubes",
        "link": "http://arxiv.org/abs/2507.20708v1",
        "abstract": "Proving the compliance of AI algorithms has become an important challenge\nwith the growing deployment of such algorithms for real-life applications.\nInspecting possible biased behaviors is mandatory to satisfy the constraints of\nthe regulations of the EU Artificial Intelligence's Act. Regulation-driven\naudits increasingly rely on global fairness metrics, with Disparate Impact\nbeing the most widely used. Yet such global measures depend highly on the\ndistribution of the sample on which the measures are computed. We investigate\nfirst how to manipulate data samples to artificially satisfy fairness criteria,\ncreating minimally perturbed datasets that remain statistically\nindistinguishable from the original distribution while satisfying prescribed\nfairness constraints. Then we study how to detect such manipulation. Our\nanalysis (i) introduces mathematically sound methods for modifying empirical\ndistributions under fairness constraints using entropic or optimal transport\nprojections, (ii) examines how an auditee could potentially circumvent fairness\ninspections, and (iii) offers recommendations to help auditors detect such data\nmanipulations. These results are validated through experiments on classical\ntabular datasets in bias detection."
    },
    {
        "date": "2025-07",
        "title": "A Novel Post-Quantum Secure Digital Signature Scheme Based on Neural Network",
        "author": "Satish Kumar, and Md. Arzoo Jamal",
        "link": "http://arxiv.org/abs/2507.20676v1",
        "abstract": "Digital signatures are fundamental cryptographic primitives that ensure the\nauthenticity and integrity of digital documents. In the post-quantum era,\nclassical public key-based signature schemes become vulnerable to brute-force\nand key-recovery attacks due to the computational power of quantum algorithms.\nMultivariate polynomial based signature schemes are among the one of the\ncryptographic constructions that offers strong security guarantees against such\nquantum threats. With the growing capabilities of neural networks, it is\nnatural to explore their potential application in the design of cryptographic\nprimitives. Neural networks inherently captures the non-linear relationships\nwithin the data, which are encoded in their synaptic weight matrices and bias\nvectors. In this paper, we propose a novel construction of a multivariate\npolynomial based digital signature scheme that leverages neural network\narchitectures. A neural network with binary weights is employed to define the\ncentral structure of the signature scheme. The design introduces a recurrent\nrandom vector, functionally analogous to an attention mechanism, which\ncontributes dynamic randomness based on the previous state, thereby enhancing\nthe scheme's security. It is demonstrated that the proposed signature scheme\nprovide security against Existential Unforgeability under adaptive\nChosen-Message Attacks (EUF-CMA). Furthermore, it is proven that direct attacks\naimed to recover the private keys are computationally infeasible within\npolynomial time, even in the presence of quantum computing abilities. The\noperational characteristics of the proposed scheme are also evaluated, with\nresults indicating notable efficiency and practical viability in post-quantum\ncryptographic applications."
    },
    {
        "date": "2025-07",
        "title": "Maximize margins for robust splicing detection",
        "author": "Julien Simon de Kergunic, Rony Abecidan, Patrick Bas, and Vincent Itier",
        "link": "http://arxiv.org/abs/2508.00897v1",
        "abstract": "Despite recent progress in splicing detection, deep learning-based forensic\ntools remain difficult to deploy in practice due to their high sensitivity to\ntraining conditions. Even mild post-processing applied to evaluation images can\nsignificantly degrade detector performance, raising concerns about their\nreliability in operational contexts. In this work, we show that the same deep\narchitecture can react very differently to unseen post-processing depending on\nthe learned weights, despite achieving similar accuracy on in-distribution test\ndata. This variability stems from differences in the latent spaces induced by\ntraining, which affect how samples are separated internally. Our experiments\nreveal a strong correlation between the distribution of latent margins and a\ndetector's ability to generalize to post-processed images. Based on this\nobservation, we propose a practical strategy for building more robust\ndetectors: train several variants of the same model under different conditions,\nand select the one that maximizes latent margins."
    },
    {
        "date": "2025-07",
        "title": "Reminiscence Attack on Residuals: Exploiting Approximate Machine Unlearning for Privacy",
        "author": "Yaxin Xiao, Qingqing Ye, Li Hu, Huadi Zheng, Haibo Hu, Zi Liang, Haoyang Li, and Yijie Jiao",
        "link": "http://arxiv.org/abs/2507.20573v1",
        "abstract": "Machine unlearning enables the removal of specific data from ML models to\nuphold the right to be forgotten. While approximate unlearning algorithms offer\nefficient alternatives to full retraining, this work reveals that they fail to\nadequately protect the privacy of unlearned data. In particular, these\nalgorithms introduce implicit residuals which facilitate privacy attacks\ntargeting at unlearned data. We observe that these residuals persist regardless\nof model architectures, parameters, and unlearning algorithms, exposing a new\nattack surface beyond conventional output-based leakage. Based on this insight,\nwe propose the Reminiscence Attack (ReA), which amplifies the correlation\nbetween residuals and membership privacy through targeted fine-tuning\nprocesses. ReA achieves up to 1.90x and 1.12x higher accuracy than prior\nattacks when inferring class-wise and sample-wise membership, respectively. To\nmitigate such residual-induced privacy risk, we develop a dual-phase\napproximate unlearning framework that first eliminates deep-layer unlearned\ndata traces and then enforces convergence stability to prevent models from\n\"pseudo-convergence\", where their outputs are similar to retrained models but\nstill preserve unlearned residuals. Our framework works for both classification\nand generation tasks. Experimental evaluations confirm that our approach\nmaintains high unlearning efficacy, while reducing the adaptive privacy attack\naccuracy to nearly random guess, at the computational cost of 2-12% of full\nretraining from scratch."
    },
    {
        "date": "2025-07",
        "title": "Security Challenges in AI Agent Deployment: Insights from a Large Scale Public Competition",
        "author": "Andy Zou, Maxwell Lin, Eliot Jones, Micha Nowak, Mateusz Dziemian, Nick Winter, Alexander Grattan, Valent Nathanael, Ayla Croft, Xander Davies, Jai Patel, Robert Kirk, Nate Burnikell, Yarin Gal, Dan Hendrycks, J. Zico Kolter, and Matt Fredrikson",
        "link": "http://arxiv.org/abs/2507.20526v1",
        "abstract": "Recent advances have enabled LLM-powered AI agents to autonomously execute\ncomplex tasks by combining language model reasoning with tools, memory, and web\naccess. But can these systems be trusted to follow deployment policies in\nrealistic environments, especially under attack? To investigate, we ran the\nlargest public red-teaming competition to date, targeting 22 frontier AI agents\nacross 44 realistic deployment scenarios. Participants submitted 1.8 million\nprompt-injection attacks, with over 60,000 successfully eliciting policy\nviolations such as unauthorized data access, illicit financial actions, and\nregulatory noncompliance. We use these results to build the Agent Red Teaming\n(ART) benchmark - a curated set of high-impact attacks - and evaluate it across\n19 state-of-the-art models. Nearly all agents exhibit policy violations for\nmost behaviors within 10-100 queries, with high attack transferability across\nmodels and tasks. Importantly, we find limited correlation between agent\nrobustness and model size, capability, or inference-time compute, suggesting\nthat additional defenses are needed against adversarial misuse. Our findings\nhighlight critical and persistent vulnerabilities in today's AI agents. By\nreleasing the ART benchmark and accompanying evaluation framework, we aim to\nsupport more rigorous security assessment and drive progress toward safer agent\ndeployment."
    },
    {
        "date": "2025-07",
        "title": "Your Attention Matters: to Improve Model Robustness to Noise and Spurious Correlations",
        "author": "Camilo Tamayo-Rousseau, Yunjia Zhao, Yiqun Zhang, and Randall Balestriero",
        "link": "http://arxiv.org/abs/2507.20453v2",
        "abstract": "Self-attention mechanisms are foundational to Transformer architectures,\nsupporting their impressive success in a wide range of tasks. While there are\nmany self-attention variants, their robustness to noise and spurious\ncorrelations has not been well studied. This study evaluates Softmax, Sigmoid,\nLinear, Doubly Stochastic, and Cosine attention within Vision Transformers\nunder different data corruption scenarios. Through testing across the CIFAR-10,\nCIFAR-100, and Imagenette datasets, we show that Doubly Stochastic attention is\nthe most robust. It consistently outperformed the next best mechanism by\n$0.1\\%-5.1\\%$ when training data, or both training and testing data, were\ncorrupted. Our findings inform self-attention selection in contexts with\nimperfect data. The code used is available at\nhttps://github.com/ctamayor/NeurIPS-Robustness-ViT."
    },
    {
        "date": "2025-07",
        "title": "MaXsive: High-Capacity and Robust Training-Free Generative Image Watermarking in Diffusion Models",
        "author": "Po-Yuan Mao, Cheng-Chang Tsai, and Chun-Shien Lu",
        "link": "http://arxiv.org/abs/2507.21195v1",
        "abstract": "The great success of the diffusion model in image synthesis led to the\nrelease of gigantic commercial models, raising the issue of copyright\nprotection and inappropriate content generation. Training-free diffusion\nwatermarking provides a low-cost solution for these issues. However, the prior\nworks remain vulnerable to rotation, scaling, and translation (RST) attacks.\nAlthough some methods employ meticulously designed patterns to mitigate this\nissue, they often reduce watermark capacity, which can result in identity (ID)\ncollusion. To address these problems, we propose MaXsive, a training-free\ndiffusion model generative watermarking technique that has high capacity and\nrobustness. MaXsive best utilizes the initial noise to watermark the diffusion\nmodel. Moreover, instead of using a meticulously repetitive ring pattern, we\npropose injecting the X-shape template to recover the RST distortions. This\ndesign significantly increases robustness without losing any capacity, making\nID collusion less likely to happen. The effectiveness of MaXsive has been\nverified on two well-known watermarking benchmarks under the scenarios of\nverification and identification."
    }
]