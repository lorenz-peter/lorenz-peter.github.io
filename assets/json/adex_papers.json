[
    {
        "date": "2025-10",
        "title": "Proactive defense against LLM Jailbreak",
        "author": "Weiliang Zhao, Jinjun Peng, Daniel Ben-Levi, Zhou Yu, and Junfeng Yang",
        "link": "http://arxiv.org/abs/2510.05052v1",
        "abstract": "The proliferation of powerful large language models (LLMs) has necessitated\nrobust safety alignment, yet these models remain vulnerable to evolving\nadversarial attacks, including multi-turn jailbreaks that iteratively search\nfor successful queries. Current defenses, primarily reactive and static, often\nfail to counter these search-based attacks. In this paper, we introduce ProAct,\na novel proactive defense framework designed to disrupt and mislead autonomous\njailbreaking processes. Our core idea is to intentionally provide adversaries\nwith \"spurious responses\" that appear to be results of successful jailbreak\nattacks but contain no actual harmful content. These misleading responses\nprovide false signals to the attacker's internal optimization loop, causing the\nadversarial search to terminate prematurely and effectively jailbreaking the\njailbreak. By conducting extensive experiments across state-of-the-art LLMs,\njailbreaking frameworks, and safety benchmarks, our method consistently and\nsignificantly reduces attack success rates by up to 92\\%. When combined with\nother defense frameworks, it further reduces the success rate of the latest\nattack strategies to 0\\%. ProAct represents an orthogonal defense strategy that\ncan serve as an additional guardrail to enhance LLM safety against the most\neffective jailbreaking attacks."
    },
    {
        "date": "2025-10",
        "title": "KEEP: Integrating Medical Ontologies with Clinical Data for Robust Code Embeddings",
        "author": "Ahmed Elhussein, Paul Meddeb, Abigail Newbury, Jeanne Mirone, Martin Stoll, and Gamze Gursoy",
        "link": "http://arxiv.org/abs/2510.05049v1",
        "abstract": "Machine learning in healthcare requires effective representation of\nstructured medical codes, but current methods face a trade off: knowledge graph\nbased approaches capture formal relationships but miss real world patterns,\nwhile data driven methods learn empirical associations but often overlook\nstructured knowledge in medical terminologies. We present KEEP (Knowledge\npreserving and Empirically refined Embedding Process), an efficient framework\nthat bridges this gap by combining knowledge graph embeddings with adaptive\nlearning from clinical data. KEEP first generates embeddings from knowledge\ngraphs, then employs regularized training on patient records to adaptively\nintegrate empirical patterns while preserving ontological relationships.\nImportantly, KEEP produces final embeddings without task specific auxiliary or\nend to end training enabling KEEP to support multiple downstream applications\nand model architectures. Evaluations on structured EHR from UK Biobank and\nMIMIC IV demonstrate that KEEP outperforms both traditional and Language Model\nbased approaches in capturing semantic relationships and predicting clinical\noutcomes. Moreover, KEEP's minimal computational requirements make it\nparticularly suitable for resource constrained environments."
    },
    {
        "date": "2025-10",
        "title": "NatGVD: Natural Adversarial Example Attack towards Graph-based Vulnerability Detection",
        "author": "Avilash Rath, Weiliang Qi, Youpeng Li, and Xinda Wang",
        "link": "http://arxiv.org/abs/2510.04987v1",
        "abstract": "Graph-based models learn rich code graph structural information and present\nsuperior performance on various code analysis tasks. However, the robustness of\nthese models against adversarial example attacks in the context of\nvulnerability detection remains an open question. This paper proposes NatGVD, a\nnovel attack methodology that generates natural adversarial vulnerable code to\ncircumvent GNN-based and graph-aware transformer-based vulnerability detectors.\nNatGVD employs a set of code transformations that modify graph structure while\npreserving code semantics. Instead of injecting dead or unrelated code like\nprevious works, NatGVD considers naturalness requirements: generated examples\nshould not be easily recognized by humans or program analysis tools. With\nextensive evaluation of NatGVD on state-of-the-art vulnerability detection\nsystems, the results reveal up to 53.04% evasion rate across GNN-based\ndetectors and graph-aware transformer-based detectors. We also explore\npotential defense strategies to enhance the robustness of these systems against\nNatGVD."
    },
    {
        "date": "2025-10",
        "title": "StructuralDecompose: A Modular Framework for Robust Time Series Decomposition in R",
        "author": "Allen Daniel Sunny",
        "link": "http://arxiv.org/abs/2510.04974v1",
        "abstract": "We present StructuralDecompose, an R package for modular and interpretable\ntime series decomposition. Unlike existing approaches that treat decomposition\nas a monolithic process, StructuralDecompose separates the analysis into\ndistinct components: changepoint detection, anomaly detection, smoothing, and\ndecomposition. This design provides flexibility and robust- ness, allowing\nusers to tailor methods to specific time series characteristics. We demonstrate\nthe package on simulated and real-world datasets, benchmark its performance\nagainst state-of-the- art tools such as Rbeast and autostsm, and discuss its\nrole in interpretable machine learning workflows."
    },
    {
        "date": "2025-10",
        "title": "\u03bcDeepIQA: deep learning-based fast and robust image quality assessment with local predictions for optical microscopy",
        "author": "Elena Corbetta, and Thomas Bocklitz",
        "link": "http://arxiv.org/abs/2510.04859v1",
        "abstract": "Optical microscopy is one of the most widely used techniques in research\nstudies for life sciences and biomedicine. These applications require reliable\nexperimental pipelines to extract valuable knowledge from the measured samples\nand must be supported by image quality assessment (IQA) to ensure correct\nprocessing and analysis of the image data. IQA methods are implemented with\nvariable complexity. However, while most quality metrics have a straightforward\nimplementation, they might be time consuming and computationally expensive when\nevaluating a large dataset. In addition, quality metrics are often designed for\nwell-defined image features and may be unstable for images out of the ideal\ndomain.\n  To overcome these limitations, recent works have proposed deep learning-based\nIQA methods, which can provide superior performance, increased generalizability\nand fast prediction. Our method, named $\\mathrm{\\mu}$DeepIQA, is inspired by\nprevious studies and applies a deep convolutional neural network designed for\nIQA on natural images to optical microscopy measurements. We retrained the same\narchitecture to predict individual quality metrics and global quality scores\nfor optical microscopy data. The resulting models provide fast and stable\npredictions of image quality by generalizing quality estimation even outside\nthe ideal range of standard methods. In addition, $\\mathrm{\\mu}$DeepIQA\nprovides patch-wise prediction of image quality and can be used to visualize\nspatially varying quality in a single image. Our study demonstrates that\noptical microscopy-based studies can benefit from the generalizability of deep\nlearning models due to their stable performance in the presence of outliers,\nthe ability to assess small image patches, and rapid predictions."
    },
    {
        "date": "2025-10",
        "title": "Distributionally Robust Causal Abstractions",
        "author": "Yorgos Felekis, Theodoros Damoulas, and Paris Giampouras",
        "link": "http://arxiv.org/abs/2510.04842v1",
        "abstract": "Causal Abstraction (CA) theory provides a principled framework for relating\ncausal models that describe the same system at different levels of granularity\nwhile ensuring interventional consistency between them. Recently, several\napproaches for learning CAs have been proposed, but all assume fixed and\nwell-specified exogenous distributions, making them vulnerable to environmental\nshifts and misspecification. In this work, we address these limitations by\nintroducing the first class of distributionally robust CAs and their associated\nlearning algorithms. The latter cast robust causal abstraction learning as a\nconstrained min-max optimization problem with Wasserstein ambiguity sets. We\nprovide theoretical results, for both empirical and Gaussian environments,\nleading to principled selection of the level of robustness via the radius of\nthese sets. Furthermore, we present empirical evidence across different\nproblems and CA learning methods, demonstrating our framework's robustness not\nonly to environmental shifts but also to structural model and intervention\nmapping misspecification."
    },
    {
        "date": "2025-10",
        "title": "A Noise Resilient Approach for Robust Hurst Exponent Estimation",
        "author": "Malith Premarathna, Fabrizio Ruggeri, and Dixon Vimalajeewa",
        "link": "http://arxiv.org/abs/2510.04811v1",
        "abstract": "Understanding signal behavior across scales is vital in areas such as natural\nphenomena analysis and financial modeling. A key property is self-similarity,\nquantified by the Hurst exponent (H), which reveals long-term dependencies.\nWavelet-based methods are effective for estimating H due to their multi-scale\nanalysis capability, but additive noise in real-world measurements often\ndegrades accuracy. We propose Noise-Controlled ALPHEE (NC-ALPHEE), an\nenhancement of the Average Level-Pairwise Hurst Exponent Estimator (ALPHEE),\nincorporating noise mitigation and generating multiple level-pairwise estimates\nfrom signal energy pairs. A neural network (NN) combines these estimates,\nreplacing traditional averaging. This adaptive learning maintains ALPHEE's\nbehavior in noise-free cases while improving performance in noisy conditions.\nExtensive simulations show that in noise-free data, NC-ALPHEE matches ALPHEE's\naccuracy using both averaging and NN-based methods. Under noise, however,\ntraditional averaging deteriorates and requires impractical level restrictions,\nwhile NC-ALPHEE consistently outperforms existing techniques without such\nconstraints. NC-ALPHEE offers a robust, adaptive approach for H estimation,\nsignificantly enhancing the reliability of wavelet-based methods in noisy\nenvironments."
    },
    {
        "date": "2025-10",
        "title": "Collusion-Resistant Quantum Secure Key Leasing Beyond Decryption",
        "author": "Fuyuki Kitagawa, Ryo Nishimaki, and Nikhil Pappu",
        "link": "http://arxiv.org/abs/2510.04754v1",
        "abstract": "Secure key leasing (SKL) enables the holder of a secret key for a\ncryptographic function to temporarily lease the key using quantum information.\nLater, the recipient can produce a deletion certificate, which proves that they\nno longer have access to the secret key. The security guarantee ensures that\neven a malicious recipient cannot continue to evaluate the function, after\nproducing a valid deletion certificate.\n  Most prior work considers an adversarial recipient that obtains a single\nleased key, which is insufficient for many applications. In the more realistic\ncollusion-resistant setting, security must hold even when polynomially many\nkeys are leased (and subsequently deleted). However, achieving\ncollusion-resistant SKL from standard assumptions remains poorly understood,\nespecially for functionalities beyond decryption.\n  We improve upon this situation by introducing new pathways for constructing\ncollusion-resistant SKL. Our main contributions are as follows:\n  - A generalization of quantum-secure collusion-resistant traitor tracing\ncalled multi-level traitor tracing (MLTT), and a compiler that transforms an\nMLTT scheme for a primitive X into a collusion-resistant SKL scheme for\nprimitive X.\n  - The first bounded collusion-resistant SKL scheme for PRFs, assuming LWE.\n  - A compiler that upgrades any single-key secure SKL scheme for digital\nsignatures into one with unbounded collusion-resistance, assuming OWFs.\n  - A compiler that upgrades collusion-resistant SKL schemes with classical\ncertificates to ones having verification-query resilience, assuming OWFs."
    },
    {
        "date": "2025-10",
        "title": "Anomaly-Aware YOLO: A Frugal yet Robust Approach to Infrared Small Target Detection",
        "author": "Alina Ciocarlan, Sylvie Le H\u00e9garat-Mascle, and Sidonie Lefebvre",
        "link": "http://arxiv.org/abs/2510.04741v1",
        "abstract": "Infrared Small Target Detection (IRSTD) is a challenging task in defense\napplications, where complex backgrounds and tiny target sizes often result in\nnumerous false alarms using conventional object detectors. To overcome this\nlimitation, we propose Anomaly-Aware YOLO (AA-YOLO), which integrates a\nstatistical anomaly detection test into its detection head. By treating small\ntargets as unexpected patterns against the background, AA-YOLO effectively\ncontrols the false alarm rate. Our approach not only achieves competitive\nperformance on several IRSTD benchmarks, but also demonstrates remarkable\nrobustness in scenarios with limited training data, noise, and domain shifts.\nFurthermore, since only the detection head is modified, our design is highly\ngeneric and has been successfully applied across various YOLO backbones,\nincluding lightweight models. It also provides promising results when\nintegrated into an instance segmentation YOLO. This versatility makes AA-YOLO\nan attractive solution for real-world deployments where resources are\nconstrained. The code will be publicly released."
    },
    {
        "date": "2025-10",
        "title": "Parameter-free Algorithms for the Stochastically Extended Adversarial Model",
        "author": "Shuche Wang, Adarsh Barik, Peng Zhao, and Vincent Y. F. Tan",
        "link": "http://arxiv.org/abs/2510.04685v1",
        "abstract": "We develop the first parameter-free algorithms for the Stochastically\nExtended Adversarial (SEA) model, a framework that bridges adversarial and\nstochastic online convex optimization. Existing approaches for the SEA model\nrequire prior knowledge of problem-specific parameters, such as the diameter of\nthe domain $D$ and the Lipschitz constant of the loss functions $G$, which\nlimits their practical applicability. Addressing this, we develop\nparameter-free methods by leveraging the Optimistic Online Newton Step (OONS)\nalgorithm to eliminate the need for these parameters. We first establish a\ncomparator-adaptive algorithm for the scenario with unknown domain diameter but\nknown Lipschitz constant, achieving an expected regret bound of\n$\\tilde{O}\\big(\\|u\\|_2^2 + \\|u\\|_2(\\sqrt{\\sigma^2_{1:T}} +\n\\sqrt{\\Sigma^2_{1:T}})\\big)$, where $u$ is the comparator vector and\n$\\sigma^2_{1:T}$ and $\\Sigma^2_{1:T}$ represent the cumulative stochastic\nvariance and cumulative adversarial variation, respectively. We then extend\nthis to the more general setting where both $D$ and $G$ are unknown, attaining\nthe comparator- and Lipschitz-adaptive algorithm. Notably, the regret bound\nexhibits the same dependence on $\\sigma^2_{1:T}$ and $\\Sigma^2_{1:T}$,\ndemonstrating the efficacy of our proposed methods even when both parameters\nare unknown in the SEA model."
    },
    {
        "date": "2025-10",
        "title": "Backing the Wrong Horse: How Bit-Level Netlist Augmentation can Counter Power Side Channel Attacks",
        "author": "Ali Asghar, Andreas Becher, and Daniel Ziener",
        "link": "http://arxiv.org/abs/2510.04640v1",
        "abstract": "The dependence of power-consumption on the processed data is a known\nvulnerability of CMOS circuits, resulting in side channels which can be\nexploited by power-based side channel attacks (SCAs). These attacks can extract\nsensitive information, such as secret keys, from the implementation of\ncryptographic algorithms. Existing countermeasures against power-based side\nchannel attacks focus on analyzing information leakage at the byte level.\nHowever, this approach neglects the impact of individual bits on the overall\nresistance of a cryptographic implementation. In this work, we present a\ncountermeasure based on single-bit leakage. The results suggest that the\nproposed countermeasure cannot be broken by attacks using conventional SCA\nleakage models."
    },
    {
        "date": "2025-10",
        "title": "Forecasting-Based Biomedical Time-series Data Synthesis for Open Data and Robust AI",
        "author": "Youngjoon Lee, Seongmin Cho, Yehhyun Jo, Jinu Gong, Hyunjoo Jenny Lee, and Joonhyuk Kang",
        "link": "http://arxiv.org/abs/2510.04622v1",
        "abstract": "The limited data availability due to strict privacy regulations and\nsignificant resource demands severely constrains biomedical time-series AI\ndevelopment, which creates a critical gap between data requirements and\naccessibility. Synthetic data generation presents a promising solution by\nproducing artificial datasets that maintain the statistical properties of real\nbiomedical time-series data without compromising patient confidentiality. We\npropose a framework for synthetic biomedical time-series data generation based\non advanced forecasting models that accurately replicates complex\nelectrophysiological signals such as EEG and EMG with high fidelity. These\nsynthetic datasets preserve essential temporal and spectral properties of real\ndata, which enables robust analysis while effectively addressing data scarcity\nand privacy challenges. Our evaluations across multiple subjects demonstrate\nthat the generated synthetic data can serve as an effective substitute for real\ndata and also significantly boost AI model performance. The approach maintains\ncritical biomedical features while provides high scalability for various\napplications and integrates seamlessly into open-source repositories,\nsubstantially expanding resources for AI-driven biomedical research."
    },
    {
        "date": "2025-10",
        "title": "Computational Certified Deletion Property of Magic Square Game and its Application to Classical Secure Key Leasing",
        "author": "Yuki Takeuchi, and Duo Xu",
        "link": "http://arxiv.org/abs/2510.04529v2",
        "abstract": "We present the first construction of a computational Certified Deletion\nProperty (CDP) achievable with classical communication, derived from the\ncompilation of the non-local Magic Square Game (MSG). We leverage the KLVY\ncompiler to transform the non-local MSG into a 2-round interactive protocol,\nrigorously demonstrating that this compilation preserves the game-specific CDP.\nPreviously, the quantum value and rigidity of the compiled game were\ninvestigated. We emphasize that we are the first to investigate CDP (local\nrandomness in [Fu and Miller, Phys. Rev. A 97, 032324 (2018)]) for the compiled\ngame. Then, we combine this CDP with the framework [Kitagawa, Morimae, and\nYamakawa, Eurocrypt 2025] to construct Secure Key Leasing with classical Lessor\n(cSKL). SKL enables the Lessor to lease the secret key to the Lessee and verify\nthat a quantum Lessee has indeed deleted the key. In this paper, we realize\ncSKL for PKE, PRF, and digital signature. Compared to prior works for cSKL, we\nrealize cSKL for PRF and digital signature for the first time. In addition, we\nsucceed in weakening the assumption needed to construct cSKL."
    },
    {
        "date": "2025-10",
        "title": "P2P: A Poison-to-Poison Remedy for Reliable Backdoor Defense in LLMs",
        "author": "Shuai Zhao, Xinyi Wu, Shiqian Zhao, Xiaobao Wu, Zhongliang Guo, Yanhao Jia, and Anh Tuan Luu",
        "link": "http://arxiv.org/abs/2510.04503v1",
        "abstract": "During fine-tuning, large language models (LLMs) are increasingly vulnerable\nto data-poisoning backdoor attacks, which compromise their reliability and\ntrustworthiness. However, existing defense strategies suffer from limited\ngeneralization: they only work on specific attack types or task settings. In\nthis study, we propose Poison-to-Poison (P2P), a general and effective backdoor\ndefense algorithm. P2P injects benign triggers with safe alternative labels\ninto a subset of training samples and fine-tunes the model on this re-poisoned\ndataset by leveraging prompt-based learning. This enforces the model to\nassociate trigger-induced representations with safe outputs, thereby overriding\nthe effects of original malicious triggers. Thanks to this robust and\ngeneralizable trigger-based fine-tuning, P2P is effective across task settings\nand attack types. Theoretically and empirically, we show that P2P can\nneutralize malicious backdoors while preserving task performance. We conduct\nextensive experiments on classification, mathematical reasoning, and summary\ngeneration tasks, involving multiple state-of-the-art LLMs. The results\ndemonstrate that our P2P algorithm significantly reduces the attack success\nrate compared with baseline models. We hope that the P2P can serve as a\nguideline for defending against backdoor attacks and foster the development of\na secure and trustworthy LLM community."
    },
    {
        "date": "2025-10",
        "title": "SECA: Semantically Equivalent and Coherent Attacks for Eliciting LLM Hallucinations",
        "author": "Buyun Liang, Liangzu Peng, Jinqi Luo, Darshan Thaker, Kwan Ho Ryan Chan, and Ren\u00e9 Vidal",
        "link": "http://arxiv.org/abs/2510.04398v1",
        "abstract": "Large Language Models (LLMs) are increasingly deployed in high-risk domains.\nHowever, state-of-the-art LLMs often produce hallucinations, raising serious\nconcerns about their reliability. Prior work has explored adversarial attacks\nfor hallucination elicitation in LLMs, but it often produces unrealistic\nprompts, either by inserting gibberish tokens or by altering the original\nmeaning. As a result, these approaches offer limited insight into how\nhallucinations may occur in practice. While adversarial attacks in computer\nvision often involve realistic modifications to input images, the problem of\nfinding realistic adversarial prompts for eliciting LLM hallucinations has\nremained largely underexplored. To address this gap, we propose Semantically\nEquivalent and Coherent Attacks (SECA) to elicit hallucinations via realistic\nmodifications to the prompt that preserve its meaning while maintaining\nsemantic coherence. Our contributions are threefold: (i) we formulate finding\nrealistic attacks for hallucination elicitation as a constrained optimization\nproblem over the input prompt space under semantic equivalence and coherence\nconstraints; (ii) we introduce a constraint-preserving zeroth-order method to\neffectively search for adversarial yet feasible prompts; and (iii) we\ndemonstrate through experiments on open-ended multiple-choice question\nanswering tasks that SECA achieves higher attack success rates while incurring\nalmost no constraint violations compared to existing methods. SECA highlights\nthe sensitivity of both open-source and commercial gradient-inaccessible LLMs\nto realistic and plausible prompt variations. Code is available at\nhttps://github.com/Buyun-Liang/SECA."
    },
    {
        "date": "2025-10",
        "title": "Unmasking Backdoors: An Explainable Defense via Gradient-Attention Anomaly Scoring for Pre-trained Language Models",
        "author": "Anindya Sundar Das, Kangjie Chen, and Monowar Bhuyan",
        "link": "http://arxiv.org/abs/2510.04347v1",
        "abstract": "Pre-trained language models have achieved remarkable success across a wide\nrange of natural language processing (NLP) tasks, particularly when fine-tuned\non large, domain-relevant datasets. However, they remain vulnerable to backdoor\nattacks, where adversaries embed malicious behaviors using trigger patterns in\nthe training data. These triggers remain dormant during normal usage, but, when\nactivated, can cause targeted misclassifications. In this work, we investigate\nthe internal behavior of backdoored pre-trained encoder-based language models,\nfocusing on the consistent shift in attention and gradient attribution when\nprocessing poisoned inputs; where the trigger token dominates both attention\nand gradient signals, overriding the surrounding context. We propose an\ninference-time defense that constructs anomaly scores by combining token-level\nattention and gradient information. Extensive experiments on text\nclassification tasks across diverse backdoor attack scenarios demonstrate that\nour method significantly reduces attack success rates compared to existing\nbaselines. Furthermore, we provide an interpretability-driven analysis of the\nscoring mechanism, shedding light on trigger localization and the robustness of\nthe proposed defense."
    },
    {
        "date": "2025-10",
        "title": "Learning to Predict Chaos: Curriculum-Driven Training for Robust Forecasting of Chaotic Dynamics",
        "author": "Harshil Vejendla",
        "link": "http://arxiv.org/abs/2510.04342v1",
        "abstract": "Forecasting chaotic systems is a cornerstone challenge in many scientific\nfields, complicated by the exponential amplification of even infinitesimal\nprediction errors. Modern machine learning approaches often falter due to two\nopposing pitfalls: over-specializing on a single, well-known chaotic system\n(e.g., Lorenz-63), which limits generalizability, or indiscriminately mixing\nvast, unrelated time-series, which prevents the model from learning the nuances\nof any specific dynamical regime. We propose Curriculum Chaos Forecasting\n(CCF), a training paradigm that bridges this gap. CCF organizes training data\nbased on fundamental principles of dynamical systems theory, creating a\ncurriculum that progresses from simple, periodic behaviors to highly complex,\nchaotic dynamics. We quantify complexity using the largest Lyapunov exponent\nand attractor dimension, two well-established metrics of chaos. By first\ntraining a sequence model on predictable systems and gradually introducing more\nchaotic trajectories, CCF enables the model to build a robust and generalizable\nrepresentation of dynamical behaviors. We curate a library of over 50 synthetic\nODE/PDE systems to build this curriculum. Our experiments show that\npre-training with CCF significantly enhances performance on unseen, real-world\nbenchmarks. On datasets including Sunspot numbers, electricity demand, and\nhuman ECG signals, CCF extends the valid prediction horizon by up to 40%\ncompared to random-order training and more than doubles it compared to training\non real-world data alone. We demonstrate that this benefit is consistent across\nvarious neural architectures (GRU, Transformer) and provide extensive ablations\nto validate the importance of the curriculum's structure."
    },
    {
        "date": "2025-10",
        "title": "VortexPIA: Indirect Prompt Injection Attack against LLMs for Efficient Extraction of User Privacy",
        "author": "Yu Cui, Sicheng Pan, Yifei Liu, Haibin Zhang, and Cong Zuo",
        "link": "http://arxiv.org/abs/2510.04261v1",
        "abstract": "Large language models (LLMs) have been widely deployed in Conversational AIs\n(CAIs), while exposing privacy and security threats. Recent research shows that\nLLM-based CAIs can be manipulated to extract private information from human\nusers, posing serious security threats. However, the methods proposed in that\nstudy rely on a white-box setting that adversaries can directly modify the\nsystem prompt. This condition is unlikely to hold in real-world deployments.\nThe limitation raises a critical question: can unprivileged attackers still\ninduce such privacy risks in practical LLM-integrated applications? To address\nthis question, we propose \\textsc{VortexPIA}, a novel indirect prompt injection\nattack that induces privacy extraction in LLM-integrated applications under\nblack-box settings. By injecting token-efficient data containing false\nmemories, \\textsc{VortexPIA} misleads LLMs to actively request private\ninformation in batches. Unlike prior methods, \\textsc{VortexPIA} allows\nattackers to flexibly define multiple categories of sensitive data. We evaluate\n\\textsc{VortexPIA} on six LLMs, covering both traditional and reasoning LLMs,\nacross four benchmark datasets. The results show that \\textsc{VortexPIA}\nsignificantly outperforms baselines and achieves state-of-the-art (SOTA)\nperformance. It also demonstrates efficient privacy requests, reduced token\nconsumption, and enhanced robustness against defense mechanisms. We further\nvalidate \\textsc{VortexPIA} on multiple realistic open-source LLM-integrated\napplications, demonstrating its practical effectiveness."
    },
    {
        "date": "2025-10",
        "title": "AgentTypo: Adaptive Typographic Prompt Injection Attacks against Black-box Multimodal Agents",
        "author": "Yanjie Li, Yiming Cao, Dong Wang, and Bin Xiao",
        "link": "http://arxiv.org/abs/2510.04257v1",
        "abstract": "Multimodal agents built on large vision-language models (LVLMs) are\nincreasingly deployed in open-world settings but remain highly vulnerable to\nprompt injection, especially through visual inputs. We introduce AgentTypo, a\nblack-box red-teaming framework that mounts adaptive typographic prompt\ninjection by embedding optimized text into webpage images. Our automatic\ntypographic prompt injection (ATPI) algorithm maximizes prompt reconstruction\nby substituting captioners while minimizing human detectability via a stealth\nloss, with a Tree-structured Parzen Estimator guiding black-box optimization\nover text placement, size, and color. To further enhance attack strength, we\ndevelop AgentTypo-pro, a multi-LLM system that iteratively refines injection\nprompts using evaluation feedback and retrieves successful past examples for\ncontinual learning. Effective prompts are abstracted into generalizable\nstrategies and stored in a strategy repository, enabling progressive knowledge\naccumulation and reuse in future attacks. Experiments on the VWA-Adv benchmark\nacross Classifieds, Shopping, and Reddit scenarios show that AgentTypo\nsignificantly outperforms the latest image-based attacks such as AgentAttack.\nOn GPT-4o agents, our image-only attack raises the success rate from 0.23 to\n0.45, with consistent results across GPT-4V, GPT-4o-mini, Gemini 1.5 Pro, and\nClaude 3 Opus. In image+text settings, AgentTypo achieves 0.68 ASR, also\noutperforming the latest baselines. Our findings reveal that AgentTypo poses a\npractical and potent threat to multimodal agents and highlight the urgent need\nfor effective defense."
    },
    {
        "date": "2025-10",
        "title": "Concept-Based Masking: A Patch-Agnostic Defense Against Adversarial Patch Attacks",
        "author": "Ayushi Mehrotra, Derek Peng, Dipkamal Bhusal, and Nidhi Rastogi",
        "link": "http://arxiv.org/abs/2510.04245v1",
        "abstract": "Adversarial patch attacks pose a practical threat to deep learning models by\nforcing targeted misclassifications through localized perturbations, often\nrealized in the physical world. Existing defenses typically assume prior\nknowledge of patch size or location, limiting their applicability. In this\nwork, we propose a patch-agnostic defense that leverages concept-based\nexplanations to identify and suppress the most influential concept activation\nvectors, thereby neutralizing patch effects without explicit detection.\nEvaluated on Imagenette with a ResNet-50, our method achieves higher robust and\nclean accuracy than the state-of-the-art PatchCleanser, while maintaining\nstrong performance across varying patch sizes and locations. Our results\nhighlight the promise of combining interpretability with robustness and suggest\nconcept-driven defenses as a scalable strategy for securing machine learning\nmodels against adversarial patch attacks."
    },
    {
        "date": "2025-10",
        "title": "Harnessing LLM for Noise-Robust Cognitive Diagnosis in Web-Based Intelligent Education Systems",
        "author": "Guixian Zhang, Guan Yuan, Ziqi Xu, Yanmei Zhang, Jing Ren, Zhenyun Deng, and Debo Cheng",
        "link": "http://arxiv.org/abs/2510.04093v2",
        "abstract": "Cognitive diagnostics in the Web-based Intelligent Education System (WIES)\naims to assess students' mastery of knowledge concepts from heterogeneous,\nnoisy interactions. Recent work has tried to utilize Large Language Models\n(LLMs) for cognitive diagnosis, yet LLMs struggle with structured data and are\nprone to noise-induced misjudgments. Specially, WIES's open environment\ncontinuously attracts new students and produces vast amounts of response logs,\nexacerbating the data imbalance and noise issues inherent in traditional\neducational systems. To address these challenges, we propose DLLM, a\nDiffusion-based LLM framework for noise-robust cognitive diagnosis. DLLM first\nconstructs independent subgraphs based on response correctness, then applies\nrelation augmentation alignment module to mitigate data imbalance. The two\nsubgraph representations are then fused and aligned with LLM-derived,\nsemantically augmented representations. Importantly, before each alignment\nstep, DLLM employs a two-stage denoising diffusion module to eliminate\nintrinsic noise while assisting structural representation alignment.\nSpecifically, unconditional denoising diffusion first removes erroneous\ninformation, followed by conditional denoising diffusion based on graph-guided\nto eliminate misleading information. Finally, the noise-robust representation\nthat integrates semantic knowledge and structural information is fed into\nexisting cognitive diagnosis models for prediction. Experimental results on\nthree publicly available web-based educational platform datasets demonstrate\nthat our DLLM achieves optimal predictive performance across varying noise\nlevels, which demonstrates that DLLM achieves noise robustness while\neffectively leveraging semantic knowledge from LLM."
    },
    {
        "date": "2025-10",
        "title": "Quantifying Distributional Robustness of Agentic Tool-Selection",
        "author": "Jehyeok Yeon, Isha Chaudhary, and Gagandeep Singh",
        "link": "http://arxiv.org/abs/2510.03992v1",
        "abstract": "Large language models (LLMs) are increasingly deployed in agentic systems\nwhere they map user intents to relevant external tools to fulfill a task. A\ncritical step in this process is tool selection, where a retriever first\nsurfaces candidate tools from a larger pool, after which the LLM selects the\nmost appropriate one. This pipeline presents an underexplored attack surface\nwhere errors in selection can lead to severe outcomes like unauthorized data\naccess or denial of service, all without modifying the agent's model or code.\nWhile existing evaluations measure task performance in benign settings, they\noverlook the specific vulnerabilities of the tool selection mechanism under\nadversarial conditions. To address this gap, we introduce ToolCert, the first\nstatistical framework that formally certifies tool selection robustness.\nToolCert models tool selection as a Bernoulli success process and evaluates it\nagainst a strong, adaptive attacker who introduces adversarial tools with\nmisleading metadata, and are iteratively refined based on the agent's previous\nchoices. By sampling these adversarial interactions, ToolCert produces a\nhigh-confidence lower bound on accuracy, formally quantifying the agent's\nworst-case performance. Our evaluation with ToolCert uncovers the severe\nfragility: under attacks injecting deceptive tools or saturating retrieval, the\ncertified accuracy bound drops near zero, an average performance drop of over\n60% compared to non-adversarial settings. For attacks targeting the retrieval\nand selection stages, the certified accuracy bound plummets to less than 20%\nafter just a single round of adversarial adaptation. ToolCert thus reveals\npreviously unexamined security threats inherent to tool selection and provides\na principled method to quantify an agent's robustness to such threats, a\nnecessary step for the safe deployment of agentic systems."
    },
    {
        "date": "2025-10",
        "title": "BONSAI: Structure-exploiting robust Bayesian optimization for networked black-box systems under uncertainty",
        "author": "Akshay Kudva, and Joel A. Paulson",
        "link": "http://arxiv.org/abs/2510.03893v1",
        "abstract": "Optimal design under uncertainty remains a fundamental challenge in advancing\nreliable, next-generation process systems. Robust optimization (RO) offers a\nprincipled approach by safeguarding against worst-case scenarios across a range\nof uncertain parameters. However, traditional RO methods typically require\nknown problem structure, which limits their applicability to high-fidelity\nsimulation environments. To overcome these limitations, recent work has\nexplored robust Bayesian optimization (RBO) as a flexible alternative that can\naccommodate expensive, black-box objectives. Existing RBO methods, however,\ngenerally ignore available structural information and struggle to scale to\nhigh-dimensional settings. In this work, we introduce BONSAI (Bayesian\nOptimization of Network Systems under uncertAInty), a new RBO framework that\nleverages partial structural knowledge commonly available in simulation-based\nmodels. Instead of treating the objective as a monolithic black box, BONSAI\nrepresents it as a directed graph of interconnected white- and black-box\ncomponents, allowing the algorithm to utilize intermediate information within\nthe optimization process. We further propose a scalable Thompson sampling-based\nacquisition function tailored to the structured RO setting, which can be\nefficiently optimized using gradient-based methods. We evaluate BONSAI across a\ndiverse set of synthetic and real-world case studies, including applications in\nprocess systems engineering. Compared to existing simulation-based RO\nalgorithms, BONSAI consistently delivers more sample-efficient and\nhigher-quality robust solutions, highlighting its practical advantages for\nuncertainty-aware design in complex engineering systems."
    },
    {
        "date": "2025-10",
        "title": "Adversarial Agent Collaboration for C to Rust Translation",
        "author": "Tianyu Li, Ruishi Li, Bo Wang, Brandon Paulsen, Umang Mathur, and Prateek Saxena",
        "link": "http://arxiv.org/abs/2510.03879v1",
        "abstract": "Translating C to memory-safe languages, like Rust, prevents critical memory\nsafety vulnerabilities that are prevalent in legacy C software. Existing\napproaches for C to safe Rust translation, including LLM-assisted ones, do not\ngeneralize on larger (> 500 LoC) C codebases because they depend on complex\nprogram analyses that frequently break. In this work, we present ACToR\n(Adversarial C To Rust translator), a simple LLM agent-based approach. Inspired\nby GANs, ACToR pits a generator agent against a discriminator agent, which\ncollaborate to iteratively generate a Rust translation. On each iteration, the\ntranslator agent synthesizes and refines a Rust translation to pass an existing\nsuite of tests, and then the discriminator agent finds new failing tests. We\ndemonstrate that ACToR translates all of the 63 real-world command line\nutilities considered in our benchmarks, which have an average size of 485 lines\nof code, and it achieves over 90% test pass rate with zero human intervention.\nTo our knowledge, it is the first such system that reliably translates C\nprograms of this scale. Furthermore, ACToR improves translation correctness by\nup to 18.9% compared to baseline, non-adversarial approaches."
    },
    {
        "date": "2025-10",
        "title": "SDAKD: Student Discriminator Assisted Knowledge Distillation for Super-Resolution Generative Adversarial Networks",
        "author": "Nikolaos Kaparinos, and Vasileios Mezaris",
        "link": "http://arxiv.org/abs/2510.03870v1",
        "abstract": "Generative Adversarial Networks (GANs) achieve excellent performance in\ngenerative tasks, such as image super-resolution, but their computational\nrequirements make difficult their deployment on resource-constrained devices.\nWhile knowledge distillation is a promising research direction for GAN\ncompression, effectively training a smaller student generator is challenging\ndue to the capacity mismatch between the student generator and the teacher\ndiscriminator. In this work, we propose Student Discriminator Assisted\nKnowledge Distillation (SDAKD), a novel GAN distillation methodology that\nintroduces a student discriminator to mitigate this capacity mismatch. SDAKD\nfollows a three-stage training strategy, and integrates an adapted feature map\ndistillation approach in its last two training stages. We evaluated SDAKD on\ntwo well-performing super-resolution GANs, GCFSR and Real-ESRGAN. Our\nexperiments demonstrate consistent improvements over the baselines and SOTA GAN\nknowledge distillation methods. The SDAKD source code will be made openly\navailable upon acceptance of the paper."
    },
    {
        "date": "2025-10",
        "title": "Technical note on Fisher Information for Robust Federated Cross-Validation",
        "author": "Behraj Khan, and Tahir Qasim Syed",
        "link": "http://arxiv.org/abs/2510.03838v1",
        "abstract": "When training data are fragmented across batches or federated-learned across\ndifferent geographic locations, trained models manifest performance\ndegradation. That degradation partly owes to covariate shift induced by data\nhaving been fragmented across time and space and producing dissimilar empirical\ntraining distributions. Each fragment's distribution is slightly different to a\nhypothetical unfragmented training distribution of covariates, and to the\nsingle validation distribution. To address this problem, we propose Fisher\nInformation for Robust fEderated validation (\\textbf{FIRE}). This method\naccumulates fragmentation-induced covariate shift divergences from the global\ntraining distribution via an approximate Fisher information. That term, which\nwe prove to be a more computationally-tractable estimate, is then used as a\nper-fragment loss penalty, enabling scalable distribution alignment. FIRE\noutperforms importance weighting benchmarks by $5.1\\%$ at maximum and federated\nlearning (FL) benchmarks by up to $5.3\\%$ on shifted validation sets."
    },
    {
        "date": "2025-10",
        "title": "Towards Robust and Generalizable Continuous Space-Time Video Super-Resolution with Events",
        "author": "Shuoyan Wei, Feng Li, Shengeng Tang, Runmin Cong, Yao Zhao, Meng Wang, and Huihui Bai",
        "link": "http://arxiv.org/abs/2510.03833v1",
        "abstract": "Continuous space-time video super-resolution (C-STVSR) has garnered\nincreasing interest for its capability to reconstruct high-resolution and\nhigh-frame-rate videos at arbitrary spatial and temporal scales. However,\nprevailing methods often generalize poorly, producing unsatisfactory results\nwhen applied to out-of-distribution (OOD) scales. To overcome this limitation,\nwe present EvEnhancer, a novel approach that marries the unique properties of\nhigh temporal resolution and high dynamic range encapsulated in event streams\nto achieve robust and generalizable C-STVSR. Our approach incorporates\nevent-adapted synthesis that capitalizes on the spatiotemporal correlations\nbetween frames and events to capture long-term motion trajectories, enabling\nadaptive interpolation and fusion across space and time. This is then coupled\nwith a local implicit video transformer that integrates local implicit video\nneural function with cross-scale spatiotemporal attention to learn continuous\nvideo representations and generate plausible videos at arbitrary resolutions\nand frame rates. We further develop EvEnhancerPlus, which builds a controllable\nswitching mechanism that dynamically determines the reconstruction difficulty\nfor each spatiotemporal pixel based on local event statistics. This allows the\nmodel to adaptively route reconstruction along the most suitable pathways at a\nfine-grained pixel level, substantially reducing computational overhead while\nmaintaining excellent performance. Furthermore, we devise a cross-derivative\ntraining strategy that stabilizes the convergence of such a multi-pathway\nframework through staged cross-optimization. Extensive experiments demonstrate\nthat our method achieves state-of-the-art performance on both synthetic and\nreal-world datasets, while maintaining superior generalizability at OOD scales.\nThe code is available at https://github.com/W-Shuoyan/EvEnhancerPlus."
    },
    {
        "date": "2025-10",
        "title": "Pilot Contamination Attacks Detection with Machine Learning for Multi-User Massive MIMO",
        "author": "Pedro Ivo da Cruz, Dimitri Silva, Tito Spadini, Ricardo Suyama, and Murilo Bellezoni Loiola",
        "link": "http://arxiv.org/abs/2510.03831v1",
        "abstract": "Massive multiple-input multiple-output (MMIMO) is essential to modern\nwireless communication systems, like 5G and 6G, but it is vulnerable to active\neavesdropping attacks. One type of such attack is the pilot contamination\nattack (PCA), where a malicious user copies pilot signals from an authentic\nuser during uplink, intentionally interfering with the base station's (BS)\nchannel estimation accuracy. In this work, we propose to use a Decision Tree\n(DT) algorithm for PCA detection at the BS in a multi-user system. We present a\nmethodology to generate training data for the DT classifier and select the best\nDT according to their depth. Then, we simulate different scenarios that could\nbe encountered in practice and compare the DT to a classical technique based on\nlikelihood ratio testing (LRT) submitted to the same scenarios. The results\nrevealed that a DT with only one level of depth is sufficient to outperform the\nLRT. The DT shows a good performance regarding the probability of detection in\nnoisy scenarios and when the malicious user transmits with low power, in which\ncase the LRT fails to detect the PCA. We also show that the reason for the good\nperformance of the DT is its ability to compute a threshold that separates PCA\ndata from non-PCA data better than the LRT's threshold. Moreover, the DT does\nnot necessitate prior knowledge of noise power or assumptions regarding the\nsignal power of malicious users, prerequisites typically essential for LRT and\nother hypothesis testing methodologies."
    },
    {
        "date": "2025-10",
        "title": "LIBERO-PRO: Towards Robust and Fair Evaluation of Vision-Language-Action Models Beyond Memorization",
        "author": "Xueyang Zhou, Yangming Xu, Guiyao Tie, Yongchao Chen, Guowen Zhang, Duanfeng Chu, Pan Zhou, and Lichao Sun",
        "link": "http://arxiv.org/abs/2510.03827v1",
        "abstract": "LIBERO has emerged as a widely adopted benchmark for evaluating\nVision-Language-Action (VLA) models; however, its current training and\nevaluation settings are problematic, often leading to inflated performance\nestimates and preventing fair model comparison. To address these issues, we\nintroduce LIBERO-PRO, an extended LIBERO benchmark that systematically\nevaluates model performance under reasonable perturbations across four\ndimensions: manipulated objects, initial states, task instructions, and\nenvironments. Experimental results reveal that, although existing models\nachieve over 90% accuracy under the standard LIBERO evaluation, their\nperformance collapses to 0.0% under our generalized setting. Crucially, this\ndiscrepancy exposes the models' reliance on rote memorization of action\nsequences and environment layouts from the training set, rather than genuine\ntask understanding or environmental perception. For instance, models persist in\nexecuting grasping actions when the target object is replaced with irrelevant\nitems, and their outputs remain unchanged even when given corrupted\ninstructions or even messy tokens. These findings expose the severe flaws in\ncurrent evaluation practices, and we call on the community to abandon\nmisleading methodologies in favor of robust assessments of model generalization\nand comprehension. Our code is available at:\nhttps://github.com/Zxy-MLlab/LIBERO-PRO."
    },
    {
        "date": "2025-10",
        "title": "Security Analysis of Ponzi Schemes in Ethereum Smart Contracts",
        "author": "Chunyi Zhang, Qinghong Wei, and Xiaoqi Li",
        "link": "http://arxiv.org/abs/2510.03819v1",
        "abstract": "The rapid advancement of blockchain technology has precipitated the\nwidespread adoption of Ethereum and smart contracts across a variety of\nsectors. However, this has also given rise to numerous fraudulent activities,\nwith many speculators embedding Ponzi schemes within smart contracts, resulting\nin significant financial losses for investors. Currently, there is a lack of\neffective methods for identifying and analyzing such new types of fraudulent\nactivities. This paper categorizes these scams into four structural types and\nexplores the intrinsic characteristics of Ponzi scheme contract source code\nfrom a program analysis perspective. The Mythril tool is employed to conduct\nstatic and dynamic analyses of representative cases, thereby revealing their\nvulnerabilities and operational mechanisms. Furthermore, this paper employs\nshell scripts and command patterns to conduct batch detection of open-source\nsmart contract code, thereby unveiling the common characteristics of Ponzi\nscheme smart contracts."
    },
    {
        "date": "2025-10",
        "title": "Robust Batched Bandits",
        "author": "Yunwen Guo, Yunlun Shu, Gongyi Zhuo, and Tianyu Wang",
        "link": "http://arxiv.org/abs/2510.03798v1",
        "abstract": "The batched multi-armed bandit (MAB) problem, in which rewards are collected\nin batches, is crucial for applications such as clinical trials. Existing\nresearch predominantly assumes light-tailed reward distributions, yet many\nreal-world scenarios, including clinical outcomes, exhibit heavy-tailed\ncharacteristics. This paper bridges this gap by proposing robust batched bandit\nalgorithms designed for heavy-tailed rewards, within both finite-arm and\nLipschitz-continuous settings. We reveal a surprising phenomenon: in the\ninstance-independent regime, as well as in the Lipschitz setting,\nheavier-tailed rewards necessitate a smaller number of batches to achieve\nnear-optimal regret. In stark contrast, for the instance-dependent setting, the\nrequired number of batches to attain near-optimal regret remains invariant with\nrespect to tail heaviness."
    },
    {
        "date": "2025-10",
        "title": "LoRA Patching: Exposing the Fragility of Proactive Defenses against Deepfakes",
        "author": "Zuomin Qu, Yimao Guo, Qianyue Hu, and Wei Lu",
        "link": "http://arxiv.org/abs/2510.03747v1",
        "abstract": "Deepfakes pose significant societal risks, motivating the development of\nproactive defenses that embed adversarial perturbations in facial images to\nprevent manipulation. However, in this paper, we show that these preemptive\ndefenses often lack robustness and reliability. We propose a novel approach,\nLow-Rank Adaptation (LoRA) patching, which injects a plug-and-play LoRA patch\ninto Deepfake generators to bypass state-of-the-art defenses. A learnable\ngating mechanism adaptively controls the effect of the LoRA patch and prevents\ngradient explosions during fine-tuning. We also introduce a Multi-Modal Feature\nAlignment (MMFA) loss, encouraging the features of adversarial outputs to align\nwith those of the desired outputs at the semantic level. Beyond bypassing, we\npresent defensive LoRA patching, embedding visible warnings in the outputs as a\ncomplementary solution to mitigate this newly identified security\nvulnerability. With only 1,000 facial examples and a single epoch of\nfine-tuning, LoRA patching successfully defeats multiple proactive defenses.\nThese results reveal a critical weakness in current paradigms and underscore\nthe need for more robust Deepfake defense strategies. Our code is available at\nhttps://github.com/ZOMIN28/LoRA-Patching."
    },
    {
        "date": "2025-10",
        "title": "Securing Operating Systems Through Fine-grained Kernel Access Limitation for IoT Systems",
        "author": "Dongyang Zhan, Zhaofeng Yu, Xiangzhan Yu, Hongli Zhang, Lin Ye, and Likun Liu",
        "link": "http://arxiv.org/abs/2510.03737v1",
        "abstract": "With the development of Internet of Things (IoT), it is gaining a lot of\nattention. It is important to secure the embedded systems with low overhead.\nThe Linux Seccomp is widely used by developers to secure the kernels by\nblocking the access of unused syscalls, which introduces less overhead.\nHowever, there are no systematic Seccomp configuration approaches for IoT\napplications without the help of developers. In addition, the existing Seccomp\nconfiguration approaches are coarse-grained, which cannot analyze and limit the\nsyscall arguments. In this paper, a novel static dependent syscall analysis\napproach for embedded applications is proposed, which can obtain all of the\npossible dependent syscalls and the corresponding arguments of the target\napplications. So, a fine-grained kernel access limitation can be performed for\nthe IoT applications. To this end, the mappings between dynamic library APIs\nand syscalls according with their arguments are built, by analyzing the control\nflow graphs and the data dependency relationships of the dynamic libraries. To\nthe best of our knowledge, this is the first work to generate the fine-grained\nSeccomp profile for embedded applications."
    },
    {
        "date": "2025-10",
        "title": "Shrinking the Kernel Attack Surface Through Static and Dynamic Syscall Limitation",
        "author": "Dongyang Zhan, Zhaofeng Yu, Xiangzhan Yu, Hongli Zhang, and Lin Ye",
        "link": "http://arxiv.org/abs/2510.03720v1",
        "abstract": "Linux Seccomp is widely used by the program developers and the system\nmaintainers to secure the operating systems, which can block unused syscalls\nfor different applications and containers to shrink the attack surface of the\noperating systems. However, it is difficult to configure the whitelist of a\ncontainer or application without the help of program developers. Docker\ncontainers block about only 50 syscalls by default, and lots of unblocked\nuseless syscalls introduce a big kernel attack surface. To obtain the dependent\nsyscalls, dynamic tracking is a straight-forward approach but it cannot get the\nfull syscall list. Static analysis can construct an over-approximated syscall\nlist, but the list contains many false positives. In this paper, a systematic\ndependent syscall analysis approach, sysverify, is proposed by combining static\nanalysis and dynamic verification together to shrink the kernel attack surface.\nThe semantic gap between the binary executables and syscalls is bridged by\nanalyzing the binary and the source code, which builds the mapping between the\nlibrary APIs and syscalls systematically. To further reduce the attack surface\nat best effort, we propose a dynamic verification approach to intercept and\nanalyze the security of the invocations of indirect-call-related or rarely\ninvoked syscalls with low overhead."
    },
    {
        "date": "2025-10",
        "title": "Backdoor-Powered Prompt Injection Attacks Nullify Defense Methods",
        "author": "Yulin Chen, Haoran Li, Yuan Sui, Yangqiu Song, and Bryan Hooi",
        "link": "http://arxiv.org/abs/2510.03705v1",
        "abstract": "With the development of technology, large language models (LLMs) have\ndominated the downstream natural language processing (NLP) tasks. However,\nbecause of the LLMs' instruction-following abilities and inability to\ndistinguish the instructions in the data content, such as web pages from search\nengines, the LLMs are vulnerable to prompt injection attacks. These attacks\ntrick the LLMs into deviating from the original input instruction and executing\nthe attackers' target instruction. Recently, various instruction hierarchy\ndefense strategies are proposed to effectively defend against prompt injection\nattacks via fine-tuning. In this paper, we explore more vicious attacks that\nnullify the prompt injection defense methods, even the instruction hierarchy:\nbackdoor-powered prompt injection attacks, where the attackers utilize the\nbackdoor attack for prompt injection attack purposes. Specifically, the\nattackers poison the supervised fine-tuning samples and insert the backdoor\ninto the model. Once the trigger is activated, the backdoored model executes\nthe injected instruction surrounded by the trigger. We construct a benchmark\nfor comprehensive evaluation. Our experiments demonstrate that backdoor-powered\nprompt injection attacks are more harmful than previous prompt injection\nattacks, nullifying existing prompt injection defense methods, even the\ninstruction hierarchy techniques."
    },
    {
        "date": "2025-10",
        "title": "REG: A Regularization Optimizer for Robust Training Dynamics",
        "author": "Zehua Liu, Han Wu, Xiaojin Fu, Shuqi Liu, Xiongwei Han, Tao Zhong, and Mingxuan Yuan",
        "link": "http://arxiv.org/abs/2510.03691v1",
        "abstract": "Optimizers are crucial for the efficient training of Large Language Models\n(LLMs). While AdamW is the de facto standard, recent structure-aware optimizers\nlike Muon have emerged, which regularize gradient updates by operating on\nentire weight matrices. The Muon optimizer balances the gradient updates along\nall the directions. However, Muon's reliance on the matrix sign function can\nlead to training instability, exhibits incompatibility when fine-tuning models\npre-trained with AdamW. To address these limitations, we propose \\textbf{REG},\na novel optimizer that replaces Muon's aggressive matrix sign operator with the\nRow-and-Column-Scaling (RACS) operator. Theoretically grounded in balancing a\nmatrix, the RACS operator regularizes the update steps in a less drastic\nmanner, making it simpler to implement and more compatible with established\ntraining dynamics. Through extensive empirical experiments on LLM training, we\ndemonstrate that our REG optimizer not only achieves superior performance and\nstability over AdamW, but also maintains consistency with the AdamW training\nparadigm. This consistency is particularly evident during the fine-tuning\nstage, where REG optimizer avoids the performance degradation observed with\nMuon."
    },
    {
        "date": "2025-10",
        "title": "From Theory to Practice: Evaluating Data Poisoning Attacks and Defenses in In-Context Learning on Social Media Health Discourse",
        "author": "Rabeya Amin Jhuma, and Mostafa Mohaimen Akand Faisal",
        "link": "http://arxiv.org/abs/2510.03636v1",
        "abstract": "This study explored how in-context learning (ICL) in large language models\ncan be disrupted by data poisoning attacks in the setting of public health\nsentiment analysis. Using tweets of Human Metapneumovirus (HMPV), small\nadversarial perturbations such as synonym replacement, negation insertion, and\nrandomized perturbation were introduced into the support examples. Even these\nminor manipulations caused major disruptions, with sentiment labels flipping in\nup to 67% of cases. To address this, a Spectral Signature Defense was applied,\nwhich filtered out poisoned examples while keeping the data's meaning and\nsentiment intact. After defense, ICL accuracy remained steady at around 46.7%,\nand logistic regression validation reached 100% accuracy, showing that the\ndefense successfully preserved the dataset's integrity. Overall, the findings\nextend prior theoretical studies of ICL poisoning to a practical, high-stakes\nsetting in public health discourse analysis, highlighting both the risks and\npotential defenses for robust LLM deployment. This study also highlights the\nfragility of ICL under attack and the value of spectral defenses in making AI\nsystems more reliable for health-related social media monitoring."
    },
    {
        "date": "2025-10",
        "title": "Explainable but Vulnerable: Adversarial Attacks on XAI Explanation in Cybersecurity Applications",
        "author": "Maraz Mia, and Mir Mehedi A. Pritom",
        "link": "http://arxiv.org/abs/2510.03623v1",
        "abstract": "Explainable Artificial Intelligence (XAI) has aided machine learning (ML)\nresearchers with the power of scrutinizing the decisions of the black-box\nmodels. XAI methods enable looking deep inside the models' behavior, eventually\ngenerating explanations along with a perceived trust and transparency. However,\ndepending on any specific XAI method, the level of trust can vary. It is\nevident that XAI methods can themselves be a victim of post-adversarial attacks\nthat manipulate the expected outcome from the explanation module. Among such\nattack tactics, fairwashing explanation (FE), manipulation explanation (ME),\nand backdoor-enabled manipulation attacks (BD) are the notable ones. In this\npaper, we try to understand these adversarial attack techniques, tactics, and\nprocedures (TTPs) on explanation alteration and thus the effect on the model's\ndecisions. We have explored a total of six different individual attack\nprocedures on post-hoc explanation methods such as SHAP (SHapley Additive\nexPlanations), LIME (Local Interpretable Model-agnostic Explanation), and IG\n(Integrated Gradients), and investigated those adversarial attacks in\ncybersecurity applications scenarios such as phishing, malware, intrusion, and\nfraudulent website detection. Our experimental study reveals the actual\neffectiveness of these attacks, thus providing an urgency for immediate\nattention to enhance the resiliency of XAI methods and their applications."
    },
    {
        "date": "2025-10",
        "title": "Machine Unlearning Meets Adversarial Robustness via Constrained Interventions on LLMs",
        "author": "Fatmazohra Rezkellah, and Ramzi Dakhmouche",
        "link": "http://arxiv.org/abs/2510.03567v1",
        "abstract": "With the increasing adoption of Large Language Models (LLMs), more\ncustomization is needed to ensure privacy-preserving and safe generation. We\naddress this objective from two critical aspects: unlearning of sensitive\ninformation and robustness to jail-breaking attacks. We investigate various\nconstrained optimization formulations that address both aspects in a\n\\emph{unified manner}, by finding the smallest possible interventions on LLM\nweights that either make a given vocabulary set unreachable or embed the LLM\nwith robustness to tailored attacks by shifting part of the weights to a\n\\emph{safer} region. Beyond unifying two key properties, this approach\ncontrasts with previous work in that it doesn't require an oracle classifier\nthat is typically not available or represents a computational overhead.\nSurprisingly, we find that the simplest point-wise constraint-based\nintervention we propose leads to better performance than max-min interventions,\nwhile having a lower computational cost. Comparison against state-of-the-art\ndefense methods demonstrates superior performance of the proposed approach."
    },
    {
        "date": "2025-10",
        "title": "A Quantum-Secure Voting Framework Using QKD, Dual-Key Symmetric Encryption, and Verifiable Receipts",
        "author": "Taha M. Mahmoud, and Naima Kaabouch",
        "link": "http://arxiv.org/abs/2510.03489v1",
        "abstract": "Electronic voting systems face growing risks from cyberattacks and data\nbreaches, which are expected to intensify with the advent of quantum computing.\nTo address these challenges, we introduce a quantum-secure voting framework\nthat integrates Quantum Key Distribution (QKD), Dual-Key Symmetric Encryption,\nand verifiable receipt mechanisms to strengthen the privacy, integrity, and\nreliability of the voting process. The framework enables voters to establish\nencryption keys securely, cast encrypted ballots, and verify their votes\nthrough receipt-based confirmation, all without exposing the vote contents. To\nevaluate performance, we simulate both quantum and classical communication\nchannels using the Message Queuing Telemetry Transport (MQTT) protocol. Results\ndemonstrate that the system can process large numbers of votes efficiently with\nlow latency and minimal error rates. This approach offers a scalable and\npractical path toward secure, transparent, and verifiable electronic voting in\nthe quantum era."
    },
    {
        "date": "2025-10",
        "title": "Security Analysis and Threat Modeling of Research Management Applications [Extended Version]",
        "author": "Boniface M. Sindala, and Ragib Hasan",
        "link": "http://arxiv.org/abs/2510.03407v1",
        "abstract": "Research management applications (RMA) are widely used in clinical research\nenvironments to collect, transmit, analyze, and store sensitive data. This data\nis so valuable making RMAs susceptible to security threats. This analysis,\nanalyzes RMAs' security, focusing on Research Electronic Data Capture (REDCap)\nas an example. We explore the strengths and vulnerabilities within RMAs by\nevaluating the architecture, data flow, and security features. We identify and\nassess potential risks using the MITRE ATT\\&CK framework and STRIDE model. We\nassess REDCap's defenses against common attack vectors focusing on security to\nprovide confidentiality, integrity, availability, non-repudiation, and\nauthentication. We conclude by proposing recommendations for enhancing the\nsecurity of RMAs, ensuring that critical research data remains protected\nwithout compromising usability. This research aims to contribute towards a more\nsecure framework for managing sensitive information in research-intensive\nenvironments."
    },
    {
        "date": "2025-10",
        "title": "Test-Time Defense Against Adversarial Attacks via Stochastic Resonance of Latent Ensembles",
        "author": "Dong Lao, Yuxiang Zhang, Haniyeh Ehsani Oskouie, Yangchao Wu, Alex Wong, and Stefano Soatto",
        "link": "http://arxiv.org/abs/2510.03224v1",
        "abstract": "We propose a test-time defense mechanism against adversarial attacks:\nimperceptible image perturbations that significantly alter the predictions of a\nmodel. Unlike existing methods that rely on feature filtering or smoothing,\nwhich can lead to information loss, we propose to \"combat noise with noise\" by\nleveraging stochastic resonance to enhance robustness while minimizing\ninformation loss. Our approach introduces small translational perturbations to\nthe input image, aligns the transformed feature embeddings, and aggregates them\nbefore mapping back to the original reference image. This can be expressed in a\nclosed-form formula, which can be deployed on diverse existing network\narchitectures without introducing additional network modules or fine-tuning for\nspecific attack types. The resulting method is entirely training-free,\narchitecture-agnostic, and attack-agnostic. Empirical results show\nstate-of-the-art robustness on image classification and, for the first time,\nestablish a generic test-time defense for dense prediction tasks, including\nstereo matching and optical flow, highlighting the method's versatility and\npracticality. Specifically, relative to clean (unperturbed) performance, our\nmethod recovers up to 68.1% of the accuracy loss on image classification, 71.9%\non stereo matching, and 29.2% on optical flow under various types of\nadversarial attacks."
    },
    {
        "date": "2025-10",
        "title": "MM-Nav: Multi-View VLA Model for Robust Visual Navigation via Multi-Expert Learning",
        "author": "Tianyu Xu, Jiawei Chen, Jiazhao Zhang, Wenyao Zhang, Zekun Qi, Minghan Li, Zhizheng Zhang, and He Wang",
        "link": "http://arxiv.org/abs/2510.03142v1",
        "abstract": "Visual navigation policy is widely regarded as a promising direction, as it\nmimics humans by using egocentric visual observations for navigation. However,\noptical information of visual observations is difficult to be explicitly\nmodeled like LiDAR point clouds or depth maps, which subsequently requires\nintelligent models and large-scale data. To this end, we propose to leverage\nthe intelligence of the Vision-Language-Action (VLA) model to learn diverse\nnavigation capabilities from synthetic expert data in a teacher-student manner.\nSpecifically, we implement the VLA model, MM-Nav, as a multi-view VLA (with 360\nobservations) based on pretrained large language models and visual foundation\nmodels. For large-scale navigation data, we collect expert data from three\nreinforcement learning (RL) experts trained with privileged depth information\nin three challenging tailor-made environments for different navigation\ncapabilities: reaching, squeezing, and avoiding. We iteratively train our VLA\nmodel using data collected online from RL experts, where the training ratio is\ndynamically balanced based on performance on individual capabilities. Through\nextensive experiments in synthetic environments, we demonstrate that our model\nachieves strong generalization capability. Moreover, we find that our student\nVLA model outperforms the RL teachers, demonstrating the synergistic effect of\nintegrating multiple capabilities. Extensive real-world experiments further\nconfirm the effectiveness of our method."
    },
    {
        "date": "2025-10",
        "title": "A Robust Clustered Federated Learning Approach for Non-IID Data with Quantity Skew",
        "author": "Michael Ben Ali, Imen Megdiche, Andr\u00e9 Peninou, and Olivier Teste",
        "link": "http://arxiv.org/abs/2510.03380v1",
        "abstract": "Federated Learning (FL) is a decentralized paradigm that enables a\nclient-server architecture to collaboratively train a global Artificial\nIntelligence model without sharing raw data, thereby preserving privacy. A key\nchallenge in FL is Non-IID data. Quantity Skew (QS) is a particular problem of\nNon-IID, where clients hold highly heterogeneous data volumes. Clustered\nFederated Learning (CFL) is an emergent variant of FL that presents a promising\nsolution to Non-IID problem. It improves models' performance by grouping\nclients with similar data distributions into clusters. CFL methods generally\nfall into two operating strategies. In the first strategy, clients select the\ncluster that minimizes the local training loss. In the second strategy, the\nserver groups clients based on local model similarities. However, most CFL\nmethods lack systematic evaluation under QS but present significant challenges\nbecause of it. In this paper, we present two main contributions. The first one\nis an evaluation of state-of-the-art CFL algorithms under various Non-IID\nsettings, applying multiple QS scenarios to assess their robustness. Our second\ncontribution is a novel iterative CFL algorithm, named CORNFLQS, which proposes\nan optimal coordination between both operating strategies of CFL. Our approach\nis robust against the different variations of QS settings. We conducted\nintensive experiments on six image classification datasets, resulting in 270\nNon-IID configurations. The results show that CORNFLQS achieves the highest\naverage ranking in both accuracy and clustering quality, as well as strong\nrobustness to QS perturbations. Overall, our approach outperforms actual CFL\nalgorithms."
    },
    {
        "date": "2025-10",
        "title": "InsideOut: An EfficientNetV2-S Based Deep Learning Framework for Robust Multi-Class Facial Emotion Recognition",
        "author": "Ahsan Farabi, Israt Khandaker, Ibrahim Khalil Shanto, Md Abdul Ahad Minhaz, and Tanisha Zaman",
        "link": "http://arxiv.org/abs/2510.03066v1",
        "abstract": "Facial Emotion Recognition (FER) is a key task in affective computing,\nenabling applications in human-computer interaction, e-learning, healthcare,\nand safety systems. Despite advances in deep learning, FER remains challenging\ndue to occlusions, illumination and pose variations, subtle intra-class\ndifferences, and dataset imbalance that hinders recognition of minority\nemotions. We present InsideOut, a reproducible FER framework built on\nEfficientNetV2-S with transfer learning, strong data augmentation, and\nimbalance-aware optimization. The approach standardizes FER2013 images, applies\nstratified splitting and augmentation, and fine-tunes a lightweight\nclassification head with class-weighted loss to address skewed distributions.\nInsideOut achieves 62.8% accuracy with a macro averaged F1 of 0.590 on FER2013,\nshowing competitive results compared to conventional CNN baselines. The novelty\nlies in demonstrating that efficient architectures, combined with tailored\nimbalance handling, can provide practical, transparent, and reproducible FER\nsolutions."
    },
    {
        "date": "2025-10",
        "title": "Learning Robust Diffusion Models from Imprecise Supervision",
        "author": "Dong-Dong Wu, Jiacheng Cui, Wei Wang, Zhiqiang She, and Masashi Sugiyama",
        "link": "http://arxiv.org/abs/2510.03016v1",
        "abstract": "Conditional diffusion models have achieved remarkable success in various\ngenerative tasks recently, but their training typically relies on large-scale\ndatasets that inevitably contain imprecise information in conditional inputs.\nSuch supervision, often stemming from noisy, ambiguous, or incomplete labels,\nwill cause condition mismatch and degrade generation quality. To address this\nchallenge, we propose DMIS, a unified framework for training robust Diffusion\nModels from Imprecise Supervision, which is the first systematic study within\ndiffusion models. Our framework is derived from likelihood maximization and\ndecomposes the objective into generative and classification components: the\ngenerative component models imprecise-label distributions, while the\nclassification component leverages a diffusion classifier to infer\nclass-posterior probabilities, with its efficiency further improved by an\noptimized timestep sampling strategy. Extensive experiments on diverse forms of\nimprecise supervision, covering tasks of image generation, weakly supervised\nlearning, and noisy dataset condensation demonstrate that DMIS consistently\nproduces high-quality and class-discriminative samples."
    },
    {
        "date": "2025-10",
        "title": "Not every day is a sunny day: Synthetic cloud injection for deep land cover segmentation robustness evaluation across data sources",
        "author": "Sara Mobsite, Renaud Hostache, Laure Berti Equille, Emmanuel Roux, and Joris Guerin",
        "link": "http://arxiv.org/abs/2510.03006v1",
        "abstract": "Supervised deep learning for land cover semantic segmentation (LCS) relies on\nlabeled satellite data. However, most existing Sentinel-2 datasets are\ncloud-free, which limits their usefulness in tropical regions where clouds are\ncommon. To properly evaluate the extent of this problem, we developed a cloud\ninjection algorithm that simulates realistic cloud cover, allowing us to test\nhow Sentinel-1 radar data can fill in the gaps caused by cloud-obstructed\noptical imagery. We also tackle the issue of losing spatial and/or spectral\ndetails during encoder downsampling in deep networks. To mitigate this loss, we\npropose a lightweight method that injects Normalized Difference Indices (NDIs)\ninto the final decoding layers, enabling the model to retain key spatial\nfeatures with minimal additional computation. Injecting NDIs enhanced land\ncover segmentation performance on the DFC2020 dataset, yielding improvements of\n1.99% for U-Net and 2.78% for DeepLabV3 on cloud-free imagery. Under\ncloud-covered conditions, incorporating Sentinel-1 data led to significant\nperformance gains across all models compared to using optical data alone,\nhighlighting the effectiveness of radar-optical fusion in challenging\natmospheric scenarios."
    },
    {
        "date": "2025-10",
        "title": "Untargeted Jailbreak Attack",
        "author": "Xinzhe Huang, Wenjing Hu, Tianhang Zheng, Kedong Xiu, Xiaojun Jia, Di Wang, Zhan Qin, and Kui Ren",
        "link": "http://arxiv.org/abs/2510.02999v1",
        "abstract": "Existing gradient-based jailbreak attacks on Large Language Models (LLMs),\nsuch as Greedy Coordinate Gradient (GCG) and COLD-Attack, typically optimize\nadversarial suffixes to align the LLM output with a predefined target response.\nHowever, by restricting the optimization objective as inducing a predefined\ntarget, these methods inherently constrain the adversarial search space, which\nlimit their overall attack efficacy. Furthermore, existing methods typically\nrequire a large number of optimization iterations to fulfill the large gap\nbetween the fixed target and the original model response, resulting in low\nattack efficiency.\n  To overcome the limitations of targeted jailbreak attacks, we propose the\nfirst gradient-based untargeted jailbreak attack (UJA), aiming to elicit an\nunsafe response without enforcing any predefined patterns. Specifically, we\nformulate an untargeted attack objective to maximize the unsafety probability\nof the LLM response, which can be quantified using a judge model. Since the\nobjective is non-differentiable, we further decompose it into two\ndifferentiable sub-objectives for optimizing an optimal harmful response and\nthe corresponding adversarial prompt, with a theoretical analysis to validate\nthe decomposition. In contrast to targeted jailbreak attacks, UJA's\nunrestricted objective significantly expands the search space, enabling a more\nflexible and efficient exploration of LLM vulnerabilities.Extensive evaluations\ndemonstrate that \\textsc{UJA} can achieve over 80\\% attack success rates\nagainst recent safety-aligned LLMs with only 100 optimization iterations,\noutperforming the state-of-the-art gradient-based attacks such as I-GCG and\nCOLD-Attack by over 20\\%."
    },
    {
        "date": "2025-10",
        "title": "External Data Extraction Attacks against Retrieval-Augmented Large Language Models",
        "author": "Yu He, Yifei Chen, Yiming Li, Shuo Shao, Leyi Qi, Boheng Li, Dacheng Tao, and Zhan Qin",
        "link": "http://arxiv.org/abs/2510.02964v1",
        "abstract": "In recent years, RAG has emerged as a key paradigm for enhancing large\nlanguage models (LLMs). By integrating externally retrieved information, RAG\nalleviates issues like outdated knowledge and, crucially, insufficient domain\nexpertise. While effective, RAG introduces new risks of external data\nextraction attacks (EDEAs), where sensitive or copyrighted data in its\nknowledge base may be extracted verbatim. These risks are particularly acute\nwhen RAG is used to customize specialized LLM applications with private\nknowledge bases. Despite initial studies exploring these risks, they often lack\na formalized framework, robust attack performance, and comprehensive\nevaluation, leaving critical questions about real-world EDEA feasibility\nunanswered.\n  In this paper, we present the first comprehensive study to formalize EDEAs\nagainst retrieval-augmented LLMs. We first formally define EDEAs and propose a\nunified framework decomposing their design into three components: extraction\ninstruction, jailbreak operator, and retrieval trigger, under which prior\nattacks can be considered instances within our framework. Guided by this\nframework, we develop SECRET: a Scalable and EffeCtive exteRnal data Extraction\naTtack. Specifically, SECRET incorporates (1) an adaptive optimization process\nusing LLMs as optimizers to generate specialized jailbreak prompts for EDEAs,\nand (2) cluster-focused triggering, an adaptive strategy that alternates\nbetween global exploration and local exploitation to efficiently generate\neffective retrieval triggers. Extensive evaluations across 4 models reveal that\nSECRET significantly outperforms previous attacks, and is highly effective\nagainst all 16 tested RAG instances. Notably, SECRET successfully extracts 35%\nof the data from RAG powered by Claude 3.7 Sonnet for the first time, whereas\nother attacks yield 0% extraction. Our findings call for attention to this\nemerging threat."
    },
    {
        "date": "2025-10",
        "title": "SoK: Kicking CAN Down the Road. Systematizing CAN Security Knowledge",
        "author": "Khaled Serag, Zhaozhou Tang, Sungwoo Kim, Vireshwar Kumar, Dave, Tian, Saman Zonouz, Raheem Beyah, Dongyan Xu, and Z. Berkay Celik",
        "link": "http://arxiv.org/abs/2510.02960v1",
        "abstract": "For decades, the Controller Area Network (CAN) has served as the primary\nin-vehicle bus (IVB) and extended its use to many non-vehicular systems. Over\nthe past years, CAN security has been intensively scrutinized, yielding\nextensive research literature. Despite its wealth, the literature lacks\nstructured systematization, complicating efforts to assess attack severity,\ndefense efficacy, identify security gaps, or root causes. This leaves non\nexperts uncertain about the relevancy of specific attacks or defenses to their\nsystems, inadvertently portraying CAN as irredeemably insecure. Further, the\nintroduction of new IVB technologies--CAN evolutions, add-ons, and alternative\nbuses--with heightened security claims risks fostering the misconception that\nmerely adopting these technologies resolves CAN's security challenges.\n  This paper systematizes existing CAN security knowledge, presenting a\ncomprehensive taxonomy and assessment models of attackers, attacks, and\ndefenses. It identifies replicable attacks and defense gaps, investigating\ntheir root causes as inherent, accidental, unique, or universal. It then\nextrapolates these insights to emerging IVB technologies by formally analyzing\nthree emerging IVBs to identify shared root causes with CAN and assess their\nability to close security gaps. The findings challenge common perceptions,\ndemonstrating that CAN is more securable than perceived, that most insecurity\nroot causes are shared across IVBs, and that merely adopting newer IVB\ntechnology does not solve persistent security issues. The paper concludes by\nhighlighting future research directions to secure IVB communication down the\nroad."
    },
    {
        "date": "2025-10",
        "title": "Zero-Shot Robustness of Vision Language Models Via Confidence-Aware Weighting",
        "author": "Nikoo Naghavian, and Mostafa Tavassolipour",
        "link": "http://arxiv.org/abs/2510.02913v1",
        "abstract": "Vision-language models like CLIP demonstrate impressive zero-shot\ngeneralization but remain highly vulnerable to adversarial attacks. In this\nwork, we propose Confidence-Aware Weighting (CAW) to enhance zero-shot\nrobustness in vision-language models. CAW consists of two components: (1) a\nConfidence-Aware loss that prioritizes uncertain adversarial examples by\nscaling the KL divergence between clean and adversarial predictions, and (2) a\nfeature alignment regularization that preserves semantic consistency by\nminimizing the distance between frozen and fine-tuned image encoder features on\nadversarial inputs. These components work jointly to improve both clean and\nrobust accuracy without sacrificing generalization. Extensive experiments on\nTinyImageNet and 14 additional datasets show that CAW outperforms recent\nmethods such as PMG-AFT and TGA-ZSR under strong attacks like AutoAttack, while\nusing less memory."
    },
    {
        "date": "2025-10",
        "title": "Attack via Overfitting: 10-shot Benign Fine-tuning to Jailbreak LLMs",
        "author": "Zhixin Xie, Xurui Song, and Jun Luo",
        "link": "http://arxiv.org/abs/2510.02833v1",
        "abstract": "Despite substantial efforts in safety alignment, recent research indicates\nthat Large Language Models (LLMs) remain highly susceptible to jailbreak\nattacks. Among these attacks, finetuning-based ones that compromise LLMs'\nsafety alignment via fine-tuning stand out due to its stable jailbreak\nperformance. In particular, a recent study indicates that fine-tuning with as\nfew as 10 harmful question-answer (QA) pairs can lead to successful\njailbreaking across various harmful questions. However, such malicious\nfine-tuning attacks are readily detectable and hence thwarted by moderation\nmodels. In this paper, we demonstrate that LLMs can be jailbroken by\nfine-tuning with only 10 benign QA pairs; our attack exploits the increased\nsensitivity of LLMs to fine-tuning data after being overfitted. Specifically,\nour fine-tuning process starts with overfitting an LLM via fine-tuning with\nbenign QA pairs involving identical refusal answers. Further fine-tuning is\nthen performed with standard benign answers, causing the overfitted LLM to\nforget the refusal attitude and thus provide compliant answers regardless of\nthe harmfulness of a question. We implement our attack on the ten LLMs and\ncompare it with five existing baselines. Experiments demonstrate that our\nmethod achieves significant advantages in both attack effectiveness and attack\nstealth. Our findings expose previously unreported security vulnerabilities in\ncurrent LLMs and provide a new perspective on understanding how LLMs' security\nis compromised, even with benign fine-tuning. Our code is available at\nhttps://github.com/ZHIXINXIE/tenBenign."
    },
    {
        "date": "2025-10",
        "title": "Mitigating Spurious Correlation via Distributionally Robust Learning with Hierarchical Ambiguity Sets",
        "author": "Sung Ho Jo, Seonghwi Kim, and Minwoo Chae",
        "link": "http://arxiv.org/abs/2510.02818v1",
        "abstract": "Conventional supervised learning methods are often vulnerable to spurious\ncorrelations, particularly under distribution shifts in test data. To address\nthis issue, several approaches, most notably Group DRO, have been developed.\nWhile these methods are highly robust to subpopulation or group shifts, they\nremain vulnerable to intra-group distributional shifts, which frequently occur\nin minority groups with limited samples. We propose a hierarchical extension of\nGroup DRO that addresses both inter-group and intra-group uncertainties,\nproviding robustness to distribution shifts at multiple levels. We also\nintroduce new benchmark settings that simulate realistic minority group\ndistribution shifts-an important yet previously underexplored challenge in\nspurious correlation research. Our method demonstrates strong robustness under\nthese conditions-where existing robust learning methods consistently fail-while\nalso achieving superior performance on standard benchmarks. These results\nhighlight the importance of broadening the ambiguity set to better capture both\ninter-group and intra-group distributional uncertainties."
    },
    {
        "date": "2025-10",
        "title": "Work Zones challenge VLM Trajectory Planning: Toward Mitigation and Robust Autonomous Driving",
        "author": "Yifan Liao, Zhen Sun, Xiaoyun Qiu, Zixiao Zhao, Wenbing Tang, Xinlei He, Xinhu Zheng, Tianwei Zhang, Xinyi Huang, and Xingshuo Han",
        "link": "http://arxiv.org/abs/2510.02803v1",
        "abstract": "Visual Language Models (VLMs), with powerful multimodal reasoning\ncapabilities, are gradually integrated into autonomous driving by several\nautomobile manufacturers to enhance planning capability in challenging\nenvironments. However, the trajectory planning capability of VLMs in work\nzones, which often include irregular layouts, temporary traffic control, and\ndynamically changing geometric structures, is still unexplored. To bridge this\ngap, we conduct the \\textit{first} systematic study of VLMs for work zone\ntrajectory planning, revealing that mainstream VLMs fail to generate correct\ntrajectories in $68.0%$ of cases. To better understand these failures, we first\nidentify candidate patterns via subgraph mining and clustering analysis, and\nthen confirm the validity of $8$ common failure patterns through human\nverification. Building on these findings, we propose REACT-Drive, a trajectory\nplanning framework that integrates VLMs with Retrieval-Augmented Generation\n(RAG). Specifically, REACT-Drive leverages VLMs to convert prior failure cases\ninto constraint rules and executable trajectory planning code, while RAG\nretrieves similar patterns in new scenarios to guide trajectory generation.\nExperimental results on the ROADWork dataset show that REACT-Drive yields a\nreduction of around $3\\times$ in average displacement error relative to VLM\nbaselines under evaluation with Qwen2.5-VL. In addition, REACT-Drive yields the\nlowest inference time ($0.58$s) compared with other methods such as fine-tuning\n($17.90$s). We further conduct experiments using a real vehicle in 15 work zone\nscenarios in the physical world, demonstrating the strong practicality of\nREACT-Drive."
    },
    {
        "date": "2025-10",
        "title": "Net2Net: When Un-trained Meets Pre-trained Networks for Robust Real-World Denoising",
        "author": "Weimin Yuan, and Cai Meng",
        "link": "http://arxiv.org/abs/2510.02733v1",
        "abstract": "Traditional denoising methods for noise removal have largely relied on\nhandcrafted priors, often perform well in controlled environments but struggle\nto address the complexity and variability of real noise. In contrast, deep\nlearning-based approaches have gained prominence for learning noise\ncharacteristics from large datasets, but these methods frequently require\nextensive labeled data and may not generalize effectively across diverse noise\ntypes and imaging conditions. In this paper, we present an innovative method,\ntermed as Net2Net, that combines the strengths of untrained and pre-trained\nnetworks to tackle the challenges of real-world noise removal. The innovation\nof Net2Net lies in its combination of unsupervised DIP and supervised\npre-trained model DRUNet by regularization by denoising (RED). The untrained\nnetwork adapts to the unique noise characteristics of each input image without\nrequiring labeled data, while the pre-trained network leverages learned\nrepresentations from large-scale datasets to deliver robust denoising\nperformance. This hybrid framework enhances generalization across varying noise\npatterns and improves performance, particularly in scenarios with limited\ntraining data. Extensive experiments on benchmark datasets demonstrate the\nsuperiority of our method for real-world noise removal."
    },
    {
        "date": "2025-10",
        "title": "Hybrid-Collaborative Augmentation and Contrastive Sample Adaptive-Differential Awareness for Robust Attributed Graph Clustering",
        "author": "Tianxiang Zhao, Youqing Wang, Jinlu Wang, Jiapu Wang, Mingliang Cui, Junbin Gao, and Jipeng Guo",
        "link": "http://arxiv.org/abs/2510.02731v1",
        "abstract": "Due to its powerful capability of self-supervised representation learning and\nclustering, contrastive attributed graph clustering (CAGC) has achieved great\nsuccess, which mainly depends on effective data augmentation and contrastive\nobjective setting. However, most CAGC methods utilize edges as auxiliary\ninformation to obtain node-level embedding representation and only focus on\nnode-level embedding augmentation. This approach overlooks edge-level embedding\naugmentation and the interactions between node-level and edge-level embedding\naugmentations across various granularity. Moreover, they often treat all\ncontrastive sample pairs equally, neglecting the significant differences\nbetween hard and easy positive-negative sample pairs, which ultimately limits\ntheir discriminative capability. To tackle these issues, a novel robust\nattributed graph clustering (RAGC), incorporating hybrid-collaborative\naugmentation (HCA) and contrastive sample adaptive-differential awareness\n(CSADA), is proposed. First, node-level and edge-level embedding\nrepresentations and augmentations are simultaneously executed to establish a\nmore comprehensive similarity measurement criterion for subsequent contrastive\nlearning. In turn, the discriminative similarity further consciously guides\nedge augmentation. Second, by leveraging pseudo-label information with high\nconfidence, a CSADA strategy is elaborately designed, which adaptively\nidentifies all contrastive sample pairs and differentially treats them by an\ninnovative weight modulation function. The HCA and CSADA modules mutually\nreinforce each other in a beneficent cycle, thereby enhancing discriminability\nin representation learning. Comprehensive graph clustering evaluations over six\nbenchmark datasets demonstrate the effectiveness of the proposed RAGC against\nseveral state-of-the-art CAGC methods."
    },
    {
        "date": "2025-10",
        "title": "Time-To-Inconsistency: A Survival Analysis of Large Language Model Robustness to Adversarial Attacks",
        "author": "Yubo Li, Ramayya Krishnan, and Rema Padman",
        "link": "http://arxiv.org/abs/2510.02712v1",
        "abstract": "Large Language Models (LLMs) have revolutionized conversational AI, yet their\nrobustness in extended multi-turn dialogues remains poorly understood. Existing\nevaluation frameworks focus on static benchmarks and single-turn assessments,\nfailing to capture the temporal dynamics of conversational degradation that\ncharacterize real-world interactions. In this work, we present the first\ncomprehensive survival analysis of conversational AI robustness, analyzing\n36,951 conversation turns across 9 state-of-the-art LLMs to model failure as a\ntime-to-event process. Our survival modeling framework-employing Cox\nproportional hazards, Accelerated Failure Time, and Random Survival Forest\napproaches-reveals extraordinary temporal dynamics. We find that abrupt,\nprompt-to-prompt(P2P) semantic drift is catastrophic, dramatically increasing\nthe hazard of conversational failure. In stark contrast, gradual, cumulative\ndrift is highly protective, vastly reducing the failure hazard and enabling\nsignificantly longer dialogues. AFT models with interactions demonstrate\nsuperior performance, achieving excellent discrimination and exceptional\ncalibration. These findings establish survival analysis as a powerful paradigm\nfor evaluating LLM robustness, offer concrete insights for designing resilient\nconversational agents, and challenge prevailing assumptions about the necessity\nof semantic consistency in conversational AI Systems."
    },
    {
        "date": "2025-10",
        "title": "A Statistical Method for Attack-Agnostic Adversarial Attack Detection with Compressive Sensing Comparison",
        "author": "Chinthana Wimalasuriya, and Spyros Tragoudas",
        "link": "http://arxiv.org/abs/2510.02707v1",
        "abstract": "Adversarial attacks present a significant threat to modern machine learning\nsystems. Yet, existing detection methods often lack the ability to detect\nunseen attacks or detect different attack types with a high level of accuracy.\nIn this work, we propose a statistical approach that establishes a detection\nbaseline before a neural network's deployment, enabling effective real-time\nadversarial detection. We generate a metric of adversarial presence by\ncomparing the behavior of a compressed/uncompressed neural network pair. Our\nmethod has been tested against state-of-the-art techniques, and it achieves\nnear-perfect detection across a wide range of attack types. Moreover, it\nsignificantly reduces false positives, making it both reliable and practical\nfor real-world applications."
    },
    {
        "date": "2025-10",
        "title": "ARMs: Adaptive Red-Teaming Agent against Multimodal Models with Plug-and-Play Attacks",
        "author": "Zhaorun Chen, Xun Liu, Mintong Kang, Jiawei Zhang, Minzhou Pan, Shuang Yang, and Bo Li",
        "link": "http://arxiv.org/abs/2510.02677v1",
        "abstract": "As vision-language models (VLMs) gain prominence, their multimodal interfaces\nalso introduce new safety vulnerabilities, making the safety evaluation\nchallenging and critical. Existing red-teaming efforts are either restricted to\na narrow set of adversarial patterns or depend heavily on manual engineering,\nlacking scalable exploration of emerging real-world VLM vulnerabilities. To\nbridge this gap, we propose ARMs, an adaptive red-teaming agent that\nsystematically conducts comprehensive risk assessments for VLMs. Given a target\nharmful behavior or risk definition, ARMs automatically optimizes diverse\nred-teaming strategies with reasoning-enhanced multi-step orchestration, to\neffectively elicit harmful outputs from target VLMs. We propose 11 novel\nmultimodal attack strategies, covering diverse adversarial patterns of VLMs\n(e.g., reasoning hijacking, contextual cloaking), and integrate 17 red-teaming\nalgorithms into ARMs via model context protocol (MCP). To balance the diversity\nand effectiveness of the attack, we design a layered memory with an\nepsilon-greedy attack exploration algorithm. Extensive experiments on instance-\nand policy-based benchmarks show that ARMs achieves SOTA attack success rates,\nexceeding baselines by an average of 52.1% and surpassing 90% on\nClaude-4-Sonnet. We show that the diversity of red-teaming instances generated\nby ARMs is significantly higher, revealing emerging vulnerabilities in VLMs.\nLeveraging ARMs, we construct ARMs-Bench, a large-scale multimodal safety\ndataset comprising over 30K red-teaming instances spanning 51 diverse risk\ncategories, grounded in both real-world multimodal threats and regulatory\nrisks. Safety fine-tuning with ARMs-Bench substantially improves the robustness\nof VLMs while preserving their general utility, providing actionable guidance\nto improve multimodal safety alignment against emerging threats."
    },
    {
        "date": "2025-10",
        "title": "Sequence-Preserving Dual-FoV Defense for Traffic Sign and Light Recognition in Autonomous Vehicles",
        "author": "Abhishek Joshi, Jahnavi Krishna Koda, and Abhishek Phadke",
        "link": "http://arxiv.org/abs/2510.02642v1",
        "abstract": "Traffic light and sign recognition are key for Autonomous Vehicles (AVs)\nbecause perception mistakes directly influence navigation and safety. In\naddition to digital adversarial attacks, models are vulnerable to existing\nperturbations (glare, rain, dirt, or graffiti), which could lead to dangerous\nmisclassifications. The current work lacks consideration of temporal\ncontinuity, multistatic field-of-view (FoV) sensing, and robustness to both\ndigital and natural degradation. This study proposes a dual FoV,\nsequence-preserving robustness framework for traffic lights and signs in the\nUSA based on a multi-source dataset built on aiMotive, Udacity, Waymo, and\nself-recorded videos from the region of Texas. Mid and long-term sequences of\nRGB images are temporally aligned for four operational design domains (ODDs):\nhighway, night, rainy, and urban. Over a series of experiments on a real-life\napplication of anomaly detection, this study outlines a unified three-layer\ndefense stack framework that incorporates feature squeezing, defensive\ndistillation, and entropy-based anomaly detection, as well as sequence-wise\ntemporal voting for further enhancement. The evaluation measures included\naccuracy, attack success rate (ASR), risk-weighted misclassification severity,\nand confidence stability. Physical transferability was confirmed using probes\nfor recapture. The results showed that the Unified Defense Stack achieved\n79.8mAP and reduced the ASR to 18.2%, which is superior to YOLOv8, YOLOv9, and\nBEVFormer, while reducing the high-risk misclassification to 32%."
    },
    {
        "date": "2025-10",
        "title": "ToolTweak: An Attack on Tool Selection in LLM-based Agents",
        "author": "Jonathan Sneh, Ruomei Yan, Jialin Yu, Philip Torr, Yarin Gal, Sunando Sengupta, Eric Sommerlade, Alasdair Paren, and Adel Bibi",
        "link": "http://arxiv.org/abs/2510.02554v1",
        "abstract": "As LLMs increasingly power agents that interact with external tools, tool use\nhas become an essential mechanism for extending their capabilities. These\nagents typically select tools from growing databases or marketplaces to solve\nuser tasks, creating implicit competition among tool providers and developers\nfor visibility and usage. In this paper, we show that this selection process\nharbors a critical vulnerability: by iteratively manipulating tool names and\ndescriptions, adversaries can systematically bias agents toward selecting\nspecific tools, gaining unfair advantage over equally capable alternatives. We\npresent ToolTweak, a lightweight automatic attack that increases selection\nrates from a baseline of around 20% to as high as 81%, with strong\ntransferability between open-source and closed-source models. Beyond individual\ntools, we show that such attacks cause distributional shifts in tool usage,\nrevealing risks to fairness, competition, and security in emerging tool\necosystems. To mitigate these risks, we evaluate two defenses: paraphrasing and\nperplexity filtering, which reduce bias and lead agents to select functionally\nsimilar tools more equally. All code will be open-sourced upon acceptance."
    },
    {
        "date": "2025-10",
        "title": "TLoRa: Implementing TLS Over LoRa for Secure HTTP Communication in IoT",
        "author": "Atonu Ghosh, Akhilesh Mohanasundaram, Srishivanth R F, and Sudip Misra",
        "link": "http://arxiv.org/abs/2510.02519v1",
        "abstract": "We present TLoRa, an end-to-end architecture for HTTPS communication over\nLoRa by integrating TCP tunneling and a complete TLS 1.3 handshake. It enables\na seamless and secure communication channel between WiFi-enabled end devices\nand the Internet over LoRa using an End Hub (EH) and a Net Relay (NR). The EH\ntethers a WiFi hotspot and a captive portal for user devices to connect and\nrequest URLs. The EH forwards the requested URLs to the NR using a secure\ntunnel over LoRa. The NR, which acts as a server-side proxy, receives and\nresolves the request from the Internet-based server. It then relays back the\nencrypted response from the server over the same secure tunnel. TLoRa operates\nin three phases -session setup, secure tunneling, and rendering. In the first\nphase, it manages the TCP socket and initiates the TLS handshake. In the\nsecond, it creates a secure tunnel and transfers encrypted TLS data over LoRa.\nFinally, it delivers the URL content to the user. TLoRa also implements a\nlightweight TLS record reassembly layer and a queuing mechanism for session\nmultiplexing. We evaluate TLoRa on real hardware using multiple accesses to a\nweb API. Results indicate that it provides a practical solution by successfully\nestablishing a TLS session over LoRa in 9.9 seconds and takes 3.58 seconds to\nfulfill API requests. To the best of our knowledge, this is the first work to\ncomprehensively design, implement, and evaluate the performance of HTTPS access\nover LoRa using full TLS."
    },
    {
        "date": "2025-10",
        "title": "A Bilevel Optimization Framework for Adversarial Control of Gas Pipeline Operations",
        "author": "Tejaswini Sanjay Katale, Lu Gao, Yunpeng Zhang, and Alaa Senouci",
        "link": "http://arxiv.org/abs/2510.02503v1",
        "abstract": "Cyberattacks on pipeline operational technology systems pose growing risks to\nenergy infrastructure. This study develops a physics-informed simulation and\noptimization framework for analyzing cyber-physical threats in petroleum\npipeline networks. The model integrates networked hydraulic dynamics,\nSCADA-based state estimation, model predictive control (MPC), and a bi-level\nformulation for stealthy false-data injection (FDI) attacks. Pipeline flow and\npressure dynamics are modeled on a directed graph using nodal pressure\nevolution and edge-based Weymouth-type relations, including control-aware\nequipment such as valves and compressors. An extended Kalman filter estimates\nthe full network state from partial SCADA telemetry. The controller computes\npressure-safe control inputs via MPC under actuator constraints and forecasted\ndemands. Adversarial manipulation is formalized as a bi-level optimization\nproblem where an attacker perturbs sensor data to degrade throughput while\nremaining undetected by bad-data detectors. This attack-control interaction is\nsolved via Karush-Kuhn-Tucker (KKT) reformulation, which results in a tractable\nmixed-integer quadratic program. Test gas pipeline case studies demonstrate the\ncovert reduction of service delivery under attack. Results show that\nundetectable attacks can cause sustained throughput loss with minimal\ninstantaneous deviation. This reveals the need for integrated detection and\ncontrol strategies in cyber-physical infrastructure."
    },
    {
        "date": "2025-10",
        "title": "Improved Robustness of Deep Reinforcement Learning for Control of Time-Varying Systems by Bounded Extremum Seeking",
        "author": "Shaifalee Saxena, Alan Williams, Rafael Fierro, and Alexander Scheinker",
        "link": "http://arxiv.org/abs/2510.02490v1",
        "abstract": "In this paper, we study the use of robust model independent bounded extremum\nseeking (ES) feedback control to improve the robustness of deep reinforcement\nlearning (DRL) controllers for a class of nonlinear time-varying systems. DRL\nhas the potential to learn from large datasets to quickly control or optimize\nthe outputs of many-parameter systems, but its performance degrades\ncatastrophically when the system model changes rapidly over time. Bounded ES\ncan handle time-varying systems with unknown control directions, but its\nconvergence speed slows down as the number of tuned parameters increases and,\nlike all local adaptive methods, it can get stuck in local minima. We\ndemonstrate that together, DRL and bounded ES result in a hybrid controller\nwhose performance exceeds the sum of its parts with DRL taking advantage of\nhistorical data to learn how to quickly control a many-parameter system to a\ndesired setpoint while bounded ES ensures its robustness to time variations. We\npresent a numerical study of a general time-varying system and a combined\nES-DRL controller for automatic tuning of the Low Energy Beam Transport section\nat the Los Alamos Neutron Science Center linear particle accelerator."
    },
    {
        "date": "2025-10",
        "title": "Interplay between Security, Privacy and Trust in 6G-enabled Intelligent Transportation Systems",
        "author": "Ahmed Danladi Abdullahi, Erfan Bahrami, Tooska Dargahi, Mohammed Al-Khalidi, and Mohammad Hammoudeh",
        "link": "http://arxiv.org/abs/2510.02487v1",
        "abstract": "The advancement of 6G technology has the potential to revolutionize the\ntransportation sector and significantly improve how we travel. 6G-enabled\nIntelligent Transportation Systems (ITS) promise to offer high-speed,\nlow-latency communication and advanced data analytics capabilities, supporting\nthe development of safer, more efficient, and more sustainable transportation\nsolutions. However, various security and privacy challenges were identified in\nthe literature that must be addressed to enable the safe and secure deployment\nof 6G-ITS and ensure people's trust in using these technologies. This paper\nreviews the opportunities and challenges of 6G-ITS, particularly focusing on\ntrust, security, and privacy, with special attention to quantum technologies\nthat both enhance security through quantum key distribution and introduce new\nvulnerabilities. It discusses the potential benefits of 6G technology in the\ntransportation sector, including improved communication, device\ninteroperability support, data analytic capabilities, and increased automation\nfor different components, such as transportation management and communication\nsystems. A taxonomy of different attack models in 6G-ITS is proposed, and a\ncomparison of the security threats in 5G-ITS and 6G-ITS is provided, along with\npotential mitigating solutions. This research highlights the urgent need for a\ncomprehensive, multi-layered security framework spanning physical\ninfrastructure protection, network protocol security, data management\nsafeguards, application security measures, and trust management systems to\neffectively mitigate emerging security and privacy risks and ensure the\nintegrity and resilience of future transportation ecosystems."
    },
    {
        "date": "2025-10",
        "title": "StealthAttack: Robust 3D Gaussian Splatting Poisoning via Density-Guided Illusions",
        "author": "Bo-Hsu Ke, You-Zhe Xie, Yu-Lun Liu, and Wei-Chen Chiu",
        "link": "http://arxiv.org/abs/2510.02314v1",
        "abstract": "3D scene representation methods like Neural Radiance Fields (NeRF) and 3D\nGaussian Splatting (3DGS) have significantly advanced novel view synthesis. As\nthese methods become prevalent, addressing their vulnerabilities becomes\ncritical. We analyze 3DGS robustness against image-level poisoning attacks and\npropose a novel density-guided poisoning method. Our method strategically\ninjects Gaussian points into low-density regions identified via Kernel Density\nEstimation (KDE), embedding viewpoint-dependent illusory objects clearly\nvisible from poisoned views while minimally affecting innocent views.\nAdditionally, we introduce an adaptive noise strategy to disrupt multi-view\nconsistency, further enhancing attack effectiveness. We propose a KDE-based\nevaluation protocol to assess attack difficulty systematically, enabling\nobjective benchmarking for future research. Extensive experiments demonstrate\nour method's superior performance compared to state-of-the-art techniques.\nProject page: https://hentci.github.io/stealthattack/"
    },
    {
        "date": "2025-10",
        "title": "Robust Tangent Space Estimation via Laplacian Eigenvector Gradient Orthogonalization",
        "author": "Dhruv Kohli, Sawyer J. Robertson, Gal Mishne, and Alexander Cloninger",
        "link": "http://arxiv.org/abs/2510.02308v1",
        "abstract": "Estimating the tangent spaces of a data manifold is a fundamental problem in\ndata analysis. The standard approach, Local Principal Component Analysis\n(LPCA), struggles in high-noise settings due to a critical trade-off in\nchoosing the neighborhood size. Selecting an optimal size requires prior\nknowledge of the geometric and noise characteristics of the data that are often\nunavailable. In this paper, we propose a spectral method, Laplacian Eigenvector\nGradient Orthogonalization (LEGO), that utilizes the global structure of the\ndata to guide local tangent space estimation. Instead of relying solely on\nlocal neighborhoods, LEGO estimates the tangent space at each data point by\northogonalizing the gradients of low-frequency eigenvectors of the graph\nLaplacian. We provide two theoretical justifications of our method. First, a\ndifferential geometric analysis on a tubular neighborhood of a manifold shows\nthat gradients of the low-frequency Laplacian eigenfunctions of the tube align\nclosely with the manifold's tangent bundle, while an eigenfunction with high\ngradient in directions orthogonal to the manifold lie deeper in the spectrum.\nSecond, a random matrix theoretic analysis also demonstrates that low-frequency\neigenvectors are robust to sub-Gaussian noise. Through comprehensive\nexperiments, we demonstrate that LEGO yields tangent space estimates that are\nsignificantly more robust to noise than those from LPCA, resulting in marked\nimprovements in downstream tasks such as manifold learning, boundary detection,\nand local intrinsic dimension estimation."
    },
    {
        "date": "2025-10",
        "title": "Tree-based Dialogue Reinforced Policy Optimization for Red-Teaming Attacks",
        "author": "Ruohao Guo, Afshin Oroojlooy, Roshan Sridhar, Miguel Ballesteros, Alan Ritter, and Dan Roth",
        "link": "http://arxiv.org/abs/2510.02286v1",
        "abstract": "Despite recent rapid progress in AI safety, current large language models\nremain vulnerable to adversarial attacks in multi-turn interaction settings,\nwhere attackers strategically adapt their prompts across conversation turns and\npose a more critical yet realistic challenge. Existing approaches that discover\nsafety vulnerabilities either rely on manual red-teaming with human experts or\nemploy automated methods using pre-defined templates and human-curated attack\ndata, with most focusing on single-turn attacks. However, these methods did not\nexplore the vast space of possible multi-turn attacks, failing to consider\nnovel attack trajectories that emerge from complex dialogue dynamics and\nstrategic conversation planning. This gap is particularly critical given recent\nfindings that LLMs exhibit significantly higher vulnerability to multi-turn\nattacks compared to single-turn attacks. We propose DialTree-RPO, an on-policy\nreinforcement learning framework integrated with tree search that autonomously\ndiscovers diverse multi-turn attack strategies by treating the dialogue as a\nsequential decision-making problem, enabling systematic exploration without\nmanually curated data. Through extensive experiments, our approach not only\nachieves more than 25.9% higher ASR across 10 target models compared to\nprevious state-of-the-art approaches, but also effectively uncovers new attack\nstrategies by learning optimal dialogue policies that maximize attack success\nacross multiple turns."
    },
    {
        "date": "2025-10",
        "title": "How to Combat Reactive and Dynamic Jamming Attacks with Reinforcement Learning",
        "author": "Yalin E. Sagduyu, Tugba Erpek, Kemal Davaslioglu, and Sastry Kompella",
        "link": "http://arxiv.org/abs/2510.02265v1",
        "abstract": "This paper studies the problem of mitigating reactive jamming, where a jammer\nadopts a dynamic policy of selecting channels and sensing thresholds to detect\nand jam ongoing transmissions. The transmitter-receiver pair learns to avoid\njamming and optimize throughput over time (without prior knowledge of channel\nconditions or jamming strategies) by using reinforcement learning (RL) to adapt\ntransmit power, modulation, and channel selection. Q-learning is employed for\ndiscrete jamming-event states, while Deep Q-Networks (DQN) are employed for\ncontinuous states based on received power. Through different reward functions\nand action sets, the results show that RL can adapt rapidly to spectrum\ndynamics and sustain high rates as channels and jamming policies change over\ntime."
    },
    {
        "date": "2025-10",
        "title": "PUL-Inter-slice Defender: An Anomaly Detection Solution for Distributed Slice Mobility Attacks",
        "author": "Ricardo Misael Ayala Molina, Hyame Assem Alameddine, Makan Pourzandi, and Chadi Assi",
        "link": "http://arxiv.org/abs/2510.02236v1",
        "abstract": "Network Slices (NSs) are virtual networks operating over a shared physical\ninfrastructure, each designed to meet specific application requirements while\nmaintaining consistent Quality of Service (QoS). In Fifth Generation (5G)\nnetworks, User Equipment (UE) can connect to and seamlessly switch between\nmultiple NSs to access diverse services. However, this flexibility, known as\nInter-Slice Switching (ISS), introduces a potential vulnerability that can be\nexploited to launch Distributed Slice Mobility (DSM) attacks, a form of\nDistributed Denial of Service (DDoS) attack. To secure 5G networks and their\nNSs against DSM attacks, we present in this work, PUL-Inter-Slice Defender; an\nanomaly detection solution that leverages Positive Unlabeled Learning (PUL) and\nincorporates a combination of Long Short-Term Memory Autoencoders and K-Means\nclustering. PUL-Inter-Slice Defender leverages the Third Generation Partnership\nProject (3GPP) key performance indicators and performance measurement counters\nas features for its machine learning models to detect DSM attack variants while\nmaintaining robustness in the presence of contaminated training data. When\nevaluated on data collected from our 5G testbed based on the open-source\nfree5GC and UERANSIM, a UE/ Radio Access Network (RAN) simulator;\nPUL-Inter-Slice Defender achieved F1-scores exceeding 98.50% on training\ndatasets with 10% to 40% attack contamination, consistently outperforming its\ncounterpart Inter-Slice Defender and other PUL based solutions combining\nOne-Class Support Vector Machine (OCSVM) with Random Forest and XGBoost."
    },
    {
        "date": "2025-10",
        "title": "Adaptive Deception Framework with Behavioral Analysis for Enhanced Cybersecurity Defense",
        "author": "Basil Abdullah AL-Zahrani",
        "link": "http://arxiv.org/abs/2510.02424v1",
        "abstract": "This paper presents CADL (Cognitive-Adaptive Deception Layer), an adaptive\ndeception framework achieving 99.88% detection rate with 0.13% false positive\nrate on the CICIDS2017 dataset. The framework employs ensemble machine learning\n(Random Forest, XGBoost, Neural Networks) combined with behavioral profiling to\nidentify and adapt responses to network intrusions. Through a coordinated\nsignal bus architecture, security components share real-time intelligence,\nenabling collective decision-making. The system profiles attackers based on\ntemporal patterns and deploys customized deception strategies across five\nescalation levels. Evaluation on 50,000 CICIDS2017 test samples demonstrates\nthat CADL significantly outperforms traditional intrusion detection systems\n(Snort: 71.2%, Suricata: 68.5%) while maintaining production-ready false\npositive rates. The framework's behavioral analysis achieves 89% accuracy in\nclassifying attacker profiles. We provide open-source implementation and\ntransparent performance metrics, offering an accessible alternative to\ncommercial deception platforms costing $150-400 per host annually."
    },
    {
        "date": "2025-10",
        "title": "Authentication Security of PRF GNSS Ranging",
        "author": "Jason Anderson",
        "link": "http://arxiv.org/abs/2510.02196v1",
        "abstract": "This work derives the authentication security of pseudorandom function (PRF)\nGNSS ranging under multiple GNSS spoofing models, including the Security Code\nEstimation and Replay (SCER) spoofer. When GNSS ranging codes derive from a PRF\nutilizing a secret known only to the broadcaster, the spoofer cannot predict\nthe ranging code before broadcast. Therefore, PRF ranging can be used to\nestablish trust in the GNSS pseudoranges and the resulting receiver position,\nnavigation, and timing (PNT) solution. I apply the methods herein to Galileo's\nSignal Authentication Service (SAS) utilizing the encrypted Galileo E6-C signal\nto compute that, at most, 400 ms of Galileo E6-C data to assert 128-bit\nauthentication security under non-SCER models. For the SCER adversary, I\npredict the adversary's needed receiving radio equipment to break\nauthentication security. One can use this work to design a PRF GNSS ranging\nprotocol to meet useful authentication security requirements by computing the\nprobability of missed detection."
    },
    {
        "date": "2025-10",
        "title": "Dynamic Target Attack",
        "author": "Kedong Xiu, Churui Zeng, Tianhang Zheng, Xinzhe Huang, Xiaojun Jia, Di Wang, Puning Zhao, Zhan Qin, and Kui Ren",
        "link": "http://arxiv.org/abs/2510.02422v1",
        "abstract": "Existing gradient-based jailbreak attacks typically optimize an adversarial\nsuffix to induce a fixed affirmative response. However, this fixed target\nusually resides in an extremely low-density region of a safety-aligned LLM's\noutput distribution conditioned on diverse harmful inputs. Due to the\nsubstantial discrepancy between the target and the original output, existing\nattacks require numerous iterations to optimize the adversarial prompt, which\nmight still fail to induce the low-probability target response from the target\nLLM. In this paper, we propose Dynamic Target Attack (DTA), a new jailbreaking\nframework relying on the target LLM's own responses as targets to optimize the\nadversarial prompts. In each optimization round, DTA iteratively samples\nmultiple candidate responses directly from the output distribution conditioned\non the current prompt, and selects the most harmful response as a temporary\ntarget for prompt optimization. In contrast to existing attacks, DTA\nsignificantly reduces the discrepancy between the target and the output\ndistribution, substantially easing the optimization process to search for an\neffective adversarial prompt.\n  Extensive experiments demonstrate the superior effectiveness and efficiency\nof DTA: under the white-box setting, DTA only needs 200 optimization iterations\nto achieve an average attack success rate (ASR) of over 87\\% on recent\nsafety-aligned LLMs, exceeding the state-of-the-art baselines by over 15\\%. The\ntime cost of DTA is 2-26 times less than existing baselines. Under the\nblack-box setting, DTA uses Llama-3-8B-Instruct as a surrogate model for target\nsampling and achieves an ASR of 85\\% against the black-box target model\nLlama-3-70B-Instruct, exceeding its counterparts by over 25\\%."
    },
    {
        "date": "2025-10",
        "title": "NoMod: A Non-modular Attack on Module Learning With Errors",
        "author": "Cristian Bassotto, Ermes Franch, Marina Kr\u010dek, and Stjepan Picek",
        "link": "http://arxiv.org/abs/2510.02162v1",
        "abstract": "The advent of quantum computing threatens classical public-key cryptography,\nmotivating NIST's adoption of post-quantum schemes such as those based on the\nModule Learning With Errors (Module-LWE) problem. We present NoMod ML-Attack, a\nhybrid white-box cryptanalytic method that circumvents the challenge of\nmodeling modular reduction by treating wrap-arounds as statistical corruption\nand casting secret recovery as robust linear estimation. Our approach combines\noptimized lattice preprocessing--including reduced-vector saving and algebraic\namplification--with robust estimators trained via Tukey's Biweight loss.\nExperiments show NoMod achieves full recovery of binary secrets for dimension\n$n = 350$, recovery of sparse binomial secrets for $n = 256$, and successful\nrecovery of sparse secrets in CRYSTALS-Kyber settings with parameters $(n, k) =\n(128, 3)$ and $(256, 2)$. We release our implementation in an anonymous\nrepository https://anonymous.4open.science/r/NoMod-3BD4."
    },
    {
        "date": "2025-10",
        "title": "Mirage Fools the Ear, Mute Hides the Truth: Precise Targeted Adversarial Attacks on Polyphonic Sound Event Detection Systems",
        "author": "Junjie Su, Weifei Jin, Yuxin Cao, Derui Wang, Kai Ye, and Jie Hao",
        "link": "http://arxiv.org/abs/2510.02158v1",
        "abstract": "Sound Event Detection (SED) systems are increasingly deployed in\nsafety-critical applications such as industrial monitoring and audio\nsurveillance. However, their robustness against adversarial attacks has not\nbeen well explored. Existing audio adversarial attacks targeting SED systems,\nwhich incorporate both detection and localization capabilities, often lack\neffectiveness due to SED's strong contextual dependencies or lack precision by\nfocusing solely on misclassifying the target region as the target event,\ninadvertently affecting non-target regions. To address these challenges, we\npropose the Mirage and Mute Attack (M2A) framework, which is designed for\ntargeted adversarial attacks on polyphonic SED systems. In our optimization\nprocess, we impose specific constraints on the non-target output, which we\nrefer to as preservation loss, ensuring that our attack does not alter the\nmodel outputs for non-target region, thus achieving precise attacks.\nFurthermore, we introduce a novel evaluation metric Editing Precison (EP) that\nbalances effectiveness and precision, enabling our method to simultaneously\nenhance both. Comprehensive experiments show that M2A achieves 94.56% and\n99.11% EP on two state-of-the-art SED models, demonstrating that the framework\nis sufficiently effective while significantly enhancing attack precision."
    },
    {
        "date": "2025-10",
        "title": "Adaptive Heterogeneous Mixtures of Normalising Flows for Robust Variational Inference",
        "author": "Benjamin Wiriyapong, Oktay Karaku\u015f, and Kirill Sidorov",
        "link": "http://arxiv.org/abs/2510.02056v1",
        "abstract": "Normalising-flow variational inference (VI) can approximate complex\nposteriors, yet single-flow models often behave inconsistently across\nqualitatively different distributions. We propose Adaptive Mixture Flow\nVariational Inference (AMF-VI), a heterogeneous mixture of complementary flows\n(MAF, RealNVP, RBIG) trained in two stages: (i) sequential expert training of\nindividual flows, and (ii) adaptive global weight estimation via\nlikelihood-driven updates, without per-sample gating or architectural changes.\nEvaluated on six canonical posterior families of banana, X-shape, two-moons,\nrings, a bimodal, and a five-mode mixture, AMF-VI achieves consistently lower\nnegative log-likelihood than each single-flow baseline and delivers stable\ngains in transport metrics (Wasserstein-2) and maximum mean discrepancy (MDD),\nindicating improved robustness across shapes and modalities. The procedure is\nefficient and architecture-agnostic, incurring minimal overhead relative to\nstandard flow training, and demonstrates that adaptive mixtures of diverse\nflows provide a reliable route to robust VI across diverse posterior families\nwhilst preserving each expert's inductive bias."
    },
    {
        "date": "2025-10",
        "title": "Lower Bounds on Adversarial Robustness for Multiclass Classification with General Loss Functions",
        "author": "Camilo Andr\u00e9s Garc\u00eda Trillos, and Nicol\u00e1s Garc\u00eda Trillos",
        "link": "http://arxiv.org/abs/2510.01969v1",
        "abstract": "We consider adversarially robust classification in a multiclass setting under\narbitrary loss functions and derive dual and barycentric reformulations of the\ncorresponding learner-agnostic robust risk minimization problem. We provide\nexplicit characterizations for important cases such as the cross-entropy loss,\nloss functions with a power form, and the quadratic loss, extending in this way\navailable results for the 0-1 loss. These reformulations enable efficient\ncomputation of sharp lower bounds for adversarial risks and facilitate the\ndesign of robust classifiers beyond the 0-1 loss setting. Our paper uncovers\ninteresting connections between adversarial robustness, $\\alpha$-fair packing\nproblems, and generalized barycenter problems for arbitrary positive measures\nwhere Kullback-Leibler and Tsallis entropies are used as penalties. Our\ntheoretical results are accompanied with illustrative numerical experiments\nwhere we obtain tighter lower bounds for adversarial risks with the\ncross-entropy loss function."
    },
    {
        "date": "2025-10",
        "title": "Are LLMs Better GNN Helpers? Rethinking Robust Graph Learning under Deficiencies with Iterative Refinement",
        "author": "Zhaoyan Wang, Zheng Gao, Arogya Kharel, and In-Young Ko",
        "link": "http://arxiv.org/abs/2510.01910v1",
        "abstract": "Graph Neural Networks (GNNs) are widely adopted in Web-related applications,\nserving as a core technique for learning from graph-structured data, such as\ntext-attributed graphs. Yet in real-world scenarios, such graphs exhibit\ndeficiencies that substantially undermine GNN performance. While prior\nGNN-based augmentation studies have explored robustness against individual\nimperfections, a systematic understanding of how graph-native and Large\nLanguage Models (LLMs) enhanced methods behave under compound deficiencies is\nstill missing. Specifically, there has been no comprehensive investigation\ncomparing conventional approaches and recent LLM-on-graph frameworks, leaving\ntheir merits unclear. To fill this gap, we conduct the first empirical study\nthat benchmarks these two lines of methods across diverse graph deficiencies,\nrevealing overlooked vulnerabilities and challenging the assumption that LLM\naugmentation is consistently superior. Building on empirical findings, we\npropose Robust Graph Learning via Retrieval-Augmented Contrastive Refinement\n(RoGRAD) framework. Unlike prior one-shot LLM-as-Enhancer designs, RoGRAD is\nthe first iterative paradigm that leverages Retrieval-Augmented Generation\n(RAG) to inject retrieval-grounded augmentations by supplying class-consistent,\ndiverse augmentations and enforcing discriminative representations through\niterative graph contrastive learning. It transforms LLM augmentation for graphs\nfrom static signal injection into dynamic refinement. Extensive experiments\ndemonstrate RoGRAD's superiority over both conventional GNN- and LLM-enhanced\nbaselines, achieving up to 82.43% average improvement."
    },
    {
        "date": "2025-10",
        "title": "REPAIR: Robust Editing via Progressive Adaptive Intervention and Reintegration",
        "author": "Yisu Wang, Ming Wang, Haoyuan Song, Wenjie Huang, Chaozheng Wang, Yi Xie, and Xuming Ran",
        "link": "http://arxiv.org/abs/2510.01879v1",
        "abstract": "Post-training for large language models (LLMs) is constrained by the high\ncost of acquiring new knowledge or correcting errors and by the unintended side\neffects that frequently arise from retraining. To address these issues, we\nintroduce REPAIR (Robust Editing via Progressive Adaptive Intervention and\nReintegration), a lifelong editing framework designed to support precise and\nlow-cost model updates while preserving non-target knowledge. REPAIR mitigates\nthe instability and conflicts of large-scale sequential edits through a\nclosed-loop feedback mechanism coupled with dynamic memory management.\nFurthermore, by incorporating frequent knowledge fusion and enforcing strong\nlocality guards, REPAIR effectively addresses the shortcomings of traditional\ndistribution-agnostic approaches that often overlook unintended ripple effects.\nOur experiments demonstrate that REPAIR boosts editing accuracy by 10%-30%\nacross multiple model families and significantly reduces knowledge forgetting.\nThis work introduces a robust framework for developing reliable, scalable, and\ncontinually evolving LLMs."
    },
    {
        "date": "2025-10",
        "title": "Secure Multi-Modal Data Fusion in Federated Digital Health Systems via MCP",
        "author": "Aueaphum Aueawatthanaphisut",
        "link": "http://arxiv.org/abs/2510.01780v1",
        "abstract": "Secure and interoperable integration of heterogeneous medical data remains a\ngrand challenge in digital health. Current federated learning (FL) frameworks\noffer privacy-preserving model training but lack standardized mechanisms to\norchestrate multi-modal data fusion across distributed and resource-constrained\nenvironments. This study introduces a novel framework that leverages the Model\nContext Protocol (MCP) as an interoperability layer for secure, cross-agent\ncommunication in multi-modal federated healthcare systems. The proposed\narchitecture unifies three pillars: (i) multi-modal feature alignment for\nclinical imaging, electronic medical records, and wearable IoT data; (ii)\nsecure aggregation with differential privacy to protect patient-sensitive\nupdates; and (iii) energy-aware scheduling to mitigate dropouts in mobile\nclients. By employing MCP as a schema-driven interface, the framework enables\nadaptive orchestration of AI agents and toolchains while ensuring compliance\nwith privacy regulations. Experimental evaluation on benchmark datasets and\npilot clinical cohorts demonstrates up to 9.8\\% improvement in diagnostic\naccuracy compared with baseline FL, a 54\\% reduction in client dropout rates,\nand clinically acceptable privacy--utility trade-offs. These results highlight\nMCP-enabled multi-modal fusion as a scalable and trustworthy pathway toward\nequitable, next-generation federated health infrastructures."
    },
    {
        "date": "2025-10",
        "title": "Unsupervised Dynamic Feature Selection for Robust Latent Spaces in Vision Tasks",
        "author": "Bruno Corcuera, Carlos Eiras-Franco, and Brais Cancela",
        "link": "http://arxiv.org/abs/2510.01758v1",
        "abstract": "Latent representations are critical for the performance and robustness of\nmachine learning models, as they encode the essential features of data in a\ncompact and informative manner. However, in vision tasks, these representations\nare often affected by noisy or irrelevant features, which can degrade the\nmodel's performance and generalization capabilities. This paper presents a\nnovel approach for enhancing latent representations using unsupervised Dynamic\nFeature Selection (DFS). For each instance, the proposed method identifies and\nremoves misleading or redundant information in images, ensuring that only the\nmost relevant features contribute to the latent space. By leveraging an\nunsupervised framework, our approach avoids reliance on labeled data, making it\nbroadly applicable across various domains and datasets. Experiments conducted\non image datasets demonstrate that models equipped with unsupervised DFS\nachieve significant improvements in generalization performance across various\ntasks, including clustering and image generation, while incurring a minimal\nincrease in the computational cost."
    },
    {
        "date": "2025-10",
        "title": "Finite-Time Bounds for Distributionally Robust TD Learning with Linear Function Approximation",
        "author": "Saptarshi Mandal, Yashaswini Murthy, and R. Srikant",
        "link": "http://arxiv.org/abs/2510.01721v1",
        "abstract": "Distributionally robust reinforcement learning (DRRL) focuses on designing\npolicies that achieve good performance under model uncertainties. In\nparticular, we are interested in maximizing the worst-case long-term discounted\nreward, where the data for RL comes from a nominal model while the deployed\nenvironment can deviate from the nominal model within a prescribed uncertainty\nset. Existing convergence guarantees for robust temporal-difference (TD)\nlearning for policy evaluation are limited to tabular MDPs or are dependent on\nrestrictive discount-factor assumptions when function approximation is used. We\npresent the first robust TD learning with linear function approximation, where\nrobustness is measured with respect to the total-variation distance and\nWasserstein-l distance uncertainty set. Additionally, our algorithm is both\nmodel-free and does not require generative access to the MDP. Our algorithm\ncombines a two-time-scale stochastic-approximation update with an outer-loop\ntarget-network update. We establish an $\\tilde{O}(1/\\epsilon^2)$ sample\ncomplexity to obtain an $\\epsilon$-accurate value estimate. Our results close a\nkey gap between the empirical success of robust RL algorithms and the\nnon-asymptotic guarantees enjoyed by their non-robust counterparts. The key\nideas in the paper also extend in a relatively straightforward fashion to\nrobust Q-learning with function approximation."
    },
    {
        "date": "2025-10",
        "title": "Towards Imperceptible Adversarial Defense: A Gradient-Driven Shield against Facial Manipulations",
        "author": "Yue Li, Linying Xue, Dongdong Lin, Qiushi Li, Hui Tian, and Hongxia Wang",
        "link": "http://arxiv.org/abs/2510.01699v1",
        "abstract": "With the flourishing prosperity of generative models, manipulated facial\nimages have become increasingly accessible, raising concerns regarding privacy\ninfringement and societal trust. In response, proactive defense strategies\nembed adversarial perturbations into facial images to counter deepfake\nmanipulation. However, existing methods often face a tradeoff between\nimperceptibility and defense effectiveness-strong perturbations may disrupt\nforgeries but degrade visual fidelity. Recent studies have attempted to address\nthis issue by introducing additional visual loss constraints, yet often\noverlook the underlying gradient conflicts among losses, ultimately weakening\ndefense performance. To bridge the gap, we propose a gradient-projection-based\nadversarial proactive defense (GRASP) method that effectively counters facial\ndeepfakes while minimizing perceptual degradation. GRASP is the first approach\nto successfully integrate both structural similarity loss and low-frequency\nloss to enhance perturbation imperceptibility. By analyzing gradient conflicts\nbetween defense effectiveness loss and visual quality losses, GRASP pioneers\nthe design of the gradient-projection mechanism to mitigate these conflicts,\nenabling balanced optimization that preserves image fidelity without\nsacrificing defensive performance. Extensive experiments validate the efficacy\nof GRASP, achieving a PSNR exceeding 40 dB, SSIM of 0.99, and a 100% defense\nsuccess rate against facial attribute manipulations, significantly\noutperforming existing approaches in visual quality."
    },
    {
        "date": "2025-10",
        "title": "Beyond Simple Fusion: Adaptive Gated Fusion for Robust Multimodal Sentiment Analysis",
        "author": "Han Wu, Yanming Sun, Yunhe Yang, and Derek F. Wong",
        "link": "http://arxiv.org/abs/2510.01677v1",
        "abstract": "Multimodal sentiment analysis (MSA) leverages information fusion from diverse\nmodalities (e.g., text, audio, visual) to enhance sentiment prediction.\nHowever, simple fusion techniques often fail to account for variations in\nmodality quality, such as those that are noisy, missing, or semantically\nconflicting. This oversight leads to suboptimal performance, especially in\ndiscerning subtle emotional nuances. To mitigate this limitation, we introduce\na simple yet efficient \\textbf{A}daptive \\textbf{G}ated \\textbf{F}usion\n\\textbf{N}etwork that adaptively adjusts feature weights via a dual gate fusion\nmechanism based on information entropy and modality importance. This mechanism\nmitigates the influence of noisy modalities and prioritizes informative cues\nfollowing unimodal encoding and cross-modal interaction. Experiments on\nCMU-MOSI and CMU-MOSEI show that AGFN significantly outperforms strong\nbaselines in accuracy, effectively discerning subtle emotions with robust\nperformance. Visualization analysis of feature representations demonstrates\nthat AGFN enhances generalization by learning from a broader feature\ndistribution, achieved by reducing the correlation between feature location and\nprediction error, thereby decreasing reliance on specific locations and\ncreating more robust multimodal feature representations."
    },
    {
        "date": "2025-10",
        "title": "Evaluating the Robustness of a Production Malware Detection System to Transferable Adversarial Attacks",
        "author": "Milad Nasr, Yanick Fratantonio, Luca Invernizzi, Ange Albertini, Loua Farah, Alex Petit-Bianco, Andreas Terzis, Kurt Thomas, Elie Bursztein, and Nicholas Carlini",
        "link": "http://arxiv.org/abs/2510.01676v1",
        "abstract": "As deep learning models become widely deployed as components within larger\nproduction systems, their individual shortcomings can create system-level\nvulnerabilities with real-world impact. This paper studies how adversarial\nattacks targeting an ML component can degrade or bypass an entire\nproduction-grade malware detection system, performing a case study analysis of\nGmail's pipeline where file-type identification relies on a ML model.\n  The malware detection pipeline in use by Gmail contains a machine learning\nmodel that routes each potential malware sample to a specialized malware\nclassifier to improve accuracy and performance. This model, called Magika, has\nbeen open sourced. By designing adversarial examples that fool Magika, we can\ncause the production malware service to incorrectly route malware to an\nunsuitable malware detector thereby increasing our chance of evading detection.\nSpecifically, by changing just 13 bytes of a malware sample, we can\nsuccessfully evade Magika in 90% of cases and thereby allow us to send malware\nfiles over Gmail. We then turn our attention to defenses, and develop an\napproach to mitigate the severity of these types of attacks. For our defended\nproduction model, a highly resourced adversary requires 50 bytes to achieve\njust a 20% attack success rate. We implement this defense, and, thanks to a\ncollaboration with Google engineers, it has already been deployed in production\nfor the Gmail classifier."
    },
    {
        "date": "2025-10",
        "title": "UniVerse: Unleashing the Scene Prior of Video Diffusion Models for Robust Radiance Field Reconstruction",
        "author": "Jin Cao, Hongrui Wu, Ziyong Feng, Hujun Bao, Xiaowei Zhou, and Sida Peng",
        "link": "http://arxiv.org/abs/2510.01669v2",
        "abstract": "This paper tackles the challenge of robust reconstruction, i.e., the task of\nreconstructing a 3D scene from a set of inconsistent multi-view images. Some\nrecent works have attempted to simultaneously remove image inconsistencies and\nperform reconstruction by integrating image degradation modeling into neural 3D\nscene representations. However, these methods rely heavily on dense\nobservations for robustly optimizing model parameters. To address this issue,\nwe propose to decouple robust reconstruction into two subtasks: restoration and\nreconstruction, which naturally simplifies the optimization process. To this\nend, we introduce UniVerse, a unified framework for robust reconstruction based\non a video diffusion model. Specifically, UniVerse first converts inconsistent\nimages into initial videos, then uses a specially designed video diffusion\nmodel to restore them into consistent images, and finally reconstructs the 3D\nscenes from these restored images. Compared with case-by-case per-view\ndegradation modeling, the diffusion model learns a general scene prior from\nlarge-scale data, making it applicable to diverse image inconsistencies.\nExtensive experiments on both synthetic and real-world datasets demonstrate the\nstrong generalization capability and superior performance of our method in\nrobust reconstruction. Moreover, UniVerse can control the style of the\nreconstructed 3D scene. Project page:\nhttps://jin-cao-tma.github.io/UniVerse.github.io/"
    },
    {
        "date": "2025-10",
        "title": "SoK: Measuring What Matters for Closed-Loop Security Agents",
        "author": "Mudita Khurana, and Raunak Jain",
        "link": "http://arxiv.org/abs/2510.01654v1",
        "abstract": "Cybersecurity is a relentless arms race, with AI driven offensive systems\nevolving faster than traditional defenses can adapt. Research and tooling\nremain fragmented across isolated defensive functions, creating blind spots\nthat adversaries exploit. Autonomous agents capable of integrating, exploit\nconfirmation, remediation, and validation into a single closed loop offer\npromise, but the field lacks three essentials: a framework defining the agentic\ncapabilities of security systems across security life cycle, a principled\nmethod for evaluating closed loop agents, and a benchmark for measuring their\nperformance in practice. We introduce CLASP: the Closed-Loop Autonomous\nSecurity Performance framework which aligns the security lifecycle\n(reconnaissance, exploitation, root cause analysis, patch synthesis,\nvalidation) with core agentic capabilities (planning, tool use, memory,\nreasoning, reflection & perception) providing a common vocabulary and rubric\nfor assessing agentic capabilities in security tasks. By applying CLASP to 21\nrepresentative works, we map where systems demonstrate strengths, and where\ncapability gaps persist. We then define the Closed-Loop Capability (CLC) Score,\na composite metric quantifying both degree of loop closure and operational\neffectiveness, and outline the requirements for a closed loop benchmark.\nTogether, CLASP and the CLC Score, provide the vocabulary, diagnostics, and\nmeasurements needed to advance both function level performance and measure\nclosed loop security agents."
    },
    {
        "date": "2025-10",
        "title": "MPMAvatar: Learning 3D Gaussian Avatars with Accurate and Robust Physics-Based Dynamics",
        "author": "Changmin Lee, Jihyun Lee, and Tae-Kyun Kim",
        "link": "http://arxiv.org/abs/2510.01619v1",
        "abstract": "While there has been significant progress in the field of 3D avatar creation\nfrom visual observations, modeling physically plausible dynamics of humans with\nloose garments remains a challenging problem. Although a few existing works\naddress this problem by leveraging physical simulation, they suffer from\nlimited accuracy or robustness to novel animation inputs. In this work, we\npresent MPMAvatar, a framework for creating 3D human avatars from multi-view\nvideos that supports highly realistic, robust animation, as well as\nphotorealistic rendering from free viewpoints. For accurate and robust dynamics\nmodeling, our key idea is to use a Material Point Method-based simulator, which\nwe carefully tailor to model garments with complex deformations and contact\nwith the underlying body by incorporating an anisotropic constitutive model and\na novel collision handling algorithm. We combine this dynamics modeling scheme\nwith our canonical avatar that can be rendered using 3D Gaussian Splatting with\nquasi-shadowing, enabling high-fidelity rendering for physically realistic\nanimations. In our experiments, we demonstrate that MPMAvatar significantly\noutperforms the existing state-of-the-art physics-based avatar in terms of (1)\ndynamics modeling accuracy, (2) rendering accuracy, and (3) robustness and\nefficiency. Additionally, we present a novel application in which our avatar\ngeneralizes to unseen interactions in a zero-shot manner-which was not\nachievable with previous learning-based methods due to their limited simulation\ngeneralizability. Our project page is at:\nhttps://KAISTChangmin.github.io/MPMAvatar/"
    },
    {
        "date": "2025-10",
        "title": "Securing generative artificial intelligence with parallel magnetic tunnel junction true randomness",
        "author": "Youwei Bao, Shuhan Yang, and Hyunsoo Yang",
        "link": "http://arxiv.org/abs/2510.01598v1",
        "abstract": "Deterministic pseudo random number generators (PRNGs) used in generative\nartificial intelligence (GAI) models produce predictable patterns vulnerable to\nexploitation by attackers. Conventional defences against the vulnerabilities\noften come with significant energy and latency overhead. Here, we embed\nhardware-generated true random bits from spin-transfer torque magnetic tunnel\njunctions (STT-MTJs) to address the challenges. A highly parallel,\nFPGA-assisted prototype computing system delivers megabit-per-second true\nrandom numbers, passing NIST randomness tests after in-situ operations with\nminimal overhead. Integrating the hardware random bits into a generative\nadversarial network (GAN) trained on CIFAR-10 reduces insecure outputs by up to\n18.6 times compared to the low-quality random number generators (RNG) baseline.\nWith nanosecond switching speed, high energy efficiency, and established\nscalability, our STT-MTJ-based system holds the potential to scale beyond 106\nparallel cells, achieving gigabit-per-second throughput suitable for large\nlanguage model sampling. This advancement highlights spintronic RNGs as\npractical security components for next-generation GAI systems."
    },
    {
        "date": "2025-10",
        "title": "Enhancing Noise Robustness of Parkinson's Disease Telemonitoring via Contrastive Feature Augmentation",
        "author": "Ziming Tang, Chengbin Hou, Tianyu Zhang, Bangxu Tian, Jinbao Wang, and Hairong Lv",
        "link": "http://arxiv.org/abs/2510.01588v1",
        "abstract": "Parkinson's disease (PD) is one of the most common neurodegenerative\ndisorder. PD telemonitoring emerges as a novel assessment modality enabling\nself-administered at-home tests of Unified Parkinson's Disease Rating Scale\n(UPDRS) scores, enhancing accessibility for PD patients. However, three types\nof noise would occur during measurements: (1) patient-induced measurement\ninaccuracies, (2) environmental noise, and (3) data packet loss during\ntransmission, resulting in higher prediction errors. To address these\nchallenges, NoRo, a noise-robust UPDRS prediction framework is proposed. First,\nthe original speech features are grouped into ordered bins, based on the\ncontinuous values of a selected feature, to construct contrastive pairs.\nSecond, the contrastive pairs are employed to train a multilayer perceptron\nencoder for generating noise-robust features. Finally, these features are\nconcatenated with the original features as the augmented features, which are\nthen fed into the UPDRS prediction models. Notably, we further introduces a\nnovel evaluation approach with customizable noise injection module, and\nextensive experiments show that NoRo can successfully enhance the noise\nrobustness of UPDRS prediction across various downstream prediction models\nunder different noisy environments."
    },
    {
        "date": "2025-10",
        "title": "AdvEvo-MARL: Shaping Internalized Safety through Adversarial Co-Evolution in Multi-Agent Reinforcement Learning",
        "author": "Zhenyu Pan, Yiting Zhang, Zhuo Liu, Yolo Yunlong Tang, Zeliang Zhang, Haozheng Luo, Yuwei Han, Jianshu Zhang, Dennis Wu, Hong-Yu Chen, Haoran Lu, Haoyang Fang, Manling Li, Chenliang Xu, Philip S. Yu, and Han Liu",
        "link": "http://arxiv.org/abs/2510.01586v1",
        "abstract": "LLM-based multi-agent systems excel at planning, tool use, and role\ncoordination, but their openness and interaction complexity also expose them to\njailbreak, prompt-injection, and adversarial collaboration. Existing defenses\nfall into two lines: (i) self-verification that asks each agent to pre-filter\nunsafe instructions before execution, and (ii) external guard modules that\npolice behaviors. The former often underperforms because a standalone agent\nlacks sufficient capacity to detect cross-agent unsafe chains and\ndelegation-induced risks; the latter increases system overhead and creates a\nsingle-point-of-failure-once compromised, system-wide safety collapses, and\nadding more guards worsens cost and complexity. To solve these challenges, we\npropose AdvEvo-MARL, a co-evolutionary multi-agent reinforcement learning\nframework that internalizes safety into task agents. Rather than relying on\nexternal guards, AdvEvo-MARL jointly optimizes attackers (which synthesize\nevolving jailbreak prompts) and defenders (task agents trained to both\naccomplish their duties and resist attacks) in adversarial learning\nenvironments. To stabilize learning and foster cooperation, we introduce a\npublic baseline for advantage estimation: agents within the same functional\ngroup share a group-level mean-return baseline, enabling lower-variance updates\nand stronger intra-group coordination. Across representative attack scenarios,\nAdvEvo-MARL consistently keeps attack-success rate (ASR) below 20%, whereas\nbaselines reach up to 38.33%, while preserving-and sometimes improving-task\naccuracy (up to +3.67% on reasoning tasks). These results show that safety and\nutility can be jointly improved without relying on extra guard agents or added\nsystem overhead."
    },
    {
        "date": "2025-10",
        "title": "Robust Classification of Oral Cancer with Limited Training Data",
        "author": "Akshay Bhagwan Sonawane, Lena D. Swamikannan, and Lakshman Tamil",
        "link": "http://arxiv.org/abs/2510.01547v1",
        "abstract": "Oral cancer ranks among the most prevalent cancers globally, with a\nparticularly high mortality rate in regions lacking adequate healthcare access.\nEarly diagnosis is crucial for reducing mortality; however, challenges persist\ndue to limited oral health programs, inadequate infrastructure, and a shortage\nof healthcare practitioners. Conventional deep learning models, while\npromising, often rely on point estimates, leading to overconfidence and reduced\nreliability. Critically, these models require large datasets to mitigate\noverfitting and ensure generalizability, an unrealistic demand in settings with\nlimited training data. To address these issues, we propose a hybrid model that\ncombines a convolutional neural network (CNN) with Bayesian deep learning for\noral cancer classification using small training sets. This approach employs\nvariational inference to enhance reliability through uncertainty\nquantification. The model was trained on photographic color images captured by\nsmartphones and evaluated on three distinct test datasets. The proposed method\nachieved 94% accuracy on a test dataset with a distribution similar to that of\nthe training data, comparable to traditional CNN performance. Notably, for\nreal-world photographic image data, despite limitations and variations\ndiffering from the training dataset, the proposed model demonstrated superior\ngeneralizability, achieving 88% accuracy on diverse datasets compared to 72.94%\nfor traditional CNNs, even with a smaller dataset. Confidence analysis revealed\nthat the model exhibits low uncertainty (high confidence) for correctly\nclassified samples and high uncertainty (low confidence) for misclassified\nsamples. These results underscore the effectiveness of Bayesian inference in\ndata-scarce environments in enhancing early oral cancer diagnosis by improving\nmodel reliability and generalizability."
    },
    {
        "date": "2025-10",
        "title": "Information Seeking for Robust Decision Making under Partial Observability",
        "author": "Djengo Cyun-Jyun Fang, and Tsung-Wei Ke",
        "link": "http://arxiv.org/abs/2510.01531v1",
        "abstract": "Explicit information seeking is essential to human problem-solving in\npractical environments characterized by incomplete information and noisy\ndynamics. When the true environmental state is not directly observable, humans\nseek information to update their internal dynamics and inform future\ndecision-making. Although existing Large Language Model (LLM) planning agents\nhave addressed observational uncertainty, they often overlook discrepancies\nbetween their internal dynamics and the actual environment. We introduce\nInformation Seeking Decision Planner (InfoSeeker), an LLM decision-making\nframework that integrates task-oriented planning with information seeking to\nalign internal dynamics and make optimal decisions under uncertainty in both\nagent observations and environmental dynamics. InfoSeeker prompts an LLM to\nactively gather information by planning actions to validate its understanding,\ndetect environmental changes, or test hypotheses before generating or revising\ntask-oriented plans. To evaluate InfoSeeker, we introduce a novel benchmark\nsuite featuring partially observable environments with incomplete observations\nand uncertain dynamics. Experiments demonstrate that InfoSeeker achieves a 74%\nabsolute performance gain over prior methods without sacrificing sample\nefficiency. Moreover, InfoSeeker generalizes across LLMs and outperforms\nbaselines on established benchmarks such as robotic manipulation and web\nnavigation. These findings underscore the importance of tightly integrating\nplanning and information seeking for robust behavior in partially observable\nenvironments. The project page is available at https://infoseekerllm.github.io"
    },
    {
        "date": "2025-10",
        "title": "Understanding Adversarial Transfer: Why Representation-Space Attacks Fail Where Data-Space Attacks Succeed",
        "author": "Isha Gupta, Rylan Schaeffer, Joshua Kazdan, Ken Ziyu Liu, and Sanmi Koyejo",
        "link": "http://arxiv.org/abs/2510.01494v2",
        "abstract": "The field of adversarial robustness has long established that adversarial\nexamples can successfully transfer between image classifiers and that text\njailbreaks can successfully transfer between language models (LMs). However, a\npair of recent studies reported being unable to successfully transfer image\njailbreaks between vision-language models (VLMs). To explain this striking\ndifference, we propose a fundamental distinction regarding the transferability\nof attacks against machine learning models: attacks in the input data-space can\ntransfer, whereas attacks in model representation space do not, at least not\nwithout geometric alignment of representations. We then provide theoretical and\nempirical evidence of this hypothesis in four different settings. First, we\nmathematically prove this distinction in a simple setting where two networks\ncompute the same input-output map but via different representations. Second, we\nconstruct representation-space attacks against image classifiers that are as\nsuccessful as well-known data-space attacks, but fail to transfer. Third, we\nconstruct representation-space attacks against LMs that successfully jailbreak\nthe attacked models but again fail to transfer. Fourth, we construct data-space\nattacks against VLMs that successfully transfer to new VLMs, and we show that\nrepresentation space attacks can transfer when VLMs' latent geometries are\nsufficiently aligned in post-projector space. Our work reveals that adversarial\ntransfer is not an inherent property of all attacks but contingent on their\noperational domain - the shared data-space versus models' unique representation\nspaces - a critical insight for building more robust models."
    },
    {
        "date": "2025-10",
        "title": "Securing IoT Devices in Smart Cities: A Review of Proposed Solutions",
        "author": "Andr\u00e9s F. Betancur-L\u00f3pez",
        "link": "http://arxiv.org/abs/2510.01445v1",
        "abstract": "Privacy and security in Smart Cities remain at constant risk due to the\nvulnerabilities introduced by Internet of Things (IoT) devices. The limited\ncomputational resources of these devices make them especially susceptible to\nattacks, while their widespread adoption increases the potential impact of\nsecurity breaches. This article presents a review of security proposals aimed\nat protecting IoT devices in Smart City environments. The review was conducted\nby analyzing recent literature on device-level security, with particular\nemphasis on lightweight cryptography, physically unclonable functions (PUFs),\nand blockchain-based solutions. Findings highlight both the strengths and\nlimitations of current approaches, as well as the need for more practical,\nscalable, and resource-efficient mechanisms to ensure user privacy and data\nprotection in IoT ecosystems."
    },
    {
        "date": "2025-10",
        "title": "E-FuzzEdge: Optimizing Embedded Device Security with Scalable In-Place Fuzzing",
        "author": "Davide Rusconi, Osama Yousef, Mirco Picca, Flavio Toffalini, and Andrea Lanzi",
        "link": "http://arxiv.org/abs/2510.01393v1",
        "abstract": "In this paper we show E-FuzzEdge, a novel fuzzing architecture targeted\ntowards improving the throughput of fuzzing campaigns in contexts where\nscalability is unavailable. E-FuzzEdge addresses the inefficiencies of\nhardware-in-the-loop fuzzing for microcontrollers by optimizing execution\nspeed. We evaluated our system against state-of-the-art benchmarks,\ndemonstrating significant performance improvements. A key advantage of\nE-FuzzEdgearchitecture is its compatibility with other embedded fuzzing\ntechniques that perform on device testing instead of firmware emulation. This\nmeans that the broader embedded fuzzing community can integrate E-FuzzEdge into\ntheir workflows to enhance overall testing efficiency."
    },
    {
        "date": "2025-10",
        "title": "Breaking the Code: Security Assessment of AI Code Agents Through Systematic Jailbreaking Attacks",
        "author": "Shoumik Saha, Jifan Chen, Sam Mayers, Sanjay Krishna Gouda, Zijian Wang, and Varun Kumar",
        "link": "http://arxiv.org/abs/2510.01359v1",
        "abstract": "Code-capable large language model (LLM) agents are increasingly embedded into\nsoftware engineering workflows where they can read, write, and execute code,\nraising the stakes of safety-bypass (\"jailbreak\") attacks beyond text-only\nsettings. Prior evaluations emphasize refusal or harmful-text detection,\nleaving open whether agents actually compile and run malicious programs. We\npresent JAWS-BENCH (Jailbreaks Across WorkSpaces), a benchmark spanning three\nescalating workspace regimes that mirror attacker capability: empty (JAWS-0),\nsingle-file (JAWS-1), and multi-file (JAWS-M). We pair this with a\nhierarchical, executable-aware Judge Framework that tests (i) compliance, (ii)\nattack success, (iii) syntactic correctness, and (iv) runtime executability,\nmoving beyond refusal to measure deployable harm. Using seven LLMs from five\nfamilies as backends, we find that under prompt-only conditions in JAWS-0, code\nagents accept 61% of attacks on average; 58% are harmful, 52% parse, and 27%\nrun end-to-end. Moving to single-file regime in JAWS-1 drives compliance to ~\n100% for capable models and yields a mean ASR (Attack Success Rate) ~ 71%; the\nmulti-file regime (JAWS-M) raises mean ASR to ~ 75%, with 32% instantly\ndeployable attack code. Across models, wrapping an LLM in an agent\nsubstantially increases vulnerability -- ASR raises by 1.6x -- because initial\nrefusals are frequently overturned during later planning/tool-use steps.\nCategory-level analyses identify which attack classes are most vulnerable and\nmost readily deployable, while others exhibit large execution gaps. These\nfindings motivate execution-aware defenses, code-contextual safety filters, and\nmechanisms that preserve refusal decisions throughout the agent's multi-step\nreasoning and tool use."
    },
    {
        "date": "2025-10",
        "title": "Integrated Security Mechanisms for Weight Protection in Memristive Crossbar Arrays",
        "author": "Muhammad Faheemur Rahman, and Wayne Burleson",
        "link": "http://arxiv.org/abs/2510.01350v1",
        "abstract": "Memristive crossbar arrays enable in-memory computing by performing parallel\nanalog computations directly within memory, making them well-suited for machine\nlearning, neural networks, and neuromorphic systems. However, despite their\nadvantages, non-volatile memristors are vulnerable to security threats (such as\nadversarial extraction of stored weights when the hardware is compromised.\nProtecting these weights is essential since they represent valuable\nintellectual property resulting from lengthy and costly training processes\nusing large, often proprietary, datasets. As a solution we propose two security\nmechanisms: Keyed Permutor and Watermark Protection Columns; where both\nsafeguard critical weights and establish verifiable ownership (even in cases of\ndata leakage). Our approach integrates efficiently with existing memristive\ncrossbar architectures without significant design modifications. Simulations\nacross 45nm, 22nm, and 7nm CMOS nodes, using a realistic interconnect model and\na large RF dataset, show that both mechanisms offer robust protection with\nunder 10% overhead in area, delay and power. We also present initial\nexperiments employing the widely known MNIST dataset; further highlighting the\nfeasibility of securing memristive in-memory computing systems with minimal\nperformance trade-offs."
    },
    {
        "date": "2025-10",
        "title": "DECOR: Deep Embedding Clustering with Orientation Robustness",
        "author": "Fiona Victoria Stanley Jothiraj, Arunaggiri Pandian Karunanidhi, and Seth A. Eichmeyer",
        "link": "http://arxiv.org/abs/2510.03328v1",
        "abstract": "In semiconductor manufacturing, early detection of wafer defects is critical\nfor product yield optimization. However, raw wafer data from wafer quality\ntests are often complex, unlabeled, imbalanced and can contain multiple defects\non a single wafer, making it crucial to design clustering methods that remain\nreliable under such imperfect data conditions. We introduce DECOR, a deep\nclustering with orientation robustness framework that groups complex defect\npatterns from wafer maps into consistent clusters. We evaluate our method on\nthe open source MixedWM38 dataset, demonstrating its ability to discover\nclusters without manual tuning. DECOR explicitly accounts for orientation\nvariations in wafer maps, ensuring that spatially similar defects are\nconsistently clustered regardless of its rotation or alignment. Experiments\nindicate that our method outperforms existing clustering baseline methods, thus\nproviding a reliable and scalable solution in automated visual inspection\nsystems."
    },
    {
        "date": "2025-10",
        "title": "How Does the Pretraining Distribution Shape In-Context Learning? Task Selection, Generalization, and Robustness",
        "author": "Wa\u00efss Azizian, and Ali Hasan",
        "link": "http://arxiv.org/abs/2510.01163v1",
        "abstract": "The emergence of in-context learning (ICL) in large language models (LLMs)\nremains poorly understood despite its consistent effectiveness, enabling models\nto adapt to new tasks from only a handful of examples. To clarify and improve\nthese capabilities, we characterize how the statistical properties of the\npretraining distribution (e.g., tail behavior, coverage) shape ICL on numerical\ntasks. We develop a theoretical framework that unifies task selection and\ngeneralization, extending and sharpening earlier results, and show how\ndistributional properties govern sample efficiency, task retrieval, and\nrobustness. To this end, we generalize Bayesian posterior consistency and\nconcentration results to heavy-tailed priors and dependent sequences, better\nreflecting the structure of LLM pretraining data. We then empirically study how\nICL performance varies with the pretraining distribution on challenging tasks\nsuch as stochastic differential equations and stochastic processes with memory.\nTogether, these findings suggest that controlling key statistical properties of\nthe pretraining distribution is essential for building ICL-capable and reliable\nLLMs."
    },
    {
        "date": "2025-10",
        "title": "Multi-Marginal Flow Matching with Adversarially Learnt Interpolants",
        "author": "Oskar Kviman, Kirill Tamogashev, Nicola Branchini, V\u00edctor Elvira, Jens Lagergren, and Nikolay Malkin",
        "link": "http://arxiv.org/abs/2510.01159v1",
        "abstract": "Learning the dynamics of a process given sampled observations at several time\npoints is an important but difficult task in many scientific applications. When\nno ground-truth trajectories are available, but one has only snapshots of data\ntaken at discrete time steps, the problem of modelling the dynamics, and thus\ninferring the underlying trajectories, can be solved by multi-marginal\ngeneralisations of flow matching algorithms. This paper proposes a novel flow\nmatching method that overcomes the limitations of existing multi-marginal\ntrajectory inference algorithms. Our proposed method, ALI-CFM, uses a\nGAN-inspired adversarial loss to fit neurally parametrised interpolant curves\nbetween source and target points such that the marginal distributions at\nintermediate time points are close to the observed distributions. The resulting\ninterpolants are smooth trajectories that, as we show, are unique under mild\nassumptions. These interpolants are subsequently marginalised by a flow\nmatching algorithm, yielding a trained vector field for the underlying\ndynamics. We showcase the versatility and scalability of our method by\noutperforming the existing baselines on spatial transcriptomics and cell\ntracking datasets, while performing on par with them on single-cell trajectory\nprediction.\n  Code: https://github.com/mmacosha/adversarially-learned-interpolants."
    },
    {
        "date": "2025-10",
        "title": "Backdoor Attacks Against Speech Language Models",
        "author": "Alexandrine Fortier, Thomas Thebaud, Jes\u00fas Villalba, Najim Dehak, and Patrick Cardinal",
        "link": "http://arxiv.org/abs/2510.01157v1",
        "abstract": "Large Language Models (LLMs) and their multimodal extensions are becoming\nincreasingly popular. One common approach to enable multimodality is to cascade\ndomain-specific encoders with an LLM, making the resulting model inherit\nvulnerabilities from all of its components. In this work, we present the first\nsystematic study of audio backdoor attacks against speech language models. We\ndemonstrate its effectiveness across four speech encoders and three datasets,\ncovering four tasks: automatic speech recognition (ASR), speech emotion\nrecognition, and gender and age prediction. The attack consistently achieves\nhigh success rates, ranging from 90.76% to 99.41%. To better understand how\nbackdoors propagate, we conduct a component-wise analysis to identify the most\nvulnerable stages of the pipeline. Finally, we propose a fine-tuning-based\ndefense that mitigates the threat of poisoned pretrained encoders."
    },
    {
        "date": "2025-10",
        "title": "Safety Instincts: LLMs Learn to Trust Their Internal Compass for Self-Defense",
        "author": "Guobin Shen, Dongcheng Zhao, Haibo Tong, Jindong Li, Feifei Zhao, and Yi Zeng",
        "link": "http://arxiv.org/abs/2510.01088v1",
        "abstract": "Ensuring Large Language Model (LLM) safety remains challenging due to the\nabsence of universal standards and reliable content validators, making it\ndifficult to obtain effective training signals. We discover that aligned models\nalready possess robust internal safety beliefs: they consistently produce\nhigh-confidence refusals to harmful requests while exhibiting high entropy when\ngenerating potentially dangerous content. This entropy gap reveals an untapped\nsignal--models intrinsically \"know\" when to refuse. We introduce Safety\nInstincts Reinforcement Learning (SIRL), which transforms this internal\nconfidence into a self-generated reward signal, eliminating dependence on\nexternal validators or human annotations. SIRL teaches models to trust their\nsafety instincts by reinforcing low-entropy refusal behaviors. Evaluated on\nLlama and Qwen models, SIRL maintains 89%+ Defense Success Rates (DSRs) against\n20+ jailbreak methods, from static prompts to adaptive attacks. Using only\n15,000 unlabeled prompts, SIRL surpasses resource-intensive supervised methods\nwhile preserving performance on mathematics, coding, and conversation\nbenchmarks. Our work demonstrates that effective alignment can emerge from\nwithin, paving the way for more autonomous and robust AI safety mechanisms that\nscale without extensive human oversight."
    },
    {
        "date": "2025-10",
        "title": "Activation-Deactivation: A General Framework for Robust Post-hoc Explainable AI",
        "author": "Akchunya Chanchal, David A. Kelly, and Hana Chockler",
        "link": "http://arxiv.org/abs/2510.01038v1",
        "abstract": "Black-box explainability methods are popular tools for explaining the\ndecisions of image classifiers. A major drawback of these tools is their\nreliance on mutants obtained by occluding parts of the input, leading to\nout-of-distribution images. This raises doubts about the quality of the\nexplanations. Moreover, choosing an appropriate occlusion value often requires\ndomain knowledge. In this paper we introduce a novel forward-pass paradigm\nActivation-Deactivation (AD), which removes the effects of occluded input\nfeatures from the model's decision-making by switching off the parts of the\nmodel that correspond to the occlusions. We introduce ConvAD, a drop-in\nmechanism that can be easily added to any trained Convolutional Neural Network\n(CNN), and which implements the AD paradigm. This leads to more robust\nexplanations without any additional training or fine-tuning. We prove that the\nConvAD mechanism does not change the decision-making process of the network. We\nprovide experimental evaluation across several datasets and model\narchitectures. We compare the quality of AD-explanations with explanations\nachieved using a set of masking values, using the proxies of robustness, size,\nand confidence drop-off. We observe a consistent improvement in robustness of\nAD explanations (up to 62.5%) compared to explanations obtained with\nocclusions, demonstrating that ConvAD extracts more robust explanations without\nthe need for domain knowledge."
    },
    {
        "date": "2025-10",
        "title": "Secure and reversible face anonymization with diffusion models",
        "author": "Pol Labarbarie, Vincent Itier, and William Puech",
        "link": "http://arxiv.org/abs/2510.01031v1",
        "abstract": "Face images processed by computer vision algorithms contain sensitive\npersonal information that malicious actors can capture without consent. These\nprivacy and security risks highlight the need for effective face anonymization\nmethods. Current methods struggle to propose a good trade-off between a secure\nscheme with high-quality image generation and reversibility for later person\nauthentication. Diffusion-based approaches produce high-quality anonymized\nimages but lack the secret key mechanism to ensure that only authorized parties\ncan reverse the process. In this paper, we introduce, to our knowledge, the\nfirst secure, high-quality reversible anonymization method based on a diffusion\nmodel. We propose to combine the secret key with the latent faces\nrepresentation of the diffusion model. To preserve identity-irrelevant\nfeatures, generation is constrained by a facial mask, maintaining high-quality\nimages. By using a deterministic forward and backward diffusion process, our\napproach enforces that the original face can be recovered with the correct\nsecret key. We also show that the proposed method produces anonymized faces\nthat are less visually similar to the original faces, compared to other\nprevious work."
    },
    {
        "date": "2025-10",
        "title": "Towards Adversarial Training under Hyperspectral Images",
        "author": "Weihua Zhang, Chengze Jiang, Jie Gui, and Lu Dong",
        "link": "http://arxiv.org/abs/2510.01014v1",
        "abstract": "Recent studies have revealed that hyperspectral classification models based\non deep learning are highly vulnerable to adversarial attacks, which pose\nsignificant security risks. Although several approaches have attempted to\nenhance adversarial robustness by modifying network architectures, these\nmethods often rely on customized designs that limit scalability and fail to\ndefend effectively against strong attacks. To address these challenges, we\nintroduce adversarial training to the hyperspectral domain, which is widely\nregarded as one of the most effective defenses against adversarial attacks.\nThrough extensive empirical analyses, we demonstrate that while adversarial\ntraining does enhance robustness across various models and datasets,\nhyperspectral data introduces unique challenges not seen in RGB images.\nSpecifically, we find that adversarial noise and the non-smooth nature of\nadversarial examples can distort or eliminate important spectral semantic\ninformation. To mitigate this issue, we employ data augmentation techniques and\npropose a novel hyperspectral adversarial training method, termed AT-RA. By\nincreasing the diversity of spectral information and ensuring spatial\nsmoothness, AT-RA preserves and corrects spectral semantics in hyperspectral\nimages. Experimental results show that AT-RA improves adversarial robustness by\n21.34% against AutoAttack and 18.78% against PGD-50 while boosting benign\naccuracy by 2.68%."
    },
    {
        "date": "2025-10",
        "title": "Adaptive Federated Few-Shot Rare-Disease Diagnosis with Energy-Aware Secure Aggregation",
        "author": "Aueaphum Aueawatthanaphisut",
        "link": "http://arxiv.org/abs/2510.00976v1",
        "abstract": "Rare-disease diagnosis remains one of the most pressing challenges in digital\nhealth, hindered by extreme data scarcity, privacy concerns, and the limited\nresources of edge devices. This paper proposes the Adaptive Federated Few-Shot\nRare-Disease Diagnosis (AFFR) framework, which integrates three pillars: (i)\nfew-shot federated optimization with meta-learning to generalize from limited\npatient samples, (ii) energy-aware client scheduling to mitigate device\ndropouts and ensure balanced participation, and (iii) secure aggregation with\ncalibrated differential privacy to safeguard sensitive model updates. Unlike\nprior work that addresses these aspects in isolation, AFFR unifies them into a\nmodular pipeline deployable on real-world clinical networks. Experimental\nevaluation on simulated rare-disease detection datasets demonstrates up to 10%\nimprovement in accuracy compared with baseline FL, while reducing client\ndropouts by over 50% without degrading convergence. Furthermore,\nprivacy-utility trade-offs remain within clinically acceptable bounds. These\nfindings highlight AFFR as a practical pathway for equitable and trustworthy\nfederated diagnosis of rare conditions."
    },
    {
        "date": "2025-10",
        "title": "On Discovering Algorithms for Adversarial Imitation Learning",
        "author": "Shashank Reddy Chirra, Jayden Teoh, Praveen Paruchuri, and Pradeep Varakantham",
        "link": "http://arxiv.org/abs/2510.00922v1",
        "abstract": "Adversarial Imitation Learning (AIL) methods, while effective in settings\nwith limited expert demonstrations, are often considered unstable. These\napproaches typically decompose into two components: Density Ratio (DR)\nestimation $\\frac{\\rho_E}{\\rho_{\\pi}}$, where a discriminator estimates the\nrelative occupancy of state-action pairs under the policy versus the expert;\nand Reward Assignment (RA), where this ratio is transformed into a reward\nsignal used to train the policy. While significant research has focused on\nimproving density estimation, the role of reward assignment in influencing\ntraining dynamics and final policy performance has been largely overlooked. RA\nfunctions in AIL are typically derived from divergence minimization objectives,\nrelying heavily on human design and ingenuity. In this work, we take a\ndifferent approach: we investigate the discovery of data-driven RA functions,\ni.e, based directly on the performance of the resulting imitation policy. To\nthis end, we leverage an LLM-guided evolutionary framework that efficiently\nexplores the space of RA functions, yielding \\emph{Discovered Adversarial\nImitation Learning} (DAIL), the first meta-learnt AIL algorithm. Remarkably,\nDAIL generalises across unseen environments and policy optimization algorithms,\noutperforming the current state-of-the-art of \\emph{human-designed} baselines.\nFinally, we analyse why DAIL leads to more stable training, offering novel\ninsights into the role of RA functions in the stability of AIL. Code is\npublicly available: https://github.com/shshnkreddy/DAIL."
    },
    {
        "date": "2025-10",
        "title": "NSARM: Next-Scale Autoregressive Modeling for Robust Real-World Image Super-Resolution",
        "author": "Xiangtao Kong, Rongyuan Wu, Shuaizheng Liu, Lingchen Sun, and Lei Zhang",
        "link": "http://arxiv.org/abs/2510.00820v1",
        "abstract": "Most recent real-world image super-resolution (Real-ISR) methods employ\npre-trained text-to-image (T2I) diffusion models to synthesize the high-quality\nimage either from random Gaussian noise, which yields realistic results but is\nslow due to iterative denoising, or directly from the input low-quality image,\nwhich is efficient but at the price of lower output quality. These approaches\ntrain ControlNet or LoRA modules while keeping the pre-trained model fixed,\nwhich often introduces over-enhanced artifacts and hallucinations, suffering\nfrom the robustness to inputs of varying degradations. Recent visual\nautoregressive (AR) models, such as pre-trained Infinity, can provide strong\nT2I generation capabilities while offering superior efficiency by using the\nbitwise next-scale prediction strategy. Building upon next-scale prediction, we\nintroduce a robust Real-ISR framework, namely Next-Scale Autoregressive\nModeling (NSARM). Specifically, we train NSARM in two stages: a transformation\nnetwork is first trained to map the input low-quality image to preliminary\nscales, followed by an end-to-end full-model fine-tuning. Such a comprehensive\nfine-tuning enhances the robustness of NSARM in Real-ISR tasks without\ncompromising its generative capability. Extensive quantitative and qualitative\nevaluations demonstrate that as a pure AR model, NSARM achieves superior visual\nresults over existing Real-ISR methods while maintaining a fast inference\nspeed. Most importantly, it demonstrates much higher robustness to the quality\nof input images, showing stronger generalization performance. Project page:\nhttps://github.com/Xiangtaokong/NSARM"
    },
    {
        "date": "2025-10",
        "title": "Fast, Secure, and High-Capacity Image Watermarking with Autoencoded Text Vectors",
        "author": "Gautier Evennou, Vivien Chappelier, and Ewa Kijak",
        "link": "http://arxiv.org/abs/2510.00799v1",
        "abstract": "Most image watermarking systems focus on robustness, capacity, and\nimperceptibility while treating the embedded payload as meaningless bits. This\nbit-centric view imposes a hard ceiling on capacity and prevents watermarks\nfrom carrying useful information. We propose LatentSeal, which reframes\nwatermarking as semantic communication: a lightweight text autoencoder maps\nfull-sentence messages into a compact 256-dimensional unit-norm latent vector,\nwhich is robustly embedded by a finetuned watermark model and secured through a\nsecret, invertible rotation. The resulting system hides full-sentence messages,\ndecodes in real time, and survives valuemetric and geometric attacks. It\nsurpasses prior state of the art in BLEU-4 and Exact Match on several\nbenchmarks, while breaking through the long-standing 256-bit payload ceiling.\nIt also introduces a statistically calibrated score that yields a ROC AUC score\nof 0.97-0.99, and practical operating points for deployment. By shifting from\nbit payloads to semantic latent vectors, LatentSeal enables watermarking that\nis not only robust and high-capacity, but also secure and interpretable,\nproviding a concrete path toward provenance, tamper explanation, and\ntrustworthy AI governance. Models, training and inference code, and data splits\nwill be available upon publication."
    },
    {
        "date": "2025-10",
        "title": "MetaLogic: Robustness Evaluation of Text-to-Image Models via Logically Equivalent Prompts",
        "author": "Yifan Shen, Yangyang Shu, Hye-young Paik, and Yulei Sui",
        "link": "http://arxiv.org/abs/2510.00796v1",
        "abstract": "Recent advances in text-to-image (T2I) models, especially diffusion-based\narchitectures, have significantly improved the visual quality of generated\nimages. However, these models continue to struggle with a critical limitation:\nmaintaining semantic consistency when input prompts undergo minor linguistic\nvariations. Despite being logically equivalent, such prompt pairs often yield\nmisaligned or semantically inconsistent images, exposing a lack of robustness\nin reasoning and generalisation. To address this, we propose MetaLogic, a novel\nevaluation framework that detects T2I misalignment without relying on ground\ntruth images. MetaLogic leverages metamorphic testing, generating image pairs\nfrom prompts that differ grammatically but are semantically identical. By\ndirectly comparing these image pairs, the framework identifies inconsistencies\nthat signal failures in preserving the intended meaning, effectively diagnosing\nrobustness issues in the model's logic understanding. Unlike existing\nevaluation methods that compare a generated image to a single prompt, MetaLogic\nevaluates semantic equivalence between paired images, offering a scalable,\nground-truth-free approach to identifying alignment failures. It categorises\nthese alignment errors (e.g., entity omission, duplication, positional\nmisalignment) and surfaces counterexamples that can be used for model debugging\nand refinement. We evaluate MetaLogic across multiple state-of-the-art T2I\nmodels and reveal consistent robustness failures across a range of logical\nconstructs. We find that even the SOTA text-to-image models like Flux.dev and\nDALLE-3 demonstrate a 59 percent and 71 percent misalignment rate,\nrespectively. Our results show that MetaLogic is not only efficient and\nscalable, but also effective in uncovering fine-grained logical inconsistencies\nthat are overlooked by existing evaluation metrics."
    },
    {
        "date": "2025-10",
        "title": "DIA: The Adversarial Exposure of Deterministic Inversion in Diffusion Models",
        "author": "Seunghoo Hong, Geonho Son, Juhun Lee, and Simon S. Woo",
        "link": "http://arxiv.org/abs/2510.00778v1",
        "abstract": "Diffusion models have shown to be strong representation learners, showcasing\nstate-of-the-art performance across multiple domains. Aside from accelerated\nsampling, DDIM also enables the inversion of real images back to their latent\ncodes. A direct inheriting application of this inversion operation is real\nimage editing, where the inversion yields latent trajectories to be utilized\nduring the synthesis of the edited image. Unfortunately, this practical tool\nhas enabled malicious users to freely synthesize misinformative or deepfake\ncontents with greater ease, which promotes the spread of unethical and abusive,\nas well as privacy-, and copyright-infringing contents. While defensive\nalgorithms such as AdvDM and Photoguard have been shown to disrupt the\ndiffusion process on these images, the misalignment between their objectives\nand the iterative denoising trajectory at test time results in weak disruptive\nperformance.In this work, we present the DDIM Inversion Attack (DIA) that\nattacks the integrated DDIM trajectory path. Our results support the effective\ndisruption, surpassing previous defensive methods across various editing\nmethods. We believe that our frameworks and results can provide practical\ndefense methods against the malicious use of AI for both the industry and the\nresearch community. Our code is available here:\nhttps://anonymous.4open.science/r/DIA-13419/."
    },
    {
        "date": "2025-10",
        "title": "ZQBA: Zero Query Black-box Adversarial Attack",
        "author": "Joana C. Costa, Tiago Roxo, Hugo Proen\u00e7a, and Pedro R. M. In\u00e1cio",
        "link": "http://arxiv.org/abs/2510.00769v1",
        "abstract": "Current black-box adversarial attacks either require multiple queries or\ndiffusion models to produce adversarial samples that can impair the target\nmodel performance. However, these methods require training a surrogate loss or\ndiffusion models to produce adversarial samples, which limits their\napplicability in real-world settings. Thus, we propose a Zero Query Black-box\nAdversarial (ZQBA) attack that exploits the representations of Deep Neural\nNetworks (DNNs) to fool other networks. Instead of requiring thousands of\nqueries to produce deceiving adversarial samples, we use the feature maps\nobtained from a DNN and add them to clean images to impair the classification\nof a target model. The results suggest that ZQBA can transfer the adversarial\nsamples to different models and across various datasets, namely CIFAR and Tiny\nImageNet. The experiments also show that ZQBA is more effective than\nstate-of-the-art black-box attacks with a single query, while maintaining the\nimperceptibility of perturbations, evaluated both quantitatively (SSIM) and\nqualitatively, emphasizing the vulnerabilities of employing DNNs in real-world\ncontexts. All the source code is available at\nhttps://github.com/Joana-Cabral/ZQBA."
    },
    {
        "date": "2025-10",
        "title": "Downgrade to Upgrade: Optimizer Simplification Enhances Robustness in LLM Unlearning",
        "author": "Yicheng Lang, Yihua Zhang, Chongyu Fan, Changsheng Wang, Jinghan Jia, and Sijia Liu",
        "link": "http://arxiv.org/abs/2510.00761v2",
        "abstract": "Large language model (LLM) unlearning aims to surgically remove the influence\nof undesired data or knowledge from an existing model while preserving its\nutility on unrelated tasks. This paradigm has shown promise in addressing\nprivacy and safety concerns. However, recent findings reveal that unlearning\neffects are often fragile: post-unlearning manipulations such as weight\nquantization or fine-tuning can quickly neutralize the intended forgetting.\nPrior efforts to improve robustness primarily reformulate unlearning objectives\nby explicitly assuming the role of vulnerability sources. In this work, we take\na different perspective by investigating the role of the optimizer, independent\nof unlearning objectives and formulations, in shaping unlearning robustness. We\nshow that the 'grade' of the optimizer, defined by the level of information it\nexploits, ranging from zeroth-order (gradient-free) to first-order\n(gradient-based) to second-order (Hessian-based), is tightly linked to the\nresilience of unlearning. Surprisingly, we find that downgrading the optimizer,\nsuch as using zeroth-order methods or compressed-gradient variants (e.g.,\ngradient sign-based optimizers), often leads to stronger robustness. While\nthese optimizers produce noisier and less precise updates, they encourage\nconvergence to harder-to-disturb basins in the loss landscape, thereby\nresisting post-training perturbations. By connecting zeroth-order methods with\nrandomized smoothing, we further highlight their natural advantage for robust\nunlearning. Motivated by these insights, we propose a hybrid optimizer that\ncombines first-order and zeroth-order updates, preserving unlearning efficacy\nwhile enhancing robustness. Extensive experiments on the MUSE and WMDP\nbenchmarks, across multiple LLM unlearning algorithms, validate that our\napproach achieves more resilient forgetting without sacrificing unlearning\nquality."
    },
    {
        "date": "2025-10",
        "title": "Erased, But Not Forgotten: Erased Rectified Flow Transformers Still Remain Unsafe Under Concept Attack",
        "author": "Nanxiang Jiang, Zhaoxin Fan, Enhan Kang, Daiheng Gao, Yun Zhou, Yanxia Chang, Zheng Zhu, Yeying Jin, and Wenjun Wu",
        "link": "http://arxiv.org/abs/2510.00635v2",
        "abstract": "Recent advances in text-to-image (T2I) diffusion models have enabled\nimpressive generative capabilities, but they also raise significant safety\nconcerns due to the potential to produce harmful or undesirable content. While\nconcept erasure has been explored as a mitigation strategy, most existing\napproaches and corresponding attack evaluations are tailored to Stable\nDiffusion (SD) and exhibit limited effectiveness when transferred to\nnext-generation rectified flow transformers such as Flux. In this work, we\npresent ReFlux, the first concept attack method specifically designed to assess\nthe robustness of concept erasure in the latest rectified flow-based T2I\nframework. Our approach is motivated by the observation that existing concept\nerasure techniques, when applied to Flux, fundamentally rely on a phenomenon\nknown as attention localization. Building on this insight, we propose a simple\nyet effective attack strategy that specifically targets this property. At its\ncore, a reverse-attention optimization strategy is introduced to effectively\nreactivate suppressed signals while stabilizing attention. This is further\nreinforced by a velocity-guided dynamic that enhances the robustness of concept\nreactivation by steering the flow matching process, and a\nconsistency-preserving objective that maintains the global layout and preserves\nunrelated content. Extensive experiments consistently demonstrate the\neffectiveness and efficiency of the proposed attack method, establishing a\nreliable benchmark for evaluating the robustness of concept erasure strategies\nin rectified flow transformers."
    },
    {
        "date": "2025-10",
        "title": "Robust Context-Aware Object Recognition",
        "author": "Klara Janouskova, Cristian Gavrus, and Jiri Matas",
        "link": "http://arxiv.org/abs/2510.00618v1",
        "abstract": "In visual recognition, both the object of interest (referred to as\nforeground, FG, for simplicity) and its surrounding context (background, BG)\nplay an important role. However, standard supervised learning often leads to\nunintended over-reliance on the BG, known as shortcut learning of spurious\ncorrelations, limiting model robustness in real-world deployment settings. In\nthe literature, the problem is mainly addressed by suppressing the BG,\nsacrificing context information for improved generalization.\n  We propose RCOR -- Robust Context-Aware Object Recognition -- the first\napproach that jointly achieves robustness and context-awareness without\ncompromising either. RCOR treats localization as an integral part of\nrecognition to decouple object-centric and context-aware modelling, followed by\na robust, non-parametric fusion. It improves the performance of both supervised\nmodels and VLM on datasets with both in-domain and out-of-domain BG, even\nwithout fine-tuning. The results confirm that localization before recognition\nis now possible even in complex scenes as in ImageNet-1k."
    },
    {
        "date": "2025-10",
        "title": "Designing Ambiguity Sets for Distributionally Robust Optimization Using Structural Causal Optimal Transport",
        "author": "Ahmad-Reza Ehyaei, Golnoosh Farnadi, and Samira Samadi",
        "link": "http://arxiv.org/abs/2510.00599v1",
        "abstract": "Distributionally robust optimization tackles out-of-sample issues like\noverfitting and distribution shifts by adopting an adversarial approach over a\nrange of possible data distributions, known as the ambiguity set. To balance\nconservatism and accuracy, these sets must include realistic probability\ndistributions by leveraging information from the nominal distribution. Assuming\nthat nominal distributions arise from a structural causal model with a directed\nacyclic graph $\\mathcal{G}$ and structural equations, previous methods such as\nadapted and $\\mathcal{G}$-causal optimal transport have only utilized causal\ngraph information in designing ambiguity sets. In this work, we propose\nincorporating structural equations, which include causal graph information, to\nenhance ambiguity sets, resulting in more realistic distributions. We introduce\nstructural causal optimal transport and its associated ambiguity set,\ndemonstrating their advantages and connections to previous methods. A key\nbenefit of our approach is a relaxed version, where a regularization term\nreplaces the complex causal constraints, enabling an efficient algorithm via\ndifference-of-convex programming to solve structural causal optimal transport.\nWe also show that when structural information is absent and must be estimated,\nour approach remains effective and provides finite sample guarantees. Lastly,\nwe address the radius of ambiguity sets, illustrating how our method overcomes\nthe curse of dimensionality in optimal transport problems, achieving faster\nshrinkage with dimension-free order."
    },
    {
        "date": "2025-10",
        "title": "Private Online Learning against an Adaptive Adversary: Realizable and Agnostic Settings",
        "author": "Bo Li, Wei Wang, and Peng Ye",
        "link": "http://arxiv.org/abs/2510.00574v1",
        "abstract": "We revisit the problem of private online learning, in which a learner\nreceives a sequence of $T$ data points and has to respond at each time-step a\nhypothesis. It is required that the entire stream of output hypotheses should\nsatisfy differential privacy. Prior work of Golowich and Livni [2021]\nestablished that every concept class $\\mathcal{H}$ with finite Littlestone\ndimension $d$ is privately online learnable in the realizable setting. In\nparticular, they proposed an algorithm that achieves an $O_{d}(\\log T)$ mistake\nbound against an oblivious adversary. However, their approach yields a\nsuboptimal $\\tilde{O}_{d}(\\sqrt{T})$ bound against an adaptive adversary. In\nthis work, we present a new algorithm with a mistake bound of $O_{d}(\\log T)$\nagainst an adaptive adversary, closing this gap. We further investigate the\nproblem in the agnostic setting, which is more general than the realizable\nsetting as it does not impose any assumptions on the data. We give an algorithm\nthat obtains a sublinear regret of $\\tilde{O}_d(\\sqrt{T})$ for generic\nLittlestone classes, demonstrating that they are also privately online\nlearnable in the agnostic setting."
    },
    {
        "date": "2025-10",
        "title": "Attack logics, not outputs: Towards efficient robustification of deep neural networks by falsifying concept-based properties",
        "author": "Raik Dankworth, and Gesina Schwalbe",
        "link": "http://arxiv.org/abs/2510.03320v1",
        "abstract": "Deep neural networks (NNs) for computer vision are vulnerable to adversarial\nattacks, i.e., miniscule malicious changes to inputs may induce unintuitive\noutputs. One key approach to verify and mitigate such robustness issues is to\nfalsify expected output behavior. This allows, e.g., to locally proof security,\nor to (re)train NNs on obtained adversarial input examples. Due to the\nblack-box nature of NNs, current attacks only falsify a class of the final\noutput, such as flipping from $\\texttt{stop_sign}$ to $\\neg\\texttt{stop_sign}$.\nIn this short position paper we generalize this to search for generally\nillogical behavior, as considered in NN verification: falsify constraints\n(concept-based properties) involving further human-interpretable concepts, like\n$\\texttt{red}\\wedge\\texttt{octogonal}\\rightarrow\\texttt{stop_sign}$. For this,\nan easy implementation of concept-based properties on already trained NNs is\nproposed using techniques from explainable artificial intelligence. Further, we\nsketch the theoretical proof that attacks on concept-based properties are\nexpected to have a reduced search space compared to simple class falsification,\nwhilst arguably be more aligned with intuitive robustness targets. As an\noutlook to this work in progress we hypothesize that this approach has\npotential to efficiently and simultaneously improve logical compliance and\nrobustness."
    },
    {
        "date": "2025-10",
        "title": "Memory-Augmented Log Analysis with Phi-4-mini: Enhancing Threat Detection in Structured Security Logs",
        "author": "Anbi Guo, and Mahfuza Farooque",
        "link": "http://arxiv.org/abs/2510.00529v1",
        "abstract": "Structured security logs are critical for detecting advanced persistent\nthreats (APTs). Large language models (LLMs) struggle in this domain due to\nlimited context and domain mismatch. We propose \\textbf{DM-RAG}, a dual-memory\nretrieval-augmented generation framework for structured log analysis. It\nintegrates a short-term memory buffer for recent summaries and a long-term\nFAISS-indexed memory for historical patterns. An instruction-tuned Phi-4-mini\nprocesses the combined context and outputs structured predictions. Bayesian\nfusion promotes reliable persistence into memory. On the UNSW-NB15 dataset,\nDM-RAG achieves 53.64% accuracy and 98.70% recall, surpassing fine-tuned and\nRAG baselines in recall. The architecture is lightweight, interpretable, and\nscalable, enabling real-time threat monitoring without extra corpora or heavy\ntuning."
    },
    {
        "date": "2025-10",
        "title": "Understanding Sensitivity of Differential Attention through the Lens of Adversarial Robustness",
        "author": "Tsubasa Takahashi, Shojiro Yamabe, Futa Waseda, and Kento Sasaki",
        "link": "http://arxiv.org/abs/2510.00517v1",
        "abstract": "Differential Attention (DA) has been proposed as a refinement to standard\nattention, suppressing redundant or noisy context through a subtractive\nstructure and thereby reducing contextual hallucination. While this design\nsharpens task-relevant focus, we show that it also introduces a structural\nfragility under adversarial perturbations. Our theoretical analysis identifies\nnegative gradient alignment-a configuration encouraged by DA's subtraction-as\nthe key driver of sensitivity amplification, leading to increased gradient\nnorms and elevated local Lipschitz constants. We empirically validate this\nFragile Principle through systematic experiments on ViT/DiffViT and evaluations\nof pretrained CLIP/DiffCLIP, spanning five datasets in total. These results\ndemonstrate higher attack success rates, frequent gradient opposition, and\nstronger local sensitivity compared to standard attention. Furthermore,\ndepth-dependent experiments reveal a robustness crossover: stacking DA layers\nattenuates small perturbations via depth-dependent noise cancellation, though\nthis protection fades under larger attack budgets. Overall, our findings\nuncover a fundamental trade-off: DA improves discriminative focus on clean\ninputs but increases adversarial vulnerability, underscoring the need to\njointly design for selectivity and robustness in future attention mechanisms."
    },
    {
        "date": "2025-10",
        "title": "On the Adversarial Robustness of Learning-based Conformal Novelty Detection",
        "author": "Daofu Zhang, Mehrdad Pournaderi, Hanne M. Clifford, Yu Xiang, and Pramod K. Varshney",
        "link": "http://arxiv.org/abs/2510.00463v1",
        "abstract": "This paper studies the adversarial robustness of conformal novelty detection.\nIn particular, we focus on AdaDetect, a powerful learning-based framework for\nnovelty detection with finite-sample false discovery rate (FDR) control. While\nAdaDetect provides rigorous statistical guarantees under benign conditions, its\nbehavior under adversarial perturbations remains unexplored. We first formulate\nan oracle attack setting that quantifies the worst-case degradation of FDR,\nderiving an upper bound that characterizes the statistical cost of attacks.\nThis idealized formulation directly motivates a practical and effective attack\nscheme that only requires query access to AdaDetect's output labels. Coupling\nthese formulations with two popular and complementary black-box adversarial\nalgorithms, we systematically evaluate the vulnerability of AdaDetect on\nsynthetic and real-world datasets. Our results show that adversarial\nperturbations can significantly increase the FDR while maintaining high\ndetection power, exposing fundamental limitations of current error-controlled\nnovelty detection methods and motivating the development of more robust\nalternatives."
    },
    {
        "date": "2025-10",
        "title": "Robust Spatiotemporally Contiguous Anomaly Detection Using Tensor Decomposition",
        "author": "Rachita Mondal, Mert Indibi, Tapabrata Maiti, and Selin Aviyente",
        "link": "http://arxiv.org/abs/2510.00460v1",
        "abstract": "Anomaly detection in spatiotemporal data is a challenging problem encountered\nin a variety of applications, including video surveillance, medical imaging\ndata, and urban traffic monitoring. Existing anomaly detection methods focus\nmainly on point anomalies and cannot deal with temporal and spatial\ndependencies that arise in spatio-temporal data. Tensor-based anomaly detection\nmethods have been proposed to address this problem. Although existing methods\ncan capture dependencies across different modes, they are primarily supervised\nand do not account for the specific structure of anomalies. Moreover, these\nmethods focus mainly on extracting anomalous features without providing any\nstatistical confidence. In this paper, we introduce an unsupervised\ntensor-based anomaly detection method that simultaneously considers the sparse\nand spatiotemporally smooth nature of anomalies. The anomaly detection problem\nis formulated as a regularized robust low-rank + sparse tensor decomposition\nwhere the total variation of the tensor with respect to the underlying spatial\nand temporal graphs quantifies the spatiotemporal smoothness of the anomalies.\nOnce the anomalous features are extracted, we introduce a statistical anomaly\nscoring framework that accounts for local spatio-temporal dependencies. The\nproposed framework is evaluated on both synthetic and real data."
    },
    {
        "date": "2025-10",
        "title": "SVDefense: Effective Defense against Gradient Inversion Attacks via Singular Value Decomposition",
        "author": "Chenxiang Luo, David K. Y. Yau, and Qun Song",
        "link": "http://arxiv.org/abs/2510.03319v1",
        "abstract": "Federated learning (FL) enables collaborative model training without sharing\nraw data but is vulnerable to gradient inversion attacks (GIAs), where\nadversaries reconstruct private data from shared gradients. Existing defenses\neither incur impractical computational overhead for embedded platforms or fail\nto achieve privacy protection and good model utility at the same time.\nMoreover, many defenses can be easily bypassed by adaptive adversaries who have\nobtained the defense details. To address these limitations, we propose\nSVDefense, a novel defense framework against GIAs that leverages the truncated\nSingular Value Decomposition (SVD) to obfuscate gradient updates. SVDefense\nintroduces three key innovations, a Self-Adaptive Energy Threshold that adapts\nto client vulnerability, a Channel-Wise Weighted Approximation that selectively\npreserves essential gradient information for effective model training while\nenhancing privacy protection, and a Layer-Wise Weighted Aggregation for\neffective model aggregation under class imbalance. Our extensive evaluation\nshows that SVDefense outperforms existing defenses across multiple\napplications, including image classification, human activity recognition, and\nkeyword spotting, by offering robust privacy protection with minimal impact on\nmodel accuracy. Furthermore, SVDefense is practical for deployment on various\nresource-constrained embedded platforms. We will make our code publicly\navailable upon paper acceptance."
    },
    {
        "date": "2025-10",
        "title": "A Call to Action for a Secure-by-Design Generative AI Paradigm",
        "author": "Dalal Alharthi, and Ivan Roberto Kawaminami Garcia",
        "link": "http://arxiv.org/abs/2510.00451v1",
        "abstract": "Large language models have gained widespread prominence, yet their\nvulnerability to prompt injection and other adversarial attacks remains a\ncritical concern. This paper argues for a security-by-design AI paradigm that\nproactively mitigates LLM vulnerabilities while enhancing performance. To\nachieve this, we introduce PromptShield, an ontology-driven framework that\nensures deterministic and secure prompt interactions. It standardizes user\ninputs through semantic validation, eliminating ambiguity and mitigating\nadversarial manipulation. To assess PromptShield's security and performance\ncapabilities, we conducted an experiment on an agent-based system to analyze\ncloud logs within Amazon Web Services (AWS), containing 493 distinct events\nrelated to malicious activities and anomalies. By simulating prompt injection\nattacks and assessing the impact of deploying PromptShield, our results\ndemonstrate a significant improvement in model security and performance,\nachieving precision, recall, and F1 scores of approximately 94%. Notably, the\nontology-based framework not only mitigates adversarial threats but also\nenhances the overall performance and reliability of the system. Furthermore,\nPromptShield's modular and adaptable design ensures its applicability beyond\ncloud security, making it a robust solution for safeguarding generative AI\napplications across various domains. By laying the groundwork for AI safety\nstandards and informing future policy development, this work stimulates a\ncrucial dialogue on the pivotal role of deterministic prompt engineering and\nontology-based validation in ensuring the safe and responsible deployment of\nLLMs in high-stakes environments."
    },
    {
        "date": "2025-10",
        "title": "EgoTraj-Bench: Towards Robust Trajectory Prediction Under Ego-view Noisy Observations",
        "author": "Jiayi Liu, Jiaming Zhou, Ke Ye, Kun-Yu Lin, Allan Wang, and Junwei Liang",
        "link": "http://arxiv.org/abs/2510.00405v1",
        "abstract": "Reliable trajectory prediction from an ego-centric perspective is crucial for\nrobotic navigation in human-centric environments. However, existing methods\ntypically assume idealized observation histories, failing to account for the\nperceptual artifacts inherent in first-person vision, such as occlusions, ID\nswitches, and tracking drift. This discrepancy between training assumptions and\ndeployment reality severely limits model robustness. To bridge this gap, we\nintroduce EgoTraj-Bench, the first real-world benchmark that grounds noisy,\nfirst-person visual histories in clean, bird's-eye-view future trajectories,\nenabling robust learning under realistic perceptual constraints. Building on\nthis benchmark, we propose BiFlow, a dual-stream flow matching model that\nconcurrently denoises historical observations and forecasts future motion by\nleveraging a shared latent representation. To better model agent intent, BiFlow\nincorporates our EgoAnchor mechanism, which conditions the prediction decoder\non distilled historical features via feature modulation. Extensive experiments\nshow that BiFlow achieves state-of-the-art performance, reducing minADE and\nminFDE by 10-15% on average and demonstrating superior robustness. We\nanticipate that our benchmark and model will provide a critical foundation for\ndeveloping trajectory forecasting systems truly resilient to the challenges of\nreal-world, ego-centric perception."
    },
    {
        "date": "2025-09",
        "title": "DiSA-IQL: Offline Reinforcement Learning for Robust Soft Robot Control under Distribution Shifts",
        "author": "Linjin He, Xinda Qi, Dong Chen, Zhaojian Li, and Xiaobo Tan",
        "link": "http://arxiv.org/abs/2510.00358v1",
        "abstract": "Soft snake robots offer remarkable flexibility and adaptability in complex\nenvironments, yet their control remains challenging due to highly nonlinear\ndynamics. Existing model-based and bio-inspired controllers rely on simplified\nassumptions that limit performance. Deep reinforcement learning (DRL) has\nrecently emerged as a promising alternative, but online training is often\nimpractical because of costly and potentially damaging real-world interactions.\nOffline RL provides a safer option by leveraging pre-collected datasets, but it\nsuffers from distribution shift, which degrades generalization to unseen\nscenarios. To overcome this challenge, we propose DiSA-IQL\n(Distribution-Shift-Aware Implicit Q-Learning), an extension of IQL that\nincorporates robustness modulation by penalizing unreliable state-action pairs\nto mitigate distribution shift. We evaluate DiSA-IQL on goal-reaching tasks\nacross two settings: in-distribution and out-of-distribution evaluation.\nSimulation results show that DiSA-IQL consistently outperforms baseline models,\nincluding Behavior Cloning (BC), Conservative Q-Learning (CQL), and vanilla\nIQL, achieving higher success rates, smoother trajectories, and improved\nrobustness. The codes are open-sourced to support reproducibility and to\nfacilitate further research in offline RL for soft robot control."
    },
    {
        "date": "2025-09",
        "title": "Security and Privacy Analysis of Tile's Location Tracking Protocol",
        "author": "Akshaya Kumar, Anna Raymaker, and Michael Specter",
        "link": "http://arxiv.org/abs/2510.00350v1",
        "abstract": "We conduct the first comprehensive security analysis of Tile, the second most\npopular crowd-sourced location-tracking service behind Apple's AirTags. We\nidentify several exploitable vulnerabilities and design flaws, disproving many\nof the platform's claimed security and privacy guarantees: Tile's servers can\npersistently learn the location of all users and tags, unprivileged adversaries\ncan track users through Bluetooth advertisements emitted by Tile's devices, and\nTile's anti-theft mode is easily subverted.\n  Despite its wide deployment -- millions of users, devices, and purpose-built\nhardware tags -- Tile provides no formal description of its protocol or threat\nmodel. Worse, Tile intentionally weakens its antistalking features to support\nan antitheft use-case and relies on a novel \"accountability\" mechanism to\npunish those abusing the system to stalk victims.\n  We examine Tile's accountability mechanism, a unique feature of independent\ninterest; no other provider attempts to guarantee accountability. While an\nideal accountability mechanism may disincentivize abuse in crowd-sourced\nlocation tracking protocols, we show that Tile's implementation is subvertible\nand introduces new exploitable vulnerabilities. We conclude with a discussion\non the need for new, formal definitions of accountability in this setting."
    },
    {
        "date": "2025-09",
        "title": "When Hallucination Costs Millions: Benchmarking AI Agents in High-Stakes Adversarial Financial Markets",
        "author": "Zeshi Dai, Zimo Peng, Zerui Cheng, and Ryan Yihe Li",
        "link": "http://arxiv.org/abs/2510.00332v1",
        "abstract": "We present CAIA, a benchmark exposing a critical blind spot in AI evaluation:\nthe inability of state-of-the-art models to operate in adversarial, high-stakes\nenvironments where misinformation is weaponized and errors are irreversible.\nWhile existing benchmarks measure task completion in controlled settings,\nreal-world deployment demands resilience against active deception. Using crypto\nmarkets as a testbed where $30 billion was lost to exploits in 2024, we\nevaluate 17 models on 178 time-anchored tasks requiring agents to distinguish\ntruth from manipulation, navigate fragmented information landscapes, and make\nirreversible financial decisions under adversarial pressure.\n  Our results reveal a fundamental capability gap: without tools, even frontier\nmodels achieve only 28% accuracy on tasks junior analysts routinely handle.\nTool augmentation improves performance but plateaus at 67.4% versus 80% human\nbaseline, despite unlimited access to professional resources. Most critically,\nwe uncover a systematic tool selection catastrophe: models preferentially\nchoose unreliable web search over authoritative data, falling for SEO-optimized\nmisinformation and social media manipulation. This behavior persists even when\ncorrect answers are directly accessible through specialized tools, suggesting\nfoundational limitations rather than knowledge gaps. We also find that Pass@k\nmetrics mask dangerous trial-and-error behavior for autonomous deployment.\n  The implications extend beyond crypto to any domain with active adversaries,\ne.g. cybersecurity, content moderation, etc. We release CAIA with contamination\ncontrols and continuous updates, establishing adversarial robustness as a\nnecessary condition for trustworthy AI autonomy. The benchmark reveals that\ncurrent models, despite impressive reasoning scores, remain fundamentally\nunprepared for environments where intelligence must survive active opposition."
    },
    {
        "date": "2025-09",
        "title": "Robust Federated Inference",
        "author": "Akash Dhasade, Sadegh Farhadkhani, Rachid Guerraoui, Nirupam Gupta, Maxime Jacovella, Anne-Marie Kermarrec, and Rafael Pinot",
        "link": "http://arxiv.org/abs/2510.00310v1",
        "abstract": "Federated inference, in the form of one-shot federated learning, edge\nensembles, or federated ensembles, has emerged as an attractive solution to\ncombine predictions from multiple models. This paradigm enables each model to\nremain local and proprietary while a central server queries them and aggregates\npredictions. Yet, the robustness of federated inference has been largely\nneglected, leaving them vulnerable to even simple attacks. To address this\ncritical gap, we formalize the problem of robust federated inference and\nprovide the first robustness analysis of this class of methods. Our analysis of\naveraging-based aggregators shows that the error of the aggregator is small\neither when the dissimilarity between honest responses is small or the margin\nbetween the two most probable classes is large. Moving beyond linear averaging,\nwe show that problem of robust federated inference with non-linear aggregators\ncan be cast as an adversarial machine learning problem. We then introduce an\nadvanced technique using the DeepSet aggregation model, proposing a novel\ncomposition of adversarial training and test-time robust aggregation to\nrobustify non-linear aggregators. Our composition yields significant\nimprovements, surpassing existing robust aggregation methods by 4.7 - 22.2% in\naccuracy points across diverse benchmarks."
    },
    {
        "date": "2025-09",
        "title": "PrunedLoRA: Robust Gradient-Based structured pruning for Low-rank Adaptation in Fine-tuning",
        "author": "Xin Yu, Cong Xie, Ziyu Zhao, Tiantian Fan, Lingzhou Xue, and Zhi Zhang",
        "link": "http://arxiv.org/abs/2510.00192v1",
        "abstract": "Low-rank adaptation (LoRA) has become a widely used paradigm for\nparameter-efficient fine-tuning of large language models, yet its\nrepresentational capacity often lags behind full fine-tuning. Within the\ncontext of LoRA, a key open question is how to obtain expressive low-rank\nadapters from over-parameterized spaces. We propose \\textit{PrunedLoRA}, a new\nframework that leverages structured pruning to obtain highly representative\nlow-rank adapters from an over-parameterized initialization. Unlike prior\napproaches that impose a fixed low-rank budget, PrunedLoRA dynamically prunes\nless important components during fine-tuning and prevents their reactivation,\nenabling flexible and adaptive rank allocation. For structured pruning, by\nminimizing the pruning error for overall loss, we provide fine-grained pruning\nand recovery updates in a gradient-based pruning strategy with grounded\ninterpretation. We provide the first theoretical analysis of the robustness of\nstructured pruning and provably show that under the impact of weight\nperturbation, gradient-based pruning is more robust than activation-based\npruning with respect to overall loss. Empirically, PrunedLoRA consistently\noutperforms LoRA and its variants across supervised fine-tuning tasks in\nmathematical reasoning, code generation, and natural language understanding,\nand it also demonstrates advantages over existing structured pruning methods\nacross diverse sparsity levels."
    },
    {
        "date": "2025-09",
        "title": "Secure and Robust Watermarking for AI-generated Images: A Comprehensive Survey",
        "author": "Jie Cao, Qi Li, Zelin Zhang, and Jianbing Ni",
        "link": "http://arxiv.org/abs/2510.02384v1",
        "abstract": "The rapid advancement of generative artificial intelligence (Gen-AI) has\nfacilitated the effortless creation of high-quality images, while\nsimultaneously raising critical concerns regarding intellectual property\nprotection, authenticity, and accountability. Watermarking has emerged as a\npromising solution to these challenges by distinguishing AI-generated images\nfrom natural content, ensuring provenance, and fostering trustworthy digital\necosystems. This paper presents a comprehensive survey of the current state of\nAI-generated image watermarking, addressing five key dimensions: (1)\nformalization of image watermarking systems; (2) an overview and comparison of\ndiverse watermarking techniques; (3) evaluation methodologies with respect to\nvisual quality, capacity, and detectability; (4) vulnerabilities to malicious\nattacks; and (5) prevailing challenges and future directions. The survey aims\nto equip researchers with a holistic understanding of AI-generated image\nwatermarking technologies, thereby promoting their continued development."
    },
    {
        "date": "2025-09",
        "title": "Noisy-Pair Robust Representation Alignment for Positive-Unlabeled Learning",
        "author": "Hengwei Zhao, Zhengzhong Tu, Zhuo Zheng, Wei Wang, Junjue Wang, Rusty Feagin, and Wenzhe Jiao",
        "link": "http://arxiv.org/abs/2510.01278v1",
        "abstract": "Positive-Unlabeled (PU) learning aims to train a binary classifier (positive\nvs. negative) where only limited positive data and abundant unlabeled data are\navailable. While widely applicable, state-of-the-art PU learning methods\nsubstantially underperform their supervised counterparts on complex datasets,\nespecially without auxiliary negatives or pre-estimated parameters (e.g., a\n14.26% gap on CIFAR-100 dataset). We identify the primary bottleneck as the\nchallenge of learning discriminative representations under unreliable\nsupervision. To tackle this challenge, we propose NcPU, a non-contrastive PU\nlearning framework that requires no auxiliary information. NcPU combines a\nnoisy-pair robust supervised non-contrastive loss (NoiSNCL), which aligns\nintra-class representations despite unreliable supervision, with a phantom\nlabel disambiguation (PLD) scheme that supplies conservative negative\nsupervision via regret-based label updates. Theoretically, NoiSNCL and PLD can\niteratively benefit each other from the perspective of the\nExpectation-Maximization framework. Empirically, extensive experiments\ndemonstrate that: (1) NoiSNCL enables simple PU methods to achieve competitive\nperformance; and (2) NcPU achieves substantial improvements over\nstate-of-the-art PU methods across diverse datasets, including challenging\ndatasets on post-disaster building damage mapping, highlighting its promise for\nreal-world applications. Code: Code will be open-sourced after review."
    },
    {
        "date": "2025-09",
        "title": "Optimizing What Matters: AUC-Driven Learning for Robust Neural Retrieval",
        "author": "Nima Sheikholeslami, Erfan Hosseini, Patrice Bechard, Srivatsava Daruru, and Sai Rajeswar",
        "link": "http://arxiv.org/abs/2510.00137v1",
        "abstract": "Dual-encoder retrievers depend on the principle that relevant documents\nshould score higher than irrelevant ones for a given query. Yet the dominant\nNoise Contrastive Estimation (NCE) objective, which underpins Contrastive Loss,\noptimizes a softened ranking surrogate that we rigorously prove is\nfundamentally oblivious to score separation quality and unrelated to AUC. This\nmismatch leads to poor calibration and suboptimal performance in downstream\ntasks like retrieval-augmented generation (RAG). To address this fundamental\nlimitation, we introduce the MW loss, a new training objective that maximizes\nthe Mann-Whitney U statistic, which is mathematically equivalent to the Area\nunder the ROC Curve (AUC). MW loss encourages each positive-negative pair to be\ncorrectly ranked by minimizing binary cross entropy over score differences. We\nprovide theoretical guarantees that MW loss directly upper-bounds the AoC,\nbetter aligning optimization with retrieval goals. We further promote ROC\ncurves and AUC as natural threshold free diagnostics for evaluating retriever\ncalibration and ranking quality. Empirically, retrievers trained with MW loss\nconsistently outperform contrastive counterparts in AUC and standard retrieval\nmetrics. Our experiments show that MW loss is an empirically superior\nalternative to Contrastive Loss, yielding better-calibrated and more\ndiscriminative retrievers for high-stakes applications like RAG."
    },
    {
        "date": "2025-09",
        "title": "Are Robust LLM Fingerprints Adversarially Robust?",
        "author": "Anshul Nasery, Edoardo Contente, Alkin Kaz, Pramod Viswanath, and Sewoong Oh",
        "link": "http://arxiv.org/abs/2509.26598v1",
        "abstract": "Model fingerprinting has emerged as a promising paradigm for claiming model\nownership. However, robustness evaluations of these schemes have mostly focused\non benign perturbations such as incremental fine-tuning, model merging, and\nprompting. Lack of systematic investigations into {\\em adversarial robustness}\nagainst a malicious model host leaves current systems vulnerable. To bridge\nthis gap, we first define a concrete, practical threat model against model\nfingerprinting. We then take a critical look at existing model fingerprinting\nschemes to identify their fundamental vulnerabilities. Based on these, we\ndevelop adaptive adversarial attacks tailored for each vulnerability, and\ndemonstrate that these can bypass model authentication completely for ten\nrecently proposed fingerprinting schemes while maintaining high utility of the\nmodel for the end users. Our work encourages fingerprint designers to adopt\nadversarial robustness by design. We end with recommendations for future\nfingerprinting methods."
    },
    {
        "date": "2025-09",
        "title": "Machine-Learning Driven Load Shedding to Mitigate Instability Attacks in Power Grids",
        "author": "Justin Tackett, Benjamin Francis, Luis Garcia, David Grimsman, and Sean Warnick",
        "link": "http://arxiv.org/abs/2509.26532v1",
        "abstract": "Every year critical infrastructure becomes more complex and we grow to rely\non it more and more. With this reliance, it becomes an attractive target for\ncyberattacks from sophisticated actors, with one of the most attractive targets\nbeing the power grid. One class of attacks, instability attacks, is a newer\ntype of attack that has relatively few protections developed. We present a cost\neffective, data-driven approach to training a supervised machine learning model\nto retrofit load shedding decision systems in power grids with the capacity to\ndefend against instability attacks. We show a proof of concept on the IEEE 14\nBus System using the Achilles Heel Technologies Power Grid Analyzer, and show\nthrough an implementation of modified Prony analysis (MPA) that MPA is a viable\nmethod for detecting instability attacks and triggering defense mechanisms."
    },
    {
        "date": "2025-09",
        "title": "Explainable and Resilient ML-Based Physical-Layer Attack Detectors",
        "author": "Aleksandra Knapi\u0144ska, and Marija Furdek",
        "link": "http://arxiv.org/abs/2509.26530v1",
        "abstract": "Detection of emerging attacks on network infrastructure is a critical aspect\nof security management. To meet the growing scale and complexity of modern\nthreats, machine learning (ML) techniques offer valuable tools for automating\nthe detection of malicious activities. However, as these techniques become more\ncomplex, their internal operations grow increasingly opaque. In this context,\nwe address the need for explainable physical-layer attack detection methods.\nFirst, we analyze the inner workings of various classifiers trained to alert\nabout physical layer intrusions, examining how the influence of different\nmonitored parameters varies depending on the type of attack being detected.\nThis analysis not only improves the interpretability of the models but also\nsuggests ways to enhance their design for increased speed. In the second part,\nwe evaluate the detectors' resilience to malicious parameter noising. The\nresults highlight a key trade-off between model speed and resilience. This work\nserves as a design guideline for developing fast and robust detectors trained\non available network monitoring data."
    },
    {
        "date": "2025-09",
        "title": "DEPTHOR++: Robust Depth Enhancement from a Real-World Lightweight dToF and RGB Guidance",
        "author": "Jijun Xiang, Longliang Liu, Xuan Zhu, Xianqi Wang, Min Lin, and Xin Yang",
        "link": "http://arxiv.org/abs/2509.26498v1",
        "abstract": "Depth enhancement, which converts raw dToF signals into dense depth maps\nusing RGB guidance, is crucial for improving depth perception in high-precision\ntasks such as 3D reconstruction and SLAM. However, existing methods often\nassume ideal dToF inputs and perfect dToF-RGB alignment, overlooking\ncalibration errors and anomalies, thus limiting real-world applicability. This\nwork systematically analyzes the noise characteristics of real-world\nlightweight dToF sensors and proposes a practical and novel depth completion\nframework, DEPTHOR++, which enhances robustness to noisy dToF inputs from three\nkey aspects. First, we introduce a simulation method based on synthetic\ndatasets to generate realistic training samples for robust model training.\nSecond, we propose a learnable-parameter-free anomaly detection mechanism to\nidentify and remove erroneous dToF measurements, preventing misleading\npropagation during completion. Third, we design a depth completion network\ntailored to noisy dToF inputs, which integrates RGB images and pre-trained\nmonocular depth estimation priors to improve depth recovery in challenging\nregions. On the ZJU-L5 dataset and real-world samples, our training strategy\nsignificantly boosts existing depth completion models, with our model achieving\nstate-of-the-art performance, improving RMSE and Rel by 22% and 11% on average.\nOn the Mirror3D-NYU dataset, by incorporating the anomaly detection method, our\nmodel improves upon the previous SOTA by 37% in mirror regions. On the Hammer\ndataset, using simulated low-cost dToF data from RealSense L515, our method\nsurpasses the L515 measurements with an average gain of 22%, demonstrating its\npotential to enable low-cost sensors to outperform higher-end devices.\nQualitative results across diverse real-world datasets further validate the\neffectiveness and generalizability of our approach."
    },
    {
        "date": "2025-09",
        "title": "STaR-Attack: A Spatio-Temporal and Narrative Reasoning Attack Framework for Unified Multimodal Understanding and Generation Models",
        "author": "Shaoxiong Guo, Tianyi Du, Lijun Li, Yuyao Wu, Jie Li, and Jing Shao",
        "link": "http://arxiv.org/abs/2509.26473v1",
        "abstract": "Unified Multimodal understanding and generation Models (UMMs) have\ndemonstrated remarkable capabilities in both understanding and generation\ntasks. However, we identify a vulnerability arising from the\ngeneration-understanding coupling in UMMs. The attackers can use the generative\nfunction to craft an information-rich adversarial image and then leverage the\nunderstanding function to absorb it in a single pass, which we call Cross-Modal\nGenerative Injection (CMGI). Current attack methods on malicious instructions\nare often limited to a single modality while also relying on prompt rewriting\nwith semantic drift, leaving the unique vulnerabilities of UMMs unexplored. We\npropose STaR-Attack, the first multi-turn jailbreak attack framework that\nexploits unique safety weaknesses of UMMs without semantic drift. Specifically,\nour method defines a malicious event that is strongly correlated with the\ntarget query within a spatio-temporal context. Using the three-act narrative\ntheory, STaR-Attack generates the pre-event and the post-event scenes while\nconcealing the malicious event as the hidden climax. When executing the attack\nstrategy, the opening two rounds exploit the UMM's generative ability to\nproduce images for these scenes. Subsequently, an image-based question guessing\nand answering game is introduced by exploiting the understanding capability.\nSTaR-Attack embeds the original malicious question among benign candidates,\nforcing the model to select and answer the most relevant one given the\nnarrative context. Extensive experiments show that STaR-Attack consistently\nsurpasses prior approaches, achieving up to 93.06% ASR on Gemini-2.0-Flash and\nsurpasses the strongest prior baseline, FlipAttack. Our work uncovers a\ncritical yet underdeveloped vulnerability and highlights the need for safety\nalignments in UMMs."
    },
    {
        "date": "2025-09",
        "title": "SoK: Systematic analysis of adversarial threats against deep learning approaches for autonomous anomaly detection systems in SDN-IoT networks",
        "author": "Tharindu Lakshan Yasarathna, and Nhien-An Le-Khac",
        "link": "http://arxiv.org/abs/2509.26350v1",
        "abstract": "Integrating SDN and the IoT enhances network control and flexibility.\nDL-based AAD systems improve security by enabling real-time threat detection in\nSDN-IoT networks. However, these systems remain vulnerable to adversarial\nattacks that manipulate input data or exploit model weaknesses, significantly\ndegrading detection accuracy. Existing research lacks a systematic analysis of\nadversarial vulnerabilities specific to DL-based AAD systems in SDN-IoT\nenvironments. This SoK study introduces a structured adversarial threat model\nand a comprehensive taxonomy of attacks, categorising them into data, model,\nand hybrid-level threats. Unlike previous studies, we systematically evaluate\nwhite, black, and grey-box attack strategies across popular benchmark datasets.\nOur findings reveal that adversarial attacks can reduce detection accuracy by\nup to 48.4%, with Membership Inference causing the most significant drop. C&W\nand DeepFool achieve high evasion success rates. However, adversarial training\nenhances robustness, and its high computational overhead limits the real-time\ndeployment of SDN-IoT applications. We propose adaptive countermeasures,\nincluding real-time adversarial mitigation, enhanced retraining mechanisms, and\nexplainable AI-driven security frameworks. By integrating structured threat\nmodels, this study offers a more comprehensive approach to attack\ncategorisation, impact assessment, and defence evaluation than previous\nresearch. Our work highlights critical vulnerabilities in existing DL-based AAD\nmodels and provides practical recommendations for improving resilience,\ninterpretability, and computational efficiency. This study serves as a\nfoundational reference for researchers and practitioners seeking to enhance\nDL-based AAD security in SDN-IoT networks, offering a systematic adversarial\nthreat model and conceptual defence evaluation based on prior empirical\nstudies."
    },
    {
        "date": "2025-09",
        "title": "SafeBehavior: Simulating Human-Like Multistage Reasoning to Mitigate Jailbreak Attacks in Large Language Models",
        "author": "Qinjian Zhao, Jiaqi Wang, Zhiqiang Gao, Zhihao Dou, Belal Abuhaija, and Kaizhu Huang",
        "link": "http://arxiv.org/abs/2509.26345v1",
        "abstract": "Large Language Models (LLMs) have achieved impressive performance across\ndiverse natural language processing tasks, but their growing power also\namplifies potential risks such as jailbreak attacks that circumvent built-in\nsafety mechanisms. Existing defenses including input paraphrasing, multi step\nevaluation, and safety expert models often suffer from high computational\ncosts, limited generalization, or rigid workflows that fail to detect subtle\nmalicious intent embedded in complex contexts. Inspired by cognitive science\nfindings on human decision making, we propose SafeBehavior, a novel\nhierarchical jailbreak defense mechanism that simulates the adaptive multistage\nreasoning process of humans. SafeBehavior decomposes safety evaluation into\nthree stages: intention inference to detect obvious input risks, self\nintrospection to assess generated responses and assign confidence based\njudgments, and self revision to adaptively rewrite uncertain outputs while\npreserving user intent and enforcing safety constraints. We extensively\nevaluate SafeBehavior against five representative jailbreak attack types\nincluding optimization based, contextual manipulation, and prompt based attacks\nand compare it with seven state of the art defense baselines. Experimental\nresults show that SafeBehavior significantly improves robustness and\nadaptability across diverse threat scenarios, offering an efficient and human\ninspired approach to safeguarding LLMs against jailbreak attempts."
    },
    {
        "date": "2025-09",
        "title": "Wasserstein Distributionally Robust Optimization Through the Lens of Structural Causal Models and Individual Fairness",
        "author": "Ahmad-Reza Ehyaei, Golnoosh Farnadi, and Samira Samadi",
        "link": "http://arxiv.org/abs/2509.26275v1",
        "abstract": "In recent years, Wasserstein Distributionally Robust Optimization (DRO) has\ngarnered substantial interest for its efficacy in data-driven decision-making\nunder distributional uncertainty. However, limited research has explored the\napplication of DRO to address individual fairness concerns, particularly when\nconsidering causal structures and sensitive attributes in learning problems. To\naddress this gap, we first formulate the DRO problem from causality and\nindividual fairness perspectives. We then present the DRO dual formulation as\nan efficient tool to convert the DRO problem into a more tractable and\ncomputationally efficient form. Next, we characterize the closed form of the\napproximate worst-case loss quantity as a regularizer, eliminating the max-step\nin the min-max DRO problem. We further estimate the regularizer in more general\ncases and explore the relationship between DRO and classical robust\noptimization. Finally, by removing the assumption of a known structural causal\nmodel, we provide finite sample error bounds when designing DRO with empirical\ndistributions and estimated causal structures to ensure efficiency and robust\nlearning."
    },
    {
        "date": "2025-09",
        "title": "Stealthy Yet Effective: Distribution-Preserving Backdoor Attacks on Graph Classification",
        "author": "Xiaobao Wang, Ruoxiao Sun, Yujun Zhang, Bingdao Feng, Dongxiao He, Luzhi Wang, and Di Jin",
        "link": "http://arxiv.org/abs/2509.26032v1",
        "abstract": "Graph Neural Networks (GNNs) have demonstrated strong performance across\ntasks such as node classification, link prediction, and graph classification,\nbut remain vulnerable to backdoor attacks that implant imperceptible triggers\nduring training to control predictions. While node-level attacks exploit local\nmessage passing, graph-level attacks face the harder challenge of manipulating\nglobal representations while maintaining stealth. We identify two main sources\nof anomaly in existing graph classification backdoor methods: structural\ndeviation from rare subgraph triggers and semantic deviation caused by label\nflipping, both of which make poisoned graphs easily detectable by anomaly\ndetection models. To address this, we propose DPSBA, a clean-label backdoor\nframework that learns in-distribution triggers via adversarial training guided\nby anomaly-aware discriminators. DPSBA effectively suppresses both structural\nand semantic anomalies, achieving high attack success while significantly\nimproving stealth. Extensive experiments on real-world datasets validate that\nDPSBA achieves a superior balance between effectiveness and detectability\ncompared to state-of-the-art baselines."
    },
    {
        "date": "2025-09",
        "title": "Reconcile Certified Robustness and Accuracy for DNN-based Smoothed Majority Vote Classifier",
        "author": "Gaojie Jin, Xinping Yi, and Xiaowei Huang",
        "link": "http://arxiv.org/abs/2509.25979v1",
        "abstract": "Within the PAC-Bayesian framework, the Gibbs classifier (defined on a\nposterior $Q$) and the corresponding $Q$-weighted majority vote classifier are\ncommonly used to analyze the generalization performance. However, there exists\na notable lack in theoretical research exploring the certified robustness of\nmajority vote classifier and its interplay with generalization. In this study,\nwe develop a generalization error bound that possesses a certified robust\nradius for the smoothed majority vote classifier (i.e., the $Q$-weighted\nmajority vote classifier with smoothed inputs); In other words, the\ngeneralization bound holds under any data perturbation within the certified\nrobust radius. As a byproduct, we find that the underpinnings of both the\ngeneralization bound and the certified robust radius draw, in part, upon weight\nspectral norm, which thereby inspires the adoption of spectral regularization\nin smooth training to boost certified robustness. Utilizing the\ndimension-independent property of spherical Gaussian inputs in smooth training,\nwe propose a novel and inexpensive spectral regularizer to enhance the smoothed\nmajority vote classifier. In addition to the theoretical contribution, a set of\nempirical results is provided to substantiate the effectiveness of our proposed\nmethod."
    },
    {
        "date": "2025-09",
        "title": "Scalable and Robust LLM Unlearning by Correcting Responses with Retrieved Exclusions",
        "author": "Junbeom Kim, Kyuyoung Kim, Jihoon Tack, Dongha Lim, and Jinwoo Shin",
        "link": "http://arxiv.org/abs/2509.25973v1",
        "abstract": "Language models trained on web-scale corpora risk memorizing and exposing\nsensitive information, prompting the need for effective machine unlearning.\nPrior methods mainly focus on input queries to suppress sensitive outputs, yet\nthis often fails to eliminate the underlying knowledge and limits scalability.\nTo address this, we propose Corrective Unlearning with Retrieved Exclusions\n(CURE), a novel unlearning framework that verifies model outputs for leakage\nand revises them into safe responses. Specifically, CURE employs a lightweight\ncorrector that is applied to the original model to verify whether outputs\ncontain target knowledge and to rewrite them if any leakage is detected. To\nefficiently handle large-scale unlearning requests, CURE retrieves unlearning\ntargets that are relevant to the initial response and provides them as\nin-context references to the corrector for detection and conditional revision.\nBy leveraging this retrieval augmentation, the corrector can adapt to new\nunlearning requests without additional training. Extensive evaluations\ndemonstrate that CURE substantially reduces information leakage, even from\nindirect queries where prior works fall short, while maintaining response\nquality and general utility. Moreover, it demonstrates robustness under\ncontinual unlearning scenarios, making it practical for real-world\napplications."
    },
    {
        "date": "2025-09",
        "title": "The Impact of Scaling Training Data on Adversarial Robustness",
        "author": "Marco Zimmerli, Andreas Plesner, Till Aczel, and Roger Wattenhofer",
        "link": "http://arxiv.org/abs/2509.25927v1",
        "abstract": "Deep neural networks remain vulnerable to adversarial examples despite\nadvances in architectures and training paradigms. We investigate how training\ndata characteristics affect adversarial robustness across 36 state-of-the-art\nvision models spanning supervised, self-supervised, and contrastive learning\napproaches, trained on datasets from 1.2M to 22B images. Models were evaluated\nunder six black-box attack categories: random perturbations, two types of\ngeometric masks, COCO object manipulations, ImageNet-C corruptions, and\nImageNet-R style shifts. Robustness follows a logarithmic scaling law with both\ndata volume and model size: a tenfold increase in data reduces attack success\nrate (ASR) on average by ~3.2%, whereas a tenfold increase in model size\nreduces ASR on average by ~13.4%. Notably, some self-supervised models trained\non curated datasets, such as DINOv2, outperform others trained on much larger\nbut less curated datasets, challenging the assumption that scale alone drives\nrobustness. Adversarial fine-tuning of ResNet50s improves generalization across\nstructural variations but not across color distributions. Human evaluation\nreveals persistent gaps between human and machine vision. These results show\nthat while scaling improves robustness, data quality, architecture, and\ntraining objectives play a more decisive role than raw scale in achieving\nbroad-spectrum adversarial resilience."
    },
    {
        "date": "2025-09",
        "title": "ASGuard: Activation-Scaling Guard to Mitigate Targeted Jailbreaking Attack",
        "author": "Yein Park, Jungwoo Park, and Jaewoo Kang",
        "link": "http://arxiv.org/abs/2509.25843v1",
        "abstract": "Large language models (LLMs), despite being safety-aligned, exhibit brittle\nrefusal behaviors that can be circumvented by simple linguistic changes. As\ntense jailbreaking demonstrates that models refusing harmful requests often\ncomply when rephrased in past tense, a critical generalization gap is revealed\nin current alignment methods whose underlying mechanisms are poorly understood.\nIn this work, we introduce Activation-Scaling Guard (ASGuard), an insightful,\nmechanistically-informed framework that surgically mitigates this specific\nvulnerability. For the first step, we use circuit analysis to identify the\nspecific attention heads causally linked to the targeted jailbreaking, the\ntense-changing attack. Second, we train a precise, channel-wise scaling vector\nto recalibrate the activation of tense vulnerable heads. Lastly, we apply it\ninto a \"preventative fine-tuning\", forcing the model to learn a more robust\nrefusal mechanism. Across three LLMs, ASGuard effectively reduces the attack\nsuccess rate of targeted jailbreaking while preserving general capabilities and\nminimizing over refusal, achieving a Pareto-optimal balance between safety and\nutility. Our findings underscore how adversarial suffixes suppress the\npropagation of the refusal-mediating direction, based on mechanistic analysis.\nFurthermore, our work showcases how a deep understanding of model internals can\nbe leveraged to develop practical, efficient, and targeted methods for\nadjusting model behavior, charting a course for more reliable and interpretable\nAI safety."
    },
    {
        "date": "2025-09",
        "title": "Enhancing Certifiable Semantic Robustness via Robust Pruning of Deep Neural Networks",
        "author": "Hanjiang Hu, Bowei Li, Ziwei Wang, Tianhao Wei, Casidhe Hutchison, Eric Sample, and Changliu Liu",
        "link": "http://arxiv.org/abs/2510.00083v1",
        "abstract": "Deep neural networks have been widely adopted in many vision and robotics\napplications with visual inputs. It is essential to verify its robustness\nagainst semantic transformation perturbations, such as brightness and contrast.\nHowever, current certified training and robustness certification methods face\nthe challenge of over-parameterization, which hinders the tightness and\nscalability due to the over-complicated neural networks. To this end, we first\nanalyze stability and variance of layers and neurons against input\nperturbation, showing that certifiable robustness can be indicated by a\nfundamental Unbiased and Smooth Neuron metric (USN). Based on USN, we introduce\na novel neural network pruning method that removes neurons with low USN and\nretains those with high USN, thereby preserving model expressiveness without\nover-parameterization. To further enhance this pruning process, we propose a\nnew Wasserstein distance loss to ensure that pruned neurons are more\nconcentrated across layers. We validate our approach through extensive\nexperiments on the challenging robust keypoint detection task, which involves\nrealistic brightness and contrast perturbations, demonstrating that our method\nachieves superior robustness certification performance and efficiency compared\nto baselines."
    },
    {
        "date": "2025-09",
        "title": "PUREVQ-GAN: Defending Data Poisoning Attacks through Vector-Quantized Bottlenecks",
        "author": "Alexander Branch, Omead Pooladzandi, Radin Khosraviani, Sunay Gajanan Bhat, Jeffrey Jiang, and Gregory Pottie",
        "link": "http://arxiv.org/abs/2509.25792v1",
        "abstract": "We introduce PureVQ-GAN, a defense against data poisoning that forces\nbackdoor triggers through a discrete bottleneck using Vector-Quantized VAE with\nGAN discriminator. By quantizing poisoned images through a learned codebook,\nPureVQ-GAN destroys fine-grained trigger patterns while preserving semantic\ncontent. A GAN discriminator ensures outputs match the natural image\ndistribution, preventing reconstruction of out-of-distribution perturbations.\nOn CIFAR-10, PureVQ-GAN achieves 0% poison success rate (PSR) against Gradient\nMatching and Bullseye Polytope attacks, and 1.64% against Narcissus while\nmaintaining 91-95% clean accuracy. Unlike diffusion-based defenses requiring\nhundreds of iterative refinement steps, PureVQ-GAN is over 50x faster, making\nit practical for real training pipelines."
    },
    {
        "date": "2025-09",
        "title": "Lightweight and Robust Federated Data Valuation",
        "author": "Guojun Tang, Jiayu Zhou, Mohammad Mamun, and Steve Drew",
        "link": "http://arxiv.org/abs/2509.25560v1",
        "abstract": "Federated learning (FL) faces persistent robustness challenges due to non-IID\ndata distributions and adversarial client behavior. A promising mitigation\nstrategy is contribution evaluation, which enables adaptive aggregation by\nquantifying each client's utility to the global model. However,\nstate-of-the-art Shapley-value-based approaches incur high computational\noverhead due to repeated model reweighting and inference, which limits their\nscalability. We propose FedIF, a novel FL aggregation framework that leverages\ntrajectory-based influence estimation to efficiently compute client\ncontributions. FedIF adapts decentralized FL by introducing normalized and\nsmoothed influence scores computed from lightweight gradient operations on\nclient updates and a public validation set. Theoretical analysis demonstrates\nthat FedIF yields a tighter bound on one-step global loss change under noisy\nconditions. Extensive experiments on CIFAR-10 and Fashion-MNIST show that FedIF\nachieves robustness comparable to or exceeding SV-based methods in the presence\nof label noise, gradient noise, and adversarial samples, while reducing\naggregation overhead by up to 450x. Ablation studies confirm the effectiveness\nof FedIF's design choices, including local weight normalization and influence\nsmoothing. Our results establish FedIF as a practical, theoretically grounded,\nand scalable alternative to Shapley-value-based approaches for efficient and\nrobust FL in real-world deployments."
    },
    {
        "date": "2025-09",
        "title": "Robust Visual Localization in Compute-Constrained Environments by Salient Edge Rendering and Weighted Hamming Similarity",
        "author": "Tu-Hoa Pham, Philip Bailey, Daniel Posada, Georgios Georgakis, Jorge Enriquez, Surya Suresh, Marco Dolci, and Philip Twu",
        "link": "http://arxiv.org/abs/2509.25520v1",
        "abstract": "We consider the problem of vision-based 6-DoF object pose estimation in the\ncontext of the notional Mars Sample Return campaign, in which a robotic arm\nwould need to localize multiple objects of interest for low-clearance pickup\nand insertion, under severely constrained hardware. We propose a novel\nlocalization algorithm leveraging a custom renderer together with a new\ntemplate matching metric tailored to the edge domain to achieve robust pose\nestimation using only low-fidelity, textureless 3D models as inputs. Extensive\nevaluations on synthetic datasets as well as from physical testbeds on Earth\nand in situ Mars imagery shows that our method consistently beats the state of\nthe art in compute and memory-constrained localization, both in terms of\nrobustness and accuracy, in turn enabling new possibilities for cheap and\nreliable localization on general-purpose hardware."
    },
    {
        "date": "2025-09",
        "title": "Environmental Rate Manipulation Attacks on Power Grid Security",
        "author": "Yonatan Gizachew Achamyeleh, Yang Xiang, Yun-Ping Hsiao, Yasamin Moghaddas, and Mohammad Abdullah Al Faruque",
        "link": "http://arxiv.org/abs/2509.25476v1",
        "abstract": "The growing complexity of global supply chains has made hardware Trojans a\nsignificant threat in sensor-based power electronics. Traditional Trojan\ndesigns depend on digital triggers or fixed threshold conditions that can be\ndetected during standard testing. In contrast, we introduce Environmental Rate\nManipulation (ERM), a novel Trojan triggering mechanism that activates by\nmonitoring the rate of change in environmental parameters rather than their\nabsolute values. This approach allows the Trojan to remain inactive under\nnormal conditions and evade redundancy and sensor-fusion defenses. We implement\na compact 14~$\\mu$m$^2$ circuit that measures capacitor charging rates in\nstandard sensor front-ends and disrupts inverter pulse-width modulation PWM\nsignals when a rapid change is induced. Experiments on a commercial Texas\nInstruments solar inverter demonstrate that ERM can trigger catastrophic driver\nchip failure. Furthermore, ETAP simulations indicate that a single compromised\n100~kW inverter may initiate cascading grid instabilities. The attack's\nsignificance extends beyond individual sensors to entire classes of\nenvironmental sensing systems common in power electronics, demonstrating\nfundamental challenges for hardware security."
    },
    {
        "date": "2025-09",
        "title": "Balancing Compliance and Privacy in Offline CBDC Transactions Using a Secure Element-based System",
        "author": "Panagiotis Michalopoulos, Anthony Mack, Cameron Clark, Linus Chen, Johannes Sedlmeir, and Andreas Veneris",
        "link": "http://arxiv.org/abs/2509.25469v1",
        "abstract": "Blockchain technology has spawned a vast ecosystem of digital currencies with\nCentral Bank Digital Currencies (CBDCs) -- digital forms of fiat currency --\nbeing one of them. An important feature of digital currencies is facilitating\ntransactions without network connectivity, which can enhance the scalability of\ncryptocurrencies and the privacy of CBDC users. However, in the case of CBDCs,\nthis characteristic also introduces new regulatory challenges, particularly\nwhen it comes to applying established Anti-Money Laundering and Countering the\nFinancing of Terrorism (AML/CFT) frameworks. This paper introduces a prototype\nfor offline digital currency payments, equally applicable to cryptocurrencies\nand CBDCs, that leverages Secure Elements and digital credentials to address\nthe tension of offline payment support with regulatory compliance. Performance\nevaluation results suggest that the prototype can be flexibly adapted to\ndifferent regulatory environments, with a transaction latency comparable to\nreal-life commercial payment systems. Furthermore, we conceptualize how the\nintegration of Zero-Knowledge Proofs into our design could accommodate various\ntiers of enhanced privacy protection."
    },
    {
        "date": "2025-09",
        "title": "Managing Differentiated Secure Connectivity using Intents",
        "author": "Loay Abdelrazek, and Filippo Rebecchi",
        "link": "http://arxiv.org/abs/2509.25462v1",
        "abstract": "Mobile networks in the 5G and 6G era require to rethink how to manage\nsecurity due to the introduction of new services, use cases, each with its own\nsecurity requirements, while simultaneously expanding the threat landscape.\nAlthough automation has emerged as a key enabler to address complexity in\nnetworks, existing approaches lack the expressiveness to define and enforce\ncomplex, goal-driven, and measurable security requirements. In this paper, we\npropose the concept of differentiated security levels and leveraging intents as\na management framework. We discuss the requirements and enablers to extend the\ncurrently defined intent-based management frameworks to pave the path for\nintent-based security management in mobile networks. Our approach formalizes\nboth functional and non-functional security requirements and demonstrates how\nthese can be expressed and modeled using an extended TM Forum (TMF) intent\nsecurity ontology. We further discuss the required standardization steps to\nachieve intent-based security management. Our work aims at advance security\nautomation, improve adaptability, and strengthen the resilience and security\nposture of the next-generation mobile networks."
    },
    {
        "date": "2025-09",
        "title": "Beyond Noisy-TVs: Noise-Robust Exploration Via Learning Progress Monitoring",
        "author": "Zhibo Hou, Zhiyu An, and Wan Du",
        "link": "http://arxiv.org/abs/2509.25438v1",
        "abstract": "When there exists an unlearnable source of randomness (noisy-TV) in the\nenvironment, a naively intrinsic reward driven exploring agent gets stuck at\nthat source of randomness and fails at exploration. Intrinsic reward based on\nuncertainty estimation or distribution similarity, while eventually escapes\nnoisy-TVs as time unfolds, suffers from poor sample efficiency and high\ncomputational cost. Inspired by recent findings from neuroscience that humans\nmonitor their improvements during exploration, we propose a novel method for\nintrinsically-motivated exploration, named Learning Progress Monitoring (LPM).\nDuring exploration, LPM rewards model improvements instead of prediction error\nor novelty, effectively rewards the agent for observing learnable transitions\nrather than the unlearnable transitions. We introduce a dual-network design\nthat uses an error model to predict the expected prediction error of the\ndynamics model in its previous iteration, and use the difference between the\nmodel errors of the current iteration and previous iteration to guide\nexploration. We theoretically show that the intrinsic reward of LPM is\nzero-equivariant and a monotone indicator of Information Gain (IG), and that\nthe error model is necessary to achieve monotonicity correspondence with IG. We\nempirically compared LPM against state-of-the-art baselines in noisy\nenvironments based on MNIST, 3D maze with 160x120 RGB inputs, and Atari.\nResults show that LPM's intrinsic reward converges faster, explores more states\nin the maze experiment, and achieves higher extrinsic reward in Atari. This\nconceptually simple approach marks a shift-of-paradigm of noise-robust\nexploration. For code to reproduce our experiments, see\nhttps://github.com/Akuna23Matata/LPM_exploration"
    },
    {
        "date": "2025-09",
        "title": "Fast Energy-Theft Attack on Frequency-Varying Wireless Power without Additional Sensors",
        "author": "Hui Wang, Nima Tashakor, Xiaoyang Tian, Hans D. Schotten, and Stefan M. Goetz",
        "link": "http://arxiv.org/abs/2509.25394v1",
        "abstract": "With the popularity of wireless charging, energy access protection and\ncybersecurity are gaining importance, especially in public places. Currently,\nthe most common energy encryption method uses frequency and associated\nimpedance variation. However, we have proven that this method is not reliable,\nsince a hacker can detect the changing frequency and adjust the compensation.\nHowever, the previously presented system needed time to follow the updated\nfrequency, while encryption systems may vary the frequency faster to avoid\nenergy theft. Furthermore, the previous system required an additional sensor\ncoil. To solve these problems, we optimized the attack and the associated\nsystem, which can intrude and steal energy within 0.2 ms. The key is the\nelimination of the time-consuming maximum receiver current regulation. Also, we\nuse the main receiving coil rather than any additional sensor antenna to detect\nthe magnetic field. Thus, the new hardware is even simpler. A simulation model\nand experimental results demonstrate the fast response speed of the attack on\nencrypted wireless power and steal 65% of the power. Overall, the applicability\nof the attack is highly improved and leaves less room for hardening the\nencryption. The results demonstrate that energy access protection needs to be\ngiven great attention."
    },
    {
        "date": "2025-09",
        "title": "UniAPL: A Unified Adversarial Preference Learning Framework for Instruct-Following",
        "author": "FaQiang Qian, WeiKun Zhang, Ziliang Wang, Kang An, Xuhui Zheng, Liangjian Wen, Mengya Gao, Yong Dai, and Yichao Wu",
        "link": "http://arxiv.org/abs/2509.25148v1",
        "abstract": "Shaping powerful LLMs to be beneficial and safe is central to AI alignment.\nWe argue that post-training alignment is fundamentally a unified Preference\nLearning problem, involving two modalities: demonstrated preferences (e.g.,\nSupervised Fine-Tuning, SFT) and comparative preferences (e.g., Reinforcement\nLearning, RL).The standard sequential pipeline-SFT followed by RL-is flawed due\nto a critical distributional mismatch: SFT uses static expert data, but as the\npolicy evolves, its generation distribution drifts, making SFT knowledge\nbrittle. Subsequent RL then explores without direct access to the rich,\nground-truth knowledge in expert demonstrations, leading to inefficient,\nungrounded updates. This separation prevents mutual regularization between data\nsources. To address this, we reframe alignment as a constrained optimization\nproblem and propose Unified Adversarial Preference Learning (UniAPL),a novel\nframework that dynamically aligns the policy's distribution with the expert's.\nUniAPL implements a single-stage unified training objective, jointly learning\nfrom mixed batches of SFT and preference data. In every gradient step, dense\nexpert demonstrations directly ground and regularize online exploration,\ninherently resolving distributional mismatch and maximizing data synergy.We\nevaluate UniAPL on instruction-following tasks using Qwen3-235B-Instruct-2507\nas the teacher. Our models match or exceed strong GRPO baselines: +5.77% on\nQwen3-0.6B (matching a 32B model) and +3.75% on Qwen3-4B,even outperforming the\nteacher. Analyses of response length and log-probability distributions confirm\nthat UniAPL outputs closely mimic expert demonstrations, achieving both\nstronger performance and better behavioral alignment."
    },
    {
        "date": "2025-09",
        "title": "Learning in an Echo Chamber: Online Learning with Replay Adversary",
        "author": "Daniil Dmitriev, Harald Eskelund Franck, Carolin Heinzler, and Amartya Sanyal",
        "link": "http://arxiv.org/abs/2509.25135v1",
        "abstract": "As machine learning systems increasingly train on self-annotated data, they\nrisk reinforcing errors and becoming echo chambers of their own beliefs. We\nmodel this phenomenon by introducing a learning-theoretic framework: Online\nLearning in the Replay Setting. In round $t$, the learner outputs a hypothesis\n$\\hat{h}_t$; the adversary then reveals either the true label $f^\\ast(x_t)$ or\na replayed label $\\hat{h}_i(x_t)$ from an earlier round $i < t$. A mistake is\ncounted only when the true label is shown, yet classical algorithms such as the\nSOA or the halving algorithm are easily misled by the replayed errors.\n  We introduce the Extended Threshold dimension, $\\mathrm{ExThD}(\\mathcal{H})$,\nand prove matching upper and lower bounds that make\n$\\mathrm{ExThD}(\\mathcal{H})$ the exact measure of learnability in this model.\nA closure-based learner makes at most $\\mathrm{ExThD}(\\mathcal{H})$ mistakes\nagainst any adaptive adversary, and no algorithm can perform better. For\nstochastic adversaries, we prove a similar bound for every intersection-closed\nclass. The replay setting is provably harder than the classical mistake bound\nsetting: some classes have constant Littlestone dimension but arbitrarily large\n$\\mathrm{ExThD}(\\mathcal{H})$. Proper learning exhibits an even sharper\nseparation: a class is properly learnable under replay if and only if it is\n(almost) intersection-closed. Otherwise, every proper learner suffers\n$\\Omega(T)$ errors, whereas our improper algorithm still achieves the\n$\\mathrm{ExThD}(\\mathcal{H})$ bound. These results give the first tight\nanalysis of learning against replay adversaries, based on new results for\nclosure-type algorithms."
    },
    {
        "date": "2025-09",
        "title": "MANI-Pure: Magnitude-Adaptive Noise Injection for Adversarial Purification",
        "author": "Xiaoyi Huang, Junwei Wu, Kejia Zhang, Carl Yang, and Zhiming Luo",
        "link": "http://arxiv.org/abs/2509.25082v1",
        "abstract": "Adversarial purification with diffusion models has emerged as a promising\ndefense strategy, but existing methods typically rely on uniform noise\ninjection, which indiscriminately perturbs all frequencies, corrupting semantic\nstructures and undermining robustness. Our empirical study reveals that\nadversarial perturbations are not uniformly distributed: they are predominantly\nconcentrated in high-frequency regions, with heterogeneous magnitude intensity\npatterns that vary across frequencies and attack types. Motivated by this\nobservation, we introduce MANI-Pure, a magnitude-adaptive purification\nframework that leverages the magnitude spectrum of inputs to guide the\npurification process. Instead of injecting homogeneous noise, MANI-Pure\nadaptively applies heterogeneous, frequency-targeted noise, effectively\nsuppressing adversarial perturbations in fragile high-frequency, low-magnitude\nbands while preserving semantically critical low-frequency content. Extensive\nexperiments on CIFAR-10 and ImageNet-1K validate the effectiveness of\nMANI-Pure. It narrows the clean accuracy gap to within 0.59 of the original\nclassifier, while boosting robust accuracy by 2.15, and achieves the top-1\nrobust accuracy on the RobustBench leaderboard, surpassing the previous\nstate-of-the-art method."
    },
    {
        "date": "2025-09",
        "title": "Domain-Robust Marine Plastic Detection Using Vision Models",
        "author": "Saanvi Kataria",
        "link": "http://arxiv.org/abs/2510.03294v1",
        "abstract": "Marine plastic pollution is a pressing environmental threat, making reliable\nautomation for underwater debris detection essential. However, vision systems\ntrained on one dataset often degrade on new imagery due to domain shift. This\nstudy benchmarks models for cross-domain robustness, training convolutional\nneural networks - CNNs (MobileNetV2, ResNet-18, EfficientNet-B0) and vision\ntransformers (DeiT-Tiny, ViT-B16) on a labeled underwater dataset and then\nevaluates them on a balanced cross-domain test set built from plastic-positive\nimages drawn from a different source and negatives from the training domain.\nTwo zero-shot models were assessed, CLIP ViT-L14 and Google's Gemini 2.0 Flash,\nthat leverage pretraining to classify images without fine-tuning. Results show\nthe lightweight MobileNetV2 delivers the strongest cross-domain performance (F1\n0.97), surpassing larger models. All fine-tuned models achieved high Precision\n(around 99%), but differ in Recall, indicating varying sensitivity to plastic\ninstances. Zero-shot CLIP is comparatively sensitive (Recall around 80%) yet\nprone to false positives (Precision around 56%), whereas Gemini exhibits the\ninverse profile (Precision around 99%, Recall around 81%). Error analysis\nhighlights recurring confusions with coral textures, suspended particulates,\nand specular glare. Overall, compact CNNs with supervised training can\ngeneralize effectively for cross-domain underwater detection, while large\npretrained vision-language models provide complementary strengths."
    },
    {
        "date": "2025-09",
        "title": "Fast Real-Time Pipeline for Robust Arm Gesture Recognition",
        "author": "Mil\u00e1n Zsolt Bagladi, L\u00e1szl\u00f3 Guly\u00e1s, and Gerg\u0151 Szalay",
        "link": "http://arxiv.org/abs/2509.25042v1",
        "abstract": "This paper presents a real-time pipeline for dynamic arm gesture recognition\nbased on OpenPose keypoint estimation, keypoint normalization, and a recurrent\nneural network classifier. The 1 x 1 normalization scheme and two feature\nrepresentations (coordinate- and angle-based) are presented for the pipeline.\nIn addition, an efficient method to improve robustness against camera angle\nvariations is also introduced by using artificially rotated training data.\nExperiments on a custom traffic-control gesture dataset demonstrate high\naccuracy across varying viewing angles and speeds. Finally, an approach to\ncalculate the speed of the arm signal (if necessary) is also presented."
    },
    {
        "date": "2025-09",
        "title": "SDPose: Exploiting Diffusion Priors for Out-of-Domain and Robust Pose Estimation",
        "author": "Shuang Liang, Jing He, Chuanmeizhi Wang, Lejun Liao, Guo Zhang, Yingcong Chen, and Yuan Yuan",
        "link": "http://arxiv.org/abs/2509.24980v1",
        "abstract": "Pre-trained diffusion models provide rich multi-scale latent features and are\nemerging as powerful vision backbones. While recent works such as\nMarigold~\\citep{ke2024repurposing} and Lotus~\\citep{he2024lotus} adapt\ndiffusion priors for dense prediction with strong cross-domain generalization,\ntheir potential for structured outputs (e.g., human pose estimation) remains\nunderexplored. In this paper, we propose \\textbf{SDPose}, a fine-tuning\nframework built upon Stable Diffusion to fully exploit pre-trained diffusion\npriors for human pose estimation. First, rather than modifying cross-attention\nmodules or introducing learnable embeddings, we directly predict keypoint\nheatmaps in the SD U-Net's image latent space to preserve the original\ngenerative priors. Second, we map these latent features into keypoint heatmaps\nthrough a lightweight convolutional pose head, which avoids disrupting the\npre-trained backbone. Finally, to prevent overfitting and enhance\nout-of-distribution robustness, we incorporate an auxiliary RGB reconstruction\nbranch that preserves domain-transferable generative semantics. To evaluate\nrobustness under domain shift, we further construct \\textbf{COCO-OOD}, a\nstyle-transferred variant of COCO with preserved annotations. With just\none-fifth of the training schedule used by Sapiens on COCO, SDPose attains\nparity with Sapiens-1B/2B on the COCO validation set and establishes a new\nstate of the art on the cross-domain benchmarks HumanArt and COCO-OOD.\nFurthermore, we showcase SDPose as a zero-shot pose annotator for downstream\ncontrollable generation tasks, including ControlNet-based image synthesis and\nvideo generation, where it delivers qualitatively superior pose guidance."
    },
    {
        "date": "2025-09",
        "title": "A-MemGuard: A Proactive Defense Framework for LLM-Based Agent Memory",
        "author": "Qianshan Wei, Tengchao Yang, Yaochen Wang, Xinfeng Li, Lijun Li, Zhenfei Yin, Yi Zhan, Thorsten Holz, Zhiqiang Lin, and XiaoFeng Wang",
        "link": "http://arxiv.org/abs/2510.02373v1",
        "abstract": "Large Language Model (LLM) agents use memory to learn from past interactions,\nenabling autonomous planning and decision-making in complex environments.\nHowever, this reliance on memory introduces a critical security risk: an\nadversary can inject seemingly harmless records into an agent's memory to\nmanipulate its future behavior. This vulnerability is characterized by two core\naspects: First, the malicious effect of injected records is only activated\nwithin a specific context, making them hard to detect when individual memory\nentries are audited in isolation. Second, once triggered, the manipulation can\ninitiate a self-reinforcing error cycle: the corrupted outcome is stored as\nprecedent, which not only amplifies the initial error but also progressively\nlowers the threshold for similar attacks in the future. To address these\nchallenges, we introduce A-MemGuard (Agent-Memory Guard), the first proactive\ndefense framework for LLM agent memory. The core idea of our work is the\ninsight that memory itself must become both self-checking and self-correcting.\nWithout modifying the agent's core architecture, A-MemGuard combines two\nmechanisms: (1) consensus-based validation, which detects anomalies by\ncomparing reasoning paths derived from multiple related memories and (2) a\ndual-memory structure, where detected failures are distilled into ``lessons''\nstored separately and consulted before future actions, breaking error cycles\nand enabling adaptation. Comprehensive evaluations on multiple benchmarks show\nthat A-MemGuard effectively cuts attack success rates by over 95% while\nincurring a minimal utility cost. This work shifts LLM memory security from\nstatic filtering to a proactive, experience-driven model where defenses\nstrengthen over time. Our code is available in\nhttps://github.com/TangciuYueng/AMemGuard"
    },
    {
        "date": "2025-09",
        "title": "Secret Leader Election in Ethereum PoS: An Empirical Security Analysis of Whisk and Homomorphic Sortition under DoS on the Leader and Censorship Attacks",
        "author": "Tereza Burianov\u00e1, Martin Pere\u0161\u00edni, and Ivan Homoliak",
        "link": "http://arxiv.org/abs/2509.24955v1",
        "abstract": "Proposer anonymity in Proof-of-Stake (PoS) blockchains is a critical concern\ndue to the risk of targeted attacks such as malicious denial-of-service (DoS)\nand censorship attacks. While several Secret Single Leader Election (SSLE)\nmechanisms have been proposed to address these threats, their practical impact\nand trade-offs remain insufficiently explored. In this work, we present a\nunified experimental framework for evaluating SSLE mechanisms under adversarial\nconditions, grounded in a simplified yet representative model of Ethereum's PoS\nconsensus layer. The framework includes configurable adversaries capable of\nlaunching targeted DoS and censorship attacks, including coordinated strategies\nthat simultaneously compromise groups of validators. We simulate and compare\nkey protection mechanisms - Whisk, and homomorphic sortition. To the best of\nour knowledge, this is the first comparative study to examine adversarial DoS\nscenarios involving multiple attackers under diverse protection mechanisms. Our\nresults show that while both designs offer strong protection against targeted\nDoS attacks on the leader, neither defends effectively against coordinated\nattacks on validator groups. Moreover, Whisk simplifies a DoS attack by\nnarrowing the target set from all validators to a smaller list of known\ncandidates. Homomorphic sortition, despite its theoretical strength, remains\nimpractical due to the complexity of cryptographic operations over large\nvalidator sets."
    },
    {
        "date": "2025-09",
        "title": "VAGUEGAN: Stealthy Poisoning and Backdoor Attacks on Image Generative Pipelines",
        "author": "Mostafa Mohaimen Akand Faisal, and Rabeya Amin Jhuma",
        "link": "http://arxiv.org/abs/2509.24891v1",
        "abstract": "Generative models such as GANs and diffusion models are widely used to\nsynthesize photorealistic images and to support downstream creative and editing\ntasks. While adversarial attacks on discriminative models are well studied,\nattacks targeting generative pipelines where small, stealthy perturbations in\ninputs lead to controlled changes in outputs are less explored. This study\nintroduces VagueGAN, an attack pipeline combining a modular perturbation\nnetwork PoisonerNet with a Generator Discriminator pair to craft stealthy\ntriggers that cause targeted changes in generated images. Attack efficacy is\nevaluated using a custom proxy metric, while stealth is analyzed through\nperceptual and frequency domain measures. The transferability of the method to\na modern diffusion based pipeline is further examined through ControlNet guided\nediting. Interestingly, the experiments show that poisoned outputs can display\nhigher visual quality compared to clean counterparts, challenging the\nassumption that poisoning necessarily reduces fidelity. Unlike conventional\npixel level perturbations, latent space poisoning in GANs and diffusion\npipelines can retain or even enhance output aesthetics, exposing a blind spot\nin pixel level defenses. Moreover, carefully optimized perturbations can\nproduce consistent, stealthy effects on generator outputs while remaining\nvisually inconspicuous, raising concerns for the integrity of image generation\npipelines."
    },
    {
        "date": "2025-09",
        "title": "Vision At Night: Exploring Biologically Inspired Preprocessing For Improved Robustness Via Color And Contrast Transformations",
        "author": "Lorena Stracke, Lia Nimmermann, Shashank Agnihotri, Margret Keuper, and Volker Blanz",
        "link": "http://arxiv.org/abs/2509.24863v1",
        "abstract": "Inspired by the human visual system's mechanisms for contrast enhancement and\ncolor-opponency, we explore biologically motivated input preprocessing for\nrobust semantic segmentation. By applying Difference-of-Gaussians (DoG)\nfiltering to RGB, grayscale, and opponent-color channels, we enhance local\ncontrast without modifying model architecture or training. Evaluations on\nCityscapes, ACDC, and Dark Zurich show that such preprocessing maintains\nin-distribution performance while improving robustness to adverse conditions\nlike night, fog, and snow. As this processing is model-agnostic and\nlightweight, it holds potential for integration into imaging pipelines,\nenabling imaging systems to deliver task-ready, robust inputs for downstream\nvision models in safety-critical environments."
    },
    {
        "date": "2025-09",
        "title": "Fidelity-Aware Data Composition for Robust Robot Generalization",
        "author": "Zizhao Tong, Di Chen, Sicheng Hu, Hongwei Fan, Liliang Chen, Guanghui Ren, Hao Tang, Hao Dong, and Ling Shao",
        "link": "http://arxiv.org/abs/2509.24797v1",
        "abstract": "Generalist robot policies trained on large-scale, visually homogeneous\ndatasets can be susceptible to shortcut learning, which impairs their\nout-of-distribution (OOD) generalization. While generative data augmentation is\na common approach to introduce diversity, it presents a subtle challenge: data\ncomposition. Naively mixing real and synthetic data can corrupt the learning\nsignal, as this process often prioritizes visual diversity at the expense of\ninformation fidelity. This paper suggests that robust generalization depends on\nprincipled, fidelity-aware data composition. We introduce Coherent Information\nFidelity Tuning (CIFT), a framework that treats data composition as an\noptimization problem. CIFT uses a practical proxy for Information Fidelity\nbased on the feature-space geometry of a dataset. This enables the\nidentification of a phase transition, termed the Decoherence Point, where\ntraining stability degrades. The framework includes a generative engine,\nMulti-View Video Augmentation (MVAug), to synthesize a causally disentangled\ndata spectrum for this tuning process. Applying CIFT to policy architectures\nsuch as $\\pi_0$ and Diffusion Policy improves OOD success rates by over 54\\%.\nThese results indicate that fidelity-aware composition, beyond data synthesis\nalone, is an important component for developing robust, general-purpose robots."
    },
    {
        "date": "2025-09",
        "title": "Robust Policy Expansion for Offline-to-Online RL under Diverse Data Corruption",
        "author": "Longxiang He, Deheng Ye, Junbo Tan, Xueqian Wang, and Li Shen",
        "link": "http://arxiv.org/abs/2509.24748v1",
        "abstract": "Pretraining a policy on offline data followed by fine-tuning through online\ninteractions, known as Offline-to-Online Reinforcement Learning (O2O RL), has\nemerged as a promising paradigm for real-world RL deployment. However, both\noffline datasets and online interactions in practical environments are often\nnoisy or even maliciously corrupted, severely degrading the performance of O2O\nRL. Existing works primarily focus on mitigating the conservatism of offline\npolicies via online exploration, while the robustness of O2O RL under data\ncorruption, including states, actions, rewards, and dynamics, is still\nunexplored. In this work, we observe that data corruption induces heavy-tailed\nbehavior in the policy, thereby substantially degrading the efficiency of\nonline exploration. To address this issue, we incorporate Inverse Probability\nWeighted (IPW) into the online exploration policy to alleviate\nheavy-tailedness, and propose a novel, simple yet effective method termed\n$\\textbf{RPEX}$: $\\textbf{R}$obust $\\textbf{P}$olicy $\\textbf{EX}$pansion.\nExtensive experimental results on D4RL datasets demonstrate that RPEX achieves\nSOTA O2O performance across a wide range of data corruption scenarios. Code is\navailable at\n$\\href{https://github.com/felix-thu/RPEX}{https://github.com/felix-thu/RPEX}$."
    },
    {
        "date": "2025-09",
        "title": "Circuit-Aware Reward Training: A Mechanistic Framework for Longtail Robustness in RLHF",
        "author": "Jing Liu",
        "link": "http://arxiv.org/abs/2509.24713v1",
        "abstract": "Reinforcement Learning from Human Feedback (RLHF) reward models exhibit\nsystematic failures on longtail distributions, leading to reward hacking and\nmisalignment. We propose a mechanistic interpretability framework that\nidentifies specialized neural circuits responsible for rare-event processing in\nreward models. Drawing from recent advances showing distributed specialization\nfor rare tokens in language models\\citep{liu2025no, liu2025emergent}, we\nhypothesize that reward models also develop functionally distinct circuits for\nlongtail scenarios. Our theoretical framework establishes formal connections\nbetween circuit specialization, reward generalization bounds, and longtail\nperformance. We introduce \\textbf{Circuit-Aware Reward Training (CART)}, which\nuses circuit analysis to guide data augmentation, regularization, and ensemble\nstrategies. This approach provides both theoretical insights into reward model\nfailures and practical interventions for improving longtail robustness."
    },
    {
        "date": "2025-09",
        "title": "Community detection robustness of graph neural networks",
        "author": "Jaidev Goel, Pablo Moriano, Ramakrishnan Kannan, and Yulia R. Gel",
        "link": "http://arxiv.org/abs/2509.24662v1",
        "abstract": "Graph neural networks (GNNs) are increasingly widely used for community\ndetection in attributed networks. They combine structural topology with node\nattributes through message passing and pooling. However, their robustness or\nlack of thereof with respect to different perturbations and targeted attacks in\nconjunction with community detection tasks is not well understood. To shed\nlight into latent mechanisms behind GNN sensitivity on community detection\ntasks, we conduct a systematic computational evaluation of six widely adopted\nGNN architectures: GCN, GAT, Graph-SAGE, DiffPool, MinCUT, and DMoN. The\nanalysis covers three perturbation categories: node attribute manipulations,\nedge topology distortions, and adversarial attacks. We use element-centric\nsimilarity as the evaluation metric on synthetic benchmarks and real-world\ncitation networks. Our findings indicate that supervised GNNs tend to achieve\nhigher baseline accuracy, while unsupervised methods, particularly DMoN,\nmaintain stronger resilience under targeted and adversarial perturbations.\nFurthermore, robustness appears to be strongly influenced by community\nstrength, with well-defined communities reducing performance loss. Across all\nmodels, node attribute perturbations associated with targeted edge deletions\nand shift in attribute distributions tend to cause the largest degradation in\ncommunity recovery. These findings highlight important trade-offs between\naccuracy and robustness in GNN-based community detection and offer new insights\ninto selecting architectures resilient to noise and adversarial attacks."
    },
    {
        "date": "2025-09",
        "title": "TokenSwap: Backdoor Attack on the Compositional Understanding of Large Vision-Language Models",
        "author": "Zhifang Zhang, Qiqi Tao, Jiaqi Lv, Na Zhao, Lei Feng, and Joey Tianyi Zhou",
        "link": "http://arxiv.org/abs/2509.24566v1",
        "abstract": "Large vision-language models (LVLMs) have achieved impressive performance\nacross a wide range of vision-language tasks, while they remain vulnerable to\nbackdoor attacks. Existing backdoor attacks on LVLMs aim to force the victim\nmodel to generate a predefined target pattern, which is either inserted into or\nreplaces the original content. We find that these fixed-pattern attacks are\nrelatively easy to detect, because the attacked LVLM tends to memorize such\nfrequent patterns in the training dataset, thereby exhibiting overconfidence on\nthese targets given poisoned inputs. To address these limitations, we introduce\nTokenSwap, a more evasive and stealthy backdoor attack that focuses on the\ncompositional understanding capabilities of LVLMs. Instead of enforcing a fixed\ntargeted content, TokenSwap subtly disrupts the understanding of object\nrelationships in text. Specifically, it causes the backdoored model to generate\noutputs that mention the correct objects in the image but misrepresent their\nrelationships (i.e., bags-of-words behavior). During training, TokenSwap\ninjects a visual trigger into selected samples and simultaneously swaps the\ngrammatical roles of key tokens in the corresponding textual answers. However,\nthe poisoned samples exhibit only subtle differences from the original ones,\nmaking it challenging for the model to learn the backdoor behavior. To address\nthis, TokenSwap employs an adaptive token-weighted loss that explicitly\nemphasizes the learning of swapped tokens, such that the visual triggers and\nbags-of-words behavior are associated. Extensive experiments demonstrate that\nTokenSwap achieves high attack success rates while maintaining superior\nevasiveness and stealthiness across multiple benchmarks and various LVLM\narchitectures."
    },
    {
        "date": "2025-09",
        "title": "LEAF: A Robust Expert-Based Framework for Few-Shot Continual Event Detection",
        "author": "Bao-Ngoc Dao, Quang Nguyen, Luyen Ngo Dinh, Minh Le, and Linh Ngo Van",
        "link": "http://arxiv.org/abs/2509.24547v1",
        "abstract": "Few-shot Continual Event Detection (FCED) poses the dual challenges of\nlearning from limited data and mitigating catastrophic forgetting across\nsequential tasks. Existing approaches often suffer from severe forgetting due\nto the full fine-tuning of a shared base model, which leads to knowledge\ninterference between tasks. Moreover, they frequently rely on data augmentation\nstrategies that can introduce unnatural or semantically distorted inputs. To\naddress these limitations, we propose LEAF, a novel and robust expert-based\nframework for FCED. LEAF integrates a specialized mixture of experts\narchitecture into the base model, where each expert is parameterized with\nlow-rank adaptation (LoRA) matrices. A semantic-aware expert selection\nmechanism dynamically routes instances to the most relevant experts, enabling\nexpert specialization and reducing knowledge interference. To improve\ngeneralization in limited-data settings, LEAF incorporates a contrastive\nlearning objective guided by label descriptions, which capture high-level\nsemantic information about event types. Furthermore, to prevent overfitting on\nthe memory buffer, our framework employs a knowledge distillation strategy that\ntransfers knowledge from previous models to the current one. Extensive\nexperiments on multiple FCED benchmarks demonstrate that LEAF consistently\nachieves state-of-the-art performance."
    },
    {
        "date": "2025-09",
        "title": "Robust Multimodal Semantic Segmentation with Balanced Modality Contributions",
        "author": "Jiaqi Tan, Xu Zheng, Fangyu Li, and Yang Liu",
        "link": "http://arxiv.org/abs/2509.24505v1",
        "abstract": "Multimodal semantic segmentation enhances model robustness by exploiting\ncross-modal complementarities. However, existing methods often suffer from\nimbalanced modal dependencies, where overall performance degrades significantly\nonce a dominant modality deteriorates in real-world scenarios. Thus, modality\nbalance has become acritical challenge for practical multimodal segmentation.\nTo address this issue, we propose EQUISeg, a multimodal segmentation framework\nthat balances modality contributions through equal encoding of modalities.\nBuilt upon a four-stage Cross-modal Transformer Block(CMTB), EQUISeg enables\nefficient multimodal fusion and hierarchical selection. Furthermore, we design\na Self-guided Module(SGM) that mitigates modality imbalance by introducing a\nmutual guidance mechanism, enabling each modality to adaptively adjust its\ncontribution and enhance robustness under degraded conditions. Extensive\nexperiments on multiple datasets demonstrate that EQUISeg achieves significant\nperformance gains and effectively alleviates the adverse effects of modality\nimbalance in segmentation tasks."
    },
    {
        "date": "2025-09",
        "title": "Federated Spatiotemporal Graph Learning for Passive Attack Detection in Smart Grids",
        "author": "Bochra Al Agha, and Razane Tajeddine",
        "link": "http://arxiv.org/abs/2510.02371v1",
        "abstract": "Smart grids are exposed to passive eavesdropping, where attackers listen\nsilently to communication links. Although no data is actively altered, such\nreconnaissance can reveal grid topology, consumption patterns, and operational\nbehavior, creating a gateway to more severe targeted attacks. Detecting this\nthreat is difficult because the signals it produces are faint, short-lived, and\noften disappear when traffic is examined by a single node or along a single\ntimeline. This paper introduces a graph-centric, multimodal detector that fuses\nphysical-layer and behavioral indicators over ego-centric star subgraphs and\nshort temporal windows to detect passive attacks. To capture stealthy\nperturbations, a two-stage encoder is introduced: graph convolution aggregates\nspatial context across ego-centric star subgraphs, while a bidirectional GRU\nmodels short-term temporal dependencies. The encoder transforms heterogeneous\nfeatures into a unified spatio-temporal representation suitable for\nclassification. Training occurs in a federated learning setup under FedProx,\nimproving robustness to heterogeneous local raw data and contributing to the\ntrustworthiness of decentralized training; raw measurements remain on client\ndevices. A synthetic, standards-informed dataset is generated to emulate\nheterogeneous HAN/NAN/WAN communications with wireless-only passive\nperturbations, event co-occurrence, and leak-safe splits. The model achieves a\ntesting accuracy of 98.32% per-timestep (F1_{attack}=0.972) and 93.35%\nper-sequence at 0.15% FPR using a simple decision rule with run-length m=2 and\nthreshold $\\tau=0.55$. The results demonstrate that combining spatial and\ntemporal context enables reliable detection of stealthy reconnaissance while\nmaintaining low false-positive rates, making the approach suitable for non-IID\nfederated smart-grid deployments."
    },
    {
        "date": "2025-09",
        "title": "Distributionally Robust Federated Learning with Outlier Resilience",
        "author": "Zifan Wang, Xinlei Yi, Xenia Konti, Michael M. Zavlanos, and Karl H. Johansson",
        "link": "http://arxiv.org/abs/2509.24462v1",
        "abstract": "Federated learning (FL) enables collaborative model training without direct\ndata sharing, but its performance can degrade significantly in the presence of\ndata distribution perturbations. Distributionally robust optimization (DRO)\nprovides a principled framework for handling this by optimizing performance\nagainst the worst-case distributions within a prescribed ambiguity set.\nHowever, existing DRO-based FL methods often overlook the detrimental impact of\noutliers in local datasets, which can disproportionately bias the learned\nmodels. In this work, we study distributionally robust federated learning with\nexplicit outlier resilience. We introduce a novel ambiguity set based on the\nunbalanced Wasserstein distance, which jointly captures geometric\ndistributional shifts and incorporates a non-geometric Kullback--Leibler\npenalization to mitigate the influence of outliers. This formulation naturally\nleads to a challenging min--max--max optimization problem. To enable\ndecentralized training, we reformulate the problem as a tractable Lagrangian\npenalty optimization, which admits robustness certificates. Building on this\nreformulation, we propose the distributionally outlier-robust federated\nlearning algorithm and establish its convergence guarantees. Extensive\nexperiments on both synthetic and real-world datasets demonstrate the\neffectiveness of our approach."
    },
    {
        "date": "2025-09",
        "title": "DRIFT: Divergent Response in Filtered Transformations for Robust Adversarial Defense",
        "author": "Amira Guesmi, and Muhammad Shafique",
        "link": "http://arxiv.org/abs/2509.24359v1",
        "abstract": "Deep neural networks remain highly vulnerable to adversarial examples, and\nmost defenses collapse once gradients can be reliably estimated. We identify\n\\emph{gradient consensus} -- the tendency of randomized transformations to\nyield aligned gradients -- as a key driver of adversarial transferability.\nAttackers exploit this consensus to construct perturbations that remain\neffective across transformations. We introduce \\textbf{DRIFT} (Divergent\nResponse in Filtered Transformations), a stochastic ensemble of lightweight,\nlearnable filters trained to actively disrupt gradient consensus. Unlike prior\nrandomized defenses that rely on gradient masking, DRIFT enforces\n\\emph{gradient dissonance} by maximizing divergence in Jacobian- and\nlogit-space responses while preserving natural predictions. Our contributions\nare threefold: (i) we formalize gradient consensus and provide a theoretical\nanalysis linking consensus to transferability; (ii) we propose a\nconsensus-divergence training strategy combining prediction consistency,\nJacobian separation, logit-space separation, and adversarial robustness; and\n(iii) we show that DRIFT achieves substantial robustness gains on ImageNet\nacross CNNs and Vision Transformers, outperforming state-of-the-art\npreprocessing, adversarial training, and diffusion-based defenses under\nadaptive white-box, transfer-based, and gradient-free attacks. DRIFT delivers\nthese improvements with negligible runtime and memory cost, establishing\ngradient divergence as a practical and generalizable principle for adversarial\ndefense."
    },
    {
        "date": "2025-09",
        "title": "OMeGa: Joint Optimization of Explicit Meshes and Gaussian Splats for Robust Scene-Level Surface Reconstruction",
        "author": "Yuhang Cao, Haojun Yan, and Danya Yao",
        "link": "http://arxiv.org/abs/2509.24308v1",
        "abstract": "Neural rendering with Gaussian splatting has advanced novel view synthesis,\nand most methods reconstruct surfaces via post-hoc mesh extraction. However,\nexisting methods suffer from two limitations: (i) inaccurate geometry in\ntexture-less indoor regions, and (ii) the decoupling of mesh extraction from\noptimization, thereby missing the opportunity to leverage mesh geometry to\nguide splat optimization. In this paper, we present OMeGa, an end-to-end\nframework that jointly optimizes an explicit triangle mesh and 2D Gaussian\nsplats via a flexible binding strategy, where spatial attributes of Gaussian\nSplats are expressed in the mesh frame and texture attributes are retained on\nsplats. To further improve reconstruction accuracy, we integrate mesh\nconstraints and monocular normal supervision into the optimization, thereby\nregularizing geometry learning. In addition, we propose a heuristic, iterative\nmesh-refinement strategy that splits high-error faces and prunes unreliable\nones to further improve the detail and accuracy of the reconstructed mesh.\nOMeGa achieves state-of-the-art performance on challenging indoor\nreconstruction benchmarks, reducing Chamfer-$L_1$ by 47.3\\% over the 2DGS\nbaseline while maintaining competitive novel-view rendering quality. The\nexperimental results demonstrate that OMeGa effectively addresses prior\nlimitations in indoor texture-less reconstruction."
    },
    {
        "date": "2025-09",
        "title": "Robust Partial 3D Point Cloud Registration via Confidence Estimation under Global Context",
        "author": "Yongqiang Wang, Weigang Li, Wenping Liu, Zhe Xu, and Zhiqiang Tian",
        "link": "http://arxiv.org/abs/2509.24275v1",
        "abstract": "Partial point cloud registration is essential for autonomous perception and\n3D scene understanding, yet it remains challenging owing to structural\nambiguity, partial visibility, and noise. We address these issues by proposing\nConfidence Estimation under Global Context (CEGC), a unified, confidence-driven\nframework for robust partial 3D registration. CEGC enables accurate alignment\nin complex scenes by jointly modeling overlap confidence and correspondence\nreliability within a shared global context. Specifically, the hybrid overlap\nconfidence estimation module integrates semantic descriptors and geometric\nsimilarity to detect overlapping regions and suppress outliers early. The\ncontext-aware matching strategy smitigates ambiguity by employing global\nattention to assign soft confidence scores to correspondences, improving\nrobustness. These scores guide a differentiable weighted singular value\ndecomposition solver to compute precise transformations. This tightly coupled\npipeline adaptively down-weights uncertain regions and emphasizes contextually\nreliable matches. Experiments on ModelNet40, ScanObjectNN, and 7Scenes 3D\nvision datasets demonstrate that CEGC outperforms state-of-the-art methods in\naccuracy, robustness, and generalization. Overall, CEGC offers an interpretable\nand scalable solution to partial point cloud registration under challenging\nconditions."
    },
    {
        "date": "2025-09",
        "title": "Adversarial Reinforcement Learning Framework for ESP Cheater Simulation",
        "author": "Inkyu Park, Jeong-Gwan Lee, Taehwan Kwon, Juheon Choi, Seungku Kim, Junsu Kim, and Kimin Lee",
        "link": "http://arxiv.org/abs/2509.24274v1",
        "abstract": "Extra-Sensory Perception (ESP) cheats, which reveal hidden in-game\ninformation such as enemy locations, are difficult to detect because their\neffects are not directly observable in player behavior. The lack of observable\nevidence makes it difficult to collect reliably labeled data, which is\nessential for training effective anti-cheat systems. Furthermore, cheaters\noften adapt their behavior by limiting or disguising their cheat usage, which\nfurther complicates detection and detector development. To address these\nchallenges, we propose a simulation framework for controlled modeling of ESP\ncheaters, non-cheaters, and trajectory-based detectors. We model cheaters and\nnon-cheaters as reinforcement learning agents with different levels of\nobservability, while detectors classify their behavioral trajectories. Next, we\nformulate the interaction between the cheater and the detector as an\nadversarial game, allowing both players to co-adapt over time. To reflect\nrealistic cheater strategies, we introduce a structured cheater model that\ndynamically switches between cheating and non-cheating behaviors based on\ndetection risk. Experiments demonstrate that our framework successfully\nsimulates adaptive cheater behaviors that strategically balance reward\noptimization and detection evasion. This work provides a controllable and\nextensible platform for studying adaptive cheating behaviors and developing\neffective cheat detectors."
    },
    {
        "date": "2025-09",
        "title": "Skeleton-based Robust Registration Framework for Corrupted 3D Point Clouds",
        "author": "Yongqiang Wang, Weigang Li, Wenping Liu, Zhiqiang Tian, and Jinling Li",
        "link": "http://arxiv.org/abs/2509.24273v1",
        "abstract": "Point cloud registration is fundamental in 3D vision applications, including\nautonomous driving, robotics, and medical imaging, where precise alignment of\nmultiple point clouds is essential for accurate environment reconstruction.\nHowever, real-world point clouds are often affected by sensor limitations,\nenvironmental noise, and preprocessing errors, making registration challenging\ndue to density distortions, noise contamination, and geometric deformations.\nExisting registration methods rely on direct point matching or surface feature\nextraction, which are highly susceptible to these corruptions and lead to\nreduced alignment accuracy. To address these challenges, a skeleton-based\nrobust registration framework is presented, which introduces a\ncorruption-resilient skeletal representation to improve registration robustness\nand accuracy. The framework integrates skeletal structures into the\nregistration process and combines the transformations obtained from both the\ncorrupted point cloud alignment and its skeleton alignment to achieve optimal\nregistration. In addition, a distribution distance loss function is designed to\nenforce the consistency between the source and target skeletons, which\nsignificantly improves the registration performance. This framework ensures\nthat the alignment considers both the original local geometric features and the\nglobal stability of the skeleton structure, resulting in robust and accurate\nregistration results. Experimental evaluations on diverse corrupted datasets\ndemonstrate that SRRF consistently outperforms state-of-the-art registration\nmethods across various corruption scenarios, including density distortions,\nnoise contamination, and geometric deformations. The results confirm the\nrobustness of SRRF in handling corrupted point clouds, making it a potential\napproach for 3D perception tasks in real-world scenarios."
    },
    {
        "date": "2025-09",
        "title": "When MCP Servers Attack: Taxonomy, Feasibility, and Mitigation",
        "author": "Weibo Zhao, Jiahao Liu, Bonan Ruan, Shaofei Li, and Zhenkai Liang",
        "link": "http://arxiv.org/abs/2509.24272v1",
        "abstract": "Model Context Protocol (MCP) servers enable AI applications to connect to\nexternal systems in a plug-and-play manner, but their rapid proliferation also\nintroduces severe security risks. Unlike mature software ecosystems with\nrigorous vetting, MCP servers still lack standardized review mechanisms, giving\nadversaries opportunities to distribute malicious implementations. Despite this\npressing risk, the security implications of MCP servers remain underexplored.\nTo address this gap, we present the first systematic study that treats MCP\nservers as active threat actors and decomposes them into core components to\nexamine how adversarial developers can implant malicious intent. Specifically,\nwe investigate three research questions: (i) what types of attacks malicious\nMCP servers can launch, (ii) how vulnerable MCP hosts and Large Language Models\n(LLMs) are to these attacks, and (iii) how feasible it is to carry out MCP\nserver attacks in practice. Our study proposes a component-based taxonomy\ncomprising twelve attack categories. For each category, we develop\nProof-of-Concept (PoC) servers and demonstrate their effectiveness across\ndiverse real-world host-LLM settings. We further show that attackers can\ngenerate large numbers of malicious servers at virtually no cost. We then test\nstate-of-the-art scanners on the generated servers and found that existing\ndetection approaches are insufficient. These findings highlight that malicious\nMCP servers are easy to implement, difficult to detect with current tools, and\ncapable of causing concrete damage to AI agent systems. Addressing this threat\nrequires coordinated efforts among protocol designers, host developers, LLM\nproviders, and end users to build a more secure and resilient MCP ecosystem."
    },
    {
        "date": "2025-09",
        "title": "AdvChain: Adversarial Chain-of-Thought Tuning for Robust Safety Alignment of Large Reasoning Models",
        "author": "Zihao Zhu, Xinyu Wu, Gehan Hu, Siwei Lyu, Ke Xu, and Baoyuan Wu",
        "link": "http://arxiv.org/abs/2509.24269v1",
        "abstract": "Large Reasoning Models (LRMs) have demonstrated remarkable capabilities in\ncomplex problem-solving through Chain-of-Thought (CoT) reasoning. However, the\nmulti-step nature of CoT introduces new safety challenges that extend beyond\nconventional language model alignment. We identify a failure mode in current\nsafety CoT tuning methods: the \\textit{snowball effect}, where minor reasoning\ndeviations progressively amplify throughout the thought process, leading to\neither harmful compliance or excessive refusal. This effect stems from models\nbeing trained to imitate perfect reasoning scripts without learning to\nself-correct. To address this limitation, we propose AdvChain, an alignment\nparadigm that teaches models dynamic self-correction through adversarial CoT\ntuning. Our method involves constructing a dataset containing\nTemptation-Correction and Hesitation-Correction samples, where models learn to\nrecover from harmful reasoning drifts and unnecessary cautions. Extensive\nexperiments show that AdvChain significantly enhances robustness against\njailbreak attacks and CoT hijacking while substantially reducing over-refusal\non benign prompts, achieving a superior safety-utility balance without\ncompromising reasoning capabilities. Our work establishes a new direction for\nbuilding more robust and reliable reasoning models."
    },
    {
        "date": "2025-09",
        "title": "PROFusion: Robust and Accurate Dense Reconstruction via Camera Pose Regression and Optimization",
        "author": "Siyan Dong, Zijun Wang, Lulu Cai, Yi Ma, and Yanchao Yang",
        "link": "http://arxiv.org/abs/2509.24236v1",
        "abstract": "Real-time dense scene reconstruction during unstable camera motions is\ncrucial for robotics, yet current RGB-D SLAM systems fail when cameras\nexperience large viewpoint changes, fast motions, or sudden shaking. Classical\noptimization-based methods deliver high accuracy but fail with poor\ninitialization during large motions, while learning-based approaches provide\nrobustness but lack sufficient accuracy for dense reconstruction. We address\nthis challenge through a combination of learning-based initialization with\noptimization-based refinement. Our method employs a camera pose regression\nnetwork to predict metric-aware relative poses from consecutive RGB-D frames,\nwhich serve as reliable starting points for a randomized optimization algorithm\nthat further aligns depth images with the scene geometry. Extensive experiments\ndemonstrate promising results: our approach outperforms the best competitor on\nchallenging benchmarks, while maintaining comparable accuracy on stable motion\nsequences. The system operates in real-time, showcasing that combining simple\nand principled techniques can achieve both robustness for unstable motions and\naccuracy for dense reconstruction. Project page:\nhttps://github.com/siyandong/PROFusion."
    },
    {
        "date": "2025-09",
        "title": "MAESTRO : Adaptive Sparse Attention and Robust Learning for Multimodal Dynamic Time Series",
        "author": "Payal Mohapatra, Yueyuan Sui, Akash Pandey, Stephen Xia, and Qi Zhu",
        "link": "http://arxiv.org/abs/2509.25278v1",
        "abstract": "From clinical healthcare to daily living, continuous sensor monitoring across\nmultiple modalities has shown great promise for real-world intelligent\ndecision-making but also faces various challenges. In this work, we introduce\nMAESTRO, a novel framework that overcomes key limitations of existing\nmultimodal learning approaches: (1) reliance on a single primary modality for\nalignment, (2) pairwise modeling of modalities, and (3) assumption of complete\nmodality observations. These limitations hinder the applicability of these\napproaches in real-world multimodal time-series settings, where primary\nmodality priors are often unclear, the number of modalities can be large\n(making pairwise modeling impractical), and sensor failures often result in\narbitrary missing observations. At its core, MAESTRO facilitates dynamic intra-\nand cross-modal interactions based on task relevance, and leverages symbolic\ntokenization and adaptive attention budgeting to construct long multimodal\nsequences, which are processed via sparse cross-modal attention. The resulting\ncross-modal tokens are routed through a sparse Mixture-of-Experts (MoE)\nmechanism, enabling black-box specialization under varying modality\ncombinations. We evaluate MAESTRO against 10 baselines on four diverse datasets\nspanning three applications, and observe average relative improvements of 4%\nand 8% over the best existing multimodal and multivariate approaches,\nrespectively, under complete observations. Under partial observations -- with\nup to 40% of missing modalities -- MAESTRO achieves an average 9% improvement.\nFurther analysis also demonstrates the robustness and efficiency of MAESTRO's\nsparse, modality-aware design for learning from dynamic time series."
    },
    {
        "date": "2025-09",
        "title": "Latent Collective Preference Optimization: A General Framework for Robust LLM Alignment",
        "author": "Xiaoyang Cao, Zelai Xu, Mo Guang, Kaiwen Long, Michiel A. Bakker, Yu Wang, and Chao Yu",
        "link": "http://arxiv.org/abs/2509.24159v2",
        "abstract": "Standard human preference-based alignment methods, such as Reinforcement\nLearning from Human Feedback (RLHF), are a cornerstone technology for aligning\nLarge Language Models (LLMs) with human values. However, these methods are all\nunderpinned by a critical, yet flawed assumption: human preferences are\nhomogeneous (representing a single, unified preference) and the collected data\nis noiseless (free from error). In reality, neither is true since human\npreference is pluralistic and annotators can make mistakes. This creates a\ndiscrepancy between the recorded data and the ground-truth preferences, which\ncan misguide the model and degrade its performance. To address this challenge,\nwe introduce Latent Collective Preference Optimization (LCPO). LCPO leverages\nan Expectation-Maximization (EM) algorithm to learn the latent collective\nconsensus from noisy data. It operates by inferring the correctness of each\npreference label and using this probability as an adaptive weight to\nre-calibrate each data point's contribution to the training loss, thereby\nmitigating noise. We generalize this approach by establishing a theoretical\nlink between arbitrary preference losses and their corresponding probabilistic\nmodels, elevating LCPO from a specific algorithm to a general framework for\nrobust preference alignment. Theoretically, we prove that under the condition\nof a perfectly calibrated model, LCPO is guaranteed to converge to the true\nnoise level of the dataset. Our experiments demonstrate LCPO's effectiveness as\na general framework, consistently enhancing four state-of-the-art alignment\nalgorithms (DPO, IPO, SimPO, and CPO). When applied to Mistral and Llama 3\nmodels, the LCPO-enhanced methods achieve substantial win rate gains on\nAlpacaEval 2 and Arena-Hard, with improvements of up to 7.0% on both\nbenchmarks."
    },
    {
        "date": "2025-09",
        "title": "Generalist Scanner Meets Specialist Locator: A Synergistic Coarse-to-Fine Framework for Robust GUI Grounding",
        "author": "Zhecheng Li, Guoxian Song, Yiwei Wang, Zhen Xiong, Junsong Yuan, and Yujun Cai",
        "link": "http://arxiv.org/abs/2509.24133v1",
        "abstract": "Grounding natural language queries in graphical user interfaces (GUIs)\npresents a challenging task that requires models to comprehend diverse UI\nelements across various applications and systems, while also accurately\npredicting the spatial coordinates for the intended operation. To tackle this\nproblem, we propose GMS: Generalist Scanner Meets Specialist Locator, a\nsynergistic coarse-to-fine framework that effectively improves GUI grounding\nperformance. GMS leverages the complementary strengths of general\nvision-language models (VLMs) and small, task-specific GUI grounding models by\nassigning them distinct roles within the framework. Specifically, the general\nVLM acts as a 'Scanner' to identify potential regions of interest, while the\nfine-tuned grounding model serves as a 'Locator' that outputs precise\ncoordinates within these regions. This design is inspired by how humans perform\nGUI grounding, where the eyes scan the interface and the brain focuses on\ninterpretation and localization. Our whole framework consists of five stages\nand incorporates hierarchical search with cross-modal communication to achieve\npromising prediction results. Experimental results on the ScreenSpot-Pro\ndataset show that while the 'Scanner' and 'Locator' models achieve only $2.0\\%$\nand $3.7\\%$ accuracy respectively when used independently, their integration\nwithin GMS framework yields an overall accuracy of $35.7\\%$, representing a $10\n\\times$ improvement. Additionally, GMS significantly outperforms other strong\nbaselines under various settings, demonstrating its robustness and potential\nfor general-purpose GUI grounding."
    },
    {
        "date": "2025-09",
        "title": "RPG360: Robust 360 Depth Estimation with Perspective Foundation Models and Graph Optimization",
        "author": "Dongki Jung, Jaehoon Choi, Yonghan Lee, and Dinesh Manocha",
        "link": "http://arxiv.org/abs/2509.23991v1",
        "abstract": "The increasing use of 360 images across various domains has emphasized the\nneed for robust depth estimation techniques tailored for omnidirectional\nimages. However, obtaining large-scale labeled datasets for 360 depth\nestimation remains a significant challenge. In this paper, we propose RPG360, a\ntraining-free robust 360 monocular depth estimation method that leverages\nperspective foundation models and graph optimization. Our approach converts 360\nimages into six-face cubemap representations, where a perspective foundation\nmodel is employed to estimate depth and surface normals. To address depth scale\ninconsistencies across different faces of the cubemap, we introduce a novel\ndepth scale alignment technique using graph-based optimization, which\nparameterizes the predicted depth and normal maps while incorporating an\nadditional per-face scale parameter. This optimization ensures depth scale\nconsistency across the six-face cubemap while preserving 3D structural\nintegrity. Furthermore, as foundation models exhibit inherent robustness in\nzero-shot settings, our method achieves superior performance across diverse\ndatasets, including Matterport3D, Stanford2D3D, and 360Loc. We also demonstrate\nthe versatility of our depth estimation approach by validating its benefits in\ndownstream tasks such as feature matching 3.2 ~ 5.4% and Structure from Motion\n0.2 ~ 9.7% in AUC@5."
    },
    {
        "date": "2025-09",
        "title": "Evaluating the Robustness of Chinchilla Compute-Optimal Scaling",
        "author": "Rylan Schaeffer, Noam Levi, Andreas Kirsch, Theo Guenais, Brando Miranda, Elyas Obbad, and Sanmi Koyejo",
        "link": "http://arxiv.org/abs/2509.23963v1",
        "abstract": "Hoffman et al (2022)'s Chinchilla paper introduced the principle of\ncompute-optimal scaling, laying a foundation for future scaling of language\nmodels. In the years since, however, valid concerns about Chinchilla have been\nraised: wide confidence intervals, discrepancies between its three approaches,\nand incongruities with other scaling laws. This raises a critical question for\nthe field: Can practitioners still rely on Chinchilla's prescriptions? Our work\ndemonstrates the answer is yes. We begin by uncovering that the model\nparameters central to Chinchilla's analyses were ambiguous: three\ninterpretations are possible, with relative differences between different\ninterpretations of model parameters as high as 15.2%. We find that, perhaps\nsurprisingly, which model parameters are used for the analyses do not\nmeaningfully affect key results: the scaling law estimates and the\ncompute-optimal tokens-to-parameter ratio. Indeed, under one interpretation,\nthe tokens-to-parameter ratio becomes more constant with the target compute\nbudget. We then ask how distorted the Chinchilla model parameters could have\nbeen without meaningfully affecting the key results. By deliberately perturbing\nmodel parameters in four structured ways, we find that key Chinchilla results\nare most sensitive to additive or systematic errors, which can alter the\notherwise flat trend of the optimal tokens-to-parameter ratio, but overall,\nChinchilla's key results withstand sizable perturbations. Altogether, our\nfindings offer the field renewed confidence in Chinchilla as a durable guide\nfor scaling language models."
    },
    {
        "date": "2025-09",
        "title": "Learning-Based Testing for Deep Learning: Enhancing Model Robustness with Adversarial Input Prioritization",
        "author": "Sheikh Md Mushfiqur Rahman, and Nasir Eisty",
        "link": "http://arxiv.org/abs/2509.23961v1",
        "abstract": "Context: Deep Neural Networks (DNNs) are increasingly deployed in critical\napplications, where resilience against adversarial inputs is paramount.\nHowever, whether coverage-based or confidence-based, existing test\nprioritization methods often fail to efficiently identify the most\nfault-revealing inputs, limiting their practical effectiveness. Aims: This\nproject aims to enhance fault detection and model robustness in DNNs by\nintegrating Learning-Based Testing (LBT) with hypothesis and mutation testing\nto efficiently prioritize adversarial test cases. Methods: Our method selects a\nsubset of adversarial inputs with a high likelihood of exposing model faults,\nwithout relying on architecture-specific characteristics or formal\nverification, making it adaptable across diverse DNNs. Results: Our results\ndemonstrate that the proposed LBT method consistently surpasses baseline\napproaches in prioritizing fault-revealing inputs and accelerating fault\ndetection. By efficiently organizing test permutations, it uncovers all\npotential faults significantly faster across various datasets, model\narchitectures, and adversarial attack techniques. Conclusion: Beyond improving\nfault detection, our method preserves input diversity and provides effective\nguidance for model retraining, further enhancing robustness. These advantages\nestablish our approach as a powerful and practical solution for adversarial\ntest prioritization in real-world DNN applications."
    },
    {
        "date": "2025-09",
        "title": "Easy Turn: Integrating Acoustic and Linguistic Modalities for Robust Turn-Taking in Full-Duplex Spoken Dialogue Systems",
        "author": "Guojian Li, Chengyou Wang, Hongfei Xue, Shuiyuan Wang, Dehui Gao, Zihan Zhang, Yuke Lin, Wenjie Li, Longshuai Xiao, Zhonghua Fu, and Lei Xie",
        "link": "http://arxiv.org/abs/2509.23938v1",
        "abstract": "Full-duplex interaction is crucial for natural human-machine communication,\nyet remains challenging as it requires robust turn-taking detection to decide\nwhen the system should speak, listen, or remain silent. Existing solutions\neither rely on dedicated turn-taking models, most of which are not\nopen-sourced. The few available ones are limited by their large parameter size\nor by supporting only a single modality, such as acoustic or linguistic.\nAlternatively, some approaches finetune LLM backbones to enable full-duplex\ncapability, but this requires large amounts of full-duplex data, which remain\nscarce in open-source form. To address these issues, we propose Easy Turn, an\nopen-source, modular turn-taking detection model that integrates acoustic and\nlinguistic bimodal information to predict four dialogue turn states: complete,\nincomplete, backchannel, and wait, accompanied by the release of Easy Turn\ntrainset, a 1,145-hour speech dataset designed for training turn-taking\ndetection models. Compared to existing open-source models like TEN Turn\nDetection and Smart Turn V2, our model achieves state-of-the-art turn-taking\ndetection accuracy on our open-source Easy Turn testset. The data and model\nwill be made publicly available on GitHub."
    },
    {
        "date": "2025-09",
        "title": "Bridging the Task Gap: Multi-Task Adversarial Transferability in CLIP and Its Derivatives",
        "author": "Kuanrong Liu, Siyuan Liang, Cheng Qian, Ming Zhang, and Xiaochun Cao",
        "link": "http://arxiv.org/abs/2509.23917v1",
        "abstract": "As a general-purpose vision-language pretraining model, CLIP demonstrates\nstrong generalization ability in image-text alignment tasks and has been widely\nadopted in downstream applications such as image classification and image-text\nretrieval. However, it struggles with fine-grained tasks such as object\ndetection and semantic segmentation. While many variants aim to improve CLIP on\nthese tasks, its robustness to adversarial perturbations remains underexplored.\nUnderstanding how adversarial examples transfer across tasks is key to\nassessing CLIP's generalization limits and security risks. In this work, we\nconduct a systematic empirical analysis of the cross-task transfer behavior of\nCLIP-based models on image-text retrieval, object detection, and semantic\nsegmentation under adversarial perturbations. We find that adversarial examples\ngenerated from fine-grained tasks (e.g., object detection and semantic\nsegmentation) often exhibit stronger transfer potential than those from\ncoarse-grained tasks, enabling more effective attacks against the original CLIP\nmodel. Motivated by this observation, we propose a novel framework, Multi-Task\nAdversarial CLIP (MT-AdvCLIP), which introduces a task-aware feature\naggregation loss and generates perturbations with enhanced cross-task\ngeneralization capability. This design strengthens the attack effectiveness of\nfine-grained task models on the shared CLIP backbone. Experimental results on\nmultiple public datasets show that MT-AdvCLIP significantly improves the\nadversarial transfer success rate (The average attack success rate across\nmultiple tasks is improved by over 39%.) against various CLIP-derived models,\nwithout increasing the perturbation budget. This study reveals the transfer\nmechanism of adversarial examples in multi-task CLIP models, offering new\ninsights into multi-task robustness evaluation and adversarial example design."
    },
    {
        "date": "2025-09",
        "title": "Adversarial Versus Federated: An Adversarial Learning based Multi-Modality Cross-Domain Federated Medical Segmentation",
        "author": "You Zhou, Lijiang Chen, Shuchang Lyu, Guangxia Cui, Wenpei Bai, Zheng Zhou, Meng Li, Guangliang Cheng, Huiyu Zhou, and Qi Zhao",
        "link": "http://arxiv.org/abs/2509.23907v1",
        "abstract": "Federated learning enables collaborative training of machine learning models\namong different clients while ensuring data privacy, emerging as the mainstream\nfor breaking data silos in the healthcare domain. However, the imbalance of\nmedical resources, data corruption or improper data preservation may lead to a\nsituation where different clients possess medical images of different modality.\nThis heterogeneity poses a significant challenge for cross-domain medical image\nsegmentation within the federated learning framework. To address this\nchallenge, we propose a new Federated Domain Adaptation (FedDA) segmentation\ntraining framework. Specifically, we propose a feature-level adversarial\nlearning among clients by aligning feature maps across clients through\nembedding an adversarial training mechanism. This design can enhance the\nmodel's generalization on multiple domains and alleviate the negative impact\nfrom domain-shift. Comprehensive experiments on three medical image datasets\ndemonstrate that our proposed FedDA substantially achieves cross-domain\nfederated aggregation, endowing single modality client with cross-modality\nprocessing capabilities, and consistently delivers robust performance compared\nto state-of-the-art federated aggregation algorithms in objective and\nsubjective assessment. Our code are available at\nhttps://github.com/GGbond-study/FedDA."
    },
    {
        "date": "2025-09",
        "title": "PCRI: Measuring Context Robustness in Multimodal Models for Enterprise Applications",
        "author": "Hitesh Laxmichand Patel, Amit Agarwal, Srikant Panda, Hansa Meghwani, Karan Dua, Paul Li, Tao Sheng, Sujith Ravi, and Dan Roth",
        "link": "http://arxiv.org/abs/2509.23879v1",
        "abstract": "The reliability of Multimodal Large Language Models (MLLMs) in real-world\nsettings is often undermined by sensitivity to irrelevant or distracting visual\ncontext, an aspect not captured by existing evaluation metrics. We introduce\nthe \\textbf{Patch Context Robustness Index (PCRI)}, the first systematic and\ninterpretable score for quantifying MLLM robustness to variations in visual\ncontext granularity, measuring performance changes between localized image\npatches and full-image input.\n  Applying PCRI to 19 state-of-the-art MLLMs across 15 vision-language\nbenchmarks, we find that most leading models remain brittle to background\nnoise, with only a few, such as InternVL2-26B and Qwen2VL-72B, demonstrating\nconsistent robustness across tasks. PCRI analysis also highlights how different\nmodel architectures handle and integrate visual context, offering actionable\ndiagnostic insight for both researchers and practitioners.\n  PCRI enables rigorous comparison of context robustness, supporting principled\nmodel selection and guiding the development of future architectures and\ntraining strategies for robust, real-world deployment."
    },
    {
        "date": "2025-09",
        "title": "Taught Well Learned Ill: Towards Distillation-conditional Backdoor Attack",
        "author": "Yukun Chen, Boheng Li, Yu Yuan, Leyi Qi, Yiming Li, Tianwei Zhang, Zhan Qin, and Kui Ren",
        "link": "http://arxiv.org/abs/2509.23871v1",
        "abstract": "Knowledge distillation (KD) is a vital technique for deploying deep neural\nnetworks (DNNs) on resource-constrained devices by transferring knowledge from\nlarge teacher models to lightweight student models. While teacher models from\nthird-party platforms may undergo security verification (\\eg, backdoor\ndetection), we uncover a novel and critical threat: distillation-conditional\nbackdoor attacks (DCBAs). DCBA injects dormant and undetectable backdoors into\nteacher models, which become activated in student models via the KD process,\neven with clean distillation datasets. While the direct extension of existing\nmethods is ineffective for DCBA, we implement this attack by formulating it as\na bilevel optimization problem and proposing a simple yet effective method\n(\\ie, SCAR). Specifically, the inner optimization simulates the KD process by\noptimizing a surrogate student model, while the outer optimization leverages\noutputs from this surrogate to optimize the teacher model for implanting the\nconditional backdoor. Our SCAR addresses this complex optimization utilizing an\nimplicit differentiation algorithm with a pre-optimized trigger injection\nfunction. Extensive experiments across diverse datasets, model architectures,\nand KD techniques validate the effectiveness of our SCAR and its resistance\nagainst existing backdoor detection, highlighting a significant yet previously\noverlooked vulnerability in the KD process. Our code is available at\nhttps://github.com/WhitolfChen/SCAR."
    },
    {
        "date": "2025-09",
        "title": "FairViT-GAN: A Hybrid Vision Transformer with Adversarial Debiasing for Fair and Explainable Facial Beauty Prediction",
        "author": "Djamel Eddine Boukhari",
        "link": "http://arxiv.org/abs/2509.23859v1",
        "abstract": "Facial Beauty Prediction (FBP) has made significant strides with the\napplication of deep learning, yet state-of-the-art models often exhibit\ncritical limitations, including architectural constraints, inherent demographic\nbiases, and a lack of transparency. Existing methods, primarily based on\nConvolutional Neural Networks (CNNs), excel at capturing local texture but\nstruggle with global facial harmony, while Vision Transformers (ViTs)\neffectively model long-range dependencies but can miss fine-grained details.\nFurthermore, models trained on benchmark datasets can inadvertently learn and\nperpetuate societal biases related to protected attributes like ethnicity. To\naddress these interconnected challenges, we propose \\textbf{FairViT-GAN}, a\nnovel hybrid framework that synergistically integrates a CNN branch for local\nfeature extraction and a ViT branch for global context modeling. More\nsignificantly, we introduce an adversarial debiasing mechanism where the\nfeature extractor is explicitly trained to produce representations that are\ninvariant to protected attributes, thereby actively mitigating algorithmic\nbias. Our framework's transparency is enhanced by visualizing the distinct\nfocus of each architectural branch. Extensive experiments on the SCUT-FBP5500\nbenchmark demonstrate that FairViT-GAN not only sets a new state-of-the-art in\npredictive accuracy, achieving a Pearson Correlation of \\textbf{0.9230} and\nreducing RMSE to \\textbf{0.2650}, but also excels in fairness. Our analysis\nreveals a remarkable \\textbf{82.9\\% reduction in the performance gap} between\nethnic subgroups, with the adversary's classification accuracy dropping to\nnear-random chance (52.1\\%). We believe FairViT-GAN provides a robust,\ntransparent, and significantly fairer blueprint for developing responsible AI\nsystems for subjective visual assessment."
    },
    {
        "date": "2025-09",
        "title": "Adversarial Diffusion for Robust Reinforcement Learning",
        "author": "Daniele Foffano, Alessio Russo, and Alexandre Proutiere",
        "link": "http://arxiv.org/abs/2509.23846v1",
        "abstract": "Robustness to modeling errors and uncertainties remains a central challenge\nin reinforcement learning (RL). In this work, we address this challenge by\nleveraging diffusion models to train robust RL policies. Diffusion models have\nrecently gained popularity in model-based RL due to their ability to generate\nfull trajectories \"all at once\", mitigating the compounding errors typical of\nstep-by-step transition models. Moreover, they can be conditioned to sample\nfrom specific distributions, making them highly flexible. We leverage\nconditional sampling to learn policies that are robust to uncertainty in\nenvironment dynamics. Building on the established connection between\nConditional Value at Risk (CVaR) optimization and robust RL, we introduce\nAdversarial Diffusion for Robust Reinforcement Learning (AD-RRL). AD-RRL guides\nthe diffusion process to generate worst-case trajectories during training,\neffectively optimizing the CVaR of the cumulative return. Empirical results\nacross standard benchmarks show that AD-RRL achieves superior robustness and\nperformance compared to existing robust RL methods."
    },
    {
        "date": "2025-09",
        "title": "Influence-Guided Concolic Testing of Transformer Robustness",
        "author": "Chih-Duo Hong, Yu Wang, Yao-Chen Chang, and Fang Yu",
        "link": "http://arxiv.org/abs/2509.23806v1",
        "abstract": "Concolic testing for deep neural networks alternates concrete execution with\nconstraint solving to search for inputs that flip decisions. We present an\n{influence-guided} concolic tester for Transformer classifiers that ranks path\npredicates by SHAP-based estimates of their impact on the model output. To\nenable SMT solving on modern architectures, we prototype a solver-compatible,\npure-Python semantics for multi-head self-attention and introduce practical\nscheduling heuristics that temper constraint growth on deeper models. In a\nwhite-box study on compact Transformers under small $L_0$ budgets, influence\nguidance finds label-flip inputs more efficiently than a FIFO baseline and\nmaintains steady progress on deeper networks. Aggregating successful attack\ninstances with a SHAP-based critical decision path analysis reveals recurring,\ncompact decision logic shared across attacks. These observations suggest that\n(i) influence signals provide a useful search bias for symbolic exploration,\nand (ii) solver-friendly attention semantics paired with lightweight scheduling\nmake concolic testing feasible for contemporary Transformer models, offering\npotential utility for debugging and model auditing."
    },
    {
        "date": "2025-09",
        "title": "GroupCoOp: Group-robust Fine-tuning via Group Prompt Learning",
        "author": "Nayeong Kim, Seong Joon Oh, and Suha Kwak",
        "link": "http://arxiv.org/abs/2509.23781v1",
        "abstract": "Parameter-efficient fine-tuning (PEFT) of vision-language models (VLMs)\nexcels in various vision tasks thanks to the rich knowledge and generalization\nability of VLMs. However, recent studies revealed that such fine-tuned VLMs are\nvulnerable to spurious correlations stemming from the subgroup imbalance in the\nfine-tuning datasets. To resolve this issue, we propose Group Context\nOptimization (GroupCoOp), a simple and effective debiased fine-tuning algorithm\nthat enhances the group robustness of fine-tuned VLMs. Its key idea is to\nemploy group-specific text prompts as group representatives serving as multiple\nclassifiers for their target class. The rich semantic knowledge of the text\nencoder of VLM enables the discovery of effective group prompts even for groups\nwith a small number of training samples. Leveraging the group prompts for each\nclass addresses the issues caused by the group-imbalanced training set, such as\nthe neglect of minority groups and the scattered distribution of each class in\nthe embedding space. GroupCoOp achieved the best results on five benchmarks\nacross five CLIP architectures and occasionally outperformed prior methods that\nfine-tune the entire network, despite training only 0.016\\% of the network's\nparameters."
    },
    {
        "date": "2025-09",
        "title": "Accuracy-Robustness Trade Off via Spiking Neural Network Gradient Sparsity Trail",
        "author": "Nhan T. Luu",
        "link": "http://arxiv.org/abs/2509.23762v2",
        "abstract": "Spiking Neural Networks (SNNs) have attracted growing interest in both\ncomputational neuroscience and artificial intelligence, primarily due to their\ninherent energy efficiency and compact memory footprint. However, achieving\nadversarial robustness in SNNs, particularly for vision-related tasks, remains\na nascent and underexplored challenge. Recent studies have proposed leveraging\nsparse gradients as a form of regularization to enhance robustness against\nadversarial perturbations. In this work, we present a surprising finding: under\nspecific architectural configurations, SNNs exhibit natural gradient sparsity\nand can achieve state-of-the-art adversarial defense performance without the\nneed for any explicit regularization. Further analysis reveals a trade-off\nbetween robustness and generalization: while sparse gradients contribute to\nimproved adversarial resilience, they can impair the model's ability to\ngeneralize; conversely, denser gradients support better generalization but\nincrease vulnerability to attacks."
    },
    {
        "date": "2025-09",
        "title": "Merge Now, Regret Later: The Hidden Cost of Model Merging is Adversarial Transferability",
        "author": "Ankit Gangwal, and Aaryan Ajay Sharma",
        "link": "http://arxiv.org/abs/2509.23689v1",
        "abstract": "Model Merging (MM) has emerged as a promising alternative to multi-task\nlearning, where multiple fine-tuned models are combined, without access to\ntasks' training data, into a single model that maintains performance across\ntasks. Recent works have explored the impact of MM on adversarial attacks,\nparticularly backdoor attacks. However, none of them have sufficiently explored\nits impact on transfer attacks using adversarial examples, i.e., a black-box\nadversarial attack where examples generated for a surrogate model successfully\nmislead a target model.\n  In this work, we study the effect of MM on the transferability of adversarial\nexamples. We perform comprehensive evaluations and statistical analysis\nconsisting of 8 MM methods, 7 datasets, and 6 attack methods, sweeping over 336\ndistinct attack settings. Through it, we first challenge the prevailing notion\nof MM conferring free adversarial robustness, and show MM cannot reliably\ndefend against transfer attacks, with over 95% relative transfer attack success\nrate. Moreover, we reveal 3 key insights for machine-learning practitioners\nregarding MM and transferability for a robust system design: (1) stronger MM\nmethods increase vulnerability to transfer attacks; (2) mitigating\nrepresentation bias increases vulnerability to transfer attacks; and (3) weight\naveraging, despite being the weakest MM method, is the most vulnerable MM\nmethod to transfer attacks. Finally, we analyze the underlying reasons for this\nincreased vulnerability, and provide potential solutions to the problem. Our\nfindings offer critical insights for designing more secure systems employing\nMM."
    }
]