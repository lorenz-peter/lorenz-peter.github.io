[{"date":"2025-01","title":"Credit Risk Identification in Supply Chains Using Generative Adversarial Networks","author":"Zizhou Zhang, Xinshi Li, Yu Cheng, Zhenrui Chen, and Qianying Liu","link":"http://arxiv.org/abs/2501.10348v1","abstract":"Credit risk management within supply chains has emerged as a critical\nresearch area due to its significant implications for operational stability and\nfinancial sustainability. The intricate interdependencies among supply chain\nparticipants mean that credit risks can propagate across networks, with impacts\nvarying by industry. This study explores the application of Generative\nAdversarial Networks (GANs) to enhance credit risk identification in supply\nchains. GANs enable the generation of synthetic credit risk scenarios,\naddressing challenges related to data scarcity and imbalanced datasets. By\nleveraging GAN-generated data, the model improves predictive accuracy while\neffectively capturing dynamic and temporal dependencies in supply chain data.\nThe research focuses on three representative industries-manufacturing (steel),\ndistribution (pharmaceuticals), and services (e-commerce) to assess\nindustry-specific credit risk contagion. Experimental results demonstrate that\nthe GAN-based model outperforms traditional methods, including logistic\nregression, decision trees, and neural networks, achieving superior accuracy,\nrecall, and F1 scores. The findings underscore the potential of GANs in\nproactive risk management, offering robust tools for mitigating financial\ndisruptions in supply chains. Future research could expand the model by\nincorporating external market factors and supplier relationships to further\nenhance predictive capabilities. Keywords- Generative Adversarial Networks\n(GANs); Supply Chain Risk; Credit Risk Identification; Machine Learning; Data\nAugmentation"},{"date":"2025-01","title":"Hierarchical Autoregressive Transformers: Combining Byte-~and Word-Level Processing for Robust, Adaptable Language Models","author":"Pit Neitemeier, Bj\u00f6rn Deiseroth, Constantin Eichenberg, and Lukas Balles","link":"http://arxiv.org/abs/2501.10322v1","abstract":"Tokenization is a fundamental step in natural language processing, breaking\ntext into units that computational models can process. While learned subword\ntokenizers have become the de-facto standard, they present challenges such as\nlarge vocabularies, limited adaptability to new domains or languages, and\nsensitivity to spelling errors and variations. To overcome these limitations,\nwe investigate a hierarchical architecture for autoregressive language\nmodelling that combines character-level and word-level processing. It employs a\nlightweight character-level encoder to convert character sequences into word\nembeddings, which are then processed by a word-level backbone model and decoded\nback into characters via a compact character-level decoder. This method retains\nthe sequence compression benefits of word-level tokenization without relying on\na rigid, predefined vocabulary. We demonstrate, at scales up to 7 billion\nparameters, that hierarchical transformers match the downstream task\nperformance of subword-tokenizer-based models while exhibiting significantly\ngreater robustness to input perturbations. Additionally, during continued\npretraining on an out-of-domain language, our model trains almost twice as\nfast, achieves superior performance on the target language, and retains more of\nits previously learned knowledge. Hierarchical transformers pave the way for\nNLP systems that are more robust, flexible, and generalizable across languages\nand domains."},{"date":"2025-01","title":"Robust Egoistic Rigid Body Localization","author":"Niclas F\u00fchrling, Giuseppe Thadeu Freitas de Abreu, David Gonz\u00e1lez G., and Osvaldo Gonsa","link":"http://arxiv.org/abs/2501.10219v1","abstract":"We consider a robust and self-reliant (or \"egoistic\") variation of the rigid\nbody localization (RBL) problem, in which a primary rigid body seeks to\nestimate the pose (i.e., location and orientation) of another rigid body (or\n\"target\"), relative to its own, without the assistance of external\ninfrastructure, without prior knowledge of the shape of the target, and taking\ninto account the possibility that the available observations are incomplete.\nThree complementary contributions are then offered for such a scenario. The\nfirst is a method to estimate the translation vector between the center point\nof both rigid bodies, which unlike existing techniques does not require that\nboth objects have the same shape or even the same number of landmark points.\nThis technique is shown to significantly outperform the state-of-the-art (SotA)\nunder complete information, but to be sensitive to data erasures, even when\nenhanced by matrix completion methods. The second contribution, designed to\noffer improved performance in the presence of incomplete information, offers a\nrobust alternative to the latter, at the expense of a slight relative loss\nunder complete information. Finally, the third contribution is a scheme for the\nestimation of the rotation matrix describing the relative orientation of the\ntarget rigid body with respect to the primary. Comparisons of the proposed\nschemes and SotA techniques demonstrate the advantage of the contributed\nmethods in terms of root mean square error (RMSE) performance under fully\ncomplete information and incomplete conditions."},{"date":"2025-01","title":"Provably Safeguarding a Classifier from OOD and Adversarial Samples: an Extreme Value Theory Approach","author":"Nicolas Atienza, Christophe Labreuche, Johanne Cohen, and Michele Sebag","link":"http://arxiv.org/abs/2501.10202v1","abstract":"This paper introduces a novel method, Sample-efficient Probabilistic\nDetection using Extreme Value Theory (SPADE), which transforms a classifier\ninto an abstaining classifier, offering provable protection against\nout-of-distribution and adversarial samples. The approach is based on a\nGeneralized Extreme Value (GEV) model of the training distribution in the\nclassifier's latent space, enabling the formal characterization of OOD samples.\nInterestingly, under mild assumptions, the GEV model also allows for formally\ncharacterizing adversarial samples. The abstaining classifier, which rejects\nsamples based on their assessment by the GEV model, provably avoids OOD and\nadversarial samples. The empirical validation of the approach, conducted on\nvarious neural architectures (ResNet, VGG, and Vision Transformer) and medium\nand large-sized datasets (CIFAR-10, CIFAR-100, and ImageNet), demonstrates its\nfrugality, stability, and efficiency compared to the state of the art."},{"date":"2025-01","title":"Contributions to the Decision Theoretic Foundations of Machine Learning and Robust Statistics under Weakly Structured Information","author":"Christoph Jansen","link":"http://arxiv.org/abs/2501.10195v1","abstract":"This habilitation thesis is cumulative and, therefore, is collecting and\nconnecting research that I (together with several co-authors) have conducted\nover the last few years. Thus, the absolute core of the work is formed by the\nten publications listed on page 5 under the name Contributions 1 to 10. The\nreferences to the complete versions of these articles are also found in this\nlist, making them as easily accessible as possible for readers wishing to dive\ndeep into the different research projects. The chapters following this thesis,\nnamely Parts A to C and the concluding remarks, serve to place the articles in\na larger scientific context, to (briefly) explain their respective content on a\nless formal level, and to highlight some interesting perspectives for future\nresearch in their respective contexts. Naturally, therefore, the following\npresentation has neither the level of detail nor the formal rigor that can\n(hopefully) be found in the papers. The purpose of the following text is to\nprovide the reader an easy and high-level access to this interesting and\nimportant research field as a whole, thereby, advertising it to a broader\naudience."},{"date":"2025-01","title":"Secure Semantic Communication With Homomorphic Encryption","author":"Rui Meng, Dayu Fan, Haixiao Gao, Yifan Yuan, Bizhu Wang, Xiaodong Xu, Mengying Sun, Chen Dong, Xiaofeng Tao, Ping Zhang, and Dusit Niyato","link":"http://arxiv.org/abs/2501.10182v1","abstract":"In recent years, Semantic Communication (SemCom), which aims to achieve\nefficient and reliable transmission of meaning between agents, has garnered\nsignificant attention from both academia and industry. To ensure the security\nof communication systems, encryption techniques are employed to safeguard\nconfidentiality and integrity. However, traditional cryptography-based\nencryption algorithms encounter obstacles when applied to SemCom. Motivated by\nthis, this paper explores the feasibility of applying homomorphic encryption to\nSemCom. Initially, we review the encryption algorithms utilized in mobile\ncommunication systems and analyze the challenges associated with their\napplication to SemCom. Subsequently, we employ scale-invariant feature\ntransform to demonstrate that semantic features can be preserved in homomorphic\nencrypted ciphertext. Based on this finding, we propose a task-oriented SemCom\nscheme secured through homomorphic encryption. We design the privacy preserved\ndeep joint source-channel coding (JSCC) encoder and decoder, and the frequency\nof key updates can be adjusted according to service requirements without\ncompromising transmission performance. Simulation results validate that, when\ncompared to plaintext images, the proposed scheme can achieve almost the same\nclassification accuracy performance when dealing with homomorphic ciphertext\nimages. Furthermore, we provide potential future research directions for\nhomomorphic encrypted SemCom."},{"date":"2025-01","title":"Robotic World Model: A Neural Network Simulator for Robust Policy Optimization in Robotics","author":"Chenhao Li, Andreas Krause, and Marco Hutter","link":"http://arxiv.org/abs/2501.10100v1","abstract":"Learning robust and generalizable world models is crucial for enabling\nefficient and scalable robotic control in real-world environments. In this\nwork, we introduce a novel framework for learning world models that accurately\ncapture complex, partially observable, and stochastic dynamics. The proposed\nmethod employs a dual-autoregressive mechanism and self-supervised training to\nachieve reliable long-horizon predictions without relying on domain-specific\ninductive biases, ensuring adaptability across diverse robotic tasks. We\nfurther propose a policy optimization framework that leverages world models for\nefficient training in imagined environments and seamless deployment in\nreal-world systems. Through extensive experiments, our approach consistently\noutperforms state-of-the-art methods, demonstrating superior autoregressive\nprediction accuracy, robustness to noise, and generalization across\nmanipulation and locomotion tasks. Notably, policies trained with our method\nare successfully deployed on ANYmal D hardware in a zero-shot transfer,\nachieving robust performance with minimal sim-to-real performance loss. This\nwork advances model-based reinforcement learning by addressing the challenges\nof long-horizon prediction, error accumulation, and sim-to-real transfer. By\nproviding a scalable and robust framework, the introduced methods pave the way\nfor adaptive and efficient robotic systems in real-world applications."},{"date":"2025-01","title":"Efficient Simulation of Quantum Secure Multiparty Computation","author":"Kartick Sutradhar","link":"http://arxiv.org/abs/2501.10083v1","abstract":"One of the key characteristics of secure quantum communication is quantum\nsecure multiparty computation. In this paper, we propose a quantum secure\nmultiparty summation (QSMS) protocol that can be applied to many complex\nquantum operations. It is based on the $(t, n)$ threshold approach. We combine\nthe classical and quantum phenomena to make this protocol realistic and secure.\nBecause the current protocols employ the $(n, n)$ threshold approach, which\nrequires all honest players to execute the quantum multiparty summation\nprotocol, they have certain security and efficiency problems. However, we\nemploy a $(t, n)$ threshold approach, which requires the quantum summation\nprotocol to be computed only by $t$ honest players. Our suggested protocol is\nmore economical, practical, and secure than alternative protocols."},{"date":"2025-01","title":"Robust Change Captioning in Remote Sensing: SECOND-CC Dataset and MModalCC Framework","author":"Ali Can Karaca, M. Enes Ozelbas, Saadettin Berber, Orkhan Karimli, Turabi Yildirim, and M. Fatih Amasyali","link":"http://arxiv.org/abs/2501.10075v1","abstract":"Remote sensing change captioning (RSICC) aims to describe changes between\nbitemporal images in natural language. Existing methods often fail under\nchallenges like illumination differences, viewpoint changes, blur effects,\nleading to inaccuracies, especially in no-change regions. Moreover, the images\nacquired at different spatial resolutions and have registration errors tend to\naffect the captions. To address these issues, we introduce SECOND-CC, a novel\nRSICC dataset featuring high-resolution RGB image pairs, semantic segmentation\nmaps, and diverse real-world scenarios. SECOND-CC which contains 6,041 pairs of\nbitemporal RS images and 30,205 sentences describing the differences between\nimages. Additionally, we propose MModalCC, a multimodal framework that\nintegrates semantic and visual data using advanced attention mechanisms,\nincluding Cross-Modal Cross Attention (CMCA) and Multimodal Gated Cross\nAttention (MGCA). Detailed ablation studies and attention visualizations\nfurther demonstrate its effectiveness and ability to address RSICC challenges.\nComprehensive experiments show that MModalCC outperforms state-of-the-art RSICC\nmethods, including RSICCformer, Chg2Cap, and PSNet with +4.6% improvement on\nBLEU4 score and +9.6% improvement on CIDEr score. We will make our dataset and\ncodebase publicly available to facilitate future research at\nhttps://github.com/ChangeCapsInRS/SecondCC"},{"date":"2025-01","title":"Spatiotemporal Prediction of Secondary Crashes by Rebalancing Dynamic and Static Data with Generative Adversarial Networks","author":"Junlan Chen, Yiqun Li, Chenyu Ling, Ziyuan Pu, and Xiucheng Guo","link":"http://arxiv.org/abs/2501.10041v1","abstract":"Data imbalance is a common issue in analyzing and predicting sudden traffic\nevents. Secondary crashes constitute only a small proportion of all crashes.\nThese secondary crashes, triggered by primary crashes, significantly exacerbate\ntraffic congestion and increase the severity of incidents. However, the severe\nimbalance of secondary crash data poses significant challenges for prediction\nmodels, affecting their generalization ability and prediction accuracy.\nExisting methods fail to fully address the complexity of traffic crash data,\nparticularly the coexistence of dynamic and static features, and often struggle\nto effectively handle data samples of varying lengths. Furthermore, most\ncurrent studies predict the occurrence probability and spatiotemporal\ndistribution of secondary crashes separately, lacking an integrated solution.\nTo address these challenges, this study proposes a hybrid model named\nVarFusiGAN-Transformer, aimed at improving the fidelity of secondary crash data\ngeneration and jointly predicting the occurrence and spatiotemporal\ndistribution of secondary crashes. The VarFusiGAN-Transformer model employs\nLong Short-Term Memory (LSTM) networks to enhance the generation of\nmultivariate long-time series data, incorporating a static data generator and\nan auxiliary discriminator to model the joint distribution of dynamic and\nstatic features. In addition, the model's prediction module achieves\nsimultaneous prediction of both the occurrence and spatiotemporal distribution\nof secondary crashes. Compared to existing methods, the proposed model\ndemonstrates superior performance in generating high-fidelity data and\nimproving prediction accuracy."},{"date":"2025-01","title":"CaFA: Cost-aware, Feasible Attacks With Database Constraints Against Neural Tabular Classifiers","author":"Matan Ben-Tov, Daniel Deutch, Nave Frost, and Mahmood Sharif","link":"http://arxiv.org/abs/2501.10013v1","abstract":"This work presents CaFA, a system for Cost-aware Feasible Attacks for\nassessing the robustness of neural tabular classifiers against adversarial\nexamples realizable in the problem space, while minimizing adversaries' effort.\nTo this end, CaFA leverages TabPGD$-$an algorithm we set forth to generate\nadversarial perturbations suitable for tabular data$-$ and incorporates\nintegrity constraints automatically mined by state-of-the-art database methods.\nAfter producing adversarial examples in the feature space via TabPGD, CaFA\nprojects them on the mined constraints, leading, in turn, to better attack\nrealizability. We tested CaFA with three datasets and two architectures and\nfound, among others, that the constraints we use are of higher quality\n(measured via soundness and completeness) than ones employed in prior work.\nMoreover, CaFA achieves higher feasible success rates$-$i.e., it generates\nadversarial examples that are often misclassified while satisfying\nconstraints$-$than prior attacks while simultaneously perturbing few features\nwith lower magnitudes, thus saving effort and improving inconspicuousness. We\nopen-source CaFA, hoping it will serve as a generic system enabling\nmachine-learning engineers to assess their models' robustness against\nrealizable attacks, thus advancing deployed models' trustworthiness."},{"date":"2025-01","title":"Advancing Image Security with Quantum Key Distribution and Multi-Layer Chaotic Encryption for Quantum Resilient Transmission","author":"Tasmin Karim, Md. Shazzad Hossain Shaon, Md. Fahim Sultan, and Mst Shapna Akter","link":"http://arxiv.org/abs/2501.09895v1","abstract":"Quantum security improves cryptographic protocols by applying quantum\nmechanics principles, assuring resistance to both quantum and conventional\ncomputer attacks. This work addresses these issues by integrating Quantum Key\nDistribution (QKD) utilizing the E91 method with Multi-Layer Chaotic\nEncryption, which employs a variety of patterns to detect eavesdropping,\nresulting in a highly secure image-transmission architecture. The method\nleverages entropy calculations to determine the unpredictability and integrity\nof encrypted and decrypted pictures, guaranteeing strong security. Extensive\nstatistical scenarios illustrate the framework's effectiveness in image\nencryption while preserving high entropy and sensitivity to the original\nvisuals. The findings indicate significant improvement in encryption and\ndecryption performance, demonstrating the framework's potential as a robust\nresponse to weaknesses introduced by advances in quantum computing. Several\nmetrics, such as Peak Signal-to-Noise Ratio (PSNR), Structural Similarity Index\n(SSIM), Normalized Cross-Correlation (NCC), Bit Error Rate (BER), entropy\nvalues for original, encrypted, and decrypted images, and the correlation\nbetween original and decrypted images, validate the framework's effectiveness.\nThe combination of QKD with Multi-Layer Chaotic Encryption provides a scalable\nand resilient technique to secure image communication. As quantum computing\nadvances, this framework offers a future-proof approach for defining secure\ncommunication protocols in crucial sectors such as medical treatment, forensic\ncomputing, and national security, where information confidentiality is\nvaluable."},{"date":"2025-01","title":"FLORA: Formal Language Model Enables Robust Training-free Zero-shot Object Referring Analysis","author":"Zhe Chen, and Zijing Chen","link":"http://arxiv.org/abs/2501.09887v1","abstract":"Object Referring Analysis (ORA), commonly known as referring expression\ncomprehension, requires the identification and localization of specific objects\nin an image based on natural descriptions. Unlike generic object detection, ORA\nrequires both accurate language understanding and precise visual localization,\nmaking it inherently more complex. Although recent pre-trained large visual\ngrounding detectors have achieved significant progress, they heavily rely on\nextensively labeled data and time-consuming learning. To address these, we\nintroduce a novel, training-free framework for zero-shot ORA, termed FLORA\n(Formal Language for Object Referring and Analysis). FLORA harnesses the\ninherent reasoning capabilities of large language models (LLMs) and integrates\na formal language model - a logical framework that regulates language within\nstructured, rule-based descriptions - to provide effective zero-shot ORA. More\nspecifically, our formal language model (FLM) enables an effective,\nlogic-driven interpretation of object descriptions without necessitating any\ntraining processes. Built upon FLM-regulated LLM outputs, we further devise a\nBayesian inference framework and employ appropriate off-the-shelf interpretive\nmodels to finalize the reasoning, delivering favorable robustness against LLM\nhallucinations and compelling ORA performance in a training-free manner. In\npractice, our FLORA boosts the zero-shot performance of existing pretrained\ngrounding detectors by up to around 45%. Our comprehensive evaluation across\ndifferent challenging datasets also confirms that FLORA consistently surpasses\ncurrent state-of-the-art zero-shot methods in both detection and segmentation\ntasks associated with zero-shot ORA. We believe our probabilistic parsing and\nreasoning of the LLM outputs elevate the reliability and interpretability of\nzero-shot ORA. We shall release codes upon publication."},{"date":"2025-01","title":"Generalized Single-Image-Based Morphing Attack Detection Using Deep Representations from Vision Transformer","author":"Haoyu Zhang, Raghavendra Ramachandra, Kiran Raja, and Christoph Busch","link":"http://arxiv.org/abs/2501.09817v1","abstract":"Face morphing attacks have posed severe threats to Face Recognition Systems\n(FRS), which are operated in border control and passport issuance use cases.\nCorrespondingly, morphing attack detection algorithms (MAD) are needed to\ndefend against such attacks. MAD approaches must be robust enough to handle\nunknown attacks in an open-set scenario where attacks can originate from\nvarious morphing generation algorithms, post-processing and the diversity of\nprinters/scanners. The problem of generalization is further pronounced when the\ndetection has to be made on a single suspected image. In this paper, we propose\na generalized single-image-based MAD (S-MAD) algorithm by learning the encoding\nfrom Vision Transformer (ViT) architecture. Compared to CNN-based\narchitectures, ViT model has the advantage on integrating local and global\ninformation and hence can be suitable to detect the morphing traces widely\ndistributed among the face region. Extensive experiments are carried out on\nface morphing datasets generated using publicly available FRGC face datasets.\nSeveral state-of-the-art (SOTA) MAD algorithms, including representative ones\nthat have been publicly evaluated, have been selected and benchmarked with our\nViT-based approach. Obtained results demonstrate the improved detection\nperformance of the proposed S-MAD method on inter-dataset testing (when\ndifferent data is used for training and testing) and comparable performance on\nintra-dataset testing (when the same data is used for training and testing)\nexperimental protocol."},{"date":"2025-01","title":"Ruling the Unruly: Designing Effective, Low-Noise Network Intrusion Detection Rules for Security Operations Centers","author":"Koen T. W. Teuwen, Tom Mulders, Emmanuele Zambon, and Luca Allodi","link":"http://arxiv.org/abs/2501.09808v1","abstract":"Many Security Operations Centers (SOCs) today still heavily rely on\nsignature-based Network Intrusion Detection Systems (NIDS) such as Suricata.\nThe specificity of intrusion detection rules and the coverage provided by\nrulesets are common concerns within the professional community surrounding\nSOCs, which impact the effectiveness of automated alert post-processing\napproaches. We postulate a better understanding of factors influencing the\nquality of rules can help address current SOC issues. In this paper, we\ncharacterize the rules in use at a collaborating commercial (managed) SOC\nserving customers in sectors including education and IT management. During this\nprocess, we discover six relevant design principles, which we consolidate\nthrough interviews with experienced rule designers at the SOC.We then validate\nour design principles by quantitatively assessing their effect on rule\nspecificity. We find that several of these design considerations significantly\nimpact unnecessary workload caused by rules. For instance, rules that leverage\nproxies for detection, and rules that do not employ alert throttling or do not\ndistinguish (un)successful malicious actions, cause significantly more workload\nfor SOC analysts. Moreover, rules that match a generalized characteristic to\ndetect malicious behavior, which is believed to increase coverage, also\nsignificantly increase workload, suggesting a tradeoff must be struck between\nrule specificity and coverage. We show that these design principles can be\napplied successfully at a SOC to reduce workload whilst maintaining coverage\ndespite the prevalence of violations of the principles."},{"date":"2025-01","title":"W3ID: A Quantum Computing-Secure Digital Identity System Redefining Standards for Web3 and Digital Twins","author":"Joseph Yun, Eli Lifton, Eunseo Lee, Yohan Yun, Abigail Song, Joshua Lee, Cristian Jimenez-Bert, Benedict Song, Yejun Lee, Alex Seo, and Sijung Yun","link":"http://arxiv.org/abs/2501.09802v1","abstract":"The rapid advancements in quantum computing present significant threats to\nexisting encryption standards and internet security. Simultaneously, the advent\nof Web 3.0 marks a transformative era in internet history, emphasizing enhanced\ndata security, decentralization, and user ownership. This white paper\nintroduces the W3ID, an abbreviation of Web3 standard meeting universal digital\nID, which is a Universal Digital Identity (UDI) model designed to meet Web3\nstandards while addressing vulnerabilities posed by quantum computing. W3ID\ninnovatively generates secure Digital Object Identifiers (DOIs) tailored for\nthe decentralized Web 3.0 ecosystem. Additionally, W3ID employs a dual-key\nsystem for secure authentication, enhancing both public and private\nverification mechanisms. To further enhance encryption strength and\nauthentication integrity in the quantum computing era, W3ID incorporates an\nadvanced security mechanism. By requiring quadruple application of SHA-256,\nwith consecutive matches for validation, the system expands the number of\npossibilities to 256^4, which is approximately 4.3 billion times the current\nSHA-256 capacity. This dramatic increase in computational complexity ensures\nthat even advanced quantum computing systems would face significant challenges\nin executing brute-force attacks. W3ID redefines digital identity standards for\nWeb 3.0 and the quantum computing era, setting a new benchmark for security,\nscalability, and decentralization in the global digital twin ecosystem."},{"date":"2025-01","title":"Unified Face Matching and Physical-Digital Spoofing Attack Detection","author":"Arun Kunwar, and Ajita Rattani","link":"http://arxiv.org/abs/2501.09635v1","abstract":"Face recognition technology has dramatically transformed the landscape of\nsecurity, surveillance, and authentication systems, offering a user-friendly\nand non-invasive biometric solution. However, despite its significant\nadvantages, face recognition systems face increasing threats from physical and\ndigital spoofing attacks. Current research typically treats face recognition\nand attack detection as distinct classification challenges. This approach\nnecessitates the implementation of separate models for each task, leading to\nconsiderable computational complexity, particularly on devices with limited\nresources. Such inefficiencies can stifle scalability and hinder performance.\nIn response to these challenges, this paper introduces an innovative unified\nmodel designed for face recognition and detection of physical and digital\nattacks. By leveraging the advanced Swin Transformer backbone and incorporating\nHiLo attention in a convolutional neural network framework, we address unified\nface recognition and spoof attack detection more effectively. Moreover, we\nintroduce augmentation techniques that replicate the traits of physical and\ndigital spoofing cues, significantly enhancing our model robustness. Through\ncomprehensive experimental evaluation across various datasets, we showcase the\neffectiveness of our model in unified face recognition and spoof detection.\nAdditionally, we confirm its resilience against unseen physical and digital\nspoofing attacks, underscoring its potential for real-world applications."},{"date":"2025-01","title":"Weight for Robustness: A Comprehensive Approach towards Optimal Fault-Tolerant Asynchronous ML","author":"Tehila Dahan, and Kfir Y. Levy","link":"http://arxiv.org/abs/2501.09621v1","abstract":"We address the challenges of Byzantine-robust training in asynchronous\ndistributed machine learning systems, aiming to enhance efficiency amid massive\nparallelization and heterogeneous computing resources. Asynchronous systems,\nmarked by independently operating workers and intermittent updates, uniquely\nstruggle with maintaining integrity against Byzantine failures, which encompass\nmalicious or erroneous actions that disrupt learning. The inherent delays in\nsuch settings not only introduce additional bias to the system but also obscure\nthe disruptions caused by Byzantine faults. To tackle these issues, we adapt\nthe Byzantine framework to asynchronous dynamics by introducing a novel\nweighted robust aggregation framework. This allows for the extension of robust\naggregators and a recent meta-aggregator to their weighted versions, mitigating\nthe effects of delayed updates. By further incorporating a recent\nvariance-reduction technique, we achieve an optimal convergence rate for the\nfirst time in an asynchronous Byzantine environment. Our methodology is\nrigorously validated through empirical and theoretical analysis, demonstrating\nits effectiveness in enhancing fault tolerance and optimizing performance in\nasynchronous ML systems."},{"date":"2025-01","title":"Adversarial-Ensemble Kolmogorov Arnold Networks for Enhancing Indoor Wi-Fi Positioning: A Defensive Approach Against Spoofing and Signal Manipulation Attacks","author":"Mitul Goswami, Romit Chatterjee, Somnath Mahato, and Prasant Kumar Pattnaik","link":"http://arxiv.org/abs/2501.09609v1","abstract":"The research presents a study on enhancing the robustness of Wi-Fi-based\nindoor positioning systems against adversarial attacks. The goal is to improve\nthe positioning accuracy and resilience of these systems under two attack\nscenarios: Wi-Fi Spoofing and Signal Strength Manipulation. Three models are\ndeveloped and evaluated: a baseline model (M_Base), an adversarially trained\nrobust model (M_Rob), and an ensemble model (M_Ens). All models utilize a\nKolmogorov-Arnold Network (KAN) architecture. The robust model is trained with\nadversarially perturbed data, while the ensemble model combines predictions\nfrom both the base and robust models. Experimental results show that the robust\nmodel reduces positioning error by approximately 10% compared to the baseline,\nachieving 2.03 meters error under Wi-Fi spoofing and 2.00 meters under signal\nstrength manipulation. The ensemble model further outperforms with errors of\n2.01 meters and 1.975 meters for the respective attack types. This analysis\nhighlights the effectiveness of adversarial training techniques in mitigating\nattack impacts. The findings underscore the importance of considering\nadversarial scenarios in developing indoor positioning systems, as improved\nresilience can significantly enhance the accuracy and reliability of such\nsystems in mission-critical environments."},{"date":"2025-01","title":"Normal-NeRF: Ambiguity-Robust Normal Estimation for Highly Reflective Scenes","author":"Ji Shi, Xianghua Ying, Ruohao Guo, Bowei Xing, and Wenzhen Yue","link":"http://arxiv.org/abs/2501.09460v1","abstract":"Neural Radiance Fields (NeRF) often struggle with reconstructing and\nrendering highly reflective scenes. Recent advancements have developed various\nreflection-aware appearance models to enhance NeRF's capability to render\nspecular reflections. However, the robust reconstruction of highly reflective\nscenes is still hindered by the inherent shape ambiguity on specular surfaces.\nExisting methods typically rely on additional geometry priors to regularize the\nshape prediction, but this can lead to oversmoothed geometry in complex scenes.\nObserving the critical role of surface normals in parameterizing reflections,\nwe introduce a transmittance-gradient-based normal estimation technique that\nremains robust even under ambiguous shape conditions. Furthermore, we propose a\ndual activated densities module that effectively bridges the gap between smooth\nsurface normals and sharp object boundaries. Combined with a reflection-aware\nappearance model, our proposed method achieves robust reconstruction and\nhigh-fidelity rendering of scenes featuring both highly specular reflections\nand intricate geometric structures. Extensive experiments demonstrate that our\nmethod outperforms existing state-of-the-art methods on various datasets."},{"date":"2025-01","title":"Double Visual Defense: Adversarial Pre-training and Instruction Tuning for Improving Vision-Language Model Robustness","author":"Zeyu Wang, Cihang Xie, Brian Bartoldson, and Bhavya Kailkhura","link":"http://arxiv.org/abs/2501.09446v1","abstract":"This paper investigates the robustness of vision-language models against\nadversarial visual perturbations and introduces a novel ``double visual\ndefense\" to enhance this robustness. Unlike previous approaches that resort to\nlightweight adversarial fine-tuning of a pre-trained CLIP model, we perform\nlarge-scale adversarial vision-language pre-training from scratch using\nweb-scale data. We then strengthen the defense by incorporating adversarial\nvisual instruction tuning. The resulting models from each stage, $\\Delta$CLIP\nand $\\Delta^2$LLaVA, show substantially enhanced zero-shot robustness and set a\nnew state-of-the-art in adversarial defense for vision-language models. For\nexample, the adversarial robustness of $\\Delta$CLIP surpasses that of the\nprevious best models on ImageNet-1k by ~20%. %For example, $\\Delta$CLIP\nsurpasses the previous best models on ImageNet-1k by ~20% in terms of\nadversarial robustness. Similarly, compared to prior art, $\\Delta^2$LLaVA\nbrings a ~30% robustness improvement to image captioning task and a ~20%\nrobustness improvement to visual question answering task. Furthermore, our\nmodels exhibit stronger zero-shot recognition capability, fewer hallucinations,\nand superior reasoning performance compared to baselines. Our project page is\nhttps://doublevisualdefense.github.io/."},{"date":"2025-01","title":"Towards Robust and Realistic Human Pose Estimation via WiFi Signals","author":"Yang Chen, Jingcai Guo, Song Guo, Jingren Zhou, and Dacheng Tao","link":"http://arxiv.org/abs/2501.09411v1","abstract":"Robust WiFi-based human pose estimation is a challenging task that bridges\ndiscrete and subtle WiFi signals to human skeletons. This paper revisits this\nproblem and reveals two critical yet overlooked issues: 1) cross-domain gap,\ni.e., due to significant variations between source-target domain pose\ndistributions; and 2) structural fidelity gap, i.e., predicted skeletal poses\nmanifest distorted topology, usually with misplaced joints and disproportionate\nbone lengths. This paper fills these gaps by reformulating the task into a\nnovel two-phase framework dubbed DT-Pose: Domain-consistent representation\nlearning and Topology-constrained Pose decoding. Concretely, we first propose a\ntemporal-consistent contrastive learning strategy with uniformity\nregularization, coupled with self-supervised masking-reconstruction operations,\nto enable robust learning of domain-consistent and motion-discriminative\nWiFi-specific representations. Beyond this, we introduce a simple yet effective\npose decoder with task prompts, which integrates Graph Convolution Network\n(GCN) and Transformer layers to constrain the topology structure of the\ngenerated skeleton by exploring the adjacent-overarching relationships among\nhuman joints. Extensive experiments conducted on various benchmark datasets\nhighlight the superior performance of our method in tackling these fundamental\nchallenges in both 2D/3D human pose estimation tasks."},{"date":"2025-01","title":"Quantum-Enhanced Transformers for Robust Acoustic Scene Classification in IoT Environments","author":"Minh K. Quan, Mayuri Wijayasundara, Sujeeva Setunge, and Pubudu N. Pathirana","link":"http://arxiv.org/abs/2501.09394v1","abstract":"The proliferation of Internet of Things (IoT) devices equipped with acoustic\nsensors necessitates robust acoustic scene classification (ASC) capabilities,\neven in noisy and data-limited environments. Traditional machine learning\nmethods often struggle to generalize effectively under such conditions. To\naddress this, we introduce Q-ASC, a novel Quantum-Inspired Acoustic Scene\nClassifier that leverages the power of quantum-inspired transformers. By\nintegrating quantum concepts like superposition and entanglement, Q-ASC\nachieves superior feature learning and enhanced noise resilience compared to\nclassical models. Furthermore, we introduce a Quantum Variational Autoencoder\n(QVAE) based data augmentation technique to mitigate the challenge of limited\nlabeled data in IoT deployments. Extensive evaluations on the Tampere\nUniversity of Technology (TUT) Acoustic Scenes 2016 benchmark dataset\ndemonstrate that Q-ASC achieves remarkable accuracy between 68.3% and 88.5%\nunder challenging conditions, outperforming state-of-the-art methods by over 5%\nin the best case. This research paves the way for deploying intelligent\nacoustic sensing in IoT networks, with potential applications in smart homes,\nindustrial monitoring, and environmental surveillance, even in adverse acoustic\nenvironments."},{"date":"2025-01","title":"Neural Honeytrace: A Robust Plug-and-Play Watermarking Framework against Model Extraction Attacks","author":"Yixiao Xu, Binxing Fang, Rui Wang, Yinghai Zhou, Shouling Ji, Yuan Liu, Mohan Li, and Zhihong Tian","link":"http://arxiv.org/abs/2501.09328v2","abstract":"Developing high-performance deep learning models is resource-intensive,\nleading model owners to utilize Machine Learning as a Service (MLaaS) platforms\ninstead of publicly releasing their models. However, malicious users may\nexploit query interfaces to execute model extraction attacks, reconstructing\nthe target model's functionality locally. While prior research has investigated\ntriggerable watermarking techniques for asserting ownership, existing methods\nface significant challenges: (1) most approaches require additional training,\nresulting in high overhead and limited flexibility, and (2) they often fail to\naccount for advanced attackers, leaving them vulnerable to adaptive attacks.\n  In this paper, we propose Neural Honeytrace, a robust plug-and-play\nwatermarking framework against model extraction attacks. We first formulate a\nwatermark transmission model from an information-theoretic perspective,\nproviding an interpretable account of the principles and limitations of\nexisting triggerable watermarking. Guided by the model, we further introduce:\n(1) a similarity-based training-free watermarking method for plug-and-play and\nflexible watermarking, and (2) a distribution-based multi-step watermark\ninformation transmission strategy for robust watermarking. Comprehensive\nexperiments on four datasets demonstrate that Neural Honeytrace outperforms\nprevious methods in efficiency and resisting adaptive attacks. Neural\nHoneytrace reduces the average number of samples required for a worst-case\nt-Test-based copyright claim from $12,000$ to $200$ with zero training cost."},{"date":"2025-01","title":"Cooperative Decentralized Backdoor Attacks on Vertical Federated Learning","author":"Seohyun Lee, Wenzhi Fang, Anindya Bijoy Das, Seyyedali Hosseinalipour, David J. Love, and Christopher G. Brinton","link":"http://arxiv.org/abs/2501.09320v1","abstract":"Federated learning (FL) is vulnerable to backdoor attacks, where adversaries\nalter model behavior on target classification labels by embedding triggers into\ndata samples. While these attacks have received considerable attention in\nhorizontal FL, they are less understood for vertical FL (VFL), where devices\nhold different features of the samples, and only the server holds the labels.\nIn this work, we propose a novel backdoor attack on VFL which (i) does not rely\non gradient information from the server and (ii) considers potential collusion\namong multiple adversaries for sample selection and trigger embedding. Our\nlabel inference model augments variational autoencoders with metric learning,\nwhich adversaries can train locally. A consensus process over the adversary\ngraph topology determines which datapoints to poison. We further propose\nmethods for trigger splitting across the adversaries, with an intensity-based\nimplantation scheme skewing the server towards the trigger. Our convergence\nanalysis reveals the impact of backdoor perturbations on VFL indicated by a\nstationarity gap for the trained model, which we verify empirically as well. We\nconduct experiments comparing our attack with recent backdoor VFL approaches,\nfinding that ours obtains significantly higher success rates for the same main\ntask performance despite not using server information. Additionally, our\nresults verify the impact of collusion on attack performance."},{"date":"2025-01","title":"Clone-Robust AI Alignment","author":"Ariel D. Procaccia, Benjamin Schiffer, and Shirley Zhang","link":"http://arxiv.org/abs/2501.09254v1","abstract":"A key challenge in training Large Language Models (LLMs) is properly aligning\nthem with human preferences. Reinforcement Learning with Human Feedback (RLHF)\nuses pairwise comparisons from human annotators to train reward functions and\nhas emerged as a popular alignment method. However, input datasets in RLHF are\nnot necessarily balanced in the types of questions and answers that are\nincluded. Therefore, we want RLHF algorithms to perform well even when the set\nof alternatives is not uniformly distributed. Drawing on insights from social\nchoice theory, we introduce robustness to approximate clones, a desirable\nproperty of RLHF algorithms which requires that adding near-duplicate\nalternatives does not significantly change the learned reward function. We\nfirst demonstrate that the standard RLHF algorithm based on regularized maximum\nlikelihood estimation (MLE) fails to satisfy this property. We then propose the\nweighted MLE, a new RLHF algorithm that modifies the standard regularized MLE\nby weighting alternatives based on their similarity to other alternatives. This\nnew algorithm guarantees robustness to approximate clones while preserving\ndesirable theoretical properties."},{"date":"2025-01","title":"Practical Spoofing Attacks on Galileo Open Service Navigation Message Authentication","author":"Haiyang Wang, Yuanyu Zhang, Xinghui Zhu, Ji He, Shuangtrui Zhao, Yulong Shen, and Xiaohong Jiang","link":"http://arxiv.org/abs/2501.09246v1","abstract":"This paper examines the Galileo Open Service Navigation Message\nAuthentication (OSNMA) and, for the first time, discovers two critical\nvulnerabilities, namely artificially-manipulated time synchronization (ATS) and\ninterruptible message authentication (IMA). ATS allows attackers falsify a\nreceiver's signals and/or local reference time (LRT) while still fulfilling the\ntime synchronization (TS) requirement. IMA allows temporary interruption of the\nnavigation data authentication process due to the reception of a broken message\n(probably caused by spoofing attacks) and restores the authentication later. By\nexploiting the ATS vulnerability, we propose a TS-comply replay (TSR) attack\nwith two variants (real-time and non-real-time), where attackers replay signals\nto a victim receiver while strictly complying with the TS rule. We further\npropose a TS-comply forgery (TSF) attack, where attackers first use a\npreviously-disclosed key to forge a message based on the OSNMA protocol, then\ntamper with the vitcim receiver's LRT correspondingly to comply with the TS\nrule and finally transmit the forged message to the receiver. Finally, we\npropose a concatenating replay (CR) attack based on the IMA vulnerability,\nwhere attackers concatenate replayed signals to the victim receiver's signals\nin a way that still enables correct verification of the navigation data in the\nreplayed signals. To validate the effectiveness of the proposed attacks, we\nconduct real-world experiments with a commercial Galileo receiver manufactured\nby Septentrio, two software-defined radio (SDR) devices, open-source\nGalileo-SDR-SIM and OSNMAlib software. The results showed that all the attacks\ncan successfully pass the OSNMA scheme and the TSF attack can spoof receivers\nto arbitrary locations."},{"date":"2025-01","title":"Benchmarking Robustness of Contrastive Learning Models for Medical Image-Report Retrieval","author":"Demetrio Deanda, Yuktha Priya Masupalli, Jeong Yang, Young Lee, Zechun Cao, and Gongbo Liang","link":"http://arxiv.org/abs/2501.09134v1","abstract":"Medical images and reports offer invaluable insights into patient health. The\nheterogeneity and complexity of these data hinder effective analysis. To bridge\nthis gap, we investigate contrastive learning models for cross-domain\nretrieval, which associates medical images with their corresponding clinical\nreports. This study benchmarks the robustness of four state-of-the-art\ncontrastive learning models: CLIP, CXR-RePaiR, MedCLIP, and CXR-CLIP. We\nintroduce an occlusion retrieval task to evaluate model performance under\nvarying levels of image corruption. Our findings reveal that all evaluated\nmodels are highly sensitive to out-of-distribution data, as evidenced by the\nproportional decrease in performance with increasing occlusion levels. While\nMedCLIP exhibits slightly more robustness, its overall performance remains\nsignificantly behind CXR-CLIP and CXR-RePaiR. CLIP, trained on a\ngeneral-purpose dataset, struggles with medical image-report retrieval,\nhighlighting the importance of domain-specific training data. The evaluation of\nthis work suggests that more effort needs to be spent on improving the\nrobustness of these models. By addressing these limitations, we can develop\nmore reliable cross-domain retrieval models for medical applications."},{"date":"2025-01","title":"Salient Information Preserving Adversarial Training Improves Clean and Robust Accuracy","author":"Timothy Redgrave, and Adam Czajka","link":"http://arxiv.org/abs/2501.09086v1","abstract":"In this work we introduce Salient Information Preserving Adversarial Training\n(SIP-AT), an intuitive method for relieving the robustness-accuracy trade-off\nincurred by traditional adversarial training. SIP-AT uses salient image regions\nto guide the adversarial training process in such a way that fragile features\ndeemed meaningful by an annotator remain unperturbed during training, allowing\nmodels to learn highly predictive non-robust features without sacrificing\noverall robustness. This technique is compatible with both human-based and\nautomatically generated salience estimates, allowing SIP-AT to be used as a\npart of human-driven model development without forcing SIP-AT to be reliant\nupon additional human data. We perform experiments across multiple datasets and\narchitectures and demonstrate that SIP-AT is able to boost the clean accuracy\nof models while maintaining a high degree of robustness against attacks at\nmultiple epsilon levels. We complement our central experiments with an\nobservational study measuring the rate at which human subjects successfully\nidentify perturbed images. This study helps build a more intuitive\nunderstanding of adversarial attack strength and demonstrates the heightened\nimportance of low-epsilon robustness. Our results demonstrate the efficacy of\nSIP-AT and provide valuable insight into the risks posed by adversarial samples\nof various strengths."},{"date":"2025-01","title":"Improving Stability Estimates in Adversarial Explainable AI through Alternate Search Methods","author":"Christopher Burger, and Charles Walter","link":"http://arxiv.org/abs/2501.09006v1","abstract":"Advances in the effectiveness of machine learning models have come at the\ncost of enormous complexity resulting in a poor understanding of how they\nfunction. Local surrogate methods have been used to approximate the workings of\nthese complex models, but recent work has revealed their vulnerability to\nadversarial attacks where the explanation produced is appreciably different\nwhile the meaning and structure of the complex model's output remains similar.\nThis prior work has focused on the existence of these weaknesses but not on\ntheir magnitude. Here we explore using an alternate search method with the goal\nof finding minimum viable perturbations, the fewest perturbations necessary to\nachieve a fixed similarity value between the original and altered text's\nexplanation. Intuitively, a method that requires fewer perturbations to expose\na given level of instability is inferior to one which requires more. This\nnuance allows for superior comparisons of the stability of explainability\nmethods."},{"date":"2025-01","title":"Exploring ChatGPT for Face Presentation Attack Detection in Zero and Few-Shot in-Context Learning","author":"Alain Komaty, Hatef Otroshi Shahreza, Anjith George, and Sebastien Marcel","link":"http://arxiv.org/abs/2501.08799v1","abstract":"This study highlights the potential of ChatGPT (specifically GPT-4o) as a\ncompetitive alternative for Face Presentation Attack Detection (PAD),\noutperforming several PAD models, including commercial solutions, in specific\nscenarios. Our results show that GPT-4o demonstrates high consistency,\nparticularly in few-shot in-context learning, where its performance improves as\nmore examples are provided (reference data). We also observe that detailed\nprompts enable the model to provide scores reliably, a behavior not observed\nwith concise prompts. Additionally, explanation-seeking prompts slightly\nenhance the model's performance by improving its interpretability. Remarkably,\nthe model exhibits emergent reasoning capabilities, correctly predicting the\nattack type (print or replay) with high accuracy in few-shot scenarios, despite\nnot being explicitly instructed to classify attack types. Despite these\nstrengths, GPT-4o faces challenges in zero-shot tasks, where its performance is\nlimited compared to specialized PAD systems. Experiments were conducted on a\nsubset of the SOTERIA dataset, ensuring compliance with data privacy\nregulations by using only data from consenting individuals. These findings\nunderscore GPT-4o's promise in PAD applications, laying the groundwork for\nfuture research to address broader data privacy concerns and improve\ncross-dataset generalization. Code available here:\nhttps://gitlab.idiap.ch/bob/bob.paper.wacv2025_chatgpt_face_pad"},{"date":"2025-01","title":"Multilingual Email Phishing Attacks Detection using OSINT and Machine Learning","author":"Panharith An, Rana Shafi, Tionge Mughogho, and Onyango Allan Onyango","link":"http://arxiv.org/abs/2501.08723v1","abstract":"Email phishing remains a prevalent cyber threat, targeting victims to extract\nsensitive information or deploy malicious software. This paper explores the\nintegration of open-source intelligence (OSINT) tools and machine learning (ML)\nmodels to enhance phishing detection across multilingual datasets. Using Nmap\nand theHarvester, this study extracted 17 features, including domain names, IP\naddresses, and open ports, to improve detection accuracy. Multilingual email\ndatasets, including English and Arabic, were analyzed to address the\nlimitations of ML models trained predominantly on English data. Experiments\nwith five classification algorithms: Decision Tree, Random Forest, Support\nVector Machine, XGBoost, and Multinomial Na\\\"ive Bayes. It revealed that Random\nForest achieved the highest performance, with an accuracy of 97.37% for both\nEnglish and Arabic datasets. For OSINT-enhanced datasets, the model\ndemonstrated an improvement in accuracy compared to baseline models without\nOSINT features. These findings highlight the potential of combining OSINT tools\nwith advanced ML models to detect phishing emails more effectively across\ndiverse languages and contexts. This study contributes an approach to phishing\ndetection by incorporating OSINT features and evaluating their impact on\nmultilingual datasets, addressing a critical gap in cybersecurity research."},{"date":"2025-01","title":"Tag&Tab: Pretraining Data Detection in Large Language Models Using Keyword-Based Membership Inference Attack","author":"Sagiv Antebi, Edan Habler, Asaf Shabtai, and Yuval Elovici","link":"http://arxiv.org/abs/2501.08454v1","abstract":"Large language models (LLMs) have become essential digital task assistance\ntools. Their training relies heavily on the collection of vast amounts of data,\nwhich may include copyright-protected or sensitive information. Recent studies\non the detection of pretraining data in LLMs have primarily focused on\nsentence-level or paragraph-level membership inference attacks (MIAs), usually\ninvolving probability analysis of the target model prediction tokens. However,\nthe proposed methods often demonstrate poor performance, specifically in terms\nof accuracy, failing to account for the semantic importance of textual content\nand word significance. To address these shortcomings, we propose Tag&Tab, a\nnovel approach for detecting data that has been used as part of the LLM\npretraining. Our method leverages advanced natural language processing (NLP)\ntechniques to tag keywords in the input text - a process we term Tagging. Then,\nthe LLM is used to obtain the probabilities of these keywords and calculate\ntheir average log-likelihood to determine input text membership, a process we\nrefer to as Tabbing. Our experiments on three benchmark datasets (BookMIA,\nMIMIR, and the Pile) and several open-source LLMs of varying sizes demonstrate\nan average increase in the AUC scores ranging from 4.1% to 12.1% over\nstate-of-the-art methods. Tag&Tab not only sets a new standard for data leakage\ndetection in LLMs, but its outstanding performance is a testament to the\nimportance of words in MIAs on LLMs."},{"date":"2025-01","title":"Secure Composition of Quantum Key Distribution and Symmetric Key Encryption","author":"Kunal Dey, and Reihaneh Safavi-Naini","link":"http://arxiv.org/abs/2501.08435v1","abstract":"Quantum key distribution (QKD) allows Alice and Bob to share a secret key\nover an insecure channel with proven information-theoretic security against an\nadversary whose strategy is bounded only by the laws of physics.\nComposability-based security proofs of QKD ensure that using the established\nkey with a one-time-pad encryption scheme provides information theoretic\nsecrecy for the message. In this paper, we consider the problem of using the\nQKD established key with a secure symmetric key-based encryption algorithm and\nuse an approach based on hybrid encryption to provide a proof of security for\nthe composition.\n  Hybrid encryption was first proposed as a public key cryptographic algorithm\nwith proven security for messages of unrestricted length. We use an extension\nof this framework to correlated randomness setting (Sharifian et al. in ISIT\n2021) to propose a quantum-enabled Key Encapsulation Mechanism (qKEM) and\nquantum-enabled hybrid encryption (qHE), and prove a composition theorem for\nthe security of the qHE. We construct a qKEM with proven security using an\nexisting QKD (Portmann et al. in Rev. of Mod. Physics 2022). Using this qKEM\nwith a secure Data Encapsulation Mechanism (DEM), that can be constructed using\na one-time symmetric key encryption scheme, results in an efficient encryption\nsystem for unrestricted length messages with proved security against an\nadversary with access to efficient computations on a quantum computer (i.e.\npost-quantum secure encryption without using any computational assumptions.)"},{"date":"2025-01","title":"Cross-Modal Transferable Image-to-Video Attack on Video Quality Metrics","author":"Georgii Gotin, Ekaterina Shumitskaya, Anastasia Antsiferova, and Dmitriy Vatolin","link":"http://arxiv.org/abs/2501.08415v1","abstract":"Recent studies have revealed that modern image and video quality assessment\n(IQA/VQA) metrics are vulnerable to adversarial attacks. An attacker can\nmanipulate a video through preprocessing to artificially increase its quality\nscore according to a certain metric, despite no actual improvement in visual\nquality. Most of the attacks studied in the literature are white-box attacks,\nwhile black-box attacks in the context of VQA have received less attention.\nMoreover, some research indicates a lack of transferability of adversarial\nexamples generated for one model to another when applied to VQA. In this paper,\nwe propose a cross-modal attack method, IC2VQA, aimed at exploring the\nvulnerabilities of modern VQA models. This approach is motivated by the\nobservation that the low-level feature spaces of images and videos are similar.\nWe investigate the transferability of adversarial perturbations across\ndifferent modalities; specifically, we analyze how adversarial perturbations\ngenerated on a white-box IQA model with an additional CLIP module can\neffectively target a VQA model. The addition of the CLIP module serves as a\nvaluable aid in increasing transferability, as the CLIP model is known for its\neffective capture of low-level semantics. Extensive experiments demonstrate\nthat IC2VQA achieves a high success rate in attacking three black-box VQA\nmodels. We compare our method with existing black-box attack strategies,\nhighlighting its superiority in terms of attack success within the same number\nof iterations and levels of attack strength. We believe that the proposed\nmethod will contribute to the deeper analysis of robust VQA metrics."},{"date":"2025-01","title":"Diffusion Adversarial Post-Training for One-Step Video Generation","author":"Shanchuan Lin, Xin Xia, Yuxi Ren, Ceyuan Yang, Xuefeng Xiao, and Lu Jiang","link":"http://arxiv.org/abs/2501.08316v1","abstract":"The diffusion models are widely used for image and video generation, but\ntheir iterative generation process is slow and expansive. While existing\ndistillation approaches have demonstrated the potential for one-step generation\nin the image domain, they still suffer from significant quality degradation. In\nthis work, we propose Adversarial Post-Training (APT) against real data\nfollowing diffusion pre-training for one-step video generation. To improve the\ntraining stability and quality, we introduce several improvements to the model\narchitecture and training procedures, along with an approximated R1\nregularization objective. Empirically, our experiments show that our\nadversarial post-trained model, Seaweed-APT, can generate 2-second, 1280x720,\n24fps videos in real time using a single forward evaluation step. Additionally,\nour model is capable of generating 1024px images in a single step, achieving\nquality comparable to state-of-the-art methods."},{"date":"2025-01","title":"Towards an End-to-End (E2E) Adversarial Learning and Application in the Physical World","author":"Dudi Biton, Jacob Shams, Satoru Koda, Asaf Shabtai, Yuval Elovici, and Ben Nassi","link":"http://arxiv.org/abs/2501.08258v2","abstract":"The traditional learning process of patch-based adversarial attacks,\nconducted in the digital domain and then applied in the physical domain (e.g.,\nvia printed stickers), may suffer from reduced performance due to adversarial\npatches' limited transferability from the digital domain to the physical\ndomain. Given that previous studies have considered using projectors to apply\nadversarial attacks, we raise the following question: can adversarial learning\n(i.e., patch generation) be performed entirely in the physical domain with a\nprojector? In this work, we propose the Physical-domain Adversarial Patch\nLearning Augmentation (PAPLA) framework, a novel end-to-end (E2E) framework\nthat converts adversarial learning from the digital domain to the physical\ndomain using a projector. We evaluate PAPLA across multiple scenarios,\nincluding controlled laboratory settings and realistic outdoor environments,\ndemonstrating its ability to ensure attack success compared to conventional\ndigital learning-physical application (DL-PA) methods. We also analyze the\nimpact of environmental factors, such as projection surface color, projector\nstrength, ambient light, distance, and angle of the target object relative to\nthe camera, on the effectiveness of projected patches. Finally, we demonstrate\nthe feasibility of the attack against a parked car and a stop sign in a\nreal-world outdoor environment. Our results show that under specific\nconditions, E2E adversarial learning in the physical domain eliminates the\ntransferability issue and ensures evasion by object detectors. Finally, we\nprovide insights into the challenges and opportunities of applying adversarial\nlearning in the physical domain and explain where such an approach is more\neffective than using a sticker."},{"date":"2025-01","title":"CWEval: Outcome-driven Evaluation on Functionality and Security of LLM Code Generation","author":"Jinjun Peng, Leyi Cui, Kele Huang, Junfeng Yang, and Baishakhi Ray","link":"http://arxiv.org/abs/2501.08200v1","abstract":"Large Language Models (LLMs) have significantly aided developers by\ngenerating or assisting in code writing, enhancing productivity across various\ntasks. While identifying incorrect code is often straightforward, detecting\nvulnerabilities in functionally correct code is more challenging, especially\nfor developers with limited security knowledge, which poses considerable\nsecurity risks of using LLM-generated code and underscores the need for robust\nevaluation benchmarks that assess both functional correctness and security.\nCurrent benchmarks like CyberSecEval and SecurityEval attempt to solve it but\nare hindered by unclear and impractical specifications, failing to assess both\nfunctionality and security accurately. To tackle these deficiencies, we\nintroduce CWEval, a novel outcome-driven evaluation framework designed to\nenhance the evaluation of secure code generation by LLMs. This framework not\nonly assesses code functionality but also its security simultaneously with\nhigh-quality task specifications and outcome-driven test oracles which provides\nhigh accuracy. Coupled with CWEval-bench, a multilingual, security-critical\ncoding benchmark, CWEval provides a rigorous empirical security evaluation on\nLLM-generated code, overcoming previous benchmarks' shortcomings. Through our\nevaluations, CWEval reveals a notable portion of functional but insecure code\nproduced by LLMs, and shows a serious inaccuracy of previous evaluations,\nultimately contributing significantly to the field of secure code generation.\nWe open-source our artifact at: https://github.com/Co1lin/CWEval ."},{"date":"2025-01","title":"Energy Backdoor Attack to Deep Neural Networks","author":"Hanene F. Z. Brachemi Meftah, Wassim Hamidouche, Sid Ahmed Fezza, Olivier D\u00e9forges, and Kassem Kallas","link":"http://arxiv.org/abs/2501.08152v1","abstract":"The rise of deep learning (DL) has increased computing complexity and energy\nuse, prompting the adoption of application specific integrated circuits (ASICs)\nfor energy-efficient edge and mobile deployment. However, recent studies have\ndemonstrated the vulnerability of these accelerators to energy attacks. Despite\nthe development of various inference time energy attacks in prior research,\nbackdoor energy attacks remain unexplored. In this paper, we design an\ninnovative energy backdoor attack against deep neural networks (DNNs) operating\non sparsity-based accelerators. Our attack is carried out in two distinct\nphases: backdoor injection and backdoor stealthiness. Experimental results\nusing ResNet-18 and MobileNet-V2 models trained on CIFAR-10 and Tiny ImageNet\ndatasets show the effectiveness of our proposed attack in increasing energy\nconsumption on trigger samples while preserving the model's performance for\nclean/regular inputs. This demonstrates the vulnerability of DNNs to energy\nbackdoor attacks. The source code of our attack is available at:\nhttps://github.com/hbrachemi/energy_backdoor."},{"date":"2025-01","title":"RoHan: Robust Hand Detection in Operation Room","author":"Roi Papo, Sapir Gershov, Tom Friedman, Itay Or, Gil Bolotin, and Shlomi Laufer","link":"http://arxiv.org/abs/2501.08115v2","abstract":"Hand-specific localization has garnered significant interest within the\ncomputer vision community. Although there are numerous datasets with hand\nannotations from various angles and settings, domain transfer techniques\nfrequently struggle in surgical environments. This is mainly due to the limited\navailability of gloved hand instances and the unique challenges of operating\nrooms (ORs). Thus, hand-detection models tailored to OR settings require\nextensive training and expensive annotation processes. To overcome these\nchallenges, we present \"RoHan\" - a novel approach for robust hand detection in\nthe OR, leveraging advanced semi-supervised domain adaptation techniques to\ntackle the challenges of varying recording conditions, diverse glove colors,\nand occlusions common in surgical settings. Our methodology encompasses two\nmain stages: (1) data augmentation strategy that utilizes \"Artificial Gloves,\"\na method for augmenting publicly available hand datasets with synthetic images\nof hands-wearing gloves; (2) semi-supervised domain adaptation pipeline that\nimproves detection performance in real-world OR settings through iterative\nprediction refinement and efficient frame filtering. We evaluate our method\nusing two datasets: simulated enterotomy repair and saphenous vein graft\nharvesting. \"RoHan\" substantially reduces the need for extensive labeling and\nmodel training, paving the way for the practical implementation of hand\ndetection technologies in medical settings."},{"date":"2025-01","title":"CellOMaps: A Compact Representation for Robust Classification of Lung Adenocarcinoma Growth Patterns","author":"Arwa Al-Rubaian, Gozde N. Gunesli, Wajd A. Althakfi, Ayesha Azam, David Snead, Nasir M. Rajpoot, and Shan E Ahmed Raza","link":"http://arxiv.org/abs/2501.08094v1","abstract":"Lung adenocarcinoma (LUAD) is a morphologically heterogeneous disease,\ncharacterized by five primary histological growth patterns. The classification\nof such patterns is crucial due to their direct relation to prognosis but the\nhigh subjectivity and observer variability pose a major challenge. Although\nseveral studies have developed machine learning methods for growth pattern\nclassification, they either only report the predominant pattern per slide or\nlack proper evaluation. We propose a generalizable machine learning pipeline\ncapable of classifying lung tissue into one of the five patterns or as\nnon-tumor. The proposed pipeline's strength lies in a novel compact Cell\nOrganization Maps (cellOMaps) representation that captures the cellular spatial\npatterns from Hematoxylin and Eosin whole slide images (WSIs). The proposed\npipeline provides state-of-the-art performance on LUAD growth pattern\nclassification when evaluated on both internal unseen slides and external\ndatasets, significantly outperforming the current approaches. In addition, our\npreliminary results show that the model's outputs can be used to predict\npatients Tumor Mutational Burden (TMB) levels."},{"date":"2025-01","title":"Robust Low-Light Human Pose Estimation through Illumination-Texture Modulation","author":"Feng Zhang, Ze Li, Xiatian Zhu, and Lei Chen","link":"http://arxiv.org/abs/2501.08038v1","abstract":"As critical visual details become obscured, the low visibility and high ISO\nnoise in extremely low-light images pose a significant challenge to human pose\nestimation. Current methods fail to provide high-quality representations due to\nreliance on pixel-level enhancements that compromise semantics and the\ninability to effectively handle extreme low-light conditions for robust feature\nlearning. In this work, we propose a frequency-based framework for low-light\nhuman pose estimation, rooted in the \"divide-and-conquer\" principle. Instead of\nuniformly enhancing the entire image, our method focuses on task-relevant\ninformation. By applying dynamic illumination correction to the low-frequency\ncomponents and low-rank denoising to the high-frequency components, we\neffectively enhance both the semantic and texture information essential for\naccurate pose estimation. As a result, this targeted enhancement method results\nin robust, high-quality representations, significantly improving pose\nestimation performance. Extensive experiments demonstrating its superiority\nover state-of-the-art methods in various challenging low-light scenarios."},{"date":"2025-01","title":"READ: Reinforcement-based Adversarial Learning for Text Classification with Limited Labeled Data","author":"Rohit Sharma, Shanu Kumar, and Avinash Kumar","link":"http://arxiv.org/abs/2501.08035v1","abstract":"Pre-trained transformer models such as BERT have shown massive gains across\nmany text classification tasks. However, these models usually need enormous\nlabeled data to achieve impressive performances. Obtaining labeled data is\noften expensive and time-consuming, whereas collecting unlabeled data using\nsome heuristics is relatively much cheaper for any task. Therefore, this paper\nproposes a method that encapsulates reinforcement learning-based text\ngeneration and semi-supervised adversarial learning approaches in a novel way\nto improve the model's performance. Our method READ, Reinforcement-based\nAdversarial learning, utilizes an unlabeled dataset to generate diverse\nsynthetic text through reinforcement learning, improving the model's\ngeneralization capability using adversarial learning. Our experimental results\nshow that READ outperforms the existing state-of-art methods on multiple\ndatasets."},{"date":"2025-01","title":"Self-Instruct Few-Shot Jailbreaking: Decompose the Attack into Pattern and Behavior Learning","author":"Jiaqi Hua, and Wanxu Wei","link":"http://arxiv.org/abs/2501.07959v1","abstract":"Recently, several works have been conducted on jailbreaking Large Language\nModels (LLMs) with few-shot malicious demos. In particular, Zheng et al. (2024)\nfocuses on improving the efficiency of Few-Shot Jailbreaking (FSJ) by injecting\nspecial tokens into the demos and employing demo-level random search.\nNevertheless, this method lacks generality since it specifies the\ninstruction-response structure. Moreover, the reason why inserting special\ntokens takes effect in inducing harmful behaviors is only empirically\ndiscussed. In this paper, we take a deeper insight into the mechanism of\nspecial token injection and propose Self-Instruct Few-Shot Jailbreaking\n(Self-Instruct-FSJ) facilitated with the demo-level greedy search. This\nframework decomposes the FSJ attack into pattern and behavior learning to\nexploit the model's vulnerabilities in a more generalized and efficient way. We\nconduct elaborate experiments to evaluate our method on common open-source\nmodels and compare it with baseline algorithms. Our code is available at\nhttps://github.com/iphosi/Self-Instruct-FSJ."},{"date":"2025-01","title":"Robust Hyperspectral Image Panshapring via Sparse Spatial-Spectral Representation","author":"Chia-Ming Lee, Yu-Fan Lin, Li-Wei Kang, and Chih-Chung Hsu","link":"http://arxiv.org/abs/2501.07953v1","abstract":"High-resolution hyperspectral imaging plays a crucial role in various remote\nsensing applications, yet its acquisition often faces fundamental limitations\ndue to hardware constraints. This paper introduces S$^{3}$RNet, a novel\nframework for hyperspectral image pansharpening that effectively combines\nlow-resolution hyperspectral images (LRHSI) with high-resolution multispectral\nimages (HRMSI) through sparse spatial-spectral representation. The core of\nS$^{3}$RNet is the Multi-Branch Fusion Network (MBFN), which employs parallel\nbranches to capture complementary features at different spatial and spectral\nscales. Unlike traditional approaches that treat all features equally, our\nSpatial-Spectral Attention Weight Block (SSAWB) dynamically adjusts feature\nweights to maintain sparse representation while suppressing noise and\nredundancy. To enhance feature propagation, we incorporate the Dense Feature\nAggregation Block (DFAB), which efficiently aggregates inputted features\nthrough dense connectivity patterns. This integrated design enables S$^{3}$RNet\nto selectively emphasize the most informative features from differnt scale\nwhile maintaining computational efficiency. Comprehensive experiments\ndemonstrate that S$^{3}$RNet achieves state-of-the-art performance across\nmultiple evaluation metrics, showing particular strength in maintaining high\nreconstruction quality even under challenging noise conditions. The code will\nbe made publicly available."},{"date":"2025-01","title":"Gandalf the Red: Adaptive Security for LLMs","author":"Niklas Pfister, V\u00e1clav Volhejn, Manuel Knott, Santiago Arias, Julia Bazi\u0144ska, Mykhailo Bichurin, Alan Commike, Janet Darling, Peter Dienes, Matthew Fiedler, David Haber, Matthias Kraft, Marco Lancini, Max Mathys, Dami\u00e1n Pascual-Ortiz, Jakub Podolak, Adri\u00e0 Romero-L\u00f3pez, Kyriacos Shiarlis, Andreas Signer, Zsolt Terek, Athanasios Theocharis, Daniel Timbrell, Samuel Trautwein, Samuel Watts, Natalie Wu, and Mateo Rojas-Carulla","link":"http://arxiv.org/abs/2501.07927v1","abstract":"Current evaluations of defenses against prompt attacks in large language\nmodel (LLM) applications often overlook two critical factors: the dynamic\nnature of adversarial behavior and the usability penalties imposed on\nlegitimate users by restrictive defenses. We propose D-SEC (Dynamic Security\nUtility Threat Model), which explicitly separates attackers from legitimate\nusers, models multi-step interactions, and rigorously expresses the\nsecurity-utility in an optimizable form. We further address the shortcomings in\nexisting evaluations by introducing Gandalf, a crowd-sourced, gamified\nred-teaming platform designed to generate realistic, adaptive attack datasets.\nUsing Gandalf, we collect and release a dataset of 279k prompt attacks.\nComplemented by benign user data, our analysis reveals the interplay between\nsecurity and utility, showing that defenses integrated in the LLM (e.g., system\nprompts) can degrade usability even without blocking requests. We demonstrate\nthat restricted application domains, defense-in-depth, and adaptive defenses\nare effective strategies for building secure and useful LLM applications. Code\nis available at\n\\href{https://github.com/lakeraai/dsec-gandalf}{\\texttt{https://github.com/lakeraai/dsec-gandalf}}."},{"date":"2025-01","title":"VENOM: Text-driven Unrestricted Adversarial Example Generation with Diffusion Models","author":"Hui Kuurila-Zhang, Haoyu Chen, and Guoying Zhao","link":"http://arxiv.org/abs/2501.07922v1","abstract":"Adversarial attacks have proven effective in deceiving machine learning\nmodels by subtly altering input images, motivating extensive research in recent\nyears. Traditional methods constrain perturbations within $l_p$-norm bounds,\nbut advancements in Unrestricted Adversarial Examples (UAEs) allow for more\ncomplex, generative-model-based manipulations. Diffusion models now lead UAE\ngeneration due to superior stability and image quality over GANs. However,\nexisting diffusion-based UAE methods are limited to using reference images and\nface challenges in generating Natural Adversarial Examples (NAEs) directly from\nrandom noise, often producing uncontrolled or distorted outputs. In this work,\nwe introduce VENOM, the first text-driven framework for high-quality\nunrestricted adversarial examples generation through diffusion models. VENOM\nunifies image content generation and adversarial synthesis into a single\nreverse diffusion process, enabling high-fidelity adversarial examples without\nsacrificing attack success rate (ASR). To stabilize this process, we\nincorporate an adaptive adversarial guidance strategy with momentum, ensuring\nthat the generated adversarial examples $x^*$ align with the distribution\n$p(x)$ of natural images. Extensive experiments demonstrate that VENOM achieves\nsuperior ASR and image quality compared to prior methods, marking a significant\nadvancement in adversarial example generation and providing insights into model\nvulnerabilities for improved defense development."},{"date":"2025-01","title":"A Comparative Analysis of DNN-based White-Box Explainable AI Methods in Network Security","author":"Osvaldo Arreche, and Mustafa Abdallah","link":"http://arxiv.org/abs/2501.07801v1","abstract":"New research focuses on creating artificial intelligence (AI) solutions for\nnetwork intrusion detection systems (NIDS), drawing its inspiration from the\never-growing number of intrusions on networked systems, increasing its\ncomplexity and intelligibility. Hence, the use of explainable AI (XAI)\ntechniques in real-world intrusion detection systems comes from the requirement\nto comprehend and elucidate black-box AI models to security analysts. In an\neffort to meet such requirements, this paper focuses on applying and evaluating\nWhite-Box XAI techniques (particularly LRP, IG, and DeepLift) for NIDS via an\nend-to-end framework for neural network models, using three widely used network\nintrusion datasets (NSL-KDD, CICIDS-2017, and RoEduNet-SIMARGL2021), assessing\nits global and local scopes, and examining six distinct assessment measures\n(descriptive accuracy, sparsity, stability, robustness, efficiency, and\ncompleteness). We also compare the performance of white-box XAI methods with\nblack-box XAI methods. The results show that using White-box XAI techniques\nscores high in robustness and completeness, which are crucial metrics for IDS.\nMoreover, the source codes for the programs developed for our XAI evaluation\nframework are available to be improved and used by the research community."},{"date":"2025-01","title":"Deep Learning for Disease Outbreak Prediction: A Robust Early Warning Signal for Transcritical Bifurcations","author":"Reza Miry, Amit K. Chakraborty, Russell Greiner, Mark A. Lewis, Hao Wang, Tianyu Guan, and Pouria Ramazi","link":"http://arxiv.org/abs/2501.07764v1","abstract":"Early Warning Signals (EWSs) are vital for implementing preventive measures\nbefore a disease turns into a pandemic. While new diseases exhibit unique\nbehaviors, they often share fundamental characteristics from a dynamical\nsystems perspective. Moreover, measurements during disease outbreaks are often\ncorrupted by different noise sources, posing challenges for Time Series\nClassification (TSC) tasks. In this study, we address the problem of having a\nrobust EWS for disease outbreak prediction using a best-performing deep\nlearning model in the domain of TSC. We employed two simulated datasets to\ntrain the model: one representing generated dynamical systems with randomly\nselected polynomial terms to model new disease behaviors, and another\nsimulating noise-induced disease dynamics to account for noisy measurements.\nThe model's performance was analyzed using both simulated data from different\ndisease models and real-world data, including influenza and COVID-19. Results\ndemonstrate that the proposed model outperforms previous models, effectively\nproviding EWSs of impending outbreaks across various scenarios. This study\nbridges advancements in deep learning with the ability to provide robust early\nwarning signals in noisy environments, making it highly applicable to\nreal-world crises involving emerging disease outbreaks."},{"date":"2025-01","title":"Distributed Identity for Zero Trust and Segmented Access Control: A Novel Approach to Securing Network Infrastructure","author":"Sina Ahmadi","link":"http://arxiv.org/abs/2501.09032v1","abstract":"\"Distributed Identity\" refers to the transition from centralized identity\nsystems using Decentralized Identifiers (DID) and Verifiable Credentials (VC)\nfor secure and privacy-preserving authentications. With distributed identity,\ncontrol of identity data is returned to the user, making credential-based\nattacks impossible due to the lack of a single point of failure. This study\nassesses the security improvements achieved when distributed identity is\nemployed with the ZTA principle, particularly concerning lateral movements\nwithin segmented networks. It also considers areas such as the implementation\nspecifications of the framework, the advantages and disadvantages of the method\nto organizations, and the issues of compatibility and generalizability.\nFurthermore, the study highlights privacy and regulatory compliance, including\nthe General Data Protection Regulation (GDPR) and California Consumer Data\nPrivacy Act (CCPA), analyzing potential solutions to these problems. The study\nimplies that adopting distributed identities can enhance overall security\npostures by an order of magnitude, providing contextual and least-privilege\nauthorization and user privacy. The research recommends refining technical\nstandards, expanding the use of distributed identity in practice, and\ndiscussing its applications for the contemporary digital security landscape."},{"date":"2025-01","title":"A Review on the Security Vulnerabilities of the IoMT against Malware Attacks and DDoS","author":"Lily Dzamesi, and Nelly Elsayed","link":"http://arxiv.org/abs/2501.07703v1","abstract":"The Internet of Medical Things (IoMT) has transformed the healthcare industry\nby connecting medical devices in monitoring treatment outcomes of patients.\nThis increased connectivity has resulted to significant security\nvulnerabilities in the case of malware and Distributed Denial of Service (DDoS)\nattacks. This literature review examines the vulnerabilities of IoMT devices,\nfocusing on critical threats and exploring mitigation strategies. We conducted\na comprehensive search across leading databases such as ACM Digital Library,\nIEEE Xplore, and Elsevier to analyze peer-reviewed studies published within the\nlast five years (from 2019 to 2024). The review shows that inadequate\nencryption protocols, weak authentication methods, and irregular firmware\nupdates are the main causes of risks associated with IoMT devices. We have\nidentified emerging solutions like machine learning algorithms, blockchain\ntechnology, and edge computing as promising approaches to enhance IoMT\nsecurity. This review emphasizes the pressing need to develop lightweight\nsecurity measures and standardized protocols to protect patient data and ensure\nthe integrity of healthcare services."},{"date":"2025-01","title":"Masking Countermeasures Against Side-Channel Attacks on Quantum Computers","author":"Jason T. LeGrow, Travis Morrison, Jamie Sikora, and Nicolas Swanson","link":"http://arxiv.org/abs/2501.07695v1","abstract":"We propose a modification to the transpiler of a quantum computer to\nsafeguard against side-channel attacks aimed at learning information about a\nquantum circuit. We demonstrate that if it is feasible to shield a specific\nsubset of gates from side-channel attacks, then it is possible to conceal all\ninformation in a quantum circuit by transpiling it into a new circuit whose\ndepth grows linearly, depending on the quantum computer's architecture. We\nprovide concrete examples of implementing this protection on IBM's quantum\ncomputers, utilizing their virtual gates and editing their transpiler."},{"date":"2025-01","title":"Exploring and Mitigating Adversarial Manipulation of Voting-Based Leaderboards","author":"Yangsibo Huang, Milad Nasr, Anastasios Angelopoulos, Nicholas Carlini, Wei-Lin Chiang, Christopher A. Choquette-Choo, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Ken Ziyu Liu, Ion Stoica, Florian Tramer, and Chiyuan Zhang","link":"http://arxiv.org/abs/2501.07493v1","abstract":"It is now common to evaluate Large Language Models (LLMs) by having humans\nmanually vote to evaluate model outputs, in contrast to typical benchmarks that\nevaluate knowledge or skill at some particular task. Chatbot Arena, the most\npopular benchmark of this type, ranks models by asking users to select the\nbetter response between two randomly selected models (without revealing which\nmodel was responsible for the generations). These platforms are widely trusted\nas a fair and accurate measure of LLM capabilities. In this paper, we show that\nif bot protection and other defenses are not implemented, these voting-based\nbenchmarks are potentially vulnerable to adversarial manipulation.\nSpecifically, we show that an attacker can alter the leaderboard (to promote\ntheir favorite model or demote competitors) at the cost of roughly a thousand\nvotes (verified in a simulated, offline version of Chatbot Arena). Our attack\nconsists of two steps: first, we show how an attacker can determine which model\nwas used to generate a given reply with more than $95\\%$ accuracy; and then,\nthe attacker can use this information to consistently vote for (or against) a\ntarget model. Working with the Chatbot Arena developers, we identify, propose,\nand implement mitigations to improve the robustness of Chatbot Arena against\nadversarial manipulation, which, based on our analysis, substantially increases\nthe cost of such attacks. Some of these defenses were present before our\ncollaboration, such as bot protection with Cloudflare, malicious user\ndetection, and rate limiting. Others, including reCAPTCHA and login are being\nintegrated to strengthen the security in Chatbot Arena."},{"date":"2025-01","title":"Encrypted Computation of Collision Probability for Secure Satellite Conjunction Analysis","author":"Jihoon Suh, Michael Hibbard, Kaoru Teranishi, Takashi Tanaka, Moriba Jah, and Maruthi Akella","link":"http://arxiv.org/abs/2501.07476v1","abstract":"The computation of collision probability ($\\mathcal{P}_c$) is crucial for\nspace environmentalism and sustainability by providing decision-making\nknowledge that can prevent collisions between anthropogenic space objects.\nHowever, the accuracy and precision of $\\mathcal{P}_c$ computations is often\ncompromised by limitations in computational resources and data availability.\nWhile significant improvements have been made in the computational aspects, the\nrising concerns regarding the privacy of collaborative data sharing can be a\nmajor limiting factor in the future conjunction analysis and risk assessment,\nespecially as the space environment grows increasingly privatized, competitive,\nand fraught with conflicting strategic interests. This paper argues that the\nimportance of privacy measures in space situational awareness (SSA) is\nunderappreciated, and regulatory and compliance measures currently in place are\nnot sufficient by themselves, presenting a significant gap.\n  To address this gap, we introduce a novel encrypted architecture that\nleverages advanced cryptographic techniques, including homomorphic encryption\n(HE) and multi-party computation (MPC), to safeguard the privacy of entities\ncomputing space sustainability metrics, inter alia, $\\mathcal{P}_c$. Our\nproposed protocol, Encrypted $\\mathcal{P}_c$, integrates the Monte Carlo\nestimation algorithm with cryptographic solutions, enabling secure collision\nprobability computation without exposing sensitive or proprietary information.\nThis research advances secure conjunction analysis by developing a secure MPC\nprotocol for $\\mathcal{P}_c$ computation and highlights the need for innovative\nprotocols to ensure a more secure and cooperative SSA landscape."},{"date":"2025-01","title":"Am I Infected? Lessons from Operating a Large-Scale IoT Security Diagnostic Service","author":"Takayuki Sasaki, Tomoya Inazawa, Youhei Yamaguchi, Simon Parkin, Michel van Eeten, Katsunari Yoshioka, and Tsutomu Matsumoto","link":"http://arxiv.org/abs/2501.07326v1","abstract":"There is an expectation that users of home IoT devices will be able to secure\nthose devices, but they may lack information about what they need to do. In\nFebruary 2022, we launched a web service that scans users' IoT devices to\ndetermine how secure they are. The service aims to diagnose and remediate\nvulnerabilities and malware infections of IoT devices of Japanese users. This\npaper reports on findings from operating this service drawn from three studies:\n(1) the engagement of 114,747 users between February, 2022 - May, 2024; (2) a\nlarge-scale evaluation survey among service users (n=4,103), and; (3) an\ninvestigation and targeted survey (n=90) around the remediation actions of\nusers of non-secure devices. During the operation, we notified 417 (0.36%)\nusers that one or more of their devices were detected as vulnerable, and 171\n(0.15%) users that one of their devices was infected with malware. The service\nfound no issues for 99% of users. Still, 96% of all users evaluated the service\npositively, most often for it providing reassurance, being free of charge, and\nshort diagnosis time. Of the 171 users with malware infections, 67 returned to\nthe service later for a new check, with 59 showing improvement. Of the 417\nusers with vulnerable devices, 151 users revisited and re-diagnosed, where 75\nshowed improvement. We report on lessons learned, including a consideration of\nthe capabilities that non-expert users will assume of a security scan."},{"date":"2025-01","title":"Generating Poisoning Attacks against Ridge Regression Models with Categorical Features","author":"Monse Guedes-Ayala, Lars Schewe, Zeynep Suvak, and Miguel Anjos","link":"http://arxiv.org/abs/2501.07275v1","abstract":"Machine Learning (ML) models have become a very powerful tool to extract\ninformation from large datasets and use it to make accurate predictions and\nautomated decisions. However, ML models can be vulnerable to external attacks,\ncausing them to underperform or deviate from their expected tasks. One way to\nattack ML models is by injecting malicious data to mislead the algorithm during\nthe training phase, which is referred to as a poisoning attack. We can prepare\nfor such situations by designing anticipated attacks, which are later used for\ncreating and testing defence strategies. In this paper, we propose an algorithm\nto generate strong poisoning attacks for a ridge regression model containing\nboth numerical and categorical features that explicitly models and poisons\ncategorical features. We model categorical features as SOS-1 sets and formulate\nthe problem of designing poisoning attacks as a bilevel optimization problem\nthat is nonconvex mixed-integer in the upper-level and unconstrained convex\nquadratic in the lower-level. We present the mathematical formulation of the\nproblem, introduce a single-level reformulation based on the Karush-Kuhn-Tucker\n(KKT) conditions of the lower level, find bounds for the lower-level variables\nto accelerate solver performance, and propose a new algorithm to poison\ncategorical features. Numerical experiments show that our method improves the\nmean squared error of all datasets compared to the previous benchmark in the\nliterature."},{"date":"2025-01","title":"MOS-Attack: A Scalable Multi-objective Adversarial Attack Framework","author":"Ping Guo, Cheng Gong, Xi Lin, Fei Liu, Zhichao Lu, Qingfu Zhang, and Zhenkun Wang","link":"http://arxiv.org/abs/2501.07251v1","abstract":"Crafting adversarial examples is crucial for evaluating and enhancing the\nrobustness of Deep Neural Networks (DNNs), presenting a challenge equivalent to\nmaximizing a non-differentiable 0-1 loss function.\n  However, existing single objective methods, namely adversarial attacks focus\non a surrogate loss function, do not fully harness the benefits of engaging\nmultiple loss functions, as a result of insufficient understanding of their\nsynergistic and conflicting nature.\n  To overcome these limitations, we propose the Multi-Objective Set-based\nAttack (MOS Attack), a novel adversarial attack framework leveraging multiple\nloss functions and automatically uncovering their interrelations.\n  The MOS Attack adopts a set-based multi-objective optimization strategy,\nenabling the incorporation of numerous loss functions without additional\nparameters.\n  It also automatically mines synergistic patterns among various losses,\nfacilitating the generation of potent adversarial attacks with fewer\nobjectives.\n  Extensive experiments have shown that our MOS Attack outperforms\nsingle-objective attacks. Furthermore, by harnessing the identified synergistic\npatterns, MOS Attack continues to show superior results with a reduced number\nof loss functions."},{"date":"2025-01","title":"A Secure Remote Password Protocol From The Learning With Errors Problem","author":"Huapeng Li, and Baocheng Wang","link":"http://arxiv.org/abs/2501.07208v1","abstract":"Secure Remote Password (SRP) protocol is an essential password-authenticated\nkey exchange (PAKE) protocol based on the discrete logarithm problem (DLP). The\nprotocol is specifically designed to obtain a session key and it has been\nwidely used in various scenarios due to its attractive security features. In\nthe SRP protocol, the server is not required to save any data directly\nassociated with passwords. And this makes attackers who manage to corrupt the\nserver fail to impersonate the client unless performing a brute-force search\nfor the password. However, the development of quantum computing has potentially\nmade classic DLP-based public-key cryptography schemes not secure, including\nthe SRP protocol. So it is significant to design a quantum-resistant SRP\nprotocol. In this paper, based on the original scheme, we propose a\npost-quantum SRP protocol from the learning with errors (LWE) problem. And we\ngive rigorous proof and analyses on the correctness and security of the scheme.\nBesides being resistant to known quantum attacks, it maintains the various\nsecure qualities of the original protocol."},{"date":"2025-01","title":"Beyond Security-by-design: Securing a compromised system","author":"Awais Rashid, Sana Belguith, Matthew Bradbury, Sadie Creese, Ivan Flechais, and Neeraj Suri","link":"http://arxiv.org/abs/2501.07207v1","abstract":"Digital infrastructures are seeing convergence and connectivity at\nunprecedented scale. This is true for both current critical national\ninfrastructures and emerging future systems that are highly cyber-physical in\nnature with complex intersections between humans and technologies, e.g., smart\ncities, intelligent transportation, high-value manufacturing and Industry 4.0.\nDiverse legacy and non-legacy software systems underpinned by heterogeneous\nhardware compose on-the-fly to deliver services to millions of users with\nvarying requirements and unpredictable actions. This complexity is compounded\nby intricate and complicated supply-chains with many digital assets and\nservices outsourced to third parties. The reality is that, at any particular\npoint in time, there will be untrusted, partially-trusted or compromised\nelements across the infrastructure. Given this reality, and the societal scale\nof digital infrastructures, delivering secure and resilient operations is a\nmajor challenge. We argue that this requires us to move beyond the paradigm of\nsecurity-by-design and embrace the challenge of securing-a-compromised-system."},{"date":"2025-01","title":"Generalizable Graph Neural Networks for Robust Power Grid Topology Control","author":"Matthijs de Jong, Jan Viebahn, and Yuliya Shapovalova","link":"http://arxiv.org/abs/2501.07186v1","abstract":"The energy transition necessitates new congestion management methods. One\nsuch method is controlling the grid topology with machine learning (ML). This\napproach has gained popularity following the Learning to Run a Power Network\n(L2RPN) competitions. Graph neural networks (GNNs) are a class of ML models\nthat reflect graph structure in their computation, which makes them suitable\nfor power grid modeling. Various GNN approaches for topology control have thus\nbeen proposed. We propose the first GNN model for grid topology control that\nuses only GNN layers. Additionally, we identify the busbar information\nasymmetry problem that the popular homogeneous graph representation suffers\nfrom, and propose a heterogeneous graph representation to resolve it. We train\nboth homogeneous and heterogeneous GNNs and fully connected neural networks\n(FCNN) baselines on an imitation learning task. We evaluate the models\naccording to their classification accuracy and grid operation ability. We find\nthat the heterogeneous GNNs perform best on in-distribution networks, followed\nby the FCNNs, and lastly, the homogeneous GNNs. We also find that both GNN\ntypes generalize better to out-of-distribution networks than FCNNs."},{"date":"2025-01","title":"Robust Single Object Tracking in LiDAR Point Clouds under Adverse Weather Conditions","author":"Xiantong Zhao, Xiuping Liu, Shengjing Tian, and Yinan Han","link":"http://arxiv.org/abs/2501.07133v1","abstract":"3D single object tracking (3DSOT) in LiDAR point clouds is a critical task\nfor outdoor perception, enabling real-time perception of object location,\norientation, and motion. Despite the impressive performance of current 3DSOT\nmethods, evaluating them on clean datasets inadequately reflects their\ncomprehensive performance, as the adverse weather conditions in real-world\nsurroundings has not been considered. One of the main obstacles is the lack of\nadverse weather benchmarks for the evaluation of 3DSOT. To this end, this work\nproposes a challenging benchmark for LiDAR-based 3DSOT in adverse weather,\nwhich comprises two synthetic datasets (KITTI-A and nuScenes-A) and one\nreal-world dataset (CADC-SOT) spanning three weather types: rain, fog, and\nsnow. Based on this benchmark, five representative 3D trackers from different\ntracking frameworks conducted robustness evaluation, resulting in significant\nperformance degradations. This prompts the question: What are the factors that\ncause current advanced methods to fail on such adverse weather samples?\nConsequently, we explore the impacts of adverse weather and answer the above\nquestion from three perspectives: 1) target distance; 2) template shape\ncorruption; and 3) target shape corruption. Finally, based on domain\nrandomization and contrastive learning, we designed a dual-branch tracking\nframework for adverse weather, named DRCT, achieving excellent performance in\nbenchmarks."},{"date":"2025-01","title":"Beyond the Surface: An NLP-based Methodology to Automatically Estimate CVE Relevance for CAPEC Attack Patterns","author":"Silvia Bonomi, Andrea Ciavotta, Simone Lenti, and Alessandro Palma","link":"http://arxiv.org/abs/2501.07131v1","abstract":"Threat analysis is continuously growing in importance due to the\nalways-increasing complexity and frequency of cyber attacks. Analyzing threats\ndemands significant effort from security experts, leading to delays in the\nsecurity analysis process. Different cybersecurity knowledge bases are\ncurrently available to support this task but manual efforts are often required\nto correlate such heterogenous sources into a unified view that would enable a\nmore comprehensive assessment. To address this gap, we propose a methodology\nleveraging Natural Language Processing (NLP) to effectively and efficiently\nassociate Common Vulnerabilities and Exposure (CVE) vulnerabilities with Common\nAttack Pattern Enumeration and Classification (CAPEC) attack patterns. The\nproposed technique combines semantic similarity with keyword analysis to\nimprove the accuracy of association estimations. Experimental evaluations\ndemonstrate superior performance compared to state-of-the-art models, reducing\nmanual effort and analysis time, and enabling cybersecurity professionals to\nprioritize critical tasks."},{"date":"2025-01","title":"SFC-GAN: A Generative Adversarial Network for Brain Functional and Structural Connectome Translation","author":"Yee-Fan Tan, Jun Lin Liow, Pei-Sze Tan, Fuad Noman, Raphael C. -W. Phan, Hernando Ombao, and Chee-Ming Ting","link":"http://arxiv.org/abs/2501.07055v1","abstract":"Modern brain imaging technologies have enabled the detailed reconstruction of\nhuman brain connectomes, capturing structural connectivity (SC) from diffusion\nMRI and functional connectivity (FC) from functional MRI. Understanding the\nintricate relationships between SC and FC is vital for gaining deeper insights\ninto the brain's functional and organizational mechanisms. However, obtaining\nboth SC and FC modalities simultaneously remains challenging, hindering\ncomprehensive analyses. Existing deep generative models typically focus on\nsynthesizing a single modality or unidirectional translation between FC and SC,\nthereby missing the potential benefits of bi-directional translation,\nespecially in scenarios where only one connectome is available. Therefore, we\npropose Structural-Functional Connectivity GAN (SFC-GAN), a novel framework for\nbidirectional translation between SC and FC. This approach leverages the\nCycleGAN architecture, incorporating convolutional layers to effectively\ncapture the spatial structures of brain connectomes. To preserve the\ntopological integrity of these connectomes, we employ a structure-preserving\nloss that guides the model in capturing both global and local connectome\npatterns while maintaining symmetry. Our framework demonstrates superior\nperformance in translating between SC and FC, outperforming baseline models in\nsimilarity and graph property evaluations compared to ground truth data, each\ntranslated modality can be effectively utilized for downstream classification."},{"date":"2025-01","title":"Protego: Detecting Adversarial Examples for Vision Transformers via Intrinsic Capabilities","author":"Jialin Wu, Kaikai Pan, Yanjiao Chen, Jiangyi Deng, Shengyuan Pang, and Wenyuan Xu","link":"http://arxiv.org/abs/2501.07044v1","abstract":"Transformer models have excelled in natural language tasks, prompting the\nvision community to explore their implementation in computer vision problems.\nHowever, these models are still influenced by adversarial examples. In this\npaper, we investigate the attack capabilities of six common adversarial attacks\non three pretrained ViT models to reveal the vulnerability of ViT models. To\nunderstand and analyse the bias in neural network decisions when the input is\nadversarial, we use two visualisation techniques that are attention rollout and\ngrad attention rollout. To prevent ViT models from adversarial attack, we\npropose Protego, a detection framework that leverages the transformer intrinsic\ncapabilities to detection adversarial examples of ViT models. Nonetheless, this\nis challenging due to a diversity of attack strategies that may be adopted by\nadversaries. Inspired by the attention mechanism, we know that the token of\nprediction contains all the information from the input sample. Additionally,\nthe attention region for adversarial examples differs from that of normal\nexamples. Given these points, we can train a detector that achieves superior\nperformance than existing detection methods to identify adversarial examples.\nOur experiments have demonstrated the high effectiveness of our detection\nmethod. For these six adversarial attack methods, our detector's AUC scores all\nexceed 0.95. Protego may advance investigations in metaverse security."},{"date":"2025-01","title":"Hybrid Scheme of Post-Quantum Cryptography and Elliptic-Curve Cryptography for Certificates -- A Case Study of Security Credential Management System in Vehicle-to-Everything Communications","author":"Abel C. H. Chen, and Bon-Yeh Lin","link":"http://arxiv.org/abs/2501.07028v1","abstract":"Due to the current standard of Security Credential Management System (SCMS)\nfor Vehicle-to-Everything (V2X) communications using asymmetric cryptography,\nspecifically Elliptic-Curve Cryptography (ECC), which may be vulnerable to\nquantum computing attacks. Therefore, the V2X SCMS is threatened by quantum\ncomputing attacks. However, although the National Institute of Standards and\nTechnology (NIST) has already selected Post-Quantum Cryptography (PQC)\nalgorithms as the standard, the current PQC algorithms may have issues such as\nlonger public key lengths, longer signature lengths, or lower signature\ngeneration and verification efficiency, which may not fully meet the\nrequirements of V2X communication applications. In view of the challenges in\nV2X communication, such as packet length, signature generation and verification\nefficiency, security level, and vehicle privacy, this study proposes a hybrid\ncertificate scheme of PQC and ECC. By leveraging the strengths of both PQC and\nECC, this scheme aims to overcome the challenges in V2X communication. PQC is\nused to establish a security level resistant to quantum computing attacks,\nwhile ECC is utilized to establish anonymous certificates and reduce packet\nlength to meet the requirements of V2X communication. In the practical\nexperiments, the study implemented the SCMS end entity based on the Chunghwa\nTelecom SCMS and the Clientron On-Board Unit (OBU) to conduct field tests in\nDanhai New Town in New Taipei City. The performance of various existing hybrid\ncertificate schemes combining PQC (e.g., Dilithium, Falcon, and SPHINCS+) and\nECC is compared, and a practical solution is provided for V2X industries."},{"date":"2025-01","title":"Layer-Wise Security Framework and Analysis for the Quantum Internet","author":"Zebo Yang, Ali Ghubaish, Raj Jain, Ala Al-Fuqaha, Aiman Erbad, Ramana Kompella, Hassan Shapourian, and Reza Nejabati","link":"http://arxiv.org/abs/2501.06989v1","abstract":"With its significant security potential, the quantum internet is poised to\nrevolutionize technologies like cryptography and communications. Although it\nboasts enhanced security over traditional networks, the quantum internet still\nencounters unique security challenges essential for safeguarding its\nConfidentiality, Integrity, and Availability (CIA). This study explores these\nchallenges by analyzing the vulnerabilities and the corresponding mitigation\nstrategies across different layers of the quantum internet, including physical,\nlink, network, and application layers. We assess the severity of potential\nattacks, evaluate the expected effectiveness of mitigation strategies, and\nidentify vulnerabilities within diverse network configurations, integrating\nboth classical and quantum approaches. Our research highlights the dynamic\nnature of these security issues and emphasizes the necessity for adaptive\nsecurity measures. The findings underline the need for ongoing research into\nthe security dimension of the quantum internet to ensure its robustness,\nencourage its adoption, and maximize its impact on society."},{"date":"2025-01","title":"ByzSFL: Achieving Byzantine-Robust Secure Federated Learning with Zero-Knowledge Proofs","author":"Yongming Fan, Rui Zhu, Zihao Wang, Chenghong Wang, Haixu Tang, Ye Dong, Hyunghoon Cho, and Lucila Ohno-Machado","link":"http://arxiv.org/abs/2501.06953v1","abstract":"The advancement of AI models, especially those powered by deep learning,\nfaces significant challenges in data-sensitive industries like healthcare and\nfinance due to the distributed and private nature of data. Federated Learning\n(FL) and Secure Federated Learning (SFL) enable collaborative model training\nwithout data sharing, enhancing privacy by encrypting shared intermediate\nresults. However, SFL currently lacks effective Byzantine robustness, a\ncritical property that ensures model performance remains intact even when some\nparticipants act maliciously. Existing Byzantine-robust methods in FL are\nincompatible with SFL due to the inefficiency and limitations of encryption\noperations in handling complex aggregation calculations. This creates a\nsignificant gap in secure and robust model training.\n  To address this gap, we propose ByzSFL, a novel SFL system that achieves\nByzantine-robust secure aggregation with high efficiency. Our approach offloads\naggregation weight calculations to individual parties and introduces a\npractical zero-knowledge proof (ZKP) protocol toolkit. This toolkit supports\nwidely used operators for calculating aggregation weights, ensuring correct\ncomputations without compromising data privacy. Not only does this method\nmaintain aggregation integrity, but it also significantly boosts computational\nefficiency, making ByzSFL approximately 100 times faster than existing\nsolutions. Furthermore, our method aligns with open-source AI trends, enabling\nplaintext publication of the final model without additional information\nleakage, thereby enhancing the practicality and robustness of SFL in real-world\napplications."},{"date":"2025-01","title":"Super-Resolution of 3D Micro-CT Images Using Generative Adversarial Networks: Enhancing Resolution and Segmentation Accuracy","author":"Evgeny Ugolkov, Xupeng He, Hyung Kwak, and Hussein Hoteit","link":"http://arxiv.org/abs/2501.06939v1","abstract":"We develop a procedure for substantially improving the quality of segmented\n3D micro-Computed Tomography (micro-CT) images of rocks with a Machine Learning\n(ML) Generative Model. The proposed model enhances the resolution eightfold\n(8x) and addresses segmentation inaccuracies due to the overlapping X-ray\nattenuation in micro-CT measurement for different rock minerals and phases. The\nproposed generative model is a 3D Deep Convolutional Wasserstein Generative\nAdversarial Network with Gradient Penalty (3D DC WGAN-GP). The algorithm is\ntrained on segmented 3D low-resolution micro-CT images and segmented unpaired\ncomplementary 2D high-resolution Laser Scanning Microscope (LSM) images. The\nalgorithm was demonstrated on multiple samples of Berea sandstones. We achieved\nhigh-quality super-resolved 3D images with a resolution of 0.4375 micro-m/voxel\nand accurate segmentation for constituting minerals and pore space. The\ndescribed procedure can significantly expand the modern capabilities of digital\nrock physics."},{"date":"2025-01","title":"OFDM-based JCAS under Attack: The Dual Threat of Spoofing and Jamming in WLAN Sensing","author":"Hasan Can Yildirim, Musa Furkan Keskin, Henk Wymeersch, and Francois Horlin","link":"http://arxiv.org/abs/2501.06798v1","abstract":"This study reveals the vulnerabilities of Wireless Local Area Networks (WLAN)\nsensing, under the scope of joint communication and sensing (JCAS), focusing on\ntarget spoofing and deceptive jamming techniques. We use orthogonal\nfrequency-division multiplexing (OFDM) to explore how adversaries can exploit\nWLAN's sensing capabilities to inject false targets and disrupt normal\noperations. Unlike traditional methods that require sophisticated digital\nradio-frequency memory hardware, we demonstrate that much simpler\nsoftware-defined radios can effectively serve as deceptive jammers in WLAN\nsettings. Through comprehensive modeling and practical experiments, we show how\ndeceptive jammers can manipulate the range-Doppler map (RDM) by altering signal\nintegrity, thereby posing significant security threats to OFDM-based JCAS\nsystems. Our findings comprehensively evaluate jammer impact on RDMs and\npropose several jamming strategies that vary in complexity and detectability."},{"date":"2025-01","title":"KeTS: Kernel-based Trust Segmentation against Model Poisoning Attacks","author":"Ankit Gangwal, Mauro Conti, and Tommaso Pauselli","link":"http://arxiv.org/abs/2501.06729v1","abstract":"Federated Learning (FL) enables multiple users to collaboratively train a\nglobal model in a distributed manner without revealing their personal data.\nHowever, FL remains vulnerable to model poisoning attacks, where malicious\nactors inject crafted updates to compromise the global model's accuracy. These\nvulnerabilities are particularly severe in non-homogeneous environments, where\nclients exhibit varying proportions of class labels, resulting in heterogeneous\nupdates. In such settings, benign outliers are often misclassified as false\npositives, while maliciously crafted uploads evade detection and are aggregated\nat the server. Existing defense mechanisms struggle in such real-world\nsettings, resulting in significant declines in the global FL model's\nperformance.\n  We propose a novel defense mechanism, Kernel-based Trust Segmentation (KeTS),\nto counter model poisoning attacks. Unlike existing approaches, KeTS analyzes\nthe evolution of each client's updates and effectively segments malicious\nclients using Kernel Density Estimation (KDE), even in the presence of benign\noutliers. We thoroughly evaluate KeTS's performance against the six most\neffective model poisoning attacks (i.e., Trim-Attack, Krum-Attack, Min-Max\nattack, Min-Sum attack, and their variants) on two different datasets (i.e.,\nMNIST and Fashion-MNIST) and compare its performance with three classical\nrobust schemes (i.e., Krum, Trim-Mean, and Median) and a state-of-the-art\ndefense (i.e., FLTrust). Our results show that KeTS outperforms the existing\ndefenses in every attack setting; beating the best-performing defense by an\noverall average of >24% (on MNIST) and >14% (on Fashion-MNIST). A series of\nfurther experiments (varying poisoning approaches, attacker population, etc.)\nreveal the consistent and superior performance of KeTS under diverse\nconditions."},{"date":"2025-01","title":"SafeSplit: A Novel Defense Against Client-Side Backdoor Attacks in Split Learning","author":"Phillip Rieger, Alessandro Pegoraro, Kavita Kumari, Tigist Abera, Jonathan Knauer, and Ahmad-Reza Sadeghi","link":"http://arxiv.org/abs/2501.06650v1","abstract":"Split Learning (SL) is a distributed deep learning approach enabling multiple\nclients and a server to collaboratively train and infer on a shared deep neural\nnetwork (DNN) without requiring clients to share their private local data. The\nDNN is partitioned in SL, with most layers residing on the server and a few\ninitial layers and inputs on the client side. This configuration allows\nresource-constrained clients to participate in training and inference. However,\nthe distributed architecture exposes SL to backdoor attacks, where malicious\nclients can manipulate local datasets to alter the DNN's behavior. Existing\ndefenses from other distributed frameworks like Federated Learning are not\napplicable, and there is a lack of effective backdoor defenses specifically\ndesigned for SL.\n  We present SafeSplit, the first defense against client-side backdoor attacks\nin Split Learning (SL). SafeSplit enables the server to detect and filter out\nmalicious client behavior by employing circular backward analysis after a\nclient's training is completed, iteratively reverting to a trained checkpoint\nwhere the model under examination is found to be benign. It uses a two-fold\nanalysis to identify client-induced changes and detect poisoned models. First,\na static analysis in the frequency domain measures the differences in the\nlayer's parameters at the server. Second, a dynamic analysis introduces a novel\nrotational distance metric that assesses the orientation shifts of the server's\nlayer parameters during training. Our comprehensive evaluation across various\ndata distributions, client counts, and attack scenarios demonstrates the high\nefficacy of this dual analysis in mitigating backdoor attacks while preserving\nmodel utility."},{"date":"2025-01","title":"RogueRFM: Attacking Refresh Management for Covert-Channel and Denial-of-Service","author":"Hritvik Taneja, and Moinuddin Qureshi","link":"http://arxiv.org/abs/2501.06646v1","abstract":"With lowering thresholds, transparently defending against Rowhammer within\nDRAM is challenging due to the lack of time to perform mitigation. Commercially\ndeployed in-DRAM defenses like TRR that steal time from normal refreshes~(REF)\nto perform mitigation have been proven ineffective against Rowhammer. In\nresponse, a new Refresh Management (RFM) interface has been added to the DDR5\nspecifications. RFM provides dedicated time to an in-DRAM defense to perform\nmitigation. Several recent works have used RFM for the intended purpose -\nbuilding better Rowhammer defenses. However, to the best of our knowledge, no\nprior study has looked at the potential security implications of this new\nfeature if an attacker subjects it to intentional misuse.\n  Our paper shows that RFM introduces new side effects in the system - the\nactivity of one bank causes interference with the operation of the other banks.\nThus, the latency of a bank becomes dependent on the activity of other banks.\nWe use these side effects to build two new attacks. First, a novel memory-based\ncovert channel, which has a bandwidth of up to 31.3 KB/s, and is also effective\neven in a bank-partitioned system. Second, a new Denial-of-Service (DOS) attack\npattern that exploits the activity within a single bank to reduce the\nperformance of the other banks. Our experiments on SPEC2017, PARSEC, and LIGRA\nworkloads show a slowdown of up to 67\\% when running alongside our DOS pattern.\nWe also discuss potential countermeasures for our attacks."},{"date":"2025-01","title":"Exploring Pose-Based Anomaly Detection for Retail Security: A Real-World Shoplifting Dataset and Benchmark","author":"Narges Rashvand, Ghazal Alinezhad Noghre, Armin Danesh Pazho, Shanle Yao, and Hamed Tabkhi","link":"http://arxiv.org/abs/2501.06591v1","abstract":"Shoplifting poses a significant challenge for retailers, resulting in\nbillions of dollars in annual losses. Traditional security measures often fall\nshort, highlighting the need for intelligent solutions capable of detecting\nshoplifting behaviors in real time. This paper frames shoplifting detection as\nan anomaly detection problem, focusing on the identification of deviations from\ntypical shopping patterns. We introduce PoseLift, a privacy-preserving dataset\nspecifically designed for shoplifting detection, addressing challenges such as\ndata scarcity, privacy concerns, and model biases. PoseLift is built in\ncollaboration with a retail store and contains anonymized human pose data from\nreal-world scenarios. By preserving essential behavioral information while\nanonymizing identities, PoseLift balances privacy and utility. We benchmark\nstate-of-the-art pose-based anomaly detection models on this dataset,\nevaluating performance using a comprehensive set of metrics. Our results\ndemonstrate that pose-based approaches achieve high detection accuracy while\neffectively addressing privacy and bias concerns inherent in traditional\nmethods. As one of the first datasets capturing real-world shoplifting\nbehaviors, PoseLift offers researchers a valuable tool to advance computer\nvision ethically and will be publicly available to foster innovation and\ncollaboration. The dataset is available at\nhttps://github.com/TeCSAR-UNCC/PoseLift."},{"date":"2025-01","title":"Determination of galaxy photometric redshifts using Conditional Generative Adversarial Networks (CGANs)","author":"M. Garcia-Fernandez","link":"http://arxiv.org/abs/2501.06532v1","abstract":"Accurate and reliable photometric redshifts determination is one of the key\naspects for wide-field photometric surveys. Determination of photometric\nredshift for galaxies, has been traditionally solved by use of machine-learning\nand artificial intelligence techniques trained on a calibration sample of\ngalaxies, where both photometry and spectrometry are determined. On this paper,\nwe present a new algorithmic approach for determining photometric redshifts of\ngalaxies using Conditional Generative Adversarial Networks (CGANs). Proposed\nCGAN implementation, approaches photometric redshift determination as a\nprobabilistic regression, where instead of determining a single value for the\nestimated redshift of the galaxy, a full probability density is computed. The\nmethodology proposed, is tested with data from Dark Energy Survey (DES) Y1 data\nand compared with other existing algorithm such as a Random Forest regressor."},{"date":"2025-01","title":"FocusDD: Real-World Scene Infusion for Robust Dataset Distillation","author":"Youbing Hu, Yun Cheng, Olga Saukh, Firat Ozdemir, Anqi Lu, Zhiqiang Cao, and Zhijun Li","link":"http://arxiv.org/abs/2501.06405v1","abstract":"Dataset distillation has emerged as a strategy to compress real-world\ndatasets for efficient training. However, it struggles with large-scale and\nhigh-resolution datasets, limiting its practicality. This paper introduces a\nnovel resolution-independent dataset distillation method Focus ed Dataset\nDistillation (FocusDD), which achieves diversity and realism in distilled data\nby identifying key information patches, thereby ensuring the generalization\ncapability of the distilled dataset across different network architectures.\nSpecifically, FocusDD leverages a pre-trained Vision Transformer (ViT) to\nextract key image patches, which are then synthesized into a single distilled\nimage. These distilled images, which capture multiple targets, are suitable not\nonly for classification tasks but also for dense tasks such as object\ndetection. To further improve the generalization of the distilled dataset, each\nsynthesized image is augmented with a downsampled view of the original image.\nExperimental results on the ImageNet-1K dataset demonstrate that, with 100\nimages per class (IPC), ResNet50 and MobileNet-v2 achieve validation accuracies\nof 71.0% and 62.6%, respectively, outperforming state-of-the-art methods by\n2.8% and 4.7%. Notably, FocusDD is the first method to use distilled datasets\nfor object detection tasks. On the COCO2017 dataset, with an IPC of 50,\nYOLOv11n and YOLOv11s achieve 24.4% and 32.1% mAP, respectively, further\nvalidating the effectiveness of our approach."},{"date":"2025-01","title":"Towards Robust Nonlinear Subspace Clustering: A Kernel Learning Approach","author":"Kunpeng Xu, Lifei Chen, and Shengrui Wang","link":"http://arxiv.org/abs/2501.06368v1","abstract":"Kernel-based subspace clustering, which addresses the nonlinear structures in\ndata, is an evolving area of research. Despite noteworthy progressions,\nprevailing methodologies predominantly grapple with limitations relating to (i)\nthe influence of predefined kernels on model performance; (ii) the difficulty\nof preserving the original manifold structures in the nonlinear space; (iii)\nthe dependency of spectral-type strategies on the ideal block diagonal\nstructure of the affinity matrix. This paper presents DKLM, a novel paradigm\nfor kernel-induced nonlinear subspace clustering. DKLM provides a data-driven\napproach that directly learns the kernel from the data's self-representation,\nensuring adaptive weighting and satisfying the multiplicative triangle\ninequality constraint, which enhances the robustness of the learned kernel. By\nleveraging this learned kernel, DKLM preserves the local manifold structure of\ndata in a nonlinear space while promoting the formation of an optimal\nblock-diagonal affinity matrix. A thorough theoretical examination of DKLM\nreveals its relationship with existing clustering paradigms. Comprehensive\nexperiments on synthetic and real-world datasets demonstrate the effectiveness\nof the proposed method."},{"date":"2025-01","title":"Resilient Endurance-Aware NVM-based PUF against Learning-based Attacks","author":"Hassan Nassar, Ming-Liang Wei, Chia-Lin Yang, J\u00f6rg Henkel, and Kuan-Hsun Chen","link":"http://arxiv.org/abs/2501.06367v1","abstract":"Physical Unclonable Functions (PUFs) based on Non-Volatile Memory (NVM)\ntechnology have emerged as a promising solution for secure authentication and\ncryptographic applications. By leveraging the multi-level cell (MLC)\ncharacteristic of NVMs, these PUFs can generate a wide range of unique\nresponses, enhancing their resilience to machine learning (ML) modeling\nattacks. However, a significant issue with NVM-based PUFs is their endurance\nproblem; frequent write operations lead to wear and degradation over time,\nreducing the reliability and lifespan of the PUF.\n  This paper addresses these issues by offering a comprehensive model to\npredict and analyze the effects of endurance changes on NVM PUFs. This model\nprovides insights into how wear impacts the PUF's quality and helps in\ndesigning more robust PUFs. Building on this model, we present a novel design\nfor NVM PUFs that significantly improves endurance. Our design approach\nincorporates advanced techniques to distribute write operations more evenly and\nreduce stress on individual cells. The result is an NVM PUF that demonstrates a\n$62\\times$ improvement in endurance compared to current state-of-the-art\nsolutions while maintaining protection against learning-based attacks."},{"date":"2025-01","title":"Towards Iris Presentation Attack Detection with Foundation Models","author":"Juan E. Tapia, L\u00e1zaro Janier Gonz\u00e1lez-Soler, and Christoph Busch","link":"http://arxiv.org/abs/2501.06312v1","abstract":"Foundation models are becoming increasingly popular due to their strong\ngeneralization capabilities resulting from being trained on huge datasets.\nThese generalization capabilities are attractive in areas such as NIR Iris\nPresentation Attack Detection (PAD), in which databases are limited in the\nnumber of subjects and diversity of attack instruments, and there is no\ncorrespondence between the bona fide and attack images because, most of the\ntime, they do not belong to the same subjects. This work explores an iris PAD\napproach based on two foundation models, DinoV2 and VisualOpenClip. The results\nshow that fine-tuning prediction with a small neural network as head overpasses\nthe state-of-the-art performance based on deep learning approaches. However,\nsystems trained from scratch have still reached better results if bona fide and\nattack images are available."},{"date":"2025-01","title":"Reinforcement Learning-Driven Adaptation Chains: A Robust Framework for Multi-Cloud Workflow Security","author":"Nafiseh Soveizi, and Dimka Karastoyanova","link":"http://arxiv.org/abs/2501.06305v1","abstract":"Cloud computing has emerged as a crucial solution for managing data- and\ncompute-intensive workflows, offering scalability to address dynamic demands.\nHowever, security concerns persist, especially for workflows involving\nsensitive data and tasks. One of the main gaps in the literature is the lack of\nrobust and flexible measures for reacting to these security violations. To\naddress this, we propose an innovative approach leveraging Reinforcement\nLearning (RL) to formulate adaptation chains, responding effectively to\nsecurity violations within cloud-based workflows. These chains consist of\nsequences of adaptation actions tailored to attack characteristics, workflow\ndependencies, and user-defined requirements. Unlike conventional single-task\nadaptations, adaptation chains provide a comprehensive mitigation strategy by\ntaking into account both control and data dependencies between tasks, thereby\naccommodating conflicting objectives effectively. Moreover, our RL-based\napproach uses insights from past responses to mitigate uncertainties associated\nwith adaptation costs. We evaluate the method using our jBPM and Cloudsim Plus\nbased implementation and compare the impact of selected adaptation chains on\nworkflows with the single adaptation approach. Results demonstrate that the\nadaptation chain approach outperforms in terms of total adaptation cost,\noffering resilience and adaptability against security threats."},{"date":"2025-01","title":"Multi-layered Authentication and Key Management Scheme for Secure IoV","author":"Morteza Azmoudeh Afshar, Nesrine Benchoubane, Busra Cayoren, Gunes Karabulut Kurt, and Enver Ozdemir","link":"http://arxiv.org/abs/2501.06087v1","abstract":"The rapid development of Vehicular Ad-hoc Networks (VANETs) within the\nInternet of Vehicles (IoV) necessitates efficient and secure authentication\nmethods to support high-speed, high-density environments. Current group\nauthentication schemes provide user identity protection and unlinkability but\nface limitations, such as reliance on a central group manager and vulnerability\nto collaborative attacks. This paper presents a privacy-preserving\nauthentication scheme that incorporates batch authentication, mutual\nauthentication, and secure key establishment, enabling users to authenticate\none another without a central authority. Our proposed scheme facilitates\nsimultaneous multi-user authentication, significantly enhancing scalability and\nsecurity in dynamic IoV networks. Results from realistic implementations show\nthat our method achieves average authentication and verification times of 10.61\nms and 1.78 ms, respectively, for a fleet of 100 vehicles, outperforming\nexisting methods. Scalability tests demonstrate efficient processing for larger\ngroups of up to 500 vehicles, where average authentication times remain low,\nestablishing our scheme as a robust solution for secure communication in IoV\nsystems."},{"date":"2025-01","title":"Enhancing, Refining, and Fusing: Towards Robust Multi-Scale and Dense Ship Detection","author":"Congxia Zhao, Xiongjun Fu, Jian Dong, Shen Cao, and Chunyan Zhang","link":"http://arxiv.org/abs/2501.06053v1","abstract":"Synthetic aperture radar (SAR) imaging, celebrated for its high resolution,\nall-weather capability, and day-night operability, is indispensable for\nmaritime applications. However, ship detection in SAR imagery faces significant\nchallenges, including complex backgrounds, densely arranged targets, and large\nscale variations. To address these issues, we propose a novel framework,\nCenter-Aware SAR Ship Detector (CASS-Det), designed for robust multi-scale and\ndensely packed ship detection. CASS-Det integrates three key innovations: (1) a\ncenter enhancement module (CEM) that employs rotational convolution to\nemphasize ship centers, improving localization while suppressing background\ninterference; (2) a neighbor attention module (NAM) that leverages cross-layer\ndependencies to refine ship boundaries in densely populated scenes; and (3) a\ncross-connected feature pyramid network (CC-FPN) that enhances multi-scale\nfeature fusion by integrating shallow and deep features. Extensive experiments\non the SSDD, HRSID, and LS-SSDD-v1.0 datasets demonstrate the state-of-the-art\nperformance of CASS-Det, excelling at detecting multi-scale and densely\narranged ships."},{"date":"2025-01","title":"Effective faking of verbal deception detection with target-aligned adversarial attacks","author":"Bennett Kleinberg, Riccardo Loconte, and Bruno Verschuere","link":"http://arxiv.org/abs/2501.05962v1","abstract":"Background: Deception detection through analysing language is a promising\navenue using both human judgments and automated machine learning judgments. For\nboth forms of credibility assessment, automated adversarial attacks that\nrewrite deceptive statements to appear truthful pose a serious threat. Methods:\nWe used a dataset of 243 truthful and 262 fabricated autobiographical stories\nin a deception detection task for humans and machine learning models. A large\nlanguage model was tasked to rewrite deceptive statements so that they appear\ntruthful. In Study 1, humans who made a deception judgment or used the\ndetailedness heuristic and two machine learning models (a fine-tuned language\nmodel and a simple n-gram model) judged original or adversarial modifications\nof deceptive statements. In Study 2, we manipulated the target alignment of the\nmodifications, i.e. tailoring the attack to whether the statements would be\nassessed by humans or computer models. Results: When adversarial modifications\nwere aligned with their target, human (d=-0.07 and d=-0.04) and machine\njudgments (51% accuracy) dropped to the chance level. When the attack was not\naligned with the target, both human heuristics judgments (d=0.30 and d=0.36)\nand machine learning predictions (63-78%) were significantly better than\nchance. Conclusions: Easily accessible language models can effectively help\nanyone fake deception detection efforts both by humans and machine learning\nmodels. Robustness against adversarial modifications for humans and machines\ndepends on that target alignment. We close with suggestions on advancing\ndeception research with adversarial attack designs."},{"date":"2025-01","title":"Security Testing Framework for Web Applications: Benchmarking ZAP V2.12.0 and V2.13.0 by OWASP as an example","author":"Usha-Sri Potti, Hong-Sheng Huang, Hsuan-Tung Chen, and Hung-Min Sun","link":"http://arxiv.org/abs/2501.05907v1","abstract":"The Huge growth in the usage of web applications has raised concerns\nregarding their security vulnerabilities, which in turn pushes toward robust\nsecurity testing tools. This study compares OWASP ZAP, the leading open-source\nweb application vulnerability scanner, across its two most recent iterations.\nWhile comparing their performance to the OWASP Benchmark, the study evaluates\ntheir efficiency in spotting vulnerabilities in the purposefully vulnerable\napplication, OWASP Benchmark project. The research methodology involves\nconducting systematic scans of OWASP Benchmark using both v2.12.0 and v2.13.0\nof OWASP ZAP. The OWASP Benchmark provides a standardized framework to evaluate\nthe scanner's abilities in identifying security flaws, Insecure Cookies, Path\ntraversal, SQL injection, and more. Results obtained from this benchmark\ncomparison offer valuable insights into the strengths and weaknesses of each\nversion of the tool. This study aids in web application security testing by\nshedding light on how well-known scanners work at spotting vulnerabilities. The\nknowledge gained from this study can assist security professionals and\ndevelopers in making informed decisions to support their web application\nsecurity status. In conclusion, this study comprehensively analyzes ZAP's\ncapabilities in detecting security flaws using OWASP Benchmark v1.2. The\nfindings add to the continuing debates about online application security tools\nand establish the framework for future studies and developments in the research\nfield of web application security testing."},{"date":"2025-01","title":"Fine-tuning is Not Fine: Mitigating Backdoor Attacks in GNNs with Limited Clean Data","author":"Jiale Zhang, Bosen Rao, Chengcheng Zhu, Xiaobing Sun, Qingming Li, Haibo Hu, Xiapu Luo, Qingqing Ye, and Shouling Ji","link":"http://arxiv.org/abs/2501.05835v1","abstract":"Graph Neural Networks (GNNs) have achieved remarkable performance through\ntheir message-passing mechanism. However, recent studies have highlighted the\nvulnerability of GNNs to backdoor attacks, which can lead the model to\nmisclassify graphs with attached triggers as the target class. The\neffectiveness of recent promising defense techniques, such as fine-tuning or\ndistillation, is heavily contingent on having comprehensive knowledge of the\nsufficient training dataset. Empirical studies have shown that fine-tuning\nmethods require a clean dataset of 20% to reduce attack accuracy to below 25%,\nwhile distillation methods require a clean dataset of 15%. However, obtaining\nsuch a large amount of clean data is commonly impractical.\n  In this paper, we propose a practical backdoor mitigation framework, denoted\nas GRAPHNAD, which can capture high-quality intermediate-layer representations\nin GNNs to enhance the distillation process with limited clean data. To achieve\nthis, we address the following key questions: How to identify the appropriate\nattention representations in graphs for distillation? How to enhance\ndistillation with limited data? By adopting the graph attention transfer\nmethod, GRAPHNAD can effectively align the intermediate-layer attention\nrepresentations of the backdoored model with that of the teacher model, forcing\nthe backdoor neurons to transform into benign ones. Besides, we extract the\nrelation maps from intermediate-layer transformation and enforce the relation\nmaps of the backdoored model to be consistent with that of the teacher model,\nthereby ensuring model accuracy while further reducing the influence of\nbackdoors. Extensive experimental results show that by fine-tuning a teacher\nmodel with only 3% of the clean data, GRAPHNAD can reduce the attack success\nrate to below 5%."},{"date":"2025-01","title":"Robust Counterfactual Explanations under Model Multiplicity Using Multi-Objective Optimization","author":"Keita Kinjo","link":"http://arxiv.org/abs/2501.05795v1","abstract":"In recent years, explainability in machine learning has gained importance. In\nthis context, counterfactual explanation (CE), which is an explanation method\nthat uses examples, has attracted attention. However, it has been pointed out\nthat CE is not robust when there are multiple machine-learning models. These\nproblems are important when using machine learning to make safe decisions. In\nthis paper, we propose robust CEs that introduce a new viewpoint - Pareto\nimprovement - and a method that uses multi-objective optimization to generate\nit. To evaluate the proposed method, we conducted experiments using both\nsimulated and actual data. The results demonstrate that the proposed method is\nrobust and useful. We believe that this research will contribute to a wide\nrange of research areas, such as explainability in machine learning,\ndecision-making, and action planning based on machine learning."},{"date":"2025-01","title":"UV-Attack: Physical-World Adversarial Attacks for Person Detection via Dynamic-NeRF-based UV Mapping","author":"Yanjie Li, Wenxuan Zhang, Kaisheng Liang, and Bin Xiao","link":"http://arxiv.org/abs/2501.05783v1","abstract":"In recent research, adversarial attacks on person detectors using patches or\nstatic 3D model-based texture modifications have struggled with low success\nrates due to the flexible nature of human movement. Modeling the 3D\ndeformations caused by various actions has been a major challenge. Fortunately,\nadvancements in Neural Radiance Fields (NeRF) for dynamic human modeling offer\nnew possibilities. In this paper, we introduce UV-Attack, a groundbreaking\napproach that achieves high success rates even with extensive and unseen human\nactions. We address the challenge above by leveraging dynamic-NeRF-based UV\nmapping. UV-Attack can generate human images across diverse actions and\nviewpoints, and even create novel actions by sampling from the SMPL parameter\nspace. While dynamic NeRF models are capable of modeling human bodies,\nmodifying clothing textures is challenging because they are embedded in neural\nnetwork parameters. To tackle this, UV-Attack generates UV maps instead of RGB\nimages and modifies the texture stacks. This approach enables real-time texture\nedits and makes the attack more practical. We also propose a novel Expectation\nover Pose Transformation loss (EoPT) to improve the evasion success rate on\nunseen poses and views. Our experiments show that UV-Attack achieves a 92.75%\nattack success rate against the FastRCNN model across varied poses in dynamic\nvideo settings, significantly outperforming the state-of-the-art AdvCamou\nattack, which only had a 28.50% ASR. Moreover, we achieve 49.5% ASR on the\nlatest YOLOv8 detector in black-box settings. This work highlights the\npotential of dynamic NeRF-based UV mapping for creating more effective\nadversarial attacks on person detectors, addressing key challenges in modeling\nhuman movement and texture modification."},{"date":"2025-01","title":"Underwater Image Enhancement using Generative Adversarial Networks: A Survey","author":"Kancharagunta Kishan Babu, Ashreen Tabassum, Bommakanti Navaneeth, Tenneti Jahnavi, and Yenka Akshaya","link":"http://arxiv.org/abs/2501.06273v1","abstract":"In recent years, there has been a surge of research focused on underwater\nimage enhancement using Generative Adversarial Networks (GANs), driven by the\nneed to overcome the challenges posed by underwater environments. Issues such\nas light attenuation, scattering, and color distortion severely degrade the\nquality of underwater images, limiting their use in critical applications.\nGenerative Adversarial Networks (GANs) have emerged as a powerful tool for\nenhancing underwater photos due to their ability to learn complex\ntransformations and generate realistic outputs. These advancements have been\napplied to real-world applications, including marine biology and ecosystem\nmonitoring, coral reef health assessment, underwater archaeology, and\nautonomous underwater vehicle (AUV) navigation. This paper explores all major\napproaches to underwater image enhancement, from physical and physics-free\nmodels to Convolutional Neural Network (CNN)-based models and state-of-the-art\nGAN-based methods. It provides a comprehensive analysis of these methods,\nevaluation metrics, datasets, and loss functions, offering a holistic view of\nthe field. Furthermore, the paper delves into the limitations and challenges\nfaced by current methods, such as generalization issues, high computational\ndemands, and dataset biases, while suggesting potential directions for future\nresearch."},{"date":"2025-01","title":"Learning-based Detection of GPS Spoofing Attack for Quadrotors","author":"Pengyu Wang, Zhaohua Yang, Jialu Li, and Ling Shi","link":"http://arxiv.org/abs/2501.07597v1","abstract":"Safety-critical cyber-physical systems (CPS), such as quadrotor UAVs, are\nparticularly prone to cyber attacks, which can result in significant\nconsequences if not detected promptly and accurately. During outdoor\noperations, the nonlinear dynamics of UAV systems, combined with non-Gaussian\nnoise, pose challenges to the effectiveness of conventional statistical and\nmachine learning methods. To overcome these limitations, we present QUADFormer,\nan advanced attack detection framework for quadrotor UAVs leveraging a\ntransformer-based architecture. This framework features a residue generator\nthat produces sequences sensitive to anomalies, which are then analyzed by the\ntransformer to capture statistical patterns for detection and classification.\nFurthermore, an alert mechanism ensures UAVs can operate safely even when under\nattack. Extensive simulations and experimental evaluations highlight that\nQUADFormer outperforms existing state-of-the-art techniques in detection\naccuracy."},{"date":"2025-01","title":"An Efficient Key Expansion Method Applied to Security Credential Management System","author":"Abel C. H. Chen","link":"http://arxiv.org/abs/2501.05627v1","abstract":"In recent years, U.S. Department of Transportation has adopts Institute of\nElectrical and Electronics Engineers (IEEE) 1609 series to build the security\ncredential management system (SCMS) for being the standard of connected cars in\nU.S. Furthermore, a butterfly key expansion (BKE) method in SCMS has been\ndesigned to provide pseudonym certificates for improving the privacy of\nconnected cars. However, the BKE method is designed based on elliptic curve\ncryptography (ECC) in the standard of IEEE 1609.2.1, but more execution time is\nrequired for key expansion. Therefore, this study proposes an original\nefficient key expansion method, and the mathematical principles have been\nproposed to prove the encryption/decryption feasibility, car privacy, and\nmethod efficiency. In a practical environment, the proposed method improves the\nefficiency of key expansion method in IEEE 1609.2.1-2022 with the same security\nstrength thousands of times."},{"date":"2025-01","title":"Session-Level Dynamic Ad Load Optimization using Offline Robust Reinforcement Learning","author":"Tao Liu, Qi Xu, Wei Shi, Zhigang Hua, and Shuang Yang","link":"http://arxiv.org/abs/2501.05591v1","abstract":"Session-level dynamic ad load optimization aims to personalize the density\nand types of delivered advertisements in real time during a user's online\nsession by dynamically balancing user experience quality and ad monetization.\nTraditional causal learning-based approaches struggle with key technical\nchallenges, especially in handling confounding bias and distribution shifts. In\nthis paper, we develop an offline deep Q-network (DQN)-based framework that\neffectively mitigates confounding bias in dynamic systems and demonstrates more\nthan 80% offline gains compared to the best causal learning-based production\nbaseline. Moreover, to improve the framework's robustness against unanticipated\ndistribution shifts, we further enhance our framework with a novel offline\nrobust dueling DQN approach. This approach achieves more stable rewards on\nmultiple OpenAI-Gym datasets as perturbations increase, and provides an\nadditional 5% offline gains on real-world ad delivery data.\n  Deployed across multiple production systems, our approach has achieved\noutsized topline gains. Post-launch online A/B tests have shown double-digit\nimprovements in the engagement-ad score trade-off efficiency, significantly\nenhancing our platform's capability to serve both consumers and advertisers."},{"date":"2025-01","title":"Enforcing Fundamental Relations via Adversarial Attacks on Input Parameter Correlations","author":"Timo Saala, Lucie Flek, Alexander Jung, Akbar Karimi, Alexander Schmidt, Matthias Schott, Philipp Soldin, and Christopher Wiebusch","link":"http://arxiv.org/abs/2501.05588v1","abstract":"Correlations between input parameters play a crucial role in many scientific\nclassification tasks, since these are often related to fundamental laws of\nnature. For example, in high energy physics, one of the common deep learning\nuse-cases is the classification of signal and background processes in particle\ncollisions. In many such cases, the fundamental principles of the correlations\nbetween observables are often better understood than the actual distributions\nof the observables themselves. In this work, we present a new adversarial\nattack algorithm called Random Distribution Shuffle Attack (RDSA), emphasizing\nthe correlations between observables in the network rather than individual\nfeature characteristics. Correct application of the proposed novel attack can\nresult in a significant improvement in classification performance -\nparticularly in the context of data augmentation - when using the generated\nadversaries within adversarial training. Given that correlations between input\nfeatures are also crucial in many other disciplines. We demonstrate the RDSA\neffectiveness on six classification tasks, including two particle collision\nchallenges (using CERN Open Data), hand-written digit recognition (MNIST784),\nhuman activity recognition (HAR), weather forecasting (Rain in Australia), and\nICU patient mortality (MIMIC-IV), demonstrating a general use case beyond\nfundamental physics for this new type of adversarial attack algorithms."},{"date":"2025-01","title":"Is Your Autonomous Vehicle Safe? Understanding the Threat of Electromagnetic Signal Injection Attacks on Traffic Scene Perception","author":"Wenhao Liao, Sineng Yan, Youqian Zhang, Xinwei Zhai, Yuanyuan Wang, and Eugene Yujun Fu","link":"http://arxiv.org/abs/2501.05239v1","abstract":"Autonomous vehicles rely on camera-based perception systems to comprehend\ntheir driving environment and make crucial decisions, thereby ensuring vehicles\nto steer safely. However, a significant threat known as Electromagnetic Signal\nInjection Attacks (ESIA) can distort the images captured by these cameras,\nleading to incorrect AI decisions and potentially compromising the safety of\nautonomous vehicles. Despite the serious implications of ESIA, there is limited\nunderstanding of its impacts on the robustness of AI models across various and\ncomplex driving scenarios. To address this gap, our research analyzes the\nperformance of different models under ESIA, revealing their vulnerabilities to\nthe attacks. Moreover, due to the challenges in obtaining real-world attack\ndata, we develop a novel ESIA simulation method and generate a simulated attack\ndataset for different driving scenarios. Our research provides a comprehensive\nsimulation and evaluation framework, aiming to enhance the development of more\nrobust AI models and secure intelligent systems, ultimately contributing to the\nadvancement of safer and more reliable technology across various fields."},{"date":"2025-01","title":"Harnessing Large Language and Vision-Language Models for Robust Out-of-Distribution Detection","author":"Pei-Kang Lee, Jun-Cheng Chen, and Ja-Ling Wu","link":"http://arxiv.org/abs/2501.05228v1","abstract":"Out-of-distribution (OOD) detection has seen significant advancements with\nzero-shot approaches by leveraging the powerful Vision-Language Models (VLMs)\nsuch as CLIP. However, prior research works have predominantly focused on\nenhancing Far-OOD performance, while potentially compromising Near-OOD\nefficacy, as observed from our pilot study. To address this issue, we propose a\nnovel strategy to enhance zero-shot OOD detection performances for both Far-OOD\nand Near-OOD scenarios by innovatively harnessing Large Language Models (LLMs)\nand VLMs. Our approach first exploit an LLM to generate superclasses of the ID\nlabels and their corresponding background descriptions followed by feature\nextraction using CLIP. We then isolate the core semantic features for ID data\nby subtracting background features from the superclass features. The refined\nrepresentation facilitates the selection of more appropriate negative labels\nfor OOD data from a comprehensive candidate label set of WordNet, thereby\nenhancing the performance of zero-shot OOD detection in both scenarios.\nFurthermore, we introduce novel few-shot prompt tuning and visual prompt tuning\nto adapt the proposed framework to better align with the target distribution.\nExperimental results demonstrate that the proposed approach consistently\noutperforms current state-of-the-art methods across multiple benchmarks, with\nan improvement of up to 2.9% in AUROC and a reduction of up to 12.6% in FPR95.\nAdditionally, our method exhibits superior robustness against covariate shift\nacross different domains, further highlighting its effectiveness in real-world\nscenarios."},{"date":"2025-01","title":"EVA-S2PLoR: A Secure Element-wise Multiplication Meets Logistic Regression on Heterogeneous Database","author":"Tianle Tao, Shizhao Peng, Tianyu Mei, Shoumo Li, and Haogang Zhu","link":"http://arxiv.org/abs/2501.05223v2","abstract":"Accurate nonlinear computation is a key challenge in privacy-preserving\nmachine learning (PPML). Most existing frameworks approximate it through linear\noperations, resulting in significant precision loss. This paper proposes an\nefficient, verifiable and accurate security 2-party logistic regression\nframework (EVA-S2PLoR), which achieves accurate nonlinear function computation\nthrough a novel secure element-wise multiplication protocol and its derived\nprotocols. Our framework primarily includes secure 2-party vector element-wise\nmultiplication, addition to multiplication, reciprocal, and sigmoid function\nbased on data disguising technology, where high efficiency and accuracy are\nguaranteed by the simple computation flow based on the real number domain and\nthe few number of fixed communication rounds. We provide secure and robust\nanomaly detection through dimension transformation and Monte Carlo methods.\nEVA-S2PLoR outperforms many advanced frameworks in terms of precision\n(improving the performance of the sigmoid function by about 10 orders of\nmagnitude compared to most frameworks) and delivers the best overall\nperformance in secure logistic regression experiments."},{"date":"2025-01","title":"FaceMe: Robust Blind Face Restoration with Personal Identification","author":"Siyu Liu, Zheng-Peng Duan, Jia OuYang, Jiayi Fu, Hyunhee Park, Zikun Liu, Chun-Le Guo, and Chongyi Li","link":"http://arxiv.org/abs/2501.05177v2","abstract":"Blind face restoration is a highly ill-posed problem due to the lack of\nnecessary context. Although existing methods produce high-quality outputs, they\noften fail to faithfully preserve the individual's identity. In this paper, we\npropose a personalized face restoration method, FaceMe, based on a diffusion\nmodel. Given a single or a few reference images, we use an identity encoder to\nextract identity-related features, which serve as prompts to guide the\ndiffusion model in restoring high-quality and identity-consistent facial\nimages. By simply combining identity-related features, we effectively minimize\nthe impact of identity-irrelevant features during training and support any\nnumber of reference image inputs during inference. Additionally, thanks to the\nrobustness of the identity encoder, synthesized images can be used as reference\nimages during training, and identity changing during inference does not require\nfine-tuning the model. We also propose a pipeline for constructing a reference\nimage training pool that simulates the poses and expressions that may appear in\nreal-world scenarios. Experimental results demonstrate that our FaceMe can\nrestore high-quality facial images while maintaining identity consistency,\nachieving excellent performance and robustness."},{"date":"2025-01","title":"Bringing Order Amidst Chaos: On the Role of Artificial Intelligence in Secure Software Engineering","author":"Matteo Esposito","link":"http://arxiv.org/abs/2501.05165v1","abstract":"Context. Developing secure and reliable software remains a key challenge in\nsoftware engineering (SE). The ever-evolving technological landscape offers\nboth opportunities and threats, creating a dynamic space where chaos and order\ncompete. Secure software engineering (SSE) must continuously address\nvulnerabilities that endanger software systems and carry broader socio-economic\nrisks, such as compromising critical national infrastructure and causing\nsignificant financial losses. Researchers and practitioners have explored\nmethodologies like Static Application Security Testing Tools (SASTTs) and\nartificial intelligence (AI) approaches, including machine learning (ML) and\nlarge language models (LLMs), to detect and mitigate these vulnerabilities.\nEach method has unique strengths and limitations.\n  Aim. This thesis seeks to bring order to the chaos in SSE by addressing\ndomain-specific differences that impact AI accuracy.\n  Methodology. The research employs a mix of empirical strategies, such as\nevaluating effort-aware metrics, analyzing SASTTs, conducting method-level\nanalysis, and leveraging evidence-based techniques like systematic dataset\nreviews. These approaches help characterize vulnerability prediction datasets.\n  Results. Key findings include limitations in static analysis tools for\nidentifying vulnerabilities, gaps in SASTT coverage of vulnerability types,\nweak relationships among vulnerability severity scores, improved defect\nprediction accuracy using just-in-time modeling, and threats posed by untouched\nmethods.\n  Conclusions. This thesis highlights the complexity of SSE and the importance\nof contextual knowledge in improving AI-driven vulnerability and defect\nprediction. The comprehensive analysis advances effective prediction models,\nbenefiting both researchers and practitioners."},{"date":"2025-01","title":"Robust Score Matching","author":"Richard Schwank, Andrew McCormack, and Mathias Drton","link":"http://arxiv.org/abs/2501.05105v1","abstract":"Proposed in Hyv\\\"arinen (2005), score matching is a parameter estimation\nprocedure that does not require computation of distributional normalizing\nconstants. In this work we utilize the geometric median of means to develop a\nrobust score matching procedure that yields consistent parameter estimates in\nsettings where the observed data has been contaminated. A special appeal of the\nproposed method is that it retains convexity in exponential family models. The\nnew method is therefore particularly attractive for non-Gaussian, exponential\nfamily graphical models where evaluation of normalizing constants is\nintractable. Support recovery guarantees for such models when contamination is\npresent are provided. Additionally, support recovery is studied in numerical\nexperiments and on a precipitation dataset. We demonstrate that the proposed\nrobust score matching estimator performs comparably to the standard score\nmatching estimator when no contamination is present but greatly outperforms\nthis estimator in a setting with contamination."},{"date":"2025-01","title":"TAPFed: Threshold Secure Aggregation for Privacy-Preserving Federated Learning","author":"Runhua Xu, Bo Li, Chao Li, James B. D. Joshi, Shuai Ma, and Jianxin Li","link":"http://arxiv.org/abs/2501.05053v1","abstract":"Federated learning is a computing paradigm that enhances privacy by enabling\nmultiple parties to collaboratively train a machine learning model without\nrevealing personal data. However, current research indicates that traditional\nfederated learning platforms are unable to ensure privacy due to privacy leaks\ncaused by the interchange of gradients. To achieve privacy-preserving federated\nlearning, integrating secure aggregation mechanisms is essential.\nUnfortunately, existing solutions are vulnerable to recently demonstrated\ninference attacks such as the disaggregation attack. This paper proposes\nTAPFed, an approach for achieving privacy-preserving federated learning in the\ncontext of multiple decentralized aggregators with malicious actors. TAPFed\nuses a proposed threshold functional encryption scheme and allows for a certain\nnumber of malicious aggregators while maintaining security and privacy. We\nprovide formal security and privacy analyses of TAPFed and compare it to\nvarious baselines through experimental evaluation. Our results show that TAPFed\noffers equivalent performance in terms of model quality compared to\nstate-of-the-art approaches while reducing transmission overhead by 29%-45%\nacross different model training scenarios. Most importantly, TAPFed can defend\nagainst recently demonstrated inference attacks caused by curious aggregators,\nwhich the majority of existing approaches are susceptible to."},{"date":"2025-01","title":"On Measuring Unnoticeability of Graph Adversarial Attacks: Observations, New Measure, and Applications","author":"Hyeonsoo Jo, Hyunjin Hwang, Fanchen Bu, Soo Yong Lee, Chanyoung Park, and Kijung Shin","link":"http://arxiv.org/abs/2501.05015v1","abstract":"Adversarial attacks are allegedly unnoticeable. Prior studies have designed\nattack noticeability measures on graphs, primarily using statistical tests to\ncompare the topology of original and (possibly) attacked graphs. However, we\nobserve two critical limitations in the existing measures. First, because the\nmeasures rely on simple rules, attackers can readily enhance their attacks to\nbypass them, reducing their attack \"noticeability\" and, yet, maintaining their\nattack performance. Second, because the measures naively leverage global\nstatistics, such as degree distributions, they may entirely overlook attacks\nuntil severe perturbations occur, letting the attacks be almost \"totally\nunnoticeable.\" To address the limitations, we introduce HideNSeek, a learnable\nmeasure for graph attack noticeability. First, to mitigate the bypass problem,\nHideNSeek learns to distinguish the original and (potential) attack edges using\na learnable edge scorer (LEO), which scores each edge on its likelihood of\nbeing an attack. Second, to mitigate the overlooking problem, HideNSeek\nconducts imbalance-aware aggregation of all the edge scores to obtain the final\nnoticeability score. Using six real-world graphs, we empirically demonstrate\nthat HideNSeek effectively alleviates the observed limitations, and LEO (i.e.,\nour learnable edge scorer) outperforms eleven competitors in distinguishing\nattack edges under five different attack methods. For an additional\napplication, we show that LEO boost the performance of robust GNNs by removing\nattack-like edges."},{"date":"2025-01","title":"Targeted Adversarial Denoising Autoencoders (TADA) for Neural Time Series Filtration","author":"Benjamin J. Choi, Griffin Milsap, Clara A. Scholl, Francesco Tenore, and Mattson Ogg","link":"http://arxiv.org/abs/2501.04967v2","abstract":"Current machine learning (ML)-based algorithms for filtering\nelectroencephalography (EEG) time series data face challenges related to\ncumbersome training times, regularization, and accurate reconstruction. To\naddress these shortcomings, we present an ML filtration algorithm driven by a\nlogistic covariance-targeted adversarial denoising autoencoder (TADA). We\nhypothesize that the expressivity of a targeted, correlation-driven\nconvolutional autoencoder will enable effective time series filtration while\nminimizing compute requirements (e.g., runtime, model size). Furthermore, we\nexpect that adversarial training with covariance rescaling will minimize signal\ndegradation. To test this hypothesis, a TADA system prototype was trained and\nevaluated on the task of removing electromyographic (EMG) noise from EEG data\nin the EEGdenoiseNet dataset, which includes EMG and EEG data from 67 subjects.\nThe TADA filter surpasses conventional signal filtration algorithms across\nquantitative metrics (Correlation Coefficient, Temporal RRMSE, Spectral RRMSE),\nand performs competitively against other deep learning architectures at a\nreduced model size of less than 400,000 trainable parameters. Further\nexperimentation will be necessary to assess the viability of TADA on a wider\nrange of deployment cases."},{"date":"2025-01","title":"LayerMix: Enhanced Data Augmentation through Fractal Integration for Robust Deep Learning","author":"Hafiz Mughees Ahmad, Dario Morle, and Afshin Rahimi","link":"http://arxiv.org/abs/2501.04861v2","abstract":"Deep learning models have demonstrated remarkable performance across various\ncomputer vision tasks, yet their vulnerability to distribution shifts remains a\ncritical challenge. Despite sophisticated neural network architectures,\nexisting models often struggle to maintain consistent performance when\nconfronted with Out-of-Distribution (OOD) samples, including natural\ncorruptions, adversarial perturbations, and anomalous patterns. We introduce\nLayerMix, an innovative data augmentation approach that systematically enhances\nmodel robustness through structured fractal-based image synthesis. By\nmeticulously integrating structural complexity into training datasets, our\nmethod generates semantically consistent synthetic samples that significantly\nimprove neural network generalization capabilities. Unlike traditional\naugmentation techniques that rely on random transformations, LayerMix employs a\nstructured mixing pipeline that preserves original image semantics while\nintroducing controlled variability. Extensive experiments across multiple\nbenchmark datasets, including CIFAR-10, CIFAR-100, ImageNet-200, and\nImageNet-1K demonstrate LayerMixs superior performance in classification\naccuracy and substantially enhances critical Machine Learning (ML) safety\nmetrics, including resilience to natural image corruptions, robustness against\nadversarial attacks, improved model calibration and enhanced prediction\nconsistency. LayerMix represents a significant advancement toward developing\nmore reliable and adaptable artificial intelligence systems by addressing the\nfundamental challenges of deep learning generalization. The code is available\nat https://github.com/ahmadmughees/layermix."},{"date":"2025-01","title":"Blockchain-Based Secure Vehicle Auction System with Smart Contracts","author":"Ka Wai Wu","link":"http://arxiv.org/abs/2501.04841v1","abstract":"The problem of a single point of failure in centralized systems poses a great\nchallenge to the stability of such systems. Meanwhile, the tamperability of\ndata within centralized systems makes users reluctant to trust and use\ncentralized applications in many scenarios, including the financial and\nbusiness sectors.\n  Blockchain, as a new decentralized technology, addresses these issues\neffectively. As a typical decentralized system, blockchain can be utilized to\nbuild a data-sharing model. Users in a blockchain do not need to trust other\nusers; instead, they trust that the majority of miner nodes are honest. Smart\ncontracts enable developers to write distributed programs based on blockchain\nsystems, ensuring that all code is immutable and secure.\n  In this paper, we analyze the security of blockchain technology to illustrate\nits advantages and justify its use. Furthermore, we design a new system for\nstoring and trading vehicle information based on the Ethereum blockchain and\nsmart contract technology. Specifically, our system allows users to upload\nvehicle information and auction vehicles to transfer ownership. Our application\nprovides great convenience to buyers and owners, while the use of smart\ncontracts enhances the security and privacy of the system."},{"date":"2025-01","title":"Leveraging Registers in Vision Transformers for Robust Adaptation","author":"Srikar Yellapragada, Kowshik Thopalli, Vivek Narayanaswamy, Wesam Sakla, Yang Liu, Yamen Mubarka, Dimitris Samaras, and Jayaraman J. Thiagarajan","link":"http://arxiv.org/abs/2501.04784v1","abstract":"Vision Transformers (ViTs) have shown success across a variety of tasks due\nto their ability to capture global image representations. Recent studies have\nidentified the existence of high-norm tokens in ViTs, which can interfere with\nunsupervised object discovery. To address this, the use of \"registers\" which\nare additional tokens that isolate high norm patch tokens while capturing\nglobal image-level information has been proposed. While registers have been\nstudied extensively for object discovery, their generalization properties\nparticularly in out-of-distribution (OOD) scenarios, remains underexplored. In\nthis paper, we examine the utility of register token embeddings in providing\nadditional features for improving generalization and anomaly rejection. To that\nend, we propose a simple method that combines the special CLS token embedding\ncommonly employed in ViTs with the average-pooled register embeddings to create\nfeature representations which are subsequently used for training a downstream\nclassifier. We find that this enhances OOD generalization and anomaly\nrejection, while maintaining in-distribution (ID) performance. Extensive\nexperiments across multiple ViT backbones trained with and without registers\nreveal consistent improvements of 2-4\\% in top-1 OOD accuracy and a 2-3\\%\nreduction in false positive rates for anomaly detection. Importantly, these\ngains are achieved without additional computational overhead."},{"date":"2025-01","title":"Efficient and Responsible Adaptation of Large Language Models for Robust and Equitable Top-k Recommendations","author":"Kirandeep Kaur, Manya Chadha, Vinayak Gupta, and Chirag Shah","link":"http://arxiv.org/abs/2501.04762v1","abstract":"Conventional recommendation systems (RSs) are typically optimized to enhance\nperformance metrics uniformly across all training samples, inadvertently\noverlooking the needs of diverse user populations. The performance disparity\namong various populations can harm the model's robustness to sub-populations\ndue to the varying user properties. While large language models (LLMs) show\npromise in enhancing RS performance, their practical applicability is hindered\nby high costs, inference latency, and degraded performance on long user\nqueries. To address these challenges, we propose a hybrid task allocation\nframework designed to promote social good by equitably serving all user groups.\nBy adopting a two-phase approach, we promote a strategic assignment of tasks\nfor efficient and responsible adaptation of LLMs. Our strategy works by first\nidentifying the weak and inactive users that receive a suboptimal ranking\nperformance by RSs. Next, we use an in-context learning approach for such\nusers, wherein each user interaction history is contextualized as a distinct\nranking task. We evaluate our hybrid framework by incorporating eight different\nrecommendation algorithms and three different LLMs -- both open and\nclose-sourced. Our results on three real-world datasets show a significant\nreduction in weak users and improved robustness to subpopulations without\ndisproportionately escalating costs."},{"date":"2025-01","title":"Microservice Deployment in Space Computing Power Networks via Robust Reinforcement Learning","author":"Zhiyong Yu, Yuning Jiang, Xin Liu, Yuanming Shi, Chunxiao Jiang, and Linling Kuang","link":"http://arxiv.org/abs/2501.06244v1","abstract":"With the growing demand for Earth observation, it is important to provide\nreliable real-time remote sensing inference services to meet the low-latency\nrequirements. The Space Computing Power Network (Space-CPN) offers a promising\nsolution by providing onboard computing and extensive coverage capabilities for\nreal-time inference. This paper presents a remote sensing artificial\nintelligence applications deployment framework designed for Low Earth Orbit\nsatellite constellations to achieve real-time inference performance. The\nframework employs the microservice architecture, decomposing monolithic\ninference tasks into reusable, independent modules to address high latency and\nresource heterogeneity. This distributed approach enables optimized\nmicroservice deployment, minimizing resource utilization while meeting quality\nof service and functional requirements. We introduce Robust Optimization to the\ndeployment problem to address data uncertainty. Additionally, we model the\nRobust Optimization problem as a Partially Observable Markov Decision Process\nand propose a robust reinforcement learning algorithm to handle the\nsemi-infinite Quality of Service constraints. Our approach yields sub-optimal\nsolutions that minimize accuracy loss while maintaining acceptable\ncomputational costs. Simulation results demonstrate the effectiveness of our\nframework."},{"date":"2025-01","title":"Learnable Scaled Gradient Descent for Guaranteed Robust Tensor PCA","author":"Lanlan Feng, Ce Zhu, Yipeng Liu, Saiprasad Ravishankar, and Longxiu Huang","link":"http://arxiv.org/abs/2501.04565v2","abstract":"Robust tensor principal component analysis (RTPCA) aims to separate the\nlow-rank and sparse components from multi-dimensional data, making it an\nessential technique in the signal processing and computer vision fields.\nRecently emerging tensor singular value decomposition (t-SVD) has gained\nconsiderable attention for its ability to better capture the low-rank structure\nof tensors compared to traditional matrix SVD. However, existing methods often\nrely on the computationally expensive tensor nuclear norm (TNN), which limits\ntheir scalability for real-world tensors. To address this issue, we explore an\nefficient scaled gradient descent (SGD) approach within the t-SVD framework for\nthe first time, and propose the RTPCA-SGD method. Theoretically, we rigorously\nestablish the recovery guarantees of RTPCA-SGD under mild assumptions,\ndemonstrating that with appropriate parameter selection, it achieves linear\nconvergence to the true low-rank tensor at a constant rate, independent of the\ncondition number. To enhance its practical applicability, we further propose a\nlearnable self-supervised deep unfolding model, which enables effective\nparameter learning. Numerical experiments on both synthetic and real-world\ndatasets demonstrate the superior performance of the proposed methods while\nmaintaining competitive computational efficiency, especially consuming less\ntime than RTPCA-TNN."},{"date":"2025-01","title":"Towards Fair Class-wise Robustness: Class Optimal Distribution Adversarial Training","author":"Hongxin Zhi, Hongtao Yu, Shaome Li, Xiuming Zhao, and Yiteng Wu","link":"http://arxiv.org/abs/2501.04527v1","abstract":"Adversarial training has proven to be a highly effective method for improving\nthe robustness of deep neural networks against adversarial attacks.\nNonetheless, it has been observed to exhibit a limitation in terms of robust\nfairness, characterized by a significant disparity in robustness across\ndifferent classes. Recent efforts to mitigate this problem have turned to\nclass-wise reweighted methods. However, these methods suffer from a lack of\nrigorous theoretical analysis and are limited in their exploration of the\nweight space, as they mainly rely on existing heuristic algorithms or intuition\nto compute weights. In addition, these methods fail to guarantee the\nconsistency of the optimization direction due to the decoupled optimization of\nweights and the model parameters. They potentially lead to suboptimal weight\nassignments and consequently, a suboptimal model. To address these problems,\nthis paper proposes a novel min-max training framework, Class Optimal\nDistribution Adversarial Training (CODAT), which employs distributionally\nrobust optimization to fully explore the class-wise weight space, thus enabling\nthe identification of the optimal weight with theoretical guarantees.\nFurthermore, we derive a closed-form optimal solution to the internal\nmaximization and then get a deterministic equivalent objective function, which\nprovides a theoretical basis for the joint optimization of weights and model\nparameters. Meanwhile, we propose a fairness elasticity coefficient for the\nevaluation of the algorithm with regard to both robustness and robust fairness.\nExperimental results on various datasets show that the proposed method can\neffectively improve the robust fairness of the model and outperform the\nstate-of-the-art approaches."},{"date":"2025-01","title":"Multichannel Steganography: A Provably Secure Hybrid Steganographic Model for Secure Communication","author":"Obinna Omego, and Michal Bosy","link":"http://arxiv.org/abs/2501.04511v1","abstract":"This study introduces a novel steganographic model that synthesizes\nSteganography by Cover Modification (CMO) and Steganography by Cover Synthesis\n(CSY), enhancing both security and undetectability by generating cover messages\nor parameters while retaining the original cover's form, thus minimizing\ndetection risks and overcoming the limitations of single-method techniques.\nBuilding upon this model, a refined Steganographic Communication Protocol is\nproposed, enhancing resilience against sophisticated threats such as\nMultichannel Replay Attacks and Multichannel Man-in-the-Middle Attacks,\nfortifying the protocol against potential tampering and improving upon prior\nworks. To evaluate the security of the proposed protocol, a novel adversarial\nmodel is developed simulating a probabilistic polynomial time (PPT) adversary\ncapable of intercepting communications across multiple channels. This model\nassesses the adversary's ability to compromise the protocol, providing a\ncomprehensive security analysis. Finally, this study explores the practicality\nand adaptability of the model to both constrained environments like SMS banking\nand resource-rich settings such as blockchain transactions, demonstrating their\npotential to enhance financial services and security. These contributions\npresent a robust and adaptable framework for secure steganographic\ncommunication, offering practical solutions for secure communications across\ndiverse environments."},{"date":"2025-01","title":"A Taxonomy of Functional Security Features and How They Can Be Located","author":"Kevin Hermann, Simon Schneider, Catherine Tony, Asli Yardim, Sven Peldszus, Thorsten Berger, Riccardo Scandariato, M. Angela Sasse, and Alena Naiakshina","link":"http://arxiv.org/abs/2501.04454v1","abstract":"Security must be considered in almost every software system. Unfortunately,\nselecting and implementing security features remains challenging due to the\nvariety of security threats and possible countermeasures. While security\nstandards are intended to help developers, they are usually too abstract and\nvague to help implement security features, or they merely help configure such.\nA resource that describes security features at an abstraction level between\nhigh-level (i.e., rather too general) and low-level (i.e., rather too specific)\nsecurity standards could facilitate secure systems development. To realize\nsecurity features, developers typically use external security frameworks, to\nminimize implementation mistakes. Even then, developers still make mistakes,\noften resulting in security vulnerabilities. When security incidents occur or\nthe system needs to be audited or maintained, it is essential to know the\nimplemented security features and, more importantly, where they are located.\nThis task, commonly referred to as feature location, is often tedious and\nerror-prone. Therefore, we have to support long-term tracking of implemented\nsecurity features.\n  We present a study of security features in the literature and their coverage\nin popular security frameworks. We contribute (1) a taxonomy of 68 functional\nimplementation-level security features including a mapping to widely used\nsecurity standards, (2) an examination of 21 popular security frameworks\nconcerning which of these security features they provide, and (3) a discussion\non the representation of security features in source code. Our taxonomy aims to\naid developers in selecting appropriate security features and frameworks and\nrelating them to security standards when they need to choose and implement\nsecurity features for a software system."},{"date":"2025-01","title":"Gradient Purification: Defense Against Poisoning Attack in Decentralized Federated Learning","author":"Bin Li, Xiaoye Miao, Yongheng Shang, Xinkui Zhao, Shuiguang Deng, and Jianwei Yin","link":"http://arxiv.org/abs/2501.04453v1","abstract":"Decentralized federated learning (DFL) is inherently vulnerable to poisoning\nattacks, as malicious clients can transmit manipulated model gradients to\nneighboring clients. Existing defense methods either reject suspicious\ngradients per iteration or restart DFL aggregation after detecting all\nmalicious clients. They overlook the potential accuracy benefit from the\ndiscarded malicious gradients. In this paper, we propose a novel gradient\npurification defense, named GPD, that integrates seamlessly with existing DFL\naggregation to defend against poisoning attacks. It aims to mitigate the harm\nin model gradients while retaining the benefit in model weights for enhancing\naccuracy. For each benign client in GPD, a recording variable is designed to\ntrack the historically aggregated gradients from one of its neighbors. It\nallows benign clients to precisely detect malicious neighbors and swiftly\nmitigate aggregated malicious gradients via historical consistency checks. Upon\nmitigation, GPD optimizes model weights via aggregating gradients solely from\nbenign clients. This retains the previously beneficial portions from malicious\nclients and exploits the contributions from benign clients, thereby\nsignificantly enhancing the model accuracy. We analyze the convergence of GPD,\nas well as its ability to harvest high accuracy. Extensive experiments over\nthree datasets demonstrate that, GPD is capable of mitigating poisoning attacks\nunder both iid and non-iid data distributions. It significantly outperforms\nstate-of-the-art defenses in terms of accuracy against various poisoning\nattacks."},{"date":"2025-01","title":"Discovering new robust local search algorithms with neuro-evolution","author":"Mohamed Salim Amri Sakhri, Adrien Go\u00ebffon, Olivier Goudet, Fr\u00e9d\u00e9ric Saubion, and Cha\u00efma\u00e2 Touhami","link":"http://arxiv.org/abs/2501.04747v1","abstract":"This paper explores a novel approach aimed at overcoming existing challenges\nin the realm of local search algorithms. Our aim is to improve the decision\nprocess that takes place within a local search algorithm so as to make the best\npossible transitions in the neighborhood at each iteration. To improve this\nprocess, we propose to use a neural network that has the same input information\nas conventional local search algorithms. In this paper, which is an extension\nof the work [Goudet et al. 2024] presented at EvoCOP2024, we investigate\ndifferent ways of representing this information so as to make the algorithm as\nefficient as possible but also robust to monotonic transformations of the\nproblem objective function. To assess the efficiency of this approach, we\ndevelop an experimental setup centered around NK landscape problems, offering\nthe flexibility to adjust problem size and ruggedness. This approach offers a\npromising avenue for the emergence of new local search algorithms and the\nimprovement of their problem-solving capabilities for black-box problems."},{"date":"2025-01","title":"Modern Hardware Security: A Review of Attacks and Countermeasures","author":"Jyotiprakash Mishra, and Sanjay K. Sahay","link":"http://arxiv.org/abs/2501.04394v1","abstract":"With the exponential rise in the use of cloud services, smart devices, and\nIoT devices, advanced cyber attacks have become increasingly sophisticated and\nubiquitous. Furthermore, the rapid evolution of computing architectures and\nmemory technologies has created an urgent need to understand and address\nhardware security vulnerabilities. In this paper, we review the current state\nof vulnerabilities and mitigation strategies in contemporary computing systems.\nWe discuss cache side-channel attacks (including Spectre and Meltdown), power\nside-channel attacks (such as Simple Power Analysis, Differential Power\nAnalysis, Correlation Power Analysis, and Template Attacks), and advanced\ntechniques like Voltage Glitching and Electromagnetic Analysis to help\nunderstand and build robust cybersecurity defense systems and guide further\nresearch. We also examine memory encryption, focusing on confidentiality,\ngranularity, key management, masking, and re-keying strategies. Additionally,\nwe cover Cryptographic Instruction Set Architectures, Secure Boot, Root of\nTrust mechanisms, Physical Unclonable Functions, and hardware fault injection\ntechniques. The paper concludes with an analysis of the RISC-V architecture's\nunique security challenges. The comprehensive analysis presented in this paper\nis essential for building resilient hardware security solutions that can\nprotect against both current and emerging threats in an increasingly\nchallenging security landscape."},{"date":"2025-01","title":"An Analysis of Model Robustness across Concurrent Distribution Shifts","author":"Myeongho Jeon, Suhwan Choi, Hyoje Lee, and Teresa Yeo","link":"http://arxiv.org/abs/2501.04288v1","abstract":"Machine learning models, meticulously optimized for source data, often fail\nto predict target data when faced with distribution shifts (DSs). Previous\nbenchmarking studies, though extensive, have mainly focused on simple DSs.\nRecognizing that DSs often occur in more complex forms in real-world scenarios,\nwe broadened our study to include multiple concurrent shifts, such as unseen\ndomain shifts combined with spurious correlations. We evaluated 26 algorithms\nthat range from simple heuristic augmentations to zero-shot inference using\nfoundation models, across 168 source-target pairs from eight datasets. Our\nanalysis of over 100K models reveals that (i) concurrent DSs typically worsen\nperformance compared to a single shift, with certain exceptions, (ii) if a\nmodel improves generalization for one distribution shift, it tends to be\neffective for others, and (iii) heuristic data augmentations achieve the best\noverall performance on both synthetic and real-world datasets."},{"date":"2025-01","title":"Fast, Secure, Adaptable: LionsOS Design, Implementation and Performance","author":"Gernot Heiser, Ivan Velickovic, Peter Chubb, Alwin Joshy, Anuraag Ganesh, Bill Nguyen, Cheng Li, Courtney Darville, Guangtao Zhu, James Archer, Jingyao Zhou, Krishnan Winter, Lucy Parker, Szymon Duchniewicz, and Tianyi Bai","link":"http://arxiv.org/abs/2501.06234v1","abstract":"We present LionsOS, an operating system for security- and safety-critical\nembedded systems. LionsOS is based on the formally verified seL4 microkernel\nand designed with verification in mind. It uses a static architecture and\nfeatures a highly modular design driven by strict separation of concerns and a\nfocus on simplicity. We demonstrate that LionsOS outperforms Linux."},{"date":"2025-01","title":"Open set label noise learning with robust sample selection and margin-guided module","author":"Yuandi Zhao, Qianxi Xia, Yang Sun, Zhijie Wen, Liyan Ma, and Shihui Ying","link":"http://arxiv.org/abs/2501.04269v1","abstract":"In recent years, the remarkable success of deep neural networks (DNNs) in\ncomputer vision is largely due to large-scale, high-quality labeled datasets.\nTraining directly on real-world datasets with label noise may result in\noverfitting. The traditional method is limited to deal with closed set label\nnoise, where noisy training data has true class labels within the known label\nspace. However, there are some real-world datasets containing open set label\nnoise, which means that some samples belong to an unknown class outside the\nknown label space. To address the open set label noise problem, we introduce a\nmethod based on Robust Sample Selection and Margin-Guided Module (RSS-MGM).\nFirstly, unlike the prior clean sample selection approach, which only select a\nlimited number of clean samples, a robust sample selection module combines\nsmall loss selection or high-confidence sample selection to obtain more clean\nsamples. Secondly, to efficiently distinguish open set label noise and closed\nset ones, margin functions are designed to filter open-set data and closed set\ndata. Thirdly, different processing methods are selected for different types of\nsamples in order to fully utilize the data's prior information and optimize the\nwhole model. Furthermore, extensive experimental results with noisy labeled\ndata from benchmark datasets and real-world datasets, such as CIFAR-100N-C,\nCIFAR80N-O, WebFG-469, and Food101N, indicate that our approach outperforms\nmany state-of-the-art label noise learning methods. Especially, it can more\naccurately divide open set label noise samples and closed set ones."},{"date":"2025-01","title":"Security by Design Issues in Autonomous Vehicles","author":"Martin Higgins, Devki Jha, David Blundell, and David Wallom","link":"http://arxiv.org/abs/2501.04104v1","abstract":"As autonomous vehicle (AV) technology advances towards maturity, it becomes\nimperative to examine the security vulnerabilities within these cyber-physical\nsystems. While conventional cyber-security concerns are often at the forefront\nof discussions, it is essential to get deeper into the various layers of\nvulnerability that are often overlooked within mainstream frameworks. Our goal\nis to spotlight imminent challenges faced by AV operators and explore emerging\ntechnologies for comprehensive solutions. This research outlines the diverse\nsecurity layers, spanning physical, cyber, coding, and communication aspects,\nin the context of AVs. Furthermore, we provide insights into potential\nsolutions for each potential attack vector, ensuring that autonomous vehicles\nremain secure and resilient in an evolving threat landscape."},{"date":"2025-01","title":"Cyber Spectrum Intelligence: Security Applications, Challenges and Road Ahead","author":"Savio Sciancalepore, and Gabriele Oligeri","link":"http://arxiv.org/abs/2501.03977v1","abstract":"Cyber Spectrum Intelligence (SpecInt) is emerging as a concept that extends\nbeyond basic {\\em spectrum sensing} and {\\em signal intelligence} to encompass\na broader set of capabilities and technologies aimed at monitoring the use of\nthe radio spectrum and extracting information. SpecInt merges traditional\nspectrum sensing techniques with Artificial Intelligence (AI) and parallel\nprocessing to enhance the ability to extract and correlate simultaneous events\noccurring on various frequencies, allowing for a new wave of intelligence\napplications.\n  This paper provides an overview of the emerging SpecInt research area,\ncharacterizing the system architecture and the most relevant applications for\ncyber-physical security. We identify five subcategories of spectrum\nintelligence for cyber-physical security, encompassing Device Intelligence,\nChannel Intelligence, Location Intelligence, Communication Intelligence, and\nAmbient Intelligence. We also provide preliminary results based on an\nexperimental testbed showing the viability, feasibility, and potential of this\nemerging application area. Finally, we point out current research challenges\nand future directions paving the way for further research in this domain."},{"date":"2025-01","title":"MADation: Face Morphing Attack Detection with Foundation Models","author":"Eduarda Caldeira, Guray Ozgur, Tahar Chettaoui, Marija Ivanovska, Peter Peer, Fadi Boutros, Vitomir Struc, and Naser Damer","link":"http://arxiv.org/abs/2501.03800v2","abstract":"Despite the considerable performance improvements of face recognition\nalgorithms in recent years, the same scientific advances responsible for this\nprogress can also be used to create efficient ways to attack them, posing a\nthreat to their secure deployment. Morphing attack detection (MAD) systems aim\nto detect a specific type of threat, morphing attacks, at an early stage,\npreventing them from being considered for verification in critical processes.\nFoundation models (FM) learn from extensive amounts of unlabeled data,\nachieving remarkable zero-shot generalization to unseen domains. Although this\ngeneralization capacity might be weak when dealing with domain-specific\ndownstream tasks such as MAD, FMs can easily adapt to these settings while\nretaining the built-in knowledge acquired during pre-training. In this work, we\nrecognize the potential of FMs to perform well in the MAD task when properly\nadapted to its specificities. To this end, we adapt FM CLIP architectures with\nLoRA weights while simultaneously training a classification header. The\nproposed framework, MADation surpasses our alternative FM and transformer-based\nframeworks and constitutes the first adaption of FMs to the MAD task. MADation\npresents competitive results with current MAD solutions in the literature and\neven surpasses them in several evaluation scenarios. To encourage\nreproducibility and facilitate further research in MAD, we publicly release the\nimplementation of MADation at https: //github.com/gurayozgur/MADation"},{"date":"2025-01","title":"BTMTrack: Robust RGB-T Tracking via Dual-template Bridging and Temporal-Modal Candidate Elimination","author":"Zhongxuan Zhang, Bi Zeng, Xinyu Ni, and Yimin Du","link":"http://arxiv.org/abs/2501.03616v2","abstract":"RGB-T tracking leverages the complementary strengths of RGB and thermal\ninfrared (TIR) modalities to address challenging scenarios such as low\nillumination and adverse weather. However, existing methods often fail to\neffectively integrate temporal information and perform efficient cross-modal\ninteractions, which constrain their adaptability to dynamic targets. In this\npaper, we propose BTMTrack, a novel framework for RGB-T tracking. The core of\nour approach lies in the dual-template backbone network and the Temporal-Modal\nCandidate Elimination (TMCE) strategy. The dual-template backbone effectively\nintegrates temporal information, while the TMCE strategy focuses the model on\ntarget-relevant tokens by evaluating temporal and modal correlations, reducing\ncomputational overhead and avoiding irrelevant background noise. Building upon\nthis foundation, we propose the Temporal Dual Template Bridging (TDTB) module,\nwhich facilitates precise cross-modal fusion through dynamically filtered\ntokens. This approach further strengthens the interaction between templates and\nthe search region. Extensive experiments conducted on three benchmark datasets\ndemonstrate the effectiveness of BTMTrack. Our method achieves state-of-the-art\nperformance, with a 72.3% precision rate on the LasHeR test set and competitive\nresults on RGBT210 and RGBT234 datasets."},{"date":"2025-01","title":"Rethinking Adversarial Attacks in Reinforcement Learning from Policy Distribution Perspective","author":"Tianyang Duan, Zongyuan Zhang, Zheng Lin, Yue Gao, Ling Xiong, Yong Cui, Hongbin Liang, Xianhao Chen, Heming Cui, and Dong Huang","link":"http://arxiv.org/abs/2501.03562v2","abstract":"Deep Reinforcement Learning (DRL) suffers from uncertainties and inaccuracies\nin the observation signal in realworld applications. Adversarial attack is an\neffective method for evaluating the robustness of DRL agents. However, existing\nattack methods targeting individual sampled actions have limited impacts on the\noverall policy distribution, particularly in continuous action spaces. To\naddress these limitations, we propose the Distribution-Aware Projected Gradient\nDescent attack (DAPGD). DAPGD uses distribution similarity as the gradient\nperturbation input to attack the policy network, which leverages the entire\npolicy distribution rather than relying on individual samples. We utilize the\nBhattacharyya distance in DAPGD to measure policy similarity, enabling\nsensitive detection of subtle but critical differences between probability\ndistributions. Our experiment results demonstrate that DAPGD achieves SOTA\nresults compared to the baselines in three robot navigation tasks, achieving an\naverage 22.03% higher reward drop compared to the best baseline."},{"date":"2025-01","title":"An Empirical Study of Accuracy-Robustness Tradeoff and Training Efficiency in Self-Supervised Learning","author":"Fatemeh Ghofrani, and Pooyan Jamshidi","link":"http://arxiv.org/abs/2501.03507v1","abstract":"Self-supervised learning (SSL) has significantly advanced image\nrepresentation learning, yet efficiency challenges persist, particularly with\nadversarial training. Many SSL methods require extensive epochs to achieve\nconvergence, a demand further amplified in adversarial settings. To address\nthis inefficiency, we revisit the robust EMP-SSL framework, emphasizing the\nimportance of increasing the number of crops per image to accelerate learning.\nUnlike traditional contrastive learning, robust EMP-SSL leverages multi-crop\nsampling, integrates an invariance term and regularization, and reduces\ntraining epochs, enhancing time efficiency. Evaluated with both standard linear\nclassifiers and multi-patch embedding aggregation, robust EMP-SSL provides new\ninsights into SSL evaluation strategies.\n  Our results show that robust crop-based EMP-SSL not only accelerates\nconvergence but also achieves a superior balance between clean accuracy and\nadversarial robustness, outperforming multi-crop embedding aggregation.\nAdditionally, we extend this approach with free adversarial training in\nMulti-Crop SSL, introducing the Cost-Free Adversarial Multi-Crop\nSelf-Supervised Learning (CF-AMC-SSL) method. CF-AMC-SSL demonstrates the\neffectiveness of free adversarial training in reducing training time while\nsimultaneously improving clean accuracy and adversarial robustness. These\nfindings underscore the potential of CF-AMC-SSL for practical SSL applications.\nOur code is publicly available at https://github.com/softsys4ai/CF-AMC-SSL."},{"date":"2025-01","title":"Feasibility of short blocklength Reed-Muller codes for physical layer security in real environment","author":"Md Munibun Billah, Tyler Sweat, and Willie K. Harrison","link":"http://arxiv.org/abs/2501.03449v2","abstract":"In this paper, we investigate the application of Reed-Muller (RM) codes for\nPhysical-layer security in a real world wiretap channel scenario. Utilizing\nsoftware-defined radios (SDRs) in a real indoor environment, we implement a\ncoset coding scheme that leverages the hierarchical structure of RM codes to\nsecure data transmission. The generator matrix of the RM code is used to\npartition codewords into cosets in the usual way, where each message\ncorresponds to a unique coset, and auxiliary bits select specific codewords\nwithin each coset. This approach enables the legitimate receiver (Bob) can\ndecode the transmitted message with minimal information leakage to eavesdropper\n(Eve) thus protecting the confidentiality of the communication with the help of\ncoset structure. Mutual information neural estimation (MINE) is used to\nquantify information leakage and validate the effectiveness of the scheme.\nExperimental results indicate that RM codes can achieve robust security even in\npractical environments affected by real-world channel impairments. These\nfindings demonstrate the potential of RM codes as an efficient solution for\nphysical-layer security, particularly for applications that require low latency\nand short blocklengths."},{"date":"2025-01","title":"On the Adversarial Robustness of Benjamini Hochberg","author":"Louis L Chen, Roberto Szechtman, and Matan Seri","link":"http://arxiv.org/abs/2501.03402v1","abstract":"The Benjamini-Hochberg (BH) procedure is widely used to control the false\ndetection rate (FDR) in multiple testing. Applications of this control abound\nin drug discovery, forensics, anomaly detection, and, in particular, machine\nlearning, ranging from nonparametric outlier detection to out-of-distribution\ndetection and one-class classification methods. Considering this control could\nbe relied upon in critical safety/security contexts, we investigate its\nadversarial robustness. More precisely, we study under what conditions BH does\nand does not exhibit adversarial robustness, we present a class of simple and\neasily implementable adversarial test-perturbation algorithms, and we perform\ncomputational experiments. With our algorithms, we demonstrate that there are\nconditions under which BH's control can be significantly broken with relatively\nfew (even just one) test score perturbation(s), and provide non-asymptotic\nguarantees on the expected adversarial-adjustment to FDR. Our technical\nanalysis involves a combinatorial reframing of the BH procedure as a ``balls\ninto bins'' process, and drawing a connection to generalized ballot problems to\nfacilitate an information-theoretic approach for deriving non-asymptotic lower\nbounds."},{"date":"2025-01","title":"ProTracker: Probabilistic Integration for Robust and Accurate Point Tracking","author":"Tingyang Zhang, Chen Wang, Zhiyang Dou, Qingzhe Gao, Jiahui Lei, Baoquan Chen, and Lingjie Liu","link":"http://arxiv.org/abs/2501.03220v1","abstract":"In this paper, we propose ProTracker, a novel framework for robust and\naccurate long-term dense tracking of arbitrary points in videos. The key idea\nof our method is incorporating probabilistic integration to refine multiple\npredictions from both optical flow and semantic features for robust short-term\nand long-term tracking. Specifically, we integrate optical flow estimations in\na probabilistic manner, producing smooth and accurate trajectories by\nmaximizing the likelihood of each prediction. To effectively re-localize\nchallenging points that disappear and reappear due to occlusion, we further\nincorporate long-term feature correspondence into our flow predictions for\ncontinuous trajectory generation. Extensive experiments show that ProTracker\nachieves the state-of-the-art performance among unsupervised and\nself-supervised approaches, and even outperforms supervised methods on several\nbenchmarks. Our code and model will be publicly available upon publication."},{"date":"2025-01","title":"The Robustness of Spiking Neural Networks in Federated Learning with Compression Against Non-omniscient Byzantine Attacks","author":"Manh V. Nguyen, Liang Zhao, Bobin Deng, and Shaoen Wu","link":"http://arxiv.org/abs/2501.03306v1","abstract":"Spiking Neural Networks (SNNs), which offer exceptional energy efficiency for\ninference, and Federated Learning (FL), which offers privacy-preserving\ndistributed training, is a rising area of interest that highly beneficial\ntowards Internet of Things (IoT) devices. Despite this, research that tackles\nByzantine attacks and bandwidth limitation in FL-SNNs, both poses significant\nthreats on model convergence and training times, still remains largely\nunexplored. Going beyond proposing a solution for both of these problems, in\nthis work we highlight the dual benefits of FL-SNNs, against non-omniscient\nByzantine adversaries (ones that restrict attackers access to local clients\ndatasets), and greater communication efficiency, over FL-ANNs. Specifically, we\ndiscovered that a simple integration of Top-\\k{appa} sparsification into the FL\napparatus can help leverage the advantages of the SNN models in both greatly\nreducing bandwidth usage and significantly boosting the robustness of FL\ntraining against non-omniscient Byzantine adversaries. Most notably, we saw a\nmassive improvement of roughly 40% accuracy gain in FL-SNNs training under the\nlethal MinMax attack"},{"date":"2025-01","title":"Noise-Robust Target-Speaker Voice Activity Detection Through Self-Supervised Pretraining","author":"Holger Severin Bovbjerg, Jan \u00d8stergaard, Jesper Jensen, and Zheng-Hua Tan","link":"http://arxiv.org/abs/2501.03184v1","abstract":"Target-Speaker Voice Activity Detection (TS-VAD) is the task of detecting the\npresence of speech from a known target-speaker in an audio frame. Recently,\ndeep neural network-based models have shown good performance in this task.\nHowever, training these models requires extensive labelled data, which is\ncostly and time-consuming to obtain, particularly if generalization to unseen\nenvironments is crucial. To mitigate this, we propose a causal, Self-Supervised\nLearning (SSL) pretraining framework, called Denoising Autoregressive\nPredictive Coding (DN-APC), to enhance TS-VAD performance in noisy conditions.\nWe also explore various speaker conditioning methods and evaluate their\nperformance under different noisy conditions. Our experiments show that DN-APC\nimproves performance in noisy conditions, with a general improvement of approx.\n2% in both seen and unseen noise. Additionally, we find that FiLM conditioning\nprovides the best overall performance. Representation analysis via tSNE plots\nreveals robust initial representations of speech and non-speech from\npretraining. This underscores the effectiveness of SSL pretraining in improving\nthe robustness and performance of TS-VAD models in noisy environments."},{"date":"2025-01","title":"From Models to Network Topologies: A Topology Inference Attack in Decentralized Federated Learning","author":"Chao Feng, Yuanzhe Gao, Alberto Huertas Celdran, Gerome Bovet, and Burkhard Stiller","link":"http://arxiv.org/abs/2501.03119v1","abstract":"Federated Learning (FL) is widely recognized as a privacy-preserving machine\nlearning paradigm due to its model-sharing mechanism that avoids direct data\nexchange. However, model training inevitably leaves exploitable traces that can\nbe used to infer sensitive information. In Decentralized FL (DFL), the overlay\ntopology significantly influences its models' convergence, robustness, and\nsecurity. This study explores the feasibility of inferring the overlay topology\nof DFL systems based solely on model behavior, introducing a novel Topology\nInference Attack. A taxonomy of topology inference attacks is proposed,\ncategorizing them by the attacker's capabilities and knowledge. Practical\nattack strategies are developed for different scenarios, and quantitative\nexperiments are conducted to identify key factors influencing the attack\neffectiveness. Experimental results demonstrate that analyzing only the public\nmodels of individual nodes can accurately infer the DFL topology, underscoring\nthe risk of sensitive information leakage in DFL systems. This finding offers\nvaluable insights for improving privacy preservation in decentralized learning\nenvironments."},{"date":"2025-01","title":"Rethinking Byzantine Robustness in Federated Recommendation from Sparse Aggregation Perspective","author":"Zhongjian Zhang, Mengmei Zhang, Xiao Wang, Lingjuan Lyu, Bo Yan, Junping Du, and Chuan Shi","link":"http://arxiv.org/abs/2501.03301v2","abstract":"To preserve user privacy in recommender systems, federated recommendation\n(FR) based on federated learning (FL) emerges, keeping the personal data on the\nlocal client and updating a model collaboratively. Unlike FL, FR has a unique\nsparse aggregation mechanism, where the embedding of each item is updated by\nonly partial clients, instead of full clients in a dense aggregation of general\nFL. Recently, as an essential principle of FL, model security has received\nincreasing attention, especially for Byzantine attacks, where malicious clients\ncan send arbitrary updates. The problem of exploring the Byzantine robustness\nof FR is particularly critical since in the domains applying FR, e.g.,\ne-commerce, malicious clients can be injected easily by registering new\naccounts. However, existing Byzantine works neglect the unique sparse\naggregation of FR, making them unsuitable for our problem. Thus, we make the\nfirst effort to investigate Byzantine attacks on FR from the perspective of\nsparse aggregation, which is non-trivial: it is not clear how to define\nByzantine robustness under sparse aggregations and design Byzantine attacks\nunder limited knowledge/capability. In this paper, we reformulate the Byzantine\nrobustness under sparse aggregation by defining the aggregation for a single\nitem as the smallest execution unit. Then we propose a family of effective\nattack strategies, named Spattack, which exploit the vulnerability in sparse\naggregation and are categorized along the adversary's knowledge and capability.\nExtensive experimental results demonstrate that Spattack can effectively\nprevent convergence and even break down defenses under a few malicious clients,\nraising alarms for securing FR systems."},{"date":"2025-01","title":"Design and implementation of tools to build an ontology of Security Requirements for Internet of Medical Things","author":"Daniel Naro, Jaime Delgado, Silvia Llorente, and Amanda Palomo","link":"http://arxiv.org/abs/2501.03067v1","abstract":"When developing devices, architectures and services for the Internet of\nMedical Things (IoMT) world, manufacturers or integrators must be aware of the\nsecurity requirements expressed by both laws and specifications. To provide\ntools guiding through these requirements and to assure a third party of the\ncorrect compliance, an ontology charting the relevant laws and specifications\n(for the European context) is very useful. We here address the development of\nthis ontology. Due to the very high number and size of the considered\nspecification documents, we have put in place a methodology and tools to\nsimplify the transition from natural text to an ontology. The first step is a\nmanual highlighting of relevant concepts in the corpus, then a manual\ntranslation to XML/XSD is operated. We have developed a tool allowing us to\nconvert this semi-structured data into an ontology. Because the different\nspecifications use similar but different wording, our approach favors the\ncreation of similar instances in the ontology. To improve the ontology\nsimplification through instance merging, we consider the use of LLMs. The\nresponses of the LLMs are compared against our manually defined correct\nresponses. The quality of the responses of the automated system does not prove\nto be good enough to be trusted blindly, and should only be used as a starting\npoint for a manual correction."},{"date":"2025-01","title":"Group Shapley with Robust Significance Testing and Its Application to Bond Recovery Rate Prediction","author":"Jingyi Wang, Ying Chen, and Paolo Giudici","link":"http://arxiv.org/abs/2501.03041v1","abstract":"We propose Group Shapley, a metric that extends the classical\nindividual-level Shapley value framework to evaluate the importance of feature\ngroups, addressing the structured nature of predictors commonly found in\nbusiness and economic data. More importantly, we develop a significance testing\nprocedure based on a three-cumulant chi-square approximation and establish the\nasymptotic properties of the test statistics for Group Shapley values. Our\napproach can effectively handle challenging scenarios, including sparse or\nskewed distributions and small sample sizes, outperforming alternative tests\nsuch as the Wald test. Simulations confirm that the proposed test maintains\nrobust empirical size and demonstrates enhanced power under diverse conditions.\nTo illustrate the method's practical relevance in advancing Explainable AI, we\napply our framework to bond recovery rate predictions using a global dataset\n(1996-2023) comprising 2,094 observations and 98 features, grouped into 16\nsubgroups and five broader categories: bond characteristics, firm fundamentals,\nindustry-specific factors, market-related variables, and macroeconomic\nindicators. Our results identify the market-related variables group as the most\ninfluential. Furthermore, Lorenz curves and Gini indices reveal that Group\nShapley assigns feature importance more equitably compared to individual\nShapley values."},{"date":"2025-01","title":"Dynamic Data Defense: Unveiling the Database in motion Chaos Encryption (DaChE) Algorithm -- A Breakthrough in Chaos Theory for Enhanced Database Security","author":"Abraham Itzhak Weinberg","link":"http://arxiv.org/abs/2501.03296v1","abstract":"Amidst the burgeoning landscape of database architectures, the surge in NoSQL\ndatabases has heralded a transformative era, liberating data storage from\ntraditional relational constraints and ushering in unprecedented scalability.\nAs organizations grapple with the escalating security threats posed by database\nbreaches, a novel theoretical framework emerges at the nexus of chaos theory\nand topology: the Database in motion Chaos Encryption (DaChE) Algorithm. This\nparadigm-shifting approach challenges the static nature of data storage,\nadvocating for dynamic data motion to fortify database security. By\nincorporating chaos theory, this innovative strategy not only enhances database\ndefenses against evolving attack vectors but also redefines the boundaries of\ndata protection, offering a paradigmatic shift in safeguarding critical\ninformation assets. Additionally, it enables parallel processing, facilitating\non-the-fly processing and optimizing the performance of the proposed framework."},{"date":"2025-01","title":"CONTINUUM: Detecting APT Attacks through Spatial-Temporal Graph Neural Networks","author":"Atmane Ayoub Mansour Bahar, Kamel Soaid Ferrahi, Mohamed-Lamine Messai, Hamida Seba, and Karima Amrouche","link":"http://arxiv.org/abs/2501.02981v2","abstract":"Advanced Persistent Threats (APTs) represent a significant challenge in\ncybersecurity due to their sophisticated and stealthy nature. Traditional\nIntrusion Detection Systems (IDS) often fall short in detecting these\nmulti-stage attacks. Recently, Graph Neural Networks (GNNs) have been employed\nto enhance IDS capabilities by analyzing the complex relationships within\nnetworked data. However, existing GNN-based solutions are hampered by high\nfalse positive rates and substantial resource consumption. In this paper, we\npresent a novel IDS designed to detect APTs using a Spatio-Temporal Graph\nNeural Network Autoencoder. Our approach leverages spatial information to\nunderstand the interactions between entities within a graph and temporal\ninformation to capture the evolution of the graph over time. This dual\nperspective is crucial for identifying the sequential stages of APTs.\nFurthermore, to address privacy and scalability concerns, we deploy our\narchitecture in a federated learning environment. This setup ensures that local\ndata remains on-premise while encrypted model-weights are shared and aggregated\nusing homomorphic encryption, maintaining data privacy and security. Our\nevaluation shows that this system effectively detects APTs with lower false\npositive rates and optimized resource usage compared to existing methods,\nhighlighting the potential of spatio-temporal analysis and federated learning\nin enhancing cybersecurity defenses."},{"date":"2025-01","title":"FoundPAD: Foundation Models Reloaded for Face Presentation Attack Detection","author":"Guray Ozgur, Eduarda Caldeira, Tahar Chettaoui, Fadi Boutros, Raghavendra Ramachandra, and Naser Damer","link":"http://arxiv.org/abs/2501.02892v1","abstract":"Although face recognition systems have seen a massive performance enhancement\nin recent years, they are still targeted by threats such as presentation\nattacks, leading to the need for generalizable presentation attack detection\n(PAD) algorithms. Current PAD solutions suffer from two main problems: low\ngeneralization to unknown cenarios and large training data requirements.\nFoundation models (FM) are pre-trained on extensive datasets, achieving\nremarkable results when generalizing to unseen domains and allowing for\nefficient task-specific adaption even when little training data are available.\nIn this work, we recognize the potential of FMs to address common PAD problems\nand tackle the PAD task with an adapted FM for the first time. The FM under\nconsideration is adapted with LoRA weights while simultaneously training a\nclassification header. The resultant architecture, FoundPAD, is highly\ngeneralizable to unseen domains, achieving competitive results in several\nsettings under different data availability scenarios and even when using\nsynthetic training data. To encourage reproducibility and facilitate further\nresearch in PAD, we publicly release the implementation of FoundPAD at\nhttps://github.com/gurayozgur/FoundPAD ."},{"date":"2025-01","title":"Tougher Text, Smarter Models: Raising the Bar for Adversarial Defence Benchmarks","author":"Yang Wang, and Chenghua Lin","link":"http://arxiv.org/abs/2501.02654v2","abstract":"Recent advancements in natural language processing have highlighted the\nvulnerability of deep learning models to adversarial attacks. While various\ndefence mechanisms have been proposed, there is a lack of comprehensive\nbenchmarks that evaluate these defences across diverse datasets, models, and\ntasks. In this work, we address this gap by presenting an extensive benchmark\nfor textual adversarial defence that significantly expands upon previous work.\nOur benchmark incorporates a wide range of datasets, evaluates state-of-the-art\ndefence mechanisms, and extends the assessment to include critical tasks such\nas single-sentence classification, similarity and paraphrase identification,\nnatural language inference, and commonsense reasoning. This work not only\nserves as a valuable resource for researchers and practitioners in the field of\nadversarial robustness but also identifies key areas for future research in\ntextual adversarial defence. By establishing a new standard for benchmarking in\nthis domain, we aim to accelerate progress towards more robust and reliable\nnatural language processing systems."},{"date":"2025-01","title":"Layer-Level Self-Exposure and Patch: Affirmative Token Mitigation for Jailbreak Attack Defense","author":"Yang Ouyang, Hengrui Gu, Shuhang Lin, Wenyue Hua, Jie Peng, Bhavya Kailkhura, Tianlong Chen, and Kaixiong Zhou","link":"http://arxiv.org/abs/2501.02629v1","abstract":"As large language models (LLMs) are increasingly deployed in diverse\napplications, including chatbot assistants and code generation, aligning their\nbehavior with safety and ethical standards has become paramount. However,\njailbreak attacks, which exploit vulnerabilities to elicit unintended or\nharmful outputs, threaten LLMs' safety significantly. In this paper, we\nintroduce Layer-AdvPatcher, a novel methodology designed to defend against\njailbreak attacks by utilizing an unlearning strategy to patch specific layers\nwithin LLMs through self-augmented datasets. Our insight is that certain\nlayer(s), tend to produce affirmative tokens when faced with harmful prompts.\nBy identifying these layers and adversarially exposing them to generate more\nharmful data, one can understand their inherent and diverse vulnerabilities to\nattacks. With these exposures, we then \"unlearn\" these issues, reducing the\nimpact of affirmative tokens and hence minimizing jailbreak risks while keeping\nthe model's responses to safe queries intact. We conduct extensive experiments\non two models, four benchmark datasets, and multiple state-of-the-art jailbreak\nbenchmarks to demonstrate the efficacy of our approach. Results indicate that\nour framework reduces the harmfulness and attack success rate of jailbreak\nattacks without compromising utility for benign queries compared to recent\ndefense methods."},{"date":"2025-01","title":"V2X-DGPE: Addressing Domain Gaps and Pose Errors for Robust Collaborative 3D Object Detection","author":"Sichao Wang, Chuang Zhang, Ming Yuan, Qing Xu, Lei He, and Jianqiang Wang","link":"http://arxiv.org/abs/2501.02363v1","abstract":"In V2X collaborative perception, the domain gaps between heterogeneous nodes\npose a significant challenge for effective information fusion. Pose errors\narising from latency and GPS localization noise further exacerbate the issue by\nleading to feature misalignment. To overcome these challenges, we propose\nV2X-DGPE, a high-accuracy and robust V2X feature-level collaborative perception\nframework. V2X-DGPE employs a Knowledge Distillation Framework and a Feature\nCompensation Module to learn domain-invariant representations from multi-source\ndata, effectively reducing the feature distribution gap between vehicles and\nroadside infrastructure. Historical information is utilized to provide the\nmodel with a more comprehensive understanding of the current scene.\nFurthermore, a Collaborative Fusion Module leverages a heterogeneous\nself-attention mechanism to extract and integrate heterogeneous representations\nfrom vehicles and infrastructure. To address pose errors, V2X-DGPE introduces a\ndeformable attention mechanism, enabling the model to adaptively focus on\ncritical parts of the input features by dynamically offsetting sampling points.\nExtensive experiments on the real-world DAIR-V2X dataset demonstrate that the\nproposed method outperforms existing approaches, achieving state-of-the-art\ndetection performance. The code is available at\nhttps://github.com/wangsch10/V2X-DGPE."},{"date":"2025-01","title":"PM-Dedup: Secure Deduplication with Partial Migration from Cloud to Edge Servers","author":"Zhaokang Ke, Haoyu Gong, and David H. C. Du","link":"http://arxiv.org/abs/2501.02350v1","abstract":"Currently, an increasing number of users and enterprises are storing their\ndata in the cloud but do not fully trust cloud providers with their data in\nplaintext form. To address this concern, they encrypt their data before\nuploading it to the cloud. However, encryption with different keys means that\neven identical data will become different ciphertexts, making deduplication\nless effective. Encrypted deduplication avoids this issue by ensuring that\nidentical data chunks generate the same ciphertext with content-based keys,\nenabling the cloud to efficiently identify and remove duplicates even in\nencrypted form. Current encrypted data deduplication work can be classified\ninto two types: target-based and source-based. Target-based encrypted\ndeduplication requires clients to upload all encrypted chunks (the basic unit\nof deduplication) to the cloud with high network bandwidth overhead.\nSource-based deduplication involves clients uploading fingerprints (hashes) of\nencrypted chunks for duplicate checking and only uploading unique encrypted\nchunks, which reduces network transfer but introduces high latency and\npotential side-channel attacks, which need to be mitigated by Proof of\nOwnership (PoW), and high computing overhead of the cloud. So, reducing the\nlatency and the overheads of network and cloud while ensuring security has\nbecome a significant challenge for secure data deduplication in cloud storage.\nIn response to this challenge, we present PM-Dedup, a novel secure source-based\ndeduplication approach that relocates a portion of the deduplication checking\nprocess and PoW tasks from the cloud to the trusted execution environments\n(TEEs) in the client-side edge servers. We also propose various designs to\nenhance the security and efficiency of data deduplication."},{"date":"2025-01","title":"Heterogeneous Graph Pre-training Based Model for Secure and Efficient Prediction of Default Risk Propagation among Bond Issuers","author":"Xurui Li, Xin Shan, Wenhao Yin, and Haijiao Wang","link":"http://arxiv.org/abs/2501.03268v1","abstract":"Efficient prediction of default risk for bond-issuing enterprises is pivotal\nfor maintaining stability and fostering growth in the bond market. Conventional\nmethods usually rely solely on an enterprise's internal data for risk\nassessment. In contrast, graph-based techniques leverage interconnected\ncorporate information to enhance default risk identification for targeted bond\nissuers. Traditional graph techniques such as label propagation algorithm or\ndeepwalk fail to effectively integrate a enterprise's inherent attribute\ninformation with its topological network data. Additionally, due to data\nscarcity and security privacy concerns between enterprises, end-to-end graph\nneural network (GNN) algorithms may struggle in delivering satisfactory\nperformance for target tasks. To address these challenges, we present a novel\ntwo-stage model. In the first stage, we employ an innovative Masked\nAutoencoders for Heterogeneous Graph (HGMAE) to pre-train on a vast enterprise\nknowledge graph. Subsequently, in the second stage, a specialized classifier\nmodel is trained to predict default risk propagation probabilities. The\nclassifier leverages concatenated feature vectors derived from the pre-trained\nencoder with the enterprise's task-specific feature vectors. Through the\ntwo-stage training approach, our model not only boosts the importance of unique\nbond characteristics for specific default prediction tasks, but also securely\nand efficiently leverage the global information pre-trained from other\nenterprises. Experimental results demonstrate that our proposed model\noutperforms existing approaches in predicting default risk for bond issuers."},{"date":"2025-01","title":"Distillation-Enhanced Physical Adversarial Attacks","author":"Wei Liu, Yonglin Wu, Chaoqun Li, Zhuodong Liu, and Huanqian Yan","link":"http://arxiv.org/abs/2501.02232v1","abstract":"The study of physical adversarial patches is crucial for identifying\nvulnerabilities in AI-based recognition systems and developing more robust deep\nlearning models. While recent research has focused on improving patch\nstealthiness for greater practical applicability, achieving an effective\nbalance between stealth and attack performance remains a significant challenge.\nTo address this issue, we propose a novel physical adversarial attack method\nthat leverages knowledge distillation. Specifically, we first define a stealthy\ncolor space tailored to the target environment to ensure smooth blending. Then,\nwe optimize an adversarial patch in an unconstrained color space, which serves\nas the 'teacher' patch. Finally, we use an adversarial knowledge distillation\nmodule to transfer the teacher patch's knowledge to the 'student' patch,\nguiding the optimization of the stealthy patch. Experimental results show that\nour approach improves attack performance by 20%, while maintaining stealth,\nhighlighting its practical value."},{"date":"2025-01","title":"Examining the Robustness of Homogeneity Bias to Hyperparameter Adjustments in GPT-4","author":"Messi H. J. Lee","link":"http://arxiv.org/abs/2501.02211v1","abstract":"Vision-Language Models trained on massive collections of human-generated data\noften reproduce and amplify societal stereotypes. One critical form of\nstereotyping reproduced by these models is homogeneity bias-the tendency to\nrepresent certain groups as more homogeneous than others. We investigate how\nthis bias responds to hyperparameter adjustments in GPT-4, specifically\nexamining sampling temperature and top p which control the randomness of model\noutputs. By generating stories about individuals from different racial and\ngender groups and comparing their similarities using vector representations, we\nassess both bias robustness and its relationship with hyperparameter values. We\nfind that (1) homogeneity bias persists across most hyperparameter\nconfigurations, with Black Americans and women being represented more\nhomogeneously than White Americans and men, (2) the relationship between\nhyperparameters and group representations shows unexpected non-linear patterns,\nparticularly at extreme values, and (3) hyperparameter adjustments affect\nracial and gender homogeneity bias differently-while increasing temperature or\ndecreasing top p can reduce racial homogeneity bias, these changes show\ndifferent effects on gender homogeneity bias. Our findings suggest that while\nhyperparameter tuning may mitigate certain biases to some extent, it cannot\nserve as a universal solution for addressing homogeneity bias across different\nsocial group dimensions."},{"date":"2025-01","title":"Robust Multi-Dimensional Scaling via Accelerated Alternating Projections","author":"Tong Deng, and Tianming Wang","link":"http://arxiv.org/abs/2501.02208v1","abstract":"We consider the robust multi-dimensional scaling (RMDS) problem in this\npaper. The goal is to localize point locations from pairwise distances that may\nbe corrupted by outliers. Inspired by classic MDS theories, and nonconvex works\nfor the robust principal component analysis (RPCA) problem, we propose an\nalternating projection based algorithm that is further accelerated by the\ntangent space projection technique. For the proposed algorithm, if the outliers\nare sparse enough, we can establish linear convergence of the reconstructed\npoints to the original points after centering and rotation alignment. Numerical\nexperiments verify the state-of-the-art performances of the proposed algorithm."},{"date":"2025-01","title":"Secure IAM on AWS with Multi-Account Strategy","author":"Sungchan Yi","link":"http://arxiv.org/abs/2501.02203v1","abstract":"Many recent IT companies use cloud services for deploying their products,\nmainly because of their convenience. As such, cloud assets have become a new\nattack surface, and the concept of cloud security has emerged. However, cloud\nsecurity is not emphasized enough compared to on-premise security, resulting in\nmany insecure cloud architectures. In particular, small organizations often\ndon't have enough human resources to design a secure architecture, leaving them\nvulnerable to cloud security breaches.\n  We suggest the multi-account strategy for securing the cloud architecture.\nThis strategy cost-effectively improves security by separating assets and\nreducing management overheads on the cloud infrastructure. When implemented, it\nautomatically provides access restriction within the boundary of an account and\neliminates redundancies in policy management. Since access control is a\ncritical objective for constructing secure architectures, this practical method\nsuccessfully enhances security even in small companies.\n  In this paper, we analyze the benefits of multi-accounts compared to single\naccounts and explain how to deploy multiple accounts effortlessly using the\nservices provided by AWS. Then, we present possible design choices for\nmulti-account structures with a concrete example. Finally, we illustrate two\ntechniques for operational excellence on multi-account structures. We take an\nincremental approach to secure policy management with the principle of least\nprivilege and introduce methods for auditing multiple accounts."},{"date":"2025-01","title":"AdaMixup: A Dynamic Defense Framework for Membership Inference Attack Mitigation","author":"Ying Chen, Jiajing Chen, Yijie Weng, ChiaHua Chang, Dezhi Yu, and Guanbiao Lin","link":"http://arxiv.org/abs/2501.02182v1","abstract":"Membership inference attacks have emerged as a significant privacy concern in\nthe training of deep learning models, where attackers can infer whether a data\npoint was part of the training set based on the model's outputs. To address\nthis challenge, we propose a novel defense mechanism, AdaMixup. AdaMixup\nemploys adaptive mixup techniques to enhance the model's robustness against\nmembership inference attacks by dynamically adjusting the mixup strategy during\ntraining. This method not only improves the model's privacy protection but also\nmaintains high performance. Experimental results across multiple datasets\ndemonstrate that AdaMixup significantly reduces the risk of membership\ninference attacks while achieving a favorable trade-off between defensive\nefficiency and model accuracy. This research provides an effective solution for\ndata privacy protection and lays the groundwork for future advancements in\nmixup training methods."},{"date":"2025-01","title":"The Integration of Blockchain and Artificial Intelligence for Secure Healthcare Systems","author":"Umar Safdar, and Simon Gabrael","link":"http://arxiv.org/abs/2501.02169v1","abstract":"Verisign reported a 125 percent increase in data breaches within the\nhealthcare sector in the United States during 2022, with 18.2 million patient\nrecords being impacted. Growing healthcare data volumes and diversification\nmean that medical information is becoming more valuable. Many Health Centers\nuse various technologies to ease the classification, storage, and exchange of\nbig data. This use can also make the health data of the users at risk and\nvulnerable. AI and blockchain are among the leading technologies at hand. With\nAI, data-driven operations and big data efficiency have been improved with\nrespect to traditional techniques. Due to its potential to bring about\nimprovements in health services and lower medical costs, this AI technology is\nregularly used in healthcare. Blockchain helps protect transactions on sharing\ninformation and private privacy as long as the exchange of knowledge is that of\nthe standard. The objective of this analysis is to investigate the research and\nunique contributions since 2008 regarding blockchain-integrated AI and\nhealthcare systems. The work sheds light on applied AI-based healthcare schemes\nwith machine, ballistic, and acrylic learning and disparate blockchain\nstructures. The use of technology in order to ensure patient data security and\nmanage medical information effectively in healthcare settings offers a highly\nsuccessful position for both healthcare providers and patients. From 2018 to\n2021, the best year was 2021 to grow, enhancing everything to examine the\ndownload of the device and the counting of Google Academies, for which the\njoining perspective was borrowed; local research experts were asked, identified\narticles in recent years, and read reviews of large research grants."},{"date":"2025-01","title":"Exploring Secure Machine Learning Through Payload Injection and FGSM Attacks on ResNet-50","author":"Umesh Yadav, Suman Niraula, Gaurav Kumar Gupta, and Bicky Yadav","link":"http://arxiv.org/abs/2501.02147v1","abstract":"This paper investigates the resilience of a ResNet-50 image classification\nmodel under two prominent security threats: Fast Gradient Sign Method (FGSM)\nadversarial attacks and malicious payload injection. Initially, the model\nattains a 53.33% accuracy on clean images. When subjected to FGSM\nperturbations, its overall accuracy remains unchanged; however, the model's\nconfidence in incorrect predictions notably increases. Concurrently, a payload\ninjection scheme is successfully executed in 93.33% of the tested samples,\nrevealing how stealthy attacks can manipulate model predictions without\ndegrading visual quality. These findings underscore the vulnerability of even\nhigh-performing neural networks and highlight the urgency of developing more\nrobust defense mechanisms for security-critical applications."},{"date":"2025-01","title":"AVTrustBench: Assessing and Enhancing Reliability and Robustness in Audio-Visual LLMs","author":"Sanjoy Chowdhury, Sayan Nag, Subhrajyoti Dasgupta, Yaoting Wang, Mohamed Elhoseiny, Ruohan Gao, and Dinesh Manocha","link":"http://arxiv.org/abs/2501.02135v1","abstract":"With the rapid advancement of Multi-modal Large Language Models (MLLMs),\nseveral diagnostic benchmarks have recently been developed to assess these\nmodels' multi-modal reasoning proficiency. However, these benchmarks are\nrestricted to assessing primarily the visual aspect and do not examine the\nholistic audio-visual (AV) understanding. Moreover, currently, there are no\nbenchmarks that investigate the capabilities of AVLLMs to calibrate their\nresponses when presented with perturbed inputs. To this end, we introduce\nAudio-Visual Trustworthiness assessment Benchmark (AVTrustBench), comprising\n600K samples spanning over 9 meticulously crafted tasks, evaluating the\ncapabilities of AVLLMs across three distinct dimensions: Adversarial attack,\nCompositional reasoning, and Modality-specific dependency. Using our benchmark\nwe extensively evaluate 13 state-of-the-art AVLLMs. The findings reveal that\nthe majority of existing models fall significantly short of achieving\nhuman-like comprehension, offering valuable insights for future research\ndirections. To alleviate the limitations in the existing approaches, we further\npropose a robust, model-agnostic calibrated audio-visual preference\noptimization based training strategy CAVPref, obtaining a gain up to 30.19%\nacross all 9 tasks. We will publicly release our code and benchmark to\nfacilitate future research in this direction."},{"date":"2025-01","title":"K-Gate Lock: Multi-Key Logic Locking Using Input Encoding Against Oracle-Guided Attacks","author":"Kevin Lopez, and Amin Rezaei","link":"http://arxiv.org/abs/2501.02118v1","abstract":"Logic locking has emerged to prevent piracy and overproduction of integrated\ncircuits ever since the split of the design house and manufacturing foundry was\nestablished. While there has been a lot of research using a single global key\nto lock the circuit, even the most sophisticated single-key locking methods\nhave been shown to be vulnerable to powerful SAT-based oracle-guided attacks\nthat can extract the correct key with the help of an activated chip bought off\nthe market and the locked netlist leaked from the untrusted foundry. To address\nthis challenge, we propose, implement, and evaluate a novel logic locking\nmethod called K-Gate Lock that encodes input patterns using multiple keys that\nare applied to one set of key inputs at different operational times. Our\ncomprehensive experimental results confirm that using multiple keys will make\nthe circuit secure against oracle-guided attacks and increase attacker efforts\nto an exponentially time-consuming brute force search. K-Gate Lock has\nreasonable power and performance overheads, making it a practical solution for\nreal-world hardware intellectual property protection."},{"date":"2025-01","title":"Towards Robust and Accurate Stability Estimation of Local Surrogate Models in Text-based Explainable AI","author":"Christopher Burger, Charles Walter, Thai Le, and Lingwei Chen","link":"http://arxiv.org/abs/2501.02042v1","abstract":"Recent work has investigated the concept of adversarial attacks on\nexplainable AI (XAI) in the NLP domain with a focus on examining the\nvulnerability of local surrogate methods such as Lime to adversarial\nperturbations or small changes on the input of a machine learning (ML) model.\nIn such attacks, the generated explanation is manipulated while the meaning and\nstructure of the original input remain similar under the ML model. Such attacks\nare especially alarming when XAI is used as a basis for decision making (e.g.,\nprescribing drugs based on AI medical predictors) or for legal action (e.g.,\nlegal dispute involving AI software). Although weaknesses across many XAI\nmethods have been shown to exist, the reasons behind why remain little\nexplored. Central to this XAI manipulation is the similarity measure used to\ncalculate how one explanation differs from another. A poor choice of similarity\nmeasure can lead to erroneous conclusions about the stability or adversarial\nrobustness of an XAI method. Therefore, this work investigates a variety of\nsimilarity measures designed for text-based ranked lists referenced in related\nwork to determine their comparative suitability for use. We find that many\nmeasures are overly sensitive, resulting in erroneous estimates of stability.\nWe then propose a weighting scheme for text-based data that incorporates the\nsynonymity between the features within an explanation, providing more accurate\nestimates of the actual weakness of XAI methods to adversarial examples."},{"date":"2025-01","title":"Detecting and Mitigating Adversarial Attacks on Deep Learning-Based MRI Reconstruction Without Any Retraining","author":"Mahdi Saberi, Chi Zhang, and Mehmet Akcakaya","link":"http://arxiv.org/abs/2501.01908v1","abstract":"Deep learning (DL) methods, especially those based on physics-driven DL, have\nbecome the state-of-the-art for reconstructing sub-sampled magnetic resonance\nimaging (MRI) data. However, studies have shown that these methods are\nsusceptible to small adversarial input perturbations, or attacks, resulting in\nmajor distortions in the output images. Various strategies have been proposed\nto reduce the effects of these attacks, but they require retraining and may\nlower reconstruction quality for non-perturbed/clean inputs. In this work, we\npropose a novel approach for detecting and mitigating adversarial attacks on\nMRI reconstruction models without any retraining. Our detection strategy is\nbased on the idea of cyclic measurement consistency. The output of the model is\nmapped to another set of MRI measurements for a different sub-sampling pattern,\nand this synthesized data is reconstructed with the same model. Intuitively,\nwithout an attack, the second reconstruction is expected to be consistent with\nthe first, while with an attack, disruptions are present. Subsequently, this\nidea is extended to devise a novel objective function, which is minimized\nwithin a small ball around the attack input for mitigation. Experimental\nresults show that our method substantially reduces the impact of adversarial\nperturbations across different datasets, attack types/strengths and PD-DL\nnetworks, and qualitatively and quantitatively outperforms conventional\nmitigation methods that involve retraining."},{"date":"2025-01","title":"Combined Hyper-Extensible Extremely-Secured Zero-Trust CIAM-PAM architecture","author":"Shivom Aggarwal, Shourya Mehra, and Safeer Sathar","link":"http://arxiv.org/abs/2501.01732v1","abstract":"Customer Identity and Access Management (CIAM) systems play a pivotal role in\nsecuring enterprise infrastructures. However, the complexity of implementing\nthese systems requires careful architectural planning to ensure positive Return\non Investment (RoI) and avoid costly delays. The proliferation of Active\nPersistent cyber threats, coupled with advancements in AI, cloud computing, and\ngeographically distributed customer populations, necessitates a paradigm shift\ntowards adaptive and zero-trust security frameworks. This paper introduces the\nCombined Hyper-Extensible Extremely-Secured Zero-Trust (CHEZ) CIAM-PAM\narchitecture, designed specifically for large-scale enterprises. The CHEZ PL\nCIAM-PAM framework addresses critical security gaps by integrating federated\nidentity management (private and public identities), password-less\nauthentication, adaptive multi-factor authentication (MFA), microservice-based\nPEP (Policy Entitlement Point), multi-layer RBAC (Role Based Access Control)\nand multi-level trust systems. This future-proof design also includes\nend-to-end data encryption, and seamless integration with state-of-the-art\nAI-based threat detection systems, while ensuring compliance with stringent\nregulatory standards."},{"date":"2025-01","title":"Cyber Shadows: Neutralizing Security Threats with AI and Targeted Policy Measures","author":"Marc Schmitt, and Pantelis Koutroumpis","link":"http://arxiv.org/abs/2501.09025v1","abstract":"The digital age, driven by the AI revolution, brings significant\nopportunities but also conceals security threats, which we refer to as cyber\nshadows. These threats pose risks at individual, organizational, and societal\nlevels. This paper examines the systemic impact of these cyber threats and\nproposes a comprehensive cybersecurity strategy that integrates AI-driven\nsolutions, such as Intrusion Detection Systems (IDS), with targeted policy\ninterventions. By combining technological and regulatory measures, we create a\nmultilevel defense capable of addressing both direct threats and indirect\nnegative externalities. We emphasize that the synergy between AI-driven\nsolutions and policy interventions is essential for neutralizing cyber threats\nand mitigating their negative impact on the digital economy. Finally, we\nunderscore the need for continuous adaptation of these strategies, especially\nin response to the rapid advancement of autonomous AI-driven attacks, to ensure\nthe creation of secure and resilient digital ecosystems."},{"date":"2025-01","title":"Robust Self-Paced Hashing for Cross-Modal Retrieval with Noisy Labels","author":"Ruitao Pu, Yuan Sun, Yang Qin, Zhenwen Ren, Xiaomin Song, Huiming Zheng, and Dezhong Peng","link":"http://arxiv.org/abs/2501.01699v1","abstract":"Cross-modal hashing (CMH) has appeared as a popular technique for cross-modal\nretrieval due to its low storage cost and high computational efficiency in\nlarge-scale data. Most existing methods implicitly assume that multi-modal data\nis correctly labeled, which is expensive and even unattainable due to the\ninevitable imperfect annotations (i.e., noisy labels) in real-world scenarios.\nInspired by human cognitive learning, a few methods introduce self-paced\nlearning (SPL) to gradually train the model from easy to hard samples, which is\noften used to mitigate the effects of feature noise or outliers. It is a\nless-touched problem that how to utilize SPL to alleviate the misleading of\nnoisy labels on the hash model. To tackle this problem, we propose a new\ncognitive cross-modal retrieval method called Robust Self-paced Hashing with\nNoisy Labels (RSHNL), which can mimic the human cognitive process to identify\nthe noise while embracing robustness against noisy labels. Specifically, we\nfirst propose a contrastive hashing learning (CHL) scheme to improve\nmulti-modal consistency, thereby reducing the inherent semantic gap. Afterward,\nwe propose center aggregation learning (CAL) to mitigate the intra-class\nvariations. Finally, we propose Noise-tolerance Self-paced Hashing (NSH) that\ndynamically estimates the learning difficulty for each instance and\ndistinguishes noisy labels through the difficulty level. For all estimated\nclean pairs, we further adopt a self-paced regularizer to gradually learn hash\ncodes from easy to hard. Extensive experiments demonstrate that the proposed\nRSHNL performs remarkably well over the state-of-the-art CMH methods."},{"date":"2025-01","title":"Practical Secure Inference Algorithm for Fine-tuned Large Language Model Based on Fully Homomorphic Encryption","author":"Zhang Ruoyan, Zheng Zhongxiang, and Bao Wankang","link":"http://arxiv.org/abs/2501.01672v2","abstract":"Large language models(LLMs) are currently at the forefront of the machine\nlearning field, which show a broad application prospect but at the same time\nexpose some risks of privacy leakage. We combined Fully Homomorphic\nEncryption(FHE) and provable security theory with Parameter-Efficient\nFine-Tuning(PEFT) to propose an efficient and secure inference scheme for LLMs.\nMore specially, we focus on pre-trained LLMs which rely on open-sourced base\nmodel and then fine-tuned with the private datasets by LoRA. This is a popular\nroad-map for Vertical Domain Models such as LawGPT and BenTsao. We use two key\ntechnologies below. Firstly, we divide the whole model into the public part and\nthe private part. The weights of public part are publicly accessible(e.g. the\nopen-sourced base model) while the private part needs to be protected(e.g. the\nLoRA matrices). In this way, the overhead brought by computing on private data\ncan be greatly reduced. Secondly, we propose a general method to transform a\nlinear layer into another one which provides security against model extraction\nattacks and preserves its original functionality, which denoted as Private\nLinear Layer(PLL). Then we use this method on the LoRA matrices to make sure\nthat the server protects their private weights without restricting the user's\ninput. We also show that the difficulty of performing model extraction attacks\nfor PLL can be reduced to the well-known hard problem Learning with\nErrors(LWE). Combing this method with FHE, we can protect user's input at the\nsame time. In this paper, we use the open-source model ChatGLM2-6B as the base\nmodel which is fine-tuned by LoRA. Experimental results show the inference\nefficiency of our scheme reaches 1.61s/token which displays that the scheme has\ngood practicality."},{"date":"2025-01","title":"BARTPredict: Empowering IoT Security with LLM-Driven Cyber Threat Prediction","author":"Alaeddine Diaf, Abdelaziz Amara Korba, Nour Elislem Karabadji, and Yacine Ghamri-Doudane","link":"http://arxiv.org/abs/2501.01664v1","abstract":"The integration of Internet of Things (IoT) technology in various domains has\nled to operational advancements, but it has also introduced new vulnerabilities\nto cybersecurity threats, as evidenced by recent widespread cyberattacks on IoT\ndevices. Intrusion detection systems are often reactive, triggered by specific\npatterns or anomalies observed within the network. To address this challenge,\nthis work proposes a proactive approach to anticipate and preemptively mitigate\nmalicious activities, aiming to prevent potential damage before it occurs. This\npaper proposes an innovative intrusion prediction framework empowered by\nPre-trained Large Language Models (LLMs). The framework incorporates two LLMs:\na fine-tuned Bidirectional and AutoRegressive Transformers (BART) model for\npredicting network traffic and a fine-tuned Bidirectional Encoder\nRepresentations from Transformers (BERT) model for evaluating the predicted\ntraffic. By harnessing the bidirectional capabilities of BART the framework\nthen identifies malicious packets among these predictions. Evaluated using the\nCICIoT2023 IoT attack dataset, our framework showcases a notable enhancement in\npredictive performance, attaining an impressive 98% overall accuracy, providing\na powerful response to the cybersecurity challenges that confront IoT networks."},{"date":"2025-01","title":"AVATAR: Adversarial Autoencoders with Autoregressive Refinement for Time Series Generation","author":"MohammadReza EskandariNasab, Shah Muhammad Hamdi, and Soukaina Filali Boubrahimi","link":"http://arxiv.org/abs/2501.01649v1","abstract":"Data augmentation can significantly enhance the performance of machine\nlearning tasks by addressing data scarcity and improving generalization.\nHowever, generating time series data presents unique challenges. A model must\nnot only learn a probability distribution that reflects the real data\ndistribution but also capture the conditional distribution at each time step to\npreserve the inherent temporal dependencies. To address these challenges, we\nintroduce AVATAR, a framework that combines Adversarial Autoencoders (AAE) with\nAutoregressive Learning to achieve both objectives. Specifically, our technique\nintegrates the autoencoder with a supervisor and introduces a novel supervised\nloss to assist the decoder in learning the temporal dynamics of time series\ndata. Additionally, we propose another innovative loss function, termed\ndistribution loss, to guide the encoder in more efficiently aligning the\naggregated posterior of the autoencoder's latent representation with a prior\nGaussian distribution. Furthermore, our framework employs a joint training\nmechanism to simultaneously train all networks using a combined loss, thereby\nfulfilling the dual objectives of time series generation. We evaluate our\ntechnique across a variety of time series datasets with diverse\ncharacteristics. Our experiments demonstrate significant improvements in both\nthe quality and practical utility of the generated data, as assessed by various\nqualitative and quantitative metrics."},{"date":"2025-01","title":"Adaptive Meta-learning-based Adversarial Training for Robust Automatic Modulation Classification","author":"Amirmohammad Bamdad, Ali Owfi, and Fatemeh Afghah","link":"http://arxiv.org/abs/2501.01620v1","abstract":"DL-based automatic modulation classification (AMC) models are highly\nsusceptible to adversarial attacks, where even minimal input perturbations can\ncause severe misclassifications. While adversarially training an AMC model\nbased on an adversarial attack significantly increases its robustness against\nthat attack, the AMC model will still be defenseless against other adversarial\nattacks. The theoretically infinite possibilities for adversarial perturbations\nmean that an AMC model will inevitably encounter new unseen adversarial attacks\nif it is ever to be deployed to a real-world communication system. Moreover,\nthe computational limitations and challenges of obtaining new data in real-time\nwill not allow a full training process for the AMC model to adapt to the new\nattack when it is online. To this end, we propose a meta-learning-based\nadversarial training framework for AMC models that substantially enhances\nrobustness against unseen adversarial attacks and enables fast adaptation to\nthese attacks using just a few new training samples, if any are available. Our\nresults demonstrate that this training framework provides superior robustness\nand accuracy with much less online training time than conventional adversarial\ntraining of AMC models, making it highly efficient for real-world deployment."},{"date":"2025-01","title":"BLAST: A Stealthy Backdoor Leverage Attack against Cooperative Multi-Agent Deep Reinforcement Learning based Systems","author":"Yinbo Yu, Saihao Yan, Xueyu Yin, Jing Fang, and Jiajia Liu","link":"http://arxiv.org/abs/2501.01593v1","abstract":"Recent studies have shown that cooperative multi-agent deep reinforcement\nlearning (c-MADRL) is under the threat of backdoor attacks. Once a backdoor\ntrigger is observed, it will perform malicious actions leading to failures or\nmalicious goals. However, existing backdoor attacks suffer from several issues,\ne.g., instant trigger patterns lack stealthiness, the backdoor is trained or\nactivated by an additional network, or all agents are backdoored. To this end,\nin this paper, we propose a novel backdoor leverage attack against c-MADRL,\nBLAST, which attacks the entire multi-agent team by embedding the backdoor only\nin a single agent. Firstly, we introduce adversary spatiotemporal behavior\npatterns as the backdoor trigger rather than manual-injected fixed visual\npatterns or instant status and control the period to perform malicious actions.\nThis method can guarantee the stealthiness and practicality of BLAST. Secondly,\nwe hack the original reward function of the backdoor agent via unilateral\nguidance to inject BLAST, so as to achieve the \\textit{leverage attack effect}\nthat can pry open the entire multi-agent system via a single backdoor agent. We\nevaluate our BLAST against 3 classic c-MADRL algorithms (VDN, QMIX, and MAPPO)\nin 2 popular c-MADRL environments (SMAC and Pursuit), and 2 existing defense\nmechanisms. The experimental results demonstrate that BLAST can achieve a high\nattack success rate while maintaining a low clean performance variance rate."},{"date":"2025-01","title":"Click-Calib: A Robust Extrinsic Calibration Method for Surround-View Systems","author":"Lihao Wang","link":"http://arxiv.org/abs/2501.01557v2","abstract":"Surround-View System (SVS) is an essential component in Advanced Driver\nAssistance System (ADAS) and requires precise calibrations. However,\nconventional offline extrinsic calibration methods are cumbersome and\ntime-consuming as they rely heavily on physical patterns. Additionally, these\nmethods primarily focus on short-range areas surrounding the vehicle, resulting\nin lower calibration quality in more distant zones. To address these\nlimitations, we propose Click-Calib, a pattern-free approach for offline SVS\nextrinsic calibration. Without requiring any special setup, the user only needs\nto click a few keypoints on the ground in natural scenes. Unlike other offline\ncalibration approaches, Click-Calib optimizes camera poses over a wide range by\nminimizing reprojection distance errors of keypoints, thereby achieving\naccurate calibrations at both short and long distances. Furthermore,\nClick-Calib supports both single-frame and multiple-frame modes, with the\nlatter offering even better results. Evaluations on our in-house dataset and\nthe public WoodScape dataset demonstrate its superior accuracy and robustness\ncompared to baseline methods. Code is available at\nhttps://github.com/lwangvaleo/click_calib."},{"date":"2025-01","title":"SAFER: Sharpness Aware layer-selective Finetuning for Enhanced Robustness in vision transformers","author":"Bhavna Gopal, Huanrui Yang, Mark Horton, and Yiran Chen","link":"http://arxiv.org/abs/2501.01529v1","abstract":"Vision transformers (ViTs) have become essential backbones in advanced\ncomputer vision applications and multi-modal foundation models. Despite their\nstrengths, ViTs remain vulnerable to adversarial perturbations, comparable to\nor even exceeding the vulnerability of convolutional neural networks (CNNs).\nFurthermore, the large parameter count and complex architecture of ViTs make\nthem particularly prone to adversarial overfitting, often compromising both\nclean and adversarial accuracy.\n  This paper mitigates adversarial overfitting in ViTs through a novel,\nlayer-selective fine-tuning approach: SAFER. Instead of optimizing the entire\nmodel, we identify and selectively fine-tune a small subset of layers most\nsusceptible to overfitting, applying sharpness-aware minimization to these\nlayers while freezing the rest of the model. Our method consistently enhances\nboth clean and adversarial accuracy over baseline approaches. Typical\nimprovements are around 5%, with some cases achieving gains as high as 20%\nacross various ViT architectures and datasets."},{"date":"2025-01","title":"Securing Wi-Fi 6 Connection Establishment Against Relay and Spoofing Threats","author":"Naureen Hoque, and Hanif Rahbari","link":"http://arxiv.org/abs/2501.01517v1","abstract":"Wireless local area networks remain vulnerable to attacks initiated during\nthe connection establishment (CE) phase. Current Wi-Fi security protocols fail\nto fully mitigate attacks like man-in-the-middle, preamble spoofing, and\nrelaying. To fortify the CE phase, in this paper we design a\nbackward-compatible scheme using a digital signature interwoven into the\npreambles at the physical (PHY) layer with time constraints to effectively\ncounter those attacks. This approach slices a MAC-layer signature and embeds\nthe slices within CE frame preambles without extending frame size, allowing one\nor multiple stations to concurrently verify their respective APs'\ntransmissions. The concurrent CEs are supported by enabling the stations to\nanalyze the consistent patterns of PHY-layer headers and identify whether the\nreceived frames are the anticipated ones from the expected APs, achieving 100%\naccuracy without needing to examine their MAC-layer headers. Additionally, we\ndesign and implement a fast relay attack to challenge our proposed defense and\ndetermine its effectiveness. We extend existing open-source tools to support\nIEEE 802.11ax to evaluate the effectiveness and practicality of our proposed\nscheme in a testbed consisting of USRPs, commercial APs, and Wi-Fi devices, and\nwe show that our relay attack detection achieves 96-100% true positive rates.\nFinally, end-to-end formal security analyses confirm the security and\ncorrectness of the proposed solution."},{"date":"2025-01","title":"Improving Robustness Estimates in Natural Language Explainable AI though Synonymity Weighted Similarity Measures","author":"Christopher Burger","link":"http://arxiv.org/abs/2501.01516v1","abstract":"Explainable AI (XAI) has seen a surge in recent interest with the\nproliferation of powerful but intractable black-box models. Moreover, XAI has\ncome under fire for techniques that may not offer reliable explanations. As\nmany of the methods in XAI are themselves models, adversarial examples have\nbeen prominent in the literature surrounding the effectiveness of XAI, with the\nobjective of these examples being to alter the explanation while maintaining\nthe output of the original model. For explanations in natural language, it is\nnatural to use measures found in the domain of information retrieval for use\nwith ranked lists to guide the adversarial XAI process. We show that the\nstandard implementation of these measures are poorly suited for the comparison\nof explanations in adversarial XAI and amend them by using information that is\ndiscarded, the synonymity of perturbed words. This synonymity weighting\nproduces more accurate estimates of the actual weakness of XAI methods to\nadversarial examples."},{"date":"2025-01","title":"R-SCoRe: Revisiting Scene Coordinate Regression for Robust Large-Scale Visual Localization","author":"Xudong Jiang, Fangjinhua Wang, Silvano Galliani, Christoph Vogel, and Marc Pollefeys","link":"http://arxiv.org/abs/2501.01421v1","abstract":"Learning-based visual localization methods that use scene coordinate\nregression (SCR) offer the advantage of smaller map sizes. However, on datasets\nwith complex illumination changes or image-level ambiguities, it remains a less\nrobust alternative to feature matching methods. This work aims to close the\ngap. We introduce a covisibility graph-based global encoding learning and data\naugmentation strategy, along with a depth-adjusted reprojection loss to\nfacilitate implicit triangulation. Additionally, we revisit the network\narchitecture and local feature extraction module. Our method achieves\nstate-of-the-art on challenging large-scale datasets without relying on network\nensembles or 3D supervision. On Aachen Day-Night, we are 10$\\times$ more\naccurate than previous SCR methods with similar map sizes and require at least\n5$\\times$ smaller map sizes than any other SCR method while still delivering\nsuperior accuracy. Code will be available at: https://github.com/cvg/scrstudio ."},{"date":"2025-01","title":"Best Transition Matrix Esitimation or Best Label Noise Robustness Classifier? Two Possible Methods to Enhance the Performance of T-revision","author":"Haixu Liu, Zerui Tao, Naihui Zhang, and Sixing Liu","link":"http://arxiv.org/abs/2501.01402v1","abstract":"Label noise refers to incorrect labels in a dataset caused by human errors or\ncollection defects, which is common in real-world applications and can\nsignificantly reduce the accuracy of models. This report explores how to\nestimate noise transition matrices and construct deep learning classifiers that\nare robust against label noise. In cases where the transition matrix is known,\nwe apply forward correction and importance reweighting methods to correct the\nimpact of label noise using the transition matrix. When the transition matrix\nis unknown or inaccurate, we use the anchor point assumption and T-Revision\nseries methods to estimate or correct the noise matrix. In this study, we\nfurther improved the T-Revision method by developing T-Revision-Alpha and\nT-Revision-Softmax to enhance stability and robustness. Additionally, we\ndesigned and implemented two baseline classifiers, a Multi-Layer Perceptron\n(MLP) and ResNet-18, based on the cross-entropy loss function. We compared the\nperformance of these methods on predicting clean labels and estimating\ntransition matrices using the FashionMINIST dataset with known noise transition\nmatrices. For the CIFAR-10 dataset, where the noise transition matrix is\nunknown, we estimated the noise matrix and evaluated the ability of the methods\nto predict clean labels."},{"date":"2025-01","title":"Analysis of Security in OS-Level Virtualization","author":"Krishna Sai Ketha, Guanqun Song, and Ting Zhu","link":"http://arxiv.org/abs/2501.01334v1","abstract":"Virtualization is a technique that allows multiple instances typically\nrunning different guest operating systems on top of single physical hardware. A\nhypervisor, a layer of software running on top of the host operating system,\ntypically runs and manages these different guest operating systems. Rather than\nto run different services on different servers for reliability and security\nreasons, companies started to employ virtualization over their servers to run\nthese services within a single server. This approach proves beneficial to the\ncompanies as it provides much better reliability, stronger isolation, improved\nsecurity and resource utilization compared to running services on multiple\nservers. Although hypervisor based virtualization offers better resource\nutilization and stronger isolation, it also suffers from high overhead as the\nhost operating system has to maintain different guest operating systems.\n  To tackle this issue, another form of virtualization known as Operating\nSystem-level virtualization has emerged. This virtualization provides\nlight-weight, minimal and efficient virtualization, as the different instances\nare run on top of the same host operating system, sharing the resources of the\nhost operating system. But due to instances sharing the same host operating\nsystem affects the isolation of the instances. In this paper, we will first\nestablish the basic concepts of virtualization and point out the differences\nbetween the hyper-visor based virtualization and operating system-level\nvirtualization. Next, we will discuss the container creation life-cycle which\nhelps in forming a container threat model for the container systems, which\nallows to map different potential attack vectors within these systems. Finally,\nwe will discuss a case study, which further looks at isolation provided by the\ncontainers."},{"date":"2025-01","title":"HybridTrack: A Hybrid Approach for Robust Multi-Object Tracking","author":"Leandro Di Bella, Yangxintong Lyu, Bruno Cornelis, and Adrian Munteanu","link":"http://arxiv.org/abs/2501.01275v1","abstract":"The evolution of Advanced Driver Assistance Systems (ADAS) has increased the\nneed for robust and generalizable algorithms for multi-object tracking.\nTraditional statistical model-based tracking methods rely on predefined motion\nmodels and assumptions about system noise distributions. Although\ncomputationally efficient, they often lack adaptability to varying traffic\nscenarios and require extensive manual design and parameter tuning. To address\nthese issues, we propose a novel 3D multi-object tracking approach for\nvehicles, HybridTrack, which integrates a data-driven Kalman Filter (KF) within\na tracking-by-detection paradigm. In particular, it learns the transition\nresidual and Kalman gain directly from data, which eliminates the need for\nmanual motion and stochastic parameter modeling. Validated on the real-world\nKITTI dataset, HybridTrack achieves 82.08% HOTA accuracy, significantly\noutperforming state-of-the-art methods. We also evaluate our method under\ndifferent configurations, achieving the fastest processing speed of 112 FPS.\nConsequently, HybridTrack eliminates the dependency on scene-specific designs\nwhile improving performance and maintaining real-time efficiency. The code will\nbe publicly available at the time of publishing:\nhttps://github.com/leandro-svg/HybridTrack.git."},{"date":"2025-01","title":"Stealthy Backdoor Attack to Real-world Models in Android Apps","author":"Jiali Wei, Ming Fan, Xicheng Zhang, Wenjing Jiao, Haijun Wang, and Ting Liu","link":"http://arxiv.org/abs/2501.01263v1","abstract":"Powered by their superior performance, deep neural networks (DNNs) have found\nwidespread applications across various domains. Many deep learning (DL) models\nare now embedded in mobile apps, making them more accessible to end users\nthrough on-device DL. However, deploying on-device DL to users' smartphones\nsimultaneously introduces several security threats. One primary threat is\nbackdoor attacks. Extensive research has explored backdoor attacks for several\nyears and has proposed numerous attack approaches. However, few studies have\ninvestigated backdoor attacks on DL models deployed in the real world, or they\nhave shown obvious deficiencies in effectiveness and stealthiness. In this\nwork, we explore more effective and stealthy backdoor attacks on real-world DL\nmodels extracted from mobile apps. Our main justification is that imperceptible\nand sample-specific backdoor triggers generated by DNN-based steganography can\nenhance the efficacy of backdoor attacks on real-world models. We first confirm\nthe effectiveness of steganography-based backdoor attacks on four\nstate-of-the-art DNN models. Subsequently, we systematically evaluate and\nanalyze the stealthiness of the attacks to ensure they are difficult to\nperceive. Finally, we implement the backdoor attacks on real-world models and\ncompare our approach with three baseline methods. We collect 38,387 mobile\napps, extract 89 DL models from them, and analyze these models to obtain the\nprerequisite model information for the attacks. After identifying the target\nmodels, our approach achieves an average of 12.50% higher attack success rate\nthan DeepPayload while better maintaining the normal performance of the models.\nExtensive experimental results demonstrate that our method enables more\neffective, robust, and stealthy backdoor attacks on real-world models."},{"date":"2025-01","title":"A Game Between the Defender and the Attacker for Trigger-based Black-box Model Watermarking","author":"Chaoyue Huang, and Hanzhou Wu","link":"http://arxiv.org/abs/2501.01194v1","abstract":"Watermarking deep neural network (DNN) models has attracted a great deal of\nattention and interest in recent years because of the increasing demand to\nprotect the intellectual property of DNN models. Many practical algorithms have\nbeen proposed by covertly embedding a secret watermark into a given DNN model\nthrough either parametric/structural modulation or backdooring against\nintellectual property infringement from the attacker while preserving the model\nperformance on the original task. Despite the performance of these approaches,\nthe lack of basic research restricts the algorithmic design to either a\ntrial-based method or a data-driven technique. This has motivated the authors\nin this paper to introduce a game between the model attacker and the model\ndefender for trigger-based black-box model watermarking. For each of the two\nplayers, we construct the payoff function and determine the optimal response,\nwhich enriches the theoretical foundation of model watermarking and may inspire\nus to develop novel schemes in the future."},{"date":"2025-01","title":"NET-SA: An Efficient Secure Aggregation Architecture Based on In-Network Computing","author":"Qingqing Ren, Wen Wang, Shuyong Zhu, Zhiyuan Wu, and Yujun Zhang","link":"http://arxiv.org/abs/2501.01187v1","abstract":"Privacy-preserving machine learning (PPML) enables clients to collaboratively\ntrain deep learning models without sharing private datasets, but faces privacy\nleakage risks due to gradient leakage attacks. Prevailing methods leverage\nsecure aggregation strategies to enhance PPML, where clients leverage masks and\nsecret sharing to further protect gradient data while tolerating participant\ndropouts. These methods, however, require frequent inter-client communication\nto negotiate keys and perform secret sharing, leading to substantial\ncommunication overhead. To tackle this issue, we propose NET-SA, an efficient\nsecure aggregation architecture for PPML based on in-network computing. NET-SA\nemploys seed homomorphic pseudorandom generators for local gradient masking and\nutilizes programmable switches for seed aggregation. Accurate and secure\ngradient aggregation is then performed on the central server based on masked\ngradients and aggregated seeds. This design effectively reduces communication\noverhead due to eliminating the communication-intensive phases of seed\nagreement and secret sharing, with enhanced dropout tolerance due to overcoming\nthe threshold limit of secret sharing. Extensive experiments on server clusters\nand Intel Tofino programmable switch demonstrate that NET-SA achieves up to 77x\nand 12x enhancements in runtime and 2x decrease in total client communication\ncost compared with state-of-the-art methods."},{"date":"2025-01","title":"An Inclusive Theoretical Framework of Robust Supervised Contrastive Loss against Label Noise","author":"Jingyi Cui, Yi-Ge Zhang, Hengyu Liu, and Yisen Wang","link":"http://arxiv.org/abs/2501.01130v1","abstract":"Learning from noisy labels is a critical challenge in machine learning, with\nvast implications for numerous real-world scenarios. While supervised\ncontrastive learning has recently emerged as a powerful tool for navigating\nlabel noise, many existing solutions remain heuristic, often devoid of a\nsystematic theoretical foundation for crafting robust supervised contrastive\nlosses. To address the gap, in this paper, we propose a unified theoretical\nframework for robust losses under the pairwise contrastive paradigm. In\nparticular, we for the first time derive a general robust condition for\narbitrary contrastive losses, which serves as a criterion to verify the\ntheoretical robustness of a supervised contrastive loss against label noise.\nThe theory indicates that the popular InfoNCE loss is in fact non-robust, and\naccordingly inspires us to develop a robust version of InfoNCE, termed\nSymmetric InfoNCE (SymNCE). Moreover, we highlight that our theory is an\ninclusive framework that provides explanations to prior robust techniques such\nas nearest-neighbor (NN) sample selection and robust contrastive loss.\nValidation experiments on benchmark datasets demonstrate the superiority of\nSymNCE against label noise."},{"date":"2025-01","title":"Robust COVID-19 Detection from Cough Sounds using Deep Neural Decision Tree and Forest: A Comprehensive Cross-Datasets Evaluation","author":"Rofiqul Islam, Nihad Karim Chowdhury, and Muhammad Ashad Kabir","link":"http://arxiv.org/abs/2501.01117v1","abstract":"This research presents a robust approach to classifying COVID-19 cough sounds\nusing cutting-edge machine-learning techniques. Leveraging deep neural decision\ntrees and deep neural decision forests, our methodology demonstrates consistent\nperformance across diverse cough sound datasets. We begin with a comprehensive\nextraction of features to capture a wide range of audio features from\nindividuals, whether COVID-19 positive or negative. To determine the most\nimportant features, we use recursive feature elimination along with\ncross-validation. Bayesian optimization fine-tunes hyper-parameters of deep\nneural decision tree and deep neural decision forest models. Additionally, we\nintegrate the SMOTE during training to ensure a balanced representation of\npositive and negative data. Model performance refinement is achieved through\nthreshold optimization, maximizing the ROC-AUC score. Our approach undergoes a\ncomprehensive evaluation in five datasets: Cambridge, Coswara, COUGHVID,\nVirufy, and the combined Virufy with the NoCoCoDa dataset. Consistently\noutperforming state-of-the-art methods, our proposed approach yields notable\nAUC scores of 0.97, 0.98, 0.92, 0.93, 0.99, and 0.99 across the respective\ndatasets. Merging all datasets into a combined dataset, our method, using a\ndeep neural decision forest classifier, achieves an AUC of 0.97. Also, our\nstudy includes a comprehensive cross-datasets analysis, revealing demographic\nand geographic differences in the cough sounds associated with COVID-19. These\ndifferences highlight the challenges in transferring learned features across\ndiverse datasets and underscore the potential benefits of dataset integration,\nimproving generalizability and enhancing COVID-19 detection from audio signals."},{"date":"2025-01","title":"AIM: Additional Image Guided Generation of Transferable Adversarial Attacks","author":"Teng Li, Xingjun Ma, and Yu-Gang Jiang","link":"http://arxiv.org/abs/2501.01106v1","abstract":"Transferable adversarial examples highlight the vulnerability of deep neural\nnetworks (DNNs) to imperceptible perturbations across various real-world\napplications. While there have been notable advancements in untargeted\ntransferable attacks, targeted transferable attacks remain a significant\nchallenge. In this work, we focus on generative approaches for targeted\ntransferable attacks. Current generative attacks focus on reducing overfitting\nto surrogate models and the source data domain, but they often overlook the\nimportance of enhancing transferability through additional semantics. To\naddress this issue, we introduce a novel plug-and-play module into the general\ngenerator architecture to enhance adversarial transferability. Specifically, we\npropose a \\emph{Semantic Injection Module} (SIM) that utilizes the semantics\ncontained in an additional guiding image to improve transferability. The\nguiding image provides a simple yet effective method to incorporate target\nsemantics from the target class to create targeted and highly transferable\nattacks. Additionally, we propose new loss formulations that can integrate the\nsemantic injection module more effectively for both targeted and untargeted\nattacks. We conduct comprehensive experiments under both targeted and\nuntargeted attack settings to demonstrate the efficacy of our proposed\napproach."},{"date":"2025-01","title":"HoneypotNet: Backdoor Attacks Against Model Extraction","author":"Yixu Wang, Tianle Gu, Yan Teng, Yingchun Wang, and Xingjun Ma","link":"http://arxiv.org/abs/2501.01090v1","abstract":"Model extraction attacks are one type of inference-time attacks that\napproximate the functionality and performance of a black-box victim model by\nlaunching a certain number of queries to the model and then leveraging the\nmodel's predictions to train a substitute model. These attacks pose severe\nsecurity threats to production models and MLaaS platforms and could cause\nsignificant monetary losses to the model owners. A body of work has proposed to\ndefend machine learning models against model extraction attacks, including both\nactive defense methods that modify the model's outputs or increase the query\noverhead to avoid extraction and passive defense methods that detect malicious\nqueries or leverage watermarks to perform post-verification. In this work, we\nintroduce a new defense paradigm called attack as defense which modifies the\nmodel's output to be poisonous such that any malicious users that attempt to\nuse the output to train a substitute model will be poisoned. To this end, we\npropose a novel lightweight backdoor attack method dubbed HoneypotNet that\nreplaces the classification layer of the victim model with a honeypot layer and\nthen fine-tunes the honeypot layer with a shadow model (to simulate model\nextraction) via bi-level optimization to modify its output to be poisonous\nwhile remaining the original performance. We empirically demonstrate on four\ncommonly used benchmark datasets that HoneypotNet can inject backdoors into\nsubstitute models with a high success rate. The injected backdoor not only\nfacilitates ownership verification but also disrupts the functionality of\nsubstitute models, serving as a significant deterrent to model extraction\nattacks."},{"date":"2025-01","title":"FAPL-DM-BC: A Secure and Scalable FL Framework with Adaptive Privacy and Dynamic Masking, Blockchain, and XAI for the IoVs","author":"Sathwik Narkedimilli, Amballa Venkata Sriram, Sujith Makam, MSVPJ Sathvik, and Sai Prashanth Mallellu","link":"http://arxiv.org/abs/2501.01063v1","abstract":"The FAPL-DM-BC solution is a new FL-based privacy, security, and scalability\nsolution for the Internet of Vehicles (IoV). It leverages Federated Adaptive\nPrivacy-Aware Learning (FAPL) and Dynamic Masking (DM) to learn and adaptively\nchange privacy policies in response to changing data sensitivity and state in\nreal-time, for the optimal privacy-utility tradeoff. Secure Logging and\nVerification, Blockchain-based provenance and decentralized validation, and\nCloud Microservices Secure Aggregation using FedAvg (Federated Averaging) and\nSecure Multi-Party Computation (SMPC). Two-model feedback, driven by\nModel-Agnostic Explainable AI (XAI), certifies local predictions and\nexplanations to drive it to the next level of efficiency. Combining local\nfeedback with world knowledge through a weighted mean computation, FAPL-DM-BC\nassures federated learning that is secure, scalable, and interpretable.\nSelf-driving cars, traffic management, and forecasting, vehicular network\ncybersecurity in real-time, and smart cities are a few possible applications of\nthis integrated, privacy-safe, and high-performance IoV platform."},{"date":"2025-01","title":"Image-based Multimodal Models as Intruders: Transferable Multimodal Attacks on Video-based MLLMs","author":"Linhao Huang, Xue Jiang, Zhiqiang Wang, Wentao Mo, Xi Xiao, Bo Han, Yongjie Yin, and Feng Zheng","link":"http://arxiv.org/abs/2501.01042v2","abstract":"Video-based multimodal large language models (V-MLLMs) have shown\nvulnerability to adversarial examples in video-text multimodal tasks. However,\nthe transferability of adversarial videos to unseen models--a common and\npractical real world scenario--remains unexplored. In this paper, we pioneer an\ninvestigation into the transferability of adversarial video samples across\nV-MLLMs. We find that existing adversarial attack methods face significant\nlimitations when applied in black-box settings for V-MLLMs, which we attribute\nto the following shortcomings: (1) lacking generalization in perturbing video\nfeatures, (2) focusing only on sparse key-frames, and (3) failing to integrate\nmultimodal information. To address these limitations and deepen the\nunderstanding of V-MLLM vulnerabilities in black-box scenarios, we introduce\nthe Image-to-Video MLLM (I2V-MLLM) attack. In I2V-MLLM, we utilize an\nimage-based multimodal model (IMM) as a surrogate model to craft adversarial\nvideo samples. Multimodal interactions and temporal information are integrated\nto disrupt video representations within the latent space, improving adversarial\ntransferability. In addition, a perturbation propagation technique is\nintroduced to handle different unknown frame sampling strategies. Experimental\nresults demonstrate that our method can generate adversarial examples that\nexhibit strong transferability across different V-MLLMs on multiple video-text\nmultimodal tasks. Compared to white-box attacks on these models, our black-box\nattacks (using BLIP-2 as surrogate model) achieve competitive performance, with\naverage attack success rates of 55.48% on MSVD-QA and 58.26% on MSRVTT-QA for\nVideoQA tasks, respectively. Our code will be released upon acceptance."},{"date":"2025-01","title":"Towards Adversarially Robust Deep Metric Learning","author":"Xiaopeng Ke","link":"http://arxiv.org/abs/2501.01025v2","abstract":"Deep Metric Learning (DML) has shown remarkable successes in many domains by\ntaking advantage of powerful deep neural networks. Deep neural networks are\nprone to adversarial attacks and could be easily fooled by adversarial\nexamples. The current progress on this robustness issue is mainly about deep\nclassification models but pays little attention to DML models. Existing works\nfail to thoroughly inspect the robustness of DML and neglect an important DML\nscenario, the clustering-based inference. In this work, we first point out the\nrobustness issue of DML models in clustering-based inference scenarios. We find\nthat, for the clustering-based inference, existing defenses designed DML are\nunable to be reused and the adaptions of defenses designed for deep\nclassification models cannot achieve satisfactory robustness performance. To\nalleviate the hazard of adversarial examples, we propose a new defense, the\nEnsemble Adversarial Training (EAT), which exploits ensemble learning and\nadversarial training. EAT promotes the diversity of the ensemble, encouraging\neach model in the ensemble to have different robustness features, and employs a\nself-transferring mechanism to make full use of the robustness statistics of\nthe whole ensemble in the update of every single model. We evaluate the EAT\nmethod on three widely-used datasets with two popular model architectures. The\nresults show that the proposed EAT method greatly outperforms the adaptions of\ndefenses designed for deep classification models."},{"date":"2025-01","title":"Boosting Adversarial Transferability with Spatial Adversarial Alignment","author":"Zhaoyu Chen, Haijing Guo, Kaixun Jiang, Jiyuan Fu, Xinyu Zhou, Dingkang Yang, Hao Tang, Bo Li, and Wenqiang Zhang","link":"http://arxiv.org/abs/2501.01015v1","abstract":"Deep neural networks are vulnerable to adversarial examples that exhibit\ntransferability across various models. Numerous approaches are proposed to\nenhance the transferability of adversarial examples, including advanced\noptimization, data augmentation, and model modifications. However, these\nmethods still show limited transferability, particularly in cross-architecture\nscenarios, such as from CNN to ViT. To achieve high transferability, we propose\na technique termed Spatial Adversarial Alignment (SAA), which employs an\nalignment loss and leverages a witness model to fine-tune the surrogate model.\nSpecifically, SAA consists of two key parts: spatial-aware alignment and\nadversarial-aware alignment. First, we minimize the divergences of features\nbetween the two models in both global and local regions, facilitating spatial\nalignment. Second, we introduce a self-adversarial strategy that leverages\nadversarial examples to impose further constraints, aligning features from an\nadversarial perspective. Through this alignment, the surrogate model is trained\nto concentrate on the common features extracted by the witness model. This\nfacilitates adversarial attacks on these shared features, thereby yielding\nperturbations that exhibit enhanced transferability. Extensive experiments on\nvarious architectures on ImageNet show that aligned surrogate models based on\nSAA can provide higher transferable adversarial examples, especially in\ncross-architecture attacks."},{"date":"2025-01","title":"Demystifying Online Clustering of Bandits: Enhanced Exploration Under Stochastic and Smoothed Adversarial Contexts","author":"Zhuohua Li, Maoli Liu, Xiangxiang Dai, and John C. S. Lui","link":"http://arxiv.org/abs/2501.00891v1","abstract":"The contextual multi-armed bandit (MAB) problem is crucial in sequential\ndecision-making. A line of research, known as online clustering of bandits,\nextends contextual MAB by grouping similar users into clusters, utilizing\nshared features to improve learning efficiency. However, existing algorithms,\nwhich rely on the upper confidence bound (UCB) strategy, struggle to gather\nadequate statistical information to accurately identify unknown user clusters.\nAs a result, their theoretical analyses require several strong assumptions\nabout the \"diversity\" of contexts generated by the environment, leading to\nimpractical settings, complicated analyses, and poor practical performance.\nRemoving these assumptions has been a long-standing open problem in the\nclustering of bandits literature. In this paper, we provide two solutions to\nthis open problem. First, following the i.i.d. context generation setting in\nexisting studies, we propose two novel algorithms, UniCLUB and PhaseUniCLUB,\nwhich incorporate enhanced exploration mechanisms to accelerate cluster\nidentification. Remarkably, our algorithms require substantially weaker\nassumptions while achieving regret bounds comparable to prior work. Second,\ninspired by the smoothed analysis framework, we propose a more practical\nsetting that eliminates the requirement for i.i.d. context generation used in\nprevious studies, thus enhancing the performance of existing algorithms for\nonline clustering of bandits. Our technique can be applied to both graph-based\nand set-based clustering of bandits frameworks. Extensive evaluations on both\nsynthetic and real-world datasets demonstrate that our proposed algorithms\nconsistently outperform existing approaches."},{"date":"2025-01","title":"A Survey of Secure Semantic Communications","author":"Rui Meng, Song Gao, Dayu Fan, Haixiao Gao, Yining Wang, Xiaodong Xu, Bizhu Wang, Suyu Lv, Zhidi Zhang, Mengying Sun, Shujun Han, Chen Dong, Xiaofeng Tao, and Ping Zhang","link":"http://arxiv.org/abs/2501.00842v1","abstract":"Semantic communication (SemCom) is regarded as a promising and revolutionary\ntechnology in 6G, aiming to transcend the constraints of ``Shannon's trap\" by\nfiltering out redundant information and extracting the core of effective data.\nCompared to traditional communication paradigms, SemCom offers several notable\nadvantages, such as reducing the burden on data transmission, enhancing network\nmanagement efficiency, and optimizing resource allocation. Numerous researchers\nhave extensively explored SemCom from various perspectives, including network\narchitecture, theoretical analysis, potential technologies, and future\napplications. However, as SemCom continues to evolve, a multitude of security\nand privacy concerns have arisen, posing threats to the confidentiality,\nintegrity, and availability of SemCom systems. This paper presents a\ncomprehensive survey of the technologies that can be utilized to secure SemCom.\nFirstly, we elaborate on the entire life cycle of SemCom, which includes the\nmodel training, model transfer, and semantic information transmission phases.\nThen, we identify the security and privacy issues that emerge during these\nthree stages. Furthermore, we summarize the techniques available to mitigate\nthese security and privacy threats, including data cleaning, robust learning,\ndefensive strategies against backdoor attacks, adversarial training,\ndifferential privacy, cryptography, blockchain technology, model compression,\nand physical-layer security. Lastly, this paper outlines future research\ndirections to guide researchers in related fields."},{"date":"2025-01","title":"Spatially-guided Temporal Aggregation for Robust Event-RGB Optical Flow Estimation","author":"Qianang Zhou, Junhui Hou, Meiyi Yang, Yongjian Deng, Youfu Li, and Junlin Xiong","link":"http://arxiv.org/abs/2501.00838v1","abstract":"Current optical flow methods exploit the stable appearance of frame (or RGB)\ndata to establish robust correspondences across time. Event cameras, on the\nother hand, provide high-temporal-resolution motion cues and excel in\nchallenging scenarios. These complementary characteristics underscore the\npotential of integrating frame and event data for optical flow estimation.\nHowever, most cross-modal approaches fail to fully utilize the complementary\nadvantages, relying instead on simply stacking information. This study\nintroduces a novel approach that uses a spatially dense modality to guide the\naggregation of the temporally dense event modality, achieving effective\ncross-modal fusion. Specifically, we propose an event-enhanced frame\nrepresentation that preserves the rich texture of frames and the basic\nstructure of events. We use the enhanced representation as the guiding modality\nand employ events to capture temporally dense motion information. The robust\nmotion features derived from the guiding modality direct the aggregation of\nmotion information from events. To further enhance fusion, we propose a\ntransformer-based module that complements sparse event motion features with\nspatially rich frame information and enhances global information propagation.\nAdditionally, a mix-fusion encoder is designed to extract comprehensive\nspatiotemporal contextual features from both modalities. Extensive experiments\non the MVSEC and DSEC-Flow datasets demonstrate the effectiveness of our\nframework. Leveraging the complementary strengths of frames and events, our\nmethod achieves leading performance on the DSEC-Flow dataset. Compared to the\nevent-only model, frame guidance improves accuracy by 10\\%. Furthermore, it\noutperforms the state-of-the-art fusion-based method with a 4\\% accuracy gain\nand a 45\\% reduction in inference time."},{"date":"2025-01","title":"Information Sifting Funnel: Privacy-preserving Collaborative Inference Against Model Inversion Attacks","author":"Rongke Liu","link":"http://arxiv.org/abs/2501.00824v2","abstract":"The complexity of neural networks and inference tasks, coupled with demands\nfor computational efficiency and real-time feedback, poses significant\nchallenges for resource-constrained edge devices. Collaborative inference\nmitigates this by assigning shallow feature extraction to edge devices and\noffloading features to the cloud for further inference, reducing computational\nload. However, transmitted features remain susceptible to model inversion\nattacks (MIAs), which can reconstruct original input data. Current defenses,\nsuch as perturbation and information bottleneck techniques, offer explainable\nprotection but face limitations, including the lack of standardized criteria\nfor assessing MIA difficulty, challenges in mutual information estimation, and\ntrade-offs among usability, privacy, and deployability.\n  To address these challenges, we introduce the first criterion to evaluate MIA\ndifficulty in collaborative inference, supported by theoretical analysis of\nexisting attacks and defenses, validated using experiments with the Mutual\nInformation Neural Estimator (MINE). Based on these findings, we propose\nSiftFunnel, a privacy-preserving framework for collaborative inference. The\nedge model is trained with linear and non-linear correlation constraints to\nreduce redundant information in transmitted features, enhancing privacy\nprotection. Label smoothing and a cloud-based upsampling module are added to\nbalance usability and privacy. To improve deployability, the edge model\nincorporates a funnel-shaped structure and attention mechanisms, preserving\nboth privacy and usability. Extensive experiments demonstrate that SiftFunnel\noutperforms state-of-the-art defenses against MIAs, achieving superior privacy\nprotection with less than 3% accuracy loss and striking an optimal balance\namong usability, privacy, and practicality."},{"date":"2025-01","title":"LENS-XAI: Redefining Lightweight and Explainable Network Security through Knowledge Distillation and Variational Autoencoders for Scalable Intrusion Detection in Cybersecurity","author":"Muhammet Anil Yagiz, and Polat Goktas","link":"http://arxiv.org/abs/2501.00790v2","abstract":"The rapid proliferation of Industrial Internet of Things (IIoT) systems\nnecessitates advanced, interpretable, and scalable intrusion detection systems\n(IDS) to combat emerging cyber threats. Traditional IDS face challenges such as\nhigh computational demands, limited explainability, and inflexibility against\nevolving attack patterns. To address these limitations, this study introduces\nthe Lightweight Explainable Network Security framework (LENS-XAI), which\ncombines robust intrusion detection with enhanced interpretability and\nscalability. LENS-XAI integrates knowledge distillation, variational\nautoencoder models, and attribution-based explainability techniques to achieve\nhigh detection accuracy and transparency in decision-making. By leveraging a\ntraining set comprising 10% of the available data, the framework optimizes\ncomputational efficiency without sacrificing performance. Experimental\nevaluation on four benchmark datasets: Edge-IIoTset, UKM-IDS20, CTU-13, and\nNSL-KDD, demonstrates the framework's superior performance, achieving detection\naccuracies of 95.34%, 99.92%, 98.42%, and 99.34%, respectively. Additionally,\nthe framework excels in reducing false positives and adapting to complex attack\nscenarios, outperforming existing state-of-the-art methods. Key strengths of\nLENS-XAI include its lightweight design, suitable for resource-constrained\nenvironments, and its scalability across diverse IIoT and cybersecurity\ncontexts. Moreover, the explainability module enhances trust and transparency,\ncritical for practical deployment in dynamic and sensitive applications. This\nresearch contributes significantly to advancing IDS by addressing computational\nefficiency, feature interpretability, and real-world applicability. Future work\ncould focus on extending the framework to ensemble AI systems for distributed\nenvironments, further enhancing its robustness and adaptability."},{"date":"2025-01","title":"Shifting-Merging: Secure, High-Capacity and Efficient Steganography via Large Language Models","author":"Minhao Bai, Jinshuai Yang, Kaiyi Pang, Yongfeng Huang, and Yue Gao","link":"http://arxiv.org/abs/2501.00786v1","abstract":"In the face of escalating surveillance and censorship within the cyberspace,\nthe sanctity of personal privacy has come under siege, necessitating the\ndevelopment of steganography, which offers a way to securely hide messages\nwithin innocent-looking texts. Previous methods alternate the texts to hide\nprivate massages, which is not secure. Large Language Models (LLMs) provide\nhigh-quality and explicit distribution, which is an available mathematical tool\nfor secure steganography methods. However, existing attempts fail to achieve\nhigh capacity, time efficiency and correctness simultaneously, and their\nstrongly coupling designs leave little room for refining them to achieve better\nperformance. To provide a secure, high-capacity and efficient steganography\nmethod, we introduce ShiMer. Specifically, ShiMer pseudorandomly shifts the\nprobability interval of the LLM's distribution to obtain a private\ndistribution, and samples a token according to the private bits. ShiMer\nproduced steganographic texts are indistinguishable in quality from the normal\ntexts directly generated by the language model. To further enhance the capacity\nof ShiMer, we design a reordering algorithm to minimize the occurrence of\ninterval splitting during decoding phase. Experimental results indicate that\nour method achieves the highest capacity and efficiency among existing secure\nsteganography techniques."},{"date":"2025-01","title":"Ensuring superior learning outcomes and data security for authorized learner","author":"Jeongho Bang, Wooyeong Song, Kyujin Shin, and Yong-Su Kim","link":"http://arxiv.org/abs/2501.00754v1","abstract":"The learner's ability to generate a hypothesis that closely approximates the\ntarget function is crucial in machine learning. Achieving this requires\nsufficient data; however, unauthorized access by an eavesdropping learner can\nlead to security risks. Thus, it is important to ensure the performance of the\n\"authorized\" learner by limiting the quality of the training data accessible to\neavesdroppers. Unlike previous studies focusing on encryption or access\ncontrols, we provide a theorem to ensure superior learning outcomes exclusively\nfor the authorized learner with quantum label encoding. In this context, we use\nthe probably-approximately-correct (PAC) learning framework and introduce the\nconcept of learning probability to quantitatively assess learner performance.\nOur theorem allows the condition that, given a training dataset, an authorized\nlearner is guaranteed to achieve a certain quality of learning outcome, while\neavesdroppers are not. Notably, this condition can be constructed based only on\nthe authorized-learning-only measurable quantities of the training data, i.e.,\nits size and noise degree. We validate our theoretical proofs and predictions\nthrough convolutional neural networks (CNNs) image classification learning."},{"date":"2025-01","title":"Dynamics of Adversarial Attacks on Large Language Model-Based Search Engines","author":"Xiyang Hu","link":"http://arxiv.org/abs/2501.00745v1","abstract":"The increasing integration of Large Language Model (LLM) based search engines\nhas transformed the landscape of information retrieval. However, these systems\nare vulnerable to adversarial attacks, especially ranking manipulation attacks,\nwhere attackers craft webpage content to manipulate the LLM's ranking and\npromote specific content, gaining an unfair advantage over competitors. In this\npaper, we study the dynamics of ranking manipulation attacks. We frame this\nproblem as an Infinitely Repeated Prisoners' Dilemma, where multiple players\nstrategically decide whether to cooperate or attack. We analyze the conditions\nunder which cooperation can be sustained, identifying key factors such as\nattack costs, discount rates, attack success rates, and trigger strategies that\ninfluence player behavior. We identify tipping points in the system dynamics,\ndemonstrating that cooperation is more likely to be sustained when players are\nforward-looking. However, from a defense perspective, we find that simply\nreducing attack success probabilities can, paradoxically, incentivize attacks\nunder certain conditions. Furthermore, defensive measures to cap the upper\nbound of attack success rates may prove futile in some scenarios. These\ninsights highlight the complexity of securing LLM-based systems. Our work\nprovides a theoretical foundation and practical insights for understanding and\nmitigating their vulnerabilities, while emphasizing the importance of adaptive\nsecurity strategies and thoughtful ecosystem design."},{"date":"2025-01","title":"RORem: Training a Robust Object Remover with Human-in-the-Loop","author":"Ruibin Li, Tao Yang, Song Guo, and Lei Zhang","link":"http://arxiv.org/abs/2501.00740v1","abstract":"Despite the significant advancements, existing object removal methods\nstruggle with incomplete removal, incorrect content synthesis and blurry\nsynthesized regions, resulting in low success rates. Such issues are mainly\ncaused by the lack of high-quality paired training data, as well as the\nself-supervised training paradigm adopted in these methods, which forces the\nmodel to in-paint the masked regions, leading to ambiguity between synthesizing\nthe masked objects and restoring the background. To address these issues, we\npropose a semi-supervised learning strategy with human-in-the-loop to create\nhigh-quality paired training data, aiming to train a Robust Object Remover\n(RORem). We first collect 60K training pairs from open-source datasets to train\nan initial object removal model for generating removal samples, and then\nutilize human feedback to select a set of high-quality object removal pairs,\nwith which we train a discriminator to automate the following training data\ngeneration process. By iterating this process for several rounds, we finally\nobtain a substantial object removal dataset with over 200K pairs. Fine-tuning\nthe pre-trained stable diffusion model with this dataset, we obtain our RORem,\nwhich demonstrates state-of-the-art object removal performance in terms of both\nreliability and image quality. Particularly, RORem improves the object removal\nsuccess rate over previous methods by more than 18\\%. The dataset, source code\nand trained model are available at https://github.com/leeruibin/RORem."},{"date":"2025-01","title":"Everywhere Attack: Attacking Locally and Globally to Boost Targeted Transferability","author":"Hui Zeng, Sanshuai Cui, Biwei Chen, and Anjie Peng","link":"http://arxiv.org/abs/2501.00707v1","abstract":"Adversarial examples' (AE) transferability refers to the phenomenon that AEs\ncrafted with one surrogate model can also fool other models. Notwithstanding\nremarkable progress in untargeted transferability, its targeted counterpart\nremains challenging. This paper proposes an everywhere scheme to boost targeted\ntransferability. Our idea is to attack a victim image both globally and\nlocally. We aim to optimize 'an army of targets' in every local image region\ninstead of the previous works that optimize a high-confidence target in the\nimage. Specifically, we split a victim image into non-overlap blocks and\njointly mount a targeted attack on each block. Such a strategy mitigates\ntransfer failures caused by attention inconsistency between surrogate and\nvictim models and thus results in stronger transferability. Our approach is\nmethod-agnostic, which means it can be easily combined with existing\ntransferable attacks for even higher transferability. Extensive experiments on\nImageNet demonstrate that the proposed approach universally improves the\nstate-of-the-art targeted attacks by a clear margin, e.g., the transferability\nof the widely adopted Logit attack can be improved by 28.8%-300%.We also\nevaluate the crafted AEs on a real-world platform: Google Cloud Vision. Results\nfurther support the superiority of the proposed method."},{"date":"2024-12","title":"Deeply Learned Robust Matrix Completion for Large-scale Low-rank Data Recovery","author":"HanQin Cai, Chandra Kundu, Jialin Liu, and Wotao Yin","link":"http://arxiv.org/abs/2501.00677v1","abstract":"Robust matrix completion (RMC) is a widely used machine learning tool that\nsimultaneously tackles two critical issues in low-rank data analysis: missing\ndata entries and extreme outliers. This paper proposes a novel scalable and\nlearnable non-convex approach, coined Learned Robust Matrix Completion (LRMC),\nfor large-scale RMC problems. LRMC enjoys low computational complexity with\nlinear convergence. Motivated by the proposed theorem, the free parameters of\nLRMC can be effectively learned via deep unfolding to achieve optimum\nperformance. Furthermore, this paper proposes a flexible\nfeedforward-recurrent-mixed neural network framework that extends deep\nunfolding from fix-number iterations to infinite iterations. The superior\nempirical performance of LRMC is verified with extensive experiments against\nstate-of-the-art on synthetic datasets and real applications, including video\nbackground subtraction, ultrasound imaging, face modeling, and cloud removal\nfrom satellite imagery."},{"date":"2024-12","title":"Extending XReason: Formal Explanations for Adversarial Detection","author":"Amira Jemaa, Adnan Rashid, and Sofiene Tahar","link":"http://arxiv.org/abs/2501.00537v1","abstract":"Explainable Artificial Intelligence (XAI) plays an important role in\nimproving the transparency and reliability of complex machine learning models,\nespecially in critical domains such as cybersecurity. Despite the prevalence of\nheuristic interpretation methods such as SHAP and LIME, these techniques often\nlack formal guarantees and may produce inconsistent local explanations. To\nfulfill this need, few tools have emerged that use formal methods to provide\nformal explanations. Among these, XReason uses a SAT solver to generate formal\ninstance-level explanation for XGBoost models. In this paper, we extend the\nXReason tool to support LightGBM models as well as class-level explanations.\nAdditionally, we implement a mechanism to generate and detect adversarial\nexamples in XReason. We evaluate the efficiency and accuracy of our approach on\nthe CICIDS-2017 dataset, a widely used benchmark for detecting network attacks."},{"date":"2024-12","title":"A Method for Enhancing the Safety of Large Model Generation Based on Multi-dimensional Attack and Defense","author":"Keke Zhai","link":"http://arxiv.org/abs/2501.00517v1","abstract":"Currently, large models are prone to generating harmful content when faced\nwith complex attack instructions, significantly reducing their defensive\ncapabilities. To address this issue, this paper proposes a method based on\nconstructing data aligned with multi-dimensional attack defense to enhance the\ngenerative security of large models. The core of our method lies in improving\nthe effectiveness of safe alignment learning for large models by innova-tively\nincreasing the diversity of attack instruction dimensions and the accuracy of\ngenerat-ing safe responses. To validate the effectiveness of our method, beyond\nexisting security evaluation benchmarks, we additionally designed new security\nevaluation benchmarks and conducted comparative experiments using Llama3.2 as\nthe baseline model. The final ex-perimental results demonstrate that our method\ncan significantly improve the generative security of large models under complex\ninstructional attacks, while also maintaining and enhancing the models' general\ncapabilities."},{"date":"2024-12","title":"Unrolled Creative Adversarial Network For Generating Novel Musical Pieces","author":"Pratik Nag","link":"http://arxiv.org/abs/2501.00452v1","abstract":"Music generation has been established as a prominent topic in artificial\nintelligence and machine learning over recent years. In most recent works on\nRNN-based neural network methods have been applied for sequence generation. In\ncontrast, generative adversarial networks (GANs) and their counterparts have\nbeen explored by very few researchersfor music generation.\n  In this paper, a classical system was employed alongside a new system to\ngenerate creative music. Both systems were designed based on adversarial\nnetworks to generate music by learning from examples. The classical system was\ntrained to learn a set of music pieces without differentiating between classes,\nwhereas the new system was trained to learn the different composers and their\nstyles to generate a creative music piece by deviating from the learned\ncomposers' styles.\n  The base structure utilized was generative adversarial networks (GANs), which\nare capable of generating novel outputs given a set of inputs to learn from and\nmimic their distribution. It has been shown in previous work that GANs are\nlimited in their original design with respect to creative outputs. Building on\nthe Creative Adversarial Networks (CAN) , this work applied them in the music\ndomain rather than the visual art domain. Additionally, unrolled CAN was\nintroduced to prevent mode collapse. Experiments were conducted on both GAN and\nCAN for generating music, and their capabilities were measured in terms of\ndeviation from the input set."},{"date":"2024-12","title":"Outlier-Robust Linear System Identification Under Heavy-tailed Noise","author":"Vinay Kanakeri, and Aritra Mitra","link":"http://arxiv.org/abs/2501.00421v1","abstract":"We consider the problem of estimating the state transition matrix of a linear\ntime-invariant (LTI) system, given access to multiple independent trajectories\nsampled from the system. Several recent papers have conducted a non-asymptotic\nanalysis of this problem, relying crucially on the assumption that the process\nnoise is either Gaussian or sub-Gaussian, i.e., \"light-tailed\". In sharp\ncontrast, we work under a significantly weaker noise model, assuming nothing\nmore than the existence of the fourth moment of the noise distribution. For\nthis setting, we provide the first set of results demonstrating that one can\nobtain sample-complexity bounds for linear system identification that are\nnearly of the same order as under sub-Gaussian noise. To achieve such results,\nwe develop a novel robust system identification algorithm that relies on\nconstructing multiple weakly-concentrated estimators, and then boosting their\nperformance using suitable tools from high-dimensional robust statistics.\nInterestingly, our analysis reveals how the kurtosis of the noise distribution,\na measure of heavy-tailedness, affects the number of trajectories needed to\nachieve desired estimation error bounds. Finally, we show that our algorithm\nand analysis technique can be easily extended to account for scenarios where an\nadversary can arbitrarily corrupt a small fraction of the collected trajectory\ndata. Our work takes the first steps towards building a robust statistical\nlearning theory for control under non-ideal assumptions on the data-generating\nprocess."},{"date":"2024-12","title":"Enhancing Deployment-Time Predictive Model Robustness for Code Analysis and Optimization","author":"Huanting Wang, Patrick Lenihan, and Zheng Wang","link":"http://arxiv.org/abs/2501.00298v1","abstract":"Supervised machine learning techniques have shown promising results in code\nanalysis and optimization problems. However, a learning-based solution can be\nbrittle because minor changes in hardware or application workloads -- such as\nfacing a new CPU architecture or code pattern -- may jeopardize decision\naccuracy, ultimately undermining model robustness. We introduce Prom, an\nopen-source library to enhance the robustness and performance of predictive\nmodels against such changes during deployment. Prom achieves this by using\nstatistical assessments to identify test samples prone to mispredictions and\nusing feedback on these samples to improve a deployed model. We showcase Prom\nby applying it to 13 representative machine learning models across 5 code\nanalysis and optimization tasks. Our extensive evaluation demonstrates that\nProm can successfully identify an average of 96% (up to 100%) of\nmispredictions. By relabeling up to 5% of the Prom-identified samples through\nincremental learning, Prom can help a deployed model achieve a performance\ncomparable to that attained during its model training phase."},{"date":"2024-12","title":"Outlier-Robust Training of Machine Learning Models","author":"Rajat Talak, Charis Georgiou, Jingnan Shi, and Luca Carlone","link":"http://arxiv.org/abs/2501.00265v1","abstract":"Robust training of machine learning models in the presence of outliers has\ngarnered attention across various domains. The use of robust losses is a\npopular approach and is known to mitigate the impact of outliers. We bring to\nlight two literatures that have diverged in their ways of designing robust\nlosses: one using M-estimation, which is popular in robotics and computer\nvision, and another using a risk-minimization framework, which is popular in\ndeep learning. We first show that a simple modification of the Black-Rangarajan\nduality provides a unifying view. The modified duality brings out a definition\nof a robust loss kernel $\\sigma$ that is satisfied by robust losses in both the\nliteratures. Secondly, using the modified duality, we propose an Adaptive\nAlternation Algorithm (AAA) for training machine learning models with outliers.\nThe algorithm iteratively trains the model by using a weighted version of the\nnon-robust loss, while updating the weights at each iteration. The algorithm is\naugmented with a novel parameter update rule by interpreting the weights as\ninlier probabilities, and obviates the need for complex parameter tuning.\nThirdly, we investigate convergence of the adaptive alternation algorithm to\noutlier-free optima. Considering arbitrary outliers (i.e., with no\ndistributional assumption on the outliers), we show that the use of robust loss\nkernels {\\sigma} increases the region of convergence. We experimentally show\nthe efficacy of our algorithm on regression, classification, and neural scene\nreconstruction problems. We release our implementation code:\nhttps://github.com/MIT-SPARK/ORT."},{"date":"2024-12","title":"Enhancing Wireless Sensor Network Security through Integration with the ServiceNow Cloud Platform","author":"Syed Atif Ali, and Salwa Din","link":"http://arxiv.org/abs/2501.00264v1","abstract":"Wireless Sensor Networks (WSNs) continue to experience rapid developments and\nintegration into modern-day applications. Overall, WSNs collect and process\nrelevant data through sensors or nodes and communicate with different networks\nfor superior information management. Nevertheless, a primary concern relative\nto WSNs is security. Considering the high constraints on throughput, battery,\nprocessing power, and memory, typical security procedures present limitations\nfor application in WSNs. This research focuses on the integration of WSNs with\nthe cloud platform, specifically to address these security risks. The cloud\nplatform also adopts a security-driven approach and has attracted many\napplications across various sectors globally. This research specifically\nexplores how cloud computing could be exploited to impede Denial of Service\nattacks from endangering WSNs. WSNs are now deployed in various low-powered\napplications, including disaster management, homeland security, battlefield\nsurveillance, agriculture, and the healthcare industry. WSNs are distinguished\nfrom traditional networks by the numerous wireless connected sensors being\ndeployed to conduct an assigned task. In testing scenarios, the size of WSNs\nranges from a few to several thousand. The overarching requirements of WSNs\ninclude rapid processing of collected data, low-cost installation and\nmaintenance, and low latency in network operations. Given that a substantial\namount of WSN applications are used in high-risk and volatile environments,\nthey must effectively address security concerns. This includes the secure\nmovement, storage, and communication of data through networks, an environment\nin which WSNs are notably vulnerable. The limitations of WSNs have meant that\nthey are predominantly used in unsecured applications despite positive\nadvancements. This study explores methods for integrating the WSN with the\ncloud."},{"date":"2024-12","title":"Detection and Prevention of Smishing Attacks","author":"Diksha Goel","link":"http://arxiv.org/abs/2501.00260v1","abstract":"Phishing is an online identity theft technique where attackers steal users\npersonal information, leading to financial losses for individuals and\norganizations. With the increasing adoption of smartphones, which provide\nfunctionalities similar to desktop computers, attackers are targeting mobile\nusers. Smishing, a phishing attack carried out through Short Messaging Service\n(SMS), has become prevalent due to the widespread use of SMS-based services. It\ninvolves deceptive messages designed to extract sensitive information. Despite\nthe growing number of smishing attacks, limited research focuses on detecting\nthese threats. This work presents a smishing detection model using a\ncontent-based analysis approach. To address the challenge posed by slang,\nabbreviations, and short forms in text communication, the model normalizes\nthese into standard forms. A machine learning classifier is employed to\nclassify messages as smishing or ham. Experimental results demonstrate the\nmodel effectiveness, achieving classification accuracies of 97.14% for smishing\nand 96.12% for ham messages, with an overall accuracy of 96.20%."},{"date":"2024-12","title":"SoS Certificates for Sparse Singular Values and Their Applications: Robust Statistics, Subspace Distortion, and More","author":"Ilias Diakonikolas, Samuel B. Hopkins, Ankit Pensia, and Stefan Tiegel","link":"http://arxiv.org/abs/2412.21203v1","abstract":"We study $\\textit{sparse singular value certificates}$ for random rectangular\nmatrices. If $M$ is an $n \\times d$ matrix with independent Gaussian entries,\nwe give a new family of polynomial-time algorithms which can certify upper\nbounds on the maximum of $\\|M u\\|$, where $u$ is a unit vector with at most\n$\\eta n$ nonzero entries for a given $\\eta \\in (0,1)$. This basic algorithmic\nprimitive lies at the heart of a wide range of problems across algorithmic\nstatistics and theoretical computer science.\n  Our algorithms certify a bound which is asymptotically smaller than the naive\none, given by the maximum singular value of $M$, for nearly the widest-possible\nrange of $n,d,$ and $\\eta$. Efficiently certifying such a bound for a range of\n$n,d$ and $\\eta$ which is larger by any polynomial factor than what is achieved\nby our algorithm would violate lower bounds in the SQ and low-degree\npolynomials models. Our certification algorithm makes essential use of the\nSum-of-Squares hierarchy. To prove the correctness of our algorithm, we develop\na new combinatorial connection between the graph matrix approach to analyze\nrandom matrices with dependent entries, and the Efron-Stein decomposition of\nfunctions of independent random variables.\n  As applications of our certification algorithm, we obtain new efficient\nalgorithms for a wide range of well-studied algorithmic tasks. In algorithmic\nrobust statistics, we obtain new algorithms for robust mean and covariance\nestimation with tradeoffs between breakdown point and sample complexity, which\nare nearly matched by SQ and low-degree polynomial lower bounds (that we\nestablish). We also obtain new polynomial-time guarantees for certification of\n$\\ell_1/\\ell_2$ distortion of random subspaces of $\\mathbb{R}^n$ (also with\nnearly matching lower bounds), sparse principal component analysis, and\ncertification of the $2\\rightarrow p$ norm of a random matrix."},{"date":"2024-12","title":"Adversarial Attack and Defense for LoRa Device Identification and Authentication via Deep Learning","author":"Yalin E. Sagduyu, and Tugba Erpek","link":"http://arxiv.org/abs/2412.21164v1","abstract":"LoRa provides long-range, energy-efficient communications in Internet of\nThings (IoT) applications that rely on Low-Power Wide-Area Network (LPWAN)\ncapabilities. Despite these merits, concerns persist regarding the security of\nLoRa networks, especially in situations where device identification and\nauthentication are imperative to secure the reliable access to the LoRa\nnetworks. This paper explores a deep learning (DL) approach to tackle these\nconcerns, focusing on two critical tasks, namely (i) identifying LoRa devices\nand (ii) classifying them to legitimate and rogue devices. Deep neural networks\n(DNNs), encompassing both convolutional and feedforward neural networks, are\ntrained for these tasks using actual LoRa signal data. In this setting, the\nadversaries may spoof rogue LoRa signals through the kernel density estimation\n(KDE) method based on legitimate device signals that are received by the\nadversaries. Two cases are considered, (i) training two separate classifiers,\none for each of the two tasks, and (ii) training a multi-task classifier for\nboth tasks. The vulnerabilities of the resulting DNNs to manipulations in input\nsamples are studied in form of untargeted and targeted adversarial attacks\nusing the Fast Gradient Sign Method (FGSM). Individual and common perturbations\nare considered against single-task and multi-task classifiers for the LoRa\nsignal analysis. To provide resilience against such attacks, a defense approach\nis presented by increasing the robustness of classifiers with adversarial\ntraining. Results quantify how vulnerable LoRa signal classification tasks are\nto adversarial attacks and emphasize the need to fortify IoT applications\nagainst these subtle yet effective threats."},{"date":"2024-12","title":"Machine Learning-Based Security Policy Analysis","author":"Krish Jain, Joann Sum, Pranav Kapoor, and Amir Eaman","link":"http://arxiv.org/abs/2501.00085v2","abstract":"Security-Enhanced Linux (SELinux) is a robust security mechanism that\nenforces mandatory access controls (MAC), but its policy language's complexity\ncreates challenges for policy analysis and management. This research\ninvestigates the automation of SELinux policy analysis using graph-based\ntechniques combined with machine learning approaches to detect policy\nanomalies. The study addresses two key questions: Can SELinux policy analysis\nbe automated through graph analysis, and how do different anomaly detection\nmodels compare in analyzing SELinux policies? We will be comparing different\nmachine learning models by evaluating their effectiveness in detecting policy\nviolations and anomalies. Our approach utilizes Neo4j for graph representation\nof policies, with Node2vec transforming these graph structures into meaningful\nvector embeddings that can be processed by our machine learning models. In our\nresults, the MLP Neural Network consistently demonstrated superior performance\nacross different dataset sizes, achieving 95% accuracy with balanced precision\nand recall metrics, while both Random Forest and SVM models showed competitive\nbut slightly lower performance in detecting policy violations. This combination\nof graph-based modeling and machine learning provides a more sophisticated and\nautomated approach to understanding and analyzing complex SELinux policies\ncompared to traditional manual analysis methods."},{"date":"2024-12","title":"Toward Intelligent and Secure Cloud: Large Language Model Empowered Proactive Defense","author":"Yuyang Zhou, Guang Cheng, Kang Du, and Zihan Chen","link":"http://arxiv.org/abs/2412.21051v1","abstract":"The rapid evolution of cloud computing technologies and the increasing number\nof cloud applications have provided a large number of benefits in daily lives.\nHowever, the diversity and complexity of different components pose a\nsignificant challenge to cloud security, especially when dealing with\nsophisticated and advanced cyberattacks. Recent advancements in generative\nfoundation models (GFMs), particularly in the large language models (LLMs),\noffer promising solutions for security intelligence. By exploiting the powerful\nabilities in language understanding, data analysis, task inference, action\nplanning, and code generation, we present LLM-PD, a novel proactive defense\narchitecture that defeats various threats in a proactive manner. LLM-PD can\nefficiently make a decision through comprehensive data analysis and sequential\nreasoning, as well as dynamically creating and deploying actionable defense\nmechanisms on the target cloud. Furthermore, it can flexibly self-evolve based\non experience learned from previous interactions and adapt to new attack\nscenarios without additional training. The experimental results demonstrate its\nremarkable ability in terms of defense effectiveness and efficiency,\nparticularly highlighting an outstanding success rate when compared with other\nexisting methods."},{"date":"2024-12","title":"RobustBlack: Challenging Black-Box Adversarial Attacks on State-of-the-Art Defenses","author":"Mohamed Djilani, Salah Ghamizi, and Maxime Cordy","link":"http://arxiv.org/abs/2412.20987v1","abstract":"Although adversarial robustness has been extensively studied in white-box\nsettings, recent advances in black-box attacks (including transfer- and\nquery-based approaches) are primarily benchmarked against weak defenses,\nleaving a significant gap in the evaluation of their effectiveness against more\nrecent and moderate robust models (e.g., those featured in the Robustbench\nleaderboard). In this paper, we question this lack of attention from black-box\nattacks to robust models. We establish a framework to evaluate the\neffectiveness of recent black-box attacks against both top-performing and\nstandard defense mechanisms, on the ImageNet dataset. Our empirical evaluation\nreveals the following key findings: (1) the most advanced black-box attacks\nstruggle to succeed even against simple adversarially trained models; (2)\nrobust models that are optimized to withstand strong white-box attacks, such as\nAutoAttack, also exhibits enhanced resilience against black-box attacks; and\n(3) robustness alignment between the surrogate models and the target model\nplays a key factor in the success rate of transfer-based attacks"}]