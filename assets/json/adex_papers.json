[
    {
        "date": "2025-02",
        "title": "Enabling AutoML for Zero-Touch Network Security: Use-Case Driven Analysis",
        "author": "Li Yang, Mirna El Rajab, Abdallah Shami, and Sami Muhaidat",
        "link": "http://arxiv.org/abs/2502.21286v1",
        "abstract": "Zero-Touch Networks (ZTNs) represent a state-of-the-art paradigm shift\ntowards fully automated and intelligent network management, enabling the\nautomation and intelligence required to manage the complexity, scale, and\ndynamic nature of next-generation (6G) networks. ZTNs leverage Artificial\nIntelligence (AI) and Machine Learning (ML) to enhance operational efficiency,\nsupport intelligent decision-making, and ensure effective resource allocation.\nHowever, the implementation of ZTNs is subject to security challenges that need\nto be resolved to achieve their full potential. In particular, two critical\nchallenges arise: the need for human expertise in developing AI/ML-based\nsecurity mechanisms, and the threat of adversarial attacks targeting AI/ML\nmodels. In this survey paper, we provide a comprehensive review of current\nsecurity issues in ZTNs, emphasizing the need for advanced AI/ML-based security\nmechanisms that require minimal human intervention and protect AI/ML models\nthemselves. Furthermore, we explore the potential of Automated ML (AutoML)\ntechnologies in developing robust security solutions for ZTNs. Through case\nstudies, we illustrate practical approaches to securing ZTNs against both\nconventional and AI/ML-specific threats, including the development of\nautonomous intrusion detection systems and strategies to combat Adversarial ML\n(AML) attacks. The paper concludes with a discussion of the future research\ndirections for the development of ZTN security approaches."
    },
    {
        "date": "2025-02",
        "title": "QFAL: Quantum Federated Adversarial Learning",
        "author": "Walid El Maouaki, Nouhaila Innan, Alberto Marchisio, Taoufik Said, Mohamed Bennai, and Muhammad Shafique",
        "link": "http://arxiv.org/abs/2502.21171v1",
        "abstract": "Quantum federated learning (QFL) merges the privacy advantages of federated\nsystems with the computational potential of quantum neural networks (QNNs), yet\nits vulnerability to adversarial attacks remains poorly understood. This work\npioneers the integration of adversarial training into QFL, proposing a robust\nframework, quantum federated adversarial learning (QFAL), where clients\ncollaboratively defend against perturbations by combining local adversarial\nexample generation with federated averaging (FedAvg). We systematically\nevaluate the interplay between three critical factors: client count (5, 10,\n15), adversarial training coverage (0-100%), and adversarial attack\nperturbation strength (epsilon = 0.01-0.5), using the MNIST dataset. Our\nexperimental results show that while fewer clients often yield higher\nclean-data accuracy, larger federations can more effectively balance accuracy\nand robustness when partially adversarially trained. Notably, even limited\nadversarial coverage (e.g., 20%-50%) can significantly improve resilience to\nmoderate perturbations, though at the cost of reduced baseline performance.\nConversely, full adversarial training (100%) may regain high clean accuracy but\nis vulnerable under stronger attacks. These findings underscore an inherent\ntrade-off between robust and standard objectives, which is further complicated\nby quantum-specific factors. We conclude that a carefully chosen combination of\nclient count and adversarial coverage is critical for mitigating adversarial\nvulnerabilities in QFL. Moreover, we highlight opportunities for future\nresearch, including adaptive adversarial training schedules, more diverse\nquantum encoding schemes, and personalized defense strategies to further\nenhance the robustness-accuracy trade-off in real-world quantum federated\nenvironments."
    },
    {
        "date": "2025-02",
        "title": "Robust Deterministic Policy Gradient for Disturbance Attenuation and Its Application to Quadrotor Control",
        "author": "Taeho Lee, and Donghwan Lee",
        "link": "http://arxiv.org/abs/2502.21057v1",
        "abstract": "Practical control systems pose significant challenges in identifying optimal\ncontrol policies due to uncertainties in the system model and external\ndisturbances. While $H_\\infty$ control techniques are commonly used to design\nrobust controllers that mitigate the effects of disturbances, these methods\noften require complex and computationally intensive calculations. To address\nthis issue, this paper proposes a reinforcement learning algorithm called\nRobust Deterministic Policy Gradient (RDPG), which formulates the $H_\\infty$\ncontrol problem as a two-player zero-sum dynamic game. In this formulation, one\nplayer (the user) aims to minimize the cost, while the other player (the\nadversary) seeks to maximize it. We then employ deterministic policy gradient\n(DPG) and its deep reinforcement learning counterpart to train a robust control\npolicy with effective disturbance attenuation. In particular, for practical\nimplementation, we introduce an algorithm called robust deep deterministic\npolicy gradient (RDDPG), which employs a deep neural network architecture and\nintegrates techniques from the twin-delayed deep deterministic policy gradient\n(TD3) to enhance stability and learning efficiency. To evaluate the proposed\nalgorithm, we implement it on an unmanned aerial vehicle (UAV) tasked with\nfollowing a predefined path in a disturbance-prone environment. The\nexperimental results demonstrate that the proposed method outperforms other\ncontrol approaches in terms of robustness against disturbances, enabling\nprecise real-time tracking of moving targets even under severe disturbance\nconditions."
    },
    {
        "date": "2025-02",
        "title": "Data-free Universal Adversarial Perturbation with Pseudo-semantic Prior",
        "author": "Chanhui Lee, Yeonghwan Song, and Jeany Son",
        "link": "http://arxiv.org/abs/2502.21048v1",
        "abstract": "Data-free Universal Adversarial Perturbation (UAP) is an image-agnostic\nadversarial attack that deceives deep neural networks using a single\nperturbation generated solely from random noise, without any data priors.\nHowever, traditional data-free UAP methods often suffer from limited\ntransferability due to the absence of semantic information in random noise. To\naddress this, we propose a novel data-free universal attack approach that\ngenerates a pseudo-semantic prior recursively from the UAPs, enriching semantic\ncontents within the data-free UAP framework. Our method is based on the\nobservation that UAPs inherently contain latent semantic information, enabling\nthe generated UAP to act as an alternative data prior, by capturing a diverse\nrange of semantics through region sampling. We further introduce a sample\nreweighting technique to emphasize hard examples by focusing on samples that\nare less affected by the UAP. By leveraging the semantic information from the\npseudo-semantic prior, we also incorporate input transformations, typically\nineffective in data-free UAPs due to the lack of semantic content in random\npriors, to boost black-box transferability. Comprehensive experiments on\nImageNet show that our method achieves state-of-the-art performance in average\nfooling rate by a substantial margin, significantly improves attack\ntransferability across various CNN architectures compared to existing data-free\nUAP methods, and even surpasses data-dependent UAP methods."
    },
    {
        "date": "2025-02",
        "title": "Fast Adversarial Training against Sparse Attacks Requires Loss Smoothing",
        "author": "Xuyang Zhong, Yixiao Huang, and Chen Liu",
        "link": "http://arxiv.org/abs/2502.21041v1",
        "abstract": "This paper studies fast adversarial training against sparse adversarial\nperturbations bounded by $l_0$ norm. We demonstrate the challenges of employing\n$1$-step attacks on $l_0$ bounded perturbations for fast adversarial training,\nincluding degraded performance and the occurrence of catastrophic overfitting\n(CO). We highlight that CO in $l_0$ adversarial training is caused by\nsub-optimal perturbation locations of $1$-step attack. Theoretical and\nempirical analyses reveal that the loss landscape of $l_0$ adversarial training\nis more craggy compared to its $l_\\infty$, $l_2$ and $l_1$ counterparts.\nMoreover, we corroborate that the craggy loss landscape can aggravate CO. To\naddress these issues, we propose Fast-LS-$l_0$ that incorporates soft labels\nand the trade-off loss function to smooth the adversarial loss landscape.\nExtensive experiments demonstrate our method can overcome the challenge of\ncatastrophic overfitting, achieve state-of-the-art performance, and narrow down\nthe performance gap between $1$-step and multi-step adversarial training\nagainst sparse attacks."
    },
    {
        "date": "2025-02",
        "title": "Synthesizing Tabular Data Using Selectivity Enhanced Generative Adversarial Networks",
        "author": "Youran Zhou, and Jianzhong Qi",
        "link": "http://arxiv.org/abs/2502.21034v1",
        "abstract": "As E-commerce platforms face surging transactions during major shopping\nevents like Black Friday, stress testing with synthesized data is crucial for\nresource planning. Most recent studies use Generative Adversarial Networks\n(GANs) to generate tabular data while ensuring privacy and machine learning\nutility. However, these methods overlook the computational demands of\nprocessing GAN-generated data, making them unsuitable for E-commerce stress\ntesting.\n  This thesis introduces a novel GAN-based approach incorporating query\nselectivity constraints, a key factor in database transaction processing. We\nintegrate a pre-trained deep neural network to maintain selectivity consistency\nbetween real and synthetic data. Our method, tested on five real-world\ndatasets, outperforms three state-of-the-art GANs and a VAE model, improving\nselectivity estimation accuracy by up to 20pct and machine learning utility by\nup to 6 pct."
    },
    {
        "date": "2025-02",
        "title": "The RAG Paradox: A Black-Box Attack Exploiting Unintentional Vulnerabilities in Retrieval-Augmented Generation Systems",
        "author": "Chanwoo Choi, Jinsoo Kim, Sukmin Cho, Soyeong Jeong, and Buru Chang",
        "link": "http://arxiv.org/abs/2502.20995v1",
        "abstract": "With the growing adoption of retrieval-augmented generation (RAG) systems,\nrecent studies have introduced attack methods aimed at degrading their\nperformance. However, these methods rely on unrealistic white-box assumptions,\nsuch as attackers having access to RAG systems' internal processes. To address\nthis issue, we introduce a realistic black-box attack scenario based on the RAG\nparadox, where RAG systems inadvertently expose vulnerabilities while\nattempting to enhance trustworthiness. Because RAG systems reference external\ndocuments during response generation, our attack targets these sources without\nrequiring internal access. Our approach first identifies the external sources\ndisclosed by RAG systems and then automatically generates poisoned documents\nwith misinformation designed to match these sources. Finally, these poisoned\ndocuments are newly published on the disclosed sources, disrupting the RAG\nsystem's response generation process. Both offline and online experiments\nconfirm that this attack significantly reduces RAG performance without\nrequiring internal access. Furthermore, from an insider perspective within the\nRAG system, we propose a re-ranking method that acts as a fundamental\nsafeguard, offering minimal protection against unforeseen attacks."
    },
    {
        "date": "2025-02",
        "title": "Robust and Efficient Writer-Independent IMU-Based Handwriting Recognization",
        "author": "Jindong Li, Tim Hamann, Jens Barth, Peter Kaempf, Dario Zanca, and Bjoern Eskofier",
        "link": "http://arxiv.org/abs/2502.20954v1",
        "abstract": "Online handwriting recognition (HWR) using data from inertial measurement\nunits (IMUs) remains challenging due to variations in writing styles and the\nlimited availability of high-quality annotated datasets. Traditional models\noften struggle to recognize handwriting from unseen writers, making\nwriter-independent (WI) recognition a crucial but difficult problem. This paper\npresents an HWR model with an encoder-decoder structure for IMU data, featuring\na CNN-based encoder for feature extraction and a BiLSTM decoder for sequence\nmodeling, which supports inputs of varying lengths. Our approach demonstrates\nstrong robustness and data efficiency, outperforming existing methods on WI\ndatasets, including the WI split of the OnHW dataset and our own dataset.\nExtensive evaluations show that our model maintains high accuracy across\ndifferent age groups and writing conditions while effectively learning from\nlimited data. Through comprehensive ablation studies, we analyze key design\nchoices, achieving a balance between accuracy and efficiency. These findings\ncontribute to the development of more adaptable and scalable HWR systems for\nreal-world applications."
    },
    {
        "date": "2025-02",
        "title": "Concealed Adversarial attacks on neural networks for sequential data",
        "author": "Petr Sokerin, Dmitry Anikin, Sofia Krehova, and Alexey Zaytsev",
        "link": "http://arxiv.org/abs/2502.20948v1",
        "abstract": "The emergence of deep learning led to the broad usage of neural networks in\nthe time series domain for various applications, including finance and\nmedicine. While powerful, these models are prone to adversarial attacks: a\nbenign targeted perturbation of input data leads to significant changes in a\nclassifier's output. However, formally small attacks in the time series domain\nbecome easily detected by the human eye or a simple detector model.\n  We develop a concealed adversarial attack for different time-series models:\nit provides more realistic perturbations, being hard to detect by a human or\nmodel discriminator. To achieve this goal, the proposed adversarial attack\nmaximizes an aggregation of a classifier and a trained discriminator loss. To\nmake the attack stronger, we also propose a training procedure for a\ndiscriminator that provides broader coverage of possible attacks. Extensive\nbenchmarking on six UCR time series datasets across four diverse architectures\n- including recurrent, convolutional, state-space, and transformer-based models\n- demonstrates the superiority of our attack for a concealability-efficiency\ntrade-off. Our findings highlight the growing challenge of designing robust\ntime series models, emphasizing the need for improved defenses against\nrealistic and effective attacks."
    },
    {
        "date": "2025-02",
        "title": "BadRefSR: Backdoor Attacks Against Reference-based Image Super Resolution",
        "author": "Xue Yang, Tao Chen, Lei Guo, Wenbo Jiang, Ji Guo, Yongming Li, and Jiaming He",
        "link": "http://arxiv.org/abs/2502.20943v1",
        "abstract": "Reference-based image super-resolution (RefSR) represents a promising\nadvancement in super-resolution (SR). In contrast to single-image\nsuper-resolution (SISR), RefSR leverages an additional reference image to help\nrecover high-frequency details, yet its vulnerability to backdoor attacks has\nnot been explored. To fill this research gap, we propose a novel attack\nframework called BadRefSR, which embeds backdoors in the RefSR model by adding\ntriggers to the reference images and training with a mixed loss function.\nExtensive experiments across various backdoor attack settings demonstrate the\neffectiveness of BadRefSR. The compromised RefSR network performs normally on\nclean input images, while outputting attacker-specified target images on\ntriggered input images. Our study aims to alert researchers to the potential\nbackdoor risks in RefSR. Codes are available at\nhttps://github.com/xuefusiji/BadRefSR."
    },
    {
        "date": "2025-02",
        "title": "The Effect of Hop-count Modification Attack on Random Walk-based SLP Schemes Developed forWSNs: a Study",
        "author": "Manjula Rajaa, Anirban Ghoshb, Chukkapalli Praveen Kumarc, Suleiman Samba, and C N Shariff",
        "link": "http://arxiv.org/abs/2502.20902v1",
        "abstract": "Source location privacy (SLP) has been of great concern in WSNs when deployed\nfor habitat monitoring applications. The issue is taken care of by employing\nprivacy-preserving routing schemes. In the existing works, the attacker is\nassumed to be passive in nature and backtracks to the source of information by\neavesdropping the message signals. In this work, we try to understand the\nimpact of active attacks by proposing a new hybrid attack model consisting of\nboth active and passive attacks. The proposed model is then applied to three\nexisting TTL-based random walk SLP solutions: phantom routing scheme (PRS),\nsource location privacy using randomized routes (SLP-R), and\nposition-independent section-based scheme (PSSLP). The performance of the\nalgorithms in terms of privacy metrics is compared in the case of pure passive\nattack and hybrid attack of varying intensity. The results indicate a\nsignificant degradation in the privacy protection performance of the reference\nalgorithms in the face of the proposed hybrid attack model indicating the\nimportance and relevance of such attacks. It is further observed that the\nhybrid attack can be optimized to increase the vulnerability of the existing\nsolutions."
    },
    {
        "date": "2025-02",
        "title": "Cyber Defense Reinvented: Large Language Models as Threat Intelligence Copilots",
        "author": "Xiaoqun Liu, Jiacheng Liang, Qiben Yan, Muchao Ye, Jinyuan Jia, and Zhaohan Xi",
        "link": "http://arxiv.org/abs/2502.20791v1",
        "abstract": "The exponential growth of cyber threat knowledge, exemplified by the\nexpansion of databases such as MITRE-CVE and NVD, poses significant challenges\nfor cyber threat analysis. Security professionals are increasingly burdened by\nthe sheer volume and complexity of information, creating an urgent need for\neffective tools to navigate, synthesize, and act on large-scale data to counter\nevolving threats proactively. However, conventional threat intelligence tools\noften fail to scale with the dynamic nature of this data and lack the\nadaptability to support diverse threat intelligence tasks.\n  In this work, we introduce CYLENS, a cyber threat intelligence copilot\npowered by large language models (LLMs). CYLENS is designed to assist security\nprofessionals throughout the entire threat management lifecycle, supporting\nthreat attribution, contextualization, detection, correlation, prioritization,\nand remediation. To ensure domain expertise, CYLENS integrates knowledge from\n271,570 threat reports into its model parameters and incorporates six\nspecialized NLP modules to enhance reasoning capabilities. Furthermore, CYLENS\ncan be customized to meet the unique needs of different or ganizations,\nunderscoring its adaptability. Through extensive evaluations, we demonstrate\nthat CYLENS consistently outperforms industry-leading LLMs and state-of-the-art\ncybersecurity agents. By detailing its design, development, and evaluation,\nthis work provides a blueprint for leveraging LLMs to address complex,\ndata-intensive cybersecurity challenges."
    },
    {
        "date": "2025-02",
        "title": "The Common Objects Underwater (COU) Dataset for Robust Underwater Object Detection",
        "author": "Rishi Mukherjee, Sakshi Singh, Jack McWilliams, and Junaed Sattar",
        "link": "http://arxiv.org/abs/2502.20651v1",
        "abstract": "We introduce COU: Common Objects Underwater, an instance-segmented image\ndataset of commonly found man-made objects in multiple aquatic and marine\nenvironments. COU contains approximately 10K segmented images, annotated from\nimages collected during a number of underwater robot field trials in diverse\nlocations. COU has been created to address the lack of datasets with robust\nclass coverage curated for underwater instance segmentation, which is\nparticularly useful for training light-weight, real-time capable detectors for\nAutonomous Underwater Vehicles (AUVs). In addition, COU addresses the lack of\ndiversity in object classes since the commonly available underwater image\ndatasets focus only on marine life. Currently, COU contains images from both\nclosed-water (pool) and open-water (lakes and oceans) environments, of 24\ndifferent classes of objects including marine debris, dive tools, and AUVs. To\nassess the efficacy of COU in training underwater object detectors, we use\nthree state-of-the-art models to evaluate its performance and accuracy, using a\ncombination of standard accuracy and efficiency metrics. The improved\nperformance of COU-trained detectors over those solely trained on terrestrial\ndata demonstrates the clear advantage of training with annotated underwater\nimages. We make COU available for broad use under open-source licenses."
    },
    {
        "date": "2025-02",
        "title": "Gungnir: Exploiting Stylistic Features in Images for Backdoor Attacks on Diffusion Models",
        "author": "Yu Pan, Bingrong Dai, Jiahao Chen, Lin Wang, Yi Du, and Jiao Liu",
        "link": "http://arxiv.org/abs/2502.20650v1",
        "abstract": "In recent years, Diffusion Models (DMs) have demonstrated significant\nadvances in the field of image generation. However, according to current\nresearch, DMs are vulnerable to backdoor attacks, which allow attackers to\ncontrol the model's output by inputting data containing covert triggers, such\nas a specific patch or phrase. Existing defense strategies are well equipped to\nthwart such attacks through backdoor detection and trigger inversion because\nprevious attack methods are constrained by limited input spaces and triggers\ndefined by low-dimensional features. To bridge these gaps, we propose Gungnir,\na novel method that enables attackers to activate the backdoor in DMs through\nhidden style triggers within input images. Our approach proposes using\nstylistic features as triggers for the first time and implements backdoor\nattacks successfully in image2image tasks by utilizing\nReconstructing-Adversarial Noise (RAN) and Short-Term-Timesteps-Retention\n(STTR) of DMs. Meanwhile, experiments demonstrate that our method can easily\nbypass existing defense methods. Among existing DM main backdoor defense\nframeworks, our approach achieves a 0\\% backdoor detection rate (BDR). Our\ncodes are available at https://github.com/paoche11/Gungnir."
    },
    {
        "date": "2025-02",
        "title": "TractCloud-FOV: Deep Learning-based Robust Tractography Parcellation in Diffusion MRI with Incomplete Field of View",
        "author": "Yuqian Chen, Leo Zekelman, Yui Lo, Suheyla Cetin-Karayumak, Tengfei Xue, Yogesh Rathi, Nikos Makris, Fan Zhang, Weidong Cai, and Lauren J. O'Donnell",
        "link": "http://arxiv.org/abs/2502.20637v1",
        "abstract": "Tractography parcellation classifies streamlines reconstructed from diffusion\nMRI into anatomically defined fiber tracts for clinical and research\napplications. However, clinical scans often have incomplete fields of view\n(FOV) where brain regions are partially imaged, leading to partial or truncated\nfiber tracts. To address this challenge, we introduce TractCloud-FOV, a deep\nlearning framework that robustly parcellates tractography under conditions of\nincomplete FOV. We propose a novel training strategy, FOV-Cut Augmentation\n(FOV-CA), in which we synthetically cut tractograms to simulate a spectrum of\nreal-world inferior FOV cutoff scenarios. This data augmentation approach\nenriches the training set with realistic truncated streamlines, enabling the\nmodel to achieve superior generalization. We evaluate the proposed\nTractCloud-FOV on both synthetically cut tractography and two real-life\ndatasets with incomplete FOV. TractCloud-FOV significantly outperforms several\nstate-of-the-art methods on all testing datasets in terms of streamline\nclassification accuracy, generalization ability, tract anatomical depiction,\nand computational efficiency. Overall, TractCloud-FOV achieves efficient and\nconsistent tractography parcellation in diffusion MRI with incomplete FOV."
    },
    {
        "date": "2025-02",
        "title": "Towards Privacy-Preserving Split Learning: Destabilizing Adversarial Inference and Reconstruction Attacks in the Cloud",
        "author": "Griffin Higgins, Roozbeh Razavi-Far, Xichen Zhang, Amir David, Ali Ghorbani, and Tongyu Ge",
        "link": "http://arxiv.org/abs/2502.20629v1",
        "abstract": "This work aims to provide both privacy and utility within a split learning\nframework while considering both forward attribute inference and backward\nreconstruction attacks. To address this, a novel approach has been proposed,\nwhich makes use of class activation maps and autoencoders as a plug-in strategy\naiming to increase the user's privacy and destabilize an adversary. The\nproposed approach is compared with a dimensionality-reduction-based plug-in\nstrategy, which makes use of principal component analysis to transform the\nfeature map onto a lower-dimensional feature space. Our work shows that our\nproposed autoencoder-based approach is preferred as it can provide protection\nat an earlier split position over the tested architectures in our setting, and,\nhence, better utility for resource-constrained devices in edge-cloud\ncollaborative inference (EC) systems."
    },
    {
        "date": "2025-02",
        "title": "Towards Zero Touch Networks: Cross-Layer Automated Security Solutions for 6G Wireless Networks",
        "author": "Li Yang, Shimaa Naser, Abdallah Shami, Sami Muhaidat, Lyndon Ong, and M\u00e9rouane Debbah",
        "link": "http://arxiv.org/abs/2502.20627v1",
        "abstract": "The transition from 5G to 6G mobile networks necessitates network automation\nto meet the escalating demands for high data rates, ultra-low latency, and\nintegrated technology. Recently, Zero-Touch Networks (ZTNs), driven by\nArtificial Intelligence (AI) and Machine Learning (ML), are designed to\nautomate the entire lifecycle of network operations with minimal human\nintervention, presenting a promising solution for enhancing automation in 5G/6G\nnetworks. However, the implementation of ZTNs brings forth the need for\nautonomous and robust cybersecurity solutions, as ZTNs rely heavily on\nautomation. AI/ML algorithms are widely used to develop cybersecurity\nmechanisms, but require substantial specialized expertise and encounter model\ndrift issues, posing significant challenges in developing autonomous\ncybersecurity measures. Therefore, this paper proposes an automated security\nframework targeting Physical Layer Authentication (PLA) and Cross-Layer\nIntrusion Detection Systems (CLIDS) to address security concerns at multiple\nInternet protocol layers. The proposed framework employs drift-adaptive online\nlearning techniques and a novel enhanced Successive Halving (SH)-based\nAutomated ML (AutoML) method to automatically generate optimized ML models for\ndynamic networking environments. Experimental results illustrate that the\nproposed framework achieves high performance on the public Radio Frequency (RF)\nfingerprinting and the Canadian Institute for CICIDS2017 datasets, showcasing\nits effectiveness in addressing PLA and CLIDS tasks within dynamic and complex\nnetworking environments. Furthermore, the paper explores open challenges and\nresearch directions in the 5G/6G cybersecurity domain. This framework\nrepresents a significant advancement towards fully autonomous and secure 6G\nnetworks, paving the way for future innovations in network automation and\ncybersecurity."
    },
    {
        "date": "2025-02",
        "title": "Continuous Adversarial Text Representation Learning for Affective Recognition",
        "author": "Seungah Son, Andrez Saurez, and Dongsoo Har",
        "link": "http://arxiv.org/abs/2502.20613v1",
        "abstract": "While pre-trained language models excel at semantic understanding, they often\nstruggle to capture nuanced affective information critical for affective\nrecognition tasks. To address these limitations, we propose a novel framework\nfor enhancing emotion-aware embeddings in transformer-based models. Our\napproach introduces a continuous valence-arousal labeling system to guide\ncontrastive learning, which captures subtle and multi-dimensional emotional\nnuances more effectively. Furthermore, we employ a dynamic token perturbation\nmechanism, using gradient-based saliency to focus on sentiment-relevant tokens,\nimproving model sensitivity to emotional cues. The experimental results\ndemonstrate that the proposed framework outperforms existing methods, achieving\nup to 15.5% improvement in the emotion classification benchmark, highlighting\nthe importance of employing continuous labels. This improvement demonstrates\nthat the proposed framework is effective in affective representation learning\nand enables precise and contextually relevant emotional understanding."
    },
    {
        "date": "2025-02",
        "title": "Exploring the Impact of Temperature Scaling in Softmax for Classification and Adversarial Robustness",
        "author": "Hao Xuan, Bokai Yang, and Xingyu Li",
        "link": "http://arxiv.org/abs/2502.20604v1",
        "abstract": "The softmax function is a fundamental component in deep learning. This study\ndelves into the often-overlooked parameter within the softmax function, known\nas \"temperature,\" providing novel insights into the practical and theoretical\naspects of temperature scaling for image classification. Our empirical studies,\nadopting convolutional neural networks and transformers on multiple benchmark\ndatasets, reveal that moderate temperatures generally introduce better overall\nperformance. Through extensive experiments and rigorous theoretical analysis,\nwe explore the role of temperature scaling in model training and unveil that\ntemperature not only influences learning step size but also shapes the model's\noptimization direction. Moreover, for the first time, we discover a surprising\nbenefit of elevated temperatures: enhanced model robustness against common\ncorruption, natural perturbation, and non-targeted adversarial attacks like\nProjected Gradient Descent. We extend our discoveries to adversarial training,\ndemonstrating that, compared to the standard softmax function with the default\ntemperature value, higher temperatures have the potential to enhance\nadversarial training. The insights of this work open new avenues for improving\nmodel performance and security in deep learning applications."
    },
    {
        "date": "2025-02",
        "title": "LISArD: Learning Image Similarity to Defend Against Gray-box Adversarial Attacks",
        "author": "Joana C. Costa, Tiago Roxo, Hugo Proen\u00e7a, and Pedro R. M. In\u00e1cio",
        "link": "http://arxiv.org/abs/2502.20562v1",
        "abstract": "State-of-the-art defense mechanisms are typically evaluated in the context of\nwhite-box attacks, which is not realistic, as it assumes the attacker can\naccess the gradients of the target network. To protect against this scenario,\nAdversarial Training (AT) and Adversarial Distillation (AD) include adversarial\nexamples during the training phase, and Adversarial Purification uses a\ngenerative model to reconstruct all the images given to the classifier. This\npaper considers an even more realistic evaluation scenario: gray-box attacks,\nwhich assume that the attacker knows the architecture and the dataset used to\ntrain the target network, but cannot access its gradients. We provide empirical\nevidence that models are vulnerable to gray-box attacks and propose LISArD, a\ndefense mechanism that does not increase computational and temporal costs but\nprovides robustness against gray-box and white-box attacks without including\nAT. Our method approximates a cross-correlation matrix, created with the\nembeddings of perturbed and clean images, to a diagonal matrix while\nsimultaneously conducting classification learning. Our results show that LISArD\ncan effectively protect against gray-box attacks, can be used in multiple\narchitectures, and carries over its resilience to the white-box scenario. Also,\nstate-of-the-art AD models underperform greatly when removing AT and/or moving\nto gray-box settings, highlighting the lack of robustness from existing\napproaches to perform in various conditions (aside from white-box settings).\nAll the source code is available at https://github.com/Joana-Cabral/LISArD."
    },
    {
        "date": "2025-02",
        "title": "Robust Multicast Origin Authentication in MACsec and CANsec for Automotive Scenarios",
        "author": "Gianluca Cena, Lucia Seno, and Stefano Scanzio",
        "link": "http://arxiv.org/abs/2502.20555v1",
        "abstract": "Having everything interconnected through the Internet, including vehicle\nonboard systems, is making security a primary concern in the automotive domain\nas well. Although Ethernet and CAN XL provide link-level security based on\nsymmetric cryptography, they do not support origin authentication for multicast\ntransmissions. Asymmetric cryptography is unsuitable for networked embedded\ncontrol systems with real-time constraints and limited computational resources.\nIn these cases, solutions derived from the TESLA broadcast authentication\nprotocol may constitute a more suitable option.\n  In this paper, some such strategies are presented and analyzed that allow for\nmulticast origin authentication, also improving robustness to frame losses by\nmeans of interleaved keychains. A flexible authentication mechanism that relies\non a unified receiver is then proposed, which enables transmitters to select\nstrategies at runtime, to achieve the best compromise among security,\nreliability, and resource consumption."
    },
    {
        "date": "2025-02",
        "title": "In-Model Merging for Enhancing the Robustness of Medical Imaging Classification Models",
        "author": "Hu Wang, Ibrahim Almakky, Congbo Ma, Numan Saeed, and Mohammad Yaqub",
        "link": "http://arxiv.org/abs/2502.20516v1",
        "abstract": "Model merging is an effective strategy to merge multiple models for enhancing\nmodel performances, and more efficient than ensemble learning as it will not\nintroduce extra computation into inference. However, limited research explores\nif the merging process can occur within one model and enhance the model's\nrobustness, which is particularly critical in the medical image domain. In the\npaper, we are the first to propose in-model merging (InMerge), a novel approach\nthat enhances the model's robustness by selectively merging similar\nconvolutional kernels in the deep layers of a single convolutional neural\nnetwork (CNN) during the training process for classification. We also\nanalytically reveal important characteristics that affect how in-model merging\nshould be performed, serving as an insightful reference for the community. We\ndemonstrate the feasibility and effectiveness of this technique for different\nCNN architectures on 4 prevalent datasets. The proposed InMerge-trained model\nsurpasses the typically-trained model by a substantial margin. The code will be\nmade public."
    },
    {
        "date": "2025-02",
        "title": "Best Foot Forward: Robust Foot Reconstruction in-the-wild",
        "author": "Kyle Fogarty, Jing Yang, Chayan Kumar Patodi, Aadi Bhanti, Steven Chacko, Cengiz Oztireli, and Ujwal Bonde",
        "link": "http://arxiv.org/abs/2502.20511v1",
        "abstract": "Accurate 3D foot reconstruction is crucial for personalized orthotics,\ndigital healthcare, and virtual fittings. However, existing methods struggle\nwith incomplete scans and anatomical variations, particularly in self-scanning\nscenarios where user mobility is limited, making it difficult to capture areas\nlike the arch and heel. We present a novel end-to-end pipeline that refines\nStructure-from-Motion (SfM) reconstruction. It first resolves scan alignment\nambiguities using SE(3) canonicalization with a viewpoint prediction module,\nthen completes missing geometry through an attention-based network trained on\nsynthetically augmented point clouds. Our approach achieves state-of-the-art\nperformance on reconstruction metrics while preserving clinically validated\nanatomical fidelity. By combining synthetic training data with learned\ngeometric priors, we enable robust foot reconstruction under real-world capture\nconditions, unlocking new opportunities for mobile-based 3D scanning in\nhealthcare and retail."
    },
    {
        "date": "2025-02",
        "title": "HELENE: An Open-Source High-Security Privacy-Preserving Blockchain Based System for Automating and Managing Laboratory Health Tests",
        "author": "Gabriel Fern\u00e1ndez-Blanco, Pedro Garc\u00eda-Cereijo, David Lema-N\u00fa\u00f1ez, Diego Ramil-L\u00f3pez, Paula Fraga-Lamas, Leire Egia-Mendikute, As\u00eds Palaz\u00f3n, and Tiago M. Fern\u00e1ndez-Caram\u00e9s",
        "link": "http://arxiv.org/abs/2502.20477v1",
        "abstract": "In the last years, especially since the COVID-19 pandemic, precision medicine\nplatforms emerged as useful tools for supporting new tests like the ones that\ndetect the presence of antibodies and antigens with better sensitivity and\nspecificity than traditional methods. In addition, the pandemic has also\ninfluenced the way people interact (decentralization), behave (digital world)\nand purchase health services (online). Moreover, there is a growing concern in\nthe way health data are managed, especially in terms of privacy. To tackle such\nissues, this article presents a sustainable direct-to-consumer health-service\nopen-source platform called HELENE that is supported by blockchain and by a\nnovel decentralized oracle that protects patient data privacy. Specifically,\nHELENE enables health test providers to compete through auctions, allowing\npatients to bid for their services and to keep the control over their health\ntest results. Moreover, data exchanges among the involved stakeholders can be\nperformed in a trustworthy, transparent and standardized way to ease software\nintegration and to avoid incompatibilities. After providing a thorough\ndescription of the platform, the proposed health platform is assessed in terms\nof smart contract performance. In addition, the response time of the developed\noracle is evaluated and NIST SP 800-22 tests are executed to demonstrate the\nadequacy of the devised random number generator. Thus, this article shows the\ncapabilities and novel propositions of HELENE for delivering health services\nproviding an open-source platform for future researchers, who can enhance it\nand adapt it to their needs."
    },
    {
        "date": "2025-02",
        "title": "Why Are Web AI Agents More Vulnerable Than Standalone LLMs? A Security Analysis",
        "author": "Jeffrey Yang Fan Chiang, Seungjae Lee, Jia-Bin Huang, Furong Huang, and Yizheng Chen",
        "link": "http://arxiv.org/abs/2502.20383v1",
        "abstract": "Recent advancements in Web AI agents have demonstrated remarkable\ncapabilities in addressing complex web navigation tasks. However, emerging\nresearch shows that these agents exhibit greater vulnerability compared to\nstandalone Large Language Models (LLMs), despite both being built upon the same\nsafety-aligned models. This discrepancy is particularly concerning given the\ngreater flexibility of Web AI Agent compared to standalone LLMs, which may\nexpose them to a wider range of adversarial user inputs. To build a scaffold\nthat addresses these concerns, this study investigates the underlying factors\nthat contribute to the increased vulnerability of Web AI agents. Notably, this\ndisparity stems from the multifaceted differences between Web AI agents and\nstandalone LLMs, as well as the complex signals - nuances that simple\nevaluation metrics, such as success rate, often fail to capture. To tackle\nthese challenges, we propose a component-level analysis and a more granular,\nsystematic evaluation framework. Through this fine-grained investigation, we\nidentify three critical factors that amplify the vulnerability of Web AI\nagents; (1) embedding user goals into the system prompt, (2) multi-step action\ngeneration, and (3) observational capabilities. Our findings highlights the\npressing need to enhance security and robustness in AI agent design and provide\nactionable insights for targeted defense strategies."
    },
    {
        "date": "2025-02",
        "title": "Adversarial Robustness in Parameter-Space Classifiers",
        "author": "Tamir Shor, Ethan Fetaya, Chaim Baskin, and Alex Bronstein",
        "link": "http://arxiv.org/abs/2502.20314v1",
        "abstract": "Implicit Neural Representations (INRs) have been recently garnering\nincreasing interest in various research fields, mainly due to their ability to\nrepresent large, complex data in a compact and continuous manner. Past work\nfurther showed that numerous popular downstream tasks can be performed directly\nin the INR parameter-space. Doing so can substantially reduce the computational\nresources required to process the represented data in their native domain. A\nmajor difficulty in using modern machine-learning approaches, is their high\nsusceptibility to adversarial attacks, which have been shown to greatly limit\nthe reliability and applicability of such methods in a wide range of settings.\nIn this work, we show that parameter-space models trained for classification\nare inherently robust to adversarial attacks -- without the need of any robust\ntraining. To support our claims, we develop a novel suite of adversarial\nattacks targeting parameter-space classifiers, and furthermore analyze\npractical considerations of attacking parameter-space classifiers. Code for\nreproducing all experiments and implementation of all proposed methods will be\nreleased upon publication."
    },
    {
        "date": "2025-02",
        "title": "SecureGaze: Defending Gaze Estimation Against Backdoor Attacks",
        "author": "Lingyu Du, Yupei Liu, Jinyuan Jia, and Guohao Lan",
        "link": "http://arxiv.org/abs/2502.20306v1",
        "abstract": "Gaze estimation models are widely used in applications such as driver\nattention monitoring and human-computer interaction. While many methods for\ngaze estimation exist, they rely heavily on data-hungry deep learning to\nachieve high performance. This reliance often forces practitioners to harvest\ntraining data from unverified public datasets, outsource model training, or\nrely on pre-trained models. However, such practices expose gaze estimation\nmodels to backdoor attacks. In such attacks, adversaries inject backdoor\ntriggers by poisoning the training data, creating a backdoor vulnerability: the\nmodel performs normally with benign inputs, but produces manipulated gaze\ndirections when a specific trigger is present. This compromises the security of\nmany gaze-based applications, such as causing the model to fail in tracking the\ndriver's attention. To date, there is no defense that addresses backdoor\nattacks on gaze estimation models. In response, we introduce SecureGaze, the\nfirst solution designed to protect gaze estimation models from such attacks.\nUnlike classification models, defending gaze estimation poses unique challenges\ndue to its continuous output space and globally activated backdoor behavior. By\nidentifying distinctive characteristics of backdoored gaze estimation models,\nwe develop a novel and effective approach to reverse-engineer the trigger\nfunction for reliable backdoor detection. Extensive evaluations in both digital\nand physical worlds demonstrate that SecureGaze effectively counters a range of\nbackdoor attacks and outperforms seven state-of-the-art defenses adapted from\nclassification models."
    },
    {
        "date": "2025-02",
        "title": "Generative adversarial neural networks for simulating neutrino interactions",
        "author": "Jose L. Bonilla, Krzysztof M. Graczyk, Artur M. Ankowski, Rwik Dharmapal Banerjee, Beata E. Kowal, Hemant Prasad, and Jan T. Sobczyk",
        "link": "http://arxiv.org/abs/2502.20244v1",
        "abstract": "We propose a new approach to simulate neutrino scattering events as an\nalternative to the standard Monte Carlo generator approach. Generative\nadversarial neural network (GAN) models are developed to simulate\nneutrino-carbon collisions in the few-GeV energy range. The models produce\nscattering events for a given neutrino energy. GAN models are trained on\nsimulation data from NuWro Monte Carlo event generator. Two GAN models have\nbeen obtained: one simulating only quasielastic neutrino-nucleus scatterings\nand another simulating all interactions at given neutrino energy. The\nperformance of both models has been assessed using two statistical metrics. It\nis shown that both GAN models successfully reproduce the event distributions."
    },
    {
        "date": "2025-02",
        "title": "4Deform: Neural Surface Deformation for Robust Shape Interpolation",
        "author": "Lu Sang, Zehranaz Canfes, Dongliang Cao, Riccardo Marin, Florian Bernard, and Daniel Cremers",
        "link": "http://arxiv.org/abs/2502.20208v1",
        "abstract": "Generating realistic intermediate shapes between non-rigidly deformed shapes\nis a challenging task in computer vision, especially with unstructured data\n(e.g., point clouds) where temporal consistency across frames is lacking, and\ntopologies are changing. Most interpolation methods are designed for structured\ndata (i.e., meshes) and do not apply to real-world point clouds. In contrast,\nour approach, 4Deform, leverages neural implicit representation (NIR) to enable\nfree topology changing shape deformation. Unlike previous mesh-based methods\nthat learn vertex-based deformation fields, our method learns a continuous\nvelocity field in Euclidean space. Thus, it is suitable for less structured\ndata such as point clouds. Additionally, our method does not require\nintermediate-shape supervision during training; instead, we incorporate\nphysical and geometrical constraints to regularize the velocity field. We\nreconstruct intermediate surfaces using a modified level-set equation, directly\nlinking our NIR with the velocity field. Experiments show that our method\nsignificantly outperforms previous NIR approaches across various scenarios\n(e.g., noisy, partial, topology-changing, non-isometric shapes) and, for the\nfirst time, enables new applications like 4D Kinect sequence upsampling and\nreal-world high-resolution mesh deformation."
    },
    {
        "date": "2025-02",
        "title": "SSD: A State-based Stealthy Backdoor Attack For Navigation System in UAV Route Planning",
        "author": "Zhaoxuan Wang, Yang Li, Jie Zhang, Xingshuo Han, Kangbo Liu, Lyu Yang, yuan Zhou, Tianwei Zhang, and Quan Pan",
        "link": "http://arxiv.org/abs/2502.20178v1",
        "abstract": "Unmanned aerial vehicles (UAVs) are increasingly employed to perform\nhigh-risk tasks that require minimal human intervention. However, UAVs face\nescalating cybersecurity threats, particularly from GNSS spoofing attacks.\nWhile previous studies have extensively investigated the impacts of GNSS\nspoofing on UAVs, few have focused on its effects on specific tasks. Moreover,\nthe influence of UAV motion states on the assessment of network security risks\nis often overlooked. To address these gaps, we first provide a detailed\nevaluation of how motion states affect the effectiveness of network attacks. We\ndemonstrate that nonlinear motion states not only enhance the effectiveness of\nposition spoofing in GNSS spoofing attacks but also reduce the probability of\nspeed-related attack detection. Building upon this, we propose a\nstate-triggered backdoor attack method (SSD) to deceive GNSS systems and assess\nits risk to trajectory planning tasks. Extensive validation of SSD's\neffectiveness and stealthiness is conducted. Experimental results show that,\nwith appropriately tuned hyperparameters, SSD significantly increases\npositioning errors and the risk of task failure, while maintaining 100% stealth\nacross three state-of-the-art detectors."
    },
    {
        "date": "2025-02",
        "title": "Robust sensitivity control in digital pathology via tile score distribution matching",
        "author": "Arthur Pignet, John Klein, Genevieve Robin, and Antoine Olivier",
        "link": "http://arxiv.org/abs/2502.20144v2",
        "abstract": "Deploying digital pathology models across medical centers is challenging due\nto distribution shifts. Recent advances in domain generalization improve model\ntransferability in terms of aggregated performance measured by the Area Under\nCurve (AUC). However, clinical regulations often require to control the\ntransferability of other metrics, such as prescribed sensitivity levels. We\nintroduce a novel approach to control the sensitivity of whole slide image\n(WSI) classification models, based on optimal transport and Multiple Instance\nLearning (MIL). Validated across multiple cohorts and tasks, our method enables\nrobust sensitivity control with only a handful of calibration samples,\nproviding a practical solution for reliable deployment of computational\npathology systems."
    },
    {
        "date": "2025-02",
        "title": "Order-Robust Class Incremental Learning: Graph-Driven Dynamic Similarity Grouping",
        "author": "Guannan Lai, Yujie Li, Xiangkun Wang, Junbo Zhang, Tianrui Li, and Xin Yang",
        "link": "http://arxiv.org/abs/2502.20032v1",
        "abstract": "Class Incremental Learning (CIL) requires a model to continuously learn new\nclasses without forgetting previously learned ones. While recent studies have\nsignificantly alleviated the problem of catastrophic forgetting (CF), more and\nmore research reveals that the order in which classes appear have significant\ninfluences on CIL models. Specifically, prioritizing the learning of classes\nwith lower similarity will enhance the model's generalization performance and\nits ability to mitigate forgetting. Hence, it is imperative to develop an\norder-robust class incremental learning model that maintains stable performance\neven when faced with varying levels of class similarity in different orders. In\nresponse, we first provide additional theoretical analysis, which reveals that\nwhen the similarity among a group of classes is lower, the model demonstrates\nincreased robustness to the class order. Then, we introduce a novel\n\\textbf{G}raph-\\textbf{D}riven \\textbf{D}ynamic \\textbf{S}imilarity\n\\textbf{G}rouping (\\textbf{GDDSG}) method, which leverages a graph coloring\nalgorithm for class-based similarity grouping. The proposed approach trains\nindependent CIL models for each group of classes, ultimately combining these\nmodels to facilitate joint prediction. Experimental results demonstrate that\nour method effectively addresses the issue of class order sensitivity while\nachieving optimal performance in both model accuracy and anti-forgetting\ncapability. Our code is available at https://github.com/AIGNLAI/GDDSG."
    },
    {
        "date": "2025-02",
        "title": "Modern DDoS Threats and Countermeasures: Insights into Emerging Attacks and Detection Strategies",
        "author": "Jincheng Wang, Le Yu, John C. S. Lui, and Xiapu Luo",
        "link": "http://arxiv.org/abs/2502.19996v1",
        "abstract": "Distributed Denial of Service (DDoS) attacks persist as significant threats\nto online services and infrastructure, evolving rapidly in sophistication and\neluding traditional detection mechanisms. This evolution demands a\ncomprehensive examination of current trends in DDoS attacks and the efficacy of\nmodern detection strategies. This paper offers an comprehensive survey of\nemerging DDoS attacks and detection strategies over the past decade. We delve\ninto the diversification of attack targets, extending beyond conventional web\nservices to include newer network protocols and systems, and the adoption of\nadvanced adversarial tactics. Additionally, we review current detection\ntechniques, highlighting essential features that modern systems must integrate\nto effectively neutralize these evolving threats. Given the technological\ndemands of contemporary network systems, such as high-volume and in-line packet\nprocessing capabilities, we also explore how innovative hardware technologies\nlike programmable switches can significantly enhance the development and\ndeployment of robust DDoS detection systems. We conclude by identifying open\nproblems and proposing future directions for DDoS research. In particular, our\nsurvey sheds light on the investigation of DDoS attack surfaces for emerging\nsystems, protocols, and adversarial strategies. Moreover, we outlines critical\nopen questions in the development of effective detection systems, e.g., the\ncreation of defense mechanisms independent of control planes."
    },
    {
        "date": "2025-02",
        "title": "ReCon: Enhancing True Correspondence Discrimination through Relation Consistency for Robust Noisy Correspondence Learning",
        "author": "Quanxing Zha, Xin Liu, Shu-Juan Peng, Yiu-ming Cheung, Xing Xu, and Nannan Wang",
        "link": "http://arxiv.org/abs/2502.19962v1",
        "abstract": "Can we accurately identify the true correspondences from multimodal datasets\ncontaining mismatched data pairs? Existing methods primarily emphasize the\nsimilarity matching between the representations of objects across modalities,\npotentially neglecting the crucial relation consistency within modalities that\nare particularly important for distinguishing the true and false\ncorrespondences. Such an omission often runs the risk of misidentifying\nnegatives as positives, thus leading to unanticipated performance degradation.\nTo address this problem, we propose a general Relation Consistency learning\nframework, namely ReCon, to accurately discriminate the true correspondences\namong the multimodal data and thus effectively mitigate the adverse impact\ncaused by mismatches. Specifically, ReCon leverages a novel relation\nconsistency learning to ensure the dual-alignment, respectively of, the\ncross-modal relation consistency between different modalities and the\nintra-modal relation consistency within modalities. Thanks to such dual\nconstrains on relations, ReCon significantly enhances its effectiveness for\ntrue correspondence discrimination and therefore reliably filters out the\nmismatched pairs to mitigate the risks of wrong supervisions. Extensive\nexperiments on three widely-used benchmark datasets, including Flickr30K,\nMS-COCO, and Conceptual Captions, are conducted to demonstrate the\neffectiveness and superiority of ReCon compared with other SOTAs. The code is\navailable at: https://github.com/qxzha/ReCon."
    },
    {
        "date": "2025-02",
        "title": "Dynamic DropConnect: Enhancing Neural Network Robustness through Adaptive Edge Dropping Strategies",
        "author": "Yuan-Chih Yang, and Hung-Hsuan Chen",
        "link": "http://arxiv.org/abs/2502.19948v1",
        "abstract": "Dropout and DropConnect are well-known techniques that apply a consistent\ndrop rate to randomly deactivate neurons or edges in a neural network layer\nduring training. This paper introduces a novel methodology that assigns dynamic\ndrop rates to each edge within a layer, uniquely tailoring the dropping process\nwithout incorporating additional learning parameters. We perform experiments on\nsynthetic and openly available datasets to validate the effectiveness of our\napproach. The results demonstrate that our method outperforms Dropout,\nDropConnect, and Standout, a classic mechanism known for its adaptive dropout\ncapabilities. Furthermore, our approach improves the robustness and\ngeneralization of neural network training without increasing computational\ncomplexity. The complete implementation of our methodology is publicly\naccessible for research and replication purposes at\nhttps://github.com/ericabd888/Adjusting-the-drop-probability-in-DropConnect-based-on-the-magnitude-of-the-gradient/."
    },
    {
        "date": "2025-02",
        "title": "Behind the Tip of Efficiency: Uncovering the Submerged Threats of Jailbreak Attacks in Small Language Models",
        "author": "Sibo Yi, Tianshuo Cong, Xinlei He, Qi Li, and Jiaxing Song",
        "link": "http://arxiv.org/abs/2502.19883v2",
        "abstract": "Small language models (SLMs) have become increasingly prominent in the\ndeployment on edge devices due to their high efficiency and low computational\ncost. While researchers continue to advance the capabilities of SLMs through\ninnovative training strategies and model compression techniques, the security\nrisks of SLMs have received considerably less attention compared to large\nlanguage models (LLMs).To fill this gap, we provide a comprehensive empirical\nstudy to evaluate the security performance of 13 state-of-the-art SLMs under\nvarious jailbreak attacks. Our experiments demonstrate that most SLMs are quite\nsusceptible to existing jailbreak attacks, while some of them are even\nvulnerable to direct harmful prompts.To address the safety concerns, we\nevaluate several representative defense methods and demonstrate their\neffectiveness in enhancing the security of SLMs. We further analyze the\npotential security degradation caused by different SLM techniques including\narchitecture compression, quantization, knowledge distillation, and so on. We\nexpect that our research can highlight the security challenges of SLMs and\nprovide valuable insights to future work in developing more robust and secure\nSLMs."
    },
    {
        "date": "2025-02",
        "title": "Snowball Adversarial Attack on Traffic Sign Classification",
        "author": "Anthony Etim, and Jakub Szefer",
        "link": "http://arxiv.org/abs/2502.19757v1",
        "abstract": "Adversarial attacks on machine learning models often rely on small,\nimperceptible perturbations to mislead classifiers. Such strategy focuses on\nminimizing the visual perturbation for humans so they are not confused, and\nalso maximizing the misclassification for machine learning algorithms. An\northogonal strategy for adversarial attacks is to create perturbations that are\nclearly visible but do not confuse humans, yet still maximize misclassification\nfor machine learning algorithms. This work follows the later strategy, and\ndemonstrates instance of it through the Snowball Adversarial Attack in the\ncontext of traffic sign recognition. The attack leverages the human brain's\nsuperior ability to recognize objects despite various occlusions, while machine\nlearning algorithms are easily confused. The evaluation shows that the Snowball\nAdversarial Attack is robust across various images and is able to confuse\nstate-of-the-art traffic sign recognition algorithm. The findings reveal that\nSnowball Adversarial Attack can significantly degrade model performance with\nminimal effort, raising important concerns about the vulnerabilities of deep\nneural networks and highlighting the necessity for improved defenses for image\nrecognition machine learning models."
    },
    {
        "date": "2025-02",
        "title": "HALO: Robust Out-of-Distribution Detection via Joint Optimisation",
        "author": "Hugo Lyons Keenan, Sarah Erfani, and Christopher Leckie",
        "link": "http://arxiv.org/abs/2502.19755v1",
        "abstract": "Effective out-of-distribution (OOD) detection is crucial for the safe\ndeployment of machine learning models in real-world scenarios. However, recent\nwork has shown that OOD detection methods are vulnerable to adversarial\nattacks, potentially leading to critical failures in high-stakes applications.\nThis discovery has motivated work on robust OOD detection methods that are\ncapable of maintaining performance under various attack settings. Prior\napproaches have made progress on this problem but face a number of limitations:\noften only exhibiting robustness to attacks on OOD data or failing to maintain\nstrong clean performance. In this work, we adapt an existing robust\nclassification framework, TRADES, extending it to the problem of robust OOD\ndetection and discovering a novel objective function. Recognising the critical\nimportance of a strong clean/robust trade-off for OOD detection, we introduce\nan additional loss term which boosts classification and detection performance.\nOur approach, called HALO (Helper-based AdversariaL OOD detection), surpasses\nexisting methods and achieves state-of-the-art performance across a number of\ndatasets and attack settings. Extensive experiments demonstrate an average\nAUROC improvement of 3.15 in clean settings and 7.07 under adversarial attacks\nwhen compared to the next best method. Furthermore, HALO exhibits resistance to\ntransferred attacks, offers tuneable performance through hyperparameter\nselection, and is compatible with existing OOD detection frameworks\nout-of-the-box, leaving open the possibility of future performance gains. Code\nis available at: https://github.com/hugo0076/HALO"
    },
    {
        "date": "2025-02",
        "title": "Tokens for Learning, Tokens for Unlearning: Mitigating Membership Inference Attacks in Large Language Models via Dual-Purpose Training",
        "author": "Toan Tran, Ruixuan Liu, and Li Xiong",
        "link": "http://arxiv.org/abs/2502.19726v1",
        "abstract": "Large language models (LLMs) have become the backbone of modern natural\nlanguage processing but pose privacy concerns about leaking sensitive training\ndata. Membership inference attacks (MIAs), which aim to infer whether a sample\nis included in a model's training dataset, can serve as a foundation for\nbroader privacy threats. Existing defenses designed for traditional\nclassification models do not account for the sequential nature of text data. As\na result, they either require significant computational resources or fail to\neffectively mitigate privacy risks in LLMs. In this work, we propose a\nlightweight yet effective empirical privacy defense for protecting training\ndata of language modeling by leveraging the token-specific characteristics. By\nanalyzing token dynamics during training, we propose a token selection strategy\nthat categorizes tokens into hard tokens for learning and memorized tokens for\nunlearning. Subsequently, our training-phase defense optimizes a novel\ndual-purpose token-level loss to achieve a Pareto-optimal balance between\nutility and privacy. Extensive experiments demonstrate that our approach not\nonly provides strong protection against MIAs but also improves language\nmodeling performance by around 10\\% across various LLM architectures and\ndatasets compared to the baselines."
    },
    {
        "date": "2025-02",
        "title": "SAP-DIFF: Semantic Adversarial Patch Generation for Black-Box Face Recognition Models via Diffusion Models",
        "author": "Mingsi Wang, Shuaiyin Yao, Chang Yue, Lijie Zhang, and Guozhu Meng",
        "link": "http://arxiv.org/abs/2502.19710v1",
        "abstract": "Given the need to evaluate the robustness of face recognition (FR) models,\nmany efforts have focused on adversarial patch attacks that mislead FR models\nby introducing localized perturbations. Impersonation attacks are a significant\nthreat because adversarial perturbations allow attackers to disguise themselves\nas legitimate users. This can lead to severe consequences, including data\nbreaches, system damage, and misuse of resources. However, research on such\nattacks in FR remains limited. Existing adversarial patch generation methods\nexhibit limited efficacy in impersonation attacks due to (1) the need for high\nattacker capabilities, (2) low attack success rates, and (3) excessive query\nrequirements. To address these challenges, we propose a novel method SAP-DIFF\nthat leverages diffusion models to generate adversarial patches via semantic\nperturbations in the latent space rather than direct pixel manipulation. We\nintroduce an attention disruption mechanism to generate features unrelated to\nthe original face, facilitating the creation of adversarial samples and a\ndirectional loss function to guide perturbations toward the target identity\nfeature space, thereby enhancing attack effectiveness and efficiency. Extensive\nexperiments on popular FR models and datasets demonstrate that our method\noutperforms state-of-the-art approaches, achieving an average attack success\nrate improvement of 45.66% (all exceeding 40%), and a reduction in the number\nof queries by about 40% compared to the SOTA approach"
    },
    {
        "date": "2025-02",
        "title": "Prompt-driven Transferable Adversarial Attack on Person Re-Identification with Attribute-aware Textual Inversion",
        "author": "Yuan Bian, Min Liu, Yunqi Yi, Xueping Wang, and Yaonan Wang",
        "link": "http://arxiv.org/abs/2502.19697v1",
        "abstract": "Person re-identification (re-id) models are vital in security surveillance\nsystems, requiring transferable adversarial attacks to explore the\nvulnerabilities of them. Recently, vision-language models (VLM) based attacks\nhave shown superior transferability by attacking generalized image and textual\nfeatures of VLM, but they lack comprehensive feature disruption due to the\noveremphasis on discriminative semantics in integral representation. In this\npaper, we introduce the Attribute-aware Prompt Attack (AP-Attack), a novel\nmethod that leverages VLM's image-text alignment capability to explicitly\ndisrupt fine-grained semantic features of pedestrian images by destroying\nattribute-specific textual embeddings. To obtain personalized textual\ndescriptions for individual attributes, textual inversion networks are designed\nto map pedestrian images to pseudo tokens that represent semantic embeddings,\ntrained in the contrastive learning manner with images and a predefined prompt\ntemplate that explicitly describes the pedestrian attributes. Inverted benign\nand adversarial fine-grained textual semantics facilitate attacker in\neffectively conducting thorough disruptions, enhancing the transferability of\nadversarial examples. Extensive experiments show that AP-Attack achieves\nstate-of-the-art transferability, significantly outperforming previous methods\nby 22.9% on mean Drop Rate in cross-model&dataset attack scenarios."
    },
    {
        "date": "2025-02",
        "title": "Unveiling Security Weaknesses in Autonomous Driving Systems: An In-Depth Empirical Study",
        "author": "Wenyuan Cheng, Zengyang Li, Peng Liang, Ran Mo, and Hui Liu",
        "link": "http://arxiv.org/abs/2502.19687v1",
        "abstract": "The advent of Autonomous Driving Systems (ADS) has marked a significant shift\ntowards intelligent transportation, with implications for public safety and\ntraffic efficiency. While these systems integrate a variety of technologies and\noffer numerous benefits, their security is paramount, as vulnerabilities can\nhave severe consequences for safety and trust. This study aims to\nsystematically investigate potential security weaknesses in the codebases of\nprominent open-source ADS projects using CodeQL, a static code analysis tool.\nThe goal is to identify common vulnerabilities, their distribution and\npersistence across versions to enhance the security of ADS. We selected three\nrepresentative open-source ADS projects, Autoware, AirSim, and Apollo, based on\ntheir high GitHub star counts and Level 4 autonomous driving capabilities.\nUsing CodeQL, we analyzed multiple versions of these projects to identify\nvulnerabilities, focusing on CWE categories such as CWE-190 (Integer Overflow\nor Wraparound) and CWE-20 (Improper Input Validation). We also tracked the\nlifecycle of these vulnerabilities across software versions. This approach\nallows us to systematically analyze vulnerabilities in projects, which has not\nbeen extensively explored in previous ADS research. Our analysis revealed that\nspecific CWE categories, particularly CWE-190 (59.6%) and CWE-20 (16.1%), were\nprevalent across the selected ADS projects. These vulnerabilities often\npersisted for over six months, spanning multiple version iterations. The\nempirical assessment showed a direct link between the severity of these\nvulnerabilities and their tangible effects on ADS performance. These security\nissues among ADS still remain to be resolved. Our findings highlight the need\nfor integrating static code analysis into ADS development to detect and\nmitigate common vulnerabilities."
    },
    {
        "date": "2025-02",
        "title": "Improving Adversarial Transferability in MLLMs via Dynamic Vision-Language Alignment Attack",
        "author": "Chenhe Gu, Jindong Gu, Andong Hua, and Yao Qin",
        "link": "http://arxiv.org/abs/2502.19672v1",
        "abstract": "Multimodal Large Language Models (MLLMs), built upon LLMs, have recently\ngained attention for their capabilities in image recognition and understanding.\nHowever, while MLLMs are vulnerable to adversarial attacks, the transferability\nof these attacks across different models remains limited, especially under\ntargeted attack setting. Existing methods primarily focus on vision-specific\nperturbations but struggle with the complex nature of vision-language modality\nalignment. In this work, we introduce the Dynamic Vision-Language Alignment\n(DynVLA) Attack, a novel approach that injects dynamic perturbations into the\nvision-language connector to enhance generalization across diverse\nvision-language alignment of different models. Our experimental results show\nthat DynVLA significantly improves the transferability of adversarial examples\nacross various MLLMs, including BLIP2, InstructBLIP, MiniGPT4, LLaVA, and\nclosed-source models such as Gemini."
    },
    {
        "date": "2025-02",
        "title": "Training Robust Graph Neural Networks by Modeling Noise Dependencies",
        "author": "Yeonjun In, Kanghoon Yoon, Sukwon Yun, Kibum Kim, Sungchul Kim, and Chanyoung Park",
        "link": "http://arxiv.org/abs/2502.19670v1",
        "abstract": "In real-world applications, node features in graphs often contain noise from\nvarious sources, leading to significant performance degradation in GNNs.\nAlthough several methods have been developed to enhance robustness, they rely\non the unrealistic assumption that noise in node features is independent of the\ngraph structure and node labels, thereby limiting their applicability. To this\nend, we introduce a more realistic noise scenario, dependency-aware noise on\ngraphs (DANG), where noise in node features create a chain of noise\ndependencies that propagates to the graph structure and node labels. We propose\na novel robust GNN, DA-GNN, which captures the causal relationships among\nvariables in the data generating process (DGP) of DANG using variational\ninference. In addition, we present new benchmark datasets that simulate DANG in\nreal-world applications, enabling more practical research on robust GNNs.\nExtensive experiments demonstrate that DA-GNN consistently outperforms existing\nbaselines across various noise scenarios, including both DANG and conventional\nnoise models commonly considered in this field."
    },
    {
        "date": "2025-02",
        "title": "Robust Gymnasium: A Unified Modular Benchmark for Robust Reinforcement Learning",
        "author": "Shangding Gu, Laixi Shi, Muning Wen, Ming Jin, Eric Mazumdar, Yuejie Chi, Adam Wierman, and Costas Spanos",
        "link": "http://arxiv.org/abs/2502.19652v1",
        "abstract": "Driven by inherent uncertainty and the sim-to-real gap, robust reinforcement\nlearning (RL) seeks to improve resilience against the complexity and\nvariability in agent-environment sequential interactions. Despite the existence\nof a large number of RL benchmarks, there is a lack of standardized benchmarks\nfor robust RL. Current robust RL policies often focus on a specific type of\nuncertainty and are evaluated in distinct, one-off environments. In this work,\nwe introduce Robust-Gymnasium, a unified modular benchmark designed for robust\nRL that supports a wide variety of disruptions across all key RL\ncomponents-agents' observed state and reward, agents' actions, and the\nenvironment. Offering over sixty diverse task environments spanning control and\nrobotics, safe RL, and multi-agent RL, it provides an open-source and\nuser-friendly tool for the community to assess current methods and foster the\ndevelopment of robust RL algorithms. In addition, we benchmark existing\nstandard and robust RL algorithms within this framework, uncovering significant\ndeficiencies in each and offering new insights."
    },
    {
        "date": "2025-02",
        "title": "Developing robust methods to handle missing data in real-world applications effectively",
        "author": "Youran Zhou, Mohamed Reda Bouadjenek, and Sunil Aryal",
        "link": "http://arxiv.org/abs/2502.19635v2",
        "abstract": "Missing data is a pervasive challenge spanning diverse data types, including\ntabular, sensor data, time-series, images and so on. Its origins are\nmultifaceted, resulting in various missing mechanisms. Prior research in this\nfield has predominantly revolved around the assumption of the Missing\nCompletely At Random (MCAR) mechanism. However, Missing At Random (MAR) and\nMissing Not At Random (MNAR) mechanisms, though equally prevalent, have often\nremained underexplored despite their significant influence. This PhD project\npresents a comprehensive research agenda designed to investigate the\nimplications of diverse missing data mechanisms. The principal aim is to devise\nrobust methodologies capable of effectively handling missing data while\naccommodating the unique characteristics of MCAR, MAR, and MNAR mechanisms. By\naddressing these gaps, this research contributes to an enriched understanding\nof the challenges posed by missing data across various industries and data\nmodalities. It seeks to provide practical solutions that enable the effective\nmanagement of missing data, empowering researchers and practitioners to\nleverage incomplete datasets confidently."
    },
    {
        "date": "2025-02",
        "title": "Unveiling Wireless Users' Locations via Modulation Classification-based Passive Attack",
        "author": "Ali Hanif, Abdulrahman Katranji, Nour Kouzayha, Muhammad Mahboob Ur Rahman, and Tareq Y. Al-Naffouri",
        "link": "http://arxiv.org/abs/2502.19341v1",
        "abstract": "The broadcast nature of the wireless medium and openness of wireless\nstandards, e.g., 3GPP releases 16-20, invite adversaries to launch various\nactive and passive attacks on cellular and other wireless networks. This work\nidentifies one such loose end of wireless standards and presents a novel\npassive attack method enabling an eavesdropper (Eve) to localize a line of\nsight wireless user (Bob) who is communicating with a base station or WiFi\naccess point (Alice). The proposed attack involves two phases. In the first\nphase, Eve performs modulation classification by intercepting the downlink\nchannel between Alice and Bob. This enables Eve to utilize the publicly\navailable modulation and coding scheme (MCS) tables to do pesudo-ranging, i.e.,\nthe Eve determines the ring within which Bob is located, which drastically\nreduces the search space. In the second phase, Eve sniffs the uplink channel,\nand employs multiple strategies to further refine Bob's location within the\nring. Towards the end, we present our thoughts on how this attack can be\nextended to non-line-of-sight scenarios, and how this attack could act as a\nscaffolding to construct a malicious digital twin map."
    },
    {
        "date": "2025-02",
        "title": "Multi-Agent Security Tax: Trading Off Security and Collaboration Capabilities in Multi-Agent Systems",
        "author": "Pierre Peigne-Lefebvre, Mikolaj Kniejski, Filip Sondej, Matthieu David, Jason Hoelscher-Obermaier, Christian Schroeder de Witt, and Esben Kran",
        "link": "http://arxiv.org/abs/2502.19145v1",
        "abstract": "As AI agents are increasingly adopted to collaborate on complex objectives,\nensuring the security of autonomous multi-agent systems becomes crucial. We\ndevelop simulations of agents collaborating on shared objectives to study these\nsecurity risks and security trade-offs. We focus on scenarios where an attacker\ncompromises one agent, using it to steer the entire system toward misaligned\noutcomes by corrupting other agents. In this context, we observe infectious\nmalicious prompts - the multi-hop spreading of malicious instructions. To\nmitigate this risk, we evaluated several strategies: two \"vaccination\"\napproaches that insert false memories of safely handling malicious input into\nthe agents' memory stream, and two versions of a generic safety instruction\nstrategy. While these defenses reduce the spread and fulfillment of malicious\ninstructions in our experiments, they tend to decrease collaboration capability\nin the agent network. Our findings illustrate potential trade-off between\nsecurity and collaborative efficiency in multi-agent systems, providing\ninsights for designing more secure yet effective AI collaborations."
    },
    {
        "date": "2025-02",
        "title": "XSS Adversarial Attacks Based on Deep Reinforcement Learning: A Replication and Extension Study",
        "author": "Samuele Pasini, Gianluca Maragliano, Jinhan Kim, and Paolo Tonella",
        "link": "http://arxiv.org/abs/2502.19095v1",
        "abstract": "Cross-site scripting (XSS) poses a significant threat to web application\nsecurity. While Deep Learning (DL) has shown remarkable success in detecting\nXSS attacks, it remains vulnerable to adversarial attacks due to the\ndiscontinuous nature of its input-output mapping. These adversarial attacks\nemploy mutation-based strategies for different components of XSS attack\nvectors, allowing adversarial agents to iteratively select mutations to evade\ndetection. Our work replicates a state-of-the-art XSS adversarial attack,\nhighlighting threats to validity in the reference work and extending it toward\na more effective evaluation strategy. Moreover, we introduce an XSS Oracle to\nmitigate these threats. The experimental results show that our approach\nachieves an escape rate above 96% when the threats to validity of the\nreplicated technique are addressed."
    },
    {
        "date": "2025-02",
        "title": "A Sample-Level Evaluation and Generative Framework for Model Inversion Attacks",
        "author": "Haoyang Li, Li Bai, Qingqing Ye, Haibo Hu, Yaxin Xiao, Huadi Zheng, and Jianliang Xu",
        "link": "http://arxiv.org/abs/2502.19070v1",
        "abstract": "Model Inversion (MI) attacks, which reconstruct the training dataset of\nneural networks, pose significant privacy concerns in machine learning. Recent\nMI attacks have managed to reconstruct realistic label-level private data, such\nas the general appearance of a target person from all training images labeled\non him. Beyond label-level privacy, in this paper we show sample-level privacy,\nthe private information of a single target sample, is also important but\nunder-explored in the MI literature due to the limitations of existing\nevaluation metrics. To address this gap, this study introduces a novel metric\ntailored for training-sample analysis, namely, the Diversity and Distance\nComposite Score (DDCS), which evaluates the reconstruction fidelity of each\ntraining sample by encompassing various MI attack attributes. This, in turn,\nenhances the precision of sample-level privacy assessments.\n  Leveraging DDCS as a new evaluative lens, we observe that many training\nsamples remain resilient against even the most advanced MI attack. As such, we\nfurther propose a transfer learning framework that augments the generative\ncapabilities of MI attackers through the integration of entropy loss and\nnatural gradient descent. Extensive experiments verify the effectiveness of our\nframework on improving state-of-the-art MI attacks over various metrics\nincluding DDCS, coverage and FID. Finally, we demonstrate that DDCS can also be\nuseful for MI defense, by identifying samples susceptible to MI attacks in an\nunsupervised manner."
    },
    {
        "date": "2025-02",
        "title": "Blending Optimal Control and Biologically Plausible Learning for Noise-Robust Physical Neural Networks",
        "author": "Satoshi Sunada, Tomoaki Niiyama, Kazutaka Kanno, Rin Nogami, Andr\u00e9 R\u00f6hm, Takato Awano, and Atsushi Uchida",
        "link": "http://arxiv.org/abs/2502.19053v1",
        "abstract": "The rapidly increasing computational demands for artificial intelligence (AI)\nhave spurred the exploration of computing principles beyond conventional\ndigital computers. Physical neural networks (PNNs) offer efficient neuromorphic\ninformation processing by harnessing the innate computational power of physical\nprocesses; however, training their weight parameters is computationally\nexpensive. We propose a training approach for substantially reducing this\ntraining cost. Our training approach merges an optimal control method for\ncontinuous-time dynamical systems with a biologically plausible training\nmethod--direct feedback alignment. In addition to the reduction of training\ntime, this approach achieves robust processing even under measurement errors\nand noise without requiring detailed system information. The effectiveness was\nnumerically and experimentally verified in an optoelectronic delay system. Our\napproach significantly extends the range of physical systems practically usable\nas PNNs."
    },
    {
        "date": "2025-02",
        "title": "A Dual-Purpose Framework for Backdoor Defense and Backdoor Amplification in Diffusion Models",
        "author": "Vu Tuan Truong Long, and Bao Le",
        "link": "http://arxiv.org/abs/2502.19047v1",
        "abstract": "Diffusion models have emerged as state-of-the-art generative frameworks,\nexcelling in producing high-quality multi-modal samples. However, recent\nstudies have revealed their vulnerability to backdoor attacks, where backdoored\nmodels generate specific, undesirable outputs called backdoor target (e.g.,\nharmful images) when a pre-defined trigger is embedded to their inputs. In this\npaper, we propose PureDiffusion, a dual-purpose framework that simultaneously\nserves two contrasting roles: backdoor defense and backdoor attack\namplification. For defense, we introduce two novel loss functions to invert\nbackdoor triggers embedded in diffusion models. The first leverages\ntrigger-induced distribution shifts across multiple timesteps of the diffusion\nprocess, while the second exploits the denoising consistency effect when a\nbackdoor is activated. Once an accurate trigger inversion is achieved, we\ndevelop a backdoor detection method that analyzes both the inverted trigger and\nthe generated backdoor targets to identify backdoor attacks. In terms of attack\namplification with the role of an attacker, we describe how our trigger\ninversion algorithm can be used to reinforce the original trigger embedded in\nthe backdoored diffusion model. This significantly boosts attack performance\nwhile reducing the required backdoor training time. Experimental results\ndemonstrate that PureDiffusion achieves near-perfect detection accuracy,\noutperforming existing defenses by a large margin, particularly against complex\ntrigger patterns. Additionally, in an attack scenario, our attack amplification\napproach elevates the attack success rate (ASR) of existing backdoor attacks to\nnearly 100\\% while reducing training time by up to 20x."
    },
    {
        "date": "2025-02",
        "title": "Beyond Surface-Level Patterns: An Essence-Driven Defense Framework Against Jailbreak Attacks in LLMs",
        "author": "Shiyu Xiang, Ansen Zhang, Yanfei Cao, Yang Fan, and Ronghao Chen",
        "link": "http://arxiv.org/abs/2502.19041v1",
        "abstract": "Although Aligned Large Language Models (LLMs) are trained to refuse harmful\nrequests, they remain vulnerable to jailbreak attacks. Unfortunately, existing\nmethods often focus on surface-level patterns, overlooking the deeper attack\nessences. As a result, defenses fail when attack prompts change, even though\nthe underlying \"attack essence\" remains the same. To address this issue, we\nintroduce EDDF, an \\textbf{E}ssence-\\textbf{D}riven \\textbf{D}efense\n\\textbf{F}ramework Against Jailbreak Attacks in LLMs. EDDF is a plug-and-play\ninput-filtering method and operates in two stages: 1) offline essence database\nconstruction, and 2) online adversarial query detection. The key idea behind\nEDDF is to extract the \"attack essence\" from a diverse set of known attack\ninstances and store it in an offline vector database. Experimental results\ndemonstrate that EDDF significantly outperforms existing methods by reducing\nthe Attack Success Rate by at least 20\\%, underscoring its superior robustness\nagainst jailbreak attacks."
    },
    {
        "date": "2025-02",
        "title": "Robust Over-the-Air Computation with Type-Based Multiple Access",
        "author": "Marc Martinez-Gost, Ana P\u00e9rez-Neira, and Miguel \u00c1ngel Lagunas",
        "link": "http://arxiv.org/abs/2502.19014v1",
        "abstract": "This paper utilizes the properties of type-based multiple access (TBMA) to\ninvestigate its effectiveness as a robust approach for over-the-air computation\n(AirComp) in the presence of Byzantine attacks, this is, adversarial strategies\nwhere malicious nodes intentionally distort their transmissions to corrupt the\naggregated result. Unlike classical direct aggregation (DA) AirComp, which\naggregates data in the amplitude of the signals and are highly vulnerable to\nattacks, TBMA distributes data over multiple radio resources, enabling the\nreceiver to construct a histogram representation of the transmitted data. This\nstructure allows the integration of classical robust estimators and supports\nthe computation of diverse functions beyond the arithmetic mean, which is not\nfeasible with DA. Through extensive simulations, we demonstrate that robust\nTBMA significantly outperforms DA, maintaining high accuracy even under\nadversarial conditions, and showcases its applicability in federated learning\n(FEEL) scenarios. Additionally, TBMA reduces channel state information (CSI)\nrequirements, lowers energy consumption, and enhances resiliency by leveraging\nthe diversity of the transmitted data. These results establish TBMA as a\nscalable and robust solution for AirComp, paving the way for secure and\nefficient aggregation in next-generation networks."
    },
    {
        "date": "2025-02",
        "title": "Evaluating Membership Inference Attacks in heterogeneous-data setups",
        "author": "Bram van Dartel, Marc Damie, and Florian Hahn",
        "link": "http://arxiv.org/abs/2502.18986v1",
        "abstract": "Among all privacy attacks against Machine Learning (ML), membership inference\nattacks (MIA) attracted the most attention. In these attacks, the attacker is\ngiven an ML model and a data point, and they must infer whether the data point\nwas used for training. The attacker also has an auxiliary dataset to tune their\ninference algorithm.\n  Attack papers commonly simulate setups in which the attacker's and the\ntarget's datasets are sampled from the same distribution. This setting is\nconvenient to perform experiments, but it rarely holds in practice. ML\nliterature commonly starts with similar simplifying assumptions (i.e., \"i.i.d.\"\ndatasets), and later generalizes the results to support heterogeneous data\ndistributions. Similarly, our work makes a first step in the generalization of\nthe MIA evaluation to heterogeneous data.\n  First, we design a metric to measure the heterogeneity between any pair of\ntabular data distributions. This metric provides a continuous scale to analyze\nthe phenomenon. Second, we compare two methodologies to simulate a data\nheterogeneity between the target and the attacker. These setups provide\nopposite performances: 90% attack accuracy vs. 50% (i.e., random guessing). Our\nresults show that the MIA accuracy depends on the experimental setup; and even\nif research on MIA considers heterogeneous data setups, we have no standardized\nbaseline of how to simulate it. The lack of such a baseline for MIA experiments\nposes a significant challenge to risk assessments in real-world machine\nlearning scenarios."
    },
    {
        "date": "2025-02",
        "title": "Invariance Pair-Guided Learning: Enhancing Robustness in Neural Networks",
        "author": "Martin Surner, Abdelmajid Khelil, and Ludwig Bothmann",
        "link": "http://arxiv.org/abs/2502.18975v1",
        "abstract": "Out-of-distribution generalization of machine learning models remains\nchallenging since the models are inherently bound to the training data\ndistribution. This especially manifests, when the learned models rely on\nspurious correlations. Most of the existing approaches apply data manipulation,\nrepresentation learning, or learning strategies to achieve generalizable\nmodels. Unfortunately, these approaches usually require multiple training\ndomains, group labels, specialized augmentation, or pre-processing to reach\ngeneralizable models. We propose a novel approach that addresses these\nlimitations by providing a technique to guide the neural network through the\ntraining phase. We first establish input pairs, representing the spurious\nattribute and describing the invariance, a characteristic that should not\naffect the outcome of the model. Based on these pairs, we form a corrective\ngradient complementing the traditional gradient descent approach. We further\nmake this correction mechanism adaptive based on a predefined invariance\ncondition. Experiments on ColoredMNIST, Waterbird-100, and CelebA datasets\ndemonstrate the effectiveness of our approach and the robustness to group\nshifts."
    },
    {
        "date": "2025-02",
        "title": "Switching multiplicative watermark design against covert attacks",
        "author": "Alexander J. Gallo, Sribalaji C. Anand, Andr\u00e9 M. H. Teixeira, and Riccardo M. G. Ferrari",
        "link": "http://arxiv.org/abs/2502.18948v1",
        "abstract": "Active techniques have been introduced to give better detectability\nperformance for cyber-attack diagnosis in cyber-physical systems (CPS). In this\npaper, switching multiplicative watermarking is considered, whereby we propose\nan optimal design strategy to define switching filter parameters. Optimality is\nevaluated exploiting the so-called output-to-output gain of the closed loop\nsystem, including some supposed attack dynamics. A worst-case scenario of a\nmatched covert attack is assumed, presuming that an attacker with full\nknowledge of the closed-loop system injects a stealthy attack of bounded\nenergy. Our algorithm, given watermark filter parameters at some time instant,\nprovides optimal next-step parameters. Analysis of the algorithm is given,\ndemonstrating its features, and demonstrating that through initialization of\ncertain parameters outside of the algorithm, the parameters of the\nmultiplicative watermarking can be randomized. Simulation shows how, by\nadopting our method for parameter design, the attacker's impact on performance\ndiminishes."
    },
    {
        "date": "2025-02",
        "title": "Towards Label-Only Membership Inference Attack against Pre-trained Large Language Models",
        "author": "Yu He, Boheng Li, Liu Liu, Zhongjie Ba, Wei Dong, Yiming Li, Zhan Qin, Kui Ren, and Chun Chen",
        "link": "http://arxiv.org/abs/2502.18943v1",
        "abstract": "Membership Inference Attacks (MIAs) aim to predict whether a data sample\nbelongs to the model's training set or not. Although prior research has\nextensively explored MIAs in Large Language Models (LLMs), they typically\nrequire accessing to complete output logits (\\ie, \\textit{logits-based\nattacks}), which are usually not available in practice. In this paper, we study\nthe vulnerability of pre-trained LLMs to MIAs in the \\textit{label-only\nsetting}, where the adversary can only access generated tokens (text). We first\nreveal that existing label-only MIAs have minor effects in attacking\npre-trained LLMs, although they are highly effective in inferring fine-tuning\ndatasets used for personalized LLMs. We find that their failure stems from two\nmain reasons, including better generalization and overly coarse perturbation.\nSpecifically, due to the extensive pre-training corpora and exposing each\nsample only a few times, LLMs exhibit minimal robustness differences between\nmembers and non-members. This makes token-level perturbations too coarse to\ncapture such differences.\n  To alleviate these problems, we propose \\textbf{PETAL}: a label-only\nmembership inference attack based on \\textbf{PE}r-\\textbf{T}oken\nsem\\textbf{A}ntic simi\\textbf{L}arity. Specifically, PETAL leverages\ntoken-level semantic similarity to approximate output probabilities and\nsubsequently calculate the perplexity. It finally exposes membership based on\nthe common assumption that members are `better' memorized and have smaller\nperplexity. We conduct extensive experiments on the WikiMIA benchmark and the\nmore challenging MIMIR benchmark. Empirically, our PETAL performs better than\nthe extensions of existing label-only attacks against personalized LLMs and\neven on par with other advanced logit-based attacks across all metrics on five\nprevalent open-source LLMs."
    },
    {
        "date": "2025-02",
        "title": "JailBench: A Comprehensive Chinese Security Assessment Benchmark for Large Language Models",
        "author": "Shuyi Liu, Simiao Cui, Haoran Bu, Yuming Shang, and Xi Zhang",
        "link": "http://arxiv.org/abs/2502.18935v1",
        "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities across\nvarious applications, highlighting the urgent need for comprehensive safety\nevaluations. In particular, the enhanced Chinese language proficiency of LLMs,\ncombined with the unique characteristics and complexity of Chinese expressions,\nhas driven the emergence of Chinese-specific benchmarks for safety assessment.\nHowever, these benchmarks generally fall short in effectively exposing LLM\nsafety vulnerabilities. To address the gap, we introduce JailBench, the first\ncomprehensive Chinese benchmark for evaluating deep-seated vulnerabilities in\nLLMs, featuring a refined hierarchical safety taxonomy tailored to the Chinese\ncontext. To improve generation efficiency, we employ a novel Automatic\nJailbreak Prompt Engineer (AJPE) framework for JailBench construction, which\nincorporates jailbreak techniques to enhance assessing effectiveness and\nleverages LLMs to automatically scale up the dataset through context-learning.\nThe proposed JailBench is extensively evaluated over 13 mainstream LLMs and\nachieves the highest attack success rate against ChatGPT compared to existing\nChinese benchmarks, underscoring its efficacy in identifying latent\nvulnerabilities in LLMs, as well as illustrating the substantial room for\nimprovement in the security and trustworthiness of LLMs within the Chinese\ncontext. Our benchmark is publicly available at\nhttps://github.com/STAIR-BUPT/JailBench."
    },
    {
        "date": "2025-02",
        "title": "Learning to Align Multi-Faceted Evaluation: A Unified and Robust Framework",
        "author": "Kaishuai Xu, Tiezheng Yu, Wenjun Hou, Yi Cheng, Liangyou Li, Xin Jiang, Lifeng Shang, Qun Liu, and Wenjie Li",
        "link": "http://arxiv.org/abs/2502.18874v1",
        "abstract": "Large Language Models (LLMs) are being used more and more extensively for\nautomated evaluation in various scenarios. Previous studies have attempted to\nfine-tune open-source LLMs to replicate the evaluation explanations and\njudgments of powerful proprietary models, such as GPT-4. However, these methods\nare largely limited to text-based analyses under predefined general criteria,\nresulting in reduced adaptability for unseen instructions and demonstrating\ninstability in evaluating adherence to quantitative and structural constraints.\nTo address these limitations, we propose a novel evaluation framework, ARJudge,\nthat adaptively formulates evaluation criteria and synthesizes both text-based\nand code-driven analyses to evaluate LLM responses. ARJudge consists of two\ncomponents: a fine-tuned Analyzer that generates multi-faceted evaluation\nanalyses and a tuning-free Refiner that combines and refines all analyses to\nmake the final judgment. We construct a Composite Analysis Corpus that\nintegrates tasks for evaluation criteria generation alongside text-based and\ncode-driven analysis generation to train the Analyzer. Our results demonstrate\nthat ARJudge outperforms existing fine-tuned evaluators in effectiveness and\nrobustness. Furthermore, it demonstrates the importance of multi-faceted\nevaluation and code-driven analyses in enhancing evaluation capabilities."
    },
    {
        "date": "2025-02",
        "title": "Adversarial Combinatorial Semi-bandits with Graph Feedback",
        "author": "Yuxiao Wen",
        "link": "http://arxiv.org/abs/2502.18826v2",
        "abstract": "In combinatorial semi-bandits, a learner repeatedly selects from a\ncombinatorial decision set of arms, receives the realized sum of rewards, and\nobserves the rewards of the individual selected arms as feedback. In this\npaper, we extend this framework to include \\emph{graph feedback}, where the\nlearner observes the rewards of all neighboring arms of the selected arms in a\nfeedback graph $G$. We establish that the optimal regret over a time horizon\n$T$ scales as $\\widetilde{\\Theta}(S\\sqrt{T}+\\sqrt{\\alpha ST})$, where $S$ is\nthe size of the combinatorial decisions and $\\alpha$ is the independence number\nof $G$. This result interpolates between the known regrets\n$\\widetilde\\Theta(S\\sqrt{T})$ under full information (i.e., $G$ is complete)\nand $\\widetilde\\Theta(\\sqrt{KST})$ under the semi-bandit feedback (i.e., $G$\nhas only self-loops), where $K$ is the total number of arms. A key technical\ningredient is to realize a convexified action using a random decision vector\nwith negative correlations."
    },
    {
        "date": "2025-02",
        "title": "Adversarial Universal Stickers: Universal Perturbation Attacks on Traffic Sign using Stickers",
        "author": "Anthony Etim, and Jakub Szefer",
        "link": "http://arxiv.org/abs/2502.18724v1",
        "abstract": "Adversarial attacks on deep learning models have proliferated in recent\nyears. In many cases, a different adversarial perturbation is required to be\nadded to each image to cause the deep learning model to misclassify it. This is\nineffective as each image has to be modified in a different way. Meanwhile,\nresearch on universal perturbations focuses on designing a single perturbation\nthat can be applied to all images in a data set, and cause a deep learning\nmodel to misclassify the images. This work advances the field of universal\nperturbations by exploring universal perturbations in the context of traffic\nsigns and autonomous vehicle systems. This work introduces a novel method for\ngenerating universal perturbations that visually look like simple black and\nwhite stickers, and using them to cause incorrect street sign predictions.\nUnlike traditional adversarial perturbations, the adversarial universal\nstickers are designed to be applicable to any street sign: same sticker, or\nstickers, can be applied in same location to any street sign and cause it to be\nmisclassified. Further, to enable safe experimentation with adversarial images\nand street signs, this work presents a virtual setting that leverages Street\nView images of street signs, rather than the need to physically modify street\nsigns, to test the attacks. The experiments in the virtual setting demonstrate\nthat these stickers can consistently mislead deep learning models used commonly\nin street sign recognition, and achieve high attack success rates on dataset of\nUS traffic signs. The findings highlight the practical security risks posed by\nsimple stickers applied to traffic signs, and the ease with which adversaries\ncan generate adversarial universal stickers that can be applied to many street\nsigns."
    },
    {
        "date": "2025-02",
        "title": "Target Defense with Multiple Defenders and an Agile Attacker via Residual Policy Learning",
        "author": "Jiyue Tao, Tongsheng Shen, Dexin Zhao, and Feitian Zhang",
        "link": "http://arxiv.org/abs/2502.18549v1",
        "abstract": "The target defense problem involves intercepting an attacker before it\nreaches a designated target region using one or more defenders. This letter\nfocuses on a particularly challenging scenario in which the attacker is more\nagile than the defenders, significantly increasing the difficulty of effective\ninterception. To address this challenge, we propose a novel residual policy\nframework that integrates deep reinforcement learning (DRL) with the\nforce-based Boids model. In this framework, the Boids model serves as a\nbaseline policy, while DRL learns a residual policy to refine and optimize the\ndefenders' actions. Simulation experiments demonstrate that the proposed method\nconsistently outperforms traditional interception policies, whether learned via\nvanilla DRL or fine-tuned from force-based methods. Moreover, the learned\npolicy exhibits strong scalability and adaptability, effectively handling\nscenarios with varying numbers of defenders and attackers with different\nagility levels."
    },
    {
        "date": "2025-02",
        "title": "Learning atomic forces from uncertainty-calibrated adversarial attacks",
        "author": "Henrique Musseli Cezar, Tilmann Bodenstein, Henrik Andersen Sveinsson, Morten Ledum, Simen Reine, and Sigbj\u00f8rn L\u00f8land Bore",
        "link": "http://arxiv.org/abs/2502.18314v2",
        "abstract": "Adversarial approaches, which intentionally challenge machine learning models\nby generating difficult examples, are increasingly being adopted to improve\nmachine learning interatomic potentials (MLIPs). While already providing great\npractical value, little is known about the actual prediction errors of MLIPs on\nadversarial structures and whether these errors can be controlled. We propose\nthe Calibrated Adversarial Geometry Optimization (CAGO) algorithm to discover\nadversarial structures with user-assigned errors. Through uncertainty\ncalibration, the estimated uncertainty of MLIPs is unified with real errors. By\nperforming geometry optimization for calibrated uncertainty, we reach\nadversarial structures with the user-assigned target MLIP prediction error.\nIntegrating with active learning pipelines, we benchmark CAGO, demonstrating\nstable MLIPs that systematically converge structural, dynamical, and\nthermodynamical properties for liquid water and water adsorption in a\nmetal-organic framework within only hundreds of training structures, where\npreviously many thousands were typically required."
    },
    {
        "date": "2025-02",
        "title": "Experimental Analysis of Efficiency of the Messaging Layer Security for Multiple Delivery Services",
        "author": "David Soler, Carlos Dafonte, Manuel Fern\u00e1ndez-Veiga, Ana Fern\u00e1ndez Vilas, and Francisco J. N\u00f3voa",
        "link": "http://arxiv.org/abs/2502.18303v1",
        "abstract": "Messaging Layer security (MLS) and its underlying Continuous Group Key\nAgreement (CGKA) protocol allows a group of users to share a cryptographic\nsecret in a dynamic manner, such that the secret is modified in member\ninsertions and deletions. One of the most relevant contributions of MLS is its\nefficiency, as its communication cost scales logarithmically with the number of\nmembers. However, this claim has only been analysed in theoretical models and\nthus it is unclear how efficient MLS is in real-world scenarios. Furthermore,\npractical decisions such as the chosen Delivery Service and paradigm can also\ninfluence the efficiency and evolution of an MLS group. In this work we analyse\nMLS from an empirical viewpoint: we provide real-world measurements for metrics\nsuch as commit generation and processing times and message sizes under\ndifferent conditions. In order to obtain these results we have developed a\nhighly configurable environment for empirical evaluations of MLS through the\nsimulation of MLS clients. Among other findings, our results show that\ncomputation costs scale linearly in practical scenarios even in the best-case\nscenario."
    },
    {
        "date": "2025-02",
        "title": "Stealthy Backdoor Attack in Self-Supervised Learning Vision Encoders for Large Vision Language Models",
        "author": "Zhaoyi Liu, and Huan Zhang",
        "link": "http://arxiv.org/abs/2502.18290v2",
        "abstract": "Self-supervised learning (SSL) vision encoders learn high-quality image\nrepresentations and thus have become a vital part of developing vision modality\nof large vision language models (LVLMs). Due to the high cost of training such\nencoders, pre-trained encoders are widely shared and deployed into many LVLMs,\nwhich are security-critical or bear societal significance. Under this practical\nscenario, we reveal a new backdoor threat that significant visual\nhallucinations can be induced into these LVLMs by merely compromising vision\nencoders. Because of the sharing and reuse of these encoders, many downstream\nLVLMs may inherit backdoor behaviors from encoders, leading to widespread\nbackdoors. In this work, we propose BadVision, the first method to exploit this\nvulnerability in SSL vision encoders for LVLMs with novel trigger optimization\nand backdoor learning techniques. We evaluate BadVision on two types of SSL\nencoders and LVLMs across eight benchmarks. We show that BadVision effectively\ndrives the LVLMs to attacker-chosen hallucination with over 99% attack success\nrate, causing a 77.6% relative visual understanding error while maintaining the\nstealthiness. SoTA backdoor detection methods cannot detect our attack\neffectively."
    },
    {
        "date": "2025-02",
        "title": "CLIPure: Purification in Latent Space via CLIP for Adversarially Robust Zero-Shot Classification",
        "author": "Mingkun Zhang, Keping Bi, Wei Chen, Jiafeng Guo, and Xueqi Cheng",
        "link": "http://arxiv.org/abs/2502.18176v1",
        "abstract": "In this paper, we aim to build an adversarially robust zero-shot image\nclassifier. We ground our work on CLIP, a vision-language pre-trained encoder\nmodel that can perform zero-shot classification by matching an image with text\nprompts ``a photo of a <class-name>.''. Purification is the path we choose\nsince it does not require adversarial training on specific attack types and\nthus can cope with any foreseen attacks. We then formulate purification risk as\nthe KL divergence between the joint distributions of the purification process\nof denoising the adversarial samples and the attack process of adding\nperturbations to benign samples, through bidirectional Stochastic Differential\nEquations (SDEs). The final derived results inspire us to explore purification\nin the multi-modal latent space of CLIP. We propose two variants for our\nCLIPure approach: CLIPure-Diff which models the likelihood of images' latent\nvectors with the DiffusionPrior module in DaLLE-2 (modeling the generation\nprocess of CLIP's latent vectors), and CLIPure-Cos which models the likelihood\nwith the cosine similarity between the embeddings of an image and ``a photo of\na.''. As far as we know, CLIPure is the first purification method in\nmulti-modal latent space and CLIPure-Cos is the first purification method that\nis not based on generative models, which substantially improves defense\nefficiency. We conducted extensive experiments on CIFAR-10, ImageNet, and 13\ndatasets that previous CLIP-based defense methods used for evaluating zero-shot\nclassification robustness. Results show that CLIPure boosts the SOTA robustness\nby a large margin, e.g., from 71.7% to 91.1% on CIFAR10, from 59.6% to 72.6% on\nImageNet, and 108% relative improvements of average robustness on the 13\ndatasets over previous SOTA. The code is available at\nhttps://github.com/TMLResearchGroup-CAS/CLIPure."
    },
    {
        "date": "2025-02",
        "title": "The Built-In Robustness of Decentralized Federated Averaging to Bad Data",
        "author": "Samuele Sabella, Chiara Boldrini, Lorenzo Valerio, Andrea Passarella, and Marco Conti",
        "link": "http://arxiv.org/abs/2502.18097v1",
        "abstract": "Decentralized federated learning (DFL) enables devices to collaboratively\ntrain models over complex network topologies without relying on a central\ncontroller. In this setting, local data remains private, but its quality and\nquantity can vary significantly across nodes. The extent to which a fully\ndecentralized system is vulnerable to poor-quality or corrupted data remains\nunclear, but several factors could contribute to potential risks. Without a\ncentral authority, there can be no unified mechanism to detect or correct\nerrors, and each node operates with a localized view of the data distribution,\nmaking it difficult for the node to assess whether its perspective aligns with\nthe true distribution. Moreover, models trained on low-quality data can\npropagate through the network, amplifying errors. To explore the impact of\nlow-quality data on DFL, we simulate two scenarios with degraded data quality\n-- one where the corrupted data is evenly distributed in a subset of nodes and\none where it is concentrated on a single node -- using a decentralized\nimplementation of FedAvg. Our results reveal that averaging-based decentralized\nlearning is remarkably robust to localized bad data, even when the corrupted\ndata resides in the most influential nodes of the network. Counterintuitively,\nthis robustness is further enhanced when the corrupted data is concentrated on\na single node, regardless of its centrality in the communication network\ntopology. This phenomenon is explained by the averaging process, which ensures\nthat no single node -- however central -- can disproportionately influence the\noverall learning process."
    },
    {
        "date": "2025-02",
        "title": "Model-Free Adversarial Purification via Coarse-To-Fine Tensor Network Representation",
        "author": "Guang Lin, Duc Thien Nguyen, Zerui Tao, Konstantinos Slavakis, Toshihisa Tanaka, and Qibin Zhao",
        "link": "http://arxiv.org/abs/2502.17972v1",
        "abstract": "Deep neural networks are known to be vulnerable to well-designed adversarial\nattacks. Although numerous defense strategies have been proposed, many are\ntailored to the specific attacks or tasks and often fail to generalize across\ndiverse scenarios. In this paper, we propose Tensor Network Purification (TNP),\na novel model-free adversarial purification method by a specially designed\ntensor network decomposition algorithm. TNP depends neither on the pre-trained\ngenerative model nor the specific dataset, resulting in strong robustness\nacross diverse adversarial scenarios. To this end, the key challenge lies in\nrelaxing Gaussian-noise assumptions of classical decompositions and\naccommodating the unknown distribution of adversarial perturbations. Unlike the\nlow-rank representation of classical decompositions, TNP aims to reconstruct\nthe unobserved clean examples from an adversarial example. Specifically, TNP\nleverages progressive downsampling and introduces a novel adversarial\noptimization objective to address the challenge of minimizing reconstruction\nerror but without inadvertently restoring adversarial perturbations. Extensive\nexperiments conducted on CIFAR-10, CIFAR-100, and ImageNet demonstrate that our\nmethod generalizes effectively across various norm threats, attack types, and\ntasks, providing a versatile and promising adversarial purification technique."
    },
    {
        "date": "2025-02",
        "title": "Robust Polyp Detection and Diagnosis through Compositional Prompt-Guided Diffusion Models",
        "author": "Jia Yu, Yan Zhu, Peiyao Fu, Tianyi Chen, Junbo Huang, Quanlin Li, Pinghong Zhou, Zhihua Wang, Fei Wu, Shuo Wang, and Xian Yang",
        "link": "http://arxiv.org/abs/2502.17951v1",
        "abstract": "Colorectal cancer (CRC) is a significant global health concern, and early\ndetection through screening plays a critical role in reducing mortality. While\ndeep learning models have shown promise in improving polyp detection,\nclassification, and segmentation, their generalization across diverse clinical\nenvironments, particularly with out-of-distribution (OOD) data, remains a\nchallenge. Multi-center datasets like PolypGen have been developed to address\nthese issues, but their collection is costly and time-consuming. Traditional\ndata augmentation techniques provide limited variability, failing to capture\nthe complexity of medical images. Diffusion models have emerged as a promising\nsolution for generating synthetic polyp images, but the image generation\nprocess in current models mainly relies on segmentation masks as the condition,\nlimiting their ability to capture the full clinical context. To overcome these\nlimitations, we propose a Progressive Spectrum Diffusion Model (PSDM) that\nintegrates diverse clinical annotations-such as segmentation masks, bounding\nboxes, and colonoscopy reports-by transforming them into compositional prompts.\nThese prompts are organized into coarse and fine components, allowing the model\nto capture both broad spatial structures and fine details, generating\nclinically accurate synthetic images. By augmenting training data with\nPSDM-generated samples, our model significantly improves polyp detection,\nclassification, and segmentation. For instance, on the PolypGen dataset, PSDM\nincreases the F1 score by 2.12% and the mean average precision by 3.09%,\ndemonstrating superior performance in OOD scenarios and enhanced\ngeneralization."
    },
    {
        "date": "2025-02",
        "title": "VVRec: Reconstruction Attacks on DL-based Volumetric Video Upstreaming via Latent Diffusion Model with Gamma Distribution",
        "author": "Rui Lu, Bihai Zhang, and Dan Wang",
        "link": "http://arxiv.org/abs/2502.17880v1",
        "abstract": "With the popularity of 3D volumetric video applications, such as Autonomous\nDriving, Virtual Reality, and Mixed Reality, current developers have turned to\ndeep learning for compressing volumetric video frames, i.e., point clouds for\nvideo upstreaming. The latest deep learning-based solutions offer higher\nefficiency, lower distortion, and better hardware support compared to\ntraditional ones like MPEG and JPEG. However, privacy threats arise, especially\nreconstruction attacks targeting to recover the original input point cloud from\nthe intermediate results. In this paper, we design VVRec, to the best of our\nknowledge, which is the first targeting DL-based Volumetric Video\nReconstruction attack scheme. VVRec demonstrates the ability to reconstruct\nhigh-quality point clouds from intercepted transmission intermediate results\nusing four well-trained neural network modules we design. Leveraging the latest\nlatent diffusion models with Gamma distribution and a refinement algorithm,\nVVRec excels in reconstruction quality, color recovery, and surpasses existing\ndefenses. We evaluate VVRec using three volumetric video datasets. The results\ndemonstrate that VVRec achieves 64.70dB reconstruction accuracy, with an\nimpressive 46.39% reduction of distortion over baselines."
    },
    {
        "date": "2025-02",
        "title": "MM-PoisonRAG: Disrupting Multimodal RAG with Local and Global Poisoning Attacks",
        "author": "Hyeonjeong Ha, Qiusi Zhan, Jeonghwan Kim, Dimitrios Bralios, Saikrishna Sanniboina, Nanyun Peng, Kai-wei Chang, Daniel Kang, and Heng Ji",
        "link": "http://arxiv.org/abs/2502.17832v1",
        "abstract": "Multimodal large language models (MLLMs) equipped with Retrieval Augmented\nGeneration (RAG) leverage both their rich parametric knowledge and the dynamic,\nexternal knowledge to excel in tasks such as Question Answering. While RAG\nenhances MLLMs by grounding responses in query-relevant external knowledge,\nthis reliance poses a critical yet underexplored safety risk: knowledge\npoisoning attacks, where misinformation or irrelevant knowledge is\nintentionally injected into external knowledge bases to manipulate model\noutputs to be incorrect and even harmful. To expose such vulnerabilities in\nmultimodal RAG, we propose MM-PoisonRAG, a novel knowledge poisoning attack\nframework with two attack strategies: Localized Poisoning Attack (LPA), which\ninjects query-specific misinformation in both text and images for targeted\nmanipulation, and Globalized Poisoning Attack (GPA) to provide false guidance\nduring MLLM generation to elicit nonsensical responses across all queries. We\nevaluate our attacks across multiple tasks, models, and access settings,\ndemonstrating that LPA successfully manipulates the MLLM to generate\nattacker-controlled answers, with a success rate of up to 56% on MultiModalQA.\nMoreover, GPA completely disrupts model generation to 0% accuracy with just a\nsingle irrelevant knowledge injection. Our results highlight the urgent need\nfor robust defenses against knowledge poisoning to safeguard multimodal RAG\nframeworks."
    },
    {
        "date": "2025-02",
        "title": "Research on Enhancing Cloud Computing Network Security using Artificial Intelligence Algorithms",
        "author": "Yuqing Wang, and Xiao Yang",
        "link": "http://arxiv.org/abs/2502.17801v1",
        "abstract": "Cloud computing environments are increasingly vulnerable to security threats\nsuch as distributed denial-of-service (DDoS) attacks and SQL injection.\nTraditional security mechanisms, based on rule matching and feature\nrecognition, struggle to adapt to evolving attack strategies. This paper\nproposes an adaptive security protection framework leveraging deep learning to\nconstruct a multi-layered defense architecture. The proposed system is\nevaluated in a real-world business environment, achieving a detection accuracy\nof 97.3%, an average response time of 18 ms, and an availability rate of\n99.999%. Experimental results demonstrate that the proposed method\nsignificantly enhances detection accuracy, response efficiency, and resource\nutilization, offering a novel and effective approach to cloud computing\nsecurity."
    },
    {
        "date": "2025-02",
        "title": "Enhancing Hepatopathy Clinical Trial Efficiency: A Secure, Large Language Model-Powered Pre-Screening Pipeline",
        "author": "Xiongbin Gui, Hanlin Lv, Xiao Wang, Longting Lv, Yi Xiao, and Lei Wang",
        "link": "http://arxiv.org/abs/2502.18531v1",
        "abstract": "Background: Recruitment for cohorts involving complex liver diseases, such as\nhepatocellular carcinoma and liver cirrhosis, often requires interpreting\nsemantically complex criteria. Traditional manual screening methods are\ntime-consuming and prone to errors. While AI-powered pre-screening offers\npotential solutions, challenges remain regarding accuracy, efficiency, and data\nprivacy. Methods: We developed a novel patient pre-screening pipeline that\nleverages clinical expertise to guide the precise, safe, and efficient\napplication of large language models. The pipeline breaks down complex criteria\ninto a series of composite questions and then employs two strategies to perform\nsemantic question-answering through electronic health records - (1) Pathway A,\nAnthropomorphized Experts' Chain of Thought strategy, and (2) Pathway B, Preset\nStances within an Agent Collaboration strategy, particularly in managing\ncomplex clinical reasoning scenarios. The pipeline is evaluated on three key\nmetrics-precision, time consumption, and counterfactual inference - at both the\nquestion and criterion levels. Results: Our pipeline achieved high precision\n(0.921, in criteria level) and efficiency (0.44s per task). Pathway B excelled\nin complex reasoning, while Pathway A was effective in precise data extraction\nwith faster processing times. Both pathways achieved comparable precision. The\npipeline showed promising results in hepatocellular carcinoma (0.878) and\ncirrhosis trials (0.843). Conclusions: This data-secure and time-efficient\npipeline shows high precision in hepatopathy trials, providing promising\nsolutions for streamlining clinical trial workflows. Its efficiency and\nadaptability make it suitable for improving patient recruitment. And its\ncapability to function in resource-constrained environments further enhances\nits utility in clinical settings."
    },
    {
        "date": "2025-02",
        "title": "Design and implementation of a distributed security threat detection system integrating federated learning and multimodal LLM",
        "author": "Yuqing Wang, and Xiao Yang",
        "link": "http://arxiv.org/abs/2502.17763v1",
        "abstract": "Traditional security protection methods struggle to address sophisticated\nattack vectors in large-scale distributed systems, particularly when balancing\ndetection accuracy with data privacy concerns. This paper presents a novel\ndistributed security threat detection system that integrates federated learning\nwith multimodal large language models (LLMs). Our system leverages federated\nlearning to ensure data privacy while employing multimodal LLMs to process\nheterogeneous data sources including network traffic, system logs, images, and\nsensor data. Experimental evaluation on a 10TB distributed dataset demonstrates\nthat our approach achieves 96.4% detection accuracy, outperforming traditional\nbaseline models by 4.1 percentage points. The system reduces both false\npositive and false negative rates by 1.8 and 2.4 percentage points\nrespectively. Performance analysis shows that our system maintains efficient\nprocessing capabilities in distributed environments, requiring 180 seconds for\nmodel training and 3.8 seconds for threat detection across the distributed\nnetwork. These results demonstrate significant improvements in detection\naccuracy and computational efficiency while preserving data privacy, suggesting\nstrong potential for real-world deployment in large-scale security systems."
    },
    {
        "date": "2025-02",
        "title": "Robust and Efficient Deep Hedging via Linearized Objective Neural Network",
        "author": "Lei Zhao, and Lin Cai",
        "link": "http://arxiv.org/abs/2502.17757v1",
        "abstract": "Deep hedging represents a cutting-edge approach to risk management for\nfinancial derivatives by leveraging the power of deep learning. However,\nexisting methods often face challenges related to computational inefficiency,\nsensitivity to noisy data, and optimization complexity, limiting their\npractical applicability in dynamic and volatile markets. To address these\nlimitations, we propose Deep Hedging with Linearized-objective Neural Network\n(DHLNN), a robust and generalizable framework that enhances the training\nprocedure of deep learning models. By integrating a periodic fixed-gradient\noptimization method with linearized training dynamics, DHLNN stabilizes the\ntraining process, accelerates convergence, and improves robustness to noisy\nfinancial data. The framework incorporates trajectory-wide optimization and\nBlack-Scholes Delta anchoring, ensuring alignment with established financial\ntheory while maintaining flexibility to adapt to real-world market conditions.\nExtensive experiments on synthetic and real market data validate the\neffectiveness of DHLNN, demonstrating its ability to achieve faster\nconvergence, improved stability, and superior hedging performance across\ndiverse market scenarios."
    },
    {
        "date": "2025-02",
        "title": "The Cyber Immune System: Harnessing Adversarial Forces for Security Resilience",
        "author": "Krti Tallam",
        "link": "http://arxiv.org/abs/2502.17698v1",
        "abstract": "Both parasites in biological systems and adversarial forces in cybersecurity\nare often perceived as threats: disruptive elements that must be eliminated.\nHowever, these entities play a critical role in revealing systemic weaknesses,\ndriving adaptation, and ultimately strengthening resilience. This paper draws\nfrom environmental epidemiology and cybersecurity to reframe parasites and\ncyber exploiters as essential stress-testers of complex systems, exposing\nhidden vulnerabilities and pushing defensive innovations forward. By examining\nhow biological and digital systems evolve in response to persistent threats, we\nhighlight the necessity of adversarial engagement in fortifying security\nframeworks. The recent breach of the DOGE website serves as a timely case\nstudy, illustrating how adversarial forces, whether biological or digital,\ncompel systems to reassess and reinforce their defenses."
    },
    {
        "date": "2025-02",
        "title": "Robust Federated Learning with Global Sensitivity Estimation for Financial Risk Management",
        "author": "Lei Zhao, Lin Cai, and Wu-Sheng Lu",
        "link": "http://arxiv.org/abs/2502.17694v1",
        "abstract": "In decentralized financial systems, robust and efficient Federated Learning\n(FL) is promising to handle diverse client environments and ensure resilience\nto systemic risks. We propose Federated Risk-Aware Learning with Central\nSensitivity Estimation (FRAL-CSE), an innovative FL framework designed to\nenhance scalability, stability, and robustness in collaborative financial\ndecision-making. The framework's core innovation lies in a central acceleration\nmechanism, guided by a quadratic sensitivity-based approximation of global\nmodel dynamics. By leveraging local sensitivity information derived from robust\nrisk measurements, FRAL-CSE performs a curvature-informed global update that\nefficiently incorporates second-order information without requiring repeated\nlocal re-evaluations, thereby enhancing training efficiency and improving\noptimization stability. Additionally, distortion risk measures are embedded\ninto the training objectives to capture tail risks and ensure robustness\nagainst extreme scenarios. Extensive experiments validate the effectiveness of\nFRAL-CSE in accelerating convergence and improving resilience across\nheterogeneous datasets compared to state-of-the-art baselines."
    },
    {
        "date": "2025-02",
        "title": "THOR: A Non-Speculative Value Dependent Timing Side Channel Attack Exploiting Intel AMX",
        "author": "Farshad Dizani, Azam Ghanbari, Joshua Kalyanapu, Darsh Asher, and Samira Mirbagher Ajorpaz",
        "link": "http://arxiv.org/abs/2502.17658v1",
        "abstract": "The rise of on-chip accelerators signifies a major shift in computing, driven\nby the growing demands of artificial intelligence (AI) and specialized\napplications. These accelerators have gained popularity due to their ability to\nsubstantially boost performance, cut energy usage, lower total cost of\nownership (TCO), and promote sustainability. Intel's Advanced Matrix Extensions\n(AMX) is one such on-chip accelerator, specifically designed for handling tasks\ninvolving large matrix multiplications commonly used in machine learning (ML)\nmodels, image processing, and other computational-heavy operations. In this\npaper, we introduce a novel value-dependent timing side-channel vulnerability\nin Intel AMX. By exploiting this weakness, we demonstrate a software-based,\nvalue-dependent timing side-channel attack capable of inferring the sparsity of\nneural network weights without requiring any knowledge of the confidence score,\nprivileged access or physical proximity. Our attack method can fully recover\nthe sparsity of weights assigned to 64 input elements within 50 minutes, which\nis 631% faster than the maximum leakage rate achieved in the Hertzbleed attack."
    },
    {
        "date": "2025-02",
        "title": "Formally-verified Security against Forgery of Remote Attestation using SSProve",
        "author": "Sara Zain, Jannik M\u00e4hn, Stefan K\u00f6psell, and Sebastian Ertel",
        "link": "http://arxiv.org/abs/2502.17653v1",
        "abstract": "Remote attestation (RA) is the foundation for trusted execution environments\nin the cloud and trusted device driver onboarding in operating systems.\nHowever, RA misses a rigorous mechanized definition of its security properties\nin one of the strongest models, i.e., the semantic model. Such a mechanization\nrequires the concept of State-Separating Proofs (SSP). However, SSP was only\nrecently implemented as a foundational framework in the Rocq Prover. Based on\nthis framework, this paper presents the first mechanized formalization of the\nfundamental security properties of RA. Our Rocq Prover development first\ndefines digital signatures and formally verifies security against forgery in\nthe strong existential attack model. Based on these results, we define RA and\nreduce the security of RA to the security of digital signatures. Our\ndevelopment provides evidence that the RA protocol is secure against forgery.\nAdditionally, we extend our reasoning to the primitives of RA and reduce their\nsecurity to the security of the primitives of the digital signatures. Finally,\nwe found that proving the security of the primitives for digital signatures was\nnot feasible. This observation contrasts textbook formalizations and sparks a\ndiscussion on reasoning about the security of libraries in SSP-based\nframeworks."
    },
    {
        "date": "2025-02",
        "title": "Towards Robust Legal Reasoning: Harnessing Logical LLMs in Law",
        "author": "Manuj Kant, Sareh Nabi, Manav Kant, Roland Scharrer, Megan Ma, and Marzieh Nabi",
        "link": "http://arxiv.org/abs/2502.17638v1",
        "abstract": "Legal services rely heavily on text processing. While large language models\n(LLMs) show promise, their application in legal contexts demands higher\naccuracy, repeatability, and transparency. Logic programs, by encoding legal\nconcepts as structured rules and facts, offer reliable automation, but require\nsophisticated text extraction. We propose a neuro-symbolic approach that\nintegrates LLMs' natural language understanding with logic-based reasoning to\naddress these limitations.\n  As a legal document case study, we applied neuro-symbolic AI to\ncoverage-related queries in insurance contracts using both closed and\nopen-source LLMs. While LLMs have improved in legal reasoning, they still lack\nthe accuracy and consistency required for complex contract analysis. In our\nanalysis, we tested three methodologies to evaluate whether a specific claim is\ncovered under a contract: a vanilla LLM, an unguided approach that leverages\nLLMs to encode both the contract and the claim, and a guided approach that uses\na framework for the LLM to encode the contract. We demonstrated the promising\ncapabilities of LLM + Logic in the guided approach."
    },
    {
        "date": "2025-02",
        "title": "A stochastic smoothing framework for nonconvex-nonconcave min-sum-max problems with applications to Wasserstein distributionally robust optimization",
        "author": "Wei Liu, Muhammad Khan, Gabriel Mancino-Ball, and Yangyang Xu",
        "link": "http://arxiv.org/abs/2502.17602v1",
        "abstract": "Applications such as adversarially robust training and Wasserstein\nDistributionally Robust Optimization (WDRO) can be naturally formulated as\nmin-sum-max optimization problems. While this formulation can be rewritten as\nan equivalent min-max problem, the summation of max terms introduces\ncomputational challenges, including increased complexity and memory demands,\nwhich must be addressed. These challenges are particularly evident in WDRO,\nwhere existing tractable algorithms often rely on restrictive assumptions on\nthe objective function, limiting their applicability to state-of-the-art\nmachine learning problems such as the training of deep neural networks. This\nstudy introduces a novel stochastic smoothing framework based on the\n\\mbox{log-sum-exp} function, efficiently approximating the max operator in\nmin-sum-max problems. By leveraging the Clarke regularity of the max operator,\nwe develop an iterative smoothing algorithm that addresses these computational\ndifficulties and guarantees almost surely convergence to a Clarke/directional\nstationary point. We further prove that the proposed algorithm finds an\n$\\epsilon$-scaled Clarke stationary point of the original problem, with a\nworst-case iteration complexity of $\\widetilde{O}(\\epsilon^{-3})$. Our\nnumerical experiments demonstrate that our approach outperforms or is\ncompetitive with state-of-the-art methods in solving the newsvendor problem,\ndeep learning regression, and adversarially robust deep learning. The results\nhighlight that our method yields more accurate and robust solutions in these\nchallenging problem settings."
    },
    {
        "date": "2025-02",
        "title": "Robust Confinement State Classification with Uncertainty Quantification through Ensembled Data-Driven Methods",
        "author": "Yoeri Poels, Cristina Venturini, Alessandro Pau, Olivier Sauter, Vlado Menkovski, the TCV team, and the WPTE team",
        "link": "http://arxiv.org/abs/2502.17397v1",
        "abstract": "Maximizing fusion performance in tokamaks relies on high energy confinement,\noften achieved through distinct operating regimes. The automated labeling of\nthese confinement states is crucial to enable large-scale analyses or for\nreal-time control applications. While this task becomes difficult to automate\nnear state transitions or in marginal scenarios, much success has been achieved\nwith data-driven models. However, these methods generally provide predictions\nas point estimates, and cannot adequately deal with missing and/or broken input\nsignals. To enable wide-range applicability, we develop methods for confinement\nstate classification with uncertainty quantification and model robustness. We\nfocus on off-line analysis for TCV discharges, distinguishing L-mode, H-mode,\nand an in-between dithering phase (D). We propose ensembling data-driven\nmethods on two axes: model formulations and feature sets. The former considers\na dynamic formulation based on a recurrent Fourier Neural Operator-architecture\nand a static formulation based on gradient-boosted decision trees. These models\nare trained using multiple feature groupings categorized by diagnostic system\nor physical quantity. A dataset of 302 TCV discharges is fully labeled, and\nwill be publicly released. We evaluate our method quantitatively using Cohen's\nkappa coefficient for predictive performance and the Expected Calibration Error\nfor the uncertainty calibration. Furthermore, we discuss performance using a\nvariety of common and alternative scenarios, the performance of individual\ncomponents, out-of-distribution performance, cases of broken or missing\nsignals, and evaluate conditionally-averaged behavior around different state\ntransitions. Overall, the proposed method can distinguish L, D and H-mode with\nhigh performance, can cope with missing or broken signals, and provides\nmeaningful uncertainty estimates."
    },
    {
        "date": "2025-02",
        "title": "Emoti-Attack: Zero-Perturbation Adversarial Attacks on NLP Systems via Emoji Sequences",
        "author": "Yangshijie Zhang",
        "link": "http://arxiv.org/abs/2502.17392v1",
        "abstract": "Deep neural networks (DNNs) have achieved remarkable success in the field of\nnatural language processing (NLP), leading to widely recognized applications\nsuch as ChatGPT. However, the vulnerability of these models to adversarial\nattacks remains a significant concern. Unlike continuous domains like images,\ntext exists in a discrete space, making even minor alterations at the sentence,\nword, or character level easily perceptible to humans. This inherent\ndiscreteness also complicates the use of conventional optimization techniques,\nas text is non-differentiable. Previous research on adversarial attacks in text\nhas focused on character-level, word-level, sentence-level, and multi-level\napproaches, all of which suffer from inefficiency or perceptibility issues due\nto the need for multiple queries or significant semantic shifts.\n  In this work, we introduce a novel adversarial attack method, Emoji-Attack,\nwhich leverages the manipulation of emojis to create subtle, yet effective,\nperturbations. Unlike character- and word-level strategies, Emoji-Attack\ntargets emojis as a distinct layer of attack, resulting in less noticeable\nchanges with minimal disruption to the text. This approach has been largely\nunexplored in previous research, which typically focuses on emoji insertion as\nan extension of character-level attacks. Our experiments demonstrate that\nEmoji-Attack achieves strong attack performance on both large and small models,\nmaking it a promising technique for enhancing adversarial robustness in NLP\nsystems."
    },
    {
        "date": "2025-02",
        "title": "Unveiling ECC Vulnerabilities: LSTM Networks for Operation Recognition in Side-Channel Attacks",
        "author": "Alberto Battistello, Guido Bertoni, Michele Corrias, Lorenzo Nava, Davide Rusconi, Matteo Zoia, Fabio Pierazzi, and Andrea Lanzi",
        "link": "http://arxiv.org/abs/2502.17330v1",
        "abstract": "We propose a novel approach for performing side-channel attacks on elliptic\ncurve cryptography. Unlike previous approaches and inspired by the ``activity\ndetection'' literature, we adopt a long-short-term memory (LSTM) neural network\nto analyze a power trace and identify patterns of operation in the scalar\nmultiplication algorithm performed during an ECDSA signature, that allows us to\nrecover bits of the ephemeral key, and thus retrieve the signer's private key.\nOur approach is based on the fact that modular reductions are conditionally\nperformed by micro-ecc and depend on key bits.\n  We evaluated the feasibility and reproducibility of our attack through\nexperiments in both simulated and real implementations. We demonstrate the\neffectiveness of our attack by implementing it on a real target device, an\nSTM32F415 with the micro-ecc library, and successfully compromise it.\nFurthermore, we show that current countermeasures, specifically the coordinate\nrandomization technique, are not sufficient to protect against side channels.\nFinally, we suggest other approaches that may be implemented to thwart our\nattack."
    },
    {
        "date": "2025-02",
        "title": "Robust Federated Learning in Unreliable Wireless Networks: A Client Selection Approach",
        "author": "Yanmeng Wang, Wenkai Ji, Jian Zhou, Fu Xiao, and Tsung-Hui Chang",
        "link": "http://arxiv.org/abs/2502.17260v2",
        "abstract": "Federated learning (FL) has emerged as a promising distributed learning\nparadigm for training deep neural networks (DNNs) at the wireless edge, but its\nperformance can be severely hindered by unreliable wireless transmission and\ninherent data heterogeneity among clients. Existing solutions primarily address\nthese challenges by incorporating wireless resource optimization strategies,\noften focusing on uplink resource allocation across clients under the\nassumption of homogeneous client-server network standards. However, these\napproaches overlooked the fact that mobile clients may connect to the server\nvia diverse network standards (e.g., 4G, 5G, Wi-Fi) with customized\nconfigurations, limiting the flexibility of server-side modifications and\nrestricting applicability in real-world commercial networks. This paper\npresents a novel theoretical analysis about how transmission failures in\nunreliable networks distort the effective label distributions of local samples,\ncausing deviations from the global data distribution and introducing\nconvergence bias in FL. Our analysis reveals that a carefully designed client\nselection strategy can mitigate biases induced by network unreliability and\ndata heterogeneity. Motivated by this insight, we propose FedCote, a client\nselection approach that optimizes client selection probabilities without\nrelying on wireless resource scheduling. Experimental results demonstrate the\nrobustness of FedCote in DNN-based classification tasks under unreliable\nnetworks with frequent transmission failures."
    },
    {
        "date": "2025-02",
        "title": "REINFORCE Adversarial Attacks on Large Language Models: An Adaptive, Distributional, and Semantic Objective",
        "author": "Simon Geisler, Tom Wollschl\u00e4ger, M. H. I. Abdalla, Vincent Cohen-Addad, Johannes Gasteiger, and Stephan G\u00fcnnemann",
        "link": "http://arxiv.org/abs/2502.17254v1",
        "abstract": "To circumvent the alignment of large language models (LLMs), current\noptimization-based adversarial attacks usually craft adversarial prompts by\nmaximizing the likelihood of a so-called affirmative response. An affirmative\nresponse is a manually designed start of a harmful answer to an inappropriate\nrequest. While it is often easy to craft prompts that yield a substantial\nlikelihood for the affirmative response, the attacked model frequently does not\ncomplete the response in a harmful manner. Moreover, the affirmative objective\nis usually not adapted to model-specific preferences and essentially ignores\nthe fact that LLMs output a distribution over responses. If low attack success\nunder such an objective is taken as a measure of robustness, the true\nrobustness might be grossly overestimated. To alleviate these flaws, we propose\nan adaptive and semantic optimization problem over the population of responses.\nWe derive a generally applicable objective via the REINFORCE policy-gradient\nformalism and demonstrate its efficacy with the state-of-the-art jailbreak\nalgorithms Greedy Coordinate Gradient (GCG) and Projected Gradient Descent\n(PGD). For example, our objective doubles the attack success rate (ASR) on\nLlama3 and increases the ASR from 2% to 50% with circuit breaker defense."
    },
    {
        "date": "2025-02",
        "title": "CAR-LOAM: Color-Assisted Robust LiDAR Odometry and Mapping",
        "author": "Yufei Lu, Yuetao Li, Zhizhou Jia, Qun Hao, and Shaohui Zhang",
        "link": "http://arxiv.org/abs/2502.17249v1",
        "abstract": "In this letter, we propose a color-assisted robust framework for accurate\nLiDAR odometry and mapping (LOAM). Simultaneously receiving data from both the\nLiDAR and the camera, the framework utilizes the color information from the\ncamera images to colorize the LiDAR point clouds and then performs iterative\npose optimization. For each LiDAR scan, the edge and planar features are\nextracted and colored using the corresponding image and then matched to a\nglobal map. Specifically, we adopt a perceptually uniform color difference\nweighting strategy to exclude color correspondence outliers and a robust error\nmetric based on the Welsch's function to mitigate the impact of positional\ncorrespondence outliers during the pose optimization process. As a result, the\nsystem achieves accurate localization and reconstructs dense, accurate, colored\nand three-dimensional (3D) maps of the environment. Thorough experiments with\nchallenging scenarios, including complex forests and a campus, show that our\nmethod provides higher robustness and accuracy compared with current\nstate-of-the-art methods."
    },
    {
        "date": "2025-02",
        "title": "Motion-Robust T2* Quantification from Gradient Echo MRI with Physics-Informed Deep Learning",
        "author": "Hannah Eichhorn, Veronika Spieker, Kerstin Hammernik, Elisa Saks, Lina Felsner, Kilian Weiss, Christine Preibisch, and Julia A. Schnabel",
        "link": "http://arxiv.org/abs/2502.17209v1",
        "abstract": "Purpose: T2* quantification from gradient echo magnetic resonance imaging is\nparticularly affected by subject motion due to the high sensitivity to magnetic\nfield inhomogeneities, which are influenced by motion and might cause signal\nloss. Thus, motion correction is crucial to obtain high-quality T2* maps.\n  Methods: We extend our previously introduced learning-based physics-informed\nmotion correction method, PHIMO, by utilizing acquisition knowledge to enhance\nthe reconstruction performance for challenging motion patterns and increase\nPHIMO's robustness to varying strengths of magnetic field inhomogeneities\nacross the brain. We perform comprehensive evaluations regarding motion\ndetection accuracy and image quality for data with simulated and real motion.\n  Results: Our extended version of PHIMO outperforms the learning-based\nbaseline methods both qualitatively and quantitatively with respect to line\ndetection and image quality. Moreover, PHIMO performs on-par with a\nconventional state-of-the-art motion correction method for T2* quantification\nfrom gradient echo MRI, which relies on redundant data acquisition.\n  Conclusion: PHIMO's competitive motion correction performance, combined with\na reduction in acquisition time by over 40% compared to the state-of-the-art\nmethod, make it a promising solution for motion-robust T2* quantification in\nresearch settings and clinical routine."
    },
    {
        "date": "2025-02",
        "title": "Adversarial Training for Defense Against Label Poisoning Attacks",
        "author": "Melis Ilayda Bal, Volkan Cevher, and Michael Muehlebach",
        "link": "http://arxiv.org/abs/2502.17121v1",
        "abstract": "As machine learning models grow in complexity and increasingly rely on\npublicly sourced data, such as the human-annotated labels used in training\nlarge language models, they become more vulnerable to label poisoning attacks.\nThese attacks, in which adversaries subtly alter the labels within a training\ndataset, can severely degrade model performance, posing significant risks in\ncritical applications. In this paper, we propose FLORAL, a novel adversarial\ntraining defense strategy based on support vector machines (SVMs) to counter\nthese threats. Utilizing a bilevel optimization framework, we cast the training\nprocess as a non-zero-sum Stackelberg game between an attacker, who\nstrategically poisons critical training labels, and the model, which seeks to\nrecover from such attacks. Our approach accommodates various model\narchitectures and employs a projected gradient descent algorithm with kernel\nSVMs for adversarial training. We provide a theoretical analysis of our\nalgorithm's convergence properties and empirically evaluate FLORAL's\neffectiveness across diverse classification tasks. Compared to robust baselines\nand foundation models such as RoBERTa, FLORAL consistently achieves higher\nrobust accuracy under increasing attacker budgets. These results underscore the\npotential of FLORAL to enhance the resilience of machine learning models\nagainst label poisoning threats, thereby ensuring robust classification in\nadversarial settings."
    },
    {
        "date": "2025-02",
        "title": "Improved Diffusion-based Generative Model with Better Adversarial Robustness",
        "author": "Zekun Wang, Mingyang Yi, Shuchen Xue, Zhenguo Li, Ming Liu, Bing Qin, and Zhi-Ming Ma",
        "link": "http://arxiv.org/abs/2502.17099v1",
        "abstract": "Diffusion Probabilistic Models (DPMs) have achieved significant success in\ngenerative tasks. However, their training and sampling processes suffer from\nthe issue of distribution mismatch. During the denoising process, the input\ndata distributions differ between the training and inference stages,\npotentially leading to inaccurate data generation. To obviate this, we analyze\nthe training objective of DPMs and theoretically demonstrate that this mismatch\ncan be alleviated through Distributionally Robust Optimization (DRO), which is\nequivalent to performing robustness-driven Adversarial Training (AT) on DPMs.\nFurthermore, for the recently proposed Consistency Model (CM), which distills\nthe inference process of the DPM, we prove that its training objective also\nencounters the mismatch issue. Fortunately, this issue can be mitigated by AT\nas well. Based on these insights, we propose to conduct efficient AT on both\nDPM and CM. Finally, extensive empirical studies validate the effectiveness of\nAT in diffusion-based models. The code is available at\nhttps://github.com/kugwzk/AT_Diff."
    },
    {
        "date": "2025-02",
        "title": "Improving the Transferability of Adversarial Examples by Inverse Knowledge Distillation",
        "author": "Wenyuan Wu, Zheng Liu, Yong Chen, Chao Su, Dezhong Peng, and Xu Wang",
        "link": "http://arxiv.org/abs/2502.17003v1",
        "abstract": "In recent years, the rapid development of deep neural networks has brought\nincreased attention to the security and robustness of these models. While\nexisting adversarial attack algorithms have demonstrated success in improving\nadversarial transferability, their performance remains suboptimal due to a lack\nof consideration for the discrepancies between target and source models. To\naddress this limitation, we propose a novel method, Inverse Knowledge\nDistillation (IKD), designed to enhance adversarial transferability\neffectively. IKD introduces a distillation-inspired loss function that\nseamlessly integrates with gradient-based attack methods, promoting diversity\nin attack gradients and mitigating overfitting to specific model architectures.\nBy diversifying gradients, IKD enables the generation of adversarial samples\nwith superior generalization capabilities across different models,\nsignificantly enhancing their effectiveness in black-box attack scenarios.\nExtensive experiments on the ImageNet dataset validate the effectiveness of our\napproach, demonstrating substantial improvements in the transferability and\nattack success rates of adversarial samples across a wide range of models."
    },
    {
        "date": "2025-02",
        "title": "FedSV: Byzantine-Robust Federated Learning via Shapley Value",
        "author": "Khaoula Otmani, Rachid Elazouzi, and Vincent Labatut",
        "link": "http://arxiv.org/abs/2502.17526v1",
        "abstract": "In Federated Learning (FL), several clients jointly learn a machine learning\nmodel: each client maintains a local model for its local learning dataset,\nwhile a master server maintains a global model by aggregating the local models\nof the client devices. However, the repetitive communication between server and\nclients leaves room for attacks aimed at compromising the integrity of the\nglobal model, causing errors in its targeted predictions. In response to such\nthreats on FL, various defense measures have been proposed in the literature.\nIn this paper, we present a powerful defense against malicious clients in FL,\ncalled FedSV, using the Shapley Value (SV), which has been proposed recently to\nmeasure user contribution in FL by computing the marginal increase of average\naccuracy of the model due to the addition of local data of a user. Our approach\nmakes the identification of malicious clients more robust, since during the\nlearning phase, it estimates the contribution of each client according to the\ndifferent groups to which the target client belongs. FedSV's effectiveness is\ndemonstrated by extensive experiments on MNIST datasets in a cross-silo context\nunder various attacks."
    },
    {
        "date": "2025-02",
        "title": "Char-mander Use mBackdoor! A Study of Cross-lingual Backdoor Attacks in Multilingual LLMs",
        "author": "Himanshu Beniwal, Sailesh Panda, and Mayank Singh",
        "link": "http://arxiv.org/abs/2502.16901v1",
        "abstract": "We explore Cross-lingual Backdoor ATtacks (X-BAT) in multilingual Large\nLanguage Models (mLLMs), revealing how backdoors inserted in one language can\nautomatically transfer to others through shared embedding spaces. Using\ntoxicity classification as a case study, we demonstrate that attackers can\ncompromise multilingual systems by poisoning data in a single language, with\nrare tokens serving as specific effective triggers. Our findings expose a\ncritical vulnerability in the fundamental architecture that enables\ncross-lingual transfer in these models. Our code and data are publicly\navailable at https://github.com/himanshubeniwal/X-BAT."
    },
    {
        "date": "2025-02",
        "title": "Distributionally Robust Active Learning for Gaussian Process Regression",
        "author": "Shion Takeno, Yoshito Okura, Yu Inatsu, Aoyama Tatsuya, Tomonari Tanaka, Akahane Satoshi, Hiroyuki Hanada, Noriaki Hashimoto, Taro Murayama, Hanju Lee, Shinya Kojima, and Ichiro Takeuchi",
        "link": "http://arxiv.org/abs/2502.16870v1",
        "abstract": "Gaussian process regression (GPR) or kernel ridge regression is a widely used\nand powerful tool for nonlinear prediction. Therefore, active learning (AL) for\nGPR, which actively collects data labels to achieve an accurate prediction with\nfewer data labels, is an important problem. However, existing AL methods do not\ntheoretically guarantee prediction accuracy for target distribution.\nFurthermore, as discussed in the distributionally robust learning literature,\nspecifying the target distribution is often difficult. Thus, this paper\nproposes two AL methods that effectively reduce the worst-case expected error\nfor GPR, which is the worst-case expectation in target distribution candidates.\nWe show an upper bound of the worst-case expected squared error, which suggests\nthat the error will be arbitrarily small by a finite number of data labels\nunder mild conditions. Finally, we demonstrate the effectiveness of the\nproposed methods through synthetic and real-world datasets."
    },
    {
        "date": "2025-02",
        "title": "Finite-Sample Analysis of Policy Evaluation for Robust Average Reward Reinforcement Learning",
        "author": "Yang Xu, Washim Uddin Mondal, and Vaneet Aggarwal",
        "link": "http://arxiv.org/abs/2502.16816v1",
        "abstract": "We present the first finite-sample analysis for policy evaluation in robust\naverage-reward Markov Decision Processes (MDPs). Prior works in this setting\nhave established only asymptotic convergence guarantees, leaving open the\nquestion of sample complexity. In this work, we address this gap by\nestablishing that the robust Bellman operator is a contraction under the span\nsemi-norm, and developing a stochastic approximation framework with controlled\nbias. Our approach builds upon Multi-Level Monte Carlo (MLMC) techniques to\nestimate the robust Bellman operator efficiently. To overcome the infinite\nexpected sample complexity inherent in standard MLMC, we introduce a truncation\nmechanism based on a geometric distribution, ensuring a finite constant sample\ncomplexity while maintaining a small bias that decays exponentially with the\ntruncation level. Our method achieves the order-optimal sample complexity of\n$\\tilde{\\mathcal{O}}(\\epsilon^{-2})$ for robust policy evaluation and robust\naverage reward estimation, marking a significant advancement in robust\nreinforcement learning theory."
    },
    {
        "date": "2025-02",
        "title": "VGFL-SA: Vertical Graph Federated Learning Structure Attack Based on Contrastive Learning",
        "author": "Yang Chen, and Bin Zhou",
        "link": "http://arxiv.org/abs/2502.16793v1",
        "abstract": "Graph Neural Networks (GNNs) have gained attention for their ability to learn\nrepresentations from graph data. Due to privacy concerns and conflicts of\ninterest that prevent clients from directly sharing graph data with one\nanother, Vertical Graph Federated Learning (VGFL) frameworks have been\ndeveloped. Recent studies have shown that VGFL is vulnerable to adversarial\nattacks that degrade performance. However, it is a common problem that client\nnodes are often unlabeled in the realm of VGFL. Consequently, the existing\nattacks, which rely on the availability of labeling information to obtain\ngradients, are inherently constrained in their applicability. This limitation\nprecludes their deployment in practical, real-world environments. To address\nthe above problems, we propose a novel graph adversarial attack against VGFL,\nreferred to as VGFL-SA, to degrade the performance of VGFL by modifying the\nlocal clients structure without using labels. Specifically, VGFL-SA uses a\ncontrastive learning method to complete the attack before the local clients are\ntrained. VGFL-SA first accesses the graph structure and node feature\ninformation of the poisoned clients, and generates the contrastive views by\nnode-degree-based edge augmentation and feature shuffling augmentation. Then,\nVGFL-SA uses the shared graph encoder to get the embedding of each view, and\nthe gradients of the adjacency matrices are obtained by the contrastive\nfunction. Finally, perturbed edges are generated using gradient modification\nrules. We validated the performance of VGFL-SA by performing a node\nclassification task on real-world datasets, and the results show that VGFL-SA\nachieves good attack effectiveness and transferability."
    },
    {
        "date": "2025-02",
        "title": "The Robustness of Structural Features in Species Interaction Networks",
        "author": "Sanaz Hasanzadeh Fard, and Emily Dolson",
        "link": "http://arxiv.org/abs/2502.16778v1",
        "abstract": "Species interaction networks are a powerful tool for describing ecological\ncommunities; they typically contain nodes representing species, and edges\nrepresenting interactions between those species. For the purposes of drawing\nabstract inferences about groups of similar networks, ecologists often use\ngraph topology metrics to summarize structural features. However, gathering the\ndata that underlies these networks is challenging, which can lead to some\ninteractions being missed. Thus, it is important to understand how much\ndifferent structural metrics are affected by missing data. To address this\nquestion, we analyzed a database of 148 real-world bipartite networks\nrepresenting four different types of species interactions (pollination,\nhost-parasite, plant-ant, and seed-dispersal). For each network, we measured\nsix different topological properties: number of connected components, variance\nin node betweenness, variance in node PageRank, largest Eigenvalue, the number\nof non-zero Eigenvalues, and community detection as determined by four\ndifferent algorithms. We then tested how these properties change as additional\nedges -- representing data that may have been missed -- are added to the\nnetworks. We found substantial variation in how robust different properties\nwere to the missing data. For example, the Clauset-Newman-Moore and Louvain\ncommunity detection algorithms showed much more gradual change as edges were\nadded than the label propagation and Girvan-Newman algorithms did, suggesting\nthat the former are more robust. Robustness also varied for some metrics based\non interaction type. These results provide a foundation for selecting network\nproperties to use when analyzing messy ecological network data."
    },
    {
        "date": "2025-02",
        "title": "Order-Optimal Projection-Free Algorithm for Adversarially Constrained Online Convex Optimization",
        "author": "Yiyang Lu, Mohammad Pedramfar, and Vaneet Aggarwal",
        "link": "http://arxiv.org/abs/2502.16744v1",
        "abstract": "Projection-based algorithms for constrained Online Convex Optimization (COCO)\nface scalability challenges in high-dimensional settings due to the\ncomputational complexity of projecting iterates onto constraint sets. This\npaper introduces a projection-free algorithm for COCO that achieves\nstate-of-the-art performance guarantees while eliminating the need for\nprojections. By integrating a separation oracle with adaptive Online Gradient\nDescent (OGD) and employing a Lyapunov-driven surrogate function, while\ndynamically adjusting step sizes using gradient norms, our method jointly\noptimizes the regret and cumulative constraint violation (CCV). We also use a\nblocked version of OGD that helps achieve tradeoffs betweeen the regret and CCV\nwith the number of calls to the separation oracle. For convex cost functions,\nour algorithm attains an optimal regret of $\\mathcal{O}(\\sqrt{T})$ and a CCV of\n$\\mathcal{O}(\\sqrt{T} \\log T)$, matching the best-known projection-based\nresults, while only using $\\tilde{\\mathcal{O}}({T})$ calls to the separation\noracle. The results also demonstrate a tradeoff where lower calls to the\nseparation oracle increase the regret and the CCV. In the strongly convex\nsetting, we further achieve a regret of $\\mathcal{O}(\\log T)$ and a CCV of\n$\\mathcal{O}(\\sqrt{T\\log T} )$, while requiring ${\\mathcal{O}}({T}^2)$ calls to\nthe separation oracle. Further, tradeoff with the decreasing oracle calls is\nstudied. These results close the gap between projection-free and\nprojection-based approaches, demonstrating that projection-free methods can\nachieve performance comparable to projection-based counterparts."
    },
    {
        "date": "2025-02",
        "title": "Keeping up with dynamic attackers: Certifying robustness to adaptive online data poisoning",
        "author": "Avinandan Bose, Laurent Lessard, Maryam Fazel, and Krishnamurthy Dj Dvijotham",
        "link": "http://arxiv.org/abs/2502.16737v1",
        "abstract": "The rise of foundation models fine-tuned on human feedback from potentially\nuntrusted users has increased the risk of adversarial data poisoning,\nnecessitating the study of robustness of learning algorithms against such\nattacks. Existing research on provable certified robustness against data\npoisoning attacks primarily focuses on certifying robustness for static\nadversaries who modify a fraction of the dataset used to train the model before\nthe training algorithm is applied. In practice, particularly when learning from\nhuman feedback in an online sense, adversaries can observe and react to the\nlearning process and inject poisoned samples that optimize adversarial\nobjectives better than when they are restricted to poisoning a static dataset\nonce, before the learning algorithm is applied. Indeed, it has been shown in\nprior work that online dynamic adversaries can be significantly more powerful\nthan static ones. We present a novel framework for computing certified bounds\non the impact of dynamic poisoning, and use these certificates to design robust\nlearning algorithms. We give an illustration of the framework for the mean\nestimation and binary classification problems and outline directions for\nextending this in further work. The code to implement our certificates and\nreplicate our results is available at\nhttps://github.com/Avinandan22/Certified-Robustness."
    },
    {
        "date": "2025-02",
        "title": "Towards Optimal Adversarial Robust Reinforcement Learning with Infinity Measurement Error",
        "author": "Haoran Li, Zicheng Zhang, Wang Luo, Congying Han, Jiayu Lv, Tiande Guo, and Yudong Hu",
        "link": "http://arxiv.org/abs/2502.16734v1",
        "abstract": "Ensuring the robustness of deep reinforcement learning (DRL) agents against\nadversarial attacks is critical for their trustworthy deployment. Recent\nresearch highlights the challenges of achieving state-adversarial robustness\nand suggests that an optimal robust policy (ORP) does not always exist,\ncomplicating the enforcement of strict robustness constraints. In this paper,\nwe further explore the concept of ORP. We first introduce the Intrinsic\nState-adversarial Markov Decision Process (ISA-MDP), a novel formulation where\nadversaries cannot fundamentally alter the intrinsic nature of state\nobservations. ISA-MDP, supported by empirical and theoretical evidence,\nuniversally characterizes decision-making under state-adversarial paradigms. We\nrigorously prove that within ISA-MDP, a deterministic and stationary ORP\nexists, aligning with the Bellman optimal policy. Our findings theoretically\nreveal that improving DRL robustness does not necessarily compromise\nperformance in natural environments. Furthermore, we demonstrate the necessity\nof infinity measurement error (IME) in both $Q$-function and probability spaces\nto achieve ORP, unveiling vulnerabilities of previous DRL algorithms that rely\non $1$-measurement errors. Motivated by these insights, we develop the\nConsistent Adversarial Robust Reinforcement Learning (CAR-RL) framework, which\noptimizes surrogates of IME. We apply CAR-RL to both value-based and\npolicy-based DRL algorithms, achieving superior performance and validating our\ntheoretical analysis."
    },
    {
        "date": "2025-02",
        "title": "The Popularity Hypothesis in Software Security: A Large-Scale Replication with PHP Packages",
        "author": "Jukka Ruohonen, and Qusai Ramadan",
        "link": "http://arxiv.org/abs/2502.16670v1",
        "abstract": "There has been a long-standing hypothesis that a software's popularity is\nrelated to its security or insecurity in both research and popular discourse.\nThere are also a few empirical studies that have examined the hypothesis,\neither explicitly or implicitly. The present work continues with and\ncontributes to this research with a replication-motivated large-scale analysis\nof software written in the PHP programming language. The dataset examined\ncontains nearly four hundred thousand open source software packages written in\nPHP. According to the results based on reported security vulnerabilities, the\nhypothesis does holds; packages having been affected by vulnerabilities over\ntheir release histories are generally more popular than packages without having\nbeen affected by a single vulnerability. With this replication results, the\npaper contributes to the efforts to strengthen the empirical knowledge base in\ncyber and software security."
    },
    {
        "date": "2025-02",
        "title": "Security Analysis of 5G NR Device-to-Device Sidelink Communications",
        "author": "Evangelos Bitsikas, and Aanjhan Ranganathan",
        "link": "http://arxiv.org/abs/2502.16650v1",
        "abstract": "5G NR sidelink communication enables new possibilities for direct\ndevice-to-device interactions, supporting applications from\nvehicle-to-everything (V2X) systems to public safety, industrial automation,\nand drone networks. However, these advancements come with significant security\nchallenges due to the decentralized trust model and increased reliance on User\nEquipment (UE) for critical functions like synchronization, resource\nallocation, and authorization. This paper presents the first comprehensive\nsecurity analysis of NR V2X sidelink. We identify vulnerabilities across\ncritical procedures and demonstrate plausible attack, including attacks that\nmanipulate data integrity feedback and block resources, ultimately undermining\nthe reliability and privacy of sidelink communications. Our analysis reveals\nthat NR operational modes are vulnerable, with the ones relying on autonomous\nresource management (without network supervision) particularly exposed. To\naddress these issues, we propose mitigation strategies to enhance the security\nof 5G sidelink communications. This work establishes a foundation for future\nefforts to strengthen 5G device-to-device sidelink communications, ensuring its\nsafe deployment in critical applications."
    },
    {
        "date": "2025-02",
        "title": "AdverX-Ray: Ensuring X-Ray Integrity Through Frequency-Sensitive Adversarial VAEs",
        "author": "Francisco Caetano, Christiaan Viviers, Lena Filatova, Peter H. N. de With, and Fons van der Sommen",
        "link": "http://arxiv.org/abs/2502.16610v1",
        "abstract": "Ensuring the quality and integrity of medical images is crucial for\nmaintaining diagnostic accuracy in deep learning-based Computer-Aided Diagnosis\nand Computer-Aided Detection (CAD) systems. Covariate shifts are subtle\nvariations in the data distribution caused by different imaging devices or\nsettings and can severely degrade model performance, similar to the effects of\nadversarial attacks. Therefore, it is vital to have a lightweight and fast\nmethod to assess the quality of these images prior to using CAD models.\nAdverX-Ray addresses this need by serving as an image-quality assessment layer,\ndesigned to detect covariate shifts effectively. This Adversarial Variational\nAutoencoder prioritizes the discriminator's role, using the suboptimal outputs\nof the generator as negative samples to fine-tune the discriminator's ability\nto identify high-frequency artifacts. Images generated by adversarial networks\noften exhibit severe high-frequency artifacts, guiding the discriminator to\nfocus excessively on these components. This makes the discriminator ideal for\nthis approach. Trained on patches from X-ray images of specific machine models,\nAdverX-Ray can evaluate whether a scan matches the training distribution, or if\na scan from the same machine is captured under different settings. Extensive\ncomparisons with various OOD detection methods show that AdverX-Ray\nsignificantly outperforms existing techniques, achieving a 96.2% average AUROC\nusing only 64 random patches from an X-ray. Its lightweight and fast\narchitecture makes it suitable for real-time applications, enhancing the\nreliability of medical imaging systems. The code and pretrained models are\npublicly available."
    },
    {
        "date": "2025-02",
        "title": "Tracking the Copyright of Large Vision-Language Models through Parameter Learning Adversarial Images",
        "author": "Yubo Wang, Jianting Tang, Chaohu Liu, and Linli Xu",
        "link": "http://arxiv.org/abs/2502.16593v1",
        "abstract": "Large vision-language models (LVLMs) have demonstrated remarkable image\nunderstanding and dialogue capabilities, allowing them to handle a variety of\nvisual question answering tasks. However, their widespread availability raises\nconcerns about unauthorized usage and copyright infringement, where users or\nindividuals can develop their own LVLMs by fine-tuning published models. In\nthis paper, we propose a novel method called Parameter Learning Attack (PLA)\nfor tracking the copyright of LVLMs without modifying the original model.\nSpecifically, we construct adversarial images through targeted attacks against\nthe original model, enabling it to generate specific outputs. To ensure these\nattacks remain effective on potential fine-tuned models to trigger copyright\ntracking, we allow the original model to learn the trigger images by updating\nparameters in the opposite direction during the adversarial attack process.\nNotably, the proposed method can be applied after the release of the original\nmodel, thus not affecting the model's performance and behavior. To simulate\nreal-world applications, we fine-tune the original model using various\nstrategies across diverse datasets, creating a range of models for copyright\nverification. Extensive experiments demonstrate that our method can more\neffectively identify the original copyright of fine-tuned models compared to\nbaseline methods. Therefore, this work provides a powerful tool for tracking\ncopyrights and detecting unlicensed usage of LVLMs."
    },
    {
        "date": "2025-02",
        "title": "UNCA: A Neutrosophic-Based Framework for Robust Clustering and Enhanced Data Interpretation",
        "author": "D. Dhinakaran, S. Edwin Raja, S. Gopalakrishnan, D. Selvaraj, and S. D. Lalitha",
        "link": "http://arxiv.org/abs/2502.17523v1",
        "abstract": "Accurately representing the complex linkages and inherent uncertainties\nincluded in huge datasets is still a major difficulty in the field of data\nclustering. We address these issues with our proposed Unified Neutrosophic\nClustering Algorithm (UNCA), which combines a multifaceted strategy with\nNeutrosophic logic to improve clustering performance. UNCA starts with a\nfull-fledged similarity examination via a {\\lambda}-cutting matrix that filters\nmeaningful relationships between each two points of data. Then, we initialize\ncentroids for Neutrosophic K-Means clustering, where the membership values are\nbased on their degrees of truth, indeterminacy and falsity. The algorithm then\nintegrates with a dynamic network visualization and MST (Minimum Spanning Tree)\nso that a visual interpretation of the relationships between the clusters can\nbe clearly represented. UNCA employs SingleValued Neutrosophic Sets (SVNSs) to\nrefine cluster assignments, and after fuzzifying similarity measures,\nguarantees a precise clustering result. The final step involves solidifying the\nclustering results through defuzzification methods, offering definitive cluster\nassignments. According to the performance evaluation results, UNCA outperforms\nconventional approaches in several metrics: it achieved a Silhouette Score of\n0.89 on the Iris Dataset, a Davies-Bouldin Index of 0.59 on the Wine Dataset,\nan Adjusted Rand Index (ARI) of 0.76 on the Digits Dataset, and a Normalized\nMutual Information (NMI) of 0.80 on the Customer Segmentation Dataset. These\nresults demonstrate how UNCA enhances interpretability and resilience in\naddition to improving clustering accuracy when contrasted with Fuzzy C-Means\n(FCM), Neutrosophic C-Means (NCM), as well as Kernel Neutrosophic C-Means\n(KNCM). This makes UNCA a useful tool for complex data processing tasks"
    },
    {
        "date": "2025-02",
        "title": "Can Indirect Prompt Injection Attacks Be Detected and Removed?",
        "author": "Yulin Chen, Haoran Li, Yuan Sui, Yufei He, Yue Liu, Yangqiu Song, and Bryan Hooi",
        "link": "http://arxiv.org/abs/2502.16580v1",
        "abstract": "Prompt injection attacks manipulate large language models (LLMs) by\nmisleading them to deviate from the original input instructions and execute\nmaliciously injected instructions, because of their instruction-following\ncapabilities and inability to distinguish between the original input\ninstructions and maliciously injected instructions. To defend against such\nattacks, recent studies have developed various detection mechanisms. While\nsignificant efforts have focused on detecting direct prompt injection attacks,\nwhere injected instructions are directly from the attacker who is also the\nuser, limited attention has been given to indirect prompt injection attacks,\nwhere injected instructions are indirectly from external tools, such as a\nsearch engine. Moreover, current works mainly investigate injection detection\nmethods and pay less attention to the post-processing method that aims to\nmitigate the injection after detection. In this paper, we investigate the\nfeasibility of detecting and removing indirect prompt injection attacks, and we\nconstruct a benchmark dataset for evaluation. For detection, we assess the\nperformance of existing LLMs and open-source detection models, and we further\ntrain detection models using our crafted training datasets. For removal, we\nevaluate two intuitive methods: (1) the segmentation removal method, which\nsegments the injected document and removes parts containing injected\ninstructions, and (2) the extraction removal method, which trains an extraction\nmodel to identify and remove injected instructions."
    },
    {
        "date": "2025-02",
        "title": "Multi-Target Federated Backdoor Attack Based on Feature Aggregation",
        "author": "Lingguag Hao, Kuangrong Hao, Bing Wei, and Xue-song Tang",
        "link": "http://arxiv.org/abs/2502.16545v1",
        "abstract": "Current federated backdoor attacks focus on collaboratively training backdoor\ntriggers, where multiple compromised clients train their local trigger patches\nand then merge them into a global trigger during the inference phase. However,\nthese methods require careful design of the shape and position of trigger\npatches and lack the feature interactions between trigger patches during\ntraining, resulting in poor backdoor attack success rates. Moreover, the pixels\nof the patches remain untruncated, thereby making abrupt areas in backdoor\nexamples easily detectable by the detection algorithm. To this end, we propose\na novel benchmark for the federated backdoor attack based on feature\naggregation. Specifically, we align the dimensions of triggers with images,\ndelimit the trigger's pixel boundaries, and facilitate feature interaction\namong local triggers trained by each compromised client. Furthermore,\nleveraging the intra-class attack strategy, we propose the simultaneous\ngeneration of backdoor triggers for all target classes, significantly reducing\nthe overall production time for triggers across all target classes and\nincreasing the risk of the federated model being attacked. Experiments\ndemonstrate that our method can not only bypass the detection of defense\nmethods while patch-based methods fail, but also achieve a zero-shot backdoor\nattack with a success rate of 77.39%. To the best of our knowledge, our work is\nthe first to implement such a zero-shot attack in federated learning. Finally,\nwe evaluate attack performance by varying the trigger's training factors,\nincluding poison location, ratio, pixel bound, and trigger training duration\n(local epochs and communication rounds)."
    },
    {
        "date": "2025-02",
        "title": "Class-Conditional Neural Polarizer: A Lightweight and Effective Backdoor Defense by Purifying Poisoned Features",
        "author": "Mingli Zhu, Shaokui Wei, Hongyuan Zha, and Baoyuan Wu",
        "link": "http://arxiv.org/abs/2502.18520v1",
        "abstract": "Recent studies have highlighted the vulnerability of deep neural networks to\nbackdoor attacks, where models are manipulated to rely on embedded triggers\nwithin poisoned samples, despite the presence of both benign and trigger\ninformation. While several defense methods have been proposed, they often\nstruggle to balance backdoor mitigation with maintaining benign performance.In\nthis work, inspired by the concept of optical polarizer-which allows light\nwaves of specific polarizations to pass while filtering others-we propose a\nlightweight backdoor defense approach, NPD. This method integrates a neural\npolarizer (NP) as an intermediate layer within the compromised model,\nimplemented as a lightweight linear transformation optimized via bi-level\noptimization. The learnable NP filters trigger information from poisoned\nsamples while preserving benign content. Despite its effectiveness, we identify\nthrough empirical studies that NPD's performance degrades when the target\nlabels (required for purification) are inaccurately estimated. To address this\nlimitation while harnessing the potential of targeted adversarial mitigation,\nwe propose class-conditional neural polarizer-based defense (CNPD). The key\ninnovation is a fusion module that integrates the backdoored model's predicted\nlabel with the features to be purified. This architecture inherently mimics\ntargeted adversarial defense mechanisms without requiring label estimation used\nin NPD. We propose three implementations of CNPD: the first is r-CNPD, which\ntrains a replicated NP layer for each class and, during inference, selects the\nappropriate NP layer for defense based on the predicted class from the\nbackdoored model. To efficiently handle a large number of classes, two variants\nare designed: e-CNPD, which embeds class information as additional features,\nand a-CNPD, which directs network attention using class information."
    },
    {
        "date": "2025-02",
        "title": "Rebalancing the Scales: A Systematic Mapping Study of Generative Adversarial Networks (GANs) in Addressing Data Imbalance",
        "author": "Pankaj Yadav, Gulshan Sihag, and Vivek Vijay",
        "link": "http://arxiv.org/abs/2502.16535v1",
        "abstract": "Machine learning algorithms are used in diverse domains, many of which face\nsignificant challenges due to data imbalance. Studies have explored various\napproaches to address the issue, like data preprocessing, cost-sensitive\nlearning, and ensemble methods. Generative Adversarial Networks (GANs) showed\nimmense potential as a data preprocessing technique that generates good quality\nsynthetic data. This study employs a systematic mapping methodology to analyze\n3041 papers on GAN-based sampling techniques for imbalanced data sourced from\nfour digital libraries. A filtering process identified 100 key studies spanning\ndomains such as healthcare, finance, and cybersecurity. Through comprehensive\nquantitative analysis, this research introduces three categorization mappings\nas application domains, GAN techniques, and GAN variants used to handle the\nimbalanced nature of the data. GAN-based over-sampling emerges as an effective\npreprocessing method. Advanced architectures and tailored frameworks helped\nGANs to improve further in the case of data imbalance. GAN variants like\nvanilla GAN, CTGAN, and CGAN show great adaptability in structured imbalanced\ndata cases. Interest in GANs for imbalanced data has grown tremendously,\ntouching a peak in recent years, with journals and conferences playing crucial\nroles in transmitting foundational theories and practical applications. While\nwith these advances, none of the reviewed studies explicitly explore hybridized\nGAN frameworks with diffusion models or reinforcement learning techniques. This\ngap leads to a future research idea develop innovative approaches for\neffectively handling data imbalance."
    },
    {
        "date": "2025-02",
        "title": "Pay Attention to Real World Perturbations! Natural Robustness Evaluation in Machine Reading Comprehension",
        "author": "Yulong Wu, Viktor Schlegel, and Riza Batista-Navarro",
        "link": "http://arxiv.org/abs/2502.16523v1",
        "abstract": "As neural language models achieve human-comparable performance on Machine\nReading Comprehension (MRC) and see widespread adoption, ensuring their\nrobustness in real-world scenarios has become increasingly important. Current\nrobustness evaluation research, though, primarily develops synthetic\nperturbation methods, leaving unclear how well they reflect real life\nscenarios. Considering this, we present a framework to automatically examine\nMRC models on naturally occurring textual perturbations, by replacing paragraph\nin MRC benchmarks with their counterparts based on available Wikipedia edit\nhistory. Such perturbation type is natural as its design does not stem from an\narteficial generative process, inherently distinct from the previously\ninvestigated synthetic approaches. In a large-scale study encompassing SQUAD\ndatasets and various model architectures we observe that natural perturbations\nresult in performance degradation in pre-trained encoder language models. More\nworryingly, these state-of-the-art Flan-T5 and Large Language Models (LLMs)\ninherit these errors. Further experiments demonstrate that our findings\ngeneralise to natural perturbations found in other more challenging MRC\nbenchmarks. In an effort to mitigate these errors, we show that it is possible\nto improve the robustness to natural perturbations by training on naturally or\nsynthetically perturbed examples, though a noticeable gap still remains\ncompared to performance on unperturbed data."
    },
    {
        "date": "2025-02",
        "title": "Auxiliary Discrminator Sequence Generative Adversarial Networks (ADSeqGAN) for Few Sample Molecule Generation",
        "author": "Haocheng Tang, Jing Long, and Junmei Wang",
        "link": "http://arxiv.org/abs/2502.16446v1",
        "abstract": "In this work, we introduce Auxiliary Discriminator Sequence Generative\nAdversarial Networks (ADSeqGAN), a novel approach for molecular generation in\nsmall-sample datasets. Traditional generative models often struggle with\nlimited training data, particularly in drug discovery, where molecular datasets\nfor specific therapeutic targets, such as nucleic acids binders and central\nnervous system (CNS) drugs, are scarce. ADSeqGAN addresses this challenge by\nintegrating an auxiliary random forest classifier as an additional\ndiscriminator into the GAN framework, significantly improves molecular\ngeneration quality and class specificity.\n  Our method incorporates pretrained generator and Wasserstein distance to\nenhance training stability and diversity. We evaluate ADSeqGAN on a dataset\ncomprising nucleic acid-targeting and protein-targeting small molecules,\ndemonstrating its superior ability to generate nucleic acid binders compared to\nbaseline models such as SeqGAN, ORGAN, and MolGPT. Through an oversampling\nstrategy, ADSeqGAN also significantly improves CNS drug generation, achieving a\nhigher yield than traditional de novo models. Critical assessments, including\ndocking simulations and molecular property analysis, confirm that\nADSeqGAN-generated molecules exhibit strong binding affinities, enhanced\nchemical diversity, and improved synthetic feasibility.\n  Overall, ADSeqGAN presents a novel framework for generative molecular design\nin data-scarce scenarios, offering potential applications in computational drug\ndiscovery. We have demonstrated the successful applications of ADSeqGAN in\ngenerating synthetic nucleic acid-targeting and CNS drugs in this work."
    },
    {
        "date": "2025-02",
        "title": "Unified Prompt Attack Against Text-to-Image Generation Models",
        "author": "Duo Peng, Qiuhong Ke, Mark He Huang, Ping Hu, and Jun Liu",
        "link": "http://arxiv.org/abs/2502.16423v1",
        "abstract": "Text-to-Image (T2I) models have advanced significantly, but their growing\npopularity raises security concerns due to their potential to generate harmful\nimages. To address these issues, we propose UPAM, a novel framework to evaluate\nthe robustness of T2I models from an attack perspective. Unlike prior methods\nthat focus solely on textual defenses, UPAM unifies the attack on both textual\nand visual defenses. Additionally, it enables gradient-based optimization,\novercoming reliance on enumeration for improved efficiency and effectiveness.\nTo handle cases where T2I models block image outputs due to defenses, we\nintroduce Sphere-Probing Learning (SPL) to enable optimization even without\nimage results. Following SPL, our model bypasses defenses, inducing the\ngeneration of harmful content. To ensure semantic alignment with attacker\nintent, we propose Semantic-Enhancing Learning (SEL) for precise semantic\ncontrol. UPAM also prioritizes the naturalness of adversarial prompts using\nIn-context Naturalness Enhancement (INE), making them harder for human\nexaminers to detect. Additionally, we address the issue of iterative\nqueries--common in prior methods and easily detectable by API defenders--by\nintroducing Transferable Attack Learning (TAL), allowing effective attacks with\nminimal queries. Extensive experiments validate UPAM's superiority in\neffectiveness, efficiency, naturalness, and low query detection rates."
    },
    {
        "date": "2025-02",
        "title": "Navigation-GPT: A Robust and Adaptive Framework Utilizing Large Language Models for Navigation Applications",
        "author": "Feng Ma, Xiu-min Wang, Chen Chen, Xiao-bin Xu, and Xin-ping Yan",
        "link": "http://arxiv.org/abs/2502.16402v1",
        "abstract": "Existing navigation decision support systems often perform poorly when\nhandling non-predefined navigation scenarios. Leveraging the generalization\ncapabilities of large language model (LLM) in handling unknown scenarios, this\nresearch proposes a dual-core framework for LLM applications to address this\nissue. Firstly, through ReAct-based prompt engineering, a larger LLM core\ndecomposes intricate navigation tasks into manageable sub-tasks, which\nautonomously invoke corresponding external tools to gather relevant\ninformation, using this feedback to mitigate the risk of LLM hallucinations.\nSubsequently, a fine-tuned and compact LLM core, acting like a first-mate is\ndesigned to process such information and unstructured external data, then to\ngenerates context-aware recommendations, ultimately delivering lookout insights\nand navigation hints that adhere to the International Regulations for\nPreventing Collisions at Sea (COLREGs) and other rules. Extensive experiments\ndemonstrate the proposed framework not only excels in traditional ship\ncollision avoidance tasks but also adapts effectively to unstructured,\nnon-predefined, and unpredictable scenarios. A comparative analysis with\nDeepSeek-R1, GPT-4o and other SOTA models highlights the efficacy and\nrationality of the proposed framework. This research bridges the gap between\nconventional navigation systems and LLMs, offering a framework to enhance\nsafety and operational efficiency across diverse navigation applications."
    },
    {
        "date": "2025-02",
        "title": "Efficient Semantic-aware Encryption for Secure Communications in Intelligent Connected Vehicles",
        "author": "Bizhu Wang, Zhiqiang Bian, Yue Chen, Xiaodong Xu, Chen Sun, Wenqi Zhang, and Ping Zhang",
        "link": "http://arxiv.org/abs/2502.16400v1",
        "abstract": "Semantic communication (SemCom) significantly improves inter-vehicle\ninteractions in intelligent connected vehicles (ICVs) within limited wireless\nspectrum. However, the open nature of wireless communications introduces\neavesdropping risks. To mitigate this, we propose the Efficient Semantic-aware\nEncryption (ESAE) mechanism, integrating cryptography into SemCom to secure\nsemantic transmission without complex key management. ESAE leverages semantic\nreciprocity between source and reconstructed information from past\ncommunications to independently generate session keys at both ends, reducing\nkey transmission costs and associated security risks. Additionally, ESAE\nintroduces a semantic-aware key pre-processing method (SA-KP) using the\nYOLO-v10 model to extract consistent semantics from bit-level diverse yet\nsemantically identical content, ensuring key consistency. Experimental results\nvalidate ESAE's effectiveness and feasibility under various wireless\nconditions, with key performance factors discussed."
    },
    {
        "date": "2025-02",
        "title": "Subspace Recovery in Winsorized PCA: Insights into Accuracy and Robustness",
        "author": "Sangil Han, Kyoowon Kim, and Sungkyu Jung",
        "link": "http://arxiv.org/abs/2502.16391v1",
        "abstract": "In this paper, we explore the theoretical properties of subspace recovery\nusing Winsorized Principal Component Analysis (WPCA), utilizing a common data\ntransformation technique that caps extreme values to mitigate the impact of\noutliers. Despite the widespread use of winsorization in various tasks of\nmultivariate analysis, its theoretical properties, particularly for subspace\nrecovery, have received limited attention. We provide a detailed analysis of\nthe accuracy of WPCA, showing that increasing the number of samples while\ndecreasing the proportion of outliers guarantees the consistency of the sample\nsubspaces from WPCA with respect to the true population subspace. Furthermore,\nwe establish perturbation bounds that ensure the WPCA subspace obtained from\ncontaminated data remains close to the subspace recovered from pure data.\nAdditionally, we extend the classical notion of breakdown points to\nsubspace-valued statistics and derive lower bounds for the breakdown points of\nWPCA. Our analysis demonstrates that WPCA exhibits strong robustness to\noutliers while maintaining consistency under mild assumptions. A toy example is\nprovided to numerically illustrate the behavior of the upper bounds for\nperturbation bounds and breakdown points, emphasizing winsorization's utility\nin subspace recovery."
    },
    {
        "date": "2025-02",
        "title": "Personhood Credentials: Human-Centered Design Recommendation Balancing Security, Usability, and Trust",
        "author": "Ayae Ide, and Tanusree Sharma",
        "link": "http://arxiv.org/abs/2502.16375v1",
        "abstract": "Building on related concepts, like, decentralized identifiers (DIDs), proof\nof personhood, anonymous credentials, personhood credentials (PHCs) emerged as\nan alternative approach, enabling individuals to verify to digital service\nproviders that they are a person without disclosing additional information.\nHowever, new technologies might introduce some friction due to users\nmisunderstandings and mismatched expectations. Despite their growing\nimportance, limited research has been done on users perceptions and preferences\nregarding PHCs. To address this gap, we conducted competitive analysis, and\nsemi-structured online user interviews with 23 participants from US and EU to\nprovide concrete design recommendations for PHCs that incorporate user needs,\nadoption rules, and preferences. Our study -- (a)surfaces how people reason\nabout unknown privacy and security guarantees of PHCs compared to current\nverification methods -- (b) presents the impact of several factors on how\npeople would like to onboard and manage PHCs, including, trusted issuers (e.g.\ngov), ground truth data to issue PHC (e.g biometrics, physical id), and\nissuance system (e.g. centralized vs decentralized). In a think-aloud\nconceptual design session, participants recommended -- conceptualized design,\nsuch as periodic biometrics verification, time-bound credentials, visually\ninteractive human-check, and supervision of government for issuance system. We\npropose actionable designs reflecting users preferences."
    },
    {
        "date": "2025-02",
        "title": "Revealing Microscopic Objects in Fluorescence Live Imaging by Video-to-video Translation Based on A Spatial-temporal Generative Adversarial Network",
        "author": "Yang Jiao, Mei Yang, and Mo Weng",
        "link": "http://arxiv.org/abs/2502.16342v1",
        "abstract": "In spite of being a valuable tool to simultaneously visualize multiple types\nof subcellular structures using spectrally distinct fluorescent labels, a\nstandard fluoresce microscope is only able to identify a few microscopic\nobjects; such a limit is largely imposed by the number of fluorescent labels\navailable to the sample. In order to simultaneously visualize more objects, in\nthis paper, we propose to use video-to-video translation that mimics the\ndevelopment process of microscopic objects. In essence, we use a microscopy\nvideo-to-video translation framework namely Spatial-temporal Generative\nAdversarial Network (STGAN) to reveal the spatial and temporal relationships\nbetween the microscopic objects, after which a microscopy video of one object\ncan be translated to another object in a different domain. The experimental\nresults confirm that the proposed STGAN is effective in microscopy\nvideo-to-video translation that mitigates the spectral conflicts caused by the\nlimited fluorescent labels, allowing multiple microscopic objects be\nsimultaneously visualized."
    },
    {
        "date": "2025-02",
        "title": "CipherFace: A Fully Homomorphic Encryption-Driven Framework for Secure Cloud-Based Facial Recognition",
        "author": "Sefik Serengil, and Alper Ozpinar",
        "link": "http://arxiv.org/abs/2502.18514v1",
        "abstract": "Facial recognition systems rely on embeddings to represent facial images and\ndetermine identity by verifying if the distance between embeddings is below a\npre-tuned threshold. While embeddings are not reversible to original images,\nthey still contain sensitive information, making their security critical.\nTraditional encryption methods like AES are limited in securely utilizing cloud\ncomputational power for distance calculations. Homomorphic Encryption, allowing\ncalculations on encrypted data, offers a robust alternative. This paper\nintroduces CipherFace, a homomorphic encryption-driven framework for secure\ncloud-based facial recognition, which we have open-sourced at\nhttp://github.com/serengil/cipherface. By leveraging FHE, CipherFace ensures\nthe privacy of embeddings while utilizing the cloud for efficient distance\ncomputation. Furthermore, we propose a novel encrypted distance computation\nmethod for both Euclidean and Cosine distances, addressing key challenges in\nperforming secure similarity calculations on encrypted data. We also conducted\nexperiments with different facial recognition models, various embedding sizes,\nand cryptosystem configurations, demonstrating the scalability and\neffectiveness of CipherFace in real-world applications."
    },
    {
        "date": "2025-02",
        "title": "Verification of Bit-Flip Attacks against Quantized Neural Networks",
        "author": "Yedi Zhang, Lei Huang, Pengfei Gao, Fu Song, Jun Sun, and Jin Song Dong",
        "link": "http://arxiv.org/abs/2502.16286v1",
        "abstract": "In the rapidly evolving landscape of neural network security, the resilience\nof neural networks against bit-flip attacks (i.e., an attacker maliciously\nflips an extremely small amount of bits within its parameter storage memory\nsystem to induce harmful behavior), has emerged as a relevant area of research.\nExisting studies suggest that quantization may serve as a viable defense\nagainst such attacks. Recognizing the documented susceptibility of real-valued\nneural networks to such attacks and the comparative robustness of quantized\nneural networks (QNNs), in this work, we introduce BFAVerifier, the first\nverification framework designed to formally verify the absence of bit-flip\nattacks or to identify all vulnerable parameters in a sound and rigorous\nmanner. BFAVerifier comprises two integral components: an abstraction-based\nmethod and an MILP-based method. Specifically, we first conduct a reachability\nanalysis with respect to symbolic parameters that represent the potential\nbit-flip attacks, based on a novel abstract domain with a sound guarantee. If\nthe reachability analysis fails to prove the resilience of such attacks, then\nwe encode this verification problem into an equivalent MILP problem which can\nbe solved by off-the-shelf solvers. Therefore, BFAVerifier is sound, complete,\nand reasonably efficient. We conduct extensive experiments, which demonstrate\nits effectiveness and efficiency across various network architectures,\nquantization bit-widths, and adversary capabilities."
    },
    {
        "date": "2025-02",
        "title": "Beyond Trusting Trust: Multi-Model Validation for Robust Code Generation",
        "author": "Bradley McDanel",
        "link": "http://arxiv.org/abs/2502.16279v1",
        "abstract": "This paper explores the parallels between Thompson's \"Reflections on Trusting\nTrust\" and modern challenges in LLM-based code generation. We examine how\nThompson's insights about compiler backdoors take on new relevance in the era\nof large language models, where the mechanisms for potential exploitation are\neven more opaque and difficult to analyze. Building on this analogy, we discuss\nhow the statistical nature of LLMs creates novel security challenges in code\ngeneration pipelines. As a potential direction forward, we propose an\nensemble-based validation approach that leverages multiple independent models\nto detect anomalous code patterns through cross-model consensus. This\nperspective piece aims to spark discussion about trust and validation in\nAI-assisted software development."
    },
    {
        "date": "2025-02",
        "title": "ELBA-Bench: An Efficient Learning Backdoor Attacks Benchmark for Large Language Models",
        "author": "Xuxu Liu, Siyuan Liang, Mengya Han, Yong Luo, Aishan Liu, Xiantao Cai, Zheng He, and Dacheng Tao",
        "link": "http://arxiv.org/abs/2502.18511v1",
        "abstract": "Generative large language models are crucial in natural language processing,\nbut they are vulnerable to backdoor attacks, where subtle triggers compromise\ntheir behavior. Although backdoor attacks against LLMs are constantly emerging,\nexisting benchmarks remain limited in terms of sufficient coverage of attack,\nmetric system integrity, backdoor attack alignment. And existing pre-trained\nbackdoor attacks are idealized in practice due to resource access constraints.\nTherefore we establish $\\textit{ELBA-Bench}$, a comprehensive and unified\nframework that allows attackers to inject backdoor through parameter efficient\nfine-tuning ($\\textit{e.g.,}$ LoRA) or without fine-tuning techniques\n($\\textit{e.g.,}$ In-context-learning). $\\textit{ELBA-Bench}$ provides over\n1300 experiments encompassing the implementations of 12 attack methods, 18\ndatasets, and 12 LLMs. Extensive experiments provide new invaluable findings\ninto the strengths and limitations of various attack strategies. For instance,\nPEFT attack consistently outperform without fine-tuning approaches in\nclassification tasks while showing strong cross-dataset generalization with\noptimized triggers boosting robustness; Task-relevant backdoor optimization\ntechniques or attack prompts along with clean and adversarial demonstrations\ncan enhance backdoor attack success while preserving model performance on clean\nsamples. Additionally, we introduce a universal toolbox designed for\nstandardized backdoor attack research, with the goal of propelling further\nprogress in this vital area."
    },
    {
        "date": "2025-02",
        "title": "Robustness and Cybersecurity in the EU Artificial Intelligence Act",
        "author": "Henrik Nolte, Miriam Rateike, and Mich\u00e8le Finck",
        "link": "http://arxiv.org/abs/2502.16184v1",
        "abstract": "The EU Artificial Intelligence Act (AIA) establishes different legal\nprinciples for different types of AI systems. While prior work has sought to\nclarify some of these principles, little attention has been paid to robustness\nand cybersecurity. This paper aims to fill this gap. We identify legal\nchallenges and shortcomings in provisions related to robustness and\ncybersecurity for high-risk AI systems (Art. 15 AIA) and general-purpose AI\nmodels (Art. 55 AIA). We show that robustness and cybersecurity demand\nresilience against performance disruptions. Furthermore, we assess potential\nchallenges in implementing these provisions in light of recent advancements in\nthe machine learning (ML) literature. Our analysis informs efforts to develop\nharmonized standards, guidelines by the European Commission, as well as\nbenchmarks and measurement methodologies under Art. 15(2) AIA. With this, we\nseek to bridge the gap between legal terminology and ML research, fostering a\nbetter alignment between research and implementation efforts."
    },
    {
        "date": "2025-02",
        "title": "PersGuard: Preventing Malicious Personalization via Backdoor Attacks on Pre-trained Text-to-Image Diffusion Models",
        "author": "Xinwei Liu, Xiaojun Jia, Yuan Xun, Hua Zhang, and Xiaochun Cao",
        "link": "http://arxiv.org/abs/2502.16167v1",
        "abstract": "Diffusion models (DMs) have revolutionized data generation, particularly in\ntext-to-image (T2I) synthesis. However, the widespread use of personalized\ngenerative models raises significant concerns regarding privacy violations and\ncopyright infringement. To address these issues, researchers have proposed\nadversarial perturbation-based protection techniques. However, these methods\nhave notable limitations, including insufficient robustness against data\ntransformations and the inability to fully eliminate identifiable features of\nprotected objects in the generated output. In this paper, we introduce\nPersGuard, a novel backdoor-based approach that prevents malicious\npersonalization of specific images. Unlike traditional adversarial perturbation\nmethods, PersGuard implant backdoor triggers into pre-trained T2I models,\npreventing the generation of customized outputs for designated protected images\nwhile allowing normal personalization for unprotected ones. Unfortunately,\nexisting backdoor methods for T2I diffusion models fail to be applied to\npersonalization scenarios due to the different backdoor objectives and the\npotential backdoor elimination during downstream fine-tuning processes. To\naddress these, we propose three novel backdoor objectives specifically designed\nfor personalization scenarios, coupled with backdoor retention loss engineered\nto resist downstream fine-tuning. These components are integrated into a\nunified optimization framework. Extensive experimental evaluations demonstrate\nPersGuard's effectiveness in preserving data privacy, even under challenging\nconditions including gray-box settings, multi-object protection, and facial\nidentity scenarios. Our method significantly outperforms existing techniques,\noffering a more robust solution for privacy and copyright protection."
    },
    {
        "date": "2025-02",
        "title": "Robust Dynamic Facial Expression Recognition",
        "author": "Feng Liu, Hanyang Wang, and Siyuan Shen",
        "link": "http://arxiv.org/abs/2502.16129v1",
        "abstract": "The study of Dynamic Facial Expression Recognition (DFER) is a nascent field\nof research that involves the automated recognition of facial expressions in\nvideo data. Although existing research has primarily focused on learning\nrepresentations under noisy and hard samples, the issue of the coexistence of\nboth types of samples remains unresolved. In order to overcome this challenge,\nthis paper proposes a robust method of distinguishing between hard and noisy\nsamples. This is achieved by evaluating the prediction agreement of the model\non different sampled clips of the video. Subsequently, methodologies that\nreinforce the learning of hard samples and mitigate the impact of noisy samples\ncan be employed. Moreover, to identify the principal expression in a video and\nenhance the model's capacity for representation learning, comprising a key\nexpression re-sampling framework and a dual-stream hierarchical network is\nproposed, namely Robust Dynamic Facial Expression Recognition (RDFER). The key\nexpression re-sampling framework is designed to identify the key expression,\nthereby mitigating the potential confusion caused by non-target expressions.\nRDFER employs two sequence models with the objective of disentangling\nshort-term facial movements and long-term emotional changes. The proposed\nmethod has been shown to outperform current State-Of-The-Art approaches in DFER\nthrough extensive experimentation on benchmark datasets such as DFEW and\nFERV39K. A comprehensive analysis provides valuable insights and observations\nregarding the proposed agreement. This work has significant implications for\nthe field of dynamic facial expression recognition and promotes the further\ndevelopment of the field of noise-consistent robust learning in dynamic facial\nexpression recognition. The code is available from\n[https://github.com/Cross-Innovation-Lab/RDFER]."
    },
    {
        "date": "2025-02",
        "title": "REFINE: Inversion-Free Backdoor Defense via Model Reprogramming",
        "author": "Yukun Chen, Shuo Shao, Enhao Huang, Yiming Li, Pin-Yu Chen, Zhan Qin, and Kui Ren",
        "link": "http://arxiv.org/abs/2502.18508v1",
        "abstract": "Backdoor attacks on deep neural networks (DNNs) have emerged as a significant\nsecurity threat, allowing adversaries to implant hidden malicious behaviors\nduring the model training phase. Pre-processing-based defense, which is one of\nthe most important defense paradigms, typically focuses on input\ntransformations or backdoor trigger inversion (BTI) to deactivate or eliminate\nembedded backdoor triggers during the inference process. However, these methods\nsuffer from inherent limitations: transformation-based defenses often fail to\nbalance model utility and defense performance, while BTI-based defenses\nstruggle to accurately reconstruct trigger patterns without prior knowledge. In\nthis paper, we propose REFINE, an inversion-free backdoor defense method based\non model reprogramming. REFINE consists of two key components: \\textbf{(1)} an\ninput transformation module that disrupts both benign and backdoor patterns,\ngenerating new benign features; and \\textbf{(2)} an output remapping module\nthat redefines the model's output domain to guide the input transformations\neffectively. By further integrating supervised contrastive loss, REFINE\nenhances the defense capabilities while maintaining model utility. Extensive\nexperiments on various benchmark datasets demonstrate the effectiveness of our\nREFINE and its resistance to potential adaptive attacks."
    },
    {
        "date": "2025-02",
        "title": "Worse than Zero-shot? A Fact-Checking Dataset for Evaluating the Robustness of RAG Against Misleading Retrievals",
        "author": "Linda Zeng, Rithwik Gupta, Divij Motwani, Diji Yang, and Yi Zhang",
        "link": "http://arxiv.org/abs/2502.16101v1",
        "abstract": "Retrieval-augmented generation (RAG) has shown impressive capabilities in\nmitigating hallucinations in large language models (LLMs). However, LLMs\nstruggle to handle misleading retrievals and often fail to maintain their own\nreasoning when exposed to conflicting or selectively-framed evidence, making\nthem vulnerable to real-world misinformation. In such real-world retrieval\nscenarios, misleading and conflicting information is rampant, particularly in\nthe political domain, where evidence is often selectively framed, incomplete,\nor polarized. However, existing RAG benchmarks largely assume a clean retrieval\nsetting, where models succeed by accurately retrieving and generating answers\nfrom gold-standard documents. This assumption fails to align with real-world\nconditions, leading to an overestimation of RAG system performance. To bridge\nthis gap, we introduce RAGuard, a fact-checking dataset designed to evaluate\nthe robustness of RAG systems against misleading retrievals. Unlike prior\nbenchmarks that rely on synthetic noise, our dataset constructs its retrieval\ncorpus from Reddit discussions, capturing naturally occurring misinformation.\nIt categorizes retrieved evidence into three types: supporting, misleading, and\nirrelevant, providing a realistic and challenging testbed for assessing how\nwell RAG systems navigate different retrieval information. Our benchmark\nexperiments reveal that when exposed to misleading retrievals, all tested\nLLM-powered RAG systems perform worse than their zero-shot baselines (i.e., no\nretrieval at all), highlighting their susceptibility to noisy environments. To\nthe best of our knowledge, RAGuard is the first benchmark to systematically\nassess RAG robustness against misleading evidence. We expect this benchmark\nwill drive future research toward improving RAG systems beyond idealized\ndatasets, making them more reliable for real-world applications."
    },
    {
        "date": "2025-02",
        "title": "Stealing Training Data from Large Language Models in Decentralized Training through Activation Inversion Attack",
        "author": "Chenxi Dai, Lin Lu, and Pan Zhou",
        "link": "http://arxiv.org/abs/2502.16086v1",
        "abstract": "Decentralized training has become a resource-efficient framework to\ndemocratize the training of large language models (LLMs). However, the privacy\nrisks associated with this framework, particularly due to the potential\ninclusion of sensitive data in training datasets, remain unexplored. This paper\nidentifies a novel and realistic attack surface: the privacy leakage from\ntraining data in decentralized training, and proposes \\textit{activation\ninversion attack} (AIA) for the first time. AIA first constructs a shadow\ndataset comprising text labels and corresponding activations using public\ndatasets. Leveraging this dataset, an attack model can be trained to\nreconstruct the training data from activations in victim decentralized\ntraining. We conduct extensive experiments on various LLMs and publicly\navailable datasets to demonstrate the susceptibility of decentralized training\nto AIA. These findings highlight the urgent need to enhance security measures\nin decentralized training to mitigate privacy risks in training LLMs."
    },
    {
        "date": "2025-02",
        "title": "A Survey of Model Extraction Attacks and Defenses in Distributed Computing Environments",
        "author": "Kaixiang Zhao, Lincan Li, Kaize Ding, Neil Zhenqiang Gong, Yue Zhao, and Yushun Dong",
        "link": "http://arxiv.org/abs/2502.16065v1",
        "abstract": "Model Extraction Attacks (MEAs) threaten modern machine learning systems by\nenabling adversaries to steal models, exposing intellectual property and\ntraining data. With the increasing deployment of machine learning models in\ndistributed computing environments, including cloud, edge, and federated\nlearning settings, each paradigm introduces distinct vulnerabilities and\nchallenges. Without a unified perspective on MEAs across these distributed\nenvironments, organizations risk fragmented defenses, inadequate risk\nassessments, and substantial economic and privacy losses. This survey is\nmotivated by the urgent need to understand how the unique characteristics of\ncloud, edge, and federated deployments shape attack vectors and defense\nrequirements. We systematically examine the evolution of attack methodologies\nand defense mechanisms across these environments, demonstrating how\nenvironmental factors influence security strategies in critical sectors such as\nautonomous vehicles, healthcare, and financial services. By synthesizing recent\nadvances in MEAs research and discussing the limitations of current evaluation\npractices, this survey provides essential insights for developing robust and\nadaptive defense strategies. Our comprehensive approach highlights the\nimportance of integrating protective measures across the entire distributed\ncomputing landscape to ensure the secure deployment of machine learning models."
    },
    {
        "date": "2025-02",
        "title": "Human-AI Collaboration in Cloud Security: Cognitive Hierarchy-Driven Deep Reinforcement Learning",
        "author": "Zahra Aref, Sheng Wei, and Narayan B. Mandayam",
        "link": "http://arxiv.org/abs/2502.16054v1",
        "abstract": "Given the complexity of multi-tenant cloud environments and the need for\nreal-time threat mitigation, Security Operations Centers (SOCs) must integrate\nAI-driven adaptive defenses against Advanced Persistent Threats (APTs).\nHowever, SOC analysts struggle with countering adaptive adversarial tactics,\nnecessitating intelligent decision-support frameworks. To enhance human-AI\ncollaboration in SOCs, we propose a Cognitive Hierarchy Theory-driven Deep\nQ-Network (CHT-DQN) framework that models SOC analysts' decision-making against\nAI-driven APT bots. The SOC analyst (defender) operates at cognitive level-1,\nanticipating attacker strategies, while the APT bot (attacker) follows a\nlevel-0 exploitative policy. By incorporating CHT into DQN, our framework\nenhances SOC defense strategies via Attack Graph (AG)-based reinforcement\nlearning. Simulation experiments across varying AG complexities show that\nCHT-DQN achieves higher data protection and lower action discrepancies compared\nto standard DQN. A theoretical lower bound analysis further validates its\nsuperior Q-value performance. A human-in-the-loop (HITL) evaluation on Amazon\nMechanical Turk (MTurk) reveals that SOC analysts using CHT-DQN-driven\ntransition probabilities align better with adaptive attackers, improving data\nprotection. Additionally, human decision patterns exhibit risk aversion after\nfailure and risk-seeking behavior after success, aligning with Prospect Theory.\nThese findings underscore the potential of integrating cognitive modeling into\ndeep reinforcement learning to enhance SOC operations and develop real-time\nadaptive cloud security mechanisms."
    },
    {
        "date": "2025-02",
        "title": "A Multi-Scale Isolation Forest Approach for Real-Time Detection and Filtering of FGSM Adversarial Attacks in Video Streams of Autonomous Vehicles",
        "author": "Richard Abhulimhen, Negash Begashaw, Gurcan Comert, Chunheng Zhao, and Pierluigi Pisu",
        "link": "http://arxiv.org/abs/2502.16044v1",
        "abstract": "Deep Neural Networks (DNNs) have demonstrated remarkable success across a\nwide range of tasks, particularly in fields such as image classification.\nHowever, DNNs are highly susceptible to adversarial attacks, where subtle\nperturbations are introduced to input images, leading to erroneous model\noutputs. In today's digital era, ensuring the security and integrity of images\nprocessed by DNNs is of critical importance. One of the most prominent\nadversarial attack methods is the Fast Gradient Sign Method (FGSM), which\nperturbs images in the direction of the loss gradient to deceive the model.\n  This paper presents a novel approach for detecting and filtering FGSM\nadversarial attacks in image processing tasks. Our proposed method evaluates\n10,000 images, each subjected to five different levels of perturbation,\ncharacterized by $\\epsilon$ values of 0.01, 0.02, 0.05, 0.1, and 0.2. These\nperturbations are applied in the direction of the loss gradient. We demonstrate\nthat our approach effectively filters adversarially perturbed images,\nmitigating the impact of FGSM attacks.\n  The method is implemented in Python, and the source code is publicly\navailable on GitHub for reproducibility and further research."
    },
    {
        "date": "2025-02",
        "title": "Cross-Model Transferability of Adversarial Patches in Real-time Segmentation for Autonomous Driving",
        "author": "Prashant Shekhar, Bidur Devkota, Dumindu Samaraweera, Laxima Niure Kandel, and Manoj Babu",
        "link": "http://arxiv.org/abs/2502.16012v1",
        "abstract": "Adversarial attacks pose a significant threat to deep learning models,\nparticularly in safety-critical applications like healthcare and autonomous\ndriving. Recently, patch based attacks have demonstrated effectiveness in\nreal-time inference scenarios owing to their 'drag and drop' nature. Following\nthis idea for Semantic Segmentation (SS), here we propose a novel Expectation\nOver Transformation (EOT) based adversarial patch attack that is more realistic\nfor autonomous vehicles. To effectively train this attack we also propose a\n'simplified' loss function that is easy to analyze and implement. Using this\nattack as our basis, we investigate whether adversarial patches once optimized\non a specific SS model, can fool other models or architectures. We conduct a\ncomprehensive cross-model transferability analysis of adversarial patches\ntrained on SOTA Convolutional Neural Network (CNN) models such PIDNet-S,\nPIDNet-M and PIDNet-L, among others. Additionally, we also include the\nSegformer model to study transferability to Vision Transformers (ViTs). All of\nour analysis is conducted on the widely used Cityscapes dataset. Our study\nreveals key insights into how model architectures (CNN vs CNN or CNN vs.\nTransformer-based) influence attack susceptibility. In particular, we conclude\nthat although the transferability (effectiveness) of attacks on unseen images\nof any dimension is really high, the attacks trained against one particular\nmodel are minimally effective on other models. And this was found to be true\nfor both ViT and CNN based models. Additionally our results also indicate that\nfor CNN-based models, the repercussions of patch attacks are local, unlike\nViTs. Per-class analysis reveals that simple-classes like 'sky' suffer less\nmisclassification than others. The code for the project is available at:\nhttps://github.com/p-shekhar/adversarial-patch-transferability"
    },
    {
        "date": "2025-02",
        "title": "IPAD: Inverse Prompt for AI Detection -- A Robust and Explainable LLM-Generated Text Detector",
        "author": "Zheng Chen, Yushi Feng, Changyang He, Yue Deng, Hongxi Pu, and Bo Li",
        "link": "http://arxiv.org/abs/2502.15902v1",
        "abstract": "Large Language Models (LLMs) have attained human-level fluency in text\ngeneration, which complicates the distinguishing between human-written and\nLLM-generated texts. This increases the risk of misuse and highlights the need\nfor reliable detectors. Yet, existing detectors exhibit poor robustness on\nout-of-distribution (OOD) data and attacked data, which is critical for\nreal-world scenarios. Also, they struggle to provide explainable evidence to\nsupport their decisions, thus undermining the reliability. In light of these\nchallenges, we propose IPAD (Inverse Prompt for AI Detection), a novel\nframework consisting of a Prompt Inverter that identifies predicted prompts\nthat could have generated the input text, and a Distinguisher that examines how\nwell the input texts align with the predicted prompts. We develop and examine\ntwo versions of Distinguishers. Empirical evaluations demonstrate that both\nDistinguishers perform significantly better than the baseline methods, with\nversion2 outperforming baselines by 9.73% on in-distribution data (F1-score)\nand 12.65% on OOD data (AUROC). Furthermore, a user study is conducted to\nillustrate that IPAD enhances the AI detection trustworthiness by allowing\nusers to directly examine the decision-making evidence, which provides\ninterpretable support for its state-of-the-art detection results."
    },
    {
        "date": "2025-02",
        "title": "Directional Gradient Projection for Robust Fine-Tuning of Foundation Models",
        "author": "Chengyue Huang, Junjiao Tian, Brisa Maneechotesuwan, Shivang Chopra, and Zsolt Kira",
        "link": "http://arxiv.org/abs/2502.15895v1",
        "abstract": "Robust fine-tuning aims to adapt large foundation models to downstream tasks\nwhile preserving their robustness to distribution shifts. Existing methods\nprimarily focus on constraining and projecting current model towards the\npre-trained initialization based on the magnitudes between fine-tuned and\npre-trained weights, which often require extensive hyper-parameter tuning and\ncan sometimes result in underfitting. In this work, we propose Directional\nGradient Projection (DiGraP), a novel layer-wise trainable method that\nincorporates directional information from gradients to bridge regularization\nand multi-objective optimization. Besides demonstrating our method on image\nclassification, as another contribution we generalize this area to the\nmulti-modal evaluation settings for robust fine-tuning. Specifically, we first\nbridge the uni-modal and multi-modal gap by performing analysis on Image\nClassification reformulated Visual Question Answering (VQA) benchmarks and\nfurther categorize ten out-of-distribution (OOD) VQA datasets by distribution\nshift types and degree (i.e. near versus far OOD). Experimental results show\nthat DiGraP consistently outperforms existing baselines across Image\nClassfication and VQA tasks with discriminative and generative backbones,\nimproving both in-distribution (ID) generalization and OOD robustness."
    },
    {
        "date": "2025-02",
        "title": "Blockchain-based Trust Management in Security Credential Management System for Vehicular Network",
        "author": "SangHyun Byun, Arijet Sarker, Sang-Yoon Chang, and Jugal Kalita",
        "link": "http://arxiv.org/abs/2502.15653v1",
        "abstract": "Cellular networking is advancing as a wireless technology to support diverse\napplications in vehicular communication, enabling vehicles to interact with\nvarious applications to enhance the driving experience, even when managed by\ndifferent authorities. Security Credential Management System (SCMS) is the\nPublic Key Infrastructure (PKI) for vehicular networking and the\nstate-of-the-art distributed PKI to protect the privacy-preserving vehicular\nnetworking against an honest-but-curious authority using multiple authorities\nand to decentralize the trust management. We build a Blockchain-Based Trust\nManagement (BBTM) to provide even greater decentralization and security.\nSpecifically, BBTM uses the blockchain to 1) replace the existing Policy\nGenerator (PG), 2) manage the policy of each authority in SCMS, 3) aggregate\nthe Global Certificate Chain File (GCCF), and 4) provide greater accountability\nand transparency on the aforementioned functionalities. We implement BBTM on\nHyperledger Fabric using a smart contract for experimentation and analyses. Our\nexperiments show that BBTM is lightweight in processing, efficient management\nin the certificate chain and ledger size, supports a bandwidth of multiple\ntransactions per second, and provides validated end-entities."
    },
    {
        "date": "2025-02",
        "title": "On the Robustness of Transformers against Context Hijacking for Linear Classification",
        "author": "Tianle Li, Chenyang Zhang, Xingwu Chen, Yuan Cao, and Difan Zou",
        "link": "http://arxiv.org/abs/2502.15609v1",
        "abstract": "Transformer-based Large Language Models (LLMs) have demonstrated powerful\nin-context learning capabilities. However, their predictions can be disrupted\nby factually correct context, a phenomenon known as context hijacking,\nrevealing a significant robustness issue. To understand this phenomenon\ntheoretically, we explore an in-context linear classification problem based on\nrecent advances in linear transformers. In our setup, context tokens are\ndesigned as factually correct query-answer pairs, where the queries are similar\nto the final query but have opposite labels. Then, we develop a general\ntheoretical analysis on the robustness of the linear transformers, which is\nformulated as a function of the model depth, training context lengths, and\nnumber of hijacking context tokens. A key finding is that a well-trained deeper\ntransformer can achieve higher robustness, which aligns with empirical\nobservations. We show that this improvement arises because deeper layers enable\nmore fine-grained optimization steps, effectively mitigating interference from\ncontext hijacking. This is also well supported by our numerical experiments.\nOur findings provide theoretical insights into the benefits of deeper\narchitectures and contribute to enhancing the understanding of transformer\narchitectures."
    },
    {
        "date": "2025-02",
        "title": "FLARE: Fault Attack Leveraging Address Reconfiguration Exploits in Multi-Tenant FPGAs",
        "author": "Jayeeta Chaudhuri, Hassan Nassar, Dennis R. E. Gnad, Jorg Henkel, Mehdi B. Tahoori, and Krishnendu Chakrabarty",
        "link": "http://arxiv.org/abs/2502.15578v1",
        "abstract": "Modern FPGAs are increasingly supporting multi-tenancy to enable dynamic\nreconfiguration of user modules. While multi-tenant FPGAs improve utilization\nand flexibility, this paradigm introduces critical security threats. In this\npaper, we present FLARE, a fault attack that exploits vulnerabilities in the\npartial reconfiguration process, specifically while a user bitstream is being\nuploaded to the FPGA by a reconfiguration manager. Unlike traditional fault\nattacks that operate during module runtime, FLARE injects faults in the\nbitstream during its reconfiguration, altering the configuration address and\nredirecting it to unintended partial reconfigurable regions (PRRs). This\nenables the overwriting of pre-configured co-tenant modules, disrupting their\nfunctionality. FLARE leverages power-wasters that activate briefly during the\nreconfiguration process, making the attack stealthy and more challenging to\ndetect with existing countermeasures. Experimental results on a Xilinx Pynq\nFPGA demonstrate the effectiveness of FLARE in compromising multiple user\nbitstreams during the reconfiguration process."
    },
    {
        "date": "2025-02",
        "title": "Context-Aware Doubly-Robust Semi-Supervised Learning",
        "author": "Clement Ruah, Houssem Sifaou, Osvaldo Simeone, and Bashir Al-Hashimi",
        "link": "http://arxiv.org/abs/2502.15577v1",
        "abstract": "The widespread adoption of artificial intelligence (AI) in next-generation\ncommunication systems is challenged by the heterogeneity of traffic and network\nconditions, which call for the use of highly contextual, site-specific, data. A\npromising solution is to rely not only on real-world data, but also on\nsynthetic pseudo-data generated by a network digital twin (NDT). However, the\neffectiveness of this approach hinges on the accuracy of the NDT, which can\nvary widely across different contexts. To address this problem, this paper\nintroduces context-aware doubly-robust (CDR) learning, a novel semi-supervised\nscheme that adapts its reliance on the pseudo-data to the different levels of\nfidelity of the NDT across contexts. CDR is evaluated on the task of downlink\nbeamforming, showing superior performance compared to previous state-of-the-art\nsemi-supervised approaches."
    },
    {
        "date": "2025-02",
        "title": "Model Privacy: A Unified Framework to Understand Model Stealing Attacks and Defenses",
        "author": "Ganghua Wang, Yuhong Yang, and Jie Ding",
        "link": "http://arxiv.org/abs/2502.15567v1",
        "abstract": "The use of machine learning (ML) has become increasingly prevalent in various\ndomains, highlighting the importance of understanding and ensuring its safety.\nOne pressing concern is the vulnerability of ML applications to model stealing\nattacks. These attacks involve adversaries attempting to recover a learned\nmodel through limited query-response interactions, such as those found in\ncloud-based services or on-chip artificial intelligence interfaces. While\nexisting literature proposes various attack and defense strategies, these often\nlack a theoretical foundation and standardized evaluation criteria. In\nresponse, this work presents a framework called ``Model Privacy'', providing a\nfoundation for comprehensively analyzing model stealing attacks and defenses.\nWe establish a rigorous formulation for the threat model and objectives,\npropose methods to quantify the goodness of attack and defense strategies, and\nanalyze the fundamental tradeoffs between utility and privacy in ML models. Our\ndeveloped theory offers valuable insights into enhancing the security of ML\nmodels, especially highlighting the importance of the attack-specific structure\nof perturbations for effective defenses. We demonstrate the application of\nmodel privacy from the defender's perspective through various learning\nscenarios. Extensive experiments corroborate the insights and the effectiveness\nof defense mechanisms developed under the proposed framework."
    },
    {
        "date": "2025-02",
        "title": "A Defensive Framework Against Adversarial Attacks on Machine Learning-Based Network Intrusion Detection Systems",
        "author": "Benyamin Tafreshian, and Shengzhi Zhang",
        "link": "http://arxiv.org/abs/2502.15561v1",
        "abstract": "As cyberattacks become increasingly sophisticated, advanced Network Intrusion\nDetection Systems (NIDS) are critical for modern network security. Traditional\nsignature-based NIDS are inadequate against zero-day and evolving attacks. In\nresponse, machine learning (ML)-based NIDS have emerged as promising solutions;\nhowever, they are vulnerable to adversarial evasion attacks that subtly\nmanipulate network traffic to bypass detection. To address this vulnerability,\nwe propose a novel defensive framework that enhances the robustness of ML-based\nNIDS by simultaneously integrating adversarial training, dataset balancing\ntechniques, advanced feature engineering, ensemble learning, and extensive\nmodel fine-tuning. We validate our framework using the NSL-KDD and UNSW-NB15\ndatasets. Experimental results show, on average, a 35% increase in detection\naccuracy and a 12.5% reduction in false positives compared to baseline models,\nparticularly under adversarial conditions. The proposed defense against\nadversarial attacks significantly advances the practical deployment of robust\nML-based NIDS in real-world networks."
    },
    {
        "date": "2025-02",
        "title": "Adversarial Prompt Evaluation: Systematic Benchmarking of Guardrails Against Prompt Input Attacks on LLMs",
        "author": "Giulio Zizzo, Giandomenico Cornacchia, Kieran Fraser, Muhammad Zaid Hameed, Ambrish Rawat, Beat Buesser, Mark Purcell, Pin-Yu Chen, Prasanna Sattigeri, and Kush Varshney",
        "link": "http://arxiv.org/abs/2502.15427v1",
        "abstract": "As large language models (LLMs) become integrated into everyday applications,\nensuring their robustness and security is increasingly critical. In particular,\nLLMs can be manipulated into unsafe behaviour by prompts known as jailbreaks.\nThe variety of jailbreak styles is growing, necessitating the use of external\ndefences known as guardrails. While many jailbreak defences have been proposed,\nnot all defences are able to handle new out-of-distribution attacks due to the\nnarrow segment of jailbreaks used to align them. Moreover, the lack of\nsystematisation around defences has created significant gaps in their practical\napplication. In this work, we perform systematic benchmarking across 15\ndifferent defences, considering a broad swathe of malicious and benign\ndatasets. We find that there is significant performance variation depending on\nthe style of jailbreak a defence is subject to. Additionally, we show that\nbased on current datasets available for evaluation, simple baselines can\ndisplay competitive out-of-distribution performance compared to many\nstate-of-the-art defences. Code is available at\nhttps://github.com/IBM/Adversarial-Prompt-Evaluation."
    },
    {
        "date": "2025-02",
        "title": "On the (In)Security of Non-resettable Device Identifiers in Custom Android Systems",
        "author": "Zikan Dong, Liu Wang, Guoai Xu, and Haoyu Wang",
        "link": "http://arxiv.org/abs/2502.15270v1",
        "abstract": "User tracking is critical in the mobile ecosystem, which relies on device\nidentifiers to build clear user profiles. In earlier ages, Android allowed easy\naccess to non-resettable device identifiers like device serial numbers and IMEI\nby third-party apps for user tracking. As privacy concerns grew, Google has\ntightened restrictions on these identifiers in native Android. Despite this,\nstakeholders in custom Android systems seek consistent and stable user tracking\ncapabilities across different system and device models, and they have\nintroduced covert channels (e.g., system properties and settings) in customized\nsystems to access identifiers, which undoubtedly increases the risk of user\nprivacy breaches. This paper examines the introduction of non-resettable\nidentifiers through system customization and their vulnerability due to poor\naccess control. We present IDRadar, a scalable and accurate approach for\nidentifying vulnerable properties and settings on custom Android ROMs. Applying\nour approach to 1,814 custom ROMs, we have identified 8,192 system properties\nand 3,620 settings that store non-resettable identifiers, with 3,477 properties\nand 1,336 settings lacking adequate access control, which can be abused by\nthird-party apps to track users without permissions. Our large-scale analysis\ncan identify a large number of security issues which are two orders of\nmagnitude greater than existing techniques. We further investigate the root\ncauses of these access control deficiencies. Validation on 32 devices through\nthe remote testing service confirmed our results. Additionally, we observe that\nthe vulnerable properties and settings occur in devices of the same OEMs. We\nhave reported our findings to the vendors and received positive confirmations.\nOur work underscores the need for greater scrutiny of covert access channels to\ndevice identifiers and better solutions to safeguard user privacy."
    },
    {
        "date": "2025-02",
        "title": "Interpreting Adversarial Attacks and Defences using Architectures with Enhanced Interpretability",
        "author": "Akshay G Rao, Chandrashekhar Lakshminarayanan, and Arun Rajkumar",
        "link": "http://arxiv.org/abs/2502.15017v1",
        "abstract": "Adversarial attacks in deep learning represent a significant threat to the\nintegrity and reliability of machine learning models. Adversarial training has\nbeen a popular defence technique against these adversarial attacks. In this\nwork, we capitalize on a network architecture, namely Deep Linearly Gated\nNetworks (DLGN), which has better interpretation capabilities than regular deep\nnetwork architectures. Using this architecture, we interpret robust models\ntrained using PGD adversarial training and compare them with standard training.\nFeature networks in DLGN act as feature extractors, making them the only medium\nthrough which an adversary can attack the model. We analyze the feature network\nof DLGN with fully connected layers with respect to properties like alignment\nof the hyperplanes, hyperplane relation with PCA, and sub-network overlap among\nclasses and compare these properties between robust and standard models. We\nalso consider this architecture having CNN layers wherein we qualitatively\n(using visualizations) and quantitatively contrast gating patterns between\nrobust and standard models. We uncover insights into hyperplanes resembling\nprincipal components in PGD-AT and STD-TR models, with PGD-AT hyperplanes\naligned farther from the data points. We use path activity analysis to show\nthat PGD-AT models create diverse, non-overlapping active subnetworks across\nclasses, preventing attack-induced gating overlaps. Our visualization ideas\nshow the nature of representations learnt by PGD-AT and STD-TR models."
    },
    {
        "date": "2025-02",
        "title": "EigenShield: Causal Subspace Filtering via Random Matrix Theory for Adversarially Robust Vision-Language Models",
        "author": "Nastaran Darabi, Devashri Naik, Sina Tayebati, Dinithi Jayasuriya, Ranganath Krishnan, and Amit Ranjan Trivedi",
        "link": "http://arxiv.org/abs/2502.14976v1",
        "abstract": "Vision-Language Models (VLMs) inherit adversarial vulnerabilities of Large\nLanguage Models (LLMs), which are further exacerbated by their multimodal\nnature. Existing defenses, including adversarial training, input\ntransformations, and heuristic detection, are computationally expensive,\narchitecture-dependent, and fragile against adaptive attacks. We introduce\nEigenShield, an inference-time defense leveraging Random Matrix Theory to\nquantify adversarial disruptions in high-dimensional VLM representations.\nUnlike prior methods that rely on empirical heuristics, EigenShield employs the\nspiked covariance model to detect structured spectral deviations. Using a\nRobustness-based Nonconformity Score (RbNS) and quantile-based thresholding, it\nseparates causal eigenvectors, which encode semantic information, from\ncorrelational eigenvectors that are susceptible to adversarial artifacts. By\nprojecting embeddings onto the causal subspace, EigenShield filters adversarial\nnoise without modifying model parameters or requiring adversarial training.\nThis architecture-independent, attack-agnostic approach significantly reduces\nthe attack success rate, establishing spectral analysis as a principled\nalternative to conventional defenses. Our results demonstrate that EigenShield\nconsistently outperforms all existing defenses, including adversarial training,\nUNIGUARD, and CIDER."
    },
    {
        "date": "2025-02",
        "title": "CyberSentinel: An Emergent Threat Detection System for AI Security",
        "author": "Krti Tallam",
        "link": "http://arxiv.org/abs/2502.14966v1",
        "abstract": "The rapid advancement of artificial intelligence (AI) has significantly\nexpanded the attack surface for AI-driven cybersecurity threats, necessitating\nadaptive defense strategies. This paper introduces CyberSentinel, a unified,\nsingle-agent system for emergent threat detection, designed to identify and\nmitigate novel security risks in real time. CyberSentinel integrates: (1)\nBrute-force attack detection through SSH log analysis, (2) Phishing threat\nassessment using domain blacklists and heuristic URL scoring, and (3) Emergent\nthreat detection via machine learning-based anomaly detection. By continuously\nadapting to evolving adversarial tactics, CyberSentinel strengthens proactive\ncybersecurity defense, addressing critical vulnerabilities in AI security."
    },
    {
        "date": "2025-02",
        "title": "Red-Teaming LLM Multi-Agent Systems via Communication Attacks",
        "author": "Pengfei He, Yupin Lin, Shen Dong, Han Xu, Yue Xing, and Hui Liu",
        "link": "http://arxiv.org/abs/2502.14847v1",
        "abstract": "Large Language Model-based Multi-Agent Systems (LLM-MAS) have revolutionized\ncomplex problem-solving capability by enabling sophisticated agent\ncollaboration through message-based communications. While the communication\nframework is crucial for agent coordination, it also introduces a critical yet\nunexplored security vulnerability. In this work, we introduce\nAgent-in-the-Middle (AiTM), a novel attack that exploits the fundamental\ncommunication mechanisms in LLM-MAS by intercepting and manipulating\ninter-agent messages. Unlike existing attacks that compromise individual\nagents, AiTM demonstrates how an adversary can compromise entire multi-agent\nsystems by only manipulating the messages passing between agents. To enable the\nattack under the challenges of limited control and role-restricted\ncommunication format, we develop an LLM-powered adversarial agent with a\nreflection mechanism that generates contextually-aware malicious instructions.\nOur comprehensive evaluation across various frameworks, communication\nstructures, and real-world applications demonstrates that LLM-MAS is vulnerable\nto communication-based attacks, highlighting the need for robust security\nmeasures in multi-agent systems."
    },
    {
        "date": "2025-02",
        "title": "Probabilistic Robustness in Deep Learning: A Concise yet Comprehensive Guide",
        "author": "Xingyu Zhao",
        "link": "http://arxiv.org/abs/2502.14833v1",
        "abstract": "Deep learning (DL) has demonstrated significant potential across various\nsafety-critical applications, yet ensuring its robustness remains a key\nchallenge. While adversarial robustness has been extensively studied in\nworst-case scenarios, probabilistic robustness (PR) offers a more practical\nperspective by quantifying the likelihood of failures under stochastic\nperturbations. This paper provides a concise yet comprehensive overview of PR,\ncovering its formal definitions, evaluation and enhancement methods. We\nintroduce a reformulated ''min-max'' optimisation framework for adversarial\ntraining specifically designed to improve PR. Furthermore, we explore the\nintegration of PR verification evidence into system-level safety assurance,\naddressing challenges in translating DL model-level robustness to system-level\nclaims. Finally, we highlight open research questions, including benchmarking\nPR evaluation methods, extending PR to generative AI tasks, and developing\nrigorous methodologies and case studies for system-level integration."
    },
    {
        "date": "2025-02",
        "title": "An Adversarial Analysis of Thompson Sampling for Full-information Online Learning: from Finite to Infinite Action Spaces",
        "author": "Alexander Terenin, and Jeffrey Negrea",
        "link": "http://arxiv.org/abs/2502.14790v3",
        "abstract": "We develop an analysis of Thompson sampling for online learning under full\nfeedback - also known as prediction with expert advice - where the learner's\nprior is defined over the space of an adversary's future actions, rather than\nthe space of experts. We show regret decomposes into regret the learner\nexpected a priori, plus a prior-robustness-type term we call excess regret. In\nthe classical finite-expert setting, this recovers optimal rates. As an initial\nstep towards practical online learning in settings with a\npotentially-uncountably-infinite number of experts, we show that Thompson\nsampling with a certain Gaussian process prior widely-used in the Bayesian\noptimization literature has a $\\mathcal{O}(\\beta\\sqrt{T\\log(1+\\lambda)})$ rate\nagainst a $\\beta$-bounded $\\lambda$-Lipschitz adversary."
    },
    {
        "date": "2025-02",
        "title": "Efficient Multivariate Robust Mean Estimation Under Mean-Shift Contamination",
        "author": "Ilias Diakonikolas, Giannis Iakovidis, Daniel M. Kane, and Thanasis Pittas",
        "link": "http://arxiv.org/abs/2502.14772v1",
        "abstract": "We study the algorithmic problem of robust mean estimation of an identity\ncovariance Gaussian in the presence of mean-shift contamination. In this\ncontamination model, we are given a set of points in $\\mathbb{R}^d$ generated\ni.i.d. via the following process. For a parameter $\\alpha<1/2$, the $i$-th\nsample $x_i$ is obtained as follows: with probability $1-\\alpha$, $x_i$ is\ndrawn from $\\mathcal{N}(\\mu, I)$, where $\\mu \\in \\mathbb{R}^d$ is the target\nmean; and with probability $\\alpha$, $x_i$ is drawn from $\\mathcal{N}(z_i, I)$,\nwhere $z_i$ is unknown and potentially arbitrary. Prior work characterized the\ninformation-theoretic limits of this task. Specifically, it was shown that, in\ncontrast to Huber contamination, in the presence of mean-shift contamination\nconsistent estimation is possible. On the other hand, all known robust\nestimators in the mean-shift model have running times exponential in the\ndimension. Here we give the first computationally efficient algorithm for\nhigh-dimensional robust mean estimation with mean-shift contamination that can\ntolerate a constant fraction of outliers. In particular, our algorithm has\nnear-optimal sample complexity, runs in sample-polynomial time, and\napproximates the target mean to any desired accuracy. Conceptually, our result\ncontributes to a growing body of work that studies inference with respect to\nnatural noise models lying in between fully adversarial and random settings."
    },
    {
        "date": "2025-02",
        "title": "Moshi Moshi? A Model Selection Hijacking Adversarial Attack",
        "author": "Riccardo Petrucci, Luca Pajola, Francesco Marchiori, Luca Pasa, and Mauro conti",
        "link": "http://arxiv.org/abs/2502.14586v1",
        "abstract": "Model selection is a fundamental task in Machine Learning~(ML), focusing on\nselecting the most suitable model from a pool of candidates by evaluating their\nperformance on specific metrics. This process ensures optimal performance,\ncomputational efficiency, and adaptability to diverse tasks and environments.\nDespite its critical role, its security from the perspective of adversarial ML\nremains unexplored. This risk is heightened in the\nMachine-Learning-as-a-Service model, where users delegate the training phase\nand the model selection process to third-party providers, supplying data and\ntraining strategies. Therefore, attacks on model selection could harm both the\nuser and the provider, undermining model performance and driving up operational\ncosts.\n  In this work, we present MOSHI (MOdel Selection HIjacking adversarial\nattack), the first adversarial attack specifically targeting model selection.\nOur novel approach manipulates model selection data to favor the adversary,\neven without prior knowledge of the system. Utilizing a framework based on\nVariational Auto Encoders, we provide evidence that an attacker can induce\ninefficiencies in ML deployment. We test our attack on diverse computer vision\nand speech recognition benchmark tasks and different settings, obtaining an\naverage attack success rate of 75.42%. In particular, our attack causes an\naverage 88.30% decrease in generalization capabilities, an 83.33% increase in\nlatency, and an increase of up to 105.85% in energy consumption. These results\nhighlight the significant vulnerabilities in model selection processes and\ntheir potential impact on real-world applications."
    },
    {
        "date": "2025-02",
        "title": "Self-supervised Monocular Depth Estimation Robust to Reflective Surface Leveraged by Triplet Mining",
        "author": "Wonhyeok Choi, Kyumin Hwang, Wei Peng, Minwoo Choi, and Sunghoon Im",
        "link": "http://arxiv.org/abs/2502.14573v1",
        "abstract": "Self-supervised monocular depth estimation (SSMDE) aims to predict the dense\ndepth map of a monocular image, by learning depth from RGB image sequences,\neliminating the need for ground-truth depth labels. Although this approach\nsimplifies data acquisition compared to supervised methods, it struggles with\nreflective surfaces, as they violate the assumptions of Lambertian reflectance,\nleading to inaccurate training on such surfaces. To tackle this problem, we\npropose a novel training strategy for an SSMDE by leveraging triplet mining to\npinpoint reflective regions at the pixel level, guided by the camera geometry\nbetween different viewpoints. The proposed reflection-aware triplet mining loss\nspecifically penalizes the inappropriate photometric error minimization on the\nlocalized reflective regions while preserving depth accuracy in non-reflective\nareas. We also incorporate a reflection-aware knowledge distillation method\nthat enables a student model to selectively learn the pixel-level knowledge\nfrom reflective and non-reflective regions. This results in robust depth\nestimation across areas. Evaluation results on multiple datasets demonstrate\nthat our method effectively enhances depth quality on reflective surfaces and\noutperforms state-of-the-art SSMDE baselines."
    },
    {
        "date": "2025-02",
        "title": "FUIA: Model Inversion Attack against Federated Unlearning",
        "author": "Lei Zhou, and Youwen Zhu",
        "link": "http://arxiv.org/abs/2502.14558v1",
        "abstract": "With the introduction of regulations related to the ``right to be forgotten\",\nfederated learning (FL) is facing new privacy compliance challenges. To address\nthese challenges, researchers have proposed federated unlearning (FU). However,\nexisting FU research has primarily focused on improving the efficiency of\nunlearning, with less attention paid to the potential privacy vulnerabilities\ninherent in these methods. To address this gap, we draw inspiration from\ngradient inversion attacks in FL and propose the federated unlearning inversion\nattack (FUIA). The FUIA is specifically designed for the three types of FU\n(sample unlearning, client unlearning, and class unlearning), aiming to provide\na comprehensive analysis of the privacy leakage risks associated with FU. In\nFUIA, the server acts as an honest-but-curious attacker, recording and\nexploiting the model differences before and after unlearning to expose the\nfeatures and labels of forgotten data. FUIA significantly leaks the privacy of\nforgotten data and can target all types of FU. This attack contradicts the goal\nof FU to eliminate specific data influence, instead exploiting its\nvulnerabilities to recover forgotten data and expose its privacy flaws.\nExtensive experimental results show that FUIA can effectively reveal the\nprivate information of forgotten data. To mitigate this privacy leakage, we\nalso explore two potential defense methods, although these come at the cost of\nreduced unlearning effectiveness and the usability of the unlearned model."
    },
    {
        "date": "2025-02",
        "title": "Soft Token Attacks Cannot Reliably Audit Unlearning in Large Language Models",
        "author": "Haokun Chen, Sebastian Szyller, Weilin Xu, and Nageen Himayat",
        "link": "http://arxiv.org/abs/2502.15836v1",
        "abstract": "Large language models (LLMs) have become increasingly popular. Their emergent\ncapabilities can be attributed to their massive training datasets. However,\nthese datasets often contain undesirable or inappropriate content, e.g.,\nharmful texts, personal information, and copyrighted material. This has\npromoted research into machine unlearning that aims to remove information from\ntrained models. In particular, approximate unlearning seeks to achieve\ninformation removal by strategically editing the model rather than complete\nmodel retraining.\n  Recent work has shown that soft token attacks (STA) can successfully extract\npurportedly unlearned information from LLMs, thereby exposing limitations in\ncurrent unlearning methodologies. In this work, we reveal that STAs are an\ninadequate tool for auditing unlearning. Through systematic evaluation on\ncommon unlearning benchmarks (Who Is Harry Potter? and TOFU), we demonstrate\nthat such attacks can elicit any information from the LLM, regardless of (1)\nthe deployed unlearning algorithm, and (2) whether the queried content was\noriginally present in the training corpus. Furthermore, we show that STA with\njust a few soft tokens (1-10) can elicit random strings over 400-characters\nlong. Thus showing that STAs are too powerful, and misrepresent the\neffectiveness of the unlearning methods.\n  Our work highlights the need for better evaluation baselines, and more\nappropriate auditing tools for assessing the effectiveness of unlearning in\nLLMs."
    },
    {
        "date": "2025-02",
        "title": "CORBA: Contagious Recursive Blocking Attacks on Multi-Agent Systems Based on Large Language Models",
        "author": "Zhenhong Zhou, Zherui Li, Jie Zhang, Yuanhe Zhang, Kun Wang, Yang Liu, and Qing Guo",
        "link": "http://arxiv.org/abs/2502.14529v1",
        "abstract": "Large Language Model-based Multi-Agent Systems (LLM-MASs) have demonstrated\nremarkable real-world capabilities, effectively collaborating to complete\ncomplex tasks. While these systems are designed with safety mechanisms, such as\nrejecting harmful instructions through alignment, their security remains\nlargely unexplored. This gap leaves LLM-MASs vulnerable to targeted\ndisruptions. In this paper, we introduce Contagious Recursive Blocking Attacks\n(Corba), a novel and simple yet highly effective attack that disrupts\ninteractions between agents within an LLM-MAS. Corba leverages two key\nproperties: its contagious nature allows it to propagate across arbitrary\nnetwork topologies, while its recursive property enables sustained depletion of\ncomputational resources. Notably, these blocking attacks often involve\nseemingly benign instructions, making them particularly challenging to mitigate\nusing conventional alignment methods. We evaluate Corba on two widely-used\nLLM-MASs, namely, AutoGen and Camel across various topologies and commercial\nmodels. Additionally, we conduct more extensive experiments in open-ended\ninteractive LLM-MASs, demonstrating the effectiveness of Corba in complex\ntopology structures and open-source models. Our code is available at:\nhttps://github.com/zhrli324/Corba."
    },
    {
        "date": "2025-02",
        "title": "Generative adversarial networks vs large language models: a comparative study on synthetic tabular data generation",
        "author": "Austin A. Barr, Robert Rozman, and Eddie Guo",
        "link": "http://arxiv.org/abs/2502.14523v1",
        "abstract": "We propose a new framework for zero-shot generation of synthetic tabular\ndata. Using the large language model (LLM) GPT-4o and plain-language prompting,\nwe demonstrate the ability to generate high-fidelity tabular data without\ntask-specific fine-tuning or access to real-world data (RWD) for pre-training.\nTo benchmark GPT-4o, we compared the fidelity and privacy of LLM-generated\nsynthetic data against data generated with the conditional tabular generative\nadversarial network (CTGAN), across three open-access datasets: Iris, Fish\nMeasurements, and Real Estate Valuation. Despite the zero-shot approach, GPT-4o\noutperformed CTGAN in preserving means, 95% confidence intervals, bivariate\ncorrelations, and data privacy of RWD, even at amplified sample sizes. Notably,\ncorrelations between parameters were consistently preserved with appropriate\ndirection and strength. However, refinement is necessary to better retain\ndistributional characteristics. These findings highlight the potential of LLMs\nin tabular data synthesis, offering an accessible alternative to generative\nadversarial networks and variational autoencoders."
    },
    {
        "date": "2025-02",
        "title": "How Jailbreak Defenses Work and Ensemble? A Mechanistic Investigation",
        "author": "Zhuohang Long, Siyuan Wang, Shujun Liu, Yuhang Lai, Xuanjing Huang, and Zhongyu Wei",
        "link": "http://arxiv.org/abs/2502.14486v1",
        "abstract": "Jailbreak attacks, where harmful prompts bypass generative models' built-in\nsafety, raise serious concerns about model vulnerability. While many defense\nmethods have been proposed, the trade-offs between safety and helpfulness, and\ntheir application to Large Vision-Language Models (LVLMs), are not well\nunderstood. This paper systematically examines jailbreak defenses by reframing\nthe standard generation task as a binary classification problem to assess model\nrefusal tendencies for both harmful and benign queries. We identify two key\ndefense mechanisms: safety shift, which increases refusal rates across all\nqueries, and harmfulness discrimination, which improves the model's ability to\ndistinguish between harmful and benign inputs. Using these mechanisms, we\ndevelop two ensemble defense strategies-inter-mechanism ensembles and\nintra-mechanism ensembles-to balance safety and helpfulness. Experiments on the\nMM-SafetyBench and MOSSBench datasets with LLaVA-1.5 models show that these\nstrategies effectively improve model safety or optimize the trade-off between\nsafety and helpfulness."
    },
    {
        "date": "2025-02",
        "title": "Generalization Certificates for Adversarially Robust Bayesian Linear Regression",
        "author": "Mahalakshmi Sabanayagam, Russell Tsuchida, Cheng Soon Ong, and Debarghya Ghoshdastidar",
        "link": "http://arxiv.org/abs/2502.14298v1",
        "abstract": "Adversarial robustness of machine learning models is critical to ensuring\nreliable performance under data perturbations. Recent progress has been on\npoint estimators, and this paper considers distributional predictors. First,\nusing the link between exponential families and Bregman divergences, we\nformulate an adversarial Bregman divergence loss as an adversarial negative\nlog-likelihood. Using the geometric properties of Bregman divergences, we\ncompute the adversarial perturbation for such models in closed-form. Second,\nunder such losses, we introduce \\emph{adversarially robust posteriors}, by\nexploiting the optimization-centric view of generalized Bayesian inference.\nThird, we derive the \\emph{first} rigorous generalization certificates in the\ncontext of an adversarial extension of Bayesian linear regression by leveraging\nthe PAC-Bayesian framework. Finally, experiments on real and synthetic datasets\ndemonstrate the superior robustness of the derived adversarially robust\nposterior over Bayes posterior, and also validate our theoretical guarantees."
    },
    {
        "date": "2025-02",
        "title": "Towards Secure Program Partitioning for Smart Contracts with LLM's In-Context Learning",
        "author": "Ye Liu, Yuqing Niu, Chengyan Ma, Ruidong Han, Wei Ma, Yi Li, Debin Gao, and David Lo",
        "link": "http://arxiv.org/abs/2502.14215v1",
        "abstract": "Smart contracts are highly susceptible to manipulation attacks due to the\nleakage of sensitive information. Addressing manipulation vulnerabilities is\nparticularly challenging because they stem from inherent data confidentiality\nissues rather than straightforward implementation bugs. To tackle this by\npreventing sensitive information leakage, we present PartitionGPT, the first\nLLM-driven approach that combines static analysis with the in-context learning\ncapabilities of large language models (LLMs) to partition smart contracts into\nprivileged and normal codebases, guided by a few annotated sensitive data\nvariables. We evaluated PartitionGPT on 18 annotated smart contracts containing\n99 sensitive functions. The results demonstrate that PartitionGPT successfully\ngenerates compilable, and verified partitions for 78% of the sensitive\nfunctions while reducing approximately 30% code compared to function-level\npartitioning approach. Furthermore, we evaluated PartitionGPT on nine\nreal-world manipulation attacks that lead to a total loss of 25 million\ndollars, PartitionGPT effectively prevents eight cases, highlighting its\npotential for broad applicability and the necessity for secure program\npartitioning during smart contract development to diminish manipulation\nvulnerabilities."
    },
    {
        "date": "2025-02",
        "title": "Towards Robust ESG Analysis Against Greenwashing Risks: Aspect-Action Analysis with Cross-Category Generalization",
        "author": "Keane Ong, Rui Mao, Deeksha Varshney, Erik Cambria, and Gianmarco Mengaldo",
        "link": "http://arxiv.org/abs/2502.15821v1",
        "abstract": "Sustainability reports are key for evaluating companies' environmental,\nsocial and governance, ESG performance, but their content is increasingly\nobscured by greenwashing - sustainability claims that are misleading,\nexaggerated, and fabricated. Yet, existing NLP approaches for ESG analysis lack\nrobustness against greenwashing risks, often extracting insights that reflect\nmisleading or exaggerated sustainability claims rather than objective ESG\nperformance. To bridge this gap, we introduce A3CG - Aspect-Action Analysis\nwith Cross-Category Generalization, as a novel dataset to improve the\nrobustness of ESG analysis amid the prevalence of greenwashing. By explicitly\nlinking sustainability aspects with their associated actions, A3CG facilitates\na more fine-grained and transparent evaluation of sustainability claims,\nensuring that insights are grounded in verifiable actions rather than vague or\nmisleading rhetoric. Additionally, A3CG emphasizes cross-category\ngeneralization. This ensures robust model performance in aspect-action analysis\neven when companies change their reports to selectively favor certain\nsustainability areas. Through experiments on A3CG, we analyze state-of-the-art\nsupervised models and LLMs, uncovering their limitations and outlining key\ndirections for future research."
    },
    {
        "date": "2025-02",
        "title": "Do LLMs Consider Security? An Empirical Study on Responses to Programming Questions",
        "author": "Amirali Sajadi, Binh Le, Anh Nguyen, Kostadin Damevski, and Preetha Chatterjee",
        "link": "http://arxiv.org/abs/2502.14202v1",
        "abstract": "The widespread adoption of conversational LLMs for software development has\nraised new security concerns regarding the safety of LLM-generated content. Our\nmotivational study outlines ChatGPT's potential in volunteering\ncontext-specific information to the developers, promoting safe coding\npractices. Motivated by this finding, we conduct a study to evaluate the degree\nof security awareness exhibited by three prominent LLMs: Claude 3, GPT-4, and\nLlama 3. We prompt these LLMs with Stack Overflow questions that contain\nvulnerable code to evaluate whether they merely provide answers to the\nquestions or if they also warn users about the insecure code, thereby\ndemonstrating a degree of security awareness. Further, we assess whether LLM\nresponses provide information about the causes, exploits, and the potential\nfixes of the vulnerability, to help raise users' awareness. Our findings show\nthat all three models struggle to accurately detect and warn users about\nvulnerabilities, achieving a detection rate of only 12.6% to 40% across our\ndatasets. We also observe that the LLMs tend to identify certain types of\nvulnerabilities related to sensitive information exposure and improper input\nneutralization much more frequently than other types, such as those involving\nexternal control of file names or paths. Furthermore, when LLMs do issue\nsecurity warnings, they often provide more information on the causes, exploits,\nand fixes of vulnerabilities compared to Stack Overflow responses. Finally, we\nprovide an in-depth discussion on the implications of our findings and present\na CLI-based prompting tool that can be used to generate significantly more\nsecure LLM responses."
    },
    {
        "date": "2025-02",
        "title": "Conformal Prediction under L\u00e9vy-Prokhorov Distribution Shifts: Robustness to Local and Global Perturbations",
        "author": "Liviu Aolaritei, Michael I. Jordan, Youssef Marzouk, Zheyu Oliver Wang, and Julie Zhu",
        "link": "http://arxiv.org/abs/2502.14105v1",
        "abstract": "Conformal prediction provides a powerful framework for constructing\nprediction intervals with finite-sample guarantees, yet its robustness under\ndistribution shifts remains a significant challenge. This paper addresses this\nlimitation by modeling distribution shifts using L\\'evy-Prokhorov (LP)\nambiguity sets, which capture both local and global perturbations. We provide a\nself-contained overview of LP ambiguity sets and their connections to popular\nmetrics such as Wasserstein and Total Variation. We show that the link between\nconformal prediction and LP ambiguity sets is a natural one: by propagating the\nLP ambiguity set through the scoring function, we reduce complex\nhigh-dimensional distribution shifts to manageable one-dimensional distribution\nshifts, enabling exact quantification of worst-case quantiles and coverage.\nBuilding on this analysis, we construct robust conformal prediction intervals\nthat remain valid under distribution shifts, explicitly linking LP parameters\nto interval width and confidence levels. Experimental results on real-world\ndatasets demonstrate the effectiveness of the proposed approach."
    },
    {
        "date": "2025-02",
        "title": "Cyber security of OT networks: A tutorial and overview",
        "author": "Sumit Kumar, and Harsh Vardhan",
        "link": "http://arxiv.org/abs/2502.14017v1",
        "abstract": "This manuscript explores the cybersecurity challenges of Operational\nTechnology (OT) networks, focusing on their critical role in industrial\nenvironments such as manufacturing, energy, and utilities. As OT systems\nincreasingly integrate with Information Technology (IT) systems due to Industry\n4.0 initiatives, they become more vulnerable to cyberattacks, which pose risks\nnot only to data but also to physical infrastructure. The study examines key\ncomponents of OT systems, such as SCADA (Supervisory Control and Data\nAcquisition), PLCs (Programmable Logic Controllers), and RTUs (Remote Terminal\nUnits), and analyzes recent cyberattacks targeting OT environments.\nFurthermore, it highlights the security concerns arising from the convergence\nof IT and OT systems, examining attack vectors and the growing threats posed by\nmalware, ransomware, and nation-state actors. Finally, the paper discusses\nmodern approaches and tools used to secure these environments, providing\ninsights into improving the cybersecurity posture of OT networks."
    },
    {
        "date": "2025-02",
        "title": "The Round Complexity of Black-Box Post-Quantum Secure Computation",
        "author": "Rohit Chatterjee, Xiao Liang, Omkant Pandey, and Takashi Yamakawa",
        "link": "http://arxiv.org/abs/2502.13830v1",
        "abstract": "We study the round complexity of secure multi-party computation (MPC) in the\npost-quantum regime. Our focus is on the fully black-box setting, where both\nthe construction and security reduction are black-box. Chia, Chung, Liu, and\nYamakawa [FOCS'22] demonstrated the infeasibility of achieving standard\nsimulation-based security within constant rounds unless $\\mathbf{NP} \\subseteq\n\\mathbf{BQP}$. This leaves crucial feasibility questions unresolved.\nSpecifically, it remains unknown whether black-box constructions are achievable\nwithin polynomial rounds; also, the existence of constant-round constructions\nwith respect to $\\epsilon$-simulation, a relaxed yet useful alternative to\nstandard simulation, remains unestablished.\n  This work provides positive answers. We introduce the first black-box\nconstruction for PQ-MPC in polynomial rounds, from the minimal assumption of\npost-quantum semi-honest oblivious transfers. In the two-party scenario, our\nconstruction requires only $\\omega(1)$ rounds. These results have already been\napplied in the oracle separation between classical-communication quantum MPC\nand $\\mathbf{P} = \\mathbf{NP}$ in Kretschmer, Qian, and Tal [STOC'25].\n  As for $\\epsilon$-simulation, Chia, Chung, Liang, and Yamakawa [CRYPTO'22]\nresolved the issue for the two-party setting, leaving the multi-party case\nopen. We complete the picture by presenting the first black-box, constant-round\nconstruction in the multi-party setting, instantiable using various standard\npost-quantum primitives.\n  En route, we obtain a black-box, constant-round post-quantum commitment\nachieving a weaker version of 1-many non-malleability, from post-quantum\none-way functions. Besides its role in our MPC construction, this commitment\nalso reduces the assumption used in the quantum parallel repetition lower bound\nby Bostanci, Qian, Spooner, and Yuen [STOC'24]. We anticipate further\napplications in the future."
    },
    {
        "date": "2025-02",
        "title": "Display Field-Of-View Agnostic Robust CT Kernel Synthesis Using Model-Based Deep Learning",
        "author": "Hemant Kumar Aggarwal, Antony Jerald, Phaneendra K. Yalavarthy, Rajesh Langoju, and Bipul Das",
        "link": "http://arxiv.org/abs/2502.14920v1",
        "abstract": "In X-ray computed tomography (CT) imaging, the choice of reconstruction\nkernel is crucial as it significantly impacts the quality of clinical images.\nDifferent kernels influence spatial resolution, image noise, and contrast in\nvarious ways. Clinical applications involving lung imaging often require images\nreconstructed with both soft and sharp kernels. The reconstruction of images\nwith different kernels requires raw sinogram data and storing images for all\nkernels increases processing time and storage requirements. The Display\nField-of-View (DFOV) adds complexity to kernel synthesis, as data acquired at\ndifferent DFOVs exhibit varying levels of sharpness and details. This work\nintroduces an efficient, DFOV-agnostic solution for image-based kernel\nsynthesis using model-based deep learning. The proposed method explicitly\nintegrates CT kernel and DFOV characteristics into the forward model.\nExperimental results on clinical data, along with quantitative analysis of the\nestimated modulation transfer function using wire phantom data, clearly\ndemonstrate the utility of the proposed method in real-time. Additionally, a\ncomparative study with a direct learning network, that lacks forward model\ninformation, shows that the proposed method is more robust to DFOV variations."
    },
    {
        "date": "2025-02",
        "title": "RobustX: Robust Counterfactual Explanations Made Easy",
        "author": "Junqi Jiang, Luca Marzari, Aaryan Purohit, and Francesco Leofante",
        "link": "http://arxiv.org/abs/2502.13751v1",
        "abstract": "The increasing use of Machine Learning (ML) models to aid decision-making in\nhigh-stakes industries demands explainability to facilitate trust.\nCounterfactual Explanations (CEs) are ideally suited for this, as they can\noffer insights into the predictions of an ML model by illustrating how changes\nin its input data may lead to different outcomes. However, for CEs to realise\ntheir explanatory potential, significant challenges remain in ensuring their\nrobustness under slight changes in the scenario being explained. Despite the\nwidespread recognition of CEs' robustness as a fundamental requirement, a lack\nof standardised tools and benchmarks hinders a comprehensive and effective\ncomparison of robust CE generation methods. In this paper, we introduce\nRobustX, an open-source Python library implementing a collection of CE\ngeneration and evaluation methods, with a focus on the robustness property.\nRobustX provides interfaces to several existing methods from the literature,\nenabling streamlined access to state-of-the-art techniques. The library is also\neasily extensible, allowing fast prototyping of novel robust CE generation and\nevaluation methods."
    },
    {
        "date": "2025-02",
        "title": "Robust Counterfactual Inference in Markov Decision Processes",
        "author": "Jessica Lally, Milad Kazemi, and Nicola Paoletti",
        "link": "http://arxiv.org/abs/2502.13731v1",
        "abstract": "This paper addresses a key limitation in existing counterfactual inference\nmethods for Markov Decision Processes (MDPs). Current approaches assume a\nspecific causal model to make counterfactuals identifiable. However, there are\nusually many causal models that align with the observational and interventional\ndistributions of an MDP, each yielding different counterfactual distributions,\nso fixing a particular causal model limits the validity (and usefulness) of\ncounterfactual inference. We propose a novel non-parametric approach that\ncomputes tight bounds on counterfactual transition probabilities across all\ncompatible causal models. Unlike previous methods that require solving\nprohibitively large optimisation problems (with variables that grow\nexponentially in the size of the MDP), our approach provides closed-form\nexpressions for these bounds, making computation highly efficient and scalable\nfor non-trivial MDPs. Once such an interval counterfactual MDP is constructed,\nour method identifies robust counterfactual policies that optimise the\nworst-case reward w.r.t. the uncertain interval MDP probabilities. We evaluate\nour method on various case studies, demonstrating improved robustness over\nexisting methods."
    },
    {
        "date": "2025-02",
        "title": "Secure Federated Data Distillation",
        "author": "Marco Arazzi, Mert Cihangiroglu, Serena Nicolazzo, and Antonino Nocera",
        "link": "http://arxiv.org/abs/2502.13728v1",
        "abstract": "Dataset Distillation (DD) is a powerful technique for reducing large datasets\ninto compact, representative synthetic datasets, accelerating Machine Learning\ntraining. However, traditional DD methods operate in a centralized manner,\nwhich poses significant privacy threats and reduces its applicability. To\nmitigate these risks, we propose a Secure Federated Data Distillation framework\n(SFDD) to decentralize the distillation process while preserving privacy.Unlike\nexisting Federated Distillation techniques that focus on training global models\nwith distilled knowledge, our approach aims to produce a distilled dataset\nwithout exposing local contributions. We leverage the gradient-matching-based\ndistillation method, adapting it for a distributed setting where clients\ncontribute to the distillation process without sharing raw data. The central\naggregator iteratively refines a synthetic dataset by integrating client-side\nupdates while ensuring data confidentiality. To make our approach resilient to\ninference attacks perpetrated by the server that could exploit gradient updates\nto reconstruct private data, we create an optimized Local Differential Privacy\napproach, called LDPO-RLD (Label Differential Privacy Obfuscation via\nRandomized Linear Dispersion). Furthermore, we assess the framework's\nresilience against malicious clients executing backdoor attacks and demonstrate\nrobustness under the assumption of a sufficient number of participating\nclients. Our experimental results demonstrate the effectiveness of SFDD and\nthat the proposed defense concretely mitigates the identified vulnerabilities,\nwith minimal impact on the performance of the distilled dataset. By addressing\nthe interplay between privacy and federation in dataset distillation, this work\nadvances the field of privacy-preserving Machine Learning making our SFDD\nframework a viable solution for sensitive data-sharing applications."
    },
    {
        "date": "2025-02",
        "title": "What Skills Do Cyber Security Professionals Need?",
        "author": "Faheem Ullah, Xiaohan Ye, Uswa Fatima, Zahid Akhtar, Yuxi Wu, and Hussain Ahmad",
        "link": "http://arxiv.org/abs/2502.13658v2",
        "abstract": "Purpose: The increasing number of cyber-attacks has elevated the importance\nof cybersecurity for organizations. This has also increased the demand for\nprofessionals with the necessary skills to protect these organizations. As a\nresult, many individuals are looking to enter the field of cybersecurity.\nHowever, there is a lack of clear understanding of the skills required for a\nsuccessful career in this field. In this paper, we identify the skills required\nfor cybersecurity professionals. We also determine how the demand for cyber\nskills relates to various cyber roles such as security analyst and security\narchitect. Furthermore, we identify the programming languages that are\nimportant for cybersecurity professionals. Design/Methodology: For this study,\nwe have collected and analyzed data from 12,161 job ads and 49,002 Stack\nOverflow posts. By examining this, we identified patterns and trends related to\nskill requirements, role-specific demands, and programming languages in\ncybersecurity. Findings: Our results reveal that (i) communication skills and\nproject management skills are the most important soft skills, (ii) as compared\nto soft skills, the demand for technical skills varies more across various\ncyber roles, and (iii) Java is the most commonly used programming language.\nOriginality: Our findings serve as a guideline for individuals aiming to get\ninto the field of cybersecurity. Moreover, our findings are useful in terms of\ninforming educational institutes to teach the correct set of skills to students\ndoing degrees in cybersecurity."
    },
    {
        "date": "2025-02",
        "title": "Toward Robust Non-Transferable Learning: A Survey and Benchmark",
        "author": "Ziming Hong, Yongli Xiang, and Tongliang Liu",
        "link": "http://arxiv.org/abs/2502.13593v1",
        "abstract": "Over the past decades, researchers have primarily focused on improving the\ngeneralization abilities of models, with limited attention given to regulating\nsuch generalization. However, the ability of models to generalize to unintended\ndata (e.g., harmful or unauthorized data) can be exploited by malicious\nadversaries in unforeseen ways, potentially resulting in violations of model\nethics. Non-transferable learning (NTL), a task aimed at reshaping the\ngeneralization abilities of deep learning models, was proposed to address these\nchallenges. While numerous methods have been proposed in this field, a\ncomprehensive review of existing progress and a thorough analysis of current\nlimitations remain lacking. In this paper, we bridge this gap by presenting the\nfirst comprehensive survey on NTL and introducing NTLBench, the first benchmark\nto evaluate NTL performance and robustness within a unified framework.\nSpecifically, we first introduce the task settings, general framework, and\ncriteria of NTL, followed by a summary of NTL approaches. Furthermore, we\nemphasize the often-overlooked issue of robustness against various attacks that\ncan destroy the non-transferable mechanism established by NTL. Experiments\nconducted via NTLBench verify the limitations of existing NTL methods in\nrobustness. Finally, we discuss the practical applications of NTL, along with\nits future directions and associated challenges."
    },
    {
        "date": "2025-02",
        "title": "Exploiting Prefix-Tree in Structured Output Interfaces for Enhancing Jailbreak Attacking",
        "author": "Yanzeng Li, Yunfan Xiong, Jialun Zhong, Jinchao Zhang, Jie Zhou, and Lei Zou",
        "link": "http://arxiv.org/abs/2502.13527v1",
        "abstract": "The rise of Large Language Models (LLMs) has led to significant applications\nbut also introduced serious security threats, particularly from jailbreak\nattacks that manipulate output generation. These attacks utilize prompt\nengineering and logit manipulation to steer models toward harmful content,\nprompting LLM providers to implement filtering and safety alignment strategies.\nWe investigate LLMs' safety mechanisms and their recent applications, revealing\na new threat model targeting structured output interfaces, which enable\nattackers to manipulate the inner logit during LLM generation, requiring only\nAPI access permissions. To demonstrate this threat model, we introduce a\nblack-box attack framework called AttackPrefixTree (APT). APT exploits\nstructured output interfaces to dynamically construct attack patterns. By\nleveraging prefixes of models' safety refusal response and latent harmful\noutputs, APT effectively bypasses safety measures. Experiments on benchmark\ndatasets indicate that this approach achieves higher attack success rate than\nexisting methods. This work highlights the urgent need for LLM providers to\nenhance security protocols to address vulnerabilities arising from the\ninteraction between safety patterns and structured outputs."
    },
    {
        "date": "2025-02",
        "title": "EvoP: Robust LLM Inference via Evolutionary Pruning",
        "author": "Shangyu Wu, Hongchao Du, Ying Xiong, Shuai Chen, Tei-wei Kuo, Nan Guan, and Chun Jason Xue",
        "link": "http://arxiv.org/abs/2502.14910v1",
        "abstract": "Large Language Models (LLMs) have achieved remarkable success in natural\nlanguage processing tasks, but their massive size and computational demands\nhinder their deployment in resource-constrained environments. Existing\nstructured pruning methods address this issue by removing redundant structures\n(e.g., elements, channels, layers) from the model. However, these methods\nemploy a heuristic pruning strategy, which leads to suboptimal performance.\nBesides, they also ignore the data characteristics when pruning the model.\n  To overcome these limitations, we propose EvoP, an evolutionary pruning\nframework for robust LLM inference. EvoP first presents a cluster-based\ncalibration dataset sampling (CCDS) strategy for creating a more diverse\ncalibration dataset. EvoP then introduces an evolutionary pruning pattern\nsearching (EPPS) method to find the optimal pruning pattern. Compared to\nexisting structured pruning techniques, EvoP achieves the best performance\nwhile maintaining the best efficiency. Experiments across different LLMs and\ndifferent downstream tasks validate the effectiveness of the proposed EvoP,\nmaking it a practical and scalable solution for deploying LLMs in real-world\napplications."
    },
    {
        "date": "2025-02",
        "title": "JL1-CD: A New Benchmark for Remote Sensing Change Detection and a Robust Multi-Teacher Knowledge Distillation Framework",
        "author": "Ziyuan Liu, Ruifei Zhu, Long Gao, Yuanxiu Zhou, Jingyu Ma, and Yuantao Gu",
        "link": "http://arxiv.org/abs/2502.13407v1",
        "abstract": "Deep learning has achieved significant success in the field of remote sensing\nimage change detection (CD), yet two major challenges remain: the scarcity of\nsub-meter, all-inclusive open-source CD datasets, and the difficulty of\nachieving consistent and satisfactory detection results across images with\nvarying change areas. To address these issues, we introduce the JL1-CD dataset,\nwhich contains 5,000 pairs of 512 x 512 pixel images with a resolution of 0.5\nto 0.75 meters. Additionally, we propose a multi-teacher knowledge distillation\n(MTKD) framework for CD. Experimental results on the JL1-CD and SYSU-CD\ndatasets demonstrate that the MTKD framework significantly improves the\nperformance of CD models with various network architectures and parameter\nsizes, achieving new state-of-the-art results. The code is available at\nhttps://github.com/circleLZY/MTKD-CD."
    },
    {
        "date": "2025-02",
        "title": "CipherGuard: Compiler-aided Mitigation against Ciphertext Side-channel Attacks",
        "author": "Ke Jiang, Sen Deng, Yinshuai Li, Shuai Wang, Tianwei Zhang, and Yinqian Zhang",
        "link": "http://arxiv.org/abs/2502.13401v1",
        "abstract": "Cryptographic implementations bolster security against timing side-channel\nattacks by integrating constant-time components. However, the new ciphertext\nside channels resulting from the deterministic memory encryption in Trusted\nExecution Environments (TEEs), enable ciphertexts to manifest identifiable\npatterns when being sequentially written to the same memory address. Attackers\nwith read access to encrypted memory in TEEs can potentially deduce plaintexts\nby analyzing these changing ciphertext patterns.\n  In this paper, we design CipherGuard, a compiler-aided mitigation methodology\nto counteract ciphertext side channels with high efficiency and security.\nCipherGuard is based on the LLVM ecosystem, and encompasses multiple mitigation\nstrategies, including software-based probabilistic encryption and secret-aware\nregister allocation. Through a comprehensive evaluation, we demonstrate that\nCipherGuard can strengthen the security of various cryptographic\nimplementations more efficiently than existing state-of-the-art defense\nmechanism, i.e., CipherFix."
    },
    {
        "date": "2025-02",
        "title": "KOALA: Knowledge Conflict Augmentations for Robustness in Vision Language Models",
        "author": "Peter Carragher, Nikitha Rao, Abhinand Jha, R Raghav, and Kathleen M. Carley",
        "link": "http://arxiv.org/abs/2502.14908v1",
        "abstract": "The robustness of large language models (LLMs) against knowledge conflicts in\nunimodal question answering systems has been well studied. However, the effect\nof conflicts in information sources on vision language models (VLMs) in\nmultimodal settings has not yet been explored. In this work, we propose\n\\segsub, a framework that applies targeted perturbations to image sources to\nstudy and improve the robustness of VLMs against three different types of\nknowledge conflicts, namely parametric, source, and counterfactual conflicts.\nContrary to prior findings that showed that LLMs are sensitive to parametric\nconflicts arising from textual perturbations, we find VLMs are largely robust\nto image perturbation. On the other hand, VLMs perform poorly on counterfactual\nexamples (<30% accuracy) and fail to reason over source conflicts (<1%\naccuracy). We also find a link between hallucinations and image context, with\nGPT-4o prone to hallucination when presented with highly contextualized\ncounterfactual examples. While challenges persist with source conflicts,\nfinetuning models significantly improves reasoning over counterfactual samples.\nOur findings highlight the need for VLM training methodologies that enhance\ntheir reasoning capabilities, particularly in addressing complex knowledge\nconflicts between multimodal sources."
    },
    {
        "date": "2025-02",
        "title": "Secure and Efficient Watermarking for Latent Diffusion Models in Model Distribution Scenarios",
        "author": "Liangqi Lei, Keke Gai, Jing Yu, Liehuang Zhu, and Qi Wu",
        "link": "http://arxiv.org/abs/2502.13345v1",
        "abstract": "Latent diffusion models have exhibited considerable potential in generative\ntasks. Watermarking is considered to be an alternative to safeguard the\ncopyright of generative models and prevent their misuse. However, in the\ncontext of model distribution scenarios, the accessibility of models to large\nscale of model users brings new challenges to the security, efficiency and\nrobustness of existing watermark solutions. To address these issues, we propose\na secure and efficient watermarking solution. A new security mechanism is\ndesigned to prevent watermark leakage and watermark escape, which considers\nwatermark randomness and watermark-model association as two constraints for\nmandatory watermark injection. To reduce the time cost of training the security\nmodule, watermark injection and the security mechanism are decoupled, ensuring\nthat fine-tuning VAE only accomplishes the security mechanism without the\nburden of learning watermark patterns. A watermark distribution-based\nverification strategy is proposed to enhance the robustness against diverse\nattacks in the model distribution scenarios. Experimental results prove that\nour watermarking consistently outperforms existing six baselines on\neffectiveness and robustness against ten image processing attacks and\nadversarial attacks, while enhancing security in the distribution scenarios."
    },
    {
        "date": "2025-02",
        "title": "Pruning as a Defense: Reducing Memorization in Large Language Models",
        "author": "Mansi Gupta, Nikhar Waghela, Sarthak Gupta, Shourya Goel, and Sanjif Shanmugavelu",
        "link": "http://arxiv.org/abs/2502.15796v1",
        "abstract": "Large language models have been shown to memorize significant portions of\ntheir training data, which they can reproduce when appropriately prompted. This\nwork investigates the impact of simple pruning techniques on this behavior. Our\nfindings reveal that pruning effectively reduces the extent of memorization in\nLLMs, demonstrating its potential as a foundational approach for mitigating\nmembership inference attacks."
    },
    {
        "date": "2025-02",
        "title": "UniGuardian: A Unified Defense for Detecting Prompt Injection, Backdoor Attacks and Adversarial Attacks in Large Language Models",
        "author": "Huawei Lin, Yingjie Lao, Tong Geng, Tan Yu, and Weijie Zhao",
        "link": "http://arxiv.org/abs/2502.13141v1",
        "abstract": "Large Language Models (LLMs) are vulnerable to attacks like prompt injection,\nbackdoor attacks, and adversarial attacks, which manipulate prompts or models\nto generate harmful outputs. In this paper, departing from traditional deep\nlearning attack paradigms, we explore their intrinsic relationship and\ncollectively term them Prompt Trigger Attacks (PTA). This raises a key\nquestion: Can we determine if a prompt is benign or poisoned? To address this,\nwe propose UniGuardian, the first unified defense mechanism designed to detect\nprompt injection, backdoor attacks, and adversarial attacks in LLMs.\nAdditionally, we introduce a single-forward strategy to optimize the detection\npipeline, enabling simultaneous attack detection and text generation within a\nsingle forward pass. Our experiments confirm that UniGuardian accurately and\nefficiently identifies malicious prompts in LLMs."
    },
    {
        "date": "2025-02",
        "title": "The Role of GitHub Copilot on Software Development: A Perspec-tive on Productivity, Security, Best Practices and Future Directions",
        "author": "Suresh Babu Nettur, Shanthi Karpurapu, Unnati Nettur, Likhit Sagar Gajja, Sravanthy Myneni, and Akhil Dusi",
        "link": "http://arxiv.org/abs/2502.13199v1",
        "abstract": "GitHub Copilot is transforming software development by automating tasks and\nboosting productivity through AI-driven code generation. In this paper, we\ncon-duct a literature survey to synthesize insights on Copilot's impact on\nproductivity and security. We review academic journal databases, industry\nreports, and official docu-mentation to highlight key findings and challenges.\nWhile Copilot accelerates coding and prototyping, concerns over security\nvulnerabilities and intellectual property risks persist. Drawing from the\nliterature, we provide a perspective on best practices and future directions\nfor responsible AI adoption in software engineering, offering action-able\ninsights for developers and organizations to integrate Copilot effectively\nwhile maintaining high standards of quality and security."
    },
    {
        "date": "2025-02",
        "title": "RobuRCDet: Enhancing Robustness of Radar-Camera Fusion in Bird's Eye View for 3D Object Detection",
        "author": "Jingtong Yue, Zhiwei Lin, Xin Lin, Xiaoyu Zhou, Xiangtai Li, Lu Qi, Yongtao Wang, and Ming-Hsuan Yang",
        "link": "http://arxiv.org/abs/2502.13071v1",
        "abstract": "While recent low-cost radar-camera approaches have shown promising results in\nmulti-modal 3D object detection, both sensors face challenges from\nenvironmental and intrinsic disturbances. Poor lighting or adverse weather\nconditions degrade camera performance, while radar suffers from noise and\npositional ambiguity. Achieving robust radar-camera 3D object detection\nrequires consistent performance across varying conditions, a topic that has not\nyet been fully explored. In this work, we first conduct a systematic analysis\nof robustness in radar-camera detection on five kinds of noises and propose\nRobuRCDet, a robust object detection model in BEV. Specifically, we design a 3D\nGaussian Expansion (3DGE) module to mitigate inaccuracies in radar points,\nincluding position, Radar Cross-Section (RCS), and velocity. The 3DGE uses RCS\nand velocity priors to generate a deformable kernel map and variance for kernel\nsize adjustment and value distribution. Additionally, we introduce a\nweather-adaptive fusion module, which adaptively fuses radar and camera\nfeatures based on camera signal confidence. Extensive experiments on the\npopular benchmark, nuScenes, show that our model achieves competitive results\nin regular and noisy conditions."
    },
    {
        "date": "2025-02",
        "title": "Sublinear-Overhead Secure Linear Algebra on a Dishonest Server",
        "author": "Mark Braverman, and Stephen Newman",
        "link": "http://arxiv.org/abs/2502.13060v1",
        "abstract": "Most heavy computation occurs on servers owned by a second party. This\nreduces data privacy, resulting in interest in data-oblivious computation,\nwhich typically severely degrades performance. Secure and fast remote\ncomputation is particularly important for linear algebra, which comprises a\nlarge fraction of total computation and is best run on highly specialized\nhardware often only accessible through the cloud. We state the natural\nefficiency and security desiderata for fast, remote, and data-oblivious linear\nalgebra, conjecture the existence of matrix and vector families implying\nsatisfactory algorithms, and provide such an algorithm contingent on common\ncryptographic assumptions. We achieve sublinear overhead for the server,\ndramatically reduced computation cost for the client, and various other\npractical advantages over previous algorithms.\n  Keywords: Data Privacy, Data-Oblivious Computation, Delegation, Homomorphic\nEncryption, Cloud Computing, Algorithm Efficiency, Sublinear Overhead, LPN,\nMatrix Multiplication."
    },
    {
        "date": "2025-02",
        "title": "Preventing the Popular Item Embedding Based Attack in Federated Recommendations",
        "author": "Jun Zhang, Huan Li, Dazhong Rong, Yan Zhao, Ke Chen, and Lidan Shou",
        "link": "http://arxiv.org/abs/2502.12958v1",
        "abstract": "Privacy concerns have led to the rise of federated recommender systems (FRS),\nwhich can create personalized models across distributed clients. However, FRS\nis vulnerable to poisoning attacks, where malicious users manipulate gradients\nto promote their target items intentionally. Existing attacks against FRS have\nlimitations, as they depend on specific models and prior knowledge, restricting\ntheir real-world applicability. In our exploration of practical FRS\nvulnerabilities, we devise a model-agnostic and prior-knowledge-free attack,\nnamed PIECK (Popular Item Embedding based Attack). The core module of PIECK is\npopular item mining, which leverages embedding changes during FRS training to\neffectively identify the popular items. Built upon the core module, PIECK\nbranches into two diverse solutions: The PIECKIPE solution employs an item\npopularity enhancement module, which aligns the embeddings of targeted items\nwith the mined popular items to increase item exposure. The PIECKUEA further\nenhances the robustness of the attack by using a user embedding approximation\nmodule, which approximates private user embeddings using mined popular items.\nUpon identifying PIECK, we evaluate existing federated defense methods and find\nthem ineffective against PIECK, as poisonous gradients inevitably overwhelm the\ncold target items. We then propose a novel defense method by introducing two\nregularization terms during user training, which constrain item popularity\nenhancement and user embedding approximation while preserving FRS performance.\nWe evaluate PIECK and its defense across two base models, three real datasets,\nfour top-tier attacks, and six general defense methods, affirming the efficacy\nof both PIECK and its defense."
    },
    {
        "date": "2025-02",
        "title": "Decentralized and Robust Privacy-Preserving Model Using Blockchain-Enabled Federated Deep Learning in Intelligent Enterprises",
        "author": "Reza Fotohi, Fereidoon Shams Aliee, and Bahar Farahani",
        "link": "http://arxiv.org/abs/2502.17485v1",
        "abstract": "In Federated Deep Learning (FDL), multiple local enterprises are allowed to\ntrain a model jointly. Then, they submit their local updates to the central\nserver, and the server aggregates the updates to create a global model.\nHowever, trained models usually perform worse than centralized models,\nespecially when the training data distribution is non-independent and\nidentically distributed (nonIID). NonIID data harms the accuracy and\nperformance of the model. Additionally, due to the centrality of federated\nlearning (FL) and the untrustworthiness of enterprises, traditional FL\nsolutions are vulnerable to security and privacy attacks. To tackle this issue,\nwe propose FedAnil, a secure blockchain enabled Federated Deep Learning Model\nthat improves enterprise models decentralization, performance, and tamper proof\nproperties, incorporating two main phases. The first phase addresses the nonIID\nchallenge (label and feature distribution skew). The second phase addresses\nsecurity and privacy concerns against poisoning and inference attacks through\nthree steps. Extensive experiments were conducted using the Sent140,\nFashionMNIST, FEMNIST, and CIFAR10 new real world datasets to evaluate FedAnils\nrobustness and performance. The simulation results demonstrate that FedAnil\nsatisfies FDL privacy preserving requirements. In terms of convergence\nanalysis, the model parameter obtained with FedAnil converges to the optimum of\nthe model parameter. In addition, it performs better in terms of accuracy (more\nthan 11, 15, and 24%) and computation overhead (less than 8, 10, and 15%)\ncompared with baseline approaches, namely ShieldFL, RVPFL, and RFA."
    },
    {
        "date": "2025-02",
        "title": "Strands Rocq: Why is a Security Protocol Correct, Mechanically?",
        "author": "Matteo Busi, Riccardo Focardi, and Flaminia L. Luccio",
        "link": "http://arxiv.org/abs/2502.12848v1",
        "abstract": "Strand spaces are a formal framework for symbolic protocol verification that\nallows for pen-and-paper proofs of security. While extremely insightful,\npen-and-paper proofs are error-prone, and it is hard to gain confidence on\ntheir correctness. To overcome this problem, we developed StrandsRocq, a full\nmechanization of the strand spaces in Coq (soon to be renamed Rocq). The\nmechanization was designed to be faithful to the original pen-and-paper\ndevelopment, and it was engineered to be modular and extensible. StrandsRocq\nincorporates new original proof techniques, a novel notion of maximal\npenetrator that enables protocol compositionality, and a set of Coq tactics\ntailored to the domain, facilitating proof automation and reuse, and\nsimplifying the work of protocol analysts. To demonstrate the versatility of\nour approach, we modelled and analyzed a family of authentication protocols,\ndrawing inspiration from ISO/IEC 9798-2 two-pass authentication, the classical\nNeedham-Schroeder-Lowe protocol, as well as a recently-proposed static analysis\nfor a key management API. The analyses in StrandsRocq confirmed the high degree\nof proof reuse, and enabled us to distill the minimal requirements for protocol\nsecurity. Through mechanization, we identified and addressed several issues in\nthe original proofs and we were able to significantly improve the precision of\nthe static analysis for the key management API. Moreover, we were able to\nleverage the novel notion of maximal penetrator to provide a compositional\nproof of security for two simple authentication protocols."
    },
    {
        "date": "2025-02",
        "title": "Iron Sharpens Iron: Defending Against Attacks in Machine-Generated Text Detection with Adversarial Training",
        "author": "Yuanfan Li, Zhaohan Zhang, Chengzhengxu Li, Chao Shen, and Xiaoming Liu",
        "link": "http://arxiv.org/abs/2502.12734v1",
        "abstract": "Machine-generated Text (MGT) detection is crucial for regulating and\nattributing online texts. While the existing MGT detectors achieve strong\nperformance, they remain vulnerable to simple perturbations and adversarial\nattacks. To build an effective defense against malicious perturbations, we view\nMGT detection from a threat modeling perspective, that is, analyzing the\nmodel's vulnerability from an adversary's point of view and exploring effective\nmitigations. To this end, we introduce an adversarial framework for training a\nrobust MGT detector, named GREedy Adversary PromoTed DefendER (GREATER). The\nGREATER consists of two key components: an adversary GREATER-A and a detector\nGREATER-D. The GREATER-D learns to defend against the adversarial attack from\nGREATER-A and generalizes the defense to other attacks. GREATER-A identifies\nand perturbs the critical tokens in embedding space, along with greedy search\nand pruning to generate stealthy and disruptive adversarial examples. Besides,\nwe update the GREATER-A and GREATER-D synchronously, encouraging the GREATER-D\nto generalize its defense to different attacks and varying attack intensities.\nOur experimental results across 9 text perturbation strategies and 5\nadversarial attacks show that our GREATER-D reduces the Attack Success Rate\n(ASR) by 10.61% compared with SOTA defense methods while our GREATER-A is\ndemonstrated to be more effective and efficient than SOTA attack approaches."
    },
    {
        "date": "2025-02",
        "title": "Chronus: Understanding and Securing the Cutting-Edge Industry Solutions to DRAM Read Disturbance",
        "author": "O\u011fuzhan Canpolat, A. Giray Ya\u011fl\u0131k\u00e7\u0131, Geraldo F. Oliveira, Ataberk Olgun, Nisa Bostanc\u0131, \u0130smail Emir Y\u00fcksel, Haocong Luo, O\u011fuz Ergin, and Onur Mutlu",
        "link": "http://arxiv.org/abs/2502.12650v1",
        "abstract": "We 1) present the first rigorous security, performance, energy, and cost\nanalyses of the state-of-the-art on-DRAM-die read disturbance mitigation\nmethod, Per Row Activation Counting (PRAC) and 2) propose Chronus, a new\nmechanism that addresses PRAC's two major weaknesses. Our analysis shows that\nPRAC's system performance overhead on benign applications is non-negligible for\nmodern DRAM chips and prohibitively large for future DRAM chips that are more\nvulnerable to read disturbance. We identify two weaknesses of PRAC that cause\nthese overheads. First, PRAC increases critical DRAM access latency parameters\ndue to the additional time required to increment activation counters. Second,\nPRAC performs a constant number of preventive refreshes at a time, making it\nvulnerable to an adversarial access pattern, known as the wave attack, and\nconsequently requiring it to be configured for significantly smaller activation\nthresholds. To address PRAC's two weaknesses, we propose a new on-DRAM-die\nRowHammer mitigation mechanism, Chronus. Chronus 1) updates row activation\ncounters concurrently while serving accesses by separating counters from the\ndata and 2) prevents the wave attack by dynamically controlling the number of\npreventive refreshes performed. Our performance analysis shows that Chronus's\nsystem performance overhead is near-zero for modern DRAM chips and very low for\nfuture DRAM chips. Chronus outperforms three variants of PRAC and three other\nstate-of-the-art read disturbance solutions. We discuss Chronus's and PRAC's\nimplications for future systems and foreshadow future research directions. To\naid future research, we open-source our Chronus implementation at\nhttps://github.com/CMU-SAFARI/Chronus."
    },
    {
        "date": "2025-02",
        "title": "Automating Prompt Leakage Attacks on Large Language Models Using Agentic Approach",
        "author": "Tvrtko Sternak, Davor Runje, Dorian Grano\u0161a, and Chi Wang",
        "link": "http://arxiv.org/abs/2502.12630v1",
        "abstract": "This paper presents a novel approach to evaluating the security of large\nlanguage models (LLMs) against prompt leakage-the exposure of system-level\nprompts or proprietary configurations. We define prompt leakage as a critical\nthreat to secure LLM deployment and introduce a framework for testing the\nrobustness of LLMs using agentic teams. Leveraging AG2 (formerly AutoGen), we\nimplement a multi-agent system where cooperative agents are tasked with probing\nand exploiting the target LLM to elicit its prompt.\n  Guided by traditional definitions of security in cryptography, we further\ndefine a prompt leakage-safe system as one in which an attacker cannot\ndistinguish between two agents: one initialized with an original prompt and the\nother with a prompt stripped of all sensitive information. In a safe system,\nthe agents' outputs will be indistinguishable to the attacker, ensuring that\nsensitive information remains secure. This cryptographically inspired framework\nprovides a rigorous standard for evaluating and designing secure LLMs.\n  This work establishes a systematic methodology for adversarial testing of\nprompt leakage, bridging the gap between automated threat modeling and\npractical LLM security.\n  You can find the implementation of our prompt leakage probing on GitHub."
    },
    {
        "date": "2025-02",
        "title": "DemonAgent: Dynamically Encrypted Multi-Backdoor Implantation Attack on LLM-based Agent",
        "author": "Pengyu Zhu, Zhenhong Zhou, Yuanhe Zhang, Shilinlu Yan, Kun Wang, and Sen Su",
        "link": "http://arxiv.org/abs/2502.12575v1",
        "abstract": "As LLM-based agents become increasingly prevalent, backdoors can be implanted\ninto agents through user queries or environment feedback, raising critical\nconcerns regarding safety vulnerabilities. However, backdoor attacks are\ntypically detectable by safety audits that analyze the reasoning process of\nagents. To this end, we propose a novel backdoor implantation strategy called\n\\textbf{Dynamically Encrypted Multi-Backdoor Implantation Attack}.\nSpecifically, we introduce dynamic encryption, which maps the backdoor into\nbenign content, effectively circumventing safety audits. To enhance\nstealthiness, we further decompose the backdoor into multiple sub-backdoor\nfragments. Based on these advancements, backdoors are allowed to bypass safety\naudits significantly. Additionally, we present AgentBackdoorEval, a dataset\ndesigned for the comprehensive evaluation of agent backdoor attacks.\nExperimental results across multiple datasets demonstrate that our method\nachieves an attack success rate nearing 100\\% while maintaining a detection\nrate of 0\\%, illustrating its effectiveness in evading safety audits. Our\nfindings highlight the limitations of existing safety mechanisms in detecting\nadvanced attacks, underscoring the urgent need for more robust defenses against\nbackdoor threats. Code and data are available at\nhttps://github.com/whfeLingYu/DemonAgent."
    },
    {
        "date": "2025-02",
        "title": "Towards Robust and Secure Embodied AI: A Survey on Vulnerabilities and Attacks",
        "author": "Wenpeng Xing, Minghao Li, Mohan Li, and Meng Han",
        "link": "http://arxiv.org/abs/2502.13175v2",
        "abstract": "Embodied AI systems, including robots and autonomous vehicles, are\nincreasingly integrated into real-world applications, where they encounter a\nrange of vulnerabilities stemming from both environmental and system-level\nfactors. These vulnerabilities manifest through sensor spoofing, adversarial\nattacks, and failures in task and motion planning, posing significant\nchallenges to robustness and safety. Despite the growing body of research,\nexisting reviews rarely focus specifically on the unique safety and security\nchallenges of embodied AI systems. Most prior work either addresses general AI\nvulnerabilities or focuses on isolated aspects, lacking a dedicated and unified\nframework tailored to embodied AI. This survey fills this critical gap by: (1)\ncategorizing vulnerabilities specific to embodied AI into exogenous (e.g.,\nphysical attacks, cybersecurity threats) and endogenous (e.g., sensor failures,\nsoftware flaws) origins; (2) systematically analyzing adversarial attack\nparadigms unique to embodied AI, with a focus on their impact on perception,\ndecision-making, and embodied interaction; (3) investigating attack vectors\ntargeting large vision-language models (LVLMs) and large language models (LLMs)\nwithin embodied systems, such as jailbreak attacks and instruction\nmisinterpretation; (4) evaluating robustness challenges in algorithms for\nembodied perception, decision-making, and task planning; and (5) proposing\ntargeted strategies to enhance the safety and reliability of embodied AI\nsystems. By integrating these dimensions, we provide a comprehensive framework\nfor understanding the interplay between vulnerabilities and safety in embodied\nAI."
    },
    {
        "date": "2025-02",
        "title": "Boost, Disentangle, and Customize: A Robust System2-to-System1 Pipeline for Code Generation",
        "author": "Kounianhua Du, Hanjing Wang, Jianxing Liu, Jizheng Chen, Xinyi Dai, Yasheng Wang, Ruiming Tang, Yong Yu, Jun Wang, and Weinan Zhang",
        "link": "http://arxiv.org/abs/2502.12492v1",
        "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities in\nvarious domains, particularly in system 1 tasks, yet the intricacies of their\nproblem-solving mechanisms in system 2 tasks are not sufficiently explored.\nRecent research on System2-to-System1 methods surge, exploring the System 2\nreasoning knowledge via inference-time computation and compressing the explored\nknowledge into System 1 process. In this paper, we focus on code generation,\nwhich is a representative System 2 task, and identify two primary challenges:\n(1) the complex hidden reasoning processes and (2) the heterogeneous data\ndistributions that complicate the exploration and training of robust LLM\nsolvers. To tackle these issues, we propose a novel BDC framework that explores\ninsightful System 2 knowledge of LLMs using a MC-Tree-Of-Agents algorithm with\nmutual \\textbf{B}oosting, \\textbf{D}isentangles the heterogeneous training data\nfor composable LoRA-experts, and obtain \\textbf{C}ustomized problem solver for\neach data instance with an input-aware hypernetwork to weight over the\nLoRA-experts, offering effectiveness, flexibility, and robustness. This\nframework leverages multiple LLMs through mutual verification and boosting,\nintegrated into a Monte-Carlo Tree Search process enhanced by reflection-based\npruning and refinement. Additionally, we introduce the DisenLora algorithm,\nwhich clusters heterogeneous data to fine-tune LLMs into composable Lora\nexperts, enabling the adaptive generation of customized problem solvers through\nan input-aware hypernetwork. This work lays the groundwork for advancing LLM\ncapabilities in complex reasoning tasks, offering a novel System2-to-System1\nsolution."
    },
    {
        "date": "2025-02",
        "title": "PKE and ABE with Collusion-Resistant Secure Key Leasing",
        "author": "Fuyuki Kitagawa, Ryo Nishimaki, and Nikhil Pappu",
        "link": "http://arxiv.org/abs/2502.12491v2",
        "abstract": "Secure key leasing (SKL) is an advanced encryption functionality that allows\na secret key holder to generate a quantum decryption key and securely lease it\nto a user. Once the user returns the quantum decryption key (or provides a\nclassical certificate confirming its deletion), they lose their decryption\ncapability. Previous works on public key encryption with SKL (PKE-SKL) have\nonly considered the single-key security model, where the adversary receives at\nmost one quantum decryption key. However, this model does not accurately\nreflect real-world applications of PKE-SKL. To address this limitation, we\nintroduce collusion-resistant security for PKE-SKL (denoted as PKE-CR-SKL). In\nthis model, the adversary can adaptively obtain multiple quantum decryption\nkeys and access a verification oracle which validates the correctness of\nqueried quantum decryption keys. Importantly, the size of the public key and\nciphertexts must remain independent of the total number of generated quantum\ndecryption keys. We present the following constructions:\n  - A PKE-CR-SKL scheme based on the learning with errors (LWE) assumption.\n  - An attribute-based encryption scheme with collusion-resistant SKL\n(ABE-CR-SKL), also based on the LWE assumption.\n  - An ABE-CR-SKL scheme with classical certificates, relying on multi-input\nABE with polynomial arity."
    },
    {
        "date": "2025-02",
        "title": "Software Security in Software-Defined Networking: A Systematic Literature Review",
        "author": "Moustapha Awwalou Diouf, Samuel Ouya, Jacques Klein, and Tegawend\u00e9 F. Bissyand\u00e9",
        "link": "http://arxiv.org/abs/2502.13828v1",
        "abstract": "Software-defined networking (SDN) has shifted network management by\ndecoupling the data and control planes. This enables programmatic control via\nsoftware applications using open APIs. SDN's programmability has fueled its\npopularity but may have opened issues extending the attack surface by\nintroducing vulnerable software. Therefore, the research community needs to\nhave a deep and broad understanding of the risks posed by SDN to propose\nmitigating measures. The literature, however, lacks a comprehensive review of\nthe current state of research in this direction. This paper addresses this gap\nby providing a comprehensive overview of the state-of-the-art research in SDN\nsecurity focusing on the software (i.e., the controller, APIs, applications)\npart. We systematically reviewed 58 relevant publications to analyze trends,\nidentify key testing and analysis methodologies, and categorize studied\nvulnerabilities. We further explore areas where the research community can make\nsignificant contributions. This work offers the most extensive and in-depth\nanalysis of SDN software security to date."
    },
    {
        "date": "2025-02",
        "title": "Robust Disentangled Counterfactual Learning for Physical Audiovisual Commonsense Reasoning",
        "author": "Mengshi Qi, Changsheng Lv, and Huadong Ma",
        "link": "http://arxiv.org/abs/2502.12425v1",
        "abstract": "In this paper, we propose a new Robust Disentangled Counterfactual Learning\n(RDCL) approach for physical audiovisual commonsense reasoning. The task aims\nto infer objects' physics commonsense based on both video and audio input, with\nthe main challenge being how to imitate the reasoning ability of humans, even\nunder the scenario of missing modalities. Most of the current methods fail to\ntake full advantage of different characteristics in multi-modal data, and\nlacking causal reasoning ability in models impedes the progress of implicit\nphysical knowledge inferring. To address these issues, our proposed RDCL method\ndecouples videos into static (time-invariant) and dynamic (time-varying)\nfactors in the latent space by the disentangled sequential encoder, which\nadopts a variational autoencoder (VAE) to maximize the mutual information with\na contrastive loss function. Furthermore, we introduce a counterfactual\nlearning module to augment the model's reasoning ability by modeling physical\nknowledge relationships among different objects under counterfactual\nintervention. To alleviate the incomplete modality data issue, we introduce a\nrobust multimodal learning method to recover the missing data by decomposing\nthe shared features and model-specific features. Our proposed method is a\nplug-and-play module that can be incorporated into any baseline including VLMs.\nIn experiments, we show that our proposed method improves the reasoning\naccuracy and robustness of baseline methods and achieves the state-of-the-art\nperformance."
    },
    {
        "date": "2025-02",
        "title": "Boosting Illuminant Estimation in Deep Color Constancy through Enhancing Brightness Robustness",
        "author": "Mengda Xie, Chengzhi Zhong, Yiling He, Zhan Qin, and Meie Fang",
        "link": "http://arxiv.org/abs/2502.12418v1",
        "abstract": "Color constancy estimates illuminant chromaticity to correct color-biased\nimages. Recently, Deep Neural Network-driven Color Constancy (DNNCC) models\nhave made substantial advancements. Nevertheless, the potential risks in DNNCC\ndue to the vulnerability of deep neural networks have not yet been explored. In\nthis paper, we conduct the first investigation into the impact of a key factor\nin color constancy-brightness-on DNNCC from a robustness perspective. Our\nevaluation reveals that several mainstream DNNCC models exhibit high\nsensitivity to brightness despite their focus on chromaticity estimation. This\nsheds light on a potential limitation of existing DNNCC models: their\nsensitivity to brightness may hinder performance given the widespread\nbrightness variations in real-world datasets. From the insights of our\nanalysis, we propose a simple yet effective brightness robustness enhancement\nstrategy for DNNCC models, termed BRE. The core of BRE is built upon the\nadaptive step-size adversarial brightness augmentation technique, which\nidentifies high-risk brightness variation and generates augmented images via\nexplicit brightness adjustment. Subsequently, BRE develops a\nbrightness-robustness-aware model optimization strategy that integrates\nadversarial brightness training and brightness contrastive loss, significantly\nbolstering the brightness robustness of DNNCC models. BRE is\nhyperparameter-free and can be integrated into existing DNNCC models, without\nincurring additional overhead during the testing phase. Experiments on two\npublic color constancy datasets-ColorChecker and Cube+-demonstrate that the\nproposed BRE consistently enhances the illuminant estimation performance of\nexisting DNNCC models, reducing the estimation error by an average of 5.04%\nacross six mainstream DNNCC models, underscoring the critical role of enhancing\nbrightness robustness in these models."
    },
    {
        "date": "2025-02",
        "title": "Alignment and Adversarial Robustness: Are More Human-Like Models More Secure?",
        "author": "Blaine Hoak, Kunyang Li, and Patrick McDaniel",
        "link": "http://arxiv.org/abs/2502.12377v1",
        "abstract": "Representational alignment refers to the extent to which a model's internal\nrepresentations mirror biological vision, offering insights into both neural\nsimilarity and functional correspondence. Recently, some more aligned models\nhave demonstrated higher resiliency to adversarial examples, raising the\nquestion of whether more human-aligned models are inherently more secure. In\nthis work, we conduct a large-scale empirical analysis to systematically\ninvestigate the relationship between representational alignment and adversarial\nrobustness. We evaluate 118 models spanning diverse architectures and training\nparadigms, measuring their neural and behavioral alignment and engineering task\nperformance across 106 benchmarks as well as their adversarial robustness via\nAutoAttack. Our findings reveal that while average alignment and robustness\nexhibit a weak overall correlation, specific alignment benchmarks serve as\nstrong predictors of adversarial robustness, particularly those that measure\nselectivity towards texture or shape. These results suggest that different\nforms of alignment play distinct roles in model robustness, motivating further\ninvestigation into how alignment-driven approaches can be leveraged to build\nmore secure and perceptually-grounded vision models."
    },
    {
        "date": "2025-02",
        "title": "Learning Plasma Dynamics and Robust Rampdown Trajectories with Predict-First Experiments at TCV",
        "author": "Allen M. Wang, Alessandro Pau, Cristina Rea, Oswin So, Charles Dawson, Olivier Sauter, Mark D. Boyer, Anna Vu, Cristian Galperti, Chuchu Fan, Antoine Merle, Yoeri Poels, Cristina Venturini, Stefano Marchioni, and the TCV Team",
        "link": "http://arxiv.org/abs/2502.12327v1",
        "abstract": "The rampdown in tokamak operations is a difficult to simulate phase during\nwhich the plasma is often pushed towards multiple instability limits. To\naddress this challenge, and reduce the risk of disrupting operations, we\nleverage recent advances in Scientific Machine Learning (SciML) to develop a\nneural state-space model (NSSM) that predicts plasma dynamics during Tokamak\n\\`a Configuration Variable (TCV) rampdowns. By integrating simple physics\nstructure and data-driven models, the NSSM efficiently learns plasma dynamics\nduring the rampdown from a modest dataset of 311 pulses with only five pulses\nin the reactor relevant high performance regime. The NSSM is parallelized\nacross uncertainties, and reinforcement learning (RL) is applied to design\ntrajectories that avoid multiple instability limits with high probability.\nExperiments at TCV ramping down high performance plasmas show statistically\nsignificant improvements in current and energy at plasma termination, with\nimprovements in speed through continuous re-training. A predict-first\nexperiment, increasing plasma current by 20\\% from baseline, demonstrates the\nNSSM's ability to make small extrapolations with sufficient accuracy to design\ntrajectories that successfully terminate the pulse. The developed approach\npaves the way for designing tokamak controls with robustness to considerable\nuncertainty, and demonstrates the relevance of the SciML approach to learning\nplasma dynamics for rapidly developing robust trajectories and controls during\nthe incremental campaigns of upcoming burning plasma tokamaks."
    },
    {
        "date": "2025-02",
        "title": "Adversarial Debiasing for Unbiased Parameter Recovery",
        "author": "Luke C Sanford, Megan Ayers, Matthew Gordon, and Eliana Stone",
        "link": "http://arxiv.org/abs/2502.12323v1",
        "abstract": "Advances in machine learning and the increasing availability of\nhigh-dimensional data have led to the proliferation of social science research\nthat uses the predictions of machine learning models as proxies for measures of\nhuman activity or environmental outcomes. However, prediction errors from\nmachine learning models can lead to bias in the estimates of regression\ncoefficients. In this paper, we show how this bias can arise, propose a test\nfor detecting bias, and demonstrate the use of an adversarial machine learning\nalgorithm in order to de-bias predictions. These methods are applicable to any\nsetting where machine-learned predictions are the dependent variable in a\nregression. We conduct simulations and empirical exercises using ground truth\nand satellite data on forest cover in Africa. Using the predictions from a\nnaive machine learning model leads to biased parameter estimates, while the\npredictions from the adversarial model recover the true coefficients."
    },
    {
        "date": "2025-02",
        "title": "Robust 6DoF Pose Tracking Considering Contour and Interior Correspondence Uncertainty for AR Assembly Guidance",
        "author": "Jixiang Chen, Jing Chen, Kai Liu, Haochen Chang, Shanfeng Fu, and Jian Yang",
        "link": "http://arxiv.org/abs/2502.11971v1",
        "abstract": "Augmented reality assembly guidance is essential for intelligent\nmanufacturing and medical applications, requiring continuous measurement of the\n6DoF poses of manipulated objects. Although current tracking methods have made\nsignificant advancements in accuracy and efficiency, they still face challenges\nin robustness when dealing with cluttered backgrounds, rotationally symmetric\nobjects, and noisy sequences. In this paper, we first propose a robust\ncontour-based pose tracking method that addresses error-prone contour\ncorrespondences and improves noise tolerance. It utilizes a fan-shaped search\nstrategy to refine correspondences and models local contour shape and noise\nuncertainty as mixed probability distribution, resulting in a highly robust\ncontour energy function. Secondly, we introduce a CPU-only strategy to better\ntrack rotationally symmetric objects and assist the contour-based method in\novercoming local minima by exploring sparse interior correspondences. This is\nachieved by pre-sampling interior points from sparse viewpoint templates\noffline and using the DIS optical flow algorithm to compute their\ncorrespondences during tracking. Finally, we formulate a unified energy\nfunction to fuse contour and interior information, which is solvable using a\nre-weighted least squares algorithm. Experiments on public datasets and real\nscenarios demonstrate that our method significantly outperforms\nstate-of-the-art monocular tracking methods and can achieve more than 100 FPS\nusing only a CPU."
    },
    {
        "date": "2025-02",
        "title": "A limited technical background is sufficient for attack-defense tree acceptability",
        "author": "Nathan Daniel Schiele, and Olga Gadyatskaya",
        "link": "http://arxiv.org/abs/2502.11920v1",
        "abstract": "Attack-defense trees (ADTs) are a prominent graphical threat modeling method\nthat is highly recommended for analyzing and communicating security-related\ninformation. Despite this, existing empirical studies of attack trees have\nestablished their acceptability only for users with highly technical (computer\nscience) backgrounds while raising questions about their suitability for threat\nmodeling stakeholders with a limited technical background. Our research\naddresses this gap by investigating the impact of the users' technical\nbackground on ADT acceptability in an empirical study.\n  Our Method Evaluation Model-based study consisted of n = 102 participants (53\nwith a strong computer science background and 49 with a limited computer\nscience background) who were asked to complete a series of ADT-related tasks.\nBy analyzing their responses and comparing the results, we reveal that a very\nlimited technical background is sufficient for ADT acceptability. This finding\nunderscores attack trees' viability as a threat modeling method."
    },
    {
        "date": "2025-02",
        "title": "On the robustness of ChatGPT in teaching Korean Mathematics",
        "author": "Phuong-Nam Nguyen, Quang Nguyen-The, An Vu-Minh, Diep-Anh Nguyen, and Xuan-Lam Pham",
        "link": "http://arxiv.org/abs/2502.11915v1",
        "abstract": "ChatGPT, an Artificial Intelligence model, has the potential to revolutionize\neducation. However, its effectiveness in solving non-English questions remains\nuncertain. This study evaluates ChatGPT's robustness using 586 Korean\nmathematics questions. ChatGPT achieves 66.72% accuracy, correctly answering\n391 out of 586 questions. We also assess its ability to rate mathematics\nquestions based on eleven criteria and perform a topic analysis. Our findings\nshow that ChatGPT's ratings align with educational theory and test-taker\nperspectives. While ChatGPT performs well in question classification, it\nstruggles with non-English contexts, highlighting areas for improvement. Future\nresearch should address linguistic biases and enhance accuracy across diverse\nlanguages. Domain-specific optimizations and multilingual training could\nimprove ChatGPT's role in personalized education."
    },
    {
        "date": "2025-02",
        "title": "Adversarial Alignment for LLMs Requires Simpler, Reproducible, and More Measurable Objectives",
        "author": "Leo Schwinn, Yan Scholten, Tom Wollschl\u00e4ger, Sophie Xhonneux, Stephen Casper, Stephan G\u00fcnnemann, and Gauthier Gidel",
        "link": "http://arxiv.org/abs/2502.11910v2",
        "abstract": "Misaligned research objectives have considerably hindered progress in\nadversarial robustness research over the past decade. For instance, an\nextensive focus on optimizing target metrics, while neglecting rigorous\nstandardized evaluation, has led researchers to pursue ad-hoc heuristic\ndefenses that were seemingly effective. Yet, most of these were exposed as\nflawed by subsequent evaluations, ultimately contributing little measurable\nprogress to the field. In this position paper, we illustrate that current\nresearch on the robustness of large language models (LLMs) risks repeating past\npatterns with potentially worsened real-world implications. To address this, we\nargue that realigned objectives are necessary for meaningful progress in\nadversarial alignment. To this end, we build on established cybersecurity\ntaxonomy to formally define differences between past and emerging threat models\nthat apply to LLMs. Using this framework, we illustrate that progress requires\ndisentangling adversarial alignment into addressable sub-problems and returning\nto core academic principles, such as measureability, reproducibility, and\ncomparability. Although the field presents significant challenges, the fresh\nstart on adversarial robustness offers the unique opportunity to build on past\nexperience while avoiding previous mistakes."
    }
]