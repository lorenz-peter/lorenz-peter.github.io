[
    {
        "date": "2025-10",
        "title": "BadGraph: A Backdoor Attack Against Latent Diffusion Model for Text-Guided Graph Generation",
        "author": "Liang Ye, Shengqin Chen, and Jiazhu Dai",
        "link": "http://arxiv.org/abs/2510.20792v1",
        "abstract": "The rapid progress of graph generation has raised new security concerns,\nparticularly regarding backdoor vulnerabilities. While prior work has explored\nbackdoor attacks in image diffusion and unconditional graph generation,\nconditional, especially text-guided graph generation remains largely\nunexamined. This paper proposes BadGraph, a backdoor attack method targeting\nlatent diffusion models for text-guided graph generation. BadGraph leverages\ntextual triggers to poison training data, covertly implanting backdoors that\ninduce attacker-specified subgraphs during inference when triggers appear,\nwhile preserving normal performance on clean inputs. Extensive experiments on\nfour benchmark datasets (PubChem, ChEBI-20, PCDes, MoMu) demonstrate the\neffectiveness and stealth of the attack: less than 10% poisoning rate can\nachieves 50% attack success rate, while 24% suffices for over 80% success rate,\nwith negligible performance degradation on benign samples. Ablation studies\nfurther reveal that the backdoor is implanted during VAE and diffusion training\nrather than pretraining. These findings reveal the security vulnerabilities in\nlatent diffusion models of text-guided graph generation, highlight the serious\nrisks in models' applications such as drug discovery and underscore the need\nfor robust defenses against the backdoor attack in such diffusion models."
    },
    {
        "date": "2025-10",
        "title": "R2-SVC: Towards Real-World Robust and Expressive Zero-shot Singing Voice Conversion",
        "author": "Junjie Zheng, Gongyu Chen, Chaofan Ding, and Zihao Chen",
        "link": "http://arxiv.org/abs/2510.20677v1",
        "abstract": "In real-world singing voice conversion (SVC) applications, environmental\nnoise and the demand for expressive output pose significant challenges.\nConventional methods, however, are typically designed without accounting for\nreal deployment scenarios, as both training and inference usually rely on clean\ndata. This mismatch hinders practical use, given the inevitable presence of\ndiverse noise sources and artifacts from music separation. To tackle these\nissues, we propose R2-SVC, a robust and expressive SVC framework. First, we\nintroduce simulation-based robustness enhancement through random fundamental\nfrequency ($F_0$) perturbations and music separation artifact simulations\n(e.g., reverberation, echo), substantially improving performance under noisy\nconditions. Second, we enrich speaker representation using domain-specific\nsinging data: alongside clean vocals, we incorporate DNSMOS-filtered separated\nvocals and public singing corpora, enabling the model to preserve speaker\ntimbre while capturing singing style nuances. Third, we integrate the Neural\nSource-Filter (NSF) model to explicitly represent harmonic and noise\ncomponents, enhancing the naturalness and controllability of converted singing.\nR2-SVC achieves state-of-the-art results on multiple SVC benchmarks under both\nclean and noisy conditions."
    },
    {
        "date": "2025-10",
        "title": "Risk Psychology & Cyber-Attack Tactics",
        "author": "Rubens Kim, Stephan Carney, Yvonne Fonken, Soham Hans, Sofia Hirschmann, Stacy Marsella, Peggy Wu, and Nikolos Gurney",
        "link": "http://arxiv.org/abs/2510.20657v1",
        "abstract": "We examine whether measured cognitive processes predict cyber-attack\nbehavior. We analyzed data that included psychometric scale responses and\nlabeled attack behaviors from cybersecurity professionals who conducted\nred-team operations against a simulated enterprise network. We employed\nmultilevel mixed-effects Poisson regression with technique counts nested within\nparticipants to test whether cognitive processes predicted technique-specific\nusage. The scales significantly predicted technique use, but effects varied by\ntechnique rather than operating uniformly. Neither expertise level nor\nexperimental treatment condition significantly predicted technique patterns,\nindicating that cognitive processes may be stronger drivers of technique\nselection than training or experience. These findings demonstrate that\nindividual cognitive differences shape cyber-attack behavior and support the\ndevelopment of psychology-informed defense strategies."
    },
    {
        "date": "2025-10",
        "title": "Decentralized Exchange that Mitigate a Bribery Attack",
        "author": "Nitin Awathare",
        "link": "http://arxiv.org/abs/2510.20645v1",
        "abstract": "Despite the popularity of Hashed Time-Locked Contracts (HTLCs) because of\ntheir use in wide areas of applications such as payment channels, atomic swaps,\netc, their use in exchange is still questionable. This is because of its\nincentive incompatibility and susceptibility to bribery attacks.\n  State-of-the-art solutions such as MAD-HTLC (Oakland'21) and He-HTLC\n(NDSS'23) address this by leveraging miners' profit-driven behaviour to\nmitigate such attacks. The former is the mitigation against passive miners;\nhowever, the latter works against both active and passive miners. However, they\nconsider only two bribing scenarios where either of the parties involved in the\ntransfer collude with the miner.\n  In this paper, we expose vulnerabilities in state-of-the-art solutions by\npresenting a miner-collusion bribery attack with implementation and\ngame-theoretic analysis. Additionally, we propose a stronger attack on MAD-HTLC\nthan He-HTLC, allowing the attacker to earn profits equivalent to attacking\nnaive HTLC.\n  Leveraging our insights, we propose \\prot, a game-theoretically secure HTLC\nprotocol resistant to all bribery scenarios. \\prot\\ employs a two-phase\napproach, preventing unauthorized token confiscation by third parties, such as\nminers. In Phase 1, parties commit to the transfer; in Phase 2, the transfer is\nexecuted without manipulation. We demonstrate \\prot's efficiency in transaction\ncost and latency via implementations on Bitcoin and Ethereum."
    },
    {
        "date": "2025-10",
        "title": "AdaDoS: Adaptive DoS Attack via Deep Adversarial Reinforcement Learning in SDN",
        "author": "Wei Shao, Yuhao Wang, Rongguang He, Muhammad Ejaz Ahmed, and Seyit Camtepe",
        "link": "http://arxiv.org/abs/2510.20566v1",
        "abstract": "Existing defence mechanisms have demonstrated significant effectiveness in\nmitigating rule-based Denial-of-Service (DoS) attacks, leveraging predefined\nsignatures and static heuristics to identify and block malicious traffic.\nHowever, the emergence of AI-driven techniques presents new challenges to SDN\nsecurity, potentially compromising the efficacy of existing defence mechanisms.\nIn this paper, we introduce~AdaDoS, an adaptive attack model that disrupt\nnetwork operations while evading detection by existing DoS-based detectors\nthrough adversarial reinforcement learning (RL). Specifically, AdaDoS models\nthe problem as a competitive game between an attacker, whose goal is to\nobstruct network traffic without being detected, and a detector, which aims to\nidentify malicious traffic. AdaDoS can solve this game by dynamically adjusting\nits attack strategy based on feedback from the SDN and the detector.\nAdditionally, recognising that attackers typically have less information than\ndefenders, AdaDoS formulates the DoS-like attack as a partially observed Markov\ndecision process (POMDP), with the attacker having access only to delay\ninformation between attacker and victim nodes. We address this challenge with a\nnovel reciprocal learning module, where the student agent, with limited\nobservations, enhances its performance by learning from the teacher agent, who\nhas full observational capabilities in the SDN environment. AdaDoS represents\nthe first application of RL to develop DoS-like attack sequences, capable of\nadaptively evading both machine learning-based and rule-based DoS-like attack\ndetectors."
    },
    {
        "date": "2025-10",
        "title": "Adversary-Aware Private Inference over Wireless Channels",
        "author": "Mohamed Seif, Malcolm Egan, Andrea J. Goldsmith, and H. Vincent Poor",
        "link": "http://arxiv.org/abs/2510.20518v1",
        "abstract": "AI-based sensing at wireless edge devices has the potential to significantly\nenhance Artificial Intelligence (AI) applications, particularly for vision and\nperception tasks such as in autonomous driving and environmental monitoring. AI\nsystems rely both on efficient model learning and inference. In the inference\nphase, features extracted from sensing data are utilized for prediction tasks\n(e.g., classification or regression). In edge networks, sensors and model\nservers are often not co-located, which requires communication of features. As\nsensitive personal data can be reconstructed by an adversary, transformation of\nthe features are required to reduce the risk of privacy violations. While\ndifferential privacy mechanisms provide a means of protecting finite datasets,\nprotection of individual features has not been addressed. In this paper, we\npropose a novel framework for privacy-preserving AI-based sensing, where\ndevices apply transformations of extracted features before transmission to a\nmodel server."
    },
    {
        "date": "2025-10",
        "title": "Neural Reasoning for Robust Instance Retrieval in $\\mathcal{SHOIQ}$",
        "author": "Louis Mozart Kamdem Teyou, Luke Friedrichs, N'Dah Jean Kouagou, Caglar Demir, Yasir Mahmood, Stefan Heindorf, and Axel-Cyrille Ngonga Ngomo",
        "link": "http://arxiv.org/abs/2510.20457v1",
        "abstract": "Concept learning exploits background knowledge in the form of description\nlogic axioms to learn explainable classification models from knowledge bases.\nDespite recent breakthroughs in neuro-symbolic concept learning, most\napproaches still cannot be deployed on real-world knowledge bases. This is due\nto their use of description logic reasoners, which are not robust against\ninconsistencies nor erroneous data. We address this challenge by presenting a\nnovel neural reasoner dubbed EBR. Our reasoner relies on embeddings to\napproximate the results of a symbolic reasoner. We show that EBR solely\nrequires retrieving instances for atomic concepts and existential restrictions\nto retrieve or approximate the set of instances of any concept in the\ndescription logic $\\mathcal{SHOIQ}$. In our experiments, we compare EBR with\nstate-of-the-art reasoners. Our results suggest that EBR is robust against\nmissing and erroneous data in contrast to existing reasoners."
    },
    {
        "date": "2025-10",
        "title": "MolBridge: Atom-Level Joint Graph Refinement for Robust Drug-Drug Interaction Event Prediction",
        "author": "Xuan Lin, Aocheng Ding, Tengfei Ma, Hua Liang, and Zhe Quan",
        "link": "http://arxiv.org/abs/2510.20448v1",
        "abstract": "Drug combinations offer therapeutic benefits but also carry the risk of\nadverse drug-drug interactions (DDIs), especially under complex molecular\nstructures. Accurate DDI event prediction requires capturing fine-grained\ninter-drug relationships, which are critical for modeling metabolic mechanisms\nsuch as enzyme-mediated competition. However, existing approaches typically\nrely on isolated drug representations and fail to explicitly model atom-level\ncross-molecular interactions, limiting their effectiveness across diverse\nmolecular complexities and DDI type distributions. To address these\nlimitations, we propose MolBridge, a novel atom-level joint graph refinement\nframework for robust DDI event prediction. MolBridge constructs a joint graph\nthat integrates atomic structures of drug pairs, enabling direct modeling of\ninter-drug associations. A central challenge in such joint graph settings is\nthe potential loss of information caused by over-smoothing when modeling\nlong-range atomic dependencies. To overcome this, we introduce a structure\nconsistency module that iteratively refines node features while preserving the\nglobal structural context. This joint design allows MolBridge to effectively\nlearn both local and global interaction outperforms state-of-the-art baselines,\nachieving superior performance across long-tail and inductive scenarios.\npatterns, yielding robust representations across both frequent and rare DDI\ntypes. Extensive experiments on two benchmark datasets show that MolBridge\nconsistently. These results demonstrate the advantages of fine-grained graph\nrefinement in improving the accuracy, robustness, and mechanistic\ninterpretability of DDI event prediction.This work contributes to Web Mining\nand Content Analysis by developing graph-based methods for mining and analyzing\ndrug-drug interaction networks."
    },
    {
        "date": "2025-10",
        "title": "Hierarchical Time Series Forecasting with Robust Reconciliation",
        "author": "Shuhei Aikawa, Aru Suzuki, Kei Yoshitake, Kanata Teshigawara, Akira Iwabuchi, Ken Kobayashi, and Kazuhide Nakata",
        "link": "http://arxiv.org/abs/2510.20383v1",
        "abstract": "This paper focuses on forecasting hierarchical time-series data, where each\nhigher-level observation equals the sum of its corresponding lower-level time\nseries. In such contexts, the forecast values should be coherent, meaning that\nthe forecast value of each parent series exactly matches the sum of the\nforecast values of its child series. Existing hierarchical forecasting methods\ntypically generate base forecasts independently for each series and then apply\na reconciliation procedure to adjust them so that the resulting forecast values\nare coherent across the hierarchy. These methods generally derive an optimal\nreconciliation, using a covariance matrix of the forecast error. In practice,\nhowever, the true covariance matrix is unknown and has to be estimated from\nfinite samples in advance. This gap between the true and estimated covariance\nmatrix may degrade forecast performance. To address this issue, we propose a\nrobust optimization framework for hierarchical reconciliation that accounts for\nuncertainty in the estimated covariance matrix. We first introduce an\nuncertainty set for the estimated covariance matrix and formulate a\nreconciliation problem that minimizes the worst-case expected squared error\nover this uncertainty set. We show that our problem can be cast as a\nsemidefinite optimization problem. Numerical experiments demonstrate that the\nproposed robust reconciliation method achieved better forecast performance than\nexisting hierarchical forecasting methods, which indicates the effectiveness of\nintegrating uncertainty into the reconciliation process."
    },
    {
        "date": "2025-10",
        "title": "Synthetic Data for Robust Runway Detection",
        "author": "Estelle Chigot, Dennis G. Wilson, Meriem Ghrib, Fabrice Jimenez, and Thomas Oberlin",
        "link": "http://arxiv.org/abs/2510.20349v1",
        "abstract": "Deep vision models are now mature enough to be integrated in industrial and\npossibly critical applications such as autonomous navigation. Yet, data\ncollection and labeling to train such models requires too much efforts and\ncosts for a single company or product. This drawback is more significant in\ncritical applications, where training data must include all possible conditions\nincluding rare scenarios. In this perspective, generating synthetic images is\nan appealing solution, since it allows a cheap yet reliable covering of all the\nconditions and environments, if the impact of the synthetic-to-real\ndistribution shift is mitigated. In this article, we consider the case of\nrunway detection that is a critical part in autonomous landing systems\ndeveloped by aircraft manufacturers. We propose an image generation approach\nbased on a commercial flight simulator that complements a few annotated real\nimages. By controlling the image generation and the integration of real and\nsynthetic data, we show that standard object detection models can achieve\naccurate prediction. We also evaluate their robustness with respect to adverse\nconditions, in our case nighttime images, that were not represented in the real\ndata, and show the interest of using a customized domain adaptation strategy."
    },
    {
        "date": "2025-10",
        "title": "Enhancing Security in Deep Reinforcement Learning: A Comprehensive Survey on Adversarial Attacks and Defenses",
        "author": "Wu Yichao, Wang Yirui, Ding Panpan, Wang Hailong, Zhu Bingqian, and Liu Chun",
        "link": "http://arxiv.org/abs/2510.20314v1",
        "abstract": "With the wide application of deep reinforcement learning (DRL) techniques in\ncomplex fields such as autonomous driving, intelligent manufacturing, and smart\nhealthcare, how to improve its security and robustness in dynamic and\nchangeable environments has become a core issue in current research. Especially\nin the face of adversarial attacks, DRL may suffer serious performance\ndegradation or even make potentially dangerous decisions, so it is crucial to\nensure their stability in security-sensitive scenarios. In this paper, we first\nintroduce the basic framework of DRL and analyze the main security challenges\nfaced in complex and changing environments. In addition, this paper proposes an\nadversarial attack classification framework based on perturbation type and\nattack target and reviews the mainstream adversarial attack methods against DRL\nin detail, including various attack methods such as perturbation state space,\naction space, reward function and model space. To effectively counter the\nattacks, this paper systematically summarizes various current robustness\ntraining strategies, including adversarial training, competitive training,\nrobust learning, adversarial detection, defense distillation and other related\ndefense techniques, we also discuss the advantages and shortcomings of these\nmethods in improving the robustness of DRL. Finally, this paper looks into the\nfuture research direction of DRL in adversarial environments, emphasizing the\nresearch needs in terms of improving generalization, reducing computational\ncomplexity, and enhancing scalability and explainability, aiming to provide\nvaluable references and directions for researchers."
    },
    {
        "date": "2025-10",
        "title": "IB-GAN: Disentangled Representation Learning with Information Bottleneck Generative Adversarial Networks",
        "author": "Insu Jeon, Wonkwang Lee, Myeongjang Pyeon, and Gunhee Kim",
        "link": "http://arxiv.org/abs/2510.20165v1",
        "abstract": "We propose a new GAN-based unsupervised model for disentangled representation\nlearning. The new model is discovered in an attempt to utilize the Information\nBottleneck (IB) framework to the optimization of GAN, thereby named IB-GAN. The\narchitecture of IB-GAN is partially similar to that of InfoGAN but has a\ncritical difference; an intermediate layer of the generator is leveraged to\nconstrain the mutual information between the input and the generated output.\nThe intermediate stochastic layer can serve as a learnable latent distribution\nthat is trained with the generator jointly in an end-to-end fashion. As a\nresult, the generator of IB-GAN can harness the latent space in a disentangled\nand interpretable manner. With the experiments on dSprites and Color-dSprites\ndataset, we demonstrate that IB-GAN achieves competitive disentanglement scores\nto those of state-of-the-art \\b{eta}-VAEs and outperforms InfoGAN. Moreover,\nthe visual quality and the diversity of samples generated by IB-GAN are often\nbetter than those by \\b{eta}-VAEs and Info-GAN in terms of FID score on CelebA\nand 3D Chairs dataset."
    },
    {
        "date": "2025-10",
        "title": "SAID: Empowering Large Language Models with Self-Activating Internal Defense",
        "author": "Yulong Chen, Yadong Liu, Jiawen Zhang, Mu Li, Chao Huang, and Jie Wen",
        "link": "http://arxiv.org/abs/2510.20129v1",
        "abstract": "Large Language Models (LLMs), despite advances in safety alignment, remain\nvulnerable to jailbreak attacks designed to circumvent protective mechanisms.\nPrevailing defense strategies rely on external interventions, such as input\nfiltering or output modification, which often lack generalizability and\ncompromise model utility while incurring significant computational overhead. In\nthis work, we introduce a new, training-free defense paradigm, Self-Activating\nInternal Defense (SAID), which reframes the defense task from external\ncorrection to internal capability activation. SAID uniquely leverages the LLM's\nown reasoning abilities to proactively identify and neutralize malicious intent\nthrough a three-stage pipeline: model-native intent distillation to extract\ncore semantics, optimal safety prefix probing to activate latent safety\nawareness, and a conservative aggregation strategy to ensure robust\ndecision-making. Extensive experiments on five open-source LLMs against six\nadvanced jailbreak attacks demonstrate that SAID substantially outperforms\nstate-of-the-art defenses in reducing harmful outputs. Crucially, it achieves\nthis while preserving model performance on benign tasks and incurring minimal\ncomputational overhead. Our work establishes that activating the intrinsic\nsafety mechanisms of LLMs is a more robust and scalable path toward building\nsafer and more reliable aligned AI systems."
    },
    {
        "date": "2025-10",
        "title": "Physics-Guided Fusion for Robust 3D Tracking of Fast Moving Small Objects",
        "author": "Prithvi Raj Singh, Raju Gottumukkala, Anthony S. Maida, Alan B. Barhorst, and Vijaya Gopu",
        "link": "http://arxiv.org/abs/2510.20126v1",
        "abstract": "While computer vision has advanced considerably for general object detection\nand tracking, the specific problem of fast-moving tiny objects remains\nunderexplored. This paper addresses the significant challenge of detecting and\ntracking rapidly moving small objects using an RGB-D camera. Our novel system\ncombines deep learning-based detection with physics-based tracking to overcome\nthe limitations of existing approaches. Our contributions include: (1) a\ncomprehensive system design for object detection and tracking of fast-moving\nsmall objects in 3D space, (2) an innovative physics-based tracking algorithm\nthat integrates kinematics motion equations to handle outliers and missed\ndetections, and (3) an outlier detection and correction module that\nsignificantly improves tracking performance in challenging scenarios such as\nocclusions and rapid direction changes. We evaluated our proposed system on a\ncustom racquetball dataset. Our evaluation shows our system surpassing kalman\nfilter based trackers with up to 70\\% less Average Displacement Error. Our\nsystem has significant applications for improving robot perception on\nautonomous platforms and demonstrates the effectiveness of combining\nphysics-based models with deep learning approaches for real-time 3D detection\nand tracking of challenging small objects."
    },
    {
        "date": "2025-10",
        "title": "Who Coordinates U.S. Cyber Defense? A Co-Authorship Network Analysis of Joint Cybersecurity Advisories (2024--2025)",
        "author": "M. Abdullah Canbaz, Hakan Otal, Tugce Unlu, Nour Alhussein, and Brian Nussbaum",
        "link": "http://arxiv.org/abs/2510.20080v1",
        "abstract": "Cyber threats increasingly demand joint responses, yet the organizational\ndynamics behind multi-agency cybersecurity collaboration remain poorly\nunderstood. Understanding who leads, who bridges, and how agencies coordinate\nis critical for strengthening both U.S. homeland security and allied defense\nefforts. In this study, we construct a co-authorship network from nine Joint\nCybersecurity Advisories (CSAs) issued between November 2024 and August 2025.\nWe map 41 agencies and 442 co-authoring ties to analyze the structure of\ncollaboration. We find a tightly knit U.S. triad -- CISA, FBI, and NSA --\ndensely connected with Five Eyes and select European allies. Degree centrality\nidentifies CISA and FBI as coordination hubs, while betweenness highlights NSA,\nthe UK's NCSC, and Australia's ASD-ACSC as key bridges linking otherwise\nfragmented clusters. By releasing the first replicable dataset and network\nanalysis of CSAs, we provide new empirical evidence on how collaborative\ncybersecurity signals are organized and where strategic influence is\nconcentrated."
    },
    {
        "date": "2025-10",
        "title": "Machine Learning-Based Localization Accuracy of RFID Sensor Networks via RSSI Decision Trees and CAD Modeling for Defense Applications",
        "author": "Curtis Lee Shull, and Merrick Green",
        "link": "http://arxiv.org/abs/2510.20019v1",
        "abstract": "Radio Frequency Identification (RFID) tracking may be a viable solution for\ndefense assets that must be stored in accordance with security guidelines.\nHowever, poor sensor specificity (vulnerabilities include long range detection,\nspoofing, and counterfeiting) can lead to erroneous detection and operational\nsecurity events. We present a supervised learning simulation with realistic\nReceived Signal Strength Indicator (RSSI) data and Decision Tree classification\nin a Computer Assisted Design (CAD)-modeled floor plan that encapsulates some\nof the challenges encountered in defense storage. In this work, we focused on\nclassifying 12 lab zones (LabZoneA-L) to perform location inference. The raw\ndataset had approximately 980,000 reads. Class frequencies were imbalanced, and\nclass weights were calculated to account for class imbalance in this\nmulti-class setting. The model, trained on stratified subsamples to 5,000\nbalanced observations, yielded an overall accuracy of 34.2% and F1-scores\ngreater than 0.40 for multiple zones (Zones F, G, H, etc.). However, rare\nclasses (most notably LabZoneC) were often misclassified, even with the use of\nclass weights. An adjacency-aware confusion matrix was calculated to allow\nbetter interpretation of physically adjacent zones. These results suggest that\nRSSI-based decision trees can be applied in realistic simulations to enable\nzone-level anomaly detection or misplacement monitoring for defense supply\nlogistics. Reliable classification performance in low-coverage and low-signal\nzones could be improved with better antenna placement or additional sensors and\nsensor fusion with other modalities."
    },
    {
        "date": "2025-10",
        "title": "A Unified Detection Pipeline for Robust Object Detection in Fisheye-Based Traffic Surveillance",
        "author": "Neema Jakisa Owor, Joshua Kofi Asamoah, Tanner Wambui Muturi, Anneliese Jakisa Owor, Blessing Agyei Kyem, Andrews Danyo, Yaw Adu-Gyamfi, and Armstrong Aboah",
        "link": "http://arxiv.org/abs/2510.20016v1",
        "abstract": "Fisheye cameras offer an efficient solution for wide-area traffic\nsurveillance by capturing large fields of view from a single vantage point.\nHowever, the strong radial distortion and nonuniform resolution inherent in\nfisheye imagery introduce substantial challenges for standard object detectors,\nparticularly near image boundaries where object appearance is severely\ndegraded. In this work, we present a detection framework designed to operate\nrobustly under these conditions. Our approach employs a simple yet effective\npre and post processing pipeline that enhances detection consistency across the\nimage, especially in regions affected by severe distortion. We train several\nstate-of-the-art detection models on the fisheye traffic imagery and combine\ntheir outputs through an ensemble strategy to improve overall detection\naccuracy. Our method achieves an F1 score of0.6366 on the 2025 AI City\nChallenge Track 4, placing 8thoverall out of 62 teams. These results\ndemonstrate the effectiveness of our framework in addressing issues inherent to\nfisheye imagery."
    },
    {
        "date": "2025-10",
        "title": "QORE : Quantum Secure 5G/B5G Core",
        "author": "Vipin Rathi, Lakshya Chopra, Rudraksh Rawal, Nitin Rajput, Shiva Valia, Madhav Aggarwal, and Aditya Gairola",
        "link": "http://arxiv.org/abs/2510.19982v1",
        "abstract": "Quantum computing is reshaping the security landscape of modern\ntelecommunications. The cryptographic foundations that secure todays 5G\nsystems, including RSA, Elliptic Curve Cryptography (ECC), and Diffie-Hellman\n(DH), are all susceptible to attacks enabled by Shors algorithm. Protecting 5G\nnetworks against future quantum adversaries has therefore become an urgent\nengineering and research priority. In this paper we introduce QORE, a\nquantum-secure 5G and Beyond 5G (B5G) Core framework that provides a clear\npathway for transitioning both the 5G Core Network Functions and User Equipment\n(UE) to Post-Quantum Cryptography (PQC). The framework uses the\nNIST-standardized lattice-based algorithms Module-Lattice Key Encapsulation\nMechanism (ML-KEM) and Module-Lattice Digital Signature Algorithm (ML-DSA) and\napplies them across the 5G Service-Based Architecture (SBA). A Hybrid PQC\n(HPQC) configuration is also proposed, combining classical and quantum-safe\nprimitives to maintain interoperability during migration. Experimental\nvalidation shows that ML-KEM achieves quantum security with minor performance\noverhead, meeting the low-latency and high-throughput requirements of\ncarrier-grade 5G systems. The proposed roadmap aligns with ongoing 3GPP SA3 and\nSA5 study activities on the security and management of post-quantum networks as\nwell as with NIST PQC standardization efforts, providing practical guidance for\nmitigating quantum-era risks while safeguarding long-term confidentiality and\nintegrity of network data."
    },
    {
        "date": "2025-10",
        "title": "Towards Strong Certified Defense with Universal Asymmetric Randomization",
        "author": "Hanbin Hong, Ashish Kundu, Ali Payani, Binghui Wang, and Yuan Hong",
        "link": "http://arxiv.org/abs/2510.19977v1",
        "abstract": "Randomized smoothing has become essential for achieving certified adversarial\nrobustness in machine learning models. However, current methods primarily use\nisotropic noise distributions that are uniform across all data dimensions, such\nas image pixels, limiting the effectiveness of robustness certification by\nignoring the heterogeneity of inputs and data dimensions. To address this\nlimitation, we propose UCAN: a novel technique that \\underline{U}niversally\n\\underline{C}ertifies adversarial robustness with \\underline{A}nisotropic\n\\underline{N}oise. UCAN is designed to enhance any existing randomized\nsmoothing method, transforming it from symmetric (isotropic) to asymmetric\n(anisotropic) noise distributions, thereby offering a more tailored defense\nagainst adversarial attacks. Our theoretical framework is versatile, supporting\na wide array of noise distributions for certified robustness in different\n$\\ell_p$-norms and applicable to any arbitrary classifier by guaranteeing the\nclassifier's prediction over perturbed inputs with provable robustness bounds\nthrough tailored noise injection. Additionally, we develop a novel framework\nequipped with three exemplary noise parameter generators (NPGs) to optimally\nfine-tune the anisotropic noise parameters for different data dimensions,\nallowing for pursuing different levels of robustness enhancements in\npractice.Empirical evaluations underscore the significant leap in UCAN's\nperformance over existing state-of-the-art methods, demonstrating up to\n$182.6\\%$ improvement in certified accuracy at large certified radii on MNIST,\nCIFAR10, and ImageNet datasets.\\footnote{Code is anonymously available at\n\\href{https://github.com/youbin2014/UCAN/}{https://github.com/youbin2014/UCAN/}}"
    },
    {
        "date": "2025-10",
        "title": "Robust Reinforcement Learning in Finance: Modeling Market Impact with Elliptic Uncertainty Sets",
        "author": "Shaocong Ma, and Heng Huang",
        "link": "http://arxiv.org/abs/2510.19950v1",
        "abstract": "In financial applications, reinforcement learning (RL) agents are commonly\ntrained on historical data, where their actions do not influence prices.\nHowever, during deployment, these agents trade in live markets where their own\ntransactions can shift asset prices, a phenomenon known as market impact. This\nmismatch between training and deployment environments can significantly degrade\nperformance. Traditional robust RL approaches address this model\nmisspecification by optimizing the worst-case performance over a set of\nuncertainties, but typically rely on symmetric structures that fail to capture\nthe directional nature of market impact. To address this issue, we develop a\nnovel class of elliptic uncertainty sets. We establish both implicit and\nexplicit closed-form solutions for the worst-case uncertainty under these sets,\nenabling efficient and tractable robust policy evaluation. Experiments on\nsingle-asset and multi-asset trading tasks demonstrate that our method achieves\nsuperior Sharpe ratio and remains robust under increasing trade volumes,\noffering a more faithful and scalable approach to RL in financial markets."
    },
    {
        "date": "2025-10",
        "title": "Designing a Secure and Resilient Distributed Smartphone Participant Data Collection System",
        "author": "Foad Namjoo, Neng Wan, Devan Mallory, Yuyi Chang, Nithin Sugavanam, Long Yin Lee, Ning Xiong, Emre Ertin, and Jeff M. Phillips",
        "link": "http://arxiv.org/abs/2510.19938v1",
        "abstract": "Real-world health studies require continuous and secure data collection from\nmobile and wearable devices. We introduce MotionPI, a smartphone-based system\ndesigned to collect behavioral and health data through sensors and surveys with\nminimal interaction from participants. The system integrates passive data\ncollection (such as GPS and wristband motion data) with Ecological Momentary\nAssessment (EMA) surveys, which can be triggered randomly or based on physical\nactivity. MotionPI is designed to work under real-life constraints, including\nlimited battery life, weak or intermittent cellular connection, and minimal\nuser supervision. It stores data both locally and on a secure cloud server,\nwith encrypted transmission and storage. It integrates through Bluetooth Low\nEnergy (BLE) into wristband devices that store raw data and communicate motion\nsummaries and trigger events. MotionPI demonstrates a practical solution for\nsecure and scalable mobile data collection in cyber-physical health studies."
    },
    {
        "date": "2025-10",
        "title": "Under Pressure: Security Analysis and Process Impacts of a Commercial Smart Air Compressor",
        "author": "Jad Zarzour, and Matthew Jablonski",
        "link": "http://arxiv.org/abs/2510.19772v2",
        "abstract": "The integration of Industrial Internet of Things (IIoT) devices into\nmanufacturing environments has accelerated the transition to Industry 4.0, but\nhas also introduced new cybersecurity risks. This paper conducts a\ncomprehensive security analysis of a commercial smart air compressor, revealing\ncritical vulnerabilities including hardcoded credentials, unauthenticated APIs,\nand an insecure update mechanism. It includes a formal threat model,\ndemonstrates practical attack scenarios in a testbed environment, and evaluates\ntheir subsequent impact on an industrial process, leading to denial of service\nand the corruption of critical process telemetry. In addition, an analysis of\nthe device's supply chain reveals how product integration from multiple vendors\nand limited security considerations can expose a device to threats. The\nfindings underscore the necessity of incorporating cybersecurity principles\ninto both IIoT device design and supply chain governance to enhance resilience\nagainst emerging industrial cyber threats."
    },
    {
        "date": "2025-10",
        "title": "Exploring the Effect of DNN Depth on Adversarial Attacks in Network Intrusion Detection Systems",
        "author": "Mohamed ElShehaby, and Ashraf Matrawy",
        "link": "http://arxiv.org/abs/2510.19761v1",
        "abstract": "Adversarial attacks pose significant challenges to Machine Learning (ML)\nsystems and especially Deep Neural Networks (DNNs) by subtly manipulating\ninputs to induce incorrect predictions. This paper investigates whether\nincreasing the layer depth of deep neural networks affects their robustness\nagainst adversarial attacks in the Network Intrusion Detection System (NIDS)\ndomain. We compare the adversarial robustness of various deep neural networks\nacross both \\ac{NIDS} and computer vision domains (the latter being widely used\nin adversarial attack experiments). Our experimental results reveal that in the\nNIDS domain, adding more layers does not necessarily improve their performance,\nyet it may actually significantly degrade their robustness against adversarial\nattacks. Conversely, in the computer vision domain, adding more layers exhibits\na more modest impact on robustness. These findings can guide the development of\nrobust neural networks for (NIDS) applications and highlight the unique\ncharacteristics of network security domains within the (ML) landscape."
    },
    {
        "date": "2025-10",
        "title": "LyTimeT: Towards Robust and Interpretable State-Variable Discovery",
        "author": "Kuai Yu, Crystal Su, Xiang Liu, Judah Goldfeder, Mingyuan Shao, and Hod Lipson",
        "link": "http://arxiv.org/abs/2510.19716v1",
        "abstract": "Extracting the true dynamical variables of a system from high-dimensional\nvideo is challenging due to distracting visual factors such as background\nmotion, occlusions, and texture changes. We propose LyTimeT, a two-phase\nframework for interpretable variable extraction that learns robust and stable\nlatent representations of dynamical systems. In Phase 1, LyTimeT employs a\nspatio-temporal TimeSformer-based autoencoder that uses global attention to\nfocus on dynamically relevant regions while suppressing nuisance variation,\nenabling distraction-robust latent state learning and accurate long-horizon\nvideo prediction. In Phase 2, we probe the learned latent space, select the\nmost physically meaningful dimensions using linear correlation analysis, and\nrefine the transition dynamics with a Lyapunov-based stability regularizer to\nenforce contraction and reduce error accumulation during roll-outs. Experiments\non five synthetic benchmarks and four real-world dynamical systems, including\nchaotic phenomena, show that LyTimeT achieves mutual information and intrinsic\ndimension estimates closest to ground truth, remains invariant under background\nperturbations, and delivers the lowest analytical mean squared error among\nCNN-based (TIDE) and transformer-only baselines. Our results demonstrate that\ncombining spatio-temporal attention with stability constraints yields\npredictive models that are not only accurate but also physically interpretable."
    },
    {
        "date": "2025-10",
        "title": "Explainable Face Presentation Attack Detection via Ensemble-CAM",
        "author": "Rashik Shadman, M G Sarwar Murshed, and Faraz Hussain",
        "link": "http://arxiv.org/abs/2510.19695v1",
        "abstract": "Presentation attacks represent a critical security threat where adversaries\nuse fake biometric data, such as face, fingerprint, or iris images, to gain\nunauthorized access to protected systems. Various presentation attack detection\n(PAD) systems have been designed leveraging deep learning (DL) models to\nmitigate this type of threat. Despite their effectiveness, most of the DL\nmodels function as black boxes - their decisions are opaque to their users. The\npurpose of explainability techniques is to provide detailed information about\nthe reason behind the behavior or decision of DL models. In particular, visual\nexplanation is necessary to better understand the decisions or predictions of\nDL-based PAD systems and determine the key regions due to which a biometric\nimage is considered real or fake by the system. In this work, a novel\ntechnique, Ensemble-CAM, is proposed for providing visual explanations for the\ndecisions made by deep learning-based face PAD systems. Our goal is to improve\nDL-based face PAD systems by providing a better understanding of their\nbehavior. Our provided visual explanations will enhance the transparency and\ntrustworthiness of DL-based face PAD systems."
    },
    {
        "date": "2025-10",
        "title": "Style Attack Disguise: When Fonts Become a Camouflage for Adversarial Intent",
        "author": "Yangshijie Zhang, Xinda Wang, Jialin Liu, Wenqiang Wang, Zhicong Ma, and Xingxing Jia",
        "link": "http://arxiv.org/abs/2510.19641v1",
        "abstract": "With social media growth, users employ stylistic fonts and font-like emoji to\nexpress individuality, creating visually appealing text that remains\nhuman-readable. However, these fonts introduce hidden vulnerabilities in NLP\nmodels: while humans easily read stylistic text, models process these\ncharacters as distinct tokens, causing interference. We identify this\nhuman-model perception gap and propose a style-based attack, Style Attack\nDisguise (SAD). We design two sizes: light for query efficiency and strong for\nsuperior attack performance. Experiments on sentiment classification and\nmachine translation across traditional models, LLMs, and commercial services\ndemonstrate SAD's strong attack performance. We also show SAD's potential\nthreats to multimodal tasks including text-to-image and text-to-speech\ngeneration."
    },
    {
        "date": "2025-10",
        "title": "Can You Trust What You See? Alpha Channel No-Box Attacks on Video Object Detection",
        "author": "Ariana Yi, Ce Zhou, Liyang Xiao, and Qiben Yan",
        "link": "http://arxiv.org/abs/2510.19574v1",
        "abstract": "As object detection models are increasingly deployed in cyber-physical\nsystems such as autonomous vehicles (AVs) and surveillance platforms, ensuring\ntheir security against adversarial threats is essential. While prior work has\nexplored adversarial attacks in the image domain, those attacks in the video\ndomain remain largely unexamined, especially in the no-box setting. In this\npaper, we present {\\alpha}-Cloak, the first no-box adversarial attack on object\ndetectors that operates entirely through the alpha channel of RGBA videos.\n{\\alpha}-Cloak exploits the alpha channel to fuse a malicious target video with\na benign video, resulting in a fused video that appears innocuous to human\nviewers but consistently fools object detectors. Our attack requires no access\nto model architecture, parameters, or outputs, and introduces no perceptible\nartifacts. We systematically study the support for alpha channels across common\nvideo formats and playback applications, and design a fusion algorithm that\nensures visual stealth and compatibility. We evaluate {\\alpha}-Cloak on five\nstate-of-the-art object detectors, a vision-language model, and a multi-modal\nlarge language model (Gemini-2.0-Flash), demonstrating a 100% attack success\nrate across all scenarios. Our findings reveal a previously unexplored\nvulnerability in video-based perception systems, highlighting the urgent need\nfor defenses that account for the alpha channel in adversarial settings."
    },
    {
        "date": "2025-10",
        "title": "PCP-GAN: Property-Constrained Pore-scale image reconstruction via conditional Generative Adversarial Networks",
        "author": "Ali Sadeghkhani, Brandon Bennett, Masoud Babaei, and Arash Rabbani",
        "link": "http://arxiv.org/abs/2510.19465v1",
        "abstract": "Obtaining truly representative pore-scale images that match bulk formation\nproperties remains a fundamental challenge in subsurface characterization, as\nnatural spatial heterogeneity causes extracted sub-images to deviate\nsignificantly from core-measured values. This challenge is compounded by data\nscarcity, where physical samples are only available at sparse well locations.\nThis study presents a multi-conditional Generative Adversarial Network (cGAN)\nframework that generates representative pore-scale images with precisely\ncontrolled properties, addressing both the representativeness challenge and\ndata availability constraints. The framework was trained on thin section\nsamples from four depths (1879.50-1943.50 m) of a carbonate formation,\nsimultaneously conditioning on porosity values and depth parameters within a\nsingle unified model. This approach captures both universal pore network\nprinciples and depth-specific geological characteristics, from grainstone\nfabrics with interparticle-intercrystalline porosity to crystalline textures\nwith anhydrite inclusions. The model achieved exceptional porosity control\n(R^2=0.95) across all formations with mean absolute errors of 0.0099-0.0197.\nMorphological validation confirmed preservation of critical pore network\ncharacteristics including average pore radius, specific surface area, and\ntortuosity, with statistical differences remaining within acceptable geological\ntolerances. Most significantly, generated images demonstrated superior\nrepresentativeness with dual-constraint errors of 1.9-11.3% compared to\n36.4-578% for randomly extracted real sub-images. This capability provides\ntransformative tools for subsurface characterization, particularly valuable for\ncarbon storage, geothermal energy, and groundwater management applications\nwhere knowing the representative morphology of the pore space is critical for\nimplementing digital rock physics."
    },
    {
        "date": "2025-10",
        "title": "Revisiting the Relation Between Robustness and Universality",
        "author": "M. Klabunde, L. Caspari, and F. Lemmerich",
        "link": "http://arxiv.org/abs/2510.19427v1",
        "abstract": "The modified universality hypothesis proposed by Jones et al. (2022) suggests\nthat adversarially robust models trained for a given task are highly similar.\nWe revisit the hypothesis and test its generality. While we verify Jones' main\nclaim of high representational similarity in specific settings, results are not\nconsistent across different datasets. We also discover that predictive behavior\ndoes not converge with increasing robustness and thus is not universal. We find\nthat differing predictions originate in the classification layer, but show that\nmore universal predictive behavior can be achieved with simple retraining of\nthe classifiers. Overall, our work points towards partial universality of\nneural networks in specific settings and away from notions of strict\nuniversality."
    },
    {
        "date": "2025-10",
        "title": "ColorAgent: Building A Robust, Personalized, and Interactive OS Agent",
        "author": "Ning Li, Qiqiang Lin, Zheng Wu, Xiaoyun Mo, Weiming Zhang, Yin Zhao, Xiangmou Qu, Jiamu Zhou, Jun Wang, Congmin Zheng, Yuanyi Song, Hongjiang Chen, Heyuan Huang, Jihong Wang, Jiaxin Yin, Jingwei Yu, Junwei Liao, Qiuying Peng, Xingyu Lou, Jun Wang, Weiwen Liu, Zhuosheng Zhang, and Weinan Zhang",
        "link": "http://arxiv.org/abs/2510.19386v1",
        "abstract": "With the advancements in hardware, software, and large language model\ntechnologies, the interaction between humans and operating systems has evolved\nfrom the command-line interface to the rapidly emerging AI agent interactions.\nBuilding an operating system (OS) agent capable of executing user instructions\nand faithfully following user desires is becoming a reality. In this technical\nreport, we present ColorAgent, an OS agent designed to engage in long-horizon,\nrobust interactions with the environment while also enabling personalized and\nproactive user interaction. To enable long-horizon interactions with the\nenvironment, we enhance the model's capabilities through step-wise\nreinforcement learning and self-evolving training, while also developing a\ntailored multi-agent framework that ensures generality, consistency, and\nrobustness. In terms of user interaction, we explore personalized user intent\nrecognition and proactive engagement, positioning the OS agent not merely as an\nautomation tool but as a warm, collaborative partner. We evaluate ColorAgent on\nthe AndroidWorld and AndroidLab benchmarks, achieving success rates of 77.2%\nand 50.7%, respectively, establishing a new state of the art. Nonetheless, we\nnote that current benchmarks are insufficient for a comprehensive evaluation of\nOS agents and propose further exploring directions in future work, particularly\nin the areas of evaluation paradigms, agent collaboration, and security. Our\ncode is available at https://github.com/MadeAgents/mobile-use."
    },
    {
        "date": "2025-10",
        "title": "AegisRF: Adversarial Perturbations Guided with Sensitivity for Protecting Intellectual Property of Neural Radiance Fields",
        "author": "Woo Jae Kim, Kyu Beom Han, Yoonki Cho, Youngju Na, Junsik Jung, Sooel Son, and Sung-eui Yoon",
        "link": "http://arxiv.org/abs/2510.19371v1",
        "abstract": "As Neural Radiance Fields (NeRFs) have emerged as a powerful tool for 3D\nscene representation and novel view synthesis, protecting their intellectual\nproperty (IP) from unauthorized use is becoming increasingly crucial. In this\nwork, we aim to protect the IP of NeRFs by injecting adversarial perturbations\nthat disrupt their unauthorized applications. However, perturbing the 3D\ngeometry of NeRFs can easily deform the underlying scene structure and thus\nsubstantially degrade the rendering quality, which has led existing attempts to\navoid geometric perturbations or restrict them to explicit spaces like meshes.\nTo overcome this limitation, we introduce a learnable sensitivity to quantify\nthe spatially varying impact of geometric perturbations on rendering quality.\nBuilding upon this, we propose AegisRF, a novel framework that consists of a\nPerturbation Field, which injects adversarial perturbations into the\npre-rendering outputs (color and volume density) of NeRF models to fool an\nunauthorized downstream target model, and a Sensitivity Field, which learns the\nsensitivity to adaptively constrain geometric perturbations, preserving\nrendering quality while disrupting unauthorized use. Our experimental\nevaluations demonstrate the generalized applicability of AegisRF across diverse\ndownstream tasks and modalities, including multi-view image classification and\nvoxel-based 3D localization, while maintaining high visual fidelity. Codes are\navailable at https://github.com/wkim97/AegisRF."
    },
    {
        "date": "2025-10",
        "title": "A New Type of Adversarial Examples",
        "author": "Xingyang Nie, Guojie Xiao, Su Pan, Biao Wang, Huilin Ge, and Tao Fang",
        "link": "http://arxiv.org/abs/2510.19347v1",
        "abstract": "Most machine learning models are vulnerable to adversarial examples, which\nposes security concerns on these models. Adversarial examples are crafted by\napplying subtle but intentionally worst-case modifications to examples from the\ndataset, leading the model to output a different answer from the original\nexample. In this paper, adversarial examples are formed in an exactly opposite\nmanner, which are significantly different from the original examples but result\nin the same answer. We propose a novel set of algorithms to produce such\nadversarial examples, including the negative iterative fast gradient sign\nmethod (NI-FGSM) and the negative iterative fast gradient method (NI-FGM),\nalong with their momentum variants: the negative momentum iterative fast\ngradient sign method (NMI-FGSM) and the negative momentum iterative fast\ngradient method (NMI-FGM). Adversarial examples constructed by these methods\ncould be used to perform an attack on machine learning systems in certain\noccasions. Moreover, our results show that the adversarial examples are not\nmerely distributed in the neighbourhood of the examples from the dataset;\ninstead, they are distributed extensively in the sample space."
    },
    {
        "date": "2025-10",
        "title": "LAPRAD: LLM-Assisted PRotocol Attack Discovery",
        "author": "R. Can Aygun, Yehuda Afek, Anat Bremler-Barr, and Leonard Kleinrock",
        "link": "http://arxiv.org/abs/2510.19264v1",
        "abstract": "With the goal of improving the security of Internet protocols, we seek\nfaster, semi-automatic methods to discover new vulnerabilities in protocols\nsuch as DNS, BGP, and others. To this end, we introduce the LLM-Assisted\nProtocol Attack Discovery (LAPRAD) methodology, enabling security researchers\nwith some DNS knowledge to efficiently uncover vulnerabilities that would\notherwise be hard to detect.\n  LAPRAD follows a three-stage process. In the first, we consult an LLM\n(GPT-o1) that has been trained on a broad corpus of DNS-related sources and\nprevious DDoS attacks to identify potential exploits. In the second stage, a\ndifferent LLM automatically constructs the corresponding attack configurations\nusing the ReACT approach implemented via LangChain (DNS zone file generation).\nFinally, in the third stage, we validate the attack's functionality and\neffectiveness.\n  Using LAPRAD, we uncovered three new DDoS attacks on the DNS protocol and\nrediscovered two recently reported ones that were not included in the LLM's\ntraining data. The first new attack employs a bait-and-switch technique to\ntrick resolvers into caching large, bogus DNSSEC RRSIGs, reducing their serving\ncapacity to as little as 6%. The second exploits large DNSSEC encryption\nalgorithms (RSA-4096) with multiple keys, thereby bypassing a recently\nimplemented default RRSet limit. The third leverages ANY-type responses to\nproduce a similar effect.\n  These variations of a cache-flushing DDoS attack, called SigCacheFlush,\ncircumvent existing patches, severely degrade resolver query capacity, and\nimpact the latest versions of major DNS resolver implementations."
    },
    {
        "date": "2025-10",
        "title": "Feature Space Adaptation for Robust Model Fine-Tuning",
        "author": "Peng Wang, Minghao Gu, and Qiang Huang",
        "link": "http://arxiv.org/abs/2510.19155v1",
        "abstract": "Catastrophic forgetting is a common issue in model fine-tuning, especially\nwhen the downstream domain contains limited labeled data or differs greatly\nfrom the pre-training distribution. Existing parameter-efficient fine-tuning\nmethods operate in the weight space by modifying or augmenting the pre-trained\nmodel's parameters, which can yield models overly specialized to the available\ndownstream data. To mitigate the risk of overwriting pre-trained knowledge and\nenhance robustness, we propose to fine-tune the pre-trained model in the\nfeature space. Two new fine-tuning methods are proposed: LoRFA (Low-Rank\nFeature Adaptation) and VeFA (Vector-Based Feature Adaptation). Feature space\nadaptation is inspired by the idea of effect equivalence modeling (EEM) of\ndownstream lurking variables causing distribution shifts, which posits that\nunobserved factors can be represented as the total equivalent amount on\nobserved features. By compensating for the effects of downstream lurking\nvariables via a lightweight feature-level transformation, the pre-trained\nrepresentations can be preserved, which improves model generalization under\ndistribution shift. We evaluate LoRFA and VeFA versus LoRA on image\nclassification, NLU, and NLG, covering both standard fine-tuning metrics and\nrobustness. Feature space adaptation achieves comparable fine-tuning results\nand consistently stronger robustness."
    },
    {
        "date": "2025-10",
        "title": "HAMLOCK: HArdware-Model LOgically Combined attacK",
        "author": "Sanskar Amgain, Daniel Lobo, Atri Chatterjee, Swarup Bhunia, and Fnu Suya",
        "link": "http://arxiv.org/abs/2510.19145v1",
        "abstract": "The growing use of third-party hardware accelerators (e.g., FPGAs, ASICs) for\ndeep neural networks (DNNs) introduces new security vulnerabilities.\nConventional model-level backdoor attacks, which only poison a model's weights\nto misclassify inputs with a specific trigger, are often detectable because the\nentire attack logic is embedded within the model (i.e., software), creating a\ntraceable layer-by-layer activation path.\n  This paper introduces the HArdware-Model Logically Combined Attack (HAMLOCK),\na far stealthier threat that distributes the attack logic across the\nhardware-software boundary. The software (model) is now only minimally altered\nby tuning the activations of few neurons to produce uniquely high activation\nvalues when a trigger is present. A malicious hardware Trojan detects those\nunique activations by monitoring the corresponding neurons' most significant\nbit or the 8-bit exponents and triggers another hardware Trojan to directly\nmanipulate the final output logits for misclassification.\n  This decoupled design is highly stealthy, as the model itself contains no\ncomplete backdoor activation path as in conventional attacks and hence, appears\nfully benign. Empirically, across benchmarks like MNIST, CIFAR10, GTSRB, and\nImageNet, HAMLOCK achieves a near-perfect attack success rate with a negligible\nclean accuracy drop. More importantly, HAMLOCK circumvents the state-of-the-art\nmodel-level defenses without any adaptive optimization. The hardware Trojan is\nalso undetectable, incurring area and power overheads as low as 0.01%, which is\neasily masked by process and environmental noise. Our findings expose a\ncritical vulnerability at the hardware-software interface, demanding new\ncross-layer defenses against this emerging threat."
    },
    {
        "date": "2025-10",
        "title": "Securing IoT Communications via Anomaly Traffic Detection: Synergy of Genetic Algorithm and Ensemble Method",
        "author": "Behnam Seyedi, and Octavian Postolache",
        "link": "http://arxiv.org/abs/2510.19121v1",
        "abstract": "The rapid growth of the Internet of Things (IoT) has transformed industries\nby enabling seamless data exchange among connected devices. However, IoT\nnetworks remain vulnerable to security threats such as denial of service (DoS)\nattacks, anomalous traffic, and data manipulation due to decentralized\narchitectures and limited resources. To address these issues, this paper\nproposes an advanced anomaly detection framework with three main phases. First,\ndata preprocessing is performed using the Median KS Test to remove noise,\nhandle missing values, and balance datasets for cleaner input. Second, a\nfeature selection phase employs a Genetic Algorithm combined with eagle\ninspired search strategies to identify the most relevant features, reduce\ndimensionality, and improve efficiency without sacrificing accuracy. Finally,\nan ensemble classifier integrates Decision Tree, Random Forest, and XGBoost\nalgorithms to achieve accurate and reliable anomaly detection. The proposed\nmodel demonstrates high adaptability and scalability across diverse IoT\nenvironments. Experimental results show that it outperforms existing methods by\nachieving 98 percent accuracy, 95 percent detection rate, and reductions in\nfalse positive (10 percent) and false negative (5 percent) rates. These results\nconfirm the framework effectiveness and robustness in improving IoT network\nsecurity against evolving cyber threats."
    },
    {
        "date": "2025-10",
        "title": "POLAR: Policy-based Layerwise Reinforcement Learning Method for Stealthy Backdoor Attacks in Federated Learning",
        "author": "Kuai Yu, Xiaoyu Wu, Peishen Yan, Qingqian Yang, Linshan Jiang, Hao Wang, Yang Hua, Tao Song, and Haibing Guan",
        "link": "http://arxiv.org/abs/2510.19056v1",
        "abstract": "Federated Learning (FL) enables decentralized model training across multiple\nclients without exposing local data, but its distributed feature makes it\nvulnerable to backdoor attacks. Despite early FL backdoor attacks modifying\nentire models, recent studies have explored the concept of backdoor-critical\n(BC) layers, which poison the chosen influential layers to maintain\nstealthiness while achieving high effectiveness. However, existing BC layers\napproaches rely on rule-based selection without consideration of the\ninterrelations between layers, making them ineffective and prone to detection\nby advanced defenses. In this paper, we propose POLAR (POlicy-based LAyerwise\nReinforcement learning), the first pipeline to creatively adopt RL to solve the\nBC layer selection problem in layer-wise backdoor attack. Different from other\ncommonly used RL paradigm, POLAR is lightweight with Bernoulli sampling. POLAR\ndynamically learns an attack strategy, optimizing layer selection using policy\ngradient updates based on backdoor success rate (BSR) improvements. To ensure\nstealthiness, we introduce a regularization constraint that limits the number\nof modified layers by penalizing large attack footprints. Extensive experiments\ndemonstrate that POLAR outperforms the latest attack methods by up to 40%\nagainst six state-of-the-art (SOTA) defenses."
    },
    {
        "date": "2025-10",
        "title": "Robust Driving QA through Metadata-Grounded Context and Task-Specific Prompts",
        "author": "Seungjun Yu, Junsung Park, Youngsun Lim, and Hyunjung Shim",
        "link": "http://arxiv.org/abs/2510.19001v1",
        "abstract": "We present a two-phase vision-language QA system for autonomous driving that\nanswers high-level perception, prediction, and planning questions. In Phase-1,\na large multimodal LLM (Qwen2.5-VL-32B) is conditioned on six-camera inputs, a\nshort temporal window of history, and a chain-of-thought prompt with few-shot\nexemplars. A self-consistency ensemble (multiple sampled reasoning chains)\nfurther improves answer reliability. In Phase-2, we augment the prompt with\nnuScenes scene metadata (object annotations, ego-vehicle state, etc.) and\ncategory-specific question instructions (separate prompts for perception,\nprediction, planning tasks). In experiments on a driving QA benchmark, our\napproach significantly outperforms the baseline Qwen2.5 models. For example,\nusing 5 history frames and 10-shot prompting in Phase-1 yields 65.1% overall\naccuracy (vs.62.61% with zero-shot); applying self-consistency raises this to\n66.85%. Phase-2 achieves 67.37% overall. Notably, the system maintains 96%\naccuracy under severe visual corruption. These results demonstrate that\ncarefully engineered prompts and contextual grounding can greatly enhance\nhigh-level driving QA with pretrained vision-language models."
    },
    {
        "date": "2025-10",
        "title": "The Black Tuesday Attack: how to crash the stock market with adversarial examples to financial forecasting models",
        "author": "Thomas Hofweber, Jefrey Bergl, Ian Reyes, and Amir Sadovnik",
        "link": "http://arxiv.org/abs/2510.18990v1",
        "abstract": "We investigate and defend the possibility of causing a stock market crash via\nsmall manipulations of individual stock values that together realize an\nadversarial example to financial forecasting models, causing these models to\nmake the self-fulfilling prediction of a crash. Such a crash triggered by an\nadversarial example would likely be hard to detect, since the model's\npredictions would be accurate and the interventions that would cause it are\nminor. This possibility is a major risk to financial stability and an\nopportunity for hostile actors to cause great economic damage to an adversary.\nThis threat also exists against individual stocks and the corresponding\nvaluation of individual companies. We outline how such an attack might proceed,\nwhat its theoretical basis is, how it can be directed towards a whole economy\nor an individual company, and how one might defend against it. We conclude that\nthis threat is vastly underappreciated and requires urgent research on how to\ndefend against it."
    },
    {
        "date": "2025-10",
        "title": "Towards Universal Solvers: Using PGD Attack in Active Learning to Increase Generalizability of Neural Operators as Knowledge Distillation from Numerical PDE Solvers",
        "author": "Yifei Sun",
        "link": "http://arxiv.org/abs/2510.18989v1",
        "abstract": "Nonlinear PDE solvers require fine space-time discretizations and local\nlinearizations, leading to high memory cost and slow runtimes. Neural operators\nsuch as FNOs and DeepONets offer fast single-shot inference by learning\nfunction-to-function mappings and truncating high-frequency components, but\nthey suffer from poor out-of-distribution (OOD) generalization, often failing\non inputs outside the training distribution. We propose an adversarial\nteacher-student distillation framework in which a differentiable numerical\nsolver supervises a compact neural operator while a PGD-style active sampling\nloop searches for worst-case inputs under smoothness and energy constraints to\nexpand the training set. Using differentiable spectral solvers enables\ngradient-based adversarial search and stabilizes sample mining. Experiments on\nBurgers and Navier-Stokes systems demonstrate that adversarial distillation\nsubstantially improves OOD robustness while preserving the low parameter cost\nand fast inference of neural operators."
    },
    {
        "date": "2025-10",
        "title": "sNVMe-oF: Secure and Efficient Disaggregated Storage",
        "author": "Marcin Chrapek, Meni Orenbach, Ahmad Atamli, Marcin Copik, Fritz Alder, and Torsten Hoefler",
        "link": "http://arxiv.org/abs/2510.18756v1",
        "abstract": "Disaggregated storage with NVMe-over-Fabrics (NVMe-oF) has emerged as the\nstandard solution in modern data centers, achieving superior performance,\nresource utilization, and power efficiency. Simultaneously, confidential\ncomputing (CC) is becoming the de facto security paradigm, enforcing stronger\nisolation and protection for sensitive workloads. However, securing\nstate-of-the-art storage with traditional CC methods struggles to scale and\ncompromises performance or security. To address these issues, we introduce\nsNVMe-oF, a storage management system extending the NVMe-oF protocol and\nadhering to the CC threat model by providing confidentiality, integrity, and\nfreshness guarantees. sNVMe-oF offers an appropriate control path and novel\nconcepts such as counter-leasing. sNVMe-oF also optimizes data path performance\nby leveraging NVMe metadata, introducing a new disaggregated Hazel Merkle Tree\n(HMT), and avoiding redundant IPSec protections. We achieve this without\nmodifying the NVMe-oF protocol. To prevent excessive resource usage while\ndelivering line rate, sNVMe-oF also uses accelerators of CC-capable smart NICs.\nWe prototype sNVMe-oF on an NVIDIA BlueField-3 and demonstrate how it can\nachieve as little as 2% performance degradation for synthetic patterns and AI\ntraining."
    },
    {
        "date": "2025-10",
        "title": "HarmNet: A Framework for Adaptive Multi-Turn Jailbreak Attacks on Large Language Models",
        "author": "Sidhant Narula, Javad Rafiei Asl, Mohammad Ghasemigol, Eduardo Blanco, and Daniel Takabi",
        "link": "http://arxiv.org/abs/2510.18728v1",
        "abstract": "Large Language Models (LLMs) remain vulnerable to multi-turn jailbreak\nattacks. We introduce HarmNet, a modular framework comprising ThoughtNet, a\nhierarchical semantic network; a feedback-driven Simulator for iterative query\nrefinement; and a Network Traverser for real-time adaptive attack execution.\nHarmNet systematically explores and refines the adversarial space to uncover\nstealthy, high-success attack paths. Experiments across closed-source and\nopen-source LLMs show that HarmNet outperforms state-of-the-art methods,\nachieving higher attack success rates. For example, on Mistral-7B, HarmNet\nachieves a 99.4% attack success rate, 13.9% higher than the best baseline.\nIndex terms: jailbreak attacks; large language models; adversarial framework;\nquery refinement."
    },
    {
        "date": "2025-10",
        "title": "Bayesian Low-Rank Factorization for Robust Model Adaptation",
        "author": "Enes Yavuz Ugan, Ngoc-Quan Pham, and Alexander Waibel",
        "link": "http://arxiv.org/abs/2510.18723v1",
        "abstract": "Large speech foundation models achieve strong performance across many\ndomains, but they often require adaptation to handle local needs such as\ncode-switching, where speakers mix languages within the same utterance. Direct\nfine-tuning of these models risks overfitting to the target domain and\noverwriting the broad capabilities of the base model. To address this\nchallenge, we explore Bayesian factorized adapters for speech foundation\nmodels, which place priors near zero to achieve sparser adaptation matrices and\nthereby retain general performance while adapting to specific domains. We apply\nour approach to the Whisper model and evaluate on different multilingual\ncode-switching scenarios. Our results show only minimal adaptation loss while\nsignificantly reducing catastrophic forgetting of the base model. Compared to\nLoRA, our method achieves a backward gain of 54% with only a 4% drop on the new\ndomain. These findings highlight the effectiveness of Bayesian adaptation for\nfine-tuning speech foundation models without sacrificing generalization."
    },
    {
        "date": "2025-10",
        "title": "Quantifying Security for Networked Control Systems: A Review",
        "author": "Sribalaji C. Anand, Anh Tung Nguyen, Andr\u00e9 M. H. Teixeira, Henrik Sandberg, and Karl H. Johansson",
        "link": "http://arxiv.org/abs/2510.18645v1",
        "abstract": "Networked Control Systems (NCSs) are integral in critical infrastructures\nsuch as power grids, transportation networks, and production systems. Ensuring\nthe resilient operation of these large-scale NCSs against cyber-attacks is\ncrucial for societal well-being. Over the past two decades, extensive research\nhas been focused on developing metrics to quantify the vulnerabilities of NCSs\nagainst attacks. Once the vulnerabilities are quantified, mitigation strategies\ncan be employed to enhance system resilience. This article provides a\ncomprehensive overview of methods developed for assessing NCS vulnerabilities\nand the corresponding mitigation strategies. Furthermore, we emphasize the\nimportance of probabilistic risk metrics to model vulnerabilities under\nadversaries with imperfect process knowledge. The article concludes by\noutlining promising directions for future research."
    },
    {
        "date": "2025-10",
        "title": "DRsam: Detection of Fault-Based Microarchitectural Side-Channel Attacks in RISC-V Using Statistical Preprocessing and Association Rule Mining",
        "author": "Muhammad Hassan, Maria Mushtaq, Jaan Raik, and Tara Ghasempouri",
        "link": "http://arxiv.org/abs/2510.18612v1",
        "abstract": "RISC-V processors are becoming ubiquitous in critical applications, but their\nsusceptibility to microarchitectural side-channel attacks is a serious concern.\nDetection of microarchitectural attacks in RISC-V is an emerging research topic\nthat is relatively underexplored, compared to x86 and ARM. The first line of\nwork to detect flush+fault-based microarchitectural attacks in RISC-V leverages\nMachine Learning (ML) models, yet it leaves several practical aspects that need\nfurther investigation. To address overlooked issues, we leveraged gem5 and\npropose a new detection method combining statistical preprocessing and\nassociation rule mining having reconfiguration capabilities to generalize the\ndetection method for any microarchitectural attack. The performance comparison\nwith state-of-the-art reveals that the proposed detection method achieves up to\n5.15% increase in accuracy, 7% rise in precision, and 3.91% improvement in\nrecall under the cryptographic, computational, and memory-intensive workloads\nalongside its flexibility to detect new variant of flush+fault attack.\nMoreover, as the attack detection relies on association rules, their\nhuman-interpretable nature provides deep insight to understand\nmicroarchitectural behavior during the execution of attack and benign\napplications."
    },
    {
        "date": "2025-10",
        "title": "Channel-Aware Vector Quantization for Robust Semantic Communication on Discrete Channels",
        "author": "Zian Meng, Qiang Li, Wenqian Tang, Mingdie Yan, and Xiaohu Ge",
        "link": "http://arxiv.org/abs/2510.18604v1",
        "abstract": "Deep learning-based semantic communication has largely relied on analog or\nsemi-digital transmission, which limits compatibility with modern digital\ncommunication infrastructures. Recent studies have employed vector quantization\n(VQ) to enable discrete semantic transmission, yet existing methods neglect\nchannel state information during codebook optimization, leading to suboptimal\nrobustness. To bridge this gap, we propose a channel-aware vector quantization\n(CAVQ) algorithm within a joint source-channel coding (JSCC) framework, termed\nVQJSCC, established on a discrete memoryless channel. In this framework,\nsemantic features are discretized and directly mapped to modulation\nconstellation symbols, while CAVQ integrates channel transition probabilities\ninto the quantization process, aligning easily confused symbols with\nsemantically similar codewords. A multi-codebook alignment mechanism is further\nintroduced to handle mismatches between codebook order and modulation order by\ndecomposing the transmission stream into multiple independently optimized\nsubchannels. Experimental results demonstrate that VQJSCC effectively mitigates\nthe digital cliff effect, achieves superior reconstruction quality across\nvarious modulation schemes, and outperforms state-of-the-art digital semantic\ncommunication baselines in both robustness and efficiency."
    },
    {
        "date": "2025-10",
        "title": "Robustness Verification of Graph Neural Networks Via Lightweight Satisfiability Testing",
        "author": "Chia-Hsuan Lu, Tony Tan, and Michael Benedikt",
        "link": "http://arxiv.org/abs/2510.18591v1",
        "abstract": "Graph neural networks (GNNs) are the predominant architecture for learning\nover graphs. As with any machine learning model, and important issue is the\ndetection of adversarial attacks, where an adversary can change the output with\na small perturbation of the input. Techniques for solving the adversarial\nrobustness problem - determining whether such an attack exists - were\noriginally developed for image classification, but there are variants for many\nother machine learning architectures. In the case of graph learning, the attack\nmodel usually considers changes to the graph structure in addition to or\ninstead of the numerical features of the input, and the state of the art\ntechniques in the area proceed via reduction to constraint solving, working on\ntop of powerful solvers, e.g. for mixed integer programming. We show that it is\npossible to improve on the state of the art in structural robustness by\nreplacing the use of powerful solvers by calls to efficient partial solvers,\nwhich run in polynomial time but may be incomplete. We evaluate our tool\nRobLight on a diverse set of GNN variants and datasets."
    },
    {
        "date": "2025-10",
        "title": "Forward to Hell? On the Potentials of Misusing Transparent DNS Forwarders in Reflective Amplification Attacks",
        "author": "Maynard Koch, Florian Dolzmann, Thomas C. Schmidt, and Matthias W\u00e4hlisch",
        "link": "http://arxiv.org/abs/2510.18572v2",
        "abstract": "The DNS infrastructure is infamous for facilitating reflective amplification\nattacks. Various countermeasures such as server shielding, access control, rate\nlimiting, and protocol restrictions have been implemented. Still, the threat\nremains throughout the deployment of DNS servers. In this paper, we report on\nand evaluate the often unnoticed threat that derives from transparent DNS\nforwarders, a widely deployed, incompletely functional set of DNS components.\nTransparent DNS forwarders transfer DNS requests without rebuilding packets\nwith correct source addresses. As such, transparent forwarders feed DNS\nrequests into (mainly powerful and anycasted) open recursive resolvers, which\nthereby can be misused to participate unwillingly in distributed reflective\namplification attacks. We show how transparent forwarders raise severe threats\nto the Internet infrastructure. They easily circumvent rate limiting and\nachieve an additional, scalable impact via the DNS anycast infrastructure. We\nempirically verify this scaling behavior up to a factor of 14. Transparent\nforwarders can also assist in bypassing firewall rules that protect recursive\nresolvers, making these shielded infrastructure entities part of the global DNS\nattack surface."
    },
    {
        "date": "2025-10",
        "title": "The Trust Paradox in LLM-Based Multi-Agent Systems: When Collaboration Becomes a Security Vulnerability",
        "author": "Zijie Xu, Minfeng Qi, Shiqing Wu, Lefeng Zhang, Qiwen Wei, Han He, and Ningran Li",
        "link": "http://arxiv.org/abs/2510.18563v1",
        "abstract": "Multi-agent systems powered by large language models are advancing rapidly,\nyet the tension between mutual trust and security remains underexplored. We\nintroduce and empirically validate the Trust-Vulnerability Paradox (TVP):\nincreasing inter-agent trust to enhance coordination simultaneously expands\nrisks of over-exposure and over-authorization. To investigate this paradox, we\nconstruct a scenario-game dataset spanning 3 macro scenes and 19 sub-scenes,\nand run extensive closed-loop interactions with trust explicitly parameterized.\nUsing Minimum Necessary Information (MNI) as the safety baseline, we propose\ntwo unified metrics: Over-Exposure Rate (OER) to detect boundary violations,\nand Authorization Drift (AD) to capture sensitivity to trust levels. Results\nacross multiple model backends and orchestration frameworks reveal consistent\ntrends: higher trust improves task success but also heightens exposure risks,\nwith heterogeneous trust-to-risk mappings across systems. We further examine\ndefenses such as Sensitive Information Repartitioning and Guardian-Agent\nenablement, both of which reduce OER and attenuate AD. Overall, this study\nformalizes TVP, establishes reproducible baselines with unified metrics, and\ndemonstrates that trust must be modeled and scheduled as a first-class security\nvariable in multi-agent system design."
    },
    {
        "date": "2025-10",
        "title": "Occluded nuScenes: A Multi-Sensor Dataset for Evaluating Perception Robustness in Automated Driving",
        "author": "Sanjay Kumar, Tim Brophy, Reenu Mohandas, Eoin Martino Grua, Ganesh Sistu, Valentina Donzella, and Ciaran Eising",
        "link": "http://arxiv.org/abs/2510.18552v2",
        "abstract": "Robust perception in automated driving requires reliable performance under\nadverse conditions, where sensors may be affected by partial failures or\nenvironmental occlusions. Although existing autonomous driving datasets\ninherently contain sensor noise and environmental variability, very few enable\ncontrolled, parameterised, and reproducible degradations across multiple\nsensing modalities. This gap limits the ability to systematically evaluate how\nperception and fusion architectures perform under well-defined adverse\nconditions. To address this limitation, we introduce the Occluded nuScenes\nDataset, a novel extension of the widely used nuScenes benchmark. For the\ncamera modality, we release both the full and mini versions with four types of\nocclusions, two adapted from public implementations and two newly designed. For\nradar and LiDAR, we provide parameterised occlusion scripts that implement\nthree types of degradations each, enabling flexible and repeatable generation\nof occluded data. This resource supports consistent, reproducible evaluation of\nperception models under partial sensor failures and environmental interference.\nBy releasing the first multi-sensor occlusion dataset with controlled and\nreproducible degradations, we aim to advance research on robust sensor fusion,\nresilience analysis, and safety-critical perception in automated driving."
    },
    {
        "date": "2025-10",
        "title": "PP3D: An In-Browser Vision-Based Defense Against Web Behavior Manipulation Attacks",
        "author": "Spencer King, Irfan Ozen, Karthika Subramani, Saranyan Senthivel, Phani Vadrevu, and Roberto Perdisci",
        "link": "http://arxiv.org/abs/2510.18465v1",
        "abstract": "Web-based behavior-manipulation attacks (BMAs) - such as scareware, fake\nsoftware downloads, tech support scams, etc. - are a class of social\nengineering (SE) attacks that exploit human decision-making vulnerabilities.\nThese attacks remain under-studied compared to other attacks such as\ninformation harvesting attacks (e.g., phishing) or malware infections. Prior\ntechnical work has primarily focused on measuring BMAs, offering little in the\nway of generic defenses.\n  To address this gap, we introduce Pixel Patrol 3D (PP3D), the first\nend-to-end browser framework for discovering, detecting, and defending against\nbehavior-manipulating SE attacks in real time. PP3D consists of a visual\ndetection model implemented within a browser extension, which deploys the model\nclient-side to protect users across desktop and mobile devices while preserving\nprivacy.\n  Our evaluation shows that PP3D can achieve above 99% detection rate at 1%\nfalse positives, while maintaining good latency and overhead performance across\ndevices. Even when faced with new BMA samples collected months after training\nthe detection model, our defense system can still achieve above 97% detection\nrate at 1% false positives. These results demonstrate that our framework offers\na practical, effective, and generalizable defense against a broad and evolving\nclass of web behavior-manipulation attacks."
    },
    {
        "date": "2025-10",
        "title": "Heterogeneous Adversarial Play in Interactive Environments",
        "author": "Manjie Xu, Xinyi Yang, Jiayu Zhan, Wei Liang, Chi Zhang, and Yixin Zhu",
        "link": "http://arxiv.org/abs/2510.18407v1",
        "abstract": "Self-play constitutes a fundamental paradigm for autonomous skill\nacquisition, whereby agents iteratively enhance their capabilities through\nself-directed environmental exploration. Conventional self-play frameworks\nexploit agent symmetry within zero-sum competitive settings, yet this approach\nproves inadequate for open-ended learning scenarios characterized by inherent\nasymmetry. Human pedagogical systems exemplify asymmetric instructional\nframeworks wherein educators systematically construct challenges calibrated to\nindividual learners' developmental trajectories. The principal challenge\nresides in operationalizing these asymmetric, adaptive pedagogical mechanisms\nwithin artificial systems capable of autonomously synthesizing appropriate\ncurricula without predetermined task hierarchies. Here we present Heterogeneous\nAdversarial Play (HAP), an adversarial Automatic Curriculum Learning framework\nthat formalizes teacher-student interactions as a minimax optimization wherein\ntask-generating instructor and problem-solving learner co-evolve through\nadversarial dynamics. In contrast to prevailing ACL methodologies that employ\nstatic curricula or unidirectional task selection mechanisms, HAP establishes a\nbidirectional feedback system wherein instructors continuously recalibrate task\ncomplexity in response to real-time learner performance metrics. Experimental\nvalidation across multi-task learning domains demonstrates that our framework\nachieves performance parity with SOTA baselines while generating curricula that\nenhance learning efficacy in both artificial agents and human subjects."
    },
    {
        "date": "2025-10",
        "title": "Entropy-Enhanced Conformal Features from Ricci Flow for Robust Alzheimer's Disease Classification",
        "author": "F. Ahmadi, B. Bidabad, and H. Nasiri",
        "link": "http://arxiv.org/abs/2510.18396v1",
        "abstract": "Background and Objective: In brain imaging, geometric surface models are\nessential for analyzing the 3D shapes of anatomical structures. Alzheimer's\ndisease (AD) is associated with significant cortical atrophy, making such shape\nanalysis a valuable diagnostic tool. The objective of this study is to\nintroduce and validate a novel local surface representation method for the\nautomated and accurate diagnosis of AD. Methods: The study utilizes T1-weighted\nMRI scans from 160 participants (80 AD patients and 80 healthy controls) from\nthe Alzheimer's Disease Neuroimaging Initiative (ADNI). Cortical surface models\nwere reconstructed from the MRI data using Freesurfer. Key geometric attributes\nwere computed from the 3D meshes. Area distortion and conformal factor were\nderived using Ricci flow for conformal parameterization, while Gaussian\ncurvature was calculated directly from the mesh geometry. Shannon entropy was\napplied to these three features to create compact and informative feature\nvectors. The feature vectors were used to train and evaluate a suite of\nclassifiers (e.g. XGBoost, MLP, Logistic Regression, etc.). Results:\nStatistical significance of performance differences between classifiers was\nevaluated using paired Welch's t-test. The method proved highly effective in\ndistinguishing AD patients from healthy controls. The Multi-Layer Perceptron\n(MLP) and Logistic Regression classifiers outperformed all others, achieving an\naccuracy and F$_1$ Score of 98.62%. Conclusions: This study confirms that the\nentropy of conformally-derived geometric features provides a powerful and\nrobust metric for cortical morphometry. The high classification accuracy\nunderscores the method's potential to enhance the study and diagnosis of\nAlzheimer's disease, offering a straightforward yet powerful tool for clinical\nresearch applications."
    },
    {
        "date": "2025-10",
        "title": "S2AP: Score-space Sharpness Minimization for Adversarial Pruning",
        "author": "Giorgio Piras, Qi Zhao, Fabio Brau, Maura Pintor, Christian Wressnegger, and Battista Biggio",
        "link": "http://arxiv.org/abs/2510.18381v1",
        "abstract": "Adversarial pruning methods have emerged as a powerful tool for compressing\nneural networks while preserving robustness against adversarial attacks. These\nmethods typically follow a three-step pipeline: (i) pretrain a robust model,\n(ii) select a binary mask for weight pruning, and (iii) finetune the pruned\nmodel. To select the binary mask, these methods minimize a robust loss by\nassigning an importance score to each weight, and then keep the weights with\nthe highest scores. However, this score-space optimization can lead to sharp\nlocal minima in the robust loss landscape and, in turn, to an unstable mask\nselection, reducing the robustness of adversarial pruning methods. To overcome\nthis issue, we propose a novel plug-in method for adversarial pruning, termed\nScore-space Sharpness-aware Adversarial Pruning (S2AP). Through our method, we\nintroduce the concept of score-space sharpness minimization, which operates\nduring the mask search by perturbing importance scores and minimizing the\ncorresponding robust loss. Extensive experiments across various datasets,\nmodels, and sparsity levels demonstrate that S2AP effectively minimizes\nsharpness in score space, stabilizing the mask selection, and ultimately\nimproving the robustness of adversarial pruning methods."
    },
    {
        "date": "2025-10",
        "title": "Scalable, Explainable and Provably Robust Anomaly Detection with One-Step Flow Matching",
        "author": "Zhong Li, Qi Huang, Yuxuan Zhu, Lincen Yang, Mohammad Mohammadi Amiri, Niki van Stein, and Matthijs van Leeuwen",
        "link": "http://arxiv.org/abs/2510.18328v1",
        "abstract": "We introduce Time-Conditioned Contraction Matching (TCCM), a novel method for\nsemi-supervised anomaly detection in tabular data. TCCM is inspired by flow\nmatching, a recent generative modeling framework that learns velocity fields\nbetween probability distributions and has shown strong performance compared to\ndiffusion models and generative adversarial networks. Instead of directly\napplying flow matching as originally formulated, TCCM builds on its core idea\n-- learning velocity fields between distributions -- but simplifies the\nframework by predicting a time-conditioned contraction vector toward a fixed\ntarget (the origin) at each sampled time step. This design offers three key\nadvantages: (1) a lightweight and scalable training objective that removes the\nneed for solving ordinary differential equations during training and inference;\n(2) an efficient scoring strategy called one time-step deviation, which\nquantifies deviation from expected contraction behavior in a single forward\npass, addressing the inference bottleneck of existing continuous-time models\nsuch as DTE (a diffusion-based model with leading anomaly detection accuracy\nbut heavy inference cost); and (3) explainability and provable robustness, as\nthe learned velocity field operates directly in input space, making the anomaly\nscore inherently feature-wise attributable; moreover, the score function is\nLipschitz-continuous with respect to the input, providing theoretical\nguarantees under small perturbations. Extensive experiments on the ADBench\nbenchmark show that TCCM strikes a favorable balance between detection accuracy\nand inference cost, outperforming state-of-the-art methods -- especially on\nhigh-dimensional and large-scale datasets. The source code is available at our\nGitHub repository."
    },
    {
        "date": "2025-10",
        "title": "Genesis: Evolving Attack Strategies for LLM Web Agent Red-Teaming",
        "author": "Zheng Zhang, Jiarui He, Yuchen Cai, Deheng Ye, Peilin Zhao, Ruili Feng, and Hao Wang",
        "link": "http://arxiv.org/abs/2510.18314v1",
        "abstract": "As large language model (LLM) agents increasingly automate complex web tasks,\nthey boost productivity while simultaneously introducing new security risks.\nHowever, relevant studies on web agent attacks remain limited. Existing\nred-teaming approaches mainly rely on manually crafted attack strategies or\nstatic models trained offline. Such methods fail to capture the underlying\nbehavioral patterns of web agents, making it difficult to generalize across\ndiverse environments. In web agent attacks, success requires the continuous\ndiscovery and evolution of attack strategies. To this end, we propose Genesis,\na novel agentic framework composed of three modules: Attacker, Scorer, and\nStrategist. The Attacker generates adversarial injections by integrating the\ngenetic algorithm with a hybrid strategy representation. The Scorer evaluates\nthe target web agent's responses to provide feedback. The Strategist\ndynamically uncovers effective strategies from interaction logs and compiles\nthem into a continuously growing strategy library, which is then re-deployed to\nenhance the Attacker's effectiveness. Extensive experiments across various web\ntasks show that our framework discovers novel strategies and consistently\noutperforms existing attack baselines."
    },
    {
        "date": "2025-10",
        "title": "RESCUE: Retrieval Augmented Secure Code Generation",
        "author": "Jiahao Shi, and Tianyi Zhang",
        "link": "http://arxiv.org/abs/2510.18204v1",
        "abstract": "Despite recent advances, Large Language Models (LLMs) still generate\nvulnerable code. Retrieval-Augmented Generation (RAG) has the potential to\nenhance LLMs for secure code generation by incorporating external security\nknowledge. However, the conventional RAG design struggles with the noise of raw\nsecurity-related documents, and existing retrieval methods overlook the\nsignificant security semantics implicitly embedded in task descriptions. To\naddress these issues, we propose RESCUE, a new RAG framework for secure code\ngeneration with two key innovations. First, we propose a hybrid knowledge base\nconstruction method that combines LLM-assisted cluster-then-summarize\ndistillation with program slicing, producing both high-level security\nguidelines and concise, security-focused code examples. Second, we design a\nhierarchical multi-faceted retrieval to traverse the constructed knowledge base\nfrom top to bottom and integrates multiple security-critical facts at each\nhierarchical level, ensuring comprehensive and accurate retrieval. We evaluated\nRESCUE on four benchmarks and compared it with five state-of-the-art secure\ncode generation methods on six LLMs. The results demonstrate that RESCUE\nimproves the SecurePass@1 metric by an average of 4.8 points, establishing a\nnew state-of-the-art performance for security. Furthermore, we performed\nin-depth analysis and ablation studies to rigorously validate the effectiveness\nof individual components in RESCUE."
    },
    {
        "date": "2025-10",
        "title": "DuoLens: A Framework for Robust Detection of Machine-Generated Multilingual Text and Code",
        "author": "Shriyansh Agrawal, Aidan Lau, Sanyam Shah, Ahan M R, Kevin Zhu, Sunishchal Dev, and Vasu Sharma",
        "link": "http://arxiv.org/abs/2510.18904v1",
        "abstract": "The prevalence of Large Language Models (LLMs) for generating multilingual\ntext and source code has only increased the imperative for machine-generated\ncontent detectors to be accurate and efficient across domains. Current\ndetectors, predominantly utilizing zero-shot methods, such as Fast DetectGPT or\nGPTZero, either incur high computational cost or lack sufficient accuracy,\noften with a trade-off between the two, leaving room for further improvement.\nTo address these gaps, we propose the fine-tuning of encoder-only Small\nLanguage Models (SLMs), in particular, the pre-trained models of RoBERTA and\nCodeBERTa using specialized datasets on source code and other natural language\nto prove that for the task of binary classification, SLMs outperform LLMs by a\nhuge margin whilst using a fraction of compute. Our encoders achieve AUROC $=\n0.97$ to $0.99$ and macro-F1 $0.89$ to $0.94$ while reducing latency by\n$8$-$12\\times$ and peak VRAM by $3$-$5\\times$ at $512$-token inputs. Under\ncross-generator shifts and adversarial transformations (paraphrase,\nback-translation; code formatting/renaming), performance retains $\\geq 92%$ of\nclean AUROC. We release training and evaluation scripts with seeds and configs;\na reproducibility checklist is also included."
    },
    {
        "date": "2025-10",
        "title": "AgentChangeBench: A Multi-Dimensional Evaluation Framework for Goal-Shift Robustness in Conversational AI",
        "author": "Manik Rana, Calissa Man, Anotida Expected Msiiwa, Jeffrey Paine, Kevin Zhu, Sunishchal Dev, Vasu Sharma, and Ahan M R",
        "link": "http://arxiv.org/abs/2510.18170v1",
        "abstract": "Goal changes are a defining feature of real world multi-turn interactions,\nyet current agent benchmarks primarily evaluate static objectives or one-shot\ntool use. We introduce AgentChangeBench, a benchmark explicitly designed to\nmeasure how tool augmented language model agents adapt to mid dialogue goal\nshifts across three enterprise domains. Our framework formalizes evaluation\nthrough four complementary metrics: Task Success Rate (TSR) for effectiveness,\nTool Use Efficiency (TUE) for reliability, Tool Call Redundancy Rate (TCRR) for\nwasted effort, and Goal-Shift Recovery Time (GSRT) for adaptation latency.\nAgentChangeBench comprises 2,835 task sequences and five user personas, each\ndesigned to trigger realistic shift points in ongoing workflows. Using this\nsetup, we evaluate several frontier models and uncover sharp contrasts obscured\nby traditional $\\text{pass}@k$ scores: for example, GPT-4o reaches $92.2\\%$\nrecovery on airline booking shifts while Gemini collapses to $48.6\\%$, and\nretail tasks show near perfect parameter validity yet redundancy rates above\n$80\\%$, revealing major inefficiencies. These findings demonstrate that high\nraw accuracy does not imply robustness under dynamic goals, and that explicit\nmeasurement of recovery time and redundancy is essential. AgentChangeBench\nestablishes a reproducible testbed for diagnosing and improving agent\nresilience in realistic enterprise settings."
    },
    {
        "date": "2025-10",
        "title": "Black-Box Evasion Attacks on Data-Driven Open RAN Apps: Tailored Design and Experimental Evaluation",
        "author": "Pranshav Gajjar, Molham Khoja, Abiodun Ganiyu, Marc Juarez, Mahesh K. Marina, Andrew Lehane, and Vijay K. Shah",
        "link": "http://arxiv.org/abs/2510.18160v1",
        "abstract": "The impending adoption of Open Radio Access Network (O-RAN) is fueling\ninnovation in the RAN towards data-driven operation. Unlike traditional RAN\nwhere the RAN data and its usage is restricted within proprietary and\nmonolithic RAN equipment, the O-RAN architecture opens up access to RAN data\nvia RAN intelligent controllers (RICs), to third-party machine learning (ML)\npowered applications - rApps and xApps - to optimize RAN operations.\nConsequently, a major focus has been placed on leveraging RAN data to unlock\ngreater efficiency gains. However, there is an increasing recognition that RAN\ndata access to apps could become a source of vulnerability and be exploited by\nmalicious actors. Motivated by this, we carry out a comprehensive investigation\nof data vulnerabilities on both xApps and rApps, respectively hosted in Near-\nand Non-real-time (RT) RIC components of O-RAN. We qualitatively analyse the\nO-RAN security mechanisms and limitations for xApps and rApps, and consider a\nthreat model informed by this analysis. We design a viable and effective\nblack-box evasion attack strategy targeting O-RAN RIC Apps while accounting for\nthe stringent timing constraints and attack effectiveness. The strategy employs\nfour key techniques: the model cloning algorithm, input-specific perturbations,\nuniversal adversarial perturbations (UAPs), and targeted UAPs. This strategy\ntargets ML models used by both xApps and rApps within the O-RAN system, aiming\nto degrade network performance. We validate the effectiveness of the designed\nevasion attack strategy and quantify the scale of performance degradation using\na real-world O-RAN testbed and emulation environments. Evaluation is conducted\nusing the Interference Classification xApp and the Power Saving rApp as\nrepresentatives for near-RT and non-RT RICs. We also show that the attack\nstrategy is effective against prominent defense techniques for adversarial ML."
    },
    {
        "date": "2025-10",
        "title": "RL-Driven Security-Aware Resource Allocation Framework for UAV-Assisted O-RAN",
        "author": "Zaineh Abughazzah, Emna Baccour, Loay Ismail, Amr Mohamed, and Mounir Hamdi",
        "link": "http://arxiv.org/abs/2510.18084v1",
        "abstract": "The integration of Unmanned Aerial Vehicles (UAVs) into Open Radio Access\nNetworks (O-RAN) enhances communication in disaster management and Search and\nRescue (SAR) operations by ensuring connectivity when infrastructure fails.\nHowever, SAR scenarios demand stringent security and low-latency communication,\nas delays or breaches can compromise mission success. While UAVs serve as\nmobile relays, they introduce challenges in energy consumption and resource\nmanagement, necessitating intelligent allocation strategies. Existing\nUAV-assisted O-RAN approaches often overlook the joint optimization of\nsecurity, latency, and energy efficiency in dynamic environments. This paper\nproposes a novel Reinforcement Learning (RL)-based framework for dynamic\nresource allocation in UAV relays, explicitly addressing these trade-offs. Our\napproach formulates an optimization problem that integrates security-aware\nresource allocation, latency minimization, and energy efficiency, which is\nsolved using RL. Unlike heuristic or static methods, our framework adapts in\nreal-time to network dynamics, ensuring robust communication. Simulations\ndemonstrate superior performance compared to heuristic baselines, achieving\nenhanced security and energy efficiency while maintaining ultra-low latency in\nSAR scenarios."
    },
    {
        "date": "2025-10",
        "title": "Attention-Guided Deep Adversarial Temporal Subspace Clustering (A-DATSC) Model for multivariate spatiotemporal data",
        "author": "Francis Ndikum Nji, Vandana Janeja, and Jianwu Wang",
        "link": "http://arxiv.org/abs/2510.18004v1",
        "abstract": "Deep subspace clustering models are vital for applications such as snowmelt\ndetection, sea ice tracking, crop health monitoring, infectious disease\nmodeling, network load prediction, and land-use planning, where multivariate\nspatiotemporal data exhibit complex temporal dependencies and reside on\nmultiple nonlinear manifolds beyond the capability of traditional clustering\nmethods. These models project data into a latent space where samples lie in\nlinear subspaces and exploit the self-expressiveness property to uncover\nintrinsic relationships. Despite their success, existing methods face major\nlimitations: they use shallow autoencoders that ignore clustering errors,\nemphasize global features while neglecting local structure, fail to model\nlong-range dependencies and positional information, and are rarely applied to\n4D spatiotemporal data. To address these issues, we propose A-DATSC\n(Attention-Guided Deep Adversarial Temporal Subspace Clustering), a model\ncombining a deep subspace clustering generator and a quality-verifying\ndiscriminator. The generator, inspired by U-Net, preserves spatial and temporal\nintegrity through stacked TimeDistributed ConvLSTM2D layers, reducing\nparameters and enhancing generalization. A graph attention transformer based\nself-expressive network captures local spatial relationships, global\ndependencies, and both short- and long-range correlations. Experiments on three\nreal-world multivariate spatiotemporal datasets show that A-DATSC achieves\nsubstantially superior clustering performance compared to state-of-the-art deep\nsubspace clustering models."
    },
    {
        "date": "2025-10",
        "title": "GAS: Improving Discretization of Diffusion ODEs via Generalized Adversarial Solver",
        "author": "Aleksandr Oganov, Ilya Bykov, Eva Neudachina, Mishan Aliev, Alexander Tolmachev, Alexander Sidorov, Aleksandr Zuev, Andrey Okhotin, Denis Rakitin, and Aibek Alanov",
        "link": "http://arxiv.org/abs/2510.17699v1",
        "abstract": "While diffusion models achieve state-of-the-art generation quality, they\nstill suffer from computationally expensive sampling. Recent works address this\nissue with gradient-based optimization methods that distill a few-step ODE\ndiffusion solver from the full sampling process, reducing the number of\nfunction evaluations from dozens to just a few. However, these approaches often\nrely on intricate training techniques and do not explicitly focus on preserving\nfine-grained details. In this paper, we introduce the Generalized Solver: a\nsimple parameterization of the ODE sampler that does not require additional\ntraining tricks and improves quality over existing approaches. We further\ncombine the original distillation loss with adversarial training, which\nmitigates artifacts and enhances detail fidelity. We call the resulting method\nthe Generalized Adversarial Solver and demonstrate its superior performance\ncompared to existing solver training methods under similar resource\nconstraints. Code is available at https://github.com/3145tttt/GAS."
    },
    {
        "date": "2025-10",
        "title": "CrossGuard: Safeguarding MLLMs against Joint-Modal Implicit Malicious Attacks",
        "author": "Xu Zhang, Hao Li, and Zhichao Lu",
        "link": "http://arxiv.org/abs/2510.17687v1",
        "abstract": "Multimodal Large Language Models (MLLMs) achieve strong reasoning and\nperception capabilities but are increasingly vulnerable to jailbreak attacks.\nWhile existing work focuses on explicit attacks, where malicious content\nresides in a single modality, recent studies reveal implicit attacks, in which\nbenign text and image inputs jointly express unsafe intent. Such joint-modal\nthreats are difficult to detect and remain underexplored, largely due to the\nscarcity of high-quality implicit data. We propose ImpForge, an automated\nred-teaming pipeline that leverages reinforcement learning with tailored reward\nmodules to generate diverse implicit samples across 14 domains. Building on\nthis dataset, we further develop CrossGuard, an intent-aware safeguard\nproviding robust and comprehensive defense against both explicit and implicit\nthreats. Extensive experiments across safe and unsafe benchmarks, implicit and\nexplicit attacks, and multiple out-of-domain settings demonstrate that\nCrossGuard significantly outperforms existing defenses, including advanced\nMLLMs and guardrails, achieving stronger security while maintaining high\nutility. This offers a balanced and practical solution for enhancing MLLM\nrobustness against real-world multimodal threats."
    },
    {
        "date": "2025-10",
        "title": "ZACH-ViT: A Zero-Token Vision Transformer with ShuffleStrides Data Augmentation for Robust Lung Ultrasound Classification",
        "author": "Athanasios Angelakis, Amne Mousa, Micah L. A. Heldeweg, Laurens A. Biesheuvel, Mark A. Haaksma, Jasper M. Smit, Pieter R. Tuinman, and Paul W. G. Elbers",
        "link": "http://arxiv.org/abs/2510.17650v1",
        "abstract": "Differentiating cardiogenic pulmonary oedema (CPE) from non-cardiogenic and\nstructurally normal lungs in lung ultrasound (LUS) videos remains challenging\ndue to the high visual variability of non-cardiogenic inflammatory patterns\n(NCIP/ARDS-like), interstitial lung disease, and healthy lungs. This\nheterogeneity complicates automated classification as overlapping B-lines and\npleural artefacts are common. We introduce ZACH-ViT (Zero-token Adaptive\nCompact Hierarchical Vision Transformer), a 0.25 M-parameter Vision Transformer\nvariant that removes both positional embeddings and the [CLS] token, making it\nfully permutation-invariant and suitable for unordered medical image data. To\nenhance generalization, we propose ShuffleStrides Data Augmentation (SSDA),\nwhich permutes probe-view sequences and frame orders while preserving\nanatomical validity. ZACH-ViT was evaluated on 380 LUS videos from 95\ncritically ill patients against nine state-of-the-art baselines. Despite the\nheterogeneity of the non-cardiogenic group, ZACH-ViT achieved the highest\nvalidation and test ROC-AUC (0.80 and 0.79) with balanced sensitivity (0.60)\nand specificity (0.91), while all competing models collapsed to trivial\nclassification. It trains 1.35x faster than Minimal ViT (0.62M parameters) with\n2.5x fewer parameters, supporting real-time clinical deployment. These results\nshow that aligning architectural design with data structure can outperform\nscale in small-data medical imaging."
    },
    {
        "date": "2025-10",
        "title": "RESample: A Robust Data Augmentation Framework via Exploratory Sampling for Robotic Manipulation",
        "author": "Yuquan Xue, Guanxing Lu, Zhenyu Wu, Chuanrui Zhang, Bofang Jia, Zhengyi Gu, Yansong Tang, and Ziwei Wang",
        "link": "http://arxiv.org/abs/2510.17640v1",
        "abstract": "Vision-Language-Action models (VLAs) have demonstrated remarkable performance\non complex robotic manipulation tasks through imitation learning. However,\nexisting imitation learning datasets contain only successful trajectories and\nlack failure or recovery data, especially for out-of-distribution (OOD) states\nwhere the robot deviates from the main policy due to minor perturbations or\nerrors, leading VLA models to struggle with states deviating from the training\ndistribution. To this end, we propose an automated OOD data augmentation\nframework named RESample through exploratory sampling. Specifically, we first\nleverage offline reinforcement learning to obtain an action-value network that\naccurately identifies sub-optimal actions under the current manipulation\npolicy. We further sample potential OOD states from trajectories via rollout,\nand design an exploratory sampling mechanism that adaptively incorporates these\naction proxies into the training dataset to ensure efficiency. Subsequently,\nour framework explicitly encourages the VLAs to recover from OOD states and\nenhances their robustness against distributional shifts. We conduct extensive\nexperiments on the LIBERO benchmark as well as real-world robotic manipulation\ntasks, demonstrating that RESample consistently improves the stability and\ngeneralization ability of VLA models."
    },
    {
        "date": "2025-10",
        "title": "Attracting Commercial Artificial Intelligence Firms to Support National Security through Collaborative Contracts",
        "author": "Andrew Bowne",
        "link": "http://arxiv.org/abs/2510.17931v1",
        "abstract": "Unlike other military technologies driven by national security needs and\ndeveloped with federal funding, AI is predominantly funded and advanced by\ncommercial industry for civilian applications. However, there is a lack of\nunderstanding of the reasons commercial AI firms decide to work with the DoD or\nchoose to abstain from the defence market. This thesis argues that the contract\nlaw and procurement framework are among the most significant obstacles. This\nresearch indicates that the commercial AI industry actually views the DoD as an\nattractive customer. However, this attraction is despite the obstacles\npresented by traditional contract law and procurement practices used to solicit\nand award contracts. Drawing on social exchange theory, this thesis introduces\na theoretical framework, optimal buyer theory, to understand the factors that\ninfluence a commercial decision to engage with the DoD. Interviews from a\nsample of the participants explain why the AI industry holds such perceptions,\nopinions, and preferences about contracts generally and the DoD, specifically,\nin its role as a customer. This thesis concludes that commercial AI firms are\nattracted to contracts that are consistent with their business and technology\nconsiderations. Additionally, it develops best practices for leveraging\nexisting contract law, primarily other transaction authority, to align\ncontracting practices with commercial preferences and the machine learning\ndevelopment and deployment lifecycle."
    },
    {
        "date": "2025-10",
        "title": "GUIDE: Enhancing Gradient Inversion Attacks in Federated Learning with Denoising Models",
        "author": "Vincenzo Carletti, Pasquale Foggia, Carlo Mazzocca, Giuseppe Parrella, and Mario Vento",
        "link": "http://arxiv.org/abs/2510.17621v2",
        "abstract": "Federated Learning (FL) enables collaborative training of Machine Learning\n(ML) models across multiple clients while preserving their privacy. Rather than\nsharing raw data, federated clients transmit locally computed updates to train\nthe global model. Although this paradigm should provide stronger privacy\nguarantees than centralized ML, client updates remain vulnerable to privacy\nleakage. Adversaries can exploit them to infer sensitive properties about the\ntraining data or even to reconstruct the original inputs via Gradient Inversion\nAttacks (GIAs). Under the honest-butcurious threat model, GIAs attempt to\nreconstruct training data by reversing intermediate updates using\noptimizationbased techniques. We observe that these approaches usually\nreconstruct noisy approximations of the original inputs, whose quality can be\nenhanced with specialized denoising models. This paper presents Gradient Update\nInversion with DEnoising (GUIDE), a novel methodology that leverages diffusion\nmodels as denoising tools to improve image reconstruction attacks in FL. GUIDE\ncan be integrated into any GIAs that exploits surrogate datasets, a widely\nadopted assumption in GIAs literature. We comprehensively evaluate our approach\nin two attack scenarios that use different FL algorithms, models, and datasets.\nOur results demonstrate that GUIDE integrates seamlessly with two state-ofthe-\nart GIAs, substantially improving reconstruction quality across multiple\nmetrics. Specifically, GUIDE achieves up to 46% higher perceptual similarity,\nas measured by the DreamSim metric."
    },
    {
        "date": "2025-10",
        "title": "WP-CrackNet: A Collaborative Adversarial Learning Framework for End-to-End Weakly-Supervised Road Crack Detection",
        "author": "Nachuan Ma, Zhengfei Song, Qiang Hu, Xiaoyu Tang, Chengxi Zhang, Rui Fan, and Lihua Xie",
        "link": "http://arxiv.org/abs/2510.17566v1",
        "abstract": "Road crack detection is essential for intelligent infrastructure maintenance\nin smart cities. To reduce reliance on costly pixel-level annotations, we\npropose WP-CrackNet, an end-to-end weakly-supervised method that trains with\nonly image-level labels for pixel-wise crack detection. WP-CrackNet integrates\nthree components: a classifier generating class activation maps (CAMs), a\nreconstructor measuring feature inferability, and a detector producing\npixel-wise road crack detection results. During training, the classifier and\nreconstructor alternate in adversarial learning to encourage crack CAMs to\ncover complete crack regions, while the detector learns from pseudo labels\nderived from post-processed crack CAMs. This mutual feedback among the three\ncomponents improves learning stability and detection accuracy. To further boost\ndetection performance, we design a path-aware attention module (PAAM) that\nfuses high-level semantics from the classifier with low-level structural cues\nfrom the reconstructor by modeling spatial and channel-wise dependencies.\nAdditionally, a center-enhanced CAM consistency module (CECCM) is proposed to\nrefine crack CAMs using center Gaussian weighting and consistency constraints,\nenabling better pseudo-label generation. We create three image-level datasets\nand extensive experiments show that WP-CrackNet achieves comparable results to\nsupervised methods and outperforms existing weakly-supervised methods,\nsignificantly advancing scalable road inspection. The source code package and\ndatasets are available at https://mias.group/WP-CrackNet/."
    },
    {
        "date": "2025-10",
        "title": "OncoReason: Structuring Clinical Reasoning in LLMs for Robust and Interpretable Survival Prediction",
        "author": "Raghu Vamshi Hemadri, Geetha Krishna Guruju, Kristi Topollai, and Anna Ewa Choromanska",
        "link": "http://arxiv.org/abs/2510.17532v1",
        "abstract": "Predicting cancer treatment outcomes requires models that are both accurate\nand interpretable, particularly in the presence of heterogeneous clinical data.\nWhile large language models (LLMs) have shown strong performance in biomedical\nNLP, they often lack structured reasoning capabilities critical for high-stakes\ndecision support. We present a unified, multi-task learning framework that\naligns autoregressive LLMs with clinical reasoning for outcome prediction on\nthe MSK-CHORD dataset. Our models are trained to jointly perform binary\nsurvival classification, continuous survival time regression, and natural\nlanguage rationale generation. We evaluate three alignment strategies: (1)\nstandard supervised fine-tuning (SFT), (2) SFT with Chain-of-Thought (CoT)\nprompting to elicit step-by-step reasoning, and (3) Group Relative Policy\nOptimization (GRPO), a reinforcement learning method that aligns model outputs\nto expert-derived reasoning trajectories. Experiments with LLaMa3-8B and\nMed42-8B backbones demonstrate that CoT prompting improves F1 by +6.0 and\nreduces MAE by 12%, while GRPO achieves state-of-the-art interpretability and\npredictive performance across BLEU, ROUGE, and BERTScore. We further show that\nexisting biomedical LLMs often fail to produce valid reasoning traces due to\narchitectural constraints. Our findings underscore the importance of\nreasoning-aware alignment in multi-task clinical modeling and set a new\nbenchmark for interpretable, trustworthy LLMs in precision oncology."
    },
    {
        "date": "2025-10",
        "title": "Cybersecurity AI: Evaluating Agentic Cybersecurity in Attack/Defense CTFs",
        "author": "Francesco Balassone, V\u00edctor Mayoral-Vilches, Stefan Rass, Martin Pinzger, Gaetano Perrone, Simon Pietro Romano, and Peter Schartner",
        "link": "http://arxiv.org/abs/2510.17521v1",
        "abstract": "We empirically evaluate whether AI systems are more effective at attacking or\ndefending in cybersecurity. Using CAI (Cybersecurity AI)'s parallel execution\nframework, we deployed autonomous agents in 23 Attack/Defense CTF\nbattlegrounds. Statistical analysis reveals defensive agents achieve 54.3%\nunconstrained patching success versus 28.3% offensive initial access\n(p=0.0193), but this advantage disappears under operational constraints: when\ndefense requires maintaining availability (23.9%) and preventing all intrusions\n(15.2%), no significant difference exists (p>0.05). Exploratory taxonomy\nanalysis suggests potential patterns in vulnerability exploitation, though\nlimited sample sizes preclude definitive conclusions. This study provides the\nfirst controlled empirical evidence challenging claims of AI attacker\nadvantage, demonstrating that defensive effectiveness critically depends on\nsuccess criteria, a nuance absent from conceptual analyses but essential for\ndeployment. These findings underscore the urgency for defenders to adopt\nopen-source Cybersecurity AI frameworks to maintain security equilibrium\nagainst accelerating offensive automation."
    },
    {
        "date": "2025-10",
        "title": "AWARE: Audio Watermarking with Adversarial Resistance to Edits",
        "author": "Kosta Pavlovi\u0107, Lazar Stanarevi\u0107, Petar Nedi\u0107, Slavko Kova\u010devi\u0107, and Igor Djurovi\u0107",
        "link": "http://arxiv.org/abs/2510.17512v1",
        "abstract": "Prevailing practice in learning-based audio watermarking is to pursue\nrobustness by expanding the set of simulated distortions during training.\nHowever, such surrogates are narrow and prone to overfitting. This paper\npresents AWARE (Audio Watermarking with Adversarial Resistance to Edits), an\nalternative approach that avoids reliance on attack-simulation stacks and\nhandcrafted differentiable distortions. Embedding is obtained via adversarial\noptimization in the time-frequency domain under a level-proportional perceptual\nbudget. Detection employs a time-order-agnostic detector with a Bitwise Readout\nHead (BRH) that aggregates temporal evidence into one score per watermark bit,\nenabling reliable watermark decoding even under desynchronization and temporal\ncuts. Empirically, AWARE attains high audio quality and speech intelligibility\n(PESQ/STOI) and consistently low BER across various audio edits, often\nsurpassing representative state-of-the-art learning-based audio watermarking\nsystems."
    },
    {
        "date": "2025-10",
        "title": "I-RAVEN-X: Benchmarking Generalization and Robustness of Analogical and Mathematical Reasoning in Large Language and Reasoning Models",
        "author": "Giacomo Camposampiero, Michael Hersche, Roger Wattenhofer, Abu Sebastian, and Abbas Rahimi",
        "link": "http://arxiv.org/abs/2510.17496v1",
        "abstract": "We introduce I-RAVEN-X, a symbolic benchmark designed to evaluate\ngeneralization and robustness in analogical and mathematical reasoning for\nLarge Language Models (LLMs) and Large Reasoning Models (LRMs). I-RAVEN-X\nextends I-RAVEN by increasing operand complexity, attribute range, and\nintroducing perceptual uncertainty. Compared to LLMs, empirical results show\nthat LRMs achieve improved productivity and systematicity on longer reasoning\nrelations and wider attribute ranges, respectively. However, LRMs are still\nsignificantly challenged by reasoning under uncertainty and cannot effectively\nexplore multiple probabilistic outcomes."
    },
    {
        "date": "2025-10",
        "title": "RINS-T: Robust Implicit Neural Solvers for Time Series Linear Inverse Problems",
        "author": "Keivan Faghih Niresi, Zepeng Zhang, and Olga Fink",
        "link": "http://arxiv.org/abs/2510.17396v1",
        "abstract": "Time series data are often affected by various forms of corruption, such as\nmissing values, noise, and outliers, which pose significant challenges for\ntasks such as forecasting and anomaly detection. To address these issues,\ninverse problems focus on reconstructing the original signal from corrupted\ndata by leveraging prior knowledge about its underlying structure. While deep\nlearning methods have demonstrated potential in this domain, they often require\nextensive pretraining and struggle to generalize under distribution shifts. In\nthis work, we propose RINS-T (Robust Implicit Neural Solvers for Time Series\nLinear Inverse Problems), a novel deep prior framework that achieves high\nrecovery performance without requiring pretraining data. RINS-T leverages\nneural networks as implicit priors and integrates robust optimization\ntechniques, making it resilient to outliers while relaxing the reliance on\nGaussian noise assumptions. To further improve optimization stability and\nrobustness, we introduce three key innovations: guided input initialization,\ninput perturbation, and convex output combination techniques. Each of these\ncontributions strengthens the framework's optimization stability and\nrobustness. These advancements make RINS-T a flexible and effective solution\nfor addressing complex real-world time series challenges. Our code is available\nat https://github.com/EPFL-IMOS/RINS-T."
    },
    {
        "date": "2025-10",
        "title": "A Single Set of Adversarial Clothes Breaks Multiple Defense Methods in the Physical World",
        "author": "Wei Zhang, Zhanhao Hu, Xiao Li, Xiaopei Zhu, and Xiaolin Hu",
        "link": "http://arxiv.org/abs/2510.17322v1",
        "abstract": "In recent years, adversarial attacks against deep learning-based object\ndetectors in the physical world have attracted much attention. To defend\nagainst these attacks, researchers have proposed various defense methods\nagainst adversarial patches, a typical form of physically-realizable attack.\nHowever, our experiments showed that simply enlarging the patch size could make\nthese defense methods fail. Motivated by this, we evaluated various defense\nmethods against adversarial clothes which have large coverage over the human\nbody. Adversarial clothes provide a good test case for adversarial defense\nagainst patch-based attacks because they not only have large sizes but also\nlook more natural than a large patch on humans. Experiments show that all the\ndefense methods had poor performance against adversarial clothes in both the\ndigital world and the physical world. In addition, we crafted a single set of\nclothes that broke multiple defense methods on Faster R-CNN. The set achieved\nan Attack Success Rate (ASR) of 96.06% against the undefended detector and over\n64.84% ASRs against nine defended models in the physical world, unveiling the\ncommon vulnerability of existing adversarial defense methods against\nadversarial clothes. Code is available at:\nhttps://github.com/weiz0823/adv-clothes-break-multiple-defenses."
    },
    {
        "date": "2025-10",
        "title": "The Hidden Dangers of Public Serverless Repositories: An Empirical Security Assessment",
        "author": "Eduard Marin, Jinwoo Kim, Alessio Pavoni, Mauro Conti, and Roberto Di Pietro",
        "link": "http://arxiv.org/abs/2510.17311v1",
        "abstract": "Serverless computing has rapidly emerged as a prominent cloud paradigm,\nenabling developers to focus solely on application logic without the burden of\nmanaging servers or underlying infrastructure. Public serverless repositories\nhave become key to accelerating the development of serverless applications.\nHowever, their growing popularity makes them attractive targets for\nadversaries. Despite this, the security posture of these repositories remains\nlargely unexplored, exposing developers and organizations to potential risks.\nIn this paper, we present the first comprehensive analysis of the security\nlandscape of serverless components hosted in public repositories. We analyse\n2,758 serverless components from five widely used public repositories popular\namong developers and enterprises, and 125,936 Infrastructure as Code (IaC)\ntemplates across three widely used IaC frameworks. Our analysis reveals\nsystemic vulnerabilities including outdated software packages, misuse of\nsensitive parameters, exploitable deployment configurations, susceptibility to\ntypo-squatting attacks and opportunities to embed malicious behaviour within\ncompressed serverless components. Finally, we provide practical recommendations\nto mitigate these threats."
    },
    {
        "date": "2025-10",
        "title": "Breaking and Fixing Defenses Against Control-Flow Hijacking in Multi-Agent Systems",
        "author": "Rishi Jha, Harold Triedman, Justin Wagle, and Vitaly Shmatikov",
        "link": "http://arxiv.org/abs/2510.17276v1",
        "abstract": "Control-flow hijacking attacks manipulate orchestration mechanisms in\nmulti-agent systems into performing unsafe actions that compromise the system\nand exfiltrate sensitive information. Recently proposed defenses, such as\nLlamaFirewall, rely on alignment checks of inter-agent communications to ensure\nthat all agent invocations are \"related to\" and \"likely to further\" the\noriginal objective.\n  We start by demonstrating control-flow hijacking attacks that evade these\ndefenses even if alignment checks are performed by advanced LLMs. We argue that\nthe safety and functionality objectives of multi-agent systems fundamentally\nconflict with each other. This conflict is exacerbated by the brittle\ndefinitions of \"alignment\" and the checkers' incomplete visibility into the\nexecution context.\n  We then propose, implement, and evaluate ControlValve, a new defense inspired\nby the principles of control-flow integrity and least privilege. ControlValve\n(1) generates permitted control-flow graphs for multi-agent systems, and (2)\nenforces that all executions comply with these graphs, along with contextual\nrules (generated in a zero-shot manner) for each agent invocation."
    },
    {
        "date": "2025-10",
        "title": "Robustness in Text-Attributed Graph Learning: Insights, Trade-offs, and New Defenses",
        "author": "Runlin Lei, Lu Yi, Mingguo He, Pengyu Qiu, Zhewei Wei, Yongchao Liu, and Chuntao Hong",
        "link": "http://arxiv.org/abs/2510.17185v1",
        "abstract": "While Graph Neural Networks (GNNs) and Large Language Models (LLMs) are\npowerful approaches for learning on Text-Attributed Graphs (TAGs), a\ncomprehensive understanding of their robustness remains elusive. Current\nevaluations are fragmented, failing to systematically investigate the distinct\neffects of textual and structural perturbations across diverse models and\nattack scenarios. To address these limitations, we introduce a unified and\ncomprehensive framework to evaluate robustness in TAG learning. Our framework\nevaluates classical GNNs, robust GNNs (RGNNs), and GraphLLMs across ten\ndatasets from four domains, under diverse text-based, structure-based, and\nhybrid perturbations in both poisoning and evasion scenarios. Our extensive\nanalysis reveals multiple findings, among which three are particularly\nnoteworthy: 1) models have inherent robustness trade-offs between text and\nstructure, 2) the performance of GNNs and RGNNs depends heavily on the text\nencoder and attack type, and 3) GraphLLMs are particularly vulnerable to\ntraining data corruption. To overcome the identified trade-offs, we introduce\nSFT-auto, a novel framework that delivers superior and balanced robustness\nagainst both textual and structural attacks within a single model. Our work\nestablishes a foundation for future research on TAG security and offers\npractical solutions for robust TAG learning in adversarial environments. Our\ncode is available at: https://github.com/Leirunlin/TGRB."
    },
    {
        "date": "2025-10",
        "title": "Investigating Adversarial Robustness against Preprocessing used in Blackbox Face Recognition",
        "author": "Roland Croft, Brian Du, Darcy Joseph, and Sharath Kumar",
        "link": "http://arxiv.org/abs/2510.17169v1",
        "abstract": "Face Recognition (FR) models have been shown to be vulnerable to adversarial\nexamples that subtly alter benign facial images, exposing blind spots in these\nsystems, as well as protecting user privacy. End-to-end FR systems first obtain\npreprocessed faces from diverse facial imagery prior to computing the\nsimilarity of the deep feature embeddings. Whilst face preprocessing is a\ncritical component of FR systems, and hence adversarial attacks against them,\nwe observe that this preprocessing is often overlooked in blackbox settings.\nOur study seeks to investigate the transferability of several out-of-the-box\nstate-of-the-art adversarial attacks against FR when applied against different\npreprocessing techniques used in a blackbox setting. We observe that the choice\nof face detection model can degrade the attack success rate by up to 78%,\nwhereas choice of interpolation method during downsampling has relatively\nminimal impacts. Furthermore, we find that the requirement for facial\npreprocessing even degrades attack strength in a whitebox setting, due to the\nunintended interaction of produced noise vectors against face detection models.\nBased on these findings, we propose a preprocessing-invariant method using\ninput transformations that improves the transferability of the studied attacks\nby up to 27%. Our findings highlight the importance of preprocessing in FR\nsystems, and the need for its consideration towards improving the adversarial\ngeneralisation of facial adversarial examples."
    },
    {
        "date": "2025-10",
        "title": "Adapting to Stochastic and Adversarial Losses in Episodic MDPs with Aggregate Bandit Feedback",
        "author": "Shinji Ito, Kevin Jamieson, Haipeng Luo, Arnab Maiti, and Taira Tsuchiya",
        "link": "http://arxiv.org/abs/2510.17103v1",
        "abstract": "We study online learning in finite-horizon episodic Markov decision processes\n(MDPs) under the challenging aggregate bandit feedback model, where the learner\nobserves only the cumulative loss incurred in each episode, rather than\nindividual losses at each state-action pair. While prior work in this setting\nhas focused exclusively on worst-case analysis, we initiate the study of\nbest-of-both-worlds (BOBW) algorithms that achieve low regret in both\nstochastic and adversarial environments. We propose the first BOBW algorithms\nfor episodic tabular MDPs with aggregate bandit feedback. In the case of known\ntransitions, our algorithms achieve $O(\\log T)$ regret in stochastic settings\nand ${O}(\\sqrt{T})$ regret in adversarial ones. Importantly, we also establish\nmatching lower bounds, showing the optimality of our algorithms in this\nsetting. We further extend our approach to unknown-transition settings by\nincorporating confidence-based techniques. Our results rely on a combination of\nFTRL over occupancy measures, self-bounding techniques, and new loss estimators\ninspired by recent advances in online shortest path problems. Along the way, we\nalso provide the first individual-gap-dependent lower bounds and demonstrate\nnear-optimal BOBW algorithms for shortest path problems with bandit feedback."
    },
    {
        "date": "2025-10",
        "title": "Watermark Robustness and Radioactivity May Be at Odds in Federated Learning",
        "author": "Leixu Huang, Zedian Shao, and Teodora Baluta",
        "link": "http://arxiv.org/abs/2510.17033v1",
        "abstract": "Federated learning (FL) enables fine-tuning large language models (LLMs)\nacross distributed data sources. As these sources increasingly include\nLLM-generated text, provenance tracking becomes essential for accountability\nand transparency. We adapt LLM watermarking for data provenance in FL where a\nsubset of clients compute local updates on watermarked data, and the server\naverages all updates into the global LLM. In this setup, watermarks are\nradioactive: the watermark signal remains detectable after fine-tuning with\nhigh confidence. The $p$-value can reach $10^{-24}$ even when as little as\n$6.6\\%$ of data is watermarked. However, the server can act as an active\nadversary that wants to preserve model utility while evading provenance\ntracking. Our observation is that updates induced by watermarked synthetic data\nappear as outliers relative to non-watermark updates. Our adversary thus\napplies strong robust aggregation that can filter these outliers, together with\nthe watermark signal. All evaluated radioactive watermarks are not robust\nagainst such an active filtering server. Our work suggests fundamental\ntrade-offs between radioactivity, robustness, and utility."
    },
    {
        "date": "2025-10",
        "title": "Bits Leaked per Query: Information-Theoretic Bounds on Adversarial Attacks against LLMs",
        "author": "Masahiro Kaneko, and Timothy Baldwin",
        "link": "http://arxiv.org/abs/2510.17000v1",
        "abstract": "Adversarial attacks by malicious users that threaten the safety of large\nlanguage models (LLMs) can be viewed as attempts to infer a target property $T$\nthat is unknown when an instruction is issued, and becomes knowable only after\nthe model's reply is observed. Examples of target properties $T$ include the\nbinary flag that triggers an LLM's harmful response or rejection, and the\ndegree to which information deleted by unlearning can be restored, both\nelicited via adversarial instructions. The LLM reveals an \\emph{observable\nsignal} $Z$ that potentially leaks hints for attacking through a response\ncontaining answer tokens, thinking process tokens, or logits. Yet the scale of\ninformation leaked remains anecdotal, leaving auditors without principled\nguidance and defenders blind to the transparency--risk trade-off. We fill this\ngap with an information-theoretic framework that computes how much information\ncan be safely disclosed, and enables auditors to gauge how close their methods\ncome to the fundamental limit. Treating the mutual information $I(Z;T)$ between\nthe observation $Z$ and the target property $T$ as the leaked bits per query,\nwe show that achieving error $\\varepsilon$ requires at least\n$\\log(1/\\varepsilon)/I(Z;T)$ queries, scaling linearly with the inverse leak\nrate and only logarithmically with the desired accuracy. Thus, even a modest\nincrease in disclosure collapses the attack cost from quadratic to logarithmic\nin terms of the desired accuracy. Experiments on seven LLMs across\nsystem-prompt leakage, jailbreak, and relearning attacks corroborate the\ntheory: exposing answer tokens alone requires about a thousand queries; adding\nlogits cuts this to about a hundred; and revealing the full thinking process\ntrims it to a few dozen. Our results provide the first principled yardstick for\nbalancing transparency and security when deploying LLMs."
    },
    {
        "date": "2025-10",
        "title": "Res-Bench: Benchmarking the Robustness of Multimodal Large Language Models to Dynamic Resolution Input",
        "author": "Chenxu Li, Zhicai Wang, Yuan Sheng, Xingyu Zhu, Yanbin Hao, and Xiang Wang",
        "link": "http://arxiv.org/abs/2510.16926v1",
        "abstract": "Multimodal Large Language Models (MLLMs) increasingly support dynamic image\nresolutions. However, current evaluation paradigms primarily assess semantic\nperformance, overlooking the critical question of resolution robustness -\nwhether performance remains stable across varying input resolutions. To address\nthis gap, we introduce \\textbf{Res-Bench}, a comprehensive benchmark comprising\n14,400 samples across 12 resolution levels and six core capability dimensions.\nWe designed a novel evaluation framework that goes beyond traditional accuracy\nmetrics to capture performance stability. This framework introduces multiple\nrobustness metrics: Spearman's correlation for assessing resolution-performance\ntrends, and Absolute/Relative Continuous Error (ACE/RCE) for measuring\nperformance volatility. Using these metrics, we conducted a large-scale\nevaluation of leading MLLMs. Our analysis encompasses: (1) model-centric and\ntask-centric robustness examination, (2) investigation of preprocessing\nstrategies including padding and super-resolution, and (3) exploration of\nfine-tuning for stability enhancement."
    },
    {
        "date": "2025-10",
        "title": "UNDREAM: Bridging Differentiable Rendering and Photorealistic Simulation for End-to-end Adversarial Attacks",
        "author": "Mansi Phute, Matthew Hull, Haoran Wang, Alec Helbling, ShengYun Peng, Willian Lunardi, Martin Andreoni, Wenke Lee, and Polo Chau",
        "link": "http://arxiv.org/abs/2510.16923v1",
        "abstract": "Deep learning models deployed in safety critical applications like autonomous\ndriving use simulations to test their robustness against adversarial attacks in\nrealistic conditions. However, these simulations are non-differentiable,\nforcing researchers to create attacks that do not integrate simulation\nenvironmental factors, reducing attack success. To address this limitation, we\nintroduce UNDREAM, the first software framework that bridges the gap between\nphotorealistic simulators and differentiable renderers to enable end-to-end\noptimization of adversarial perturbations on any 3D objects. UNDREAM enables\nmanipulation of the environment by offering complete control over weather,\nlighting, backgrounds, camera angles, trajectories, and realistic human and\nobject movements, thereby allowing the creation of diverse scenes. We showcase\na wide array of distinct physically plausible adversarial objects that UNDREAM\nenables researchers to swiftly explore in different configurable environments.\nThis combination of photorealistic simulation and differentiable optimization\nopens new avenues for advancing research of physical adversarial attacks."
    },
    {
        "date": "2025-10",
        "title": "ThreatIntel-Andro: Expert-Verified Benchmarking for Robust Android Malware Research",
        "author": "Hongpeng Bai, Minhong Dong, Yao Zhang, Shunzhe Zhao, Haobo Zhang, Lingyue Li, Yude Bai, and Guangquan Xu",
        "link": "http://arxiv.org/abs/2510.16835v1",
        "abstract": "The rapidly evolving Android malware ecosystem demands high-quality,\nreal-time datasets as a foundation for effective detection and defense. With\nthe widespread adoption of mobile devices across industrial systems, they have\nbecome a critical yet often overlooked attack surface in industrial\ncybersecurity. However, mainstream datasets widely used in academia and\nindustry (e.g., Drebin) exhibit significant limitations: on one hand, their\nheavy reliance on VirusTotal's multi-engine aggregation results introduces\nsubstantial label noise; on the other hand, outdated samples reduce their\ntemporal relevance. Moreover, automated labeling tools (e.g., AVClass2) suffer\nfrom suboptimal aggregation strategies, further compounding labeling errors and\npropagating inaccuracies throughout the research community."
    },
    {
        "date": "2025-10",
        "title": "Robust Cross-Domain Adaptation in Texture Features Transferring for Wood Chip Moisture Content Prediction",
        "author": "Abdur Rahman, Mohammad Marufuzzaman, Jason Street, Haifeng Wang, Veera G. Gude, and Randy Buchanan",
        "link": "http://arxiv.org/abs/2510.16832v1",
        "abstract": "Accurate and quick prediction of wood chip moisture content is critical for\noptimizing biofuel production and ensuring energy efficiency. The current\nwidely used direct method (oven drying) is limited by its longer processing\ntime and sample destructiveness. On the other hand, existing indirect methods,\nincluding near-infrared spectroscopy-based, electrical capacitance-based, and\nimage-based approaches, are quick but not accurate when wood chips come from\nvarious sources. Variability in the source material can alter data\ndistributions, undermining the performance of data-driven models. Therefore,\nthere is a need for a robust approach that effectively mitigates the impact of\nsource variability. Previous studies show that manually extracted texture\nfeatures have the potential to predict wood chip moisture class. Building on\nthis, in this study, we conduct a comprehensive analysis of five distinct\ntexture feature types extracted from wood chip images to predict moisture\ncontent. Our findings reveal that a combined feature set incorporating all five\ntexture features achieves an accuracy of 95% and consistently outperforms\nindividual texture features in predicting moisture content. To ensure robust\nmoisture prediction, we propose a domain adaptation method named AdaptMoist\nthat utilizes the texture features to transfer knowledge from one source of\nwood chip data to another, addressing variability across different domains. We\nalso proposed a criterion for model saving based on adjusted mutual\ninformation. The AdaptMoist method improves prediction accuracy across domains\nby 23%, achieving an average accuracy of 80%, compared to 57% for non-adapted\nmodels. These results highlight the effectiveness of AdaptMoist as a robust\nsolution for wood chip moisture content estimation across domains, making it a\npotential solution for wood chip-reliant industries."
    },
    {
        "date": "2025-10",
        "title": "When AI Takes the Wheel: Security Analysis of Framework-Constrained Program Generation",
        "author": "Yue Liu, Zhenchang Xing, Shidong Pan, and Chakkrit Tantithamthavorn",
        "link": "http://arxiv.org/abs/2510.16823v1",
        "abstract": "In recent years, the AI wave has grown rapidly in software development. Even\nnovice developers can now design and generate complex framework-constrained\nsoftware systems based on their high-level requirements with the help of Large\nLanguage Models (LLMs). However, when LLMs gradually \"take the wheel\" of\nsoftware development, developers may only check whether the program works. They\noften miss security problems hidden in how the generated programs are\nimplemented.\n  In this work, we investigate the security properties of framework-constrained\nprograms generated by state-of-the-art LLMs. We focus specifically on Chrome\nextensions due to their complex security model involving multiple privilege\nboundaries and isolated components. To achieve this, we built ChromeSecBench, a\ndataset with 140 prompts based on known vulnerable extensions. We used these\nprompts to instruct nine state-of-the-art LLMs to generate complete Chrome\nextensions, and then analyzed them for vulnerabilities across three dimensions:\nscenario types, model differences, and vulnerability categories. Our results\nshow that LLMs produced vulnerable programs at alarmingly high rates (18%-50%),\nparticularly in Authentication & Identity and Cookie Management scenarios (up\nto 83% and 78% respectively). Most vulnerabilities exposed sensitive browser\ndata like cookies, history, or bookmarks to untrusted code. Interestingly, we\nfound that advanced reasoning models performed worse, generating more\nvulnerabilities than simpler models. These findings highlight a critical gap\nbetween LLMs' coding skills and their ability to write secure\nframework-constrained programs."
    },
    {
        "date": "2025-10",
        "title": "UKANFormer: Noise-Robust Semantic Segmentation for Coral Reef Mapping via a Kolmogorov-Arnold Network-Transformer Hybrid",
        "author": "Tianyang Dou, Ming Li, Jiangying Qin, Xuan Liao, Jiageng Zhong, Armin Gruen, and Mengyi Deng",
        "link": "http://arxiv.org/abs/2510.16730v1",
        "abstract": "Coral reefs are vital yet fragile ecosystems that require accurate\nlarge-scale mapping for effective conservation. Although global products such\nas the Allen Coral Atlas provide unprecedented coverage of global coral reef\ndistri-bution, their predictions are frequently limited in spatial precision\nand semantic consistency, especially in regions requiring fine-grained boundary\ndelineation. To address these challenges, we propose UKANFormer, a novel\nse-mantic segmentation model designed to achieve high-precision mapping under\nnoisy supervision derived from Allen Coral Atlas. Building upon the UKAN\narchitecture, UKANFormer incorporates a Global-Local Transformer (GL-Trans)\nblock in the decoder, enabling the extraction of both global semantic\nstructures and local boundary details. In experiments, UKANFormer achieved a\ncoral-class IoU of 67.00% and pixel accuracy of 83.98%, outperforming\nconventional baselines under the same noisy labels setting. Remarkably, the\nmodel produces predictions that are visually and structurally more accurate\nthan the noisy labels used for training. These results challenge the notion\nthat data quality directly limits model performance, showing that architectural\ndesign can mitigate label noise and sup-port scalable mapping under imperfect\nsupervision. UKANFormer provides a foundation for ecological monitoring where\nreliable labels are scarce."
    },
    {
        "date": "2025-10",
        "title": "CLIP: Client-Side Invariant Pruning for Mitigating Stragglers in Secure Federated Learning",
        "author": "Anthony DiMaggio, Raghav Sharma, and Gururaj Saileshwar",
        "link": "http://arxiv.org/abs/2510.16694v1",
        "abstract": "Secure federated learning (FL) preserves data privacy during distributed\nmodel training. However, deploying such frameworks across heterogeneous devices\nresults in performance bottlenecks, due to straggler clients with limited\ncomputational or network capabilities, slowing training for all participating\nclients. This paper introduces the first straggler mitigation technique for\nsecure aggregation with deep neural networks. We propose CLIP, a client-side\ninvariant neuron pruning technique coupled with network-aware pruning, that\naddresses compute and network bottlenecks due to stragglers during training\nwith minimal accuracy loss. Our technique accelerates secure FL training by 13%\nto 34% across multiple datasets (CIFAR10, Shakespeare, FEMNIST) with an\naccuracy impact of between 1.3% improvement to 2.6% reduction."
    },
    {
        "date": "2025-10",
        "title": "Robust Dynamic Staffing with Predictions",
        "author": "Yiding Feng, Vahideh Manshadi, Rad Niazadeh, and Saba Neyshabouri",
        "link": "http://arxiv.org/abs/2510.16663v1",
        "abstract": "We consider a natural dynamic staffing problem in which a decision-maker\nsequentially hires workers over a finite horizon to meet an unknown demand\nrevealed at the end. Predictions about demand arrive over time and become\nincreasingly accurate, while worker availability decreases. This creates a\nfundamental trade-off between hiring early to avoid understaffing (when workers\nare more available but forecasts are less reliable) and hiring late to avoid\noverstaffing (when forecasts are more accurate but availability is lower). This\nproblem is motivated by last-mile delivery operations, where companies such as\nAmazon rely on gig-economy workers whose availability declines closer to the\noperating day.\n  To address practical limitations of Bayesian models (in particular, to remain\nagnostic to the underlying forecasting method), we study this problem under\nadversarial predictions. In this model, sequential predictions are\nadversarially chosen uncertainty intervals that (approximately) contain the\ntrue demand. The objective is to minimize worst-case staffing imbalance cost.\nOur main result is a simple and computationally efficient online algorithm that\nis minimax optimal. We first characterize the minimax cost against a restricted\nadversary via a polynomial-size linear program, then show how to emulate this\nsolution in the general case. While our base model focuses on a single demand,\nwe extend the framework to multiple demands (with egalitarian/utilitarian\nobjectives), to settings with costly reversals of hiring decisions, and to\ninconsistent prediction intervals. We also introduce a practical \"re-solving\"\nvariant of our algorithm, which we prove is also minimax optimal. Finally we\nconduct numerical experiments showing that our algorithms outperform Bayesian\nheuristics in both cost and speed, and are competitive with (approximate or\nexact) Bayesian-optimal policies when those can be computed."
    },
    {
        "date": "2025-10",
        "title": "Universal and Transferable Attacks on Pathology Foundation Models",
        "author": "Yuntian Wang, Xilin Yang, Che-Yung Shen, Nir Pillar, and Aydogan Ozcan",
        "link": "http://arxiv.org/abs/2510.16660v1",
        "abstract": "We introduce Universal and Transferable Adversarial Perturbations (UTAP) for\npathology foundation models that reveal critical vulnerabilities in their\ncapabilities. Optimized using deep learning, UTAP comprises a fixed and weak\nnoise pattern that, when added to a pathology image, systematically disrupts\nthe feature representation capabilities of multiple pathology foundation\nmodels. Therefore, UTAP induces performance drops in downstream tasks that\nutilize foundation models, including misclassification across a wide range of\nunseen data distributions. In addition to compromising the model performance,\nwe demonstrate two key features of UTAP: (1) universality: its perturbation can\nbe applied across diverse field-of-views independent of the dataset that UTAP\nwas developed on, and (2) transferability: its perturbation can successfully\ndegrade the performance of various external, black-box pathology foundation\nmodels - never seen before. These two features indicate that UTAP is not a\ndedicated attack associated with a specific foundation model or image dataset,\nbut rather constitutes a broad threat to various emerging pathology foundation\nmodels and their applications. We systematically evaluated UTAP across various\nstate-of-the-art pathology foundation models on multiple datasets, causing a\nsignificant drop in their performance with visually imperceptible modifications\nto the input images using a fixed noise pattern. The development of these\npotent attacks establishes a critical, high-standard benchmark for model\nrobustness evaluation, highlighting a need for advancing defense mechanisms and\npotentially providing the necessary assets for adversarial training to ensure\nthe safe and reliable deployment of AI in pathology."
    },
    {
        "date": "2025-10",
        "title": "A Versatile Framework for Designing Group-Sparse Adversarial Attacks",
        "author": "Alireza Heshmati, Saman Soleimani Roudi, Sajjad Amini, Shahrokh Ghaemmaghami, and Farokh Marvasti",
        "link": "http://arxiv.org/abs/2510.16637v1",
        "abstract": "Existing adversarial attacks often neglect perturbation sparsity, limiting\ntheir ability to model structural changes and to explain how deep neural\nnetworks (DNNs) process meaningful input patterns. We propose ATOS (Attack\nThrough Overlapping Sparsity), a differentiable optimization framework that\ngenerates structured, sparse adversarial perturbations in element-wise,\npixel-wise, and group-wise forms. For white-box attacks on image classifiers,\nwe introduce the Overlapping Smoothed L0 (OSL0) function, which promotes\nconvergence to a stationary point while encouraging sparse, structured\nperturbations. By grouping channels and adjacent pixels, ATOS improves\ninterpretability and helps identify robust versus non-robust features. We\napproximate the L-infinity gradient using the logarithm of the sum of\nexponential absolute values to tightly control perturbation magnitude. On\nCIFAR-10 and ImageNet, ATOS achieves a 100% attack success rate while producing\nsignificantly sparser and more structurally coherent perturbations than prior\nmethods. The structured group-wise attack highlights critical regions from the\nnetwork's perspective, providing counterfactual explanations by replacing\nclass-defining regions with robust features from the target class."
    },
    {
        "date": "2025-10",
        "title": "Structuring Security: A Survey of Cybersecurity Ontologies, Semantic Log Processing, and LLMs Application",
        "author": "Bruno Louren\u00e7o, Pedro Ad\u00e3o, Jo\u00e3o F. Ferreira, Mario Monteiro Marques, and C\u00e1tia Vaz",
        "link": "http://arxiv.org/abs/2510.16610v1",
        "abstract": "This survey investigates how ontologies, semantic log processing, and Large\nLanguage Models (LLMs) enhance cybersecurity. Ontologies structure domain\nknowledge, enabling interoperability, data integration, and advanced threat\nanalysis. Security logs, though critical, are often unstructured and complex.\nTo address this, automated construction of Knowledge Graphs (KGs) from raw logs\nis emerging as a key strategy for organizing and reasoning over security data.\nLLMs enrich this process by providing contextual understanding and extracting\ninsights from unstructured content. This work aligns with European Union (EU)\nefforts such as NIS 2 and the Cybersecurity Taxonomy, highlighting challenges\nand opportunities in intelligent ontology-driven cyber defense."
    },
    {
        "date": "2025-10",
        "title": "SHIELD: Suppressing Hallucinations In LVLM Encoders via Bias and Vulnerability Defense",
        "author": "Yiyang Huang, Liang Shi, Yitian Zhang, Yi Xu, and Yun Fu",
        "link": "http://arxiv.org/abs/2510.16596v1",
        "abstract": "Large Vision-Language Models (LVLMs) excel in diverse cross-modal tasks.\nHowever, object hallucination, where models produce plausible but inaccurate\nobject descriptions, remains a significant challenge. In contrast to previous\nwork focusing on LLM components, this paper is the first to trace LVLM\nhallucinations to visual encoders and identifies three key issues: statistical\nbias, inherent bias, and vulnerability. To address these challenges, we propose\nSHIELD, a training-free framework that mitigates hallucinations through three\nstrategies: re-weighting visual tokens to reduce statistical bias, introducing\nnoise-derived tokens to counter inherent bias, and applying adversarial attacks\nwith contrastive decoding to address vulnerability. Experiments demonstrate\nthat SHIELD effectively mitigates object hallucinations across diverse\nbenchmarks and LVLM families. Moreover, SHIELD achieves strong performance on\nthe general LVLM benchmark, highlighting its broad applicability. Code will be\nreleased."
    },
    {
        "date": "2025-10",
        "title": "Patronus: Safeguarding Text-to-Image Models against White-Box Adversaries",
        "author": "Xinfeng Li, Shengyuan Pang, Jialin Wu, Jiangyi Deng, Huanlong Zhong, Yanjiao Chen, Jie Zhang, and Wenyuan Xu",
        "link": "http://arxiv.org/abs/2510.16581v1",
        "abstract": "Text-to-image (T2I) models, though exhibiting remarkable creativity in image\ngeneration, can be exploited to produce unsafe images. Existing safety\nmeasures, e.g., content moderation or model alignment, fail in the presence of\nwhite-box adversaries who know and can adjust model parameters, e.g., by\nfine-tuning. This paper presents a novel defensive framework, named Patronus,\nwhich equips T2I models with holistic protection to defend against white-box\nadversaries. Specifically, we design an internal moderator that decodes unsafe\ninput features into zero vectors while ensuring the decoding performance of\nbenign input features. Furthermore, we strengthen the model alignment with a\ncarefully designed non-fine-tunable learning mechanism, ensuring the T2I model\nwill not be compromised by malicious fine-tuning. We conduct extensive\nexperiments to validate the intactness of the performance on safe content\ngeneration and the effectiveness of rejecting unsafe content generation.\nResults also confirm the resilience of Patronus against various fine-tuning\nattacks by white-box adversaries."
    },
    {
        "date": "2025-10",
        "title": "Toward Understanding Security Issues in the Model Context Protocol Ecosystem",
        "author": "Xiaofan Li, and Xing Gao",
        "link": "http://arxiv.org/abs/2510.16558v1",
        "abstract": "The Model Context Protocol (MCP) is an emerging open standard that enables\nAI-powered applications to interact with external tools through structured\nmetadata. A rapidly growing ecosystem has formed around MCP, including a wide\nrange of MCP hosts (i.e., Cursor, Windsurf, Claude Desktop, and Cline), MCP\nregistries (i.e., mcp.so, MCP Market, MCP Store, Pulse MCP, Smithery, and npm),\nand thousands of community-contributed MCP servers. Although the MCP ecosystem\nis gaining traction, there has been little systematic study of its architecture\nand associated security risks. In this paper, we present the first\ncomprehensive security analysis of the MCP ecosystem. We decompose MCP\necosystem into three core components: hosts, registries, and servers, and study\nthe interactions and trust relationships among them. Users search for servers\non registries and configure them in the host, which translates LLM-generated\noutput into external tool invocations provided by the servers and executes\nthem. Our qualitative analysis reveals that hosts lack output verification\nmechanisms for LLM-generated outputs, enabling malicious servers to manipulate\nmodel behavior and induce a variety of security threats, including but not\nlimited to sensitive data exfiltration. We uncover a wide range of\nvulnerabilities that enable attackers to hijack servers, due to the lack of a\nvetted server submission process in registries. To support our analysis, we\ncollect and analyze a dataset of 67,057 servers from six public registries. Our\nquantitative analysis demonstrates that a substantial number of servers can be\nhijacked by attackers. Finally, we propose practical defense strategies for MCP\nhosts, registries, and users. We responsibly disclosed our findings to affected\nhosts and registries."
    },
    {
        "date": "2025-10",
        "title": "$\u03c1$Hammer: Reviving RowHammer Attacks on New Architectures via Prefetching",
        "author": "Weijie Chen, Shan Tang, Yulin Tang, Xiapu Luo, Yinqian Zhang, and Weizhong Qiang",
        "link": "http://arxiv.org/abs/2510.16544v1",
        "abstract": "Rowhammer is a critical vulnerability in dynamic random access memory (DRAM)\nthat continues to pose a significant threat to various systems. However, we\nfind that conventional load-based attacks are becoming highly ineffective on\nthe most recent architectures such as Intel Alder and Raptor Lake. In this\npaper, we present $\\rho$Hammer, a new Rowhammer framework that systematically\novercomes three core challenges impeding attacks on these new architectures.\nFirst, we design an efficient and generic DRAM address mapping\nreverse-engineering method that uses selective pairwise measurements and\nstructured deduction, enabling recovery of complex mappings within seconds on\nthe latest memory controllers. Second, to break through the activation rate\nbottleneck of load-based hammering, we introduce a novel prefetch-based\nhammering paradigm that leverages the asynchronous nature of x86 prefetch\ninstructions and is further enhanced by multi-bank parallelism to maximize\nthroughput. Third, recognizing that speculative execution causes more severe\ndisorder issues for prefetching, which cannot be simply mitigated by memory\nbarriers, we develop a counter-speculation hammering technique using\ncontrol-flow obfuscation and optimized NOP-based pseudo-barriers to maintain\nprefetch order with minimal overhead. Evaluations across four latest Intel\narchitectures demonstrate $\\rho$Hammer's breakthrough effectiveness: it induces\nup to 200K+ additional bit flips within 2-hour attack pattern fuzzing processes\nand has a 112x higher flip rate than the load-based hammering baselines on\nComet and Rocket Lake. Also, we are the first to revive Rowhammer attacks on\nthe latest Raptor Lake architecture, where baselines completely fail, achieving\nstable flip rates of 2,291/min and fast end-to-end exploitation."
    },
    {
        "date": "2025-10",
        "title": "Colliding with Adversaries at ECML-PKDD 2025 Model Robustness Competition 1st Prize Solution",
        "author": "Dimitris Stefanopoulos, and Andreas Voskou",
        "link": "http://arxiv.org/abs/2510.16443v1",
        "abstract": "This report presents the winning solution for Task 2 of Colliding with\nAdversaries: A Challenge on Robust Learning in High Energy Physics Discovery at\nECML-PKDD 2025. The goal of the challenge was to design and train a robust\nANN-based model capable of achieving high accuracy in a binary classification\ntask on both clean and adversarial data generated with the Random Distribution\nShuffle Attack (RDSA). Our solution consists of two components: a data\ngeneration phase and a robust model training phase. In the first phase, we\nproduced 15 million artificial training samples using a custom methodology\nderived from Random Distribution Shuffle Attack (RDSA). In the second phase, we\nintroduced a robust architecture comprising (i)a Feature Embedding Block with\nshared weights among features of the same type and (ii)a Dense Fusion Tail\nresponsible for the final prediction. Training this architecture on our\nadversarial dataset achieved a mixed accuracy score of 80\\%, exceeding the\nsecond-place solution by two percentage points."
    },
    {
        "date": "2025-10",
        "title": "Colliding with Adversaries at ECML-PKDD 2025 Adversarial Attack Competition 1st Prize Solution",
        "author": "Dimitris Stefanopoulos, and Andreas Voskou",
        "link": "http://arxiv.org/abs/2510.16440v1",
        "abstract": "This report presents the winning solution for Task 1 of Colliding with\nAdversaries: A Challenge on Robust Learning in High Energy Physics Discovery at\nECML-PKDD 2025. The task required designing an adversarial attack against a\nprovided classification model that maximizes misclassification while minimizing\nperturbations. Our approach employs a multi-round gradient-based strategy that\nleverages the differentiable structure of the model, augmented with random\ninitialization and sample-mixing techniques to enhance effectiveness. The\nresulting attack achieved the best results in perturbation size and fooling\nsuccess rate, securing first place in the competition."
    },
    {
        "date": "2025-10",
        "title": "LightGlueStick: a Fast and Robust Glue for Joint Point-Line Matching",
        "author": "Aidyn Ubingazhibov, R\u00e9mi Pautrat, Iago Su\u00e1rez, Shaohui Liu, Marc Pollefeys, and Viktor Larsson",
        "link": "http://arxiv.org/abs/2510.16438v1",
        "abstract": "Lines and points are complementary local features, whose combination has\nproven effective for applications such as SLAM and Structure-from-Motion. The\nbackbone of these pipelines are the local feature matchers, establishing\ncorrespondences across images. Traditionally, point and line matching have been\ntreated as independent tasks. Recently, GlueStick proposed a GNN-based network\nthat simultaneously operates on points and lines to establish matches. While\nrunning a single joint matching reduced the overall computational complexity,\nthe heavy architecture prevented real-time applications or deployment to edge\ndevices.\n  Inspired by recent progress in point matching, we propose LightGlueStick, a\nlightweight matcher for points and line segments. The key novel component in\nour architecture is the Attentional Line Message Passing (ALMP), which\nexplicitly exposes the connectivity of the lines to the network, allowing for\nefficient communication between nodes. In thorough experiments we show that\nLightGlueStick establishes a new state-of-the-art across different benchmarks.\nThe code is available at https://github.com/aubingazhib/LightGlueStick."
    },
    {
        "date": "2025-10",
        "title": "MIRAD - A comprehensive real-world robust anomaly detection dataset for Mass Individualization",
        "author": "Pulin Li, Guocheng Wu, Li Yin, Yuxin Zheng, Wei Zhang, and Yanjie Zhou",
        "link": "http://arxiv.org/abs/2510.16370v1",
        "abstract": "Social manufacturing leverages community collaboration and scattered\nresources to realize mass individualization in modern industry. However, this\nparadigm shift also introduces substantial challenges in quality control,\nparticularly in defect detection. The main difficulties stem from three\naspects. First, products often have highly customized configurations. Second,\nproduction typically involves fragmented, small-batch orders. Third, imaging\nenvironments vary considerably across distributed sites. To overcome the\nscarcity of real-world datasets and tailored algorithms, we introduce the Mass\nIndividualization Robust Anomaly Detection (MIRAD) dataset. As the first\nbenchmark explicitly designed for anomaly detection in social manufacturing,\nMIRAD captures three critical dimensions of this domain: (1) diverse\nindividualized products with large intra-class variation, (2) data collected\nfrom six geographically dispersed manufacturing nodes, and (3) substantial\nimaging heterogeneity, including variations in lighting, background, and motion\nconditions. We then conduct extensive evaluations of state-of-the-art (SOTA)\nanomaly detection methods on MIRAD, covering one-class, multi-class, and\nzero-shot approaches. Results show a significant performance drop across all\nmodels compared with conventional benchmarks, highlighting the unresolved\ncomplexities of defect detection in real-world individualized production. By\nbridging industrial requirements and academic research, MIRAD provides a\nrealistic foundation for developing robust quality control solutions essential\nfor Industry 5.0. The dataset is publicly available at\nhttps://github.com/wu33learn/MIRAD."
    },
    {
        "date": "2025-10",
        "title": "OpenLVLM-MIA: A Controlled Benchmark Revealing the Limits of Membership Inference Attacks on Large Vision-Language Models",
        "author": "Ryoto Miyamoto, Xin Fan, Fuyuko Kido, Tsuneo Matsumoto, and Hayato Yamana",
        "link": "http://arxiv.org/abs/2510.16295v1",
        "abstract": "OpenLVLM-MIA is a new benchmark that highlights fundamental challenges in\nevaluating membership inference attacks (MIA) against large vision-language\nmodels (LVLMs). While prior work has reported high attack success rates, our\nanalysis suggests that these results often arise from detecting distributional\nbias introduced during dataset construction rather than from identifying true\nmembership status. To address this issue, we introduce a controlled benchmark\nof 6{,}000 images where the distributions of member and non-member samples are\ncarefully balanced, and ground-truth membership labels are provided across\nthree distinct training stages. Experiments using OpenLVLM-MIA demonstrated\nthat the performance of state-of-the-art MIA methods converged to random chance\nunder unbiased conditions. By offering a transparent and unbiased benchmark,\nOpenLVLM-MIA clarifies the current limitations of MIA research on LVLMs and\nprovides a solid foundation for developing stronger privacy-preserving\ntechniques."
    },
    {
        "date": "2025-10",
        "title": "Distractor Injection Attacks on Large Reasoning Models: Characterization and Defense",
        "author": "Zhehao Zhang, Weijie Xu, Shixian Cui, and Chandan K. Reddy",
        "link": "http://arxiv.org/abs/2510.16259v1",
        "abstract": "Recent advances in large reasoning models (LRMs) have enabled remarkable\nperformance on complex tasks such as mathematics and coding by generating long\nChain-of-Thought (CoT) traces. In this paper, we identify and systematically\nanalyze a critical vulnerability we term reasoning distraction, where LRMs are\ndiverted from their primary objective by irrelevant yet complex tasks\nmaliciously embedded in the prompt. Through a comprehensive study across\ndiverse models and benchmarks, we show that even state-of-the-art LRMs are\nhighly susceptible, with injected distractors reducing task accuracy by up to\n60%. We further reveal that certain alignment techniques can amplify this\nweakness and that models may exhibit covert compliance, following hidden\nadversarial instructions in reasoning while concealing them in the final\noutput. To mitigate these risks, we propose a training-based defense that\ncombines Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) on\nsynthetic adversarial data, improving robustness by over 50 points on\nchallenging distractor attacks. Our findings establish reasoning distraction as\na distinct and urgent threat to LRM reliability and provide a practical step\ntoward safer and more trustworthy reasoning systems."
    },
    {
        "date": "2025-10",
        "title": "Detecting Adversarial Fine-tuning with Auditing Agents",
        "author": "Sarah Egler, John Schulman, and Nicholas Carlini",
        "link": "http://arxiv.org/abs/2510.16255v1",
        "abstract": "Large Language Model (LLM) providers expose fine-tuning APIs that let end\nusers fine-tune their frontier LLMs. Unfortunately, it has been shown that an\nadversary with fine-tuning access to an LLM can bypass safeguards. Particularly\nconcerning, such attacks may avoid detection with datasets that are only\nimplicitly harmful. Our work studies robust detection mechanisms for\nadversarial use of fine-tuning APIs. We introduce the concept of a fine-tuning\nauditing agent and show it can detect harmful fine-tuning prior to model\ndeployment. We provide our auditing agent with access to the fine-tuning\ndataset, as well as the fine-tuned and pre-fine-tuned models, and request the\nagent assigns a risk score for the fine-tuning job. We evaluate our detection\napproach on a diverse set of eight strong fine-tuning attacks from the\nliterature, along with five benign fine-tuned models, totaling over 1400\nindependent audits. These attacks are undetectable with basic content\nmoderation on the dataset, highlighting the challenge of the task. With the\nbest set of affordances, our auditing agent achieves a 56.2% detection rate of\nadversarial fine-tuning at a 1% false positive rate. Most promising, the\nauditor is able to detect covert cipher attacks that evade safety evaluations\nand content moderation of the dataset. While benign fine-tuning with\nunintentional subtle safety degradation remains a challenge, we establish a\nbaseline configuration for further work in this area. We release our auditing\nagent at https://github.com/safety-research/finetuning-auditor."
    },
    {
        "date": "2025-10",
        "title": "Bridging Symmetry and Robustness: On the Role of Equivariance in Enhancing Adversarial Robustness",
        "author": "Longwei Wang, Ifrat Ikhtear Uddin, KC Santosh, Chaowei Zhang, Xiao Qin, and Yang Zhou",
        "link": "http://arxiv.org/abs/2510.16171v2",
        "abstract": "Adversarial examples reveal critical vulnerabilities in deep neural networks\nby exploiting their sensitivity to imperceptible input perturbations. While\nadversarial training remains the predominant defense strategy, it often incurs\nsignificant computational cost and may compromise clean-data accuracy. In this\nwork, we investigate an architectural approach to adversarial robustness by\nembedding group-equivariant convolutions-specifically, rotation- and\nscale-equivariant layers-into standard convolutional neural networks (CNNs).\nThese layers encode symmetry priors that align model behavior with structured\ntransformations in the input space, promoting smoother decision boundaries and\ngreater resilience to adversarial attacks. We propose and evaluate two\nsymmetry-aware architectures: a parallel design that processes standard and\nequivariant features independently before fusion, and a cascaded design that\napplies equivariant operations sequentially. Theoretically, we demonstrate that\nsuch models reduce hypothesis space complexity, regularize gradients, and yield\ntighter certified robustness bounds under the CLEVER (Cross Lipschitz Extreme\nValue for nEtwork Robustness) framework. Empirically, our models consistently\nimprove adversarial robustness and generalization across CIFAR-10, CIFAR-100,\nand CIFAR-10C under both FGSM and PGD attacks, without requiring adversarial\ntraining. These findings underscore the potential of symmetry-enforcing\narchitectures as efficient and principled alternatives to data\naugmentation-based defenses."
    },
    {
        "date": "2025-10",
        "title": "The Hidden Cost of Modeling P(X): Vulnerability to Membership Inference Attacks in Generative Text Classifiers",
        "author": "Owais Makroo, Siva Rajesh Kasa, Sumegh Roychowdhury, Karan Gupta, Nikhil Pattisapu, Santhosh Kasa, and Sumit Negi",
        "link": "http://arxiv.org/abs/2510.16122v1",
        "abstract": "Membership Inference Attacks (MIAs) pose a critical privacy threat by\nenabling adversaries to determine whether a specific sample was included in a\nmodel's training dataset. Despite extensive research on MIAs, systematic\ncomparisons between generative and discriminative classifiers remain limited.\nThis work addresses this gap by first providing theoretical motivation for why\ngenerative classifiers exhibit heightened susceptibility to MIAs, then\nvalidating these insights through comprehensive empirical evaluation. Our study\nencompasses discriminative, generative, and pseudo-generative text classifiers\nacross varying training data volumes, evaluated on nine benchmark datasets.\nEmploying a diverse array of MIA strategies, we consistently demonstrate that\nfully generative classifiers which explicitly model the joint likelihood\n$P(X,Y)$ are most vulnerable to membership leakage. Furthermore, we observe\nthat the canonical inference approach commonly used in generative classifiers\nsignificantly amplifies this privacy risk. These findings reveal a fundamental\nutility-privacy trade-off inherent in classifier design, underscoring the\ncritical need for caution when deploying generative classifiers in\nprivacy-sensitive applications. Our results motivate future research directions\nin developing privacy-preserving generative classifiers that can maintain\nutility while mitigating membership inference vulnerabilities."
    },
    {
        "date": "2025-10",
        "title": "PokeeResearch: Effective Deep Research via Reinforcement Learning from AI Feedback and Robust Reasoning Scaffold",
        "author": "Yi Wan, Jiuqi Wang, Liam Li, Jinsong Liu, Ruihao Zhu, and Zheqing Zhu",
        "link": "http://arxiv.org/abs/2510.15862v3",
        "abstract": "Tool-augmented large language models (LLMs) are emerging as deep research\nagents, systems that decompose complex queries, retrieve external evidence, and\nsynthesize grounded responses. Yet current agents remain limited by shallow\nretrieval, weak alignment metrics, and brittle tool-use behavior. We introduce\nPokeeResearch-7B, a 7B-parameter deep research agent built under a unified\nreinforcement learning framework for robustness, alignment, and scalability.\nPokeeResearch-7B is trained by an annotation-free Reinforcement Learning from\nAI Feedback (RLAIF) framework to optimize policies using LLM-based reward\nsignals that capture factual accuracy, citation faithfulness, and instruction\nadherence. A chain-of-thought-driven multi-call reasoning scaffold further\nenhances robustness through self-verification and adaptive recovery from tool\nfailures. Among 10 popular deep research benchmarks, PokeeResearch-7B achieves\nstate-of-the-art performance among 7B-scale deep research agents. This\nhighlights that careful reinforcement learning and reasoning design can produce\nefficient, resilient, and research-grade AI agents. The model and inference\ncode is open-sourced under Apache 2.0 license at\nhttps://github.com/Pokee-AI/PokeeResearchOSS."
    },
    {
        "date": "2025-10",
        "title": "Towards Proactive Defense Against Cyber Cognitive Attacks",
        "author": "Bonnie Rushing, Mac-Rufus Umeokolo, and Shouhuai Xu",
        "link": "http://arxiv.org/abs/2510.15801v1",
        "abstract": "Cyber cognitive attacks leverage disruptive innovations (DIs) to exploit\npsychological biases and manipulate decision-making processes. Emerging\ntechnologies, such as AI-driven disinformation and synthetic media, have\naccelerated the scale and sophistication of these threats. Prior studies\nprimarily categorize current cognitive attack tactics, lacking predictive\nmechanisms to anticipate future DIs and their malicious use in cognitive\nattacks. This paper addresses these gaps by introducing a novel predictive\nmethodology for forecasting the emergence of DIs and their malicious uses in\ncognitive attacks. We identify trends in adversarial tactics and propose\nproactive defense strategies."
    },
    {
        "date": "2025-10",
        "title": "Ambusher: Exploring the Security of Distributed SDN Controllers Through Protocol State Fuzzing",
        "author": "Jinwoo Kim, Minjae Seo, Eduard Marin, Seungsoo Lee, Jaehyun Nam, and Seungwon Shin",
        "link": "http://arxiv.org/abs/2510.15798v1",
        "abstract": "Distributed SDN (Software-Defined Networking) controllers have rapidly become\nan integral element of Wide Area Networks (WAN), particularly within SD-WAN,\nproviding scalability and fault-tolerance for expansive network\ninfrastructures. However, the architecture of these controllers introduces new\npotential attack surfaces that have thus far received inadequate attention. In\nresponse to these concerns, we introduce Ambusher, a testing tool designed to\ndiscover vulnerabilities within protocols used in distributed SDN controllers.\nAmbusher achieves this by leveraging protocol state fuzzing, which\nsystematically finds attack scenarios based on an inferred state machine. Since\nlearning states from a cluster is complicated, Ambusher proposes a novel\nmethodology that extracts a single and relatively simple state machine,\nachieving efficient state-based fuzzing. Our evaluation of Ambusher, conducted\non a real SD-WAN deployment spanning two campus networks and one enterprise\nnetwork, illustrates its ability to uncover 6 potential vulnerabilities in the\nwidely used distributed controller platform."
    },
    {
        "date": "2025-10",
        "title": "Towards a Blockchain-Based CI/CD Framework to Enhance Security in Cloud Environments",
        "author": "Sabbir M Saleh, Nazim Madhavji, and John Steinbacher",
        "link": "http://arxiv.org/abs/2510.16087v1",
        "abstract": "Security is becoming a pivotal point in cloud platforms. Several divisions,\nsuch as business organisations, health care, government, etc., have experienced\ncyber-attacks on their infrastructures. This research focuses on security\nissues within Continuous Integration and Deployment (CI/CD) pipelines in a\ncloud platform as a reaction to recent cyber breaches. This research proposes a\nblockchain-based solution to enhance CI/CD pipeline security. This research\naims to develop a framework that leverages blockchain's distributed ledger\ntechnology and tamper-resistant features to improve CI/CD pipeline security.\nThe goal is to emphasise secure software deployment by integrating threat\nmodelling frameworks and adherence to coding standards. It also aims to employ\ntools to automate security testing to detect publicly disclosed vulnerabilities\nand flaws, such as an outdated version of Java Spring Framework, a JavaScript\nlibrary from an unverified source, or a database library that allows SQL\ninjection attacks in the deployed software through the framework."
    },
    {
        "date": "2025-10",
        "title": "Grassroots Logic Programs: A Secure, Multiagent, Concurrent, Logic Programming Language",
        "author": "Ehud Shapiro",
        "link": "http://arxiv.org/abs/2510.15747v1",
        "abstract": "Grassroots platforms are distributed applications run by\\linebreak\ncryptographically-identified people on their networked personal devices, where\nmultiple disjoint platform instances emerge independently and coalesce when\nthey interoperate. Their foundation is the grassroots social graph, upon which\ngrassroots social networks, grassroots cryptocurrencies, and grassroots\ndemocratic federations can be built.\n  Grassroots platforms have yet to be implemented, the key challenge being\nfaulty and malicious participants: without secure programming support, correct\nparticipants cannot reliably identify each other, establish secure\ncommunication, or verify each other's code integrity.\n  We present Grassroots Logic Programs (GLP), a secure, multiagent, concurrent,\nlogic programming language for implementing grassroots platforms. GLP extends\nlogic programs with paired single-reader/single-writer (SRSW) logic variables,\nproviding secure communication channels among cryptographically-identified\npeople through encrypted, signed and attested messages, which enable identity\nand code integrity verification. We present GLP progressively: logic programs,\nconcurrent GLP, multiagent GLP, augmenting it with cryptographic security, and\nproviding smartphone implementation-ready specifications. We prove safety\nproperties including that GLP computations are deductions, SRSW preservation,\nacyclicity, and monotonicity. We prove multiagent GLP is grassroots and that\nGLP streams achieve blockchain security properties. We present a grassroots\nsocial graph protocol establishing authenticated peer-to-peer connections and\ndemonstrate secure grassroots social networking applications."
    },
    {
        "date": "2025-10",
        "title": "Constrained Adversarial Perturbation",
        "author": "Virendra Nishad, Bhaskar Mukhoty, Hilal AlQuabeh, Sandeep K. Shukla, and Sayak Ray Chowdhury",
        "link": "http://arxiv.org/abs/2510.15699v1",
        "abstract": "Deep neural networks have achieved remarkable success in a wide range of\nclassification tasks. However, they remain highly susceptible to adversarial\nexamples - inputs that are subtly perturbed to induce misclassification while\nappearing unchanged to humans. Among various attack strategies, Universal\nAdversarial Perturbations (UAPs) have emerged as a powerful tool for both\nstress testing model robustness and facilitating scalable adversarial training.\nDespite their effectiveness, most existing UAP methods neglect domain specific\nconstraints that govern feature relationships. Violating such constraints, such\nas debt to income ratios in credit scoring or packet flow invariants in network\ncommunication, can render adversarial examples implausible or easily\ndetectable, thereby limiting their real world applicability.\n  In this work, we advance universal adversarial attacks to constrained feature\nspaces by formulating an augmented Lagrangian based min max optimization\nproblem that enforces multiple, potentially complex constraints of varying\nimportance. We propose Constrained Adversarial Perturbation (CAP), an efficient\nalgorithm that solves this problem using a gradient based alternating\noptimization strategy. We evaluate CAP across diverse domains including\nfinance, IT networks, and cyber physical systems, and demonstrate that it\nachieves higher attack success rates while significantly reducing runtime\ncompared to existing baselines. Our approach also generalizes seamlessly to\nindividual adversarial perturbations, where we observe similar strong\nperformance gains. Finally, we introduce a principled procedure for learning\nfeature constraints directly from data, enabling broad applicability across\ndomains with structured input spaces."
    },
    {
        "date": "2025-10",
        "title": "Unmasking Facial DeepFakes: A Robust Multiview Detection Framework for Natural Images",
        "author": "Sami Belguesmia, Mohand Sa\u00efd Allili, and Assia Hamadene",
        "link": "http://arxiv.org/abs/2510.15576v1",
        "abstract": "DeepFake technology has advanced significantly in recent years, enabling the\ncreation of highly realistic synthetic face images. Existing DeepFake detection\nmethods often struggle with pose variations, occlusions, and artifacts that are\ndifficult to detect in real-world conditions. To address these challenges, we\npropose a multi-view architecture that enhances DeepFake detection by analyzing\nfacial features at multiple levels. Our approach integrates three specialized\nencoders, a global view encoder for detecting boundary inconsistencies, a\nmiddle view encoder for analyzing texture and color alignment, and a local view\nencoder for capturing distortions in expressive facial regions such as the\neyes, nose, and mouth, where DeepFake artifacts frequently occur. Additionally,\nwe incorporate a face orientation encoder, trained to classify face poses,\nensuring robust detection across various viewing angles. By fusing features\nfrom these encoders, our model achieves superior performance in detecting\nmanipulated images, even under challenging pose and lighting\nconditions.Experimental results on challenging datasets demonstrate the\neffectiveness of our method, outperforming conventional single-view approaches"
    },
    {
        "date": "2025-10",
        "title": "Doubly Robust Estimation of Causal Effects in Strategic Equilibrium Systems",
        "author": "Sibo Xiao",
        "link": "http://arxiv.org/abs/2510.15555v2",
        "abstract": "We introduce the Strategic Doubly Robust (SDR) estimator, a novel framework\nthat integrates strategic equilibrium modeling with doubly robust estimation\nfor causal inference in strategic environments. SDR addresses endogenous\ntreatment assignment arising from strategic agent behavior, maintaining double\nrobustness while incorporating strategic considerations. Theoretical analysis\nconfirms SDR's consistency and asymptotic normality under strategic\nunconfoundedness. Empirical evaluations demonstrate SDR's superior performance\nover baseline methods, achieving 7.6\\%-29.3\\% bias reduction across varying\nstrategic strengths and maintaining robust scalability with agent populations.\nThe framework provides a principled approach for reliable causal inference when\nagents respond strategically to interventions."
    },
    {
        "date": "2025-10",
        "title": "MCA: Modality Composition Awareness for Robust Composed Multimodal Retrieval",
        "author": "Qiyu Wu, Shuyang Cui, Satoshi Hayakawa, Wei-Yao Wang, Hiromi Wakaki, and Yuki Mitsufuji",
        "link": "http://arxiv.org/abs/2510.15543v1",
        "abstract": "Multimodal retrieval, which seeks to retrieve relevant content across\nmodalities such as text or image, supports applications from AI search to\ncontents production. Despite the success of separate-encoder approaches like\nCLIP align modality-specific embeddings with contrastive learning, recent\nmultimodal large language models (MLLMs) enable a unified encoder that directly\nprocesses composed inputs. While flexible and advanced, we identify that\nunified encoders trained with conventional contrastive learning are prone to\nlearn modality shortcut, leading to poor robustness under distribution shifts.\nWe propose a modality composition awareness framework to mitigate this issue.\nConcretely, a preference loss enforces multimodal embeddings to outperform\ntheir unimodal counterparts, while a composition regularization objective\naligns multimodal embeddings with prototypes composed from its unimodal parts.\nThese objectives explicitly model structural relationships between the composed\nrepresentation and its unimodal counterparts. Experiments on various benchmarks\nshow gains in out-of-distribution retrieval, highlighting modality composition\nawareness as a effective principle for robust composed multimodal retrieval\nwhen utilizing MLLMs as the unified encoder."
    },
    {
        "date": "2025-10",
        "title": "Adversary-Free Counterfactual Prediction via Information-Regularized Representations",
        "author": "Shiqin Tang, Rong Feng, Shuxin Zhuang, Hongzong Li, and Youzhi Zhang",
        "link": "http://arxiv.org/abs/2510.15479v1",
        "abstract": "We study counterfactual prediction under assignment bias and propose a\nmathematically grounded, information-theoretic approach that removes\ntreatment-covariate dependence without adversarial training. Starting from a\nbound that links the counterfactual-factual risk gap to mutual information, we\nlearn a stochastic representation Z that is predictive of outcomes while\nminimizing I(Z; T). We derive a tractable variational objective that\nupper-bounds the information term and couples it with a supervised decoder,\nyielding a stable, provably motivated training criterion. The framework extends\nnaturally to dynamic settings by applying the information penalty to sequential\nrepresentations at each decision time. We evaluate the method on controlled\nnumerical simulations and a real-world clinical dataset, comparing against\nrecent state-of-the-art balancing, reweighting, and adversarial baselines.\nAcross metrics of likelihood, counterfactual error, and policy evaluation, our\napproach performs favorably while avoiding the training instabilities and\ntuning burden of adversarial schemes."
    },
    {
        "date": "2025-10",
        "title": "SoK: Taxonomy and Evaluation of Prompt Security in Large Language Models",
        "author": "Hanbin Hong, Shuya Feng, Nima Naderloui, Shenao Yan, Jingyu Zhang, Biying Liu, Ali Arastehfard, Heqing Huang, and Yuan Hong",
        "link": "http://arxiv.org/abs/2510.15476v2",
        "abstract": "Large Language Models (LLMs) have rapidly become integral to real-world\napplications, powering services across diverse sectors. However, their\nwidespread deployment has exposed critical security risks, particularly through\njailbreak prompts that can bypass model alignment and induce harmful outputs.\nDespite intense research into both attack and defense techniques, the field\nremains fragmented: definitions, threat models, and evaluation criteria vary\nwidely, impeding systematic progress and fair comparison. In this\nSystematization of Knowledge (SoK), we address these challenges by (1)\nproposing a holistic, multi-level taxonomy that organizes attacks, defenses,\nand vulnerabilities in LLM prompt security; (2) formalizing threat models and\ncost assumptions into machine-readable profiles for reproducible evaluation;\n(3) introducing an open-source evaluation toolkit for standardized, auditable\ncomparison of attacks and defenses; (4) releasing JAILBREAKDB, the largest\nannotated dataset of jailbreak and benign prompts to date;\\footnote{The dataset\nis released at\n\\href{https://huggingface.co/datasets/youbin2014/JailbreakDB}{\\textcolor{purple}{https://huggingface.co/datasets/youbin2014/JailbreakDB}}.}\nand (5) presenting a comprehensive evaluation platform and leaderboard of\nstate-of-the-art methods \\footnote{will be released soon.}. Our work unifies\nfragmented research, provides rigorous foundations for future studies, and\nsupports the development of robust, trustworthy LLMs suitable for high-stakes\ndeployment."
    },
    {
        "date": "2025-10",
        "title": "Robust Optimization in Causal Models and G-Causal Normalizing Flows",
        "author": "Gabriele Visentin, and Patrick Cheridito",
        "link": "http://arxiv.org/abs/2510.15458v1",
        "abstract": "In this paper, we show that interventionally robust optimization problems in\ncausal models are continuous under the $G$-causal Wasserstein distance, but may\nbe discontinuous under the standard Wasserstein distance. This highlights the\nimportance of using generative models that respect the causal structure when\naugmenting data for such tasks. To this end, we propose a new normalizing flow\narchitecture that satisfies a universal approximation property for causal\nstructural models and can be efficiently trained to minimize the $G$-causal\nWasserstein distance. Empirically, we demonstrate that our model outperforms\nstandard (non-causal) generative models in data augmentation for causal\nregression and mean-variance portfolio optimization in causal factor models."
    },
    {
        "date": "2025-10",
        "title": "DPTrack:Directional Kernel-Guided Prompt Learning for Robust Nighttime Aerial Tracking",
        "author": "Zhiqiang Zhu, Xinbo Gao, Wen Lu, Jie Li, Zhaoyang Wang, and Mingqian Ge",
        "link": "http://arxiv.org/abs/2510.15449v1",
        "abstract": "Existing nighttime aerial trackers based on prompt learning rely solely on\nspatial localization supervision, which fails to provide fine-grained cues that\npoint to target features and inevitably produces vague prompts. This limitation\nimpairs the tracker's ability to accurately focus on the object features and\nresults in trackers still performing poorly. To address this issue, we propose\nDPTrack, a prompt-based aerial tracker designed for nighttime scenarios by\nencoding the given object's attribute features into the directional kernel\nenriched with fine-grained cues to generate precise prompts. Specifically,\ndrawing inspiration from visual bionics, DPTrack first hierarchically captures\nthe object's topological structure, leveraging topological attributes to enrich\nthe feature representation. Subsequently, an encoder condenses these\ntopology-aware features into the directional kernel, which serves as the core\nguidance signal that explicitly encapsulates the object's fine-grained\nattribute cues. Finally, a kernel-guided prompt module built on\nchannel-category correspondence attributes propagates the kernel across the\nfeatures of the search region to pinpoint the positions of target features and\nconvert them into precise prompts, integrating spatial gating for robust\nnighttime tracking. Extensive evaluations on established benchmarks demonstrate\nDPTrack's superior performance. Our code will be available at\nhttps://github.com/zzq-vipsl/DPTrack."
    },
    {
        "date": "2025-10",
        "title": "MAVR-Net: Robust Multi-View Learning for MAV Action Recognition with Cross-View Attention",
        "author": "Nengbo Zhang, and Hann Woei Ho",
        "link": "http://arxiv.org/abs/2510.15448v1",
        "abstract": "Recognizing the motion of Micro Aerial Vehicles (MAVs) is crucial for\nenabling cooperative perception and control in autonomous aerial swarms. Yet,\nvision-based recognition models relying only on RGB data often fail to capture\nthe complex spatial temporal characteristics of MAV motion, which limits their\nability to distinguish different actions. To overcome this problem, this paper\npresents MAVR-Net, a multi-view learning-based MAV action recognition\nframework. Unlike traditional single-view methods, the proposed approach\ncombines three complementary types of data, including raw RGB frames, optical\nflow, and segmentation masks, to improve the robustness and accuracy of MAV\nmotion recognition. Specifically, ResNet-based encoders are used to extract\ndiscriminative features from each view, and a multi-scale feature pyramid is\nadopted to preserve the spatiotemporal details of MAV motion patterns. To\nenhance the interaction between different views, a cross-view attention module\nis introduced to model the dependencies among various modalities and feature\nscales. In addition, a multi-view alignment loss is designed to ensure semantic\nconsistency and strengthen cross-view feature representations. Experimental\nresults on benchmark MAV action datasets show that our method clearly\noutperforms existing approaches, achieving 97.8\\%, 96.5\\%, and 92.8\\% accuracy\non the Short MAV, Medium MAV, and Long MAV datasets, respectively."
    },
    {
        "date": "2025-10",
        "title": "Learning to Detect Unknown Jailbreak Attacks in Large Vision-Language Models",
        "author": "Shuang Liang, Zhihao Xu, Jialing Tao, Hui Xue, and Xiting Wang",
        "link": "http://arxiv.org/abs/2510.15430v2",
        "abstract": "Despite extensive alignment efforts, Large Vision-Language Models (LVLMs)\nremain vulnerable to jailbreak attacks, posing serious safety risks. To address\nthis, existing detection methods either learn attack-specific parameters, which\nhinders generalization to unseen attacks, or rely on heuristically sound\nprinciples, which limit accuracy and efficiency. To overcome these limitations,\nwe propose Learning to Detect (LoD), a general framework that accurately\ndetects unknown jailbreak attacks by shifting the focus from attack-specific\nlearning to task-specific learning. This framework includes a Multi-modal\nSafety Concept Activation Vector module for safety-oriented representation\nlearning and a Safety Pattern Auto-Encoder module for unsupervised attack\nclassification. Extensive experiments show that our method achieves\nconsistently higher detection AUROC on diverse unknown attacks while improving\nefficiency. The code is available at\nhttps://anonymous.4open.science/r/Learning-to-Detect-51CB."
    },
    {
        "date": "2025-10",
        "title": "Safe, Efficient, and Robust Reinforcement Learning for Ranking and Diffusion Models",
        "author": "Shashank Gupta",
        "link": "http://arxiv.org/abs/2510.15429v1",
        "abstract": "This dissertation investigates how reinforcement learning (RL) methods can be\ndesigned to be safe, sample-efficient, and robust. Framed through the unifying\nperspective of contextual-bandit RL, the work addresses two major application\ndomains - ranking and recommendation, and text-to-image diffusion models. The\nfirst part of the thesis develops theory and algorithms for safe deployment in\nranking systems. An exposure-based generalisation bound is derived, leading to\na counterfactual risk-minimisation objective whose solution is guaranteed not\nto underperform the logging policy, even with sparse feedback. This guarantee\nis extended to doubly robust estimators, enabling safety even under adversarial\nor misspecified user models and offering practitioners explicit control over\npermissible utility loss. The second part turns to single-action bandits, where\nvarious off-policy estimators are unified within a baseline-correction\nframework. A closed-form optimal baseline is proposed and shown to minimise\nboth evaluation and policy-gradient variance, thereby improving off-policy\nlearning reliability. The final part examines the trade-offs between efficiency\nand effectiveness in generative RL. A systematic study of PPO and REINFORCE\nmotivates the Leave-One-Out PPO (LOOP) algorithm, which combines multiple\ndiffusion trajectories with a REINFORCE-style baseline inside PPO's clipped\nobjective. LOOP achieves PPO-level sample efficiency while producing\ngenerations that align more faithfully with textual attributes."
    },
    {
        "date": "2025-10",
        "title": "Robust High-Resolution Multi-Organ Diffusion MRI Using Synthetic-Data-Tuned Prompt Learning",
        "author": "Chen Qian, Haoyu Zhang, Junnan Ma, Liuhong Zhu, Qingrui Cai, Yu Wang, Ruibo Song, Lv Li, Lin Mei, Xianwang Jiang, Qin Xu, Boyu Jiang, Ran Tao, Chunmiao Chen, Shufang Chen, Dongyun Liang, Qiu Guo, Jianzhong Lin, Taishan Kang, Mengtian Lu, Liyuan Fu, Ruibin Huang, Huijuan Wan, Xu Huang, Jianhua Wang, Di Guo, Hai Zhong, Jianjun Zhou, and Xiaobo Qu",
        "link": "http://arxiv.org/abs/2510.15400v1",
        "abstract": "Clinical adoption of multi-shot diffusion-weighted magnetic resonance imaging\n(multi-shot DWI) for body-wide tumor diagnostics is limited by severe\nmotion-induced phase artifacts from respiration, peristalsis, and so on,\ncompounded by multi-organ, multi-slice, multi-direction and multi-b-value\ncomplexities. Here, we introduce a reconstruction framework, LoSP-Prompt, that\novercomes these challenges through physics-informed modeling and\nsynthetic-data-driven prompt learning. We model inter-shot phase variations as\na high-order Locally Smooth Phase (LoSP), integrated into a low-rank Hankel\nmatrix reconstruction. Crucially, the algorithm's rank parameter is\nautomatically set via prompt learning trained exclusively on synthetic\nabdominal DWI data emulating physiological motion. Validated across 10,000+\nclinical images (43 subjects, 4 scanner models, 5 centers), LoSP-Prompt: (1)\nAchieved twice the spatial resolution of clinical single-shot DWI, enhancing\nliver lesion conspicuity; (2) Generalized to seven diverse anatomical regions\n(liver, kidney, sacroiliac, pelvis, knee, spinal cord, brain) with a single\nmodel; (3) Outperformed state-of-the-art methods in image quality, artifact\nsuppression, and noise reduction (11 radiologists' evaluations on a 5-point\nscale, $p<0.05$), achieving 4-5 points (excellent) on kidney DWI, 4 points\n(good to excellent) on liver, sacroiliac and spinal cord DWI, and 3-4 points\n(good) on knee and tumor brain. The approach eliminates navigator signals and\nrealistic data supervision, providing an interpretable, robust solution for\nhigh-resolution multi-organ multi-shot DWI. Its scanner-agnostic performance\nsignifies transformative potential for precision oncology."
    },
    {
        "date": "2025-10",
        "title": "Towards Robust Zero-Shot Reinforcement Learning",
        "author": "Kexin Zheng, Lauriane Teyssier, Yinan Zheng, Yu Luo, and Xianyuan Zhan",
        "link": "http://arxiv.org/abs/2510.15382v2",
        "abstract": "The recent development of zero-shot reinforcement learning (RL) has opened a\nnew avenue for learning pre-trained generalist policies that can adapt to\narbitrary new tasks in a zero-shot manner. While the popular Forward-Backward\nrepresentations (FB) and related methods have shown promise in zero-shot RL, we\nempirically found that their modeling lacks expressivity and that extrapolation\nerrors caused by out-of-distribution (OOD) actions during offline learning\nsometimes lead to biased representations, ultimately resulting in suboptimal\nperformance. To address these issues, we propose Behavior-REgularizEd Zero-shot\nRL with Expressivity enhancement (BREEZE), an upgraded FB-based framework that\nsimultaneously enhances learning stability, policy extraction capability, and\nrepresentation learning quality. BREEZE introduces behavioral regularization in\nzero-shot RL policy learning, transforming policy optimization into a stable\nin-sample learning paradigm. Additionally, BREEZE extracts the policy using a\ntask-conditioned diffusion model, enabling the generation of high-quality and\nmultimodal action distributions in zero-shot RL settings. Moreover, BREEZE\nemploys expressive attention-based architectures for representation modeling to\ncapture the complex relationships between environmental dynamics. Extensive\nexperiments on ExORL and D4RL Kitchen demonstrate that BREEZE achieves the best\nor near-the-best performance while exhibiting superior robustness compared to\nprior offline zero-shot RL methods. The official implementation is available\nat: https://github.com/Whiterrrrr/BREEZE."
    },
    {
        "date": "2025-10",
        "title": "Bilinear Compressive Security",
        "author": "Axel Flinth, Hubert Orlicki, Semira Einsele, and Gerhard Wunder",
        "link": "http://arxiv.org/abs/2510.15380v1",
        "abstract": "Beyond its widespread application in signal and image processing,\n\\emph{compressed sensing} principles have been greatly applied to secure\ninformation transmission (often termed 'compressive security'). In this\nscenario, the measurement matrix $Q$ acts as a one time pad encryption key (in\ncomplex number domain) which can achieve perfect information-theoretic security\ntogether with other benefits such as reduced complexity and energy efficiency\nparticularly useful in IoT. However, unless the matrix is changed for every\nmessage it is vulnerable towards known plain text attacks: only $n$\nobservations suffices to recover a key $Q$ with $n$ columns. In this paper, we\ninvent and analyze a new method (termed 'Bilinear Compressive Security (BCS)')\naddressing these shortcomings: In addition to the linear encoding of the\nmessage $x$ with a matrix $Q$, the sender convolves the resulting vector with a\nrandomly generated filter $h$. Assuming that $h$ and $x$ are sparse, the\nreceiver can then recover $x$ without knowledge of $h$ from $y=h*Qx$ through\nblind deconvolution. We study a rather idealized known plaintext attack for\nrecovering $Q$ from repeated observations of $y$'s for different, known $x_k$,\nwith varying and unknown $h$ ,giving Eve a number of advantages not present in\npractice. Our main result for BCS states that under a weak symmetry condition\non the filter $h$, recovering $Q$ will require extensive sampling from\ntransmissions of $\\Omega\\left(\\max\\left(n,(n/s)^2\\right)\\right)$ messages $x_k$\nif they are $s$-sparse. Remarkably, with $s=1$ it is impossible to recover the\nkey. In this way, the scheme is much safer than standard compressed sensing\neven though our assumptions are much in favor towards a potential attacker."
    },
    {
        "date": "2025-10",
        "title": "Backdoor or Manipulation? Graph Mixture of Experts Can Defend Against Various Graph Adversarial Attacks",
        "author": "Yuyuan Feng, Bin Ma, and Enyan Dai",
        "link": "http://arxiv.org/abs/2510.15333v1",
        "abstract": "Extensive research has highlighted the vulnerability of graph neural networks\n(GNNs) to adversarial attacks, including manipulation, node injection, and the\nrecently emerging threat of backdoor attacks. However, existing defenses\ntypically focus on a single type of attack, lacking a unified approach to\nsimultaneously defend against multiple threats. In this work, we leverage the\nflexibility of the Mixture of Experts (MoE) architecture to design a scalable\nand unified framework for defending against backdoor, edge manipulation, and\nnode injection attacks. Specifically, we propose an MI-based logic diversity\nloss to encourage individual experts to focus on distinct neighborhood\nstructures in their decision processes, thus ensuring a sufficient subset of\nexperts remains unaffected under perturbations in local structures. Moreover,\nwe introduce a robustness-aware router that identifies perturbation patterns\nand adaptively routes perturbed nodes to corresponding robust experts.\nExtensive experiments conducted under various adversarial settings demonstrate\nthat our method consistently achieves superior robustness against multiple\ngraph adversarial attacks."
    },
    {
        "date": "2025-10",
        "title": "Hyperbolic Structured Classification for Robust Single Positive Multi-label Learning",
        "author": "Yiming Lin, Shang Wang, Junkai Zhou, Qiufeng Wang, Xiao-Bo Jin, and Kaizhu Huang",
        "link": "http://arxiv.org/abs/2510.15296v1",
        "abstract": "Single Positive Multi-Label Learning (SPMLL) addresses the challenging\nscenario where each training sample is annotated with only one positive label\ndespite potentially belonging to multiple categories, making it difficult to\ncapture complex label relationships and hierarchical structures. While existing\nmethods implicitly model label relationships through distance-based similarity,\nlacking explicit geometric definitions for different relationship types. To\naddress these limitations, we propose the first hyperbolic classification\nframework for SPMLL that represents each label as a hyperbolic ball rather than\na point or vector, enabling rich inter-label relationship modeling through\ngeometric ball interactions. Our ball-based approach naturally captures\nmultiple relationship types simultaneously: inclusion for hierarchical\nstructures, overlap for co-occurrence patterns, and separation for semantic\nindependence. Further, we introduce two key component innovations: a\ntemperature-adaptive hyperbolic ball classifier and a physics-inspired\ndouble-well regularization that guides balls toward meaningful configurations.\nTo validate our approach, extensive experiments on four benchmark datasets\n(MS-COCO, PASCAL VOC, NUS-WIDE, CUB-200-2011) demonstrate competitive\nperformance with superior interpretability compared to existing methods.\nFurthermore, statistical analysis reveals strong correlation between learned\nembeddings and real-world co-occurrence patterns, establishing hyperbolic\ngeometry as a more robust paradigm for structured classification under\nincomplete supervision."
    },
    {
        "date": "2025-10",
        "title": "Robust Layerwise Scaling Rules by Proper Weight Decay Tuning",
        "author": "Zhiyuan Fan, Yifeng Liu, Qingyue Zhao, Angela Yuan, and Quanquan Gu",
        "link": "http://arxiv.org/abs/2510.15262v1",
        "abstract": "Empirical scaling laws prescribe how to allocate parameters, data, and\ncompute, while maximal-update parameterization ($\\mu$P) enables learning-rate\ntransfer across widths by equalizing early-time update magnitudes. However, in\nmodern scale-invariant architectures, training quickly enters an\noptimizer-governed steady state where normalization layers create backward\nscale sensitivity and the effective learning rate becomes width dependent,\ndegrading $\\mu$P transfer. We address this by introducing a weight-decay\nscaling rule for AdamW that preserves sublayer gain across widths. Empirically,\nthe singular-value spectrum of each matrix parameter scales in norm as\n$\\sqrt{\\eta/\\lambda}$ with an approximately invariant shape; under width\nscaling $d$, we observe that the top singular value scales approximately as\n$\\sqrt{\\eta/\\lambda}\\cdot d^{0.75}$. Combining this observation with the $\\mu$P\nlearning-rate rule $\\eta_2\\propto d^{-1}$ for matrix-like parameters implies an\nempirical weight-decay scaling rule $\\lambda_2\\propto \\sqrt{d}$ that\napproximately keeps sublayer gains width invariant. Together with vector-like\nparameters trained at $\\eta_1=\\Theta_d(1)$ and $\\lambda_1=0$, this yields\n\\emph{zero-shot} transfer of both learning rate and weight decay from proxy to\ntarget widths, removing per-width sweeps. We validate the rule on LLaMA-style\nTransformers and in a minimal synthetic setting, and we provide a simple\ndiagnostic, matching top singular values, to check sublayer-gain invariance.\nOur results extend $\\mu$P beyond the near-init regime by explicitly controlling\nsteady-state scales set by the optimizer, offering a practical recipe for\nwidth-robust hyperparameter transfer under AdamW."
    },
    {
        "date": "2025-10",
        "title": "DRO-InstructZero: Distributionally Robust Prompt Optimization for Large Language Models",
        "author": "Yangyang Li",
        "link": "http://arxiv.org/abs/2510.15260v1",
        "abstract": "Large language models are highly sensitive to prompt wording. However,\npopular automatic prompt search methods, including InstructZero, often degrade\nunder distribution shift and adversarial evaluation because they optimize\nexpected performance under a single evaluation distribution. Consequently,\nprompts that work in one setting frequently fail to transfer. To address this,\nDRO-InstructZero formulates zero-shot prompt optimization as robust Bayesian\noptimization. Specifically, an f-divergence ball defines an ambiguity set\naround the evaluation distribution, and a robust acquisition rule maximizes\nworst-case expected utility while retaining the query efficiency of Bayesian\nsearch. Therefore, the search explicitly targets reliability under distribution\nshift rather than average behavior alone. Experiments follow the\ninstruction-induction protocol with matched query budgets across formality\nrewriting, code debugging, and translation. For example, on BIG-Bench\ninformative-to-formal rewriting, accuracy improves from 61.3 +/- 0.7% to\napproximately 85-90%, yielding an absolute gain of about 25-30 points.\nMoreover, auto-debugging shows about +25-point gains under domain shift.\nMeanwhile, stable tasks such as cause-and-effect remain above 96%, indicating\nno loss on in-distribution cases. Furthermore, improvements are consistent\nacross divergence choices and decoding temperatures. Overall, DRO-InstructZero\nconnects distributionally robust optimization with prompt learning, offering a\nplug-and-play and general approach for reliable, transferable prompt alignment\nunder real-world uncertainty."
    },
    {
        "date": "2025-10",
        "title": "Beyond the Voice: Inertial Sensing of Mouth Motion for High Security Speech Verification",
        "author": "Ynes Ineza, Muhammad A. Ullah, Abdul Serwadda, and Aurore Munyaneza",
        "link": "http://arxiv.org/abs/2510.15173v1",
        "abstract": "Voice interfaces are increasingly used in high stakes domains such as mobile\nbanking, smart home security, and hands free healthcare. Meanwhile, modern\ngenerative models have made high quality voice forgeries inexpensive and easy\nto create, eroding confidence in voice authentication alone. To strengthen\nprotection against such attacks, we present a second authentication factor that\ncombines acoustic evidence with the unique motion patterns of a speaker's lower\nface. By placing lightweight inertial sensors around the mouth to capture mouth\nopening and evolving lower facial geometry, our system records a distinct\nmotion signature with strong discriminative power across individuals. We built\na prototype and recruited 43 participants to evaluate the system under four\nconditions seated, walking on level ground, walking on stairs, and speaking\nwith different language backgrounds (native vs. non native English). Across all\nscenarios, our approach consistently achieved a median equal error rate (EER)\nof 0.01 or lower, indicating that mouth movement data remain robust under\nvariations in gait, posture, and spoken language. We discuss specific use cases\nwhere this second line of defense could provide tangible security benefits to\nvoice authentication systems."
    },
    {
        "date": "2025-10",
        "title": "Targeted Attacks and Defenses for Distributed Federated Learning in Vehicular Networks",
        "author": "Utku Demir, Tugba Erpek, Yalin E. Sagduyu, Sastry Kompella, and Mengran Xue",
        "link": "http://arxiv.org/abs/2510.15109v1",
        "abstract": "In emerging networked systems, mobile edge devices such as ground vehicles\nand unmanned aerial system (UAS) swarms collectively aggregate vast amounts of\ndata to make machine learning decisions such as threat detection in remote,\ndynamic, and infrastructure-constrained environments where power and bandwidth\nare scarce. Federated learning (FL) addresses these constraints and privacy\nconcerns by enabling nodes to share local model weights for deep neural\nnetworks instead of raw data, facilitating more reliable decision-making than\nindividual learning. However, conventional FL relies on a central server to\ncoordinate model updates in each learning round, which imposes significant\ncomputational burdens on the central node and may not be feasible due to the\nconnectivity constraints. By eliminating dependence on a central server,\ndistributed federated learning (DFL) offers scalability, resilience to node\nfailures, learning robustness, and more effective defense strategies. Despite\nthese advantages, DFL remains vulnerable to increasingly advanced and stealthy\ncyberattacks. In this paper, we design sophisticated targeted training data\npoisoning and backdoor (Trojan) attacks, and characterize the emerging\nvulnerabilities in a vehicular network. We analyze how DFL provides resilience\nagainst such attacks compared to individual learning and present effective\ndefense mechanisms to further strengthen DFL against the emerging cyber\nthreats."
    },
    {
        "date": "2025-10",
        "title": "DialectGen: Benchmarking and Improving Dialect Robustness in Multimodal Generation",
        "author": "Yu Zhou, Sohyun An, Haikang Deng, Da Yin, Clark Peng, Cho-Jui Hsieh, Kai-Wei Chang, and Nanyun Peng",
        "link": "http://arxiv.org/abs/2510.14949v1",
        "abstract": "Contact languages like English exhibit rich regional variations in the form\nof dialects, which are often used by dialect speakers interacting with\ngenerative models. However, can multimodal generative models effectively\nproduce content given dialectal textual input? In this work, we study this\nquestion by constructing a new large-scale benchmark spanning six common\nEnglish dialects. We work with dialect speakers to collect and verify over 4200\nunique prompts and evaluate on 17 image and video generative models. Our\nautomatic and human evaluation results show that current state-of-the-art\nmultimodal generative models exhibit 32.26% to 48.17% performance degradation\nwhen a single dialect word is used in the prompt. Common mitigation methods\nsuch as fine-tuning and prompt rewriting can only improve dialect performance\nby small margins (< 7%), while potentially incurring significant performance\ndegradation in Standard American English (SAE). To this end, we design a\ngeneral encoder-based mitigation strategy for multimodal generative models. Our\nmethod teaches the model to recognize new dialect features while preserving SAE\nperformance. Experiments on models such as Stable Diffusion 1.5 show that our\nmethod is able to simultaneously raise performance on five dialects to be on\npar with SAE (+34.4%), while incurring near zero cost to SAE performance."
    },
    {
        "date": "2025-10",
        "title": "Architecture Is All You Need: Diversity-Enabled Sweet Spots for Robust Humanoid Locomotion",
        "author": "Blake Werner, Lizhi Yang, and Aaron D. Ames",
        "link": "http://arxiv.org/abs/2510.14947v2",
        "abstract": "Robust humanoid locomotion in unstructured environments requires\narchitectures that balance fast low-level stabilization with slower perceptual\ndecision-making. We show that a simple layered control architecture (LCA), a\nproprioceptive stabilizer running at high rate, coupled with a compact low-rate\nperceptual policy, enables substantially more robust performance than\nmonolithic end-to-end designs, even when using minimal perception encoders.\nThrough a two-stage training curriculum (blind stabilizer pretraining followed\nby perceptual fine-tuning), we demonstrate that layered policies consistently\noutperform one-stage alternatives in both simulation and hardware. On a Unitree\nG1 humanoid, our approach succeeds across stair and ledge tasks where one-stage\nperceptual policies fail. These results highlight that architectural separation\nof timescales, rather than network scale or complexity, is the key enabler for\nrobust perception-conditioned locomotion."
    },
    {
        "date": "2025-10",
        "title": "A Hard-Label Black-Box Evasion Attack against ML-based Malicious Traffic Detection Systems",
        "author": "Zixuan Liu, Yi Zhao, Zhuotao Liu, Qi Li, Chuanpu Fu, Guangmeng Zhou, and Ke Xu",
        "link": "http://arxiv.org/abs/2510.14906v1",
        "abstract": "Machine Learning (ML)-based malicious traffic detection is a promising\nsecurity paradigm. It outperforms rule-based traditional detection by\nidentifying various advanced attacks. However, the robustness of these ML\nmodels is largely unexplored, thereby allowing attackers to craft adversarial\ntraffic examples that evade detection. Existing evasion attacks typically rely\non overly restrictive conditions (e.g., encrypted protocols, Tor, or\nspecialized setups), or require detailed prior knowledge of the target (e.g.,\ntraining data and model parameters), which is impractical in realistic\nblack-box scenarios. The feasibility of a hard-label black-box evasion attack\n(i.e., applicable across diverse tasks and protocols without internal target\ninsights) thus remains an open challenge. To this end, we develop\nNetMasquerade, which leverages reinforcement learning (RL) to manipulate attack\nflows to mimic benign traffic and evade detection. Specifically, we establish a\ntailored pre-trained model called Traffic-BERT, utilizing a network-specialized\ntokenizer and an attention mechanism to extract diverse benign traffic\npatterns. Subsequently, we integrate Traffic-BERT into the RL framework,\nallowing NetMasquerade to effectively manipulate malicious packet sequences\nbased on benign traffic patterns with minimal modifications. Experimental\nresults demonstrate that NetMasquerade enables both brute-force and stealthy\nattacks to evade 6 existing detection methods under 80 attack scenarios,\nachieving over 96.65% attack success rate. Notably, it can evade the methods\nthat are either empirically or certifiably robust against existing evasion\nattacks. Finally, NetMasquerade achieves low-latency adversarial traffic\ngeneration, demonstrating its practicality in real-world scenarios."
    },
    {
        "date": "2025-10",
        "title": "Secure Sparse Matrix Multiplications and their Applications to Privacy-Preserving Machine Learning",
        "author": "Marc Damie, Florian Hahn, Andreas Peter, and Jan Ramon",
        "link": "http://arxiv.org/abs/2510.14894v1",
        "abstract": "To preserve privacy, multi-party computation (MPC) enables executing Machine\nLearning (ML) algorithms on secret-shared or encrypted data. However, existing\nMPC frameworks are not optimized for sparse data. This makes them unsuitable\nfor ML applications involving sparse data, e.g., recommender systems or\ngenomics. Even in plaintext, such applications involve high-dimensional sparse\ndata, that cannot be processed without sparsity-related optimizations due to\nprohibitively large memory requirements.\n  Since matrix multiplication is central in ML algorithms, we propose MPC\nalgorithms to multiply secret sparse matrices. On the one hand, our algorithms\navoid the memory issues of the \"dense\" data representation of classic secure\nmatrix multiplication algorithms. On the other hand, our algorithms can\nsignificantly reduce communication costs (some experiments show a factor 1000)\nfor realistic problem sizes. We validate our algorithms in two ML applications\nin which existing protocols are impractical.\n  An important question when developing MPC algorithms is what assumptions can\nbe made. In our case, if the number of non-zeros in a row is a sensitive piece\nof information then a short runtime may reveal that the number of non-zeros is\nsmall. Existing approaches make relatively simple assumptions, e.g., that there\nis a universal upper bound to the number of non-zeros in a row. This often\ndoesn't align with statistical reality, in a lot of sparse datasets the amount\nof data per instance satisfies a power law. We propose an approach which allows\nadopting a safe upper bound on the distribution of non-zeros in rows/columns of\nsparse matrices."
    },
    {
        "date": "2025-10",
        "title": "Leveraging Code Cohesion Analysis to Identify Source Code Supply Chain Attacks",
        "author": "Maor Reuben, Ido Mendel, Or Feldman, Moshe Kravchik, Mordehai Guri, and Rami Puzis",
        "link": "http://arxiv.org/abs/2510.14778v1",
        "abstract": "Supply chain attacks significantly threaten software security with malicious\ncode injections within legitimate projects. Such attacks are very rare but may\nhave a devastating impact. Detecting spurious code injections using automated\ntools is further complicated as it often requires deciphering the intention of\nboth the inserted code and its context. In this study, we propose an\nunsupervised approach for highlighting spurious code injections by quantifying\ncohesion disruptions in the source code. Using a name-prediction-based cohesion\n(NPC) metric, we analyze how function cohesion changes when malicious code is\nintroduced compared to natural cohesion fluctuations. An analysis of 54,707\nfunctions over 369 open-source C++ repositories reveals that code injection\nreduces cohesion and shifts naming patterns toward shorter, less descriptive\nnames compared to genuine function updates. Considering the sporadic nature of\nreal supply-chain attacks, we evaluate the proposed method with extreme\ntest-set imbalance and show that monitoring high-cohesion functions with NPC\ncan effectively detect functions with injected code, achieving a Precision@100\nof 36.41% at a 1:1,000 ratio and 12.47% at 1:10,000. These results suggest that\nautomated cohesion measurements, in general, and name-prediction-based\ncohesion, in particular, may help identify supply chain attacks, improving\nsource code integrity."
    },
    {
        "date": "2025-10",
        "title": "SLIE: A Secure and Lightweight Cryptosystem for Data Sharing in IoT Healthcare Services",
        "author": "Ha Xuan Son, Nguyen Quoc Anh, Phat T. Tran-Truong, Le Thanh Tuan, and Pham Thanh Nghiem",
        "link": "http://arxiv.org/abs/2510.14708v1",
        "abstract": "The Internet of Medical Things (IoMT) has revolutionized healthcare by\ntransforming medical operations into standardized, interoperable services.\nHowever, this service-oriented model introduces significant security\nvulnerabilities in device management and communication, which are especially\ncritical given the sensitivity of medical data. To address these risks, this\npaper proposes SLIE (Secure and Lightweight Identity Encryption), a novel\ncryptosystem based on Wildcard Key Derivation Identity-Based Encryption\n(WKD-IBE). SLIE ensures scalable trust and secure omnidirectional communication\nthrough end-to-end encryption, hierarchical access control, and a lightweight\nkey management system designed for resource-constrained devices. It\nincorporates constant-time operations, memory obfuscation, and expiry-based key\nrevocation to counter side-channel, man-in-the-middle, and unauthorized access\nattacks, thereby ensuring compliance with standards like HIPAA and GDPR.\nEvaluations show that SLIE significantly outperforms RSA, with encryption and\ndecryption times of 0.936ms and 0.217ms for 1KB of data, an 84.54% improvement\nin encryption speed, a 99.70% improvement in decryption speed, and an energy\nefficiency of 0.014 J/KB."
    },
    {
        "date": "2025-10",
        "title": "AEX-NStep: Probabilistic Interrupt Counting Attacks on Intel SGX",
        "author": "Nicolas Dutly, Friederike Groschupp, Ivan Puddu, Kari Kostiainen, and Srdjan Capkun",
        "link": "http://arxiv.org/abs/2510.14675v1",
        "abstract": "To mitigate interrupt-based stepping attacks (notably using SGX-Step), Intel\nintroduced AEX-Notify, an ISA extension to Intel SGX that aims to prevent\ndeterministic single-stepping. In this work, we introduce AEX-NStep, the first\ninterrupt counting attack on AEX-Notify-enabled Enclaves. We show that\ndeterministic single-stepping is not required for interrupt counting attacks to\nbe practical and that, therefore, AEX-Notify does not entirely prevent such\nattacks. We specifically show that one of AEX-Notify's security guarantees,\nobfuscated forward progress, does not hold, and we introduce two new\nprobabilistic interrupt counting attacks. We use these attacks to construct a\npractical ECDSA key leakage attack on an AEX-Notify-enabled SGX enclave. Our\nresults extend the original security analysis of AEX-Notify and inform the\ndesign of future mitigations."
    },
    {
        "date": "2025-10",
        "title": "SteeringTTA: Guiding Diffusion Trajectories for Robust Test-Time-Adaptation",
        "author": "Jihyun Yu, Yoojin Oh, Wonho Bae, Mingyu Kim, and Junhyug Noh",
        "link": "http://arxiv.org/abs/2510.14634v1",
        "abstract": "Test-time adaptation (TTA) aims to correct performance degradation of deep\nmodels under distribution shifts by updating models or inputs using unlabeled\ntest data. Input-only diffusion-based TTA methods improve robustness for\nclassification to corruptions but rely on gradient guidance, limiting\nexploration and generalization across distortion types. We propose SteeringTTA,\nan inference-only framework that adapts Feynman-Kac steering to guide\ndiffusion-based input adaptation for classification with rewards driven by\npseudo-label. SteeringTTA maintains multiple particle trajectories, steered by\na combination of cumulative top-K probabilities and an entropy schedule, to\nbalance exploration and confidence. On ImageNet-C, SteeringTTA consistently\noutperforms the baseline without any model updates or source data."
    },
    {
        "date": "2025-10",
        "title": "Lexo: Eliminating Stealthy Supply-Chain Attacks via LLM-Assisted Program Regeneration",
        "author": "Evangelos Lamprou, Julian Dai, Grigoris Ntousakis, Martin C. Rinard, and Nikos Vasilakis",
        "link": "http://arxiv.org/abs/2510.14522v2",
        "abstract": "Software supply-chain attacks are an important and ongoing concern in the\nopen source software ecosystem. These attacks maintain the standard\nfunctionality that a component implements, but additionally hide malicious\nfunctionality activated only when the component reaches its target environment.\nLexo addresses such stealthy attacks by automatically learning and regenerating\nvulnerability-free versions of potentially malicious components. Lexo first\ngenerates a set of input-output pairs to model a component's full observable\nbehavior, which it then uses to synthesize a new version of the original\ncomponent. The new component implements the original functionality but avoids\nstealthy malicious behavior. Throughout this regeneration process, Lexo\nconsults several distinct instances of Large Language Models (LLMs), uses\ncorrectness and coverage metrics to shepherd these instances, and guardrails\ntheir results. Our evaluation on 100+ real-world packages, including high\nprofile stealthy supply-chain attacks, indicates that Lexo scales across\nmultiple domains, regenerates code efficiently (<100s on average), maintains\ncompatibility, and succeeds in eliminating malicious code in several real-world\nsupply-chain-attacks, even in cases when a state-of-the-art LLM fails to\neliminate malicious code when prompted to do so."
    },
    {
        "date": "2025-10",
        "title": "Stealthy Dual-Trigger Backdoors: Attacking Prompt Tuning in LM-Empowered Graph Foundation Models",
        "author": "Xiaoyu Xue, Yuni Lai, Chenxi Huang, Yulin Zhu, Gaolei Li, Xiaoge Zhang, and Kai Zhou",
        "link": "http://arxiv.org/abs/2510.14470v1",
        "abstract": "The emergence of graph foundation models (GFMs), particularly those\nincorporating language models (LMs), has revolutionized graph learning and\ndemonstrated remarkable performance on text-attributed graphs (TAGs). However,\ncompared to traditional GNNs, these LM-empowered GFMs introduce unique security\nvulnerabilities during the unsecured prompt tuning phase that remain\nunderstudied in current research. Through empirical investigation, we reveal a\nsignificant performance degradation in traditional graph backdoor attacks when\noperating in attribute-inaccessible constrained TAG systems without explicit\ntrigger node attribute optimization. To address this, we propose a novel\ndual-trigger backdoor attack framework that operates at both text-level and\nstruct-level, enabling effective attacks without explicit optimization of\ntrigger node text attributes through the strategic utilization of a\npre-established text pool. Extensive experimental evaluations demonstrate that\nour attack maintains superior clean accuracy while achieving outstanding attack\nsuccess rates, including scenarios with highly concealed single-trigger nodes.\nOur work highlights critical backdoor risks in web-deployed LM-empowered GFMs\nand contributes to the development of more robust supervision mechanisms for\nopen-source platforms in the era of foundation models."
    },
    {
        "date": "2025-10",
        "title": "LiRA: Linguistic Robust Anchoring for Cross-lingual Large Language Models",
        "author": "Haolin Li, Haipeng Zhang, Mang Li, Yaohua Wang, Lijie Wen, Yu Zhang, and Biqing Huang",
        "link": "http://arxiv.org/abs/2510.14466v1",
        "abstract": "As large language models (LLMs) rapidly advance, performance on high-resource\nlanguages (e.g., English, Chinese) is nearing saturation, yet remains\nsubstantially lower for low-resource languages (e.g., Urdu, Thai) due to\nlimited training data, machine-translation noise, and unstable cross-lingual\nalignment. We introduce LiRA (Linguistic Robust Anchoring for Large Language\nModels), a training framework that robustly improves cross-lingual\nrepresentations under low-resource conditions while jointly strengthening\nretrieval and reasoning. LiRA comprises two modules: (i) Arca (Anchored\nRepresentation Composition Architecture), which anchors low-resource languages\nto an English semantic space via anchor-based alignment and multi-agent\ncollaborative encoding, preserving geometric stability in a shared embedding\nspace; and (ii) LaSR (Language-coupled Semantic Reasoner), which adds a\nlanguage-aware lightweight reasoning head with consistency regularization on\ntop of Arca's multilingual representations, unifying the training objective to\nenhance cross-lingual understanding, retrieval, and reasoning robustness. We\nfurther construct and release a multilingual product retrieval dataset covering\nfive Southeast Asian and two South Asian languages. Experiments across\nlow-resource benchmarks (cross-lingual retrieval, semantic similarity, and\nreasoning) show consistent gains and robustness under few-shot and\nnoise-amplified settings; ablations validate the contribution of both Arca and\nLaSR. Code will be released on GitHub and the dataset on Hugging Face."
    },
    {
        "date": "2025-10",
        "title": "Structured Universal Adversarial Attacks on Object Detection for Video Sequences",
        "author": "Sven Jacob, Weijia Shao, and Gjergji Kasneci",
        "link": "http://arxiv.org/abs/2510.14460v1",
        "abstract": "Video-based object detection plays a vital role in safety-critical\napplications. While deep learning-based object detectors have achieved\nimpressive performance, they remain vulnerable to adversarial attacks,\nparticularly those involving universal perturbations. In this work, we propose\na minimally distorted universal adversarial attack tailored for video object\ndetection, which leverages nuclear norm regularization to promote structured\nperturbations concentrated in the background. To optimize this formulation\nefficiently, we employ an adaptive, optimistic exponentiated gradient method\nthat enhances both scalability and convergence. Our results demonstrate that\nthe proposed attack outperforms both low-rank projected gradient descent and\nFrank-Wolfe based attacks in effectiveness while maintaining high stealthiness.\nAll code and data are publicly available at\nhttps://github.com/jsve96/AO-Exp-Attack."
    },
    {
        "date": "2025-10",
        "title": "BoardVision: Deployment-ready and Robust Motherboard Defect Detection with YOLO+Faster-RCNN Ensemble",
        "author": "Brandon Hill, and Kma Solaiman",
        "link": "http://arxiv.org/abs/2510.14389v1",
        "abstract": "Motherboard defect detection is critical for ensuring reliability in\nhigh-volume electronics manufacturing. While prior research in PCB inspection\nhas largely targeted bare-board or trace-level defects, assembly-level\ninspection of full motherboards inspection remains underexplored. In this work,\nwe present BoardVision, a reproducible framework for detecting assembly-level\ndefects such as missing screws, loose fan wiring, and surface scratches. We\nbenchmark two representative detectors - YOLOv7 and Faster R-CNN, under\ncontrolled conditions on the MiracleFactory motherboard dataset, providing the\nfirst systematic comparison in this domain. To mitigate the limitations of\nsingle models, where YOLO excels in precision but underperforms in recall and\nFaster R-CNN shows the reverse, we propose a lightweight ensemble,\nConfidence-Temporal Voting (CTV Voter), that balances precision and recall\nthrough interpretable rules. We further evaluate robustness under realistic\nperturbations including sharpness, brightness, and orientation changes,\nhighlighting stability challenges often overlooked in motherboard defect\ndetection. Finally, we release a deployable GUI-driven inspection tool that\nbridges research evaluation with operator usability. Together, these\ncontributions demonstrate how computer vision techniques can transition from\nbenchmark results to practical quality assurance for assembly-level motherboard\nmanufacturing."
    },
    {
        "date": "2025-10",
        "title": "DRBD-Mamba for Robust and Efficient Brain Tumor Segmentation with Analytical Insights",
        "author": "Danish Ali, Ajmal Mian, Naveed Akhtar, and Ghulam Mubashar Hassan",
        "link": "http://arxiv.org/abs/2510.14383v1",
        "abstract": "Accurate brain tumor segmentation is significant for clinical diagnosis and\ntreatment. It is challenging due to the heterogeneity of tumor subregions.\nMamba-based State Space Models have demonstrated promising performance.\nHowever, they incur significant computational overhead due to sequential\nfeature computation across multiple spatial axes. Moreover, their robustness\nacross diverse BraTS data partitions remains largely unexplored, leaving a\ncritical gap in reliable evaluation. To address these limitations, we propose\ndual-resolution bi-directional Mamba (DRBD-Mamba), an efficient 3D segmentation\nmodel that captures multi-scale long-range dependencies with minimal\ncomputational overhead. We leverage a space-filling curve to preserve spatial\nlocality during 3D-to-1D feature mapping, thereby reducing reliance on\ncomputationally expensive multi-axial feature scans. To enrich feature\nrepresentation, we propose a gated fusion module that adaptively integrates\nforward and reverse contexts, along with a quantization block that discretizes\nfeatures to improve robustness. In addition, we propose five systematic folds\non BraTS2023 for rigorous evaluation of segmentation techniques under diverse\nconditions and present detailed analysis of common failure scenarios. On the\n20\\% test set used by recent methods, our model achieves Dice improvements of\n0.10\\% for whole tumor, 1.75\\% for tumor core, and 0.93\\% for enhancing tumor.\nEvaluations on the proposed systematic five folds demonstrate that our model\nmaintains competitive whole tumor accuracy while achieving clear average Dice\ngains of 0.86\\% for tumor core and 1.45\\% for enhancing tumor over existing\nstate-of-the-art. Furthermore, our model attains 15 times improvement in\nefficiency while maintaining high segmentation accuracy, highlighting its\nrobustness and computational advantage over existing approaches."
    },
    {
        "date": "2025-10",
        "title": "BinCtx: Multi-Modal Representation Learning for Robust Android App Behavior Detection",
        "author": "Zichen Liu, Shao Yang, and Xusheng Xiao",
        "link": "http://arxiv.org/abs/2510.14344v1",
        "abstract": "Mobile app markets host millions of apps, yet undesired behaviors (e.g.,\ndisruptive ads, illegal redirection, payment deception) remain hard to catch\nbecause they often do not rely on permission-protected APIs and can be easily\ncamouflaged via UI or metadata edits. We present BINCTX, a learning approach\nthat builds multi-modal representations of an app from (i) a global\nbytecode-as-image view that captures code-level semantics and family-style\npatterns, (ii) a contextual view (manifested actions, components, declared\npermissions, URL/IP constants) indicating how behaviors are triggered, and\n(iii) a third-party-library usage view summarizing invocation frequencies along\ninter-component call paths. The three views are embedded and fused to train a\ncontextual-aware classifier. On real-world malware and benign apps, BINCTX\nattains a macro F1 of 94.73%, outperforming strong baselines by at least\n14.92%. It remains robust under commercial obfuscation (F1 84%\npost-obfuscation) and is more resistant to adversarial samples than\nstate-of-the-art bytecode-only systems."
    },
    {
        "date": "2025-10",
        "title": "A Robust Classification Method using Hybrid Word Embedding for Early Diagnosis of Alzheimer's Disease",
        "author": "Yangyang Li",
        "link": "http://arxiv.org/abs/2510.14332v1",
        "abstract": "Early detection of Alzheimer's Disease (AD) is greatly beneficial to AD\npatients, leading to early treatments that lessen symptoms and alleviating\nfinancial burden of health care. As one of the leading signs of AD, language\ncapability changes can be used for early diagnosis of AD. In this paper, I\ndevelop a robust classification method using hybrid word embedding and\nfine-tuned hyperparameters to achieve state-of-the-art accuracy in the early\ndetection of AD. Specifically, we create a hybrid word embedding based on word\nvectors from Doc2Vec and ELMo to obtain perplexity scores of the sentences. The\nscores identify whether a sentence is fluent or not and capture semantic\ncontext of the sentences. I enrich the word embedding by adding linguistic\nfeatures to analyze syntax and semantics. Further, we input an embedded feature\nvector into logistic regression and fine tune hyperparameters throughout the\npipeline. By tuning hyperparameters of the machine learning pipeline (e.g.,\nmodel regularization parameter, learning rate and vector size of Doc2Vec, and\nvector size of ELMo), I achieve 91% classification accuracy and an Area Under\nthe Curve (AUC) of 97% in distinguishing early AD from healthy subjects. Based\non my knowledge, my model with 91% accuracy and 97% AUC outperforms the best\nexisting NLP model for AD diagnosis with an accuracy of 88% [32]. I study the\nmodel stability through repeated experiments and find that the model is stable\neven though the training data is split randomly (standard deviation of accuracy\n= 0.0403; standard deviation of AUC = 0.0174). This affirms our proposed method\nis accurate and stable. This model can be used as a large-scale screening\nmethod for AD, as well as a complementary examination for doctors to detect AD."
    },
    {
        "date": "2025-10",
        "title": "TangledFeatures: Robust Feature Selection in Highly Correlated Spaces",
        "author": "Allen Daniel Sunny",
        "link": "http://arxiv.org/abs/2510.15005v1",
        "abstract": "Feature selection is a fundamental step in model development, shaping both\npredictive performance and interpretability. Yet, most widely used methods\nfocus on predictive accuracy, and their performance degrades in the presence of\ncorrelated predictors. To address this gap, we introduce TangledFeatures, a\nframework for feature selection in correlated feature spaces. It identifies\nrepresentative features from groups of entangled predictors, reducing\nredundancy while retaining explanatory power. The resulting feature subset can\nbe directly applied in downstream models, offering a more interpretable and\nstable basis for analysis compared to traditional selection techniques. We\ndemonstrate the effectiveness of TangledFeatures on Alanine Dipeptide, applying\nit to the prediction of backbone torsional angles and show that the selected\nfeatures correspond to structurally meaningful intra-atomic distances that\nexplain variation in these angles."
    },
    {
        "date": "2025-10",
        "title": "A Multi-domain Image Translative Diffusion StyleGAN for Iris Presentation Attack Detection",
        "author": "Shivangi Yadav, and Arun Ross",
        "link": "http://arxiv.org/abs/2510.14314v1",
        "abstract": "An iris biometric system can be compromised by presentation attacks (PAs)\nwhere artifacts such as artificial eyes, printed eye images, or cosmetic\ncontact lenses are presented to the system. To counteract this, several\npresentation attack detection (PAD) methods have been developed. However, there\nis a scarcity of datasets for training and evaluating iris PAD techniques due\nto the implicit difficulties in constructing and imaging PAs. To address this,\nwe introduce the Multi-domain Image Translative Diffusion StyleGAN\n(MID-StyleGAN), a new framework for generating synthetic ocular images that\ncaptures the PA and bonafide characteristics in multiple domains such as\nbonafide, printed eyes and cosmetic contact lens. MID-StyleGAN combines the\nstrengths of diffusion models and generative adversarial networks (GANs) to\nproduce realistic and diverse synthetic data. Our approach utilizes a\nmulti-domain architecture that enables the translation between bonafide ocular\nimages and different PA domains. The model employs an adaptive loss function\ntailored for ocular data to maintain domain consistency. Extensive experiments\ndemonstrate that MID-StyleGAN outperforms existing methods in generating\nhigh-quality synthetic ocular images. The generated data was used to\nsignificantly enhance the performance of PAD systems, providing a scalable\nsolution to the data scarcity problem in iris and ocular biometrics. For\nexample, on the LivDet2020 dataset, the true detect rate at 1% false detect\nrate improved from 93.41% to 98.72%, showcasing the impact of the proposed\nmethod."
    },
    {
        "date": "2025-10",
        "title": "Terrarium: Revisiting the Blackboard for Multi-Agent Safety, Privacy, and Security Studies",
        "author": "Mason Nakamura, Abhinav Kumar, Saaduddin Mahmud, Sahar Abdelnabi, Shlomo Zilberstein, and Eugene Bagdasarian",
        "link": "http://arxiv.org/abs/2510.14312v1",
        "abstract": "A multi-agent system (MAS) powered by large language models (LLMs) can\nautomate tedious user tasks such as meeting scheduling that requires\ninter-agent collaboration. LLMs enable nuanced protocols that account for\nunstructured private data, user constraints, and preferences. However, this\ndesign introduces new risks, including misalignment and attacks by malicious\nparties that compromise agents or steal user data. In this paper, we propose\nthe Terrarium framework for fine-grained study on safety, privacy, and security\nin LLM-based MAS. We repurpose the blackboard design, an early approach in\nmulti-agent systems, to create a modular, configurable testbed for multi-agent\ncollaboration. We identify key attack vectors such as misalignment, malicious\nagents, compromised communication, and data poisoning. We implement three\ncollaborative MAS scenarios with four representative attacks to demonstrate the\nframework's flexibility. By providing tools to rapidly prototype, evaluate, and\niterate on defenses and designs, Terrarium aims to accelerate progress toward\ntrustworthy multi-agent systems."
    },
    {
        "date": "2025-10",
        "title": "Beyond a Single Perspective: Towards a Realistic Evaluation of Website Fingerprinting Attacks",
        "author": "Xinhao Deng, Jingyou Chen, Linxiao Yu, Yixiang Zhang, Zhongyi Gu, Changhao Qiu, Xiyuan Zhao, Ke Xu, and Qi Li",
        "link": "http://arxiv.org/abs/2510.14283v1",
        "abstract": "Website Fingerprinting (WF) attacks exploit patterns in encrypted traffic to\ninfer the websites visited by users, posing a serious threat to anonymous\ncommunication systems. Although recent WF techniques achieve over 90% accuracy\nin controlled experimental settings, most studies remain confined to single\nscenarios, overlooking the complexity of real-world environments. This paper\npresents the first systematic and comprehensive evaluation of existing WF\nattacks under diverse realistic conditions, including defense mechanisms,\ntraffic drift, multi-tab browsing, early-stage detection, open-world settings,\nand few-shot scenarios. Experimental results show that many WF techniques with\nstrong performance in isolated settings degrade significantly when facing other\nconditions. Since real-world environments often combine multiple challenges,\ncurrent WF attacks are difficult to apply directly in practice. This study\nhighlights the limitations of WF attacks and introduces a multidimensional\nevaluation framework, offering critical insights for developing more robust and\npractical WF attacks."
    },
    {
        "date": "2025-10",
        "title": "CLEAR: Causal Learning Framework For Robust Histopathology Tumor Detection Under Out-Of-Distribution Shifts",
        "author": "Kieu-Anh Truong Thi, Huy-Hieu Pham, and Duc-Trong Le",
        "link": "http://arxiv.org/abs/2510.14273v1",
        "abstract": "Domain shift in histopathology, often caused by differences in acquisition\nprocesses or data sources, poses a major challenge to the generalization\nability of deep learning models. Existing methods primarily rely on modeling\nstatistical correlations by aligning feature distributions or introducing\nstatistical variation, yet they often overlook causal relationships. In this\nwork, we propose a novel causal-inference-based framework that leverages\nsemantic features while mitigating the impact of confounders. Our method\nimplements the front-door principle by designing transformation strategies that\nexplicitly incorporate mediators and observed tissue slides. We validate our\nmethod on the CAMELYON17 dataset and a private histopathology dataset,\ndemonstrating consistent performance gains across unseen domains. As a result,\nour approach achieved up to a 7% improvement in both the CAMELYON17 dataset and\nthe private histopathology dataset, outperforming existing baselines. These\nresults highlight the potential of causal inference as a powerful tool for\naddressing domain shift in histopathology image analysis."
    },
    {
        "date": "2025-10",
        "title": "Policy Regularized Distributionally Robust Markov Decision Processes with Linear Function Approximation",
        "author": "Jingwen Gu, Yiting He, Zhishuai Liu, and Pan Xu",
        "link": "http://arxiv.org/abs/2510.14246v1",
        "abstract": "Decision-making under distribution shift is a central challenge in\nreinforcement learning (RL), where training and deployment environments differ.\nWe study this problem through the lens of robust Markov decision processes\n(RMDPs), which optimize performance against adversarial transition dynamics.\nOur focus is the online setting, where the agent has only limited interaction\nwith the environment, making sample efficiency and exploration especially\ncritical. Policy optimization, despite its success in standard RL, remains\ntheoretically and empirically underexplored in robust RL. To bridge this gap,\nwe propose \\textbf{D}istributionally \\textbf{R}obust \\textbf{R}egularized\n\\textbf{P}olicy \\textbf{O}ptimization algorithm (DR-RPO), a model-free online\npolicy optimization method that learns robust policies with sublinear regret.\nTo enable tractable optimization within the softmax policy class, DR-RPO\nincorporates reference-policy regularization, yielding RMDP variants that are\ndoubly constrained in both transitions and policies. To scale to large\nstate-action spaces, we adopt the $d$-rectangular linear MDP formulation and\ncombine linear function approximation with an upper confidence bonus for\noptimistic exploration. We provide theoretical guarantees showing that policy\noptimization can achieve polynomial suboptimality bounds and sample efficiency\nin robust RL, matching the performance of value-based approaches. Finally,\nempirical results across diverse domains corroborate our theory and demonstrate\nthe robustness of DR-RPO."
    },
    {
        "date": "2025-10",
        "title": "Flip-Flop Consistency: Unsupervised Training for Robustness to Prompt Perturbations in LLMs",
        "author": "Parsa Hejabi, Elnaz Rahmati, Alireza S. Ziabari, and Morteza Dehghani",
        "link": "http://arxiv.org/abs/2510.14242v1",
        "abstract": "Large Language Models (LLMs) often produce inconsistent answers when faced\nwith different phrasings of the same prompt. In this paper, we propose\nFlip-Flop Consistency ($F^2C$), an unsupervised training method that improves\nrobustness to such perturbations. $F^2C$ is composed of two key components. The\nfirst, Consensus Cross-Entropy (CCE), uses a majority vote across prompt\nvariations to create a hard pseudo-label. The second is a representation\nalignment loss that pulls lower-confidence and non-majority predictors toward\nthe consensus established by high-confidence, majority-voting variations. We\nevaluate our method on 11 datasets spanning four NLP tasks, with 4-15 prompt\nvariations per dataset. On average, $F^2C$ raises observed agreement by 11.62%,\nimproves mean $F_1$ by 8.94%, and reduces performance variance across formats\nby 3.29%. In out-of-domain evaluations, $F^2C$ generalizes effectively,\nincreasing $\\overline{F_1}$ and agreement while decreasing variance across most\nsource-target pairs. Finally, when trained on only a subset of prompt\nperturbations and evaluated on held-out formats, $F^2C$ consistently improves\nboth performance and agreement while reducing variance. These findings\nhighlight $F^2C$ as an effective unsupervised method for enhancing LLM\nconsistency, performance, and generalization under prompt perturbations. Code\nis available at\nhttps://github.com/ParsaHejabi/Flip-Flop-Consistency-Unsupervised-Training-for-Robustness-to-Prompt-Perturbations-in-LLMs."
    },
    {
        "date": "2025-10",
        "title": "RoBCtrl: Attacking GNN-Based Social Bot Detectors via Reinforced Manipulation of Bots Control Interaction",
        "author": "Yingguang Yang, Xianghua Zeng, Qi Wu, Hao Peng, Yutong Xia, Hao Liu, Bin Chong, and Philip S. Yu",
        "link": "http://arxiv.org/abs/2510.16035v1",
        "abstract": "Social networks have become a crucial source of real-time information for\nindividuals. The influence of social bots within these platforms has garnered\nconsiderable attention from researchers, leading to the development of numerous\ndetection technologies. However, the vulnerability and robustness of these\ndetection methods is still underexplored. Existing Graph Neural Network\n(GNN)-based methods cannot be directly applied due to the issues of limited\ncontrol over social agents, the black-box nature of bot detectors, and the\nheterogeneity of bots. To address these challenges, this paper proposes the\nfirst adversarial multi-agent Reinforcement learning framework for social Bot\ncontrol attacks (RoBCtrl) targeting GNN-based social bot detectors.\nSpecifically, we use a diffusion model to generate high-fidelity bot accounts\nby reconstructing existing account data with minor modifications, thereby\nevading detection on social platforms. To the best of our knowledge, this is\nthe first application of diffusion models to mimic the behavior of evolving\nsocial bots effectively. We then employ a Multi-Agent Reinforcement Learning\n(MARL) method to simulate bots adversarial behavior. We categorize social\naccounts based on their influence and budget. Different agents are then\nemployed to control bot accounts across various categories, optimizing the\nattachment strategy through reinforcement learning. Additionally, a\nhierarchical state abstraction based on structural entropy is designed to\naccelerate the reinforcement learning. Extensive experiments on social bot\ndetection datasets demonstrate that our framework can effectively undermine the\nperformance of GNN-based detectors."
    },
    {
        "date": "2025-10",
        "title": "RHINO: Guided Reasoning for Mapping Network Logs to Adversarial Tactics and Techniques with Large Language Models",
        "author": "Fanchao Meng, Jiaping Gui, Yunbo Li, and Yue Wu",
        "link": "http://arxiv.org/abs/2510.14233v1",
        "abstract": "Modern Network Intrusion Detection Systems generate vast volumes of low-level\nalerts, yet these outputs remain semantically fragmented, requiring\nlabor-intensive manual correlation with high-level adversarial behaviors.\nExisting solutions for automating this mapping-rule-based systems and machine\nlearning classifiers-suffer from critical limitations: rule-based approaches\nfail to adapt to novel attack variations, while machine learning methods lack\ncontextual awareness and treat tactic-technique mapping as a syntactic matching\nproblem rather than a reasoning task. Although Large Language Models have shown\npromise in cybersecurity tasks, preliminary experiments reveal that existing\nLLM-based methods frequently hallucinate technique names or produce\ndecontextualized mappings due to their single-step classification approach.\n  To address these challenges, we introduce RHINO, a novel framework that\ndecomposes LLM-based attack analysis into three interpretable phases mirroring\nhuman reasoning: (1) behavioral abstraction, where raw logs are translated into\ncontextualized narratives; (2) multi-role collaborative inference, generating\ncandidate techniques by evaluating behavioral evidence against MITRE ATT&CK\nknowledge; and (3) validation, cross-referencing predictions with official\nMITRE definitions to rectify hallucinations. RHINO bridges the semantic gap\nbetween low-level observations and adversarial intent while improving output\nreliability through structured reasoning.\n  We evaluate RHINO on three benchmarks across four backbone models. RHINO\nachieved high accuracy, with model performance ranging from 86.38% to 88.45%,\nresulting in relative gains from 24.25% to 76.50% across different models. Our\nresults demonstrate that RHINO significantly enhances the interpretability and\nscalability of threat analysis, offering a blueprint for deploying LLMs in\noperational security settings."
    },
    {
        "date": "2025-10",
        "title": "When Flatness Does (Not) Guarantee Adversarial Robustness",
        "author": "Nils Philipp Walter, Linara Adilova, Jilles Vreeken, and Michael Kamp",
        "link": "http://arxiv.org/abs/2510.14231v1",
        "abstract": "Despite their empirical success, neural networks remain vulnerable to small,\nadversarial perturbations. A longstanding hypothesis suggests that flat minima,\nregions of low curvature in the loss landscape, offer increased robustness.\nWhile intuitive, this connection has remained largely informal and incomplete.\nBy rigorously formalizing the relationship, we show this intuition is only\npartially correct: flatness implies local but not global adversarial\nrobustness. To arrive at this result, we first derive a closed-form expression\nfor relative flatness in the penultimate layer, and then show we can use this\nto constrain the variation of the loss in input space. This allows us to\nformally analyze the adversarial robustness of the entire network. We then show\nthat to maintain robustness beyond a local neighborhood, the loss needs to\ncurve sharply away from the data manifold. We validate our theoretical\npredictions empirically across architectures and datasets, uncovering the\ngeometric structure that governs adversarial vulnerability, and linking\nflatness to model confidence: adversarial examples often lie in large, flat\nregions where the model is confidently wrong. Our results challenge simplified\nviews of flatness and provide a nuanced understanding of its role in\nrobustness."
    },
    {
        "date": "2025-10",
        "title": "Echoes of Human Malice in Agents: Benchmarking LLMs for Multi-Turn Online Harassment Attacks",
        "author": "Trilok Padhi, Pinxian Lu, Abdulkadir Erol, Tanmay Sutar, Gauri Sharma, Mina Sonmez, Munmun De Choudhury, and Ugur Kursuncu",
        "link": "http://arxiv.org/abs/2510.14207v2",
        "abstract": "Large Language Model (LLM) agents are powering a growing share of interactive\nweb applications, yet remain vulnerable to misuse and harm. Prior jailbreak\nresearch has largely focused on single-turn prompts, whereas real harassment\noften unfolds over multi-turn interactions. In this work, we present the Online\nHarassment Agentic Benchmark consisting of: (i) a synthetic multi-turn\nharassment conversation dataset, (ii) a multi-agent (e.g., harasser, victim)\nsimulation informed by repeated game theory, (iii) three jailbreak methods\nattacking agents across memory, planning, and fine-tuning, and (iv) a\nmixed-methods evaluation framework. We utilize two prominent LLMs,\nLLaMA-3.1-8B-Instruct (open-source) and Gemini-2.0-flash (closed-source). Our\nresults show that jailbreak tuning makes harassment nearly guaranteed with an\nattack success rate of 95.78--96.89% vs. 57.25--64.19% without tuning in Llama,\nand 99.33% vs. 98.46% without tuning in Gemini, while sharply reducing refusal\nrate to 1-2% in both models. The most prevalent toxic behaviors are Insult with\n84.9--87.8% vs. 44.2--50.8% without tuning, and Flaming with 81.2--85.1% vs.\n31.5--38.8% without tuning, indicating weaker guardrails compared to sensitive\ncategories such as sexual or racial harassment. Qualitative evaluation further\nreveals that attacked agents reproduce human-like aggression profiles, such as\nMachiavellian/psychopathic patterns under planning, and narcissistic tendencies\nwith memory. Counterintuitively, closed-source and open-source models exhibit\ndistinct escalation trajectories across turns, with closed-source models\nshowing significant vulnerability. Overall, our findings show that multi-turn\nand theory-grounded attacks not only succeed at high rates but also mimic\nhuman-like harassment dynamics, motivating the development of robust safety\nguardrails to ultimately keep online platforms safe and responsible."
    },
    {
        "date": "2025-10",
        "title": "Global-focal Adaptation with Information Separation for Noise-robust Transfer Fault Diagnosis",
        "author": "Junyu Ren, Wensheng Gan, Guangyu Zhang, Wei Zhong, and Philip S. Yu",
        "link": "http://arxiv.org/abs/2510.16033v1",
        "abstract": "Existing transfer fault diagnosis methods typically assume either clean data\nor sufficient domain similarity, which limits their effectiveness in industrial\nenvironments where severe noise interference and domain shifts coexist. To\naddress this challenge, we propose an information separation global-focal\nadversarial network (ISGFAN), a robust framework for cross-domain fault\ndiagnosis under noise conditions. ISGFAN is built on an information separation\narchitecture that integrates adversarial learning with an improved orthogonal\nloss to decouple domain-invariant fault representation, thereby isolating noise\ninterference and domain-specific characteristics. To further strengthen\ntransfer robustness, ISGFAN employs a global-focal domain-adversarial scheme\nthat constrains both the conditional and marginal distributions of the model.\nSpecifically, the focal domain-adversarial component mitigates\ncategory-specific transfer obstacles caused by noise in unsupervised scenarios,\nwhile the global domain classifier ensures alignment of the overall\ndistribution. Experiments conducted on three public benchmark datasets\ndemonstrate that the proposed method outperforms other prominent existing\napproaches, confirming the superiority of the ISGFAN framework. Data and code\nare available at https://github.com/JYREN-Source/ISGFAN"
    },
    {
        "date": "2025-10",
        "title": "Securing U.S. Critical Infrastructure: Lessons from Stuxnet and the Ukraine Power Grid Attacks",
        "author": "Jack Vanlyssel",
        "link": "http://arxiv.org/abs/2510.14185v1",
        "abstract": "Industrial Control Systems (ICS) underpin the United States' critical\ninfrastructure, managing essential services such as power, water, and\ntransportation that are vital to national security and public safety. However,\nincreasing digital integration has exposed these systems to escalating cyber\nthreats. Historical attacks like Stuxnet and the Ukraine power grid incident\nrevealed exploitable weaknesses-poor network segmentation, outdated software,\nweak authentication, and inadequate monitoring-that persist in many U.S. ICS\nenvironments today. This paper analyzes these landmark attacks to identify\nrecurring vulnerabilities and assess their relevance to current U.S.\ninfrastructure. It argues that without immediate reforms, similar exploits\ncould lead to catastrophic disruptions and national security crises. To address\nthese risks, the paper proposes policy measures focused on implementing\nzero-trust architecture and improved network segmentation to enhance system\nresilience. These recommendations aim to guide policymakers and industry\nleaders in securing the nation's most critical operational technologies against\nfuture cyber threats."
    },
    {
        "date": "2025-10",
        "title": "High-Dimensional BWDM: A Robust Nonparametric Clustering Validation Index for Large-Scale Data",
        "author": "Mohammed Baragilly, and Hend Gabr",
        "link": "http://arxiv.org/abs/2510.14145v1",
        "abstract": "Determining the appropriate number of clusters in unsupervised learning is a\ncentral problem in statistics and data science. Traditional validity indices\nsuch as Calinski-Harabasz, Silhouette, and Davies-Bouldin-depend on\ncentroid-based distances and therefore degrade in high-dimensional or\ncontaminated data. This paper proposes a new robust, nonparametric clustering\nvalidation framework, the High-Dimensional Between-Within Distance Median\n(HD-BWDM), which extends the recently introduced BWDM criterion to\nhigh-dimensional spaces. HD-BWDM integrates random projection and principal\ncomponent analysis to mitigate the curse of dimensionality and applies trimmed\nclustering and medoid-based distances to ensure robustness against outliers. We\nderive theoretical results showing consistency and convergence under\nJohnson-Lindenstrauss embeddings. Extensive simulations demonstrate that\nHD-BWDM remains stable and interpretable under high-dimensional projections and\ncontamination, providing a robust alternative to traditional centroid-based\nvalidation criteria. The proposed method provides a theoretically grounded,\ncomputationally efficient stopping rule for nonparametric clustering in modern\nhigh-dimensional applications."
    },
    {
        "date": "2025-10",
        "title": "Formalizing the Safety, Security, and Functional Properties of Agentic AI Systems",
        "author": "Edoardo Allegrini, Ananth Shreekumar, and Z. Berkay Celik",
        "link": "http://arxiv.org/abs/2510.14133v1",
        "abstract": "Agentic AI systems, which leverage multiple autonomous agents and Large\nLanguage Models (LLMs), are increasingly used to address complex, multi-step\ntasks. The safety, security, and functionality of these systems are critical,\nespecially in high-stakes applications. However, the current ecosystem of\ninter-agent communication is fragmented, with protocols such as the Model\nContext Protocol (MCP) for tool access and the Agent-to-Agent (A2A) protocol\nfor coordination being analyzed in isolation. This fragmentation creates a\nsemantic gap that prevents the rigorous analysis of system properties and\nintroduces risks such as architectural misalignment and exploitable\ncoordination issues. To address these challenges, we introduce a modeling\nframework for agentic AI systems composed of two foundational models. The\nfirst, the host agent model, formalizes the top-level entity that interacts\nwith the user, decomposes tasks, and orchestrates their execution by leveraging\nexternal agents and tools. The second, the task lifecycle model, details the\nstates and transitions of individual sub-tasks from creation to completion,\nproviding a fine-grained view of task management and error handling. Together,\nthese models provide a unified semantic framework for reasoning about the\nbehavior of multi-AI agent systems. Grounded in this framework, we define 17\nproperties for the host agent and 14 for the task lifecycle, categorized into\nliveness, safety, completeness, and fairness. Expressed in temporal logic,\nthese properties enable formal verification of system behavior, detection of\ncoordination edge cases, and prevention of deadlocks and security\nvulnerabilities. Through this effort, we introduce the first rigorously\ngrounded, domain-agnostic framework for the systematic analysis, design, and\ndeployment of correct, reliable, and robust agentic AI systems."
    },
    {
        "date": "2025-10",
        "title": "Neural Network-enabled Domain-consistent Robust Optimisation for Global CO$_2$ Reduction Potential of Gas Power Plants",
        "author": "Waqar Muhammad Ashraf, Talha Ansar, Abdulelah S. Alshehri, Peipei Chen, Ramit Debnath, and Vivek Dua",
        "link": "http://arxiv.org/abs/2510.14125v1",
        "abstract": "We introduce a neural network-driven robust optimisation framework that\nintegrates data-driven domain as a constraint into the nonlinear programming\ntechnique, addressing the overlooked issue of domain-inconsistent solutions\narising from the interaction of parametrised neural network models with\noptimisation solvers. Applied to a 1180 MW capacity combined cycle gas power\nplant, our framework delivers domain-consistent robust optimal solutions that\nachieve a verified 0.76 percentage point mean improvement in energy efficiency.\nFor the first time, scaling this efficiency gain to the global fleet of gas\npower plants, we estimate an annual 26 Mt reduction potential in CO$_2$ (with\n10.6 Mt in Asia, 9.0 Mt in the Americas, and 4.5 Mt in Europe). These results\nunderscore the synergetic role of machine learning in delivering near-term,\nscalable decarbonisation pathways for global climate action."
    },
    {
        "date": "2025-10",
        "title": "Quantitative Analysis of UAV Intrusion Mitigation for Border Security in 5G with LEO Backhaul Impairments",
        "author": "Rajendra Upadhyay, Al Nahian Bin Emran, Rajendra Paudyal, Lisa Donnan, and Duminda Wijesekera",
        "link": "http://arxiv.org/abs/2510.14066v1",
        "abstract": "Uncooperative unmanned aerial vehicles (UAVs) pose emerging threats to\ncritical infrastructure and border protection by operating as rogue user\nequipment (UE) within cellular networks, consuming resources, creating\ninterference, and potentially violating restricted airspaces. This paper\npresents minimal features of the operating space, yet an end-to-end simulation\nframework to analyze detect-to-mitigate latency of such intrusions in a hybrid\nterrestrial-non-terrestrial (LEO satellite) 5G system. The system model\nincludes terrestrial gNBs, satellite backhaul (with stochastic outages), and a\ndetection logic (triggered by handover instability and signal quality\nvariance). A lockdown mechanism is invoked upon detection, with optional local\nfallback to cap mitigation delays. Monte Carlo sweeps across UAV altitudes,\nspeeds, and satellite outage rates yield several insights. First, satellite\nbackhaul outages can cause arbitrarily long mitigation delays, yet, to meet\nfallback deadlines, they need to be effectively bounded. Second, while handover\ninstability was hypothesized, our results show that extra handovers have a\nnegligible effect within the range of parameters we considered. The main\nbenefit of resilience from fallback comes from the delay in limiting\nmitigation. Third, patrol UEs experience negligible collateral impact, with\nhandover rates close to terrestrial baselines. Stress scenarios further\nhighlight that fallback is indispensable in preventing extreme control-plane\nand physical security vulnerabilities: Without fallback, prolonged outages in\nthe satellite backhaul delay lockdown commands, allowing rogue UAVs to linger\ninside restricted corridors for several seconds longer. These results\nunderscore the importance of complementing non-terrestrial links with local\ncontrol to ensure robust and timely response against uncooperative UAV\nintrusions."
    },
    {
        "date": "2025-10",
        "title": "NAPPure: Adversarial Purification for Robust Image Classification under Non-Additive Perturbations",
        "author": "Junjie Nan, Jianing Li, Wei Chen, Mingkun Zhang, and Xueqi Cheng",
        "link": "http://arxiv.org/abs/2510.14025v1",
        "abstract": "Adversarial purification has achieved great success in combating adversarial\nimage perturbations, which are usually assumed to be additive. However,\nnon-additive adversarial perturbations such as blur, occlusion, and distortion\nare also common in the real world. Under such perturbations, existing\nadversarial purification methods are much less effective since they are\ndesigned to fit the additive nature. In this paper, we propose an extended\nadversarial purification framework named NAPPure, which can further handle\nnon-additive perturbations. Specifically, we first establish the generation\nprocess of an adversarial image, and then disentangle the underlying clean\nimage and perturbation parameters through likelihood maximization. Experiments\non GTSRB and CIFAR-10 datasets show that NAPPure significantly boosts the\nrobustness of image classification models against non-additive perturbations."
    },
    {
        "date": "2025-10",
        "title": "On-Chain Decentralized Learning and Cost-Effective Inference for DeFi Attack Mitigation",
        "author": "Abdulrahman Alhaidari, Balaji Palanisamy, and Prashant Krishnamurthy",
        "link": "http://arxiv.org/abs/2510.16024v1",
        "abstract": "Billions of dollars are lost every year in DeFi platforms by transactions\nexploiting business logic or accounting vulnerabilities. Existing defenses\nfocus on static code analysis, public mempool screening, attacker contract\ndetection, or trusted off-chain monitors, none of which prevents exploits\nsubmitted through private relays or malicious contracts that execute within the\nsame block. We present the first decentralized, fully on-chain learning\nframework that: (i) performs gas-prohibitive computation on Layer-2 to reduce\ncost, (ii) propagates verified model updates to Layer-1, and (iii) enables\ngas-bounded, low-latency inference inside smart contracts. A novel\nProof-of-Improvement (PoIm) protocol governs the training process and verifies\neach decentralized micro update as a self-verifying training transaction.\nUpdates are accepted by \\textit{PoIm} only if they demonstrably improve at\nleast one core metric (e.g., accuracy, F1-score, precision, or recall) on a\npublic benchmark without degrading any of the other core metrics, while\nadversarial proposals get financially penalized through an adaptable test set\nfor evolving threats. We develop quantization and loop-unrolling techniques\nthat enable inference for logistic regression, SVM, MLPs, CNNs, and gated RNNs\n(with support for formally verified decision tree inference) within the\nEthereum block gas limit, while remaining bit-exact to their off-chain\ncounterparts, formally proven in Z3. We curate 298 unique real-world exploits\n(2020 - 2025) with 402 exploit transactions across eight EVM chains,\ncollectively responsible for \\$3.74 B in losses."
    },
    {
        "date": "2025-10",
        "title": "PIShield: Detecting Prompt Injection Attacks via Intrinsic LLM Features",
        "author": "Wei Zou, Yupei Liu, Yanting Wang, Ying Chen, Neil Gong, and Jinyuan Jia",
        "link": "http://arxiv.org/abs/2510.14005v2",
        "abstract": "LLM-integrated applications are vulnerable to prompt injection attacks, where\nan attacker contaminates the input to inject malicious prompts, causing the LLM\nto follow the attacker's intent instead of the original user's. Existing prompt\ninjection detection methods often have sub-optimal performance and/or high\ncomputational overhead. In this work, we propose PIShield, a detection method\nthat is both effective and efficient. Our key observation is that the internal\nrepresentation of the final token in a prompt-extracted from a specific layer\nof the LLM, which we term the injection-critical layer-captures distinguishing\nfeatures between clean and contaminated prompts. Leveraging this insight, we\ntrain a simple linear classifier on these internal representations using a\nlabeled set of clean and contaminated prompts. We compare PIShield against 11\nbaselines across 5 diverse benchmark datasets and 8 prompt injection attacks.\nThe results demonstrate that PIShield is both highly effective and efficient,\nsubstantially outperforming existing methods. Additionally, we show that\nPIShield resists strong adaptive attacks."
    },
    {
        "date": "2025-10",
        "title": "Provably Invincible Adversarial Attacks on Reinforcement Learning Systems: A Rate-Distortion Information-Theoretic Approach",
        "author": "Ziqing Lu, Lifeng Lai, and Weiyu Xu",
        "link": "http://arxiv.org/abs/2510.13792v1",
        "abstract": "Reinforcement learning (RL) for the Markov Decision Process (MDP) has emerged\nin many security-related applications, such as autonomous driving, financial\ndecisions, and drone/robot algorithms. In order to improve the\nrobustness/defense of RL systems against adversaries, studying various\nadversarial attacks on RL systems is very important. Most previous work\nconsidered deterministic adversarial attack strategies in MDP, which the\nrecipient (victim) agent can defeat by reversing the deterministic attacks. In\nthis paper, we propose a provably ``invincible'' or ``uncounterable'' type of\nadversarial attack on RL. The attackers apply a rate-distortion\ninformation-theoretic approach to randomly change agents' observations of the\ntransition kernel (or other properties) so that the agent gains zero or very\nlimited information about the ground-truth kernel (or other properties) during\nthe training. We derive an information-theoretic lower bound on the recipient\nagent's reward regret and show the impact of rate-distortion attacks on\nstate-of-the-art model-based and model-free algorithms. We also extend this\nnotion of an information-theoretic approach to other types of adversarial\nattack, such as state observation attacks."
    },
    {
        "date": "2025-10",
        "title": "UrbanFusion: Stochastic Multimodal Fusion for Contrastive Learning of Robust Spatial Representations",
        "author": "Dominik J. M\u00fchlematter, Lin Che, Ye Hong, Martin Raubal, and Nina Wiedemann",
        "link": "http://arxiv.org/abs/2510.13774v1",
        "abstract": "Forecasting urban phenomena such as housing prices and public health\nindicators requires the effective integration of various geospatial data.\nCurrent methods primarily utilize task-specific models, while recent foundation\nmodels for spatial representations often support only limited modalities and\nlack multimodal fusion capabilities. To overcome these challenges, we present\nUrbanFusion, a Geo-Foundation Model (GeoFM) that features Stochastic Multimodal\nFusion (SMF). The framework employs modality-specific encoders to process\ndifferent types of inputs, including street view imagery, remote sensing data,\ncartographic maps, and points of interest (POIs) data. These multimodal inputs\nare integrated via a Transformer-based fusion module that learns unified\nrepresentations. An extensive evaluation across 41 tasks in 56 cities worldwide\ndemonstrates UrbanFusion's strong generalization and predictive performance\ncompared to state-of-the-art GeoAI models. Specifically, it 1) outperforms\nprior foundation models on location-encoding, 2) allows multimodal input during\ninference, and 3) generalizes well to regions unseen during training.\nUrbanFusion can flexibly utilize any subset of available modalities for a given\nlocation during both pretraining and inference, enabling broad applicability\nacross diverse data availability scenarios. All source code is available at\nhttps://github.com/DominikM198/UrbanFusion."
    },
    {
        "date": "2025-10",
        "title": "Local Information-Theoretic Security via Euclidean Geometry",
        "author": "Emmanouil M. Athanasakos, Nicholas Kalouptsidis, and Hariprasad Manjunath",
        "link": "http://arxiv.org/abs/2510.13661v1",
        "abstract": "This paper introduces a methodology based on Euclidean information theory to\ninvestigate local properties of secure communication over discrete memoryless\nwiretap channels. We formulate a constrained optimization problem that\nmaximizes a legitimate user's information rate while imposing explicit upper\nbounds on both the information leakage to an eavesdropper and the informational\ncost of encoding the secret message. By leveraging local geometric\napproximations, this inherently non-convex problem is transformed into a\ntractable quadratic programming structure. It is demonstrated that the optimal\nLagrange multipliers governing this approximated problem can be found by\nsolving a linear program. The constraints of this linear program are derived\nfrom Karush-Kuhn-Tucker conditions and are expressed in terms of the\ngeneralized eigenvalues of channel-derived matrices. This framework facilitates\nthe derivation of an analytical formula for an approximate local secrecy\ncapacity. Furthermore, we define and analyze a new class of secret local\ncontraction coefficients. These coefficients, characterized as the largest\ngeneralized eigenvalues of a matrix pencil, quantify the maximum achievable\nratio of approximate utility to approximate leakage, thus measuring the\nintrinsic local leakage efficiency of the channel. We establish bounds\nconnecting these local coefficients to their global counterparts defined over\ntrue mutual information measures. The efficacy of the proposed framework is\ndemonstrated through detailed analysis and numerical illustrations for both\ngeneral multi-mode channels and the canonical binary symmetric wiretap channel."
    },
    {
        "date": "2025-10",
        "title": "Towards Adversarial Robustness and Uncertainty Quantification in DINOv2-based Few-Shot Anomaly Detection",
        "author": "Akib Mohammed Khan, and Bartosz Krawczyk",
        "link": "http://arxiv.org/abs/2510.13643v1",
        "abstract": "Foundation models such as DINOv2 have shown strong performance in few-shot\nanomaly detection, yet two key questions remain unexamined: (i) how susceptible\nare these detectors to adversarial perturbations; and (ii) how well do their\nanomaly scores reflect calibrated uncertainty? Building on AnomalyDINO, a\ntraining-free deep nearest-neighbor detector over DINOv2 features, we present\none of the first systematic studies of adversarial attacks and uncertainty\nestimation in this setting. To enable white-box gradient attacks while\npreserving test-time behavior, we attach a lightweight linear head to frozen\nDINOv2 features only for crafting perturbations. Using this heuristic, we\nevaluate the impact of FGSM across the MVTec-AD and VisA datasets and observe\nconsistent drops in F1, AUROC, AP, and G-mean, indicating that imperceptible\nperturbations can flip nearest-neighbor relations in feature space to induce\nconfident misclassification. Complementing robustness, we probe reliability and\nfind that raw anomaly scores are poorly calibrated, revealing a gap between\nconfidence and correctness that limits safety-critical use. As a simple, strong\nbaseline toward trustworthiness, we apply post-hoc Platt scaling to the anomaly\nscores for uncertainty estimation. The resulting calibrated posteriors yield\nsignificantly higher predictive entropy on adversarially perturbed inputs than\non clean ones, enabling a practical flagging mechanism for attack detection\nwhile reducing calibration error (ECE). Our findings surface concrete\nvulnerabilities in DINOv2-based few-shot anomaly detectors and establish an\nevaluation protocol and baseline for robust, uncertainty-aware anomaly\ndetection. We argue that adversarial robustness and principled uncertainty\nquantification are not optional add-ons but essential capabilities if anomaly\ndetection systems are to be trustworthy and ready for real-world deployment."
    },
    {
        "date": "2025-10",
        "title": "LIBERO-Plus: In-depth Robustness Analysis of Vision-Language-Action Models",
        "author": "Senyu Fei, Siyin Wang, Junhao Shi, Zihao Dai, Jikun Cai, Pengfang Qian, Li Ji, Xinzhe He, Shiduo Zhang, Zhaoye Fei, Jinlan Fu, Jingjing Gong, and Xipeng Qiu",
        "link": "http://arxiv.org/abs/2510.13626v1",
        "abstract": "Visual-Language-Action (VLA) models report impressive success rates on\nrobotic manipulation benchmarks, yet these results may mask fundamental\nweaknesses in robustness. We perform a systematic vulnerability analysis by\nintroducing controlled perturbations across seven dimensions: objects layout,\ncamera viewpoints, robot initial states, language instructions, light\nconditions, background textures and sensor noise. We comprehensively analyzed\nmultiple state-of-the-art models and revealed consistent brittleness beneath\napparent competence. Our analysis exposes critical weaknesses: models exhibit\nextreme sensitivity to perturbation factors, including camera viewpoints and\nrobot initial states, with performance dropping from 95% to below 30% under\nmodest perturbations. Surprisingly, models are largely insensitive to language\nvariations, with further experiments revealing that models tend to ignore\nlanguage instructions completely. Our findings challenge the assumption that\nhigh benchmark scores equate to true competency and highlight the need for\nevaluation practices that assess reliability under realistic variation."
    },
    {
        "date": "2025-10",
        "title": "Towards Robust Knowledge Removal in Federated Learning with High Data Heterogeneity",
        "author": "Riccardo Santi, Riccardo Salami, and Simone Calderara",
        "link": "http://arxiv.org/abs/2510.13606v1",
        "abstract": "Nowdays, there are an abundance of portable devices capable of collecting\nlarge amounts of data and with decent computational power. This opened the\npossibility to train AI models in a distributed manner, preserving the\nparticipating clients' privacy. However, because of privacy regulations and\nsafety requirements, elimination upon necessity of a client contribution to the\nmodel has become mandatory. The cleansing process must satisfy specific\nefficacy and time requirements. In recent years, research efforts have produced\nseveral knowledge removal methods, but these require multiple communication\nrounds between the data holders and the process coordinator. This can cause the\nunavailability of an effective model up to the end of the removal process,\nwhich can result in a disservice to the system users. In this paper, we\nintroduce an innovative solution based on Task Arithmetic and the Neural\nTangent Kernel, to rapidly remove a client's influence from a model."
    },
    {
        "date": "2025-10",
        "title": "Selective Adversarial Attacks on LLM Benchmarks",
        "author": "Ivan Dubrovsky, Anastasia Orlova, Illarion Iov, Nina Gubina, Irena Gureeva, and Alexey Zaytsev",
        "link": "http://arxiv.org/abs/2510.13570v1",
        "abstract": "Benchmarking outcomes increasingly govern trust, selection, and deployment of\nLLMs, yet these evaluations remain vulnerable to semantically equivalent\nadversarial perturbations. Prior work on adversarial robustness in NLP has\nemphasized text attacks that affect many models equally, leaving open the\nquestion of whether it is possible to selectively degrade or enhance\nperformance while minimally affecting other models. We formalize this problem\nand study selective adversarial attacks on MMLU - a widely used benchmark\ndesigned to measure a language model's broad general knowledge and reasoning\nability across different subjects. Using canonical attacks integrated into\nTextAttack framework, we introduce a protocol for selectivity assessment,\ndevelop a custom constraint to increase selectivity of attacks and propose a\nsurrogate-LLM pipeline that generates selective perturbations. Empirically, we\nfind that selective adversarial attacks exist and can materially alter relative\nrankings, challenging the fairness, reproducibility, and transparency of\nleaderboard-driven evaluation. Our results motivate perturbation-aware\nreporting and robustness diagnostics for LLM evaluation and demonstrate that\neven subtle edits can shift comparative judgments."
    },
    {
        "date": "2025-10",
        "title": "Data-driven learning of feedback maps for explicit robust predictive control: an approximation theoretic view",
        "author": "Siddhartha Ganguly, Shubham Gupta, and Debasish Chatterjee",
        "link": "http://arxiv.org/abs/2510.13522v1",
        "abstract": "We establish an algorithm to learn feedback maps from data for a class of\nrobust model predictive control (MPC) problems. The algorithm accounts for the\napproximation errors due to the learning directly at the synthesis stage,\nensuring recursive feasibility by construction. The optimal control problem\nconsists of a linear noisy dynamical system, a quadratic stage and quadratic\nterminal costs as the objective, and convex constraints on the state, control,\nand disturbance sequences; the control minimizes and the disturbance maximizes\nthe objective. We proceed via two steps -- (a) Data generation: First, we\nreformulate the given minmax problem into a convex semi-infinite program and\nemploy recently developed tools to solve it in an exact fashion on grid points\nof the state space to generate (state, action) data. (b) Learning approximate\nfeedback maps: We employ a couple of approximation schemes that furnish tight\napproximations within preassigned uniform error bounds on the admissible state\nspace to learn the unknown feedback policy. The stability of the closed-loop\nsystem under the approximate feedback policies is also guaranteed under a\nstandard set of hypotheses. Two benchmark numerical examples are provided to\nillustrate the results."
    },
    {
        "date": "2025-10",
        "title": "Through the Lens of Doubt: Robust and Efficient Uncertainty Estimation for Visual Place Recognition",
        "author": "Emily Miller, Michael Milford, Muhammad Burhan Hafez, SD Ramchurn, and Shoaib Ehsan",
        "link": "http://arxiv.org/abs/2510.13464v1",
        "abstract": "Visual Place Recognition (VPR) enables robots and autonomous vehicles to\nidentify previously visited locations by matching current observations against\na database of known places. However, VPR systems face significant challenges\nwhen deployed across varying visual environments, lighting conditions, seasonal\nchanges, and viewpoints changes. Failure-critical VPR applications, such as\nloop closure detection in simultaneous localization and mapping (SLAM)\npipelines, require robust estimation of place matching uncertainty. We propose\nthree training-free uncertainty metrics that estimate prediction confidence by\nanalyzing inherent statistical patterns in similarity scores from any existing\nVPR method. Similarity Distribution (SD) quantifies match distinctiveness by\nmeasuring score separation between candidates; Ratio Spread (RS) evaluates\ncompetitive ambiguity among top-scoring locations; and Statistical Uncertainty\n(SU) is a combination of SD and RS that provides a unified metric that\ngeneralizes across datasets and VPR methods without requiring validation data\nto select the optimal metric. All three metrics operate without additional\nmodel training, architectural modifications, or computationally expensive\ngeometric verification. Comprehensive evaluation across nine state-of-the-art\nVPR methods and six benchmark datasets confirms that our metrics excel at\ndiscriminating between correct and incorrect VPR matches, and consistently\noutperform existing approaches while maintaining negligible computational\noverhead, making it deployable for real-time robotic applications across varied\nenvironmental conditions with improved precision-recall performance."
    },
    {
        "date": "2025-10",
        "title": "Toward Efficient Inference Attacks: Shadow Model Sharing via Mixture-of-Experts",
        "author": "Li Bai, Qingqing Ye, Xinwei Zhang, Sen Zhang, Zi Liang, Jianliang Xu, and Haibo Hu",
        "link": "http://arxiv.org/abs/2510.13451v1",
        "abstract": "Machine learning models are often vulnerable to inference attacks that expose\nsensitive information from their training data. Shadow model technique is\ncommonly employed in such attacks, such as membership inference. However, the\nneed for a large number of shadow models leads to high computational costs,\nlimiting their practical applicability. Such inefficiency mainly stems from the\nindependent training and use of these shadow models. To address this issue, we\npresent a novel shadow pool training framework SHAPOOL, which constructs\nmultiple shared models and trains them jointly within a single process. In\nparticular, we leverage the Mixture-of-Experts mechanism as the shadow pool to\ninterconnect individual models, enabling them to share some sub-networks and\nthereby improving efficiency. To ensure the shared models closely resemble\nindependent models and serve as effective substitutes, we introduce three novel\nmodules: path-choice routing, pathway regularization, and pathway alignment.\nThese modules guarantee random data allocation for pathway learning, promote\ndiversity among shared models, and maintain consistency with target models. We\nevaluate SHAPOOL in the context of various membership inference attacks and\nshow that it significantly reduces the computational cost of shadow model\nconstruction while maintaining comparable attack performance."
    },
    {
        "date": "2025-10",
        "title": "Robust Minimax Boosting with Performance Guarantees",
        "author": "Santiago Mazuelas, and Veronica Alvarez",
        "link": "http://arxiv.org/abs/2510.13445v1",
        "abstract": "Boosting methods often achieve excellent classification accuracy, but can\nexperience notable performance degradation in the presence of label noise.\nExisting robust methods for boosting provide theoretical robustness guarantees\nfor certain types of label noise, and can exhibit only moderate performance\ndegradation. However, previous theoretical results do not account for realistic\ntypes of noise and finite training sizes, and existing robust methods can\nprovide unsatisfactory accuracies, even without noise. This paper presents\nmethods for robust minimax boosting (RMBoost) that minimize worst-case error\nprobabilities and are robust to general types of label noise. In addition, we\nprovide finite-sample performance guarantees for RMBoost with respect to the\nerror obtained without noise and with respect to the best possible error (Bayes\nrisk). The experimental results corroborate that RMBoost is not only resilient\nto label noise but can also provide strong classification accuracy."
    },
    {
        "date": "2025-10",
        "title": "Assessing the robustness of heterogeneous treatment effects in survival analysis under informative censoring",
        "author": "Yuxin Wang, Dennis Frauen, Jonas Schweisthal, Maresa Schr\u00f6der, and Stefan Feuerriegel",
        "link": "http://arxiv.org/abs/2510.13397v1",
        "abstract": "Dropout is common in clinical studies, with up to half of patients leaving\nearly due to side effects or other reasons. When dropout is informative (i.e.,\ndependent on survival time), it introduces censoring bias, because of which\ntreatment effect estimates are also biased. In this paper, we propose an\nassumption-lean framework to assess the robustness of conditional average\ntreatment effect (CATE) estimates in survival analysis when facing censoring\nbias. Unlike existing works that rely on strong assumptions, such as\nnon-informative censoring, to obtain point estimation, we use partial\nidentification to derive informative bounds on the CATE. Thereby, our framework\nhelps to identify patient subgroups where treatment is effective despite\ninformative censoring. We further develop a novel meta-learner that estimates\nthe bounds using arbitrary machine learning models and with favorable\ntheoretical properties, including double robustness and quasi-oracle\nefficiency. We demonstrate the practical value of our meta-learner through\nnumerical experiments and in an application to a cancer drug trial. Together,\nour framework offers a practical tool for assessing the robustness of estimated\ntreatment effects in the presence of censoring and thus promotes the reliable\nuse of survival data for evidence generation in medicine and epidemiology."
    },
    {
        "date": "2025-10",
        "title": "Generalist++: A Meta-learning Framework for Mitigating Trade-off in Adversarial Training",
        "author": "Yisen Wang, Yichuan Mo, Hongjun Wang, Junyi Li, and Zhouchen Lin",
        "link": "http://arxiv.org/abs/2510.13361v1",
        "abstract": "Despite the rapid progress of neural networks, they remain highly vulnerable\nto adversarial examples, for which adversarial training (AT) is currently the\nmost effective defense. While AT has been extensively studied, its practical\napplications expose two major limitations: natural accuracy tends to degrade\nsignificantly compared with standard training, and robustness does not transfer\nwell across attacks crafted under different norm constraints. Unlike prior\nworks that attempt to address only one issue within a single network, we\npropose to partition the overall generalization goal into multiple sub-tasks,\neach assigned to a dedicated base learner. By specializing in its designated\nobjective, each base learner quickly becomes an expert in its field. In the\nlater stages of training, we interpolate their parameters to form a\nknowledgeable global learner, while periodically redistributing the global\nparameters back to the base learners to prevent their optimization trajectories\nfrom drifting too far from the shared target. We term this framework Generalist\nand introduce three variants tailored to different application scenarios. Both\ntheoretical analysis and extensive experiments demonstrate that Generalist\nachieves lower generalization error and significantly alleviates the trade-off\nproblems compared with baseline methods. Our results suggest that Generalist\nprovides a promising step toward developing fully robust classifiers in the\nfuture."
    },
    {
        "date": "2025-10",
        "title": "Adversarial Fine-tuning in Offline-to-Online Reinforcement Learning for Robust Robot Control",
        "author": "Shingo Ayabe, Hiroshi Kera, and Kazuhiko Kawamoto",
        "link": "http://arxiv.org/abs/2510.13358v1",
        "abstract": "Offline reinforcement learning enables sample-efficient policy acquisition\nwithout risky online interaction, yet policies trained on static datasets\nremain brittle under action-space perturbations such as actuator faults. This\nstudy introduces an offline-to-online framework that trains policies on clean\ndata and then performs adversarial fine-tuning, where perturbations are\ninjected into executed actions to induce compensatory behavior and improve\nresilience. A performance-aware curriculum further adjusts the perturbation\nprobability during training via an exponential-moving-average signal, balancing\nrobustness and stability throughout the learning process. Experiments on\ncontinuous-control locomotion tasks demonstrate that the proposed method\nconsistently improves robustness over offline-only baselines and converges\nfaster than training from scratch. Matching the fine-tuning and evaluation\nconditions yields the strongest robustness to action-space perturbations, while\nthe adaptive curriculum strategy mitigates the degradation of nominal\nperformance observed with the linear curriculum strategy. Overall, the results\nshow that adversarial fine-tuning enables adaptive and robust control under\nuncertain environments, bridging the gap between offline efficiency and online\nadaptability."
    },
    {
        "date": "2025-10",
        "title": "Protect: Towards Robust Guardrailing Stack for Trustworthy Enterprise LLM Systems",
        "author": "Karthik Avinash, Nikhil Pareek, and Rishav Hada",
        "link": "http://arxiv.org/abs/2510.13351v1",
        "abstract": "The increasing deployment of Large Language Models (LLMs) across enterprise\nand mission-critical domains has underscored the urgent need for robust\nguardrailing systems that ensure safety, reliability, and compliance. Existing\nsolutions often struggle with real-time oversight, multi-modal data handling,\nand explainability -- limitations that hinder their adoption in regulated\nenvironments. Existing guardrails largely operate in isolation, focused on text\nalone making them inadequate for multi-modal, production-scale environments. We\nintroduce Protect, natively multi-modal guardrailing model designed to operate\nseamlessly across text, image, and audio inputs, designed for enterprise-grade\ndeployment. Protect integrates fine-tuned, category-specific adapters trained\nvia Low-Rank Adaptation (LoRA) on an extensive, multi-modal dataset covering\nfour safety dimensions: toxicity, sexism, data privacy, and prompt injection.\nOur teacher-assisted annotation pipeline leverages reasoning and explanation\ntraces to generate high-fidelity, context-aware labels across modalities.\nExperimental results demonstrate state-of-the-art performance across all safety\ndimensions, surpassing existing open and proprietary models such as WildGuard,\nLlamaGuard-4, and GPT-4.1. Protect establishes a strong foundation for\ntrustworthy, auditable, and production-ready safety systems capable of\noperating across text, image, and audio modalities."
    },
    {
        "date": "2025-10",
        "title": "Injection, Attack and Erasure: Revocable Backdoor Attacks via Machine Unlearning",
        "author": "Baogang Song, Dongdong Zhao, Jianwen Xiang, Qiben Xu, and Zizhuo Yu",
        "link": "http://arxiv.org/abs/2510.13322v1",
        "abstract": "Backdoor attacks pose a persistent security risk to deep neural networks\n(DNNs) due to their stealth and durability. While recent research has explored\nleveraging model unlearning mechanisms to enhance backdoor concealment,\nexisting attack strategies still leave persistent traces that may be detected\nthrough static analysis. In this work, we introduce the first paradigm of\nrevocable backdoor attacks, where the backdoor can be proactively and\nthoroughly removed after the attack objective is achieved. We formulate the\ntrigger optimization in revocable backdoor attacks as a bilevel optimization\nproblem: by simulating both backdoor injection and unlearning processes, the\ntrigger generator is optimized to achieve a high attack success rate (ASR)\nwhile ensuring that the backdoor can be easily erased through unlearning. To\nmitigate the optimization conflict between injection and removal objectives, we\nemploy a deterministic partition of poisoning and unlearning samples to reduce\nsampling-induced variance, and further apply the Projected Conflicting Gradient\n(PCGrad) technique to resolve the remaining gradient conflicts. Experiments on\nCIFAR-10 and ImageNet demonstrate that our method maintains ASR comparable to\nstate-of-the-art backdoor attacks, while enabling effective removal of backdoor\nbehavior after unlearning. This work opens a new direction for backdoor attack\nresearch and presents new challenges for the security of machine learning\nsystems."
    },
    {
        "date": "2025-10",
        "title": "SAJA: A State-Action Joint Attack Framework on Multi-Agent Deep Reinforcement Learning",
        "author": "Weiqi Guo, Guanjun Liu, and Ziyuan Zhou",
        "link": "http://arxiv.org/abs/2510.13262v1",
        "abstract": "Multi-Agent Deep Reinforcement Learning (MADRL) has shown potential for\ncooperative and competitive tasks such as autonomous driving and strategic\ngaming. However, models trained by MADRL are vulnerable to adversarial\nperturbations on states and actions. Therefore, it is essential to investigate\nthe robustness of MADRL models from an attack perspective. Existing studies\nfocus on either state-only attacks or action-only attacks, but do not consider\nhow to effectively joint them. Simply combining state and action perturbations\nsuch as randomly perturbing states and actions does not exploit their potential\nsynergistic effects. In this paper, we propose the State-Action Joint Attack\n(SAJA) framework that has a good synergistic effects. SAJA consists of two\nimportant phases: (1) In the state attack phase, a multi-step gradient ascent\nmethod utilizes both the actor network and the critic network to compute an\nadversarial state, and (2) in the action attack phase, based on the perturbed\nstate, a second gradient ascent uses the critic network to craft the final\nadversarial action. Additionally, a heuristic regularizer measuring the\ndistance between the perturbed actions and the original clean ones is added\ninto the loss function to enhance the effectiveness of the critic's guidance.\nWe evaluate SAJA in the Multi-Agent Particle Environment (MPE), demonstrating\nthat (1) it outperforms and is more stealthy than state-only or action-only\nattacks, and (2) existing state or action defense methods cannot defend its\nattacks."
    },
    {
        "date": "2025-10",
        "title": "Model-agnostic Adversarial Attack and Defense for Vision-Language-Action Models",
        "author": "Haochuan Xu, Yun Sing Koh, Shuhuai Huang, Zirun Zhou, Di Wang, Jun Sakuma, and Jingfeng Zhang",
        "link": "http://arxiv.org/abs/2510.13237v1",
        "abstract": "Vision-Language-Action (VLA) models have achieved revolutionary progress in\nrobot learning, enabling robots to execute complex physical robot tasks from\nnatural language instructions. Despite this progress, their adversarial\nrobustness remains underexplored. In this work, we propose both adversarial\npatch attack and corresponding defense strategies for VLA models. We first\nintroduce the Embedding Disruption Patch Attack (EDPA), a model-agnostic\nadversarial attack that generates patches directly placeable within the\ncamera's view. In comparison to prior methods, EDPA can be readily applied to\ndifferent VLA models without requiring prior knowledge of the model\narchitecture, or the controlled robotic manipulator. EDPA constructs these\npatches by (i) disrupting the semantic alignment between visual and textual\nlatent representations, and (ii) maximizing the discrepancy of latent\nrepresentations between adversarial and corresponding clean visual inputs.\nThrough the optimization of these objectives, EDPA distorts the VLA's\ninterpretation of visual information, causing the model to repeatedly generate\nincorrect actions and ultimately result in failure to complete the given\nrobotic task. To counter this, we propose an adversarial fine-tuning scheme for\nthe visual encoder, in which the encoder is optimized to produce similar latent\nrepresentations for both clean and adversarially perturbed visual inputs.\nExtensive evaluations on the widely recognized LIBERO robotic simulation\nbenchmark demonstrate that EDPA substantially increases the task failure rate\nof cutting-edge VLA models, while our proposed defense effectively mitigates\nthis degradation. The codebase is accessible via the homepage at\nhttps://edpa-attack.github.io/."
    },
    {
        "date": "2025-10",
        "title": "Searching for a Farang: Collective Security among Women in Pattaya, Thailand",
        "author": "Taylor Robinson, and Rikke Bjerg Jensen",
        "link": "http://arxiv.org/abs/2510.13162v1",
        "abstract": "We report on two months of ethnographic fieldwork in a women's centre in\nPattaya, and interviews with 76 participants. Our findings, as they relate to\ndigital security, show how (i) women in Pattaya, often working in the sex and\nmassage industries, perceived relationships with farang men as their best, and\nsometimes only, option to achieve security; (ii) the strategies used by the\nwomen to appeal to a farang involved presenting themselves online, mirroring\nhow they were being advertised by bar owners to attract customers; (iii)\nappealing to what they considered `Western ideals', the women sought out\n`Western technologies' and appropriated them for their benefit; (iv) the women\nnavigated a series of online security risks, such as scams and abuse, which\nshaped their search for a farang; (v) the women developed collective security\nthrough knowledge-sharing to protect themselves and each other in their search\nfor a farang. We situate our work in emerging digital security scholarship\nwithin marginalised contexts."
    },
    {
        "date": "2025-10",
        "title": "Privacy-Aware Framework of Robust Malware Detection in Indoor Robots: Hybrid Quantum Computing and Deep Neural Networks",
        "author": "Tan Le, Van Le, and Sachin Shetty",
        "link": "http://arxiv.org/abs/2510.13136v1",
        "abstract": "Indoor robotic systems within Cyber-Physical Systems (CPS) are increasingly\nexposed to Denial of Service (DoS) attacks that compromise localization,\ncontrol and telemetry integrity. We propose a privacy-aware malware detection\nframework for indoor robotic systems, which leverages hybrid quantum computing\nand deep neural networks to counter DoS threats in CPS, while preserving\nprivacy information. By integrating quantum-enhanced feature encoding with\ndropout-optimized deep learning, our architecture achieves up to 95.2%\ndetection accuracy under privacy-constrained conditions. The system operates\nwithout handcrafted thresholds or persistent beacon data, enabling scalable\ndeployment in adversarial environments. Benchmarking reveals robust\ngeneralization, interpretability and resilience against training instability\nthrough modular circuit design. This work advances trustworthy AI for secure,\nautonomous CPS operations."
    },
    {
        "date": "2025-10",
        "title": "ShuffleV: A Microarchitectural Defense Strategy against Electromagnetic Side-Channel Attacks in Microprocessors",
        "author": "Nuntipat Narkthong, Yukui Luo, and Xiaolin Xu",
        "link": "http://arxiv.org/abs/2510.13111v1",
        "abstract": "The run-time electromagnetic (EM) emanation of microprocessors presents a\nside-channel that leaks the confidentiality of the applications running on\nthem. Many recent works have demonstrated successful attacks leveraging such\nside-channels to extract the confidentiality of diverse applications, such as\nthe key of cryptographic algorithms and the hyperparameter of neural network\nmodels. This paper proposes ShuffleV, a microarchitecture defense strategy\nagainst EM Side-Channel Attacks (SCAs). ShuffleV adopts the moving target\ndefense (MTD) philosophy, by integrating hardware units to randomly shuffle the\nexecution order of program instructions and optionally insert dummy\ninstructions, to nullify the statistical observation by attackers across\nrepetitive runs. We build ShuffleV on the open-source RISC-V core and provide\nsix design options, to suit different application scenarios. To enable rapid\nevaluation, we develop a ShuffleV simulator that can help users to (1) simulate\nthe performance overhead for each design option and (2) generate an execution\ntrace to validate the randomness of execution on their workload. We implement\nShuffleV on a Xilinx PYNQ-Z2 FPGA and validate its performance with two\nrepresentative victim applications against EM SCAs, AES encryption, and neural\nnetwork inference. The experimental results demonstrate that ShuffleV can\nprovide automatic protection for these applications, without any user\nintervention or software modification."
    },
    {
        "date": "2025-10",
        "title": "CSI-4CAST: A Hybrid Deep Learning Model for CSI Prediction with Comprehensive Robustness and Generalization Testing",
        "author": "Sikai Cheng, Reza Zandehshahvar, Haoruo Zhao, Daniel A. Garcia-Ulloa, Alejandro Villena-Rodriguez, Carles Navarro Manch\u00f3n, and Pascal Van Hentenryck",
        "link": "http://arxiv.org/abs/2510.12996v1",
        "abstract": "Channel state information (CSI) prediction is a promising strategy for\nensuring reliable and efficient operation of massive multiple-input\nmultiple-output (mMIMO) systems by providing timely downlink (DL) CSI. While\ndeep learning-based methods have advanced beyond conventional model-driven and\nstatistical approaches, they remain limited in robustness to practical\nnon-Gaussian noise, generalization across diverse channel conditions, and\ncomputational efficiency. This paper introduces CSI-4CAST, a hybrid deep\nlearning architecture that integrates 4 key components, i.e., Convolutional\nneural network residuals, Adaptive correction layers, ShuffleNet blocks, and\nTransformers, to efficiently capture both local and long-range dependencies in\nCSI prediction. To enable rigorous evaluation, this work further presents a\ncomprehensive benchmark, CSI-RRG for Regular, Robustness and Generalization\ntesting, which includes more than 300,000 samples across 3,060 realistic\nscenarios for both TDD and FDD systems. The dataset spans multiple channel\nmodels, a wide range of delay spreads and user velocities, and diverse noise\ntypes and intensity degrees. Experimental results show that CSI-4CAST achieves\nsuperior prediction accuracy with substantially lower computational cost,\noutperforming baselines in 88.9% of TDD scenarios and 43.8% of FDD scenario,\nthe best performance among all evaluated models, while reducing FLOPs by 5x and\n3x compared to LLM4CP, the strongest baseline. In addition, evaluation over\nCSI-RRG provides valuable insights into how different channel factors affect\nthe performance and generalization capability of deep learning models. Both the\ndataset (https://huggingface.co/CSI-4CAST) and evaluation protocols\n(https://github.com/AI4OPT/CSI-4CAST) are publicly released to establish a\nstandardized benchmark and to encourage further research on robust and\nefficient CSI prediction."
    },
    {
        "date": "2025-10",
        "title": "Pruning Cannot Hurt Robustness: Certified Trade-offs in Reinforcement Learning",
        "author": "James Pedley, Benjamin Etheridge, Stephen J. Roberts, and Francesco Quinzan",
        "link": "http://arxiv.org/abs/2510.12939v1",
        "abstract": "Reinforcement learning (RL) policies deployed in real-world environments must\nremain reliable under adversarial perturbations. At the same time, modern deep\nRL agents are heavily over-parameterized, raising costs and fragility concerns.\nWhile pruning has been shown to improve robustness in supervised learning, its\nrole in adversarial RL remains poorly understood. We develop the first\ntheoretical framework for certified robustness under pruning in\nstate-adversarial Markov decision processes (SA-MDPs). For Gaussian and\ncategorical policies with Lipschitz networks, we prove that element-wise\npruning can only tighten certified robustness bounds; pruning never makes the\npolicy less robust. Building on this, we derive a novel three-term regret\ndecomposition that disentangles clean-task performance, pruning-induced\nperformance loss, and robustness gains, exposing a fundamental\nperformance--robustness frontier. Empirically, we evaluate magnitude and\nmicro-pruning schedules on continuous-control benchmarks with strong\npolicy-aware adversaries. Across tasks, pruning consistently uncovers\nreproducible ``sweet spots'' at moderate sparsity levels, where robustness\nimproves substantially without harming - and sometimes even enhancing - clean\nperformance. These results position pruning not merely as a compression tool\nbut as a structural intervention for robust RL."
    },
    {
        "date": "2025-10",
        "title": "Who's Asking? Evaluating LLM Robustness to Inquiry Personas in Factual Question Answering",
        "author": "Nil-Jana Akpinar, Chia-Jung Lee, Vanessa Murdock, and Pietro Perona",
        "link": "http://arxiv.org/abs/2510.12925v1",
        "abstract": "Large Language Models (LLMs) should answer factual questions truthfully,\ngrounded in objective knowledge, regardless of user context such as\nself-disclosed personal information, or system personalization. In this paper,\nwe present the first systematic evaluation of LLM robustness to inquiry\npersonas, i.e. user profiles that convey attributes like identity, expertise,\nor belief. While prior work has primarily focused on adversarial inputs or\ndistractors for robustness testing, we evaluate plausible, human-centered\ninquiry persona cues that users disclose in real-world interactions. We find\nthat such cues can meaningfully alter QA accuracy and trigger failure modes\nsuch as refusals, hallucinated limitations, and role confusion. These effects\nhighlight how model sensitivity to user framing can compromise factual\nreliability, and position inquiry persona testing as an effective tool for\nrobustness evaluation."
    },
    {
        "date": "2025-10",
        "title": "Robust Plant Disease Diagnosis with Few Target-Domain Samples",
        "author": "Takafumi Nogami, Satoshi Kagiwada, and Hitoshi Iyatomi",
        "link": "http://arxiv.org/abs/2510.12909v1",
        "abstract": "Various deep learning-based systems have been proposed for accurate and\nconvenient plant disease diagnosis, achieving impressive performance. However,\nrecent studies show that these systems often fail to maintain diagnostic\naccuracy on images captured under different conditions from the training\nenvironment -- an essential criterion for model robustness. Many deep learning\nmethods have shown high accuracy in plant disease diagnosis. However, they\noften struggle to generalize to images taken in conditions that differ from the\ntraining setting. This drop in performance stems from the subtle variability of\ndisease symptoms and domain gaps -- differences in image context and\nenvironment. The root cause is the limited diversity of training data relative\nto task complexity, making even advanced models vulnerable in unseen domains.\nTo tackle this challenge, we propose a simple yet highly adaptable learning\nframework called Target-Aware Metric Learning with Prioritized Sampling (TMPS),\ngrounded in metric learning. TMPS operates under the assumption of access to a\nlimited number of labeled samples from the target (deployment) domain and\nleverages these samples effectively to improve diagnostic robustness. We assess\nTMPS on a large-scale automated plant disease diagnostic task using a dataset\ncomprising 223,073 leaf images sourced from 23 agricultural fields, spanning 21\ndiseases and healthy instances across three crop species. By incorporating just\n10 target domain samples per disease into training, TMPS surpasses models\ntrained using the same combined source and target samples, and those fine-tuned\nwith these target samples after pre-training on source data. It achieves\naverage macro F1 score improvements of 7.3 and 3.6 points, respectively, and a\nremarkable 18.7 and 17.1 point improvement over the baseline and conventional\nmetric learning."
    },
    {
        "date": "2025-10",
        "title": "KoALA: KL-L0 Adversarial Detector via Label Agreement",
        "author": "Siqi Li, and Yasser Shoukry",
        "link": "http://arxiv.org/abs/2510.12752v1",
        "abstract": "Deep neural networks are highly susceptible to adversarial attacks, which\npose significant risks to security- and safety-critical applications. We\npresent KoALA (KL-L0 Adversarial detection via Label Agreement), a novel,\nsemantics-free adversarial detector that requires no architectural changes or\nadversarial retraining. KoALA operates on a simple principle: it detects an\nadversarial attack when class predictions from two complementary similarity\nmetrics disagree. These metrics-KL divergence and an L0-based similarity-are\nspecifically chosen to detect different types of perturbations. The KL\ndivergence metric is sensitive to dense, low-amplitude shifts, while the\nL0-based similarity is designed for sparse, high-impact changes. We provide a\nformal proof of correctness for our approach. The only training required is a\nsimple fine-tuning step on a pre-trained image encoder using clean images to\nensure the embeddings align well with both metrics. This makes KOALA a\nlightweight, plug-and-play solution for existing models and various data\nmodalities. Our extensive experiments on ResNet/CIFAR-10 and CLIP/Tiny-ImageNet\nconfirm our theoretical claims. When the theorem's conditions are met, KoALA\nconsistently and effectively detects adversarial examples. On the full test\nsets, KoALA achieves a precision of 0.94 and a recall of 0.81 on\nResNet/CIFAR-10, and a precision of 0.66 and a recall of 0.85 on\nCLIP/Tiny-ImageNet."
    },
    {
        "date": "2025-10",
        "title": "Towards Robust Artificial Intelligence: Self-Supervised Learning Approach for Out-of-Distribution Detection",
        "author": "Wissam Salhab, Darine Ameyed, Hamid Mcheick, and Fehmi Jaafar",
        "link": "http://arxiv.org/abs/2510.12713v1",
        "abstract": "Robustness in AI systems refers to their ability to maintain reliable and\naccurate performance under various conditions, including out-of-distribution\n(OOD) samples, adversarial attacks, and environmental changes. This is crucial\nin safety-critical systems, such as autonomous vehicles, transportation, or\nhealthcare, where malfunctions could have severe consequences. This paper\nproposes an approach to improve OOD detection without the need of labeled data,\nthereby increasing the AI systems' robustness. The proposed approach leverages\nthe principles of self-supervised learning, allowing the model to learn useful\nrepresentations from unlabeled data. Combined with graph-theoretical\ntechniques, this enables the more efficient identification and categorization\nof OOD samples. Compared to existing state-of-the-art methods, this approach\nachieved an Area Under the Receiver Operating Characteristic Curve (AUROC) =\n0.99."
    },
    {
        "date": "2025-10",
        "title": "Hash chaining degrades security at Facebook",
        "author": "Thomas Rivasseau",
        "link": "http://arxiv.org/abs/2510.12665v1",
        "abstract": "Modern web and digital application password storage relies on password\nhashing for storage and security. Ad-hoc upgrade of password storage to keep up\nwith hash algorithm norms may be used to save costs but can introduce\nunforeseen vulnerabilities. This is the case in the password storage scheme\nused by Meta Platforms which services several billion monthly users worldwide.\nIn this paper we present the first example of an exploit which demonstrates the\nsecurity weakness of Facebook's password storage scheme, and discuss its\nimplications. Proper ethical disclosure guidelines and vendor notification were\nfollowed."
    },
    {
        "date": "2025-10",
        "title": "Noisy Neighbor: Exploiting RDMA for Resource Exhaustion Attacks in Containerized Clouds",
        "author": "Gunwoo Kim, Taejune Park, and Jinwoo Kim",
        "link": "http://arxiv.org/abs/2510.12629v1",
        "abstract": "In modern containerized cloud environments, the adoption of RDMA (Remote\nDirect Memory Access) has expanded to reduce CPU overhead and enable\nhigh-performance data exchange. Achieving this requires strong performance\nisolation to ensure that one container's RDMA workload does not degrade the\nperformance of others, thereby maintaining critical security assurances.\nHowever, existing isolation techniques are difficult to apply effectively due\nto the complexity of microarchitectural resource management within RDMA NICs\n(RNICs). This paper experimentally analyzes two types of resource exhaustion\nattacks on NVIDIA BlueField-3: (i) state saturation attacks and (ii) pipeline\nsaturation attacks. Our results show that state saturation attacks can cause up\nto a 93.9% loss in bandwidth, a 1,117x increase in latency, and a 115% rise in\ncache misses for victim containers, while pipeline saturation attacks lead to\nsevere link-level congestion and significant amplification, where small verb\nrequests result in disproportionately high resource consumption. To mitigate\nthese threats and restore predictable security assurances, we propose HT-Verbs,\na threshold-driven framework based on real-time per-container RDMA verb\ntelemetry and adaptive resource classification that partitions RNIC resources\ninto hot, warm, and cold tiers and throttles abusive workloads without\nrequiring hardware modifications."
    },
    {
        "date": "2025-10",
        "title": "Multi-Copy Security in Unclonable Cryptography",
        "author": "Alper \u00c7akan, Vipul Goyal, Fuyuki Kitagawa, Ryo Nishimaki, and Takashi Yamakawa",
        "link": "http://arxiv.org/abs/2510.12626v1",
        "abstract": "Unclonable cryptography leverages the quantum no-cloning principle to\ncopy-protect cryptographic functionalities. While most existing works address\nthe basic single-copy security, the stronger notion of multi-copy security\nremains largely unexplored.\n  We introduce a generic compiler that upgrades collusion-resistant unclonable\nprimitives to achieve multi-copy security, assuming only one-way functions.\nUsing this framework, we obtain the first multi-copy secure constructions of\npublic-key quantum money (termed quantum coins), single-decryptor encryption,\nunclonable encryption, and more. We also introduce an extended notion of\nquantum coins, called upgradable quantum coins, which allow weak\n(almost-public) verification under weaker assumptions and can be upgraded to\nfull public verification under stronger assumptions by the bank simply\npublishing additional classical information.\n  Along the way, we give a generic compiler that upgrades single-copy secure\nsingle-decryptor encryption to a collusion-resistant one, assuming the\nexistence of functional encryption, and construct the first multi-challenge\nsecure unclonable encryption scheme, which we believe are of independent\ninterest."
    },
    {
        "date": "2025-10",
        "title": "StyleDecipher: Robust and Explainable Detection of LLM-Generated Texts with Stylistic Analysis",
        "author": "Siyuan Li, Aodu Wulianghai, Xi Lin, Guangyan Li, Xiang Chen, Jun Wu, and Jianhua Li",
        "link": "http://arxiv.org/abs/2510.12608v1",
        "abstract": "With the increasing integration of large language models (LLMs) into\nopen-domain writing, detecting machine-generated text has become a critical\ntask for ensuring content authenticity and trust. Existing approaches rely on\nstatistical discrepancies or model-specific heuristics to distinguish between\nLLM-generated and human-written text. However, these methods struggle in\nreal-world scenarios due to limited generalization, vulnerability to\nparaphrasing, and lack of explainability, particularly when facing stylistic\ndiversity or hybrid human-AI authorship. In this work, we propose\nStyleDecipher, a robust and explainable detection framework that revisits\nLLM-generated text detection using combined feature extractors to quantify\nstylistic differences. By jointly modeling discrete stylistic indicators and\ncontinuous stylistic representations derived from semantic embeddings,\nStyleDecipher captures distinctive style-level divergences between human and\nLLM outputs within a unified representation space. This framework enables\naccurate, explainable, and domain-agnostic detection without requiring access\nto model internals or labeled segments. Extensive experiments across five\ndiverse domains, including news, code, essays, reviews, and academic abstracts,\ndemonstrate that StyleDecipher consistently achieves state-of-the-art in-domain\naccuracy. Moreover, in cross-domain evaluations, it surpasses existing\nbaselines by up to 36.30%, while maintaining robustness against adversarial\nperturbations and mixed human-AI content. Further qualitative and quantitative\nanalysis confirms that stylistic signals provide explainable evidence for\ndistinguishing machine-generated text. Our source code can be accessed at\nhttps://github.com/SiyuanLi00/StyleDecipher."
    },
    {
        "date": "2025-10",
        "title": "Breaking Guardrails, Facing Walls: Insights on Adversarial AI for Defenders & Researchers",
        "author": "Giacomo Bertollo, Naz Bodemir, and Jonah Burgess",
        "link": "http://arxiv.org/abs/2510.16005v1",
        "abstract": "Analyzing 500 CTF participants, this paper shows that while participants\nreadily bypassed simple AI guardrails using common techniques, layered\nmulti-step defenses still posed significant challenges, offering concrete\ninsights for building safer AI systems."
    },
    {
        "date": "2025-10",
        "title": "The Robustness of Differentiable Causal Discovery in Misspecified Scenarios",
        "author": "Huiyang Yi, Yanyan He, Duxin Chen, Mingyu Kang, He Wang, and Wenwu Yu",
        "link": "http://arxiv.org/abs/2510.12503v1",
        "abstract": "Causal discovery aims to learn causal relationships between variables from\ntargeted data, making it a fundamental task in machine learning. However,\ncausal discovery algorithms often rely on unverifiable causal assumptions,\nwhich are usually difficult to satisfy in real-world data, thereby limiting the\nbroad application of causal discovery in practical scenarios. Inspired by these\nconsiderations, this work extensively benchmarks the empirical performance of\nvarious mainstream causal discovery algorithms, which assume i.i.d. data, under\neight model assumption violations. Our experimental results show that\ndifferentiable causal discovery methods exhibit robustness under the metrics of\nStructural Hamming Distance and Structural Intervention Distance of the\ninferred graphs in commonly used challenging scenarios, except for scale\nvariation. We also provide the theoretical explanations for the performance of\ndifferentiable causal discovery methods. Finally, our work aims to\ncomprehensively benchmark the performance of recent differentiable causal\ndiscovery methods under model assumption violations, and provide the standard\nfor reasonable evaluation of causal discovery, as well as to further promote\nits application in real-world scenarios."
    }
]