[
    {
        "date": "2025-02",
        "title": "A Hybrid Blockchain-IPFS Solution for Secure and Scalable Data Collection and Storage for Smart Water Meters",
        "author": "Thandile Nododile, and Clement Nyirenda",
        "link": "http://arxiv.org/abs/2502.03427v1",
        "abstract": "Scalable and secure data management is important in Internet of Things (IoT)\napplications such as smart water meters, where traditional blockchain storage\ncan be restrictive due to high data volumes. This paper investigates a hybrid\nblockchain and InterPlanetary File System (IPFS) approach designed to optimise\nstorage efficiency, enhance throughput, and reduce block time by offloading\nlarge data off-chain to IPFS while preserving on-chain integrity. A\nsubstrate-based private blockchain was developed to store smart water meter\n(SWM) data, and controlled experiments were conducted to evaluate blockchain\nperformance with and without IPFS. Key metrics, including block size, block\ntime, and transaction throughput, were analysed across varying data volumes and\nnode counts. Results show that integrating IPFS significantly reduces on-chain\nstorage demands, leading to smaller block sizes, increased throughput, and\nimproved block times compared to blockchain-only storage. These findings\nhighlight the potential of hybrid blockchain-IPFS models for efficiently and\nsecurely managing high-volume IoT data."
    },
    {
        "date": "2025-02",
        "title": "The Adoption of Artificial Intelligence in Different Network Security Concepts",
        "author": "Mamoon A. Al Jbaar, Adel Jalal Yousif, and Qutaiba I. Ali",
        "link": "http://arxiv.org/abs/2502.03398v1",
        "abstract": "The obstacles of each security system combined with the increase of\ncyber-attacks, negatively affect the effectiveness of network security\nmanagement and rise the activities to be taken by the security staff and\nnetwork administrators. So, there is a growing need for the automated auditing\nand intelligent reporting strategies for reliable network security with as less\nmodel complexity as possible. Newly, artificial intelligence has been\neffectively applied to various network security issues, and numerous studies\nhave been conducted that utilize various artificial intelligence techniques for\nthe purposes of encryption and secure communication, in addition to using\nartificial intelligence to perform a large number of data encryption operations\nin record time. The aim of the study is to present and discuss the most\nprominent methods of artificial intelligence recently used in the field of\nnetwork security including user authentication, Key exchanging,\nencryption/decryption, data integrity and intrusion detection system."
    },
    {
        "date": "2025-02",
        "title": "Robust Autonomy Emerges from Self-Play",
        "author": "Marco Cusumano-Towner, David Hafner, Alex Hertzberg, Brody Huval, Aleksei Petrenko, Eugene Vinitsky, Erik Wijmans, Taylor Killian, Stuart Bowers, Ozan Sener, Philipp Kr\u00e4henb\u00fchl, and Vladlen Koltun",
        "link": "http://arxiv.org/abs/2502.03349v1",
        "abstract": "Self-play has powered breakthroughs in two-player and multi-player games.\nHere we show that self-play is a surprisingly effective strategy in another\ndomain. We show that robust and naturalistic driving emerges entirely from\nself-play in simulation at unprecedented scale -- 1.6~billion~km of driving.\nThis is enabled by Gigaflow, a batched simulator that can synthesize and train\non 42 years of subjective driving experience per hour on a single 8-GPU node.\nThe resulting policy achieves state-of-the-art performance on three independent\nautonomous driving benchmarks. The policy outperforms the prior state of the\nart when tested on recorded real-world scenarios, amidst human drivers, without\never seeing human data during training. The policy is realistic when assessed\nagainst human references and achieves unprecedented robustness, averaging 17.5\nyears of continuous driving between incidents in simulation."
    },
    {
        "date": "2025-02",
        "title": "Exploring the Security Threats of Knowledge Base Poisoning in Retrieval-Augmented Code Generation",
        "author": "Bo Lin, Shangwen Wang, Liqian Chen, and Xiaoguang Mao",
        "link": "http://arxiv.org/abs/2502.03233v1",
        "abstract": "The integration of Large Language Models (LLMs) into software development has\nrevolutionized the field, particularly through the use of Retrieval-Augmented\nCode Generation (RACG) systems that enhance code generation with information\nfrom external knowledge bases. However, the security implications of RACG\nsystems, particularly the risks posed by vulnerable code examples in the\nknowledge base, remain largely unexplored. This risk is particularly concerning\ngiven that public code repositories, which often serve as the sources for\nknowledge base collection in RACG systems, are usually accessible to anyone in\nthe community. Malicious attackers can exploit this accessibility to inject\nvulnerable code into the knowledge base, making it toxic. Once these poisoned\nsamples are retrieved and incorporated into the generated code, they can\npropagate security vulnerabilities into the final product. This paper presents\nthe first comprehensive study on the security risks associated with RACG\nsystems, focusing on how vulnerable code in the knowledge base compromises the\nsecurity of generated code. We investigate the LLM-generated code security\nacross different settings through extensive experiments using four major LLMs,\ntwo retrievers, and two poisoning scenarios. Our findings highlight the\nsignificant threat of knowledge base poisoning, where even a single poisoned\ncode example can compromise up to 48% of generated code. Our findings provide\ncrucial insights into vulnerability introduction in RACG systems and offer\npractical mitigation recommendations, thereby helping improve the security of\nLLM-generated code in future works."
    },
    {
        "date": "2025-02",
        "title": "Adversarial Dependence Minimization",
        "author": "Pierre-Fran\u00e7ois De Plaen, Tinne Tuytelaars, Marc Proesmans, and Luc Van Gool",
        "link": "http://arxiv.org/abs/2502.03227v1",
        "abstract": "Many machine learning techniques rely on minimizing the covariance between\noutput feature dimensions to extract minimally redundant representations from\ndata. However, these methods do not eliminate all dependencies/redundancies, as\nlinearly uncorrelated variables can still exhibit nonlinear relationships. This\nwork provides a differentiable and scalable algorithm for dependence\nminimization that goes beyond linear pairwise decorrelation. Our method employs\nan adversarial game where small networks identify dependencies among feature\ndimensions, while the encoder exploits this information to reduce dependencies.\nWe provide empirical evidence of the algorithm's convergence and demonstrate\nits utility in three applications: extending PCA to nonlinear decorrelation,\nimproving the generalization of image classification methods, and preventing\ndimensional collapse in self-supervised representation learning."
    },
    {
        "date": "2025-02",
        "title": "Secure Resource Management in Cloud Computing: Challenges, Strategies and Meta-Analysis",
        "author": "Deepika Saxena, Smruti Rekha Swain, Jatinder Kumar, Sakshi Patni, Kishu Gupta, Ashutosh Kumar Singh, and Volker Lindenstruth",
        "link": "http://arxiv.org/abs/2502.03149v1",
        "abstract": "Secure resource management (SRM) within a cloud computing environment is a\ncritical yet infrequently studied research topic. This paper provides a\ncomprehensive survey and comparative performance evaluation of potential cyber\nthreat countermeasure strategies that address security challenges during cloud\nworkload execution and resource management. Cybersecurity is explored\nspecifically in the context of cloud resource management, with an emphasis on\nidentifying the associated challenges. The cyber threat countermeasure methods\nare categorized into three classes: defensive strategies, mitigating\nstrategies, and hybrid strategies. The existing countermeasure strategies\nbelonging to each class are thoroughly discussed and compared. In addition to\nconceptual and theoretical analysis, the leading countermeasure strategies\nwithin these categories are implemented on a common platform and examined using\ntwo real-world virtual machine (VM) data traces. Based on this comprehensive\nstudy and performance evaluation, the paper discusses the trade-offs among\nthese countermeasure strategies and their utility, providing imperative\nconcluding remarks on the holistic study of cloud cyber threat countermeasures\nand secure resource management. Furthermore, the study suggests future\nmethodologies that could effectively address the emerging challenges of secure\ncloud resource management."
    },
    {
        "date": "2025-02",
        "title": "Gotham Dataset 2025: A Reproducible Large-Scale IoT Network Dataset for Intrusion Detection and Security Research",
        "author": "Othmane Belarbi, Theodoros Spyridopoulos, Eirini Anthi, Omer Rana, Pietro Carnelli, and Aftab Khan",
        "link": "http://arxiv.org/abs/2502.03134v1",
        "abstract": "In this paper, a dataset of IoT network traffic is presented. Our dataset was\ngenerated by utilising the Gotham testbed, an emulated large-scale Internet of\nThings (IoT) network designed to provide a realistic and heterogeneous\nenvironment for network security research. The testbed includes 78 emulated IoT\ndevices operating on various protocols, including MQTT, CoAP, and RTSP. Network\ntraffic was captured in Packet Capture (PCAP) format using tcpdump, and both\nbenign and malicious traffic were recorded. Malicious traffic was generated\nthrough scripted attacks, covering a variety of attack types, such as Denial of\nService (DoS), Telnet Brute Force, Network Scanning, CoAP Amplification, and\nvarious stages of Command and Control (C&C) communication. The data were\nsubsequently processed in Python for feature extraction using the Tshark tool,\nand the resulting data was converted to Comma Separated Values (CSV) format and\nlabelled. The data repository includes the raw network traffic in PCAP format\nand the processed labelled data in CSV format. Our dataset was collected in a\ndistributed manner, where network traffic was captured separately for each IoT\ndevice at the interface between the IoT gateway and the device. Our dataset was\ncollected in a distributed manner, where network traffic was separately\ncaptured for each IoT device at the interface between the IoT gateway and the\ndevice. With its diverse traffic patterns and attack scenarios, this dataset\nprovides a valuable resource for developing Intrusion Detection Systems and\nsecurity mechanisms tailored to complex, large-scale IoT environments. The\ndataset is publicly available at Zenodo."
    },
    {
        "date": "2025-02",
        "title": "RoboGrasp: A Universal Grasping Policy for Robust Robotic Control",
        "author": "Yiqi Huang, Travis Davies, Jiahuan Yan, Xiang Chen, Yu Tian, and Luhui Hu",
        "link": "http://arxiv.org/abs/2502.03072v1",
        "abstract": "Imitation learning and world models have shown significant promise in\nadvancing generalizable robotic learning, with robotic grasping remaining a\ncritical challenge for achieving precise manipulation. Existing methods often\nrely heavily on robot arm state data and RGB images, leading to overfitting to\nspecific object shapes or positions. To address these limitations, we propose\nRoboGrasp, a universal grasping policy framework that integrates pretrained\ngrasp detection models with robotic learning. By leveraging robust visual\nguidance from object detection and segmentation tasks, RoboGrasp significantly\nenhances grasp precision, stability, and generalizability, achieving up to 34%\nhigher success rates in few-shot learning and grasping box prompt tasks. Built\non diffusion-based methods, RoboGrasp is adaptable to various robotic learning\nparadigms, enabling precise and reliable manipulation across diverse and\ncomplex scenarios. This framework represents a scalable and versatile solution\nfor tackling real-world challenges in robotic grasping."
    },
    {
        "date": "2025-02",
        "title": "Understanding and Enhancing the Transferability of Jailbreaking Attacks",
        "author": "Runqi Lin, Bo Han, Fengwang Li, and Tongling Liu",
        "link": "http://arxiv.org/abs/2502.03052v1",
        "abstract": "Jailbreaking attacks can effectively manipulate open-source large language\nmodels (LLMs) to produce harmful responses. However, these attacks exhibit\nlimited transferability, failing to disrupt proprietary LLMs consistently. To\nreliably identify vulnerabilities in proprietary LLMs, this work investigates\nthe transferability of jailbreaking attacks by analysing their impact on the\nmodel's intent perception. By incorporating adversarial sequences, these\nattacks can redirect the source LLM's focus away from malicious-intent tokens\nin the original input, thereby obstructing the model's intent recognition and\neliciting harmful responses. Nevertheless, these adversarial sequences fail to\nmislead the target LLM's intent perception, allowing the target LLM to refocus\non malicious-intent tokens and abstain from responding. Our analysis further\nreveals the inherent distributional dependency within the generated adversarial\nsequences, whose effectiveness stems from overfitting the source LLM's\nparameters, resulting in limited transferability to target LLMs. To this end,\nwe propose the Perceived-importance Flatten (PiF) method, which uniformly\ndisperses the model's focus across neutral-intent tokens in the original input,\nthus obscuring malicious-intent tokens without relying on overfitted\nadversarial sequences. Extensive experiments demonstrate that PiF provides an\neffective and efficient red-teaming evaluation for proprietary LLMs."
    },
    {
        "date": "2025-02",
        "title": "Membership Inference Attack Should Move On to Distributional Statistics for Distilled Generative Models",
        "author": "Muxing Li, Zesheng Ye, Yixuan Li, Andy Song, Guangquan Zhang, and Feng Liu",
        "link": "http://arxiv.org/abs/2502.02970v1",
        "abstract": "Membership inference attacks (MIAs) determine whether certain data instances\nwere used to train a model by exploiting the differences in how the model\nresponds to seen versus unseen instances. This capability makes MIAs important\nin assessing privacy leakage within modern generative AI systems. However, this\npaper reveals an oversight in existing MIAs against \\emph{distilled generative\nmodels}: attackers can no longer detect a teacher model's training instances\nindividually when targeting the distilled student model, as the student learns\nfrom the teacher-generated data rather than its original member data,\npreventing direct instance-level memorization. Nevertheless, we find that\nstudent-generated samples exhibit a significantly stronger distributional\nalignment with teacher's member data than non-member data. This leads us to\nposit that MIAs \\emph{on distilled generative models should shift from\ninstance-level to distribution-level statistics}. We thereby introduce a\n\\emph{set-based} MIA framework that measures \\emph{relative} distributional\ndiscrepancies between student-generated data\\emph{sets} and potential\nmember/non-member data\\emph{sets}, Empirically, distributional statistics\nreliably distinguish a teacher's member data from non-member data through the\ndistilled model. Finally, we discuss scenarios in which our setup faces\nlimitations."
    },
    {
        "date": "2025-02",
        "title": "Large Language Model Adversarial Landscape Through the Lens of Attack Objectives",
        "author": "Nan Wang, Kane Walter, Yansong Gao, and Alsharif Abuadbba",
        "link": "http://arxiv.org/abs/2502.02960v1",
        "abstract": "Large Language Models (LLMs) represent a transformative leap in artificial\nintelligence, enabling the comprehension, generation, and nuanced interaction\nwith human language on an unparalleled scale. However, LLMs are increasingly\nvulnerable to a range of adversarial attacks that threaten their privacy,\nreliability, security, and trustworthiness. These attacks can distort outputs,\ninject biases, leak sensitive information, or disrupt the normal functioning of\nLLMs, posing significant challenges across various applications.\n  In this paper, we provide a novel comprehensive analysis of the adversarial\nlandscape of LLMs, framed through the lens of attack objectives. By\nconcentrating on the core goals of adversarial actors, we offer a fresh\nperspective that examines threats from the angles of privacy, integrity,\navailability, and misuse, moving beyond conventional taxonomies that focus\nsolely on attack techniques. This objective-driven adversarial landscape not\nonly highlights the strategic intent behind different adversarial approaches\nbut also sheds light on the evolving nature of these threats and the\neffectiveness of current defenses. Our analysis aims to guide researchers and\npractitioners in better understanding, anticipating, and mitigating these\nattacks, ultimately contributing to the development of more resilient and\nrobust LLM systems."
    },
    {
        "date": "2025-02",
        "title": "Robust Reward Alignment in Hypothesis Space",
        "author": "Zhixian Xie, Haode Zhang, Yizhe Feng, and Wanxin Jin",
        "link": "http://arxiv.org/abs/2502.02921v1",
        "abstract": "Reward design for reinforcement learning and optimal control agents is\nchallenging. Preference-based alignment addresses this by enabling agents to\nlearn rewards from ranked trajectory pairs provided by humans. However,\nexisting methods often struggle from poor robustness to unknown false human\npreferences. In this work, we propose a robust and efficient reward alignment\nmethod based on a novel and geometrically interpretable perspective: hypothesis\nspace batched cutting. Our method iteratively refines the reward hypothesis\nspace through \"cuts\" based on batches of human preferences. Within each batch,\nhuman preferences, queried based on disagreement, are grouped using a voting\nfunction to determine the appropriate cut, ensuring a bounded human query\ncomplexity. To handle unknown erroneous preferences, we introduce a\nconservative cutting method within each batch, preventing erroneous human\npreferences from making overly aggressive cuts to the hypothesis space. This\nguarantees provable robustness against false preferences. We evaluate our\nmethod in a model predictive control setting across diverse tasks, including\nDM-Control, dexterous in-hand manipulation, and locomotion. The results\ndemonstrate that our framework achieves comparable or superior performance to\nstate-of-the-art methods in error-free settings while significantly\noutperforming existing method when handling high percentage of erroneous human\npreferences."
    },
    {
        "date": "2025-02",
        "title": "SPARC: Subspace-Aware Prompt Adaptation for Robust Continual Learning in LLMs",
        "author": "Dinithi Jayasuriya, Sina Tayebati, Davide Ettori, Ranganath Krishnan, and Amit Ranjan Trivedi",
        "link": "http://arxiv.org/abs/2502.02909v1",
        "abstract": "We propose SPARC, a lightweight continual learning framework for large\nlanguage models (LLMs) that enables efficient task adaptation through prompt\ntuning in a lower-dimensional space. By leveraging principal component analysis\n(PCA), we identify a compact subspace of the training data. Optimizing prompts\nin this lower-dimensional space enhances training efficiency, as it focuses\nupdates on the most relevant features while reducing computational overhead.\nFurthermore, since the model's internal structure remains unaltered, the\nextensive knowledge gained from pretraining is fully preserved, ensuring that\npreviously learned information is not compromised during adaptation. Our method\nachieves high knowledge retention in both task-incremental and\ndomain-incremental continual learning setups while fine-tuning only 0.04% of\nthe model's parameters. Additionally, by integrating LoRA, we enhance\nadaptability to computational constraints, allowing for a tradeoff between\naccuracy and training cost. Experiments on the SuperGLUE benchmark demonstrate\nthat our PCA-based prompt tuning combined with LoRA maintains full knowledge\nretention while improving accuracy, utilizing only 1% of the model's\nparameters. These results establish our approach as a scalable and\nresource-efficient solution for continual learning in LLMs."
    },
    {
        "date": "2025-02",
        "title": "PoleStack: Robust Pole Estimation of Irregular Objects from Silhouette Stacking",
        "author": "Jacopo Villa, Jay W. McMahon, and Issa A. D. Nesnas",
        "link": "http://arxiv.org/abs/2502.02907v1",
        "abstract": "We present an algorithm to estimate the rotation pole of a principal-axis\nrotator using silhouette images collected from multiple camera poses. First, a\nset of images is stacked to form a single silhouette-stack image, where the\nobject's rotation introduces reflective symmetry about the imaged pole\ndirection. We estimate this projected-pole direction by identifying maximum\nsymmetry in the silhouette stack. To handle unknown center-of-mass image\nlocation, we apply the Discrete Fourier Transform to produce the\nsilhouette-stack amplitude spectrum, achieving translation invariance and\nincreased robustness to noise. Second, the 3D pole orientation is estimated by\ncombining two or more projected-pole measurements collected from different\ncamera orientations. We demonstrate degree-level pole estimation accuracy using\nlow-resolution imagery, showing robustness to severe surface shadowing and\ncentroid-based image-registration errors. The proposed approach could be\nsuitable for pole estimation during both the approach phase toward a target\nobject and while hovering."
    },
    {
        "date": "2025-02",
        "title": "Wolfpack Adversarial Attack for Robust Multi-Agent Reinforcement Learning",
        "author": "Sunwoo Lee, Jaebak Hwang, Yonghyeon Jo, and Seungyul Han",
        "link": "http://arxiv.org/abs/2502.02844v1",
        "abstract": "Traditional robust methods in multi-agent reinforcement learning (MARL) often\nstruggle against coordinated adversarial attacks in cooperative scenarios. To\naddress this limitation, we propose the Wolfpack Adversarial Attack framework,\ninspired by wolf hunting strategies, which targets an initial agent and its\nassisting agents to disrupt cooperation. Additionally, we introduce the\nWolfpack-Adversarial Learning for MARL (WALL) framework, which trains robust\nMARL policies to defend against the proposed Wolfpack attack by fostering\nsystem-wide collaboration. Experimental results underscore the devastating\nimpact of the Wolfpack attack and the significant robustness improvements\nachieved by WALL."
    },
    {
        "date": "2025-02",
        "title": "SimMark: A Robust Sentence-Level Similarity-Based Watermarking Algorithm for Large Language Models",
        "author": "Amirhossein Dabiriaghdam, and Lele Wang",
        "link": "http://arxiv.org/abs/2502.02787v1",
        "abstract": "The rapid proliferation of large language models (LLMs) has created an urgent\nneed for reliable methods to detect whether a text is generated by such models.\nIn this paper, we propose SimMark, a posthoc watermarking algorithm that makes\nLLMs' outputs traceable without requiring access to the model's internal\nlogits, enabling compatibility with a wide range of LLMs, including API-only\nmodels. By leveraging the similarity of semantic sentence embeddings and\nrejection sampling to impose detectable statistical patterns imperceptible to\nhumans, and employing a soft counting mechanism, SimMark achieves robustness\nagainst paraphrasing attacks. Experimental results demonstrate that SimMark\nsets a new benchmark for robust watermarking of LLM-generated content,\nsurpassing prior sentence-level watermarking techniques in robustness, sampling\nefficiency, and applicability across diverse domains, all while preserving the\ntext quality."
    },
    {
        "date": "2025-02",
        "title": "Unveiling Privacy and Security Gaps in Female Health Apps",
        "author": "Muhammad Hassan, Mahnoor Jameel, Tian Wang, and Masooda Bashir",
        "link": "http://arxiv.org/abs/2502.02749v1",
        "abstract": "Female Health Applications (FHA), a growing segment of FemTech, aim to\nprovide affordable and accessible healthcare solutions for women globally.\nThese applications gather and monitor health and reproductive data from\nmillions of users. With ongoing debates on women's reproductive rights and\nprivacy, it's crucial to assess how these apps protect users' privacy. In this\npaper, we undertake a security and data protection assessment of 45 popular\nFHAs. Our investigation uncovers harmful permissions, extensive collection of\nsensitive personal and medical data, and the presence of numerous third-party\ntracking libraries. Furthermore, our examination of their privacy policies\nreveals deviations from fundamental data privacy principles. These findings\nhighlight a significant lack of privacy and security measures for FemTech apps,\nespecially as women's reproductive rights face growing political challenges.\nThe results and recommendations provide valuable insights for users, app\ndevelopers, and policymakers, paving the way for better privacy and security in\nFemale Health Applications."
    },
    {
        "date": "2025-02",
        "title": "Achievable distributional robustness when the robust risk is only partially identified",
        "author": "Julia Kostin, Nicola Gnecco, and Fanny Yang",
        "link": "http://arxiv.org/abs/2502.02710v1",
        "abstract": "In safety-critical applications, machine learning models should generalize\nwell under worst-case distribution shifts, that is, have a small robust risk.\nInvariance-based algorithms can provably take advantage of structural\nassumptions on the shifts when the training distributions are heterogeneous\nenough to identify the robust risk. However, in practice, such identifiability\nconditions are rarely satisfied -- a scenario so far underexplored in the\ntheoretical literature. In this paper, we aim to fill the gap and propose to\nstudy the more general setting when the robust risk is only partially\nidentifiable. In particular, we introduce the worst-case robust risk as a new\nmeasure of robustness that is always well-defined regardless of\nidentifiability. Its minimum corresponds to an algorithm-independent\n(population) minimax quantity that measures the best achievable robustness\nunder partial identifiability. While these concepts can be defined more\nbroadly, in this paper we introduce and derive them explicitly for a linear\nmodel for concreteness of the presentation. First, we show that existing\nrobustness methods are provably suboptimal in the partially identifiable case.\nWe then evaluate these methods and the minimizer of the (empirical) worst-case\nrobust risk on real-world gene expression data and find a similar trend: the\ntest error of existing robustness methods grows increasingly suboptimal as the\nfraction of data from unseen environments increases, whereas accounting for\npartial identifiability allows for better generalization."
    },
    {
        "date": "2025-02",
        "title": "Intelligent Sensing-to-Action for Robust Autonomy at the Edge: Opportunities and Challenges",
        "author": "Amit Ranjan Trivedi, Sina Tayebati, Hemant Kumawat, Nastaran Darabi, Divake Kumar, Adarsh Kumar Kosta, Yeshwanth Venkatesha, Dinithi Jayasuriya, Nethmi Jayasinghe, Priyadarshini Panda, Saibal Mukhopadhyay, and Kaushik Roy",
        "link": "http://arxiv.org/abs/2502.02692v1",
        "abstract": "Autonomous edge computing in robotics, smart cities, and autonomous vehicles\nrelies on the seamless integration of sensing, processing, and actuation for\nreal-time decision-making in dynamic environments. At its core is the\nsensing-to-action loop, which iteratively aligns sensor inputs with\ncomputational models to drive adaptive control strategies. These loops can\nadapt to hyper-local conditions, enhancing resource efficiency and\nresponsiveness, but also face challenges such as resource constraints,\nsynchronization delays in multi-modal data fusion, and the risk of cascading\nerrors in feedback loops. This article explores how proactive, context-aware\nsensing-to-action and action-to-sensing adaptations can enhance efficiency by\ndynamically adjusting sensing and computation based on task demands, such as\nsensing a very limited part of the environment and predicting the rest. By\nguiding sensing through control actions, action-to-sensing pathways can improve\ntask relevance and resource use, but they also require robust monitoring to\nprevent cascading errors and maintain reliability. Multi-agent sensing-action\nloops further extend these capabilities through coordinated sensing and actions\nacross distributed agents, optimizing resource use via collaboration.\nAdditionally, neuromorphic computing, inspired by biological systems, provides\nan efficient framework for spike-based, event-driven processing that conserves\nenergy, reduces latency, and supports hierarchical control--making it ideal for\nmulti-agent optimization. This article highlights the importance of end-to-end\nco-design strategies that align algorithmic models with hardware and\nenvironmental dynamics and improve cross-layer interdependencies to improve\nthroughput, precision, and adaptability for energy-efficient edge autonomy in\ncomplex environments."
    },
    {
        "date": "2025-02",
        "title": "OverThink: Slowdown Attacks on Reasoning LLMs",
        "author": "Abhinav Kumar, Jaechul Roh, Ali Naseh, Marzena Karpinska, Mohit Iyyer, Amir Houmansadr, and Eugene Bagdasarian",
        "link": "http://arxiv.org/abs/2502.02542v2",
        "abstract": "We increase overhead for applications that rely on reasoning LLMs-we force\nmodels to spend an amplified number of reasoning tokens, i.e., \"overthink\", to\nrespond to the user query while providing contextually correct answers. The\nadversary performs an OVERTHINK attack by injecting decoy reasoning problems\ninto the public content that is used by the reasoning LLM (e.g., for RAG\napplications) during inference time. Due to the nature of our decoy problems\n(e.g., a Markov Decision Process), modified texts do not violate safety\nguardrails. We evaluated our attack across closed-(OpenAI o1, o1-mini, o3-mini)\nand open-(DeepSeek R1) weights reasoning models on the FreshQA and SQuAD\ndatasets. Our results show up to 18x slowdown on FreshQA dataset and 46x\nslowdown on SQuAD dataset. The attack also shows high transferability across\nmodels. To protect applications, we discuss and implement defenses leveraging\nLLM-based and system design approaches. Finally, we discuss societal,\nfinancial, and energy impacts of OVERTHINK attack which could amplify the costs\nfor third-party applications operating reasoning models."
    },
    {
        "date": "2025-02",
        "title": "Optimal Security Response to Network Intrusions in IT Systems",
        "author": "Kim Hammar",
        "link": "http://arxiv.org/abs/2502.02541v1",
        "abstract": "Cybersecurity is one of the most pressing technological challenges of our\ntime and requires measures from all sectors of society. A key measure is\nautomated security response, which enables automated mitigation and recovery\nfrom cyber attacks. Significant strides toward such automation have been made\ndue to the development of rule-based response systems. However, these systems\nhave a critical drawback: they depend on domain experts to configure the rules,\na process that is both error-prone and inefficient. Framing security response\nas an optimal control problem shows promise in addressing this limitation but\nintroduces new challenges. Chief among them is bridging the gap between\ntheoretical optimality and operational performance. Current response systems\nwith theoretical optimality guarantees have only been validated analytically or\nin simulation, leaving their practical utility unproven.\n  This thesis tackles the aforementioned challenges by developing a practical\nmethodology for optimal security response in IT infrastructures. It encompasses\ntwo systems. First, it includes an emulation system that replicates key\ncomponents of the target infrastructure. We use this system to gather\nmeasurements and logs, based on which we identify a game-theoretic model.\nSecond, it includes a simulation system where game-theoretic response\nstrategies are optimized through stochastic approximation to meet a given\nobjective, such as mitigating potential attacks while maintaining operational\nservices. These strategies are then evaluated and refined in the emulation\nsystem to close the gap between theoretical and operational performance. We\nprove structural properties of optimal response strategies and derive efficient\nalgorithms for computing them. This enables us to solve a previously unsolved\nproblem: demonstrating optimal security response against network intrusions on\nan IT infrastructure."
    },
    {
        "date": "2025-02",
        "title": "Uncertainty Quantification for Collaborative Object Detection Under Adversarial Attacks",
        "author": "Huiqun Huang, Cong Chen, Jean-Philippe Monteuuis, Jonathan Petit, and Fei Miao",
        "link": "http://arxiv.org/abs/2502.02537v1",
        "abstract": "Collaborative Object Detection (COD) and collaborative perception can\nintegrate data or features from various entities, and improve object detection\naccuracy compared with individual perception. However, adversarial attacks pose\na potential threat to the deep learning COD models, and introduce high output\nuncertainty. With unknown attack models, it becomes even more challenging to\nimprove COD resiliency and quantify the output uncertainty for highly dynamic\nperception scenes such as autonomous vehicles. In this study, we propose the\nTrusted Uncertainty Quantification in Collaborative Perception framework\n(TUQCP). TUQCP leverages both adversarial training and uncertainty\nquantification techniques to enhance the adversarial robustness of existing COD\nmodels. More specifically, TUQCP first adds perturbations to the shared\ninformation of randomly selected agents during object detection collaboration\nby adversarial training. TUQCP then alleviates the impacts of adversarial\nattacks by providing output uncertainty estimation through learning-based\nmodule and uncertainty calibration through conformal prediction. Our framework\nworks for early and intermediate collaboration COD models and single-agent\nobject detection models. We evaluate TUQCP on V2X-Sim, a comprehensive\ncollaborative perception dataset for autonomous driving, and demonstrate a\n80.41% improvement in object detection accuracy compared to the baselines under\nthe same adversarial attacks. TUQCP demonstrates the importance of uncertainty\nquantification to COD under adversarial attacks."
    },
    {
        "date": "2025-02",
        "title": "Privacy Attacks on Image AutoRegressive Models",
        "author": "Antoni Kowalczuk, Jan Dubi\u0144ski, Franziska Boenisch, and Adam Dziedzic",
        "link": "http://arxiv.org/abs/2502.02514v1",
        "abstract": "Image autoregressive (IAR) models have surpassed diffusion models (DMs) in\nboth image quality (FID: 1.48 vs. 1.58) and generation speed. However, their\nprivacy risks remain largely unexplored. To address this, we conduct a\ncomprehensive privacy analysis comparing IARs to DMs. We develop a novel\nmembership inference attack (MIA) that achieves a significantly higher success\nrate in detecting training images (TPR@FPR=1%: 86.38% for IARs vs. 4.91% for\nDMs). Using this MIA, we perform dataset inference (DI) and find that IARs\nrequire as few as six samples to detect dataset membership, compared to 200 for\nDMs, indicating higher information leakage. Additionally, we extract hundreds\nof training images from an IAR (e.g., 698 from VAR-d30). Our findings highlight\na fundamental privacy-utility trade-off: while IARs excel in generation quality\nand speed, they are significantly more vulnerable to privacy attacks. This\nsuggests that incorporating techniques from DMs, such as per-token probability\nmodeling using diffusion, could help mitigate IARs' privacy risks. Our code is\navailable at https://github.com/sprintml/privacy_attacks_against_iars."
    },
    {
        "date": "2025-02",
        "title": "Catoni Contextual Bandits are Robust to Heavy-tailed Rewards",
        "author": "Chenlu Ye, Yujia Jin, Alekh Agarwal, and Tong Zhang",
        "link": "http://arxiv.org/abs/2502.02486v1",
        "abstract": "Typical contextual bandit algorithms assume that the rewards at each round\nlie in some fixed range $[0, R]$, and their regret scales polynomially with\nthis reward range $R$. However, many practical scenarios naturally involve\nheavy-tailed rewards or rewards where the worst-case range can be substantially\nlarger than the variance. In this paper, we develop an algorithmic approach\nbuilding on Catoni's estimator from robust statistics, and apply it to\ncontextual bandits with general function approximation. When the variance of\nthe reward at each round is known, we use a variance-weighted regression\napproach and establish a regret bound that depends only on the cumulative\nreward variance and logarithmically on the reward range $R$ as well as the\nnumber of rounds $T$. For the unknown-variance case, we further propose a\ncareful peeling-based algorithm and remove the need for cumbersome variance\nestimation. With additional dependence on the fourth moment, our algorithm also\nenjoys a variance-based bound with logarithmic reward-range dependence.\nMoreover, we demonstrate the optimality of the leading-order term in our regret\nbound through a matching lower bound."
    },
    {
        "date": "2025-02",
        "title": "Medical Multimodal Model Stealing Attacks via Adversarial Domain Alignment",
        "author": "Yaling Shen, Zhixiong Zhuang, Kun Yuan, Maria-Irina Nicolae, Nassir Navab, Nicolas Padoy, and Mario Fritz",
        "link": "http://arxiv.org/abs/2502.02438v1",
        "abstract": "Medical multimodal large language models (MLLMs) are becoming an instrumental\npart of healthcare systems, assisting medical personnel with decision making\nand results analysis. Models for radiology report generation are able to\ninterpret medical imagery, thus reducing the workload of radiologists. As\nmedical data is scarce and protected by privacy regulations, medical MLLMs\nrepresent valuable intellectual property. However, these assets are potentially\nvulnerable to model stealing, where attackers aim to replicate their\nfunctionality via black-box access. So far, model stealing for the medical\ndomain has focused on classification; however, existing attacks are not\neffective against MLLMs. In this paper, we introduce Adversarial Domain\nAlignment (ADA-STEAL), the first stealing attack against medical MLLMs.\nADA-STEAL relies on natural images, which are public and widely available, as\nopposed to their medical counterparts. We show that data augmentation with\nadversarial noise is sufficient to overcome the data distribution gap between\nnatural images and the domain-specific distribution of the victim MLLM.\nExperiments on the IU X-RAY and MIMIC-CXR radiology datasets demonstrate that\nAdversarial Domain Alignment enables attackers to steal the medical MLLM\nwithout any access to medical data."
    },
    {
        "date": "2025-02",
        "title": "TransformDAS: Mapping \u03a6-OTDR Signals to Riemannian Manifold for Robust Classification",
        "author": "Jiaju Kang, Puyu Han, Yang Chun, Xu Wang, and Luqi Gong",
        "link": "http://arxiv.org/abs/2502.02428v1",
        "abstract": "Phase-sensitive optical time-domain reflectometry ({\\Phi}-OTDR) is a widely\nused distributed fiber optic sensing system in engineering. Machine learning\nalgorithms for {\\Phi}-OTDR event classification require high volumes and\nquality of datasets; however, high-quality datasets are currently extremely\nscarce in the field, leading to a lack of robustness in models, which is\nmanifested by higher false alarm rates in real-world scenarios. One promising\napproach to address this issue is to augment existing data using generative\nmodels combined with a small amount of real-world data. We explored mapping\nboth {\\Phi}-OTDR features in a GAN-based generative pipeline and signal\nfeatures in a Transformer classifier to hyperbolic space to seek more effective\nmodel generalization. The results indicate that state-of-the-art models exhibit\nstronger generalization performance and lower false alarm rates in real-world\nscenarios when trained on augmented datasets. TransformDAS, in particular,\ndemonstrates the best classification performance, highlighting the benefits of\nRiemannian manifold mapping in {\\Phi}-OTDR data generation and model\nclassification."
    },
    {
        "date": "2025-02",
        "title": "Target Attack Backdoor Malware Analysis and Attribution",
        "author": "Anthony Cheuk Tung Lai, Vitaly Kamluk, Alan Ho, Ping Fan Ke, and Byron Wai",
        "link": "http://arxiv.org/abs/2502.02335v1",
        "abstract": "Backdoor Malware are installed by an attacker on the victim's server(s) for\nauthorized access. A customized backdoor is weaponized to execute unauthorized\nsystem, database and application commands to access the user credentials and\nconfidential digital assets. Recently, we discovered and analyzed a targeted\npersistent module backdoor in Web Server in an online business company that was\nundetectable by their deployed Anti-Virus software for a year. This led us to\ncarry out research to detect this specific type of persistent module backdoor\ninstalled in Web servers. Other than typical Malware static analysis, we carry\nout analysis with binary similarity, strings, and command obfuscation over the\nbackdoor, resulting in the Target Attack Backdoor Malware Analysis Matrix\n(TABMAX) for organizations to detect this sophisticated target attack backdoor\ninstead of a general one which can be detected by Anti-Virus detectors. Our\nfindings show that backdoor malware can be designed with different APIs,\ncommands, strings, and query language on top of preferred libraries used by\ntypical Malware."
    },
    {
        "date": "2025-02",
        "title": "FRAUD-RLA: A new reinforcement learning adversarial attack against credit card fraud detection",
        "author": "Daniele Lunghi, Yannick Molinghen, Alkis Simitsis, Tom Lenaerts, and Gianluca Bontempi",
        "link": "http://arxiv.org/abs/2502.02290v1",
        "abstract": "Adversarial attacks pose a significant threat to data-driven systems, and\nresearchers have spent considerable resources studying them. Despite its\neconomic relevance, this trend largely overlooked the issue of credit card\nfraud detection. To address this gap, we propose a new threat model that\ndemonstrates the limitations of existing attacks and highlights the necessity\nto investigate new approaches. We then design a new adversarial attack for\ncredit card fraud detection, employing reinforcement learning to bypass\nclassifiers. This attack, called FRAUD-RLA, is designed to maximize the\nattacker's reward by optimizing the exploration-exploitation tradeoff and\nworking with significantly less required knowledge than competitors. Our\nexperiments, conducted on three different heterogeneous datasets and against\ntwo fraud detection systems, indicate that FRAUD-RLA is effective, even\nconsidering the severe limitations imposed by our threat model."
    },
    {
        "date": "2025-02",
        "title": "Adversarial ML Problems Are Getting Harder to Solve and to Evaluate",
        "author": "Javier Rando, Jie Zhang, Nicholas Carlini, and Florian Tram\u00e8r",
        "link": "http://arxiv.org/abs/2502.02260v1",
        "abstract": "In the past decade, considerable research effort has been devoted to securing\nmachine learning (ML) models that operate in adversarial settings. Yet,\nprogress has been slow even for simple \"toy\" problems (e.g., robustness to\nsmall adversarial perturbations) and is often hindered by non-rigorous\nevaluations. Today, adversarial ML research has shifted towards studying\nlarger, general-purpose language models. In this position paper, we argue that\nthe situation is now even worse: in the era of LLMs, the field of adversarial\nML studies problems that are (1) less clearly defined, (2) harder to solve, and\n(3) even more challenging to evaluate. As a result, we caution that yet another\ndecade of work on adversarial ML may fail to produce meaningful progress."
    },
    {
        "date": "2025-02",
        "title": "An Attack-Driven Incident Response and Defense System (ADIRDS)",
        "author": "Anthony Cheuk Tung Lai, Siu Ming Yiu, Ping Fan Ke, and Alan Ho",
        "link": "http://arxiv.org/abs/2502.02230v1",
        "abstract": "One of the major goals of incident response is to help an organization or a\nsystem owner to quickly identify and halt the attacks to minimize the damages\n(and financial loss) to the system being attacked. Typical incident responses\nrely very much on the log information captured by the system during the attacks\nand if needed, may need to isolate the victim from the network to avoid further\ndestructive attacks. However, there are real cases that there are insufficient\nlog records/information for the incident response team to identify the attacks\nand their origins while the attacked system cannot be stopped due to service\nrequirements (zero downtime online systems) such as online gaming sites.\nTypical incident response procedures and industrial standards do not provide an\nadequate solution to address this scenario. In this paper, being motivated by a\nreal case, we propose a solution, called \"Attack-Driven Incident Response and\nDefense System (ADIRDS)\" to tackle this problem. ADIRDS is an online monitoring\nsystem to run with the real system. By modeling the real system as a graph,\ncritical nodes/assets of the system are closely monitored. Instead of relying\non the original logging system, evidence will be collected from the attack\ntechnique perspectives. To migrate the risks, realistic honeypots with very\nsimilar business context as the real system are deployed to trap the attackers.\nWe successfully apply this system to a real case. Based on our experiments, we\nverify that our new approach of designing the realistic honeypots is effective,\n38 unique attacker's IP addresses were captured. We also compare the\nperformance of our realistic honey with both low and high interactive honeypots\nproposed in the literature, the results found that our proposed honeypot can\nsuccessfully cheat the attackers to attack our honeypot, which verifies that\nour honeypot is more effective."
    },
    {
        "date": "2025-02",
        "title": "A Robust Remote Photoplethysmography Method",
        "author": "Alexey Protopopov",
        "link": "http://arxiv.org/abs/2502.02229v1",
        "abstract": "Remote photoplethysmography (rPPG) is a method for measuring a subjects heart\nrate remotely using a camera. Factors such as subject movement, ambient light\nlevel, makeup etc. complicate such measurements by distorting the observed\npulse. Recent works on this topic have proposed a variety of approaches for\naccurately measuring heart rate in humans, however these methods were tested in\nideal conditions, where the subject does not make significant movements and all\nmeasurements are taken at the same level of illumination. In more realistic\nconditions these methods suffer from decreased accuracy. The study proposes a\nmore robust method that is less susceptible to distortions and has minimal\nhardware requirements. The proposed method uses a combination of mathematical\ntransforms to calculate the subjects heart rate. It performs best when used\nwith a camera that has been modified by removing its infrared filter, although\nusing an unmodified camera is also possible. The method was tested on 26 videos\ntaken from 19 volunteers of varying gender and age. The obtained results were\ncompared to reference data and the average mean absolute error was found to be\nat 1.95 beats per minute, which is noticeably better than the results from\nprevious works. The remote photoplethysmography method proposed in the present\narticle is more resistant to distortions than methods from previous\npublications and thus allows one to remotely and accurately measure the\nsubjects heart rate without imposing any significant limitations on the\nsubjects behavior."
    },
    {
        "date": "2025-02",
        "title": "Progressive Correspondence Regenerator for Robust 3D Registration",
        "author": "Guiyu Zhao, Sheng Ao, Ye Zhang, and Kai Xu Yulan Guo",
        "link": "http://arxiv.org/abs/2502.02163v1",
        "abstract": "Obtaining enough high-quality correspondences is crucial for robust\nregistration. Existing correspondence refinement methods mostly follow the\nparadigm of outlier removal, which either fails to correctly identify the\naccurate correspondences under extreme outlier ratios, or select too few\ncorrect correspondences to support robust registration. To address this\nchallenge, we propose a novel approach named Regor, which is a progressive\ncorrespondence regenerator that generates higher-quality matches whist\nsufficiently robust for numerous outliers. In each iteration, we first apply\nprior-guided local grouping and generalized mutual matching to generate the\nlocal region correspondences. A powerful center-aware three-point consistency\nis then presented to achieve local correspondence correction, instead of\nremoval. Further, we employ global correspondence refinement to obtain accurate\ncorrespondences from a global perspective. Through progressive iterations, this\nprocess yields a large number of high-quality correspondences. Extensive\nexperiments on both indoor and outdoor datasets demonstrate that the proposed\nRegor significantly outperforms existing outlier removal techniques. More\ncritically, our approach obtain 10 times more correct correspondences than\noutlier removal methods. As a result, our method is able to achieve robust\nregistration even with weak features. The code will be released."
    },
    {
        "date": "2025-02",
        "title": "Dual-Flow: Transferable Multi-Target, Instance-Agnostic Attacks via In-the-wild Cascading Flow Optimization",
        "author": "Yixiao Chen, Shikun Sun, Jianshu Li, Ruoyu Li, Zhe Li, and Junliang Xing",
        "link": "http://arxiv.org/abs/2502.02096v2",
        "abstract": "Adversarial attacks are widely used to evaluate model robustness, and in\nblack-box scenarios, the transferability of these attacks becomes crucial.\nExisting generator-based attacks have excellent generalization and\ntransferability due to their instance-agnostic nature. However, when training\ngenerators for multi-target tasks, the success rate of transfer attacks is\nrelatively low due to the limitations of the model's capacity. To address these\nchallenges, we propose a novel Dual-Flow framework for multi-target\ninstance-agnostic adversarial attacks, utilizing Cascading Distribution Shift\nTraining to develop an adversarial velocity function. Extensive experiments\ndemonstrate that Dual-Flow significantly improves transferability over previous\nmulti-target generative attacks. For example, it increases the success rate\nfrom Inception-v3 to ResNet-152 by 34.58%. Furthermore, our attack method shows\nsubstantially stronger robustness against defense mechanisms, such as\nadversarially trained models."
    },
    {
        "date": "2025-02",
        "title": "Robust and Secure Code Watermarking for Large Language Models via ML/Crypto Codesign",
        "author": "Ruisi Zhang, Neusha Javidnia, Nojan Sheybani, and Farinaz Koushanfar",
        "link": "http://arxiv.org/abs/2502.02068v1",
        "abstract": "This paper introduces RoSe, the first-of-its-kind ML/Crypto codesign\nwatermarking framework that regulates LLM-generated code to avoid intellectual\nproperty rights violations and inappropriate misuse in software development.\nHigh-quality watermarks adhering to the detectability-fidelity-robustness\ntri-objective are limited due to codes' low-entropy nature. Watermark\nverification, however, often needs to reveal the signature and requires\nre-encoding new ones for code reuse, which potentially compromising the\nsystem's usability. To overcome these challenges, RoSe obtains high-quality\nwatermarks by training the watermark insertion and extraction modules\nend-to-end to ensure (i) unaltered watermarked code functionality and (ii)\nenhanced detectability and robustness leveraging pre-trained CodeT5 as the\ninsertion backbone to enlarge the code syntactic and variable rename\ntransformation search space. In the deployment, RoSe uses zero-knowledge proofs\nfor secure verification without revealing the underlying signatures. Extensive\nevaluations demonstrated RoSe achieves high detection accuracy while preserving\nthe code functionality. RoSe is also robust against attacks and provides\nefficient secure watermark verification."
    },
    {
        "date": "2025-02",
        "title": "RAPID: Robust and Agile Planner Using Inverse Reinforcement Learning for Vision-Based Drone Navigation",
        "author": "Minwoo Kim, Geunsik Bae, Jinwoo Lee, Woojae Shin, Changseung Kim, Myong-Yol Choi, Heejung Shin, and Hyondong Oh",
        "link": "http://arxiv.org/abs/2502.02054v1",
        "abstract": "This paper introduces a learning-based visual planner for agile drone flight\nin cluttered environments. The proposed planner generates collision-free\nwaypoints in milliseconds, enabling drones to perform agile maneuvers in\ncomplex environments without building separate perception, mapping, and\nplanning modules. Learning-based methods, such as behavior cloning (BC) and\nreinforcement learning (RL), demonstrate promising performance in visual\nnavigation but still face inherent limitations. BC is susceptible to\ncompounding errors due to limited expert imitation, while RL struggles with\nreward function design and sample inefficiency. To address these limitations,\nthis paper proposes an inverse reinforcement learning (IRL)-based framework for\nhigh-speed visual navigation. By leveraging IRL, it is possible to reduce the\nnumber of interactions with simulation environments and improve capability to\ndeal with high-dimensional spaces while preserving the robustness of RL\npolicies. A motion primitive-based path planning algorithm collects an expert\ndataset with privileged map data from diverse environments, ensuring\ncomprehensive scenario coverage. By leveraging both the acquired expert and\nlearner dataset gathered from the agent's interactions with the simulation\nenvironments, a robust reward function and policy are learned across diverse\nstates. While the proposed method is trained in a simulation environment only,\nit can be directly applied to real-world scenarios without additional training\nor tuning. The performance of the proposed method is validated in both\nsimulation and real-world environments, including forests and various\nstructures. The trained policy achieves an average speed of 7 m/s and a maximum\nspeed of 8.8 m/s in real flight experiments. To the best of our knowledge, this\nis the first work to successfully apply an IRL framework for high-speed visual\nnavigation of drones."
    },
    {
        "date": "2025-02",
        "title": "SMTFL: Secure Model Training to Untrusted Participants in Federated Learning",
        "author": "Zhihui Zhao, Xiaorong Dong, Yimo Ren, Jianhua Wang, Dan Yu, Hongsong Zhu, and Yongle Chen",
        "link": "http://arxiv.org/abs/2502.02038v1",
        "abstract": "Federated learning is an essential distributed model training technique.\nHowever, threats such as gradient inversion attacks and poisoning attacks pose\nsignificant risks to the privacy of training data and the model correctness. We\npropose a novel approach called SMTFL to achieve secure model training in\nfederated learning without relying on trusted participants. To safeguard\ngradients privacy against gradient inversion attacks, clients are dynamically\ngrouped, allowing one client's gradient to be divided to obfuscate the\ngradients of other clients within the group. This method incorporates checks\nand balances to reduce the collusion for inferring specific client data. To\ndetect poisoning attacks from malicious clients, we assess the impact of\naggregated gradients on the global model's performance, enabling effective\nidentification and exclusion of malicious clients. Each client's gradients are\nencrypted and stored, with decryption collectively managed by all clients. The\ndetected poisoning gradients are invalidated from the global model through a\nunlearning method. To our best knowledge, we present the first practical secure\naggregation scheme, which does not require trusted participants, avoids the\nperformance degradation associated with traditional noise-injection, and aviods\ncomplex cryptographic operations during gradient aggregation. Evaluation\nresults are encouraging based on four datasets and two models: SMTFL is\neffective against poisoning attacks and gradient inversion attacks, achieving\nan accuracy rate of over 95% in locating malicious clients, while keeping the\nfalse positive rate for honest clients within 5%. The model accuracy is also\nnearly restored to its pre-attack state when SMTFL is deployed."
    },
    {
        "date": "2025-02",
        "title": "Multi-Domain Graph Foundation Models: Robust Knowledge Transfer via Topology Alignment",
        "author": "Shuo Wang, Bokui Wang, Zhixiang Shen, Boyan Deng, and Zhao Kang",
        "link": "http://arxiv.org/abs/2502.02017v1",
        "abstract": "Recent advances in CV and NLP have inspired researchers to develop\ngeneral-purpose graph foundation models through pre-training across diverse\ndomains. However, a fundamental challenge arises from the substantial\ndifferences in graph topologies across domains. Additionally, real-world graphs\nare often sparse and prone to noisy connections and adversarial attacks. To\naddress these issues, we propose the Multi-Domain Graph Foundation Model\n(MDGFM), a unified framework that aligns and leverages cross-domain topological\ninformation to facilitate robust knowledge transfer. MDGFM bridges different\ndomains by adaptively balancing features and topology while refining original\ngraphs to eliminate noise and align topological structures. To further enhance\nknowledge transfer, we introduce an efficient prompt-tuning approach. By\naligning topologies, MDGFM not only improves multi-domain pre-training but also\nenables robust knowledge transfer to unseen domains. Theoretical analyses\nprovide guarantees of MDGFM's effectiveness and domain generalization\ncapabilities. Extensive experiments on both homophilic and heterophilic graph\ndatasets validate the robustness and efficacy of our method."
    },
    {
        "date": "2025-02",
        "title": "Optimizing Spot Instance Reliability and Security Using Cloud-Native Data and Tools",
        "author": "Shubham Malhotra",
        "link": "http://arxiv.org/abs/2502.01966v1",
        "abstract": "This paper represents \"Cloudlab\", a comprehensive, cloud - native laboratory\ndesigned to support network security research and training. Built on Google\nCloud and adhering to GitOps methodologies, Cloudlab facilitates the the\ncreation, testing, and deployment of secure, containerized workloads using\nKubernetes and serverless architectures. The lab integrates tools like Palo\nAlto Networks firewalls, Bridgecrew for \"Security as Code,\" and automated\nGitHub workflows to establish a robust Continuous Integration/Continuous\nMachine Learning pipeline. By providing an adaptive and scalable environment,\nCloudlab supports advanced security concepts such as role-based access control,\nPolicy as Code, and container security. This initiative enables data scientists\nand engineers to explore cutting-edge practices in a dynamic cloud-native\necosystem, fostering innovation and improving operational resilience in modern\nIT infrastructures."
    },
    {
        "date": "2025-02",
        "title": "Query-Based and Unnoticeable Graph Injection Attack from Neighborhood Perspective",
        "author": "Chang Liu, Hai Huang, Yujie Xing, and Xingquan Zuo",
        "link": "http://arxiv.org/abs/2502.01936v1",
        "abstract": "The robustness of Graph Neural Networks (GNNs) has become an increasingly\nimportant topic due to their expanding range of applications. Various attack\nmethods have been proposed to explore the vulnerabilities of GNNs, ranging from\nGraph Modification Attacks (GMA) to the more practical and flexible Graph\nInjection Attacks (GIA). However, existing methods face two key challenges: (i)\ntheir reliance on surrogate models, which often leads to reduced attack\neffectiveness due to structural differences and prior biases, and (ii) existing\nGIA methods often sacrifice attack success rates in undefended settings to\nbypass certain defense models, thereby limiting their overall effectiveness. To\novercome these limitations, we propose QUGIA, a Query-based and Unnoticeable\nGraph Injection Attack. QUGIA injects nodes by first selecting edges based on\nvictim node connections and then generating node features using a Bayesian\nframework. This ensures that the injected nodes are similar to the original\ngraph nodes, implicitly preserving homophily and making the attack more\nunnoticeable. Unlike previous methods, QUGIA does not rely on surrogate models,\nthereby avoiding performance degradation and achieving better generalization.\nExtensive experiments on six real-world datasets with diverse characteristics\ndemonstrate that QUGIA achieves unnoticeable attacks and outperforms\nstate-of-the-art attackers. The code will be released upon acceptance."
    },
    {
        "date": "2025-02",
        "title": "Distributionally Robust Direct Preference Optimization",
        "author": "Zaiyan Xu, Sushil Vemuri, Kishan Panaganti, Dileep Kalathil, Rahul Jain, and Deepak Ramachandran",
        "link": "http://arxiv.org/abs/2502.01930v1",
        "abstract": "A major challenge in aligning large language models (LLMs) with human\npreferences is the issue of distribution shift. LLM alignment algorithms rely\non static preference datasets, assuming that they accurately represent\nreal-world user preferences. However, user preferences vary significantly\nacross geographical regions, demographics, linguistic patterns, and evolving\ncultural trends. This preference distribution shift leads to catastrophic\nalignment failures in many real-world applications. We address this problem\nusing the principled framework of distributionally robust optimization, and\ndevelop two novel distributionally robust direct preference optimization (DPO)\nalgorithms, namely, Wasserstein DPO (WDPO) and Kullback-Leibler DPO (KLDPO). We\ncharacterize the sample complexity of learning the optimal policy parameters\nfor WDPO and KLDPO. Moreover, we propose scalable gradient descent-style\nlearning algorithms by developing suitable approximations for the challenging\nminimax loss functions of WDPO and KLDPO. Our empirical experiments demonstrate\nthe superior performance of WDPO and KLDPO in substantially improving the\nalignment when there is a preference distribution shift."
    },
    {
        "date": "2025-02",
        "title": "INTACT: Inducing Noise Tolerance through Adversarial Curriculum Training for LiDAR-based Safety-Critical Perception and Autonomy",
        "author": "Nastaran Darabi, Divake Kumar, Sina Tayebati, and Amit Ranjan Trivedi",
        "link": "http://arxiv.org/abs/2502.01896v1",
        "abstract": "In this work, we present INTACT, a novel two-phase framework designed to\nenhance the robustness of deep neural networks (DNNs) against noisy LiDAR data\nin safety-critical perception tasks. INTACT combines meta-learning with\nadversarial curriculum training (ACT) to systematically address challenges\nposed by data corruption and sparsity in 3D point clouds. The meta-learning\nphase equips a teacher network with task-agnostic priors, enabling it to\ngenerate robust saliency maps that identify critical data regions. The ACT\nphase leverages these saliency maps to progressively expose a student network\nto increasingly complex noise patterns, ensuring targeted perturbation and\nimproved noise resilience. INTACT's effectiveness is demonstrated through\ncomprehensive evaluations on object detection, tracking, and classification\nbenchmarks using diverse datasets, including KITTI, Argoverse, and ModelNet40.\nResults indicate that INTACT improves model robustness by up to 20% across all\ntasks, outperforming standard adversarial and curriculum training methods. This\nframework not only addresses the limitations of conventional training\nstrategies but also offers a scalable and efficient solution for real-world\ndeployment in resource-constrained safety-critical systems. INTACT's principled\nintegration of meta-learning and adversarial training establishes a new\nparadigm for noise-tolerant 3D perception in safety-critical applications.\nINTACT improved KITTI Multiple Object Tracking Accuracy (MOTA) by 9.6% (64.1%\n-> 75.1%) and by 12.4% under Gaussian noise (52.5% -> 73.7%). Similarly, KITTI\nmean Average Precision (mAP) rose from 59.8% to 69.8% (50% point drop) and\n49.3% to 70.9% (Gaussian noise), highlighting the framework's ability to\nenhance deep learning model resilience in safety-critical object tracking\nscenarios."
    },
    {
        "date": "2025-02",
        "title": "A Privacy-Preserving Domain Adversarial Federated learning for multi-site brain functional connectivity analysis",
        "author": "Yipu Zhang, Likai Wang, Kuan-Jui Su, Aiying Zhang, Hao Zhu, Xiaowen Liu, Hui Shen, Vince D. Calhoun, Yuping Wang, and Hongwen Deng",
        "link": "http://arxiv.org/abs/2502.01885v1",
        "abstract": "Resting-state functional magnetic resonance imaging (rs-fMRI) and its derived\nfunctional connectivity networks (FCNs) have become critical for understanding\nneurological disorders. However, collaborative analyses and the\ngeneralizability of models still face significant challenges due to privacy\nregulations and the non-IID (non-independent and identically distributed)\nproperty of multiple data sources. To mitigate these difficulties, we propose\nDomain Adversarial Federated Learning (DAFed), a novel federated deep learning\nframework specifically designed for non-IID fMRI data analysis in multi-site\nsettings. DAFed addresses these challenges through feature disentanglement,\ndecomposing the latent feature space into domain-invariant and domain-specific\ncomponents, to ensure robust global learning while preserving local data\nspecificity. Furthermore, adversarial training facilitates effective knowledge\ntransfer between labeled and unlabeled datasets, while a contrastive learning\nmodule enhances the global representation of domain-invariant features. We\nevaluated DAFed on the diagnosis of ASD and further validated its\ngeneralizability in the classification of AD, demonstrating its superior\nclassification accuracy compared to state-of-the-art methods. Additionally, an\nenhanced Score-CAM module identifies key brain regions and functional\nconnectivity significantly associated with ASD and MCI, respectively,\nuncovering shared neurobiological patterns across sites. These findings\nhighlight the potential of DAFed to advance multi-site collaborative research\nin neuroimaging while protecting data confidentiality."
    },
    {
        "date": "2025-02",
        "title": "Reliability-Driven LiDAR-Camera Fusion for Robust 3D Object Detection",
        "author": "Reza Sadeghian, Niloofar Hooshyaripour, Chris Joslin, and WonSook Lee",
        "link": "http://arxiv.org/abs/2502.01856v1",
        "abstract": "Accurate and robust 3D object detection is essential for autonomous driving,\nwhere fusing data from sensors like LiDAR and camera enhances detection\naccuracy. However, sensor malfunctions such as corruption or disconnection can\ndegrade performance, and existing fusion models often struggle to maintain\nreliability when one modality fails. To address this, we propose ReliFusion, a\nnovel LiDAR-camera fusion framework operating in the bird's-eye view (BEV)\nspace. ReliFusion integrates three key components: the Spatio-Temporal Feature\nAggregation (STFA) module, which captures dependencies across frames to\nstabilize predictions over time; the Reliability module, which assigns\nconfidence scores to quantify the dependability of each modality under\nchallenging conditions; and the Confidence-Weighted Mutual Cross-Attention\n(CW-MCA) module, which dynamically balances information from LiDAR and camera\nmodalities based on these confidence scores. Experiments on the nuScenes\ndataset show that ReliFusion significantly outperforms state-of-the-art\nmethods, achieving superior robustness and accuracy in scenarios with limited\nLiDAR fields of view and severe sensor malfunctions."
    },
    {
        "date": "2025-02",
        "title": "Security and Quality in LLM-Generated Code: A Multi-Language, Multi-Model Analysis",
        "author": "Mohammed Kharma, Soohyeon Choi, Mohammed AlKhanafseh, and David Mohaisen",
        "link": "http://arxiv.org/abs/2502.01853v1",
        "abstract": "Artificial Intelligence (AI)-driven code generation tools are increasingly\nused throughout the software development lifecycle to accelerate coding tasks.\nHowever, the security of AI-generated code using Large Language Models (LLMs)\nremains underexplored, with studies revealing various risks and weaknesses.\nThis paper analyzes the security of code generated by LLMs across different\nprogramming languages. We introduce a dataset of 200 tasks grouped into six\ncategories to evaluate the performance of LLMs in generating secure and\nmaintainable code. Our research shows that while LLMs can automate code\ncreation, their security effectiveness varies by language. Many models fail to\nutilize modern security features in recent compiler and toolkit updates, such\nas Java 17. Moreover, outdated methods are still commonly used, particularly in\nC++. This highlights the need for advancing LLMs to enhance security and\nquality while incorporating emerging best practices in programming languages."
    },
    {
        "date": "2025-02",
        "title": "Preparing for Kyber in Securing Intelligent Transportation Systems Communications: A Case Study on Fault-Enabled Chosen-Ciphertext Attack",
        "author": "Kaiyuan Zhang, M Sabbir Salek, Antian Wang, Mizanur Rahman, Mashrur Chowdhury, and Yingjie Lao",
        "link": "http://arxiv.org/abs/2502.01848v1",
        "abstract": "Intelligent transportation systems (ITS) are characterized by wired or\nwireless communication among different entities, such as vehicles, roadside\ninfrastructure, and traffic management infrastructure. These communications\ndemand different levels of security, depending on how sensitive the data is.\nThe national ITS reference architecture (ARC-IT) defines three security levels,\ni.e., high, moderate, and low-security levels, based on the different security\nrequirements of ITS applications. In this study, we present a generalized\napproach to secure ITS communications using a standardized key encapsulation\nmechanism, known as Kyber, designed for post-quantum cryptography (PQC). We\nmodified the encryption and decryption systems for ITS communications while\nmapping the security levels of ITS applications to the three versions of Kyber,\ni.e., Kyber-512, Kyber-768, and Kyber-1024. Then, we conducted a case study\nusing a benchmark fault-enabled chosen-ciphertext attack to evaluate the\nsecurity provided by the different Kyber versions. The encryption and\ndecryption times observed for different Kyber security levels and the total\nnumber of iterations required to recover the secret key using the\nchosen-ciphertext attack are presented. Our analyses show that higher security\nlevels increase the time required for a successful attack, with Kyber-512 being\nbreached in 183 seconds, Kyber-768 in 337 seconds, and Kyber-1024 in 615\nseconds. In addition, attack time instabilities are observed for Kyber-512,\n768, and 1024 under 5,000, 6,000, and 8,000 inequalities, respectively. The\nrelationships among the different Kyber versions, and the respective attack\nrequirements and performances underscore the ITS communication security Kyber\ncould provide in the PQC era."
    },
    {
        "date": "2025-02",
        "title": "Efficient Denial of Service Attack Detection in IoT using Kolmogorov-Arnold Networks",
        "author": "Oleksandr Kuznetsov",
        "link": "http://arxiv.org/abs/2502.01835v1",
        "abstract": "The proliferation of Internet of Things (IoT) devices has created a pressing\nneed for efficient security solutions, particularly against Denial of Service\n(DoS) attacks. While existing detection approaches demonstrate high accuracy,\nthey often require substantial computational resources, making them impractical\nfor IoT deployment. This paper introduces a novel lightweight approach to DoS\nattack detection based on Kolmogorov-Arnold Networks (KANs). By leveraging\nspline-based transformations instead of traditional weight matrices, our\nsolution achieves state-of-the-art detection performance while maintaining\nminimal resource requirements. Experimental evaluation on the CICIDS2017\ndataset demonstrates 99.0% detection accuracy with only 0.19 MB memory\nfootprint and 2.00 ms inference time per sample. Compared to existing\nsolutions, KAN reduces memory requirements by up to 98% while maintaining\ncompetitive detection rates. The model's linear computational complexity\nensures efficient scaling with input size, making it particularly suitable for\nlarge-scale IoT deployments. We provide comprehensive performance comparisons\nwith recent approaches and demonstrate effectiveness across various DoS attack\npatterns. Our solution addresses the critical challenge of implementing\nsophisticated attack detection on resource-constrained devices, offering a\npractical approach to enhancing IoT security without compromising computational\nefficiency."
    },
    {
        "date": "2025-02",
        "title": "Firewalls to Secure Dynamic LLM Agentic Networks",
        "author": "Sahar Abdelnabi, Amr Gomaa, Eugene Bagdasarian, Per Ola Kristensson, and Reza Shokri",
        "link": "http://arxiv.org/abs/2502.01822v1",
        "abstract": "Future LLM agents are likely to communicate on behalf of users with other\nentity-representing agents on tasks that entail long-horizon plans with\ninterdependent goals. Current work does not focus on such agentic networks, nor\ndoes it address their challenges. Thus, we first identify the required\nproperties of agents' communication, which should be proactive and adaptable.\nIt needs to satisfy 1) privacy: agents should not share more than what is\nneeded for the task, and 2) security: the communication must preserve integrity\nand maintain utility against selfish entities. We design a use case (travel\nplanning) as a testbed that exemplifies these requirements, and we show\nexamples of how this can go wrong. Next, we propose a practical design,\ninspired by established network security principles, for constrained LLM\nagentic networks that balance adaptability, security, and privacy. Our\nframework automatically constructs and updates task-specific rules from prior\nsimulations to build firewalls. We offer layers of defense to 1) convert\nfree-form input to a task-specific protocol, 2) dynamically abstract users'\ndata to a task-specific degree of permissiveness, and 3) self-correct the\nagents' trajectory."
    },
    {
        "date": "2025-02",
        "title": "CTC-DRO: Robust Optimization for Reducing Language Disparities in Speech Recognition",
        "author": "Martijn Bartelds, Ananjan Nandi, Moussa Koulako Bala Doumbouya, Dan Jurafsky, Tatsunori Hashimoto, and Karen Livescu",
        "link": "http://arxiv.org/abs/2502.01777v1",
        "abstract": "Modern deep learning models often achieve high overall performance, but\nconsistently fail on specific subgroups. Group distributionally robust\noptimization (group DRO) addresses this problem by minimizing the worst-group\nloss, but it fails when group losses misrepresent performance differences\nbetween groups. This is common in domains like speech, where the widely used\nconnectionist temporal classification (CTC) loss scales with input length and\nvaries with linguistic and acoustic properties, leading to spurious differences\nbetween group losses. We present CTC-DRO, which addresses the shortcomings of\nthe group DRO objective by smoothing the group weight update to prevent\noveremphasis on consistently high-loss groups, while using input length-matched\nbatching to mitigate CTC's scaling issues. We evaluate CTC-DRO on the task of\nmultilingual automatic speech recognition (ASR) across five language sets from\nthe ML-SUPERB 2.0 benchmark. CTC-DRO consistently outperforms group DRO and\nCTC-based baseline models, reducing the worst-language error by up to 65.9% and\nthe average error by up to 47.7%. CTC-DRO can be applied to ASR with minimal\ncomputational costs, and offers the potential for reducing group disparities in\nother domains with similar challenges."
    },
    {
        "date": "2025-02",
        "title": "Robust Federated Finetuning of LLMs via Alternating Optimization of LoRA",
        "author": "Shuangyi Chen, Yuanxin Guo, Yue Ju, Harik Dalal, and Ashish Khisti",
        "link": "http://arxiv.org/abs/2502.01755v1",
        "abstract": "Parameter-Efficient Fine-Tuning (PEFT) methods like Low-Rank Adaptation\n(LoRA) optimize federated training by reducing computational and communication\ncosts. We propose RoLoRA, a federated framework using alternating optimization\nto fine-tune LoRA adapters. Our approach emphasizes the importance of learning\nup and down projection matrices to enhance expressiveness and robustness. We\nuse both theoretical analysis and extensive experiments to demonstrate the\nadvantages of RoLoRA over prior approaches that either generate imperfect model\nupdates or limit expressiveness of the model. We present theoretical analysis\non a simplified linear model to demonstrate the importance of learning both\ndown-projection and up-projection matrices in LoRA. We provide extensive\nexperimental evaluations on a toy neural network on MNIST as well as large\nlanguage models including RoBERTa-Large, Llama-2-7B on diverse tasks to\ndemonstrate the advantages of RoLoRA over other methods."
    },
    {
        "date": "2025-02",
        "title": "Adversarial Reasoning at Jailbreaking Time",
        "author": "Mahdi Sabbaghi, Paul Kassianik, George Pappas, Yaron Singer, Amin Karbasi, and Hamed Hassani",
        "link": "http://arxiv.org/abs/2502.01633v1",
        "abstract": "As large language models (LLMs) are becoming more capable and widespread, the\nstudy of their failure cases is becoming increasingly important. Recent\nadvances in standardizing, measuring, and scaling test-time compute suggest new\nmethodologies for optimizing models to achieve high performance on hard tasks.\nIn this paper, we apply these advances to the task of model jailbreaking:\neliciting harmful responses from aligned LLMs. We develop an adversarial\nreasoning approach to automatic jailbreaking via test-time computation that\nachieves SOTA attack success rates (ASR) against many aligned LLMs, even the\nones that aim to trade inference-time compute for adversarial robustness. Our\napproach introduces a new paradigm in understanding LLM vulnerabilities, laying\nthe foundation for the development of more robust and trustworthy AI systems."
    },
    {
        "date": "2025-02",
        "title": "Robust-LLaVA: On the Effectiveness of Large-Scale Robust Image Encoders for Multi-modal Large Language Models",
        "author": "Hashmat Shadab Malik, Fahad Shamshad, Muzammal Naseer, Karthik Nandakumar, Fahad Khan, and Salman Khan",
        "link": "http://arxiv.org/abs/2502.01576v1",
        "abstract": "Multi-modal Large Language Models (MLLMs) excel in vision-language tasks but\nremain vulnerable to visual adversarial perturbations that can induce\nhallucinations, manipulate responses, or bypass safety mechanisms. Existing\nmethods seek to mitigate these risks by applying constrained adversarial\nfine-tuning to CLIP vision encoders on ImageNet-scale data, ensuring their\ngeneralization ability is preserved. However, this limited adversarial training\nrestricts robustness and broader generalization. In this work, we explore an\nalternative approach of leveraging existing vision classification models that\nhave been adversarially pre-trained on large-scale data. Our analysis reveals\ntwo principal contributions: (1) the extensive scale and diversity of\nadversarial pre-training enables these models to demonstrate superior\nrobustness against diverse adversarial threats, ranging from imperceptible\nperturbations to advanced jailbreaking attempts, without requiring additional\nadversarial training, and (2) end-to-end MLLM integration with these robust\nmodels facilitates enhanced adaptation of language components to robust visual\nfeatures, outperforming existing plug-and-play methodologies on complex\nreasoning tasks. Through systematic evaluation across visual\nquestion-answering, image captioning, and jail-break attacks, we demonstrate\nthat MLLMs trained with these robust models achieve superior adversarial\nrobustness while maintaining favorable clean performance. Our framework\nachieves 2x and 1.5x average robustness gains in captioning and VQA tasks,\nrespectively, and delivers over 10% improvement against jailbreak attacks. Code\nand pretrained models will be available at\nhttps://github.com/HashmatShadab/Robust-LLaVA."
    },
    {
        "date": "2025-02",
        "title": "Search-Based Adversarial Estimates for Improving Sample Efficiency in Off-Policy Reinforcement Learning",
        "author": "Federico Malato, and Ville Hautamaki",
        "link": "http://arxiv.org/abs/2502.01558v1",
        "abstract": "Sample inefficiency is a long-lasting challenge in deep reinforcement\nlearning (DRL). Despite dramatic improvements have been made, the problem is\nfar from being solved and is especially challenging in environments with sparse\nor delayed rewards. In our work, we propose to use Adversarial Estimates as a\nnew, simple and efficient approach to mitigate this problem for a class of\nfeedback-based DRL algorithms. Our approach leverages latent similarity search\nfrom a small set of human-collected trajectories to boost learning, using only\nfive minutes of human-recorded experience. The results of our study show\nalgorithms trained with Adversarial Estimates converge faster than their\noriginal version. Moreover, we discuss how our approach could enable learning\nin feedback-based algorithms in extreme scenarios with very sparse rewards."
    },
    {
        "date": "2025-02",
        "title": "mWhisper-Flamingo for Multilingual Audio-Visual Noise-Robust Speech Recognition",
        "author": "Andrew Rouditchenko, Saurabhchand Bhati, Samuel Thomas, Hilde Kuehne, Rogerio Feris, and James Glass",
        "link": "http://arxiv.org/abs/2502.01547v1",
        "abstract": "Audio-Visual Speech Recognition (AVSR) combines lip-based video with audio\nand can improve performance in noise, but most methods are trained only on\nEnglish data. One limitation is the lack of large-scale multilingual video\ndata, which makes it hard hard to train models from scratch. In this work, we\npropose mWhisper-Flamingo for multilingual AVSR which combines the strengths of\na pre-trained audio model (Whisper) and video model (AV-HuBERT). To enable\nbetter multi-modal integration and improve the noisy multilingual performance,\nwe introduce decoder modality dropout where the model is trained both on paired\naudio-visual inputs and separate audio/visual inputs. mWhisper-Flamingo\nachieves state-of-the-art WER on MuAViC, an AVSR dataset of 9 languages.\nAudio-visual mWhisper-Flamingo consistently outperforms audio-only Whisper on\nall languages in noisy conditions."
    },
    {
        "date": "2025-02",
        "title": "A Multi-Scale Feature Fusion Framework Integrating Frequency Domain and Cross-View Attention for Dual-View X-ray Security Inspections",
        "author": "Shilong Hong, Yanzhou Zhou, and Weichao Xu",
        "link": "http://arxiv.org/abs/2502.01710v1",
        "abstract": "With the rapid development of modern transportation systems and the\nexponential growth of logistics volumes, intelligent X-ray-based security\ninspection systems play a crucial role in public safety. Although single-view\nX-ray equipment is widely deployed, it struggles to accurately identify\ncontraband in complex stacking scenarios due to strong viewpoint dependency and\ninadequate feature representation. To address this, we propose an innovative\nmulti-scale interactive feature fusion framework tailored for dual-view X-ray\nsecurity inspection image classification. The framework comprises three core\nmodules: the Frequency Domain Interaction Module (FDIM) enhances\nfrequency-domain features through Fourier transform; the Multi-Scale Cross-View\nFeature Enhancement (MSCFE) leverages cross-view attention mechanisms to\nstrengthen feature interactions; and the Convolutional Attention Fusion Module\n(CAFM) efficiently fuses features by integrating channel attention with\ndepthwise-separable convolutions. Experimental results demonstrate that our\nmethod outperforms existing state-of-the-art approaches across multiple\nbackbone architectures, particularly excelling in complex scenarios with\nocclusions and object stacking."
    },
    {
        "date": "2025-02",
        "title": "Topic-FlipRAG: Topic-Orientated Adversarial Opinion Manipulation Attacks to Retrieval-Augmented Generation Models",
        "author": "Yuyang Gong, Zhuo Chen, Miaokun Chen, Fengchang Yu, Wei Lu, Xiaofeng Wang, Xiaozhong Liu, and Jiawei Liu",
        "link": "http://arxiv.org/abs/2502.01386v1",
        "abstract": "Retrieval-Augmented Generation (RAG) systems based on Large Language Models\n(LLMs) have become essential for tasks such as question answering and content\ngeneration. However, their increasing impact on public opinion and information\ndissemination has made them a critical focus for security research due to\ninherent vulnerabilities. Previous studies have predominantly addressed attacks\ntargeting factual or single-query manipulations. In this paper, we address a\nmore practical scenario: topic-oriented adversarial opinion manipulation\nattacks on RAG models, where LLMs are required to reason and synthesize\nmultiple perspectives, rendering them particularly susceptible to systematic\nknowledge poisoning. Specifically, we propose Topic-FlipRAG, a two-stage\nmanipulation attack pipeline that strategically crafts adversarial\nperturbations to influence opinions across related queries. This approach\ncombines traditional adversarial ranking attack techniques and leverages the\nextensive internal relevant knowledge and reasoning capabilities of LLMs to\nexecute semantic-level perturbations. Experiments show that the proposed\nattacks effectively shift the opinion of the model's outputs on specific\ntopics, significantly impacting user information perception. Current mitigation\nmethods cannot effectively defend against such attacks, highlighting the\nnecessity for enhanced safeguards for RAG systems, and offering crucial\ninsights for LLM security research."
    },
    {
        "date": "2025-02",
        "title": "Metric Privacy in Federated Learning for Medical Imaging: Improving Convergence and Preventing Client Inference Attacks",
        "author": "Judith S\u00e1inz-Pardo D\u00edaz, Andreas Athanasiou, Kangsoo Jung, Catuscia Palamidessi, and \u00c1lvaro L\u00f3pez Garc\u00eda",
        "link": "http://arxiv.org/abs/2502.01352v1",
        "abstract": "Federated learning is a distributed learning technique that allows training a\nglobal model with the participation of different data owners without the need\nto share raw data. This architecture is orchestrated by a central server that\naggregates the local models from the clients. This server may be trusted, but\nnot all nodes in the network. Then, differential privacy (DP) can be used to\nprivatize the global model by adding noise. However, this may affect\nconvergence across the rounds of the federated architecture, depending also on\nthe aggregation strategy employed. In this work, we aim to introduce the notion\nof metric-privacy to mitigate the impact of classical server side global-DP on\nthe convergence of the aggregated model. Metric-privacy is a relaxation of DP,\nsuitable for domains provided with a notion of distance. We apply it from the\nserver side by computing a distance for the difference between the local\nmodels. We compare our approach with standard DP by analyzing the impact on six\nclassical aggregation strategies. The proposed methodology is applied to an\nexample of medical imaging and different scenarios are simulated across\nhomogeneous and non-i.i.d clients. Finally, we introduce a novel client\ninference attack, where a semi-honest client tries to find whether another\nclient participated in the training and study how it can be mitigated using DP\nand metric-privacy. Our evaluation shows that metric-privacy can increase the\nperformance of the model compared to standard DP, while offering similar\nprotection against client inference attacks."
    },
    {
        "date": "2025-02",
        "title": "A Statistical Learning Perspective on Semi-dual Adversarial Neural Optimal Transport Solvers",
        "author": "Roman Tarasov, Petr Mokrov, Milena Gazdieva, Evgeny Burnaev, and Alexander Korotin",
        "link": "http://arxiv.org/abs/2502.01310v1",
        "abstract": "Neural network based Optimal Transport (OT) is a recent and fruitful\ndirection in the generative modeling community. It finds its applications in\nvarious fields such as domain translation, image super-resolution,\ncomputational biology and others. Among the existing approaches to OT, of\nconsiderable interest are adversarial minimax solvers based on semi-dual\nformulations of OT problems. While promising, these methods lack theoretical\ninvestigation from a statistical learning perspective. Our work fills this gap\nby establishing upper bounds on the generalization error of an approximate OT\nmap recovered by the minimax quadratic OT solver. Importantly, the bounds we\nderive depend solely on some standard statistical and mathematical properties\nof the considered functional classes (neural networks). While our analysis\nfocuses on the quadratic OT, we believe that similar bounds could be derived\nfor more general OT formulations, paving the promising direction for future\nresearch."
    },
    {
        "date": "2025-02",
        "title": "Boosting Graph Robustness Against Backdoor Attacks: An Over-Similarity Perspective",
        "author": "Chang Liu, Hai Huang, Yujie Xing, and Xingquan Zuo",
        "link": "http://arxiv.org/abs/2502.01272v1",
        "abstract": "Graph Neural Networks (GNNs) have achieved notable success in tasks such as\nsocial and transportation networks. However, recent studies have highlighted\nthe vulnerability of GNNs to backdoor attacks, raising significant concerns\nabout their reliability in real-world applications. Despite initial efforts to\ndefend against specific graph backdoor attacks, existing defense methods face\ntwo main challenges: either the inability to establish a clear distinction\nbetween triggers and clean nodes, resulting in the removal of many clean nodes,\nor the failure to eliminate the impact of triggers, making it challenging to\nrestore the target nodes to their pre-attack state. Through empirical analysis\nof various existing graph backdoor attacks, we observe that the triggers\ngenerated by these methods exhibit over-similarity in both features and\nstructure. Based on this observation, we propose a novel graph backdoor defense\nmethod SimGuard. We first utilizes a similarity-based metric to detect triggers\nand then employs contrastive learning to train a backdoor detector that\ngenerates embeddings capable of separating triggers from clean nodes, thereby\nimproving detection efficiency. Extensive experiments conducted on real-world\ndatasets demonstrate that our proposed method effectively defends against\nvarious graph backdoor attacks while preserving performance on clean nodes. The\ncode will be released upon acceptance."
    },
    {
        "date": "2025-02",
        "title": "FSPGD: Rethinking Black-box Attacks on Semantic Segmentation",
        "author": "Eun-Sol Park, MiSo Park, Seung Park, and Yong-Goo Shin",
        "link": "http://arxiv.org/abs/2502.01262v1",
        "abstract": "Transferability, the ability of adversarial examples crafted for one model to\ndeceive other models, is crucial for black-box attacks. Despite advancements in\nattack methods for semantic segmentation, transferability remains limited,\nreducing their effectiveness in real-world applications. To address this, we\nintroduce the Feature Similarity Projected Gradient Descent (FSPGD) attack, a\nnovel black-box approach that enhances both attack performance and\ntransferability. Unlike conventional segmentation attacks that rely on output\npredictions for gradient calculation, FSPGD computes gradients from\nintermediate layer features. Specifically, our method introduces a loss\nfunction that targets local information by comparing features between clean\nimages and adversarial examples, while also disrupting contextual information\nby accounting for spatial relationships between objects. Experiments on Pascal\nVOC 2012 and Cityscapes datasets demonstrate that FSPGD achieves superior\ntransferability and attack performance, establishing a new state-of-the-art\nbenchmark. Code is available at https://github.com/KU-AIVS/FSPGD."
    },
    {
        "date": "2025-02",
        "title": "The dark deep side of DeepSeek: Fine-tuning attacks against the safety alignment of CoT-enabled models",
        "author": "Zhiyuan Xu, Joseph Gardiner, and Sana Belguith",
        "link": "http://arxiv.org/abs/2502.01225v1",
        "abstract": "Large language models are typically trained on vast amounts of data during\nthe pre-training phase, which may include some potentially harmful information.\nFine-tuning attacks can exploit this by prompting the model to reveal such\nbehaviours, leading to the generation of harmful content. In this paper, we\nfocus on investigating the performance of the Chain of Thought based reasoning\nmodel, DeepSeek, when subjected to fine-tuning attacks. Specifically, we\nexplore how fine-tuning manipulates the model's output, exacerbating the\nharmfulness of its responses while examining the interaction between the Chain\nof Thought reasoning and adversarial inputs. Through this study, we aim to shed\nlight on the vulnerability of Chain of Thought enabled models to fine-tuning\nattacks and the implications for their safety and ethical deployment."
    },
    {
        "date": "2025-02",
        "title": "On the Robustness of Temporal Factual Knowledge in Language Models",
        "author": "Hichem Ammar Khodja, Fr\u00e9d\u00e9ric B\u00e9chet, Quentin Brabant, Alexis Nasr, and Gw\u00e9nol\u00e9 Lecorv\u00e9",
        "link": "http://arxiv.org/abs/2502.01220v1",
        "abstract": "This paper explores the temporal robustness of language models (LMs) in\nhandling factual knowledge. While LMs can often complete simple factual\nstatements, their ability to manage temporal facts (those valid only within\nspecific timeframes) remains uncertain. We design a controlled experiment to\ntest the robustness of temporal factual knowledge inside LMs, which we use to\nevaluate several pretrained and instruction-tuned models using prompts on\npopular Wikidata facts, assessing their performance across different temporal\ngranularities (Day, Month, and Year). Our findings indicate that even very\nlarge state-of-the-art models, such as Llama-3.1-70B, vastly lack robust\nknowledge of temporal facts. In addition, they are incapable of generalizing\ntheir knowledge from one granularity to another. These results highlight the\ninherent limitations of using LMs as temporal knowledge bases. The source code\nand data to reproduce our experiments will be released."
    },
    {
        "date": "2025-02",
        "title": "Towards Robust and Reliable Concept Representations: Reliability-Enhanced Concept Embedding Model",
        "author": "Yuxuan Cai, Xiyu Wang, Satoshi Tsutsui, Winnie Pang, and Bihan Wen",
        "link": "http://arxiv.org/abs/2502.01191v1",
        "abstract": "Concept Bottleneck Models (CBMs) aim to enhance interpretability by\npredicting human-understandable concepts as intermediates for decision-making.\nHowever, these models often face challenges in ensuring reliable concept\nrepresentations, which can propagate to downstream tasks and undermine\nrobustness, especially under distribution shifts. Two inherent issues\ncontribute to concept unreliability: sensitivity to concept-irrelevant features\n(e.g., background variations) and lack of semantic consistency for the same\nconcept across different samples. To address these limitations, we propose the\nReliability-Enhanced Concept Embedding Model (RECEM), which introduces a\ntwo-fold strategy: Concept-Level Disentanglement to separate irrelevant\nfeatures from concept-relevant information and a Concept Mixup mechanism to\nensure semantic alignment across samples. These mechanisms work together to\nimprove concept reliability, enabling the model to focus on meaningful object\nattributes and generate faithful concept representations. Experimental results\ndemonstrate that RECEM consistently outperforms existing baselines across\nmultiple datasets, showing superior performance under background and domain\nshifts. These findings highlight the effectiveness of disentanglement and\nalignment strategies in enhancing both reliability and robustness in CBMs."
    },
    {
        "date": "2025-02",
        "title": "Enhancing Environmental Robustness in Few-shot Learning via Conditional Representation Learning",
        "author": "Qianyu Guo, Jingrong Wu, Tianxing Wu, Haofen Wang, Weifeng Ge, and Wenqiang Zhang",
        "link": "http://arxiv.org/abs/2502.01183v1",
        "abstract": "Few-shot learning (FSL) has recently been extensively utilized to overcome\nthe scarcity of training data in domain-specific visual recognition. In\nreal-world scenarios, environmental factors such as complex backgrounds,\nvarying lighting conditions, long-distance shooting, and moving targets often\ncause test images to exhibit numerous incomplete targets or noise disruptions.\nHowever, current research on evaluation datasets and methodologies has largely\nignored the concept of \"environmental robustness\", which refers to maintaining\nconsistent performance in complex and diverse physical environments. This\nneglect has led to a notable decline in the performance of FSL models during\npractical testing compared to their training performance. To bridge this gap,\nwe introduce a new real-world multi-domain few-shot learning (RD-FSL)\nbenchmark, which includes four domains and six evaluation datasets. The test\nimages in this benchmark feature various challenging elements, such as\ncamouflaged objects, small targets, and blurriness. Our evaluation experiments\nreveal that existing methods struggle to utilize training images effectively to\ngenerate accurate feature representations for challenging test images. To\naddress this problem, we propose a novel conditional representation learning\nnetwork (CRLNet) that integrates the interactions between training and testing\nimages as conditional information in their respective representation processes.\nThe main goal is to reduce intra-class variance or enhance inter-class variance\nat the feature representation level. Finally, comparative experiments reveal\nthat CRLNet surpasses the current state-of-the-art methods, achieving\nperformance improvements ranging from 6.83% to 16.98% across diverse settings\nand backbones. The source code and dataset are available at\nhttps://github.com/guoqianyu-alberta/Conditional-Representation-Learning."
    },
    {
        "date": "2025-02",
        "title": "Gradient Norm-based Fine-Tuning for Backdoor Defense in Automatic Speech Recognition",
        "author": "Nanjun Zhou, Weilin Lin, and Li Liu",
        "link": "http://arxiv.org/abs/2502.01152v1",
        "abstract": "Backdoor attacks have posed a significant threat to the security of deep\nneural networks (DNNs). Despite considerable strides in developing defenses\nagainst backdoor attacks in the visual domain, the specialized defenses for the\naudio domain remain empty. Furthermore, the defenses adapted from the visual to\naudio domain demonstrate limited effectiveness. To fill this gap, we propose\nGradient Norm-based FineTuning (GN-FT), a novel defense strategy against the\nattacks in the audio domain, based on the observation from the corresponding\nbackdoored models. Specifically, we first empirically find that the backdoored\nneurons exhibit greater gradient values compared to other neurons, while clean\nneurons stay the lowest. On this basis, we fine-tune the backdoored model by\nincorporating the gradient norm regularization, aiming to weaken and reduce the\nbackdoored neurons. We further approximate the loss computation for lower\nimplementation costs. Extensive experiments on two speech recognition datasets\nacross five models demonstrate the superior performance of our proposed method.\nTo the best of our knowledge, this work is the first specialized and effective\ndefense against backdoor attacks in the audio domain."
    },
    {
        "date": "2025-02",
        "title": "Towards Robust and Generalizable Lensless Imaging with Modular Learned Reconstruction",
        "author": "Eric Bezzam, Yohann Perron, and Martin Vetterli",
        "link": "http://arxiv.org/abs/2502.01102v1",
        "abstract": "Lensless cameras disregard the conventional design that imaging should mimic\nthe human eye. This is done by replacing the lens with a thin mask, and moving\nimage formation to the digital post-processing. State-of-the-art lensless\nimaging techniques use learned approaches that combine physical modeling and\nneural networks. However, these approaches make simplifying modeling\nassumptions for ease of calibration and computation. Moreover, the\ngeneralizability of learned approaches to lensless measurements of new masks\nhas not been studied. To this end, we utilize a modular learned reconstruction\nin which a key component is a pre-processor prior to image recovery. We\ntheoretically demonstrate the pre-processor's necessity for standard image\nrecovery techniques (Wiener filtering and iterative algorithms), and through\nextensive experiments show its effectiveness for multiple lensless imaging\napproaches and across datasets of different mask types (amplitude and phase).\nWe also perform the first generalization benchmark across mask types to\nevaluate how well reconstructions trained with one system generalize to others.\nOur modular reconstruction enables us to use pre-trained components and\ntransfer learning on new systems to cut down weeks of tedious measurements and\ntraining. As part of our work, we open-source four datasets, and software for\nmeasuring datasets and for training our modular reconstruction."
    },
    {
        "date": "2025-02",
        "title": "BC-GAN: A Generative Adversarial Network for Synthesizing a Batch of Collocated Clothing",
        "author": "Dongliang Zhou, Haijun Zhang, Jianghong Ma, and Jianyang Shi",
        "link": "http://arxiv.org/abs/2502.01080v1",
        "abstract": "Collocated clothing synthesis using generative networks has become an\nemerging topic in the field of fashion intelligence, as it has significant\npotential economic value to increase revenue in the fashion industry. In\nprevious studies, several works have attempted to synthesize\nvisually-collocated clothing based on a given clothing item using generative\nadversarial networks (GANs) with promising results. These works, however, can\nonly accomplish the synthesis of one collocated clothing item each time.\nNevertheless, users may require different clothing items to meet their multiple\nchoices due to their personal tastes and different dressing scenarios. To\naddress this limitation, we introduce a novel batch clothing generation\nframework, named BC-GAN, which is able to synthesize multiple\nvisually-collocated clothing images simultaneously. In particular, to further\nimprove the fashion compatibility of synthetic results, BC-GAN proposes a new\nfashion compatibility discriminator in a contrastive learning perspective by\nfully exploiting the collocation relationship among all clothing items. Our\nmodel was examined in a large-scale dataset with compatible outfits constructed\nby ourselves. Extensive experiment results confirmed the effectiveness of our\nproposed BC-GAN in comparison to state-of-the-art methods in terms of\ndiversity, visual authenticity, and fashion compatibility."
    },
    {
        "date": "2025-02",
        "title": "Adversarial Robustness in Two-Stage Learning-to-Defer: Algorithms and Guarantees",
        "author": "Yannis Montreuil, Axel Carlier, Lai Xing Ng, and Wei Tsang Ooi",
        "link": "http://arxiv.org/abs/2502.01027v1",
        "abstract": "Learning-to-Defer (L2D) facilitates optimal task allocation between AI\nsystems and decision-makers. Despite its potential, we show that current\ntwo-stage L2D frameworks are highly vulnerable to adversarial attacks, which\ncan misdirect queries or overwhelm decision agents, significantly degrading\nsystem performance. This paper conducts the first comprehensive analysis of\nadversarial robustness in two-stage L2D frameworks. We introduce two novel\nattack strategies -- untargeted and targeted -- that exploit inherent\nstructural vulnerabilities in these systems. To mitigate these threats, we\npropose SARD, a robust, convex, deferral algorithm rooted in Bayes and\n$(\\mathcal{R},\\mathcal{G})$-consistency. Our approach guarantees optimal task\nallocation under adversarial perturbations for all surrogates in the\ncross-entropy family. Extensive experiments on classification, regression, and\nmulti-task benchmarks validate the robustness of SARD."
    },
    {
        "date": "2025-02",
        "title": "Secure & Personalized Music-to-Video Generation via CHARCHA",
        "author": "Mehul Agarwal, Gauri Agarwal, Santiago Benoit, Andrew Lippman, and Jean Oh",
        "link": "http://arxiv.org/abs/2502.02610v1",
        "abstract": "Music is a deeply personal experience and our aim is to enhance this with a\nfully-automated pipeline for personalized music video generation. Our work\nallows listeners to not just be consumers but co-creators in the music video\ngeneration process by creating personalized, consistent and context-driven\nvisuals based on lyrics, rhythm and emotion in the music. The pipeline combines\nmultimodal translation and generation techniques and utilizes low-rank\nadaptation on listeners' images to create immersive music videos that reflect\nboth the music and the individual. To ensure the ethical use of users'\nidentity, we also introduce CHARCHA (patent pending), a facial identity\nverification protocol that protects people against unauthorized use of their\nface while at the same time collecting authorized images from users for\npersonalizing their videos. This paper thus provides a secure and innovative\nframework for creating deeply personalized music videos."
    },
    {
        "date": "2025-02",
        "title": "Detection of Distributed Denial of Service Attacks based on Machine Learning Algorithms",
        "author": "Md. Abdur Rahman",
        "link": "http://arxiv.org/abs/2502.00975v1",
        "abstract": "Distributed Denial of Service (DDoS) attacks make the challenges to provide\nthe services of the data resources to the web clients. In this paper, we\nconcern to study and apply different Machine Learning (ML) techniques to\nseparate the DDoS attack instances from benign instances. Our experimental\nresults show that forward and backward data bytes of our dataset are observed\nmore similar for DDoS attacks compared to the data bytes for benign attempts.\nThis paper uses different machine learning techniques for the detection of the\nattacks efficiently in order to make sure the offered services from web servers\navailable. This results from the proposed approach suggest that 97.1% of DDoS\nattacks are successfully detected by the Support Vector Machine (SVM). These\naccuracies are better while comparing to the several existing machine learning\napproaches."
    },
    {
        "date": "2025-02",
        "title": "AI-Powered Spearphishing Cyber Attacks: Fact or Fiction?",
        "author": "Matthew Kemp, Harsha Kalutarage, and M. Omar Al-Kadri",
        "link": "http://arxiv.org/abs/2502.00961v1",
        "abstract": "Due to society's continuing technological advance, the capabilities of\nmachine learning-based artificial intelligence systems continue to expand and\ninfluence a wider degree of topics. Alongside this expansion of technology,\nthere is a growing number of individuals willing to misuse these systems to\ndefraud and mislead others. Deepfake technology, a set of deep learning\nalgorithms that are capable of replacing the likeness or voice of one\nindividual with another with alarming accuracy, is one of these technologies.\nThis paper investigates the threat posed by malicious use of this technology,\nparticularly in the form of spearphishing attacks. It uses deepfake technology\nto create spearphishing-like attack scenarios and validate them against average\nindividuals. Experimental results show that 66% of participants failed to\nidentify AI created audio as fake while 43% failed to identify such videos as\nfake, confirming the growing fear of threats posed by the use of these\ntechnologies by cybercriminals."
    },
    {
        "date": "2025-02",
        "title": "SecPE: Secure Prompt Ensembling for Private and Robust Large Language Models",
        "author": "Jiawen Zhang, Kejia Chen, Zunlei Feng, Jian Lou, Mingli Song, Jian Liu, and Xiaohu Yang",
        "link": "http://arxiv.org/abs/2502.00847v1",
        "abstract": "With the growing popularity of LLMs among the general public users,\nprivacy-preserving and adversarial robustness have become two pressing demands\nfor LLM-based services, which have largely been pursued separately but rarely\njointly. In this paper, to the best of our knowledge, we are among the first\nattempts towards robust and private LLM inference by tightly integrating two\ndisconnected fields: private inference and prompt ensembling. The former\nprotects users' privacy by encrypting inference data transmitted and processed\nby LLMs, while the latter enhances adversarial robustness by yielding an\naggregated output from multiple prompted LLM responses. Although widely\nrecognized as effective individually, private inference for prompt ensembling\ntogether entails new challenges that render the naive combination of existing\ntechniques inefficient. To overcome the hurdles, we propose SecPE, which\ndesigns efficient fully homomorphic encryption (FHE) counterparts for the core\nalgorithmic building blocks of prompt ensembling. We conduct extensive\nexperiments on 8 tasks to evaluate the accuracy, robustness, and efficiency of\nSecPE. The results show that SecPE maintains high clean accuracy and offers\nbetter robustness at the expense of merely $2.5\\%$ efficiency overhead compared\nto baseline private inference methods, indicating a satisfactory\n``accuracy-robustness-efficiency'' tradeoff. For the efficiency of the\nencrypted Argmax operation that incurs major slowdown for prompt ensembling,\nSecPE is 35.4x faster than the state-of-the-art peers, which can be of\nindependent interest beyond this work."
    },
    {
        "date": "2025-02",
        "title": "Federated Generalised Variational Inference: A Robust Probabilistic Federated Learning Framework",
        "author": "Terje Mildner, Oliver Hamelijnck, Paris Giampouras, and Theodoros Damoulas",
        "link": "http://arxiv.org/abs/2502.00846v1",
        "abstract": "We introduce FedGVI, a probabilistic Federated Learning (FL) framework that\nis provably robust to both prior and likelihood misspecification. FedGVI\naddresses limitations in both frequentist and Bayesian FL by providing unbiased\npredictions under model misspecification, with calibrated uncertainty\nquantification. Our approach generalises previous FL approaches, specifically\nPartitioned Variational Inference (Ashman et al., 2022), by allowing robust and\nconjugate updates, decreasing computational complexity at the clients. We offer\ntheoretical analysis in terms of fixed-point convergence, optimality of the\ncavity distribution, and provable robustness. Additionally, we empirically\ndemonstrate the effectiveness of FedGVI in terms of improved robustness and\npredictive performance on multiple synthetic and real world classification data\nsets."
    },
    {
        "date": "2025-02",
        "title": "Activation Approximations Can Incur Safety Vulnerabilities Even in Aligned LLMs: Comprehensive Analysis and Defense",
        "author": "Jiawen Zhang, Kejia Chen, Lipeng He, Jian Lou, Dan Li, Zunlei Feng, Mingli Song, Jian Liu, Kui Ren, and Xiaohu Yang",
        "link": "http://arxiv.org/abs/2502.00840v1",
        "abstract": "Large Language Models (LLMs) have showcased remarkable capabilities across\nvarious domains. Accompanying the evolving capabilities and expanding\ndeployment scenarios of LLMs, their deployment challenges escalate due to their\nsheer scale and the advanced yet complex activation designs prevalent in\nnotable model series, such as Llama, Gemma, and Mistral. These challenges have\nbecome particularly pronounced in resource-constrained deployment scenarios,\nwhere mitigating inference efficiency bottlenecks is imperative. Among various\nrecent efforts, activation approximation has emerged as a promising avenue for\npursuing inference efficiency, sometimes considered indispensable in\napplications such as private inference. Despite achieving substantial speedups\nwith minimal impact on utility, even appearing sound and practical for\nreal-world deployment, the safety implications of activation approximations\nremain unclear. In this work, we fill this critical gap in LLM safety by\nconducting the first systematic safety evaluation of activation approximations.\nOur safety vetting spans seven sota techniques across three popular categories,\nrevealing consistent safety degradation across ten safety-aligned LLMs."
    },
    {
        "date": "2025-02",
        "title": "Boosting Adversarial Robustness and Generalization with Structural Prior",
        "author": "Zhichao Hou, Weizhi Gao, Hamid Krim, and Xiaorui Liu",
        "link": "http://arxiv.org/abs/2502.00834v1",
        "abstract": "This work investigates a novel approach to boost adversarial robustness and\ngeneralization by incorporating structural prior into the design of deep\nlearning models. Specifically, our study surprisingly reveals that existing\ndictionary learning-inspired convolutional neural networks (CNNs) provide a\nfalse sense of security against adversarial attacks. To address this, we\npropose Elastic Dictionary Learning Networks (EDLNets), a novel ResNet\narchitecture that significantly enhances adversarial robustness and\ngeneralization. This novel and effective approach is supported by a theoretical\nrobustness analysis using influence functions. Moreover, extensive and reliable\nexperiments demonstrate consistent and significant performance improvement on\nopen robustness leaderboards such as RobustBench, surpassing state-of-the-art\nbaselines. To the best of our knowledge, this is the first work to discover and\nvalidate that structural prior can reliably enhance deep learning robustness\nunder strong adaptive attacks, unveiling a promising direction for future\nresearch."
    },
    {
        "date": "2025-02",
        "title": "Adversarial Semantic Augmentation for Training Generative Adversarial Networks under Limited Data",
        "author": "Mengping Yang, Zhe Wang, Ziqiu Chi, Dongdong Li, and Wenli Du",
        "link": "http://arxiv.org/abs/2502.00800v1",
        "abstract": "Generative adversarial networks (GANs) have made remarkable achievements in\nsynthesizing images in recent years. Typically, training GANs requires massive\ndata, and the performance of GANs deteriorates significantly when training data\nis limited. To improve the synthesis performance of GANs in low-data regimes,\nexisting approaches use various data augmentation techniques to enlarge the\ntraining sets. However, it is identified that these augmentation techniques may\nleak or even alter the data distribution. To remedy this, we propose an\nadversarial semantic augmentation (ASA) technique to enlarge the training data\nat the semantic level instead of the image level. Concretely, considering\nsemantic features usually encode informative information of images, we estimate\nthe covariance matrices of semantic features for both real and generated images\nto find meaningful transformation directions. Such directions translate\noriginal features to another semantic representation, e.g., changing the\nbackgrounds or expressions of the human face dataset. Moreover, we derive an\nupper bound of the expected adversarial loss. By optimizing the upper bound,\nour semantic augmentation is implicitly achieved. Such design avoids redundant\nsampling of the augmented features and introduces negligible computation\noverhead, making our approach computation efficient. Extensive experiments on\nboth few-shot and large-scale datasets demonstrate that our method consistently\nimprove the synthesis quality under various data regimes, and further\nvisualized and analytic results suggesting satisfactory versatility of our\nproposed method."
    },
    {
        "date": "2025-02",
        "title": "From Compliance to Exploitation: Jailbreak Prompt Attacks on Multimodal LLMs",
        "author": "Chun Wai Chiu, Linghan Huang, Bo Li, and Huaming Chen",
        "link": "http://arxiv.org/abs/2502.00735v1",
        "abstract": "Large Language Models (LLMs) have seen widespread applications across various\ndomains due to their growing ability to process diverse types of input data,\nincluding text, audio, image and video. While LLMs have demonstrated\noutstanding performance in understanding and generating contexts for different\nscenarios, they are vulnerable to prompt-based attacks, which are mostly via\ntext input. In this paper, we introduce the first voice-based jailbreak attack\nagainst multimodal LLMs, termed as Flanking Attack, which can process different\ntypes of input simultaneously towards the multimodal LLMs. Our work is\nmotivated by recent advancements in monolingual voice-driven large language\nmodels, which have introduced new attack surfaces beyond traditional text-based\nvulnerabilities for LLMs. To investigate these risks, we examine the frontier\nmultimodal LLMs, which can be accessed via different types of inputs such as\naudio input, focusing on how adversarial prompts can bypass its defense\nmechanisms. We propose a novel strategy, in which the disallowed prompt is\nflanked by benign, narrative-driven prompts. It is integrated in the Flanking\nAttack which attempts to humanizes the interaction context and execute the\nattack through a fictional setting. To better evaluate the attack performance,\nwe present a semi-automated self-assessment framework for policy violation\ndetection. We demonstrate that Flank Attack is capable of manipulating\nstate-of-the-art LLMs into generating misaligned and forbidden outputs, which\nachieves an average attack success rate ranging from 0.67 to 0.93 across seven\nforbidden scenarios. These findings highlight both the potency of prompt-based\nobfuscation in voice-enabled contexts and the limitations of current LLMs'\nmoderation safeguards and the urgent need for advanced defense strategies to\naddress the challenges posed by evolving, context-rich attacks."
    },
    {
        "date": "2025-02",
        "title": "\"I am bad\": Interpreting Stealthy, Universal and Robust Audio Jailbreaks in Audio-Language Models",
        "author": "Isha Gupta, David Khachaturov, and Robert Mullins",
        "link": "http://arxiv.org/abs/2502.00718v1",
        "abstract": "The rise of multimodal large language models has introduced innovative\nhuman-machine interaction paradigms but also significant challenges in machine\nlearning safety. Audio-Language Models (ALMs) are especially relevant due to\nthe intuitive nature of spoken communication, yet little is known about their\nfailure modes. This paper explores audio jailbreaks targeting ALMs, focusing on\ntheir ability to bypass alignment mechanisms. We construct adversarial\nperturbations that generalize across prompts, tasks, and even base audio\nsamples, demonstrating the first universal jailbreaks in the audio modality,\nand show that these remain effective in simulated real-world conditions. Beyond\ndemonstrating attack feasibility, we analyze how ALMs interpret these audio\nadversarial examples and reveal them to encode imperceptible first-person toxic\nspeech - suggesting that the most effective perturbations for eliciting toxic\noutputs specifically embed linguistic features within the audio signal. These\nresults have important implications for understanding the interactions between\ndifferent modalities in multimodal models, and offer actionable insights for\nenhancing defenses against adversarial audio attacks."
    },
    {
        "date": "2025-02",
        "title": "DPBloomfilter: Securing Bloom Filters with Differential Privacy",
        "author": "Yekun Ke, Yingyu Liang, Zhizhou Sha, Zhenmei Shi, and Zhao Song",
        "link": "http://arxiv.org/abs/2502.00693v1",
        "abstract": "The Bloom filter is a simple yet space-efficient probabilistic data structure\nthat supports membership queries for dramatically large datasets. It is widely\nutilized and implemented across various industrial scenarios, often handling\nmassive datasets that include sensitive user information necessitating privacy\npreservation. To address the challenge of maintaining privacy within the Bloom\nfilter, we have developed the DPBloomfilter. This innovation integrates the\nclassical differential privacy mechanism, specifically the Random Response\ntechnique, into the Bloom filter, offering robust privacy guarantees under the\nsame running complexity as the standard Bloom filter. Through rigorous\nsimulation experiments, we have demonstrated that our DPBloomfilter algorithm\nmaintains high utility while ensuring privacy protections. To the best of our\nknowledge, this is the first work to provide differential privacy guarantees\nfor the Bloom filter for membership query problems."
    },
    {
        "date": "2025-02",
        "title": "Towards Robust Multimodal Large Language Models Against Jailbreak Attacks",
        "author": "Ziyi Yin, Yuanpu Cao, Han Liu, Ting Wang, Jinghui Chen, and Fenhlong Ma",
        "link": "http://arxiv.org/abs/2502.00653v1",
        "abstract": "While multimodal large language models (MLLMs) have achieved remarkable\nsuccess in recent advancements, their susceptibility to jailbreak attacks has\ncome to light. In such attacks, adversaries exploit carefully crafted prompts\nto coerce models into generating harmful or undesirable content. Existing\ndefense mechanisms often rely on external inference steps or safety alignment\ntraining, both of which are less effective and impractical when facing\nsophisticated adversarial perturbations in white-box scenarios. To address\nthese challenges and bolster MLLM robustness, we introduce SafeMLLM by adopting\nan adversarial training framework that alternates between an attack step for\ngenerating adversarial noise and a model updating step. At the attack step,\nSafeMLLM generates adversarial perturbations through a newly proposed\ncontrastive embedding attack (CoE-Attack), which optimizes token embeddings\nunder a contrastive objective. SafeMLLM then updates model parameters to\nneutralize the perturbation effects while preserving model utility on benign\ninputs. We evaluate SafeMLLM across six MLLMs and six jailbreak methods\nspanning multiple modalities. Experimental results show that SafeMLLM\neffectively defends against diverse attacks, maintaining robust performance and\nutilities."
    },
    {
        "date": "2025-02",
        "title": "Integrating Cybersecurity Frameworks into IT Security: A Comprehensive Analysis of Threat Mitigation Strategies and Adaptive Technologies",
        "author": "Amit Lokare, Shripad Bankar, and Padmajeet Mhaske",
        "link": "http://arxiv.org/abs/2502.00651v1",
        "abstract": "The cybersecurity threat landscape is constantly actively making it\nimperative to develop sound frameworks to protect the IT structures. Based on\nthis introduction, this paper aims to discuss the application of cybersecurity\nframeworks into the IT security with focus placed on the role of such\nframeworks in addressing the changing nature of cybersecurity threats. It\nexplores widely used models, including the NIST Cybersecurity Framework, Zero\nTrust Architecture, and the ISO/IEC 27001, and how they apply to industries\nincluding finance, healthcare and government. The discussion also singles out\nsuch technologies as Artificial Intelligence (AI) and Machine Learning (ML) as\nthe core for real-time threat detection and response mechanisms. As these\nintegration challenges demonstrate, the study provides tangible and proven\napproaches to tackle framework implementation issues such as legitimate\nsecurity issues, limited availability of funds and resources, and compliance\nwith legal requirements. By capturing current trends and exposures, the\nfindings promote strong, portfolio-based and risk-appropriate security\napproaches adjusted for organizational goals and capable to prevent advanced\ncyber threats."
    },
    {
        "date": "2025-02",
        "title": "TrojanTime: Backdoor Attacks on Time Series Classification",
        "author": "Chang Dong, Zechao Sun, Guangdong Bai, Shuying Piao, Weitong Chen, and Wei Emma Zhang",
        "link": "http://arxiv.org/abs/2502.00646v1",
        "abstract": "Time Series Classification (TSC) is highly vulnerable to backdoor attacks,\nposing significant security threats. Existing methods primarily focus on data\npoisoning during the training phase, designing sophisticated triggers to\nimprove stealthiness and attack success rate (ASR). However, in practical\nscenarios, attackers often face restrictions in accessing training data.\nMoreover, it is a challenge for the model to maintain generalization ability on\nclean test data while remaining vulnerable to poisoned inputs when data is\ninaccessible. To address these challenges, we propose TrojanTime, a novel\ntwo-step training algorithm. In the first stage, we generate a pseudo-dataset\nusing an external arbitrary dataset through target adversarial attacks. The\nclean model is then continually trained on this pseudo-dataset and its poisoned\nversion. To ensure generalization ability, the second stage employs a carefully\ndesigned training strategy, combining logits alignment and batch norm freezing.\nWe evaluate TrojanTime using five types of triggers across four TSC\narchitectures in UCR benchmark datasets from diverse domains. The results\ndemonstrate the effectiveness of TrojanTime in executing backdoor attacks while\nmaintaining clean accuracy. Finally, to mitigate this threat, we propose a\ndefensive unlearning strategy that effectively reduces the ASR while preserving\nclean accuracy."
    },
    {
        "date": "2025-02",
        "title": "DesCLIP: Robust Continual Adaptation via General Attribute Descriptions for Pretrained Vision-Language Models",
        "author": "Chiyuan He, Zihuan Qiu, Fanman Meng, Linfeng Xu, Qingbo Wu, and Hongliang Li",
        "link": "http://arxiv.org/abs/2502.00618v1",
        "abstract": "Continual adaptation of vision-language models (VLMs) focuses on leveraging\ncross-modal pretrained knowledge to incrementally adapt for expanding\ndownstream tasks and datasets, while tackling the challenge of knowledge\nforgetting. Existing research often focuses on connecting visual features with\nspecific class text in downstream tasks, overlooking the latent relationships\nbetween general and specialized knowledge. Our findings reveal that forcing\nmodels to optimize inappropriate visual-text matches exacerbates forgetting of\nVLMs. To tackle this issue, we propose DesCLIP, which leverages general\nattribute (GA) descriptions to guide the understanding of specific class\nobjects, enabling VLMs to establish robust \\textit{vision-GA-class} trilateral\nassociations rather than relying solely on \\textit{vision-class} connections.\nSpecifically, we introduce a language assistant to generate concrete GA\ndescription candidates via proper request prompts. Then, an anchor-based\nembedding filter is designed to obtain highly relevant GA description\nembeddings, which are leveraged as the paired text embeddings for\nvisual-textual instance matching, thereby tuning the visual encoder.\nCorrespondingly, the class text embeddings are gradually calibrated to align\nwith these shared GA description embeddings. Extensive experiments demonstrate\nthe advancements and efficacy of our proposed method, with comprehensive\nempirical evaluations highlighting its superior performance compared to\nexisting pretrained and VLM-based continual learning methods."
    },
    {
        "date": "2025-02",
        "title": "Robust Knowledge Distillation in Federated Learning: Counteracting Backdoor Attacks",
        "author": "Ebtisaam Alharbi, Leandro Soriano Marcolino, Qiang Ni, and Antonios Gouglidis",
        "link": "http://arxiv.org/abs/2502.00587v1",
        "abstract": "Federated Learning (FL) enables collaborative model training across multiple\ndevices while preserving data privacy. However, it remains susceptible to\nbackdoor attacks, where malicious participants can compromise the global model.\nExisting defence methods are limited by strict assumptions on data\nheterogeneity (Non-Independent and Identically Distributed data) and the\nproportion of malicious clients, reducing their practicality and effectiveness.\nTo overcome these limitations, we propose Robust Knowledge Distillation (RKD),\na novel defence mechanism that enhances model integrity without relying on\nrestrictive assumptions. RKD integrates clustering and model selection\ntechniques to identify and filter out malicious updates, forming a reliable\nensemble of models. It then employs knowledge distillation to transfer the\ncollective insights from this ensemble to a global model. Extensive evaluations\ndemonstrate that RKD effectively mitigates backdoor threats while maintaining\nhigh model performance, outperforming current state-of-the-art defence methods\nacross various scenarios."
    },
    {
        "date": "2025-02",
        "title": "Defense Against the Dark Prompts: Mitigating Best-of-N Jailbreaking with Prompt Evaluation",
        "author": "Stuart Armstrong, Matija Franklin, Connor Stevens, and Rebecca Gorman",
        "link": "http://arxiv.org/abs/2502.00580v1",
        "abstract": "Recent work showed Best-of-N (BoN) jailbreaking using repeated use of random\naugmentations (such as capitalization, punctuation, etc) is effective against\nall major large language models (LLMs). We have found that $100\\%$ of the BoN\npaper's successful jailbreaks (confidence interval $[99.65\\%, 100.00\\%]$) and\n$99.8\\%$ of successful jailbreaks in our replication (confidence interval\n$[99.28\\%, 99.98\\%]$) were blocked with our Defense Against The Dark Prompts\n(DATDP) method. The DATDP algorithm works by repeatedly utilizing an evaluation\nLLM to evaluate a prompt for dangerous or manipulative behaviors--unlike some\nother approaches, DATDP also explicitly looks for jailbreaking attempts--until\na robust safety rating is generated. This success persisted even when utilizing\nsmaller LLMs to power the evaluation (Claude and LLaMa-3-8B-instruct proved\nalmost equally capable). These results show that, though language models are\nsensitive to seemingly innocuous changes to inputs, they seem also capable of\nsuccessfully evaluating the dangers of these inputs. Versions of DATDP can\ntherefore be added cheaply to generative AI systems to produce an immediate\nsignificant increase in safety."
    },
    {
        "date": "2025-02",
        "title": "Doubly Robust Monte Carlo Tree Search",
        "author": "Manqing Liu, and Andrew L. Beam",
        "link": "http://arxiv.org/abs/2502.01672v1",
        "abstract": "We present Doubly Robust Monte Carlo Tree Search (DR-MCTS), a novel algorithm\nthat integrates Doubly Robust (DR) off-policy estimation into Monte Carlo Tree\nSearch (MCTS) to enhance sample efficiency and decision quality in complex\nenvironments. Our approach introduces a hybrid estimator that combines MCTS\nrollouts with DR estimation, offering theoretical guarantees of unbiasedness\nand variance reduction under specified conditions. Empirical evaluations in\nTic-Tac-Toe and the partially observable VirtualHome environment demonstrate\nDR-MCTS's superior performance over standard MCTS. In Tic-Tac-Toe, DR-MCTS\nachieves an 88% win rate compared to a 10% win rate for standard MCTS. In\ncompound VirtualHome tasks, DR-MCTS attains a 20.7% success rate versus 10.3%\nfor standard MCTS. Our scaling analysis reveals that DR-MCTS exhibits better\nsample efficiency, notably outperforming standard MCTS with larger language\nmodels while using a smaller model. These results underscore DR-MCTS's\npotential for efficient decision-making in complex, real-world scenarios where\nsample efficiency is paramount."
    },
    {
        "date": "2025-02",
        "title": "Data Overvaluation Attack and Truthful Data Valuation",
        "author": "Shuyuan Zheng, Sudong Cai, Chuan Xiao, Yang Cao, Jianbin Qin, Masatoshi Yoshikawa, and Makoto Onizuka",
        "link": "http://arxiv.org/abs/2502.00494v2",
        "abstract": "In collaborative machine learning, data valuation, i.e., evaluating the\ncontribution of each client' data to the machine learning model, has become a\ncritical task for incentivizing and selecting positive data contributions.\nHowever, existing studies often assume that clients engage in data valuation\ntruthfully, overlooking the practical motivation for clients to exaggerate\ntheir contributions. To unlock this threat, this paper introduces the first\ndata overvaluation attack, enabling strategic clients to have their data\nsignificantly overvalued. Furthermore, we propose a truthful data valuation\nmetric, named Truth-Shapley. Truth-Shapley is the unique metric that guarantees\nsome promising axioms for data valuation while ensuring that clients' optimal\nstrategy is to perform truthful data valuation. Our experiments demonstrate the\nvulnerability of existing data valuation metrics to the data overvaluation\nattack and validate the robustness and effectiveness of Truth-Shapley."
    },
    {
        "date": "2025-02",
        "title": "Oscillations Make Neural Networks Robust to Quantization",
        "author": "Jonathan Wensh\u00f8j, Bob Pepin, and Raghavendra Selvan",
        "link": "http://arxiv.org/abs/2502.00490v1",
        "abstract": "We challenge the prevailing view that oscillations in Quantization Aware\nTraining (QAT) are merely undesirable artifacts caused by the Straight-Through\nEstimator (STE). Through theoretical analysis of QAT in linear models, we\ndemonstrate that the gradient of the loss function can be decomposed into two\nterms: the original full-precision loss and a term that causes quantization\noscillations. Based on these insights, we propose a novel regularization method\nthat induces oscillations to improve quantization robustness. Contrary to\ntraditional methods that focuses on minimizing the effects of oscillations, our\napproach leverages the beneficial aspects of weight oscillations to preserve\nmodel performance under quantization. Our empirical results on ResNet-18 and\nTiny ViT demonstrate that this counter-intuitive strategy matches QAT accuracy\nat >= 3-bit weight quantization, while maintaining close to full precision\naccuracy at bits greater than the target bit. Our work therefore provides a new\nperspective on model preparation for quantization, particularly for finding\nweights that are robust to changes in the bit of the quantizer -- an area where\ncurrent methods struggle to match the accuracy of QAT at specific bits."
    },
    {
        "date": "2025-02",
        "title": "MambaGlue: Fast and Robust Local Feature Matching With Mamba",
        "author": "Kihwan Ryoo, Hyungtae Lim, and Hyun Myung",
        "link": "http://arxiv.org/abs/2502.00462v1",
        "abstract": "In recent years, robust matching methods using deep learning-based approaches\nhave been actively studied and improved in computer vision tasks. However,\nthere remains a persistent demand for both robust and fast matching techniques.\nTo address this, we propose a novel Mamba-based local feature matching\napproach, called MambaGlue, where Mamba is an emerging state-of-the-art\narchitecture rapidly gaining recognition for its superior speed in both\ntraining and inference, and promising performance compared with Transformer\narchitectures. In particular, we propose two modules: a) MambaAttention mixer\nto simultaneously and selectively understand the local and global context\nthrough the Mamba-based self-attention structure and b) deep confidence score\nregressor, which is a multi-layer perceptron (MLP)-based architecture that\nevaluates a score indicating how confidently matching predictions correspond to\nthe ground-truth correspondences. Consequently, our MambaGlue achieves a\nbalance between robustness and efficiency in real-world applications. As\nverified on various public datasets, we demonstrate that our MambaGlue yields a\nsubstantial performance improvement over baseline approaches while maintaining\nfast inference speed. Our code will be available on\nhttps://github.com/url-kaist/MambaGlue"
    },
    {
        "date": "2025-02",
        "title": "Improving realistic semi-supervised learning with doubly robust estimation",
        "author": "Khiem Pham, Charles Herrmann, and Ramin Zabih",
        "link": "http://arxiv.org/abs/2502.00279v1",
        "abstract": "A major challenge in Semi-Supervised Learning (SSL) is the limited\ninformation available about the class distribution in the unlabeled data. In\nmany real-world applications this arises from the prevalence of long-tailed\ndistributions, where the standard pseudo-label approach to SSL is biased\ntowards the labeled class distribution and thus performs poorly on unlabeled\ndata. Existing methods typically assume that the unlabeled class distribution\nis either known a priori, which is unrealistic in most situations, or estimate\nit on-the-fly using the pseudo-labels themselves. We propose to explicitly\nestimate the unlabeled class distribution, which is a finite-dimensional\nparameter, \\emph{as an initial step}, using a doubly robust estimator with a\nstrong theoretical guarantee; this estimate can then be integrated into\nexisting methods to pseudo-label the unlabeled data during training more\naccurately. Experimental results demonstrate that incorporating our techniques\ninto common pseudo-labeling approaches improves their performance."
    },
    {
        "date": "2025-01",
        "title": "ALBAR: Adversarial Learning approach to mitigate Biases in Action Recognition",
        "author": "Joseph Fioresi, Ishan Rajendrakumar Dave, and Mubarak Shah",
        "link": "http://arxiv.org/abs/2502.00156v1",
        "abstract": "Bias in machine learning models can lead to unfair decision making, and while\nit has been well-studied in the image and text domains, it remains\nunderexplored in action recognition. Action recognition models often suffer\nfrom background bias (i.e., inferring actions based on background cues) and\nforeground bias (i.e., relying on subject appearance), which can be detrimental\nto real-life applications such as autonomous vehicles or assisted living\nmonitoring. While prior approaches have mainly focused on mitigating background\nbias using specialized augmentations, we thoroughly study both biases. We\npropose ALBAR, a novel adversarial training method that mitigates foreground\nand background biases without requiring specialized knowledge of the bias\nattributes. Our framework applies an adversarial cross-entropy loss to the\nsampled static clip (where all the frames are the same) and aims to make its\nclass probabilities uniform using a proposed entropy maximization loss.\nAdditionally, we introduce a gradient penalty loss for regularization against\nthe debiasing process. We evaluate our method on established background and\nforeground bias protocols, setting a new state-of-the-art and strongly\nimproving combined debiasing performance by over 12% on HMDB51. Furthermore, we\nidentify an issue of background leakage in the existing UCF101 protocol for\nbias evaluation which provides a shortcut to predict actions and does not\nprovide an accurate measure of the debiasing capability of a model. We address\nthis issue by proposing more fine-grained segmentation boundaries for the\nactor, where our method also outperforms existing approaches. Project Page:\nhttps://joefioresi718.github.io/ALBAR_webpage/"
    },
    {
        "date": "2025-01",
        "title": "A Direct Semi-Exhaustive Search Method for Robust, Partial-to-Full Point Cloud Registration",
        "author": "Richard Cheng, Chavdar Papozov, Dan Helmick, and Mark Tjersland",
        "link": "http://arxiv.org/abs/2502.00115v1",
        "abstract": "Point cloud registration refers to the problem of finding the rigid\ntransformation that aligns two given point clouds, and is crucial for many\napplications in robotics and computer vision. The main insight of this paper is\nthat we can directly optimize the point cloud registration problem without\ncorrespondences by utilizing an algorithmically simple, yet computationally\ncomplex, semi-exhaustive search approach that is very well-suited for\nparallelization on modern GPUs. Our proposed algorithm, Direct Semi-Exhaustive\nSearch (DSES), iterates over potential rotation matrices and efficiently\ncomputes the inlier-maximizing translation associated with each rotation. It\nthen computes the optimal rigid transformation based on any desired distance\nmetric by directly computing the error associated with each transformation\ncandidate $\\{R, t\\}$. By leveraging the parallelism of modern GPUs, DSES\noutperforms state-of-the-art methods for partial-to-full point cloud\nregistration on the simulated ModelNet40 benchmark and demonstrates high\nperformance and robustness for pose estimation on a real-world robotics problem\n(https://youtu.be/q0q2-s2KSuA)."
    },
    {
        "date": "2025-01",
        "title": "Application of Generative Adversarial Network (GAN) for Synthetic Training Data Creation to improve performance of ANN Classifier for extracting Built-Up pixels from Landsat Satellite Imagery",
        "author": "Amritendu Mukherjee, Dipanwita Sinha Mukherjee, and Parthasarathy Ramachandran",
        "link": "http://arxiv.org/abs/2501.19283v1",
        "abstract": "Training a neural network for pixel based classification task using low\nresolution Landsat images is difficult as the size of the training data is\nusually small due to less number of available pixels that represent a single\nclass without any mixing with other classes. Due to this scarcity of training\ndata, neural network may not be able to attain expected level of accuracy. This\nlimitation could be overcome using a generative network that aims to generate\nsynthetic data having the same distribution as the sample data with which it is\ntrained. In this work, we have proposed a methodology for improving the\nperformance of ANN classifier to identify built-up pixels in the Landsat$7$\nimage with the help of developing a simple GAN architecture that could generate\nsynthetic training pixels when trained using original set of sample built-up\npixels. To ensure that the marginal and joint distributions of all the bands\ncorresponding to the generated and original set of pixels are\nindistinguishable, non-parametric Kolmogorov Smirnov Test and Ball Divergence\nbased Equality of Distributions Test have been performed respectively. It has\nbeen observed that the overall accuracy and kappa coefficient of the ANN model\nfor built-up classification have continuously improved from $0.9331$ to\n$0.9983$ and $0.8277$ to $0.9958$ respectively, with the inclusion of generated\nsets of built-up pixels to the original one."
    },
    {
        "date": "2025-01",
        "title": "RIGNO: A Graph-based framework for robust and accurate operator learning for PDEs on arbitrary domains",
        "author": "Sepehr Mousavi, Shizheng Wen, Levi Lingsch, Maximilian Herde, Bogdan Raoni\u0107, and Siddhartha Mishra",
        "link": "http://arxiv.org/abs/2501.19205v1",
        "abstract": "Learning the solution operators of PDEs on arbitrary domains is challenging\ndue to the diversity of possible domain shapes, in addition to the often\nintricate underlying physics. We propose an end-to-end graph neural network\n(GNN) based neural operator to learn PDE solution operators from data on point\nclouds in arbitrary domains. Our multi-scale model maps data between\ninput/output point clouds by passing it through a downsampled regional mesh.\nMany novel elements are also incorporated to ensure resolution invariance and\ntemporal continuity. Our model, termed RIGNO, is tested on a challenging suite\nof benchmarks, composed of various time-dependent and steady PDEs defined on a\ndiverse set of domains. We demonstrate that RIGNO is significantly more\naccurate than neural operator baselines and robustly generalizes to unseen\nspatial resolutions and time instances."
    },
    {
        "date": "2025-01",
        "title": "Secured Communication Schemes for UAVs in 5G: CRYSTALS-Kyber and IDS",
        "author": "Taneya Sharma, Seyed Ahmad Soleymani, Mohammad Shojafar, and Rahim Tafazolli",
        "link": "http://arxiv.org/abs/2501.19191v1",
        "abstract": "This paper introduces a secure communication architecture for Unmanned Aerial\nVehicles (UAVs) and ground stations in 5G networks, addressing critical\nchallenges in network security. The proposed solution integrates the Advanced\nEncryption Standard (AES) with Elliptic Curve Cryptography (ECC) and\nCRYSTALS-Kyber for key encapsulation, offering a hybrid cryptographic approach.\nBy incorporating CRYSTALS-Kyber, the framework mitigates vulnerabilities in ECC\nagainst quantum attacks, positioning it as a quantum-resistant alternative. The\narchitecture is based on a server-client model, with UAVs functioning as\nclients and the ground station acting as the server. The system was rigorously\nevaluated in both VPN and 5G environments. Experimental results confirm that\nCRYSTALS-Kyber delivers strong protection against quantum threats with minimal\nperformance overhead, making it highly suitable for UAVs with resource\nconstraints. Moreover, the proposed architecture integrates an Artificial\nIntelligence (AI)-based Intrusion Detection System (IDS) to further enhance\nsecurity. In performance evaluations, the IDS demonstrated strong results\nacross multiple models with XGBoost, particularly in more demanding scenarios,\noutperforming other models with an accuracy of 97.33% and an AUC of 0.94. These\nfindings underscore the potential of combining quantum-resistant encryption\nmechanisms with AI-driven IDS to create a robust, scalable, and secure\ncommunication framework for UAV networks, particularly within the\nhigh-performance requirements of 5G environments."
    },
    {
        "date": "2025-01",
        "title": "Enhancing Model Defense Against Jailbreaks with Proactive Safety Reasoning",
        "author": "Xianglin Yang, Gelei Deng, Jieming Shi, Tianwei Zhang, and Jin Song Dong",
        "link": "http://arxiv.org/abs/2501.19180v1",
        "abstract": "Large language models (LLMs) are vital for a wide range of applications yet\nremain susceptible to jailbreak threats, which could lead to the generation of\ninappropriate responses. Conventional defenses, such as refusal and adversarial\ntraining, often fail to cover corner cases or rare domains, leaving LLMs still\nvulnerable to more sophisticated attacks. We propose a novel defense strategy,\nSafety Chain-of-Thought (SCoT), which harnesses the enhanced \\textit{reasoning\ncapabilities} of LLMs for proactive assessment of harmful inputs, rather than\nsimply blocking them. SCoT augments any refusal training datasets to critically\nanalyze the intent behind each request before generating answers. By employing\nproactive reasoning, SCoT enhances the generalization of LLMs across varied\nharmful queries and scenarios not covered in the safety alignment corpus.\nAdditionally, it generates detailed refusals specifying the rules violated.\nComparative evaluations show that SCoT significantly surpasses existing\ndefenses, reducing vulnerability to out-of-distribution issues and adversarial\nmanipulations while maintaining strong general capabilities."
    },
    {
        "date": "2025-01",
        "title": "SWAT: Sliding Window Adversarial Training for Gradual Domain Adaptation",
        "author": "Zixi Wang, Yubo Huang, Wenwei Luo, Tonglan Xie, Mengmeng Jing, and Lin Zuo",
        "link": "http://arxiv.org/abs/2501.19155v1",
        "abstract": "Domain shifts are critical issues that harm the performance of machine\nlearning. Unsupervised Domain Adaptation (UDA) mitigates this issue but suffers\nwhen the domain shifts are steep and drastic. Gradual Domain Adaptation (GDA)\nalleviates this problem in a mild way by gradually adapting from the source to\nthe target domain using multiple intermediate domains. In this paper, we\npropose Sliding Window Adversarial Training (SWAT) for Gradual Domain\nAdaptation. SWAT uses the construction of adversarial streams to connect the\nfeature spaces of the source and target domains. In order to gradually narrow\nthe small gap between adjacent intermediate domains, a sliding window paradigm\nis designed that moves along the adversarial stream. When the window moves to\nthe end of the stream, i.e., the target domain, the domain shift is drastically\nreduced. Extensive experiments are conducted on public GDA benchmarks, and the\nresults demonstrate that the proposed SWAT significantly outperforms the\nstate-of-the-art approaches. The implementation is available at:\nhttps://anonymous.4open.science/r/SWAT-8677."
    },
    {
        "date": "2025-01",
        "title": "Imitation Game for Adversarial Disillusion with Multimodal Generative Chain-of-Thought Role-Play",
        "author": "Ching-Chun Chang, Fan-Yun Chen, Shih-Hong Gu, Kai Gao, Hanrui Wang, and Isao Echizen",
        "link": "http://arxiv.org/abs/2501.19143v1",
        "abstract": "As the cornerstone of artificial intelligence, machine perception confronts a\nfundamental threat posed by adversarial illusions. These adversarial attacks\nmanifest in two primary forms: deductive illusion, where specific stimuli are\ncrafted based on the victim model's general decision logic, and inductive\nillusion, where the victim model's general decision logic is shaped by specific\nstimuli. The former exploits the model's decision boundaries to create a\nstimulus that, when applied, interferes with its decision-making process. The\nlatter reinforces a conditioned reflex in the model, embedding a backdoor\nduring its learning phase that, when triggered by a stimulus, causes aberrant\nbehaviours. The multifaceted nature of adversarial illusions calls for a\nunified defence framework, addressing vulnerabilities across various forms of\nattack. In this study, we propose a disillusion paradigm based on the concept\nof an imitation game. At the heart of the imitation game lies a multimodal\ngenerative agent, steered by chain-of-thought reasoning, which observes,\ninternalises and reconstructs the semantic essence of a sample, liberated from\nthe classic pursuit of reversing the sample to its original state. As a proof\nof concept, we conduct experimental simulations using a multimodal generative\ndialogue agent and evaluates the methodology under a variety of attack\nscenarios."
    },
    {
        "date": "2025-01",
        "title": "FedRTS: Federated Robust Pruning via Combinatorial Thompson Sampling",
        "author": "Hong Huang, Hai Yang, Yuan Chen, Jiaxun Ye, and Dapeng Wu",
        "link": "http://arxiv.org/abs/2501.19122v1",
        "abstract": "Federated Learning (FL) enables collaborative model training across\ndistributed clients without data sharing, but its high computational and\ncommunication demands strain resource-constrained devices. While existing\nmethods use dynamic pruning to improve efficiency by periodically adjusting\nsparse model topologies while maintaining sparsity, these approaches suffer\nfrom issues such as greedy adjustments, unstable topologies, and communication\ninefficiency, resulting in less robust models and suboptimal performance under\ndata heterogeneity and partial client availability. To address these\nchallenges, we propose Federated Robust pruning via combinatorial Thompson\nSampling (FedRTS), a novel framework designed to develop robust sparse models.\nFedRTS enhances robustness and performance through its Thompson Sampling-based\nAdjustment (TSAdj) mechanism, which uses probabilistic decisions informed by\nstable, farsighted information instead of deterministic decisions reliant on\nunstable and myopic information in previous methods. Extensive experiments\ndemonstrate that FedRTS achieves state-of-the-art performance in computer\nvision and natural language processing tasks while reducing communication\ncosts, particularly excelling in scenarios with heterogeneous data\ndistributions and partial client participation. Our codes are available at:\nhttps://github.com/Little0o0/FedRTS"
    },
    {
        "date": "2025-01",
        "title": "Ambient Denoising Diffusion Generative Adversarial Networks for Establishing Stochastic Object Models from Noisy Image Data",
        "author": "Xichen Xu, Wentao Chen, and Weimin Zhou",
        "link": "http://arxiv.org/abs/2501.19094v1",
        "abstract": "It is widely accepted that medical imaging systems should be objectively\nassessed via task-based image quality (IQ) measures that ideally account for\nall sources of randomness in the measured image data, including the variation\nin the ensemble of objects to be imaged. Stochastic object models (SOMs) that\ncan randomly draw samples from the object distribution can be employed to\ncharacterize object variability. To establish realistic SOMs for task-based IQ\nanalysis, it is desirable to employ experimental image data. However,\nexperimental image data acquired from medical imaging systems are subject to\nmeasurement noise. Previous work investigated the ability of deep generative\nmodels (DGMs) that employ an augmented generative adversarial network (GAN),\nAmbientGAN, for establishing SOMs from noisy measured image data. Recently,\ndenoising diffusion models (DDMs) have emerged as a leading DGM for image\nsynthesis and can produce superior image quality than GANs. However, original\nDDMs possess a slow image-generation process because of the Gaussian assumption\nin the denoising steps. More recently, denoising diffusion GAN (DDGAN) was\nproposed to permit fast image generation while maintain high generated image\nquality that is comparable to the original DDMs. In this work, we propose an\naugmented DDGAN architecture, Ambient DDGAN (ADDGAN), for learning SOMs from\nnoisy image data. Numerical studies that consider clinical computed tomography\n(CT) images and digital breast tomosynthesis (DBT) images are conducted. The\nability of the proposed ADDGAN to learn realistic SOMs from noisy image data is\ndemonstrated. It has been shown that the ADDGAN significantly outperforms the\nadvanced AmbientGAN models for synthesizing high resolution medical images with\ncomplex textures."
    },
    {
        "date": "2025-01",
        "title": "Towards the Worst-case Robustness of Large Language Models",
        "author": "Huanran Chen, Yinpeng Dong, Zeming Wei, Hang Su, and Jun Zhu",
        "link": "http://arxiv.org/abs/2501.19040v1",
        "abstract": "Recent studies have revealed the vulnerability of Large Language Models\n(LLMs) to adversarial attacks, where the adversary crafts specific input\nsequences to induce harmful, violent, private, or incorrect outputs. Although\nvarious defenses have been proposed, they have not been evaluated by strong\nadaptive attacks, leaving the worst-case robustness of LLMs still intractable.\nBy developing a stronger white-box attack, our evaluation results indicate that\nmost typical defenses achieve nearly 0\\% robustness.To solve this, we propose\n\\textit{DiffTextPure}, a general defense that diffuses the (adversarial) input\nprompt using any pre-defined smoothing distribution, and purifies the diffused\ninput using a pre-trained language model. Theoretically, we derive tight\nrobustness lower bounds for all smoothing distributions using Fractal Knapsack\nor 0-1 Knapsack solvers. Under this framework, we certify the robustness of a\nspecific case -- smoothing LLMs using a uniform kernel -- against \\textit{any\npossible attack} with an average $\\ell_0$ perturbation of 2.02 or an average\nsuffix length of 6.41."
    },
    {
        "date": "2025-01",
        "title": "Adversarial Attacks on AI-Generated Text Detection Models: A Token Probability-Based Approach Using Embeddings",
        "author": "Ahmed K. Kadhim, Lei Jiao, Rishad Shafik, and Ole-Christoffer Granmo",
        "link": "http://arxiv.org/abs/2501.18998v1",
        "abstract": "In recent years, text generation tools utilizing Artificial Intelligence (AI)\nhave occasionally been misused across various domains, such as generating\nstudent reports or creative writings. This issue prompts plagiarism detection\nservices to enhance their capabilities in identifying AI-generated content.\nAdversarial attacks are often used to test the robustness of AI-text generated\ndetectors. This work proposes a novel textual adversarial attack on the\ndetection models such as Fast-DetectGPT. The method employs embedding models\nfor data perturbation, aiming at reconstructing the AI generated texts to\nreduce the likelihood of detection of the true origin of the texts.\nSpecifically, we employ different embedding techniques, including the Tsetlin\nMachine (TM), an interpretable approach in machine learning for this purpose.\nBy combining synonyms and embedding similarity vectors, we demonstrates the\nstate-of-the-art reduction in detection scores against Fast-DetectGPT.\nParticularly, in the XSum dataset, the detection score decreased from 0.4431 to\n0.2744 AUROC, and in the SQuAD dataset, it dropped from 0.5068 to 0.3532 AUROC."
    },
    {
        "date": "2025-01",
        "title": "Deep Learning Model Inversion Attacks and Defenses: A Comprehensive Survey",
        "author": "Wencheng Yang, Song Wang, Di Wu, Taotao Cai, Yanming Zhu, Shicheng Wei, Yiying Zhang, Xu Yang, and Yan Li",
        "link": "http://arxiv.org/abs/2501.18934v1",
        "abstract": "The rapid adoption of deep learning in sensitive domains has brought\ntremendous benefits. However, this widespread adoption has also given rise to\nserious vulnerabilities, particularly model inversion (MI) attacks, posing a\nsignificant threat to the privacy and integrity of personal data. The\nincreasing prevalence of these attacks in applications such as biometrics,\nhealthcare, and finance has created an urgent need to understand their\nmechanisms, impacts, and defense methods. This survey aims to fill the gap in\nthe literature by providing a structured and in-depth review of MI attacks and\ndefense strategies. Our contributions include a systematic taxonomy of MI\nattacks, extensive research on attack techniques and defense mechanisms, and a\ndiscussion about the challenges and future research directions in this evolving\nfield. By exploring the technical and ethical implications of MI attacks, this\nsurvey aims to offer insights into the impact of AI-powered systems on privacy,\nsecurity, and trust. In conjunction with this survey, we have developed a\ncomprehensive repository to support research on MI attacks and defenses. The\nrepository includes state-of-the-art research papers, datasets, evaluation\nmetrics, and other resources to meet the needs of both novice and experienced\nresearchers interested in MI attacks and defenses, as well as the broader field\nof AI security and privacy. The repository will be continuously maintained to\nensure its relevance and utility. It is accessible at\nhttps://github.com/overgter/Deep-Learning-Model-Inversion-Attacks-and-Defenses."
    },
    {
        "date": "2025-01",
        "title": "Distorting Embedding Space for Safety: A Defense Mechanism for Adversarially Robust Diffusion Models",
        "author": "Jaesin Ahn, and Heechul Jung",
        "link": "http://arxiv.org/abs/2501.18877v1",
        "abstract": "Text-to-image diffusion models show remarkable generation performance\nfollowing text prompts, but risk generating Not Safe For Work (NSFW) contents\nfrom unsafe prompts. Existing approaches, such as prompt filtering or concept\nunlearning, fail to defend against adversarial attacks while maintaining benign\nimage quality. In this paper, we propose a novel approach called Distorting\nEmbedding Space (DES), a text encoder-based defense mechanism that effectively\ntackles these issues through innovative embedding space control. DES transforms\nunsafe embeddings, extracted from a text encoder using unsafe prompts, toward\ncarefully calculated safe embedding regions to prevent unsafe contents\ngeneration, while reproducing the original safe embeddings. DES also\nneutralizes the nudity embedding, extracted using prompt ``nudity\", by aligning\nit with neutral embedding to enhance robustness against adversarial attacks.\nThese methods ensure both robust defense and high-quality image generation.\nAdditionally, DES can be adopted in a plug-and-play manner and requires zero\ninference overhead, facilitating its deployment. Extensive experiments on\ndiverse attack types, including black-box and white-box scenarios, demonstrate\nDES's state-of-the-art performance in both defense capability and benign image\ngeneration quality. Our model is available at https://github.com/aei13/DES."
    },
    {
        "date": "2025-01",
        "title": "Enforcing MAVLink Safety & Security Properties Via Refined Multiparty Session Types",
        "author": "Arthur Amorim, Max Taylor, Gary T. Leavens, Bill Harrison, Lance Joneckis, and Trevor Kann",
        "link": "http://arxiv.org/abs/2501.18874v1",
        "abstract": "A compromised system component can issue message sequences that are perfectly\nlegal while also leading the system itself into unsafe states. Such attacks are\nchallenging to characterize, because message interfaces in standard languages\ndefine the individual messages possible but cannot express designers'\nintentions for how they should be used. We present initial results from ongoing\nwork applying refined multiparty session types as a mechanism for expressing\nand enforcing proper message usage to exclude legal, but unsafe, sequences. We\nillustrate our approach by using refined multiparty session types to mitigate\nsafety and security issues in the MAVLink protocol commonly used in UAVs."
    },
    {
        "date": "2025-01",
        "title": "QPRAC: Towards Secure and Practical PRAC-based Rowhammer Mitigation using Priority Queues",
        "author": "Jeonghyun Woo, Shaopeng, Lin, Prashant J. Nair, Aamer Jaleel, and Gururaj Saileshwar",
        "link": "http://arxiv.org/abs/2501.18861v1",
        "abstract": "JEDEC has introduced the Per Row Activation Counting (PRAC) framework for\nDDR5 and future DRAMs to enable precise counting of DRAM row activations. PRAC\nenables a holistic mitigation of Rowhammer attacks even at ultra-low Rowhammer\nthresholds. PRAC uses an Alert Back-Off (ABO) protocol to request the memory\ncontroller to issue Rowhammer mitigation requests. However, recent PRAC\nimplementations are either insecure or impractical. For example, Panopticon,\nthe inspiration for PRAC, is rendered insecure if implemented per JEDEC's PRAC\nspecification. On the other hand, the recent UPRAC proposal is impractical\nsince it needs oracular knowledge of the `top-N' activated DRAM rows that\nrequire mitigation.\n  This paper provides the first secure, scalable, and practical RowHammer\nsolution using the PRAC framework. The crux of our proposal is the design of a\npriority-based service queue (PSQ) for mitigations that prioritizes pending\nmitigations based on activation counts to avoid the security risks of prior\nsolutions. This provides principled security using the reactive ABO protocol.\nFurthermore, we co-design our PSQ, with opportunistic mitigation on Refresh\nManagement (RFM) operations and proactive mitigation during refresh (REF), to\nlimit the performance impact of ABO-based mitigations. QPRAC provides secure\nand practical RowHammer mitigation that scales to Rowhammer thresholds as low\nas 71 while incurring a 0.8% slowdown for benign workloads, which further\nreduces to 0% with proactive mitigations."
    },
    {
        "date": "2025-01",
        "title": "DAPPER: A Performance-Attack-Resilient Tracker for RowHammer Defense",
        "author": "Jeonghyun Woo, and Prashant J. Nair",
        "link": "http://arxiv.org/abs/2501.18857v1",
        "abstract": "RowHammer vulnerabilities pose a significant threat to modern DRAM-based\nsystems, where rapid activation of DRAM rows can induce bit-flips in\nneighboring rows. To mitigate this, state-of-the-art host-side RowHammer\nmitigations typically rely on shared counters or tracking structures. While\nthese optimizations benefit benign applications, they are vulnerable to\nPerformance Attacks (Perf-Attacks), where adversaries exploit shared structures\nto reduce DRAM bandwidth for co-running benign applications by increasing DRAM\naccesses for RowHammer counters or triggering repetitive refreshes required for\nthe early reset of structures, significantly degrading performance.\n  In this paper, we propose secure hashing mechanisms to thwart adversarial\nattempts to capture the mapping of shared structures. We propose DAPPER, a\nnovel low-cost tracker resilient to Perf-Attacks even at ultra-low RowHammer\nthresholds. We first present a secure hashing template in the form of DAPPER-S.\nWe then develop DAPPER-H, an enhanced version of DAPPER-S, incorporating\ndouble-hashing, novel reset strategies, and mitigative refresh techniques. Our\nsecurity analysis demonstrates the effectiveness of DAPPER-H against both\nRowHammer and Perf-Attacks. Experiments with 57 workloads from SPEC2006,\nSPEC2017, TPC, Hadoop, MediaBench, and YCSB show that, even at an ultra-low\nRowHammer threshold of 500, DAPPER-H incurs only a 0.9% slowdown in the\npresence of Perf-Attacks while using only 96KB of SRAM per 32GB of DRAM memory."
    },
    {
        "date": "2025-01",
        "title": "Trading Inference-Time Compute for Adversarial Robustness",
        "author": "Wojciech Zaremba, Evgenia Nitishinskaya, Boaz Barak, Stephanie Lin, Sam Toyer, Yaodong Yu, Rachel Dias, Eric Wallace, Kai Xiao, Johannes Heidecke, and Amelia Glaese",
        "link": "http://arxiv.org/abs/2501.18841v1",
        "abstract": "We conduct experiments on the impact of increasing inference-time compute in\nreasoning models (specifically OpenAI o1-preview and o1-mini) on their\nrobustness to adversarial attacks. We find that across a variety of attacks,\nincreased inference-time compute leads to improved robustness. In many cases\n(with important exceptions), the fraction of model samples where the attack\nsucceeds tends to zero as the amount of test-time compute grows. We perform no\nadversarial training for the tasks we study, and we increase inference-time\ncompute by simply allowing the models to spend more compute on reasoning,\nindependently of the form of attack. Our results suggest that inference-time\ncompute has the potential to improve adversarial robustness for Large Language\nModels. We also explore new attacks directed at reasoning models, as well as\nsettings where inference-time compute does not improve reliability, and\nspeculate on the reasons for these as well as ways to address them."
    },
    {
        "date": "2025-01",
        "title": "Decoding User Concerns in AI Health Chatbots: An Exploration of Security and Privacy in App Reviews",
        "author": "Muhammad Hassan, Abdullah Ghani, Muhammad Fareed Zaffar, and Masooda Bashir",
        "link": "http://arxiv.org/abs/2502.00067v1",
        "abstract": "AI powered health chatbot applications are increasingly utilized for\npersonalized healthcare services, yet they pose significant challenges related\nto user data security and privacy. This study evaluates the effectiveness of\nautomated methods, specifically BART and Gemini GenAI, in identifying security\nprivacy related (SPR) concerns within these applications' user reviews,\nbenchmarking their performance against manual qualitative analysis. Our results\nindicate that while Gemini's performance in SPR classification is comparable to\nmanual labeling, both automated methods have limitations, including the\nmisclassification of unrelated issues. Qualitative analysis revealed critical\nuser concerns, such as data collection practices, data misuse, and insufficient\ntransparency and consent mechanisms. This research enhances the understanding\nof the relationship between user trust, privacy, and emerging mobile AI health\nchatbot technologies, offering actionable insights for improving security and\nprivacy practices in AI driven health chatbots. Although exploratory, our\nfindings highlight the necessity for rigorous audits and transparent\ncommunication strategies, providing valuable guidance for app developers and\nvendors in addressing user security and privacy concerns."
    },
    {
        "date": "2025-01",
        "title": "An Adversarial Approach to Register Extreme Resolution Tissue Cleared 3D Brain Images",
        "author": "Abdullah Naziba, Clinton Fookes, and Dimitri Perrin",
        "link": "http://arxiv.org/abs/2501.18815v1",
        "abstract": "We developed a generative patch based 3D image registration model that can\nregister very high resolution images obtained from a biochemical process name\ntissue clearing. Tissue clearing process removes lipids and fats from the\ntissue and make the tissue transparent. When cleared tissues are imaged with\nLight-sheet fluorescent microscopy, the resulting images give a clear window to\nthe cellular activities and dynamics inside the tissue.Thus the images obtained\nare very rich with cellular information and hence their resolution is extremely\nhigh (eg .2560x2160x676). Analyzing images with such high resolution is a\ndifficult task for any image analysis pipeline.Image registration is a common\nstep in image analysis pipeline when comparison between images are required.\nTraditional image registration methods fail to register images with such\nextant. In this paper we addressed this very high resolution image registration\nissue by proposing a patch-based generative network named InvGAN. Our proposed\nnetwork can register very high resolution tissue cleared images. The tissue\ncleared dataset used in this paper are obtained from a tissue clearing protocol\nnamed CUBIC. We compared our method both with traditional and deep-learning\nbased registration methods.Two different versions of CUBIC dataset are used,\nrepresenting two different resolutions 25% and 100% respectively. Experiments\non two different resolutions clearly show the impact of resolution on the\nregistration quality. At 25% resolution, our method achieves comparable\nregistration accuracy with very short time (7 minutes approximately). At 100%\nresolution, most of the traditional registration methods fail except Elastix\nregistration tool.Elastix takes 28 hours to register where proposed InvGAN\ntakes only 10 minutes."
    },
    {
        "date": "2025-01",
        "title": "REDACTOR: eFPGA Redaction for DNN Accelerator Security",
        "author": "Yazan Baddour, Ava Hedayatipour, and Amin Rezaei",
        "link": "http://arxiv.org/abs/2501.18740v1",
        "abstract": "With the ever-increasing integration of artificial intelligence into daily\nlife and the growing importance of well-trained models, the security of\nhardware accelerators supporting Deep Neural Networks (DNNs) has become\nparamount. As a promising solution to prevent hardware intellectual property\ntheft, eFPGA redaction has emerged. This technique selectively conceals\ncritical components of the design, allowing authorized users to restore\nfunctionality post-fabrication by inserting the correct bitstream. In this\npaper, we explore the redaction of DNN accelerators using eFPGAs, from\nspecification to physical design implementation. Specifically, we investigate\nthe selection of critical DNN modules for redaction using both regular and\nfracturable look-up tables. We perform synthesis, timing verification, and\nplace & route on redacted DNN accelerators. Furthermore, we evaluate the\noverhead of incorporating eFPGAs into DNN accelerators in terms of power, area,\nand delay, finding it reasonable given the security benefits."
    },
    {
        "date": "2025-01",
        "title": "Exploring Audio Editing Features as User-Centric Privacy Defenses Against Emotion Inference Attacks",
        "author": "Mohd. Farhan Israk Soumik, W. K. M. Mithsara, Abdur R. Shahid, and Ahmed Imteaj",
        "link": "http://arxiv.org/abs/2501.18727v1",
        "abstract": "The rapid proliferation of speech-enabled technologies, including virtual\nassistants, video conferencing platforms, and wearable devices, has raised\nsignificant privacy concerns, particularly regarding the inference of sensitive\nemotional information from audio data. Existing privacy-preserving methods\noften compromise usability and security, limiting their adoption in practical\nscenarios. This paper introduces a novel, user-centric approach that leverages\nfamiliar audio editing techniques, specifically pitch and tempo manipulation,\nto protect emotional privacy without sacrificing usability. By analyzing\npopular audio editing applications on Android and iOS platforms, we identified\nthese features as both widely available and usable. We rigorously evaluated\ntheir effectiveness against a threat model, considering adversarial attacks\nfrom diverse sources, including Deep Neural Networks (DNNs), Large Language\nModels (LLMs), and and reversibility testing. Our experiments, conducted on\nthree distinct datasets, demonstrate that pitch and tempo manipulation\neffectively obfuscates emotional data. Additionally, we explore the design\nprinciples for lightweight, on-device implementation to ensure broad\napplicability across various devices and platforms."
    },
    {
        "date": "2025-01",
        "title": "Accuracy and Robustness of Weight-Balancing Methods for Training PINNs",
        "author": "Matthieu Barreau, and Haoming Shen",
        "link": "http://arxiv.org/abs/2501.18582v1",
        "abstract": "Physics-Informed Neural Networks (PINNs) have emerged as powerful tools for\nintegrating physics-based models with data by minimizing both data and physics\nlosses. However, this multi-objective optimization problem is notoriously\nchallenging, with some benchmark problems leading to unfeasible solutions. To\naddress these issues, various strategies have been proposed, including adaptive\nweight adjustments in the loss function. In this work, we introduce clear\ndefinitions of accuracy and robustness in the context of PINNs and propose a\nnovel training algorithm based on the Primal-Dual (PD) optimization framework.\nOur approach enhances the robustness of PINNs while maintaining comparable\nperformance to existing weight-balancing methods. Numerical experiments\ndemonstrate that the PD method consistently achieves reliable solutions across\nall investigated cases and can be easily implemented, facilitating its\npractical adoption. The code is available at\nhttps://github.com/haoming-SHEN/Accuracy-and-Robustness-of-Weight-Balancing-Methods-for-Training-PINNs.git."
    },
    {
        "date": "2025-01",
        "title": "Curriculum-based Sample Efficient Reinforcement Learning for Robust Stabilization of a Quadrotor",
        "author": "Fausto Mauricio Lagos Suarez, Akshit Saradagi, Vidya Sumathy, Shruti Kotpaliwar, and George Nikolakopoulos",
        "link": "http://arxiv.org/abs/2501.18490v1",
        "abstract": "This article introduces a curriculum learning approach to develop a\nreinforcement learning-based robust stabilizing controller for a Quadrotor that\nmeets predefined performance criteria. The learning objective is to achieve\ndesired positions from random initial conditions while adhering to both\ntransient and steady-state performance specifications. This objective is\nchallenging for conventional one-stage end-to-end reinforcement learning, due\nto the strong coupling between position and orientation dynamics, the\ncomplexity in designing and tuning the reward function, and poor sample\nefficiency, which necessitates substantial computational resources and leads to\nextended convergence times. To address these challenges, this work decomposes\nthe learning objective into a three-stage curriculum that incrementally\nincreases task complexity. The curriculum begins with learning to achieve\nstable hovering from a fixed initial condition, followed by progressively\nintroducing randomization in initial positions, orientations and velocities. A\nnovel additive reward function is proposed, to incorporate transient and\nsteady-state performance specifications. The results demonstrate that the\nProximal Policy Optimization (PPO)-based curriculum learning approach, coupled\nwith the proposed reward structure, achieves superior performance compared to a\nsingle-stage PPO-trained policy with the same reward function, while\nsignificantly reducing computational resource requirements and convergence\ntime. The curriculum-trained policy's performance and robustness are thoroughly\nvalidated under random initial conditions and in the presence of disturbances."
    },
    {
        "date": "2025-01",
        "title": "The Pitfalls of \"Security by Obscurity\" And What They Mean for Transparent AI",
        "author": "Peter Hall, Olivia Mundahl, and Sunoo Park",
        "link": "http://arxiv.org/abs/2501.18669v1",
        "abstract": "Calls for transparency in AI systems are growing in number and urgency from\ndiverse stakeholders ranging from regulators to researchers to users (with a\ncomparative absence of companies developing AI). Notions of transparency for AI\nabound, each addressing distinct interests and concerns.\n  In computer security, transparency is likewise regarded as a key concept. The\nsecurity community has for decades pushed back against so-called security by\nobscurity -- the idea that hiding how a system works protects it from attack --\nagainst significant pressure from industry and other stakeholders. Over the\ndecades, in a community process that is imperfect and ongoing, security\nresearchers and practitioners have gradually built up some norms and practices\naround how to balance transparency interests with possible negative side\neffects. This paper asks: What insights can the AI community take from the\nsecurity community's experience with transparency?\n  We identify three key themes in the security community's perspective on the\nbenefits of transparency and their approach to balancing transparency against\ncountervailing interests. For each, we investigate parallels and insights\nrelevant to transparency in AI. We then provide a case study discussion on how\ntransparency has shaped the research subfield of anonymization. Finally,\nshifting our focus from similarities to differences, we highlight key\ntransparency issues where modern AI systems present challenges different from\nother kinds of security-critical systems, raising interesting open questions\nfor the security and AI communities alike."
    },
    {
        "date": "2025-01",
        "title": "Exploring Potential Prompt Injection Attacks in Federated Military LLMs and Their Mitigation",
        "author": "Youngjoon Lee, Taehyun Park, Yunho Lee, Jinu Gong, and Joonhyuk Kang",
        "link": "http://arxiv.org/abs/2501.18416v1",
        "abstract": "Federated Learning (FL) is increasingly being adopted in military\ncollaborations to develop Large Language Models (LLMs) while preserving data\nsovereignty. However, prompt injection attacks-malicious manipulations of input\nprompts-pose new threats that may undermine operational security, disrupt\ndecision-making, and erode trust among allies. This perspective paper\nhighlights four potential vulnerabilities in federated military LLMs: secret\ndata leakage, free-rider exploitation, system disruption, and misinformation\nspread. To address these potential risks, we propose a human-AI collaborative\nframework that introduces both technical and policy countermeasures. On the\ntechnical side, our framework uses red/blue team wargaming and quality\nassurance to detect and mitigate adversarial behaviors of shared LLM weights.\nOn the policy side, it promotes joint AI-human policy development and\nverification of security protocols. Our findings will guide future research and\nemphasize proactive strategies for emerging military contexts."
    },
    {
        "date": "2025-01",
        "title": "GBFRS: Robust Fuzzy Rough Sets via Granular-ball Computing",
        "author": "Shuyin Xia, Xiaoyu Lian, Binbin Sang, Guoyin Wang, and Xinbo Gao",
        "link": "http://arxiv.org/abs/2501.18413v1",
        "abstract": "Fuzzy rough set theory is effective for processing datasets with complex\nattributes, supported by a solid mathematical foundation and closely linked to\nkernel methods in machine learning. Attribute reduction algorithms and\nclassifiers based on fuzzy rough set theory exhibit promising performance in\nthe analysis of high-dimensional multivariate complex data. However, most\nexisting models operate at the finest granularity, rendering them inefficient\nand sensitive to noise, especially for high-dimensional big data. Thus,\nenhancing the robustness of fuzzy rough set models is crucial for effective\nfeature selection. Muiti-garanularty granular-ball computing, a recent\ndevelopment, uses granular-balls of different sizes to adaptively represent and\ncover the sample space, performing learning based on these granular-balls. This\npaper proposes integrating multi-granularity granular-ball computing into fuzzy\nrough set theory, using granular-balls to replace sample points. The\ncoarse-grained characteristics of granular-balls make the model more robust.\nAdditionally, we propose a new method for generating granular-balls, scalable\nto the entire supervised method based on granular-ball computing. A forward\nsearch algorithm is used to select feature sequences by defining the\ncorrelation between features and categories through dependence functions.\nExperiments demonstrate the proposed model's effectiveness and superiority over\nbaseline methods."
    },
    {
        "date": "2025-01",
        "title": "Joint Optimization of Prompt Security and System Performance in Edge-Cloud LLM Systems",
        "author": "Haiyang Huang, Tianhui Meng, and Weijia Jia",
        "link": "http://arxiv.org/abs/2501.18663v1",
        "abstract": "Large language models (LLMs) have significantly facilitated human life, and\nprompt engineering has improved the efficiency of these models. However, recent\nyears have witnessed a rise in prompt engineering-empowered attacks, leading to\nissues such as privacy leaks, increased latency, and system resource wastage.\nThough safety fine-tuning based methods with Reinforcement Learning from Human\nFeedback (RLHF) are proposed to align the LLMs, existing security mechanisms\nfail to cope with fickle prompt attacks, highlighting the necessity of\nperforming security detection on prompts. In this paper, we jointly consider\nprompt security, service latency, and system resource optimization in\nEdge-Cloud LLM (EC-LLM) systems under various prompt attacks. To enhance prompt\nsecurity, a vector-database-enabled lightweight attack detector is proposed. We\nformalize the problem of joint prompt detection, latency, and resource\noptimization into a multi-stage dynamic Bayesian game model. The equilibrium\nstrategy is determined by predicting the number of malicious tasks and updating\nbeliefs at each stage through Bayesian updates. The proposed scheme is\nevaluated on a real implemented EC-LLM system, and the results demonstrate that\nour approach offers enhanced security, reduces the service latency for benign\nusers, and decreases system resource consumption compared to state-of-the-art\nalgorithms."
    },
    {
        "date": "2025-01",
        "title": "Robust Online Conformal Prediction under Uniform Label Noise",
        "author": "Huajun Xi, Kangdao Liu, Hao Zeng, Wenguang Sun, and Hongxin Wei",
        "link": "http://arxiv.org/abs/2501.18363v2",
        "abstract": "Conformal prediction is an emerging technique for uncertainty quantification\nthat constructs prediction sets guaranteed to contain the true label with a\npredefined probability. Recent work develops online conformal prediction\nmethods that adaptively construct prediction sets to accommodate distribution\nshifts. However, existing algorithms typically assume perfect label accuracy\nwhich rarely holds in practice. In this work, we investigate the robustness of\nonline conformal prediction under uniform label noise with a known noise rate,\nin both constant and dynamic learning rate schedules. We show that label noise\ncauses a persistent gap between the actual mis-coverage rate and the desired\nrate $\\alpha$, leading to either overestimated or underestimated coverage\nguarantees. To address this issue, we propose Noise Robust Online Conformal\nPrediction (dubbed NR-OCP) by updating the threshold with a novel robust\npinball loss, which provides an unbiased estimate of clean pinball loss without\nrequiring ground-truth labels. Our theoretical analysis shows that NR-OCP\neliminates the coverage gap in both constant and dynamic learning rate\nschedules, achieving a convergence rate of $\\mathcal{O}(T^{-1/2})$ for both\nempirical and expected coverage errors under uniform label noise. Extensive\nexperiments demonstrate the effectiveness of our method by achieving both\nprecise coverage and improved efficiency."
    },
    {
        "date": "2025-01",
        "title": "Experimental relativistic zero-knowledge proofs with unconditional security",
        "author": "Chen-Xun Weng, Ming-Yang Li, Nai-Rui Xu, Yanglin Hu, Ian George, Jiawei Wu, Shengjun Wu, Hua-Lei Yin, and Zeng-Bing Chen",
        "link": "http://arxiv.org/abs/2501.18176v1",
        "abstract": "Zero-knowledge proofs (ZKPs) are widely applied in digital economies, such as\ncryptocurrencies and smart contracts, for establishing trust and ensuring\nprivacy between untrusted parties. However, almost all ZKPs rely on unproven\ncomputational assumptions or are vulnerable to quantum adversaries. We propose\nand experimentally implement an unconditionally secure ZKP for the graph\nthree-coloring problem by combining subset relativistic bit commitments with\nquantum nonlocality game. Our protocol achieves a linear relationship between\ninteractive rounds and the number of edges, reducing round complexity and\nstorage requirements by thirteen orders of magnitude, thereby significantly\nenhancing practical feasibility. Our work illustrates the powerful potential of\nintegrating special relativity with quantum theory in trustless cryptography,\npaving the way for robust applications against quantum attacks in distrustful\ninternet environments."
    },
    {
        "date": "2025-01",
        "title": "Security for IEEE P1451.1.6-based Sensor Networks for IoT Applications",
        "author": "Hiroaki Nishi, Janaka Wijekoon, Eugene Y. Song, and Kang B. Lee",
        "link": "http://arxiv.org/abs/2501.18102v1",
        "abstract": "There are many challenges for Internet of Things (IoT) sensor networks\nincluding the lack of robust standards, diverse wireline and wireless\nconnectivity, interoperability, security, and privacy. Addressing these\nchallenges, the Institute of Electrical and Electronics Engineers (IEEE)\nP1451.0 standard defines network services, transducer services, transducer\nelectronic data sheets (TEDS) format, and a security framework to achieve\nsensor data security and interoperability for IoT applications. This paper\nproposes a security solution for IEEE P1451.1.6-based sensor networks for IoT\napplications utilizing the security framework defined in IEEE P1451.0. The\nproposed solution includes an architecture, a security policy with six security\nlevels, security standards, and security TEDS. Further, this paper introduces a\nnew service to update access control lists (ACLs) to regulate the access for\ntopic names by the applications and provides an implementation of the security\nTEDS for IEEE P1451.1.6-based sensor networks. The paper also illustrates how\nto access security TEDS that contain metadata on security standards to achieve\nsensor data security and interoperability."
    },
    {
        "date": "2025-01",
        "title": "Topological Signatures of Adversaries in Multimodal Alignments",
        "author": "Minh Vu, Geigh Zollicoffer, Huy Mai, Ben Nebgen, Boian Alexandrov, and Manish Bhattarai",
        "link": "http://arxiv.org/abs/2501.18006v1",
        "abstract": "Multimodal Machine Learning systems, particularly those aligning text and\nimage data like CLIP/BLIP models, have become increasingly prevalent, yet\nremain susceptible to adversarial attacks. While substantial research has\naddressed adversarial robustness in unimodal contexts, defense strategies for\nmultimodal systems are underexplored. This work investigates the topological\nsignatures that arise between image and text embeddings and shows how\nadversarial attacks disrupt their alignment, introducing distinctive\nsignatures. We specifically leverage persistent homology and introduce two\nnovel Topological-Contrastive losses based on Total Persistence and Multi-scale\nkernel methods to analyze the topological signatures introduced by adversarial\nperturbations. We observe a pattern of monotonic changes in the proposed\ntopological losses emerging in a wide range of attacks on image-text\nalignments, as more adversarial samples are introduced in the data. By\ndesigning an algorithm to back-propagate these signatures to input samples, we\nare able to integrate these signatures into Maximum Mean Discrepancy tests,\ncreating a novel class of tests that leverage topological signatures for better\nadversarial detection."
    },
    {
        "date": "2025-01",
        "title": "SMT-Boosted Security Types for Low-Level MPC",
        "author": "Christian Skalka, and Joseph P. Near",
        "link": "http://arxiv.org/abs/2501.17824v1",
        "abstract": "Secure Multi-Party Computation (MPC) is an important enabling technology for\ndata privacy in modern distributed applications. We develop a new type theory\nto automatically enforce correctness,confidentiality, and integrity properties\nof protocols written in the \\emph{Prelude/Overture} language framework.\nJudgements in the type theory are predicated on SMT verifications in a theory\nof finite fields, which supports precise and efficient analysis. Our approach\nis automated, compositional, scalable, and generalizes to arbitrary prime\nfields for data and key sizes."
    },
    {
        "date": "2025-01",
        "title": "U2A: Unified Unimodal Adaptation for Robust and Efficient Multimodal Learning",
        "author": "Md Kaykobad Reza, Niki Nezakati, Ameya Patil, Mashhour Solh, and M. Salman Asif",
        "link": "http://arxiv.org/abs/2501.17823v1",
        "abstract": "Multimodal learning often relies on designing new models and complex training\nstrategies to achieve optimal performance. We present Unified Unimodal\nAdaptation (U2A), which jointly fine-tunes pretrained unimodal encoders using\nlow-rank adaptation (LoRA) for various multimodal tasks. Our method\nsignificantly reduces the number of learnable parameters and eliminates the\nneed for complex training strategies, such as alternating training, gradient\nmodifications, or unimodal fine-tuning. To address missing modalities during\nboth training and testing, we introduce Mask Tokens (MT), which generate\nmissing modality features from available modalities using a single token per\nmodality. This simplifies the process, removing the need for specialized\nfeature estimation or prompt-tuning methods. Our evaluation demonstrates that\nU2A matches or outperforms state-of-the-art methods in both complete and\nmissing modality settings, showcasing strong performance and robustness across\nvarious modalities, tasks, and datasets. We also analyze and report the\neffectiveness of Mask Tokens in different missing modality scenarios. Overall,\nour method provides a robust, flexible, and efficient solution for multimodal\nlearning, with minimal computational overhead."
    },
    {
        "date": "2025-01",
        "title": "Atomic Transfer Graphs: Secure-by-design Protocols for Heterogeneous Blockchain Ecosystems",
        "author": "Stephan D\u00fcbler, Federico Badaloni, Pedro Moreno-Sanchez, and Clara Schneidewind",
        "link": "http://arxiv.org/abs/2501.17786v1",
        "abstract": "The heterogeneity of the blockchain landscape has motivated the design of\nblockchain protocols tailored to specific blockchains and applications that,\nhence, require custom security proofs. We observe that many blockchain\nprotocols share common security and functionality goals, which can be captured\nby an atomic transfer graph (ATG) describing the structure of desired\ntransfers. Based on this observation, we contribute a framework for generating\nsecure-by-design protocols that realize these goals. The resulting protocols\nbuild upon Conditional Timelock Contracts (CTLCs), a novel minimal smart\ncontract functionality that can be implemented in a large variety of\ncryptocurrencies with a restricted scripting language (e.g., Bitcoin), and\npayment channels. We show how ATGs, in addition to enabling novel applications,\ncapture the security and functionality goals of existing applications,\nincluding many examples from payment channel networks and complex multi-party\ncross-currency swaps among Ethereum-style cryptocurrencies. Our framework is\nthe first to provide generic and provably secure protocols for all these use\ncases while matching or improving the performance of existing use-case-specific\nprotocols."
    },
    {
        "date": "2025-01",
        "title": "Investigating Vulnerability Disclosures in Open-Source Software Using Bug Bounty Reports and Security Advisories",
        "author": "Jessy Ayala, Yu-Jye Tung, and Joshua Garcia",
        "link": "http://arxiv.org/abs/2501.17748v1",
        "abstract": "In the world of open-source software (OSS), the number of known\nvulnerabilities has tremendously increased. The GitHub Advisory Database\ncontains advisories for security risks in GitHub-hosted OSS projects. As of\n09/25/2023, there are 197,609 unreviewed GitHub security advisories. Of those\nunreviewed, at least 63,852 are publicly documented vulnerabilities,\npotentially leaving many OSS projects vulnerable. Recently, bug bounty\nplatforms have emerged to focus solely on providing bounties to help secure\nOSS. In this paper, we conduct an empirical study on 3,798 reviewed GitHub\nsecurity advisories and 4,033 disclosed OSS bug bounty reports, a perspective\nthat is currently understudied, because they contain comprehensive information\nabout security incidents, e.g., the nature of vulnerabilities, their impact,\nand how they were resolved. We are the first to determine the explicit process\ndescribing how OSS vulnerabilities propagate from security advisories and bug\nbounty reports, which are the main intermediaries between vulnerability\nreporters, OSS maintainers, and dependent projects, to vulnerable OSS projects\nand entries in global vulnerability databases and possibly back. This process\nuncovers how missing or delayed CVE assignments for OSS vulnerabilities result\nin projects, both in and out of OSS, not being notified of necessary security\nupdates promptly and corresponding bottlenecks. Based on our findings, we\nprovide suggestions, actionable items, and future research directions to help\nimprove the security posture of OSS projects."
    },
    {
        "date": "2025-01",
        "title": "Attacker Control and Bug Prioritization",
        "author": "Guilhem Lacombe, and S\u00e9bastien Bardin",
        "link": "http://arxiv.org/abs/2501.17740v1",
        "abstract": "As bug-finding methods improve, bug-fixing capabilities are exceeded,\nresulting in an accumulation of potential vulnerabilities. There is thus a need\nfor efficient and precise bug prioritization based on exploitability. In this\nwork, we explore the notion of control of an attacker over a vulnerability's\nparameters, which is an often overlooked factor of exploitability. We show that\ntaint as well as straightforward qualitative and quantitative notions of\ncontrol are not enough to effectively differentiate vulnerabilities. Instead,\nwe propose to focus analysis on feasible value sets, which we call domains of\ncontrol, in order to better take into account threat models and expert insight.\nOur new Shrink and Split algorithm efficiently extracts domains of control from\npath constraints obtained with symbolic execution and renders them in an easily\nprocessed, human-readable form. This in turn allows to automatically compute\nmore complex control metrics, such as weighted Quantitative Control, which\nfactors in the varying threat levels of different values. Experiments show that\nour method is both efficient and precise. In particular, it is the only one\nable to distinguish between vulnerabilities such as cve-2019-14192 and\ncve-2022-30552, while revealing a mistake in the human evaluation of\ncve-2022-30790. The high degree of automation of our tool also brings us closer\nto a fully-automated evaluation pipeline."
    },
    {
        "date": "2025-01",
        "title": "BitMLx: Secure Cross-chain Smart Contracts For Bitcoin-style Cryptocurrencies",
        "author": "Federico Badaloni, Sebastian Holler, Chrysoula Oikonomou, Pedro Moreno-Sanchez, and Clara Schneidewind",
        "link": "http://arxiv.org/abs/2501.17733v1",
        "abstract": "A smart contract is an interactive program that governs funds in the realm of\na single cryptocurrency. Yet, the many existing cryptocurrencies have spurred\nthe design of cross-chain applications that require interactions with multiple\ncryptocurrencies simultaneously. Currently, cross-chain applications are\nimplemented as use-case-specific cryptographic protocols that serve as overlay\nto synchronize smart contract executions in the different cryptocurrencies.\nHence, their design requires substantial expertise, as well as a security\nanalysis in complex cryptographic frameworks.\n  In this work, we present BitMLx, the first domain-specific language for\ncross-chain smart contracts, enabling interactions with several users that hold\nfunds across multiple Bitcoin-like cryptocurrencies. We contribute a compiler\nto automatically translate a BitMLx contract into one contract per involved\ncryptocurrency and a user strategy that synchronizes the execution of these\ncontracts. We prove that an honest user, who follows the prescribed strategy\nwhen interacting with the several contracts, ends up with at least as many\nfunds as in the corresponding execution of the BitMLx contract. Last, but not\nleast, we implement the BitMLx compiler and demonstrate its utility in the\ndesign of illustrative examples of cross-chain applications such as multi-chain\ndonations or loans across different cryptocurrencies."
    },
    {
        "date": "2025-01",
        "title": "CAMP in the Odyssey: Provably Robust Reinforcement Learning with Certified Radius Maximization",
        "author": "Derui Wang, Kristen Moore, Diksha Goel, Minjune Kim, Gang Li, Yang Li, Robin Doss, Minhui Xue, Bo Li, Seyit Camtepe, and Liming Zhu",
        "link": "http://arxiv.org/abs/2501.17667v1",
        "abstract": "Deep reinforcement learning (DRL) has gained widespread adoption in control\nand decision-making tasks due to its strong performance in dynamic\nenvironments. However, DRL agents are vulnerable to noisy observations and\nadversarial attacks, and concerns about the adversarial robustness of DRL\nsystems have emerged. Recent efforts have focused on addressing these\nrobustness issues by establishing rigorous theoretical guarantees for the\nreturns achieved by DRL agents in adversarial settings. Among these approaches,\npolicy smoothing has proven to be an effective and scalable method for\ncertifying the robustness of DRL agents. Nevertheless, existing certifiably\nrobust DRL relies on policies trained with simple Gaussian augmentations,\nresulting in a suboptimal trade-off between certified robustness and certified\nreturn. To address this issue, we introduce a novel paradigm dubbed\n\\texttt{C}ertified-r\\texttt{A}dius-\\texttt{M}aximizing \\texttt{P}olicy\n(\\texttt{CAMP}) training. \\texttt{CAMP} is designed to enhance DRL policies,\nachieving better utility without compromising provable robustness. By\nleveraging the insight that the global certified radius can be derived from\nlocal certified radii based on training-time statistics, \\texttt{CAMP}\nformulates a surrogate loss related to the local certified radius and optimizes\nthe policy guided by this surrogate loss. We also introduce \\textit{policy\nimitation} as a novel technique to stabilize \\texttt{CAMP} training.\nExperimental results demonstrate that \\texttt{CAMP} significantly improves the\nrobustness-return trade-off across various tasks. Based on the results,\n\\texttt{CAMP} can achieve up to twice the certified expected return compared to\nthat of baselines. Our code is available at\nhttps://github.com/NeuralSec/camp-robust-rl."
    },
    {
        "date": "2025-01",
        "title": "A Robust Support Vector Machine Approach for Raman COVID-19 Data Classification",
        "author": "Marco Piazza, Andrea Spinelli, Francesca Maggioni, Marzia Bedoni, and Enza Messina",
        "link": "http://arxiv.org/abs/2501.17904v1",
        "abstract": "Recent advances in healthcare technologies have led to the availability of\nlarge amounts of biological samples across several techniques and applications.\nIn particular, in the last few years, Raman spectroscopy analysis of biological\nsamples has been successfully applied for early-stage diagnosis. However,\nspectra' inherent complexity and variability make the manual analysis\nchallenging, even for domain experts. For the same reason, the use of\ntraditional Statistical and Machine Learning (ML) techniques could not\nguarantee for accurate and reliable results. ML models, combined with robust\noptimization techniques, offer the possibility to improve the classification\naccuracy and enhance the resilience of predictive models. In this paper, we\ninvestigate the performance of a novel robust formulation for Support Vector\nMachine (SVM) in classifying COVID-19 samples obtained from Raman Spectroscopy.\nGiven the noisy and perturbed nature of biological samples, we protect the\nclassification process against uncertainty through the application of robust\noptimization techniques. Specifically, we derive robust counterpart models of\ndeterministic formulations using bounded-by-norm uncertainty sets around each\nobservation. We explore the cases of both linear and kernel-induced classifiers\nto address binary and multiclass classification tasks. The effectiveness of our\napproach is validated on real-world COVID-19 datasets provided by Italian\nhospitals by comparing the results of our simulations with a state-of-the-art\nclassifier."
    },
    {
        "date": "2025-01",
        "title": "VoicePrompter: Robust Zero-Shot Voice Conversion with Voice Prompt and Conditional Flow Matching",
        "author": "Ha-Yeong Choi, and Jaehan Park",
        "link": "http://arxiv.org/abs/2501.17612v1",
        "abstract": "Despite remarkable advancements in recent voice conversion (VC) systems,\nenhancing speaker similarity in zero-shot scenarios remains challenging. This\nchallenge arises from the difficulty of generalizing and adapting speaker\ncharacteristics in speech within zero-shot environments, which is further\ncomplicated by mismatch between the training and inference processes. To\naddress these challenges, we propose VoicePrompter, a robust zero-shot VC model\nthat leverages in-context learning with voice prompts. VoicePrompter is\ncomposed of (1) a factorization method that disentangles speech components and\n(2) a DiT-based conditional flow matching (CFM) decoder that conditions on\nthese factorized features and voice prompts. Additionally, (3) latent mixup is\nused to enhance in-context learning by combining various speaker features. This\napproach improves speaker similarity and naturalness in zero-shot VC by\napplying mixup to latent representations. Experimental results demonstrate that\nVoicePrompter outperforms existing zero-shot VC systems in terms of speaker\nsimilarity, speech intelligibility, and audio quality. Our demo is available at\n\\url{https://hayeong0.github.io/VoicePrompter-demo/}."
    },
    {
        "date": "2025-01",
        "title": "Solving Urban Network Security Games: Learning Platform, Benchmark, and Challenge for AI Research",
        "author": "Shuxin Zhuang, Shuxin Li, Tianji Yang, Muheng Li, Xianjie Shi, Bo An, and Youzhi Zhang",
        "link": "http://arxiv.org/abs/2501.17559v1",
        "abstract": "After the great achievement of solving two-player zero-sum games, more and\nmore AI researchers focus on solving multiplayer games. To facilitate the\ndevelopment of designing efficient learning algorithms for solving multiplayer\ngames, we propose a multiplayer game platform for solving Urban Network\nSecurity Games (\\textbf{UNSG}) that model real-world scenarios. That is,\npreventing criminal activity is a highly significant responsibility assigned to\npolice officers in cities, and police officers have to allocate their limited\nsecurity resources to interdict the escaping criminal when a crime takes place\nin a city. This interaction between multiple police officers and the escaping\ncriminal can be modeled as a UNSG. The variants of UNSGs can model different\nreal-world settings, e.g., whether real-time information is available or not,\nand whether police officers can communicate or not. The main challenges of\nsolving this game include the large size of the game and the co-existence of\ncooperation and competition. While previous efforts have been made to tackle\nUNSGs, they have been hampered by performance and scalability issues.\nTherefore, we propose an open-source UNSG platform (\\textbf{GraphChase}) for\ndesigning efficient learning algorithms for solving UNSGs. Specifically,\nGraphChase offers a unified and flexible game environment for modeling various\nvariants of UNSGs, supporting the development, testing, and benchmarking of\nalgorithms. We believe that GraphChase not only facilitates the development of\nefficient algorithms for solving real-world problems but also paves the way for\nsignificant advancements in algorithmic development for solving general\nmultiplayer games."
    },
    {
        "date": "2025-01",
        "title": "How Much Do Code Language Models Remember? An Investigation on Data Extraction Attacks before and after Fine-tuning",
        "author": "Fabio Salerno, Ali Al-Kaswan, and Maliheh Izadi",
        "link": "http://arxiv.org/abs/2501.17501v2",
        "abstract": "Code language models, while widely popular, are often trained on unsanitized\nsource code gathered from across the Internet. Previous work revealed that\npre-trained models can remember the content of their training data and\nregurgitate them through data extraction attacks. Due to the large size of\ncurrent models, only a few entities have the resources for pre-training such\nmodels. However, fine-tuning requires fewer resources and is increasingly used\nby both small and large entities for its effectiveness on specialized data.\nSuch small curated data for fine-tuning might contain sensitive information or\nproprietary assets. In this study, we attack both pre-trained and fine-tuned\ncode language models to investigate the extent of data extractability. We first\ndevelop a custom benchmark to assess the vulnerability of both pre-training and\nfine-tuning samples to extraction attacks. Our findings reveal that 54.9% of\nextractable pre-training data could be retrieved from StarCoder2-15B, whereas\nthis number decreased to 23.5% after fine-tuning. This indicates that\nfine-tuning reduces the extractability of pre-training data. However, compared\nto larger models, fine-tuning smaller models increases their vulnerability to\ndata extraction attacks on fine-tuning data. Given the potential sensitivity of\nfine-tuning data, this can lead to more severe consequences. Lastly, we also\nmanually analyzed 2000 extractable samples before and after fine-tuning. We\nalso found that data carriers and licensing information are the most likely\ndata categories to be memorized from pre-trained and fine-tuned models, while\nthe latter is the most likely to be forgotten after fine-tuning."
    },
    {
        "date": "2025-01",
        "title": "Virus: Harmful Fine-tuning Attack for Large Language Models Bypassing Guardrail Moderation",
        "author": "Tiansheng Huang, Sihao Hu, Fatih Ilhan, Selim Furkan Tekin, and Ling Liu",
        "link": "http://arxiv.org/abs/2501.17433v1",
        "abstract": "Recent research shows that Large Language Models (LLMs) are vulnerable to\nharmful fine-tuning attacks -- models lose their safety alignment ability after\nfine-tuning on a few harmful samples. For risk mitigation, a guardrail is\ntypically used to filter out harmful samples before fine-tuning. By designing a\nnew red-teaming method, we in this paper show that purely relying on the\nmoderation guardrail for data filtration is not reliable. Our proposed attack\nmethod, dubbed Virus, easily bypasses the guardrail moderation by slightly\nmodifying the harmful data. Experimental results show that the harmful data\noptimized by Virus is not detectable by the guardrail with up to 100\\% leakage\nratio, and can simultaneously achieve superior attack performance. Finally, the\nkey message we want to convey through this paper is that: \\textbf{it is\nreckless to consider guardrail moderation as a clutch at straws towards harmful\nfine-tuning attack}, as it cannot solve the inherent safety issue of the\npre-trained LLMs. Our code is available at https://github.com/git-disl/Virus"
    },
    {
        "date": "2025-01",
        "title": "Reqo: A Robust and Explainable Query Optimization Cost Model",
        "author": "Baoming Chang, Amin Kamali, and Verena Kantere",
        "link": "http://arxiv.org/abs/2501.17414v1",
        "abstract": "In recent years, there has been a growing interest in using machine learning\n(ML) in query optimization to select more efficient plans. Existing\nlearning-based query optimizers use certain model architectures to convert\ntree-structured query plans into representations suitable for downstream ML\ntasks. As the design of these architectures significantly impacts cost\nestimation, we propose a tree model architecture based on Bidirectional Graph\nNeural Networks (Bi-GNN) aggregated by Gated Recurrent Units (GRUs) to achieve\nmore accurate cost estimates. The inherent uncertainty of data and model\nparameters also leads to inaccurate cost estimates, resulting in suboptimal\nplans and less robust query performance. To address this, we implement a novel\nlearning-to-rank cost model that effectively quantifies the uncertainty in cost\nestimates using approximate probabilistic ML. This model adaptively integrates\nquantified uncertainty with estimated costs and learns from comparing pairwise\nplans, achieving more robust performance. In addition, we propose the first\nexplainability technique specifically designed for learning-based cost models.\nThis technique explains the contribution of any subgraphs in the query plan to\nthe final predicted cost, which can be integrated and trained with any\nlearning-based cost model to significantly boost the model's explainability. By\nincorporating these innovations, we propose a cost model for a Robust and\nExplainable Query Optimizer, Reqo, that improves the accuracy, robustness, and\nexplainability of cost estimation, outperforming state-of-the-art approaches in\nall three dimensions."
    },
    {
        "date": "2025-01",
        "title": "When Everyday Devices Become Weapons: A Closer Look at the Pager and Walkie-talkie Attacks",
        "author": "Pantha Protim Sarker, Upoma Das, Nitin Varshney, Shang Shi, Akshay Kulkarni, Farimah Farahmandi, and Mark Tehranipoor",
        "link": "http://arxiv.org/abs/2501.17405v1",
        "abstract": "Battery-powered technologies like pagers and walkie-talkies have long been\nintegral to civilian and military operations. However, the potential for such\neveryday devices to be weaponized has largely been underestimated in the realm\nof cybersecurity. In September 2024, Lebanon experienced a series of\nunprecedented, coordinated explosions triggered through compromised pagers and\nwalkie-talkies, creating a new category of attack in the domain of\ncyber-physical warfare. This attack not only disrupted critical communication\nnetworks but also resulted in injuries, loss of life, and exposed significant\nnational security vulnerabilities, prompting governments and organizations\nworldwide to reevaluate their cybersecurity frameworks. This article provides\nan in-depth investigation into the infamous Pager and Walkie-Talkie attacks,\nanalyzing both technical and non-technical dimensions. Furthermore, the study\nextends its scope to explore vulnerabilities in other battery-powered\ninfrastructures, such as battery management systems, highlighting their\npotential exploitation. Existing prevention and detection techniques are\nreviewed, with an emphasis on their limitations and the challenges they face in\naddressing emerging threats. Finally, the article discusses emerging\nmethodologies, particularly focusing on the role of physical inspection, as a\ncritical component of future security measures. This research aims to provide\nactionable insights to bolster the resilience of cyber-physical systems in an\nincreasingly interconnected world."
    },
    {
        "date": "2025-01",
        "title": "Poisoning Attacks and Defenses to Federated Unlearning",
        "author": "Wenbin Wang, Qiwen Ma, Zifan Zhang, Yuchen Liu, Zhuqing Liu, and Minghong Fang",
        "link": "http://arxiv.org/abs/2501.17396v1",
        "abstract": "Federated learning allows multiple clients to collaboratively train a global\nmodel with the assistance of a server. However, its distributed nature makes it\nsusceptible to poisoning attacks, where malicious clients can compromise the\nglobal model by sending harmful local model updates to the server. To unlearn\nan accurate global model from a poisoned one after identifying malicious\nclients, federated unlearning has been introduced. Yet, current research on\nfederated unlearning has primarily concentrated on its effectiveness and\nefficiency, overlooking the security challenges it presents. In this work, we\nbridge the gap via proposing BadUnlearn, the first poisoning attacks targeting\nfederated unlearning. In BadUnlearn, malicious clients send specifically\ndesigned local model updates to the server during the unlearning process,\naiming to ensure that the resulting unlearned model remains poisoned. To\nmitigate these threats, we propose UnlearnGuard, a robust federated unlearning\nframework that is provably robust against both existing poisoning attacks and\nour BadUnlearn. The core concept of UnlearnGuard is for the server to estimate\nthe clients' local model updates during the unlearning process and employ a\nfiltering strategy to verify the accuracy of these estimations. Theoretically,\nwe prove that the model unlearned through UnlearnGuard closely resembles one\nobtained by train-from-scratch. Empirically, we show that BadUnlearn can\neffectively corrupt existing federated unlearning methods, while UnlearnGuard\nremains secure against poisoning attacks."
    },
    {
        "date": "2025-01",
        "title": "Byzantine-Robust Federated Learning over Ring-All-Reduce Distributed Computing",
        "author": "Minghong Fang, Zhuqing Liu, Xuecen Zhao, and Jia Liu",
        "link": "http://arxiv.org/abs/2501.17392v1",
        "abstract": "Federated learning (FL) has gained attention as a distributed learning\nparadigm for its data privacy benefits and accelerated convergence through\nparallel computation. Traditional FL relies on a server-client (SC)\narchitecture, where a central server coordinates multiple clients to train a\nglobal model, but this approach faces scalability challenges due to server\ncommunication bottlenecks. To overcome this, the ring-all-reduce (RAR)\narchitecture has been introduced, eliminating the central server and achieving\nbandwidth optimality. However, the tightly coupled nature of RAR's ring\ntopology exposes it to unique Byzantine attack risks not present in SC-based\nFL. Despite its potential, designing Byzantine-robust RAR-based FL algorithms\nremains an open problem. To address this gap, we propose BRACE\n(Byzantine-robust ring-all-reduce), the first RAR-based FL algorithm to achieve\nboth Byzantine robustness and communication efficiency. We provide theoretical\nguarantees for the convergence of BRACE under Byzantine attacks, demonstrate\nits bandwidth efficiency, and validate its practical effectiveness through\nexperiments. Our work offers a foundational understanding of Byzantine-robust\nRAR-based FL design."
    },
    {
        "date": "2025-01",
        "title": "A Dual-Agent Adversarial Framework for Robust Generalization in Deep Reinforcement Learning",
        "author": "Zhengpeng Xie, Jiahang Cao, Yulong Zhang, Qiang Zhang, and Renjing Xu",
        "link": "http://arxiv.org/abs/2501.17384v1",
        "abstract": "Recently, empowered with the powerful capabilities of neural networks,\nreinforcement learning (RL) has successfully tackled numerous challenging\ntasks. However, while these models demonstrate enhanced decision-making\nabilities, they are increasingly prone to overfitting. For instance, a trained\nRL model often fails to generalize to even minor variations of the same task,\nsuch as a change in background color or other minor semantic differences. To\naddress this issue, we propose a dual-agent adversarial policy learning\nframework, which allows agents to spontaneously learn the underlying semantics\nwithout introducing any human prior knowledge. Specifically, our framework\ninvolves a game process between two agents: each agent seeks to maximize the\nimpact of perturbing on the opponent's policy by producing representation\ndifferences for the same state, while maintaining its own stability against\nsuch perturbations. This interaction encourages agents to learn generalizable\npolicies, capable of handling irrelevant features from the high-dimensional\nobservations. Extensive experimental results on the Procgen benchmark\ndemonstrate that the adversarial process significantly improves the\ngeneralization performance of both agents, while also being applied to various\nRL algorithms, e.g., Proximal Policy Optimization (PPO). With the adversarial\nframework, the RL agent outperforms the baseline methods by a significant\nmargin, especially in hard-level tasks, marking a significant step forward in\nthe generalization capabilities of deep reinforcement learning."
    },
    {
        "date": "2025-01",
        "title": "Do We Really Need to Design New Byzantine-robust Aggregation Rules?",
        "author": "Minghong Fang, Seyedsina Nabavirazavi, Zhuqing Liu, Wei Sun, Sundararaja Sitharama Iyengar, and Haibo Yang",
        "link": "http://arxiv.org/abs/2501.17381v1",
        "abstract": "Federated learning (FL) allows multiple clients to collaboratively train a\nglobal machine learning model through a server, without exchanging their\nprivate training data. However, the decentralized aspect of FL makes it\nsusceptible to poisoning attacks, where malicious clients can manipulate the\nglobal model by sending altered local model updates. To counter these attacks,\na variety of aggregation rules designed to be resilient to Byzantine failures\nhave been introduced. Nonetheless, these methods can still be vulnerable to\nsophisticated attacks or depend on unrealistic assumptions about the server. In\nthis paper, we demonstrate that there is no need to design new Byzantine-robust\naggregation rules; instead, FL can be secured by enhancing the robustness of\nwell-established aggregation rules. To this end, we present FoundationFL, a\nnovel defense mechanism against poisoning attacks. FoundationFL involves the\nserver generating synthetic updates after receiving local model updates from\nclients. It then applies existing Byzantine-robust foundational aggregation\nrules, such as Trimmed-mean or Median, to combine clients' model updates with\nthe synthetic ones. We theoretically establish the convergence performance of\nFoundationFL under Byzantine settings. Comprehensive experiments across several\nreal-world datasets validate the efficiency of our FoundationFL method."
    },
    {
        "date": "2025-01",
        "title": "Enabling Low-Cost Secure Computing on Untrusted In-Memory Architectures",
        "author": "Sahar Ghoflsaz Ghinani, Jingyao Zhang, and Elaheh Sadredini",
        "link": "http://arxiv.org/abs/2501.17292v1",
        "abstract": "Modern computing systems are limited in performance by the memory bandwidth\navailable to processors, a problem known as the memory wall.\nProcessing-in-Memory (PIM) promises to substantially improve this problem by\nmoving processing closer to the data, improving effective data bandwidth, and\nleading to superior performance on memory-intensive workloads. However,\nintegrating PIM modules within a secure computing system raises an interesting\nchallenge: unencrypted data has to move off-chip to the PIM, exposing the data\nto attackers and breaking assumptions on Trusted Computing Bases (TCBs). To\ntackle this challenge, this paper leverages multi-party computation (MPC)\ntechniques, specifically arithmetic secret sharing and Yao's garbled circuits,\nto outsource bandwidth-intensive computation securely to PIM. Additionally, we\nleverage precomputation optimization to prevent the CPU's portion of the MPC\nfrom becoming a bottleneck. We evaluate our approach using the UPMEM PIM system\nover various applications such as Deep Learning Recommendation Model inference\nand Logistic Regression. Our evaluations demonstrate up to a $14.66\\times$\nspeedup compared to a secure CPU configuration while maintaining data\nconfidentiality and integrity when outsourcing linear and/or nonlinear\ncomputation."
    },
    {
        "date": "2025-01",
        "title": "Hybrid Deep Learning Model for Multiple Cache Side Channel Attacks Detection: A Comparative Analysis",
        "author": "Tejal Joshi, Aarya Kawalay, Anvi Jamkhande, and Amit Joshi",
        "link": "http://arxiv.org/abs/2501.17123v1",
        "abstract": "Cache side channel attacks are a sophisticated and persistent threat that\nexploit vulnerabilities in modern processors to extract sensitive information.\nThese attacks leverage weaknesses in shared computational resources,\nparticularly the last level cache, to infer patterns in data access and\nexecution flows, often bypassing traditional security defenses. Such attacks\nare especially dangerous as they can be executed remotely without requiring\nphysical access to the victim's device. This study focuses on a specific class\nof these threats: fingerprinting attacks, where an adversary monitors and\nanalyzes the behavior of co-located processes via cache side channels. This can\npotentially reveal confidential information, such as encryption keys or user\nactivity patterns. A comprehensive threat model illustrates how attackers\nsharing computational resources with target systems exploit these side channels\nto compromise sensitive data. To mitigate such risks, a hybrid deep learning\nmodel is proposed for detecting cache side channel attacks. Its performance is\ncompared with five widely used deep learning models: Multi-Layer Perceptron,\nConvolutional Neural Network, Simple Recurrent Neural Network, Long Short-Term\nMemory, and Gated Recurrent Unit. The experimental results demonstrate that the\nhybrid model achieves a detection rate of up to 99.96%. These findings\nhighlight the limitations of existing models, the need for enhanced defensive\nmechanisms, and directions for future research to secure sensitive data against\nevolving side channel threats."
    },
    {
        "date": "2025-01",
        "title": "Graph of Attacks with Pruning: Optimizing Stealthy Jailbreak Prompt Generation for Enhanced LLM Content Moderation",
        "author": "Daniel Schwartz, Dmitriy Bespalov, Zhe Wang, Ninad Kulkarni, and Yanjun Qi",
        "link": "http://arxiv.org/abs/2501.18638v1",
        "abstract": "We present a modular pipeline that automates the generation of stealthy\njailbreak prompts derived from high-level content policies, enhancing LLM\ncontent moderation. First, we address query inefficiency and jailbreak strength\nby developing Graph of Attacks with Pruning (GAP), a method that utilizes\nstrategies from prior jailbreaks, resulting in 92% attack success rate on\nGPT-3.5 using only 54% of the queries of the prior algorithm. Second, we\naddress the cold-start issue by automatically generating seed prompts from the\nhigh-level policy using LLMs. Finally, we demonstrate the utility of these\ngenerated jailbreak prompts of improving content moderation by fine-tuning\nPromptGuard, a model trained to detect jailbreaks, increasing its accuracy on\nthe Toxic-Chat dataset from 5.1% to 93.89%."
    },
    {
        "date": "2025-01",
        "title": "Machine learning of microstructure--property relationships in materials with robust features from foundational vision transformers",
        "author": "Sheila E. Whitman, and Marat I. Latypov",
        "link": "http://arxiv.org/abs/2501.18637v1",
        "abstract": "Machine learning of microstructure--property relationships from data is an\nemerging approach in computational materials science. Most existing machine\nlearning efforts focus on the development of task-specific models for each\nmicrostructure--property relationship. We propose utilizing pre-trained\nfoundational vision transformers for the extraction of task-agnostic\nmicrostructure features and subsequent light-weight machine learning of a\nmicrostructure-dependent property. We demonstrate our approach with pre-trained\nstate-of-the-art vision transformers (CLIP, DINOV2, SAM) in two case studies on\nmachine-learning: (i) elastic modulus of two-phase microstructures based on\nsimulations data; and (ii) Vicker's hardness of Ni-base and Co-base superalloys\nbased on experimental data published in literature. Our results show the\npotential of foundational vision transformers for robust microstructure\nrepresentation and efficient machine learning of microstructure--property\nrelationships without the need for expensive task-specific training or\nfine-tuning of bespoke deep learning models."
    },
    {
        "date": "2025-01",
        "title": "SafeRAG: Benchmarking Security in Retrieval-Augmented Generation of Large Language Model",
        "author": "Xun Liang, Simin Niu, Zhiyu Li, Sensen Zhang, Hanyu Wang, Feiyu Xiong, Jason Zhaoxin Fan, Bo Tang, Shichao Song, Mengwei Wang, and Jiawei Yang",
        "link": "http://arxiv.org/abs/2501.18636v1",
        "abstract": "The indexing-retrieval-generation paradigm of retrieval-augmented generation\n(RAG) has been highly successful in solving knowledge-intensive tasks by\nintegrating external knowledge into large language models (LLMs). However, the\nincorporation of external and unverified knowledge increases the vulnerability\nof LLMs because attackers can perform attack tasks by manipulating knowledge.\nIn this paper, we introduce a benchmark named SafeRAG designed to evaluate the\nRAG security. First, we classify attack tasks into silver noise, inter-context\nconflict, soft ad, and white Denial-of-Service. Next, we construct RAG security\nevaluation dataset (i.e., SafeRAG dataset) primarily manually for each task. We\nthen utilize the SafeRAG dataset to simulate various attack scenarios that RAG\nmay encounter. Experiments conducted on 14 representative RAG components\ndemonstrate that RAG exhibits significant vulnerability to all attack tasks and\neven the most apparent attack task can easily bypass existing retrievers,\nfilters, or advanced LLMs, resulting in the degradation of RAG service quality.\nCode is available at: https://github.com/IAAR-Shanghai/SafeRAG."
    },
    {
        "date": "2025-01",
        "title": "Context is Key for Agent Security",
        "author": "Lillian Tsai, and Eugene Bagdasarian",
        "link": "http://arxiv.org/abs/2501.17070v2",
        "abstract": "Judging the safety of an action, whether taken by a human or a system, must\ntake into account the context in which the action takes place. For example,\ndeleting an email from a user's mailbox may or may not be appropriate depending\non the email's content, the user's goals, or even available space. Systems\ntoday that make these judgements -- providing security against harmful or\ninappropriate actions -- rely on manually-crafted policies or user confirmation\nfor each relevant context. With the upcoming deployment of systems like\ngeneralist agents, we argue that we must rethink security designs to adapt to\nthe scale of contexts and capabilities of these systems. As a first step, this\npaper explores contextual security in the domain of agents and proposes\ncontextual security for agents (Conseca), a framework to generate just-in-time,\ncontextual, and human-verifiable security policies."
    },
    {
        "date": "2025-01",
        "title": "RODEO: Robust Outlier Detection via Exposing Adaptive Out-of-Distribution Samples",
        "author": "Hossein Mirzaei, Mohammad Jafari, Hamid Reza Dehbashi, Ali Ansari, Sepehr Ghobadi, Masoud Hadi, Arshia Soltani Moakhar, Mohammad Azizmalayeri, Mahdieh Soleymani Baghshah, and Mohammad Hossein Rohban",
        "link": "http://arxiv.org/abs/2501.16971v1",
        "abstract": "In recent years, there have been significant improvements in various forms of\nimage outlier detection. However, outlier detection performance under\nadversarial settings lags far behind that in standard settings. This is due to\nthe lack of effective exposure to adversarial scenarios during training,\nespecially on unseen outliers, leading to detection models failing to learn\nrobust features. To bridge this gap, we introduce RODEO, a data-centric\napproach that generates effective outliers for robust outlier detection. More\nspecifically, we show that incorporating outlier exposure (OE) and adversarial\ntraining can be an effective strategy for this purpose, as long as the exposed\ntraining outliers meet certain characteristics, including diversity, and both\nconceptual differentiability and analogy to the inlier samples. We leverage a\ntext-to-image model to achieve this goal. We demonstrate both quantitatively\nand qualitatively that our adaptive OE method effectively generates ``diverse''\nand ``near-distribution'' outliers, leveraging information from both text and\nimage domains. Moreover, our experimental results show that utilizing our\nsynthesized outliers significantly enhances the performance of the outlier\ndetector, particularly in adversarial settings."
    },
    {
        "date": "2025-01",
        "title": "Few Edges Are Enough: Few-Shot Network Attack Detection with Graph Neural Networks",
        "author": "Tristan Bilot, Nour El Madhoun, Khaldoun Al Agha, and Anis Zouaoui",
        "link": "http://arxiv.org/abs/2501.16964v1",
        "abstract": "Detecting cyberattacks using Graph Neural Networks (GNNs) has seen promising\nresults recently. Most of the state-of-the-art models that leverage these\ntechniques require labeled examples, hard to obtain in many real-world\nscenarios. To address this issue, unsupervised learning and Self-Supervised\nLearning (SSL) have emerged as interesting approaches to reduce the dependency\non labeled data. Nonetheless, these methods tend to yield more anomalous\ndetection algorithms rather than effective attack detection systems. This paper\nintroduces Few Edges Are Enough (FEAE), a GNN-based architecture trained with\nSSL and Few-Shot Learning (FSL) to better distinguish between false positive\nanomalies and actual attacks. To maximize the potential of few-shot examples,\nour model employs a hybrid self-supervised objective that combines the\nadvantages of contrastive-based and reconstruction-based SSL. By leveraging\nonly a minimal number of labeled attack events, represented as attack edges,\nFEAE achieves competitive performance on two well-known network datasets\ncompared to both supervised and unsupervised methods. Remarkably, our\nexperimental results unveil that employing only 1 malicious event for each\nattack type in the dataset is sufficient to achieve substantial improvements.\nFEAE not only outperforms self-supervised GNN baselines but also surpasses some\nsupervised approaches on one of the datasets."
    },
    {
        "date": "2025-01",
        "title": "Stack Overflow Meets Replication: Security Research Amid Evolving Code Snippets (Extended Version)",
        "author": "Alfusainey Jallow, and Sven Bugiel",
        "link": "http://arxiv.org/abs/2501.16948v2",
        "abstract": "We study the impact of Stack Overflow code evolution on the stability of\nprior research findings derived from Stack Overflow data and provide\nrecommendations for future studies. We systematically reviewed papers published\nbetween 2005--2023 to identify key aspects of Stack Overflow that can affect\nstudy results, such as the language or context of code snippets. Our analysis\nreveals that certain aspects are non-stationary over time, which could lead to\ndifferent conclusions if experiments are repeated at different times. We\nreplicated six studies using a more recent dataset to demonstrate this risk.\nOur findings show that four papers produced significantly different results\nthan the original findings, preventing the same conclusions from being drawn\nwith a newer dataset version. Consequently, we recommend treating Stack\nOverflow as a time series data source to provide context for interpreting\ncross-sectional research conclusions."
    },
    {
        "date": "2025-01",
        "title": "Projection-free Algorithms for Online Convex Optimization with Adversarial Constraints",
        "author": "Dhruv Sarkar, Aprameyo Chakrabartty, Subhamon Supantha, Palash Dey, and Abhishek Sinha",
        "link": "http://arxiv.org/abs/2501.16919v1",
        "abstract": "We study a generalization of the Online Convex Optimization (OCO) framework\nwith time-varying adversarial constraints. In this problem, after selecting a\nfeasible action from the convex decision set $X,$ a convex constraint function\nis revealed alongside the cost function in each round. Our goal is to design a\ncomputationally efficient learning policy that achieves a small regret with\nrespect to the cost functions and a small cumulative constraint violation (CCV)\nwith respect to the constraint functions over a horizon of length $T$. It is\nwell-known that the projection step constitutes the major computational\nbottleneck of the standard OCO algorithms. However, for many structured\ndecision sets, linear functions can be efficiently optimized over the decision\nset. We propose a *projection-free* online policy which makes a single call to\na Linear Program (LP) solver per round. Our method outperforms state-of-the-art\nprojection-free online algorithms with adversarial constraints, achieving\nimproved bounds of $\\tilde{O}(T^{\\frac{3}{4}})$ for both regret and CCV. The\nproposed algorithm is conceptually simple - it first constructs a surrogate\ncost function as a non-negative linear combination of the cost and constraint\nfunctions. Then, it passes the surrogate costs to a new, adaptive version of\nthe online conditional gradient subroutine, which we propose in this paper."
    },
    {
        "date": "2025-01",
        "title": "Adversarial Masked Autoencoder Purifier with Defense Transferability",
        "author": "Yuan-Chih Chen, and Chun-Shien Lu",
        "link": "http://arxiv.org/abs/2501.16904v1",
        "abstract": "The study of adversarial defense still struggles to combat with advanced\nadversarial attacks. In contrast to most prior studies that rely on the\ndiffusion model for test-time defense to remarkably increase the inference\ntime, we propose Masked AutoEncoder Purifier (MAEP), which integrates Masked\nAutoEncoder (MAE) into an adversarial purifier framework for test-time\npurification. While MAEP achieves promising adversarial robustness, it\nparticularly features model defense transferability and attack generalization\nwithout relying on using additional data that is different from the training\ndataset. To our knowledge, MAEP is the first study of adversarial purifier\nbased on MAE. Extensive experimental results demonstrate that our method can\nnot only maintain clear accuracy with only a slight drop but also exhibit a\nclose gap between the clean and robust accuracy. Notably, MAEP trained on\nCIFAR10 achieves state-of-the-art performance even when tested directly on\nImageNet, outperforming existing diffusion-based models trained specifically on\nImageNet."
    },
    {
        "date": "2025-01",
        "title": "RAINER: A Robust Ensemble Learning Grid Search-Tuned Framework for Rainfall Patterns Prediction",
        "author": "Zhenqi Li, Junhao Zhong, Hewei Wang, Jinfeng Xu, Yijie Li, Jinjiang You, Jiayi Zhang, Runzhi Wu, and Soumyabrata Dev",
        "link": "http://arxiv.org/abs/2501.16900v1",
        "abstract": "Rainfall prediction remains a persistent challenge due to the highly\nnonlinear and complex nature of meteorological data. Existing approaches lack\nsystematic utilization of grid search for optimal hyperparameter tuning,\nrelying instead on heuristic or manual selection, frequently resulting in\nsub-optimal results. Additionally, these methods rarely incorporate newly\nconstructed meteorological features such as differences between temperature and\nhumidity to capture critical weather dynamics. Furthermore, there is a lack of\nsystematic evaluation of ensemble learning techniques and limited exploration\nof diverse advanced models introduced in the past one or two years. To address\nthese limitations, we propose a robust ensemble learning grid search-tuned\nframework (RAINER) for rainfall prediction. RAINER incorporates a comprehensive\nfeature engineering pipeline, including outlier removal, imputation of missing\nvalues, feature reconstruction, and dimensionality reduction via Principal\nComponent Analysis (PCA). The framework integrates novel meteorological\nfeatures to capture dynamic weather patterns and systematically evaluates\nnon-learning mathematical-based methods and a variety of machine learning\nmodels, from weak classifiers to advanced neural networks such as\nKolmogorov-Arnold Networks (KAN). By leveraging grid search for hyperparameter\ntuning and ensemble voting techniques, RAINER achieves promising results within\nreal-world datasets."
    },
    {
        "date": "2025-01",
        "title": "Secure Federated Graph-Filtering for Recommender Systems",
        "author": "Julien Nicolas, C\u00e9sar Sabater, Mohamed Maouche, Sonia Ben Mokhtar, and Mark Coates",
        "link": "http://arxiv.org/abs/2501.16888v1",
        "abstract": "Recommender systems often rely on graph-based filters, such as normalized\nitem-item adjacency matrices and low-pass filters. While effective, the\ncentralized computation of these components raises concerns about privacy,\nsecurity, and the ethical use of user data. This work proposes two\ndecentralized frameworks for securely computing these critical graph components\nwithout centralizing sensitive information. The first approach leverages\nlightweight Multi-Party Computation and distributed singular vector\ncomputations to privately compute key graph filters. The second extends this\nframework by incorporating low-rank approximations, enabling a trade-off\nbetween communication efficiency and predictive performance. Empirical\nevaluations on benchmark datasets demonstrate that the proposed methods achieve\ncomparable accuracy to centralized state-of-the-art systems while ensuring data\nconfidentiality and maintaining low communication costs. Our results highlight\nthe potential for privacy-preserving decentralized architectures to bridge the\ngap between utility and user data protection in modern recommender systems."
    },
    {
        "date": "2025-01",
        "title": "Bones of Contention: Exploring Query-Efficient Attacks Against Skeleton Recognition Systems",
        "author": "Yuxin Cao, Kai Ye, Derui Wang, Minhui Xue, Hao Ge, Chenxiong Qian, and Jin Song Dong",
        "link": "http://arxiv.org/abs/2501.16843v1",
        "abstract": "Skeleton action recognition models have secured more attention than\nvideo-based ones in various applications due to privacy preservation and lower\nstorage requirements. Skeleton data are typically transmitted to cloud servers\nfor action recognition, with results returned to clients via Apps/APIs.\nHowever, the vulnerability of skeletal models against adversarial perturbations\ngradually reveals the unreliability of these systems. Existing black-box\nattacks all operate in a decision-based manner, resulting in numerous queries\nthat hinder efficiency and feasibility in real-world applications. Moreover,\nall attacks off the shelf focus on only restricted perturbations, while\nignoring model weaknesses when encountered with non-semantic perturbations. In\nthis paper, we propose two query-effIcient Skeletal Adversarial AttaCks,\nISAAC-K and ISAAC-N. As a black-box attack, ISAAC-K utilizes Grad-CAM in a\nsurrogate model to extract key joints where minor sparse perturbations are then\nadded to fool the classifier. To guarantee natural adversarial motions, we\nintroduce constraints of both bone length and temporal consistency. ISAAC-K\nfinds stronger adversarial examples on $\\ell_\\infty$ norm, which can encompass\nthose on other norms. Exhaustive experiments substantiate that ISAAC-K can\nuplift the attack efficiency of the perturbations under 10 skeletal models.\nAdditionally, as a byproduct, ISAAC-N fools the classifier by replacing\nskeletons unrelated to the action. We surprisingly find that skeletal models\nare vulnerable to large perturbations where the part-wise non-semantic joints\nare just replaced, leading to a query-free no-box attack without any prior\nknowledge. Based on that, four adaptive defenses are eventually proposed to\nimprove the robustness of skeleton recognition models."
    },
    {
        "date": "2025-01",
        "title": "TORCHLIGHT: Shedding LIGHT on Real-World Attacks on Cloudless IoT Devices Concealed within the Tor Network",
        "author": "Yumingzhi Pan, Zhen Ling, Yue Zhang, Hongze Wang, Guangchi Liu, Junzhou Luo, and Xinwen Fu",
        "link": "http://arxiv.org/abs/2501.16784v1",
        "abstract": "The rapidly expanding Internet of Things (IoT) landscape is shifting toward\ncloudless architectures, removing reliance on centralized cloud services but\nexposing devices directly to the internet and increasing their vulnerability to\ncyberattacks. Our research revealed an unexpected pattern of substantial Tor\nnetwork traffic targeting cloudless IoT devices. suggesting that attackers are\nusing Tor to anonymously exploit undisclosed vulnerabilities (possibly obtained\nfrom underground markets). To delve deeper into this phenomenon, we developed\nTORCHLIGHT, a tool designed to detect both known and unknown threats targeting\ncloudless IoT devices by analyzing Tor traffic. TORCHLIGHT filters traffic via\nspecific IP patterns, strategically deploys virtual private server (VPS) nodes\nfor cost-effective detection, and uses a chain-of-thought (CoT) process with\nlarge language models (LLMs) for accurate threat identification.\n  Our results are significant: for the first time, we have demonstrated that\nattackers are indeed using Tor to conceal their identities while targeting\ncloudless IoT devices. Over a period of 12 months, TORCHLIGHT analyzed 26 TB of\ntraffic, revealing 45 vulnerabilities, including 29 zero-day exploits with 25\nCVE-IDs assigned (5 CRITICAL, 3 HIGH, 16 MEDIUM, and 1 LOW) and an estimated\nvalue of approximately $312,000. These vulnerabilities affect around 12.71\nmillion devices across 148 countries, exposing them to severe risks such as\ninformation disclosure, authentication bypass, and arbitrary command execution.\nThe findings have attracted significant attention, sparking widespread\ndiscussion in cybersecurity circles, reaching the top 25 on Hacker News, and\ngenerating over 190,000 views."
    },
    {
        "date": "2025-01",
        "title": "Information security control as a task of control a dynamic system",
        "author": "Sergey Masaev, Andrey Minkin, Yuri Bezborodov, Dmitry Edimichev, and Yass Salal",
        "link": "http://arxiv.org/abs/2501.16732v1",
        "abstract": "The area of research includes control theory, dynamic systems, parameters of\nthe external environment, mode, integral indicators, British standards. The\nmain idea of the article is information security. The activity of a large-scale\nobject (enterprise) is considered. The activity of the enterprise is presented\nas a multidimensional dynamic system and is displayed as a digital copy of 1.2\nmillion parameters. A British digital copy-based information security standard\nis being introduced. Information security equipment and software were\npurchased. The training of the company's personnel was completed. Evaluation of\nimplementation (activities) is done as an integral indicator. The dynamics of\nthe integral indicator assesses the implementation of the British standard."
    },
    {
        "date": "2025-01",
        "title": "DFCon: Attention-Driven Supervised Contrastive Learning for Robust Deepfake Detection",
        "author": "MD Sadik Hossain Shanto, Mahir Labib Dihan, Souvik Ghosh, Riad Ahmed Anonto, Hafijul Hoque Chowdhury, Abir Muhtasim, Rakib Ahsan, MD Tanvir Hassan, MD Roqunuzzaman Sojib, Sheikh Azizul Hakim, and M. Saifur Rahman",
        "link": "http://arxiv.org/abs/2501.16704v1",
        "abstract": "This report presents our approach for the IEEE SP Cup 2025: Deepfake Face\nDetection in the Wild (DFWild-Cup), focusing on detecting deepfakes across\ndiverse datasets. Our methodology employs advanced backbone models, including\nMaxViT, CoAtNet, and EVA-02, fine-tuned using supervised contrastive loss to\nenhance feature separation. These models were specifically chosen for their\ncomplementary strengths. Integration of convolution layers and strided\nattention in MaxViT is well-suited for detecting local features. In contrast,\nhybrid use of convolution and attention mechanisms in CoAtNet effectively\ncaptures multi-scale features. Robust pretraining with masked image modeling of\nEVA-02 excels at capturing global features. After training, we freeze the\nparameters of these models and train the classification heads. Finally, a\nmajority voting ensemble is employed to combine the predictions from these\nmodels, improving robustness and generalization to unseen scenarios. The\nproposed system addresses the challenges of detecting deepfakes in real-world\nconditions and achieves a commendable accuracy of 95.83% on the validation\ndataset."
    },
    {
        "date": "2025-01",
        "title": "Data-Free Model-Related Attacks: Unleashing the Potential of Generative AI",
        "author": "Dayong Ye, Tianqing Zhu, Shang Wang, Bo Liu, Leo Yu Zhang, Wanlei Zhou, and Yang Zhang",
        "link": "http://arxiv.org/abs/2501.16671v1",
        "abstract": "Generative AI technology has become increasingly integrated into our daily\nlives, offering powerful capabilities to enhance productivity. However, these\nsame capabilities can be exploited by adversaries for malicious purposes. While\nexisting research on adversarial applications of generative AI predominantly\nfocuses on cyberattacks, less attention has been given to attacks targeting\ndeep learning models. In this paper, we introduce the use of generative AI for\nfacilitating model-related attacks, including model extraction, membership\ninference, and model inversion. Our study reveals that adversaries can launch a\nvariety of model-related attacks against both image and text models in a\ndata-free and black-box manner, achieving comparable performance to baseline\nmethods that have access to the target models' training data and parameters in\na white-box manner. This research serves as an important early warning to the\ncommunity about the potential risks associated with generative AI-powered\nattacks on deep learning models."
    },
    {
        "date": "2025-01",
        "title": "Data Duplication: A Novel Multi-Purpose Attack Paradigm in Machine Unlearning",
        "author": "Dayong Ye, Tainqing Zhu, Jiayang Li, Kun Gao, Bo Liu, Leo Yu Zhang, Wanlei Zhou, and Yang Zhang",
        "link": "http://arxiv.org/abs/2501.16663v1",
        "abstract": "Duplication is a prevalent issue within datasets. Existing research has\ndemonstrated that the presence of duplicated data in training datasets can\nsignificantly influence both model performance and data privacy. However, the\nimpact of data duplication on the unlearning process remains largely\nunexplored. This paper addresses this gap by pioneering a comprehensive\ninvestigation into the role of data duplication, not only in standard machine\nunlearning but also in federated and reinforcement unlearning paradigms.\nSpecifically, we propose an adversary who duplicates a subset of the target\nmodel's training set and incorporates it into the training set. After training,\nthe adversary requests the model owner to unlearn this duplicated subset, and\nanalyzes the impact on the unlearned model. For example, the adversary can\nchallenge the model owner by revealing that, despite efforts to unlearn it, the\ninfluence of the duplicated subset remains in the model. Moreover, to\ncircumvent detection by de-duplication techniques, we propose three novel\nnear-duplication methods for the adversary, each tailored to a specific\nunlearning paradigm. We then examine their impacts on the unlearning process\nwhen de-duplication techniques are applied. Our findings reveal several crucial\ninsights: 1) the gold standard unlearning method, retraining from scratch,\nfails to effectively conduct unlearning under certain conditions; 2) unlearning\nduplicated data can lead to significant model degradation in specific\nscenarios; and 3) meticulously crafted duplicates can evade detection by\nde-duplication methods."
    },
    {
        "date": "2025-01",
        "title": "Analysis of Zero Day Attack Detection Using MLP and XAI",
        "author": "Ashim Dahal, Prabin Bajgai, and Nick Rahimi",
        "link": "http://arxiv.org/abs/2501.16638v1",
        "abstract": "Any exploit taking advantage of zero-day is called a zero-day attack.\nPrevious research and social media trends show a massive demand for research in\nzero-day attack detection. This paper analyzes Machine Learning (ML) and Deep\nLearning (DL) based approaches to create Intrusion Detection Systems (IDS) and\nscrutinizing them using Explainable AI (XAI) by training an explainer based on\nrandomly sampled data from the testing set. The focus is on using the KDD99\ndataset, which has the most research done among all the datasets for detecting\nzero-day attacks. The paper aims to synthesize the dataset to have fewer\nclasses for multi-class classification, test ML and DL approaches on pattern\nrecognition, establish the robustness and dependability of the model, and\nestablish the interpretability and scalability of the model. We evaluated the\nperformance of four multilayer perceptron (MLP) trained on the KDD99 dataset,\nincluding baseline ML models, weighted ML models, truncated ML models, and\nweighted truncated ML models. Our results demonstrate that the truncated ML\nmodel achieves the highest accuracy (99.62%), precision, and recall, while\nweighted truncated ML model shows lower accuracy (97.26%) but better class\nrepresentation (less bias) among all the classes with improved unweighted\nrecall score. We also used Shapely Additive exPlanations (SHAP) to train\nexplainer for our truncated models to check for feature importance among the\ntwo weighted and unweighted models."
    },
    {
        "date": "2025-01",
        "title": "SHIELD: Secure Host-Independent Extensible Logging for SATA/Network Storage Towards Ransomware Detection",
        "author": "Md Raz, P. V. Sai Charan, Prashanth Krishnamurthy, Farshad Khorrami, and Ramesh Karri",
        "link": "http://arxiv.org/abs/2501.16619v1",
        "abstract": "As malware such as ransomware becomes sophisticated, the ability to find and\nneutralize it requires more robust and tamper-resistant solutions. Current\nmethods rely on data from compromised hosts, lack hardware isolation, and\ncannot detect emerging threats. To address these limitations, we introduce\nSHIELD - a detection architecture leveraging FPGA-based open-source SATA and\nNetwork Block Device (NBD) technology to provide off-host, tamper-proof\nmeasurements for continuous observation of disk activity for software executing\non a target device. SHIELD provides three distinct contributions: It (1)\ndevelops a framework to obtain and analyze multi-level hardware metrics at NBD,\nFPGA, and SATA storage levels, and shows their ability to differentiate between\nharmless and malicious software; (2) Broadens the functionality of an\nopen-source FPGA-driven SATA Host Bus Adapter (HBA) to offer complete data\nstorage capabilities through NBD without relying on the host system; (3)\nProvides a foundation for using the methodology and metrics in automated\nmachine learning-assisted detection and ASIC integration for advanced\nmitigation capabilities in data storage devices. SHIELD analyzes 10 benign\nprograms and 10 modern ransomware families to illustrate its capacity for\nreal-time monitoring and use in distinguishing between ransomware and benign\nsoftware. Experimental evidence shows SHIELD's robust host-independent and\nhardware-assisted metrics are a basis for detection, allowing to observe\nprogram execution and detect malicious activities at the storage level."
    },
    {
        "date": "2025-01",
        "title": "UniPET-SPK: A Unified Framework for Parameter-Efficient Tuning of Pre-trained Speech Models for Robust Speaker Verification",
        "author": "Mufan Sang, and John H. L. Hansen",
        "link": "http://arxiv.org/abs/2501.16542v1",
        "abstract": "With excellent generalization ability, SSL speech models have shown\nimpressive performance on various downstream tasks in the pre-training and\nfine-tuning paradigm. However, as the size of pre-trained models grows,\nfine-tuning becomes practically unfeasible due to expanding computation and\nstorage requirements and the risk of overfitting. This study explores\nparameter-efficient tuning (PET) methods for adapting large-scale pre-trained\nSSL speech models to speaker verification task. Correspondingly, we propose\nthree PET methods: (i)an adapter-tuning method, (ii)a prompt-tuning method, and\n(iii)a unified framework that effectively incorporates adapter-tuning and\nprompt-tuning with a dynamically learnable gating mechanism. First, we propose\nthe Inner+Inter Adapter framework, which inserts two types of adapters into\npre-trained models, allowing for adaptation of latent features within the\nintermediate Transformer layers and output embeddings from all Transformer\nlayers, through a parallel adapter design. Second, we propose the Deep Speaker\nPrompting method that concatenates trainable prompt tokens into the input space\nof pre-trained models to guide adaptation. Lastly, we propose the UniPET-SPK, a\nunified framework that effectively incorporates these two alternate PET methods\ninto a single framework with a dynamic trainable gating mechanism. The proposed\nUniPET-SPK learns to find the optimal mixture of PET methods to match different\ndatasets and scenarios. We conduct a comprehensive set of experiments on\nseveral datasets to validate the effectiveness of the proposed PET methods.\nExperimental results on VoxCeleb, CN-Celeb, and 1st 48-UTD forensic datasets\ndemonstrate that the proposed UniPET-SPK consistently outperforms the two PET\nmethods, fine-tuning, and other parameter-efficient tuning methods, achieving\nsuperior performance while updating only 5.4% of the parameters."
    },
    {
        "date": "2025-01",
        "title": "Smoothed Embeddings for Robust Language Models",
        "author": "Ryo Hase, Md Rafi Ur Rashid, Ashley Lewis, Jing Liu, Toshiaki Koike-Akino, Kieran Parsons, and Ye Wang",
        "link": "http://arxiv.org/abs/2501.16497v1",
        "abstract": "Improving the safety and reliability of large language models (LLMs) is a\ncrucial aspect of realizing trustworthy AI systems. Although alignment methods\naim to suppress harmful content generation, LLMs are often still vulnerable to\njailbreaking attacks that employ adversarial inputs that subvert alignment and\ninduce harmful outputs. We propose the Randomized Embedding Smoothing and Token\nAggregation (RESTA) defense, which adds random noise to the embedding vectors\nand performs aggregation during the generation of each output token, with the\naim of better preserving semantic information. Our experiments demonstrate that\nour approach achieves superior robustness versus utility tradeoffs compared to\nthe baseline defenses."
    },
    {
        "date": "2025-01",
        "title": "Towards Robust Stability Prediction in Smart Grids: GAN-based Approach under Data Constraints and Adversarial Challenges",
        "author": "Emad Efatinasab, Alessandro Brighente, Denis Donadel, Mauro Conti, and Mirco Rampazzo",
        "link": "http://arxiv.org/abs/2501.16490v1",
        "abstract": "Smart grids are critical for addressing the growing energy demand due to\nglobal population growth and urbanization. They enhance efficiency,\nreliability, and sustainability by integrating renewable energy. Ensuring their\navailability and safety requires advanced operational control and safety\nmeasures. Researchers employ AI and machine learning to assess grid stability,\nbut challenges like the lack of datasets and cybersecurity threats, including\nadversarial attacks, persist. In particular, data scarcity is a key issue:\nobtaining grid instability instances is tough due to the need for significant\nexpertise, resources, and time. However, they are essential to test novel\nresearch advancements and security mitigations. In this paper, we introduce a\nnovel framework to detect instability in smart grids by employing only stable\ndata. It relies on a Generative Adversarial Network (GAN) where the generator\nis trained to create instability data that are used along with stable data to\ntrain the discriminator. Moreover, we include a new adversarial training layer\nto improve robustness against adversarial attacks. Our solution, tested on a\ndataset composed of real-world stable and unstable samples, achieve accuracy up\nto 97.5\\% in predicting grid stability and up to 98.9\\% in detecting\nadversarial attacks. Moreover, we implemented our model in a single-board\ncomputer demonstrating efficient real-time decision-making with an average\nresponse time of less than 7ms. Our solution improves prediction accuracy and\nresilience while addressing data scarcity in smart grid management."
    },
    {
        "date": "2025-01",
        "title": "On the Feasibility of Using LLMs to Execute Multistage Network Attacks",
        "author": "Brian Singer, Keane Lucas, Lakshmi Adiga, Meghna Jain, Lujo Bauer, and Vyas Sekar",
        "link": "http://arxiv.org/abs/2501.16466v1",
        "abstract": "LLMs have shown preliminary promise in some security tasks and CTF\nchallenges. However, it is unclear whether LLMs are able to realize multistage\nnetwork attacks, which involve executing a wide variety of actions across\nmultiple hosts such as conducting reconnaissance, exploiting vulnerabilities to\ngain initial access, leveraging internal hosts to move laterally, and using\nmultiple compromised hosts to exfiltrate data. We evaluate LLMs across 10\nmultistage networks and find that popular LLMs are unable to realize these\nattacks. To enable LLMs to realize these attacks, we introduce Incalmo, an\nLLM-agnostic high-level attack abstraction layer that sits between an LLM and\nthe environment. Rather than LLMs issuing low-level command-line instructions,\nwhich can lead to incorrect implementations, Incalmo allows LLMs to specify\nhigh-level tasks (e.g., infect a host, scan a network), which are then carried\nout by Incalmo. Incalmo realizes these tasks by translating them into low-level\nprimitives (e.g., commands to exploit tools). Incalmo also provides an\nenvironment state service and an attack graph service to provide structure to\nLLMs in selecting actions relevant to a multistage attack. Across 9 out of 10\nrealistic emulated networks (from 25 to 50 hosts), LLMs using Incalmo can\nsuccessfully autonomously execute multistage attacks. We also conduct an\nablation analysis to show the key role the high-level abstractions play. For\ninstance, we find that both Incalmo's high-level tasks and services are\ncrucial. Furthermore, even smaller-parameter LLMs with Incalmo can fully\nsucceed in 5 of 10 environments, while larger-parameter LLMs without Incalmo do\nnot fully succeed in any."
    },
    {
        "date": "2025-01",
        "title": "Detecting Zero-Day Attacks in Digital Substations via In-Context Learning",
        "author": "Faizan Manzoor, Vanshaj Khattar, Akila Herath, Clifton Black, Matthew C Nielsen, Junho Hong, Chen-Ching Liu, and Ming Jin",
        "link": "http://arxiv.org/abs/2501.16453v1",
        "abstract": "The occurrences of cyber attacks on the power grids have been increasing\nevery year, with novel attack techniques emerging every year. In this paper, we\naddress the critical challenge of detecting novel/zero-day attacks in digital\nsubstations that employ the IEC-61850 communication protocol. While many\nheuristic and machine learning (ML)-based methods have been proposed for attack\ndetection in IEC-61850 digital substations, generalization to novel or zero-day\nattacks remains challenging. We propose an approach that leverages the\nin-context learning (ICL) capability of the transformer architecture, the\nfundamental building block of large language models. The ICL approach enables\nthe model to detect zero-day attacks and learn from a few examples of that\nattack without explicit retraining. Our experiments on the IEC-61850 dataset\ndemonstrate that the proposed method achieves more than $85\\%$ detection\naccuracy on zero-day attacks while the existing state-of-the-art baselines\nfail. This work paves the way for building more secure and resilient digital\nsubstations of the future."
    },
    {
        "date": "2025-01",
        "title": "Distilling foundation models for robust and efficient models in digital pathology",
        "author": "Alexandre Filiot, Nicolas Dop, Oussama Tchita, Auriane Riou, R\u00e9my Dubois, Thomas Peeters, Daria Valter, Marin Scalbert, Charlie Saillard, Genevi\u00e8ve Robin, and Antoine Olivier",
        "link": "http://arxiv.org/abs/2501.16239v2",
        "abstract": "In recent years, the advent of foundation models (FM) for digital pathology\nhas relied heavily on scaling the pre-training datasets and the model size,\nyielding large and powerful models. While it resulted in improving the\nperformance on diverse downstream tasks, it also introduced increased\ncomputational cost and inference time. In this work, we explore the\ndistillation of a large foundation model into a smaller one, reducing the\nnumber of parameters by several orders of magnitude. Leveraging distillation\ntechniques, our distilled model, H0-mini, achieves nearly comparable\nperformance to large FMs at a significantly reduced inference cost. It is\nevaluated on several public benchmarks, achieving 3rd place on the HEST\nbenchmark and 5th place on the EVA benchmark. Additionally, a robustness\nanalysis conducted on the PLISM dataset demonstrates that our distilled model\nreaches excellent robustness to variations in staining and scanning conditions,\nsignificantly outperforming other state-of-the art models. This opens new\nperspectives to design lightweight and robust models for digital pathology,\nwithout compromising on performance."
    },
    {
        "date": "2025-01",
        "title": "The Relationship Between Network Similarity and Transferability of Adversarial Attacks",
        "author": "Gerrit Klause, and Niklas Bunzel",
        "link": "http://arxiv.org/abs/2501.18629v1",
        "abstract": "Neural networks are vulnerable to adversarial attacks, and several defenses\nhave been proposed. Designing a robust network is a challenging task given the\nwide range of attacks that have been developed. Therefore, we aim to provide\ninsight into the influence of network similarity on the success rate of\ntransferred adversarial attacks. Network designers can then compare their new\nnetwork with existing ones to estimate its vulnerability. To achieve this, we\ninvestigate the complex relationship between network similarity and the success\nrate of transferred adversarial attacks. We applied the Centered Kernel\nAlignment (CKA) network similarity score and used various methods to find a\ncorrelation between a large number of Convolutional Neural Networks (CNNs) and\nadversarial attacks. Network similarity was found to be moderate across\ndifferent CNN architectures, with more complex models such as DenseNet showing\nlower similarity scores due to their architectural complexity. Layer similarity\nwas highest for consistent, basic layers such as DataParallel, Dropout and\nConv2d, while specialized layers showed greater variability. Adversarial attack\nsuccess rates were generally consistent for non-transferred attacks, but varied\nsignificantly for some transferred attacks, with complex networks being more\nvulnerable. We found that a DecisionTreeRegressor can predict the success rate\nof transferred attacks for all black-box and Carlini & Wagner attacks with an\naccuracy of over 90%, suggesting that predictive models may be viable under\ncertain conditions. However, the variability of results across different data\nsubsets underscores the complexity of these relationships and suggests that\nfurther research is needed to generalize these findings across different attack\nscenarios and network architectures."
    },
    {
        "date": "2025-01",
        "title": "Generating Spatial Synthetic Populations Using Wasserstein Generative Adversarial Network: A Case Study with EU-SILC Data for Helsinki and Thessaloniki",
        "author": "Vanja Falck",
        "link": "http://arxiv.org/abs/2501.16080v1",
        "abstract": "Using agent-based social simulations can enhance our understanding of urban\nplanning, public health, and economic forecasting. Realistic synthetic\npopulations with numerous attributes strengthen these simulations. The\nWasserstein Generative Adversarial Network, trained on census data like\nEU-SILC, can create robust synthetic populations. These methods, aided by\nexternal statistics or EU-SILC weights, generate spatial synthetic populations\nfor agent-based models. The increased access to high-quality micro-data has\nsparked interest in synthetic populations, which preserve demographic profiles\nand analytical strength while ensuring privacy and preventing discrimination.\nThis study uses national data from Finland and Greece for Helsinki and\nThessaloniki to explore balanced spatial synthetic population generation.\nResults show challenges related to balancing data with or without aggregated\nstatistics for the target population and the general under-representation of\nfringe profiles by deep generative methods. The latter can lead to\ndiscrimination in agent-based simulations."
    },
    {
        "date": "2025-01",
        "title": "The TIP of the Iceberg: Revealing a Hidden Class of Task-in-Prompt Adversarial Attacks on LLMs",
        "author": "Sergey Berezin, Reza Farahbakhsh, and Noel Crespi",
        "link": "http://arxiv.org/abs/2501.18626v3",
        "abstract": "We present a novel class of jailbreak adversarial attacks on LLMs, termed\nTask-in-Prompt (TIP) attacks. Our approach embeds sequence-to-sequence tasks\n(e.g., cipher decoding, riddles, code execution) into the model's prompt to\nindirectly generate prohibited inputs. To systematically assess the\neffectiveness of these attacks, we introduce the PHRYGE benchmark. We\ndemonstrate that our techniques successfully circumvent safeguards in six\nstate-of-the-art language models, including GPT-4o and LLaMA 3.2. Our findings\nhighlight critical weaknesses in current LLM safety alignments and underscore\nthe urgent need for more sophisticated defence strategies.\n  Warning: this paper contains examples of unethical inquiries used solely for\nresearch purposes."
    },
    {
        "date": "2025-01",
        "title": "Provisioning Time-Based Subscription in NDN: A Secure and Efficient Access Control Scheme",
        "author": "Nazatul H. Sultan, Chandan Kumar, Saurab Dulal, Vijay Varadharajan, Seyit Camtepe, and Surya Nepal",
        "link": "http://arxiv.org/abs/2501.15975v1",
        "abstract": "This paper proposes a novel encryption-based access control mechanism for\nNamed Data Networking (NDN). The scheme allows data producers to share their\ncontent in encrypted form before transmitting it to consumers. The encryption\nmechanism incorporates time-based subscription access policies directly into\nthe encrypted content, enabling only consumers with valid subscriptions to\ndecrypt it. This makes the scheme well-suited for real-world,\nsubscription-based applications like Netflix. Additionally, the scheme\nintroduces an anonymous and unlinkable signature-based authentication mechanism\nthat empowers edge routers to block bogus content requests at the network's\nentry point, thereby mitigating Denial of Service (DoS) attacks. A formal\nsecurity proof demonstrates the scheme's resistance to Chosen Plaintext Attacks\n(CPA). Performance analysis, using Mini-NDN-based emulation and a Charm library\nimplementation, further confirms the practicality of the scheme. Moreover, it\noutperforms closely related works in terms of functionality, security, and\ncommunication overhead."
    },
    {
        "date": "2025-01",
        "title": "LLM-attacker: Enhancing Closed-loop Adversarial Scenario Generation for Autonomous Driving with Large Language Models",
        "author": "Yuewen Mei, Tong Nie, Jian Sun, and Ye Tian",
        "link": "http://arxiv.org/abs/2501.15850v1",
        "abstract": "Ensuring and improving the safety of autonomous driving systems (ADS) is\ncrucial for the deployment of highly automated vehicles, especially in\nsafety-critical events. To address the rarity issue, adversarial scenario\ngeneration methods are developed, in which behaviors of traffic participants\nare manipulated to induce safety-critical events. However, existing methods\nstill face two limitations. First, identification of the adversarial\nparticipant directly impacts the effectiveness of the generation. However, the\ncomplexity of real-world scenarios, with numerous participants and diverse\nbehaviors, makes identification challenging. Second, the potential of generated\nsafety-critical scenarios to continuously improve ADS performance remains\nunderexplored. To address these issues, we propose LLM-attacker: a closed-loop\nadversarial scenario generation framework leveraging large language models\n(LLMs). Specifically, multiple LLM agents are designed and coordinated to\nidentify optimal attackers. Then, the trajectories of the attackers are\noptimized to generate adversarial scenarios. These scenarios are iteratively\nrefined based on the performance of ADS, forming a feedback loop to improve\nADS. Experimental results show that LLM-attacker can create more dangerous\nscenarios than other methods, and the ADS trained with it achieves a collision\nrate half that of training with normal scenarios. This indicates the ability of\nLLM-attacker to test and enhance the safety and robustness of ADS. Video\ndemonstrations are provided at:\nhttps://drive.google.com/file/d/1Zv4V3iG7825oyiKbUwS2Y-rR0DQIE1ZA/view."
    },
    {
        "date": "2025-01",
        "title": "Beyond In-Distribution Performance: A Cross-Dataset Study of Trajectory Prediction Robustness",
        "author": "Yue Yao, Daniel Goehring, and Joerg Reichardt",
        "link": "http://arxiv.org/abs/2501.15842v1",
        "abstract": "We study the Out-of-Distribution (OoD) generalization ability of three SotA\ntrajectory prediction models with comparable In-Distribution (ID) performance\nbut different model designs. We investigate the influence of inductive bias,\nsize of training data and data augmentation strategy by training the models on\nArgoverse 2 (A2) and testing on Waymo Open Motion (WO) and vice versa. We find\nthat the smallest model with highest inductive bias exhibits the best OoD\ngeneralization across different augmentation strategies when trained on the\nsmaller A2 dataset and tested on the large WO dataset. In the converse setting,\ntraining all models on the larger WO dataset and testing on the smaller A2\ndataset, we find that all models generalize poorly, even though the model with\nthe highest inductive bias still exhibits the best generalization ability. We\ndiscuss possible reasons for this surprising finding and draw conclusions about\nthe design and test of trajectory prediction models and benchmarks."
    },
    {
        "date": "2025-01",
        "title": "FuzzyLight: A Robust Two-Stage Fuzzy Approach for Traffic Signal Control Works in Real Cities",
        "author": "Mingyuan Li, Jiahao Wang, Bo Du, Jun Shen, and Qiang Wu",
        "link": "http://arxiv.org/abs/2501.15820v1",
        "abstract": "Effective traffic signal control (TSC) is crucial in mitigating urban\ncongestion and reducing emissions. Recently, reinforcement learning (RL) has\nbeen the research trend for TSC. However, existing RL algorithms face several\nreal-world challenges that hinder their practical deployment in TSC: (1) Sensor\naccuracy deteriorates with increased sensor detection range, and data\ntransmission is prone to noise, potentially resulting in unsafe TSC decisions.\n(2) During the training of online RL, interactions with the environment could\nbe unstable, potentially leading to inappropriate traffic signal phase (TSP)\nselection and traffic congestion. (3) Most current TSC algorithms focus only on\nTSP decisions, overlooking the critical aspect of phase duration, affecting\nsafety and efficiency. To overcome these challenges, we propose a robust\ntwo-stage fuzzy approach called FuzzyLight, which integrates compressed sensing\nand RL for TSC deployment. FuzzyLight offers several key contributions: (1) It\nemploys fuzzy logic and compressed sensing to address sensor noise and enhances\nthe efficiency of TSP decisions. (2) It maintains stable performance during\ntraining and combines fuzzy logic with RL to generate precise phases. (3) It\nworks in real cities across 22 intersections and demonstrates superior\nperformance in both real-world and simulated environments. Experimental results\nindicate that FuzzyLight enhances traffic efficiency by 48% compared to\nexpert-designed timings in the real world. Furthermore, it achieves\nstate-of-the-art (SOTA) performance in simulated environments using six\nreal-world datasets with transmission noise. The code and deployment video are\navailable at the URL1"
    },
    {
        "date": "2025-01",
        "title": "Membership Inference Attacks Against Vision-Language Models",
        "author": "Yuke Hu, Zheng Li, Zhihao Liu, Yang Zhang, Zhan Qin, Kui Ren, and Chun Chen",
        "link": "http://arxiv.org/abs/2501.18624v1",
        "abstract": "Vision-Language Models (VLMs), built on pre-trained vision encoders and large\nlanguage models (LLMs), have shown exceptional multi-modal understanding and\ndialog capabilities, positioning them as catalysts for the next technological\nrevolution. However, while most VLM research focuses on enhancing multi-modal\ninteraction, the risks of data misuse and leakage have been largely unexplored.\nThis prompts the need for a comprehensive investigation of such risks in VLMs.\nIn this paper, we conduct the first analysis of misuse and leakage detection in\nVLMs through the lens of membership inference attack (MIA). In specific, we\nfocus on the instruction tuning data of VLMs, which is more likely to contain\nsensitive or unauthorized information. To address the limitation of existing\nMIA methods, we introduce a novel approach that infers membership based on a\nset of samples and their sensitivity to temperature, a unique parameter in\nVLMs. Based on this, we propose four membership inference methods, each\ntailored to different levels of background knowledge, ultimately arriving at\nthe most challenging scenario. Our comprehensive evaluations show that these\nmethods can accurately determine membership status, e.g., achieving an AUC\ngreater than 0.8 targeting a small set consisting of only 5 samples on LLaVA."
    },
    {
        "date": "2025-01",
        "title": "CENSOR: Defense Against Gradient Inversion via Orthogonal Subspace Bayesian Sampling",
        "author": "Kaiyuan Zhang, Siyuan Cheng, Guangyu Shen, Bruno Ribeiro, Shengwei An, Pin-Yu Chen, Xiangyu Zhang, and Ninghui Li",
        "link": "http://arxiv.org/abs/2501.15718v1",
        "abstract": "Federated learning collaboratively trains a neural network on a global\nserver, where each local client receives the current global model weights and\nsends back parameter updates (gradients) based on its local private data. The\nprocess of sending these model updates may leak client's private data\ninformation. Existing gradient inversion attacks can exploit this vulnerability\nto recover private training instances from a client's gradient vectors.\nRecently, researchers have proposed advanced gradient inversion techniques that\nexisting defenses struggle to handle effectively. In this work, we present a\nnovel defense tailored for large neural network models. Our defense capitalizes\non the high dimensionality of the model parameters to perturb gradients within\na subspace orthogonal to the original gradient. By leveraging cold posteriors\nover orthogonal subspaces, our defense implements a refined gradient update\nmechanism. This enables the selection of an optimal gradient that not only\nsafeguards against gradient inversion attacks but also maintains model utility.\nWe conduct comprehensive experiments across three different datasets and\nevaluate our defense against various state-of-the-art attacks and defenses.\nCode is available at https://censor-gradient.github.io."
    },
    {
        "date": "2025-01",
        "title": "People who frequently use ChatGPT for writing tasks are accurate and robust detectors of AI-generated text",
        "author": "Jenna Russell, Marzena Karpinska, and Mohit Iyyer",
        "link": "http://arxiv.org/abs/2501.15654v1",
        "abstract": "In this paper, we study how well humans can detect text generated by\ncommercial LLMs (GPT-4o, Claude, o1). We hire annotators to read 300\nnon-fiction English articles, label them as either human-written or\nAI-generated, and provide paragraph-length explanations for their decisions.\nOur experiments show that annotators who frequently use LLMs for writing tasks\nexcel at detecting AI-generated text, even without any specialized training or\nfeedback. In fact, the majority vote among five such \"expert\" annotators\nmisclassifies only 1 of 300 articles, significantly outperforming most\ncommercial and open-source detectors we evaluated even in the presence of\nevasion tactics like paraphrasing and humanization. Qualitative analysis of the\nexperts' free-form explanations shows that while they rely heavily on specific\nlexical clues ('AI vocabulary'), they also pick up on more complex phenomena\nwithin the text (e.g., formality, originality, clarity) that are challenging to\nassess for automatic detectors. We release our annotated dataset and code to\nspur future research into both human and automated detection of AI-generated\ntext."
    },
    {
        "date": "2025-01",
        "title": "A Privacy Enhancing Technique to Evade Detection by Street Video Cameras Without Using Adversarial Accessories",
        "author": "Jacob Shams, Ben Nassi, Satoru Koda, Asaf Shabtai, and Yuval Elovici",
        "link": "http://arxiv.org/abs/2501.15653v1",
        "abstract": "In this paper, we propose a privacy-enhancing technique leveraging an\ninherent property of automatic pedestrian detection algorithms, namely, that\nthe training of deep neural network (DNN) based methods is generally performed\nusing curated datasets and laboratory settings, while the operational areas of\nthese methods are dynamic real-world environments. In particular, we leverage a\nnovel side effect of this gap between the laboratory and the real world:\nlocation-based weakness in pedestrian detection. We demonstrate that the\nposition (distance, angle, height) of a person, and ambient light level,\ndirectly impact the confidence of a pedestrian detector when detecting the\nperson. We then demonstrate that this phenomenon is present in pedestrian\ndetectors observing a stationary scene of pedestrian traffic, with blind spot\nareas of weak detection of pedestrians with low confidence. We show how\nprivacy-concerned pedestrians can leverage these blind spots to evade detection\nby constructing a minimum confidence path between two points in a scene,\nreducing the maximum confidence and average confidence of the path by up to\n0.09 and 0.13, respectively, over direct and random paths through the scene. To\ncounter this phenomenon, and force the use of more costly and sophisticated\nmethods to leverage this vulnerability, we propose a novel countermeasure to\nimprove the confidence of pedestrian detectors in blind spots, raising the\nmax/average confidence of paths generated by our technique by 0.09 and 0.05,\nrespectively. In addition, we demonstrate that our countermeasure improves a\nFaster R-CNN-based pedestrian detector's TPR and average true positive\nconfidence by 0.03 and 0.15, respectively."
    },
    {
        "date": "2025-01",
        "title": "Comparative clinical evaluation of \"memory-efficient\" synthetic 3d generative adversarial networks (gan) head-to-head to state of art: results on computed tomography of the chest",
        "author": "Mahshid shiri, Chandra Bortolotto, Alessandro Bruno, Alessio Consonni, Daniela Maria Grasso, Leonardo Brizzi, Daniele Loiacono, and Lorenzo Preda",
        "link": "http://arxiv.org/abs/2501.15572v1",
        "abstract": "Introduction: Generative Adversarial Networks (GANs) are increasingly used to\ngenerate synthetic medical images, addressing the critical shortage of\nannotated data for training Artificial Intelligence (AI) systems. This study\nintroduces a novel memory-efficient GAN architecture, incorporating Conditional\nRandom Fields (CRFs) to generate high-resolution 3D medical images and\nevaluates its performance against the state-of-the-art hierarchical (HA)-GAN\nmodel.\n  Materials and Methods: The CRF-GAN was trained using the open-source lung CT\nLUNA16 dataset. The architecture was compared to HA-GAN through a quantitative\nevaluation, using Frechet Inception Distance (FID) and Maximum Mean Discrepancy\n(MMD) metrics, and a qualitative evaluation, through a two-alternative forced\nchoice (2AFC) test completed by a pool of 12 resident radiologists, in order to\nassess the realism of the generated images.\n  Results: CRF-GAN outperformed HA-GAN with lower FID (0.047 vs. 0.061) and MMD\n(0.084 vs. 0.086) scores, indicating better image fidelity. The 2AFC test\nshowed a significant preference for images generated by CRF-Gan over those\ngenerated by HA-GAN with a p-value of 1.93e-05. Additionally, CRF-GAN\ndemonstrated 9.34% lower memory usage at 256 resolution and achieved up to\n14.6% faster training speeds, offering substantial computational savings.\n  Discussion: CRF-GAN model successfully generates high-resolution 3D medical\nimages with non-inferior quality to conventional models, while being more\nmemory-efficient and faster. Computational power and time saved can be used to\nimprove the spatial resolution and anatomical accuracy of generated images,\nwhich is still a critical factor limiting their direct clinical applicability."
    },
    {
        "date": "2025-01",
        "title": "Distributionally Robust Graph Out-of-Distribution Recommendation via Diffusion Model",
        "author": "Chu Zhao, Enneng Yang, Yuliang Liang, Jianzhe Zhao, Guibing Guo, and Xingwei Wang",
        "link": "http://arxiv.org/abs/2501.15555v1",
        "abstract": "The distributionally robust optimization (DRO)-based graph neural network\nmethods improve recommendation systems' out-of-distribution (OOD)\ngeneralization by optimizing the model's worst-case performance. However, these\nstudies fail to consider the impact of noisy samples in the training data,\nwhich results in diminished generalization capabilities and lower accuracy.\nThrough experimental and theoretical analysis, this paper reveals that current\nDRO-based graph recommendation methods assign greater weight to noise\ndistribution, leading to model parameter learning being dominated by it. When\nthe model overly focuses on fitting noise samples in the training data, it may\nlearn irrelevant or meaningless features that cannot be generalized to OOD\ndata. To address this challenge, we design a Distributionally Robust Graph\nmodel for OOD recommendation (DRGO). Specifically, our method first employs a\nsimple and effective diffusion paradigm to alleviate the noisy effect in the\nlatent space. Additionally, an entropy regularization term is introduced in the\nDRO objective function to avoid extreme sample weights in the worst-case\ndistribution. Finally, we provide a theoretical proof of the generalization\nerror bound of DRGO as well as a theoretical analysis of how our approach\nmitigates noisy sample effects, which helps to better understand the proposed\nframework from a theoretical perspective. We conduct extensive experiments on\nfour datasets to evaluate the effectiveness of our framework against three\ntypical distribution shifts, and the results demonstrate its superiority in\nboth independently and identically distributed distributions (IID) and OOD."
    },
    {
        "date": "2025-01",
        "title": "UNIDOOR: A Universal Framework for Action-Level Backdoor Attacks in Deep Reinforcement Learning",
        "author": "Oubo Ma, Linkang Du, Yang Dai, Chunyi Zhou, Qingming Li, Yuwen Pu, and Shouling Ji",
        "link": "http://arxiv.org/abs/2501.15529v1",
        "abstract": "Deep reinforcement learning (DRL) is widely applied to safety-critical\ndecision-making scenarios. However, DRL is vulnerable to backdoor attacks,\nespecially action-level backdoors, which pose significant threats through\nprecise manipulation and flexible activation, risking outcomes like vehicle\ncollisions or drone crashes. The key distinction of action-level backdoors lies\nin the utilization of the backdoor reward function to associate triggers with\ntarget actions. Nevertheless, existing studies typically rely on backdoor\nreward functions with fixed values or conditional flipping, which lack\nuniversality across diverse DRL tasks and backdoor designs, resulting in\nfluctuations or even failure in practice.\n  This paper proposes the first universal action-level backdoor attack\nframework, called UNIDOOR, which enables adaptive exploration of backdoor\nreward functions through performance monitoring, eliminating the reliance on\nexpert knowledge and grid search. We highlight that action tampering serves as\na crucial component of action-level backdoor attacks in continuous action\nscenarios, as it addresses attack failures caused by low-frequency target\nactions. Extensive evaluations demonstrate that UNIDOOR significantly enhances\nthe attack performance of action-level backdoors, showcasing its universality\nacross diverse attack scenarios, including single/multiple agents,\nsingle/multiple backdoors, discrete/continuous action spaces, and sparse/dense\nreward signals. Furthermore, visualization results encompassing state\ndistribution, neuron activation, and animations demonstrate the stealthiness of\nUNIDOOR. The source code of UNIDOOR can be found at\nhttps://github.com/maoubo/UNIDOOR."
    },
    {
        "date": "2025-01",
        "title": "Mitigating Spurious Negative Pairs for Robust Industrial Anomaly Detection",
        "author": "Hossein Mirzaei, Mojtaba Nafez, Jafar Habibi, Mohammad Sabokrou, and Mohammad Hossein Rohban",
        "link": "http://arxiv.org/abs/2501.15434v1",
        "abstract": "Despite significant progress in Anomaly Detection (AD), the robustness of\nexisting detection methods against adversarial attacks remains a challenge,\ncompromising their reliability in critical real-world applications such as\nautonomous driving. This issue primarily arises from the AD setup, which\nassumes that training data is limited to a group of unlabeled normal samples,\nmaking the detectors vulnerable to adversarial anomaly samples during testing.\nAdditionally, implementing adversarial training as a safeguard encounters\ndifficulties, such as formulating an effective objective function without\naccess to labels. An ideal objective function for adversarial training in AD\nshould promote strong perturbations both within and between the normal and\nanomaly groups to maximize margin between normal and anomaly distribution. To\naddress these issues, we first propose crafting a pseudo-anomaly group derived\nfrom normal group samples. Then, we demonstrate that adversarial training with\ncontrastive loss could serve as an ideal objective function, as it creates both\ninter- and intra-group perturbations. However, we notice that spurious negative\npairs compromise the conventional contrastive loss to achieve robust AD.\nSpurious negative pairs are those that should be closely mapped but are\nerroneously separated. These pairs introduce noise and misguide the direction\nof inter-group adversarial perturbations. To overcome the effect of spurious\nnegative pairs, we define opposite pairs and adversarially pull them apart to\nstrengthen inter-group perturbations. Experimental results demonstrate our\nsuperior performance in both clean and adversarial scenarios, with a 26.1%\nimprovement in robust detection across various challenging benchmark datasets.\nThe implementation of our work is available at:\nhttps://github.com/rohban-lab/COBRA."
    },
    {
        "date": "2025-01",
        "title": "Learning-Enhanced Safeguard Control for High-Relative-Degree Systems: Robust Optimization under Disturbances and Faults",
        "author": "Xinyang Wang, Hongwei Zhang, Shimin Wang, Wei Xiao, and Martin Guay",
        "link": "http://arxiv.org/abs/2501.15373v1",
        "abstract": "Merely pursuing performance may adversely affect the safety, while a\nconservative policy for safe exploration will degrade the performance. How to\nbalance the safety and performance in learning-based control problems is an\ninteresting yet challenging issue. This paper aims to enhance system\nperformance with safety guarantee in solving the reinforcement learning\n(RL)-based optimal control problems of nonlinear systems subject to\nhigh-relative-degree state constraints and unknown time-varying\ndisturbance/actuator faults. First, to combine control barrier functions (CBFs)\nwith RL, a new type of CBFs, termed high-order reciprocal control barrier\nfunction (HO-RCBF) is proposed to deal with high-relative-degree constraints\nduring the learning process. Then, the concept of gradient similarity is\nproposed to quantify the relationship between the gradient of safety and the\ngradient of performance. Finally, gradient manipulation and adaptive mechanisms\nare introduced in the safe RL framework to enhance the performance with a\nsafety guarantee. Two simulation examples illustrate that the proposed safe RL\nframework can address high-relative-degree constraint, enhance safety\nrobustness and improve system performance."
    },
    {
        "date": "2025-01",
        "title": "AI-Driven Secure Data Sharing: A Trustworthy and Privacy-Preserving Approach",
        "author": "Al Amin, Kamrul Hasan, Sharif Ullah, and Liang Hong",
        "link": "http://arxiv.org/abs/2501.15363v1",
        "abstract": "In the era of data-driven decision-making, ensuring the privacy and security\nof shared data is paramount across various domains. Applying existing deep\nneural networks (DNNs) to encrypted data is critical and often compromises\nperformance, security, and computational overhead. To address these\nlimitations, this research introduces a secure framework consisting of a\nlearnable encryption method based on the block-pixel operation to encrypt the\ndata and subsequently integrate it with the Vision Transformer (ViT). The\nproposed framework ensures data privacy and security by creating unique\nscrambling patterns per key, providing robust performance against adversarial\nattacks without compromising computational efficiency and data integrity. The\nframework was tested on sensitive medical datasets to validate its efficacy,\nproving its ability to handle highly confidential information securely. The\nsuggested framework was validated with a 94\\% success rate after extensive\ntesting on real-world datasets, such as MRI brain tumors and histological scans\nof lung and colon cancers. Additionally, the framework was tested under diverse\nadversarial attempts against secure data sharing with optimum performance and\ndemonstrated its effectiveness in various threat scenarios. These comprehensive\nanalyses underscore its robustness, making it a trustworthy solution for secure\ndata sharing in critical applications."
    },
    {
        "date": "2025-01",
        "title": "Killing it with Zero-Shot: Adversarially Robust Novelty Detection",
        "author": "Hossein Mirzaei, Mohammad Jafari, Hamid Reza Dehbashi, Zeinab Sadat Taghavi, Mohammad Sabokrou, and Mohammad Hossein Rohban",
        "link": "http://arxiv.org/abs/2501.15271v1",
        "abstract": "Novelty Detection (ND) plays a crucial role in machine learning by\nidentifying new or unseen data during model inference. This capability is\nespecially important for the safe and reliable operation of automated systems.\nDespite advances in this field, existing techniques often fail to maintain\ntheir performance when subject to adversarial attacks. Our research addresses\nthis gap by marrying the merits of nearest-neighbor algorithms with robust\nfeatures obtained from models pretrained on ImageNet. We focus on enhancing the\nrobustness and performance of ND algorithms. Experimental results demonstrate\nthat our approach significantly outperforms current state-of-the-art methods\nacross various benchmarks, particularly under adversarial conditions. By\nincorporating robust pretrained features into the k-NN algorithm, we establish\na new standard for performance and robustness in the field of robust ND. This\nwork opens up new avenues for research aimed at fortifying machine learning\nsystems against adversarial vulnerabilities. Our implementation is publicly\navailable at https://github.com/rohban-lab/ZARND."
    },
    {
        "date": "2025-01",
        "title": "Mirage in the Eyes: Hallucination Attack on Multi-modal Large Language Models with Only Attention Sink",
        "author": "Yining Wang, Mi Zhang, Junjie Sun, Chenyue Wang, Min Yang, Hui Xue, Jialing Tao, Ranjie Duan, and Jiexi Liu",
        "link": "http://arxiv.org/abs/2501.15269v1",
        "abstract": "Fusing visual understanding into language generation, Multi-modal Large\nLanguage Models (MLLMs) are revolutionizing visual-language applications. Yet,\nthese models are often plagued by the hallucination problem, which involves\ngenerating inaccurate objects, attributes, and relationships that do not match\nthe visual content. In this work, we delve into the internal attention\nmechanisms of MLLMs to reveal the underlying causes of hallucination, exposing\nthe inherent vulnerabilities in the instruction-tuning process.\n  We propose a novel hallucination attack against MLLMs that exploits attention\nsink behaviors to trigger hallucinated content with minimal image-text\nrelevance, posing a significant threat to critical downstream applications.\nDistinguished from previous adversarial methods that rely on fixed patterns,\nour approach generates dynamic, effective, and highly transferable visual\nadversarial inputs, without sacrificing the quality of model responses.\nComprehensive experiments on 6 prominent MLLMs demonstrate the efficacy of our\nattack in compromising black-box MLLMs even with extensive mitigating\nmechanisms, as well as the promising results against cutting-edge commercial\nAPIs, such as GPT-4o and Gemini 1.5. Our code is available at\nhttps://huggingface.co/RachelHGF/Mirage-in-the-Eyes."
    },
    {
        "date": "2025-01",
        "title": "Pre-trained Model Guided Mixture Knowledge Distillation for Adversarial Federated Learning",
        "author": "Yu Qiao, Huy Q. Le, Apurba Adhikary, and Choong Seon Hong",
        "link": "http://arxiv.org/abs/2501.15257v1",
        "abstract": "This paper aims to improve the robustness of a small global model while\nmaintaining clean accuracy under adversarial attacks and non-IID challenges in\nfederated learning. By leveraging the concise knowledge embedded in the class\nprobabilities from a pre-trained model for both clean and adversarial image\nclassification, we propose a Pre-trained Model-guided Adversarial Federated\nLearning (PM-AFL) training paradigm. This paradigm integrates vanilla mixture\nand adversarial mixture knowledge distillation to effectively balance accuracy\nand robustness while promoting local models to learn from diverse data.\nSpecifically, for clean accuracy, we adopt a dual distillation strategy where\nthe class probabilities of randomly paired images and their blended versions\nare aligned between the teacher model and the local models. For adversarial\nrobustness, we use a similar distillation approach but replace clean samples on\nthe local side with adversarial examples. Moreover, considering the bias\nbetween local and global models, we also incorporate a consistency\nregularization term to ensure that local adversarial predictions stay aligned\nwith their corresponding global clean ones. These strategies collectively\nenable local models to absorb diverse knowledge from the teacher model while\nmaintaining close alignment with the global model, thereby mitigating\noverfitting to local optima and enhancing the generalization of the global\nmodel. Experiments demonstrate that the PM-AFL-based paradigm outperforms other\nmethods that integrate defense strategies by a notable margin."
    },
    {
        "date": "2025-01",
        "title": "Median of Forests for Robust Density Estimation",
        "author": "Hongwei Wen, Annika Betken, and Tao Huang",
        "link": "http://arxiv.org/abs/2501.15157v1",
        "abstract": "Robust density estimation refers to the consistent estimation of the density\nfunction even when the data is contaminated by outliers. We find that existing\nforest density estimation at a certain point is inherently resistant to the\noutliers outside the cells containing the point, which we call\n\\textit{non-local outliers}, but not resistant to the rest \\textit{local\noutliers}. To achieve robustness against all outliers, we propose an ensemble\nlearning algorithm called \\textit{medians of forests for robust density\nestimation} (\\textit{MFRDE}), which adopts a pointwise median operation on\nforest density estimators fitted on subsampled datasets. Compared to existing\nrobust kernel-based methods, MFRDE enables us to choose larger subsampling\nsizes, sacrificing less accuracy for density estimation while achieving\nrobustness. On the theoretical side, we introduce the local outlier exponent to\nquantify the number of local outliers. Under this exponent, we show that even\nif the number of outliers reaches a certain polynomial order in the sample\nsize, MFRDE is able to achieve almost the same convergence rate as the same\nalgorithm on uncontaminated data, whereas robust kernel-based methods fail. On\nthe practical side, real data experiments show that MFRDE outperforms existing\nrobust kernel-based methods. Moreover, we apply MFRDE to anomaly detection to\nshowcase a further application."
    },
    {
        "date": "2025-01",
        "title": "PromptShield: Deployable Detection for Prompt Injection Attacks",
        "author": "Dennis Jacob, Hend Alzahrani, Zhanhao Hu, Basel Alomair, and David Wagner",
        "link": "http://arxiv.org/abs/2501.15145v1",
        "abstract": "Current application designers have moved to integrate large language models\n(LLMs) into their products. These LLM-integrated applications are vulnerable to\nprompt injection vulnerabilities. While attempts have been made to address this\nproblem by building a detector that can monitor inputs to the LLM and detect\nattacks, we find that many detectors are not yet suitable for practical\ndeployment. To support research in this area, we design the PromptShield\nbenchmark for evaluating practical prompt injection detectors. We also\nconstruct a new detector, the PromptShield detector, which achieves\nsignificantly better performance at detecting prompt injection attacks than any\nprior scheme. Our work suggests that larger models, more training data,\nappropriate metrics, and careful curation of training data can contribute to\nstrong detector performance."
    },
    {
        "date": "2025-01",
        "title": "TranStable: Towards Robust Pixel-level Online Video Stabilization by Jointing Transformer and CNN",
        "author": "zhizhen li, tianyi zhuo, Yifei Cao, Jizhe Yu, and Yu Liu",
        "link": "http://arxiv.org/abs/2501.15138v1",
        "abstract": "Video stabilization often struggles with distortion and excessive cropping.\nThis paper proposes a novel end-to-end framework, named TranStable, to address\nthese challenges, comprising a genera tor and a discriminator. We establish\nTransformerUNet (TUNet) as the generator to utilize the Hierarchical Adaptive\nFusion Module (HAFM), integrating Transformer and CNN to leverage both global\nand local features across multiple visual cues. By modeling frame-wise\nrelationships, it generates robust pixel-level warping maps for stable\ngeometric transformations. Furthermore, we design the Stability Discriminator\nModule (SDM), which provides pixel-wise supervision for authenticity and\nconsistency in training period, ensuring more complete field-of-view while\nminimizing jitter artifacts and enhancing visual fidelity. Extensive\nexperiments on NUS, DeepStab, and Selfie benchmarks demonstrate\nstate-of-the-art performance."
    },
    {
        "date": "2025-01",
        "title": "Comprehensive Evaluation of Cloaking Backdoor Attacks on Object Detector in Real-World",
        "author": "Hua Ma, Alsharif Abuadbba, Yansong Gao, Hyoungshick Kim, and Surya Nepal",
        "link": "http://arxiv.org/abs/2501.15101v1",
        "abstract": "The exploration of backdoor vulnerabilities in object detectors, particularly\nin real-world scenarios, remains limited. A significant challenge lies in the\nabsence of a natural physical backdoor dataset, and constructing such a dataset\nis both time- and labor-intensive. In this work, we address this gap by\ncreating a large-scale dataset comprising approximately 11,800 images/frames\nwith annotations featuring natural objects (e.g., T-shirts and hats) as\ntriggers to incur cloaking adversarial effects in diverse real-world scenarios.\nThis dataset is tailored for the study of physical backdoors in object\ndetectors. Leveraging this dataset, we conduct a comprehensive evaluation of an\ninsidious cloaking backdoor effect against object detectors, wherein the\nbounding box around a person vanishes when the individual is near a natural\nobject (e.g., a commonly available T-shirt) in front of the detector. Our\nevaluations encompass three prevalent attack surfaces: data outsourcing, model\noutsourcing, and the use of pretrained models. The cloaking effect is\nsuccessfully implanted in object detectors across all three attack surfaces. We\nextensively evaluate four popular object detection algorithms (anchor-based\nYolo-V3, Yolo-V4, Faster R-CNN, and anchor-free CenterNet) using 19 videos\n(totaling approximately 11,800 frames) in real-world scenarios. Our results\ndemonstrate that the backdoor attack exhibits remarkable robustness against\nvarious factors, including movement, distance, angle, non-rigid deformation,\nand lighting. In data and model outsourcing scenarios, the attack success rate\n(ASR) in most videos reaches 100% or near it, while the clean data accuracy of\nthe backdoored model remains indistinguishable from that of the clean model,\nmaking it impossible to detect backdoor behavior through a validation set."
    },
    {
        "date": "2025-01",
        "title": "Bringing RGB and IR Together: Hierarchical Multi-Modal Enhancement for Robust Transmission Line Detection",
        "author": "Shengdong Zhang, Xiaoqin Zhang, Wenqi Ren, Linlin Shen, Shaohua Wan, Jun Zhang, and Yujing M Jiang",
        "link": "http://arxiv.org/abs/2501.15099v1",
        "abstract": "Ensuring a stable power supply in rural areas relies heavily on effective\ninspection of power equipment, particularly transmission lines (TLs). However,\ndetecting TLs from aerial imagery can be challenging when dealing with\nmisalignments between visible light (RGB) and infrared (IR) images, as well as\nmismatched high- and low-level features in convolutional networks. To address\nthese limitations, we propose a novel Hierarchical Multi-Modal Enhancement\nNetwork (HMMEN) that integrates RGB and IR data for robust and accurate TL\ndetection. Our method introduces two key components: (1) a Mutual Multi-Modal\nEnhanced Block (MMEB), which fuses and enhances hierarchical RGB and IR feature\nmaps in a coarse-to-fine manner, and (2) a Feature Alignment Block (FAB) that\ncorrects misalignments between decoder outputs and IR feature maps by\nleveraging deformable convolutions. We employ MobileNet-based encoders for both\nRGB and IR inputs to accommodate edge-computing constraints and reduce\ncomputational overhead. Experimental results on diverse weather and lighting\nconditionsfog, night, snow, and daytimedemonstrate the superiority and\nrobustness of our approach compared to state-of-the-art methods, resulting in\nfewer false positives, enhanced boundary delineation, and better overall\ndetection performance. This framework thus shows promise for practical\nlarge-scale power line inspections with unmanned aerial vehicles."
    },
    {
        "date": "2025-01",
        "title": "Towards Better Robustness: Progressively Joint Pose-3DGS Learning for Arbitrarily Long Videos",
        "author": "Zhen-Hui Dong, Sheng Ye, Yu-Hui Wen, Nannan Li, and Yong-Jin Liu",
        "link": "http://arxiv.org/abs/2501.15096v1",
        "abstract": "3D Gaussian Splatting (3DGS) has emerged as a powerful representation due to\nits efficiency and high-fidelity rendering. However, 3DGS training requires a\nknown camera pose for each input view, typically obtained by\nStructure-from-Motion (SfM) pipelines. Pioneering works have attempted to relax\nthis restriction but still face difficulties when handling long sequences with\ncomplex camera trajectories. In this work, we propose Rob-GS, a robust\nframework to progressively estimate camera poses and optimize 3DGS for\narbitrarily long video sequences. Leveraging the inherent continuity of videos,\nwe design an adjacent pose tracking method to ensure stable pose estimation\nbetween consecutive frames. To handle arbitrarily long inputs, we adopt a\n\"divide and conquer\" scheme that adaptively splits the video sequence into\nseveral segments and optimizes them separately. Extensive experiments on the\nTanks and Temples dataset and our collected real-world dataset show that our\nRob-GS outperforms the state-of-the-arts."
    },
    {
        "date": "2025-01",
        "title": "Towards Robust Unsupervised Attention Prediction in Autonomous Driving",
        "author": "Mengshi Qi, Xiaoyang Bi, Pengfei Zhu, and Huadong Ma",
        "link": "http://arxiv.org/abs/2501.15045v2",
        "abstract": "Robustly predicting attention regions of interest for self-driving systems is\ncrucial for driving safety but presents significant challenges due to the\nlabor-intensive nature of obtaining large-scale attention labels and the domain\ngap between self-driving scenarios and natural scenes. These challenges are\nfurther exacerbated by complex traffic environments, including camera\ncorruption under adverse weather, noise interferences, and central bias from\nlong-tail distributions. To address these issues, we propose a robust\nunsupervised attention prediction method. An Uncertainty Mining Branch refines\npredictions by analyzing commonalities and differences across multiple\npre-trained models on natural scenes, while a Knowledge Embedding Block bridges\nthe domain gap by incorporating driving knowledge to adaptively enhance\npseudo-labels. Additionally, we introduce RoboMixup, a novel data augmentation\nmethod that improves robustness against corruption through soft attention and\ndynamic augmentation, and mitigates central bias by integrating random cropping\ninto Mixup as a regularizer. To systematically evaluate robustness in\nself-driving attention prediction, we introduce the DriverAttention-C\nbenchmark, comprising over 100k frames across three subsets: BDD-A-C,\nDR(eye)VE-C, and DADA-2000-C. Our method achieves performance equivalent to or\nsurpassing fully supervised state-of-the-art approaches on three public\ndatasets and the proposed robustness benchmark, reducing relative corruption\ndegradation by 58.8% and 52.8%, and improving central bias robustness by 12.4%\nand 11.4% in KLD and CC metrics, respectively. Code and data are available at\nhttps://github.com/zaplm/DriverAttention."
    },
    {
        "date": "2025-01",
        "title": "A Portable and Stealthy Inaudible Voice Attack Based on Acoustic Metamaterials",
        "author": "Zhiyuan Ning, Juan He, Zhanyong Tang, Weihang Hu, and Xiaojiang Chen",
        "link": "http://arxiv.org/abs/2501.15031v1",
        "abstract": "We present METAATTACK, the first approach to leverage acoustic metamaterials\nfor inaudible attacks for voice control systems. Compared to the\nstate-of-the-art inaudible attacks requiring complex and large speaker setups,\nMETAATTACK achieves a longer attacking range and higher accuracy using a\ncompact, portable device small enough to be put into a carry bag. These\nimprovements in portability and stealth have led to the practical applicability\nof inaudible attacks and their adaptation to a wider range of scenarios. We\ndemonstrate how the recent advancement in metamaterials can be utilized to\ndesign a voice attack system with carefully selected implementation parameters\nand commercial off-the-shelf components. We showcase that METAATTACK can be\nused to launch inaudible attacks for representative voice-controlled personal\nassistants, including Siri, Alexa, Google Assistant, XiaoAI, and Xiaoyi. The\naverage word accuracy of all assistants is 76%, with a range of 8.85 m."
    },
    {
        "date": "2025-01",
        "title": "RotateKV: Accurate and Robust 2-Bit KV Cache Quantization for LLMs via Outlier-Aware Adaptive Rotations",
        "author": "Zunhai Su, Zhe Chen, Wang Shen, Hanyu Wei, Linge Li, Huangqi Yu, and Kehong Yuan",
        "link": "http://arxiv.org/abs/2501.16383v2",
        "abstract": "Key-Value (KV) cache facilitates efficient large language models (LLMs)\ninference by avoiding recomputation of past KVs. As the batch size and context\nlength increase, the oversized KV caches become a significant memory\nbottleneck, highlighting the need for efficient compression. Existing KV\nquantization rely on fine-grained quantization or the retention of a\nsignificant portion of high bit-widths caches, both of which compromise\ncompression ratio and often fail to maintain robustness at extremely low\naverage bit-widths. In this work, we explore the potential of rotation\ntechnique for 2-bit KV quantization and propose RotateKV, which achieves\naccurate and robust performance through the following innovations: (i)\nOutlier-Aware Rotation, which utilizes channel-reordering to adapt the\nrotations to varying channel-wise outlier distributions without sacrificing the\ncomputational efficiency of the fast Walsh-Hadamard transform (FWHT); (ii)\nPre-RoPE Grouped-Head Rotation, which mitigates the impact of rotary position\nembedding (RoPE) on proposed outlier-aware rotation and further smooths\noutliers across heads; (iii) Attention-Sink-Aware Quantization, which leverages\nthe massive activations to precisely identify and protect attention sinks.\nRotateKV achieves less than 0.3 perplexity (PPL) degradation with 2-bit\nquantization on WikiText-2 using LLaMA-2-13B, maintains strong CoT reasoning\nand long-context capabilities, with less than 1.7\\% degradation on GSM8K,\noutperforming existing methods even at lower average bit-widths. RotateKV also\nshowcases a 3.97x reduction in peak memory usage, supports 5.75x larger batch\nsizes, and achieves a 2.32x speedup in decoding stage."
    },
    {
        "date": "2025-01",
        "title": "Towards Distributed Backdoor Attacks with Network Detection in Decentralized Federated Learning",
        "author": "Bohan Liu, Yang Xiao, Ruimeng Ye, Zinan Ling, Xiaolong Ma, and Bo Hui",
        "link": "http://arxiv.org/abs/2501.15005v1",
        "abstract": "Distributed backdoor attacks (DBA) have shown a higher attack success rate\nthan centralized attacks in centralized federated learning (FL). However, it\nhas not been investigated in the decentralized FL. In this paper, we\nexperimentally demonstrate that, while directly applying DBA to decentralized\nFL, the attack success rate depends on the distribution of attackers in the\nnetwork architecture. Considering that the attackers can not decide their\nlocation, this paper aims to achieve a high attack success rate regardless of\nthe attackers' location distribution. Specifically, we first design a method to\ndetect the network by predicting the distance between any two attackers on the\nnetwork. Then, based on the distance, we organize the attackers in different\nclusters. Lastly, we propose an algorithm to \\textit{dynamically} embed local\npatterns decomposed from a global pattern into the different attackers in each\ncluster. We conduct a thorough empirical investigation and find that our method\ncan, in benchmark datasets, outperform both centralized attacks and naive DBA\nin different decentralized frameworks."
    },
    {
        "date": "2025-01",
        "title": "VideoPure: Diffusion-based Adversarial Purification for Video Recognition",
        "author": "Kaixun Jiang, Zhaoyu Chen, Jiyuan Fu, Lingyi Hong, Jinglun Li, and Wenqiang Zhang",
        "link": "http://arxiv.org/abs/2501.14999v1",
        "abstract": "Recent work indicates that video recognition models are vulnerable to\nadversarial examples, posing a serious security risk to downstream\napplications. However, current research has primarily focused on adversarial\nattacks, with limited work exploring defense mechanisms. Furthermore, due to\nthe spatial-temporal complexity of videos, existing video defense methods face\nissues of high cost, overfitting, and limited defense performance. Recently,\ndiffusion-based adversarial purification methods have achieved robust defense\nperformance in the image domain. However, due to the additional temporal\ndimension in videos, directly applying these diffusion-based adversarial\npurification methods to the video domain suffers performance and efficiency\ndegradation. To achieve an efficient and effective video adversarial defense\nmethod, we propose the first diffusion-based video purification framework to\nimprove video recognition models' adversarial robustness: VideoPure. Given an\nadversarial example, we first employ temporal DDIM inversion to transform the\ninput distribution into a temporally consistent and trajectory-defined\ndistribution, covering adversarial noise while preserving more video structure.\nThen, during DDIM denoising, we leverage intermediate results at each denoising\nstep and conduct guided spatial-temporal optimization, removing adversarial\nnoise while maintaining temporal consistency. Finally, we input the list of\noptimized intermediate results into the video recognition model for multi-step\nvoting to obtain the predicted class. We investigate the defense performance of\nour method against black-box, gray-box, and adaptive attacks on benchmark\ndatasets and models. Compared with other adversarial purification methods, our\nmethod overall demonstrates better defense performance against different\nattacks. Our code is available at https://github.com/deep-kaixun/VideoPure."
    },
    {
        "date": "2025-01",
        "title": "Robust Cross-Etiology and Speaker-Independent Dysarthric Speech Recognition",
        "author": "Satwinder Singh, Qianli Wang, Zihan Zhong, Clarion Mendes, Mark Hasegawa-Johnson, Waleed Abdulla, and Seyed Reza Shahamiri",
        "link": "http://arxiv.org/abs/2501.14994v1",
        "abstract": "In this paper, we present a speaker-independent dysarthric speech recognition\nsystem, with a focus on evaluating the recently released Speech Accessibility\nProject (SAP-1005) dataset, which includes speech data from individuals with\nParkinson's disease (PD). Despite the growing body of research in dysarthric\nspeech recognition, many existing systems are speaker-dependent and adaptive,\nlimiting their generalizability across different speakers and etiologies. Our\nprimary objective is to develop a robust speaker-independent model capable of\naccurately recognizing dysarthric speech, irrespective of the speaker.\nAdditionally, as a secondary objective, we aim to test the cross-etiology\nperformance of our model by evaluating it on the TORGO dataset, which contains\nspeech samples from individuals with cerebral palsy (CP) and amyotrophic\nlateral sclerosis (ALS). By leveraging the Whisper model, our\nspeaker-independent system achieved a CER of 6.99% and a WER of 10.71% on the\nSAP-1005 dataset. Further, in cross-etiology settings, we achieved a CER of\n25.08% and a WER of 39.56% on the TORGO dataset. These results highlight the\npotential of our approach to generalize across unseen speakers and different\netiologies of dysarthria."
    },
    {
        "date": "2025-01",
        "title": "Decision Making in Changing Environments: Robustness, Query-Based Learning, and Differential Privacy",
        "author": "Fan Chen, and Alexander Rakhlin",
        "link": "http://arxiv.org/abs/2501.14928v1",
        "abstract": "We study the problem of interactive decision making in which the underlying\nenvironment changes over time subject to given constraints. We propose a\nframework, which we call \\textit{hybrid Decision Making with Structured\nObservations} (hybrid DMSO), that provides an interpolation between the\nstochastic and adversarial settings of decision making. Within this framework,\nwe can analyze local differentially private (LDP) decision making, query-based\nlearning (in particular, SQ learning), and robust and smooth decision making\nunder the same umbrella, deriving upper and lower bounds based on variants of\nthe Decision-Estimation Coefficient (DEC). We further establish strong\nconnections between the DEC's behavior, the SQ dimension, local minimax\ncomplexity, learnability, and joint differential privacy. To showcase the\nframework's power, we provide new results for contextual bandits under the LDP\nconstraint."
    },
    {
        "date": "2025-01",
        "title": "A Note on Implementation Errors in Recent Adaptive Attacks Against Multi-Resolution Self-Ensembles",
        "author": "Stanislav Fort",
        "link": "http://arxiv.org/abs/2501.14496v1",
        "abstract": "This note documents an implementation issue in recent adaptive attacks (Zhang\net al. [2024]) against the multi-resolution self-ensemble defense (Fort and\nLakshminarayanan [2024]). The implementation allowed adversarial perturbations\nto exceed the standard $L_\\infty = 8/255$ bound by up to a factor of\n20$\\times$, reaching magnitudes of up to $L_\\infty = 160/255$. When attacks are\nproperly constrained within the intended bounds, the defense maintains\nnon-trivial robustness. Beyond highlighting the importance of careful\nvalidation in adversarial machine learning research, our analysis reveals an\nintriguing finding: properly bounded adaptive attacks against strong\nmulti-resolution self-ensembles often align with human perception, suggesting\nthe need to reconsider how we measure adversarial robustness."
    }
]