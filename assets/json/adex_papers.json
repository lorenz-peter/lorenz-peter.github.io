[
    {
        "date": "2025-03",
        "title": "A Frustratingly Simple Yet Highly Effective Attack Baseline: Over 90% Success Rate Against the Strong Black-box Models of GPT-4.5/4o/o1",
        "author": "Zhaoyi Li, Xiaohan Zhao, Dong-Dong Wu, Jiacheng Cui, and Zhiqiang Shen",
        "link": "http://arxiv.org/abs/2503.10635v1",
        "abstract": "Despite promising performance on open-source large vision-language models\n(LVLMs), transfer-based targeted attacks often fail against black-box\ncommercial LVLMs. Analyzing failed adversarial perturbations reveals that the\nlearned perturbations typically originate from a uniform distribution and lack\nclear semantic details, resulting in unintended responses. This critical\nabsence of semantic information leads commercial LVLMs to either ignore the\nperturbation entirely or misinterpret its embedded semantics, thereby causing\nthe attack to fail. To overcome these issues, we notice that identifying core\nsemantic objects is a key objective for models trained with various datasets\nand methodologies. This insight motivates our approach that refines semantic\nclarity by encoding explicit semantic details within local regions, thus\nensuring interoperability and capturing finer-grained features, and by\nconcentrating modifications on semantically rich areas rather than applying\nthem uniformly. To achieve this, we propose a simple yet highly effective\nsolution: at each optimization step, the adversarial image is cropped randomly\nby a controlled aspect ratio and scale, resized, and then aligned with the\ntarget image in the embedding space. Experimental results confirm our\nhypothesis. Our adversarial examples crafted with local-aggregated\nperturbations focused on crucial regions exhibit surprisingly good\ntransferability to commercial LVLMs, including GPT-4.5, GPT-4o,\nGemini-2.0-flash, Claude-3.5-sonnet, Claude-3.7-sonnet, and even reasoning\nmodels like o1, Claude-3.7-thinking and Gemini-2.0-flash-thinking. Our approach\nachieves success rates exceeding 90% on GPT-4.5, 4o, and o1, significantly\noutperforming all prior state-of-the-art attack methods. Our optimized\nadversarial examples under different configurations and training code are\navailable at https://github.com/VILA-Lab/M-Attack."
    },
    {
        "date": "2025-03",
        "title": "Hierarchical Self-Supervised Adversarial Training for Robust Vision Models in Histopathology",
        "author": "Hashmat Shadab Malik, Shahina Kunhimon, Muzammal Naseer, Fahad Shahbaz Khan, and Salman Khan",
        "link": "http://arxiv.org/abs/2503.10629v1",
        "abstract": "Adversarial attacks pose significant challenges for vision models in critical\nfields like healthcare, where reliability is essential. Although adversarial\ntraining has been well studied in natural images, its application to biomedical\nand microscopy data remains limited. Existing self-supervised adversarial\ntraining methods overlook the hierarchical structure of histopathology images,\nwhere patient-slide-patch relationships provide valuable discriminative\nsignals. To address this, we propose Hierarchical Self-Supervised Adversarial\nTraining (HSAT), which exploits these properties to craft adversarial examples\nusing multi-level contrastive learning and integrate it into adversarial\ntraining for enhanced robustness. We evaluate HSAT on multiclass histopathology\ndataset OpenSRH and the results show that HSAT outperforms existing methods\nfrom both biomedical and natural image domains. HSAT enhances robustness,\nachieving an average gain of 54.31% in the white-box setting and reducing\nperformance drops to 3-4% in the black-box setting, compared to 25-30% for the\nbaseline. These results set a new benchmark for adversarial training in this\ndomain, paving the way for more robust models. Our Code for training and\nevaluation is available at https://github.com/HashmatShadab/HSAT."
    },
    {
        "date": "2025-03",
        "title": "FedPCA: Noise-Robust Fair Federated Learning via Performance-Capacity Analysis",
        "author": "Nannan Wu, Zengqiang Yan, Nong Sang, Li Yu, and Chang Wen Chen",
        "link": "http://arxiv.org/abs/2503.10567v1",
        "abstract": "Training a model that effectively handles both common and rare data-i.e.,\nachieving performance fairness-is crucial in federated learning (FL). While\nexisting fair FL methods have shown effectiveness, they remain vulnerable to\nmislabeled data. Ensuring robustness in fair FL is therefore essential.\nHowever, fairness and robustness inherently compete, which causes robust\nstrategies to hinder fairness. In this paper, we attribute this competition to\nthe homogeneity in loss patterns exhibited by rare and mislabeled data clients,\npreventing existing loss-based fair and robust FL methods from effectively\ndistinguishing and handling these two distinct client types. To address this,\nwe propose performance-capacity analysis, which jointly considers model\nperformance on each client and its capacity to handle the dataset, measured by\nloss and a newly introduced feature dispersion score. This allows mislabeled\nclients to be identified by their significantly deviated performance relative\nto capacity while preserving rare data clients. Building on this, we introduce\nFedPCA, an FL method that robustly achieves fairness. FedPCA first identifies\nmislabeled clients via a Gaussian Mixture Model on loss-dispersion pairs, then\napplies fairness and robustness strategies in global aggregation and local\ntraining by adjusting client weights and selectively using reliable data.\nExtensive experiments on three datasets demonstrate FedPCA's effectiveness in\ntackling this complex challenge. Code will be publicly available upon\nacceptance."
    },
    {
        "date": "2025-03",
        "title": "MASQUE: A Text-Guided Diffusion-Based Framework for Localized and Customized Adversarial Makeup",
        "author": "Youngjin Kwon, and Xiao Zhang",
        "link": "http://arxiv.org/abs/2503.10549v1",
        "abstract": "As facial recognition is increasingly adopted for government and commercial\nservices, its potential misuse has raised serious concerns about privacy and\ncivil rights. To counteract, various anti-facial recognition techniques have\nbeen proposed for privacy protection by adversarially perturbing face images,\namong which generative makeup-based approaches are the most popular. However,\nthese methods, designed primarily to impersonate specific target identities,\ncan only achieve weak dodging success rates while increasing the risk of\ntargeted abuse. In addition, they often introduce global visual artifacts or a\nlack of adaptability to accommodate diverse makeup prompts, compromising user\nsatisfaction. To address the above limitations, we develop MASQUE, a novel\ndiffusion-based framework that generates localized adversarial makeups guided\nby user-defined text prompts. Built upon precise null-text inversion,\ncustomized cross-attention fusion with masking, and a pairwise adversarial\nguidance mechanism using images of the same individual, MASQUE achieves robust\ndodging performance without requiring any external identity. Comprehensive\nevaluations on open-source facial recognition models and commercial APIs\ndemonstrate that MASQUE significantly improves dodging success rates over all\nbaselines, along with higher perceptual fidelity and stronger adaptability to\nvarious text makeup prompts."
    },
    {
        "date": "2025-03",
        "title": "Whisper Speaker Identification: Leveraging Pre-Trained Multilingual Transformers for Robust Speaker Embeddings",
        "author": "Jakaria Islam Emon, Md Abu Salek, and Kazi Tamanna Alam",
        "link": "http://arxiv.org/abs/2503.10446v1",
        "abstract": "Speaker identification in multilingual settings presents unique challenges,\nparticularly when conventional models are predominantly trained on English\ndata. In this paper, we propose WSI (Whisper Speaker Identification), a\nframework that repurposes the encoder of the Whisper automatic speech\nrecognition model pre trained on extensive multilingual data to generate robust\nspeaker embeddings via a joint loss optimization strategy that leverages online\nhard triplet mining and self supervised Normalized Temperature-scaled Cross\nEntropy loss. By capitalizing on Whisper language-agnostic acoustic\nrepresentations, our approach effectively distinguishes speakers across diverse\nlanguages and recording conditions. Extensive evaluations on multiple corpora,\nincluding VoxTube (multilingual), JVS (Japanese), CallHome (German, Spanish,\nChinese, and Japanese), and Voxconverse (English), demonstrate that WSI\nconsistently outperforms state-of-the-art baselines, namely Pyannote Embedding,\nECAPA TDNN, and Xvector, in terms of lower equal error rates and higher AUC\nscores. These results validate our hypothesis that a multilingual pre-trained\nASR encoder, combined with joint loss optimization, substantially improves\nspeaker identification performance in non-English languages."
    },
    {
        "date": "2025-03",
        "title": "HyperArm Bandit Optimization: A Novel approach to Hyperparameter Optimization and an Analysis of Bandit Algorithms in Stochastic and Adversarial Settings",
        "author": "Samih Karroum, and Saad Mazhar",
        "link": "http://arxiv.org/abs/2503.10282v1",
        "abstract": "This paper explores the application of bandit algorithms in both stochastic\nand adversarial settings, with a focus on theoretical analysis and practical\napplications. The study begins by introducing bandit problems, distinguishing\nbetween stochastic and adversarial variants, and examining key algorithms such\nas Explore-Then-Commit (ETC), Upper Confidence Bound (UCB), and\nExponential-Weight Algorithm for Exploration and Exploitation (EXP3).\nTheoretical regret bounds are analyzed to compare the performance of these\nalgorithms. The paper then introduces a novel framework, HyperArm Bandit\nOptimization (HABO), which applies EXP3 to hyperparameter tuning in machine\nlearning models. Unlike traditional methods that treat entire configurations as\narms, HABO treats individual hyperparameters as super-arms, and its potential\nconfigurations as sub-arms, enabling dynamic resource allocation and efficient\nexploration. Experimental results demonstrate HABO's effectiveness in\nclassification and regression tasks, outperforming Bayesian Optimization in\nterms of computational efficiency and accuracy. The paper concludes with\ninsights into the convergence guarantees of HABO and its potential for scalable\nand robust hyperparameter optimization."
    },
    {
        "date": "2025-03",
        "title": "Robust Learning-Based Sparse Recovery for Device Activity Detection in Grant-Free Random Access Cell-Free Massive MIMO: Enhancing Resilience to Impairments",
        "author": "Ali Elkeshawy, Haifa Fares, and Amor Nafkha",
        "link": "http://arxiv.org/abs/2503.10280v1",
        "abstract": "Massive MIMO is considered a key enabler to support massive machine-type\ncommunication (mMTC). While massive access schemes have been extensively\nanalyzed for co-located massive MIMO arrays, this paper explores activity\ndetection in grant-free random access for mMTC within the context of cell-free\nmassive MIMO systems, employing distributed antenna arrays. This sparse support\nrecovery of device activity status is performed by a finite cluster of access\npoints (APs) from a large number of geographically distributed APs\ncollaborating to serve a larger number of devices. Active devices transmit\nnon-orthogonal pilot sequences to APs, which forward the received signals to a\ncentral processing unit (CPU) for collaborative activity detection. This paper\nproposes a simple and efficient data-driven algorithm tailored for device\nactivity detection, implemented centrally at the CPU. Furthermore, the study\nassesses the algorithm's robustness to input perturbations and examines the\neffects of adopting fixed-point representation on its performance."
    },
    {
        "date": "2025-03",
        "title": "Numerically robust Gaussian state estimation with singular observation noise",
        "author": "Nicholas Kr\u00e4mer, and Filip Tronarp",
        "link": "http://arxiv.org/abs/2503.10279v1",
        "abstract": "This article proposes numerically robust algorithms for Gaussian state\nestimation with singular observation noise. Our approach combines a series of\nbasis changes with Bayes' rule, transforming the singular estimation problem\ninto a nonsingular one with reduced state dimension. In addition to ensuring\nlow runtime and numerical stability, our proposal facilitates\nmarginal-likelihood computations and Gauss-Markov representations of the\nposterior process. We analyse the proposed method's computational savings and\nnumerical robustness and validate our findings in a series of simulations."
    },
    {
        "date": "2025-03",
        "title": "Robustness Tokens: Towards Adversarial Robustness of Transformers",
        "author": "Brian Pulfer, Yury Belousov, and Slava Voloshynovskiy",
        "link": "http://arxiv.org/abs/2503.10191v1",
        "abstract": "Recently, large pre-trained foundation models have become widely adopted by\nmachine learning practitioners for a multitude of tasks. Given that such models\nare publicly available, relying on their use as backbone models for downstream\ntasks might result in high vulnerability to adversarial attacks crafted with\nthe same public model. In this work, we propose Robustness Tokens, a novel\napproach specific to the transformer architecture that fine-tunes a few\nadditional private tokens with low computational requirements instead of tuning\nmodel parameters as done in traditional adversarial training. We show that\nRobustness Tokens make Vision Transformer models significantly more robust to\nwhite-box adversarial attacks while also retaining the original downstream\nperformances."
    },
    {
        "date": "2025-03",
        "title": "AdvPaint: Protecting Images from Inpainting Manipulation via Adversarial Attention Disruption",
        "author": "Joonsung Jeon, Woo Jae Kim, Suhyeon Ha, Sooel Son, and Sung-eui Yoon",
        "link": "http://arxiv.org/abs/2503.10081v1",
        "abstract": "The outstanding capability of diffusion models in generating high-quality\nimages poses significant threats when misused by adversaries. In particular, we\nassume malicious adversaries exploiting diffusion models for inpainting tasks,\nsuch as replacing a specific region with a celebrity. While existing methods\nfor protecting images from manipulation in diffusion-based generative models\nhave primarily focused on image-to-image and text-to-image tasks, the challenge\nof preventing unauthorized inpainting has been rarely addressed, often\nresulting in suboptimal protection performance. To mitigate inpainting abuses,\nwe propose ADVPAINT, a novel defensive framework that generates adversarial\nperturbations that effectively disrupt the adversary's inpainting tasks.\nADVPAINT targets the self- and cross-attention blocks in a target diffusion\ninpainting model to distract semantic understanding and prompt interactions\nduring image generation. ADVPAINT also employs a two-stage perturbation\nstrategy, dividing the perturbation region based on an enlarged bounding box\naround the object, enhancing robustness across diverse masks of varying shapes\nand sizes. Our experimental results demonstrate that ADVPAINT's perturbations\nare highly effective in disrupting the adversary's inpainting tasks,\noutperforming existing methods; ADVPAINT attains over a 100-point increase in\nFID and substantial decreases in precision."
    },
    {
        "date": "2025-03",
        "title": "Demoting Security via Exploitation of Cache Demote Operation in Intel's Latest ISA Extension",
        "author": "Taehun Kim, Hyerean Jang, and Youngjoo Shin",
        "link": "http://arxiv.org/abs/2503.10074v1",
        "abstract": "ISA extensions are increasingly adopted to boost the performance of\nspecialized workloads without requiring an entire architectural redesign.\nHowever, these enhancements can inadvertently expose new attack surfaces in the\nmicroarchitecture. In this paper, we investigate Intel's recently introduced\ncldemote extension, which promotes efficient data sharing by transferring cache\nlines from upper-level caches to the Last Level Cache (LLC). Despite its\nperformance benefits, we uncover critical properties-unprivileged access,\ninter-cache state transition, and fault suppression-that render cldemote\nexploitable for microarchitectural attacks. We propose two new attack\nprimitives, Flush+Demote and Demote+Time, built on our analysis. Flush+Demote\nconstructs a covert channel with a bandwidth of 2.84 Mbps and a bit error rate\nof 0.018%, while Demote+Time derandomizes the kernel base address in 2.49 ms on\nLinux. Furthermore, we show that leveraging cldemote accelerates eviction set\nconstruction in non-inclusive LLC designs by obviating the need for helper\nthreads or extensive cache conflicts, thereby reducing construction time by 36%\nyet retaining comparable success rates. Finally, we examine how ISA extensions\ncontribute to broader microarchitectural attacks, identifying five key\nexploitable characteristics and categorizing four distinct attack types. We\nalso discuss potential countermeasures, highlighting the far-reaching security\nimplications of emerging ISA extensions."
    },
    {
        "date": "2025-03",
        "title": "Provably Secure Covert Messaging Using Image-based Diffusion Processes",
        "author": "Luke A. Bauer, Wenxuan Bao, and Vincent Bindschaedler",
        "link": "http://arxiv.org/abs/2503.10063v1",
        "abstract": "We consider the problem of securely and robustly embedding covert messages\ninto an image-based diffusion model's output. The sender and receiver want to\nexchange the maximum amount of information possible per diffusion sampled image\nwhile remaining undetected. The adversary wants to detect that such\ncommunication is taking place by identifying those diffusion samples that\ncontain covert messages. To maximize robustness to transformations of the\ndiffusion sample, a strategy is for the sender and the receiver to embed the\nmessage in the initial latents. We first show that prior work that attempted\nthis is easily broken because their embedding technique alters the latents'\ndistribution. We then propose a straightforward method to embed covert messages\nin the initial latent {\\em without} altering the distribution. We prove that\nour construction achieves indistinguishability to any probabilistic polynomial\ntime adversary. Finally, we discuss and analyze empirically the tradeoffs\nbetween embedding capacity, message recovery rates, and robustness. We find\nthat optimizing the inversion method for error correction is crucial for\nreliability."
    },
    {
        "date": "2025-03",
        "title": "Distributionally Robust Multi-Agent Reinforcement Learning for Dynamic Chute Mapping",
        "author": "Guangyi Liu, Suzan Iloglu, Michael Caldara, Joseph W. Durham, and Michael M. Zavlanos",
        "link": "http://arxiv.org/abs/2503.09755v1",
        "abstract": "In Amazon robotic warehouses, the destination-to-chute mapping problem is\ncrucial for efficient package sorting. Often, however, this problem is\ncomplicated by uncertain and dynamic package induction rates, which can lead to\nincreased package recirculation. To tackle this challenge, we introduce a\nDistributionally Robust Multi-Agent Reinforcement Learning (DRMARL) framework\nthat learns a destination-to-chute mapping policy that is resilient to\nadversarial variations in induction rates. Specifically, DRMARL relies on group\ndistributionally robust optimization (DRO) to learn a policy that performs well\nnot only on average but also on each individual subpopulation of induction\nrates within the group that capture, for example, different seasonality or\noperation modes of the system. This approach is then combined with a novel\ncontextual bandit-based predictor of the worst-case induction distribution for\neach state-action pair, significantly reducing the cost of exploration and\nthereby increasing the learning efficiency and scalability of our framework.\nExtensive simulations demonstrate that DRMARL achieves robust chute mapping in\nthe presence of varying induction distributions, reducing package recirculation\nby an average of 80\\% in the simulation scenario."
    },
    {
        "date": "2025-03",
        "title": "Enhancing Adversarial Example Detection Through Model Explanation",
        "author": "Qian Ma, and Ziping Ye",
        "link": "http://arxiv.org/abs/2503.09735v1",
        "abstract": "Adversarial examples are a major problem for machine learning models, leading\nto a continuous search for effective defenses. One promising direction is to\nleverage model explanations to better understand and defend against these\nattacks. We looked at AmI, a method proposed by a NeurIPS 2018 spotlight paper\nthat uses model explanations to detect adversarial examples. Our study shows\nthat while AmI is a promising idea, its performance is too dependent on\nspecific settings (e.g., hyperparameter) and external factors such as the\noperating system and the deep learning framework used, and such drawbacks limit\nAmI's practical usage. Our findings highlight the need for more robust defense\nmechanisms that are effective under various conditions. In addition, we\nadvocate for a comprehensive evaluation framework for defense techniques."
    },
    {
        "date": "2025-03",
        "title": "How Feasible is Augmenting Fake Nodes with Learnable Features as a Counter-strategy against Link Stealing Attacks?",
        "author": "Mir Imtiaz Mostafiz, Imtiaz Karim, and Elisa Bertino",
        "link": "http://arxiv.org/abs/2503.09726v1",
        "abstract": "Graph Neural Networks (GNNs) are widely used and deployed for graph-based\nprediction tasks. However, as good as GNNs are for learning graph data, they\nalso come with the risk of privacy leakage. For instance, an attacker can run\ncarefully crafted queries on the GNNs and, from the responses, can infer the\nexistence of an edge between a pair of nodes. This attack, dubbed as a\n\"link-stealing\" attack, can jeopardize the user's privacy by leaking\npotentially sensitive information. To protect against this attack, we propose\nan approach called \"$(N)$ode $(A)$ugmentation for $(R)$estricting $(G)$raphs\nfrom $(I)$nsinuating their $(S)$tructure\" ($NARGIS$) and study its feasibility.\n$NARGIS$ is focused on reshaping the graph embedding space so that the\nposterior from the GNN model will still provide utility for the prediction task\nbut will introduce ambiguity for the link-stealing attackers. To this end,\n$NARGIS$ applies spectral clustering on the given graph to facilitate it being\naugmented with new nodes -- that have learned features instead of fixed ones.\nIt utilizes tri-level optimization for learning parameters for the GNN model,\nsurrogate attacker model, and our defense model (i.e. learnable node features).\nWe extensively evaluate $NARGIS$ on three benchmark citation datasets over\neight knowledge availability settings for the attackers. We also evaluate the\nmodel fidelity and defense performance on influence-based link inference\nattacks. Through our studies, we have figured out the best feature of $NARGIS$\n-- its superior fidelity-privacy performance trade-off in a significant number\nof cases. We also have discovered in which cases the model needs to be\nimproved, and proposed ways to integrate different schemes to make the model\nmore robust against link stealing attacks."
    },
    {
        "date": "2025-03",
        "title": "Revisiting Backdoor Attacks on Time Series Classification in the Frequency Domain",
        "author": "Yuanmin Huang, Mi Zhang, Zhaoxiang Wang, Wenxuan Li, and Min Yang",
        "link": "http://arxiv.org/abs/2503.09712v1",
        "abstract": "Time series classification (TSC) is a cornerstone of modern web applications,\npowering tasks such as financial data analysis, network traffic monitoring, and\nuser behavior analysis. In recent years, deep neural networks (DNNs) have\ngreatly enhanced the performance of TSC models in these critical domains.\nHowever, DNNs are vulnerable to backdoor attacks, where attackers can covertly\nimplant triggers into models to induce malicious outcomes. Existing backdoor\nattacks targeting DNN-based TSC models remain elementary. In particular, early\nmethods borrow trigger designs from computer vision, which are ineffective for\ntime series data. More recent approaches utilize generative models for trigger\ngeneration, but at the cost of significant computational complexity. In this\nwork, we analyze the limitations of existing attacks and introduce an enhanced\nmethod, FreqBack. Drawing inspiration from the fact that DNN models inherently\ncapture frequency domain features in time series data, we identify that\nimproper perturbations in the frequency domain are the root cause of\nineffective attacks. To address this, we propose to generate triggers both\neffectively and efficiently, guided by frequency analysis. FreqBack exhibits\nsubstantial performance across five models and eight datasets, achieving an\nimpressive attack success rate of over 90%, while maintaining less than a 3%\ndrop in model accuracy on clean data."
    },
    {
        "date": "2025-03",
        "title": "Silent Branding Attack: Trigger-free Data Poisoning Attack on Text-to-Image Diffusion Models",
        "author": "Sangwon Jang, June Suk Choi, Jaehyeong Jo, Kimin Lee, and Sung Ju Hwang",
        "link": "http://arxiv.org/abs/2503.09669v1",
        "abstract": "Text-to-image diffusion models have achieved remarkable success in generating\nhigh-quality contents from text prompts. However, their reliance on publicly\navailable data and the growing trend of data sharing for fine-tuning make these\nmodels particularly vulnerable to data poisoning attacks. In this work, we\nintroduce the Silent Branding Attack, a novel data poisoning method that\nmanipulates text-to-image diffusion models to generate images containing\nspecific brand logos or symbols without any text triggers. We find that when\ncertain visual patterns are repeatedly in the training data, the model learns\nto reproduce them naturally in its outputs, even without prompt mentions.\nLeveraging this, we develop an automated data poisoning algorithm that\nunobtrusively injects logos into original images, ensuring they blend naturally\nand remain undetected. Models trained on this poisoned dataset generate images\ncontaining logos without degrading image quality or text alignment. We\nexperimentally validate our silent branding attack across two realistic\nsettings on large-scale high-quality image datasets and style personalization\ndatasets, achieving high success rates even without a specific text trigger.\nHuman evaluation and quantitative metrics including logo detection show that\nour method can stealthily embed logos."
    },
    {
        "date": "2025-03",
        "title": "RESTRAIN: Reinforcement Learning-Based Secure Framework for Trigger-Action IoT Environment",
        "author": "Md Morshed Alam, Lokesh Chandra Das, Sandip Roy, Sachin Shetty, and Weichao Wang",
        "link": "http://arxiv.org/abs/2503.09513v1",
        "abstract": "Internet of Things (IoT) platforms with trigger-action capability allow event\nconditions to trigger actions in IoT devices autonomously by creating a chain\nof interactions. Adversaries exploit this chain of interactions to maliciously\ninject fake event conditions into IoT hubs, triggering unauthorized actions on\ntarget IoT devices to implement remote injection attacks. Existing defense\nmechanisms focus mainly on the verification of event transactions using\nphysical event fingerprints to enforce the security policies to block unsafe\nevent transactions. These approaches are designed to provide offline defense\nagainst injection attacks. The state-of-the-art online defense mechanisms offer\nreal-time defense, but extensive reliability on the inference of attack impacts\non the IoT network limits the generalization capability of these approaches. In\nthis paper, we propose a platform-independent multi-agent online defense\nsystem, namely RESTRAIN, to counter remote injection attacks at runtime.\nRESTRAIN allows the defense agent to profile attack actions at runtime and\nleverages reinforcement learning to optimize a defense policy that complies\nwith the security requirements of the IoT network. The experimental results\nshow that the defense agent effectively takes real-time defense actions against\ncomplex and dynamic remote injection attacks and maximizes the security gain\nwith minimal computational overhead."
    },
    {
        "date": "2025-03",
        "title": "Towards Robust Multimodal Representation: A Unified Approach with Adaptive Experts and Alignment",
        "author": "Nazanin Moradinasab, Saurav Sengupta, Jiebei Liu, Sana Syed, and Donald E. Brown",
        "link": "http://arxiv.org/abs/2503.09498v1",
        "abstract": "Healthcare relies on multiple types of data, such as medical images, genetic\ninformation, and clinical records, to improve diagnosis and treatment. However,\nmissing data is a common challenge due to privacy restrictions, cost, and\ntechnical issues, making many existing multi-modal models unreliable. To\naddress this, we propose a new multi-model model called Mixture of Experts,\nSymmetric Aligning, and Reconstruction (MoSARe), a deep learning framework that\nhandles incomplete multimodal data while maintaining high accuracy. MoSARe\nintegrates expert selection, cross-modal attention, and contrastive learning to\nimprove feature representation and decision-making. Our results show that\nMoSARe outperforms existing models in situations when the data is complete.\nFurthermore, it provides reliable predictions even when some data are missing.\nThis makes it especially useful in real-world healthcare settings, including\nresource-limited environments. Our code is publicly available at\nhttps://github.com/NazaninMn/MoSARe."
    },
    {
        "date": "2025-03",
        "title": "Robust Multimodal Survival Prediction with the Latent Differentiation Conditional Variational AutoEncoder",
        "author": "Junjie Zhou, Jiao Tang, Yingli Zuo, Peng Wan, Daoqiang Zhang, and Wei Shao",
        "link": "http://arxiv.org/abs/2503.09496v1",
        "abstract": "The integrative analysis of histopathological images and genomic data has\nreceived increasing attention for survival prediction of human cancers.\nHowever, the existing studies always hold the assumption that full modalities\nare available. As a matter of fact, the cost for collecting genomic data is\nhigh, which sometimes makes genomic data unavailable in testing samples. A\ncommon way of tackling such incompleteness is to generate the genomic\nrepresentations from the pathology images. Nevertheless, such strategy still\nfaces the following two challenges: (1) The gigapixel whole slide images (WSIs)\nare huge and thus hard for representation. (2) It is difficult to generate the\ngenomic embeddings with diverse function categories in a unified generative\nframework. To address the above challenges, we propose a Conditional Latent\nDifferentiation Variational AutoEncoder (LD-CVAE) for robust multimodal\nsurvival prediction, even with missing genomic data. Specifically, a\nVariational Information Bottleneck Transformer (VIB-Trans) module is proposed\nto learn compressed pathological representations from the gigapixel WSIs. To\ngenerate different functional genomic features, we develop a novel Latent\nDifferentiation Variational AutoEncoder (LD-VAE) to learn the common and\nspecific posteriors for the genomic embeddings with diverse functions. Finally,\nwe use the product-of-experts technique to integrate the genomic common\nposterior and image posterior for the joint latent distribution estimation in\nLD-CVAE. We test the effectiveness of our method on five different cancer\ndatasets, and the experimental results demonstrate its superiority in both\ncomplete and missing modality scenarios."
    },
    {
        "date": "2025-03",
        "title": "Project-Probe-Aggregate: Efficient Fine-Tuning for Group Robustness",
        "author": "Beier Zhu, Jiequan Cui, Hanwang Zhang, and Chi Zhang",
        "link": "http://arxiv.org/abs/2503.09487v1",
        "abstract": "While image-text foundation models have succeeded across diverse downstream\ntasks, they still face challenges in the presence of spurious correlations\nbetween the input and label. To address this issue, we propose a simple\nthree-step approach,Project-Probe-Aggregate (PPA), that enables\nparameter-efficient fine-tuning for foundation models without relying on group\nannotations. Building upon the failure-based debiasing scheme, our method, PPA,\nimproves its two key components: minority samples identification and the robust\ntraining algorithm. Specifically, we first train biased classifiers by\nprojecting image features onto the nullspace of class proxies from text\nencoders. Next, we infer group labels using the biased classifier and probe\ngroup targets with prior correction. Finally, we aggregate group weights of\neach class to produce the debiased classifier. Our theoretical analysis shows\nthat our PPA enhances minority group identification and is Bayes optimal for\nminimizing the balanced group error, mitigating spurious correlations.\nExtensive experimental results confirm the effectiveness of our PPA: it\noutperforms the state-of-the-art by an average worst-group accuracy while\nrequiring less than 0.01% tunable parameters without training group labels."
    },
    {
        "date": "2025-03",
        "title": "Automatic Association of Quality Requirements and Quantifiable Metrics for Cloud Security Certification",
        "author": "John Bianchi, Shuya Dong, Luca Petrillo, and Marinella Petrocchi",
        "link": "http://arxiv.org/abs/2503.09460v1",
        "abstract": "The European Cybersecurity Certification Scheme for Cloud Services (EUCS) is\none of the first cybersecurity schemes in Europe, defined by the European Union\nAgency for Cybersecurity (ENISA). It aims to encourage cloud providers to\nstrengthen their cybersecurity policies in order to receive an official seal of\napproval from European authorities. EUCS defines a set of security requirements\nthat the cloud provider must meet, in whole or in part, in order to achieve the\nsecurity certification. The requirements are written in natural language and\ncover every aspect of security in the cloud environment, from logging access to\nprotecting the system with anti-malware tools to training staff. Operationally,\neach requirement is associated with one or more evaluable metrics. For example,\na requirement to monitor access attempts to a service will have associated\nmetrics that take into account the number of accesses, the number of access\nattempts, who is accessing, and what resources are being used. Partners in the\nEuropean project Medina, which ended in October 2023, defined 163 metrics and\nmanually mapped them to 70 EUCS requirements. Manual mapping is intuitively a\nlong and costly process in terms of human resources. This paper proposes an\napproach based on Sentence Transformers to automatically associate requirements\nand metrics. In terms of correctness of associations, the proposed method\nachieves a Normalized Discounted Cumulative Gain of 0.640, improving a previous\nexperiment by 0.146 points."
    },
    {
        "date": "2025-03",
        "title": "Benefits of Learning Rate Annealing for Tuning-Robustness in Stochastic Optimization",
        "author": "Amit Attia, and Tomer Koren",
        "link": "http://arxiv.org/abs/2503.09411v1",
        "abstract": "The learning rate in stochastic gradient methods is a critical hyperparameter\nthat is notoriously costly to tune via standard grid search, especially for\ntraining modern large-scale models with billions of parameters. We identify a\ntheoretical advantage of learning rate annealing schemes that decay the\nlearning rate to zero at a polynomial rate, such as the widely-used cosine\nschedule, by demonstrating their increased robustness to initial parameter\nmisspecification due to a coarse grid search. We present an analysis in a\nstochastic convex optimization setup demonstrating that the convergence rate of\nstochastic gradient descent with annealed schedules depends sublinearly on the\nmultiplicative misspecification factor $\\rho$ (i.e., the grid resolution),\nachieving a rate of $O(\\rho^{1/(2p+1)}/\\sqrt{T})$ where $p$ is the degree of\npolynomial decay and $T$ is the number of steps, in contrast to the\n$O(\\rho/\\sqrt{T})$ rate that arises with fixed stepsizes and exhibits a linear\ndependence on $\\rho$. Experiments confirm the increased robustness compared to\ntuning with a fixed stepsize, that has significant implications for the\ncomputational overhead of hyperparameter search in practical training\nscenarios."
    },
    {
        "date": "2025-03",
        "title": "AI-based Framework for Robust Model-Based Connector Mating in Robotic Wire Harness Installation",
        "author": "Claudius Kienle, Benjamin Alt, Finn Schneider, Tobias Pertlwieser, Rainer J\u00e4kel, and Rania Rayyes",
        "link": "http://arxiv.org/abs/2503.09409v1",
        "abstract": "Despite the widespread adoption of industrial robots in automotive assembly,\nwire harness installation remains a largely manual process, as it requires\nprecise and flexible manipulation. To address this challenge, we design a novel\nAI-based framework that automates cable connector mating by integrating force\ncontrol with deep visuotactile learning. Our system optimizes\nsearch-and-insertion strategies using first-order optimization over a\nmultimodal transformer architecture trained on visual, tactile, and\nproprioceptive data. Additionally, we design a novel automated data collection\nand optimization pipeline that minimizes the need for machine learning\nexpertise. The framework optimizes robot programs that run natively on standard\nindustrial controllers, permitting human experts to audit and certify them.\nExperimental validations on a center console assembly task demonstrate\nsignificant improvements in cycle times and robustness compared to conventional\nrobot programming approaches. Videos are available under\nhttps://claudius-kienle.github.io/AppMuTT."
    },
    {
        "date": "2025-03",
        "title": "Membership Inference Attacks fueled by Few-Short Learning to detect privacy leakage tackling data integrity",
        "author": "Daniel Jim\u00e9nez-L\u00f3pez, Nuria Rodr\u00edguez-Barroso, M. Victoria Luz\u00f3n, and Francisco Herrera",
        "link": "http://arxiv.org/abs/2503.09365v1",
        "abstract": "Deep learning models have an intrinsic privacy issue as they memorize parts\nof their training data, creating a privacy leakage. Membership Inference\nAttacks (MIA) exploit it to obtain confidential information about the data used\nfor training, aiming to steal information. They can be repurposed as a\nmeasurement of data integrity by inferring whether it was used to train a\nmachine learning model. While state-of-the-art attacks achieve a significant\nprivacy leakage, their requirements are not feasible enough, hindering their\nrole as practical tools to assess the magnitude of the privacy risk. Moreover,\nthe most appropriate evaluation metric of MIA, the True Positive Rate at low\nFalse Positive Rate lacks interpretability. We claim that the incorporation of\nFew-Shot Learning techniques to the MIA field and a proper qualitative and\nquantitative privacy evaluation measure should deal with these issues. In this\ncontext, our proposal is twofold. We propose a Few-Shot learning based MIA,\ncoined as the FeS-MIA model, which eases the evaluation of the privacy breach\nof a deep learning model by significantly reducing the number of resources\nrequired for the purpose. Furthermore, we propose an interpretable quantitative\nand qualitative measure of privacy, referred to as Log-MIA measure. Jointly,\nthese proposals provide new tools to assess the privacy leakage and to ease the\nevaluation of the training data integrity of deep learning models, that is, to\nanalyze the privacy breach of a deep learning model. Experiments carried out\nwith MIA over image classification and language modeling tasks and its\ncomparison to the state-of-the-art show that our proposals excel at reporting\nthe privacy leakage of a deep learning model with little extra information."
    },
    {
        "date": "2025-03",
        "title": "Safer or Luckier? LLMs as Safety Evaluators Are Not Robust to Artifacts",
        "author": "Hongyu Chen, and Seraphina Goldfarb-Tarrant",
        "link": "http://arxiv.org/abs/2503.09347v1",
        "abstract": "Large Language Models (LLMs) are increasingly employed as automated\nevaluators to assess the safety of generated content, yet their reliability in\nthis role remains uncertain. This study evaluates a diverse set of 11 LLM judge\nmodels across critical safety domains, examining three key aspects:\nself-consistency in repeated judging tasks, alignment with human judgments, and\nsusceptibility to input artifacts such as apologetic or verbose phrasing. Our\nfindings reveal that biases in LLM judges can significantly distort the final\nverdict on which content source is safer, undermining the validity of\ncomparative evaluations. Notably, apologetic language artifacts alone can skew\nevaluator preferences by up to 98\\%. Contrary to expectations, larger models do\nnot consistently exhibit greater robustness, while smaller models sometimes\nshow higher resistance to specific artifacts. To mitigate LLM evaluator\nrobustness issues, we investigate jury-based evaluations aggregating decisions\nfrom multiple models. Although this approach both improves robustness and\nenhances alignment to human judgements, artifact sensitivity persists even with\nthe best jury configurations. These results highlight the urgent need for\ndiversified, artifact-resistant methodologies to ensure reliable safety\nassessments."
    },
    {
        "date": "2025-03",
        "title": "Stealthy Patch-Wise Backdoor Attack in 3D Point Cloud via Curvature Awareness",
        "author": "Yu Feng, Dingxin Zhang, Runkai Zhao, Yong Xia, Heng Huang, and Weidong Cai",
        "link": "http://arxiv.org/abs/2503.09336v1",
        "abstract": "Backdoor attacks pose a severe threat to deep neural networks (DNN) by\nimplanting hidden backdoors that can be activated with predefined triggers to\nmanipulate model behaviors maliciously. Existing 3D point cloud backdoor\nattacks primarily rely on sample-wise global modifications, resulting in\nsuboptimal stealthiness. To address this limitation, we propose Stealthy\nPatch-Wise Backdoor Attack (SPBA), which employs the first patch-wise trigger\nfor 3D point clouds and restricts perturbations to local regions, significantly\nenhancing stealthiness. Specifically, SPBA decomposes point clouds into local\npatches and evaluates their geometric complexity using a curvature-based patch\nimperceptibility score, ensuring that the trigger remains less perceptible to\nthe human eye by strategically applying it across multiple geometrically\ncomplex patches with lower visual sensitivity. By leveraging the Graph Fourier\nTransform (GFT), SPBA optimizes a patch-wise spectral trigger that perturbs the\nspectral features of selected patches, enhancing attack effectiveness while\npreserving the global geometric structure of the point cloud. Extensive\nexperiments on ModelNet40 and ShapeNetPart demonstrate that SPBA consistently\nachieves an attack success rate (ASR) exceeding 96.5% across different models\nwhile achieving state-of-the-art imperceptibility compared to existing backdoor\nattack methods."
    },
    {
        "date": "2025-03",
        "title": "CyberLLMInstruct: A New Dataset for Analysing Safety of Fine-Tuned LLMs Using Cyber Security Data",
        "author": "Adel ElZemity, Budi Arief, and Shujun Li",
        "link": "http://arxiv.org/abs/2503.09334v1",
        "abstract": "The integration of large language models (LLMs) into cyber security\napplications presents significant opportunities, such as enhancing threat\nanalysis and malware detection, but can also introduce critical risks and\nsafety concerns, including personal data leakage and automated generation of\nnew malware. To address these challenges, we developed CyberLLMInstruct, a\ndataset of 54,928 instruction-response pairs spanning cyber security tasks such\nas malware analysis, phishing simulations, and zero-day vulnerabilities. The\ndataset was constructed through a multi-stage process. This involved sourcing\ndata from multiple resources, filtering and structuring it into\ninstruction-response pairs, and aligning it with real-world scenarios to\nenhance its applicability. Seven open-source LLMs were chosen to test the\nusefulness of CyberLLMInstruct: Phi 3 Mini 3.8B, Mistral 7B, Qwen 2.5 7B, Llama\n3 8B, Llama 3.1 8B, Gemma 2 9B, and Llama 2 70B. In our primary example, we\nrigorously assess the safety of fine-tuned models using the OWASP top 10\nframework, finding that fine-tuning reduces safety resilience across all tested\nLLMs and every adversarial attack (e.g., the security score of Llama 3.1 8B\nagainst prompt injection drops from 0.95 to 0.15). In our second example, we\nshow that these same fine-tuned models can also achieve up to 92.50 percent\naccuracy on the CyberMetric benchmark. These findings highlight a trade-off\nbetween performance and safety, showing the importance of adversarial testing\nand further research into fine-tuning methodologies that can mitigate safety\nrisks while still improving performance across diverse datasets and domains.\nAll scripts required to reproduce the dataset, along with examples and relevant\nresources for replicating our results, will be made available upon the paper's\nacceptance."
    },
    {
        "date": "2025-03",
        "title": "Group-robust Machine Unlearning",
        "author": "Thomas De Min, Subhankar Roy, St\u00e9phane Lathuili\u00e8re, Elisa Ricci, and Massimiliano Mancini",
        "link": "http://arxiv.org/abs/2503.09330v1",
        "abstract": "Machine unlearning is an emerging paradigm to remove the influence of\nspecific training data (i.e., the forget set) from a model while preserving its\nknowledge of the rest of the data (i.e., the retain set). Previous approaches\nassume the forget data to be uniformly distributed from all training\ndatapoints. However, if the data to unlearn is dominant in one group, we\nempirically show that performance for this group degrades, leading to fairness\nissues. This work tackles the overlooked problem of non-uniformly distributed\nforget sets, which we call group-robust machine unlearning, by presenting a\nsimple, effective strategy that mitigates the performance loss in dominant\ngroups via sample distribution reweighting. Moreover, we present MIU (Mutual\nInformation-aware Machine Unlearning), the first approach for group robustness\nin approximate machine unlearning. MIU minimizes the mutual information between\nmodel features and group information, achieving unlearning while reducing\nperformance degradation in the dominant group of the forget set. Additionally,\nMIU exploits sample distribution reweighting and mutual information calibration\nwith the original model to preserve group robustness. We conduct experiments on\nthree datasets and show that MIU outperforms standard methods, achieving\nunlearning without compromising model robustness. Source code available at\nhttps://github.com/tdemin16/group-robust_machine_unlearning."
    },
    {
        "date": "2025-03",
        "title": "Towards Robust Model Evolution with Algorithmic Recourse",
        "author": "Hao-Tsung Yang, Jie Gao, Bo-Yi Liu, and Zhi-Xuan Liu",
        "link": "http://arxiv.org/abs/2503.09658v1",
        "abstract": "Algorithmic Recourse is a way for users to modify their attributes to align\nwith a model's expectations, thereby improving their outcomes after receiving\nunfavorable decisions. In real-world scenarios, users often need to\nstrategically adjust their attributes to compete for limited resources.\nHowever, such strategic behavior induces users to \"game\" algorithms, causing\nmodel collapse due to distribution shifts. These shifts arise from user\ncompetition, resource constraints, and adaptive user responses. While prior\nresearch on Algorithmic Recourse has explored its effects on both systems and\nusers, the impact of resource constraints and competition over time remains\nunderexplored. In this work, we develop a general framework to model user\nstrategic behaviors and their interactions with decision-making systems under\nresource constraints and competitive dynamics. Through theoretical analysis and\nempirical evaluation, we identify three key phenomena that arise consistently\nin both synthetic and real-world datasets: escalating decision boundaries,\nnon-robust model predictions, and inequitable recourse actions. Finally, we\ndiscuss the broader social implications of these findings and present two\nalgorithmic strategies aimed at mitigating these challenges."
    },
    {
        "date": "2025-03",
        "title": "Detecting and Preventing Data Poisoning Attacks on AI Models",
        "author": "Halima I. Kure, Pradipta Sarkar, Ahmed B. Ndanusa, and Augustine O. Nwajana",
        "link": "http://arxiv.org/abs/2503.09302v1",
        "abstract": "This paper investigates the critical issue of data poisoning attacks on AI\nmodels, a growing concern in the ever-evolving landscape of artificial\nintelligence and cybersecurity. As advanced technology systems become\nincreasingly prevalent across various sectors, the need for robust defence\nmechanisms against adversarial attacks becomes paramount. The study aims to\ndevelop and evaluate novel techniques for detecting and preventing data\npoisoning attacks, focusing on both theoretical frameworks and practical\napplications. Through a comprehensive literature review, experimental\nvalidation using the CIFAR-10 and Insurance Claims datasets, and the\ndevelopment of innovative algorithms, this paper seeks to enhance the\nresilience of AI models against malicious data manipulation. The study explores\nvarious methods, including anomaly detection, robust optimization strategies,\nand ensemble learning, to identify and mitigate the effects of poisoned data\nduring model training. Experimental results indicate that data poisoning\nsignificantly degrades model performance, reducing classification accuracy by\nup to 27% in image recognition tasks (CIFAR-10) and 22% in fraud detection\nmodels (Insurance Claims dataset). The proposed defence mechanisms, including\nstatistical anomaly detection and adversarial training, successfully mitigated\npoisoning effects, improving model robustness and restoring accuracy levels by\nan average of 15-20%. The findings further demonstrate that ensemble learning\ntechniques provide an additional layer of resilience, reducing false positives\nand false negatives caused by adversarial data injections."
    },
    {
        "date": "2025-03",
        "title": "Prompt Inference Attack on Distributed Large Language Model Inference Frameworks",
        "author": "Xinjian Luo, Ting Yu, and Xiaokui Xiao",
        "link": "http://arxiv.org/abs/2503.09291v1",
        "abstract": "The inference process of modern large language models (LLMs) demands\nprohibitive computational resources, rendering them infeasible for deployment\non consumer-grade devices. To address this limitation, recent studies propose\ndistributed LLM inference frameworks, which employ split learning principles to\nenable collaborative LLM inference on resource-constrained hardware. However,\ndistributing LLM layers across participants requires the transmission of\nintermediate outputs, which may introduce privacy risks to the original input\nprompts - a critical issue that has yet to be thoroughly explored in the\nliterature.\n  In this paper, we rigorously examine the privacy vulnerabilities of\ndistributed LLM inference frameworks by designing and evaluating three prompt\ninference attacks aimed at reconstructing input prompts from intermediate LLM\noutputs. These attacks are developed under various query and data constraints\nto reflect diverse real-world LLM service scenarios. Specifically, the first\nattack assumes an unlimited query budget and access to an auxiliary dataset\nsharing the same distribution as the target prompts. The second attack also\nleverages unlimited queries but uses an auxiliary dataset with a distribution\ndiffering from the target prompts. The third attack operates under the most\nrestrictive scenario, with limited query budgets and no auxiliary dataset\navailable. We evaluate these attacks on a range of LLMs, including\nstate-of-the-art models such as Llama-3.2 and Phi-3.5, as well as widely-used\nmodels like GPT-2 and BERT for comparative analysis. Our experiments show that\nthe first two attacks achieve reconstruction accuracies exceeding 90%, while\nthe third achieves accuracies typically above 50%, even under stringent\nconstraints. These findings highlight privacy risks in distributed LLM\ninference frameworks, issuing a strong alert on their deployment in real-world\napplications."
    },
    {
        "date": "2025-03",
        "title": "In-Context Defense in Computer Agents: An Empirical Study",
        "author": "Pei Yang, Hai Ci, and Mike Zheng Shou",
        "link": "http://arxiv.org/abs/2503.09241v1",
        "abstract": "Computer agents powered by vision-language models (VLMs) have significantly\nadvanced human-computer interaction, enabling users to perform complex tasks\nthrough natural language instructions. However, these agents are vulnerable to\ncontext deception attacks, an emerging threat where adversaries embed\nmisleading content into the agent's operational environment, such as a pop-up\nwindow containing deceptive instructions. Existing defenses, such as\ninstructing agents to ignore deceptive elements, have proven largely\nineffective. As the first systematic study on protecting computer agents, we\nintroduce textbf{in-context defense}, leveraging in-context learning and\nchain-of-thought (CoT) reasoning to counter such attacks. Our approach involves\naugmenting the agent's context with a small set of carefully curated exemplars\ncontaining both malicious environments and corresponding defensive responses.\nThese exemplars guide the agent to first perform explicit defensive reasoning\nbefore action planning, reducing susceptibility to deceptive attacks.\nExperiments demonstrate the effectiveness of our method, reducing attack\nsuccess rates by 91.2% on pop-up window attacks, 74.6% on average on\nenvironment injection attacks, while achieving 100% successful defenses against\ndistracting advertisements. Our findings highlight that (1) defensive reasoning\nmust precede action planning for optimal performance, and (2) a minimal number\nof exemplars (fewer than three) is sufficient to induce an agent's defensive\nbehavior."
    },
    {
        "date": "2025-03",
        "title": "Robust Asymmetric Heterogeneous Federated Learning with Corrupted Clients",
        "author": "Xiuwen Fang, Mang Ye, and Bo Du",
        "link": "http://arxiv.org/abs/2503.09206v1",
        "abstract": "This paper studies a challenging robust federated learning task with model\nheterogeneous and data corrupted clients, where the clients have different\nlocal model structures. Data corruption is unavoidable due to factors such as\nrandom noise, compression artifacts, or environmental conditions in real-world\ndeployment, drastically crippling the entire federated system. To address these\nissues, this paper introduces a novel Robust Asymmetric Heterogeneous Federated\nLearning (RAHFL) framework. We propose a Diversity-enhanced supervised\nContrastive Learning technique to enhance the resilience and adaptability of\nlocal models on various data corruption patterns. Its basic idea is to utilize\ncomplex augmented samples obtained by the mixed-data augmentation strategy for\nsupervised contrastive learning, thereby enhancing the ability of the model to\nlearn robust and diverse feature representations. Furthermore, we design an\nAsymmetric Heterogeneous Federated Learning strategy to resist corrupt feedback\nfrom external clients. The strategy allows clients to perform selective one-way\nlearning during collaborative learning phase, enabling clients to refrain from\nincorporating lower-quality information from less robust or underperforming\ncollaborators. Extensive experimental results demonstrate the effectiveness and\nrobustness of our approach in diverse, challenging federated learning\nenvironments. Our code and models are public available at\nhttps://github.com/FangXiuwen/RAHFL."
    },
    {
        "date": "2025-03",
        "title": "AdvAD: Exploring Non-Parametric Diffusion for Imperceptible Adversarial Attacks",
        "author": "Jin Li, Ziqiang He, Anwei Luo, Jian-Fang Hu, Z. Jane Wang, and Xiangui Kang",
        "link": "http://arxiv.org/abs/2503.09124v1",
        "abstract": "Imperceptible adversarial attacks aim to fool DNNs by adding imperceptible\nperturbation to the input data. Previous methods typically improve the\nimperceptibility of attacks by integrating common attack paradigms with\nspecifically designed perception-based losses or the capabilities of generative\nmodels. In this paper, we propose Adversarial Attacks in Diffusion (AdvAD), a\nnovel modeling framework distinct from existing attack paradigms. AdvAD\ninnovatively conceptualizes attacking as a non-parametric diffusion process by\ntheoretically exploring basic modeling approach rather than using the denoising\nor generation abilities of regular diffusion models requiring neural networks.\nAt each step, much subtler yet effective adversarial guidance is crafted using\nonly the attacked model without any additional network, which gradually leads\nthe end of diffusion process from the original image to a desired imperceptible\nadversarial example. Grounded in a solid theoretical foundation of the proposed\nnon-parametric diffusion process, AdvAD achieves high attack efficacy and\nimperceptibility with intrinsically lower overall perturbation strength.\nAdditionally, an enhanced version AdvAD-X is proposed to evaluate the extreme\nof our novel framework under an ideal scenario. Extensive experiments\ndemonstrate the effectiveness of the proposed AdvAD and AdvAD-X. Compared with\nstate-of-the-art imperceptible attacks, AdvAD achieves an average of 99.9$\\%$\n(+17.3$\\%$) ASR with 1.34 (-0.97) $l_2$ distance, 49.74 (+4.76) PSNR and 0.9971\n(+0.0043) SSIM against four prevalent DNNs with three different architectures\non the ImageNet-compatible dataset. Code is available at\nhttps://github.com/XianguiKang/AdvAD."
    },
    {
        "date": "2025-03",
        "title": "C^2 ATTACK: Towards Representation Backdoor on CLIP via Concept Confusion",
        "author": "Lijie Hu, Junchi Liao, Weimin Lyu, Shaopeng Fu, Tianhao Huang, Shu Yang, Guimin Hu, and Di Wang",
        "link": "http://arxiv.org/abs/2503.09095v1",
        "abstract": "Backdoor attacks pose a significant threat to deep learning models, enabling\nadversaries to embed hidden triggers that manipulate the behavior of the model\nduring inference. Traditional backdoor attacks typically rely on inserting\nexplicit triggers (e.g., external patches, or perturbations) into input data,\nbut they often struggle to evade existing defense mechanisms. To address this\nlimitation, we investigate backdoor attacks through the lens of the reasoning\nprocess in deep learning systems, drawing insights from interpretable AI. We\nconceptualize backdoor activation as the manipulation of learned concepts\nwithin the model's latent representations. Thus, existing attacks can be seen\nas implicit manipulations of these activated concepts during inference. This\nraises interesting questions: why not manipulate the concepts explicitly? This\nidea leads to our novel backdoor attack framework, Concept Confusion Attack\n(C^2 ATTACK), which leverages internal concepts in the model's reasoning as\n\"triggers\" without introducing explicit external modifications. By avoiding the\nuse of real triggers and directly activating or deactivating specific concepts\nin latent spaces, our approach enhances stealth, making detection by existing\ndefenses significantly harder. Using CLIP as a case study, experimental results\ndemonstrate the effectiveness of C^2 ATTACK, achieving high attack success\nrates while maintaining robustness against advanced defenses."
    },
    {
        "date": "2025-03",
        "title": "Probing Latent Subspaces in LLM for AI Security: Identifying and Manipulating Adversarial States",
        "author": "Xin Wei Chia, and Jonathan Pan",
        "link": "http://arxiv.org/abs/2503.09066v1",
        "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nvarious tasks, yet they remain vulnerable to adversarial manipulations such as\njailbreaking via prompt injection attacks. These attacks bypass safety\nmechanisms to generate restricted or harmful content. In this study, we\ninvestigated the underlying latent subspaces of safe and jailbroken states by\nextracting hidden activations from a LLM. Inspired by attractor dynamics in\nneuroscience, we hypothesized that LLM activations settle into semi stable\nstates that can be identified and perturbed to induce state transitions. Using\ndimensionality reduction techniques, we projected activations from safe and\njailbroken responses to reveal latent subspaces in lower dimensional spaces. We\nthen derived a perturbation vector that when applied to safe representations,\nshifted the model towards a jailbreak state. Our results demonstrate that this\ncausal intervention results in statistically significant jailbreak responses in\na subset of prompts. Next, we probed how these perturbations propagate through\nthe model's layers, testing whether the induced state change remains localized\nor cascades throughout the network. Our findings indicate that targeted\nperturbations induced distinct shifts in activations and model responses. Our\napproach paves the way for potential proactive defenses, shifting from\ntraditional guardrail based methods to preemptive, model agnostic techniques\nthat neutralize adversarial states at the representation level."
    },
    {
        "date": "2025-03",
        "title": "Mono2D: A Trainable Monogenic Layer for Robust Knee Cartilage Segmentation on Out-of-Distribution 2D Ultrasound Data",
        "author": "Alvin Kimbowa, Arjun Parmar, Maziar Badii, David Liu, Matthew Harkey, and Ilker Hacihaliloglu",
        "link": "http://arxiv.org/abs/2503.09050v1",
        "abstract": "Automated knee cartilage segmentation using point-of-care ultrasound devices\nand deep-learning networks has the potential to enhance the management of knee\nosteoarthritis. However, segmentation algorithms often struggle with domain\nshifts caused by variations in ultrasound devices and acquisition parameters,\nlimiting their generalizability. In this paper, we propose Mono2D, a monogenic\nlayer that extracts multi-scale, contrast- and intensity-invariant local phase\nfeatures using trainable bandpass quadrature filters. This layer mitigates\ndomain shifts, improving generalization to out-of-distribution domains. Mono2D\nis integrated before the first layer of a segmentation network, and its\nparameters jointly trained alongside the network's parameters. We evaluated\nMono2D on a multi-domain 2D ultrasound knee cartilage dataset for single-source\ndomain generalization (SSDG). Our results demonstrate that Mono2D outperforms\nother SSDG methods in terms of Dice score and mean average surface distance. To\nfurther assess its generalizability, we evaluate Mono2D on a multi-site\nprostate MRI dataset, where it continues to outperform other SSDG methods,\nhighlighting its potential to improve domain generalization in medical imaging.\nNevertheless, further evaluation on diverse datasets is still necessary to\nassess its clinical utility."
    },
    {
        "date": "2025-03",
        "title": "Adaptive Backdoor Attacks with Reasonable Constraints on Graph Neural Networks",
        "author": "Xuewen Dong, Jiachen Li, Shujun Li, Zhichao You, Qiang Qu, Yaroslav Kholodov, and Yulong Shen",
        "link": "http://arxiv.org/abs/2503.09049v1",
        "abstract": "Recent studies show that graph neural networks (GNNs) are vulnerable to\nbackdoor attacks. Existing backdoor attacks against GNNs use fixed-pattern\ntriggers and lack reasonable trigger constraints, overlooking individual graph\ncharacteristics and rendering insufficient evasiveness. To tackle the above\nissues, we propose ABARC, the first Adaptive Backdoor Attack with Reasonable\nConstraints, applying to both graph-level and node-level tasks in GNNs. For\ngraph-level tasks, we propose a subgraph backdoor attack independent of the\ngraph's topology. It dynamically selects trigger nodes for each target graph\nand modifies node features with constraints based on graph similarity, feature\nrange, and feature type. For node-level tasks, our attack begins with an\nanalysis of node features, followed by selecting and modifying trigger\nfeatures, which are then constrained by node similarity, feature range, and\nfeature type. Furthermore, an adaptive edge-pruning mechanism is designed to\nreduce the impact of neighbors on target nodes, ensuring a high attack success\nrate (ASR). Experimental results show that even with reasonable constraints for\nattack evasiveness, our attack achieves a high ASR while incurring a marginal\nclean accuracy drop (CAD). When combined with the state-of-the-art defense\nrandomized smoothing (RS) method, our attack maintains an ASR over 94%,\nsurpassing existing attacks by more than 7%."
    },
    {
        "date": "2025-03",
        "title": "Prompt Inversion Attack against Collaborative Inference of Large Language Models",
        "author": "Wenjie Qu, Yuguang Zhou, Yongji Wu, Tingsong Xiao, Binhang Yuan, Yiming Li, and Jiaheng Zhang",
        "link": "http://arxiv.org/abs/2503.09022v2",
        "abstract": "Large language models (LLMs) have been widely applied for their remarkable\ncapability of content generation. However, the practical use of open-source\nLLMs is hindered by high resource requirements, making deployment expensive and\nlimiting widespread development. The collaborative inference is a promising\nsolution for this problem, in which users collaborate by each hosting a subset\nof layers and transmitting intermediate activation. Many companies are building\ncollaborative inference platforms to reduce LLM serving costs, leveraging\nusers' underutilized GPUs. Despite widespread interest in collaborative\ninference within academia and industry, the privacy risks associated with LLM\ncollaborative inference have not been well studied. This is largely because of\nthe challenge posed by inverting LLM activation due to its strong\nnon-linearity.\n  In this paper, to validate the severity of privacy threats in LLM\ncollaborative inference, we introduce the concept of prompt inversion attack\n(PIA), where a malicious participant intends to recover the input prompt\nthrough the activation transmitted by its previous participant. Extensive\nexperiments show that our PIA method substantially outperforms existing\nbaselines. For example, our method achieves an 88.4\\% token accuracy on the\nSkytrax dataset with the Llama-65B model when inverting the maximum number of\ntransformer layers, while the best baseline method only achieves 22.8\\%\naccuracy. The results verify the effectiveness of our PIA attack and highlights\nits practical threat to LLM collaborative inference systems."
    },
    {
        "date": "2025-03",
        "title": "Not All Edges are Equally Robust: Evaluating the Robustness of Ranking-Based Federated Learning",
        "author": "Zirui Gong, Yanjun Zhang, Leo Yu Zhang, Zhaoxi Zhang, Yong Xiang, and Shirui Pan",
        "link": "http://arxiv.org/abs/2503.08976v1",
        "abstract": "Federated Ranking Learning (FRL) is a state-of-the-art FL framework that\nstands out for its communication efficiency and resilience to poisoning\nattacks. It diverges from the traditional FL framework in two ways: 1) it\nleverages discrete rankings instead of gradient updates, significantly reducing\ncommunication costs and limiting the potential space for malicious updates, and\n2) it uses majority voting on the server side to establish the global ranking,\nensuring that individual updates have minimal influence since each client\ncontributes only a single vote. These features enhance the system's scalability\nand position FRL as a promising paradigm for FL training.\n  However, our analysis reveals that FRL is not inherently robust, as certain\nedges are particularly vulnerable to poisoning attacks. Through a theoretical\ninvestigation, we prove the existence of these vulnerable edges and establish a\nlower bound and an upper bound for identifying them in each layer. Based on\nthis finding, we introduce a novel local model poisoning attack against FRL,\nnamely the Vulnerable Edge Manipulation (VEM) attack. The VEM attack focuses on\nidentifying and perturbing the most vulnerable edges in each layer and\nleveraging an optimization-based approach to maximize the attack's impact.\nThrough extensive experiments on benchmark datasets, we demonstrate that our\nattack achieves an overall 53.23% attack impact and is 3.7x more impactful than\nexisting methods. Our findings highlight significant vulnerabilities in\nranking-based FL systems and underline the urgency for the development of new\nrobust FL frameworks."
    },
    {
        "date": "2025-03",
        "title": "Quantitative Analysis of Deeply Quantized Tiny Neural Networks Robust to Adversarial Attacks",
        "author": "Idris Zakariyya, Ferheen Ayaz, Mounia Kharbouche-Harrari, Jeremy Singer, Sye Loong Keoh, Danilo Pau, and Jos\u00e9 Cano",
        "link": "http://arxiv.org/abs/2503.08973v1",
        "abstract": "Reducing the memory footprint of Machine Learning (ML) models, especially\nDeep Neural Networks (DNNs), is imperative to facilitate their deployment on\nresource-constrained edge devices. However, a notable drawback of DNN models\nlies in their susceptibility to adversarial attacks, wherein minor input\nperturbations can deceive them. A primary challenge revolves around the\ndevelopment of accurate, resilient, and compact DNN models suitable for\ndeployment on resource-constrained edge devices. This paper presents the\noutcomes of a compact DNN model that exhibits resilience against both black-box\nand white-box adversarial attacks. This work has achieved this resilience\nthrough training with the QKeras quantization-aware training framework. The\nstudy explores the potential of QKeras and an adversarial robustness technique,\nJacobian Regularization (JR), to co-optimize the DNN architecture through\nper-layer JR methodology. As a result, this paper has devised a DNN model\nemploying this co-optimization strategy based on Stochastic Ternary\nQuantization (STQ). Its performance was compared against existing DNN models in\nthe face of various white-box and black-box attacks. The experimental findings\nrevealed that, the proposed DNN model had small footprint and on average, it\nexhibited better performance than Quanos and DS-CNN MLCommons/TinyML (MLC/T)\nbenchmarks when challenged with white-box and black-box attacks, respectively,\non the CIFAR-10 image and Google Speech Commands audio datasets."
    },
    {
        "date": "2025-03",
        "title": "Leaky Batteries: A Novel Set of Side-Channel Attacks on Electric Vehicles",
        "author": "Francesco Marchiori, and Mauro Conti",
        "link": "http://arxiv.org/abs/2503.08956v1",
        "abstract": "Advancements in battery technology have accelerated the adoption of Electric\nVehicles (EVs) due to their environmental benefits. However, their growing\nsophistication introduces security and privacy challenges. Often seen as mere\noperational data, battery consumption patterns can unintentionally reveal\ncritical information exploitable for malicious purposes. These risks go beyond\nprivacy, impacting vehicle security and regulatory compliance. Despite these\nconcerns, current research has largely overlooked the broader implications of\nbattery consumption data exposure. As EVs integrate further into smart\ntransportation networks, addressing these gaps is crucial to ensure their\nsafety, reliability, and resilience. In this work, we introduce a novel class\nof side-channel attacks that exploit EV battery data to extract sensitive user\ninformation. Leveraging only battery consumption patterns, we demonstrate a\nmethodology to accurately identify the EV driver and their driving style,\ndetermine the number of occupants, and infer the vehicle's start and end\nlocations when user habits are known. We utilize several machine learning\nmodels and feature extraction techniques to analyze EV power consumption\npatterns, validating our approach on simulated and real-world datasets\ncollected from actual drivers. Our attacks achieve an average success rate of\n95.4% across all attack objectives. Our findings highlight the privacy risks\nassociated with EV battery data, emphasizing the need for stronger protections\nto safeguard user privacy and vehicle security."
    },
    {
        "date": "2025-03",
        "title": "Robust Unsupervised Fault Diagnosis For High-Dimensional Nonlinear Noisy Data",
        "author": "Dandan Zhao, Hongpeng Yin, Jintang Bian, and Han Zhou",
        "link": "http://arxiv.org/abs/2503.08916v1",
        "abstract": "Traditional fault diagnosis methods struggle to handle fault data, with\ncomplex data characteristics such as high dimensions and large noise. Deep\nlearning is a promising solution, which typically works well only when labeled\nfault data are available. To address these problems, a robust unsupervised\nfault diagnosis using machine learning is proposed in this paper. First, a\nspecial dimension reduction method for the high-dimensional fault data is\ndesigned. Second, the extracted features are enhanced by incorporating\nnonlinear information through the learning of a graph structure. Third, to\nalleviate the problem of reduced fault-diagnosis accuracy attributed to noise\nand outliers, $l_{2,1}$-norm and typicality-aware constraints are introduced\nfrom the perspective of model optimization, respectively. Finally, this paper\nprovides comprehensive theoretical and experimental evidence supporting the\neffectiveness and robustness of the proposed method. The experiments on both\nthe benchmark Tennessee-Eastman process and a real hot-steel milling process\nshow that the proposed method exhibits better robustness compared to other\nmethods, maintaining high diagnostic accuracy even in the presence of outliers\nor noise."
    },
    {
        "date": "2025-03",
        "title": "A Deep Bayesian Nonparametric Framework for Robust Mutual Information Estimation",
        "author": "Forough Fazeliasl, Michael Minyi Zhang, Bei Jiang, and Linglong Kong",
        "link": "http://arxiv.org/abs/2503.08902v1",
        "abstract": "Mutual Information (MI) is a crucial measure for capturing dependencies\nbetween variables, but exact computation is challenging in high dimensions with\nintractable likelihoods, impacting accuracy and robustness. One idea is to use\nan auxiliary neural network to train an MI estimator; however, methods based on\nthe empirical distribution function (EDF) can introduce sharp fluctuations in\nthe MI loss due to poor out-of-sample performance, destabilizing convergence.\nWe present a Bayesian nonparametric (BNP) solution for training an MI estimator\nby constructing the MI loss with a finite representation of the Dirichlet\nprocess posterior to incorporate regularization in the training process. With\nthis regularization, the MI loss integrates both prior knowledge and empirical\ndata to reduce the loss sensitivity to fluctuations and outliers in the sample\ndata, especially in small sample settings like mini-batches. This approach\naddresses the challenge of balancing accuracy and low variance by effectively\nreducing variance, leading to stabilized and robust MI loss gradients during\ntraining and enhancing the convergence of the MI approximation while offering\nstronger theoretical guarantees for convergence. We explore the application of\nour estimator in maximizing MI between the data space and the latent space of a\nvariational autoencoder. Experimental results demonstrate significant\nimprovements in convergence over EDF-based methods, with applications across\nsynthetic and real datasets, notably in 3D CT image generation, yielding\nenhanced structure discovery and reduced overfitting in data synthesis. While\nthis paper focuses on generative models in application, the proposed estimator\nis not restricted to this setting and can be applied more broadly in various\nBNP learning procedures."
    },
    {
        "date": "2025-03",
        "title": "Seal Your Backdoor with Variational Defense",
        "author": "Ivan Saboli\u0107, Matej Grci\u0107, and Sini\u0161a \u0160egvi\u0107",
        "link": "http://arxiv.org/abs/2503.08829v1",
        "abstract": "We propose VIBE, a model-agnostic framework that trains classifiers resilient\nto backdoor attacks. The key concept behind our approach is to treat malicious\ninputs and corrupted labels from the training dataset as observed random\nvariables, while the actual clean labels are latent. VIBE then recovers the\ncorresponding latent clean label posterior through variational inference. The\nresulting training procedure follows the expectation-maximization (EM)\nalgorithm. The E-step infers the clean pseudolabels by solving an\nentropy-regularized optimal transport problem, while the M-step updates the\nclassifier parameters via gradient descent. Being modular, VIBE can seamlessly\nintegrate with recent advancements in self-supervised representation learning,\nwhich enhance its ability to resist backdoor attacks. We experimentally\nvalidate the method effectiveness against contemporary backdoor attacks on\nstandard datasets, a large-scale setup with 1$k$ classes, and a dataset\npoisoned with multiple attacks. VIBE consistently outperforms previous defenses\nacross all tested scenarios."
    },
    {
        "date": "2025-03",
        "title": "Robust Multi-Objective Controlled Decoding of Large Language Models",
        "author": "Seongho Son, William Bankes, Sangwoong Yoon, Shyam Sundhar Ramesh, Xiaohang Tang, and Ilija Bogunovic",
        "link": "http://arxiv.org/abs/2503.08796v1",
        "abstract": "Test-time alignment of Large Language Models (LLMs) to human preferences\noffers a flexible way to generate responses aligned to diverse objectives\nwithout extensive retraining of LLMs. Existing methods achieve alignment to\nmultiple objectives simultaneously (e.g., instruction-following, helpfulness,\nconciseness) by optimizing their corresponding reward functions. However, they\noften rely on predefined weights or optimize for averages, sacrificing one\nobjective for another and leading to unbalanced outcomes. To address this, we\nintroduce Robust Multi-Objective Decoding (RMOD), a novel inference-time\nalgorithm that optimizes for improving worst-case rewards. RMOD formalizes the\nrobust decoding problem as a maximin two-player game between reward weights and\nthe sampling policy, solving for the Nash equilibrium. We show that the game\nreduces to a convex optimization problem to find the worst-case weights, while\nthe best response policy can be computed analytically. We also introduce a\npractical RMOD variant designed for efficient decoding with contemporary LLMs,\nincurring minimal computational overhead compared to non-robust Multi-Objective\nDecoding (MOD) methods. Our experimental results showcase the effectiveness of\nRMOD in generating responses equitably aligned with diverse objectives,\noutperforming baselines up to 20%."
    },
    {
        "date": "2025-03",
        "title": "Birds look like cars: Adversarial analysis of intrinsically interpretable deep learning",
        "author": "Hubert Baniecki, and Przemyslaw Biecek",
        "link": "http://arxiv.org/abs/2503.08636v1",
        "abstract": "A common belief is that intrinsically interpretable deep learning models\nensure a correct, intuitive understanding of their behavior and offer greater\nrobustness against accidental errors or intentional manipulation. However,\nthese beliefs have not been comprehensively verified, and growing evidence\ncasts doubt on them. In this paper, we highlight the risks related to\noverreliance and susceptibility to adversarial manipulation of these so-called\n\"intrinsically (aka inherently) interpretable\" models by design. We introduce\ntwo strategies for adversarial analysis with prototype manipulation and\nbackdoor attacks against prototype-based networks, and discuss how concept\nbottleneck models defend against these attacks. Fooling the model's reasoning\nby exploiting its use of latent prototypes manifests the inherent\nuninterpretability of deep neural networks, leading to a false sense of\nsecurity reinforced by a visual confirmation bias. The reported limitations of\nprototype-based networks put their trustworthiness and applicability into\nquestion, motivating further work on the robustness and alignment of (deep)\ninterpretable models."
    },
    {
        "date": "2025-03",
        "title": "Soft Actor-Critic-based Control Barrier Adaptation for Robust Autonomous Navigation in Unknown Environments",
        "author": "Nicholas Mohammad, and Nicola Bezzo",
        "link": "http://arxiv.org/abs/2503.08479v1",
        "abstract": "Motion planning failures during autonomous navigation often occur when safety\nconstraints are either too conservative, leading to deadlocks, or too liberal,\nresulting in collisions. To improve robustness, a robot must dynamically adapt\nits safety constraints to ensure it reaches its goal while balancing safety and\nperformance measures. To this end, we propose a Soft Actor-Critic (SAC)-based\npolicy for adapting Control Barrier Function (CBF) constraint parameters at\nruntime, ensuring safe yet non-conservative motion. The proposed approach is\ndesigned for a general high-level motion planner, low-level controller, and\ntarget system model, and is trained in simulation only. Through extensive\nsimulations and physical experiments, we demonstrate that our framework\neffectively adapts CBF constraints, enabling the robot to reach its final goal\nwithout compromising safety."
    },
    {
        "date": "2025-03",
        "title": "Robust Latent Matters: Boosting Image Generation with Sampling Error",
        "author": "Kai Qiu, Xiang Li, Jason Kuen, Hao Chen, Xiaohao Xu, Jiuxiang Gu, Yinyi Luo, Bhiksha Raj, Zhe Lin, and Marios Savvides",
        "link": "http://arxiv.org/abs/2503.08354v1",
        "abstract": "Recent image generation schemes typically capture image distribution in a\npre-constructed latent space relying on a frozen image tokenizer. Though the\nperformance of tokenizer plays an essential role to the successful generation,\nits current evaluation metrics (e.g. rFID) fail to precisely assess the\ntokenizer and correlate its performance to the generation quality (e.g. gFID).\nIn this paper, we comprehensively analyze the reason for the discrepancy of\nreconstruction and generation qualities in a discrete latent space, and, from\nwhich, we propose a novel plug-and-play tokenizer training scheme to facilitate\nlatent space construction. Specifically, a latent perturbation approach is\nproposed to simulate sampling noises, i.e., the unexpected tokens sampled, from\nthe generative process. With the latent perturbation, we further propose (1) a\nnovel tokenizer evaluation metric, i.e., pFID, which successfully correlates\nthe tokenizer performance to generation quality and (2) a plug-and-play\ntokenizer training scheme, which significantly enhances the robustness of\ntokenizer thus boosting the generation quality and convergence speed. Extensive\nbenchmarking are conducted with 11 advanced discrete image tokenizers with 2\nautoregressive generation models to validate our approach. The tokenizer\ntrained with our proposed latent perturbation achieve a notable 1.60 gFID with\nclassifier-free guidance (CFG) and 3.45 gFID without CFG with a $\\sim$400M\ngenerator. Code: https://github.com/lxa9867/ImageFolder."
    },
    {
        "date": "2025-03",
        "title": "Adv-CPG: A Customized Portrait Generation Framework with Facial Adversarial Attacks",
        "author": "Junying Wang, Hongyuan Zhang, and Yuan Yuan",
        "link": "http://arxiv.org/abs/2503.08269v1",
        "abstract": "Recent Customized Portrait Generation (CPG) methods, taking a facial image\nand a textual prompt as inputs, have attracted substantial attention. Although\nthese methods generate high-fidelity portraits, they fail to prevent the\ngenerated portraits from being tracked and misused by malicious face\nrecognition systems. To address this, this paper proposes a Customized Portrait\nGeneration framework with facial Adversarial attacks (Adv-CPG). Specifically,\nto achieve facial privacy protection, we devise a lightweight local ID\nencryptor and an encryption enhancer. They implement progressive double-layer\nencryption protection by directly injecting the target identity and adding\nadditional identity guidance, respectively. Furthermore, to accomplish\nfine-grained and personalized portrait generation, we develop a multi-modal\nimage customizer capable of generating controlled fine-grained facial features.\nTo the best of our knowledge, Adv-CPG is the first study that introduces facial\nadversarial attacks into CPG. Extensive experiments demonstrate the superiority\nof Adv-CPG, e.g., the average attack success rate of the proposed Adv-CPG is\n28.1% and 2.86% higher compared to the SOTA noise-based attack methods and\nunconstrained attack methods, respectively."
    },
    {
        "date": "2025-03",
        "title": "SARA: Structural and Adversarial Representation Alignment for Training-efficient Diffusion Models",
        "author": "Hesen Chen, Junyan Wang, Zhiyu Tan, and Hao Li",
        "link": "http://arxiv.org/abs/2503.08253v1",
        "abstract": "Modern diffusion models encounter a fundamental trade-off between training\nefficiency and generation quality. While existing representation alignment\nmethods, such as REPA, accelerate convergence through patch-wise alignment,\nthey often fail to capture structural relationships within visual\nrepresentations and ensure global distribution consistency between pretrained\nencoders and denoising networks. To address these limitations, we introduce\nSARA, a hierarchical alignment framework that enforces multi-level\nrepresentation constraints: (1) patch-wise alignment to preserve local semantic\ndetails, (2) autocorrelation matrix alignment to maintain structural\nconsistency within representations, and (3) adversarial distribution alignment\nto mitigate global representation discrepancies. Unlike previous approaches,\nSARA explicitly models both intra-representation correlations via\nself-similarity matrices and inter-distribution coherence via adversarial\nalignment, enabling comprehensive alignment across local and global scales.\nExperiments on ImageNet-256 show that SARA achieves an FID of 1.36 while\nconverging twice as fast as REPA, surpassing recent state-of-the-art image\ngeneration methods. This work establishes a systematic paradigm for optimizing\ndiffusion training through hierarchical representation alignment."
    },
    {
        "date": "2025-03",
        "title": "A Grey-box Text Attack Framework using Explainable AI",
        "author": "Esther Chiramal, and Kelvin Soh Boon Kai",
        "link": "http://arxiv.org/abs/2503.08226v1",
        "abstract": "Explainable AI is a strong strategy implemented to understand complex\nblack-box model predictions in a human interpretable language. It provides the\nevidence required to execute the use of trustworthy and reliable AI systems. On\nthe other hand, however, it also opens the door to locating possible\nvulnerabilities in an AI model. Traditional adversarial text attack uses word\nsubstitution, data augmentation techniques and gradient-based attacks on\npowerful pre-trained Bidirectional Encoder Representations from Transformers\n(BERT) variants to generate adversarial sentences. These attacks are generally\nwhitebox in nature and not practical as they can be easily detected by humans\nE.g. Changing the word from \"Poor\" to \"Rich\". We proposed a simple yet\neffective Grey-box cum Black-box approach that does not require the knowledge\nof the model while using a set of surrogate Transformer/BERT models to perform\nthe attack using Explainable AI techniques. As Transformers are the current\nstate-of-the-art models for almost all Natural Language Processing (NLP) tasks,\nan attack generated from BERT1 is transferable to BERT2. This transferability\nis made possible due to the attention mechanism in the transformer that allows\nthe model to capture long-range dependencies in a sequence. Using the power of\nBERT generalisation via attention, we attempt to exploit how transformers learn\nby attacking a few surrogate transformer variants which are all based on a\ndifferent architecture. We demonstrate that this approach is highly effective\nto generate semantically good sentences by changing as little as one word that\nis not detectable by humans while still fooling other BERT models."
    },
    {
        "date": "2025-03",
        "title": "MVGSR: Multi-View Consistency Gaussian Splatting for Robust Surface Reconstruction",
        "author": "Chenfeng Hou, Qi Xun Yeo, Mengqi Guo, Yongxin Su, Yanyan Li, and Gim Hee Lee",
        "link": "http://arxiv.org/abs/2503.08093v2",
        "abstract": "3D Gaussian Splatting (3DGS) has gained significant attention for its\nhigh-quality rendering capabilities, ultra-fast training, and inference speeds.\nHowever, when we apply 3DGS to surface reconstruction tasks, especially in\nenvironments with dynamic objects and distractors, the method suffers from\nfloating artifacts and color errors due to inconsistency from different\nviewpoints. To address this challenge, we propose Multi-View Consistency\nGaussian Splatting for the domain of Robust Surface Reconstruction\n(\\textbf{MVGSR}), which takes advantage of lightweight Gaussian models and a\n{heuristics-guided distractor masking} strategy for robust surface\nreconstruction in non-static environments. Compared to existing methods that\nrely on MLPs for distractor segmentation strategies, our approach separates\ndistractors from static scene elements by comparing multi-view feature\nconsistency, allowing us to obtain precise distractor masks early in training.\nFurthermore, we introduce a pruning measure based on multi-view contributions\nto reset transmittance, effectively reducing floating artifacts. Finally, a\nmulti-view consistency loss is applied to achieve high-quality performance in\nsurface reconstruction tasks. Experimental results demonstrate that MVGSR\nachieves competitive geometric accuracy and rendering fidelity compared to the\nstate-of-the-art surface reconstruction algorithms. More information is\navailable on our project page (https://mvgsr.github.io)."
    },
    {
        "date": "2025-03",
        "title": "\"We just did not have that on the embedded system\": Insights and Challenges for Securing Microcontroller Systems from the Embedded CTF Competitions",
        "author": "Zheyuan Ma, Gaoxiang Liu, Alex Eastman, Kai Kaufman, Md Armanuzzaman, Xi Tan, Katherine Jesse, Robert Walls, and Ziming Zhao",
        "link": "http://arxiv.org/abs/2503.08053v1",
        "abstract": "Microcontroller systems are integral to our daily lives, powering\nmission-critical applications such as vehicles, medical devices, and industrial\ncontrol systems. Therefore, it is essential to investigate and outline the\nchallenges encountered in developing secure microcontroller systems. While\nprevious research has focused solely on microcontroller firmware analysis to\nidentify and characterize vulnerabilities, our study uniquely leverages data\nfrom the 2023 and 2024 MITRE eCTF team submissions and post-competition\ninterviews. This approach allows us to dissect the entire lifecycle of secure\nmicrocontroller system development from both technical and perceptual\nperspectives, providing deeper insights into how these vulnerabilities emerge\nin the first place.\n  Through the lens of eCTF, we identify fundamental conceptual and practical\nchallenges in securing microcontroller systems. Conceptually, it is difficult\nto adapt from a microprocessor system to a microcontroller system, and\nparticipants are not wholly aware of the unique attacks against\nmicrocontrollers. Practically, security-enhancing tools, such as the\nmemory-safe language Rust, lack adequate support on microcontrollers.\nAdditionally, poor-quality entropy sources weaken cryptography and secret\ngeneration. Additionally, our findings articulate specific research,\ndevelopmental, and educational deficiencies, leading to targeted\nrecommendations for researchers, developers, vendors, and educators to enhance\nthe security of microcontroller systems."
    },
    {
        "date": "2025-03",
        "title": "Detecting Backdoor Attacks in Federated Learning via Direction Alignment Inspection",
        "author": "Jiahao Xu, Zikai Zhang, and Rui Hu",
        "link": "http://arxiv.org/abs/2503.07978v1",
        "abstract": "The distributed nature of training makes Federated Learning (FL) vulnerable\nto backdoor attacks, where malicious model updates aim to compromise the global\nmodel's performance on specific tasks. Existing defense methods show limited\nefficacy as they overlook the inconsistency between benign and malicious model\nupdates regarding both general and fine-grained directions. To fill this gap,\nwe introduce AlignIns, a novel defense method designed to safeguard FL systems\nagainst backdoor attacks. AlignIns looks into the direction of each model\nupdate through a direction alignment inspection process. Specifically, it\nexamines the alignment of model updates with the overall update direction and\nanalyzes the distribution of the signs of their significant parameters,\ncomparing them with the principle sign across all model updates. Model updates\nthat exhibit an unusual degree of alignment are considered malicious and thus\nbe filtered out. We provide the theoretical analysis of the robustness of\nAlignIns and its propagation error in FL. Our empirical results on both\nindependent and identically distributed (IID) and non-IID datasets demonstrate\nthat AlignIns achieves higher robustness compared to the state-of-the-art\ndefense methods. The code is available at\nhttps://github.com/JiiahaoXU/AlignIns."
    },
    {
        "date": "2025-03",
        "title": "FairDeFace: Evaluating the Fairness and Adversarial Robustness of Face Obfuscation Methods",
        "author": "Seyyed Mohammad Sadegh Moosavi Khorzooghi, Poojitha Thota, Mohit Singhal, Abolfazl Asudeh, Gautam Das, and Shirin Nilizadeh",
        "link": "http://arxiv.org/abs/2503.08731v1",
        "abstract": "The lack of a common platform and benchmark datasets for evaluating face\nobfuscation methods has been a challenge, with every method being tested using\narbitrary experiments, datasets, and metrics. While prior work has demonstrated\nthat face recognition systems exhibit bias against some demographic groups,\nthere exists a substantial gap in our understanding regarding the fairness of\nface obfuscation methods. Providing fair face obfuscation methods can ensure\nequitable protection across diverse demographic groups, especially since they\ncan be used to preserve the privacy of vulnerable populations. To address these\ngaps, this paper introduces a comprehensive framework, named FairDeFace,\ndesigned to assess the adversarial robustness and fairness of face obfuscation\nmethods. The framework introduces a set of modules encompassing data\nbenchmarks, face detection and recognition algorithms, adversarial models,\nutility detection models, and fairness metrics. FairDeFace serves as a\nversatile platform where any face obfuscation method can be integrated,\nallowing for rigorous testing and comparison with other state-of-the-art\nmethods. In its current implementation, FairDeFace incorporates 6 attacks, and\nseveral privacy, utility and fairness metrics. Using FairDeFace, and by\nconducting more than 500 experiments, we evaluated and compared the adversarial\nrobustness of seven face obfuscation methods. This extensive analysis led to\nmany interesting findings both in terms of the degree of robustness of existing\nmethods and their biases against some gender or racial groups. FairDeFace also\nuses visualization of focused areas for both obfuscation and verification\nattacks to show not only which areas are mostly changed in the obfuscation\nprocess for some demographics, but also why they failed through focus area\ncomparison of obfuscation and verification."
    },
    {
        "date": "2025-03",
        "title": "Certainly Bot Or Not? Trustworthy Social Bot Detection via Robust Multi-Modal Neural Processes",
        "author": "Qi Wu, Yingguang Yang, hao liu, Hao Peng, Buyun He, Yutong Xia, and Yong Liao",
        "link": "http://arxiv.org/abs/2503.09626v1",
        "abstract": "Social bot detection is crucial for mitigating misinformation, online\nmanipulation, and coordinated inauthentic behavior. While existing neural\nnetwork-based detectors perform well on benchmarks, they struggle with\ngeneralization due to distribution shifts across datasets and frequently\nproduce overconfident predictions for out-of-distribution accounts beyond the\ntraining data. To address this, we introduce a novel Uncertainty Estimation for\nSocial Bot Detection (UESBD) framework, which quantifies the predictive\nuncertainty of detectors beyond mere classification. For this task, we propose\nRobust Multi-modal Neural Processes (RMNP), which aims to enhance the\nrobustness of multi-modal neural processes to modality inconsistencies caused\nby social bot camouflage. RMNP first learns unimodal representations through\nmodality-specific encoders. Then, unimodal attentive neural processes are\nemployed to encode the Gaussian distribution of unimodal latent variables.\nFurthermore, to avoid social bots stealing human features to camouflage\nthemselves thus causing certain modalities to provide conflictive information,\nwe introduce an evidential gating network to explicitly model the reliability\nof modalities. The joint latent distribution is learned through the generalized\nproduct of experts, which takes the reliability of each modality into\nconsideration during fusion. The final prediction is obtained through Monte\nCarlo sampling of the joint latent distribution followed by a decoder.\nExperiments on three real-world benchmarks show the effectiveness of RMNP in\nclassification and uncertainty estimation, as well as its robustness to\nmodality conflicts."
    },
    {
        "date": "2025-03",
        "title": "ReLATE: Resilient Learner Selection for Multivariate Time-Series Classification Against Adversarial Attacks",
        "author": "Cagla Ipek Kocal, Onat Gungor, Aaron Tartz, Tajana Rosing, and Baris Aksanli",
        "link": "http://arxiv.org/abs/2503.07882v1",
        "abstract": "Minimizing computational overhead in time-series classification, particularly\nin deep learning models, presents a significant challenge. This challenge is\nfurther compounded by adversarial attacks, emphasizing the need for resilient\nmethods that ensure robust performance and efficient model selection. We\nintroduce ReLATE, a framework that identifies robust learners based on dataset\nsimilarity, reduces computational overhead, and enhances resilience. ReLATE\nmaintains multiple deep learning models in well-known adversarial attack\nscenarios, capturing model performance. ReLATE identifies the most analogous\ndataset to a given target using a similarity metric, then applies the optimal\nmodel from the most similar dataset. ReLATE reduces computational overhead by\nan average of 81.2%, enhancing adversarial resilience and streamlining robust\nmodel selection, all without sacrificing performance, within 4.2% of Oracle."
    },
    {
        "date": "2025-03",
        "title": "Efficient Resource Management for Secure and Low-Latency O-RAN Communication",
        "author": "Zaineh Abughazzah, Emna Baccour, Ahmed Refaey, Amr Mohamed, and Mounir Hamdi",
        "link": "http://arxiv.org/abs/2503.07857v1",
        "abstract": "Open Radio Access Networks (O-RAN) are transforming telecommunications by\nshifting from centralized to distributed architectures, promoting flexibility,\ninteroperability, and innovation through open interfaces and multi-vendor\nenvironments. However, O-RAN's reliance on cloud-based architecture and\nenhanced observability introduces significant security and resource management\nchallenges. Efficient resource management is crucial for secure and reliable\ncommunication in O-RAN, within the resource-constrained environment and\nheterogeneity of requirements, where multiple User Equipment (UE) and O-RAN\nRadio Units (O-RUs) coexist. This paper develops a framework to manage these\naspects, ensuring each O-RU is associated with UEs based on their communication\nchannel qualities and computational resources, and selecting appropriate\nencryption algorithms to safeguard data confidentiality, integrity, and\nauthentication. A Multi-objective Optimization Problem (MOP) is formulated to\nminimize latency and maximize security within resource constraints. Different\napproaches are proposed to relax the complexity of the problem and achieve\nnear-optimal performance, facilitating trade-offs between latency, security,\nand solution complexity. Simulation results demonstrate that the proposed\napproaches are close enough to the optimal solution, proving that our approach\nis both effective and efficient."
    },
    {
        "date": "2025-03",
        "title": "Helios 2.0: A Robust, Ultra-Low Power Gesture Recognition System Optimised for Event-Sensor based Wearables",
        "author": "Prarthana Bhattacharyya, Joshua Mitton, Ryan Page, Owen Morgan, Oliver Powell, Benjamin Menzies, Gabriel Homewood, Kemi Jacobs, Paolo Baesso, Taru Muhonen, Richard Vigars, and Louis Berridge",
        "link": "http://arxiv.org/abs/2503.07825v1",
        "abstract": "We present an advance in wearable technology: a mobile-optimized, real-time,\nultra-low-power event camera system that enables natural hand gesture control\nfor smart glasses, dramatically improving user experience. While hand gesture\nrecognition in computer vision has advanced significantly, critical challenges\nremain in creating systems that are intuitive, adaptable across diverse users\nand environments, and energy-efficient enough for practical wearable\napplications. Our approach tackles these challenges through carefully selected\nmicrogestures: lateral thumb swipes across the index finger (in both\ndirections) and a double pinch between thumb and index fingertips. These\nhuman-centered interactions leverage natural hand movements, ensuring intuitive\nusability without requiring users to learn complex command sequences. To\novercome variability in users and environments, we developed a novel simulation\nmethodology that enables comprehensive domain sampling without extensive\nreal-world data collection. Our power-optimised architecture maintains\nexceptional performance, achieving F1 scores above 80\\% on benchmark datasets\nfeaturing diverse users and environments. The resulting models operate at just\n6-8 mW when exploiting the Qualcomm Snapdragon Hexagon DSP, with our 2-channel\nimplementation exceeding 70\\% F1 accuracy and our 6-channel model surpassing\n80\\% F1 accuracy across all gesture classes in user studies. These results were\nachieved using only synthetic training data. This improves on the\nstate-of-the-art for F1 accuracy by 20\\% with a power reduction 25x when using\nDSP. This advancement brings deploying ultra-low-power vision systems in\nwearable devices closer and opens new possibilities for seamless human-computer\ninteraction."
    },
    {
        "date": "2025-03",
        "title": "Strengthening the Internal Adversarial Robustness in Lifted Neural Networks",
        "author": "Christopher Zach",
        "link": "http://arxiv.org/abs/2503.07818v1",
        "abstract": "Lifted neural networks (i.e. neural architectures explicitly optimizing over\nrespective network potentials to determine the neural activities) can be\ncombined with a type of adversarial training to gain robustness for internal as\nwell as input layers, in addition to improved generalization performance. In\nthis work we first investigate how adversarial robustness in this framework can\nbe further strengthened by solely modifying the training loss. In a second step\nwe fix some remaining limitations and arrive at a novel training loss for\nlifted neural networks, that combines targeted and untargeted adversarial\nperturbations."
    },
    {
        "date": "2025-03",
        "title": "On the Semantic Security of NTRU -- with a gentle introduction to cryptography",
        "author": "Liam Peet-Pare",
        "link": "http://arxiv.org/abs/2503.07790v1",
        "abstract": "This paper provides an explanation of NTRU, a post quantum encryption scheme,\nwhile also providing a gentle introduction to cryptography. NTRU is a very\nefficient lattice based cryptosystem that appears to be safe against attacks by\nquantum computers. NTRU's efficiency suggests that it is a strong candidate as\nan alternative to RSA, ElGamal, and ECC for the post quantum world. The paper\nbegins with an introduction to cryptography and security proofs for\ncryptographic schemes before explaining the NTRU cryptosystem and culminating\nwith a proof that the original presentation of NTRU is not IND-CPA secure. We\nwill conclude by mentioning padding schemes to NTRU that are provably IND-CCA2\nsecure in the random oracle model. The paper is designed to be accessible to\nanyone with minimal background in abstract algebra and number theory - no\nprevious knowledge of cryptography is assumed. Given the author's lack of\nfamiliarity with the subject, this paper aims to be an expository work rather\nthan to provide new insights to the subject matter."
    },
    {
        "date": "2025-03",
        "title": "Better Pose Initialization for Fast and Robust 2D/3D Pelvis Registration",
        "author": "Yehyun Suh, J. Ryan Martin, and Daniel Moyer",
        "link": "http://arxiv.org/abs/2503.07767v1",
        "abstract": "This paper presents an approach for improving 2D/3D pelvis registration in\noptimization-based pose estimators using a learned initialization function.\nCurrent methods often fail to converge to the optimal solution when initialized\nnaively. We find that even a coarse initializer greatly improves pose estimator\naccuracy, and improves overall computational efficiency. This approach proves\nto be effective also in challenging cases under more extreme pose variation.\nExperimental validation demonstrates that our method consistently achieves\nrobust and accurate registration, enhancing the reliability of 2D/3D\nregistration for clinical applications."
    },
    {
        "date": "2025-03",
        "title": "SANDRO: a Robust Solver with a Splitting Strategy for Point Cloud Registration",
        "author": "Michael Adlerstein, Jo\u00e3o Carlos Virgolino Soares, Angelo Bratta, and Claudio Semini",
        "link": "http://arxiv.org/abs/2503.07743v1",
        "abstract": "Point cloud registration is a critical problem in computer vision and\nrobotics, especially in the field of navigation. Current methods often fail\nwhen faced with high outlier rates or take a long time to converge to a\nsuitable solution. In this work, we introduce a novel algorithm for point cloud\nregistration called SANDRO (Splitting strategy for point cloud Alignment using\nNon-convex anD Robust Optimization), which combines an Iteratively Reweighted\nLeast Squares (IRLS) framework with a robust loss function with graduated\nnon-convexity. This approach is further enhanced by a splitting strategy\ndesigned to handle high outlier rates and skewed distributions of outliers.\nSANDRO is capable of addressing important limitations of existing methods, as\nin challenging scenarios where the presence of high outlier rates and point\ncloud symmetries significantly hinder convergence. SANDRO achieves superior\nperformance in terms of success rate when compared to the state-of-the-art\nmethods, demonstrating a 20% improvement from the current state of the art when\ntested on the Redwood real dataset and 60% improvement when tested on synthetic\ndata."
    },
    {
        "date": "2025-03",
        "title": "Runtime Detection of Adversarial Attacks in AI Accelerators Using Performance Counters",
        "author": "Habibur Rahaman, Atri Chatterjee, and Swarup Bhunia",
        "link": "http://arxiv.org/abs/2503.07568v1",
        "abstract": "Rapid adoption of AI technologies raises several major security concerns,\nincluding the risks of adversarial perturbations, which threaten the\nconfidentiality and integrity of AI applications. Protecting AI hardware from\nmisuse and diverse security threats is a challenging task. To address this\nchallenge, we propose SAMURAI, a novel framework for safeguarding against\nmalicious usage of AI hardware and its resilience to attacks. SAMURAI\nintroduces an AI Performance Counter (APC) for tracking dynamic behavior of an\nAI model coupled with an on-chip Machine Learning (ML) analysis engine, known\nas TANTO (Trained Anomaly Inspection Through Trace Observation). APC records\nthe runtime profile of the low-level hardware events of different AI\noperations. Subsequently, the summary information recorded by the APC is\nprocessed by TANTO to efficiently identify potential security breaches and\nensure secure, responsible use of AI. SAMURAI enables real-time detection of\nsecurity threats and misuse without relying on traditional software-based\nsolutions that require model integration. Experimental results demonstrate that\nSAMURAI achieves up to 97% accuracy in detecting adversarial attacks with\nmoderate overhead on various AI models, significantly outperforming\nconventional software-based approaches. It enhances security and regulatory\ncompliance, providing a comprehensive solution for safeguarding AI against\nemergent threats."
    },
    {
        "date": "2025-03",
        "title": "PoisonedParrot: Subtle Data Poisoning Attacks to Elicit Copyright-Infringing Content from Large Language Models",
        "author": "Michael-Andrei Panaitescu-Liess, Pankayaraj Pathmanathan, Yigitcan Kaya, Zora Che, Bang An, Sicheng Zhu, Aakriti Agrawal, and Furong Huang",
        "link": "http://arxiv.org/abs/2503.07697v1",
        "abstract": "As the capabilities of large language models (LLMs) continue to expand, their\nusage has become increasingly prevalent. However, as reflected in numerous\nongoing lawsuits regarding LLM-generated content, addressing copyright\ninfringement remains a significant challenge. In this paper, we introduce\nPoisonedParrot: the first stealthy data poisoning attack that induces an LLM to\ngenerate copyrighted content even when the model has not been directly trained\non the specific copyrighted material. PoisonedParrot integrates small fragments\nof copyrighted text into the poison samples using an off-the-shelf LLM. Despite\nits simplicity, evaluated in a wide range of experiments, PoisonedParrot is\nsurprisingly effective at priming the model to generate copyrighted content\nwith no discernible side effects. Moreover, we discover that existing defenses\nare largely ineffective against our attack. Finally, we make the first attempt\nat mitigating copyright-infringement poisoning attacks by proposing a defense:\nParrotTrap. We encourage the community to explore this emerging threat model\nfurther."
    },
    {
        "date": "2025-03",
        "title": "ADROIT: A Self-Supervised Framework for Learning Robust Representations for Active Learning",
        "author": "Soumya Banerjee, and Vinay Kumar Verma",
        "link": "http://arxiv.org/abs/2503.07506v1",
        "abstract": "Active learning aims to select optimal samples for labeling, minimizing\nannotation costs. This paper introduces a unified representation learning\nframework tailored for active learning with task awareness. It integrates\ndiverse sources, comprising reconstruction, adversarial, self-supervised,\nknowledge-distillation, and classification losses into a unified VAE-based\nADROIT approach. The proposed approach comprises three key components - a\nunified representation generator (VAE), a state discriminator, and a (proxy)\ntask-learner or classifier. ADROIT learns a latent code using both labeled and\nunlabeled data, incorporating task-awareness by leveraging labeled data with\nthe proxy classifier. Unlike previous approaches, the proxy classifier\nadditionally employs a self-supervised loss on unlabeled data and utilizes\nknowledge distillation to align with the target task-learner. The state\ndiscriminator distinguishes between labeled and unlabeled data, facilitating\nthe selection of informative unlabeled samples. The dynamic interaction between\nVAE and the state discriminator creates a competitive environment, with the VAE\nattempting to deceive the discriminator, while the state discriminator learns\nto differentiate between labeled and unlabeled inputs. Extensive evaluations on\ndiverse datasets and ablation analysis affirm the effectiveness of the proposed\nmodel."
    },
    {
        "date": "2025-03",
        "title": "From Centralized to Decentralized Federated Learning: Theoretical Insights, Privacy Preservation, and Robustness Challenges",
        "author": "Qiongxiu Li, Wenrui Yu, Yufei Xia, and Jun Pang",
        "link": "http://arxiv.org/abs/2503.07505v1",
        "abstract": "Federated Learning (FL) enables collaborative learning without directly\nsharing individual's raw data. FL can be implemented in either a centralized\n(server-based) or decentralized (peer-to-peer) manner. In this survey, we\npresent a novel perspective: the fundamental difference between centralized FL\n(CFL) and decentralized FL (DFL) is not merely the network topology, but the\nunderlying training protocol: separate aggregation vs. joint optimization. We\nargue that this distinction in protocol leads to significant differences in\nmodel utility, privacy preservation, and robustness to attacks. We\nsystematically review and categorize existing works in both CFL and DFL\naccording to the type of protocol they employ. This taxonomy provides deeper\ninsights into prior research and clarifies how various approaches relate or\ndiffer. Through our analysis, we identify key gaps in the literature. In\nparticular, we observe a surprising lack of exploration of DFL approaches based\non distributed optimization methods, despite their potential advantages. We\nhighlight this under-explored direction and call for more research on\nleveraging distributed optimization for federated learning. Overall, this work\noffers a comprehensive overview from centralized to decentralized FL, sheds new\nlight on the core distinctions between approaches, and outlines open challenges\nand future directions for the field."
    },
    {
        "date": "2025-03",
        "title": "Efficient Membership Inference Attacks by Bayesian Neural Network",
        "author": "Zhenlong Liu, Wenyu Jiang, Feng Zhou, and Hongxin Wei",
        "link": "http://arxiv.org/abs/2503.07482v1",
        "abstract": "Membership Inference Attacks (MIAs) aim to estimate whether a specific data\npoint was used in the training of a given model. Previous attacks often utilize\nmultiple reference models to approximate the conditional score distribution,\nleading to significant computational overhead. While recent work leverages\nquantile regression to estimate conditional thresholds, it fails to capture\nepistemic uncertainty, resulting in bias in low-density regions. In this work,\nwe propose a novel approach - Bayesian Membership Inference Attack (BMIA),\nwhich performs conditional attack through Bayesian inference. In particular, we\ntransform a trained reference model into Bayesian neural networks by Laplace\napproximation, enabling the direct estimation of the conditional score\ndistribution by probabilistic model parameters. Our method addresses both\nepistemic and aleatoric uncertainty with only a reference model, enabling\nefficient and powerful MIA. Extensive experiments on five datasets demonstrate\nthe effectiveness and efficiency of BMIA."
    },
    {
        "date": "2025-03",
        "title": "Probabilistic Segmentation for Robust Field of View Estimation",
        "author": "R. Spencer Hallyburton, David Hunt, Yiwei He, Judy He, and Miroslav Pajic",
        "link": "http://arxiv.org/abs/2503.07375v1",
        "abstract": "Attacks on sensing and perception threaten the safe deployment of autonomous\nvehicles (AVs). Security-aware sensor fusion helps mitigate threats but\nrequires accurate field of view (FOV) estimation which has not been evaluated\nautonomy. To address this gap, we adapt classical computer graphics algorithms\nto develop the first autonomy-relevant FOV estimators and create the first\ndatasets with ground truth FOV labels. Unfortunately, we find that these\napproaches are themselves highly vulnerable to attacks on sensing. To improve\nrobustness of FOV estimation against attacks, we propose a learning-based\nsegmentation model that captures FOV features, integrates Monte Carlo dropout\n(MCD) for uncertainty quantification, and performs anomaly detection on\nconfidence maps. We illustrate through comprehensive evaluations attack\nresistance and strong generalization across environments. Architecture trade\nstudies demonstrate the model is feasible for real-time deployment in multiple\napplications."
    },
    {
        "date": "2025-03",
        "title": "Group-robust Sample Reweighting for Subpopulation Shifts via Influence Functions",
        "author": "Rui Qiao, Zhaoxuan Wu, Jingtan Wang, Pang Wei Koh, and Bryan Kian Hsiang Low",
        "link": "http://arxiv.org/abs/2503.07315v1",
        "abstract": "Machine learning models often have uneven performance among subpopulations\n(a.k.a., groups) in the data distributions. This poses a significant challenge\nfor the models to generalize when the proportions of the groups shift during\ndeployment. To improve robustness to such shifts, existing approaches have\ndeveloped strategies that train models or perform hyperparameter tuning using\nthe group-labeled data to minimize the worst-case loss over groups. However, a\nnon-trivial amount of high-quality labels is often required to obtain\nnoticeable improvements. Given the costliness of the labels, we propose to\nadopt a different paradigm to enhance group label efficiency: utilizing the\ngroup-labeled data as a target set to optimize the weights of other\ngroup-unlabeled data. We introduce Group-robust Sample Reweighting (GSR), a\ntwo-stage approach that first learns the representations from group-unlabeled\ndata, and then tinkers the model by iteratively retraining its last layer on\nthe reweighted data using influence functions. Our GSR is theoretically sound,\npractically lightweight, and effective in improving the robustness to\nsubpopulation shifts. In particular, GSR outperforms the previous\nstate-of-the-art approaches that require the same amount or even more group\nlabels."
    },
    {
        "date": "2025-03",
        "title": "All That Glitters Is Not Gold: Key-Secured 3D Secrets within 3D Gaussian Splatting",
        "author": "Yan Ren, Shilin Lu, and Adams Wai-Kin Kong",
        "link": "http://arxiv.org/abs/2503.07191v1",
        "abstract": "Recent advances in 3D Gaussian Splatting (3DGS) have revolutionized scene\nreconstruction, opening new possibilities for 3D steganography by hiding 3D\nsecrets within 3D covers. The key challenge in steganography is ensuring\nimperceptibility while maintaining high-fidelity reconstruction. However,\nexisting methods often suffer from detectability risks and utilize only\nsuboptimal 3DGS features, limiting their full potential. We propose a novel\nend-to-end key-secured 3D steganography framework (KeySS) that jointly\noptimizes a 3DGS model and a key-secured decoder for secret reconstruction. Our\napproach reveals that Gaussian features contribute unequally to secret hiding.\nThe framework incorporates a key-controllable mechanism enabling multi-secret\nhiding and unauthorized access prevention, while systematically exploring\noptimal feature update to balance fidelity and security. To rigorously evaluate\nsteganographic imperceptibility beyond conventional 2D metrics, we introduce\n3D-Sinkhorn distance analysis, which quantifies distributional differences\nbetween original and steganographic Gaussian parameters in the representation\nspace. Extensive experiments demonstrate that our method achieves\nstate-of-the-art performance in both cover and secret reconstruction while\nmaintaining high security levels, advancing the field of 3D steganography. Code\nis available at https://github.com/RY-Paper/KeySS"
    },
    {
        "date": "2025-03",
        "title": "Breaking the Limits of Quantization-Aware Defenses: QADT-R for Robustness Against Patch-Based Adversarial Attacks in QNNs",
        "author": "Amira Guesmi, Bassem Ouni, and Muhammad Shafique",
        "link": "http://arxiv.org/abs/2503.07058v1",
        "abstract": "Quantized Neural Networks (QNNs) have emerged as a promising solution for\nreducing model size and computational costs, making them well-suited for\ndeployment in edge and resource-constrained environments. While quantization is\nknown to disrupt gradient propagation and enhance robustness against\npixel-level adversarial attacks, its effectiveness against patch-based\nadversarial attacks remains largely unexplored. In this work, we demonstrate\nthat adversarial patches remain highly transferable across quantized models,\nachieving over 70\\% attack success rates (ASR) even at extreme bit-width\nreductions (e.g., 2-bit). This challenges the common assumption that\nquantization inherently mitigates adversarial threats. To address this, we\npropose Quantization-Aware Defense Training with Randomization (QADT-R), a\nnovel defense strategy that integrates Adaptive Quantization-Aware Patch\nGeneration (A-QAPA), Dynamic Bit-Width Training (DBWT), and\nGradient-Inconsistent Regularization (GIR) to enhance resilience against highly\ntransferable patch-based attacks. A-QAPA generates adversarial patches within\nquantized models, ensuring robustness across different bit-widths. DBWT\nintroduces bit-width cycling during training to prevent overfitting to a\nspecific quantization setting, while GIR injects controlled gradient\nperturbations to disrupt adversarial optimization. Extensive evaluations on\nCIFAR-10 and ImageNet show that QADT-R reduces ASR by up to 25\\% compared to\nprior defenses such as PBAT and DWQ. Our findings further reveal that\nPBAT-trained models, while effective against seen patch configurations, fail to\ngeneralize to unseen patches due to quantization shift. Additionally, our\nempirical analysis of gradient alignment, spatial sensitivity, and patch\nvisibility provides insights into the mechanisms that contribute to the high\ntransferability of patch-based attacks in QNNs."
    },
    {
        "date": "2025-03",
        "title": "HybridReg: Robust 3D Point Cloud Registration with Hybrid Motions",
        "author": "Keyu Du, Hao Xu, Haipeng Li, Hong Qu, Chi-Wing Fu, and Shuaicheng Liu",
        "link": "http://arxiv.org/abs/2503.07019v1",
        "abstract": "Scene-level point cloud registration is very challenging when considering\ndynamic foregrounds. Existing indoor datasets mostly assume rigid motions, so\nthe trained models cannot robustly handle scenes with non-rigid motions. On the\nother hand, non-rigid datasets are mainly object-level, so the trained models\ncannot generalize well to complex scenes. This paper presents HybridReg, a new\napproach to 3D point cloud registration, learning uncertainty mask to account\nfor hybrid motions: rigid for backgrounds and non-rigid/rigid for\ninstance-level foregrounds. First, we build a scene-level 3D registration\ndataset, namely HybridMatch, designed specifically with strategies to arrange\ndiverse deforming foregrounds in a controllable manner. Second, we account for\ndifferent motion types and formulate a mask-learning module to alleviate the\ninterference of deforming outliers. Third, we exploit a simple yet effective\nnegative log-likelihood loss to adopt uncertainty to guide the feature\nextraction and correlation computation. To our best knowledge, HybridReg is the\nfirst work that exploits hybrid motions for robust point cloud registration.\nExtensive experiments show HybridReg's strengths, leading it to achieve\nstate-of-the-art performance on both widely-used indoor and outdoor datasets."
    },
    {
        "date": "2025-03",
        "title": "Public space security management using digital twin technologies",
        "author": "Stylianos Zindros, Christos Chronis, Panagiotis Radoglou-Grammatikis, Vasileios Argyriou, Panagiotis Sarigiannidis, Iraklis Varlamis, and Georgios Th. Papadopoulos",
        "link": "http://arxiv.org/abs/2503.06996v1",
        "abstract": "As the security of public spaces remains a critical issue in today's world,\nDigital Twin technologies have emerged in recent years as a promising solution\nfor detecting and predicting potential future threats. The applied methodology\nleverages a Digital Twin of a metro station in Athens, Greece, using the\nFlexSim simulation software. The model encompasses points of interest and\npassenger flows, and sets their corresponding parameters. These elements\ninfluence and allow the model to provide reasonable predictions on the security\nmanagement of the station under various scenarios. Experimental tests are\nconducted with different configurations of surveillance cameras and\noptimizations of camera angles to evaluate the effectiveness of the space\nsurveillance setup. The results show that the strategic positioning of\nsurveillance cameras and the adjustment of their angles significantly improves\nthe detection of suspicious behaviors and with the use of the DT it is possible\nto evaluate different scenarios and find the optimal camera setup for each\ncase. In summary, this study highlights the value of Digital Twins in real-time\nsimulation and data-driven security management. The proposed approach\ncontributes to the ongoing development of smart security solutions for public\nspaces and provides an innovative framework for threat detection and\nprevention."
    },
    {
        "date": "2025-03",
        "title": "Utilizing Jailbreak Probability to Attack and Safeguard Multimodal LLMs",
        "author": "Wenzhuo Xu, Zhipeng Wei, Xiongtao Sun, Deyue Zhang, Dongdong Yang, Quanchen Zou, and Xiangzheng Zhang",
        "link": "http://arxiv.org/abs/2503.06989v1",
        "abstract": "Recently, Multimodal Large Language Models (MLLMs) have demonstrated their\nsuperior ability in understanding multimodal contents. However, they remain\nvulnerable to jailbreak attacks, which exploit weaknesses in their safety\nalignment to generate harmful responses. Previous studies categorize jailbreaks\nas successful or failed based on whether responses contain malicious content.\nHowever, given the stochastic nature of MLLM responses, this binary\nclassification of an input's ability to jailbreak MLLMs is inappropriate.\nDerived from this viewpoint, we introduce jailbreak probability to quantify the\njailbreak potential of an input, which represents the likelihood that MLLMs\ngenerated a malicious response when prompted with this input. We approximate\nthis probability through multiple queries to MLLMs. After modeling the\nrelationship between input hidden states and their corresponding jailbreak\nprobability using Jailbreak Probability Prediction Network (JPPN), we use\ncontinuous jailbreak probability for optimization. Specifically, we propose\nJailbreak-Probability-based Attack (JPA) that optimizes adversarial\nperturbations on inputs to maximize jailbreak probability. To counteract\nattacks, we also propose two defensive methods: Jailbreak-Probability-based\nFinetuning (JPF) and Jailbreak-Probability-based Defensive Noise (JPDN), which\nminimizes jailbreak probability in the MLLM parameters and input space,\nrespectively. Extensive experiments show that (1) JPA yields improvements (up\nto 28.38\\%) under both white and black box settings compared to previous\nmethods with small perturbation bounds and few iterations. (2) JPF and JPDN\nsignificantly reduce jailbreaks by at most over 60\\%. Both of the above results\ndemonstrate the significance of introducing jailbreak probability to make\nnuanced distinctions among input jailbreak abilities."
    },
    {
        "date": "2025-03",
        "title": "ConcreTizer: Model Inversion Attack via Occupancy Classification and Dispersion Control for 3D Point Cloud Restoration",
        "author": "Youngseok Kim, Sunwook Hwang, Hyung-Sin Kim, and Saewoong Bahk",
        "link": "http://arxiv.org/abs/2503.06986v1",
        "abstract": "The growing use of 3D point cloud data in autonomous vehicles (AVs) has\nraised serious privacy concerns, particularly due to the sensitive information\nthat can be extracted from 3D data. While model inversion attacks have been\nwidely studied in the context of 2D data, their application to 3D point clouds\nremains largely unexplored. To fill this gap, we present the first in-depth\nstudy of model inversion attacks aimed at restoring 3D point cloud scenes. Our\nanalysis reveals the unique challenges, the inherent sparsity of 3D point\nclouds and the ambiguity between empty and non-empty voxels after voxelization,\nwhich are further exacerbated by the dispersion of non-empty voxels across\nfeature extractor layers. To address these challenges, we introduce\nConcreTizer, a simple yet effective model inversion attack designed\nspecifically for voxel-based 3D point cloud data. ConcreTizer incorporates\nVoxel Occupancy Classification to distinguish between empty and non-empty\nvoxels and Dispersion-Controlled Supervision to mitigate non-empty voxel\ndispersion. Extensive experiments on widely used 3D feature extractors and\nbenchmark datasets, such as KITTI and Waymo, demonstrate that ConcreTizer\nconcretely restores the original 3D point cloud scene from disrupted 3D feature\ndata. Our findings highlight both the vulnerability of 3D data to inversion\nattacks and the urgent need for robust defense strategies."
    },
    {
        "date": "2025-03",
        "title": "MIGA: Mutual Information-Guided Attack on Denoising Models for Semantic Manipulation",
        "author": "Guanghao Li, Mingzhi Chen, Hao Yu, Shuting Dong, Wenhao Jiang, Ming Tang, and Chun Yuan",
        "link": "http://arxiv.org/abs/2503.06966v2",
        "abstract": "Deep learning-based denoising models have been widely employed in vision\ntasks, functioning as filters to eliminate noise while retaining crucial\nsemantic information. Additionally, they play a vital role in defending against\nadversarial perturbations that threaten downstream tasks. However, these models\ncan be intrinsically susceptible to adversarial attacks due to their dependence\non specific noise assumptions. Existing attacks on denoising models mainly aim\nat deteriorating visual clarity while neglecting semantic manipulation,\nrendering them either easily detectable or limited in effectiveness. In this\npaper, we propose Mutual Information-Guided Attack (MIGA), the first method\ndesigned to directly attack deep denoising models by strategically disrupting\ntheir ability to preserve semantic content via adversarial perturbations. By\nminimizing the mutual information between the original and denoised images, a\nmeasure of semantic similarity. MIGA forces the denoiser to produce\nperceptually clean yet semantically altered outputs. While these images appear\nvisually plausible, they encode systematically distorted semantics, revealing a\nfundamental vulnerability in denoising models. These distortions persist in\ndenoised outputs and can be quantitatively assessed through downstream task\nperformance. We propose new evaluation metrics and systematically assess MIGA\non four denoising models across five datasets, demonstrating its consistent\neffectiveness in disrupting semantic fidelity. Our findings suggest that\ndenoising models are not always robust and can introduce security risks in\nreal-world applications."
    },
    {
        "date": "2025-03",
        "title": "When Lighting Deceives: Exposing Vision-Language Models' Illumination Vulnerability Through Illumination Transformation Attack",
        "author": "Hanqing Liu, Shouwei Ruan, Yao Huang, Shiji Zhao, and Xingxing Wei",
        "link": "http://arxiv.org/abs/2503.06903v1",
        "abstract": "Vision-Language Models (VLMs) have achieved remarkable success in various\ntasks, yet their robustness to real-world illumination variations remains\nlargely unexplored. To bridge this gap, we propose \\textbf{I}llumination\n\\textbf{T}ransformation \\textbf{A}ttack (\\textbf{ITA}), the first framework to\nsystematically assess VLMs' robustness against illumination changes. However,\nthere still exist two key challenges: (1) how to model global illumination with\nfine-grained control to achieve diverse lighting conditions and (2) how to\nensure adversarial effectiveness while maintaining naturalness. To address the\nfirst challenge, we innovatively decompose global illumination into multiple\nparameterized point light sources based on the illumination rendering equation.\nThis design enables us to model more diverse lighting variations that previous\nmethods could not capture. Then, by integrating these parameterized lighting\nvariations with physics-based lighting reconstruction techniques, we could\nprecisely render such light interactions in the original scenes, finally\nmeeting the goal of fine-grained lighting control. For the second challenge, by\ncontrolling illumination through the lighting reconstrution model's latent\nspace rather than direct pixel manipulation, we inherently preserve physical\nlighting priors. Furthermore, to prevent potential reconstruction artifacts, we\ndesign additional perceptual constraints for maintaining visual consistency\nwith original images and diversity constraints for avoiding light source\nconvergence.\n  Extensive experiments demonstrate that our ITA could significantly reduce the\nperformance of advanced VLMs, e.g., LLaVA-1.6, while possessing competitive\nnaturalness, exposing VLMS' critical illuminiation vulnerabilities."
    },
    {
        "date": "2025-03",
        "title": "TH-Bench: Evaluating Evading Attacks via Humanizing AI Text on Machine-Generated Text Detectors",
        "author": "Jingyi Zheng, Junfeng Wang, Zhen Sun, Wenhan Dong, Yule Liu, and Xinlei He",
        "link": "http://arxiv.org/abs/2503.08708v2",
        "abstract": "As Large Language Models (LLMs) advance, Machine-Generated Texts (MGTs) have\nbecome increasingly fluent, high-quality, and informative. Existing wide-range\nMGT detectors are designed to identify MGTs to prevent the spread of plagiarism\nand misinformation. However, adversaries attempt to humanize MGTs to evade\ndetection (named evading attacks), which requires only minor modifications to\nbypass MGT detectors. Unfortunately, existing attacks generally lack a unified\nand comprehensive evaluation framework, as they are assessed using different\nexperimental settings, model architectures, and datasets. To fill this gap, we\nintroduce the Text-Humanization Benchmark (TH-Bench), the first comprehensive\nbenchmark to evaluate evading attacks against MGT detectors. TH-Bench evaluates\nattacks across three key dimensions: evading effectiveness, text quality, and\ncomputational overhead. Our extensive experiments evaluate 6 state-of-the-art\nattacks against 13 MGT detectors across 6 datasets, spanning 19 domains and\ngenerated by 11 widely used LLMs. Our findings reveal that no single evading\nattack excels across all three dimensions. Through in-depth analysis, we\nhighlight the strengths and limitations of different attacks. More importantly,\nwe identify a trade-off among three dimensions and propose two optimization\ninsights. Through preliminary experiments, we validate their correctness and\neffectiveness, offering potential directions for future research."
    },
    {
        "date": "2025-03",
        "title": "A Secure Blockchain-Assisted Framework for Real-Time Maritime Environmental Compliance Monitoring",
        "author": "William C. Quigley, Mohamed Rahouti, and Gary M. Weiss",
        "link": "http://arxiv.org/abs/2503.08707v1",
        "abstract": "The maritime industry is governed by stringent environmental regulations,\nmost notably the International Convention for the Prevention of Pollution from\nShips (MARPOL). Ensuring compliance with these regulations is difficult due to\nlow inspection rates and the risk of data fabrication. To address these issues,\nthis paper proposes a secure blockchain-assisted framework for real-time\nmaritime environmental compliance monitoring. By integrating IoT and shipboard\nsensors with blockchain technology, the framework ensures immutable and\ntransparent record-keeping of environmental data. Smart contracts automate\ncompliance verification and notify relevant authorities in case of\nnon-compliance. A proof-of-concept case study on sulfur emissions demonstrates\nthe framework's efficacy in enhancing MARPOL enforcement through real-time data\nintegrity and regulatory adherence. The proposed system leverages the Polygon\nblockchain for scalability and efficiency, providing a robust solution for\nmaritime environmental protection. The evaluation results demonstrate that the\nproposed blockchain-enhanced compliance monitoring system effectively and\nsecurely ensures real-time regulatory adherence with high scalability,\nefficiency, and cost-effectiveness, leveraging the robust capabilities of the\nPolygon blockchain."
    },
    {
        "date": "2025-03",
        "title": "Unique Rashomon Sets for Robust Active Learning",
        "author": "Simon Nguyen, Kentaro Hoffman, and Tyler McCormick",
        "link": "http://arxiv.org/abs/2503.06770v2",
        "abstract": "Collecting labeled data for machine learning models is often expensive and\ntime-consuming. Active learning addresses this challenge by selectively\nlabeling the most informative observations, but when initial labeled data is\nlimited, it becomes difficult to distinguish genuinely informative points from\nthose appearing uncertain primarily due to noise. Ensemble methods like random\nforests are a powerful approach to quantifying this uncertainty but do so by\naggregating all models indiscriminately. This includes poor performing models\nand redundant models, a problem that worsens in the presence of noisy data. We\nintroduce UNique Rashomon Ensembled Active Learning (UNREAL), which selectively\nensembles only distinct models from the Rashomon set, which is the set of\nnearly optimal models. Restricting ensemble membership to high-performing\nmodels with different explanations helps distinguish genuine uncertainty from\nnoise-induced variation. We show that UNREAL achieves faster theoretical\nconvergence rates than traditional active learning approaches and demonstrates\nempirical improvements of up to 20% in predictive accuracy across five\nbenchmark datasets, while simultaneously enhancing model interpretability."
    },
    {
        "date": "2025-03",
        "title": "Enhancing NLP Robustness and Generalization through LLM-Generated Contrast Sets: A Scalable Framework for Systematic Evaluation and Adversarial Training",
        "author": "Hender Lin",
        "link": "http://arxiv.org/abs/2503.06648v1",
        "abstract": "Standard NLP benchmarks often fail to capture vulnerabilities stemming from\ndataset artifacts and spurious correlations. Contrast sets address this gap by\nchallenging models near decision boundaries but are traditionally\nlabor-intensive to create and limited in diversity. This study leverages large\nlanguage models to automate the generation of diverse contrast sets. Using the\nSNLI dataset, we created a 3,000-example contrast set to evaluate and improve\nmodel robustness. Fine-tuning on these contrast sets enhanced performance on\nsystematically perturbed examples, maintained standard test accuracy, and\nmodestly improved generalization to novel perturbations. This automated\napproach offers a scalable solution for evaluating and improving NLP models,\naddressing systematic generalization challenges, and advancing robustness in\nreal-world applications."
    },
    {
        "date": "2025-03",
        "title": "MMARD: Improving the Min-Max Optimization Process in Adversarial Robustness Distillation",
        "author": "Yuzheng Wang, Zhaoyu Chen, Dingkang Yang, Yuanhang Wang, and Lizhe Qi",
        "link": "http://arxiv.org/abs/2503.06559v1",
        "abstract": "Adversarial Robustness Distillation (ARD) is a promising task to boost the\nrobustness of small-capacity models with the guidance of the pre-trained robust\nteacher. The ARD can be summarized as a min-max optimization process, i.e.,\nsynthesizing adversarial examples (inner) & training the student (outer).\nAlthough competitive robustness performance, existing ARD methods still have\nissues. In the inner process, the synthetic training examples are far from the\nteacher's decision boundary leading to important robust information missing. In\nthe outer process, the student model is decoupled from learning natural and\nrobust scenarios, leading to the robustness saturation, i.e., student\nperformance is highly susceptible to customized teacher selection. To tackle\nthese issues, this paper proposes a general Min-Max optimization Adversarial\nRobustness Distillation (MMARD) method. For the inner process, we introduce the\nteacher's robust predictions, which drive the training examples closer to the\nteacher's decision boundary to explore more robust knowledge. For the outer\nprocess, we propose a structured information modeling method based on\ntriangular relationships to measure the mutual information of the model in\nnatural and robust scenarios and enhance the model's ability to understand\nmulti-scenario mapping relationships. Experiments show our MMARD achieves\nstate-of-the-art performance on multiple benchmarks. Besides, MMARD is\nplug-and-play and convenient to combine with existing methods."
    },
    {
        "date": "2025-03",
        "title": "BDPFL: Backdoor Defense for Personalized Federated Learning via Explainable Distillation",
        "author": "Chengcheng Zhu, Jiale Zhang, Di Wu, and Guodong Long",
        "link": "http://arxiv.org/abs/2503.06554v1",
        "abstract": "Federated learning is a distributed learning paradigm that facilitates the\ncollaborative training of a global model across multiple clients while\npreserving the privacy of local datasets. To address inherent challenges\nrelated to data heterogeneity and satisfy personalized needs, a new direction\nwithin FL, known as personalized Federated Learning (pFL), has gradually\nevolved. Extensive attention has been directed toward developing novel\nframeworks and methods to enhance the performance of pFL. Regrettably, the\naspect of security in pFL has been largely overlooked. Our objective is to fill\nthis gap. Similar to FL, pFL is susceptible to backdoor attacks. However,\nexisting backdoor defense strategies are primarily tailored to general FL\nframeworks, and pFL lacks robustness against backdoor attacks. We propose a\nnovel, backdoor-robust pFL framework named BDPFL to address these challenges.\nFirst, BDPFL introduces layer-wise mutual distillation that enables clients to\nlearn their personalized local models while mitigating potential backdoors.\nThen, BDPFL employs explanation heatmap to learn high-quality intermediate\nrepresentations and enhance the effect of eliminating deeper and more\nentrenched backdoors. Moreover, we perform empirical evaluations of BDPFL's\nperformance on three datasets and compare BDPFL with four backdoor defense\nmethods. The experiments demonstrate that BDPFL outperforms baseline methods\nand is effective under various settings."
    },
    {
        "date": "2025-03",
        "title": "AnywhereDoor: Multi-Target Backdoor Attacks on Object Detection",
        "author": "Jialin Lu, Junjie Shan, Ziqi Zhao, and Ka-Ho Chow",
        "link": "http://arxiv.org/abs/2503.06529v2",
        "abstract": "As object detection becomes integral to many safety-critical applications,\nunderstanding its vulnerabilities is essential. Backdoor attacks, in\nparticular, pose a serious threat by implanting hidden triggers in victim\nmodels, which adversaries can later exploit to induce malicious behaviors\nduring inference. However, current understanding is limited to single-target\nattacks, where adversaries must define a fixed malicious behavior (target)\nbefore training, making inference-time adaptability impossible. Given the large\noutput space of object detection (including object existence prediction,\nbounding box estimation, and classification), the feasibility of flexible,\ninference-time model control remains unexplored. This paper introduces\nAnywhereDoor, a multi-target backdoor attack for object detection. Once\nimplanted, AnywhereDoor allows adversaries to make objects disappear, fabricate\nnew ones, or mislabel them, either across all object classes or specific ones,\noffering an unprecedented degree of control. This flexibility is enabled by\nthree key innovations: (i) objective disentanglement to scale the number of\nsupported targets; (ii) trigger mosaicking to ensure robustness even against\nregion-based detectors; and (iii) strategic batching to address object-level\ndata imbalances that hinder manipulation. Extensive experiments demonstrate\nthat AnywhereDoor grants attackers a high degree of control, improving attack\nsuccess rates by 26% compared to adaptations of existing methods for such\nflexible control."
    },
    {
        "date": "2025-03",
        "title": "Can Small Language Models Reliably Resist Jailbreak Attacks? A Comprehensive Evaluation",
        "author": "Wenhui Zhang, Huiyu Xu, Zhibo Wang, Zeqing He, Ziqi Zhu, and Kui Ren",
        "link": "http://arxiv.org/abs/2503.06519v1",
        "abstract": "Small language models (SLMs) have emerged as promising alternatives to large\nlanguage models (LLMs) due to their low computational demands, enhanced privacy\nguarantees and comparable performance in specific domains through light-weight\nfine-tuning. Deploying SLMs on edge devices, such as smartphones and smart\nvehicles, has become a growing trend. However, the security implications of\nSLMs have received less attention than LLMs, particularly regarding jailbreak\nattacks, which is recognized as one of the top threats of LLMs by the OWASP. In\nthis paper, we conduct the first large-scale empirical study of SLMs'\nvulnerabilities to jailbreak attacks. Through systematically evaluation on 63\nSLMs from 15 mainstream SLM families against 8 state-of-the-art jailbreak\nmethods, we demonstrate that 47.6% of evaluated SLMs show high susceptibility\nto jailbreak attacks (ASR > 40%) and 38.1% of them can not even resist direct\nharmful query (ASR > 50%). We further analyze the reasons behind the\nvulnerabilities and identify four key factors: model size, model architecture,\ntraining datasets and training techniques. Moreover, we assess the\neffectiveness of three prompt-level defense methods and find that none of them\nachieve perfect performance, with detection accuracy varying across different\nSLMs and attack methods. Notably, we point out that the inherent security\nawareness play a critical role in SLM security, and models with strong security\nawareness could timely terminate unsafe response with little reminder. Building\nupon the findings, we highlight the urgent need for security-by-design\napproaches in SLM development and provide valuable insights for building more\ntrustworthy SLM ecosystem."
    },
    {
        "date": "2025-03",
        "title": "HFedCKD: Toward Robust Heterogeneous Federated Learning via Data-free Knowledge Distillation and Two-way Contrast",
        "author": "Yiting Zheng, Bohan Lin, Jinqian Chen, and Jihua Zhu",
        "link": "http://arxiv.org/abs/2503.06511v1",
        "abstract": "Most current federated learning frameworks are modeled as static processes,\nignoring the dynamic characteristics of the learning system. Under the limited\ncommunication budget of the central server, the flexible model architecture of\na large number of clients participating in knowledge transfer requires a lower\nparticipation rate, active clients have uneven contributions, and the client\nscale seriously hinders the performance of FL. We consider a more general and\npractical federation scenario and propose a system heterogeneous federation\nmethod based on data-free knowledge distillation and two-way contrast\n(HFedCKD). We apply the Inverse Probability Weighted Distillation (IPWD)\nstrategy to the data-free knowledge transfer framework. The generator completes\nthe data features of the nonparticipating clients. IPWD implements a dynamic\nevaluation of the prediction contribution of each client under different data\ndistributions. Based on the antibiased weighting of its prediction loss, the\nweight distribution of each client is effectively adjusted to fairly integrate\nthe knowledge of participating clients. At the same time, the local model is\nsplit into a feature extractor and a classifier. Through differential contrast\nlearning, the feature extractor is aligned with the global model in the feature\nspace, while the classifier maintains personalized decision-making\ncapabilities. HFedCKD effectively alleviates the knowledge offset caused by a\nlow participation rate under data-free knowledge distillation and improves the\nperformance and stability of the model. We conduct extensive experiments on\nimage and IoT datasets to comprehensively evaluate and verify the\ngeneralization and robustness of the proposed HFedCKD framework."
    },
    {
        "date": "2025-03",
        "title": "Long-tailed Adversarial Training with Self-Distillation",
        "author": "Seungju Cho, Hongsin Lee, and Changick Kim",
        "link": "http://arxiv.org/abs/2503.06461v1",
        "abstract": "Adversarial training significantly enhances adversarial robustness, yet\nsuperior performance is predominantly achieved on balanced datasets.\n  Addressing adversarial robustness in the context of unbalanced or long-tailed\ndistributions is considerably more challenging, mainly due to the scarcity of\ntail data instances.\n  Previous research on adversarial robustness within long-tailed distributions\nhas primarily focused on combining traditional long-tailed natural training\nwith existing adversarial robustness methods.\n  In this study, we provide an in-depth analysis for the challenge that\nadversarial training struggles to achieve high performance on tail classes in\nlong-tailed distributions.\n  Furthermore, we propose a simple yet effective solution to advance\nadversarial robustness on long-tailed distributions through a novel\nself-distillation technique.\n  Specifically, this approach leverages a balanced self-teacher model, which is\ntrained using a balanced dataset sampled from the original long-tailed dataset.\nOur extensive experiments demonstrate state-of-the-art performance in both\nclean and robust accuracy for long-tailed adversarial robustness, with\nsignificant improvements in tail class performance on various datasets. We\nimprove the accuracy against PGD attacks for tail classes by 20.3, 7.1, and 3.8\npercentage points on CIFAR-10, CIFAR-100, and Tiny-ImageNet, respectively,\nwhile achieving the highest robust accuracy."
    },
    {
        "date": "2025-03",
        "title": "R+R: Security Vulnerability Dataset Quality Is Critical",
        "author": "Anurag Swarnim Yadav, and Joseph N. Wilson",
        "link": "http://arxiv.org/abs/2503.06387v1",
        "abstract": "Large Language Models (LLMs) are of great interest in vulnerability detection\nand repair. The effectiveness of these models hinges on the quality of the\ndatasets used for both training and evaluation. Our investigation reveals that\na number of studies featured in prominent software engineering conferences have\nemployed datasets that are plagued by high duplication rates, questionable\nlabel accuracy, and incomplete samples. Using these datasets for\nexperimentation will yield incorrect results that are significantly different\nfrom actual expected behavior. For example, the state-of-the-art VulRepair\nModel, which is reported to have 44% accuracy, on average yielded 9% accuracy\nwhen test-set duplicates were removed from its training set and 13% accuracy\nwhen training-set duplicates were removed from its test set. In an effort to\ntackle these data quality concerns, we have retrained models from several\npapers without duplicates and conducted an accuracy assessment of labels for\nthe top ten most hazardous Common Weakness Enumerations (CWEs). Our findings\nindicate that 56% of the samples had incorrect labels and 44% comprised\nincomplete samples--only 31% were both accurate and complete. Finally, we\nemploy transfer learning using a large deduplicated bugfix corpus to show that\nthese models can exhibit better performance if given larger amounts of\nhigh-quality pre-training data, leading us to conclude that while previous\nstudies have over-estimated performance due to poor dataset quality, this does\nnot demonstrate that better performance is not possible."
    },
    {
        "date": "2025-03",
        "title": "Bayesian Optimization for Robust Identification of Ornstein-Uhlenbeck Model",
        "author": "Jinwen Xu, Qin Lu, and Yaakov Bar-Shalom",
        "link": "http://arxiv.org/abs/2503.06381v1",
        "abstract": "This paper deals with the identification of the stochastic Ornstein-Uhlenbeck\n(OU) process error model, which is characterized by an inverse time constant,\nand the unknown variances of the process and observation noises. Although the\navailability of the explicit expression of the log-likelihood function allows\none to obtain the maximum likelihood estimator (MLE), this entails evaluating\nthe nontrivial gradient and also often struggles with local optima. To address\nthese limitations, we put forth a sample-efficient global optimization approach\nbased on the Bayesian optimization (BO) framework, which relies on a Gaussian\nprocess (GP) surrogate model for the objective function that effectively\nbalances exploration and exploitation to select the query points. Specifically,\neach evaluation of the objective is implemented efficiently through the Kalman\nfilter (KF) recursion. Comprehensive experiments on various parameter settings\nand sampling intervals corroborate that BO-based estimator consistently\noutperforms MLE implemented by the steady-state KF approximation and the\nexpectation-maximization algorithm (whose derivation is a side contribution) in\nterms of root mean-square error (RMSE) and statistical consistency, confirming\nthe effectiveness and robustness of the BO for identification of the stochastic\nOU process. Notably, the RMSE values produced by the BO-based estimator are\nsmaller than the classical Cram\\'{e}r-Rao lower bound, especially for the\ninverse time constant, estimating which has been a long-standing challenge.\nThis seemingly counterintuitive result can be explained by the data-driven\nprior for the learning parameters indirectly injected by BO through the GP\nprior over the objective function."
    },
    {
        "date": "2025-03",
        "title": "Adversarial Robustness of Discriminative Self-Supervised Learning in Vision",
        "author": "\u00d6mer Veysel \u00c7a\u011fatan, \u00d6mer Faruk Tal, and M. Emre G\u00fcrsoy",
        "link": "http://arxiv.org/abs/2503.06361v1",
        "abstract": "Self-supervised learning (SSL) has advanced significantly in visual\nrepresentation learning, yet comprehensive evaluations of its adversarial\nrobustness remain limited. In this study, we evaluate the adversarial\nrobustness of seven discriminative self-supervised models and one supervised\nmodel across diverse tasks, including ImageNet classification, transfer\nlearning, segmentation, and detection. Our findings suggest that discriminative\nSSL models generally exhibit better robustness to adversarial attacks compared\nto their supervised counterpart on ImageNet, with this advantage extending to\ntransfer learning when using linear evaluation. However, when fine-tuning is\napplied, the robustness gap between SSL and supervised models narrows\nconsiderably. Similarly, this robustness advantage diminishes in segmentation\nand detection tasks. We also investigate how various factors might influence\nadversarial robustness, including architectural choices, training duration,\ndata augmentations, and batch sizes. Our analysis contributes to the ongoing\nexploration of adversarial robustness in visual self-supervised representation\nsystems."
    },
    {
        "date": "2025-03",
        "title": "Backdoor Attacks on Discrete Graph Diffusion Models",
        "author": "Jiawen Wang, Samin Karim, Yuan Hong, and Binghui Wang",
        "link": "http://arxiv.org/abs/2503.06340v1",
        "abstract": "Diffusion models are powerful generative models in continuous data domains\nsuch as image and video data. Discrete graph diffusion models (DGDMs) have\nrecently extended them for graph generation, which are crucial in fields like\nmolecule and protein modeling, and obtained the SOTA performance. However, it\nis risky to deploy DGDMs for safety-critical applications (e.g., drug\ndiscovery) without understanding their security vulnerabilities. In this work,\nwe perform the first study on graph diffusion models against backdoor attacks,\na severe attack that manipulates both the training and inference/generation\nphases in graph diffusion models. We first define the threat model, under which\nwe design the attack such that the backdoored graph diffusion model can\ngenerate 1) high-quality graphs without backdoor activation, 2) effective,\nstealthy, and persistent backdoored graphs with backdoor activation, and 3)\ngraphs that are permutation invariant and exchangeable--two core properties in\ngraph generative models. 1) and 2) are validated via empirical evaluations\nwithout and with backdoor defenses, while 3) is validated via theoretical\nresults."
    },
    {
        "date": "2025-03",
        "title": "Advancing Autonomous Vehicle Intelligence: Deep Learning and Multimodal LLM for Traffic Sign Recognition and Robust Lane Detection",
        "author": "Chandan Kumar Sah, Ankit Kumar Shaw, Xiaoli Lian, Arsalan Shahid Baig, Tuopu Wen, Kun Jiang, Mengmeng Yang, and Diange Yang",
        "link": "http://arxiv.org/abs/2503.06313v1",
        "abstract": "Autonomous vehicles (AVs) require reliable traffic sign recognition and\nrobust lane detection capabilities to ensure safe navigation in complex and\ndynamic environments. This paper introduces an integrated approach combining\nadvanced deep learning techniques and Multimodal Large Language Models (MLLMs)\nfor comprehensive road perception. For traffic sign recognition, we\nsystematically evaluate ResNet-50, YOLOv8, and RT-DETR, achieving\nstate-of-the-art performance of 99.8% with ResNet-50, 98.0% accuracy with\nYOLOv8, and achieved 96.6% accuracy in RT-DETR despite its higher computational\ncomplexity. For lane detection, we propose a CNN-based segmentation method\nenhanced by polynomial curve fitting, which delivers high accuracy under\nfavorable conditions. Furthermore, we introduce a lightweight, Multimodal,\nLLM-based framework that directly undergoes instruction tuning using small yet\ndiverse datasets, eliminating the need for initial pretraining. This framework\neffectively handles various lane types, complex intersections, and merging\nzones, significantly enhancing lane detection reliability by reasoning under\nadverse conditions. Despite constraints in available training resources, our\nmultimodal approach demonstrates advanced reasoning capabilities, achieving a\nFrame Overall Accuracy (FRM) of 53.87%, a Question Overall Accuracy (QNS) of\n82.83%, lane detection accuracies of 99.6% in clear conditions and 93.0% at\nnight, and robust performance in reasoning about lane invisibility due to rain\n(88.4%) or road degradation (95.6%). The proposed comprehensive framework\nmarkedly enhances AV perception reliability, thus contributing significantly to\nsafer autonomous driving across diverse and challenging road scenarios."
    },
    {
        "date": "2025-03",
        "title": "Synergizing AI and Digital Twins for Next-Generation Network Optimization, Forecasting, and Security",
        "author": "Zifan Zhang, Minghong Fang, Dianwei Chen, Xianfeng Yang, and Yuchen Liu",
        "link": "http://arxiv.org/abs/2503.06302v1",
        "abstract": "Digital network twins (DNTs) are virtual representations of physical\nnetworks, designed to enable real-time monitoring, simulation, and optimization\nof network performance. When integrated with machine learning (ML) techniques,\nparticularly federated learning (FL) and reinforcement learning (RL), DNTs\nemerge as powerful solutions for managing the complexities of network\noperations. This article presents a comprehensive analysis of the synergy of\nDNTs, FL, and RL techniques, showcasing their collective potential to address\ncritical challenges in 6G networks. We highlight key technical challenges that\nneed to be addressed, such as ensuring network reliability, achieving joint\ndata-scenario forecasting, and maintaining security in high-risk environments.\nAdditionally, we propose several pipelines that integrate DNT and ML within\ncoherent frameworks to enhance network optimization and security. Case studies\ndemonstrate the practical applications of our proposed pipelines in edge\ncaching and vehicular networks. In edge caching, the pipeline achieves over 80%\ncache hit rates while balancing base station loads. In autonomous vehicular\nsystem, it ensure a 100% no-collision rate, showcasing its reliability in\nsafety-critical scenarios. By exploring these synergies, we offer insights into\nthe future of intelligent and adaptive network systems that automate\ndecision-making and problem-solving."
    },
    {
        "date": "2025-03",
        "title": "Single Domain Generalization with Adversarial Memory",
        "author": "Hao Yan, Marzi Heidari, and Yuhong Guo",
        "link": "http://arxiv.org/abs/2503.06288v1",
        "abstract": "Domain Generalization (DG) aims to train models that can generalize to unseen\ntesting domains by leveraging data from multiple training domains. However,\ntraditional DG methods rely on the availability of multiple diverse training\ndomains, limiting their applicability in data-constrained scenarios. Single\nDomain Generalization (SDG) addresses the more realistic and challenging\nsetting by restricting the training data to a single domain distribution. The\nmain challenges in SDG stem from the limited diversity of training data and the\ninaccessibility of unseen testing data distributions. To tackle these\nchallenges, we propose a single domain generalization method that leverages an\nadversarial memory bank to augment training features. Our memory-based feature\naugmentation network maps both training and testing features into an invariant\nsubspace spanned by diverse memory features, implicitly aligning the training\nand testing domains in the projected space. To maintain a diverse and\nrepresentative feature memory bank, we introduce an adversarial feature\ngeneration method that creates features extending beyond the training domain\ndistribution. Experimental results demonstrate that our approach achieves\nstate-of-the-art performance on standard single domain generalization\nbenchmarks."
    },
    {
        "date": "2025-03",
        "title": "Exploring Adversarial Transferability between Kolmogorov-arnold Networks",
        "author": "Songping Wang, Xinquan Yue, Yueming Lyu, and Caifeng Shan",
        "link": "http://arxiv.org/abs/2503.06276v1",
        "abstract": "Kolmogorov-Arnold Networks (KANs) have emerged as a transformative model\nparadigm, significantly impacting various fields. However, their adversarial\nrobustness remains less underexplored, especially across different KAN\narchitectures. To explore this critical safety issue, we conduct an analysis\nand find that due to overfitting to the specific basis functions of KANs, they\npossess poor adversarial transferability among different KANs. To tackle this\nchallenge, we propose AdvKAN, the first transfer attack method for KANs. AdvKAN\nintegrates two key components: 1) a Breakthrough-Defense Surrogate Model\n(BDSM), which employs a breakthrough-defense training strategy to mitigate\noverfitting to the specific structures of KANs. 2) a Global-Local Interaction\n(GLI) technique, which promotes sufficient interaction between adversarial\ngradients of hierarchical levels, further smoothing out loss surfaces of KANs.\nBoth of them work together to enhance the strength of transfer attack among\ndifferent KANs. Extensive experimental results on various KANs and datasets\ndemonstrate the effectiveness of AdvKAN, which possesses notably superior\nattack capabilities and deeply reveals the vulnerabilities of KANs. Code will\nbe released upon acceptance."
    },
    {
        "date": "2025-03",
        "title": "Using Mechanistic Interpretability to Craft Adversarial Attacks against Large Language Models",
        "author": "Thomas Winninger, Boussad Addad, and Katarzyna Kapusta",
        "link": "http://arxiv.org/abs/2503.06269v1",
        "abstract": "Traditional white-box methods for creating adversarial perturbations against\nLLMs typically rely only on gradient computation from the targeted model,\nignoring the internal mechanisms responsible for attack success or failure.\nConversely, interpretability studies that analyze these internal mechanisms\nlack practical applications beyond runtime interventions. We bridge this gap by\nintroducing a novel white-box approach that leverages mechanistic\ninterpretability techniques to craft practical adversarial inputs.\nSpecifically, we first identify acceptance subspaces - sets of feature vectors\nthat do not trigger the model's refusal mechanisms - then use gradient-based\noptimization to reroute embeddings from refusal subspaces to acceptance\nsubspaces, effectively achieving jailbreaks. This targeted approach\nsignificantly reduces computation cost, achieving attack success rates of\n80-95\\% on state-of-the-art models including Gemma2, Llama3.2, and Qwen2.5\nwithin minutes or even seconds, compared to existing techniques that often fail\nor require hours of computation. We believe this approach opens a new direction\nfor both attack research and defense development. Furthermore, it showcases a\npractical application of mechanistic interpretability where other methods are\nless efficient, which highlights its utility. The code and generated datasets\nare available at https://github.com/Sckathach/subspace-rerouting."
    },
    {
        "date": "2025-03",
        "title": "Poisoned-MRAG: Knowledge Poisoning Attacks to Multimodal Retrieval Augmented Generation",
        "author": "Yinuo Liu, Zenghui Yuan, Guiyao Tie, Jiawen Shi, Pan Zhou, Lichao Sun, and Neil Zhenqiang Gong",
        "link": "http://arxiv.org/abs/2503.06254v2",
        "abstract": "Multimodal retrieval-augmented generation (RAG) enhances the visual reasoning\ncapability of vision-language models (VLMs) by dynamically accessing\ninformation from external knowledge bases. In this work, we introduce\n\\textit{Poisoned-MRAG}, the first knowledge poisoning attack on multimodal RAG\nsystems. Poisoned-MRAG injects a few carefully crafted image-text pairs into\nthe multimodal knowledge database, manipulating VLMs to generate the\nattacker-desired response to a target query. Specifically, we formalize the\nattack as an optimization problem and propose two cross-modal attack\nstrategies, dirty-label and clean-label, tailored to the attacker's knowledge\nand goals. Our extensive experiments across multiple knowledge databases and\nVLMs show that Poisoned-MRAG outperforms existing methods, achieving up to 98\\%\nattack success rate with just five malicious image-text pairs injected into the\nInfoSeek database (481,782 pairs). Additionally, We evaluate 4 different\ndefense strategies, including paraphrasing, duplicate removal, structure-driven\nmitigation, and purification, demonstrating their limited effectiveness and\ntrade-offs against Poisoned-MRAG. Our results highlight the effectiveness and\nscalability of Poisoned-MRAG, underscoring its potential as a significant\nthreat to multimodal RAG systems."
    },
    {
        "date": "2025-03",
        "title": "MAD-MAX: Modular And Diverse Malicious Attack MiXtures for Automated LLM Red Teaming",
        "author": "Stefan Schoepf, Muhammad Zaid Hameed, Ambrish Rawat, Kieran Fraser, Giulio Zizzo, Giandomenico Cornacchia, and Mark Purcell",
        "link": "http://arxiv.org/abs/2503.06253v1",
        "abstract": "With LLM usage rapidly increasing, their vulnerability to jailbreaks that\ncreate harmful outputs are a major security risk. As new jailbreaking\nstrategies emerge and models are changed by fine-tuning, continuous testing for\nsecurity vulnerabilities is necessary. Existing Red Teaming methods fall short\nin cost efficiency, attack success rate, attack diversity, or extensibility as\nnew attack types emerge. We address these challenges with Modular And Diverse\nMalicious Attack MiXtures (MAD-MAX) for Automated LLM Red Teaming. MAD-MAX uses\nautomatic assignment of attack strategies into relevant attack clusters,\nchooses the most relevant clusters for a malicious goal, and then combines\nstrategies from the selected clusters to achieve diverse novel attacks with\nhigh attack success rates. MAD-MAX further merges promising attacks together at\neach iteration of Red Teaming to boost performance and introduces a similarity\nfilter to prune out similar attacks for increased cost efficiency. The MAD-MAX\napproach is designed to be easily extensible with newly discovered attack\nstrategies and outperforms the prominent Red Teaming method Tree of Attacks\nwith Pruning (TAP) significantly in terms of Attack Success Rate (ASR) and\nqueries needed to achieve jailbreaks. MAD-MAX jailbreaks 97% of malicious goals\nin our benchmarks on GPT-4o and Gemini-Pro compared to TAP with 66%. MAD-MAX\ndoes so with only 10.9 average queries to the target LLM compared to TAP with\n23.3.\n  WARNING: This paper contains contents which are offensive in nature."
    },
    {
        "date": "2025-03",
        "title": "Attackers Can Do Better: Over- and Understated Factors of Model Stealing Attacks",
        "author": "Daryna Oliynyk, Rudolf Mayer, and Andreas Rauber",
        "link": "http://arxiv.org/abs/2503.06188v1",
        "abstract": "Machine learning models were shown to be vulnerable to model stealing\nattacks, which lead to intellectual property infringement. Among other methods,\nsubstitute model training is an all-encompassing attack applicable to any\nmachine learning model whose behaviour can be approximated from input-output\nqueries. Whereas prior works mainly focused on improving the performance of\nsubstitute models by, e.g. developing a new substitute training method, there\nhave been only limited ablation studies on the impact the attacker's strength\nhas on the substitute model's performance. As a result, different authors came\nto diverse, sometimes contradicting, conclusions. In this work, we exhaustively\nexamine the ambivalent influence of different factors resulting from varying\nthe attacker's capabilities and knowledge on a substitute training attack. Our\nfindings suggest that some of the factors that have been considered important\nin the past are, in fact, not that influential; instead, we discover new\ncorrelations between attack conditions and success rate. In particular, we\ndemonstrate that better-performing target models enable higher-fidelity attacks\nand explain the intuition behind this phenomenon. Further, we propose to shift\nthe focus from the complexity of target models toward the complexity of their\nlearning tasks. Therefore, for the substitute model, rather than aiming for a\nhigher architecture complexity, we suggest focusing on getting data of higher\ncomplexity and an appropriate architecture. Finally, we demonstrate that even\nin the most limited data-free scenario, there is no need to overcompensate weak\nknowledge with millions of queries. Our results often exceed or match the\nperformance of previous attacks that assume a stronger attacker, suggesting\nthat these stronger attacks are likely endangering a model owner's intellectual\nproperty to a significantly higher degree than shown until now."
    },
    {
        "date": "2025-03",
        "title": "Secure On-Device Video OOD Detection Without Backpropagation",
        "author": "Li Li, Peilin Cai, Yuxiao Zhou, Zhiyu Ni, Renjie Liang, You Qin, Yi Nian, Zhengzhong Tu, Xiyang Hu, and Yue Zhao",
        "link": "http://arxiv.org/abs/2503.06166v1",
        "abstract": "Out-of-Distribution (OOD) detection is critical for ensuring the reliability\nof machine learning models in safety-critical applications such as autonomous\ndriving and medical diagnosis. While deploying personalized OOD detection\ndirectly on edge devices is desirable, it remains challenging due to large\nmodel sizes and the computational infeasibility of on-device training.\nFederated learning partially addresses this but still requires gradient\ncomputation and backpropagation, exceeding the capabilities of many edge\ndevices. To overcome these challenges, we propose SecDOOD, a secure\ncloud-device collaboration framework for efficient on-device OOD detection\nwithout requiring device-side backpropagation. SecDOOD utilizes cloud resources\nfor model training while ensuring user data privacy by retaining sensitive\ninformation on-device. Central to SecDOOD is a HyperNetwork-based personalized\nparameter generation module, which adapts cloud-trained models to\ndevice-specific distributions by dynamically generating local weight\nadjustments, effectively combining central and local information without local\nfine-tuning. Additionally, our dynamic feature sampling and encryption strategy\nselectively encrypts only the most informative feature channels, largely\nreducing encryption overhead without compromising detection performance.\nExtensive experiments across multiple datasets and OOD scenarios demonstrate\nthat SecDOOD achieves performance comparable to fully fine-tuned models,\nenabling secure, efficient, and personalized OOD detection on resource-limited\nedge devices. To enhance accessibility and reproducibility, our code is\npublicly available at https://github.com/Dystopians/SecDOOD."
    },
    {
        "date": "2025-03",
        "title": "Boosting the Local Invariance for Better Adversarial Transferability",
        "author": "Bohan Liu, and Xiaosen Wang",
        "link": "http://arxiv.org/abs/2503.06140v1",
        "abstract": "Transfer-based attacks pose a significant threat to real-world applications\nby directly targeting victim models with adversarial examples generated on\nsurrogate models. While numerous approaches have been proposed to enhance\nadversarial transferability, existing works often overlook the intrinsic\nrelationship between adversarial perturbations and input images. In this work,\nwe find that adversarial perturbation often exhibits poor translation\ninvariance for a given clean image and model, which is attributed to local\ninvariance. Through empirical analysis, we demonstrate that there is a positive\ncorrelation between the local invariance of adversarial perturbations w.r.t.\nthe input image and their transferability across different models. Based on\nthis finding, we propose a general adversarial transferability boosting\ntechnique called Local Invariance Boosting approach (LI-Boost). Extensive\nexperiments on the standard ImageNet dataset demonstrate that LI-Boost could\nsignificantly boost various types of transfer-based attacks (e.g.,\ngradient-based, input transformation-based, model-related, advanced objective\nfunction, ensemble, etc.) on CNNs, ViTs, and defense mechanisms. Our approach\npresents a promising direction for future research in improving adversarial\ntransferability across different models."
    },
    {
        "date": "2025-03",
        "title": "SecureGS: Boosting the Security and Fidelity of 3D Gaussian Splatting Steganography",
        "author": "Xuanyu Zhang, Jiarui Meng, Zhipei Xu, Shuzhou Yang, Yanmin Wu, Ronggang Wang, and Jian Zhang",
        "link": "http://arxiv.org/abs/2503.06118v1",
        "abstract": "3D Gaussian Splatting (3DGS) has emerged as a premier method for 3D\nrepresentation due to its real-time rendering and high-quality outputs,\nunderscoring the critical need to protect the privacy of 3D assets. Traditional\nNeRF steganography methods fail to address the explicit nature of 3DGS since\nits point cloud files are publicly accessible. Existing GS steganography\nsolutions mitigate some issues but still struggle with reduced rendering\nfidelity, increased computational demands, and security flaws, especially in\nthe security of the geometric structure of the visualized point cloud. To\naddress these demands, we propose a SecureGS, a secure and efficient 3DGS\nsteganography framework inspired by Scaffold-GS's anchor point design and\nneural decoding. SecureGS uses a hybrid decoupled Gaussian encryption mechanism\nto embed offsets, scales, rotations, and RGB attributes of the hidden 3D\nGaussian points in anchor point features, retrievable only by authorized users\nthrough privacy-preserving neural networks. To further enhance security, we\npropose a density region-aware anchor growing and pruning strategy that\nadaptively locates optimal hiding regions without exposing hidden information.\nExtensive experiments show that SecureGS significantly surpasses existing GS\nsteganography methods in rendering fidelity, speed, and security."
    },
    {
        "date": "2025-03",
        "title": "Disrupting Model Merging: A Parameter-Level Defense Without Sacrificing Accuracy",
        "author": "Wei Junhao, Yu Zhe, and Sakuma Jun",
        "link": "http://arxiv.org/abs/2503.07661v1",
        "abstract": "Model merging is a technique that combines multiple finetuned models into a\nsingle model without additional training, allowing a free-rider to cheaply\ninherit specialized capabilities. This study investigates methodologies to\nsuppress unwanted model merging by free-riders. Existing methods such as model\nwatermarking or fingerprinting can only detect merging in hindsight. In\ncontrast, we propose a first proactive defense against model merging.\nSpecifically, our defense method modifies the model parameters so that the\nmodel is disrupted if the model is merged with any other model, while its\nfunctionality is kept unchanged if not merged with others. Our approach\nconsists of two modules, rearranging MLP parameters and scaling attention\nheads, which push the model out of the shared basin in parameter space, causing\nthe merging performance with other models to degrade significantly. We conduct\nextensive experiments on image classification, image generation, and text\nclassification to demonstrate that our defense severely disrupts merging while\nretaining the functionality of the post-protect model. Moreover, we analyze\npotential adaptive attacks and further propose a dropout-based pruning to\nimprove our proposal's robustness."
    },
    {
        "date": "2025-03",
        "title": "STAR: A Foundation Model-driven Framework for Robust Task Planning and Failure Recovery in Robotic Systems",
        "author": "Md Sadman Sakib, and Yu Sun",
        "link": "http://arxiv.org/abs/2503.06060v1",
        "abstract": "Modern robotic systems, deployed across domains from industrial automation to\ndomestic assistance, face a critical challenge: executing tasks with precision\nand adaptability in dynamic, unpredictable environments. To address this, we\npropose STAR (Smart Task Adaptation and Recovery), a novel framework that\nsynergizes Foundation Models (FMs) with dynamically expanding Knowledge Graphs\n(KGs) to enable resilient task planning and autonomous failure recovery. While\nFMs offer remarkable generalization and contextual reasoning, their\nlimitations, including computational inefficiency, hallucinations, and output\ninconsistencies hinder reliable deployment. STAR mitigates these issues by\nembedding learned knowledge into structured, reusable KGs, which streamline\ninformation retrieval, reduce redundant FM computations, and provide precise,\nscenario-specific insights. The framework leverages FM-driven reasoning to\ndiagnose failures, generate context-aware recovery strategies, and execute\ncorrective actions without human intervention or system restarts. Unlike\nconventional approaches that rely on rigid protocols, STAR dynamically expands\nits KG with experiential knowledge, ensuring continuous adaptation to novel\nscenarios. To evaluate the effectiveness of this approach, we developed a\ncomprehensive dataset that includes various robotic tasks and failure\nscenarios. Through extensive experimentation, STAR demonstrated an 86% task\nplanning accuracy and 78% recovery success rate, showing significant\nimprovements over baseline methods. The framework's ability to continuously\nlearn from experience while maintaining structured knowledge representation\nmakes it particularly suitable for long-term deployment in real-world\napplications."
    },
    {
        "date": "2025-03",
        "title": "SAS: Segment Anything Small for Ultrasound -- A Non-Generative Data Augmentation Technique for Robust Deep Learning in Ultrasound Imaging",
        "author": "Danielle L. Ferreira, Ahana Gangopadhyay, Hsi-Ming Chang, Ravi Soni, and Gopal Avinash",
        "link": "http://arxiv.org/abs/2503.05916v1",
        "abstract": "Accurate segmentation of anatomical structures in ultrasound (US) images,\nparticularly small ones, is challenging due to noise and variability in imaging\nconditions (e.g., probe position, patient anatomy, tissue characteristics and\npathology). To address this, we introduce Segment Anything Small (SAS), a\nsimple yet effective scale- and texture-aware data augmentation technique\ndesigned to enhance the performance of deep learning models for segmenting\nsmall anatomical structures in ultrasound images. SAS employs a dual\ntransformation strategy: (1) simulating diverse organ scales by resizing and\nembedding organ thumbnails into a black background, and (2) injecting noise\ninto regions of interest to simulate varying tissue textures. These\ntransformations generate realistic and diverse training data without\nintroducing hallucinations or artifacts, improving the model's robustness to\nnoise and variability. We fine-tuned a promptable foundation model on a\ncontrolled organ-specific medical imaging dataset and evaluated its performance\non one internal and five external datasets. Experimental results demonstrate\nsignificant improvements in segmentation performance, with Dice score gains of\nup to 0.35 and an average improvement of 0.16 [95% CI 0.132,0.188].\nAdditionally, our iterative point prompts provide precise control and adaptive\nrefinement, achieving performance comparable to bounding box prompts with just\ntwo points. SAS enhances model robustness and generalizability across diverse\nanatomical structures and imaging conditions, particularly for small\nstructures, without compromising the accuracy of larger ones. By offering a\ncomputationally efficient solution that eliminates the need for extensive human\nlabeling efforts, SAS emerges as a powerful tool for advancing medical image\nanalysis, particularly in resource-constrained settings."
    },
    {
        "date": "2025-03",
        "title": "Generalizable Image Repair for Robust Visual Autonomous Racing",
        "author": "Carson Sobolewski, Zhenjiang Mao, Kshitij Vejre, and Ivan Ruchkin",
        "link": "http://arxiv.org/abs/2503.05911v1",
        "abstract": "Vision-based autonomous racing relies on accurate perception for robust\ncontrol. However, image distribution changes caused by sensor noise, adverse\nweather, and dynamic lighting can degrade perception, leading to suboptimal\ncontrol decisions. Existing approaches, including domain adaptation and\nadversarial training, improve robustness but struggle to generalize to unseen\ncorruptions while introducing computational overhead. To address this\nchallenge, we propose a real-time image repair module that restores corrupted\nimages before they are used by the controller. Our method leverages generative\nadversarial models, specifically CycleGAN and pix2pix, for image repair.\nCycleGAN enables unpaired image-to-image translation to adapt to novel\ncorruptions, while pix2pix exploits paired image data when available to improve\nthe quality. To ensure alignment with control performance, we introduce a\ncontrol-focused loss function that prioritizes perceptual consistency in\nrepaired images. We evaluated our method in a simulated autonomous racing\nenvironment with various visual corruptions. The results show that our approach\nsignificantly improves performance compared to baselines, mitigating\ndistribution shift and enhancing controller reliability."
    },
    {
        "date": "2025-03",
        "title": "Quantifying the Robustness of Retrieval-Augmented Language Models Against Spurious Features in Grounding Data",
        "author": "Shiping Yang, Jie Wu, Wenbiao Ding, Ning Wu, Shining Liang, Ming Gong, Hengyuan Zhang, and Dongmei Zhang",
        "link": "http://arxiv.org/abs/2503.05587v1",
        "abstract": "Robustness has become a critical attribute for the deployment of RAG systems\nin real-world applications. Existing research focuses on robustness to explicit\nnoise (e.g., document semantics) but overlooks spurious features (a.k.a.\nimplicit noise). While previous works have explored spurious features in LLMs,\nthey are limited to specific features (e.g., formats) and narrow scenarios\n(e.g., ICL). In this work, we statistically confirm the presence of spurious\nfeatures in the RAG paradigm, a robustness problem caused by the sensitivity of\nLLMs to semantic-agnostic features. Moreover, we provide a comprehensive\ntaxonomy of spurious features and empirically quantify their impact through\ncontrolled experiments. Further analysis reveals that not all spurious features\nare harmful and they can even be beneficial sometimes. Extensive evaluation\nresults across multiple LLMs suggest that spurious features are a widespread\nand challenging problem in the field of RAG. The code and dataset will be\nreleased to facilitate future research. We release all codes and data at:\n$\\\\\\href{https://github.com/maybenotime/RAG-SpuriousFeatures}{https://github.com/maybenotime/RAG-SpuriousFeatures}$."
    },
    {
        "date": "2025-03",
        "title": "Noise-Robust Radio Frequency Fingerprint Identification Using Denoise Diffusion Model",
        "author": "Guolin Yin, Junqing Zhang, Yuan Ding, and Simon Cotton",
        "link": "http://arxiv.org/abs/2503.05514v1",
        "abstract": "Securing Internet of Things (IoT) devices presents increasing challenges due\nto their limited computational and energy resources. Radio Frequency\nFingerprint Identification (RFFI) emerges as a promising authentication\ntechnique to identify wireless devices through hardware impairments. RFFI\nperformance under low signal-to-noise ratio (SNR) scenarios is significantly\ndegraded because the minute hardware features can be easily swamped in noise.\nIn this paper, we leveraged the diffusion model to effectively restore the RFF\nunder low SNR scenarios. Specifically, we trained a powerful noise predictor\nand tailored a noise removal algorithm to effectively reduce the noise level in\nthe received signal and restore the device fingerprints. We used Wi-Fi as a\ncase study and created a testbed involving 6 commercial off-the-shelf Wi-Fi\ndongles and a USRP N210 software-defined radio (SDR) platform. We conducted\nexperimental evaluations on various SNR scenarios. The experimental results\nshow that the proposed algorithm can improve the classification accuracy by up\nto 34.9%."
    },
    {
        "date": "2025-03",
        "title": "Enhancing Network Security: A Hybrid Approach for Detection and Mitigation of Distributed Denial-of-Service Attacks Using Machine Learning",
        "author": "Nizo Jaman Shohan, Gazi Tanbhir, Faria Elahi, Ahsan Ullah, and Md. Nazmus Sakib",
        "link": "http://arxiv.org/abs/2503.05477v1",
        "abstract": "The distributed denial-of-service (DDoS) attack stands out as a highly\nformidable cyber threat, representing an advanced form of the denial-of-service\n(DoS) attack. A DDoS attack involves multiple computers working together to\noverwhelm a system, making it unavailable. On the other hand, a DoS attack is a\none-on-one attempt to make a system or website inaccessible. Thus, it is\ncrucial to construct an effective model for identifying various DDoS incidents.\nAlthough extensive research has focused on binary detection models for DDoS\nidentification, they face challenges to adapt evolving threats, necessitating\nfrequent updates. Whereas multiclass detection models offer a comprehensive\ndefense against diverse DDoS attacks, ensuring adaptability in the\never-changing cyber threat landscape. In this paper, we propose a Hybrid Model\nto strengthen network security by combining the featureextraction abilities of\n1D Convolutional Neural Networks (CNNs) with the classification skills of\nRandom Forest (RF) and Multi-layer Perceptron (MLP) classifiers. Using the\nCIC-DDoS2019 dataset, we perform multiclass classification of various DDoS\nattacks and conduct a comparative analysis of evaluation metrics for RF, MLP,\nand our proposed Hybrid Model. After analyzing the results, we draw meaningful\nconclusions and confirm the superiority of our Hybrid Model by performing\nthorough cross-validation. Additionally, we integrate our machine learning\nmodel with Snort, which provides a robust and adaptive solution for detecting\nand mitigating various DDoS attacks."
    },
    {
        "date": "2025-03",
        "title": "This Is Your Doge, If It Please You: Exploring Deception and Robustness in Mixture of LLMs",
        "author": "Lorenz Wolf, Sangwoong Yoon, and Ilija Bogunovic",
        "link": "http://arxiv.org/abs/2503.05856v1",
        "abstract": "Mixture of large language model (LLMs) Agents (MoA) architectures achieve\nstate-of-the-art performance on prominent benchmarks like AlpacaEval 2.0 by\nleveraging the collaboration of multiple LLMs at inference time. Despite these\nsuccesses, an evaluation of the safety and reliability of MoA is missing. We\npresent the first comprehensive study of MoA's robustness against deceptive LLM\nagents that deliberately provide misleading responses. We examine factors like\nthe propagation of deceptive information, model size, and information\navailability, and uncover critical vulnerabilities. On AlpacaEval 2.0, the\npopular LLaMA 3.1-70B model achieves a length-controlled Win Rate (LC WR) of\n49.2% when coupled with 3-layer MoA (6 LLM agents). However, we demonstrate\nthat introducing only a $\\textit{single}$ carefully-instructed deceptive agent\ninto the MoA can reduce performance to 37.9%, effectively nullifying all MoA\ngains. On QuALITY, a multiple-choice comprehension task, the impact is also\nsevere, with accuracy plummeting by a staggering 48.5%. Inspired in part by the\nhistorical Doge of Venice voting process, designed to minimize influence and\ndeception, we propose a range of unsupervised defense mechanisms that recover\nmost of the lost performance."
    },
    {
        "date": "2025-03",
        "title": "Are Your LLM-based Text-to-SQL Models Secure? Exploring SQL Injection via Backdoor Attacks",
        "author": "Meiyu Lin, Haichuan Zhang, Jiale Lao, Renyuan Li, Yuanchun Zhou, Carl Yang, Yang Cao, and Mingjie Tang",
        "link": "http://arxiv.org/abs/2503.05445v1",
        "abstract": "Large language models (LLMs) have shown state-of-the-art results in\ntranslating natural language questions into SQL queries (Text-to-SQL), a\nlong-standing challenge within the database community. However, security\nconcerns remain largely unexplored, particularly the threat of backdoor\nattacks, which can introduce malicious behaviors into models through\nfine-tuning with poisoned datasets. In this work, we systematically investigate\nthe vulnerabilities of LLM-based Text-to-SQL models and present ToxicSQL, a\nnovel backdoor attack framework. Our approach leverages stealthy {semantic and\ncharacter-level triggers} to make backdoors difficult to detect and remove,\nensuring that malicious behaviors remain covert while maintaining high model\naccuracy on benign inputs. Furthermore, we propose leveraging SQL injection\npayloads as backdoor targets, enabling the generation of malicious yet\nexecutable SQL queries, which pose severe security and privacy risks in\nlanguage model-based SQL development. We demonstrate that injecting only 0.44%\nof poisoned data can result in an attack success rate of 79.41%, posing a\nsignificant risk to database security. Additionally, we propose detection and\nmitigation strategies to enhance model reliability. Our findings highlight the\nurgent need for security-aware Text-to-SQL development, emphasizing the\nimportance of robust defenses against backdoor threats."
    },
    {
        "date": "2025-03",
        "title": "An Empirical Study of Conformal Prediction in LLM with ASP Scaffolds for Robust Reasoning",
        "author": "Navdeep Kaur, Lachlan McPheat, Alessandra Russo, Anthony G Cohn, and Pranava Madhyastha",
        "link": "http://arxiv.org/abs/2503.05439v1",
        "abstract": "In this paper, we examine the use of Conformal Language Modelling (CLM)\nalongside Answer Set Programming (ASP) to enhance the performance of standard\nopen-weight LLMs on complex multi-step reasoning tasks. Using the StepGame\ndataset, which requires spatial reasoning, we apply CLM to generate sets of ASP\nprograms from an LLM, providing statistical guarantees on the correctness of\nthe outputs. Experimental results show that CLM significantly outperforms\nbaseline models that use standard sampling methods, achieving substantial\naccuracy improvements across different levels of reasoning complexity.\nAdditionally, the LLM-as-Judge metric enhances CLM's performance, especially in\nassessing structurally and logically correct ASP outputs. However, calibrating\nCLM with diverse calibration sets did not improve generalizability for tasks\nrequiring much longer reasoning steps, indicating limitations in handling more\ncomplex tasks."
    },
    {
        "date": "2025-03",
        "title": "Femur: A Flexible Framework for Fast and Secure Querying from Public Key-Value Store",
        "author": "Jiaoyi Zhang, Liqiang Peng, Mo Sha, Weiran Liu, Xiang Li, Sheng Wang, Feifei Li, Mingyu Gao, and Huanchen Zhang",
        "link": "http://arxiv.org/abs/2503.05376v1",
        "abstract": "With increasing demands for privacy, it becomes necessary to protect\nsensitive user query data when accessing public key-value databases. Existing\nPrivate Information Retrieval (PIR) schemes provide full security but suffer\nfrom poor scalability, limiting their applicability in large-scale deployment.\nWe argue that in many real-world scenarios, a more practical solution should\nallow users to flexibly determine the privacy levels of their queries in a\ntheoretically guided way, balancing security and performance based on specific\nneeds. To formally provide provable guarantees, we introduce a novel concept of\ndistance-based indistinguishability, which can facilitate users to comfortably\nrelax their security requirements. We then design Femur, an efficient framework\nto securely query public key-value stores with flexible security and\nperformance trade-offs. It uses a space-efficient learned index to convert\nquery keys into storage locations, obfuscates these locations with extra noise\nprovably derived by the distance-based indistinguishability theory, and sends\nthe expanded range to the server. The server then adaptively utilizes the best\nscheme to retrieve data. We also propose a novel variable-range PIR scheme\noptimized for bandwidth-constrained environments. Experiments show that Femur\noutperforms the state-of-the-art designs even when ensuring the same full\nsecurity level. When users are willing to relax their privacy requirements,\nFemur can further improve the performance gains to up to 163.9X, demonstrating\nan effective trade-off between security and performance."
    },
    {
        "date": "2025-03",
        "title": "Shifting Perspectives: Steering Vector Ensembles for Robust Bias Mitigation in LLMs",
        "author": "Zara Siddique, Irtaza Khalid, Liam D. Turner, and Luis Espinosa-Anke",
        "link": "http://arxiv.org/abs/2503.05371v1",
        "abstract": "We present a novel approach to bias mitigation in large language models\n(LLMs) by applying steering vectors to modify model activations in forward\npasses. We employ Bayesian optimization to systematically identify effective\ncontrastive pair datasets across nine bias axes. When optimized on the BBQ\ndataset, our individually tuned steering vectors achieve average improvements\nof 12.2%, 4.7%, and 3.2% over the baseline for Mistral, Llama, and Qwen,\nrespectively. Building on these promising results, we introduce Steering Vector\nEnsembles (SVE), a method that averages multiple individually optimized\nsteering vectors, each targeting a specific bias axis such as age, race, or\ngender. By leveraging their collective strength, SVE outperforms individual\nsteering vectors in both bias reduction and maintaining model performance. The\nwork presents the first systematic investigation of steering vectors for bias\nmitigation, and we demonstrate that SVE is a powerful and computationally\nefficient strategy for reducing bias in LLMs, with broader implications for\nenhancing AI safety."
    },
    {
        "date": "2025-03",
        "title": "Pretext Task Adversarial Learning for Unpaired Low-field to Ultra High-field MRI Synthesis",
        "author": "Zhenxuan Zhang, Peiyuan Jing, Coraline Beitone, Jiahao Huang, Zhifan Gao, Guang Yang, and Pete Lally",
        "link": "http://arxiv.org/abs/2503.05339v1",
        "abstract": "Given the scarcity and cost of high-field MRI, the synthesis of high-field\nMRI from low-field MRI holds significant potential when there is limited data\nfor training downstream tasks (e.g. segmentation). Low-field MRI often suffers\nfrom a reduced signal-to-noise ratio (SNR) and spatial resolution compared to\nhigh-field MRI. However, synthesizing high-field MRI data presents challenges.\nThese involve aligning image features across domains while preserving\nanatomical accuracy and enhancing fine details. To address these challenges, we\npropose a Pretext Task Adversarial (PTA) learning framework for high-field MRI\nsynthesis from low-field MRI data. The framework comprises three processes: (1)\nThe slice-wise gap perception (SGP) network aligns the slice inconsistencies of\nlow-field and high-field datasets based on contrastive learning. (2) The local\nstructure correction (LSC) network extracts local structures by restoring the\nlocally rotated and masked images. (3) The pretext task-guided adversarial\ntraining process introduces additional supervision and incorporates a\ndiscriminator to improve image realism. Extensive experiments on low-field to\nultra high-field task demonstrate the effectiveness of our method, achieving\nstate-of-the-art performance (16.892 in FID, 1.933 in IS, and 0.324 in\nMS-SSIM). This enables the generation of high-quality high-field-like MRI data\nfrom low-field MRI data to augment training datasets for downstream tasks. The\ncode is available at:\nhttps://github.com/Zhenxuan-Zhang/PTA4Unpaired_HF_MRI_SYN."
    },
    {
        "date": "2025-03",
        "title": "Robust Multimodal Learning for Ophthalmic Disease Grading via Disentangled Representation",
        "author": "Xinkun Wang, Yifang Wang, Senwei Liang, Feilong Tang, Chengzhi Liu, Ming Hu, Chao Hu, Junjun He, Zongyuan Ge, and Imran Razzak",
        "link": "http://arxiv.org/abs/2503.05319v1",
        "abstract": "This paper discusses how ophthalmologists often rely on multimodal data to\nimprove diagnostic accuracy. However, complete multimodal data is rare in\nreal-world applications due to a lack of medical equipment and concerns about\ndata privacy. Traditional deep learning methods typically address these issues\nby learning representations in latent space. However, the paper highlights two\nkey limitations of these approaches: (i) Task-irrelevant redundant information\n(e.g., numerous slices) in complex modalities leads to significant redundancy\nin latent space representations. (ii) Overlapping multimodal representations\nmake it difficult to extract unique features for each modality. To overcome\nthese challenges, the authors propose the Essence-Point and Disentangle\nRepresentation Learning (EDRL) strategy, which integrates a self-distillation\nmechanism into an end-to-end framework to enhance feature selection and\ndisentanglement for more robust multimodal learning. Specifically, the\nEssence-Point Representation Learning module selects discriminative features\nthat improve disease grading performance. The Disentangled Representation\nLearning module separates multimodal data into modality-common and\nmodality-unique representations, reducing feature entanglement and enhancing\nboth robustness and interpretability in ophthalmic disease diagnosis.\nExperiments on multimodal ophthalmology datasets show that the proposed EDRL\nstrategy significantly outperforms current state-of-the-art methods."
    },
    {
        "date": "2025-03",
        "title": "Adversarial Policy Optimization for Offline Preference-based Reinforcement Learning",
        "author": "Hyungkyu Kang, and Min-hwan Oh",
        "link": "http://arxiv.org/abs/2503.05306v1",
        "abstract": "In this paper, we study offline preference-based reinforcement learning\n(PbRL), where learning is based on pre-collected preference feedback over pairs\nof trajectories. While offline PbRL has demonstrated remarkable empirical\nsuccess, existing theoretical approaches face challenges in ensuring\nconservatism under uncertainty, requiring computationally intractable\nconfidence set constructions. We address this limitation by proposing\nAdversarial Preference-based Policy Optimization (APPO), a computationally\nefficient algorithm for offline PbRL that guarantees sample complexity bounds\nwithout relying on explicit confidence sets. By framing PbRL as a two-player\ngame between a policy and a model, our approach enforces conservatism in a\ntractable manner. Using standard assumptions on function approximation and\nbounded trajectory concentrability, we derive a sample complexity bound. To our\nknowledge, APPO is the first offline PbRL algorithm to offer both statistical\nefficiency and practical applicability. Experimental results on continuous\ncontrol tasks demonstrate that APPO effectively learns from complex datasets,\nshowing comparable performance with existing state-of-the-art methods."
    },
    {
        "date": "2025-03",
        "title": "Robust Intrusion Detection System with Explainable Artificial Intelligence",
        "author": "Bet\u00fcl G\u00fcven\u00e7 Paltun, Ramin Fuladi, and Rim El Malki",
        "link": "http://arxiv.org/abs/2503.05303v1",
        "abstract": "Machine learning (ML) models serve as powerful tools for threat detection and\nmitigation; however, they also introduce potential new risks. Adversarial input\ncan exploit these models through standard interfaces, thus creating new attack\npathways that threaten critical network operations. As ML advancements\nprogress, adversarial strategies become more advanced, and conventional\ndefenses such as adversarial training are costly in computational terms and\noften fail to provide real-time detection. These methods typically require a\nbalance between robustness and model performance, which presents challenges for\napplications that demand instant response. To further investigate this\nvulnerability, we suggest a novel strategy for detecting and mitigating\nadversarial attacks using eXplainable Artificial Intelligence (XAI). This\napproach is evaluated in real time within intrusion detection systems (IDS),\nleading to the development of a zero-touch mitigation strategy. Additionally,\nwe explore various scenarios in the Radio Resource Control (RRC) layer within\nthe Open Radio Access Network (O-RAN) framework, emphasizing the critical need\nfor enhanced mitigation techniques to strengthen IDS defenses against advanced\nthreats and implement a zero-touch mitigation solution. Extensive testing\nacross different scenarios in the RRC layer of the O-RAN infrastructure\nvalidates the ability of the framework to detect and counteract integrated\nRRC-layer attacks when paired with adversarial strategies, emphasizing the\nessential need for robust defensive mechanisms to strengthen IDS against\ncomplex threats."
    },
    {
        "date": "2025-03",
        "title": "ColFigPhotoAttnNet: Reliable Finger Photo Presentation Attack Detection Leveraging Window-Attention on Color Spaces",
        "author": "Anudeep Vurity, Emanuela Marasco, Raghavendra Ramachandra, and Jongwoo Park",
        "link": "http://arxiv.org/abs/2503.05247v1",
        "abstract": "Finger photo Presentation Attack Detection (PAD) can significantly strengthen\nsmartphone device security. However, these algorithms are trained to detect\ncertain types of attacks. Furthermore, they are designed to operate on images\nacquired by specific capture devices, leading to poor generalization and a lack\nof robustness in handling the evolving nature of mobile hardware. The proposed\ninvestigation is the first to systematically analyze the performance\ndegradation of existing deep learning PAD systems, convolutional and\ntransformers, in cross-capture device settings. In this paper, we introduce the\nColFigPhotoAttnNet architecture designed based on window attention on color\nchannels, followed by the nested residual network as the predictor to achieve a\nreliable PAD. Extensive experiments using various capture devices, including\niPhone13 Pro, GooglePixel 3, Nokia C5, and OnePlusOne, were carried out to\nevaluate the performance of proposed and existing methods on three publicly\navailable databases. The findings underscore the effectiveness of our approach."
    },
    {
        "date": "2025-03",
        "title": "Robust Conformal Prediction with a Single Binary Certificate",
        "author": "Soroush H. Zargarbashi, and Aleksandar Bojchevski",
        "link": "http://arxiv.org/abs/2503.05239v1",
        "abstract": "Conformal prediction (CP) converts any model's output to prediction sets with\na guarantee to cover the true label with (adjustable) high probability. Robust\nCP extends this guarantee to worst-case (adversarial) inputs. Existing\nbaselines achieve robustness by bounding randomly smoothed conformity scores.\nIn practice, they need expensive Monte-Carlo (MC) sampling (e.g. $\\sim10^4$\nsamples per point) to maintain an acceptable set size. We propose a robust\nconformal prediction that produces smaller sets even with significantly lower\nMC samples (e.g. 150 for CIFAR10). Our approach binarizes samples with an\nadjustable (or automatically adjusted) threshold selected to preserve the\ncoverage guarantee. Remarkably, we prove that robustness can be achieved by\ncomputing only one binary certificate, unlike previous methods that certify\neach calibration (or test) point. Thus, our method is faster and returns\nsmaller robust sets. We also eliminate a previous limitation that requires a\nbounded score function."
    },
    {
        "date": "2025-03",
        "title": "Reward-Centered ReST-MCTS: A Robust Decision-Making Framework for Robotic Manipulation in High Uncertainty Environments",
        "author": "Xibai Wang",
        "link": "http://arxiv.org/abs/2503.05226v1",
        "abstract": "Monte Carlo Tree Search (MCTS) has emerged as a powerful tool for\ndecision-making in robotics, enabling efficient exploration of large search\nspaces. However, traditional MCTS methods struggle in environments\ncharacterized by high uncertainty and noisy data due to their reliance on\nfinal-step reward evaluation. The lack of intermediate feedback during search\noften results in suboptimal decision-making and computational inefficiencies.\n  This paper introduces Reward-Centered ReST-MCTS, a novel framework that\nenhances MCTS by incorporating intermediate reward shaping. The core of our\napproach is the Rewarding Center, which refines search trajectories by\ndynamically assigning partial rewards using rule-based validation, heuristic\nguidance, and neural estimation. By integrating these mechanisms, our method\nenables real-time optimization of search paths, mitigating the effects of error\npropagation.\n  We evaluate Reward-Centered ReST-MCTS in robotic manipulation tasks under\nhigh uncertainty, demonstrating consistent improvements in decision accuracy.\nCompared to baseline methods, including Chain-of-Thought (CoT) prompting and\nVanilla ReST-MCTS, our framework achieves a 2-4% accuracy improvement while\nmaintaining computational feasibility. Ablation studies confirm the\neffectiveness of intermediate feedback in search refinement, particularly in\npruning incorrect decision paths early. Furthermore, robustness tests show that\nour method retains high performance across varying levels of uncertainty."
    },
    {
        "date": "2025-03",
        "title": "Robustness of Generalized Median Computation for Consensus Learning in Arbitrary Spaces",
        "author": "Andreas Nienk\u00f6tter, Sandro Vega-Pons, and Xiaoyi Jiang",
        "link": "http://arxiv.org/abs/2503.05215v1",
        "abstract": "Robustness in terms of outliers is an important topic and has been formally\nstudied for a variety of problems in machine learning and computer vision.\nGeneralized median computation is a special instance of consensus learning and\na common approach to finding prototypes. Related research can be found in\nnumerous problem domains with a broad range of applications. So far, however,\nrobustness of generalized median has only been studied in a few specific\nspaces. To our knowledge, there is no robustness characterization in a general\nsetting, i.e. for arbitrary spaces. We address this open issue in our work. The\nbreakdown point >=0.5 is proved for generalized median with metric distance\nfunctions in general. We also study the detailed behavior in case of outliers\nfrom different perspectives. In addition, we present robustness results for\nweighted generalized median computation and non-metric distance functions.\nGiven the importance of robustness, our work contributes to closing a gap in\nthe literature. The presented results have general impact and applicability,\ne.g. providing deeper understanding of generalized median computation and\npractical guidance to avoid non-robust computation."
    },
    {
        "date": "2025-03",
        "title": "Safety-Critical Traffic Simulation with Adversarial Transfer of Driving Intentions",
        "author": "Zherui Huang, Xing Gao, Guanjie Zheng, Licheng Wen, Xuemeng Yang, and Xiao Sun",
        "link": "http://arxiv.org/abs/2503.05180v1",
        "abstract": "Traffic simulation, complementing real-world data with a long-tail\ndistribution, allows for effective evaluation and enhancement of the ability of\nautonomous vehicles to handle accident-prone scenarios. Simulating such\nsafety-critical scenarios is nontrivial, however, from log data that are\ntypically regular scenarios, especially in consideration of dynamic adversarial\ninteractions between the future motions of autonomous vehicles and surrounding\ntraffic participants. To address it, this paper proposes an innovative and\nefficient strategy, termed IntSim, that explicitly decouples the driving\nintentions of surrounding actors from their motion planning for realistic and\nefficient safety-critical simulation. We formulate the adversarial transfer of\ndriving intention as an optimization problem, facilitating extensive\nexploration of diverse attack behaviors and efficient solution convergence.\nSimultaneously, intention-conditioned motion planning benefits from powerful\ndeep models and large-scale real-world data, permitting the simulation of\nrealistic motion behaviors for actors. Specially, through adapting driving\nintentions based on environments, IntSim facilitates the flexible realization\nof dynamic adversarial interactions with autonomous vehicles. Finally,\nextensive open-loop and closed-loop experiments on real-world datasets,\nincluding nuScenes and Waymo, demonstrate that the proposed IntSim achieves\nstate-of-the-art performance in simulating realistic safety-critical scenarios\nand further improves planners in handling such scenarios."
    },
    {
        "date": "2025-03",
        "title": "Self-Supervised Penalty-Based Learning for Robust Constrained Optimization",
        "author": "Wyame Benslimane, and Paul Grigas",
        "link": "http://arxiv.org/abs/2503.05175v1",
        "abstract": "We propose a new methodology for parameterized constrained robust\noptimization, an important class of optimization problems under uncertainty,\nbased on learning with a self-supervised penalty-based loss function. Whereas\nsupervised learning requires pre-solved instances for training, our approach\nleverages a custom loss function derived from the exact penalty method in\noptimization to learn an approximation, typically defined by a neural network\nmodel, of the parameterized optimal solution mapping. Additionally, we adapt\nour approach to robust constrained combinatorial optimization problems by\nincorporating a surrogate linear cost over mixed integer domains, and a smooth\napproximations thereof, into the final layer of the network architecture. We\nperform computational experiments to test our approach on three different\napplications: multidimensional knapsack with continuous variables,\ncombinatorial multidimensional knapsack with discrete variables, and an\ninventory management problem. Our results demonstrate that our self-supervised\napproach is able to effectively learn neural network approximations whose\ninference time is significantly smaller than the computation time of\ntraditional solvers for this class of robust optimization problems.\nFurthermore, our results demonstrate that by varying the penalty parameter we\nare able to effectively balance the trade-off between sub-optimality and robust\nfeasibility of the obtained solutions."
    },
    {
        "date": "2025-03",
        "title": "GaussianCAD: Robust Self-Supervised CAD Reconstruction from Three Orthographic Views Using 3D Gaussian Splatting",
        "author": "Zheng Zhou, Zhe Li, Bo Yu, Lina Hu, Liang Dong, Zijian Yang, Xiaoli Liu, Ning Xu, Ziwei Wang, Yonghao Dang, and Jianqin Yin",
        "link": "http://arxiv.org/abs/2503.05161v1",
        "abstract": "The automatic reconstruction of 3D computer-aided design (CAD) models from\nCAD sketches has recently gained significant attention in the computer vision\ncommunity. Most existing methods, however, rely on vector CAD sketches and 3D\nground truth for supervision, which are often difficult to be obtained in\nindustrial applications and are sensitive to noise inputs. We propose viewing\nCAD reconstruction as a specific instance of sparse-view 3D reconstruction to\novercome these limitations. While this reformulation offers a promising\nperspective, existing 3D reconstruction methods typically require natural\nimages and corresponding camera poses as inputs, which introduces two major\nsignificant challenges: (1) modality discrepancy between CAD sketches and\nnatural images, and (2) difficulty of accurate camera pose estimation for CAD\nsketches. To solve these issues, we first transform the CAD sketches into\nrepresentations resembling natural images and extract corresponding masks.\nNext, we manually calculate the camera poses for the orthographic views to\nensure accurate alignment within the 3D coordinate system. Finally, we employ a\ncustomized sparse-view 3D reconstruction method to achieve high-quality\nreconstructions from aligned orthographic views. By leveraging raster CAD\nsketches for self-supervision, our approach eliminates the reliance on vector\nCAD sketches and 3D ground truth. Experiments on the Sub-Fusion360 dataset\ndemonstrate that our proposed method significantly outperforms previous\napproaches in CAD reconstruction performance and exhibits strong robustness to\nnoisy inputs."
    },
    {
        "date": "2025-03",
        "title": "Continual Pre-training of MoEs: How robust is your router?",
        "author": "Benjamin Th\u00e9rien, Charles-\u00c9tienne Joseph, Zain Sarwar, Ashwinee Panda, Anirban Das, Shi-Xiong Zhang, Stephen Rawls, Sambit Sahu, Eugene Belilovsky, and Irina Rish",
        "link": "http://arxiv.org/abs/2503.05029v1",
        "abstract": "Sparsely-activated Mixture of Experts (MoE) transformers are promising\narchitectures for foundation models. Compared to dense transformers that\nrequire the same amount of floating point operations (FLOPs) per forward pass,\nMoEs benefit from improved sample efficiency at training time and achieve much\nstronger performance. Many closed-source and open-source frontier language\nmodels have thus adopted an MoE architecture. Naturally, practitioners will\nwant to extend the capabilities of these models with large amounts of newly\ncollected data without completely re-training them. Prior work has shown that a\nsimple combination of replay and learning rate re-warming and re-decaying can\nenable the continual pre-training (CPT) of dense decoder-only transformers with\nminimal performance degradation compared to full re-training. In the case of\ndecoder-only MoE transformers, however, it is unclear how the routing algorithm\nwill impact continual pre-training performance: 1) do the MoE transformer's\nrouters exacerbate forgetting relative to a dense model?; 2) do the routers\nmaintain a balanced load on previous distributions after CPT?; 3) are the same\nstrategies applied to dense models sufficient to continually pre-train MoE\nLLMs? In what follows, we conduct a large-scale (>2B parameter switch and\nDeepSeek MoE LLMs trained for 600B tokens) empirical study across four MoE\ntransformers to answer these questions. Our results establish a surprising\nrobustness to distribution shifts for both Sinkhorn-Balanced and\nZ-and-Aux-loss-balanced routing algorithms, even in MoEs continually\npre-trained without replay. Moreover, we show that MoE LLMs maintain their\nsample efficiency (relative to a FLOP-matched dense model) during CPT and that\nthey can match the performance of a fully re-trained MoE at a fraction of the\ncost."
    },
    {
        "date": "2025-03",
        "title": "Energy-Latency Attacks: A New Adversarial Threat to Deep Learning",
        "author": "Hanene F. Z. Brachemi Meftah, Wassim Hamidouche, Sid Ahmed Fezza, and Olivier Deforges",
        "link": "http://arxiv.org/abs/2503.04963v1",
        "abstract": "The growing computational demand for deep neural networks ( DNNs) has raised\nconcerns about their energy consumption and carbon footprint, particularly as\nthe size and complexity of the models continue to increase. To address these\nchallenges, energy-efficient hardware and custom accelerators have become\nessential. Additionally, adaptable DNN s are being developed to dynamically\nbalance performance and efficiency. The use of these strategies became more\ncommon to enable sustainable AI deployment. However, these efficiency-focused\ndesigns may also introduce vulnerabilities, as attackers can potentially\nexploit them to increase latency and energy usage by triggering their\nworst-case-performance scenarios. This new type of attack, called\nenergy-latency attacks, has recently gained significant research attention,\nfocusing on the vulnerability of DNN s to this emerging attack paradigm, which\ncan trigger denial-of-service ( DoS) attacks. This paper provides a\ncomprehensive overview of current research on energy-latency attacks,\ncategorizing them using the established taxonomy for traditional adversarial\nattacks. We explore different metrics used to measure the success of these\nattacks and provide an analysis and comparison of existing attack strategies.\nWe also analyze existing defense mechanisms and highlight current challenges\nand potential areas for future research in this developing field. The GitHub\npage for this work can be accessed at\nhttps://github.com/hbrachemi/Survey_energy_attacks/"
    },
    {
        "date": "2025-03",
        "title": "Security-Aware Sensor Fusion with MATE: the Multi-Agent Trust Estimator",
        "author": "R. Spencer Hallyburton, and Miroslav Pajic",
        "link": "http://arxiv.org/abs/2503.04954v1",
        "abstract": "Lacking security awareness, sensor fusion in systems with multi-agent\nnetworks such as smart cities is vulnerable to attacks. To guard against recent\nthreats, we design security-aware sensor fusion that is based on the estimates\nof distributions over trust. Trust estimation can be cast as a hidden Markov\nmodel, and we solve it by mapping sensor data to trust pseudomeasurements\n(PSMs) that recursively update trust posteriors in a Bayesian context. Trust\nthen feeds sensor fusion to facilitate trust-weighted updates to situational\nawareness. Essential to security-awareness are a novel field of view estimator,\nlogic to map sensor data into PSMs, and the derivation of efficient Bayesian\nupdates. We evaluate security-aware fusion under attacks on agents using case\nstudies and Monte Carlo simulation in the physics-based Unreal Engine\nsimulator, CARLA. A mix of novel and classical security-relevant metrics show\nthat our security-aware fusion enables building trustworthy situational\nawareness even in hostile conditions."
    },
    {
        "date": "2025-03",
        "title": "Spectral Informed Mamba for Robust Point Cloud Processing",
        "author": "Ali Bahri, Moslem Yazdanpanah, Mehrdad Noori, Sahar Dastani, Milad Cheraghalikhani, David Osowiechi, Gustavo Adolfo Vargas Hakim, Farzad Beizaee, Ismail Ben Ayed, and Christian Desrosiers",
        "link": "http://arxiv.org/abs/2503.04953v1",
        "abstract": "State space models have shown significant promise in Natural Language\nProcessing (NLP) and, more recently, computer vision. This paper introduces a\nnew methodology leveraging Mamba and Masked Autoencoder networks for point\ncloud data in both supervised and self-supervised learning. We propose three\nkey contributions to enhance Mamba's capability in processing complex point\ncloud structures. First, we exploit the spectrum of a graph Laplacian to\ncapture patch connectivity, defining an isometry-invariant traversal order that\nis robust to viewpoints and better captures shape manifolds than traditional 3D\ngrid-based traversals. Second, we adapt segmentation via a recursive patch\npartitioning strategy informed by Laplacian spectral components, allowing finer\nintegration and segment analysis. Third, we address token placement in Masked\nAutoencoder for Mamba by restoring tokens to their original positions, which\npreserves essential order and improves learning. Extensive experiments\ndemonstrate the improvements of our approach in classification, segmentation,\nand few-shot tasks over state-of-the-art baselines."
    },
    {
        "date": "2025-03",
        "title": "Fundamental Limits of Hierarchical Secure Aggregation with Cyclic User Association",
        "author": "Xiang Zhang, Zhou Li, Kai Wan, Hua Sun, Mingyue Ji, and Giuseppe Caire",
        "link": "http://arxiv.org/abs/2503.04564v2",
        "abstract": "Secure aggregation is motivated by federated learning (FL) where a cloud\nserver aims to compute an averaged model (i.e., weights of deep neural\nnetworks) of the locally-trained models of numerous clients, while adhering to\ndata security requirements. Hierarchical secure aggregation (HSA) extends this\nconcept to a three-layer network, where clustered users communicate with the\nserver through an intermediate layer of relays. In HSA, beyond conventional\nserver security, relay security is also enforced to ensure that the relays\nremain oblivious to the users' inputs (an abstraction of the local models in\nFL). Existing study on HSA assumes that each user is associated with only one\nrelay, limiting opportunities for coding across inter-cluster users to achieve\nefficient communication and key generation. In this paper, we consider HSA with\na cyclic association pattern where each user is connected to $B$ consecutive\nrelays in a wrap-around manner. We propose an efficient aggregation scheme\nwhich includes a message design for the inputs inspired by gradient coding-a\nwell-known technique for efficient communication in distributed computing-along\nwith a highly nontrivial security key design. We also derive novel converse\nbounds on the minimum achievable communication and key rates using\ninformation-theoretic arguments."
    },
    {
        "date": "2025-03",
        "title": "Benchmarking Reasoning Robustness in Large Language Models",
        "author": "Tong Yu, Yongcheng Jing, Xikun Zhang, Wentao Jiang, Wenjie Wu, Yingjie Wang, Wenbin Hu, Bo Du, and Dacheng Tao",
        "link": "http://arxiv.org/abs/2503.04550v1",
        "abstract": "Despite the recent success of large language models (LLMs) in reasoning such\nas DeepSeek, we for the first time identify a key dilemma in reasoning\nrobustness and generalization: significant performance degradation on novel or\nincomplete data, suggesting a reliance on memorized patterns rather than\nsystematic reasoning. Our closer examination reveals four key unique\nlimitations underlying this issue:(1) Positional bias--models favor earlier\nqueries in multi-query inputs but answering the wrong one in the latter (e.g.,\nGPT-4o's accuracy drops from 75.8 percent to 72.8 percent); (2) Instruction\nsensitivity--performance declines by 5.0 to 7.5 percent in the Qwen2.5 Series\nand by 5.0 percent in DeepSeek-V3 with auxiliary guidance; (3) Numerical\nfragility--value substitution sharply reduces accuracy (e.g., GPT-4o drops from\n97.5 percent to 82.5 percent, GPT-o1-mini drops from 97.5 percent to 92.5\npercent); and (4) Memory dependence--models resort to guesswork when missing\ncritical data. These findings further highlight the reliance on heuristic\nrecall over rigorous logical inference, demonstrating challenges in reasoning\nrobustness. To comprehensively investigate these robustness challenges, this\npaper introduces a novel benchmark, termed as Math-RoB, that exploits\nhallucinations triggered by missing information to expose reasoning gaps. This\nis achieved by an instruction-based approach to generate diverse datasets that\nclosely resemble training distributions, facilitating a holistic robustness\nassessment and advancing the development of more robust reasoning frameworks.\nBad character(s) in field Abstract."
    },
    {
        "date": "2025-03",
        "title": "Know Thy Judge: On the Robustness Meta-Evaluation of LLM Safety Judges",
        "author": "Francisco Eiras, Eliott Zemour, Eric Lin, and Vaikkunth Mugunthan",
        "link": "http://arxiv.org/abs/2503.04474v1",
        "abstract": "Large Language Model (LLM) based judges form the underpinnings of key safety\nevaluation processes such as offline benchmarking, automated red-teaming, and\nonline guardrailing. This widespread requirement raises the crucial question:\ncan we trust the evaluations of these evaluators? In this paper, we highlight\ntwo critical challenges that are typically overlooked: (i) evaluations in the\nwild where factors like prompt sensitivity and distribution shifts can affect\nperformance and (ii) adversarial attacks that target the judge. We highlight\nthe importance of these through a study of commonly used safety judges, showing\nthat small changes such as the style of the model output can lead to jumps of\nup to 0.24 in the false negative rate on the same dataset, whereas adversarial\nattacks on the model generation can fool some judges into misclassifying 100%\nof harmful generations as safe ones. These findings reveal gaps in commonly\nused meta-evaluation benchmarks and weaknesses in the robustness of current LLM\njudges, indicating that low attack success under certain judges could create a\nfalse sense of security."
    },
    {
        "date": "2025-03",
        "title": "Privacy Preserving and Robust Aggregation for Cross-Silo Federated Learning in Non-IID Settings",
        "author": "Marco Arazzi, Mert Cihangiroglu, and Antonino Nocera",
        "link": "http://arxiv.org/abs/2503.04451v1",
        "abstract": "Federated Averaging remains the most widely used aggregation strategy in\nfederated learning due to its simplicity and scalability. However, its\nperformance degrades significantly in non-IID data settings, where client\ndistributions are highly imbalanced or skewed. Additionally, it relies on\nclients transmitting metadata, specifically the number of training samples,\nwhich introduces privacy risks and may conflict with regulatory frameworks like\nthe European GDPR. In this paper, we propose a novel aggregation strategy that\naddresses these challenges by introducing class-aware gradient masking. Unlike\ntraditional approaches, our method relies solely on gradient updates,\neliminating the need for any additional client metadata, thereby enhancing\nprivacy protection. Furthermore, our approach validates and dynamically weights\nclient contributions based on class-specific importance, ensuring robustness\nagainst non-IID distributions, convergence prevention, and backdoor attacks.\nExtensive experiments on benchmark datasets demonstrate that our method not\nonly outperforms FedAvg and other widely accepted aggregation strategies in\nnon-IID settings but also preserves model integrity in adversarial scenarios.\nOur results establish the effectiveness of gradient masking as a practical and\nsecure solution for federated learning."
    },
    {
        "date": "2025-03",
        "title": "Scale-Invariant Adversarial Attack against Arbitrary-scale Super-resolution",
        "author": "Yihao Huang, Xin Luo, Qing Guo, Felix Juefei-Xu, Xiaojun Jia, Weikai Miao, Geguang Pu, and Yang Liu",
        "link": "http://arxiv.org/abs/2503.04385v2",
        "abstract": "The advent of local continuous image function (LIIF) has garnered significant\nattention for arbitrary-scale super-resolution (SR) techniques. However, while\nthe vulnerabilities of fixed-scale SR have been assessed, the robustness of\ncontinuous representation-based arbitrary-scale SR against adversarial attacks\nremains an area warranting further exploration. The elaborately designed\nadversarial attacks for fixed-scale SR are scale-dependent, which will cause\ntime-consuming and memory-consuming problems when applied to arbitrary-scale\nSR. To address this concern, we propose a simple yet effective\n``scale-invariant'' SR adversarial attack method with good transferability,\ntermed SIAGT. Specifically, we propose to construct resource-saving attacks by\nexploiting finite discrete points of continuous representation. In addition, we\nformulate a coordinate-dependent loss to enhance the cross-model\ntransferability of the attack. The attack can significantly deteriorate the SR\nimages while introducing imperceptible distortion to the targeted\nlow-resolution (LR) images. Experiments carried out on three popular LIIF-based\nSR approaches and four classical SR datasets show remarkable attack performance\nand transferability of SIAGT."
    },
    {
        "date": "2025-03",
        "title": "Security and Real-time FPGA integration for Learned Image Compression",
        "author": "Alaa Mazouz, Carl De Sousa Tria, Sumanta Chaudhuri, Attilio Fiandrotti, Marco Cagnanzzo, Mihai Mitrea, and Enzo Tartaglione",
        "link": "http://arxiv.org/abs/2503.04867v2",
        "abstract": "Learnable Image Compression (LIC) has proven capable of outperforming\nstandardized video codecs in compression efficiency. However, achieving both\nreal-time and secure LIC operations on hardware presents significant conceptual\nand methodological challenges. The present work addresses these challenges by\nproviding an integrated workflow and platform for training, securing, and\ndeploying LIC models on hardware. To this end, a hardware-friendly LIC model is\nobtained by iteratively pruning and quantizing the model within a standard\nend-to-end learning framework. Notably, we introduce a novel Quantization-Aware\nWatermarking (QAW) technique, where the model is watermarked during\nquantization using a joint loss function, ensuring robust security without\ncompromising model performance. The watermarked weights are then public-key\nencrypted, guaranteeing both content protection and user traceability.\nExperimental results across different FPGA platforms evaluate real-time\nperformance, latency, energy consumption, and compression efficiency. The\nfindings highlight that the watermarking and encryption processes maintain\nnegligible impact on compression efficiency (average of -0.4 PSNR) and energy\nconsumption (average of +2%), while still meeting real-time constraints and\npreserving security properties."
    },
    {
        "date": "2025-03",
        "title": "Provable Robust Overfitting Mitigation in Wasserstein Distributionally Robust Optimization",
        "author": "Shuang Liu, Yihan Wang, Yifan Zhu, Yibo Miao, and Xiao-Shan Gao",
        "link": "http://arxiv.org/abs/2503.04315v1",
        "abstract": "Wasserstein distributionally robust optimization (WDRO) optimizes against\nworst-case distributional shifts within a specified uncertainty set, leading to\nenhanced generalization on unseen adversarial examples, compared to standard\nadversarial training which focuses on pointwise adversarial perturbations.\nHowever, WDRO still suffers fundamentally from the robust overfitting problem,\nas it does not consider statistical error. We address this gap by proposing a\nnovel robust optimization framework under a new uncertainty set for adversarial\nnoise via Wasserstein distance and statistical error via Kullback-Leibler\ndivergence, called the Statistically Robust WDRO. We establish a robust\ngeneralization bound for the new optimization framework, implying that\nout-of-distribution adversarial performance is at least as good as the\nstatistically robust training loss with high probability. Furthermore, we\nderive conditions under which Stackelberg and Nash equilibria exist between the\nlearner and the adversary, giving an optimal robust model in certain sense.\nFinally, through extensive experiments, we demonstrate that our method\nsignificantly mitigates robust overfitting and enhances robustness within the\nframework of WDRO."
    },
    {
        "date": "2025-03",
        "title": "No Silver Bullet: Towards Demonstrating Secure Software Development for Danish Small and Medium Enterprises in a Business-to-Business Model",
        "author": "Raha Asadi, Bodil Biering, Vincent van Dijk, Oksana Kulyk, and Elda Paja",
        "link": "http://arxiv.org/abs/2503.04293v1",
        "abstract": "Software developing small and medium enterprises (SMEs) play a crucial role\nas suppliers to larger corporations and public administration. It is therefore\nnecessary for them to be able to demonstrate that their products meet certain\nsecurity criteria, both to gain trust of their customers and to comply to\nstandards that demand such a demonstration. In this study we have investigated\nways for SMEs to demonstrate their security when operating in a\nbusiness-to-business model, conducting semi-structured interviews (N=16) with\npractitioners from different SMEs in Denmark and validating our findings in a\nfollow-up workshop (N=6). Our findings indicate five distinctive security\ndemonstration approaches, namely: Certifications, Reports, Questionnaires,\nInteractive Sessions and Social Proof. We discuss the challenges, benefits, and\nrecommendations related to these approaches, concluding that none of them is a\none-size-fits all solution and that more research into relative advantages of\nthese approaches and their combinations is needed."
    },
    {
        "date": "2025-03",
        "title": "Got Ya! -- Sensors for Identity Management Specific Security Situational Awareness",
        "author": "Daniela P\u00f6hn, and Heiner L\u00fcken",
        "link": "http://arxiv.org/abs/2503.04274v1",
        "abstract": "Security situational awareness refers to identifying, mitigating, and\npreventing digital cyber threats by gathering information to understand the\ncurrent situation. With awareness, the basis for decisions is present,\nparticularly in complex situations. However, while logging can track the\nsuccessful login into a system, it typically cannot determine if the login was\nperformed by the user assigned to the account. An account takeover, for\nexample, by a successful phishing attack, can be used as an entry into an\norganization's network. All identities within an organization are managed in an\nidentity management system. Thereby, these systems are an interesting goal for\nmalicious actors. Even within identity management systems, it is difficult to\ndifferentiate legitimate from malicious actions. We propose a security\nsituational awareness approach specifically to identity management. We focus on\nprotocol-specifics and identity-related sources in a general concept before\nproviding the example of the protocol OAuth with a proof-of-concept\nimplementation."
    },
    {
        "date": "2025-03",
        "title": "UniNet: A Unified Multi-granular Traffic Modeling Framework for Network Security",
        "author": "Binghui Wu, Dinil Mon Divakaran, and Mohan Gurusamy",
        "link": "http://arxiv.org/abs/2503.04174v1",
        "abstract": "As modern networks grow increasingly complex--driven by diverse devices,\nencrypted protocols, and evolving threats--network traffic analysis has become\ncritically important. Existing machine learning models often rely only on a\nsingle representation of packets or flows, limiting their ability to capture\nthe contextual relationships essential for robust analysis. Furthermore,\ntask-specific architectures for supervised, semi-supervised, and unsupervised\nlearning lead to inefficiencies in adapting to varying data formats and\nsecurity tasks. To address these gaps, we propose UniNet, a unified framework\nthat introduces a novel multi-granular traffic representation (T-Matrix),\nintegrating session, flow, and packet-level features to provide comprehensive\ncontextual information. Combined with T-Attent, a lightweight attention-based\nmodel, UniNet efficiently learns latent embeddings for diverse security tasks.\nExtensive evaluations across four key network security and privacy\nproblems--anomaly detection, attack classification, IoT device identification,\nand encrypted website fingerprinting--demonstrate UniNet's significant\nperformance gain over state-of-the-art methods, achieving higher accuracy,\nlower false positive rates, and improved scalability. By addressing the\nlimitations of single-level models and unifying traffic analysis paradigms,\nUniNet sets a new benchmark for modern network security."
    },
    {
        "date": "2025-03",
        "title": "One-Shot is Enough: Consolidating Multi-Turn Attacks into Efficient Single-Turn Prompts for LLMs",
        "author": "Junwoo Ha, Hyunjun Kim, Sangyoon Yu, Haon Park, Ashkan Yousefpour, Yuna Park, and Suhyun Kim",
        "link": "http://arxiv.org/abs/2503.04856v1",
        "abstract": "Despite extensive safety enhancements in large language models (LLMs),\nmulti-turn \"jailbreak\" conversations crafted by skilled human adversaries can\nstill breach even the most sophisticated guardrails. However, these multi-turn\nattacks demand considerable manual effort, limiting their scalability. In this\nwork, we introduce a novel approach called Multi-turn-to-Single-turn (M2S) that\nsystematically converts multi-turn jailbreak prompts into single-turn attacks.\nSpecifically, we propose three conversion strategies - Hyphenize, Numberize,\nand Pythonize - each preserving sequential context yet packaging it in a single\nquery. Our experiments on the Multi-turn Human Jailbreak (MHJ) dataset show\nthat M2S often increases or maintains high Attack Success Rates (ASRs) compared\nto original multi-turn conversations. Notably, using a StrongREJECT-based\nevaluation of harmfulness, M2S achieves up to 95.9% ASR on Mistral-7B and\noutperforms original multi-turn prompts by as much as 17.5% in absolute\nimprovement on GPT-4o. Further analysis reveals that certain adversarial\ntactics, when consolidated into a single prompt, exploit structural formatting\ncues to evade standard policy checks. These findings underscore that\nsingle-turn attacks - despite being simpler and cheaper to conduct - can be\njust as potent, if not more, than their multi-turn counterparts. Our findings\nunderscore the urgent need to reevaluate and reinforce LLM safety strategies,\ngiven how adversarial queries can be compacted into a single prompt while still\nretaining sufficient complexity to bypass existing safety measures."
    },
    {
        "date": "2025-03",
        "title": "Robust Multi-View Learning via Representation Fusion of Sample-Level Attention and Alignment of Simulated Perturbation",
        "author": "Jie Xu, Na Zhao, Gang Niu, Masashi Sugiyama, and Xiaofeng Zhu",
        "link": "http://arxiv.org/abs/2503.04151v1",
        "abstract": "Recently, multi-view learning (MVL) has garnered significant attention due to\nits ability to fuse discriminative information from multiple views. However,\nreal-world multi-view datasets are often heterogeneous and imperfect, which\nusually makes MVL methods designed for specific combinations of views lack\napplication potential and limits their effectiveness. To address this issue, we\npropose a novel robust MVL method (namely RML) with simultaneous representation\nfusion and alignment. Specifically, we introduce a simple yet effective\nmulti-view transformer fusion network where we transform heterogeneous\nmulti-view data into homogeneous word embeddings, and then integrate multiple\nviews by the sample-level attention mechanism to obtain a fused representation.\nFurthermore, we propose a simulated perturbation based multi-view contrastive\nlearning framework that dynamically generates the noise and unusable\nperturbations for simulating imperfect data conditions. The simulated noisy and\nunusable data obtain two distinct fused representations, and we utilize\ncontrastive learning to align them for learning discriminative and robust\nrepresentations. Our RML is self-supervised and can also be applied for\ndownstream tasks as a regularization. In experiments, we employ it in\nunsupervised multi-view clustering, noise-label classification, and as a\nplug-and-play module for cross-modal hashing retrieval. Extensive comparison\nexperiments and ablation studies validate the effectiveness of RML."
    },
    {
        "date": "2025-03",
        "title": "Robust Computer-Vision based Construction Site Detection for Assistive-Technology Applications",
        "author": "Junchi Feng, Giles Hamilton-Fletcher, Nikhil Ballem, Michael Batavia, Yifei Wang, Jiuling Zhong, Maurizio Porfiri, and John-Ross Rizzo",
        "link": "http://arxiv.org/abs/2503.04139v1",
        "abstract": "Navigating urban environments poses significant challenges for people with\ndisabilities, particularly those with blindness and low vision. Environments\nwith dynamic and unpredictable elements like construction sites are especially\nchallenging. Construction sites introduce hazards like uneven surfaces,\nobstructive barriers, hazardous materials, and excessive noise, and they can\nalter routing, complicating safe mobility. Existing assistive technologies are\nlimited, as navigation apps do not account for construction sites during trip\nplanning, and detection tools that attempt hazard recognition struggle to\naddress the extreme variability of construction paraphernalia. This study\nintroduces a novel computer vision-based system that integrates open-vocabulary\nobject detection, a YOLO-based scaffolding-pole detection model, and an optical\ncharacter recognition (OCR) module to comprehensively identify and interpret\nconstruction site elements for assistive navigation. In static testing across\nseven construction sites, the system achieved an overall accuracy of 88.56\\%,\nreliably detecting objects from 2m to 10m within a 0$^\\circ$ -- 75$^\\circ$\nangular offset. At closer distances (2--4m), the detection rate was 100\\% at\nall tested angles. At"
    },
    {
        "date": "2025-03",
        "title": "From Pixels to Trajectory: Universal Adversarial Example Detection via Temporal Imprints",
        "author": "Yansong Gao, Huaibing Peng, Hua Ma, Zhiyang Dai, Shuo Wang, Hongsheng Hu, Anmin Fu, and Minhui Xue",
        "link": "http://arxiv.org/abs/2503.04853v1",
        "abstract": "For the first time, we unveil discernible temporal (or historical) trajectory\nimprints resulting from adversarial example (AE) attacks. Standing in contrast\nto existing studies all focusing on spatial (or static) imprints within the\ntargeted underlying victim models, we present a fresh temporal paradigm for\nunderstanding these attacks. Of paramount discovery is that these imprints are\nencapsulated within a single loss metric, spanning universally across diverse\ntasks such as classification and regression, and modalities including image,\ntext, and audio. Recognizing the distinct nature of loss between adversarial\nand clean examples, we exploit this temporal imprint for AE detection by\nproposing TRAIT (TRaceable Adversarial temporal trajectory ImprinTs). TRAIT\noperates under minimal assumptions without prior knowledge of attacks, thereby\nframing the detection challenge as a one-class classification problem. However,\ndetecting AEs is still challenged by significant overlaps between the\nconstructed synthetic losses of adversarial and clean examples due to the\nabsence of ground truth for incoming inputs. TRAIT addresses this challenge by\nconverting the synthetic loss into a spectrum signature, using the technique of\nFast Fourier Transform to highlight the discrepancies, drawing inspiration from\nthe temporal nature of the imprints, analogous to time-series signals. Across\n12 AE attacks including SMACK (USENIX Sec'2023), TRAIT demonstrates consistent\noutstanding performance across comprehensively evaluated modalities, tasks,\ndatasets, and model architectures. In all scenarios, TRAIT achieves an AE\ndetection accuracy exceeding 97%, often around 99%, while maintaining a false\nrejection rate of 1%. TRAIT remains effective under the formulated strong\nadaptive attacks."
    },
    {
        "date": "2025-03",
        "title": "Robust Data Watermarking in Language Models by Injecting Fictitious Knowledge",
        "author": "Xinyue Cui, Johnny Tian-Zheng Wei, Swabha Swayamdipta, and Robin Jia",
        "link": "http://arxiv.org/abs/2503.04036v2",
        "abstract": "Data watermarking in language models injects traceable signals, such as\nspecific token sequences or stylistic patterns, into copyrighted text, allowing\ncopyright holders to track and verify training data ownership. Previous data\nwatermarking techniques primarily focus on effective memorization after\npretraining, while overlooking challenges that arise in other stages of the LLM\npipeline, such as the risk of watermark filtering during data preprocessing, or\npotential forgetting through post-training, or verification difficulties due to\nAPI-only access. We propose a novel data watermarking approach that injects\ncoherent and plausible yet fictitious knowledge into training data using\ngenerated passages describing a fictitious entity and its associated\nattributes. Our watermarks are designed to be memorized by the LLM through\nseamlessly integrating in its training data, making them harder to detect\nlexically during preprocessing. We demonstrate that our watermarks can be\neffectively memorized by LLMs, and that increasing our watermarks' density,\nlength, and diversity of attributes strengthens their memorization. We further\nshow that our watermarks remain robust throughout LLM development, maintaining\ntheir effectiveness after continual pretraining and supervised finetuning.\nFinally, we show that our data watermarks can be evaluated even under API-only\naccess via question answering."
    },
    {
        "date": "2025-03",
        "title": "Poisoning Attacks to Local Differential Privacy Protocols for Trajectory Data",
        "author": "I-Jung Hsu, Chih-Hsun Lin, Chia-Mu Yu, Sy-Yen Kuo, and Chun-Ying Huang",
        "link": "http://arxiv.org/abs/2503.07483v1",
        "abstract": "Trajectory data, which tracks movements through geographic locations, is\ncrucial for improving real-world applications. However, collecting such\nsensitive data raises considerable privacy concerns. Local differential privacy\n(LDP) offers a solution by allowing individuals to locally perturb their\ntrajectory data before sharing it. Despite its privacy benefits, LDP protocols\nare vulnerable to data poisoning attacks, where attackers inject fake data to\nmanipulate aggregated results. In this work, we make the first attempt to\nanalyze vulnerabilities in several representative LDP trajectory protocols. We\npropose \\textsc{TraP}, a heuristic algorithm for data \\underline{P}oisoning\nattacks using a prefix-suffix method to optimize fake \\underline{Tra}jectory\nselection, significantly reducing computational complexity. Our experimental\nresults demonstrate that our attack can substantially increase target pattern\noccurrences in the perturbed trajectory dataset with few fake users. This study\nunderscores the urgent need for robust defenses and better protocol designs to\nsafeguard LDP trajectory data against malicious manipulation."
    },
    {
        "date": "2025-03",
        "title": "DSV-LFS: Unifying LLM-Driven Semantic Cues with Visual Features for Robust Few-Shot Segmentation",
        "author": "Amin Karimi, and Charalambos Poullis",
        "link": "http://arxiv.org/abs/2503.04006v1",
        "abstract": "Few-shot semantic segmentation (FSS) aims to enable models to segment\nnovel/unseen object classes using only a limited number of labeled examples.\nHowever, current FSS methods frequently struggle with generalization due to\nincomplete and biased feature representations, especially when support images\ndo not capture the full appearance variability of the target class. To improve\nthe FSS pipeline, we propose a novel framework that utilizes large language\nmodels (LLMs) to adapt general class semantic information to the query image.\nFurthermore, the framework employs dense pixel-wise matching to identify\nsimilarities between query and support images, resulting in enhanced FSS\nperformance. Inspired by reasoning-based segmentation frameworks, our method,\nnamed DSV-LFS, introduces an additional token into the LLM vocabulary, allowing\na multimodal LLM to generate a \"semantic prompt\" from class descriptions. In\nparallel, a dense matching module identifies visual similarities between the\nquery and support images, generating a \"visual prompt\". These prompts are then\njointly employed to guide the prompt-based decoder for accurate segmentation of\nthe query image. Comprehensive experiments on the benchmark datasets\nPascal-$5^{i}$ and COCO-$20^{i}$ demonstrate that our framework achieves\nstate-of-the-art performance-by a significant margin-demonstrating superior\ngeneralization to novel classes and robustness across diverse scenarios. The\nsource code is available at\n\\href{https://github.com/aminpdik/DSV-LFS}{https://github.com/aminpdik/DSV-LFS}"
    },
    {
        "date": "2025-03",
        "title": "Image Data Augmentation for the TAIGA-IACT Experiment with Conditional Generative Adversarial Networks",
        "author": "Yu. Yu. Dubenskaya, A. P. Kryukov, E. O. Gres, S. P. Polyakov, E. B. Postnikov, P. A. Volchugov, A. A. Vlaskina, and D. P. Zhurov",
        "link": "http://arxiv.org/abs/2503.03982v1",
        "abstract": "Modern Imaging Atmospheric Cherenkov Telescopes (IACTs) generate a huge\namount of data that must be classified automatically, ideally in real time.\nCurrently, machine learning-based solutions are increasingly being used to\nsolve classification problems. However, these classifiers require proper\ntraining data sets to work correctly. The problem with training neural networks\non real IACT data is that these data need to be pre-labeled, whereas such\nlabeling is difficult and its results are estimates. In addition, the\ndistribution of incoming events is highly imbalanced. Firstly, there is an\nimbalance in the types of events, since the number of detected gamma quanta is\nsignificantly less than the number of protons. Secondly, the energy\ndistribution of particles of the same type is also imbalanced, since\nhigh-energy particles are extremely rare. This imbalance results in poorly\ntrained classifiers that, once trained, do not handle rare events correctly.\nUsing only conventional Monte Carlo event simulation methods to solve this\nproblem is possible, but extremely resource-intensive and time-consuming. To\naddress this issue, we propose to perform data augmentation with artificially\ngenerated events of the desired type and energy using conditional generative\nadversarial networks (cGANs), distinguishing classes by energy values. In the\npaper, we describe a simple algorithm for generating balanced data sets using\ncGANs. Thus, the proposed neural network model produces both imbalanced data\nsets for physical analysis as well as balanced data sets suitable for training\nother neural networks."
    },
    {
        "date": "2025-03",
        "title": "Multimodal Stock Price Prediction: A Case Study of the Russian Securities Market",
        "author": "Kasymkhan Khubiev, and Mikhail Semenov",
        "link": "http://arxiv.org/abs/2503.08696v1",
        "abstract": "Classical asset price forecasting methods primarily rely on numerical data,\nsuch as price time series, trading volumes, limit order book data, and\ntechnical analysis indicators. However, the news flow plays a significant role\nin price formation, making the development of multimodal approaches that\ncombine textual and numerical data for improved prediction accuracy highly\nrelevant. This paper addresses the problem of forecasting financial asset\nprices using the multimodal approach that combines candlestick time series and\ntextual news flow data. A unique dataset was collected for the study, which\nincludes time series for 176 Russian stocks traded on the Moscow Exchange and\n79,555 financial news articles in Russian. For processing textual data,\npre-trained models RuBERT and Vikhr-Qwen2.5-0.5b-Instruct (a large language\nmodel) were used, while time series and vectorized text data were processed\nusing an LSTM recurrent neural network. The experiments compared models based\non a single modality (time series only) and two modalities, as well as various\nmethods for aggregating text vector representations. Prediction quality was\nestimated using two key metrics: Accuracy (direction of price movement\nprediction: up or down) and Mean Absolute Percentage Error (MAPE), which\nmeasures the deviation of the predicted price from the true price. The\nexperiments showed that incorporating textual modality reduced the MAPE value\nby 55%. The resulting multimodal dataset holds value for the further adaptation\nof language models in the financial sector. Future research directions include\noptimizing textual modality parameters, such as the time window, sentiment, and\nchronological order of news messages."
    },
    {
        "date": "2025-03",
        "title": "Neural Descriptors: Self-Supervised Learning of Robust Local Surface Descriptors Using Polynomial Patches",
        "author": "Gal Yona, Roy Velich, Ron Kimmel, and Ehud Rivlin",
        "link": "http://arxiv.org/abs/2503.03907v1",
        "abstract": "Classical shape descriptors such as Heat Kernel Signature (HKS), Wave Kernel\nSignature (WKS), and Signature of Histograms of OrienTations (SHOT), while\nwidely used in shape analysis, exhibit sensitivity to mesh connectivity,\nsampling patterns, and topological noise. While differential geometry offers a\npromising alternative through its theory of differential invariants, which are\ntheoretically guaranteed to be robust shape descriptors, the computation of\nthese invariants on discrete meshes often leads to unstable numerical\napproximations, limiting their practical utility. We present a self-supervised\nlearning approach for extracting geometric features from 3D surfaces. Our\nmethod combines synthetic data generation with a neural architecture designed\nto learn sampling-invariant features. By integrating our features into existing\nshape correspondence frameworks, we demonstrate improved performance on\nstandard benchmarks including FAUST, SCAPE, TOPKIDS, and SHREC'16, showing\nparticular robustness to topological noise and partial shapes."
    },
    {
        "date": "2025-03",
        "title": "Honest to a Fault: Root-Causing Fault Attacks with Pre-Silicon RISC Pipeline Characterization",
        "author": "Arsalan Ali Malik, Harshvadan Mihir, and Aydin Aysu",
        "link": "http://arxiv.org/abs/2503.04846v1",
        "abstract": "Fault injection attacks represent a class of threats that can compromise\nembedded systems across multiple layers of abstraction, such as system\nsoftware, instruction set architecture (ISA), microarchitecture, and physical\nimplementation. Early detection of these vulnerabilities and understanding\ntheir root causes along with their propagation from the physical layer to the\nsystem software is critical to secure the cyberinfrastructure.\n  This present presents a comprehensive methodology for conducting controlled\nfault injection attacks at the pre-silicon level and an analysis of the\nunderlying system for root-causing behavior. As the driving application, we use\nthe clock glitch attacks in AI/ML applications for critical misclassification.\nOur study aims to characterize and diagnose the impact of faults within the\nRISC-V instruction set and pipeline stages, while tracing fault propagation\nfrom the circuit level to the AI/ML application software. This analysis\nresulted in discovering a novel vulnerability through controlled clock glitch\nparameters, specifically targeting the RISC-V decode stage."
    },
    {
        "date": "2025-03",
        "title": "Task-Agnostic Attacks Against Vision Foundation Models",
        "author": "Brian Pulfer, Yury Belousov, Vitaliy Kinakh, Teddy Furon, and Slava Voloshynovskiy",
        "link": "http://arxiv.org/abs/2503.03842v1",
        "abstract": "The study of security in machine learning mainly focuses on downstream\ntask-specific attacks, where the adversarial example is obtained by optimizing\na loss function specific to the downstream task. At the same time, it has\nbecome standard practice for machine learning practitioners to adopt publicly\navailable pre-trained vision foundation models, effectively sharing a common\nbackbone architecture across a multitude of applications such as\nclassification, segmentation, depth estimation, retrieval, question-answering\nand more. The study of attacks on such foundation models and their impact to\nmultiple downstream tasks remains vastly unexplored. This work proposes a\ngeneral framework that forges task-agnostic adversarial examples by maximally\ndisrupting the feature representation obtained with foundation models. We\nextensively evaluate the security of the feature representations obtained by\npopular vision foundation models by measuring the impact of this attack on\nmultiple downstream tasks and its transferability between models."
    },
    {
        "date": "2025-03",
        "title": "A Practical Memory Injection Attack against LLM Agents",
        "author": "Shen Dong, Shaochen Xu, Pengfei He, Yige Li, Jiliang Tang, Tianming Liu, Hui Liu, and Zhen Xiang",
        "link": "http://arxiv.org/abs/2503.03704v2",
        "abstract": "Agents based on large language models (LLMs) have demonstrated strong\ncapabilities in a wide range of complex, real-world applications. However, LLM\nagents with a compromised memory bank may easily produce harmful outputs when\nthe past records retrieved for demonstration are malicious. In this paper, we\npropose a novel Memory INJection Attack, MINJA, that enables the injection of\nmalicious records into the memory bank by only interacting with the agent via\nqueries and output observations. These malicious records are designed to elicit\na sequence of malicious reasoning steps leading to undesirable agent actions\nwhen executing the victim user's query. Specifically, we introduce a sequence\nof bridging steps to link the victim query to the malicious reasoning steps.\nDuring the injection of the malicious record, we propose an indication prompt\nto guide the agent to autonomously generate our designed bridging steps. We\nalso propose a progressive shortening strategy that gradually removes the\nindication prompt, such that the malicious record will be easily retrieved when\nprocessing the victim query comes after. Our extensive experiments across\ndiverse agents demonstrate the effectiveness of MINJA in compromising agent\nmemory. With minimal requirements for execution, MINJA enables any user to\ninfluence agent memory, highlighting practical risks of LLM agents."
    },
    {
        "date": "2025-03",
        "title": "Robust Learning of Diverse Code Edits",
        "author": "Tushar Aggarwal, Swayam Singh, Abhijeet Awasthi, Aditya Kanade, and Nagarajan Natarajan",
        "link": "http://arxiv.org/abs/2503.03656v1",
        "abstract": "Software engineering activities frequently involve edits to existing code.\nHowever, contemporary code language models (LMs) lack the ability to handle\ndiverse types of code-edit requirements. In this work, we attempt to overcome\nthis shortcoming through (1) a novel synthetic data generation pipeline and (2)\na robust model adaptation algorithm. Starting with seed code examples and\ndiverse editing criteria, our pipeline generates high-quality samples\ncomprising original and modified code, along with natural language instructions\nin different styles and verbosity. Today's code LMs come bundled with strong\nabilities, such as code generation and instruction following, which should not\nbe lost due to fine-tuning. To ensure this, we propose a novel adaptation\nalgorithm, SeleKT, that (a) leverages a dense gradient-based step to identify\nthe weights that are most important for code editing, and (b) does a sparse\nprojection onto the base model to avoid overfitting. Using our approach, we\nobtain a new series of models NextCoder (adapted from QwenCoder-2.5) that\nachieves strong results on five code-editing benchmarks, outperforming\ncomparable size models and even several larger ones. We show the generality of\nour approach on two model families (DeepSeekCoder and QwenCoder), compare\nagainst other fine-tuning approaches, and demonstrate robustness by showing\nretention of code generation abilities post adaptation."
    },
    {
        "date": "2025-03",
        "title": "CLIP is Strong Enough to Fight Back: Test-time Counterattacks towards Zero-shot Adversarial Robustness of CLIP",
        "author": "Songlong Xing, Zhengyu Zhao, and Nicu Sebe",
        "link": "http://arxiv.org/abs/2503.03613v1",
        "abstract": "Despite its prevalent use in image-text matching tasks in a zero-shot manner,\nCLIP has been shown to be highly vulnerable to adversarial perturbations added\nonto images. Recent studies propose to finetune the vision encoder of CLIP with\nadversarial samples generated on the fly, and show improved robustness against\nadversarial attacks on a spectrum of downstream datasets, a property termed as\nzero-shot robustness. In this paper, we show that malicious perturbations that\nseek to maximise the classification loss lead to `falsely stable' images, and\npropose to leverage the pre-trained vision encoder of CLIP to counterattack\nsuch adversarial images during inference to achieve robustness. Our paradigm is\nsimple and training-free, providing the first method to defend CLIP from\nadversarial attacks at test time, which is orthogonal to existing methods\naiming to boost zero-shot adversarial robustness of CLIP. We conduct\nexperiments across 16 classification datasets, and demonstrate stable and\nconsistent gains compared to test-time defence methods adapted from existing\nadversarial robustness studies that do not rely on external networks, without\nnoticeably impairing performance on clean images. We also show that our\nparadigm can be employed on CLIP models that have been adversarially finetuned\nto further enhance their robustness at test time. Our code is available\n\\href{https://github.com/Sxing2/CLIP-Test-time-Counterattacks}{here}."
    },
    {
        "date": "2025-03",
        "title": "REGRACE: A Robust and Efficient Graph-based Re-localization Algorithm using Consistency Evaluation",
        "author": "D\u00e9bora N. P. Oliveira, Joshua Knights, Sebasti\u00e1n Barbas Laina, Simon Boche, Wolfram Burgard, and Stefan Leutenegger",
        "link": "http://arxiv.org/abs/2503.03599v1",
        "abstract": "Loop closures are essential for correcting odometry drift and creating\nconsistent maps, especially in the context of large-scale navigation. Current\nmethods using dense point clouds for accurate place recognition do not scale\nwell due to computationally expensive scan-to-scan comparisons. Alternative\nobject-centric approaches are more efficient but often struggle with\nsensitivity to viewpoint variation. In this work, we introduce REGRACE, a novel\napproach that addresses these challenges of scalability and perspective\ndifference in re-localization by using LiDAR-based submaps. We introduce\nrotation-invariant features for each labeled object and enhance them with\nneighborhood context through a graph neural network. To identify potential\nrevisits, we employ a scalable bag-of-words approach, pooling one learned\nglobal feature per submap. Additionally, we define a revisit with geometrical\nconsistency cues rather than embedding distance, allowing us to recognize\nfar-away loop closures. Our evaluations demonstrate that REGRACE achieves\nsimilar results compared to state-of-the-art place recognition and registration\nbaselines while being twice as fast."
    },
    {
        "date": "2025-03",
        "title": "Data Sharing, Privacy and Security Considerations in the Energy Sector: A Review from Technical Landscape to Regulatory Specifications",
        "author": "Shiliang Zhang, Sabita Maharjan, Lee Andrew Bygrave, and Shui Yu",
        "link": "http://arxiv.org/abs/2503.03539v1",
        "abstract": "Decarbonization, decentralization and digitalization are the three key\nelements driving the twin energy transition. The energy system is evolving to a\nmore data driven ecosystem, leading to the need of communication and storage of\nlarge amount of data of different resolution from the prosumers and other\nstakeholders in the energy ecosystem. While the energy system is certainly\nadvancing, this paradigm shift is bringing in new privacy and security issues\nrelated to collection, processing and storage of data - not only from the\ntechnical dimension, but also from the regulatory perspective. Understanding\ndata privacy and security in the evolving energy system, regarding regulatory\ncompliance, is an immature field of research. Contextualized knowledge of how\nrelated issues are regulated is still in its infancy, and the practical and\ntechnical basis for the regulatory framework for data privacy and security is\nnot clear. To fill this gap, this paper conducts a comprehensive review of the\ndata-related issues for the energy system by integrating both technical and\nregulatory dimensions. We start by reviewing open-access data, data\ncommunication and data-processing techniques for the energy system, and use it\nas the basis to connect the analysis of data-related issues from the integrated\nperspective. We classify the issues into three categories: (i) data-sharing\namong energy end users and stakeholders (ii) privacy of end users, and (iii)\ncyber security, and then explore these issues from a regulatory perspective. We\nanalyze the evolution of related regulations, and introduce the relevant\nregulatory initiatives for the categorized issues in terms of regulatory\ndefinitions, concepts, principles, rights and obligations in the context of\nenergy systems. Finally, we provide reflections on the gaps that still exist,\nand guidelines for regulatory frameworks for a truly participatory energy\nsystem."
    },
    {
        "date": "2025-03",
        "title": "Adversarial Training for Multimodal Large Language Models against Jailbreak Attacks",
        "author": "Liming Lu, Shuchao Pang, Siyuan Liang, Haotian Zhu, Xiyu Zeng, Aishan Liu, Yunhuai Liu, and Yongbin Zhou",
        "link": "http://arxiv.org/abs/2503.04833v1",
        "abstract": "Multimodal large language models (MLLMs) have made remarkable strides in\ncross-modal comprehension and generation tasks. However, they remain vulnerable\nto jailbreak attacks, where crafted perturbations bypass security guardrails\nand elicit harmful outputs. In this paper, we present the first adversarial\ntraining (AT) paradigm tailored to defend against jailbreak attacks during the\nMLLM training phase. Extending traditional AT to this domain poses two critical\nchallenges: efficiently tuning massive parameters and ensuring robustness\nagainst attacks across multiple modalities. To address these challenges, we\nintroduce Projection Layer Against Adversarial Training (ProEAT), an end-to-end\nAT framework. ProEAT incorporates a projector-based adversarial training\narchitecture that efficiently handles large-scale parameters while maintaining\ncomputational feasibility by focusing adversarial training on a lightweight\nprojector layer instead of the entire model; additionally, we design a dynamic\nweight adjustment mechanism that optimizes the loss function's weight\nallocation based on task demands, streamlining the tuning process. To enhance\ndefense performance, we propose a joint optimization strategy across visual and\ntextual modalities, ensuring robust resistance to jailbreak attacks originating\nfrom either modality. Extensive experiments conducted on five major jailbreak\nattack methods across three mainstream MLLMs demonstrate the effectiveness of\nour approach. ProEAT achieves state-of-the-art defense performance,\noutperforming existing baselines by an average margin of +34% across text and\nimage modalities, while incurring only a 1% reduction in clean accuracy.\nFurthermore, evaluations on real-world embodied intelligent systems highlight\nthe practical applicability of our framework, paving the way for the\ndevelopment of more secure and reliable multimodal systems."
    },
    {
        "date": "2025-03",
        "title": "CURVALID: Geometrically-guided Adversarial Prompt Detection",
        "author": "Canaan Yung, Hanxun Huang, Sarah Monazam Erfani, and Christopher Leckie",
        "link": "http://arxiv.org/abs/2503.03502v1",
        "abstract": "Adversarial prompts capable of jailbreaking large language models (LLMs) and\ninducing undesirable behaviours pose a significant obstacle to their safe\ndeployment. Current mitigation strategies rely on activating built-in defence\nmechanisms or fine-tuning the LLMs, but the fundamental distinctions between\nadversarial and benign prompts are yet to be understood. In this work, we\nintroduce CurvaLID, a novel defense framework that efficiently detects\nadversarial prompts by leveraging their geometric properties. It is agnostic to\nthe type of LLM, offering a unified detection framework across diverse\nadversarial prompts and LLM architectures. CurvaLID builds on the geometric\nanalysis of text prompts to uncover their underlying differences. We\ntheoretically extend the concept of curvature via the Whewell equation into an\n$n$-dimensional word embedding space, enabling us to quantify local geometric\nproperties, including semantic shifts and curvature in the underlying\nmanifolds. Additionally, we employ Local Intrinsic Dimensionality (LID) to\ncapture geometric features of text prompts within adversarial subspaces. Our\nfindings reveal that adversarial prompts differ fundamentally from benign\nprompts in terms of their geometric characteristics. Our results demonstrate\nthat CurvaLID delivers superior detection and rejection of adversarial queries,\npaving the way for safer LLM deployment. The source code can be found at\nhttps://github.com/Cancanxxx/CurvaLID"
    },
    {
        "date": "2025-03",
        "title": "Data Poisoning Attacks to Locally Differentially Private Range Query Protocols",
        "author": "Ting-Wei Liao, Chih-Hsun Lin, Yu-Lin Tsai, Takao Murakami, Chia-Mu Yu, Jun Sakuma, Chun-Ying Huang, and Hiroaki Kikuchi",
        "link": "http://arxiv.org/abs/2503.03454v2",
        "abstract": "Local Differential Privacy (LDP) has been widely adopted to protect user\nprivacy in decentralized data collection. However, recent studies have revealed\nthat LDP protocols are vulnerable to data poisoning attacks, where malicious\nusers manipulate their reported data to distort aggregated results. In this\nwork, we present the first study on data poisoning attacks targeting LDP range\nquery protocols, focusing on both tree-based and grid-based approaches. We\nidentify three key challenges in executing such attacks, including crafting\nconsistent and effective fake data, maintaining data consistency across levels\nor grids, and preventing server detection. To address the first two challenges,\nwe propose novel attack methods that are provably optimal, including a\ntree-based attack and a grid-based attack, designed to manipulate range query\nresults with high effectiveness. \\textbf{Our key finding is that the common\npost-processing procedure, Norm-Sub, in LDP range query protocols can help the\nattacker massively amplify their attack effectiveness.} In addition, we study a\npotential countermeasure, but also propose an adaptive attack capable of\nevading this defense to address the third challenge. We evaluate our methods\nthrough theoretical analysis and extensive experiments on synthetic and\nreal-world datasets. Our results show that the proposed attacks can\nsignificantly amplify estimations for arbitrary range queries by manipulating a\nsmall fraction of users, providing 5-10x more influence than a normal user to\nthe estimation."
    },
    {
        "date": "2025-03",
        "title": "When Claims Evolve: Evaluating and Enhancing the Robustness of Embedding Models Against Misinformation Edits",
        "author": "Jabez Magomere, Emanuele La Malfa, Manuel Tonneau, Ashkan Kazemi, and Scott Hale",
        "link": "http://arxiv.org/abs/2503.03417v2",
        "abstract": "Online misinformation remains a critical challenge, and fact-checkers\nincreasingly rely on embedding-based methods to retrieve relevant fact-checks.\nYet, when debunked claims reappear in edited forms, the performance of these\nmethods is unclear. In this work, we introduce a taxonomy of six common\nreal-world misinformation edits and propose a perturbation framework that\ngenerates valid, natural claim variations. Our multi-stage retrieval evaluation\nreveals that standard embedding models struggle with user-introduced edits,\nwhile LLM-distilled embeddings offer improved robustness at a higher\ncomputational cost. Although a strong reranker helps mitigate some issues, it\ncannot fully compensate for first-stage retrieval gaps. Addressing these\nretrieval gaps, our train- and inference-time mitigation approaches enhance\nin-domain robustness by up to 17 percentage points and boost out-of-domain\ngeneralization by 10 percentage points over baseline models. Overall, our\nfindings provide practical improvements to claim-matching systems, enabling\nmore reliable fact-checking of evolving misinformation."
    },
    {
        "date": "2025-03",
        "title": "Towards Effective and Sparse Adversarial Attack on Spiking Neural Networks via Breaking Invisible Surrogate Gradients",
        "author": "Li Lun, Kunyu Feng, Qinglong Ni, Ling Liang, Yuan Wang, Ying Li, Dunshan Yu, and Xiaoxin Cui",
        "link": "http://arxiv.org/abs/2503.03272v2",
        "abstract": "Spiking neural networks (SNNs) have shown their competence in handling\nspatial-temporal event-based data with low energy consumption. Similar to\nconventional artificial neural networks (ANNs), SNNs are also vulnerable to\ngradient-based adversarial attacks, wherein gradients are calculated by\nspatial-temporal back-propagation (STBP) and surrogate gradients (SGs).\nHowever, the SGs may be invisible for an inference-only model as they do not\ninfluence the inference results, and current gradient-based attacks are\nineffective for binary dynamic images captured by the dynamic vision sensor\n(DVS). While some approaches addressed the issue of invisible SGs through\nuniversal SGs, their SGs lack a correlation with the victim model, resulting in\nsub-optimal performance. Moreover, the imperceptibility of existing SNN-based\nbinary attacks is still insufficient. In this paper, we introduce an innovative\npotential-dependent surrogate gradient (PDSG) method to establish a robust\nconnection between the SG and the model, thereby enhancing the adaptability of\nadversarial attacks across various models with invisible SGs. Additionally, we\npropose the sparse dynamic attack (SDA) to effectively attack binary dynamic\nimages. Utilizing a generation-reduction paradigm, SDA can fully optimize the\nsparsity of adversarial perturbations. Experimental results demonstrate that\nour PDSG and SDA outperform state-of-the-art SNN-based attacks across various\nmodels and datasets. Specifically, our PDSG achieves 100% attack success rate\non ImageNet, and our SDA obtains 82% attack success rate by modifying only\n0.24% of the pixels on CIFAR10DVS. The code is available at\nhttps://github.com/ryime/PDSG-SDA ."
    },
    {
        "date": "2025-03",
        "title": "Quantum-Inspired Privacy-Preserving Federated Learning Framework for Secure Dementia Classification",
        "author": "Gazi Tanbhir, and Md. Farhan Shahriyar",
        "link": "http://arxiv.org/abs/2503.03267v1",
        "abstract": "Dementia, a neurological disorder impacting millions globally, presents\nsignificant challenges in diagnosis and patient care. With the rise of privacy\nconcerns and security threats in healthcare, federated learning (FL) has\nemerged as a promising approach to enable collaborative model training across\ndecentralized datasets without exposing sensitive patient information. However,\nFL remains vulnerable to advanced security breaches such as gradient inversion\nand eavesdropping attacks. This paper introduces a novel framework that\nintegrates federated learning with quantum-inspired encryption techniques for\ndementia classification, emphasizing privacy preservation and security.\nLeveraging quantum key distribution (QKD), the framework ensures secure\ntransmission of model weights, protecting against unauthorized access and\ninterception during training. The methodology utilizes a convolutional neural\nnetwork (CNN) for dementia classification, with federated training conducted\nacross distributed healthcare nodes, incorporating QKD-encrypted weight sharing\nto secure the aggregation process. Experimental evaluations conducted on MRI\ndata from the OASIS dataset demonstrate that the proposed framework achieves\nidentical accuracy levels to a baseline model while enhancing data security and\nreducing loss by almost 1% compared to the classical baseline model. The\nframework offers significant implications for democratizing access to AI-driven\ndementia diagnostics in low- and middle-income countries, addressing critical\nresource and privacy constraints. This work contributes a robust, scalable, and\nsecure federated learning solution for healthcare applications, paving the way\nfor broader adoption of quantum-inspired techniques in AI-driven medical\nresearch."
    },
    {
        "date": "2025-03",
        "title": "Adversarial Example Based Fingerprinting for Robust Copyright Protection in Split Learning",
        "author": "Zhangting Lin, Mingfu Xue, Kewei Chen, Wenmao Liu, Xiang Gao, Leo Yu Zhang, Jian Wang, and Yushu Zhang",
        "link": "http://arxiv.org/abs/2503.04825v1",
        "abstract": "Currently, deep learning models are easily exposed to data leakage risks. As\na distributed model, Split Learning thus emerged as a solution to address this\nissue. The model is splitted to avoid data uploading to the server and reduce\ncomputing requirements while ensuring data privacy and security. However, the\ntransmission of data between clients and server creates a potential\nvulnerability. In particular, model is vulnerable to intellectual property (IP)\ninfringement such as piracy. Alarmingly, a dedicated copyright protection\nframework tailored for Split Learning models is still lacking. To this end, we\npropose the first copyright protection scheme for Split Learning model,\nleveraging fingerprint to ensure effective and robust copyright protection. The\nproposed method first generates a set of specifically designed adversarial\nexamples. Then, we select those examples that would induce misclassifications\nto form the fingerprint set. These adversarial examples are embedded as\nfingerprints into the model during the training process. Exhaustive experiments\nhighlight the effectiveness of the scheme. This is demonstrated by a remarkable\nfingerprint verification success rate (FVSR) of 100% on MNIST, 98% on CIFAR-10,\nand 100% on ImageNet, respectively. Meanwhile, the model's accuracy only\ndecreases slightly, indicating that the embedded fingerprints do not compromise\nmodel performance. Even under label inference attack, our approach consistently\nachieves a high fingerprint verification success rate that ensures robust\nverification."
    },
    {
        "date": "2025-03",
        "title": "Towards Robust Universal Information Extraction: Benchmark, Evaluation, and Solution",
        "author": "Jizhao Zhu, Akang Shi, Zixuan Li, Long Bai, Xiaolong Jin, Jiafeng Guo, and Xueqi Cheng",
        "link": "http://arxiv.org/abs/2503.03201v1",
        "abstract": "In this paper, we aim to enhance the robustness of Universal Information\nExtraction (UIE) by introducing a new benchmark dataset, a comprehensive\nevaluation, and a feasible solution. Existing robust benchmark datasets have\ntwo key limitations: 1) They generate only a limited range of perturbations for\na single Information Extraction (IE) task, which fails to evaluate the\nrobustness of UIE models effectively; 2) They rely on small models or\nhandcrafted rules to generate perturbations, often resulting in unnatural\nadversarial examples. Considering the powerful generation capabilities of Large\nLanguage Models (LLMs), we introduce a new benchmark dataset for Robust UIE,\ncalled RUIE-Bench, which utilizes LLMs to generate more diverse and realistic\nperturbations across different IE tasks. Based on this dataset, we\ncomprehensively evaluate existing UIE models and reveal that both LLM-based\nmodels and other models suffer from significant performance drops. To improve\nrobustness and reduce training costs, we propose a data-augmentation solution\nthat dynamically selects hard samples for iterative training based on the\nmodel's inference loss. Experimental results show that training with only\n\\textbf{15\\%} of the data leads to an average \\textbf{7.5\\%} relative\nperformance improvement across three IE tasks."
    },
    {
        "date": "2025-03",
        "title": "DSPNet: Dual-vision Scene Perception for Robust 3D Question Answering",
        "author": "Jingzhou Luo, Yang Liu, Weixing Chen, Zhen Li, Yaowei Wang, Guanbin Li, and Liang Lin",
        "link": "http://arxiv.org/abs/2503.03190v2",
        "abstract": "3D Question Answering (3D QA) requires the model to comprehensively\nunderstand its situated 3D scene described by the text, then reason about its\nsurrounding environment and answer a question under that situation. However,\nexisting methods usually rely on global scene perception from pure 3D point\nclouds and overlook the importance of rich local texture details from\nmulti-view images. Moreover, due to the inherent noise in camera poses and\ncomplex occlusions, there exists significant feature degradation and reduced\nfeature robustness problems when aligning 3D point cloud with multi-view\nimages. In this paper, we propose a Dual-vision Scene Perception Network\n(DSPNet), to comprehensively integrate multi-view and point cloud features to\nimprove robustness in 3D QA. Our Text-guided Multi-view Fusion (TGMF) module\nprioritizes image views that closely match the semantic content of the text. To\nadaptively fuse back-projected multi-view images with point cloud features, we\ndesign the Adaptive Dual-vision Perception (ADVP) module, enhancing 3D scene\ncomprehension. Additionally, our Multimodal Context-guided Reasoning (MCGR)\nmodule facilitates robust reasoning by integrating contextual information\nacross visual and linguistic modalities. Experimental results on SQA3D and\nScanQA datasets demonstrate the superiority of our DSPNet. Codes will be\navailable at https://github.com/LZ-CH/DSPNet."
    },
    {
        "date": "2025-03",
        "title": "AttackSeqBench: Benchmarking Large Language Models' Understanding of Sequential Patterns in Cyber Attacks",
        "author": "Javier Yong, Haokai Ma, Yunshan Ma, Anis Yusof, Zhenkai Liang, and Ee-Chien Chang",
        "link": "http://arxiv.org/abs/2503.03170v1",
        "abstract": "The observations documented in Cyber Threat Intelligence (CTI) reports play a\ncritical role in describing adversarial behaviors, providing valuable insights\nfor security practitioners to respond to evolving threats. Recent advancements\nof Large Language Models (LLMs) have demonstrated significant potential in\nvarious cybersecurity applications, including CTI report understanding and\nattack knowledge graph construction. While previous works have proposed\nbenchmarks that focus on the CTI extraction ability of LLMs, the sequential\ncharacteristic of adversarial behaviors within CTI reports remains largely\nunexplored, which holds considerable significance in developing a comprehensive\nunderstanding of how adversaries operate. To address this gap, we introduce\nAttackSeqBench, a benchmark tailored to systematically evaluate LLMs'\ncapability to understand and reason attack sequences in CTI reports. Our\nbenchmark encompasses three distinct Question Answering (QA) tasks, each task\nfocuses on the varying granularity in adversarial behavior. To alleviate the\nlaborious effort of QA construction, we carefully design an automated dataset\nconstruction pipeline to create scalable and well-formulated QA datasets based\non real-world CTI reports. To ensure the quality of our dataset, we adopt a\nhybrid approach of combining human evaluation and systematic evaluation\nmetrics. We conduct extensive experiments and analysis with both fast-thinking\nand slow-thinking LLMs, while highlighting their strengths and limitations in\nanalyzing the sequential patterns in cyber attacks. The overarching goal of\nthis work is to provide a benchmark that advances LLM-driven CTI report\nunderstanding and fosters its application in real-world cybersecurity\noperations. Our dataset and code are available at\nhttps://github.com/Javiery3889/AttackSeqBench ."
    },
    {
        "date": "2025-03",
        "title": "BEVDriver: Leveraging BEV Maps in LLMs for Robust Closed-Loop Driving",
        "author": "Katharina Winter, Mark Azer, and Fabian B. Flohr",
        "link": "http://arxiv.org/abs/2503.03074v1",
        "abstract": "Autonomous driving has the potential to set the stage for more efficient\nfuture mobility, requiring the research domain to establish trust through safe,\nreliable and transparent driving. Large Language Models (LLMs) possess\nreasoning capabilities and natural language understanding, presenting the\npotential to serve as generalized decision-makers for ego-motion planning that\ncan interact with humans and navigate environments designed for human drivers.\nWhile this research avenue is promising, current autonomous driving approaches\nare challenged by combining 3D spatial grounding and the reasoning and language\ncapabilities of LLMs. We introduce BEVDriver, an LLM-based model for end-to-end\nclosed-loop driving in CARLA that utilizes latent BEV features as perception\ninput. BEVDriver includes a BEV encoder to efficiently process multi-view\nimages and 3D LiDAR point clouds. Within a common latent space, the BEV\nfeatures are propagated through a Q-Former to align with natural language\ninstructions and passed to the LLM that predicts and plans precise future\ntrajectories while considering navigation instructions and critical scenarios.\nOn the LangAuto benchmark, our model reaches up to 18.9% higher performance on\nthe Driving Score compared to SoTA methods."
    },
    {
        "date": "2025-03",
        "title": "LLM Misalignment via Adversarial RLHF Platforms",
        "author": "Erfan Entezami, and Ali Naseh",
        "link": "http://arxiv.org/abs/2503.03039v1",
        "abstract": "Reinforcement learning has shown remarkable performance in aligning language\nmodels with human preferences, leading to the rise of attention towards\ndeveloping RLHF platforms. These platforms enable users to fine-tune models\nwithout requiring any expertise in developing complex machine learning\nalgorithms. While these platforms offer useful features such as reward modeling\nand RLHF fine-tuning, their security and reliability remain largely unexplored.\nGiven the growing adoption of RLHF and open-source RLHF frameworks, we\ninvestigate the trustworthiness of these systems and their potential impact on\nbehavior of LLMs. In this paper, we present an attack targeting publicly\navailable RLHF tools. In our proposed attack, an adversarial RLHF platform\ncorrupts the LLM alignment process by selectively manipulating data samples in\nthe preference dataset. In this scenario, when a user's task aligns with the\nattacker's objective, the platform manipulates a subset of the preference\ndataset that contains samples related to the attacker's target. This\nmanipulation results in a corrupted reward model, which ultimately leads to the\nmisalignment of the language model. Our results demonstrate that such an attack\ncan effectively steer LLMs toward undesirable behaviors within the targeted\ndomains. Our work highlights the critical need to explore the vulnerabilities\nof RLHF platforms and their potential to cause misalignment in LLMs during the\nRLHF fine-tuning process."
    },
    {
        "date": "2025-03",
        "title": "Mind the Gap: Detecting Black-box Adversarial Attacks in the Making through Query Update Analysis",
        "author": "Jeonghwan Park, Niall McLaughlin, and Ihsen Alouani",
        "link": "http://arxiv.org/abs/2503.02986v2",
        "abstract": "Adversarial attacks remain a significant threat that can jeopardize the\nintegrity of Machine Learning (ML) models. In particular, query-based black-box\nattacks can generate malicious noise without having access to the victim\nmodel's architecture, making them practical in real-world contexts. The\ncommunity has proposed several defenses against adversarial attacks, only to be\nbroken by more advanced and adaptive attack strategies. In this paper, we\npropose a framework that detects if an adversarial noise instance is being\ngenerated. Unlike existing stateful defenses that detect adversarial noise\ngeneration by monitoring the input space, our approach learns adversarial\npatterns in the input update similarity space. In fact, we propose to observe a\nnew metric called Delta Similarity (DS), which we show it captures more\nefficiently the adversarial behavior. We evaluate our approach against 8\nstate-of-the-art attacks, including adaptive attacks, where the adversary is\naware of the defense and tries to evade detection. We find that our approach is\nsignificantly more robust than existing defenses both in terms of specificity\nand sensitivity."
    },
    {
        "date": "2025-03",
        "title": "Comparative Analysis of Lightweight Kubernetes Distributions for Edge Computing: Security, Resilience and Maintainability",
        "author": "Diyaz Yakubov, and David H\u00e4stbacka",
        "link": "http://arxiv.org/abs/2503.04815v1",
        "abstract": "The increasing demand for real-time data processing in Internet of Things\n(IoT) devices has elevated the importance of edge computing, necessitating\nefficient and secure deployment of applications on resource-constrained\ndevices. Kubernetes and its lightweight distributions (k0s, k3s, KubeEdge, and\nOpenYurt) extend container orchestration to edge environments, but their\nsecurity, reliability, and maintainability have not been comprehensively\nanalyzed. This study compares Kubernetes and these lightweight distributions by\nevaluating security compliance using kube-bench, simulating network outages to\nassess resiliency, and documenting maintainability. Results indicate that while\nk3s and k0s offer superior ease of development due to their simplicity, they\nhave lower security compliance compared to Kubernetes, KubeEdge, and OpenYurt.\nKubernetes provides a balanced approach but may be resource-intensive for edge\ndeployments. KubeEdge and OpenYurt enhance security features and reliability\nunder network outages but increase complexity and resource consumption. The\nfindings highlight trade-offs between performance, security, resiliency, and\nmaintainability, offering insights for practitioners deploying Kubernetes in\nedge environments."
    },
    {
        "date": "2025-03",
        "title": "Robust time series generation via Schr\u00f6dinger Bridge: a comprehensive evaluation",
        "author": "Alexandre Alouadi, Baptiste Barreau, Laurent Carlier, and Huy\u00ean Pham",
        "link": "http://arxiv.org/abs/2503.02943v2",
        "abstract": "We investigate the generative capabilities of the Schr\\\"odinger Bridge (SB)\napproach for time series. The SB framework formulates time series synthesis as\nan entropic optimal interpolation transport problem between a reference\nprobability measure on path space and a target joint distribution. This results\nin a stochastic differential equation over a finite horizon that accurately\ncaptures the temporal dynamics of the target time series. While the SB approach\nhas been largely explored in fields like image generation, there is a scarcity\nof studies for its application to time series. In this work, we bridge this gap\nby conducting a comprehensive evaluation of the SB method's robustness and\ngenerative performance. We benchmark it against state-of-the-art (SOTA) time\nseries generation methods across diverse datasets, assessing its strengths,\nlimitations, and capacity to model complex temporal dependencies. Our results\noffer valuable insights into the SB framework's potential as a versatile and\nrobust tool for time series generation."
    },
    {
        "date": "2025-03",
        "title": "A Causal Framework for Aligning Image Quality Metrics and Deep Neural Network Robustness",
        "author": "Nathan Drenkow, and Mathias Unberath",
        "link": "http://arxiv.org/abs/2503.02797v1",
        "abstract": "Image quality plays an important role in the performance of deep neural\nnetworks (DNNs) and DNNs have been widely shown to exhibit sensitivity to\nchanges in imaging conditions. Large-scale datasets often contain images under\na wide range of conditions prompting a need to quantify and understand their\nunderlying quality distribution in order to better characterize DNN performance\nand robustness. Aligning the sensitivities of image quality metrics and DNNs\nensures that estimates of quality can act as proxies for image/dataset\ndifficulty independent of the task models trained/evaluated on the data.\nConventional image quality assessment (IQA) seeks to measure and align quality\nrelative to human perceptual judgments, but here we seek a quality measure that\nis not only sensitive to imaging conditions but also well-aligned with DNN\nsensitivities. We first ask whether conventional IQA metrics are also\ninformative of DNN performance. In order to answer this question, we reframe\nIQA from a causal perspective and examine conditions under which quality\nmetrics are predictive of DNN performance. We show theoretically and\nempirically that current IQA metrics are weak predictors of DNN performance in\nthe context of classification. We then use our causal framework to provide an\nalternative formulation and a new image quality metric that is more strongly\ncorrelated with DNN performance and can act as a prior on performance without\ntraining new task models. Our approach provides a means to directly estimate\nthe quality distribution of large-scale image datasets towards characterizing\nthe relationship between dataset composition and DNN performance."
    },
    {
        "date": "2025-03",
        "title": "Quantitative Resilience Modeling for Autonomous Cyber Defense",
        "author": "Xavier Cadet, Simona Boboila, Edward Koh, Peter Chin, and Alina Oprea",
        "link": "http://arxiv.org/abs/2503.02780v1",
        "abstract": "Cyber resilience is the ability of a system to recover from an attack with\nminimal impact on system operations. However, characterizing a network's\nresilience under a cyber attack is challenging, as there are no formal\ndefinitions of resilience applicable to diverse network topologies and attack\npatterns. In this work, we propose a quantifiable formulation of resilience\nthat considers multiple defender operational goals, the criticality of various\nnetwork resources for daily operations, and provides interpretability to\nsecurity operators about their system's resilience under attack. We evaluate\nour approach within the CybORG environment, a reinforcement learning (RL)\nframework for autonomous cyber defense, analyzing trade-offs between\nresilience, costs, and prioritization of operational goals. Furthermore, we\nintroduce methods to aggregate resilience metrics across time-variable attack\npatterns and multiple network topologies, comprehensively characterizing system\nresilience. Using insights gained from our resilience metrics, we design RL\nautonomous defensive agents and compare them against several heuristic\nbaselines, showing that proactive network hardening techniques and prompt\nrecovery of compromised machines are critical for effective cyber defenses."
    },
    {
        "date": "2025-03",
        "title": "Optimisation of cyber insurance coverage with selection of cost effective security controls",
        "author": "Ganbayar Uuganbayar, Artsiom Yautsiukhin, Fabio Martinelli, and Fabio Massacci",
        "link": "http://arxiv.org/abs/2503.02706v1",
        "abstract": "Nowadays, cyber threats are considered among the most dangerous risks by top\nmanagement of enterprises. One way to deal with these risks is to insure them,\nbut cyber insurance is still quite expensive. The insurance fee can be reduced\nif organisations improve their cyber security protection, i.e., reducing the\ninsured risk. In other words, organisations need an investment strategy to\ndecide the optimal amount of investments into cyber insurance and\nself-protection. In this work, we propose an approach to help a risk-averse\norganisation to distribute its cyber security investments in a cost-efficient\nway. What makes our approach unique is that next to defining the amount of\ninvestments in cyber insurance and self-protection, our proposal also\nexplicitly defines how these investments should be spent by selecting the most\ncost-efficient security controls. Moreover, we provide an exact algorithm for\nthe control selection problem considering several threats at the same time and\ncompare this algorithm with other approximate algorithmic solutions."
    },
    {
        "date": "2025-03",
        "title": "Weight transport through spike timing for robust local gradients",
        "author": "Timo Gierlich, Andreas Baumbach, Akos F. Kungl, Kevin Max, and Mihai A. Petrovici",
        "link": "http://arxiv.org/abs/2503.02642v1",
        "abstract": "In both machine learning and in computational neuroscience, plasticity in\nfunctional neural networks is frequently expressed as gradient descent on a\ncost. Often, this imposes symmetry constraints that are difficult to reconcile\nwith local computation, as is required for biological networks or neuromorphic\nhardware. For example, wake-sleep learning in networks characterized by\nBoltzmann distributions builds on the assumption of symmetric connectivity.\nSimilarly, the error backpropagation algorithm is notoriously plagued by the\nweight transport problem between the representation and the error stream.\nExisting solutions such as feedback alignment tend to circumvent the problem by\ndeferring to the robustness of these algorithms to weight asymmetry. However,\nthey are known to scale poorly with network size and depth. We introduce\nspike-based alignment learning (SAL), a complementary learning rule for spiking\nneural networks, which uses spike timing statistics to extract and correct the\nasymmetry between effective reciprocal connections. Apart from being\nspike-based and fully local, our proposed mechanism takes advantage of noise.\nBased on an interplay between Hebbian and anti-Hebbian plasticity, synapses can\nthereby recover the true local gradient. This also alleviates discrepancies\nthat arise from neuron and synapse variability -- an omnipresent property of\nphysical neuronal networks. We demonstrate the efficacy of our mechanism using\ndifferent spiking network models. First, we show how SAL can significantly\nimprove convergence to the target distribution in probabilistic spiking\nnetworks as compared to Hebbian plasticity alone. Second, in neuronal\nhierarchies based on cortical microcircuits, we show how our proposed mechanism\neffectively enables the alignment of feedback weights to the forward pathway,\nthus allowing the backpropagation of correct feedback errors."
    },
    {
        "date": "2025-03",
        "title": "LLM-Safety Evaluations Lack Robustness",
        "author": "Tim Beyer, Sophie Xhonneux, Simon Geisler, Gauthier Gidel, Leo Schwinn, and Stephan G\u00fcnnemann",
        "link": "http://arxiv.org/abs/2503.02574v1",
        "abstract": "In this paper, we argue that current safety alignment research efforts for\nlarge language models are hindered by many intertwined sources of noise, such\nas small datasets, methodological inconsistencies, and unreliable evaluation\nsetups. This can, at times, make it impossible to evaluate and compare attacks\nand defenses fairly, thereby slowing progress. We systematically analyze the\nLLM safety evaluation pipeline, covering dataset curation, optimization\nstrategies for automated red-teaming, response generation, and response\nevaluation using LLM judges. At each stage, we identify key issues and\nhighlight their practical impact. We also propose a set of guidelines for\nreducing noise and bias in evaluations of future attack and defense papers.\nLastly, we offer an opposing perspective, highlighting practical reasons for\nexisting limitations. We believe that addressing the outlined problems in\nfuture research will improve the field's ability to generate easily comparable\nresults and make measurable progress."
    },
    {
        "date": "2025-03",
        "title": "Towards a robust R2D2 paradigm for radio-interferometric imaging: revisiting DNN training and architecture",
        "author": "Amir Aghabiglou, Chung San Chu, Chao Tang, Arwa Dabbech, and Yves Wiaux",
        "link": "http://arxiv.org/abs/2503.02554v1",
        "abstract": "The R2D2 Deep Neural Network (DNN) series was recently introduced for image\nformation in radio interferometry. It can be understood as a learned version of\nCLEAN, whose minor cycles are substituted with DNNs. We revisit R2D2 on the\ngrounds of series convergence, training methodology, and DNN architecture,\nimproving its robustness in terms of generalisability beyond training\nconditions, capability to deliver high data fidelity, and epistemic\nuncertainty. Firstly, while still focusing on telescope-specific training, we\nenhance the learning process by randomising Fourier sampling integration times,\nincorporating multi-scan multi-noise configurations, and varying imaging\nsettings, including pixel resolution and visibility-weighting scheme. Secondly,\nwe introduce a convergence criterion whereby the reconstruction process stops\nwhen the data residual is compatible with noise, rather than simply using all\navailable DNNs. This not only increases the reconstruction efficiency by\nreducing its computational cost, but also refines training by pruning out the\ndata/image pairs for which optimal data fidelity is reached before training the\nnext DNN. Thirdly, we substitute R2D2's early U-Net DNN with a novel\narchitecture (U-WDSR) combining U-Net and WDSR, which leverages wide\nactivation, dense connections, weight normalisation, and low-rank convolution\nto improve feature reuse and reconstruction precision. As previously, R2D2 was\ntrained for monochromatic intensity imaging with the Very Large Array (VLA) at\nfixed $512 \\times 512$ image size. Simulations on a wide range of inverse\nproblems and a case study on real data reveal that the new R2D2 model\nconsistently outperforms its earlier version in image reconstruction quality,\ndata fidelity, and epistemic uncertainty."
    },
    {
        "date": "2025-03",
        "title": "Attack Tree Distance: a practical examination of tree difference measurement within cyber security",
        "author": "Nathan D. Schiele, and Olga Gadyatskaya",
        "link": "http://arxiv.org/abs/2503.02499v1",
        "abstract": "CONTEXT. Attack treesare a recommended threat modeling tool, but there is no\nestablished method to compare them. OBJECTIVE. We aim to establish a method to\ncompare \"real\" attack trees, based on both the structure of the tree itself and\nthe meaning of the node labels. METHOD. We define four methods of comparison\n(three novel and one established) and compare them to a dataset of attack trees\ncreated from a study run on students (n = 39). These attack trees all follow\nfrom the same scenario, but have slightly different labels. RESULTS. We find\nthat applying semantic similarity as a means of comparing node labels is a\nvalid approach. Further, we find that treeedit distance (established) and\nradical distance (novel) are themost promising methods of comparison in most\ncircumstances. CONCLUSION. We show that these two methods are valid as means of\ncomparing attack trees, and suggest a novel technique for using semantic\nsimilarity to compare node labels. We further suggest that these methods can be\nused to compare attack trees in a real-world scenario, and that they can be\nused to identify similar attack trees."
    },
    {
        "date": "2025-03",
        "title": "The Distributionally Robust Optimization Model of Sparse Principal Component Analysis",
        "author": "Lei Wang, Xin Liu, and Xiaojun Chen",
        "link": "http://arxiv.org/abs/2503.02494v1",
        "abstract": "We consider sparse principal component analysis (PCA) under a stochastic\nsetting where the underlying probability distribution of the random parameter\nis uncertain. This problem is formulated as a distributionally robust\noptimization (DRO) model based on a constructive approach to capturing\nuncertainty in the covariance matrix, which constitutes a nonsmooth constrained\nmin-max optimization problem. We further prove that the inner maximization\nproblem admits a closed-form solution, reformulating the original DRO model\ninto an equivalent minimization problem on the Stiefel manifold. This\ntransformation leads to a Riemannian optimization problem with intricate\nnonsmooth terms, a challenging formulation beyond the reach of existing\nalgorithms. To address this issue, we devise an efficient smoothing manifold\nproximal gradient algorithm. We prove the Riemannian gradient consistency and\nglobal convergence of our algorithm to a stationary point of the nonsmooth\nminimization problem. Moreover, we establish the iteration complexity of our\nalgorithm. Finally, numerical experiments are conducted to validate the\neffectiveness and scalability of our algorithm, as well as to highlight the\nnecessity and rationality of adopting the DRO model for sparse PCA."
    },
    {
        "date": "2025-03",
        "title": "Deep Robust Reversible Watermarking",
        "author": "Jiale Chen, Wei Wang, Chongyang Shi, Li Dong, Yuanman Li, and Xiping Hu",
        "link": "http://arxiv.org/abs/2503.02490v1",
        "abstract": "Robust Reversible Watermarking (RRW) enables perfect recovery of cover images\nand watermarks in lossless channels while ensuring robust watermark extraction\nin lossy channels. Existing RRW methods, mostly non-deep learning-based, face\ncomplex designs, high computational costs, and poor robustness, limiting their\npractical use. This paper proposes Deep Robust Reversible Watermarking (DRRW),\na deep learning-based RRW scheme. DRRW uses an Integer Invertible Watermark\nNetwork (iIWN) to map integer data distributions invertibly, addressing\nconventional RRW limitations. Unlike traditional RRW, which needs\ndistortion-specific designs, DRRW employs an encoder-noise layer-decoder\nframework for adaptive robustness via end-to-end training. In inference, cover\nimage and watermark map to an overflowed stego image and latent variables,\ncompressed by arithmetic coding into a bitstream embedded via reversible data\nhiding for lossless recovery. We introduce an overflow penalty loss to reduce\npixel overflow, shortening the auxiliary bitstream while enhancing robustness\nand stego image quality. An adaptive weight adjustment strategy avoids manual\nwatermark loss weighting, improving training stability and performance.\nExperiments show DRRW outperforms state-of-the-art RRW methods, boosting\nrobustness and cutting embedding, extraction, and recovery complexities by\n55.14\\(\\times\\), 5.95\\(\\times\\), and 3.57\\(\\times\\), respectively. The\nauxiliary bitstream shrinks by 43.86\\(\\times\\), with reversible embedding\nsucceeding on 16,762 PASCAL VOC 2012 images, advancing practical RRW. DRRW\nexceeds irreversible robust watermarking in robustness and quality while\nmaintaining reversibility."
    },
    {
        "date": "2025-03",
        "title": "Robust detection of overlapping bioacoustic sound events",
        "author": "Louis Mahon, Benjamin Hoffman, Logan S James, Maddie Cusimano, Masato Hagiwara, Sarah C Woolley, and Olivier Pietquin",
        "link": "http://arxiv.org/abs/2503.02389v1",
        "abstract": "We propose a method for accurately detecting bioacoustic sound events that is\nrobust to overlapping events, a common issue in domains such as ethology,\necology and conservation. While standard methods employ a frame-based,\nmulti-label approach, we introduce an onset-based detection method which we\nname Voxaboxen. It takes inspiration from object detection methods in computer\nvision, but simultaneously takes advantage of recent advances in\nself-supervised audio encoders. For each time window, Voxaboxen predicts\nwhether it contains the start of a vocalization and how long the vocalization\nis. It also does the same in reverse, predicting whether each window contains\nthe end of a vocalization, and how long ago it started. The two resulting sets\nof bounding boxes are then fused using a graph-matching algorithm. We also\nrelease a new dataset designed to measure performance on detecting overlapping\nvocalizations. This consists of recordings of zebra finches annotated with\ntemporally-strong labels and showing frequent overlaps. We test Voxaboxen on\nseven existing data sets and on our new data set. We compare Voxaboxen to\nnatural baselines and existing sound event detection methods and demonstrate\nSotA results. Further experiments show that improvements are robust to frequent\nvocalization overlap."
    },
    {
        "date": "2025-03",
        "title": "Towards Robust Multi-UAV Collaboration: MARL with Noise-Resilient Communication and Attention Mechanisms",
        "author": "Zilin Zhao, Chishui Chen, Haotian Shi, Jiale Chen, Xuanlin Yue, Zhejian Yang, and Yang Liu",
        "link": "http://arxiv.org/abs/2503.02913v1",
        "abstract": "Efficient path planning for unmanned aerial vehicles (UAVs) is crucial in\nremote sensing and information collection. As task scales expand, the\ncooperative deployment of multiple UAVs significantly improves information\ncollection efficiency. However, collaborative communication and decision-making\nfor multiple UAVs remain major challenges in path planning, especially in noisy\nenvironments. To efficiently accomplish complex information collection tasks in\n3D space and address robust communication issues, we propose a multi-agent\nreinforcement learning (MARL) framework for UAV path planning based on the\nCounterfactual Multi-Agent Policy Gradients (COMA) algorithm. The framework\nincorporates attention mechanism-based UAV communication protocol and\ntraining-deployment system, significantly improving communication robustness\nand individual decision-making capabilities in noisy conditions. Experiments\nconducted on both synthetic and real-world datasets demonstrate that our method\noutperforms existing algorithms in terms of path planning efficiency and\nrobustness, especially in noisy environments, achieving a 78\\% improvement in\nentropy reduction."
    },
    {
        "date": "2025-03",
        "title": "Low-Level Matters: An Efficient Hybrid Architecture for Robust Multi-frame Infrared Small Target Detection",
        "author": "Zhihua Shen, Siyang Chen, Han Wang, Tongsu Zhang, Xiaohu Zhang, Xiangpeng Xu, and Xia Yang",
        "link": "http://arxiv.org/abs/2503.02220v1",
        "abstract": "Multi-frame infrared small target detection (IRSTD) plays a crucial role in\nlow-altitude and maritime surveillance. The hybrid architecture combining CNNs\nand Transformers shows great promise for enhancing multi-frame IRSTD\nperformance. In this paper, we propose LVNet, a simple yet powerful hybrid\narchitecture that redefines low-level feature learning in hybrid frameworks for\nmulti-frame IRSTD. Our key insight is that the standard linear patch embeddings\nin Vision Transformers are insufficient for capturing the scale-sensitive local\nfeatures critical to infrared small targets. To address this limitation, we\nintroduce a multi-scale CNN frontend that explicitly models local features by\nleveraging the local spatial bias of convolution. Additionally, we design a\nU-shaped video Transformer for multi-frame spatiotemporal context modeling,\neffectively capturing the motion characteristics of targets. Experiments on the\npublicly available datasets IRDST and NUDT-MIRSDT demonstrate that LVNet\noutperforms existing state-of-the-art methods. Notably, compared to the current\nbest-performing method, LMAFormer, LVNet achieves an improvement of 5.63\\% /\n18.36\\% in nIoU, while using only 1/221 of the parameters and 1/92 / 1/21 of\nthe computational cost. Ablation studies further validate the importance of\nlow-level representation learning in hybrid architectures. Our code and trained\nmodels are available at https://github.com/ZhihuaShen/LVNet."
    },
    {
        "date": "2025-03",
        "title": "Client-Aided Secure Two-Party Computation of Dynamic Controllers",
        "author": "Kaoru Teranishi, and Takashi Tanaka",
        "link": "http://arxiv.org/abs/2503.02176v1",
        "abstract": "In this paper, we propose a secure two-party computation protocol for dynamic\ncontrollers using a secret sharing scheme. The proposed protocol realizes\noutsourcing of controller computation to two servers, while controller\nparameters, states, inputs, and outputs are kept secret against the servers.\nUnlike previous encrypted controls in a single-server setting, the proposed\nmethod can operate a dynamic controller for an infinite time horizon without\ncontroller state decryption or input re-encryption. We show that the control\nperformance achievable by the proposed protocol can be made arbitrarily close\nto that attained by the unencrypted controller. Furthermore, system-theoretic\nand cryptographic modifications of the protocol are presented to improve the\ncommunication complexity. The feasibility of the protocol is demonstrated\nthrough numerical examples of PID and observer-based controls."
    },
    {
        "date": "2025-03",
        "title": "Adversarial Tokenization",
        "author": "Renato Lui Geh, Zilei Shao, and Guy Van den Broeck",
        "link": "http://arxiv.org/abs/2503.02174v1",
        "abstract": "Current LLM pipelines account for only one possible tokenization for a given\nstring, ignoring exponentially many alternative tokenizations during training\nand inference. For example, the standard Llama3 tokenization of penguin is\n[p,enguin], yet [peng,uin] is another perfectly valid alternative. In this\npaper, we show that despite LLMs being trained solely on one tokenization, they\nstill retain semantic understanding of other tokenizations, raising questions\nabout their implications in LLM safety. Put succinctly, we answer the following\nquestion: can we adversarially tokenize an obviously malicious string to evade\nsafety and alignment restrictions? We show that not only is adversarial\ntokenization an effective yet previously neglected axis of attack, but it is\nalso competitive against existing state-of-the-art adversarial approaches\nwithout changing the text of the harmful request. We empirically validate this\nexploit across three state-of-the-art LLMs and adversarial datasets, revealing\na previously unknown vulnerability in subword models."
    },
    {
        "date": "2025-03",
        "title": "DDAD: A Two-pronged Adversarial Defense Based on Distributional Discrepancy",
        "author": "Jiacheng Zhang, Benjamin I. P. Rubinstein, Jingfeng Zhang, and Feng Liu",
        "link": "http://arxiv.org/abs/2503.02169v1",
        "abstract": "Statistical adversarial data detection (SADD) detects whether an upcoming\nbatch contains adversarial examples (AEs) by measuring the distributional\ndiscrepancies between clean examples (CEs) and AEs. In this paper, we reveal\nthe potential strength of SADD-based methods by theoretically showing that\nminimizing distributional discrepancy can help reduce the expected loss on AEs.\nNevertheless, despite these advantages, SADD-based methods have a potential\nlimitation: they discard inputs that are detected as AEs, leading to the loss\nof clean information within those inputs. To address this limitation, we\npropose a two-pronged adversarial defense method, named\nDistributional-Discrepancy-based Adversarial Defense (DDAD). In the training\nphase, DDAD first optimizes the test power of the maximum mean discrepancy\n(MMD) to derive MMD-OPT, and then trains a denoiser by minimizing the MMD-OPT\nbetween CEs and AEs. In the inference phase, DDAD first leverages MMD-OPT to\ndifferentiate CEs and AEs, and then applies a two-pronged process: (1) directly\nfeeding the detected CEs into the classifier, and (2) removing noise from the\ndetected AEs by the distributional-discrepancy-based denoiser. Extensive\nexperiments show that DDAD outperforms current state-of-the-art (SOTA) defense\nmethods by notably improving clean and robust accuracy on CIFAR-10 and\nImageNet-1K against adaptive white-box attacks."
    },
    {
        "date": "2025-03",
        "title": "Generalized Diffusion Detector: Mining Robust Features from Diffusion Models for Domain-Generalized Detection",
        "author": "Boyong He, Yuxiang Ji, Qianwen Ye, Zhuoyue Tan, and Liaoni Wu",
        "link": "http://arxiv.org/abs/2503.02101v1",
        "abstract": "Domain generalization (DG) for object detection aims to enhance detectors'\nperformance in unseen scenarios. This task remains challenging due to complex\nvariations in real-world applications. Recently, diffusion models have\ndemonstrated remarkable capabilities in diverse scene generation, which\ninspires us to explore their potential for improving DG tasks. Instead of\ngenerating images, our method extracts multi-step intermediate features during\nthe diffusion process to obtain domain-invariant features for generalized\ndetection. Furthermore, we propose an efficient knowledge transfer framework\nthat enables detectors to inherit the generalization capabilities of diffusion\nmodels through feature and object-level alignment, without increasing inference\ntime. We conduct extensive experiments on six challenging DG benchmarks. The\nresults demonstrate that our method achieves substantial improvements of 14.0%\nmAP over existing DG approaches across different domains and corruption types.\nNotably, our method even outperforms most domain adaptation methods without\naccessing any target domain data. Moreover, the diffusion-guided detectors show\nconsistent improvements of 15.9% mAP on average compared to the baseline. Our\nwork aims to present an effective approach for domain-generalized detection and\nprovide potential insights for robust visual recognition in real-world\nscenarios. The code is available at\n\\href{https://github.com/heboyong/Generalized-Diffusion-Detector}{Generalized\nDiffusion Detector}"
    },
    {
        "date": "2025-03",
        "title": "Robustness to Geographic Distribution Shift using Location Encoders",
        "author": "Ruth Crasto",
        "link": "http://arxiv.org/abs/2503.02036v1",
        "abstract": "Geographic distribution shift arises when the distribution of locations on\nEarth in a training dataset is different from what is seen at test time. The\nmost common approaches to tackling geographic distribution shift treat regions\ndelimited by administrative boundaries such as countries or continents as\nseparate domains and apply standard domain adaptation methods, ignoring\ngeographic coordinates that are often available as metadata. This paper\nproposes the use of location encoders for training models that are more robust\nto geographic distribution shift. We show how both simple sine-cosine encoders\nand pre-trained location encoders can be used to improve standard domain\nadaptation methods for the special case of geographic distribution shift. Our\nproposed methods achieve state-of-the-art results on geo-tagged imagery\ndatasets from the WILDS benchmark."
    },
    {
        "date": "2025-03",
        "title": "SLAP: Secure Location-proof and Anonymous Privacy-preserving Spectrum Access",
        "author": "Saleh Darzi, and Attila A. Yavuz",
        "link": "http://arxiv.org/abs/2503.02019v1",
        "abstract": "The rapid advancements in wireless technology have significantly increased\nthe demand for communication resources, leading to the development of Spectrum\nAccess Systems (SAS). However, network regulations require disclosing sensitive\nuser information, such as location coordinates and transmission details,\nraising critical privacy concerns. Moreover, as a database-driven architecture\nreliant on user-provided data, SAS necessitates robust location verification to\ncounter identity and location spoofing attacks and remains a primary target for\ndenial-of-service (DoS) attacks. Addressing these security challenges while\nadhering to regulatory requirements is essential. In this paper, we propose\nSLAP, a novel framework that ensures location privacy and anonymity during\nspectrum queries, usage notifications, and location-proof acquisition. Our\nsolution includes an adaptive dual-scenario location verification mechanism\nwith architectural flexibility and a fallback option, along with a counter-DoS\napproach using time-lock puzzles. We prove the security of SLAP and demonstrate\nits advantages over existing solutions through comprehensive performance\nevaluations."
    },
    {
        "date": "2025-03",
        "title": "A Lightweight and Secure Deep Learning Model for Privacy-Preserving Federated Learning in Intelligent Enterprises",
        "author": "Reza Fotohi, Fereidoon Shams Aliee, and Bahar Farahani",
        "link": "http://arxiv.org/abs/2503.02017v1",
        "abstract": "The ever growing Internet of Things (IoT) connections drive a new type of\norganization, the Intelligent Enterprise. In intelligent enterprises, machine\nlearning based models are adopted to extract insights from data. Due to the\nefficiency and privacy challenges of these traditional models, a new federated\nlearning (FL) paradigm has emerged. In FL, multiple enterprises can jointly\ntrain a model to update a final model. However, firstly, FL trained models\nusually perform worse than centralized models, especially when enterprises\ntraining data is non-IID (Independent and Identically Distributed). Second, due\nto the centrality of FL and the untrustworthiness of local enterprises,\ntraditional FL solutions are vulnerable to poisoning and inference attacks and\nviolate privacy. Thirdly, the continuous transfer of parameters between\nenterprises and servers increases communication costs. To this end, the\nFedAnil+ model is proposed, a novel, lightweight, and secure Federated Deep\nLearning Model that includes three main phases. In the first phase, the goal is\nto solve the data type distribution skew challenge. Addressing privacy concerns\nagainst poisoning and inference attacks is covered in the second phase.\nFinally, to alleviate the communication overhead, a novel compression approach\nis proposed that significantly reduces the size of the updates. The experiment\nresults validate that FedAnil+ is secure against inference and poisoning\nattacks with better accuracy. In addition, it shows improvements over existing\napproaches in terms of model accuracy (13%, 16%, and 26%), communication cost\n(17%, 21%, and 25%), and computation cost (7%, 9%, and 11%)."
    },
    {
        "date": "2025-03",
        "title": "Recurrence-Enhanced Vision-and-Language Transformers for Robust Multimodal Document Retrieval",
        "author": "Davide Caffagni, Sara Sarto, Marcella Cornia, Lorenzo Baraldi, and Rita Cucchiara",
        "link": "http://arxiv.org/abs/2503.01980v1",
        "abstract": "Cross-modal retrieval is gaining increasing efficacy and interest from the\nresearch community, thanks to large-scale training, novel architectural and\nlearning designs, and its application in LLMs and multimodal LLMs. In this\npaper, we move a step forward and design an approach that allows for multimodal\nqueries, composed of both an image and a text, and can search within\ncollections of multimodal documents, where images and text are interleaved. Our\nmodel, ReT, employs multi-level representations extracted from different layers\nof both visual and textual backbones, both at the query and document side. To\nallow for multi-level and cross-modal understanding and feature extraction, ReT\nemploys a novel Transformer-based recurrent cell that integrates both textual\nand visual features at different layers, and leverages sigmoidal gates inspired\nby the classical design of LSTMs. Extensive experiments on M2KR and M-BEIR\nbenchmarks show that ReT achieves state-of-the-art performance across diverse\nsettings. Our source code and trained models are publicly available at\nhttps://github.com/aimagelab/ReT."
    },
    {
        "date": "2025-03",
        "title": "AutoAdvExBench: Benchmarking autonomous exploitation of adversarial example defenses",
        "author": "Nicholas Carlini, Javier Rando, Edoardo Debenedetti, Milad Nasr, and Florian Tram\u00e8r",
        "link": "http://arxiv.org/abs/2503.01811v1",
        "abstract": "We introduce AutoAdvExBench, a benchmark to evaluate if large language models\n(LLMs) can autonomously exploit defenses to adversarial examples. Unlike\nexisting security benchmarks that often serve as proxies for real-world tasks,\nbench directly measures LLMs' success on tasks regularly performed by machine\nlearning security experts. This approach offers a significant advantage: if a\nLLM could solve the challenges presented in bench, it would immediately present\npractical utility for adversarial machine learning researchers. We then design\na strong agent that is capable of breaking 75% of CTF-like (\"homework\nexercise\") adversarial example defenses. However, we show that this agent is\nonly able to succeed on 13% of the real-world defenses in our benchmark,\nindicating the large gap between difficulty in attacking \"real\" code, and\nCTF-like code. In contrast, a stronger LLM that can attack 21% of real defenses\nonly succeeds on 54% of CTF-like defenses. We make this benchmark available at\nhttps://github.com/ethz-spylab/AutoAdvExBench."
    },
    {
        "date": "2025-03",
        "title": "Protecting DeFi Platforms against Non-Price Flash Loan Attacks",
        "author": "Abdulrahman Alhaidari, Balaji Palanisamy, and Prashant Krishnamurthy",
        "link": "http://arxiv.org/abs/2503.01944v1",
        "abstract": "Smart contracts in Decentralized Finance (DeFi) platforms are attractive\ntargets for attacks as their vulnerabilities can lead to massive amounts of\nfinancial losses. Flash loan attacks, in particular, pose a major threat to\nDeFi protocols that hold a Total Value Locked (TVL) exceeding \\$106 billion.\nThese attacks use the atomicity property of blockchains to drain funds from\nsmart contracts in a single transaction. While existing research primarily\nfocuses on price manipulation attacks, such as oracle manipulation, mitigating\nnon-price flash loan attacks that often exploit smart contracts' zero-day\nvulnerabilities remains largely unaddressed. These attacks are challenging to\ndetect because of their unique patterns, time sensitivity, and complexity. In\nthis paper, we present FlashGuard, a runtime detection and mitigation method\nfor non-price flash loan attacks. Our approach targets smart contract function\nsignatures to identify attacks in real-time and counterattack by disrupting the\nattack transaction atomicity by leveraging the short window when transactions\nare visible in the mempool but not yet confirmed. When FlashGuard detects an\nattack, it dispatches a stealthy dusting counterattack transaction to miners to\nchange the victim contract's state which disrupts the attack's atomicity and\nforces the attack transaction to revert. We evaluate our approach using 20\nhistorical attacks and several unseen attacks. FlashGuard achieves an average\nreal-time detection latency of 150.31ms, a detection accuracy of over 99.93\\%,\nand an average disruption time of 410.92ms. FlashGuard could have potentially\nrescued over \\$405.71 million in losses if it were deployed prior to these\nattack instances. FlashGuard demonstrates significant potential as a DeFi\nsecurity solution to mitigate and handle rising threats of non-price flash loan\nattacks."
    },
    {
        "date": "2025-03",
        "title": "Zero-Trust Artificial Intelligence Model Security Based on Moving Target Defense and Content Disarm and Reconstruction",
        "author": "Daniel Gilkarov, and Ran Dubin",
        "link": "http://arxiv.org/abs/2503.01758v1",
        "abstract": "This paper examines the challenges in distributing AI models through model\nzoos and file transfer mechanisms. Despite advancements in security measures,\nvulnerabilities persist, necessitating a multi-layered approach to mitigate\nrisks effectively. The physical security of model files is critical, requiring\nstringent access controls and attack prevention solutions. This paper proposes\na novel solution architecture composed of two prevention approaches. The first\nis Content Disarm and Reconstruction (CDR), which focuses on disarming\nserialization attacks that enable attackers to run malicious code as soon as\nthe model is loaded. The second is protecting the model architecture and\nweights from attacks by using Moving Target Defense (MTD), alerting the model\nstructure, and providing verification steps to detect such attacks. The paper\nfocuses on the highly exploitable Pickle and PyTorch file formats. It\ndemonstrates a 100% disarm rate while validated against known AI model\nrepositories and actual malware attacks from the HuggingFace model zoo."
    },
    {
        "date": "2025-03",
        "title": "Adversarial Agents: Black-Box Evasion Attacks with Reinforcement Learning",
        "author": "Kyle Domico, Jean-Charles Noirot Ferrand, Ryan Sheatsley, Eric Pauley, Josiah Hanna, and Patrick McDaniel",
        "link": "http://arxiv.org/abs/2503.01734v1",
        "abstract": "Reinforcement learning (RL) offers powerful techniques for solving complex\nsequential decision-making tasks from experience. In this paper, we demonstrate\nhow RL can be applied to adversarial machine learning (AML) to develop a new\nclass of attacks that learn to generate adversarial examples: inputs designed\nto fool machine learning models. Unlike traditional AML methods that craft\nadversarial examples independently, our RL-based approach retains and exploits\npast attack experience to improve future attacks. We formulate adversarial\nexample generation as a Markov Decision Process and evaluate RL's ability to\n(a) learn effective and efficient attack strategies and (b) compete with\nstate-of-the-art AML. On CIFAR-10, our agent increases the success rate of\nadversarial examples by 19.4% and decreases the median number of victim model\nqueries per adversarial example by 53.2% from the start to the end of training.\nIn a head-to-head comparison with a state-of-the-art image attack,\nSquareAttack, our approach enables an adversary to generate adversarial\nexamples with 13.1% more success after 5000 episodes of training. From a\nsecurity perspective, this work demonstrates a powerful new attack vector that\nuses RL to attack ML models efficiently and at scale."
    },
    {
        "date": "2025-03",
        "title": "Perceptual Motor Learning with Active Inference Framework for Robust Lateral Control",
        "author": "Elahe Delavari, John Moore, Junho Hong, and Jaerock Kwon",
        "link": "http://arxiv.org/abs/2503.01676v2",
        "abstract": "This paper presents a novel Perceptual Motor Learning (PML) framework\nintegrated with Active Inference (AIF) to enhance lateral control in Highly\nAutomated Vehicles (HAVs). PML, inspired by human motor learning, emphasizes\nthe seamless integration of perception and action, enabling efficient\ndecision-making in dynamic environments. Traditional autonomous driving\napproaches--including modular pipelines, imitation learning, and reinforcement\nlearning--struggle with adaptability, generalization, and computational\nefficiency. In contrast, PML with AIF leverages a generative model to minimize\nprediction error (\"surprise\") and actively shape vehicle control based on\nlearned perceptual-motor representations. Our approach unifies deep learning\nwith active inference principles, allowing HAVs to perform lane-keeping\nmaneuvers with minimal data and without extensive retraining across different\nenvironments. Extensive experiments in the CARLA simulator demonstrate that PML\nwith AIF enhances adaptability without increasing computational overhead while\nachieving performance comparable to conventional methods. These findings\nhighlight the potential of PML-driven active inference as a robust alternative\nfor real-world autonomous driving applications."
    },
    {
        "date": "2025-03",
        "title": "Robust Palm-Vein Recognition Using the MMD Filter: Improving SIFT-Based Feature Matching",
        "author": "Kaveen Perera, Fouad Khelifi, and Ammar Belatreche",
        "link": "http://arxiv.org/abs/2503.01612v1",
        "abstract": "A major challenge with palm vein images is that slight movements of the\nfingers and thumb, or variations in hand posture, can stretch the skin in\ndifferent areas and alter the vein patterns. This can result in an infinite\nnumber of variations in palm vein images for a given individual. This paper\nintroduces a novel filtering technique for SIFT-based feature matching, known\nas the Mean and Median Distance (MMD) Filter. This method evaluates the\ndifferences in keypoint coordinates and computes the mean and median in each\ndirection to eliminate incorrect matches. Experiments conducted on the 850nm\nsubset of the CASIA dataset indicate that the proposed MMD filter effectively\npreserves correct points while reducing false positives detected by other\nfiltering methods. A comparison with existing SIFT-based palm vein recognition\nsystems demonstrates that the proposed MMD filter delivers outstanding\nperformance, achieving lower Equal Error Rate (EER) values. This article\npresents an extended author's version based on our previous work, A Keypoint\nFiltering Method for SIFT based Palm-Vein Recognition."
    }
]