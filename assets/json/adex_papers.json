[
    {
        "date": "2025-04",
        "title": "CO-DEFEND: Continuous Decentralized Federated Learning for Secure DoH-Based Threat Detection",
        "author": "Diego Cajaraville-Aboy, Marta Moure-Garrido, Carlos Beis-Penedo, Carlos Garcia-Rubio, Rebeca P. D\u00edaz-Redondo, Celeste Campo, Ana Fern\u00e1ndez-Vilas, and Manuel Fern\u00e1ndez-Veiga",
        "link": "http://arxiv.org/abs/2504.01882v1",
        "abstract": "The use of DNS over HTTPS (DoH) tunneling by an attacker to hide malicious\nactivity within encrypted DNS traffic poses a serious threat to network\nsecurity, as it allows malicious actors to bypass traditional monitoring and\nintrusion detection systems while evading detection by conventional traffic\nanalysis techniques. Machine Learning (ML) techniques can be used to detect DoH\ntunnels; however, their effectiveness relies on large datasets containing both\nbenign and malicious traffic. Sharing such datasets across entities is\nchallenging due to privacy concerns. In this work, we propose CO-DEFEND\n(Continuous Decentralized Federated Learning for Secure DoH-Based Threat\nDetection), a Decentralized Federated Learning (DFL) framework that enables\nmultiple entities to collaboratively train a classification machine learning\nmodel while preserving data privacy and enhancing resilience against single\npoints of failure. The proposed DFL framework, which is scalable and\nprivacy-preserving, is based on a federation process that allows multiple\nentities to train online their local models using incoming DoH flows in real\ntime as they are processed by the entity. In addition, we adapt four classical\nmachine learning algorithms, Support Vector Machines (SVM), Logistic Regression\n(LR), Decision Trees (DT), and Random Forest (RF), for federated scenarios,\ncomparing their results with more computationally complex alternatives such as\nneural networks. We compare our proposed method by using the dataset\nCIRA-CIC-DoHBrw-2020 with existing machine learning approaches to demonstrate\nits effectiveness in detecting malicious DoH tunnels and the benefits it\nbrings."
    },
    {
        "date": "2025-04",
        "title": "An Approach to Technical AGI Safety and Security",
        "author": "Rohin Shah, Alex Irpan, Alexander Matt Turner, Anna Wang, Arthur Conmy, David Lindner, Jonah Brown-Cohen, Lewis Ho, Neel Nanda, Raluca Ada Popa, Rishub Jain, Rory Greig, Samuel Albanie, Scott Emmons, Sebastian Farquhar, S\u00e9bastien Krier, Senthooran Rajamanoharan, Sophie Bridgers, Tobi Ijitoye, Tom Everitt, Victoria Krakovna, Vikrant Varma, Vladimir Mikulik, Zachary Kenton, Dave Orr, Shane Legg, Noah Goodman, Allan Dafoe, Four Flynn, and Anca Dragan",
        "link": "http://arxiv.org/abs/2504.01849v1",
        "abstract": "Artificial General Intelligence (AGI) promises transformative benefits but\nalso presents significant risks. We develop an approach to address the risk of\nharms consequential enough to significantly harm humanity. We identify four\nareas of risk: misuse, misalignment, mistakes, and structural risks. Of these,\nwe focus on technical approaches to misuse and misalignment. For misuse, our\nstrategy aims to prevent threat actors from accessing dangerous capabilities,\nby proactively identifying dangerous capabilities, and implementing robust\nsecurity, access restrictions, monitoring, and model safety mitigations. To\naddress misalignment, we outline two lines of defense. First, model-level\nmitigations such as amplified oversight and robust training can help to build\nan aligned model. Second, system-level security measures such as monitoring and\naccess control can mitigate harm even if the model is misaligned. Techniques\nfrom interpretability, uncertainty estimation, and safer design patterns can\nenhance the effectiveness of these mitigations. Finally, we briefly outline how\nthese ingredients could be combined to produce safety cases for AGI systems."
    },
    {
        "date": "2025-04",
        "title": "Autonomous optical navigation for DESTINY+: Enhancing misalignment robustness in flyby observations with a rotating telescope",
        "author": "Takayuki Hosonuma, Takeshi Miyabara, Naoya Ozaki, Ko Ishibashi, Yuta Suzaki, Peng Hong, Masayuki Ohta, and Takeshi Takashima",
        "link": "http://arxiv.org/abs/2504.01835v1",
        "abstract": "DESTINY+ is an upcoming JAXA Epsilon medium-class mission to flyby multiple\nasteroids including Phaethon. As an asteroid flyby observation instrument, a\ntelescope mechanically capable of single-axis rotation, named TCAP, is mounted\non the spacecraft to track and observe the target asteroids during flyby. As in\npast flyby missions utilizing rotating telescopes, TCAP is also used as a\nnavigation camera for autonomous optical navigation during the closest-approach\nphase. To mitigate the degradation of the navigation accuracy, past missions\nperformed calibration of the navigation camera's alignment before starting\noptical navigation. However, such calibration requires significant operational\ntime to complete and imposes constraints on the operation sequence. From the\nabove background, the DESTINY+ team has studied the possibility of reducing\noperational costs by allowing TCAP alignment errors to remain. This paper\ndescribes an autonomous optical navigation algorithm robust to the misalignment\nof rotating telescopes, proposed in this context. In the proposed method, the\nmisalignment of the telescope is estimated simultaneously with the spacecraft's\norbit relative to the flyby target. To deal with the nonlinearity between the\nmisalignment and the observation value, the proposed method utilizes the\nunscented Kalman filter, instead of the extended Kalman filter widely used in\npast studies. The proposed method was evaluated with numerical simulations on a\nPC and with hardware-in-the-loop simulation, taking the Phaethon flyby in the\nDESTINY+ mission as an example. The validation results suggest that the\nproposed method can mitigate the misalignment-induced degradation of the\noptical navigation accuracy with reasonable computational costs suited for\nonboard computers."
    },
    {
        "date": "2025-04",
        "title": "Implicit Bias Injection Attacks against Text-to-Image Diffusion Models",
        "author": "Huayang Huang, Xiangye Jin, Jiaxu Miao, and Yu Wu",
        "link": "http://arxiv.org/abs/2504.01819v1",
        "abstract": "The proliferation of text-to-image diffusion models (T2I DMs) has led to an\nincreased presence of AI-generated images in daily life. However, biased T2I\nmodels can generate content with specific tendencies, potentially influencing\npeople's perceptions. Intentional exploitation of these biases risks conveying\nmisleading information to the public. Current research on bias primarily\naddresses explicit biases with recognizable visual patterns, such as skin color\nand gender. This paper introduces a novel form of implicit bias that lacks\nexplicit visual features but can manifest in diverse ways across various\nsemantic contexts. This subtle and versatile nature makes this bias challenging\nto detect, easy to propagate, and adaptable to a wide range of scenarios. We\nfurther propose an implicit bias injection attack framework (IBI-Attacks)\nagainst T2I diffusion models by precomputing a general bias direction in the\nprompt embedding space and adaptively adjusting it based on different inputs.\nOur attack module can be seamlessly integrated into pre-trained diffusion\nmodels in a plug-and-play manner without direct manipulation of user input or\nmodel retraining. Extensive experiments validate the effectiveness of our\nscheme in introducing bias through subtle and diverse modifications while\npreserving the original semantics. The strong concealment and transferability\nof our attack across various scenarios further underscore the significance of\nour approach. Code is available at https://github.com/Hannah1102/IBI-attacks."
    },
    {
        "date": "2025-04",
        "title": "AdPO: Enhancing the Adversarial Robustness of Large Vision-Language Models with Preference Optimization",
        "author": "Chaohu Liu, Tianyi Gui, Yu Liu, and Linli Xu",
        "link": "http://arxiv.org/abs/2504.01735v1",
        "abstract": "Large Vision-Language Models (LVLMs), such as GPT-4o and LLaVA, have recently\nwitnessed remarkable advancements and are increasingly being deployed in\nreal-world applications. However, inheriting the sensitivity of visual neural\nnetworks, LVLMs remain vulnerable to adversarial attacks, which can result in\nerroneous or malicious outputs. While existing efforts utilize adversarial\nfine-tuning to enhance robustness, they often suffer from performance\ndegradation on clean inputs. In this paper, we proposes AdPO, a novel\nadversarial defense strategy for LVLMs based on preference optimization. For\nthe first time, we reframe adversarial training as a preference optimization\nproblem, aiming to enhance the model's preference for generating normal outputs\non clean inputs while rejecting the potential misleading outputs for\nadversarial examples. Notably, AdPO achieves this by solely modifying the image\nencoder, e.g., CLIP ViT, resulting in superior clean and adversarial\nperformance in a variety of downsream tasks. Considering that training involves\nlarge language models (LLMs), the computational cost increases significantly.\nWe validate that training on smaller LVLMs and subsequently transferring to\nlarger models can achieve competitive performance while maintaining efficiency\ncomparable to baseline methods. Our comprehensive experiments confirm the\neffectiveness of the proposed AdPO, which provides a novel perspective for\nfuture adversarial defense research."
    },
    {
        "date": "2025-04",
        "title": "DreamActor-M1: Holistic, Expressive and Robust Human Image Animation with Hybrid Guidance",
        "author": "Yuxuan Luo, Zhengkun Rong, Lizhen Wang, Longhao Zhang, Tianshu Hu, and Yongming Zhu",
        "link": "http://arxiv.org/abs/2504.01724v2",
        "abstract": "While recent image-based human animation methods achieve realistic body and\nfacial motion synthesis, critical gaps remain in fine-grained holistic\ncontrollability, multi-scale adaptability, and long-term temporal coherence,\nwhich leads to their lower expressiveness and robustness. We propose a\ndiffusion transformer (DiT) based framework, DreamActor-M1, with hybrid\nguidance to overcome these limitations. For motion guidance, our hybrid control\nsignals that integrate implicit facial representations, 3D head spheres, and 3D\nbody skeletons achieve robust control of facial expressions and body movements,\nwhile producing expressive and identity-preserving animations. For scale\nadaptation, to handle various body poses and image scales ranging from\nportraits to full-body views, we employ a progressive training strategy using\ndata with varying resolutions and scales. For appearance guidance, we integrate\nmotion patterns from sequential frames with complementary visual references,\nensuring long-term temporal coherence for unseen regions during complex\nmovements. Experiments demonstrate that our method outperforms the\nstate-of-the-art works, delivering expressive results for portraits,\nupper-body, and full-body generation with robust long-term consistency. Project\nPage: https://grisoon.github.io/DreamActor-M1/."
    },
    {
        "date": "2025-04",
        "title": "TransforMerger: Transformer-based Voice-Gesture Fusion for Robust Human-Robot Communication",
        "author": "Petr Vanc, and Karla Stepanova",
        "link": "http://arxiv.org/abs/2504.01708v1",
        "abstract": "As human-robot collaboration advances, natural and flexible communication\nmethods are essential for effective robot control. Traditional methods relying\non a single modality or rigid rules struggle with noisy or misaligned data as\nwell as with object descriptions that do not perfectly fit the predefined\nobject names (e.g. 'Pick that red object'). We introduce TransforMerger, a\ntransformer-based reasoning model that infers a structured action command for\nrobotic manipulation based on fused voice and gesture inputs. Our approach\nmerges multimodal data into a single unified sentence, which is then processed\nby the language model. We employ probabilistic embeddings to handle uncertainty\nand we integrate contextual scene understanding to resolve ambiguous references\n(e.g., gestures pointing to multiple objects or vague verbal cues like \"this\").\nWe evaluate TransforMerger in simulated and real-world experiments,\ndemonstrating its robustness to noise, misalignment, and missing information.\nOur results show that TransforMerger outperforms deterministic baselines,\nespecially in scenarios requiring more contextual knowledge, enabling more\nrobust and flexible human-robot communication. Code and datasets are available\nat: http://imitrob.ciirc.cvut.cz/publications/transformerger."
    },
    {
        "date": "2025-04",
        "title": "Overlap-Aware Feature Learning for Robust Unsupervised Domain Adaptation for 3D Semantic Segmentation",
        "author": "Junjie Chen, Yuecong Xu, Haosheng Li, and Kemi Ding",
        "link": "http://arxiv.org/abs/2504.01668v1",
        "abstract": "3D point cloud semantic segmentation (PCSS) is a cornerstone for\nenvironmental perception in robotic systems and autonomous driving, enabling\nprecise scene understanding through point-wise classification. While\nunsupervised domain adaptation (UDA) mitigates label scarcity in PCSS, existing\nmethods critically overlook the inherent vulnerability to real-world\nperturbations (e.g., snow, fog, rain) and adversarial distortions. This work\nfirst identifies two intrinsic limitations that undermine current PCSS-UDA\nrobustness: (a) unsupervised features overlap from unaligned boundaries in\nshared-class regions and (b) feature structure erosion caused by\ndomain-invariant learning that suppresses target-specific patterns. To address\nthe proposed problems, we propose a tripartite framework consisting of: 1) a\nrobustness evaluation model quantifying resilience against adversarial\nattack/corruption types through robustness metrics; 2) an invertible attention\nalignment module (IAAM) enabling bidirectional domain mapping while preserving\ndiscriminative structure via attention-guided overlap suppression; and 3) a\ncontrastive memory bank with quality-aware contrastive learning that\nprogressively refines pseudo-labels with feature quality for more\ndiscriminative representations. Extensive experiments on\nSynLiDAR-to-SemanticPOSS adaptation demonstrate a maximum mIoU improvement of\n14.3\\% under adversarial attack."
    },
    {
        "date": "2025-04",
        "title": "Robust Unsupervised Domain Adaptation for 3D Point Cloud Segmentation Under Source Adversarial Attacks",
        "author": "Haosheng Li, Junjie Chen, Yuecong Xu, and Kemi Ding",
        "link": "http://arxiv.org/abs/2504.01659v2",
        "abstract": "Unsupervised domain adaptation (UDA) frameworks have shown good\ngeneralization capabilities for 3D point cloud semantic segmentation models on\nclean data. However, existing works overlook adversarial robustness when the\nsource domain itself is compromised. To comprehensively explore the robustness\nof the UDA frameworks, we first design a stealthy adversarial point cloud\ngeneration attack that can significantly contaminate datasets with only minor\nperturbations to the point cloud surface. Based on that, we propose a novel\ndataset, AdvSynLiDAR, comprising synthesized contaminated LiDAR point clouds.\nWith the generated corrupted data, we further develop the Adversarial\nAdaptation Framework (AAF) as the countermeasure. Specifically, by extending\nthe key point sensitive (KPS) loss towards the Robust Long-Tail loss (RLT loss)\nand utilizing a decoder branch, our approach enables the model to focus on\nlong-tail classes during the pre-training phase and leverages high-confidence\ndecoded point cloud information to restore point cloud structures during the\nadaptation phase. We evaluated our AAF method on the AdvSynLiDAR dataset, where\nthe results demonstrate that our AAF method can mitigate performance\ndegradation under source adversarial perturbations for UDA in the 3D point\ncloud segmentation application."
    },
    {
        "date": "2025-04",
        "title": "Benchmarking the Spatial Robustness of DNNs via Natural and Adversarial Localized Corruptions",
        "author": "Giulia Marchiori Pietrosanti, Giulio Rossolini, Alessandro Biondi, and Giorgio Buttazzo",
        "link": "http://arxiv.org/abs/2504.01632v1",
        "abstract": "The robustness of DNNs is a crucial factor in safety-critical applications,\nparticularly in complex and dynamic environments where localized corruptions\ncan arise. While previous studies have evaluated the robustness of semantic\nsegmentation (SS) models under whole-image natural or adversarial corruptions,\na comprehensive investigation into the spatial robustness of dense vision\nmodels under localized corruptions remained underexplored. This paper fills\nthis gap by introducing specialized metrics for benchmarking the spatial\nrobustness of segmentation models, alongside with an evaluation framework to\nassess the impact of localized corruptions. Furthermore, we uncover the\ninherent complexity of characterizing worst-case robustness using a single\nlocalized adversarial perturbation. To address this, we propose region-aware\nmulti-attack adversarial analysis, a method that enables a deeper understanding\nof model robustness against adversarial perturbations applied to specific\nregions. The proposed metrics and analysis were evaluated on 15 segmentation\nmodels in driving scenarios, uncovering key insights into the effects of\nlocalized corruption in both natural and adversarial forms. The results reveal\nthat models respond to these two types of threats differently; for instance,\ntransformer-based segmentation models demonstrate notable robustness to\nlocalized natural corruptions but are highly vulnerable to adversarial ones and\nvice-versa for CNN-based models. Consequently, we also address the challenge of\nbalancing robustness to both natural and adversarial localized corruptions by\nmeans of ensemble models, thereby achieving a broader threat coverage and\nimproved reliability for dense vision tasks."
    },
    {
        "date": "2025-04",
        "title": "LightDefense: A Lightweight Uncertainty-Driven Defense against Jailbreaks via Shifted Token Distribution",
        "author": "Zhuoran Yang, Jie Peng, Zhen Tan, Tianlong Chen, and Yanyong Zhang",
        "link": "http://arxiv.org/abs/2504.01533v1",
        "abstract": "Large Language Models (LLMs) face threats from jailbreak prompts. Existing\nmethods for defending against jailbreak attacks are primarily based on\nauxiliary models. These strategies, however, often require extensive data\ncollection or training. We propose LightDefense, a lightweight defense\nmechanism targeted at white-box models, which utilizes a safety-oriented\ndirection to adjust the probabilities of tokens in the vocabulary, making\nsafety disclaimers appear among the top tokens after sorting tokens by\nprobability in descending order. We further innovatively leverage LLM's\nuncertainty about prompts to measure their harmfulness and adaptively adjust\ndefense strength, effectively balancing safety and helpfulness. The\neffectiveness of LightDefense in defending against 5 attack methods across 2\ntarget LLMs, without compromising helpfulness to benign user queries,\nhighlights its potential as a novel and lightweight defense mechanism,\nenhancing security of LLMs."
    },
    {
        "date": "2025-04",
        "title": "A Robust Model-Based Approach for Continuous-Time Policy Evaluation with Unknown L\u00e9vy Process Dynamics",
        "author": "Qihao Ye, Xiaochuan Tian, and Yuhua Zhu",
        "link": "http://arxiv.org/abs/2504.01482v1",
        "abstract": "This paper develops a model-based framework for continuous-time policy\nevaluation (CTPE) in reinforcement learning, incorporating both Brownian and\nL\\'evy noise to model stochastic dynamics influenced by rare and extreme\nevents. Our approach formulates the policy evaluation problem as solving a\npartial integro-differential equation (PIDE) for the value function with\nunknown coefficients. A key challenge in this setting is accurately recovering\nthe unknown coefficients in the stochastic dynamics, particularly when driven\nby L\\'evy processes with heavy tail effects. To address this, we propose a\nrobust numerical approach that effectively handles both unbiased and censored\ntrajectory datasets. This method combines maximum likelihood estimation with an\niterative tail correction mechanism, improving the stability and accuracy of\ncoefficient recovery. Additionally, we establish a theoretical bound for the\npolicy evaluation error based on coefficient recovery error. Through numerical\nexperiments, we demonstrate the effectiveness and robustness of our method in\nrecovering heavy-tailed L\\'evy dynamics and verify the theoretical error\nanalysis in policy evaluation."
    },
    {
        "date": "2025-04",
        "title": "Leveraging Generalizability of Image-to-Image Translation for Enhanced Adversarial Defense",
        "author": "Haibo Zhang, Zhihua Yao, Kouichi Sakurai, and Takeshi Saitoh",
        "link": "http://arxiv.org/abs/2504.01399v1",
        "abstract": "In the rapidly evolving field of artificial intelligence, machine learning\nemerges as a key technology characterized by its vast potential and inherent\nrisks. The stability and reliability of these models are important, as they are\nfrequent targets of security threats. Adversarial attacks, first rigorously\ndefined by Ian Goodfellow et al. in 2013, highlight a critical vulnerability:\nthey can trick machine learning models into making incorrect predictions by\napplying nearly invisible perturbations to images. Although many studies have\nfocused on constructing sophisticated defensive mechanisms to mitigate such\nattacks, they often overlook the substantial time and computational costs of\ntraining and maintaining these models. Ideally, a defense method should be able\nto generalize across various, even unseen, adversarial attacks with minimal\noverhead. Building on our previous work on image-to-image translation-based\ndefenses, this study introduces an improved model that incorporates residual\nblocks to enhance generalizability. The proposed method requires training only\na single model, effectively defends against diverse attack types, and is\nwell-transferable between different target models. Experiments show that our\nmodel can restore the classification accuracy from near zero to an average of\n72\\% while maintaining competitive performance compared to state-of-the-art\nmethods."
    },
    {
        "date": "2025-04",
        "title": "FireGuard: A Generalized Microarchitecture for Fine-Grained Security Analysis on OoO Superscalar Cores",
        "author": "Zhe Jiang, Sam Ainsworth, and Timothy Jones",
        "link": "http://arxiv.org/abs/2504.01380v1",
        "abstract": "High-performance security guarantees rely on hardware support. Generic\nprogrammable support for fine-grained instruction analysis has gained broad\ninterest in the literature as a fundamental building block for the security of\nfuture processors. Yet, implementation in real out-of-order (OoO) superscalar\nprocessors presents tough challenges that cannot be explored in highly abstract\nsimulators. We detail the challenges of implementing complex programmable\npathways without critical paths or contention. We then introduce FireGuard, the\nfirst implementation of fine-grained instruction analysis on a real OoO\nsuperscalar processor. We establish an end-to-end system, including\nmicroarchitecture, SoC, ISA and programming model. Experiments show that our\nsolution simultaneously ensures both security and performance of the system,\nwith parallel scalability. We examine the feasibility of building FireGuard\ninto modern SoCs: Apple's M1-Pro, Huawei's Kirin-960, and Intel's i7-12700F,\nwhere less than 1% silicon area is introduced. The Repo. of FireGuard's source\ncode: https://github.com/SEU-ACAL/reproduce-FireGuard-DAC-25."
    },
    {
        "date": "2025-04",
        "title": "Breaking BERT: Gradient Attack on Twitter Sentiment Analysis for Targeted Misclassification",
        "author": "Akil Raj Subedi, Taniya Shah, Aswani Kumar Cherukuri, and Thanos Vasilakos",
        "link": "http://arxiv.org/abs/2504.01345v1",
        "abstract": "Social media platforms like Twitter have increasingly relied on Natural\nLanguage Processing NLP techniques to analyze and understand the sentiments\nexpressed in the user generated content. One such state of the art NLP model is\nBidirectional Encoder Representations from Transformers BERT which has been\nwidely adapted in sentiment analysis. BERT is susceptible to adversarial\nattacks. This paper aims to scrutinize the inherent vulnerabilities of such\nmodels in Twitter sentiment analysis. It aims to formulate a framework for\nconstructing targeted adversarial texts capable of deceiving these models,\nwhile maintaining stealth. In contrast to conventional methodologies, such as\nImportance Reweighting, this framework core idea resides in its reliance on\ngradients to prioritize the importance of individual words within the text. It\nuses a whitebox approach to attain fine grained sensitivity, pinpointing words\nthat exert maximal influence on the classification outcome. This paper is\norganized into three interdependent phases. It starts with fine-tuning a\npre-trained BERT model on Twitter data. It then analyzes gradients of the model\nto rank words on their importance, and iteratively replaces those with feasible\ncandidates until an acceptable solution is found. Finally, it evaluates the\neffectiveness of the adversarial text against the custom trained sentiment\nclassification model. This assessment would help in gauging the capacity of the\nadversarial text to successfully subvert classification without raising any\nalarm."
    },
    {
        "date": "2025-04",
        "title": "Safeguarding Vision-Language Models: Mitigating Vulnerabilities to Gaussian Noise in Perturbation-based Attacks",
        "author": "Jiawei Wang, Yushen Zuo, Yuanjun Chai, Zhendong Liu, Yichen Fu, Yichun Feng, and Kin-man Lam",
        "link": "http://arxiv.org/abs/2504.01308v1",
        "abstract": "Vision-Language Models (VLMs) extend the capabilities of Large Language\nModels (LLMs) by incorporating visual information, yet they remain vulnerable\nto jailbreak attacks, especially when processing noisy or corrupted images.\nAlthough existing VLMs adopt security measures during training to mitigate such\nattacks, vulnerabilities associated with noise-augmented visual inputs are\noverlooked. In this work, we identify that missing noise-augmented training\ncauses critical security gaps: many VLMs are susceptible to even simple\nperturbations such as Gaussian noise. To address this challenge, we propose\nRobust-VLGuard, a multimodal safety dataset with aligned / misaligned\nimage-text pairs, combined with noise-augmented fine-tuning that reduces attack\nsuccess rates while preserving functionality of VLM. For stronger\noptimization-based visual perturbation attacks, we propose DiffPure-VLM,\nleveraging diffusion models to convert adversarial perturbations into\nGaussian-like noise, which can be defended by VLMs with noise-augmented safety\nfine-tuning. Experimental results demonstrate that the distribution-shifting\nproperty of diffusion model aligns well with our fine-tuned VLMs, significantly\nmitigating adversarial perturbations across varying intensities. The dataset\nand code are available at https://github.com/JarvisUSTC/DiffPure-RobustVLM."
    },
    {
        "date": "2025-04",
        "title": "TenAd: A Tensor-based Low-rank Black Box Adversarial Attack for Video Classification",
        "author": "Kimia haghjooei, and Mansoor Rezghi",
        "link": "http://arxiv.org/abs/2504.01228v1",
        "abstract": "Deep learning models have achieved remarkable success in computer vision but\nremain vulnerable to adversarial attacks, particularly in black-box settings\nwhere model details are unknown. Existing adversarial attack methods(even those\nworks with key frames) often treat video data as simple vectors, ignoring their\ninherent multi-dimensional structure, and require a large number of queries,\nmaking them inefficient and detectable. In this paper, we propose\n\\textbf{TenAd}, a novel tensor-based low-rank adversarial attack that leverages\nthe multi-dimensional properties of video data by representing videos as\nfourth-order tensors. By exploiting low-rank attack, our method significantly\nreduces the search space and the number of queries needed to generate\nadversarial examples in black-box settings. Experimental results on standard\nvideo classification datasets demonstrate that \\textbf{TenAd} effectively\ngenerates imperceptible adversarial perturbations while achieving higher attack\nsuccess rates and query efficiency compared to state-of-the-art methods. Our\napproach outperforms existing black-box adversarial attacks in terms of success\nrate, query efficiency, and perturbation imperceptibility, highlighting the\npotential of tensor-based methods for adversarial attacks on video models."
    },
    {
        "date": "2025-04",
        "title": "rPPG-SysDiaGAN: Systolic-Diastolic Feature Localization in rPPG Using Generative Adversarial Network with Multi-Domain Discriminator",
        "author": "Banafsheh Adami, and Nima Karimian",
        "link": "http://arxiv.org/abs/2504.01220v1",
        "abstract": "Remote photoplethysmography (rPPG) offers a novel approach to noninvasive\nmonitoring of vital signs, such as respiratory rate, utilizing a camera.\nAlthough several supervised and self-supervised methods have been proposed,\nthey often fail to accurately reconstruct the PPG signal, particularly in\ndistinguishing between systolic and diastolic components. Their primary focus\ntends to be solely on extracting heart rate, which may not accurately represent\nthe complete PPG signal. To address this limitation, this paper proposes a\nnovel deep learning architecture using Generative Adversarial Networks by\nintroducing multi-discriminators to extract rPPG signals from facial videos.\nThese discriminators focus on the time domain, the frequency domain, and the\nsecond derivative of the original time domain signal. The discriminator\nintegrates four loss functions: variance loss to mitigate local minima caused\nby noise; dynamic time warping loss to address local minima induced by\nalignment and sequences of variable lengths; Sparsity Loss for heart rate\nadjustment, and Variance Loss to ensure a uniform distribution across the\ndesired frequency domain and time interval between systolic and diastolic\nphases of the PPG signal."
    },
    {
        "date": "2025-04",
        "title": "GRU-AUNet: A Domain Adaptation Framework for Contactless Fingerprint Presentation Attack Detection",
        "author": "Banafsheh Adami, and Nima Karimian",
        "link": "http://arxiv.org/abs/2504.01213v1",
        "abstract": "Although contactless fingerprints offer user comfort, they are more\nvulnerable to spoofing. The current solution for anti-spoofing in the area of\ncontactless fingerprints relies on domain adaptation learning, limiting their\ngeneralization and scalability. To address these limitations, we introduce\nGRU-AUNet, a domain adaptation approach that integrates a Swin\nTransformer-based UNet architecture with GRU-enhanced attention mechanisms, a\nDynamic Filter Network in the bottleneck, and a combined Focal and Contrastive\nLoss function. Trained in both genuine and spoof fingerprint images, GRU-AUNet\ndemonstrates robust resilience against presentation attacks, achieving an\naverage BPCER of 0.09\\% and APCER of 1.2\\% in the CLARKSON, COLFISPOOF, and\nIIITD datasets, outperforming state-of-the-art domain adaptation methods."
    },
    {
        "date": "2025-04",
        "title": "Performative Drift Resistant Classification Using Generative Domain Adversarial Networks",
        "author": "Maciej Makowski, Brandon Gower-Winter, and Georg Krempl",
        "link": "http://arxiv.org/abs/2504.01135v1",
        "abstract": "Performative Drift is a special type of Concept Drift that occurs when a\nmodel's predictions influence the future instances the model will encounter. In\nthese settings, retraining is not always feasible. In this work, we instead\nfocus on drift understanding as a method for creating drift-resistant\nclassifiers. To achieve this, we introduce the Generative Domain Adversarial\nNetwork (GDAN) which combines both Domain and Generative Adversarial Networks.\nUsing GDAN, domain-invariant representations of incoming data are created and a\ngenerative network is used to reverse the effects of performative drift. Using\nsemi-real and synthetic data generators, we empirically evaluate GDAN's ability\nto provide drift-resistant classification. Initial results are promising with\nGDAN limiting performance degradation over several timesteps. Additionally,\nGDAN's generative network can be used in tandem with other models to limit\ntheir performance degradation in the presence of performative drift. Lastly, we\nhighlight the relationship between model retraining and the unpredictability of\nperformative drift, providing deeper insights into the challenges faced when\nusing traditional Concept Drift mitigation strategies in the performative\nsetting."
    },
    {
        "date": "2025-04",
        "title": "ShieldGemma 2: Robust and Tractable Image Content Moderation",
        "author": "Wenjun Zeng, Dana Kurniawan, Ryan Mullins, Yuchi Liu, Tamoghna Saha, Dirichi Ike-Njoku, Jindong Gu, Yiwen Song, Cai Xu, Jingjing Zhou, Aparna Joshi, Shravan Dheep, Mani Malek, Hamid Palangi, Joon Baek, Rick Pereira, and Karthik Narasimhan",
        "link": "http://arxiv.org/abs/2504.01081v1",
        "abstract": "We introduce ShieldGemma 2, a 4B parameter image content moderation model\nbuilt on Gemma 3. This model provides robust safety risk predictions across the\nfollowing key harm categories: Sexually Explicit, Violence \\& Gore, and\nDangerous Content for synthetic images (e.g. output of any image generation\nmodel) and natural images (e.g. any image input to a Vision-Language Model). We\nevaluated on both internal and external benchmarks to demonstrate\nstate-of-the-art performance compared to LlavaGuard\n\\citep{helff2024llavaguard}, GPT-4o mini \\citep{hurst2024gpt}, and the base\nGemma 3 model \\citep{gemma_2025} based on our policies. Additionally, we\npresent a novel adversarial data generation pipeline which enables a\ncontrolled, diverse, and robust image generation. ShieldGemma 2 provides an\nopen image moderation tool to advance multimodal safety and responsible AI\ndevelopment."
    },
    {
        "date": "2025-04",
        "title": "Safety and Security Risk Mitigation in Satellite Missions via Attack-Fault-Defense Trees",
        "author": "Reza Soltani, Pablo Diale, Milan Lopuha\u00e4-Zwakenberg, and Mari\u00eblle Stoelinga",
        "link": "http://arxiv.org/abs/2504.00988v1",
        "abstract": "Cyber-physical systems, such as self-driving cars or digitized electrical\ngrids, often involve complex interactions between security, safety, and\ndefense. Proper risk management strategies must account for these three\ncritical domains and their interaction because the failure to address one\ndomain can exacerbate risks in the others, leading to cascading effects that\ncompromise the overall system resilience. This work presents a case study from\nAscentio Technologies, a mission-critical system company in Argentina\nspecializing in aerospace, where the interplay between safety, security, and\ndefenses is critical for ensuring the resilience and reliability of their\nsystems. The main focus will be on the Ground Segment for the satellite project\ncurrently developed by the company. Analyzing safety, security, and defense\nmechanisms together in the Ground Segment of a satellite project is crucial\nbecause these domains are deeply interconnected--for instance, a security\nbreach could disable critical safety functions, or a safety failure could\ncreate opportunities for attackers to exploit vulnerabilities, amplifying the\nrisks to the entire system. This paper showcases the application of the\nAttack-Fault-Defense Tree (AFDT) framework, which integrates attack trees,\nfault trees, and defense mechanisms into a unified model. AFDT provides an\nintuitive visual language that facilitates interdisciplinary collaboration,\nenabling experts from various fields to better assess system vulnerabilities\nand defenses. By applying AFDT to the Ground Segment of the satellite project,\nwe demonstrate how qualitative analyses can be performed to identify weaknesses\nand enhance the overall system's security and safety. This case highlights the\nimportance of jointly analyzing attacks, faults, and defenses to improve\nresilience in complex cyber-physical environments."
    },
    {
        "date": "2025-04",
        "title": "S3C2 Summit 2024-08: Government Secure Supply Chain Summit",
        "author": "Courtney Miller, William Enck, Yasemin Acar, Michel Cukier, Alexandros Kapravelos, Christian Kastner, Dominik Wermke, and Laurie Williams",
        "link": "http://arxiv.org/abs/2504.00924v1",
        "abstract": "Supply chain security has become a very important vector to consider when\ndefending against adversary attacks. Due to this, more and more developers are\nkeen on improving their supply chains to make them more robust against future\nthreats. On August 29, 2024 researchers from the Secure Software Supply Chain\nCenter (S3C2) gathered 14 practitioners from 10 government agencies to discuss\nthe state of supply chain security. The goal of the summit is to share insights\nbetween companies and developers alike to foster new collaborations and ideas\nmoving forward. Through this meeting, participants were questions on best\npractices and thoughts how to improve things for the future. In this paper we\nsummarize the responses and discussions of the summit."
    },
    {
        "date": "2025-04",
        "title": "TAMIS: Tailored Membership Inference Attacks on Synthetic Data",
        "author": "Paul Andrey, Batiste Le Bars, and Marc Tommasi",
        "link": "http://arxiv.org/abs/2504.00758v1",
        "abstract": "Membership Inference Attacks (MIA) enable to empirically assess the privacy\nof a machine learning algorithm. In this paper, we propose TAMIS, a novel MIA\nagainst differentially-private synthetic data generation methods that rely on\ngraphical models. This attack builds upon MAMA-MIA, a recently-published\nstate-of-the-art method. It lowers its computational cost and requires less\nattacker knowledge. Our attack is the product of a two-fold improvement. First,\nwe recover the graphical model having generated a synthetic dataset by using\nsolely that dataset, rather than shadow-modeling over an auxiliary one. This\nproves less costly and more performant. Second, we introduce a more\nmathematically-grounded attack score, that provides a natural threshold for\nbinary predictions. In our experiments, TAMIS achieves better or similar\nperformance as MAMA-MIA on replicas of the SNAKE challenge."
    },
    {
        "date": "2025-04",
        "title": "Alleviating Performance Disparity in Adversarial Spatiotemporal Graph Learning Under Zero-Inflated Distribution",
        "author": "Songran Bai, Yuheng Ji, Yue Liu, Xingwei Zhang, Xiaolong Zheng, and Daniel Dajun Zeng",
        "link": "http://arxiv.org/abs/2504.00721v1",
        "abstract": "Spatiotemporal Graph Learning (SGL) under Zero-Inflated Distribution (ZID) is\ncrucial for urban risk management tasks, including crime prediction and traffic\naccident profiling. However, SGL models are vulnerable to adversarial attacks,\ncompromising their practical utility. While adversarial training (AT) has been\nwidely used to bolster model robustness, our study finds that traditional AT\nexacerbates performance disparities between majority and minority classes under\nZID, potentially leading to irreparable losses due to underreporting critical\nrisk events. In this paper, we first demonstrate the smaller top-k gradients\nand lower separability of minority class are key factors contributing to this\ndisparity. To address these issues, we propose MinGRE, a framework for Minority\nClass Gradients and Representations Enhancement. MinGRE employs a\nmulti-dimensional attention mechanism to reweight spatiotemporal gradients,\nminimizing the gradient distribution discrepancies across classes.\nAdditionally, we introduce an uncertainty-guided contrastive loss to improve\nthe inter-class separability and intra-class compactness of minority\nrepresentations with higher uncertainty. Extensive experiments demonstrate that\nthe MinGRE framework not only significantly reduces the performance disparity\nacross classes but also achieves enhanced robustness compared to existing\nbaselines. These findings underscore the potential of our method in fostering\nthe development of more equitable and robust models."
    },
    {
        "date": "2025-04",
        "title": "Impact of Data Duplication on Deep Neural Network-Based Image Classifiers: Robust vs. Standard Models",
        "author": "Alireza Aghabagherloo, Aydin Abadi, Sumanta Sarkar, Vishnu Asutosh Dasu, and Bart Preneel",
        "link": "http://arxiv.org/abs/2504.00638v1",
        "abstract": "The accuracy and robustness of machine learning models against adversarial\nattacks are significantly influenced by factors such as training data quality,\nmodel architecture, the training process, and the deployment environment. In\nrecent years, duplicated data in training sets, especially in language models,\nhas attracted considerable attention. It has been shown that deduplication\nenhances both training performance and model accuracy in language models. While\nthe importance of data quality in training image classifier Deep Neural\nNetworks (DNNs) is widely recognized, the impact of duplicated images in the\ntraining set on model generalization and performance has received little\nattention.\n  In this paper, we address this gap and provide a comprehensive study on the\neffect of duplicates in image classification. Our analysis indicates that the\npresence of duplicated images in the training set not only negatively affects\nthe efficiency of model training but also may result in lower accuracy of the\nimage classifier. This negative impact of duplication on accuracy is\nparticularly evident when duplicated data is non-uniform across classes or when\nduplication, whether uniform or non-uniform, occurs in the training set of an\nadversarially trained model. Even when duplicated samples are selected in a\nuniform way, increasing the amount of duplication does not lead to a\nsignificant improvement in accuracy."
    },
    {
        "date": "2025-04",
        "title": "Geometric Median Matching for Robust k-Subset Selection from Noisy Data",
        "author": "Anish Acharya, Sujay Sanghavi, Alexandros G. Dimakis, and Inderjit S Dhillon",
        "link": "http://arxiv.org/abs/2504.00564v2",
        "abstract": "Data pruning -- the combinatorial task of selecting a small and\nrepresentative subset from a large dataset, is crucial for mitigating the\nenormous computational costs associated with training data-hungry modern deep\nlearning models at scale. Since large scale data collections are invariably\nnoisy, developing data pruning strategies that remain robust even in the\npresence of corruption is critical in practice. However, existing data pruning\nmethods often fail under high corruption rates due to their reliance on\nempirical mean estimation, which is highly sensitive to outliers.\n  In response, we propose Geometric Median (GM) Matching, a novel k-subset\nselection strategy that leverages Geometric Median -- a robust estimator with\nan optimal breakdown point of 1/2; to enhance resilience against noisy data.\nOur method iteratively selects a k-subset such that the mean of the subset\napproximates the GM of the (potentially) noisy dataset, ensuring robustness\neven under arbitrary corruption. We provide theoretical guarantees, showing\nthat GM Matching enjoys an improved O(1/k) convergence rate -- a quadratic\nimprovement over random sampling, even under arbitrary corruption. Extensive\nexperiments across image classification and image generation tasks demonstrate\nthat GM Matching consistently outperforms existing pruning approaches,\nparticularly in high-corruption settings and at high pruning rates; making it a\nstrong baseline for robust data pruning."
    },
    {
        "date": "2025-04",
        "title": "Adversarial Curriculum Graph-Free Knowledge Distillation for Graph Neural Networks",
        "author": "Yuang Jia, Xiaojuan Shan, Jun Xia, Guancheng Wan, Yuchen Zhang, Wenke Huang, Mang Ye, and Stan Z. Li",
        "link": "http://arxiv.org/abs/2504.00540v2",
        "abstract": "Data-free Knowledge Distillation (DFKD) is a method that constructs\npseudo-samples using a generator without real data, and transfers knowledge\nfrom a teacher model to a student by enforcing the student to overcome\ndimensional differences and learn to mimic the teacher's outputs on these\npseudo-samples. In recent years, various studies in the vision domain have made\nnotable advancements in this area. However, the varying topological structures\nand non-grid nature of graph data render the methods from the vision domain\nineffective. Building upon prior research into differentiable methods for graph\nneural networks, we propose a fast and high-quality data-free knowledge\ndistillation approach in this paper. Without compromising distillation quality,\nthe proposed graph-free KD method (ACGKD) significantly reduces the spatial\ncomplexity of pseudo-graphs by leveraging the Binary Concrete distribution to\nmodel the graph structure and introducing a spatial complexity tuning\nparameter. This approach enables efficient gradient computation for the graph\nstructure, thereby accelerating the overall distillation process. Additionally,\nACGKD eliminates the dimensional ambiguity between the student and teacher\nmodels by increasing the student's dimensions and reusing the teacher's\nclassifier. Moreover, it equips graph knowledge distillation with a CL-based\nstrategy to ensure the student learns graph structures progressively. Extensive\nexperiments demonstrate that ACGKD achieves state-of-the-art performance in\ndistilling knowledge from GNNs without training data."
    },
    {
        "date": "2025-04",
        "title": "Robust LiDAR-Camera Calibration with 2D Gaussian Splatting",
        "author": "Shuyi Zhou, Shuxiang Xie, Ryoichi Ishikawa, and Takeshi Oishi",
        "link": "http://arxiv.org/abs/2504.00525v1",
        "abstract": "LiDAR-camera systems have become increasingly popular in robotics recently. A\ncritical and initial step in integrating the LiDAR and camera data is the\ncalibration of the LiDAR-camera system. Most existing calibration methods rely\non auxiliary target objects, which often involve complex manual operations,\nwhereas targetless methods have yet to achieve practical effectiveness.\nRecognizing that 2D Gaussian Splatting (2DGS) can reconstruct geometric\ninformation from camera image sequences, we propose a calibration method that\nestimates LiDAR-camera extrinsic parameters using geometric constraints. The\nproposed method begins by reconstructing colorless 2DGS using LiDAR point\nclouds. Subsequently, we update the colors of the Gaussian splats by minimizing\nthe photometric loss. The extrinsic parameters are optimized during this\nprocess. Additionally, we address the limitations of the photometric loss by\nincorporating the reprojection and triangulation losses, thereby enhancing the\ncalibration robustness and accuracy."
    },
    {
        "date": "2025-04",
        "title": "FortisAVQA and MAVEN: a Benchmark Dataset and Debiasing Framework for Robust Multimodal Reasoning",
        "author": "Jie Ma, Zhitao Gao, Qi Chai, Jun Liu, Pinghui Wang, Jing Tao, and Zhou Su",
        "link": "http://arxiv.org/abs/2504.00487v2",
        "abstract": "Audio-Visual Question Answering (AVQA) is a challenging multimodal reasoning\ntask requiring intelligent systems to answer natural language queries based on\npaired audio-video inputs accurately. However, existing AVQA approaches often\nsuffer from overfitting to dataset biases, leading to poor robustness.\nMoreover, current datasets may not effectively diagnose these methods. To\naddress these challenges, we first introduce a novel dataset, FortisAVQA,\nconstructed in two stages: (1) rephrasing questions in the test split of the\npublic MUSIC-AVQA dataset and (2) introducing distribution shifts across\nquestions. The first stage expands the test space with greater diversity, while\nthe second enables a refined robustness evaluation across rare, frequent, and\noverall question distributions. Second, we introduce a robust Multimodal\nAudio-Visual Epistemic Network (MAVEN) that leverages a multifaceted cycle\ncollaborative debiasing strategy to mitigate bias learning. Experimental\nresults demonstrate that our architecture achieves state-of-the-art performance\non FortisAVQA, with a notable improvement of 7.81\\%. Extensive ablation studies\non both datasets validate the effectiveness of our debiasing components.\nAdditionally, our evaluation reveals the limited robustness of existing\nmultimodal QA methods. We also verify the plug-and-play capability of our\nstrategy by integrating it with various baseline models across both datasets.\nOur dataset and code are available at https://github.com/reml-group/fortisavqa."
    },
    {
        "date": "2025-04",
        "title": "Efficient Near-Optimal Algorithm for Online Shortest Paths in Directed Acyclic Graphs with Bandit Feedback Against Adaptive Adversaries",
        "author": "Arnab Maiti, Zhiyuan Fan, Kevin Jamieson, Lillian J. Ratliff, and Gabriele Farina",
        "link": "http://arxiv.org/abs/2504.00461v1",
        "abstract": "In this paper, we study the online shortest path problem in directed acyclic\ngraphs (DAGs) under bandit feedback against an adaptive adversary. Given a DAG\n$G = (V, E)$ with a source node $v_{\\mathsf{s}}$ and a sink node\n$v_{\\mathsf{t}}$, let $X \\subseteq \\{0,1\\}^{|E|}$ denote the set of all paths\nfrom $v_{\\mathsf{s}}$ to $v_{\\mathsf{t}}$. At each round $t$, we select a path\n$\\mathbf{x}_t \\in X$ and receive bandit feedback on our loss $\\langle\n\\mathbf{x}_t, \\mathbf{y}_t \\rangle \\in [-1,1]$, where $\\mathbf{y}_t$ is an\nadversarially chosen loss vector. Our goal is to minimize regret with respect\nto the best path in hindsight over $T$ rounds. We propose the first\ncomputationally efficient algorithm to achieve a near-minimax optimal regret\nbound of $\\tilde O(\\sqrt{|E|T\\log |X|})$ with high probability against any\nadaptive adversary, where $\\tilde O(\\cdot)$ hides logarithmic factors in the\nnumber of edges $|E|$. Our algorithm leverages a novel loss estimator and a\ncentroid-based decomposition in a nontrivial manner to attain this regret\nbound.\n  As an application, we show that our algorithm for DAGs provides\nstate-of-the-art efficient algorithms for $m$-sets, extensive-form games, the\nColonel Blotto game, shortest walks in directed graphs, hypercubes, and\nmulti-task multi-armed bandits, achieving improved high-probability regret\nguarantees in all these settings."
    },
    {
        "date": "2025-04",
        "title": "Mixture-of-Attack-Experts with Class Regularization for Unified Physical-Digital Face Attack Detection",
        "author": "Shunxin Chen, Ajian Liu, Junze Zheng, Jun Wan, Kailai Peng, Sergio Escalera, and Zhen Lei",
        "link": "http://arxiv.org/abs/2504.00458v1",
        "abstract": "Facial recognition systems in real-world scenarios are susceptible to both\ndigital and physical attacks. Previous methods have attempted to achieve\nclassification by learning a comprehensive feature space. However, these\nmethods have not adequately accounted for the inherent characteristics of\nphysical and digital attack data, particularly the large intra class variation\nin attacks and the small inter-class variation between live and fake faces. To\naddress these limitations, we propose the Fine-Grained MoE with Class-Aware\nRegularization CLIP framework (FG-MoE-CLIP-CAR), incorporating key improvements\nat both the feature and loss levels. At the feature level, we employ a Soft\nMixture of Experts (Soft MoE) architecture to leverage different experts for\nspecialized feature processing. Additionally, we refine the Soft MoE to capture\nmore subtle differences among various types of fake faces. At the loss level,\nwe introduce two constraint modules: the Disentanglement Module (DM) and the\nCluster Distillation Module (CDM). The DM enhances class separability by\nincreasing the distance between the centers of live and fake face classes.\nHowever, center-to-center constraints alone are insufficient to ensure\ndistinctive representations for individual features. Thus, we propose the CDM\nto further cluster features around their respective class centers while\nmaintaining separation from other classes. Moreover, specific attacks that\nsignificantly deviate from common attack patterns are often overlooked. To\naddress this issue, our distance calculation prioritizes more distant features.\nExperimental results on two unified physical-digital attack datasets\ndemonstrate that the proposed method achieves state-of-the-art (SOTA)\nperformance."
    },
    {
        "date": "2025-04",
        "title": "FA^{3}-CLIP: Frequency-Aware Cues Fusion and Attack-Agnostic Prompt Learning for Unified Face Attack Detection",
        "author": "Yongze Li, Ning Li, Ajian Liu, Hui Ma, Liying Yang, Xihong Chen, Zhiyao Liang, Yanyan Liang, Jun Wan, and Zhen Lei",
        "link": "http://arxiv.org/abs/2504.00454v1",
        "abstract": "Facial recognition systems are vulnerable to physical (e.g., printed photos)\nand digital (e.g., DeepFake) face attacks. Existing methods struggle to\nsimultaneously detect physical and digital attacks due to: 1) significant\nintra-class variations between these attack types, and 2) the inadequacy of\nspatial information alone to comprehensively capture live and fake cues. To\naddress these issues, we propose a unified attack detection model termed\nFrequency-Aware and Attack-Agnostic CLIP (FA\\textsuperscript{3}-CLIP), which\nintroduces attack-agnostic prompt learning to express generic live and fake\ncues derived from the fusion of spatial and frequency features, enabling\nunified detection of live faces and all categories of attacks. Specifically,\nthe attack-agnostic prompt module generates generic live and fake prompts\nwithin the language branch to extract corresponding generic representations\nfrom both live and fake faces, guiding the model to learn a unified feature\nspace for unified attack detection. Meanwhile, the module adaptively generates\nthe live/fake conditional bias from the original spatial and frequency\ninformation to optimize the generic prompts accordingly, reducing the impact of\nintra-class variations. We further propose a dual-stream cues fusion framework\nin the vision branch, which leverages frequency information to complement\nsubtle cues that are difficult to capture in the spatial domain. In addition, a\nfrequency compression block is utilized in the frequency stream, which reduces\nredundancy in frequency features while preserving the diversity of crucial\ncues. We also establish new challenging protocols to facilitate unified face\nattack detection effectiveness. Experimental results demonstrate that the\nproposed method significantly improves performance in detecting physical and\ndigital face attacks, achieving state-of-the-art results."
    },
    {
        "date": "2025-04",
        "title": "Suite-IN++: A FlexiWear BodyNet Integrating Global and Local Motion Features from Apple Suite for Robust Inertial Navigation",
        "author": "Lan Sun, Songpengcheng Xia, Jiarui Yang, and Ling Pei",
        "link": "http://arxiv.org/abs/2504.00438v1",
        "abstract": "The proliferation of wearable technology has established multi-device\necosystems comprising smartphones, smartwatches, and headphones as critical\nenablers for ubiquitous pedestrian localization. However, traditional\npedestrian dead reckoning (PDR) struggles with diverse motion modes, while\ndata-driven methods, despite improving accuracy, often lack robustness due to\ntheir reliance on a single-device setup. Therefore, a promising solution is to\nfully leverage existing wearable devices to form a flexiwear bodynet for robust\nand accurate pedestrian localization. This paper presents Suite-IN++, a deep\nlearning framework for flexiwear bodynet-based pedestrian localization.\nSuite-IN++ integrates motion data from wearable devices on different body\nparts, using contrastive learning to separate global and local motion features.\nIt fuses global features based on the data reliability of each device to\ncapture overall motion trends and employs an attention mechanism to uncover\ncross-device correlations in local features, extracting motion details helpful\nfor accurate localization. To evaluate our method, we construct a real-life\nflexiwear bodynet dataset, incorporating Apple Suite (iPhone, Apple Watch, and\nAirPods) across diverse walking modes and device configurations. Experimental\nresults demonstrate that Suite-IN++ achieves superior localization accuracy and\nrobustness, significantly outperforming state-of-the-art models in real-life\npedestrian tracking scenarios."
    },
    {
        "date": "2025-04",
        "title": "Unleashing the Power of Pre-trained Encoders for Universal Adversarial Attack Detection",
        "author": "Yinghe Zhang, Chi Liu, Shuai Zhou, Sheng Shen, and Peng Gui",
        "link": "http://arxiv.org/abs/2504.00429v1",
        "abstract": "Adversarial attacks pose a critical security threat to real-world AI systems\nby injecting human-imperceptible perturbations into benign samples to induce\nmisclassification in deep learning models. While existing detection methods,\nsuch as Bayesian uncertainty estimation and activation pattern analysis, have\nachieved progress through feature engineering, their reliance on handcrafted\nfeature design and prior knowledge of attack patterns limits generalization\ncapabilities and incurs high engineering costs. To address these limitations,\nthis paper proposes a lightweight adversarial detection framework based on the\nlarge-scale pre-trained vision-language model CLIP. Departing from conventional\nadversarial feature characterization paradigms, we innovatively adopt an\nanomaly detection perspective. By jointly fine-tuning CLIP's dual visual-text\nencoders with trainable adapter networks and learnable prompts, we construct a\ncompact representation space tailored for natural images. Notably, our\ndetection architecture achieves substantial improvements in generalization\ncapability across both known and unknown attack patterns compared to\ntraditional methods, while significantly reducing training overhead. This study\nprovides a novel technical pathway for establishing a parameter-efficient and\nattack-agnostic defense paradigm, markedly enhancing the robustness of vision\nsystems against evolving adversarial threats."
    },
    {
        "date": "2025-04",
        "title": "CopyQNN: Quantum Neural Network Extraction Attack under Varying Quantum Noise",
        "author": "Zhenxiao Fu, Leyi Zhao, Xuhong Zhang, Yilun Xu, Gang Huang, and Fan Chen",
        "link": "http://arxiv.org/abs/2504.00366v1",
        "abstract": "Quantum Neural Networks (QNNs) have shown significant value across domains,\nwith well-trained QNNs representing critical intellectual property often\ndeployed via cloud-based QNN-as-a-Service (QNNaaS) platforms. Recent work has\nexamined QNN model extraction attacks using classical and emerging quantum\nstrategies. These attacks involve adversaries querying QNNaaS platforms to\nobtain labeled data for training local substitute QNNs that replicate the\nfunctionality of cloud-based models. However, existing approaches have largely\noverlooked the impact of varying quantum noise inherent in noisy\nintermediate-scale quantum (NISQ) computers, limiting their effectiveness in\nreal-world settings. To address this limitation, we propose the CopyQNN\nframework, which employs a three-step data cleaning method to eliminate noisy\ndata based on its noise sensitivity. This is followed by the integration of\ncontrastive and transfer learning within the quantum domain, enabling efficient\ntraining of substitute QNNs using a limited but cleaned set of queried data.\nExperimental results on NISQ computers demonstrate that a practical\nimplementation of CopyQNN significantly outperforms state-of-the-art QNN\nextraction attacks, achieving an average performance improvement of 8.73%\nacross all tasks while reducing the number of required queries by 90x, with\nonly a modest increase in hardware overhead."
    },
    {
        "date": "2025-04",
        "title": "Integrated LLM-Based Intrusion Detection with Secure Slicing xApp for Securing O-RAN-Enabled Wireless Network Deployments",
        "author": "Joshua Moore, Aly Sabri Abdalla, Prabesh Khanal, and Vuk Marojevic",
        "link": "http://arxiv.org/abs/2504.00341v1",
        "abstract": "The Open Radio Access Network (O-RAN) architecture is reshaping\ntelecommunications by promoting openness, flexibility, and intelligent\nclosed-loop optimization. By decoupling hardware and software and enabling\nmulti-vendor deployments, O-RAN reduces costs, enhances performance, and allows\nrapid adaptation to new technologies. A key innovation is intelligent network\nslicing, which partitions networks into isolated slices tailored for specific\nuse cases or quality of service requirements. The RAN Intelligent Controller\nfurther optimizes resource allocation, ensuring efficient utilization and\nimproved service quality for user equipment (UEs). However, the modular and\ndynamic nature of O-RAN expands the threat surface, necessitating advanced\nsecurity measures to maintain network integrity, confidentiality, and\navailability. Intrusion detection systems have become essential for identifying\nand mitigating attacks. This research explores using large language models\n(LLMs) to generate security recommendations based on the temporal traffic\npatterns of connected UEs. The paper introduces an LLM-driven intrusion\ndetection framework and demonstrates its efficacy through experimental\ndeployments, comparing non fine-tuned and fine-tuned models for task-specific\naccuracy."
    },
    {
        "date": "2025-03",
        "title": "Federated Learning for Cross-Domain Data Privacy: A Distributed Approach to Secure Collaboration",
        "author": "Yiwei Zhang, Jie Liu, Jiawei Wang, Lu Dai, Fan Guo, and Guohui Cai",
        "link": "http://arxiv.org/abs/2504.00282v1",
        "abstract": "This paper proposes a data privacy protection framework based on federated\nlearning, which aims to realize effective cross-domain data collaboration under\nthe premise of ensuring data privacy through distributed learning. Federated\nlearning greatly reduces the risk of privacy breaches by training the model\nlocally on each client and sharing only model parameters rather than raw data.\nThe experiment verifies the high efficiency and privacy protection ability of\nfederated learning under different data sources through the simulation of\nmedical, financial, and user data. The results show that federated learning can\nnot only maintain high model performance in a multi-domain data environment but\nalso ensure effective protection of data privacy. The research in this paper\nprovides a new technical path for cross-domain data collaboration and promotes\nthe application of large-scale data analysis and machine learning while\nprotecting privacy."
    },
    {
        "date": "2025-03",
        "title": "$\\textit{Agents Under Siege}$: Breaking Pragmatic Multi-Agent LLM Systems with Optimized Prompt Attacks",
        "author": "Rana Muhammad Shahroz Khan, Zhen Tan, Sukwon Yun, Charles Flemming, and Tianlong Chen",
        "link": "http://arxiv.org/abs/2504.00218v1",
        "abstract": "Most discussions about Large Language Model (LLM) safety have focused on\nsingle-agent settings but multi-agent LLM systems now create novel adversarial\nrisks because their behavior depends on communication between agents and\ndecentralized reasoning. In this work, we innovatively focus on attacking\npragmatic systems that have constrains such as limited token bandwidth, latency\nbetween message delivery, and defense mechanisms. We design a\n$\\textit{permutation-invariant adversarial attack}$ that optimizes prompt\ndistribution across latency and bandwidth-constraint network topologies to\nbypass distributed safety mechanisms within the system. Formulating the attack\npath as a problem of $\\textit{maximum-flow minimum-cost}$, coupled with the\nnovel $\\textit{Permutation-Invariant Evasion Loss (PIEL)}$, we leverage\ngraph-based optimization to maximize attack success rate while minimizing\ndetection risk. Evaluating across models including $\\texttt{Llama}$,\n$\\texttt{Mistral}$, $\\texttt{Gemma}$, $\\texttt{DeepSeek}$ and other variants on\nvarious datasets like $\\texttt{JailBreakBench}$ and\n$\\texttt{AdversarialBench}$, our method outperforms conventional attacks by up\nto $7\\times$, exposing critical vulnerabilities in multi-agent systems.\nMoreover, we demonstrate that existing defenses, including variants of\n$\\texttt{Llama-Guard}$ and $\\texttt{PromptGuard}$, fail to prohibit our attack,\nemphasizing the urgent need for multi-agent specific safety mechanisms."
    },
    {
        "date": "2025-03",
        "title": "Few-Shot Generation of Brain Tumors for Secure and Fair Data Sharing",
        "author": "Yongyi Shi, and Ge Wang",
        "link": "http://arxiv.org/abs/2504.00150v1",
        "abstract": "Leveraging multi-center data for medical analytics presents challenges due to\nprivacy concerns and data heterogeneity. While distributed approaches such as\nfederated learning has gained traction, they remain vulnerable to privacy\nbreaches, particularly in sensitive domains like medical imaging. Generative\nmodels, such as diffusion models, enhance privacy by synthesizing realistic\ndata. However, they are prone to memorization, especially when trained on small\ndatasets. This study proposes a decentralized few-shot generative model (DFGM)\nto synthesize brain tumor images while fully preserving privacy. DFGM\nharmonizes private tumor data with publicly shareable healthy images from\nmultiple medical centers, constructing a new dataset by blending tumor\nforegrounds with healthy backgrounds. This approach ensures stringent privacy\nprotection and enables controllable, high-quality synthesis by preserving both\nthe healthy backgrounds and tumor foregrounds. We assess DFGM's effectiveness\nin brain tumor segmentation using a UNet, achieving Dice score improvements of\n3.9% for data augmentation and 4.6% for fairness on a separate dataset."
    },
    {
        "date": "2025-03",
        "title": "Value of Information-based Deceptive Path Planning Under Adversarial Interventions",
        "author": "Wesley A. Suttle, Jesse Milzman, Mustafa O. Karabag, Brian M. Sadler, and Ufuk Topcu",
        "link": "http://arxiv.org/abs/2503.24284v1",
        "abstract": "Existing methods for deceptive path planning (DPP) address the problem of\ndesigning paths that conceal their true goal from a passive, external observer.\nSuch methods do not apply to problems where the observer has the ability to\nperform adversarial interventions to impede the path planning agent. In this\npaper, we propose a novel Markov decision process (MDP)-based model for the DPP\nproblem under adversarial interventions and develop new value of information\n(VoI) objectives to guide the design of DPP policies. Using the VoI objectives\nwe propose, path planning agents deceive the adversarial observer into choosing\nsuboptimal interventions by selecting trajectories that are of low\ninformational value to the observer. Leveraging connections to the linear\nprogramming theory for MDPs, we derive computationally efficient solution\nmethods for synthesizing policies for performing DPP under adversarial\ninterventions. In our experiments, we illustrate the effectiveness of the\nproposed solution method in achieving deceptiveness under adversarial\ninterventions and demonstrate the superior performance of our approach to both\nexisting DPP methods and conservative path planning approaches on illustrative\ngridworld problems."
    },
    {
        "date": "2025-03",
        "title": "Output Constraints as Attack Surface: Exploiting Structured Generation to Bypass LLM Safety Mechanisms",
        "author": "Shuoming Zhang, Jiacheng Zhao, Ruiyuan Xu, Xiaobing Feng, and Huimin Cui",
        "link": "http://arxiv.org/abs/2503.24191v1",
        "abstract": "Content Warning: This paper may contain unsafe or harmful content generated\nby LLMs that may be offensive to readers. Large Language Models (LLMs) are\nextensively used as tooling platforms through structured output APIs to ensure\nsyntax compliance so that robust integration with existing softwares like agent\nsystems, could be achieved. However, the feature enabling functionality of\ngrammar-guided structured output presents significant security vulnerabilities.\nIn this work, we reveal a critical control-plane attack surface orthogonal to\ntraditional data-plane vulnerabilities. We introduce Constrained Decoding\nAttack (CDA), a novel jailbreak class that weaponizes structured output\nconstraints to bypass safety mechanisms. Unlike prior attacks focused on input\nprompts, CDA operates by embedding malicious intent in schema-level grammar\nrules (control-plane) while maintaining benign surface prompts (data-plane). We\ninstantiate this with a proof-of-concept Chain Enum Attack, achieves 96.2%\nattack success rates across proprietary and open-weight LLMs on five safety\nbenchmarks with a single query, including GPT-4o and Gemini-2.0-flash. Our\nfindings identify a critical security blind spot in current LLM architectures\nand urge a paradigm shift in LLM safety to address control-plane\nvulnerabilities, as current mechanisms focused solely on data-plane threats\nleave critical systems exposed."
    },
    {
        "date": "2025-03",
        "title": "CIBR: Cross-modal Information Bottleneck Regularization for Robust CLIP Generalization",
        "author": "Yingrui Ji, Xi Xiao, Gaofei Chen, Hao Xu, Chenrui Ma, Lijing Zhu, Aokun Liang, and Jiansheng Chen",
        "link": "http://arxiv.org/abs/2503.24182v1",
        "abstract": "Contrastive Language-Image Pretraining (CLIP) has achieved remarkable success\nin cross-modal tasks such as zero-shot image classification and text-image\nretrieval by effectively aligning visual and textual representations. However,\nthe theoretical foundations underlying CLIP's strong generalization remain\nunclear. In this work, we address this gap by proposing the Cross-modal\nInformation Bottleneck (CIB) framework. CIB offers a principled interpretation\nof CLIP's contrastive learning objective as an implicit Information Bottleneck\noptimization. Under this view, the model maximizes shared cross-modal\ninformation while discarding modality-specific redundancies, thereby preserving\nessential semantic alignment across modalities. Building on this insight, we\nintroduce a Cross-modal Information Bottleneck Regularization (CIBR) method\nthat explicitly enforces these IB principles during training. CIBR introduces a\npenalty term to discourage modality-specific redundancy, thereby enhancing\nsemantic alignment between image and text features. We validate CIBR on\nextensive vision-language benchmarks, including zero-shot classification across\nseven diverse image datasets and text-image retrieval on MSCOCO and Flickr30K.\nThe results show consistent performance gains over standard CLIP. These\nfindings provide the first theoretical understanding of CLIP's generalization\nthrough the IB lens. They also demonstrate practical improvements, offering\nguidance for future cross-modal representation learning."
    },
    {
        "date": "2025-03",
        "title": "Pay More Attention to the Robustness of Prompt for Instruction Data Mining",
        "author": "Qiang Wang, Dawei Feng, Xu Zhang, Ao Shen, Yang Xu, Bo Ding, and Huaimin Wang",
        "link": "http://arxiv.org/abs/2503.24028v1",
        "abstract": "Instruction tuning has emerged as a paramount method for tailoring the\nbehaviors of LLMs. Recent work has unveiled the potential for LLMs to achieve\nhigh performance through fine-tuning with a limited quantity of high-quality\ninstruction data. Building upon this approach, we further explore the impact of\nprompt's robustness on the selection of high-quality instruction data. This\npaper proposes a pioneering framework of high-quality online instruction data\nmining for instruction tuning, focusing on the impact of prompt's robustness on\nthe data mining process. Our notable innovation, is to generate the adversarial\ninstruction data by conducting the attack for the prompt of online instruction\ndata. Then, we introduce an Adversarial Instruction-Following Difficulty metric\nto measure how much help the adversarial instruction data can provide to the\ngeneration of the corresponding response. Apart from it, we propose a novel\nAdversarial Instruction Output Embedding Consistency approach to select\nhigh-quality online instruction data. We conduct extensive experiments on two\nbenchmark datasets to assess the performance. The experimental results serve to\nunderscore the effectiveness of our proposed two methods. Moreover, the results\nunderscore the critical practical significance of considering prompt's\nrobustness."
    },
    {
        "date": "2025-03",
        "title": "Crossmodal Knowledge Distillation with WordNet-Relaxed Text Embeddings for Robust Image Classification",
        "author": "Chenqi Guo, Mengshuo Rong, Qianli Feng, Rongfan Feng, and Yinglong Ma",
        "link": "http://arxiv.org/abs/2503.24017v1",
        "abstract": "Crossmodal knowledge distillation (KD) aims to enhance a unimodal student\nusing a multimodal teacher model. In particular, when the teacher's modalities\ninclude the student's, additional complementary information can be exploited to\nimprove knowledge transfer. In supervised image classification, image datasets\ntypically include class labels that represent high-level concepts, suggesting a\nnatural avenue to incorporate textual cues for crossmodal KD. However, these\nlabels rarely capture the deeper semantic structures in real-world visuals and\ncan lead to label leakage if used directly as inputs, ultimately limiting KD\nperformance. To address these issues, we propose a multi-teacher crossmodal KD\nframework that integrates CLIP image embeddings with learnable WordNet-relaxed\ntext embeddings under a hierarchical loss. By avoiding direct use of exact\nclass names and instead using semantically richer WordNet expansions, we\nmitigate label leakage and introduce more diverse textual cues. Experiments\nshow that this strategy significantly boosts student performance, whereas noisy\nor overly precise text embeddings hinder distillation efficiency.\nInterpretability analyses confirm that WordNet-relaxed prompts encourage\nheavier reliance on visual features over textual shortcuts, while still\neffectively incorporating the newly introduced textual cues. Our method\nachieves state-of-the-art or second-best results on six public datasets,\ndemonstrating its effectiveness in advancing crossmodal KD."
    },
    {
        "date": "2025-03",
        "title": "Model Hemorrhage and the Robustness Limits of Large Language Models",
        "author": "Ziyang Ma, Zuchao Li, Lefei Zhang, Gui-Song Xia, Bo Du, Liangpei Zhang, and Dacheng Tao",
        "link": "http://arxiv.org/abs/2503.23924v1",
        "abstract": "Large language models (LLMs) demonstrate strong performance across natural\nlanguage processing tasks, yet undergo significant performance degradation when\nmodified for deployment through quantization, pruning, or decoding strategy\nadjustments. We define this phenomenon as model hemorrhage - performance\ndecline caused by parameter alterations and architectural changes. Through\nsystematic analysis of various LLM frameworks, we identify key vulnerability\npatterns: layer expansion frequently disrupts attention mechanisms, compression\ntechniques induce information loss cascades, and decoding adjustments amplify\nprediction divergences. Our investigation reveals transformer architectures\nexhibit inherent robustness thresholds that determine hemorrhage severity\nacross modification types. We propose three mitigation strategies:\ngradient-aware pruning preserves critical weight pathways, dynamic quantization\nscaling maintains activation integrity, and decoding calibration aligns\ngeneration trajectories with original model distributions. This work\nestablishes foundational metrics for evaluating model stability during\nadaptation, providing practical guidelines for maintaining performance while\nenabling efficient LLM deployment. Our findings advance understanding of neural\nnetwork resilience under architectural transformations, particularly for\nlarge-scale language models."
    },
    {
        "date": "2025-03",
        "title": "A Channel-Triggered Backdoor Attack on Wireless Semantic Image Reconstruction",
        "author": "Jialin Wan, Nan Cheng, and Jinglong Shen",
        "link": "http://arxiv.org/abs/2503.23866v1",
        "abstract": "Despite the transformative impact of deep learning (DL) on wireless\ncommunication systems through data-driven end-to-end (E2E) learning, the\nsecurity vulnerabilities of these systems have been largely overlooked. Unlike\nthe extensively studied image domain, limited research has explored the threat\nof backdoor attacks on the reconstruction of symbols in semantic communication\n(SemCom) systems. Previous work has investigated such backdoor attacks at the\ninput level, but these approaches are infeasible in applications with strict\ninput control. In this paper, we propose a novel attack paradigm, termed\nChannel-Triggered Backdoor Attack (CT-BA), where the backdoor trigger is a\nspecific wireless channel. This attack leverages fundamental physical layer\ncharacteristics, making it more covert and potentially more threatening\ncompared to previous input-level attacks. Specifically, we utilize channel gain\nwith different fading distributions or channel noise with different power\nspectral densities as potential triggers. This approach establishes\nunprecedented attack flexibility as the adversary can select backdoor triggers\nfrom both fading characteristics and noise variations in diverse channel\nenvironments. Moreover, during the testing phase, CT-BA enables automatic\ntrigger activation through natural channel variations without requiring active\nadversary participation. We evaluate the robustness of CT-BA on a ViT-based\nJoint Source-Channel Coding (JSCC) model across three datasets: MNIST,\nCIFAR-10, and ImageNet. Furthermore, we apply CT-BA to three typical E2E SemCom\nsystems: BDJSCC, ADJSCC, and JSCCOFDM. Experimental results demonstrate that\nour attack achieves near-perfect attack success rate (ASR) while maintaining\neffective stealth. Finally, we discuss potential defense mechanisms against\nsuch attacks."
    },
    {
        "date": "2025-03",
        "title": "Towards Benchmarking and Assessing the Safety and Robustness of Autonomous Driving on Safety-critical Scenarios",
        "author": "Jingzheng Li, Xianglong Liu, Shikui Wei, Zhijun Chen, Bing Li, Qing Guo, Xianqi Yang, Yanjun Pu, and Jiakai Wang",
        "link": "http://arxiv.org/abs/2503.23708v1",
        "abstract": "Autonomous driving has made significant progress in both academia and\nindustry, including performance improvements in perception task and the\ndevelopment of end-to-end autonomous driving systems. However, the safety and\nrobustness assessment of autonomous driving has not received sufficient\nattention. Current evaluations of autonomous driving are typically conducted in\nnatural driving scenarios. However, many accidents often occur in edge cases,\nalso known as safety-critical scenarios. These safety-critical scenarios are\ndifficult to collect, and there is currently no clear definition of what\nconstitutes a safety-critical scenario. In this work, we explore the safety and\nrobustness of autonomous driving in safety-critical scenarios. First, we\nprovide a definition of safety-critical scenarios, including static traffic\nscenarios such as adversarial attack scenarios and natural distribution shifts,\nas well as dynamic traffic scenarios such as accident scenarios. Then, we\ndevelop an autonomous driving safety testing platform to comprehensively\nevaluate autonomous driving systems, encompassing not only the assessment of\nperception modules but also system-level evaluations. Our work systematically\nconstructs a safety verification process for autonomous driving, providing\ntechnical support for the industry to establish standardized test framework and\nreduce risks in real-world road deployment."
    },
    {
        "date": "2025-03",
        "title": "Security Analysis of Chain-FS service",
        "author": "Vanessa Teague, and Arash Mirzaei",
        "link": "http://arxiv.org/abs/2503.23627v1",
        "abstract": "We examine the security of a cloud storage service that makes very strong\nclaims about the ``trustless'' nature of its security. We find that, although\nstored files are end-to-end encrypted, the encryption method allows for\neffective dictionary attacks by a malicious server when passwords only just\nmeet the minimum length required. Furthermore, the file sharing function simply\nsends the decryption passwords to the server with no protection other than TLS."
    },
    {
        "date": "2025-03",
        "title": "Buffer is All You Need: Defending Federated Learning against Backdoor Attacks under Non-iids via Buffering",
        "author": "Xingyu Lyu, Ning Wang, Yang Xiao, Shixiong Li, Tao Li, Danjue Chen, and Yimin Chen",
        "link": "http://arxiv.org/abs/2503.23511v1",
        "abstract": "Federated Learning (FL) is a popular paradigm enabling clients to jointly\ntrain a global model without sharing raw data. However, FL is known to be\nvulnerable towards backdoor attacks due to its distributed nature. As\nparticipants, attackers can upload model updates that effectively compromise\nFL. What's worse, existing defenses are mostly designed under\nindependent-and-identically-distributed (iid) settings, hence neglecting the\nfundamental non-iid characteristic of FL. Here we propose FLBuff for tackling\nbackdoor attacks even under non-iids. The main challenge for such defenses is\nthat non-iids bring benign and malicious updates closer, hence harder to\nseparate. FLBuff is inspired by our insight that non-iids can be modeled as\nomni-directional expansion in representation space while backdoor attacks as\nuni-directional. This leads to the key design of FLBuff, i.e., a\nsupervised-contrastive-learning model extracting penultimate-layer\nrepresentations to create a large in-between buffer layer. Comprehensive\nevaluations demonstrate that FLBuff consistently outperforms state-of-the-art\ndefenses."
    },
    {
        "date": "2025-03",
        "title": "Revisiting the Relationship between Adversarial and Clean Training: Why Clean Training Can Make Adversarial Training Better",
        "author": "MingWei Zhou, and Xiaobing Pei",
        "link": "http://arxiv.org/abs/2504.00038v1",
        "abstract": "Adversarial training (AT) is an effective technique for enhancing adversarial\nrobustness, but it usually comes at the cost of a decline in generalization\nability. Recent studies have attempted to use clean training to assist\nadversarial training, yet there are contradictions among the conclusions. We\ncomprehensively summarize the representative strategies and, with a focus on\nthe multi - view hypothesis, provide a unified explanation for the\ncontradictory phenomena among different studies. In addition, we conduct an in\n- depth analysis of the knowledge combinations transferred from clean - trained\nmodels to adversarially - trained models in previous studies, and find that\nthey can be divided into two categories: reducing the learning difficulty and\nproviding correct guidance. Based on this finding, we propose a new idea of\nleveraging clean training to further improve the performance of advanced AT\nmethods.We reveal that the problem of generalization degradation faced by AT\npartly stems from the difficulty of adversarial training in learning certain\nsample features, and this problem can be alleviated by making full use of clean\ntraining."
    },
    {
        "date": "2025-03",
        "title": "Codehacks: A Dataset of Adversarial Tests for Competitive Programming Problems Obtained from Codeforces",
        "author": "Max Hort, and Leon Moonen",
        "link": "http://arxiv.org/abs/2503.23466v1",
        "abstract": "Software is used in critical applications in our day-to-day life and it is\nimportant to ensure its correctness. One popular approach to assess correctness\nis to evaluate software on tests. If a test fails, it indicates a fault in the\nsoftware under test; if all tests pass correctly, one may assume that the\nsoftware is correct. However, the reliability of these results depends on the\ntest suite considered, and there is a risk of false negatives (i.e. software\nthat passes all available tests but contains bugs because some cases are not\ntested). Therefore, it is important to consider error-inducing test cases when\nevaluating software.\n  To support data-driven creation of such a test-suite, which is especially of\ninterest for testing software synthesized from large language models, we curate\na dataset (Codehacks) of programming problems together with corresponding\nerror-inducing test cases (i.e., \"hacks\"). This dataset is collected from the\nwild, in particular, from the Codeforces online judge platform. The dataset\ncomprises 288,617 hacks for 5,578 programming problems, each with a natural\nlanguage description, as well as the source code for 2,196 submitted solutions\nto these problems that can be broken with their corresponding hacks.\n  Keywords: competitive programming, language model, dataset"
    },
    {
        "date": "2025-03",
        "title": "COSMIC: Clique-Oriented Semantic Multi-space Integration for Robust CLIP Test-Time Adaptation",
        "author": "Fanding Huang, Jingyan Jiang, Qinting Jiang, Hebei Li, Faisal Nadeem Khan, and Zhi Wang",
        "link": "http://arxiv.org/abs/2503.23388v1",
        "abstract": "Recent vision-language models (VLMs) face significant challenges in test-time\nadaptation to novel domains. While cache-based methods show promise by\nleveraging historical information, they struggle with both caching unreliable\nfeature-label pairs and indiscriminately using single-class information during\nquerying, significantly compromising adaptation accuracy. To address these\nlimitations, we propose COSMIC (Clique-Oriented Semantic Multi-space\nIntegration for CLIP), a robust test-time adaptation framework that enhances\nadaptability through multi-granular, cross-modal semantic caching and\ngraph-based querying mechanisms. Our framework introduces two key innovations:\nDual Semantics Graph (DSG) and Clique Guided Hyper-class (CGH). The Dual\nSemantics Graph constructs complementary semantic spaces by incorporating\ntextual features, coarse-grained CLIP features, and fine-grained DINOv2\nfeatures to capture rich semantic relationships. Building upon these dual\ngraphs, the Clique Guided Hyper-class component leverages structured class\nrelationships to enhance prediction robustness through correlated class\nselection. Extensive experiments demonstrate COSMIC's superior performance\nacross multiple benchmarks, achieving significant improvements over\nstate-of-the-art methods: 15.81% gain on out-of-distribution tasks and 5.33% on\ncross-domain generation with CLIP RN-50. Code is available at\ngithub.com/hf618/COSMIC."
    },
    {
        "date": "2025-03",
        "title": "Two Heads Are Better than One: Model-Weight and Latent-Space Analysis for Federated Learning on Non-iid Data against Poisoning Attacks",
        "author": "Xingyu Lyu, Ning Wang, Yang Xiao, Shixiong Li, Tao Li, Danjue Chen, and Yimin Chen",
        "link": "http://arxiv.org/abs/2503.23288v1",
        "abstract": "Federated Learning is a popular paradigm that enables remote clients to\njointly train a global model without sharing their raw data. However, FL has\nbeen shown to be vulnerable towards model poisoning attacks due to its\ndistributed nature. Particularly, attackers acting as participants can upload\narbitrary model updates that effectively compromise the global model of FL.\nWhile extensive research has been focusing on fighting against these attacks,\nwe find that most of them assume data at remote clients are under iid while in\npractice they are inevitably non-iid. Our benchmark evaluations reveal that\nexisting defenses generally fail to live up to their reputation when applied to\nvarious non-iid scenarios. In this paper, we propose a novel approach,\nGeminiGuard, that aims to address such a significant gap. We design GeminiGuard\nto be lightweight, versatile, and unsupervised so that it aligns well with the\npractical requirements of deploying such defenses. The key challenge from\nnon-iids is that they make benign model updates look more similar to malicious\nones. GeminiGuard is mainly built on two fundamental observations: (1) existing\ndefenses based on either model-weight analysis or latent-space analysis face\nlimitations in covering different MPAs and non-iid scenarios, and (2)\nmodel-weight and latent-space analysis are sufficiently different yet\npotentially complementary methods as MPA defenses. We hence incorporate a novel\nmodel-weight analysis component as well as a custom latent-space analysis\ncomponent in GeminiGuard, aiming to further enhance its defense performance. We\nconduct extensive experiments to evaluate our defense across various settings,\ndemonstrating its effectiveness in countering multiple types of untargeted and\ntargeted MPAs, including adaptive ones. Our comprehensive evaluations show that\nGeminiGuard consistently outperforms SOTA defenses under various settings."
    },
    {
        "date": "2025-03",
        "title": "Model Context Protocol (MCP): Landscape, Security Threats, and Future Research Directions",
        "author": "Xinyi Hou, Yanjie Zhao, Shenao Wang, and Haoyu Wang",
        "link": "http://arxiv.org/abs/2503.23278v1",
        "abstract": "The Model Context Protocol (MCP) is a standardized interface designed to\nenable seamless interaction between AI models and external tools and resources,\nbreaking down data silos and facilitating interoperability across diverse\nsystems. This paper provides a comprehensive overview of MCP, focusing on its\ncore components, workflow, and the lifecycle of MCP servers, which consists of\nthree key phases: creation, operation, and update. We analyze the security and\nprivacy risks associated with each phase and propose strategies to mitigate\npotential threats. The paper also examines the current MCP landscape, including\nits adoption by industry leaders and various use cases, as well as the tools\nand platforms supporting its integration. We explore future directions for MCP,\nhighlighting the challenges and opportunities that will influence its adoption\nand evolution within the broader AI ecosystem. Finally, we offer\nrecommendations for MCP stakeholders to ensure its secure and sustainable\ndevelopment as the AI landscape continues to evolve."
    },
    {
        "date": "2025-03",
        "title": "Comprehensive Survey towards Security Authentication Methods for Satellite Communication Systems",
        "author": "Yunfei Meng, Changbo Ke, and Zhiqiu Huang",
        "link": "http://arxiv.org/abs/2503.23277v1",
        "abstract": "Satellite communication systems (SatCom) is a brand-new network that uses\nartificial Earth satellites as relay stations to provide communication services\nsuch as broadband Internet access to various users on land, sea, air and in\nspace. It features wide coverage, relatively high transmission rates and strong\nanti-interference capabilities. Security authentication is of crucial\nsignificance for the stable operation and widespread application of satellite\ncommunication systems. It can effectively prevent unauthorized access, ensuring\nthat only users and devices that pass security authentication can access the\nsatellite network. It also ensures the confidentiality, integrity, and\navailability of data during transmission and storage, preventing data from\nbeing stolen, tampered with, or damaged. By means of literature research and\ncomparative analysis, this paper carries out on a comprehensive survey towards\nthe security authentication methods used by SatCom. This paper first summarizes\nthe existing SatCom authentication methods as five categories, namely, those\nbased on cryptography, Blockchain, satellite orbital information, the AKA\nprotocol and physical hardware respectively. Subsequently, a comprehensive\ncomparative analysis is carried out on the above-mentioned five categories of\nsecurity authentication methods from four dimensions, i.e., security,\nimplementation difficulty and cost, applicable scenarios and real-time\nperformance, and the final comparison results are following obtained. Finally,\nprospects are made for several important future research directions of security\nauthentication methods for SatCom, laying a well foundation for further\ncarrying on the related research works."
    },
    {
        "date": "2025-03",
        "title": "OwlSight: A Robust Illumination Adaptation Framework for Dark Video Human Action Recognition",
        "author": "Shihao Cheng, Jinlu Zhang, Yue Liu, and Zhigang Tu",
        "link": "http://arxiv.org/abs/2503.23266v1",
        "abstract": "Human action recognition in low-light environments is crucial for various\nreal-world applications. However, the existing approaches overlook the full\nutilization of brightness information throughout the training phase, leading to\nsuboptimal performance. To address this limitation, we propose OwlSight, a\nbiomimetic-inspired framework with whole-stage illumination enhancement to\ninteract with action classification for accurate dark video human action\nrecognition. Specifically, OwlSight incorporates a Time-Consistency Module\n(TCM) to capture shallow spatiotemporal features meanwhile maintaining temporal\ncoherence, which are then processed by a Luminance Adaptation Module (LAM) to\ndynamically adjust the brightness based on the input luminance distribution.\nFurthermore, a Reflect Augmentation Module (RAM) is presented to maximize\nillumination utilization and simultaneously enhance action recognition via two\ninteractive paths. Additionally, we build Dark-101, a large-scale dataset\ncomprising 18,310 dark videos across 101 action categories, significantly\nsurpassing existing datasets (e.g., ARID1.5 and Dark-48) in scale and\ndiversity. Extensive experiments demonstrate that the proposed OwlSight\nachieves state-of-the-art performance across four low-light action recognition\nbenchmarks. Notably, it outperforms previous best approaches by 5.36% on\nARID1.5 and 1.72% on Dark-101, highlighting its effectiveness in challenging\ndark environments."
    },
    {
        "date": "2025-03",
        "title": "Mismatch-Robust Underwater Acoustic Localization Using A Differentiable Modular Forward Model",
        "author": "Dariush Kari, Yongjie Zhuang, and Andrew C. Singer",
        "link": "http://arxiv.org/abs/2503.23260v1",
        "abstract": "In this paper, we study the underwater acoustic localization in the presence\nof environmental mismatch. Especially, we exploit a pre-trained neural network\nfor the acoustic wave propagation in a gradient-based optimization framework to\nestimate the source location. To alleviate the effect of mismatch between the\ntraining data and the test data, we simultaneously optimize over the network\nweights at the inference time, and provide conditions under which this method\nis effective. Moreover, we introduce a physics-inspired modularity in the\nforward model that enables us to learn the path lengths of the multipath\nstructure in an end-to-end training manner without access to the specific path\nlabels. We investigate the validity of the assumptions in a simple yet\nillustrative environment model."
    },
    {
        "date": "2025-03",
        "title": "Encrypted Prompt: Securing LLM Applications Against Unauthorized Actions",
        "author": "Shih-Han Chan",
        "link": "http://arxiv.org/abs/2503.23250v1",
        "abstract": "Security threats like prompt injection attacks pose significant risks to\napplications that integrate Large Language Models (LLMs), potentially leading\nto unauthorized actions such as API misuse. Unlike previous approaches that aim\nto detect these attacks on a best-effort basis, this paper introduces a novel\nmethod that appends an Encrypted Prompt to each user prompt, embedding current\npermissions. These permissions are verified before executing any actions (such\nas API calls) generated by the LLM. If the permissions are insufficient, the\nLLM's actions will not be executed, ensuring safety. This approach guarantees\nthat only actions within the scope of the current permissions from the LLM can\nproceed. In scenarios where adversarial prompts are introduced to mislead the\nLLM, this method ensures that any unauthorized actions from LLM wouldn't be\nexecuted by verifying permissions in Encrypted Prompt. Thus, threats like\nprompt injection attacks that trigger LLM to generate harmful actions can be\neffectively mitigated."
    },
    {
        "date": "2025-03",
        "title": "Agent-Based Modeling and Deep Neural Networks for Establishing Digital Twins of Secure Facilities under Sensing Restrictions",
        "author": "Chathika Gunaratne, Mason Stott, Debraj De, Gautam Malviya Thakur, and Chris Young",
        "link": "http://arxiv.org/abs/2503.23147v1",
        "abstract": "Digital twin technologies help practitioners simulate, monitor, and predict\nundesirable outcomes in-silico, while avoiding the cost and risks of conducting\nlive simulation exercises. Virtual reality (VR) based digital twin technologies\nare especially useful when monitoring human Patterns of Life (POL) in secure\nnuclear facilities, where live simulation exercises are too dangerous and\ncostly to ever perform. However, the high-security status of such facilities\nmay restrict modelers from deploying human activity sensors for data\ncollection. This problem was encountered when deploying MetaPOL, a digital twin\nsystem to prevent insider threat or sabotage of secure facilities, at a secure\nnuclear reactor facility at Oak Ridge National Laboratory (ORNL). This\nchallenge was addressed using an agent-based model (ABM), driven by anecdotal\nevidence of facility personnel POL, to generate synthetic movement\ntrajectories. These synthetic trajectories were then used to train deep neural\nnetwork surrogates for next location and stay duration prediction to drive NPCs\nin the VR environment. In this study, we evaluate the efficacy of this\ntechnique for establishing NPC movement within MetaPOL and the ability to\ndistinguish NPC movement during normal operations from that during a simulated\nemergency response. Our results demonstrate the success of using a multi-layer\nperceptron for next location prediction and mixture density network for stay\nduration prediction to predict the ABM generated trajectories. We also find\nthat NPC movement in the VR environment driven by the deep neural networks\nunder normal operations remain significantly different to that seen when\nsimulating responses to a simulated emergency scenario."
    },
    {
        "date": "2025-03",
        "title": "AuditVotes: A Framework Towards More Deployable Certified Robustness for Graph Neural Networks",
        "author": "Yuni Lai, Yulin Zhu, Yixuan Sun, Yulun Wu, Bin Xiao, Gaolei Li, Jianhua Li, and Kai Zhou",
        "link": "http://arxiv.org/abs/2503.22998v1",
        "abstract": "Despite advancements in Graph Neural Networks (GNNs), adaptive attacks\ncontinue to challenge their robustness. Certified robustness based on\nrandomized smoothing has emerged as a promising solution, offering provable\nguarantees that a model's predictions remain stable under adversarial\nperturbations within a specified range. However, existing methods face a\ncritical trade-off between accuracy and robustness, as achieving stronger\nrobustness requires introducing greater noise into the input graph. This\nexcessive randomization degrades data quality and disrupts prediction\nconsistency, limiting the practical deployment of certifiably robust GNNs in\nreal-world scenarios where both accuracy and robustness are essential. To\naddress this challenge, we propose \\textbf{AuditVotes}, the first framework to\nachieve both high clean accuracy and certifiably robust accuracy for GNNs. It\nintegrates randomized smoothing with two key components,\n\\underline{au}gmentation and con\\underline{dit}ional smoothing, aiming to\nimprove data quality and prediction consistency. The augmentation, acting as a\npre-processing step, de-noises the randomized graph, significantly improving\ndata quality and clean accuracy. The conditional smoothing, serving as a\npost-processing step, employs a filtering function to selectively count votes,\nthereby filtering low-quality predictions and improving voting consistency.\nExtensive experimental results demonstrate that AuditVotes significantly\nenhances clean accuracy, certified robustness, and empirical robustness while\nmaintaining high computational efficiency. Notably, compared to baseline\nrandomized smoothing, AuditVotes improves clean accuracy by $437.1\\%$ and\ncertified accuracy by $409.3\\%$ when the attacker can arbitrarily insert $20$\nedges on the Cora-ML datasets, representing a substantial step toward deploying\ncertifiably robust GNNs in real-world applications."
    },
    {
        "date": "2025-03",
        "title": "Enhancing Federated Learning Through Secure Cluster-Weighted Client Aggregation",
        "author": "Kanishka Ranaweera, Azadeh Ghari Neiat, Xiao Liu, Bipasha Kashyap, and Pubudu N. Pathirana",
        "link": "http://arxiv.org/abs/2503.22971v1",
        "abstract": "Federated learning (FL) has emerged as a promising paradigm in machine\nlearning, enabling collaborative model training across decentralized devices\nwithout the need for raw data sharing. In FL, a global model is trained\niteratively on local datasets residing on individual devices, each contributing\nto the model's improvement. However, the heterogeneous nature of these local\ndatasets, stemming from diverse user behaviours, device capabilities, and data\ndistributions, poses a significant challenge. The inherent heterogeneity in\nfederated learning gives rise to various issues, including model performance\ndiscrepancies, convergence challenges, and potential privacy concerns. As the\nglobal model progresses through rounds of training, the disparities in local\ndata quality and quantity can impede the overall effectiveness of federated\nlearning systems. Moreover, maintaining fairness and privacy across diverse\nuser groups becomes a paramount concern. To address this issue, this paper\nintroduces a novel FL framework, ClusterGuardFL, that employs dissimilarity\nscores, k-means clustering, and reconciliation confidence scores to dynamically\nassign weights to client updates. The dissimilarity scores between global and\nlocal models guide the formation of clusters, with cluster size influencing the\nweight allocation. Within each cluster, a reconciliation confidence score is\ncalculated for individual data points, and a softmax layer generates customized\nweights for clients. These weights are utilized in the aggregation process,\nenhancing the model's robustness and privacy. Experimental results demonstrate\nthe efficacy of the proposed approach in achieving improved model performance\nin diverse datasets."
    },
    {
        "date": "2025-03",
        "title": "Improving the Context Length and Efficiency of Code Retrieval for Tracing Security Vulnerability Fixes",
        "author": "Xueqing Liu, Jiangrui Zheng, Guanqun Yang, Siyan Wen, and Qiushi Liu",
        "link": "http://arxiv.org/abs/2503.22935v1",
        "abstract": "In recent years, the rapid increase of security vulnerabilities has caused\nmajor challenges in managing them. One critical task in vulnerability\nmanagement is tracing the patches that fix a vulnerability. By accurately\ntracing the patching commits, security stakeholders can precisely identify\naffected software components, determine vulnerable and fixed versions, assess\nthe severity etc., which facilitates rapid deployment of mitigations. However,\nprevious work has shown that the patch information is often missing in\nvulnerability databases, including both the National Vulnerability Databases\n(NVD) and the GitHub Advisory Database, which increases the risk of delayed\nmitigation, incorrect vulnerability assessment, and potential exploits.\n  Although existing work has proposed several approaches for patch tracing,\nthey suffer from two major challenges: (1) the lack of scalability to the\nfull-repository level, and (2) the lack of study on how to model the semantic\nsimilarity between the CVE and the full diff code. Upon identifying this gap,\nwe propose SITPatchTracer, a scalable full-repo full-context retrieval system\nfor security vulnerability patch tracing. SITPatchTracer leverages\nElasticSearch, learning-to-rank, and a hierarchical embedding approach based on\nGritLM, a top-ranked LLM for text embedding with unlimited context length and\nfast inference speed. The evaluation of SITPatchTracer shows that it achieves a\nhigh recall on both evaluated datasets. SITPatchTracer's recall not only\noutperforms several existing works (PatchFinder, PatchScout, VFCFinder), but\nalso Voyage, the SOTA commercial code embedding API by 13\\% and 28\\%."
    },
    {
        "date": "2025-03",
        "title": "Factored Agents: Decoupling In-Context Learning and Memorization for Robust Tool Use",
        "author": "Nicholas Roth, Christopher Hidey, Lucas Spangher, William F. Arnold, Chang Ye, Nick Masiewicki, Jinoo Baek, Peter Grabowski, and Eugene Ie",
        "link": "http://arxiv.org/abs/2503.22931v2",
        "abstract": "In this paper, we propose a novel factored agent architecture designed to\novercome the limitations of traditional single-agent systems in agentic AI. Our\napproach decomposes the agent into two specialized components: (1) a large\nlanguage model (LLM) that serves as a high level planner and in-context\nlearner, which may use dynamically available information in user prompts, (2) a\nsmaller language model which acts as a memorizer of tool format and output.\nThis decoupling addresses prevalent issues in monolithic designs, including\nmalformed, missing, and hallucinated API fields, as well as suboptimal planning\nin dynamic environments. Empirical evaluations demonstrate that our factored\narchitecture significantly improves planning accuracy and error resilience,\nwhile elucidating the inherent trade-off between in-context learning and static\nmemorization. These findings suggest that a factored approach is a promising\npathway for developing more robust and adaptable agentic AI systems."
    },
    {
        "date": "2025-03",
        "title": "Nested Stochastic Gradient Descent for (Generalized) Sinkhorn Distance-Regularized Distributionally Robust Optimization",
        "author": "Yufeng Yang, Yi Zhou, and Zhaosong Lu",
        "link": "http://arxiv.org/abs/2503.22923v1",
        "abstract": "Distributionally robust optimization (DRO) is a powerful technique to train\nrobust models against data distribution shift. This paper aims to solve\nregularized nonconvex DRO problems, where the uncertainty set is modeled by a\nso-called generalized Sinkhorn distance and the loss function is nonconvex and\npossibly unbounded. Such a distance allows to model uncertainty of\ndistributions with different probability supports and divergence functions. For\nthis class of regularized DRO problems, we derive a novel dual formulation\ntaking the form of nested stochastic programming, where the dual variable\ndepends on the data sample. To solve the dual problem, we provide theoretical\nevidence to design a nested stochastic gradient descent (SGD) algorithm, which\nleverages stochastic approximation to estimate the nested stochastic gradients.\nWe study the convergence rate of nested SGD and establish polynomial iteration\nand sample complexities that are independent of the data size and parameter\ndimension, indicating its potential for solving large-scale DRO problems. We\nconduct numerical experiments to demonstrate the efficiency and robustness of\nthe proposed algorithm."
    },
    {
        "date": "2025-03",
        "title": "Quamba2: A Robust and Scalable Post-training Quantization Framework for Selective State Space Models",
        "author": "Hung-Yueh Chiang, Chi-Chih Chang, Natalia Frumkin, Kai-Chiang Wu, Mohamed S. Abdelfattah, and Diana Marculescu",
        "link": "http://arxiv.org/abs/2503.22879v2",
        "abstract": "State Space Models (SSMs) are emerging as a compelling alternative to\nTransformers because of their consistent memory usage and high performance.\nDespite this, scaling up SSMs on cloud services or limited-resource devices is\nchallenging due to their storage requirements and computational power. To\novercome this, quantizing SSMs with low bit-width data formats can reduce model\nsize and benefit from hardware acceleration. As SSMs are prone to\nquantization-induced errors, recent efforts have focused on optimizing a\nparticular model or bit-width for efficiency without sacrificing performance.\nHowever, distinct bit-width configurations are essential for different\nscenarios, like W4A8 for boosting large-batch decoding speed, and W4A16 for\nenhancing generation speed in short prompt applications for a single user. To\nthis end, we present Quamba2, compatible with W8A8, W4A8, and W4A16 for both\nMamba1 and Mamba2 backbones, addressing the growing demand for SSM deployment\non various platforms. Based on the channel order preserving and activation\npersistence of SSMs, we propose an offline approach to quantize inputs of a\nlinear recurrence in 8-bit by sorting and clustering for input $x$, combined\nwith a per-state-group quantization for input-dependent parameters $B$ and $C$.\nTo ensure compute-invariance in the SSM output, we rearrange weights offline\naccording to the clustering sequence. The experiments show that Quamba2-8B\noutperforms several state-of-the-art SSM quantization methods and delivers\n1.3$\\times$ and 3$\\times$ speed-ups in the pre-filling and generation stages,\nrespectively, while offering 4$\\times$ memory reduction with only a $1.6\\%$\naverage accuracy drop. The evaluation on MMLU shows the generalizability and\nrobustness of our framework. The code and quantized models will be released at:\nhttps://github.com/enyac-group/Quamba."
    },
    {
        "date": "2025-03",
        "title": "RobuNFR: Evaluating the Robustness of Large Language Models on Non-Functional Requirements Aware Code Generation",
        "author": "Feng Lin, Dong Jae Kim, Zhenhao Li, Jinqiu Yang, Tse-Hsun, and Chen",
        "link": "http://arxiv.org/abs/2503.22851v2",
        "abstract": "When using LLMs to address Non-Functional Requirements (NFRs), developers may\nbehave differently (e.g., expressing the same NFR in different words). Robust\nLLMs should output consistent results across these variations; however, this\naspect remains underexplored. We propose RobuNFR for evaluating the robustness\nof LLMs in NFR-aware code generation across four NFR dimensions: design,\nreadability, reliability, and performance, using three methodologies: prompt\nvariation, regression testing, and diverse workflows. Our experiments show that\nRobuNFR reveals robustness issues in the tested LLMs when considering NFRs in\ncode generation. Specifically, under prompt variation, including NFRs leads to\na decrease in Pass@1 by up to 39 percent and an increase in the standard\ndeviation from 0.48 to 2.48 compared to the baseline without NFRs (i.e.,\nFunction-Only). While incorporating NFRs generally improves overall NFR\nmetrics, it also results in higher prompt sensitivity. In regression settings,\nsome LLMs exhibit differences across versions, with improvements in one aspect\n(e.g., reduced code smells) often accompanied by regressions in another (e.g.,\ndecreased correctness), revealing inconsistencies that challenge their\nrobustness. When varying workflows, the tested LLMs show significantly\ndifferent NFR-aware code generation capabilities between two workflows: (1)\nintegrating NFRs and functional requirements into the initial prompt and (2)\nenhancing Function-Only-generated code with the same NFR."
    },
    {
        "date": "2025-03",
        "title": "Tropical Bisectors and Carlini-Wagner Attacks",
        "author": "Gillian Grindstaff, Julia Lindberg, Daniela Schkoda, Miruna-Stefana Sorea, and Ruriko Yoshida",
        "link": "http://arxiv.org/abs/2503.22653v1",
        "abstract": "Pasque et al. showed that using a tropical symmetric metric as an activation\nfunction in the last layer can improve the robustness of convolutional neural\nnetworks (CNNs) against state-of-the-art attacks, including the Carlini-Wagner\nattack. This improvement occurs when the attacks are not specifically adapted\nto the non-differentiability of the tropical layer. Moreover, they showed that\nthe decision boundary of a tropical CNN is defined by tropical bisectors. In\nthis paper, we explore the combinatorics of tropical bisectors and analyze how\nthe tropical embedding layer enhances robustness against Carlini-Wagner\nattacks. We prove an upper bound on the number of linear segments the decision\nboundary of a tropical CNN can have. We then propose a refined version of the\nCarlini-Wagner attack, specifically tailored for the tropical architecture.\nComputational experiments with MNIST and LeNet5 showcase our attacks improved\nsuccess rate."
    },
    {
        "date": "2025-03",
        "title": "Advancing DevSecOps in SMEs: Challenges and Best Practices for Secure CI/CD Pipelines",
        "author": "Jayaprakashreddy Cheenepalli, John D. Hastings, Khandaker Mamun Ahmed, and Chad Fenner",
        "link": "http://arxiv.org/abs/2503.22612v1",
        "abstract": "This study evaluates the adoption of DevSecOps among small and medium-sized\nenterprises (SMEs), identifying key challenges, best practices, and future\ntrends. Through a mixed methods approach backed by the Technology Acceptance\nModel (TAM) and Diffusion of Innovations (DOI) theory, we analyzed survey data\nfrom 405 SME professionals, revealing that while 68% have implemented\nDevSecOps, adoption is hindered by technical complexity (41%), resource\nconstraints (35%), and cultural resistance (38%). Despite strong leadership\nprioritization of security (73%), automation gaps persist, with only 12% of\norganizations conducting security scans per commit.\n  Our findings highlight a growing integration of security tools, particularly\nAPI security (63%) and software composition analysis (62%), although container\nsecurity adoption remains low (34%). Looking ahead, SMEs anticipate artificial\nintelligence and machine learning to significantly influence DevSecOps,\nunderscoring the need for proactive adoption of AI-driven security\nenhancements. Based on our findings, this research proposes strategic best\npractices to enhance CI/CD pipeline security including automation,\nleadership-driven security culture, and cross-team collaboration."
    },
    {
        "date": "2025-03",
        "title": "Robust Offline Imitation Learning Through State-level Trajectory Stitching",
        "author": "Shuze Wang, Yunpeng Mei, Hongjie Cao, Yetian Yuan, Gang Wang, Jian Sun, and Jie Chen",
        "link": "http://arxiv.org/abs/2503.22524v1",
        "abstract": "Imitation learning (IL) has proven effective for enabling robots to acquire\nvisuomotor skills through expert demonstrations. However, traditional IL\nmethods are limited by their reliance on high-quality, often scarce, expert\ndata, and suffer from covariate shift. To address these challenges, recent\nadvances in offline IL have incorporated suboptimal, unlabeled datasets into\nthe training. In this paper, we propose a novel approach to enhance policy\nlearning from mixed-quality offline datasets by leveraging task-relevant\ntrajectory fragments and rich environmental dynamics. Specifically, we\nintroduce a state-based search framework that stitches state-action pairs from\nimperfect demonstrations, generating more diverse and informative training\ntrajectories. Experimental results on standard IL benchmarks and real-world\nrobotic tasks showcase that our proposed method significantly improves both\ngeneralization and performance."
    },
    {
        "date": "2025-03",
        "title": "Robustness quantification and how it allows for reliable classification, even in the presence of distribution shift and for small training sets",
        "author": "Adri\u00e1n Detavernier, and Jasper De Bock",
        "link": "http://arxiv.org/abs/2503.22418v1",
        "abstract": "Based on existing ideas in the field of imprecise probabilities, we present a\nnew approach for assessing the reliability of the individual predictions of a\ngenerative probabilistic classifier. We call this approach robustness\nquantification, compare it to uncertainty quantification, and demonstrate that\nit continues to work well even for classifiers that are learned from small\ntraining sets that are sampled from a shifted distribution."
    },
    {
        "date": "2025-03",
        "title": "Endo-TTAP: Robust Endoscopic Tissue Tracking via Multi-Facet Guided Attention and Hybrid Flow-point Supervision",
        "author": "Rulin Zhou, Wenlong He, An Wang, Qiqi Yao, Haijun Hu, Jiankun Wang, and Xi Zhang an Hongliang Ren",
        "link": "http://arxiv.org/abs/2503.22394v1",
        "abstract": "Accurate tissue point tracking in endoscopic videos is critical for\nrobotic-assisted surgical navigation and scene understanding, but remains\nchallenging due to complex deformations, instrument occlusion, and the scarcity\nof dense trajectory annotations. Existing methods struggle with long-term\ntracking under these conditions due to limited feature utilization and\nannotation dependence. We present Endo-TTAP, a novel framework addressing these\nchallenges through: (1) A Multi-Facet Guided Attention (MFGA) module that\nsynergizes multi-scale flow dynamics, DINOv2 semantic embeddings, and explicit\nmotion patterns to jointly predict point positions with uncertainty and\nocclusion awareness; (2) A two-stage curriculum learning strategy employing an\nAuxiliary Curriculum Adapter (ACA) for progressive initialization and hybrid\nsupervision. Stage I utilizes synthetic data with optical flow ground truth for\nuncertainty-occlusion regularization, while Stage II combines unsupervised flow\nconsistency and semi-supervised learning with refined pseudo-labels from\noff-the-shelf trackers. Extensive validation on two MICCAI Challenge datasets\nand our collected dataset demonstrates that Endo-TTAP achieves state-of-the-art\nperformance in tissue point tracking, particularly in scenarios characterized\nby complex endoscopic conditions. The source code and dataset will be available\nat https://anonymous.4open.science/r/Endo-TTAP-36E5."
    },
    {
        "date": "2025-03",
        "title": "Privacy-Preserving Secure Neighbor Discovery for Wireless Networks",
        "author": "Ahmed Mohamed Hussain, and Panos Papadimitratos",
        "link": "http://arxiv.org/abs/2503.22232v2",
        "abstract": "Traditional Neighbor Discovery (ND) and Secure Neighbor Discovery (SND) are\nkey elements for network functionality. SND is a hard problem, satisfying not\nonly typical security properties (authentication, integrity) but also\nverification of direct communication, which involves distance estimation based\non time measurements and device coordinates. Defeating relay attacks, also\nknown as \"wormholes\", leading to stealthy Byzantine links and significant\ndegradation of communication and adversarial control, is key in many wireless\nnetworked systems. However, SND is not concerned with privacy; it necessitates\nrevealing the identity and location of the device(s) participating in the\nprotocol execution. This can be a deterrent for deployment, especially\ninvolving user-held devices in the emerging Internet of Things (IoT) enabled\nsmart environments. To address this challenge, we present a novel\nPrivacy-Preserving Secure Neighbor Discovery (PP-SND) protocol, enabling\ndevices to perform SND without revealing their actual identities and locations,\neffectively decoupling discovery from the exposure of sensitive information. We\nuse Homomorphic Encryption (HE) for computing device distances without\nrevealing their actual coordinates, as well as employing a pseudonymous device\nauthentication to hide identities while preserving communication integrity.\nPP-SND provides SND [1] along with pseudonymity, confidentiality, and\nunlinkability. Our presentation here is not specific to one wireless\ntechnology, and we assess the performance of the protocols (cryptographic\noverhead) on a Raspberry Pi 4 and provide a security and privacy analysis."
    },
    {
        "date": "2025-03",
        "title": "Intrinsic Image Decomposition for Robust Self-supervised Monocular Depth Estimation on Reflective Surfaces",
        "author": "Wonhyeok Choi, Kyumin Hwang, Minwoo Choi, Kiljoon Han, Wonjoon Choi, Mingyu Shin, and Sunghoon Im",
        "link": "http://arxiv.org/abs/2503.22209v1",
        "abstract": "Self-supervised monocular depth estimation (SSMDE) has gained attention in\nthe field of deep learning as it estimates depth without requiring ground truth\ndepth maps. This approach typically uses a photometric consistency loss between\na synthesized image, generated from the estimated depth, and the original\nimage, thereby reducing the need for extensive dataset acquisition. However,\nthe conventional photometric consistency loss relies on the Lambertian\nassumption, which often leads to significant errors when dealing with\nreflective surfaces that deviate from this model. To address this limitation,\nwe propose a novel framework that incorporates intrinsic image decomposition\ninto SSMDE. Our method synergistically trains for both monocular depth\nestimation and intrinsic image decomposition. The accurate depth estimation\nfacilitates multi-image consistency for intrinsic image decomposition by\naligning different view coordinate systems, while the decomposition process\nidentifies reflective areas and excludes corrupted gradients from the depth\ntraining process. Furthermore, our framework introduces a pseudo-depth\ngeneration and knowledge distillation technique to further enhance the\nperformance of the student model across both reflective and non-reflective\nsurfaces. Comprehensive evaluations on multiple datasets show that our approach\nsignificantly outperforms existing SSMDE baselines in depth prediction,\nespecially on reflective surfaces."
    },
    {
        "date": "2025-03",
        "title": "Data-Free Universal Attack by Exploiting the Intrinsic Vulnerability of Deep Models",
        "author": "YangTian Yan, and Jinyu Tian",
        "link": "http://arxiv.org/abs/2503.22205v1",
        "abstract": "Deep neural networks (DNNs) are susceptible to Universal Adversarial\nPerturbations (UAPs), which are instance agnostic perturbations that can\ndeceive a target model across a wide range of samples. Unlike instance-specific\nadversarial examples, UAPs present a greater challenge as they must generalize\nacross different samples and models. Generating UAPs typically requires access\nto numerous examples, which is a strong assumption in real-world tasks. In this\npaper, we propose a novel data-free method called Intrinsic UAP (IntriUAP), by\nexploiting the intrinsic vulnerabilities of deep models. We analyze a series of\npopular deep models composed of linear and nonlinear layers with a Lipschitz\nconstant of 1, revealing that the vulnerability of these models is\npredominantly influenced by their linear components. Based on this observation,\nwe leverage the ill-conditioned nature of the linear components by aligning the\nUAP with the right singular vectors corresponding to the maximum singular value\nof each linear layer. Remarkably, our method achieves highly competitive\nperformance in attacking popular image classification deep models without using\nany image samples. We also evaluate the black-box attack performance of our\nmethod, showing that it matches the state-of-the-art baseline for data-free\nmethods on models that conform to our theoretical framework. Beyond the\ndata-free assumption, IntriUAP also operates under a weaker assumption, where\nthe adversary only can access a few of the victim model's layers. Experiments\ndemonstrate that the attack success rate decreases by only 4% when the\nadversary has access to just 50% of the linear layers in the victim model."
    },
    {
        "date": "2025-03",
        "title": "T-CIL: Temperature Scaling using Adversarial Perturbation for Calibration in Class-Incremental Learning",
        "author": "Seong-Hyeon Hwang, Minsu Kim, and Steven Euijong Whang",
        "link": "http://arxiv.org/abs/2503.22163v1",
        "abstract": "We study model confidence calibration in class-incremental learning, where\nmodels learn from sequential tasks with different class sets. While existing\nworks primarily focus on accuracy, maintaining calibrated confidence has been\nlargely overlooked. Unfortunately, most post-hoc calibration techniques are not\ndesigned to work with the limited memories of old-task data typical in\nclass-incremental learning, as retaining a sufficient validation set would be\nimpractical. Thus, we propose T-CIL, a novel temperature scaling approach for\nclass-incremental learning without a validation set for old tasks, that\nleverages adversarially perturbed exemplars from memory. Directly using\nexemplars is inadequate for temperature optimization, since they are already\nused for training. The key idea of T-CIL is to perturb exemplars more strongly\nfor old tasks than for the new task by adjusting the perturbation direction\nbased on feature distance, with the single magnitude determined using the\nnew-task validation set. This strategy makes the perturbation magnitude\ncomputed from the new task also applicable to old tasks, leveraging the\ntendency that the accuracy of old tasks is lower than that of the new task. We\nempirically show that T-CIL significantly outperforms various baselines in\nterms of calibration on real datasets and can be integrated with existing\nclass-incremental learning techniques with minimal impact on accuracy."
    },
    {
        "date": "2025-03",
        "title": "Traffic Modeling for Network Security and Privacy: Challenges Ahead",
        "author": "Dinil Mon Divakaran",
        "link": "http://arxiv.org/abs/2503.22161v1",
        "abstract": "Traffic analysis using machine learning and deep learning models has made\nsignificant progress over the past decades. These models address various tasks\nin network security and privacy, including detection of anomalies and attacks,\ncountering censorship, etc. They also reveal privacy risks to users as\ndemonstrated by the research on LLM token inference as well as fingerprinting\n(and counter-fingerprinting) of user-visiting websites, IoT devices, and\ndifferent applications. However, challenges remain in securing our networks\nfrom threats and attacks. After briefly reviewing the tasks and recent ML\nmodels in network security and privacy, we discuss the challenges that lie\nahead."
    },
    {
        "date": "2025-03",
        "title": "SoK: Security Analysis of Blockchain-based Cryptocurrency",
        "author": "Zekai Liu, and Xiaoqi Li",
        "link": "http://arxiv.org/abs/2503.22156v1",
        "abstract": "Cryptocurrency is a novel exploration of a form of currency that proposes a\ndecentralized electronic payment scheme based on blockchain technology and\ncryptographic theory. While cryptocurrency has the security characteristics of\nbeing distributed and tamper-proof, increasing market demand has led to a rise\nin malicious transactions and attacks, thereby exposing cryptocurrency to\nvulnerabilities, privacy issues, and security threats. Particularly concerning\nare the emerging types of attacks and threats, which have made securing\ncryptocurrency increasingly urgent. Therefore, this paper classifies existing\ncryptocurrency security threats and attacks into five fundamental categories\nbased on the blockchain infrastructure and analyzes in detail the vulnerability\nprinciples exploited by each type of threat and attack. Additionally, the paper\nexamines the attackers' logic and methods and successfully reproduces the\nvulnerabilities. Furthermore, the author summarizes the existing detection and\ndefense solutions and evaluates them, all of which provide important references\nfor ensuring the security of cryptocurrency. Finally, the paper discusses the\nfuture development trends of cryptocurrency, as well as the public challenges\nit may face."
    },
    {
        "date": "2025-03",
        "title": "Non-control-Data Attacks and Defenses: A review",
        "author": "Lei Chong",
        "link": "http://arxiv.org/abs/2503.22765v1",
        "abstract": "In recent years, non-control-data attacks have be come a research hotspot in\nthe field of network security, driven\n  by the increasing number of defense methods against control-flow\n  hijacking attacks. These attacks exploit memory vulnerabilities\n  to modify non-control data within a program, thereby altering its\n  behavior without compromising control-flow integrity. Research\n  has shown that non-control-data attacks can be just as damaging\n  as control-flow hijacking attacks and are even Turing complete,\n  making them a serious security threat. However, despite being\n  discovered long ago, the threat of non-control-data attacks has\n  not been adequately addressed. In this review, we first classify\n  non-control-data attacks into two categories based on their\n  evolution: security-sensitive function attacks and data-oriented\n  programming (DOP) attacks. Subsequently, based on the non control-data attack\nmodel, we categorize existing defense methods\n  into three main strategies: memory safety, data confidentiality,\n  and data integrity protection. We then analyze recent defense\n  techniques specifically designed for DOP attacks. Finally, we\n  identify the key challenges hindering the widespread adoption\n  of defenses against non-control-data attacks and explore future\n  research directions in this field."
    },
    {
        "date": "2025-03",
        "title": "Information Theoretic One-Time Programs from Geometrically Local $\\text{QNC}_0$ Adversaries",
        "author": "Lev Stambler",
        "link": "http://arxiv.org/abs/2503.22016v2",
        "abstract": "We show how to construct simulation secure one-time memories, and thus\none-time programs, without computational assumptions in the presence of\nconstraints on quantum hardware. Specifically, we build one-time memories from\nrandom linear codes and quantum random access codes (QRACs) when constrained to\nnon-adaptive, constant depth, and $D$-dimensional geometrically-local quantum\ncircuit for some constant $D$. We place no restrictions on the adversary's\nclassical computational power, number of qubits it can use, or the coherence\ntime of its qubits. Notably, our construction can still be secure even in the\npresence of fault tolerant quantum computation as long as the input qubits are\nencoded in a non-fault tolerant manner (e.g. encoded as high energy states in\nnon-ideal hardware). Unfortunately though, our construction requires decoding\nrandom linear codes and thus does not run in polynomial time. We leave open the\nquestion of whether one can construct a polynomial time information\ntheoretically secure one-time memory from geometrically local quantum circuits.\n  Of potentially independent interest, we develop a progress bound for\ninformation leakage via collision entropy (Renyi entropy of order $2$) along\nwith a few key technical lemmas for a \"mutual information\" for collision\nentropies. We also develop new bounds on how much information a specific $2\n\\mapsto 1$ QRAC can leak about its input, which may be of independent interest\nas well."
    },
    {
        "date": "2025-03",
        "title": "SandboxEval: Towards Securing Test Environment for Untrusted Code",
        "author": "Rafiqul Rabin, Jesse Hostetler, Sean McGregor, Brett Weir, and Nick Judd",
        "link": "http://arxiv.org/abs/2504.00018v1",
        "abstract": "While large language models (LLMs) are powerful assistants in programming\ntasks, they may also produce malicious code. Testing LLM-generated code\ntherefore poses significant risks to assessment infrastructure tasked with\nexecuting untrusted code. To address these risks, this work focuses on\nevaluating the security and confidentiality properties of test environments,\nreducing the risk that LLM-generated code may compromise the assessment\ninfrastructure. We introduce SandboxEval, a test suite featuring manually\ncrafted test cases that simulate real-world safety scenarios for LLM assessment\nenvironments in the context of untrusted code execution. The suite evaluates\nvulnerabilities to sensitive information exposure, filesystem manipulation,\nexternal communication, and other potentially dangerous operations in the\ncourse of assessment activity. We demonstrate the utility of SandboxEval by\ndeploying it on an open-source implementation of Dyff, an established AI\nassessment framework used to evaluate the safety of LLMs at scale. We show,\nfirst, that the test suite accurately describes limitations placed on an LLM\noperating under instructions to generate malicious code. Second, we show that\nthe test results provide valuable insights for developers seeking to harden\nassessment infrastructure and identify risks associated with LLM execution\nactivities."
    },
    {
        "date": "2025-03",
        "title": "OntoAligner: A Comprehensive Modular and Robust Python Toolkit for Ontology Alignment",
        "author": "Hamed Babaei Giglou, Jennifer D'Souza, Oliver Karras, and S\u00f6ren Auer",
        "link": "http://arxiv.org/abs/2503.21902v1",
        "abstract": "Ontology Alignment (OA) is fundamental for achieving semantic\ninteroperability across diverse knowledge systems. We present OntoAligner, a\ncomprehensive, modular, and robust Python toolkit for ontology alignment,\ndesigned to address current limitations with existing tools faced by\npractitioners. Existing tools are limited in scalability, modularity, and ease\nof integration with recent AI advances. OntoAligner provides a flexible\narchitecture integrating existing lightweight OA techniques such as fuzzy\nmatching but goes beyond by supporting contemporary methods with\nretrieval-augmented generation and large language models for OA. The framework\nprioritizes extensibility, enabling researchers to integrate custom alignment\nalgorithms and datasets. This paper details the design principles,\narchitecture, and implementation of the OntoAligner, demonstrating its utility\nthrough benchmarks on standard OA tasks. Our evaluation highlights\nOntoAligner's ability to handle large-scale ontologies efficiently with few\nlines of code while delivering high alignment quality. By making OntoAligner\nopen-source, we aim to provide a resource that fosters innovation and\ncollaboration within the OA community, empowering researchers and practitioners\nwith a toolkit for reproducible OA research and real-world applications."
    },
    {
        "date": "2025-03",
        "title": "Poster Abstract: Time Attacks using Kernel Vulnerabilities",
        "author": "Muhammad Abdullah Soomro, Adeel Nasrullah, and Fatima Muhammad Anwar",
        "link": "http://arxiv.org/abs/2503.21891v1",
        "abstract": "Timekeeping is a fundamental component of modern computing; however, the\nsecurity of system time remains an overlooked attack surface, leaving critical\nsystems vulnerable to manipulation."
    },
    {
        "date": "2025-03",
        "title": "OccRobNet : Occlusion Robust Network for Accurate 3D Interacting Hand-Object Pose Estimation",
        "author": "Mallika Garg, Debashis Ghosh, and Pyari Mohan Pradhan",
        "link": "http://arxiv.org/abs/2503.21723v1",
        "abstract": "Occlusion is one of the challenging issues when estimating 3D hand pose. This\nproblem becomes more prominent when hand interacts with an object or two hands\nare involved. In the past works, much attention has not been given to these\noccluded regions. But these regions contain important and beneficial\ninformation that is vital for 3D hand pose estimation. Thus, in this paper, we\npropose an occlusion robust and accurate method for the estimation of 3D\nhand-object pose from the input RGB image. Our method includes first localising\nthe hand joints using a CNN based model and then refining them by extracting\ncontextual information. The self attention transformer then identifies the\nspecific joints along with the hand identity. This helps the model to identify\nthe hand belongingness of a particular joint which helps to detect the joint\neven in the occluded region. Further, these joints with hand identity are then\nused to estimate the pose using cross attention mechanism. Thus, by identifying\nthe joints in the occluded region, the obtained network becomes robust to\nocclusion. Hence, this network achieves state-of-the-art results when evaluated\non the InterHand2.6M, HO3D and H$_2$O3D datasets."
    },
    {
        "date": "2025-03",
        "title": "AMA-SAM: Adversarial Multi-Domain Alignment of Segment Anything Model for High-Fidelity Histology Nuclei Segmentation",
        "author": "Jiahe Qian, Yaoyu Fang, Jinkui Hao, and Bo Zhou",
        "link": "http://arxiv.org/abs/2503.21695v1",
        "abstract": "Accurate segmentation of cell nuclei in histopathology images is essential\nfor numerous biomedical research and clinical applications. However, existing\ncell nucleus segmentation methods only consider a single dataset (i.e., primary\ndomain), while neglecting to leverage supplementary data from diverse sources\n(i.e., auxiliary domains) to reduce overfitting and enhance the performance.\nAlthough incorporating multiple datasets could alleviate overfitting, it often\nexacerbates performance drops caused by domain shifts. In this work, we\nintroduce Adversarial Multi-domain Alignment of Segment Anything Model\n(AMA-SAM) that extends the Segment Anything Model (SAM) to overcome these\nobstacles through two key innovations. First, we propose a Conditional Gradient\nReversal Layer (CGRL), a multi-domain alignment module that harmonizes features\nfrom diverse domains to promote domain-invariant representation learning while\npreserving crucial discriminative features for the primary dataset. Second, we\naddress SAM's inherent low-resolution output by designing a High-Resolution\nDecoder (HR-Decoder), which directly produces fine-grained segmentation maps in\norder to capture intricate nuclei boundaries in high-resolution histology\nimages. To the best of our knowledge, this is the first attempt to adapt SAM\nfor multi-dataset learning with application to histology nuclei segmentation.\nWe validate our method on several publicly available datasets, demonstrating\nconsistent and significant improvements over state-of-the-art approaches."
    },
    {
        "date": "2025-03",
        "title": "Intelligent IoT Attack Detection Design via ODLLM with Feature Ranking-based Knowledge Base",
        "author": "Satvik Verma, Qun Wang, and E. Wes Bethel",
        "link": "http://arxiv.org/abs/2503.21674v1",
        "abstract": "The widespread adoption of Internet of Things (IoT) devices has introduced\nsignificant cybersecurity challenges, particularly with the increasing\nfrequency and sophistication of Distributed Denial of Service (DDoS) attacks.\nTraditional machine learning (ML) techniques often fall short in detecting such\nattacks due to the complexity of blended and evolving patterns. To address\nthis, we propose a novel framework leveraging On-Device Large Language Models\n(ODLLMs) augmented with fine-tuning and knowledge base (KB) integration for\nintelligent IoT network attack detection. By implementing feature ranking\ntechniques and constructing both long and short KBs tailored to model\ncapacities, the proposed framework ensures efficient and accurate detection of\nDDoS attacks while overcoming computational and privacy limitations. Simulation\nresults demonstrate that the optimized framework achieves superior accuracy\nacross diverse attack types, especially when using compact models in edge\ncomputing environments. This work provides a scalable and secure solution for\nreal-time IoT security, advancing the applicability of edge intelligence in\ncybersecurity."
    },
    {
        "date": "2025-03",
        "title": "Advancing CAN Network Security through RBM-Based Synthetic Attack Data Generation for Intrusion Detection Systems",
        "author": "Huacheng Li, Jingyong Su, and Kai Wang",
        "link": "http://arxiv.org/abs/2503.21496v1",
        "abstract": "The rapid development of network technologies and industrial intelligence has\naugmented the connectivity and intelligence within the automotive industry.\nNotably, in the Internet of Vehicles (IoV), the Controller Area Network (CAN),\nwhich is crucial for the communication of electronic control units but lacks\ninbuilt security measures, has become extremely vulnerable to severe\ncybersecurity threats. Meanwhile, the efficacy of Intrusion Detection Systems\n(IDS) is hampered by the scarcity of sufficient attack data for robust model\ntraining. To overcome this limitation, we introduce a novel methodology\nleveraging the Restricted Boltzmann Machine (RBM) to generate synthetic CAN\nattack data, thereby producing training datasets with a more balanced sample\ndistribution. Specifically, we design a CAN Data Processing Module for\ntransforming raw CAN data into an RBM-trainable format, and a Negative Sample\nGeneration Module to generate data reflecting the distribution of CAN data\nframes denoting network intrusions. Experimental results show the generated\ndata significantly improves IDS performance, with CANet accuracy rising from\n0.6477 to 0.9725 and EfficientNet from 0.1067 to 0.1555. Code is available at\nhttps://github.com/wangkai-tech23/CANDataSynthetic."
    },
    {
        "date": "2025-03",
        "title": "Robust DNN Partitioning and Resource Allocation Under Uncertain Inference Time",
        "author": "Zhaojun Nan, Yunchu Han, Sheng Zhou, and Zhisheng Niu",
        "link": "http://arxiv.org/abs/2503.21476v1",
        "abstract": "In edge intelligence systems, deep neural network (DNN) partitioning and data\noffloading can provide real-time task inference for resource-constrained mobile\ndevices. However, the inference time of DNNs is typically uncertain and cannot\nbe precisely determined in advance, presenting significant challenges in\nensuring timely task processing within deadlines. To address the uncertain\ninference time, we propose a robust optimization scheme to minimize the total\nenergy consumption of mobile devices while meeting task probabilistic\ndeadlines. The scheme only requires the mean and variance information of the\ninference time, without any prediction methods or distribution functions. The\nproblem is formulated as a mixed-integer nonlinear programming (MINLP) that\ninvolves jointly optimizing the DNN model partitioning and the allocation of\nlocal CPU/GPU frequencies and uplink bandwidth. To tackle the problem, we first\ndecompose the original problem into two subproblems: resource allocation and\nDNN model partitioning. Subsequently, the two subproblems with probability\nconstraints are equivalently transformed into deterministic optimization\nproblems using the chance-constrained programming (CCP) method. Finally, the\nconvex optimization technique and the penalty convex-concave procedure (PCCP)\ntechnique are employed to obtain the optimal solution of the resource\nallocation subproblem and a stationary point of the DNN model partitioning\nsubproblem, respectively. The proposed algorithm leverages real-world data from\npopular hardware platforms and is evaluated on widely used DNN models.\nExtensive simulations show that our proposed algorithm effectively addresses\nthe inference time uncertainty with probabilistic deadline guarantees while\nminimizing the energy consumption of mobile devices."
    },
    {
        "date": "2025-03",
        "title": "Harnessing Chain-of-Thought Metadata for Task Routing and Adversarial Prompt Detection",
        "author": "Ryan Marinelli, Josef Pichlmeier, and Tamas Bisztray",
        "link": "http://arxiv.org/abs/2503.21464v1",
        "abstract": "In this work, we propose a metric called Number of Thoughts (NofT) to\ndetermine the difficulty of tasks pre-prompting and support Large Language\nModels (LLMs) in production contexts. By setting thresholds based on the number\nof thoughts, this metric can discern the difficulty of prompts and support more\neffective prompt routing. A 2% decrease in latency is achieved when routing\nprompts from the MathInstruct dataset through quantized, distilled versions of\nDeepseek with 1.7 billion, 7 billion, and 14 billion parameters. Moreover, this\nmetric can be used to detect adversarial prompts used in prompt injection\nattacks with high efficacy. The Number of Thoughts can inform a classifier that\nachieves 95% accuracy in adversarial prompt detection. Our experiments ad\ndatasets used are available on our GitHub page:\nhttps://github.com/rymarinelli/Number_Of_Thoughts/tree/main."
    },
    {
        "date": "2025-03",
        "title": "AdvSGM: Differentially Private Graph Learning via Adversarial Skip-gram Model",
        "author": "Sen Zhang, Qingqing Ye, Haibo Hu, and Jianliang Xu",
        "link": "http://arxiv.org/abs/2503.21426v1",
        "abstract": "The skip-gram model (SGM), which employs a neural network to generate node\nvectors, serves as the basis for numerous popular graph embedding techniques.\nHowever, since the training datasets contain sensitive linkage information, the\nparameters of a released SGM may encode private information and pose\nsignificant privacy risks. Differential privacy (DP) is a rigorous standard for\nprotecting individual privacy in data analysis. Nevertheless, when applying\ndifferential privacy to skip-gram in graphs, it becomes highly challenging due\nto the complex link relationships, which potentially result in high sensitivity\nand necessitate substantial noise injection. To tackle this challenge, we\npresent AdvSGM, a differentially private skip-gram for graphs via adversarial\ntraining. Our core idea is to leverage adversarial training to privatize\nskip-gram while improving its utility. Towards this end, we develop a novel\nadversarial training module by devising two optimizable noise terms that\ncorrespond to the parameters of a skip-gram. By fine-tuning the weights between\nmodules within AdvSGM, we can achieve differentially private gradient updates\nwithout additional noise injection. Extensive experimental results on six\nreal-world graph datasets show that AdvSGM preserves high data utility across\ndifferent downstream tasks."
    },
    {
        "date": "2025-03",
        "title": "Tricking Retrievers with Influential Tokens: An Efficient Black-Box Corpus Poisoning Attack",
        "author": "Cheng Wang, Yiwei Wang, Yujun Cai, and Bryan Hooi",
        "link": "http://arxiv.org/abs/2503.21315v1",
        "abstract": "Retrieval-augmented generation (RAG) systems enhance large language models by\nincorporating external knowledge, addressing issues like outdated internal\nknowledge and hallucination. However, their reliance on external knowledge\nbases makes them vulnerable to corpus poisoning attacks, where adversarial\npassages can be injected to manipulate retrieval results. Existing methods for\ncrafting such passages, such as random token replacement or training inversion\nmodels, are often slow and computationally expensive, requiring either access\nto retriever's gradients or large computational resources. To address these\nlimitations, we propose Dynamic Importance-Guided Genetic Algorithm (DIGA), an\nefficient black-box method that leverages two key properties of retrievers:\ninsensitivity to token order and bias towards influential tokens. By focusing\non these characteristics, DIGA dynamically adjusts its genetic operations to\ngenerate effective adversarial passages with significantly reduced time and\nmemory usage. Our experimental evaluation shows that DIGA achieves superior\nefficiency and scalability compared to existing methods, while maintaining\ncomparable or better attack success rates across multiple datasets."
    },
    {
        "date": "2025-03",
        "title": "DeBackdoor: A Deductive Framework for Detecting Backdoor Attacks on Deep Models with Limited Data",
        "author": "Dorde Popovic, Amin Sadeghi, Ting Yu, Sanjay Chawla, and Issa Khalil",
        "link": "http://arxiv.org/abs/2503.21305v1",
        "abstract": "Backdoor attacks are among the most effective, practical, and stealthy\nattacks in deep learning. In this paper, we consider a practical scenario where\na developer obtains a deep model from a third party and uses it as part of a\nsafety-critical system. The developer wants to inspect the model for potential\nbackdoors prior to system deployment. We find that most existing detection\ntechniques make assumptions that are not applicable to this scenario. In this\npaper, we present a novel framework for detecting backdoors under realistic\nrestrictions. We generate candidate triggers by deductively searching over the\nspace of possible triggers. We construct and optimize a smoothed version of\nAttack Success Rate as our search objective. Starting from a broad class of\ntemplate attacks and just using the forward pass of a deep model, we reverse\nengineer the backdoor attack. We conduct extensive evaluation on a wide range\nof attacks, models, and datasets, with our technique performing almost\nperfectly across these settings."
    },
    {
        "date": "2025-03",
        "title": "OminiAdapt: Learning Cross-Task Invariance for Robust and Environment-Aware Robotic Manipulation",
        "author": "Yongxu Wang, Weiyun Yi, Xinhao Kong, and Wanting Li",
        "link": "http://arxiv.org/abs/2503.21257v1",
        "abstract": "With the rapid development of embodied intelligence, leveraging large-scale\nhuman data for high-level imitation learning on humanoid robots has become a\nfocal point of interest in both academia and industry. However, applying\nhumanoid robots to precision operation domains remains challenging due to the\ncomplexities they face in perception and control processes, the long-standing\nphysical differences in morphology and actuation mechanisms between humanoid\nrobots and humans, and the lack of task-relevant features obtained from\negocentric vision. To address the issue of covariate shift in imitation\nlearning, this paper proposes an imitation learning algorithm tailored for\nhumanoid robots. By focusing on the primary task objectives, filtering out\nbackground information, and incorporating channel feature fusion with spatial\nattention mechanisms, the proposed algorithm suppresses environmental\ndisturbances and utilizes a dynamic weight update strategy to significantly\nimprove the success rate of humanoid robots in accomplishing target tasks.\nExperimental results demonstrate that the proposed method exhibits robustness\nand scalability across various typical task scenarios, providing new ideas and\napproaches for autonomous learning and control in humanoid robots. The project\nwill be open-sourced on GitHub."
    },
    {
        "date": "2025-03",
        "title": "Clean Image May be Dangerous: Data Poisoning Attacks Against Deep Hashing",
        "author": "Shuai Li, Jie Zhang, Yuang Qi, Kejiang Chen, Tianwei Zhang, Weiming Zhang, and Nenghai Yu",
        "link": "http://arxiv.org/abs/2503.21236v1",
        "abstract": "Large-scale image retrieval using deep hashing has become increasingly\npopular due to the exponential growth of image data and the remarkable feature\nextraction capabilities of deep neural networks (DNNs). However, deep hashing\nmethods are vulnerable to malicious attacks, including adversarial and backdoor\nattacks. It is worth noting that these attacks typically involve altering the\nquery images, which is not a practical concern in real-world scenarios. In this\npaper, we point out that even clean query images can be dangerous, inducing\nmalicious target retrieval results, like undesired or illegal images. To the\nbest of our knowledge, we are the first to study data \\textbf{p}oisoning\n\\textbf{a}ttacks against \\textbf{d}eep \\textbf{hash}ing\n\\textbf{(\\textit{PADHASH})}. Specifically, we first train a surrogate model to\nsimulate the behavior of the target deep hashing model. Then, a strict gradient\nmatching strategy is proposed to generate the poisoned images. Extensive\nexperiments on different models, datasets, hash methods, and hash code lengths\ndemonstrate the effectiveness and generality of our attack method."
    },
    {
        "date": "2025-03",
        "title": "Adversarial Wear and Tear: Exploiting Natural Damage for Generating Physical-World Adversarial Examples",
        "author": "Samra Irshad, Seungkyu Lee, Nassir Navab, Hong Joo Lee, and Seong Tae Kim",
        "link": "http://arxiv.org/abs/2503.21164v1",
        "abstract": "The presence of adversarial examples in the physical world poses significant\nchallenges to the deployment of Deep Neural Networks in safety-critical\napplications such as autonomous driving. Most existing methods for crafting\nphysical-world adversarial examples are ad-hoc, relying on temporary\nmodifications like shadows, laser beams, or stickers that are tailored to\nspecific scenarios. In this paper, we introduce a new class of physical-world\nadversarial examples, AdvWT, which draws inspiration from the naturally\noccurring phenomenon of `wear and tear', an inherent property of physical\nobjects. Unlike manually crafted perturbations, `wear and tear' emerges\norganically over time due to environmental degradation, as seen in the gradual\ndeterioration of outdoor signboards. To achieve this, AdvWT follows a two-step\napproach. First, a GAN-based, unsupervised image-to-image translation network\nis employed to model these naturally occurring damages, particularly in the\ncontext of outdoor signboards. The translation network encodes the\ncharacteristics of damaged signs into a latent `damage style code'. In the\nsecond step, we introduce adversarial perturbations into the style code,\nstrategically optimizing its transformation process. This manipulation subtly\nalters the damage style representation, guiding the network to generate\nadversarial images where the appearance of damages remains perceptually\nrealistic, while simultaneously ensuring their effectiveness in misleading\nneural networks. Through comprehensive experiments on two traffic sign\ndatasets, we show that AdvWT effectively misleads DNNs in both digital and\nphysical domains. AdvWT achieves an effective attack success rate, greater\nrobustness, and a more natural appearance compared to existing physical-world\nadversarial examples. Additionally, integrating AdvWT into training enhances a\nmodel's generalizability to real-world damaged signs."
    },
    {
        "date": "2025-03",
        "title": "How to Secure Existing C and C++ Software without Memory Safety",
        "author": "\u00dalfar Erlingsson",
        "link": "http://arxiv.org/abs/2503.21145v1",
        "abstract": "The most important security benefit of software memory safety is easy to\nstate: for C and C++ software, attackers can exploit most bugs and\nvulnerabilities to gain full, unfettered control of software behavior, whereas\nthis is not true for most bugs in memory-safe software.\n  Fortunately, this security benefit -- most bugs don't give attackers full\ncontrol -- can be had for unmodified C/C++ software, without per-application\neffort.\n  This doesn't require trying to establish memory safety; instead, it is\nsufficient to eliminate most of the combinatorial ways in which software with\ncorrupted memory can execute. To eliminate these interleavings, there already\nexist practical compiler and runtime mechanisms that incur little overhead and\nneed no special hardware or platform support.\n  Each of the mechanisms described here is already in production use, at scale,\non one or more platforms. By supporting their combined use in development\ntoolchains, the security of all C and C++ software against remote code\nexecution attacks can be rapidly, and dramatically, improved."
    },
    {
        "date": "2025-03",
        "title": "Generator Cost Coefficients Inference Attack via Exploitation of Locational Marginal Prices in Smart Grid",
        "author": "Junfei Wang, and Pirathayini Srikantha",
        "link": "http://arxiv.org/abs/2503.20976v1",
        "abstract": "Real-time price signals and power generation levels (disaggregated or\naggregated) are commonly made available to the public by Independent System\nOperators (ISOs) to promote efficiency and transparency. However, they may\ninadvertently reveal crucial private information about the power grid, such as\nthe cost functions of generators. Adversaries can exploit these vulnerabilities\nfor strategic bidding, potentially leading to financial losses for power market\nparticipants and consumers. In this paper, we prove the existence of a\nclosed-form solution for recovering coefficients in cost functions when LMPs\nand disaggregated power generation data are available. Additionally, we\nestablish the convergence conditions for inference the quadratic coefficients\nof cost functions when LMPs and aggregated generation data are given. Our\ntheoretical analysis provides the conditions under which the algorithm is\nguaranteed to converge, and our experiments demonstrate the efficacy of this\nmethod on IEEE benchmark systems, including 14-bus and 30-bus and 118-bus\nsystems."
    },
    {
        "date": "2025-03",
        "title": "TS-Inverse: A Gradient Inversion Attack Tailored for Federated Time Series Forecasting Models",
        "author": "Caspar Meijer, Jiyue Huang, Shreshtha Sharma, Elena Lazovik, and Lydia Y. Chen",
        "link": "http://arxiv.org/abs/2503.20952v1",
        "abstract": "Federated learning (FL) for time series forecasting (TSF) enables clients\nwith privacy-sensitive time series (TS) data to collaboratively learn accurate\nforecasting models, for example, in energy load prediction. Unfortunately,\nprivacy risks in FL persist, as servers can potentially reconstruct clients'\ntraining data through gradient inversion attacks (GIA). Although GIA is\ndemonstrated for image classification tasks, little is known about time series\nregression tasks. In this paper, we first conduct an extensive empirical study\non inverting TS data across 4 TSF models and 4 datasets, identifying the unique\nchallenges of reconstructing both observations and targets of TS data. We then\npropose TS-Inverse, a novel GIA that improves the inversion of TS data by (i)\nlearning a gradient inversion model that outputs quantile predictions, (ii) a\nunique loss function that incorporates periodicity and trend regularization,\nand (iii) regularization according to the quantile predictions. Our evaluations\ndemonstrate a remarkable performance of TS-Inverse, achieving at least a 2x-10x\nimprovement in terms of the sMAPE metric over existing GIA methods on TS data.\nCode repository: https://github.com/Capsar/ts-inverse"
    },
    {
        "date": "2025-03",
        "title": "Prototype Guided Backdoor Defense",
        "author": "Venkat Adithya Amula, Sunayana Samavedam, Saurabh Saini, Avani Gupta, and Narayanan P J",
        "link": "http://arxiv.org/abs/2503.20925v1",
        "abstract": "Deep learning models are susceptible to {\\em backdoor attacks} involving\nmalicious attackers perturbing a small subset of training data with a {\\em\ntrigger} to causes misclassifications. Various triggers have been used,\nincluding semantic triggers that are easily realizable without requiring the\nattacker to manipulate the image. The emergence of generative AI has eased the\ngeneration of varied poisoned samples. Robustness across types of triggers is\ncrucial to effective defense. We propose Prototype Guided Backdoor Defense\n(PGBD), a robust post-hoc defense that scales across different trigger types,\nincluding previously unsolved semantic triggers. PGBD exploits displacements in\nthe geometric spaces of activations to penalize movements toward the trigger.\nThis is done using a novel sanitization loss of a post-hoc fine-tuning step.\nThe geometric approach scales easily to all types of attacks. PGBD achieves\nbetter performance across all settings. We also present the first defense\nagainst a new semantic attack on celebrity face images. Project page:\n\\hyperlink{https://venkatadithya9.github.io/pgbd.github.io/}{this https URL}."
    },
    {
        "date": "2025-03",
        "title": "Robust Federated Learning Against Poisoning Attacks: A GAN-Based Defense Framework",
        "author": "Usama Zafar, Andr\u00e9 Teixeira, and Salman Toor",
        "link": "http://arxiv.org/abs/2503.20884v1",
        "abstract": "Federated Learning (FL) enables collaborative model training across\ndecentralized devices without sharing raw data, but it remains vulnerable to\npoisoning attacks that compromise model integrity. Existing defenses often rely\non external datasets or predefined heuristics (e.g. number of malicious\nclients), limiting their effectiveness and scalability. To address these\nlimitations, we propose a privacy-preserving defense framework that leverages a\nConditional Generative Adversarial Network (cGAN) to generate synthetic data at\nthe server for authenticating client updates, eliminating the need for external\ndatasets. Our framework is scalable, adaptive, and seamlessly integrates into\nFL workflows. Extensive experiments on benchmark datasets demonstrate its\nrobust performance against a variety of poisoning attacks, achieving high True\nPositive Rate (TPR) and True Negative Rate (TNR) of malicious and benign\nclients, respectively, while maintaining model accuracy. The proposed framework\noffers a practical and effective solution for securing federated learning\nsystems."
    },
    {
        "date": "2025-03",
        "title": "DR-PETS: Learning-Based Control With Planning in Adversarial Environments",
        "author": "Hozefa Jesawada, Antonio Acernese, Giovanni Russo, and Carmen Del Vecchio",
        "link": "http://arxiv.org/abs/2503.20660v2",
        "abstract": "Ensuring robustness against epistemic, possibly adversarial, perturbations is\nessential for reliable real-world decision-making. While the Probabilistic\nEnsembles with Trajectory Sampling (PETS) algorithm inherently handles\nuncertainty via ensemble-based probabilistic models, it lacks guarantees\nagainst structured adversarial or worst-case uncertainty distributions. To\naddress this, we propose DR-PETS, a distributionally robust extension of PETS\nthat certifies robustness against adversarial perturbations. We formalize\nuncertainty via a p-Wasserstein ambiguity set, enabling worst-case-aware\nplanning through a min-max optimization framework. While PETS passively\naccounts for stochasticity, DR-PETS actively optimizes robustness via a\ntractable convex approximation integrated into PETS planning loop. Experiments\non pendulum stabilization and cart-pole balancing show that DR-PETS certifies\nrobustness against adversarial parameter perturbations, achieving consistent\nperformance in worst-case scenarios where PETS deteriorates."
    },
    {
        "date": "2025-03",
        "title": "Robust Flower Cluster Matching Using The Unscented Transform",
        "author": "Andy Chu, Rashik Shrestha, Yu Gu, and Jason N. Gross",
        "link": "http://arxiv.org/abs/2503.20631v1",
        "abstract": "Monitoring flowers over time is essential for precision robotic pollination\nin agriculture. To accomplish this, a continuous spatial-temporal observation\nof plant growth can be done using stationary RGB-D cameras. However, image\nregistration becomes a serious challenge due to changes in the visual\nappearance of the plant caused by the pollination process and occlusions from\ngrowth and camera angles. Plants flower in a manner that produces distinct\nclusters on branches. This paper presents a method for matching flower clusters\nusing descriptors generated from RGB-D data and considers allowing for spatial\nuncertainty within the cluster. The proposed approach leverages the Unscented\nTransform to efficiently estimate plant descriptor uncertainty tolerances,\nenabling a robust image-registration process despite temporal changes. The\nUnscented Transform is used to handle the nonlinear transformations by\npropagating the uncertainty of flower positions to determine the variations in\nthe descriptor domain. A Monte Carlo simulation is used to validate the\nUnscented Transform results, confirming our method's effectiveness for flower\ncluster matching. Therefore, it can facilitate improved robotics pollination in\ndynamic environments."
    },
    {
        "date": "2025-03",
        "title": "$\u03b2$-GNN: A Robust Ensemble Approach Against Graph Structure Perturbation",
        "author": "Haci Ismail Aslan, Philipp Wiesner, Ping Xiong, and Odej Kao",
        "link": "http://arxiv.org/abs/2503.20630v1",
        "abstract": "Graph Neural Networks (GNNs) are playing an increasingly important role in\nthe efficient operation and security of computing systems, with applications in\nworkload scheduling, anomaly detection, and resource management. However, their\nvulnerability to network perturbations poses a significant challenge. We\npropose $\\beta$-GNN, a model enhancing GNN robustness without sacrificing clean\ndata performance. $\\beta$-GNN uses a weighted ensemble, combining any GNN with\na multi-layer perceptron. A learned dynamic weight, $\\beta$, modulates the\nGNN's contribution. This $\\beta$ not only weights GNN influence but also\nindicates data perturbation levels, enabling proactive mitigation. Experimental\nresults on diverse datasets show $\\beta$-GNN's superior adversarial accuracy\nand attack severity quantification. Crucially, $\\beta$-GNN avoids perturbation\nassumptions, preserving clean data structure and performance."
    },
    {
        "date": "2025-03",
        "title": "Robust Deep Reinforcement Learning in Robotics via Adaptive Gradient-Masked Adversarial Attacks",
        "author": "Zongyuan Zhang, Tianyang Duan, Zheng Lin, Dong Huang, Zihan Fang, Zekai Sun, Ling Xiong, Hongbin Liang, Heming Cui, Yong Cui, and Yue Gao",
        "link": "http://arxiv.org/abs/2503.20844v1",
        "abstract": "Deep reinforcement learning (DRL) has emerged as a promising approach for\nrobotic control, but its realworld deployment remains challenging due to its\nvulnerability to environmental perturbations. Existing white-box adversarial\nattack methods, adapted from supervised learning, fail to effectively target\nDRL agents as they overlook temporal dynamics and indiscriminately perturb all\nstate dimensions, limiting their impact on long-term rewards. To address these\nchallenges, we propose the Adaptive Gradient-Masked Reinforcement (AGMR)\nAttack, a white-box attack method that combines DRL with a gradient-based soft\nmasking mechanism to dynamically identify critical state dimensions and\noptimize adversarial policies. AGMR selectively allocates perturbations to the\nmost impactful state features and incorporates a dynamic adjustment mechanism\nto balance exploration and exploitation during training. Extensive experiments\ndemonstrate that AGMR outperforms state-of-the-art adversarial attack methods\nin degrading the performance of the victim agent and enhances the victim\nagent's robustness through adversarial defense mechanisms."
    },
    {
        "date": "2025-03",
        "title": "State-Aware Perturbation Optimization for Robust Deep Reinforcement Learning",
        "author": "Zongyuan Zhang, Tianyang Duan, Zheng Lin, Dong Huang, Zihan Fang, Zekai Sun, Ling Xiong, Hongbin Liang, Heming Cui, and Yong Cui",
        "link": "http://arxiv.org/abs/2503.20613v1",
        "abstract": "Recently, deep reinforcement learning (DRL) has emerged as a promising\napproach for robotic control. However, the deployment of DRL in real-world\nrobots is hindered by its sensitivity to environmental perturbations. While\nexisting whitebox adversarial attacks rely on local gradient information and\napply uniform perturbations across all states to evaluate DRL robustness, they\nfail to account for temporal dynamics and state-specific vulnerabilities. To\ncombat the above challenge, we first conduct a theoretical analysis of\nwhite-box attacks in DRL by establishing the adversarial victim-dynamics Markov\ndecision process (AVD-MDP), to derive the necessary and sufficient conditions\nfor a successful attack. Based on this, we propose a selective state-aware\nreinforcement adversarial attack method, named STAR, to optimize perturbation\nstealthiness and state visitation dispersion. STAR first employs a soft\nmask-based state-targeting mechanism to minimize redundant perturbations,\nenhancing stealthiness and attack effectiveness. Then, it incorporates an\ninformation-theoretic optimization objective to maximize mutual information\nbetween perturbations, environmental states, and victim actions, ensuring a\ndispersed state-visitation distribution that steers the victim agent into\nvulnerable states for maximum return reduction. Extensive experiments\ndemonstrate that STAR outperforms state-of-the-art benchmarks."
    },
    {
        "date": "2025-03",
        "title": "Feature Statistics with Uncertainty Help Adversarial Robustness",
        "author": "Ran Wang, Xinlei Zhou, Rihao Li, Meng Hu, Wenhui Wu, and Yuheng Jia",
        "link": "http://arxiv.org/abs/2503.20583v1",
        "abstract": "Despite the remarkable success of deep neural networks (DNNs), the security\nthreat of adversarial attacks poses a significant challenge to the reliability\nof DNNs. By introducing randomness into different parts of DNNs, stochastic\nmethods can enable the model to learn some uncertainty, thereby improving model\nrobustness efficiently. In this paper, we theoretically discover a universal\nphenomenon that adversarial attacks will shift the distributions of feature\nstatistics. Motivated by this theoretical finding, we propose a robustness\nenhancement module called Feature Statistics with Uncertainty (FSU). It\nresamples channel-wise feature means and standard deviations of examples from\nmultivariate Gaussian distributions, which helps to reconstruct the attacked\nexamples and calibrate the shifted distributions. The calibration recovers some\ndomain characteristics of the data for classification, thereby mitigating the\ninfluence of perturbations and weakening the ability of attacks to deceive\nmodels. The proposed FSU module has universal applicability in training,\nattacking, predicting and fine-tuning, demonstrating impressive robustness\nenhancement ability at trivial additional time cost. For example, against\npowerful optimization-based CW attacks, by incorporating FSU into attacking and\npredicting phases, it endows many collapsed state-of-the-art models with\n50%-80% robust accuracy on CIFAR10, CIFAR100 and SVHN."
    },
    {
        "date": "2025-03",
        "title": "Exploring Robustness of Cortical Morphometry in the presence of white matter lesions, using Diffusion Models for Lesion Filling",
        "author": "Vinzenz Uhr, Ivan Diaz, Christian Rummel, and Richard McKinley",
        "link": "http://arxiv.org/abs/2503.20571v1",
        "abstract": "Cortical thickness measurements from magnetic resonance imaging, an important\nbiomarker in many neurodegenerative and neurological disorders, are derived by\nmany tools from an initial voxel-wise tissue segmentation. White matter (WM)\nhypointensities in T1-weighted imaging, such as those arising from multiple\nsclerosis or small vessel disease, are known to affect the output of brain\nsegmentation methods and therefore bias cortical thickness measurements. These\neffects are well-documented among traditional brain segmentation tools but have\nnot been studied extensively in tools based on deep-learning segmentations,\nwhich promise to be more robust. In this paper, we explore the potential of\ndeep learning to enhance the accuracy and efficiency of cortical thickness\nmeasurement in the presence of WM lesions, using a high-quality lesion filling\nalgorithm leveraging denoising diffusion networks.\n  A pseudo-3D U-Net architecture trained on the OASIS dataset to generate\nsynthetic healthy tissue, conditioned on binary lesion masks derived from the\nMSSEG dataset, allows realistic removal of white matter lesions in multiple\nsclerosis patients. By applying morphometry methods to patient images before\nand after lesion filling, we analysed robustness of global and regional\ncortical thickness measurements in the presence of white matter lesions.\nMethods based on a deep learning-based segmentation of the brain (Fastsurfer,\nDL+DiReCT, ANTsPyNet) exhibited greater robustness than those using classical\nsegmentation methods (Freesurfer, ANTs)."
    },
    {
        "date": "2025-03",
        "title": "Lipschitz Constant Meets Condition Number: Learning Robust and Compact Deep Neural Networks",
        "author": "Yangqi Feng, Shing-Ho J. Lin, Baoyuan Gao, and Xian Wei",
        "link": "http://arxiv.org/abs/2503.20454v1",
        "abstract": "Recent research has revealed that high compression of Deep Neural Networks\n(DNNs), e.g., massive pruning of the weight matrix of a DNN, leads to a severe\ndrop in accuracy and susceptibility to adversarial attacks. Integration of\nnetwork pruning into an adversarial training framework has been proposed to\npromote adversarial robustness. It has been observed that a highly pruned\nweight matrix tends to be ill-conditioned, i.e., increasing the condition\nnumber of the weight matrix. This phenomenon aggravates the vulnerability of a\nDNN to input noise. Although a highly pruned weight matrix is considered to be\nable to lower the upper bound of the local Lipschitz constant to tolerate large\ndistortion, the ill-conditionedness of such a weight matrix results in a\nnon-robust DNN model. To overcome this challenge, this work develops novel\njoint constraints to adjust the weight distribution of networks, namely, the\nTransformed Sparse Constraint joint with Condition Number Constraint (TSCNC),\nwhich copes with smoothing distribution and differentiable constraint functions\nto reduce condition number and thus avoid the ill-conditionedness of weight\nmatrices. Furthermore, our theoretical analyses unveil the relevance between\nthe condition number and the local Lipschitz constant of the weight matrix,\nnamely, the sharply increasing condition number becomes the dominant factor\nthat restricts the robustness of over-sparsified models. Extensive experiments\nare conducted on several public datasets, and the results show that the\nproposed constraints significantly improve the robustness of a DNN with high\npruning rates."
    },
    {
        "date": "2025-03",
        "title": "Learning Data-Driven Uncertainty Set Partitions for Robust and Adaptive Energy Forecasting with Missing Data",
        "author": "Akylas Stratigakos, and Panagiotis Andrianesis",
        "link": "http://arxiv.org/abs/2503.20410v1",
        "abstract": "Short-term forecasting models typically assume the availability of input data\n(features) when they are deployed and in use. However, equipment failures,\ndisruptions, cyberattacks, may lead to missing features when such models are\nused operationally, which could negatively affect forecast accuracy, and result\nin suboptimal operational decisions. In this paper, we use adaptive robust\noptimization and adversarial machine learning to develop forecasting models\nthat seamlessly handle missing data operationally. We propose linear- and\nneural network-based forecasting models with parameters that adapt to available\nfeatures, combining linear adaptation with a novel algorithm for learning\ndata-driven uncertainty set partitions. The proposed adaptive models do not\nrely on identifying historical missing data patterns and are suitable for\nreal-time operations under stringent time constraints. Extensive numerical\nexperiments on short-term wind power forecasting considering horizons from 15\nminutes to 4 hours ahead illustrate that our proposed adaptive models are on\npar with imputation when data are missing for very short periods (e.g., when\nonly the latest measurement is missing) whereas they significantly outperform\nimputation when data are missing for longer periods. We further provide\ninsights by showcasing how linear adaptation and data-driven partitions (even\nwith a few subsets) approach the performance of the optimal, yet impractical,\nmethod of retraining for every possible realization of missing data."
    },
    {
        "date": "2025-03",
        "title": "I'm Sorry Dave: How the old world of personnel security can inform the new world of AI insider risk",
        "author": "Paul Martin, and Sarah Mercer",
        "link": "http://arxiv.org/abs/2504.00012v1",
        "abstract": "Organisations are rapidly adopting artificial intelligence (AI) tools to\nperform tasks previously undertaken by people. The potential benefits are\nenormous. Separately, some organisations deploy personnel security measures to\nmitigate the security risks arising from trusted human insiders. Unfortunately,\nthere is no meaningful interplay between the rapidly evolving domain of AI and\nthe traditional world of personnel security. This is a problem. The complex\nrisks from human insiders are hard enough to understand and manage, despite\nmany decades of effort. The emerging security risks from AI insiders are even\nmore opaque. Both sides need all the help they can get. Some of the concepts\nand approaches that have proved useful in dealing with human insiders are also\napplicable to the emerging risks from AI insiders. Furthermore, AI can be used\ndefensively to protect against both human and AI insiders."
    },
    {
        "date": "2025-03",
        "title": "Wasserstein Distributionally Robust Bayesian Optimization with Continuous Context",
        "author": "Francesco Micheli, Efe C. Balta, Anastasios Tsiamis, and John Lygeros",
        "link": "http://arxiv.org/abs/2503.20341v1",
        "abstract": "We address the challenge of sequential data-driven decision-making under\ncontext distributional uncertainty. This problem arises in numerous real-world\nscenarios where the learner optimizes black-box objective functions in the\npresence of uncontrollable contextual variables. We consider the setting where\nthe context distribution is uncertain but known to lie within an ambiguity set\ndefined as a ball in the Wasserstein distance. We propose a novel algorithm for\nWasserstein Distributionally Robust Bayesian Optimization that can handle\ncontinuous context distributions while maintaining computational tractability.\nOur theoretical analysis combines recent results in self-normalized\nconcentration in Hilbert spaces and finite-sample bounds for distributionally\nrobust optimization to establish sublinear regret bounds that match\nstate-of-the-art results. Through extensive comparisons with existing\napproaches on both synthetic and real-world problems, we demonstrate the\nsimplicity, effectiveness, and practical applicability of our proposed method."
    },
    {
        "date": "2025-03",
        "title": "Ancestral Mamba: Enhancing Selective Discriminant Space Model with Online Visual Prototype Learning for Efficient and Robust Discriminant Approach",
        "author": "Jiahao Qin, Feng Liu, and Lu Zong",
        "link": "http://arxiv.org/abs/2503.22729v1",
        "abstract": "In the realm of computer graphics, the ability to learn continuously from\nnon-stationary data streams while adapting to new visual patterns and\nmitigating catastrophic forgetting is of paramount importance. Existing\napproaches often struggle to capture and represent the essential\ncharacteristics of evolving visual concepts, hindering their applicability to\ndynamic graphics tasks. In this paper, we propose Ancestral Mamba, a novel\napproach that integrates online prototype learning into a selective\ndiscriminant space model for efficient and robust online continual learning.\nThe key components of our approach include Ancestral Prototype Adaptation\n(APA), which continuously refines and builds upon learned visual prototypes,\nand Mamba Feedback (MF), which provides targeted feedback to adapt to\nchallenging visual patterns. APA enables the model to continuously adapt its\nprototypes, building upon ancestral knowledge to tackle new challenges, while\nMF acts as a targeted feedback mechanism, focusing on challenging classes and\nrefining their representations. Extensive experiments on graphics-oriented\ndatasets, such as CIFAR-10 and CIFAR-100, demonstrate the superior performance\nof Ancestral Mamba compared to state-of-the-art baselines, achieving\nsignificant improvements in accuracy and forgetting mitigation."
    },
    {
        "date": "2025-03",
        "title": "Enabling Heterogeneous Adversarial Transferability via Feature Permutation Attacks",
        "author": "Tao Wu, and Tie Luo",
        "link": "http://arxiv.org/abs/2503.20310v1",
        "abstract": "Adversarial attacks in black-box settings are highly practical, with\ntransfer-based attacks being the most effective at generating adversarial\nexamples (AEs) that transfer from surrogate models to unseen target models.\nHowever, their performance significantly degrades when transferring across\nheterogeneous architectures -- such as CNNs, MLPs, and Vision Transformers\n(ViTs) -- due to fundamental architectural differences. To address this, we\npropose Feature Permutation Attack (FPA), a zero-FLOP, parameter-free method\nthat enhances adversarial transferability across diverse architectures. FPA\nintroduces a novel feature permutation (FP) operation, which rearranges pixel\nvalues in selected feature maps to simulate long-range dependencies,\neffectively making CNNs behave more like ViTs and MLPs. This enhances feature\ndiversity and improves transferability both across heterogeneous architectures\nand within homogeneous CNNs. Extensive evaluations on 14 state-of-the-art\narchitectures show that FPA achieves maximum absolute gains in attack success\nrates of 7.68% on CNNs, 14.57% on ViTs, and 14.48% on MLPs, outperforming\nexisting black-box attacks. Additionally, FPA is highly generalizable and can\nseamlessly integrate with other transfer-based attacks to further boost their\nperformance. Our findings establish FPA as a robust, efficient, and\ncomputationally lightweight strategy for enhancing adversarial transferability\nacross heterogeneous architectures."
    },
    {
        "date": "2025-03",
        "title": "Model-Based Offline Reinforcement Learning with Adversarial Data Augmentation",
        "author": "Hongye Cao, Fan Feng, Jing Huo, Shangdong Yang, Meng Fang, Tianpei Yang, and Yang Gao",
        "link": "http://arxiv.org/abs/2503.20285v1",
        "abstract": "Model-based offline Reinforcement Learning (RL) constructs environment models\nfrom offline datasets to perform conservative policy optimization. Existing\napproaches focus on learning state transitions through ensemble models,\nrollouting conservative estimation to mitigate extrapolation errors. However,\nthe static data makes it challenging to develop a robust policy, and offline\nagents cannot access the environment to gather new data. To address these\nchallenges, we introduce Model-based Offline Reinforcement learning with\nAdversariaL data augmentation (MORAL). In MORAL, we replace the fixed horizon\nrollout by employing adversaria data augmentation to execute alternating\nsampling with ensemble models to enrich training data. Specifically, this\nadversarial process dynamically selects ensemble models against policy for\nbiased sampling, mitigating the optimistic estimation of fixed models, thus\nrobustly expanding the training data for policy optimization. Moreover, a\ndifferential factor is integrated into the adversarial process for\nregularization, ensuring error minimization in extrapolations. This\ndata-augmented optimization adapts to diverse offline tasks without rollout\nhorizon tuning, showing remarkable applicability. Extensive experiments on D4RL\nbenchmark demonstrate that MORAL outperforms other model-based offline RL\nmethods in terms of policy learning and sample efficiency."
    },
    {
        "date": "2025-03",
        "title": "How Secure is Forgetting? Linking Machine Unlearning to Machine Learning Attacks",
        "author": "Muhammed Shafi K. P., Serena Nicolazzo, Antonino Nocera, and Vinod P",
        "link": "http://arxiv.org/abs/2503.20257v1",
        "abstract": "As Machine Learning (ML) evolves, the complexity and sophistication of\nsecurity threats against this paradigm continue to grow as well, threatening\ndata privacy and model integrity. In response, Machine Unlearning (MU) is a\nrecent technology that aims to remove the influence of specific data from a\ntrained model, enabling compliance with privacy regulations and user requests.\nThis can be done for privacy compliance (e.g., GDPR's right to be forgotten) or\nmodel refinement. However, the intersection between classical threats in ML and\nMU remains largely unexplored. In this Systematization of Knowledge (SoK), we\nprovide a structured analysis of security threats in ML and their implications\nfor MU. We analyze four major attack classes, namely, Backdoor Attacks,\nMembership Inference Attacks (MIA), Adversarial Attacks, and Inversion Attacks,\nwe investigate their impact on MU and propose a novel classification based on\nhow they are usually used in this context. Finally, we identify open\nchallenges, including ethical considerations, and explore promising future\nresearch directions, paving the way for future research in secure and\nprivacy-preserving Machine Unlearning."
    },
    {
        "date": "2025-03",
        "title": "Synthetic-to-Real Self-supervised Robust Depth Estimation via Learning with Motion and Structure Priors",
        "author": "Weilong Yan, Ming Li, Haipeng Li, Shuwei Shao, and Robby T. Tan",
        "link": "http://arxiv.org/abs/2503.20211v1",
        "abstract": "Self-supervised depth estimation from monocular cameras in diverse outdoor\nconditions, such as daytime, rain, and nighttime, is challenging due to the\ndifficulty of learning universal representations and the severe lack of labeled\nreal-world adverse data. Previous methods either rely on synthetic inputs and\npseudo-depth labels or directly apply daytime strategies to adverse conditions,\nresulting in suboptimal results. In this paper, we present the first\nsynthetic-to-real robust depth estimation framework, incorporating motion and\nstructure priors to capture real-world knowledge effectively. In the synthetic\nadaptation, we transfer motion-structure knowledge inside cost volumes for\nbetter robust representation, using a frozen daytime model to train a depth\nestimator in synthetic adverse conditions. In the innovative real adaptation,\nwhich targets to fix synthetic-real gaps, models trained earlier identify the\nweather-insensitive regions with a designed consistency-reweighting strategy to\nemphasize valid pseudo-labels. We introduce a new regularization by gathering\nexplicit depth distributions to constrain the model when facing real-world\ndata. Experiments show that our method outperforms the state-of-the-art across\ndiverse conditions in multi-frame and single-frame evaluations. We achieve\nimprovements of 7.5% and 4.3% in AbsRel and RMSE on average for nuScenes and\nRobotcar datasets (daytime, nighttime, rain). In zero-shot evaluation of\nDrivingStereo (rain, fog), our method generalizes better than the previous\nones."
    },
    {
        "date": "2025-03",
        "title": "On the Robustness of Kernel Ridge Regression Using the Cauchy Loss Function",
        "author": "Hongwei Wen, Annika Betken, and Wouter Koolen",
        "link": "http://arxiv.org/abs/2503.20120v1",
        "abstract": "Robust regression aims to develop methods for estimating an unknown\nregression function in the presence of outliers, heavy-tailed distributions, or\ncontaminated data, which can severely impact performance. Most existing\ntheoretical results in robust regression assume that the noise has a finite\nabsolute mean, an assumption violated by certain distributions, such as Cauchy\nand some Pareto noise. In this paper, we introduce a generalized Cauchy noise\nframework that accommodates all noise distributions with finite moments of any\norder, even when the absolute mean is infinite. Within this framework, we study\nthe \\textit{kernel Cauchy ridge regressor} (\\textit{KCRR}), which minimizes a\nregularized empirical Cauchy risk to achieve robustness. To derive the\n$L_2$-risk bound for KCRR, we establish a connection between the excess Cauchy\nrisk and $L_2$-risk for sufficiently large scale parameters of the Cauchy loss,\nwhich reveals that these two risks are equivalent. Furthermore, under the\nassumption that the regression function satisfies H\\\"older smoothness, we\nderive excess Cauchy risk bounds for KCRR, showing improved performance as the\nscale parameter decreases. By considering the twofold effect of the scale\nparameter on the excess Cauchy risk and its equivalence with the $L_2$-risk, we\nestablish the almost minimax-optimal convergence rate for KCRR in terms of\n$L_2$-risk, highlighting the robustness of the Cauchy loss in handling various\ntypes of noise. Finally, we validate the effectiveness of KCRR through\nexperiments on both synthetic and real-world datasets under diverse noise\ncorruption scenarios."
    },
    {
        "date": "2025-03",
        "title": "ARGO-SLSA: Software Supply Chain Security in Argo Workflows",
        "author": "Mohomed Thariq, and Indrajith Ekanayake",
        "link": "http://arxiv.org/abs/2503.20079v1",
        "abstract": "Distributed systems widely adopt microservice architecture to handle growing\ncomplexity and scale. This approach breaks applications into independent,\nloosely coupled services. Kubernetes has become the de facto standard for\nmanaging microservices, and automating complex, multi-step workflows is a\ncommon requirement in Kubernetes. Argo Workflows is a Kubernetes-native engine\nfor managing these workflows in an automated fashion. These workflows generate\nartifacts such as executables, logs, container images, and packages, which\noften require proper management through software supply chain security.\nHowever, Argo Workflows does not include built-in functionality for frameworks\nlike Supply-chain Levels for Software Artifacts (SLSA), which is essential for\nensuring artifact integrity, traceability, and security. This gap compels\npractitioners to rely on external tools to meet software supply chain security\nstandards. In response, this paper proposes a Kubernetes-native controller\nbuilt on top of existing open-source Argo Workflows to enhance artifact\nsecurity. By generating cryptographic signing and provenance attestations, the\ncontroller enables Argo Workflows to comply with SLSA standards. We demonstrate\nthat implementations can provide such cryptographic signing and provenance\nattestations for artifacts produced by the controller, allowing software\nartifacts built with Argo Workflows to adhere to SLSA requirements. The\nproposed validation model evaluates the proof of concept of the controller,\nincluding its ability to reconcile workflows, detect pods associated with\nworkflow nodes, operate without disrupting existing operations, enforce\nintegrity, and monitor software artifacts."
    },
    {
        "date": "2025-03",
        "title": "A proposal for an incident regime that tracks and counters threats to national security posed by AI systems",
        "author": "Alejandro Ortega",
        "link": "http://arxiv.org/abs/2503.19887v1",
        "abstract": "Recent progress in AI capabilities has heightened concerns that AI systems\ncould pose a threat to national security, for example, by making it easier for\nmalicious actors to perform cyberattacks on critical national infrastructure,\nor through loss of control of autonomous AI systems. In parallel, federal\nlegislators in the US have proposed nascent 'AI incident regimes' to identify\nand counter similar threats. In this paper, we consolidate these two trends and\npresent a proposal for a legally mandated post-deployment AI incident regie\nthat aims to counter potential national security threats from AI systems. We\nstart the paper by introducing the concept of 'security-critical' to describe\ndoctors that pose extreme risks to national security, before arguing that\n'security-critical' describes civilian nuclear power, aviation, life science\ndual-use research of concern, and frontier AI development. We then present in\ndetail our AI incident regime proposal,, justifying each component of the\nproposal by demonstrating its similarity to US domestic incident regimes in\nother 'security-critical' sectors. Finally, we sketch a hypothetical scenario\nwhere our proposed AI incident regime deals with an AI cyber incident. Our\nproposed AI incident regime is split into three phases. The first phase\nrevolves around a novel operationalization of what counts as an 'AI incident'\nand we suggest that AI providers must create a 'national security case' before\ndeploying a frontier AI system. The second and third phases spell out that AI\nproviders should notify a government agency about incidents, and that the\ngovernment agency should be involved in amending AI providers' security and\nsafety procedures, in order to counter future threats to national security. Our\nproposal is timely, given ongoing policy interest in the potential national\nsecurity threats posed by AI systems."
    },
    {
        "date": "2025-03",
        "title": "RCC-PFL: Robust Client Clustering under Noisy Labels in Personalized Federated Learning",
        "author": "Abdulmoneam Ali, and Ahmed Arafa",
        "link": "http://arxiv.org/abs/2503.19886v1",
        "abstract": "We address the problem of cluster identity estimation in a personalized\nfederated learning (PFL) setting in which users aim to learn different personal\nmodels. The backbone of effective learning in such a setting is to cluster\nusers into groups whose objectives are similar. A typical approach in the\nliterature is to achieve this by training users' data on different proposed\npersonal models and assign them to groups based on which model achieves the\nlowest value of the users' loss functions. This process is to be done\niteratively until group identities converge. A key challenge in such a setting\narises when users have noisy labeled data, which may produce misleading values\nof their loss functions, and hence lead to ineffective clustering. To overcome\nthis challenge, we propose a label-agnostic data similarity-based clustering\nalgorithm, coined RCC-PFL, with three main advantages: the cluster identity\nestimation procedure is independent from the training labels; it is a one-shot\nclustering algorithm performed prior to the training; and it requires fewer\ncommunication rounds and less computation compared to iterative-based\nclustering methods. We validate our proposed algorithm using various models and\ndatasets and show that it outperforms multiple baselines in terms of average\naccuracy and variance reduction."
    },
    {
        "date": "2025-03",
        "title": "Bitstream Collisions in Neural Image Compression via Adversarial Perturbations",
        "author": "Jordan Madden, Lhamo Dorje, and Xiaohua Li",
        "link": "http://arxiv.org/abs/2503.19817v1",
        "abstract": "Neural image compression (NIC) has emerged as a promising alternative to\nclassical compression techniques, offering improved compression ratios. Despite\nits progress towards standardization and practical deployment, there has been\nminimal exploration into it's robustness and security. This study reveals an\nunexpected vulnerability in NIC - bitstream collisions - where semantically\ndifferent images produce identical compressed bitstreams. Utilizing a novel\nwhitebox adversarial attack algorithm, this paper demonstrates that adding\ncarefully crafted perturbations to semantically different images can cause\ntheir compressed bitstreams to collide exactly. The collision vulnerability\nposes a threat to the practical usability of NIC, particularly in\nsecurity-critical applications. The cause of the collision is analyzed, and a\nsimple yet effective mitigation method is presented."
    },
    {
        "date": "2025-03",
        "title": "SITA: Structurally Imperceptible and Transferable Adversarial Attacks for Stylized Image Generation",
        "author": "Jingdan Kang, Haoxin Yang, Yan Cai, Huaidong Zhang, Xuemiao Xu, Yong Du, and Shengfeng He",
        "link": "http://arxiv.org/abs/2503.19791v1",
        "abstract": "Image generation technology has brought significant advancements across\nvarious fields but has also raised concerns about data misuse and potential\nrights infringements, particularly with respect to creating visual artworks.\nCurrent methods aimed at safeguarding artworks often employ adversarial\nattacks. However, these methods face challenges such as poor transferability,\nhigh computational costs, and the introduction of noticeable noise, which\ncompromises the aesthetic quality of the original artwork. To address these\nlimitations, we propose a Structurally Imperceptible and Transferable\nAdversarial (SITA) attacks. SITA leverages a CLIP-based destylization loss,\nwhich decouples and disrupts the robust style representation of the image. This\ndisruption hinders style extraction during stylized image generation, thereby\nimpairing the overall stylization process. Importantly, SITA eliminates the\nneed for a surrogate diffusion model, leading to significantly reduced\ncomputational overhead. The method's robust style feature disruption ensures\nhigh transferability across diverse models. Moreover, SITA introduces\nperturbations by embedding noise within the imperceptible structural details of\nthe image. This approach effectively protects against style extraction without\ncompromising the visual quality of the artwork. Extensive experiments\ndemonstrate that SITA offers superior protection for artworks against\nunauthorized use in stylized generation. It significantly outperforms existing\nmethods in terms of transferability, computational efficiency, and noise\nimperceptibility. Code is available at https://github.com/A-raniy-day/SITA."
    },
    {
        "date": "2025-03",
        "title": "PyGraph: Robust Compiler Support for CUDA Graphs in PyTorch",
        "author": "Abhishek Ghosh, Ajay Nayak, Ashish Panwar, and Arkaprava Basu",
        "link": "http://arxiv.org/abs/2503.19779v1",
        "abstract": "CUDA Graphs -- a recent hardware feature introduced for NVIDIA GPUs -- aim to\nreduce CPU launch overhead by capturing and launching a series of GPU tasks\n(kernels) as a DAG. However, deploying CUDA Graphs faces several challenges\ntoday due to the static structure of a graph. It also incurs performance\noverhead due to data copy. In fact, we show a counter-intuitive result --\ndeploying CUDA Graphs hurts performance in many cases.\n  We introduce PyGraph, a novel approach to automatically harness the power of\nCUDA Graphs within PyTorch2. Driven by three key observations, PyGraph embodies\nthree novel optimizations: it enables wider deployment of CUDA Graphs, reduces\nGPU kernel parameter copy overheads, and selectively deploys CUDA Graphs based\non a cost-benefit analysis. PyGraph seamlessly integrates with PyTorch2's\ncompilation toolchain, enabling efficient use of CUDA Graphs without manual\nmodifications to the code. We evaluate PyGraph across various machine learning\nbenchmarks, demonstrating substantial performance improvements over PyTorch2."
    },
    {
        "date": "2025-03",
        "title": "A Managed Tokens Service for Securely Keeping and Distributing Grid Tokens",
        "author": "Shreyas Bhat, and Dave Dykstra",
        "link": "http://arxiv.org/abs/2503.19768v1",
        "abstract": "Fermilab is transitioning authentication and authorization for grid\noperations to using bearer tokens based on the WLCG Common JWT (JSON Web Token)\nProfile. One of the functionalities that Fermilab experimenters rely on is the\nability to automate batch job submission, which in turn depends on the ability\nto securely refresh and distribute the necessary credentials to experiment job\nsubmit points. Thus, with the transition to using tokens for grid operations,\nwe needed to create a service that would obtain, refresh, and distribute tokens\nfor experimenters' use. This service would avoid the need for experimenters to\nbe experts in obtaining their own tokens and would better protect the most\nsensitive long-lived credentials. Further, the service needed to be widely\nscalable, as Fermilab hosts many experiments, each of which would need their\nown credentials. To address these issues, we created and deployed a Managed\nTokens Service. The service is written in Go, taking advantage of that\nlanguage's native concurrency primitives to easily be able to scale operations\nas we onboard experiments. The service uses as its first credentials a set of\nkerberos keytabs, stored on the same secure machine that the Managed Tokens\nservice runs on. These kerberos credentials allow the service to use htgettoken\nvia condor_vault_storer to store vault tokens in the HTCondor credential\nmanagers (credds) that run on the batch system scheduler machines (HTCondor\nschedds); as well as downloading a local, shorter-lived copy of the vault\ntoken. The kerberos credentials are then also used to distribute copies of the\nlocally-stored vault tokens to experiment submit points."
    },
    {
        "date": "2025-03",
        "title": "FUSE: Label-Free Image-Event Joint Monocular Depth Estimation via Frequency-Decoupled Alignment and Degradation-Robust Fusion",
        "author": "Pihai Sun, Junjun Jiang, Yuanqi Yao, Youyu Chen, Wenbo Zhao, Kui Jiang, and Xianming Liu",
        "link": "http://arxiv.org/abs/2503.19739v2",
        "abstract": "Image-event joint depth estimation methods leverage complementary modalities\nfor robust perception, yet face challenges in generalizability stemming from\ntwo factors: 1) limited annotated image-event-depth datasets causing\ninsufficient cross-modal supervision, and 2) inherent frequency mismatches\nbetween static images and dynamic event streams with distinct spatiotemporal\npatterns, leading to ineffective feature fusion. To address this dual\nchallenge, we propose Frequency-decoupled Unified Self-supervised Encoder\n(FUSE) with two synergistic components: The Parameter-efficient Self-supervised\nTransfer (PST) establishes cross-modal knowledge transfer through latent space\nalignment with image foundation models, effectively mitigating data scarcity by\nenabling joint encoding without depth ground truth. Complementing this, we\npropose the Frequency-Decoupled Fusion module (FreDFuse) to explicitly decouple\nhigh-frequency edge features from low-frequency structural components,\nresolving modality-specific frequency mismatches through physics-aware fusion.\nThis combined approach enables FUSE to construct a universal image-event\nencoder that only requires lightweight decoder adaptation for target datasets.\nExtensive experiments demonstrate state-of-the-art performance with 14% and\n24.9% improvements in Abs.Rel on MVSEC and DENSE datasets. The framework\nexhibits remarkable zero-shot adaptability to challenging scenarios including\nextreme lighting and motion blur, significantly advancing real-world deployment\ncapabilities. The source code for our method is publicly available at:\nhttps://github.com/sunpihai-up/FUSE"
    },
    {
        "date": "2025-03",
        "title": "On What Depends the Robustness of Multi-source Models to Missing Data in Earth Observation?",
        "author": "Francisco Mena, Diego Arenas, Miro Miranda, and Andreas Dengel",
        "link": "http://arxiv.org/abs/2503.19719v1",
        "abstract": "In recent years, the development of robust multi-source models has emerged in\nthe Earth Observation (EO) field. These are models that leverage data from\ndiverse sources to improve predictive accuracy when there is missing data.\nDespite these advancements, the factors influencing the varying effectiveness\nof such models remain poorly understood. In this study, we evaluate the\npredictive performance of six state-of-the-art multi-source models in\npredicting scenarios where either a single data source is missing or only a\nsingle source is available. Our analysis reveals that the efficacy of these\nmodels is intricately tied to the nature of the task, the complementarity among\ndata sources, and the model design. Surprisingly, we observe instances where\nthe removal of certain data sources leads to improved predictive performance,\nchallenging the assumption that incorporating all available data is always\nbeneficial. These findings prompt critical reflections on model complexity and\nthe necessity of all collected data sources, potentially shaping the way for\nmore streamlined approaches in EO applications."
    },
    {
        "date": "2025-03",
        "title": "Enhancing Graphical Lasso: A Robust Scheme for Non-Stationary Mean Data",
        "author": "Samuel Rey, Ernesto Curbelo, Luca Martino, Fernando Llorente, and Antonio G. Marques",
        "link": "http://arxiv.org/abs/2503.19651v1",
        "abstract": "This work addresses the problem of graph learning from data following a\nGaussian Graphical Model (GGM) with a time-varying mean. Graphical Lasso (GL),\nthe standard method for estimating sparse precision matrices, assumes that the\nobserved data follows a zero-mean Gaussian distribution. However, this\nassumption is often violated in real-world scenarios where the mean evolves\nover time due to external influences, trends, or regime shifts. When the mean\nis not properly accounted for, applying GL directly can lead to estimating a\nbiased precision matrix, hence hindering the graph learning task. To overcome\nthis limitation, we propose Graphical Lasso with Adaptive Targeted Adaptive\nImportance Sampling (GL-ATAIS), an iterative method that jointly estimates the\ntime-varying mean and the precision matrix. Our approach integrates Bayesian\ninference with frequentist estimation, leveraging importance sampling to obtain\nan estimate of the mean while using a regularized maximum likelihood estimator\nto infer the precision matrix. By iteratively refining both estimates, GL-ATAIS\nmitigates the bias introduced by time-varying means, leading to more accurate\ngraph recovery. Our numerical evaluation demonstrates the impact of properly\naccounting for time-dependent means and highlights the advantages of GL-ATAIS\nover standard GL in recovering the true graph structure."
    },
    {
        "date": "2025-03",
        "title": "Nanopass Back-Translation of Call-Return Trees for Mechanized Secure Compilation Proofs",
        "author": "J\u00e9r\u00e9my Thibault, Joseph Lenormand, and Catalin Hritcu",
        "link": "http://arxiv.org/abs/2503.19609v1",
        "abstract": "Researchers aim to build secure compilation chains enforcing that if there is\nno attack a source context can mount against a source program then there is\nalso no attack an adversarial target context can mount against the compiled\nprogram. Proving that these compilation chains are secure is, however,\nchallenging, and involves a non-trivial back-translation step: for any attack a\ntarget context mounts against the compiled program one has to exhibit a source\ncontext mounting the same attack against the source program. We describe a\nnovel back-translation technique, which results in simpler proofs that can be\nmore easily mechanized in a proof assistant. Given a finite set of finite trace\nprefixes, capturing the interaction recorded during an attack between a target\ncontext and the compiled program, we build a call-return tree that we\nback-translate into a source context producing the same trace prefixes. We use\nstate in the generated source context to record the current location in the\ncall-return tree. The back-translation is done in several small steps, each\nadding to the tree new information describing how the location should change\ndepending on how the context regains control. To prove this back-translation\ncorrect we give semantics to every intermediate call-return tree language,\nusing ghost state to store information and explicitly enforce execution\ninvariants. We prove several small forward simulations, basically seeing the\nback-translation as a verified nanopass compiler. Thanks to this modular\nstructure, we are able to mechanize this complex back-translation and its\ncorrectness proof in the Rocq prover without too much effort."
    },
    {
        "date": "2025-03",
        "title": "Boosting the Transferability of Audio Adversarial Examples with Acoustic Representation Optimization",
        "author": "Weifei Jin, Junjie Su, Hejia Wang, Yulin Ye, and Jie Hao",
        "link": "http://arxiv.org/abs/2503.19591v1",
        "abstract": "With the widespread application of automatic speech recognition (ASR)\nsystems, their vulnerability to adversarial attacks has been extensively\nstudied. However, most existing adversarial examples are generated on specific\nindividual models, resulting in a lack of transferability. In real-world\nscenarios, attackers often cannot access detailed information about the target\nmodel, making query-based attacks unfeasible. To address this challenge, we\npropose a technique called Acoustic Representation Optimization that aligns\nadversarial perturbations with low-level acoustic characteristics derived from\nspeech representation models. Rather than relying on model-specific,\nhigher-layer abstractions, our approach leverages fundamental acoustic\nrepresentations that remain consistent across diverse ASR architectures. By\nenforcing an acoustic representation loss to guide perturbations toward these\nrobust, lower-level representations, we enhance the cross-model transferability\nof adversarial examples without degrading audio quality. Our method is\nplug-and-play and can be integrated with any existing attack methods. We\nevaluate our approach on three modern ASR models, and the experimental results\ndemonstrate that our method significantly improves the transferability of\nadversarial examples generated by previous methods while preserving the audio\nquality."
    },
    {
        "date": "2025-03",
        "title": "FLEX: A Benchmark for Evaluating Robustness of Fairness in Large Language Models",
        "author": "Dahyun Jung, Seungyoon Lee, Hyeonseok Moon, Chanjun Park, and Heuiseok Lim",
        "link": "http://arxiv.org/abs/2503.19540v1",
        "abstract": "Recent advancements in Large Language Models (LLMs) have significantly\nenhanced interactions between users and models. These advancements concurrently\nunderscore the need for rigorous safety evaluations due to the manifestation of\nsocial biases, which can lead to harmful societal impacts. Despite these\nconcerns, existing benchmarks may overlook the intrinsic weaknesses of LLMs,\nwhich can generate biased responses even with simple adversarial instructions.\nTo address this critical gap, we introduce a new benchmark, Fairness Benchmark\nin LLM under Extreme Scenarios (FLEX), designed to test whether LLMs can\nsustain fairness even when exposed to prompts constructed to induce bias. To\nthoroughly evaluate the robustness of LLMs, we integrate prompts that amplify\npotential biases into the fairness assessment. Comparative experiments between\nFLEX and existing benchmarks demonstrate that traditional evaluations may\nunderestimate the inherent risks in models. This highlights the need for more\nstringent LLM evaluation benchmarks to guarantee safety and fairness."
    },
    {
        "date": "2025-03",
        "title": "Towards Imperceptible Adversarial Attacks for Time Series Classification with Local Perturbations and Frequency Analysis",
        "author": "Wenwei Gu, Renyi Zhong, Jianping Zhang, and Michael R. Lyu",
        "link": "http://arxiv.org/abs/2503.19519v1",
        "abstract": "Adversarial attacks in time series classification (TSC) models have recently\ngained attention due to their potential to compromise model robustness.\nImperceptibility is crucial, as adversarial examples detected by the human\nvision system (HVS) can render attacks ineffective. Many existing methods fail\nto produce high-quality imperceptible examples, often generating perturbations\nwith more perceptible low-frequency components, like square waves, and global\nperturbations that reduce stealthiness. This paper aims to improve the\nimperceptibility of adversarial attacks on TSC models by addressing frequency\ncomponents and time series locality. We propose the Shapelet-based\nFrequency-domain Attack (SFAttack), which uses local perturbations focused on\ntime series shapelets to enhance discriminative information and stealthiness.\nAdditionally, we introduce a low-frequency constraint to confine perturbations\nto high-frequency components, enhancing imperceptibility."
    },
    {
        "date": "2025-03",
        "title": "SparSamp: Efficient Provably Secure Steganography Based on Sparse Sampling",
        "author": "Yaofei Wang, Gang Pei, Kejiang Chen, Jinyang Ding, Chao Pan, Weilong Pang, Donghui Hu, and Weiming Zhang",
        "link": "http://arxiv.org/abs/2503.19499v1",
        "abstract": "Steganography embeds confidential data within seemingly innocuous\ncommunications. Provable security in steganography, a long-sought goal, has\nbecome feasible with deep generative models. However, existing methods face a\ncritical trade-off between security and efficiency. This paper introduces\nSparSamp, an efficient provably secure steganography method based on sparse\nsampling. SparSamp embeds messages by combining them with pseudo-random numbers\nto obtain message-derived random numbers for sampling. It enhances extraction\naccuracy and embedding capacity by increasing the sampling intervals and making\nthe sampling process sparse. SparSamp preserves the original probability\ndistribution of the generative model, thus ensuring security. It introduces\nonly $O(1)$ additional complexity per sampling step, enabling the fastest\nembedding speed without compromising generation speed. SparSamp is designed to\nbe plug-and-play; message embedding can be achieved by simply replacing the\nsampling component of an existing generative model with SparSamp. We\nimplemented SparSamp in text, image, and audio generation models. It can\nachieve embedding speeds of up to 755 bits/second with GPT-2, 5046 bits/second\nwith DDPM, and 9,223 bits/second with WaveRNN."
    },
    {
        "date": "2025-03",
        "title": "Towards Robust Time-of-Flight Depth Denoising with Confidence-Aware Diffusion Model",
        "author": "Changyong He, Jin Zeng, Jiawei Zhang, and Jiajie Guo",
        "link": "http://arxiv.org/abs/2503.19448v1",
        "abstract": "Time-of-Flight (ToF) sensors efficiently capture scene depth, but the\nnonlinear depth construction procedure often results in extremely large noise\nvariance or even invalid areas. Recent methods based on deep neural networks\n(DNNs) achieve enhanced ToF denoising accuracy but tend to struggle when\npresented with severe noise corruption due to limited prior knowledge of ToF\ndata distribution. In this paper, we propose DepthCAD, a novel ToF denoising\napproach that ensures global structural smoothness by leveraging the rich prior\nknowledge in Stable Diffusion and maintains local metric accuracy by steering\nthe diffusion process with confidence guidance. To adopt the pretrained image\ndiffusion model to ToF depth denoising, we apply the diffusion on raw ToF\ncorrelation measurements with dynamic range normalization before converting to\ndepth maps. Experimental results validate the state-of-the-art performance of\nthe proposed scheme, and the evaluation on real data further verifies its\nrobustness against real-world ToF noise."
    },
    {
        "date": "2025-03",
        "title": "Membership Inference Attacks on Large-Scale Models: A Survey",
        "author": "Hengyu Wu, and Yang Cao",
        "link": "http://arxiv.org/abs/2503.19338v1",
        "abstract": "The adoption of the Large Language Model (LLM) has accelerated dramatically\nsince the ChatGPT from OpenAI went online in November 2022. Recent advances in\nLarge Multimodal Models (LMMs), which process diverse data types and enable\ninteraction through various channels, have expanded beyond the text-to-text\nlimitations of early LLMs, attracting significant and concurrent attention from\nboth researchers and industry. While LLMs and LMMs are starting to spread\nwidely, concerns about their privacy risks are increasing as well. Membership\nInference Attacks (MIAs), techniques used to determine whether a particular\ndata point was part of a model's training set, serve as a key metric for\nassessing the privacy vulnerabilities of machine learning models. Hu et al.\nshow that various machine learning algorithms are vulnerable to MIA. Despite\nextensive studies on MIAs in traditional models, there remains a lack of\nsystematic surveys addressing their effectiveness and implications in modern\nlarge-scale models like LLMs and LMMs. In this paper, we systematically\nreviewed recent studies of MIA against LLMs and LMMs. We analyzed and\ncategorized each attack based on their methodology and scenario and discussed\nthe limitations in existing research. Additionally, we examine privacy concerns\nassociated with the fine-tuning process. Finally, we provided some suggestions\nfor future research in this direction."
    },
    {
        "date": "2025-03",
        "title": "Efficient Adversarial Detection Frameworks for Vehicle-to-Microgrid Services in Edge Computing",
        "author": "Ahmed Omara, and Burak Kantarci",
        "link": "http://arxiv.org/abs/2503.19318v1",
        "abstract": "As Artificial Intelligence (AI) becomes increasingly integrated into\nmicrogrid control systems, the risk of malicious actors exploiting\nvulnerabilities in Machine Learning (ML) algorithms to disrupt power generation\nand distribution grows. Detection models to identify adversarial attacks need\nto meet the constraints of edge environments, where computational power and\nmemory are often limited. To address this issue, we propose a novel strategy\nthat optimizes detection models for Vehicle-to-Microgrid (V2M) edge\nenvironments without compromising performance against inference and evasion\nattacks. Our approach integrates model design and compression into a unified\nprocess and results in a highly compact detection model that maintains high\naccuracy. We evaluated our method against four benchmark evasion attacks-Fast\nGradient Sign Method (FGSM), Basic Iterative Method (BIM), Carlini & Wagner\nmethod (C&W) and Conditional Generative Adversarial Network (CGAN) method-and\ntwo knowledge-based attacks, white-box and gray-box. Our optimized model\nreduces memory usage from 20MB to 1.3MB, inference time from 3.2 seconds to 0.9\nseconds, and GPU utilization from 5% to 2.68%."
    },
    {
        "date": "2025-03",
        "title": "SoK: How Robust is Audio Watermarking in Generative AI models?",
        "author": "Yizhu Wen, Ashwin Innuganti, Aaron Bien Ramos, Hanqing Guo, and Qiben Yan",
        "link": "http://arxiv.org/abs/2503.19176v2",
        "abstract": "Audio watermarking is increasingly used to verify the provenance of\nAI-generated content, enabling applications such as detecting AI-generated\nspeech, protecting music IP, and defending against voice cloning. To be\neffective, audio watermarks must resist removal attacks that distort signals to\nevade detection. While many schemes claim robustness, these claims are\ntypically tested in isolation and against a limited set of attacks. A\nsystematic evaluation against diverse removal attacks is lacking, hindering\npractical deployment. In this paper, we investigate whether recent watermarking\nschemes that claim robustness can withstand a broad range of removal attacks.\nFirst, we introduce a taxonomy covering 22 audio watermarking schemes. Next, we\nsummarize their underlying technologies and potential vulnerabilities. We then\npresent a large-scale empirical study to assess their robustness. To support\nthis, we build an evaluation framework encompassing 22 types of removal attacks\n(109 configurations) including signal-level, physical-level, and AI-induced\ndistortions. We reproduce 9 watermarking schemes using open-source code,\nidentify 8 new highly effective attacks, and highlight 11 key findings that\nexpose the fundamental limitations of these methods across 3 public datasets.\nOur results reveal that none of the surveyed schemes can withstand all tested\ndistortions. This evaluation offers a comprehensive view of how current\nwatermarking methods perform under real-world threats. Our demo and code are\navailable at https://sokaudiowm.github.io/."
    },
    {
        "date": "2025-03",
        "title": "MIRAGE: Multimodal Immersive Reasoning and Guided Exploration for Red-Team Jailbreak Attacks",
        "author": "Wenhao You, Bryan Hooi, Yiwei Wang, Youke Wang, Zong Ke, Ming-Hsuan Yang, Zi Huang, and Yujun Cai",
        "link": "http://arxiv.org/abs/2503.19134v1",
        "abstract": "While safety mechanisms have significantly progressed in filtering harmful\ntext inputs, MLLMs remain vulnerable to multimodal jailbreaks that exploit\ntheir cross-modal reasoning capabilities. We present MIRAGE, a novel multimodal\njailbreak framework that exploits narrative-driven context and role immersion\nto circumvent safety mechanisms in Multimodal Large Language Models (MLLMs). By\nsystematically decomposing the toxic query into environment, role, and action\ntriplets, MIRAGE constructs a multi-turn visual storytelling sequence of images\nand text using Stable Diffusion, guiding the target model through an engaging\ndetective narrative. This process progressively lowers the model's defences and\nsubtly guides its reasoning through structured contextual cues, ultimately\neliciting harmful responses. In extensive experiments on the selected datasets\nwith six mainstream MLLMs, MIRAGE achieves state-of-the-art performance,\nimproving attack success rates by up to 17.5% over the best baselines.\nMoreover, we demonstrate that role immersion and structured semantic\nreconstruction can activate inherent model biases, facilitating the model's\nspontaneous violation of ethical safeguards. These results highlight critical\nweaknesses in current multimodal safety mechanisms and underscore the urgent\nneed for more robust defences against cross-modal threats."
    },
    {
        "date": "2025-03",
        "title": "Paving the way for scientific foundation models: enhancing generalization and robustness in PDEs with constraint-aware pre-training",
        "author": "Amin Totounferoush, Serge Kotchourko, Michael W. Mahoney, and Steffen Staab",
        "link": "http://arxiv.org/abs/2503.19081v1",
        "abstract": "Partial differential equations (PDEs) govern a wide range of physical\nsystems, but solving them efficiently remains a major challenge. The idea of a\nscientific foundation model (SciFM) is emerging as a promising tool for\nlearning transferable representations across diverse domains. However, SciFMs\nrequire large amounts of solution data, which may be scarce or computationally\nexpensive to generate. To maximize generalization while reducing data\ndependence, we propose incorporating PDE residuals into pre-training either as\nthe sole learning signal or in combination with data loss to compensate for\nlimited or infeasible training data. We evaluate this constraint-aware\npre-training across three key benchmarks: (i) generalization to new physics,\nwhere material properties, e.g., the diffusion coefficient, is shifted with\nrespect to the training distribution; (ii) generalization to entirely new PDEs,\nrequiring adaptation to different operators; and (iii) robustness against noisy\nfine-tuning data, ensuring stability in real-world applications. Our results\nshow that pre-training with PDE constraints significantly enhances\ngeneralization, outperforming models trained solely on solution data across all\nbenchmarks. These findings prove the effectiveness of our proposed\nconstraint-aware pre-training as a crucial component for SciFMs, providing a\nscalable approach to data-efficient, generalizable PDE solvers."
    },
    {
        "date": "2025-03",
        "title": "Graph-Level Label-Only Membership Inference Attack against Graph Neural Networks",
        "author": "Jiazhu Dai, and Yubing Lu",
        "link": "http://arxiv.org/abs/2503.19070v2",
        "abstract": "Graph neural networks (GNNs) are widely used for graph-structured data but\nare vulnerable to membership inference attacks (MIAs) in graph classification\ntasks, which determine if a graph was part of the training dataset, potentially\ncausing data leakage. Existing MIAs rely on prediction probability vectors, but\nthey become ineffective when only prediction labels are available. We propose a\nGraph-level Label-Only Membership Inference Attack (GLO-MIA), which is based on\nthe intuition that the target model's predictions on training data are more\nstable than those on testing data. GLO-MIA generates a set of perturbed graphs\nfor target graph by adding perturbations to its effective features and queries\nthe target model with the perturbed graphs to get their prediction labels,\nwhich are then used to calculate robustness score of the target graph. Finally,\nby comparing the robustness score with a predefined threshold, the membership\nof the target graph can be inferred correctly with high probability. Our\nevaluation on three datasets and four GNN models shows that GLO-MIA achieves an\nattack accuracy of up to 0.825, outperforming baseline work by 8.5% and closely\nmatching the performance of probability-based MIAs, even with only prediction\nlabels."
    },
    {
        "date": "2025-03",
        "title": "strideSEA: A STRIDE-centric Security Evaluation Approach",
        "author": "Alvi Jawad, Jason Jaskolka, Ashraf Matrawy, and Mohamed Ibnkahla",
        "link": "http://arxiv.org/abs/2503.19030v1",
        "abstract": "Microsoft's STRIDE methodology is at the forefront of threat modeling,\nsupporting the increasingly critical quality attribute of security in\nsoftware-intensive systems. However, in a comprehensive security evaluation\nprocess, the general consensus is that the STRIDE classification is only useful\nfor threat elicitation, isolating threat modeling from the other security\nevaluation activities involved in a secure software development life cycle\n(SDLC). We present strideSEA, a STRIDE-centric Security Evaluation Approach\nthat integrates STRIDE as the central classification scheme into the security\nactivities of threat modeling, attack scenario analysis, risk analysis, and\ncountermeasure recommendation that are conducted alongside software engineering\nactivities in secure SDLCs. The application of strideSEA is demonstrated in a\nreal-world online immunization system case study. Using STRIDE as a single\nunifying thread, we bind existing security evaluation approaches in the four\nsecurity activities of strideSEA to analyze (1) threats using Microsoft threat\nmodeling tool, (2) attack scenarios using attack trees, (3) systemic risk using\nNASA's defect detection and prevention (DDP) technique, and (4) recommend\ncountermeasures based on their effectiveness in reducing the most critical\nrisks using DDP. The results include a detailed quantitative assessment of the\nsecurity of the online immunization system with a clear definition of the role\nand advantages of integrating STRIDE in the evaluation process. Overall, the\nunified approach in strideSEA enables a more structured security evaluation\nprocess, allowing easier identification and recommendation of countermeasures,\nthus supporting the security requirements and eliciting design considerations,\ninforming the software development life cycle of future software-based\ninformation systems."
    },
    {
        "date": "2025-03",
        "title": "Building Blocks for Robust and Effective Semi-Supervised Real-World Object Detection",
        "author": "Moussa Kassem Sbeyti, Nadja Klein, Azarm Nowzad, Fikret Sivrikaya, and Sahin Albayrak",
        "link": "http://arxiv.org/abs/2503.18903v1",
        "abstract": "Semi-supervised object detection (SSOD) based on pseudo-labeling\nsignificantly reduces dependence on large labeled datasets by effectively\nleveraging both labeled and unlabeled data. However, real-world applications of\nSSOD often face critical challenges, including class imbalance, label noise,\nand labeling errors. We present an in-depth analysis of SSOD under real-world\nconditions, uncovering causes of suboptimal pseudo-labeling and key trade-offs\nbetween label quality and quantity. Based on our findings, we propose four\nbuilding blocks that can be seamlessly integrated into an SSOD framework. Rare\nClass Collage (RCC): a data augmentation method that enhances the\nrepresentation of rare classes by creating collages of rare objects. Rare Class\nFocus (RCF): a stratified batch sampling strategy that ensures a more balanced\nrepresentation of all classes during training. Ground Truth Label Correction\n(GLC): a label refinement method that identifies and corrects false, missing,\nand noisy ground truth labels by leveraging the consistency of teacher model\npredictions. Pseudo-Label Selection (PLS): a selection method for removing\nlow-quality pseudo-labeled images, guided by a novel metric estimating the\nmissing detection rate while accounting for class rarity. We validate our\nmethods through comprehensive experiments on autonomous driving datasets,\nresulting in up to 6% increase in SSOD performance. Overall, our investigation\nand novel, data-centric, and broadly applicable building blocks enable robust\nand effective SSOD in complex, real-world scenarios. Code is available at\nhttps://mos-ks.github.io/publications."
    },
    {
        "date": "2025-03",
        "title": "Secure Edge Computing Reference Architecture for Data-driven Structural Health Monitoring: Lessons Learned from Implementation and Benchmarking",
        "author": "Sheikh Muhammad Farjad, Sandeep Reddy Patllola, Yonas Kassa, George Grispos, and Robin Gandhi",
        "link": "http://arxiv.org/abs/2503.18857v1",
        "abstract": "Structural Health Monitoring (SHM) plays a crucial role in maintaining aging\nand critical infrastructure, supporting applications such as smart cities and\ndigital twinning. These applications demand machine learning models capable of\nprocessing large volumes of real-time sensor data at the network edge. However,\nexisting approaches often neglect the challenges of deploying machine learning\nmodels at the edge or are constrained by vendor-specific platforms. This paper\nintroduces a scalable and secure edge-computing reference architecture tailored\nfor data-driven SHM. We share practical insights from deploying this\narchitecture at the Memorial Bridge in New Hampshire, US, referred to as the\nLiving Bridge project. Our solution integrates a commercial data acquisition\nsystem with off-the-shelf hardware running an open-source edge-computing\nplatform, remotely managed and scaled through cloud services. To support the\ndevelopment of data-driven SHM systems, we propose a resource consumption\nbenchmarking framework called edgeOps to evaluate the performance of machine\nlearning models on edge devices. We study this framework by collecting resource\nutilization data for machine learning models typically used in SHM applications\non two different edge computing hardware platforms. edgeOps was specifically\nstudied on off-the-shelf Linux and ARM-based edge devices. Our findings\ndemonstrate the impact of platform and model selection on system performance,\nproviding actionable guidance for edge-based SHM system design."
    },
    {
        "date": "2025-03",
        "title": "Leveraging Perturbation Robustness to Enhance Out-of-Distribution Detection",
        "author": "Wenxi Chen, Raymond A. Yeh, Shaoshuai Mou, and Yan Gu",
        "link": "http://arxiv.org/abs/2503.18784v1",
        "abstract": "Out-of-distribution (OOD) detection is the task of identifying inputs that\ndeviate from the training data distribution. This capability is essential for\nsafely deploying deep computer vision models in open-world environments. In\nthis work, we propose a post-hoc method, Perturbation-Rectified OOD detection\n(PRO), based on the insight that prediction confidence for OOD inputs is more\nsusceptible to reduction under perturbation than in-distribution (IND) inputs.\nBased on the observation, we propose an adversarial score function that\nsearches for the local minimum scores near the original inputs by applying\ngradient descent. This procedure enhances the separability between IND and OOD\nsamples. Importantly, the approach improves OOD detection performance without\ncomplex modifications to the underlying model architectures. We conduct\nextensive experiments using the OpenOOD benchmark~\\cite{yang2022openood}. Our\napproach further pushes the limit of softmax-based OOD detection and is the\nleading post-hoc method for small-scale models. On a CIFAR-10 model with\nadversarial training, PRO effectively detects near-OOD inputs, achieving a\nreduction of more than 10\\% on FPR@95 compared to state-of-the-art methods."
    },
    {
        "date": "2025-03",
        "title": "Robust Tube-based Control Strategy for Vision-guided Autonomous Vehicles",
        "author": "Der-Hau Lee",
        "link": "http://arxiv.org/abs/2503.18752v1",
        "abstract": "A robust control strategy for autonomous vehicles can improve system\nstability, enhance riding comfort, and prevent driving accidents. This paper\npresents a novel interpolation tube-based constrained iterative linear\nquadratic regulator (itube-CILQR) algorithm for autonomous\ncomputer-vision-based vehicle lane-keeping. The goal of the algorithm is to\nenhance robustness during high-speed cornering on tight turns. The advantages\nof itube-CILQR over the standard tube-approach include reduced system\nconservatism and increased computational speed. Numerical and vision-based\nexperiments were conducted to examine the feasibility of the proposed\nalgorithm. The proposed itube-CILQR algorithm is better suited to vehicle\nlane-keeping than variational CILQR-based methods and model predictive control\n(MPC) approaches using a classical interior-point solver. Specifically, in\nevaluation experiments, itube-CILQR achieved an average runtime of 3.16 ms to\ngenerate a control signal to guide a self-driving vehicle; itube-MPC typically\nrequired a 4.67-times longer computation time to complete the same task.\nMoreover, the influence of conservatism on system behavior was investigated by\nexploring the interpolation variable trajectories derived from the proposed\nitube-CILQR algorithm during lane-keeping maneuvers."
    },
    {
        "date": "2025-03",
        "title": "GS-Marker: Generalizable and Robust Watermarking for 3D Gaussian Splatting",
        "author": "Lijiang Li, Jinglu Wang, Xiang Ming, and Yan Lu",
        "link": "http://arxiv.org/abs/2503.18718v1",
        "abstract": "In the Generative AI era, safeguarding 3D models has become increasingly\nurgent. While invisible watermarking is well-established for 2D images with\nencoder-decoder frameworks, generalizable and robust solutions for 3D remain\nelusive. The main difficulty arises from the renderer between the 3D encoder\nand 2D decoder, which disrupts direct gradient flow and complicates training.\nExisting 3D methods typically rely on per-scene iterative optimization,\nresulting in time inefficiency and limited generalization. In this work, we\npropose a single-pass watermarking approach for 3D Gaussian Splatting (3DGS), a\nwell-known yet underexplored representation for watermarking. We identify two\nmajor challenges: (1) ensuring effective training generalized across diverse 3D\nmodels, and (2) reliably extracting watermarks from free-view renderings, even\nunder distortions. Our framework, named GS-Marker, incorporates a 3D encoder to\nembed messages, distortion layers to enhance resilience against various\ndistortions, and a 2D decoder to extract watermarks from renderings. A key\ninnovation is the Adaptive Marker Control mechanism that adaptively perturbs\nthe initially optimized 3DGS, escaping local minima and improving both training\nstability and convergence. Extensive experiments show that GS-Marker\noutperforms per-scene training approaches in terms of decoding accuracy and\nmodel fidelity, while also significantly reducing computation time."
    },
    {
        "date": "2025-03",
        "title": "Robust face recognition based on the wing loss and the $\\ell_1$ regularization",
        "author": "Yaoyao Yun, and Jianwen Xu",
        "link": "http://arxiv.org/abs/2503.18652v1",
        "abstract": "In recent years, sparse sampling techniques based on regression analysis have\nwitnessed extensive applications in face recognition research. Presently,\nnumerous sparse sampling models based on regression analysis have been explored\nby various researchers. Nevertheless, the recognition rates of the majority of\nthese models would be significantly decreased when confronted with highly\noccluded and highly damaged face images. In this paper, a new wing-constrained\nsparse coding model(WCSC) and its weighted version(WWCSC) are introduced, so as\nto deal with the face recognition problem in complex circumstances, where the\nalternating direction method of multipliers (ADMM) algorithm is employed to\nsolve the corresponding minimization problems. In addition, performances of the\nproposed method are examined based on the four well-known facial databases,\nnamely the ORL facial database, the Yale facial database, the AR facial\ndatabase and the FERET facial database. Also, compared to the other methods in\nthe literatures, the WWCSC has a very high recognition rate even in complex\nsituations where face images have high occlusion or high damage, which\nillustrates the robustness of the WWCSC method in facial recognition."
    },
    {
        "date": "2025-03",
        "title": "Robust Lane Detection with Wavelet-Enhanced Context Modeling and Adaptive Sampling",
        "author": "Kunyang Li, and Ming Hou",
        "link": "http://arxiv.org/abs/2503.18631v1",
        "abstract": "Lane detection is critical for autonomous driving and ad-vanced driver\nassistance systems (ADAS). While recent methods like CLRNet achieve strong\nperformance, they struggle under adverse con-ditions such as extreme weather,\nillumination changes, occlusions, and complex curves. We propose a\nWavelet-Enhanced Feature Pyramid Net-work (WE-FPN) to address these challenges.\nA wavelet-based non-local block is integrated before the feature pyramid to\nimprove global context modeling, especially for occluded and curved lanes.\nAdditionally, we de-sign an adaptive preprocessing module to enhance lane\nvisibility under poor lighting. An attention-guided sampling strategy further\nreffnes spa-tial features, boosting accuracy on distant and curved lanes.\nExperiments on CULane and TuSimple demonstrate that our approach signiffcantly\noutperforms baselines in challenging scenarios, achieving better robust-ness\nand accuracy in real-world driving conditions."
    },
    {
        "date": "2025-03",
        "title": "Anchor-based oversampling for imbalanced tabular data via contrastive and adversarial learning",
        "author": "Hadi Mohammadi, Ehsan Nazerfard, and Mostafa Haghir Chehreghani",
        "link": "http://arxiv.org/abs/2503.18569v1",
        "abstract": "Imbalanced data represent a distribution with more frequencies of one class\n(majority) than the other (minority). This phenomenon occurs across various\ndomains, such as security, medical care and human activity. In imbalanced\nlearning, classification algorithms are typically inclined to classify the\nmajority class accurately, resulting in artificially high accuracy rates. As a\nresult, many minority samples are mistakenly labelled as majority-class\ninstances, resulting in a bias that benefits the majority class. This study\npresents a framework based on boundary anchor samples to tackle the imbalance\nlearning challenge. First, we select and use anchor samples to train a\nmultilayer perceptron (MLP) classifier, which acts as a prior knowledge model\nand aids the adversarial and contrastive learning procedures. Then, we designed\na novel deep generative model called Anchor Stabilized Conditional Generative\nAdversarial Network or Anch-SCGAN in short. Anch-SCGAN is supported with two\ngenerators for the minority and majority classes and a discriminator\nincorporating additional class-specific information from the pre-trained\nfeature extractor MLP. In addition, we facilitate the generator's training\nprocedure in two ways. First, we define a new generator loss function based on\nreprocessed anchor samples and contrastive learning. Second, we apply a scoring\nstrategy to stabilize the adversarial training part in generators. We train\nAnch-SCGAN and further finetune it with anchor samples to improve the precision\nof the generated samples. Our experiments on 16 real-world imbalanced datasets\nillustrate that Anch-SCGAN outperforms the renowned methods in imbalanced\nlearning."
    },
    {
        "date": "2025-03",
        "title": "DiN: Diffusion Model for Robust Medical VQA with Semantic Noisy Labels",
        "author": "Erjian Guo, Zhen Zhao, Zicheng Wang, Tong Chen, Yunyi Liu, and Luping Zhou",
        "link": "http://arxiv.org/abs/2503.18536v1",
        "abstract": "Medical Visual Question Answering (Med-VQA) systems benefit the\ninterpretation of medical images containing critical clinical information.\nHowever, the challenge of noisy labels and limited high-quality datasets\nremains underexplored. To address this, we establish the first benchmark for\nnoisy labels in Med-VQA by simulating human mislabeling with semantically\ndesigned noise types. More importantly, we introduce the DiN framework, which\nleverages a diffusion model to handle noisy labels in Med-VQA. Unlike the\ndominant classification-based VQA approaches that directly predict answers, our\nAnswer Diffuser (AD) module employs a coarse-to-fine process, refining answer\ncandidates with a diffusion model for improved accuracy. The Answer Condition\nGenerator (ACG) further enhances this process by generating task-specific\nconditional information via integrating answer embeddings with fused\nimage-question features. To address label noise, our Noisy Label\nRefinement(NLR) module introduces a robust loss function and dynamic answer\nadjustment to further boost the performance of the AD module."
    },
    {
        "date": "2025-03",
        "title": "Deterministic Certification of Graph Neural Networks against Graph Poisoning Attacks with Arbitrary Perturbations",
        "author": "Jiate Li, Meng Pang, Yun Dong, and Binghui Wang",
        "link": "http://arxiv.org/abs/2503.18503v1",
        "abstract": "Graph neural networks (GNNs) are becoming the de facto method to learn on the\ngraph data and have achieved the state-of-the-art on node and graph\nclassification tasks. However, recent works show GNNs are vulnerable to\ntraining-time poisoning attacks -- marginally perturbing edges, nodes, or/and\nnode features of training graph(s) can largely degrade GNNs' testing\nperformance. Most previous defenses against graph poisoning attacks are\nempirical and are soon broken by adaptive / stronger ones. A few provable\ndefenses provide robustness guarantees, but have large gaps when applied in\npractice: 1) restrict the attacker on only one type of perturbation; 2) design\nfor a particular GNN architecture or task; and 3) robustness guarantees are not\n100\\% accurate.\n  In this work, we bridge all these gaps by developing PGNNCert, the first\ncertified defense of GNNs against poisoning attacks under arbitrary (edge,\nnode, and node feature) perturbations with deterministic robustness guarantees.\nExtensive evaluations on multiple node and graph classification datasets and\nGNNs demonstrate the effectiveness of PGNNCert to provably defend against\narbitrary poisoning perturbations. PGNNCert is also shown to significantly\noutperform the state-of-the-art certified defenses against edge perturbation or\nnode perturbation during GNN training."
    },
    {
        "date": "2025-03",
        "title": "Large Language Models powered Network Attack Detection: Architecture, Opportunities and Case Study",
        "author": "Xinggong Zhang, Qingyang Li, Yunpeng Tan, Zongming Guo, Lei Zhang, and Yong Cui",
        "link": "http://arxiv.org/abs/2503.18487v1",
        "abstract": "Network attack detection is a pivotal technology to identify network anomaly\nand classify malicious traffic. Large Language Models (LLMs) are trained on a\nvast corpus of text, have amassed remarkable capabilities of\ncontext-understanding and commonsense knowledge. This has opened up a new door\nfor network threat detection. Researchers have already initiated discussions\nregarding the application of LLMs on specific cyber-security tasks.\nUnfortunately, there is still a lack of comprehensive elaboration how to mine\nLLMs' potentials in network threat detections, as well as the opportunities and\nchallenges. In this paper, we mainly focus on the classification of malicious\ntraffic from the perspective of LLMs' capability. We present a holistic view of\nthe architecture of LLM-powered network attack detection, including\nPre-training, Fine-tuning, and Detection. Especially, by exploring the\nknowledge and capabilities of LLM, we identify three distinct roles LLM can act\nin network attack detection: \\textit{Classifier, Encoder, and Predictor}. For\neach of them, the modeling paradigm, opportunities and challenges are\nelaborated. Finally, we present our design on LLM-powered DDoS detection as a\ncase study. The proposed framework attains accurate detection on carpet bombing\nDDoS by exploiting LLMs' capabilities in contextual mining. The evaluation\nshows its efficacy, exhibiting a nearly $35$\\% improvement compared to existing\nsystems."
    },
    {
        "date": "2025-03",
        "title": "Benchmarking Multi-modal Semantic Segmentation under Sensor Failures: Missing and Noisy Modality Robustness",
        "author": "Chenfei Liao, Kaiyu Lei, Xu Zheng, Junha Moon, Zhixiong Wang, Yixuan Wang, Danda Pani Paudel, Luc Van Gool, and Xuming Hu",
        "link": "http://arxiv.org/abs/2503.18445v1",
        "abstract": "Multi-modal semantic segmentation (MMSS) addresses the limitations of\nsingle-modality data by integrating complementary information across\nmodalities. Despite notable progress, a significant gap persists between\nresearch and real-world deployment due to variability and uncertainty in\nmulti-modal data quality. Robustness has thus become essential for practical\nMMSS applications. However, the absence of standardized benchmarks for\nevaluating robustness hinders further advancement. To address this, we first\nsurvey existing MMSS literature and categorize representative methods to\nprovide a structured overview. We then introduce a robustness benchmark that\nevaluates MMSS models under three scenarios: Entire-Missing Modality (EMM),\nRandom-Missing Modality (RMM), and Noisy Modality (NM). From a probabilistic\nstandpoint, we model modality failure under two conditions: (1) all damaged\ncombinations are equally probable; (2) each modality fails independently\nfollowing a Bernoulli distribution. Based on these, we propose four\nmetrics-$mIoU^{Avg}_{EMM}$, $mIoU^{E}_{EMM}$, $mIoU^{Avg}_{RMM}$, and\n$mIoU^{E}_{RMM}$-to assess model robustness under EMM and RMM. This work\nprovides the first dedicated benchmark for MMSS robustness, offering new\ninsights and tools to advance the field. Source code is available at\nhttps://github.com/Chenfei-Liao/Multi-Modal-Semantic-Segmentation-Robustness-Benchmark."
    },
    {
        "date": "2025-03",
        "title": "Distributionally Robust Federated Learning: An ADMM Algorithm",
        "author": "Wen Bai, Yi Wong, Xiao Qiao, and Chin Pang Ho",
        "link": "http://arxiv.org/abs/2503.18436v1",
        "abstract": "Federated learning (FL) aims to train machine learning (ML) models\ncollaboratively using decentralized data, bypassing the need for centralized\ndata aggregation. Standard FL models often assume that all data come from the\nsame unknown distribution. However, in practical situations, decentralized data\nfrequently exhibit heterogeneity. We propose a novel FL model, Distributionally\nRobust Federated Learning (DRFL), that applies distributionally robust\noptimization to overcome the challenges posed by data heterogeneity and\ndistributional ambiguity. We derive a tractable reformulation for DRFL and\ndevelop a novel solution method based on the alternating direction method of\nmultipliers (ADMM) algorithm to solve this problem. Our experimental results\ndemonstrate that DRFL outperforms standard FL models under data heterogeneity\nand ambiguity."
    },
    {
        "date": "2025-03",
        "title": "RoCA: Robust Contrastive One-class Time Series Anomaly Detection with Contaminated Data",
        "author": "Xudong Mou, Rui Wang, Bo Li, Tianyu Wo, Jie Sun, Hui Wang, and Xudong Liu",
        "link": "http://arxiv.org/abs/2503.18385v1",
        "abstract": "The accumulation of time-series signals and the absence of labels make\ntime-series Anomaly Detection (AD) a self-supervised task of deep learning.\nMethods based on normality assumptions face the following three limitations:\n(1) A single assumption could hardly characterize the whole normality or lead\nto some deviation. (2) Some assumptions may go against the principle of AD. (3)\nTheir basic assumption is that the training data is uncontaminated (free of\nanomalies), which is unrealistic in practice, leading to a decline in\nrobustness. This paper proposes a novel robust approach, RoCA, which is the\nfirst to address all of the above three challenges, as far as we are aware. It\nfuses the separated assumptions of one-class classification and contrastive\nlearning in a single training process to characterize a more complete so-called\nnormality. Additionally, it monitors the training data and computes a carefully\ndesigned anomaly score throughout the training process. This score helps\nidentify latent anomalies, which are then used to define the classification\nboundary, inspired by the concept of outlier exposure. The performance on AIOps\ndatasets improved by 6% compared to when contamination was not considered\n(COCA). On two large and high-dimensional multivariate datasets, the\nperformance increased by 5% to 10%. RoCA achieves the highest average\nperformance on both univariate and multivariate datasets. The source code is\navailable at https://github.com/ruiking04/RoCA."
    },
    {
        "date": "2025-03",
        "title": "Attacking and Improving the Tor Directory Protocol",
        "author": "Zhongtang Luo, Adithya Bhat, Kartik Nayak, and Aniket Kate",
        "link": "http://arxiv.org/abs/2503.18345v1",
        "abstract": "The Tor network enhances clients' privacy by routing traffic through an\noverlay network of volunteered intermediate relays. Tor employs a distributed\nprotocol among nine hard-coded Directory Authority (DA) servers to securely\ndisseminate information about these relays to produce a new consensus document\nevery hour. With a straightforward voting mechanism to ensure consistency, the\nprotocol is expected to be secure even when a minority of those authorities get\ncompromised. However, the current consensus protocol is flawed: it allows an\nequivocation attack that enables only a single compromised authority to create\na valid consensus document with malicious relays. Importantly the vulnerability\nis not innocuous: We demonstrate that the compromised authority can effectively\ntrick a targeted client into using the equivocated consensus document in an\nundetectable manner. Moreover, even if we have archived Tor consensus documents\navailable since its beginning, we cannot be sure that no client was ever\ntricked.\n  We propose a two-stage solution to deal with this exploit. In the short term,\nwe have developed and deployed TorEq, a monitor to detect such exploits\nreactively: the Tor clients can refer to the monitor before updating the\nconsensus to ensure no equivocation. To solve the problem proactively, we first\ndefine the Tor DA consensus problem as the interactive consistency (IC) problem\nfrom the distributed computing literature. We then design DirCast, a novel\nsecure Byzantine Broadcast protocol that requires minimal code change from the\ncurrent Tor DA code base. Our protocol has near-optimal efficiency that uses\noptimistically five rounds and at most nine rounds to reach an agreement in the\ncurrent nine-authority system. We are communicating with the Tor security team\nto incorporate the solutions into the Tor project."
    },
    {
        "date": "2025-03",
        "title": "PS-EIP: Robust Photometric Stereo Based on Event Interval Profile",
        "author": "Kazuma Kitazawa, Takahito Aoto, Satoshi Ikehata, and Tsuyoshi Takatani",
        "link": "http://arxiv.org/abs/2503.18341v1",
        "abstract": "Recently, the energy-efficient photometric stereo method using an event\ncamera has been proposed to recover surface normals from events triggered by\nchanges in logarithmic Lambertian reflections under a moving directional light\nsource. However, EventPS treats each event interval independently, making it\nsensitive to noise, shadows, and non-Lambertian reflections. This paper\nproposes Photometric Stereo based on Event Interval Profile (PS-EIP), a robust\nmethod that recovers pixelwise surface normals from a time-series profile of\nevent intervals. By exploiting the continuity of the profile and introducing an\noutlier detection method based on profile shape, our approach enhances\nrobustness against outliers from shadows and specular reflections. Experiments\nusing real event data from 3D-printed objects demonstrate that PS-EIP\nsignificantly improves robustness to outliers compared to EventPS's\ndeep-learning variant, EventPS-FCN, without relying on deep learning."
    },
    {
        "date": "2025-03",
        "title": "When is dataset cartography ineffective? Using training dynamics does not improve robustness against Adversarial SQuAD",
        "author": "Paul K. Mandal",
        "link": "http://arxiv.org/abs/2503.18290v1",
        "abstract": "In this paper, I investigate the effectiveness of dataset cartography for\nextractive question answering on the SQuAD dataset. I begin by analyzing\nannotation artifacts in SQuAD and evaluate the impact of two adversarial\ndatasets, AddSent and AddOneSent, on an ELECTRA-small model. Using training\ndynamics, I partition SQuAD into easy-to-learn, ambiguous, and hard-to-learn\nsubsets. I then compare the performance of models trained on these subsets to\nthose trained on randomly selected samples of equal size. Results show that\ntraining on cartography-based subsets does not improve generalization to the\nSQuAD validation set or the AddSent adversarial set. While the hard-to-learn\nsubset yields a slightly higher F1 score on the AddOneSent dataset, the overall\ngains are limited. These findings suggest that dataset cartography provides\nlittle benefit for adversarial robustness in SQuAD-style QA tasks. I conclude\nby comparing these results to prior findings on SNLI and discuss possible\nreasons for the observed differences."
    },
    {
        "date": "2025-03",
        "title": "Enhance GNNs with Reliable Confidence Estimation via Adversarial Calibration Learning",
        "author": "Yilong Wang, Jiahao Zhang, Tianxiang Zhao, and Suhang Wang",
        "link": "http://arxiv.org/abs/2503.18235v1",
        "abstract": "Despite their impressive predictive performance, GNNs often exhibit poor\nconfidence calibration, i.e., their predicted confidence scores do not\naccurately reflect true correctness likelihood. This issue raises concerns\nabout their reliability in high-stakes domains such as fraud detection, and\nrisk assessment, where well-calibrated predictions are essential for\ndecision-making. To ensure trustworthy predictions, several GNN calibration\nmethods are proposed. Though they can improve global calibration, our\nexperiments reveal that they often fail to generalize across different node\ngroups, leading to inaccurate confidence in node groups with different degree\nlevels, classes, and local structures. In certain cases, they even degrade\ncalibration compared to the original uncalibrated GNN. To address this\nchallenge, we propose a novel AdvCali framework that adaptively enhances\ncalibration across different node groups. Our method leverages adversarial\ntraining to automatically identify mis-calibrated node groups and applies a\ndifferentiable Group Expected Calibration Error (ECE) loss term to refine\nconfidence estimation within these groups. This allows the model to dynamically\nadjust its calibration strategy without relying on dataset-specific prior\nknowledge about miscalibrated subgroups. Extensive experiments on real-world\ndatasets demonstrate that our approach not only improves global calibration but\nalso significantly enhances calibration within groups defined by feature\nsimilarity, topology, and connectivity, outperforming previous methods and\ndemonstrating its effectiveness in practical scenarios."
    },
    {
        "date": "2025-03",
        "title": "Literature Review: Cyber Security Monitoring in Maritime",
        "author": "Risto Vaarandi, Leonidas Tsiopoulos, Gabor Visky, Muaan Ur Rehman, and Hayretdin Bahsi",
        "link": "http://arxiv.org/abs/2503.18173v1",
        "abstract": "In recent years, many cyber incidents have happened in the maritime sector,\ntargeting the information technology (IT) and operational technology (OT)\ninfrastructure. Although several systematization-of-knowledge papers have been\npublished in the maritime field, none of the previous studies has focused on\ncyber security monitoring, which aims at timely detection of cyber attacks with\nautomated methods. The current article addresses this research gap and surveys\nthe methods, algorithms, tools and architectures used for cyber security\nmonitoring in the maritime sector. For the survey, a systematic literature\nreview of cyber security monitoring studies is conducted in this article,\nfollowing the Preferred Reporting Items for Systematic Reviews and\nMeta-Analyses (PRISMA) protocol. The first contribution of this article is the\nbibliometric analysis of related literature and the identification of the main\nresearch themes in previous works. For that purpose, our article presents a\ntaxonomy for existing studies which highlights the main properties of maritime\ncyber security monitoring research. The second contribution of this article is\nan in-depth analysis of previous works and the identification of research gaps\nand limitations in existing literature. Based on our findings, we outline\nfuture research directions for cyber security monitoring in the maritime field."
    },
    {
        "date": "2025-03",
        "title": "Metaphor-based Jailbreaking Attacks on Text-to-Image Models",
        "author": "Chenyu Zhang, Yiwen Ma, Lanjun Wang, Wenhui Li, Yi Tu, and An-An Liu",
        "link": "http://arxiv.org/abs/2503.17987v1",
        "abstract": "To mitigate misuse, text-to-image~(T2I) models commonly incorporate safety\nfilters to prevent the generation of sensitive images. Unfortunately, recent\njailbreaking attack methods use LLMs to generate adversarial prompts that\neffectively bypass safety filters while generating sensitive images, revealing\nthe safety vulnerabilities within the T2I model. However, existing LLM-based\nattack methods lack explicit guidance, relying on substantial queries to\nachieve a successful attack, which limits their practicality in real-world\nscenarios. In this work, we introduce \\textbf{MJA}, a \\textbf{m}etaphor-based\n\\textbf{j}ailbreaking \\textbf{a}ttack method inspired by the Taboo game, aiming\nto balance the attack effectiveness and query efficiency by generating\nmetaphor-based adversarial prompts. Specifically, MJA consists of two modules:\nan LLM-based multi-agent generation module~(MLAG) and an adversarial prompt\noptimization module~(APO). MLAG decomposes the generation of metaphor-based\nadversarial prompts into three subtasks: metaphor retrieval, context matching,\nand adversarial prompt generation. Subsequently, MLAG coordinates three\nLLM-based agents to generate diverse adversarial prompts by exploring various\nmetaphors and contexts. To enhance the attack efficiency, APO first trains a\nsurrogate model to predict the attack results of adversarial prompts and then\ndesigns an acquisition strategy to adaptively identify optimal adversarial\nprompts. Experiments demonstrate that MJA achieves better attack effectiveness\nwhile requiring fewer queries compared to baseline methods. Moreover, our\nadversarial prompts exhibit strong transferability across various open-source\nand commercial T2I models. \\textcolor{red}{This paper includes model-generated\ncontent that may contain offensive or distressing material.}"
    },
    {
        "date": "2025-03",
        "title": "Generative Data Imputation for Sparse Learner Performance Data Using Generative Adversarial Imputation Networks",
        "author": "Liang Zhang, Jionghao Lin, John Sabatini, Diego Zapata-Rivera, Carol Forsyth, Yang Jiang, John Hollander, Xiangen Hu, and Arthur C. Graesser",
        "link": "http://arxiv.org/abs/2503.18982v1",
        "abstract": "Learner performance data collected by Intelligent Tutoring Systems (ITSs),\nsuch as responses to questions, is essential for modeling and predicting\nlearners' knowledge states. However, missing responses due to skips or\nincomplete attempts create data sparsity, challenging accurate assessment and\npersonalized instruction. To address this, we propose a generative imputation\napproach using Generative Adversarial Imputation Networks (GAIN). Our method\nfeatures a three-dimensional (3D) framework (learners, questions, and\nattempts), flexibly accommodating various sparsity levels. Enhanced by\nconvolutional neural networks and optimized with a least squares loss function,\nthe GAIN-based method aligns input and output dimensions to question-attempt\nmatrices along the learners' dimension. Extensive experiments using datasets\nfrom AutoTutor Adult Reading Comprehension (ARC), ASSISTments, and MATHia\ndemonstrate that our approach significantly outperforms tensor factorization\nand alternative GAN methods in imputation accuracy across different attempt\nscenarios. Bayesian Knowledge Tracing (BKT) further validates the effectiveness\nof the imputed data by estimating learning parameters: initial knowledge\n(P(L0)), learning rate (P(T)), guess rate (P(G)), and slip rate (P(S)). Results\nindicate the imputed data enhances model fit and closely mirrors original\ndistributions, capturing underlying learning behaviors reliably.\nKullback-Leibler (KL) divergence assessments confirm minimal divergence,\nshowing the imputed data preserves essential learning characteristics\neffectively. These findings underscore GAIN's capability as a robust imputation\ntool in ITSs, alleviating data sparsity and supporting adaptive, individualized\ninstruction, ultimately leading to more precise and responsive learner\nassessments and improved educational outcomes."
    },
    {
        "date": "2025-03",
        "title": "FisherTune: Fisher-Guided Robust Tuning of Vision Foundation Models for Domain Generalized Segmentation",
        "author": "Dong Zhao, Jinlong Li, Shuang Wang, Mengyao Wu, Qi Zang, Nicu Sebe, and Zhun Zhong",
        "link": "http://arxiv.org/abs/2503.17940v2",
        "abstract": "Vision Foundation Models (VFMs) excel in generalization due to large-scale\npretraining, but fine-tuning them for Domain Generalized Semantic Segmentation\n(DGSS) while maintaining this ability remains challenging. Existing approaches\neither selectively fine-tune parameters or freeze the VFMs and update only the\nadapters, both of which may underutilize the VFMs' full potential in DGSS\ntasks. We observe that domain-sensitive parameters in VFMs, arising from task\nand distribution differences, can hinder generalization. To address this, we\npropose \\textbf{FisherTune}, a robust fine-tuning method guided by the\nDomain-Related Fisher Information Matrix (DR-FIM). DR-FIM measures parameter\nsensitivity across tasks and domains, enabling selective updates that preserve\ngeneralization and enhance DGSS adaptability. FisherTune incorporates\nvariational inference to stabilize DR-FIM estimation, treating parameters as\nGaussian-distributed variables and leveraging pre-trained priors. Extensive\nexperiments show that FisherTune achieves superior cross-domain segmentation\nwhile maintaining generalization, outperforming selective-parameter and\nadapter-based methods."
    },
    {
        "date": "2025-03",
        "title": "Understanding and Mitigating Side and Covert Channel Vulnerabilities Introduced by RowHammer Defenses",
        "author": "F. Nisa Bostanc\u0131, O\u011fuzhan Canpolat, Ataberk Olgun, \u0130smail Emir Y\u00fcksel, Mohammad Sadrosadati, A. Giray Ya\u011fl\u0131k\u00e7\u0131, and Onur Mutlu",
        "link": "http://arxiv.org/abs/2503.17891v1",
        "abstract": "DRAM chips are vulnerable to read disturbance phenomena (e.g., RowHammer and\nRowPress), where repeatedly accessing or keeping open a DRAM row causes\nbitflips in nearby rows, due to DRAM density scaling. Attackers can leverage\nRowHammer bitflips in real systems to take over systems and leak data.\nConsequently, many prior works propose mitigations, including recent DDR\nspecifications introducing new mitigation frameworks (e.g., PRAC and RFM). For\nrobustness, it is timely and critical to analyze other security implications\nthat widely-adopted RowHammer mitigations can introduce. Unfortunately, no\nprior work analyzes the timing channel vulnerabilities introduced by RowHammer\nmitigations. In this work, we present the first analysis and evaluation of\ntiming channel vulnerabilities introduced by RowHammer mitigations. Our key\nobservation is that RowHammer mitigations' preventive actions have two features\nthat enable timing channels. First, preventive actions often reduce DRAM\nbandwidth availability because they block access to DRAM, thereby delaying\nregular memory requests and resulting in increased memory latencies. Second,\npreventive actions can be triggered on demand as they depend on memory access\npatterns. We systematically analyze two latest industry mitigations and\nintroduce LeakyHammer, a new class of attacks that leverage the RowHammer\nmitigation-induced memory latency differences to establish communication\nchannels between processes and leak secrets. First, we build two covert channel\nattacks exploiting two state-of-the-art RowHammer mitigations, providing 41.9\nKbps and 54.0 Kbps channel capacity. Second, we demonstrate a proof-of-concept\nwebsite fingerprinting attack that can identify visited websites based on the\nRowHammer mitigation behavior. We discuss 3 mitigations against LeakyHammer and\nshow that fundamentally mitigating LeakyHammer induces significant performance\noverheads."
    },
    {
        "date": "2025-03",
        "title": "Detecting and Mitigating DDoS Attacks with AI: A Survey",
        "author": "Alexandru Apostu, Silviu Gheorghe, Andrei H\u00eeji, Nicolae Cleju, Andrei P\u0103tra\u015fcu, Cristian Rusu, Radu Ionescu, and Paul Irofti",
        "link": "http://arxiv.org/abs/2503.17867v1",
        "abstract": "Distributed Denial of Service attacks represent an active cybersecurity\nresearch problem. Recent research shifted from static rule-based defenses\ntowards AI-based detection and mitigation. This comprehensive survey covers\nseveral key topics. Preeminently, state-of-the-art AI detection methods are\ndiscussed. An in-depth taxonomy based on manual expert hierarchies and an\nAI-generated dendrogram are provided, thus settling DDoS categorization\nambiguities. An important discussion on available datasets follows, covering\ndata format options and their role in training AI detection methods together\nwith adversarial training and examples augmentation. Beyond detection, AI based\nmitigation techniques are surveyed as well. Finally, multiple open research\ndirections are proposed."
    },
    {
        "date": "2025-03",
        "title": "NVBleed: Covert and Side-Channel Attacks on NVIDIA Multi-GPU Interconnect",
        "author": "Yicheng Zhang, Ravan Nazaraliyev, Sankha Baran Dutta, Andres Marquez, Kevin Barker, and Nael Abu-Ghazaleh",
        "link": "http://arxiv.org/abs/2503.17847v1",
        "abstract": "Multi-GPU systems are becoming increasingly important in highperformance\ncomputing (HPC) and cloud infrastructure, providing acceleration for\ndata-intensive applications, including machine learning workloads. These\nsystems consist of multiple GPUs interconnected through high-speed networking\nlinks such as NVIDIA's NVLink. In this work, we explore whether the\ninterconnect on such systems can offer a novel source of leakage, enabling new\nforms of covert and side-channel attacks. Specifically, we reverse engineer the\noperations of NVlink and identify two primary sources of leakage: timing\nvariations due to contention and accessible performance counters that disclose\ncommunication patterns. The leakage is visible remotely and even across VM\ninstances in the cloud, enabling potentially dangerous attacks. Building on\nthese observations, we develop two types of covert-channel attacks across two\nGPUs, achieving a bandwidth of over 70 Kbps with an error rate of 4.78% for the\ncontention channel. We develop two end-to-end crossGPU side-channel attacks:\napplication fingerprinting (including 18 high-performance computing and deep\nlearning applications) and 3D graphics character identification within Blender,\na multi-GPU rendering application. These attacks are highly effective,\nachieving F1 scores of up to 97.78% and 91.56%, respectively. We also discover\nthat leakage surprisingly occurs across Virtual Machines on the Google Cloud\nPlatform (GCP) and demonstrate a side-channel attack on Blender, achieving F1\nscores exceeding 88%. We also explore potential defenses such as managing\naccess to counters and reducing the resolution of the clock to mitigate the two\nsources of leakage."
    },
    {
        "date": "2025-03",
        "title": "Connectedness: a dimension of security bug severity assessment for measuring uncertainty",
        "author": "Shue Long Chan",
        "link": "http://arxiv.org/abs/2503.17813v1",
        "abstract": "Current frameworks for evaluating security bug severity, such as the Common\nVulnerability Scoring System (CVSS), prioritize the ratio of exploitability to\nimpact. This paper suggests that the above approach measures the \"known knowns\"\nbut inadequately addresses the \"known unknowns\" especially when there exist\nmultiple possible exploit paths and side effects, which introduce significant\nuncertainty. This paper introduces the concept of connectedness, which measures\nhow strongly a security bug is connected with different entities, thereby\nreflecting the uncertainty of impact and the exploit potential. This work\nhighlights the critical but underappreciated role connectedness plays in\nseverity assessments."
    },
    {
        "date": "2025-03",
        "title": "Design and implementation of a novel cryptographically secure pseudorandom number generator",
        "author": "Juan Di Mauro, Eduardo Salazar, and Hugo D. Scolnik",
        "link": "http://arxiv.org/abs/2503.17767v1",
        "abstract": "The aim of this paper is to present a new design for a pseudorandom number\ngenerator (PRNG) that is cryptographically secure, passes all of the usual\nstatistical tests referenced in the literature and hence generates high quality\nrandom sequences, that is compact and easy to implement in practice, of\nportable design and offering reasonable execution times. Our procedure achieves\nthose objectives through the use of a sequence of modular exponentiations\nfollowed by the application of Feistel-like boxes that mix up bits using a\nnonlinear function. The results of extensive statistical tests on sequences of\nabout 2^40 bits in size generated by our algorithm are also presented."
    },
    {
        "date": "2025-03",
        "title": "Towards Invisible Backdoor Attack on Text-to-Image Diffusion Model",
        "author": "Jie Zhang, Zhongqi Wang, Shiguang Shan, and Xilin Chen",
        "link": "http://arxiv.org/abs/2503.17724v1",
        "abstract": "Backdoor attacks targeting text-to-image diffusion models have advanced\nrapidly, enabling attackers to implant malicious triggers into these models to\nmanipulate their outputs. However, current backdoor samples often exhibit two\nkey abnormalities compared to benign samples: 1) Semantic Consistency, where\nbackdoor prompts tend to generate images with similar semantic content even\nwith significant textual variations to the prompts; 2) Attention Consistency,\nwhere the trigger induces consistent structural responses in the\ncross-attention maps. These consistencies leave detectable traces for\ndefenders, making backdoors easier to identify. To enhance the stealthiness of\nbackdoor samples, we propose a novel Invisible Backdoor Attack (IBA) by\nexplicitly mitigating these consistencies. Specifically, our approach leverages\nsyntactic structures as backdoor triggers to amplify the sensitivity to textual\nvariations, effectively breaking down the semantic consistency. Besides, a\nregularization method based on Kernel Maximum Mean Discrepancy (KMMD) is\nproposed to align the distribution of cross-attention responses between\nbackdoor and benign samples, thereby disrupting attention consistency.\nExtensive experiments demonstrate that our IBA achieves a 97.5% attack success\nrate while exhibiting stronger resistance to defenses, with an average of over\n98% backdoor samples bypassing three state-of-the-art detection mechanisms. The\ncode is available at https://github.com/Robin-WZQ/IBA."
    },
    {
        "date": "2025-03",
        "title": "Measuring the Robustness of Audio Deepfake Detectors",
        "author": "Xiang Li, Pin-Yu Chen, and Wenqi Wei",
        "link": "http://arxiv.org/abs/2503.17577v1",
        "abstract": "Deepfakes have become a universal and rapidly intensifying concern of\ngenerative AI across various media types such as images, audio, and videos.\nAmong these, audio deepfakes have been of particular concern due to the ease of\nhigh-quality voice synthesis and distribution via platforms such as social\nmedia and robocalls. Consequently, detecting audio deepfakes plays a critical\nrole in combating the growing misuse of AI-synthesized speech. However,\nreal-world scenarios often introduce various audio corruptions, such as noise,\nmodification, and compression, that may significantly impact detection\nperformance. This work systematically evaluates the robustness of 10 audio\ndeepfake detection models against 16 common corruptions, categorized into noise\nperturbation, audio modification, and compression. Using both traditional deep\nlearning models and state-of-the-art foundation models, we make four unique\nobservations. First, our findings show that while most models demonstrate\nstrong robustness to noise, they are notably more vulnerable to modifications\nand compression, especially when neural codecs are applied. Second, speech\nfoundation models generally outperform traditional models across most\nscenarios, likely due to their self-supervised learning paradigm and\nlarge-scale pre-training. Third, our results show that increasing model size\nimproves robustness, albeit with diminishing returns. Fourth, we demonstrate\nhow targeted data augmentation during training can enhance model resilience to\nunseen perturbations. A case study on political speech deepfakes highlights the\neffectiveness of foundation models in achieving high accuracy under real-world\nconditions. These findings emphasize the importance of developing more robust\ndetection frameworks to ensure reliability in practical deployment settings."
    },
    {
        "date": "2025-03",
        "title": "Time-Series U-Net with Recurrence for Noise-Robust Imaging Photoplethysmography",
        "author": "Vineet R. Shenoy, Shaoju Wu, Armand Comas, Tim K. Marks, Suhas Lohit, and Hassan Mansour",
        "link": "http://arxiv.org/abs/2503.17351v1",
        "abstract": "Remote estimation of vital signs enables health monitoring for situations in\nwhich contact-based devices are either not available, too intrusive, or too\nexpensive. In this paper, we present a modular, interpretable pipeline for\npulse signal estimation from video of the face that achieves state-of-the-art\nresults on publicly available datasets.Our imaging photoplethysmography (iPPG)\nsystem consists of three modules: face and landmark detection, time-series\nextraction, and pulse signal/pulse rate estimation. Unlike many deep learning\nmethods that make use of a single black-box model that maps directly from input\nvideo to output signal or heart rate, our modular approach enables each of the\nthree parts of the pipeline to be interpreted individually. The pulse signal\nestimation module, which we call TURNIP (Time-Series U-Net with Recurrence for\nNoise-Robust Imaging Photoplethysmography), allows the system to faithfully\nreconstruct the underlying pulse signal waveform and uses it to measure heart\nrate and pulse rate variability metrics, even in the presence of motion. When\nparts of the face are occluded due to extreme head poses, our system explicitly\ndetects such \"self-occluded\" regions and maintains estimation robustness\ndespite the missing information. Our algorithm provides reliable heart rate\nestimates without the need for specialized sensors or contact with the skin,\noutperforming previous iPPG methods on both color (RGB) and near-infrared (NIR)\ndatasets."
    },
    {
        "date": "2025-03",
        "title": "Bugdar: AI-Augmented Secure Code Review for GitHub Pull Requests",
        "author": "John Naulty, Eason Chen, Joy Wang, George Digkas, and Kostas Chalkias",
        "link": "http://arxiv.org/abs/2503.17302v1",
        "abstract": "As software systems grow increasingly complex, ensuring security during\ndevelopment poses significant challenges. Traditional manual code audits are\noften expensive, time-intensive, and ill-suited for fast-paced workflows, while\nautomated tools frequently suffer from high false-positive rates, limiting\ntheir reliability. To address these issues, we introduce Bugdar, an\nAI-augmented code review system that integrates seamlessly into GitHub pull\nrequests, providing near real-time, context-aware vulnerability analysis.\nBugdar leverages fine-tunable Large Language Models (LLMs) and Retrieval\nAugmented Generation (RAGs) to deliver project-specific, actionable feedback\nthat aligns with each codebase's unique requirements and developer practices.\nSupporting multiple programming languages, including Solidity, Move, Rust, and\nPython, Bugdar demonstrates exceptional efficiency, processing an average of\n56.4 seconds per pull request or 30 lines of code per second. This is\nsignificantly faster than manual reviews, which could take hours per pull\nrequest. By facilitating a proactive approach to secure coding, Bugdar reduces\nthe reliance on manual reviews, accelerates development cycles, and enhances\nthe security posture of software systems without compromising productivity."
    },
    {
        "date": "2025-03",
        "title": "UAV Resilience Against Stealthy Attacks",
        "author": "Arthur Amorim, Max Taylor, Trevor Kann, Gary T. Leavens, William L. Harrison, and Lance Joneckis",
        "link": "http://arxiv.org/abs/2503.17298v1",
        "abstract": "Unmanned aerial vehicles (UAVs) depend on untrusted software components to\nautomate dangerous or critical missions, making them a desirable target for\nattacks. Some work has been done to prevent an attacker who has either\ncompromised a ground control station or parts of a UAV's software from\nsabotaging the vehicle, but not both. We present an architecture running a UAV\nsoftware stack with runtime monitoring and seL4-based software isolation that\nprevents attackers from both exploiting software bugs and utilizing stealthy\nattacks. Our architecture retrofits legacy UAVs and secures the popular MAVLink\nprotocol, making wide adoption possible."
    },
    {
        "date": "2025-03",
        "title": "Calibration Strategies for Robust Causal Estimation: Theoretical and Empirical Insights on Propensity Score Based Estimators",
        "author": "Jan Rabenseifner, Sven Klaassen, Jannis Kueck, and Philipp Bach",
        "link": "http://arxiv.org/abs/2503.17290v2",
        "abstract": "The partitioning of data for estimation and calibration critically impacts\nthe performance of propensity score based estimators like inverse probability\nweighting (IPW) and double/debiased machine learning (DML) frameworks. We\nextend recent advances in calibration techniques for propensity score\nestimation, improving the robustness of propensity scores in challenging\nsettings such as limited overlap, small sample sizes, or unbalanced data. Our\ncontributions are twofold: First, we provide a theoretical analysis of the\nproperties of calibrated estimators in the context of DML. To this end, we\nrefine existing calibration frameworks for propensity score models, with a\nparticular emphasis on the role of sample-splitting schemes in ensuring valid\ncausal inference. Second, through extensive simulations, we show that\ncalibration reduces variance of inverse-based propensity score estimators while\nalso mitigating bias in IPW, even in small-sample regimes. Notably, calibration\nimproves stability for flexible learners (e.g., gradient boosting) while\npreserving the doubly robust properties of DML. A key insight is that, even\nwhen methods perform well without calibration, incorporating a calibration step\ndoes not degrade performance, provided that an appropriate sample-splitting\napproach is chosen."
    },
    {
        "date": "2025-03",
        "title": "Cyber Campaign Fractals -- Geometric Analysis of Hierarchical Cyber Attack Taxonomies",
        "author": "Ronan Mouchoux, and Fran\u00e7ois Moerman",
        "link": "http://arxiv.org/abs/2503.17219v1",
        "abstract": "This paper introduces a novel mathematical framework for analyzing cyber\nthreat campaigns through fractal geometry. By conceptualizing hierarchical\ntaxonomies (MITRE ATT&CK, DISARM) as snowflake-like structures with tactics,\ntechniques, and sub-techniques forming concentric layers, we establish a\nrigorous method for campaign comparison using Hutchinson's Theorem and\nHausdorff distance metrics. Evaluation results confirm that our fractal\nrepresentation preserves hierarchical integrity while providing a\ndimensionality-based complexity assessment that correlates with campaign\ncomplexity. The proposed methodology bridges taxonomy-driven cyber threat\nanalysis and computational geometry, providing analysts with both mathematical\nrigor and interpretable visualizations for addressing the growing complexity of\nadversarial operations across multiple threat domains."
    },
    {
        "date": "2025-03",
        "title": "A Language Anchor-Guided Method for Robust Noisy Domain Generalization",
        "author": "Zilin Dai, Lehong Wang, Fangzhou Lin, Yidong Wang, Zhigang Li, Kazunori D Yamada, Ziming Zhang, and Wang Lu",
        "link": "http://arxiv.org/abs/2503.17211v1",
        "abstract": "Real-world machine learning applications often struggle with two major\nchallenges: distribution shift and label noise. Models tend to overfit by\nfocusing on redundant and uninformative features in the training data, which\nmakes it hard for them to generalize to the target domain. Noisy data worsens\nthis problem by causing further overfitting to the noise, meaning that existing\nmethods often fail to tell the difference between true, invariant features and\nmisleading, spurious ones. To tackle these issues, we introduce Anchor\nAlignment and Adaptive Weighting (A3W). This new algorithm uses sample\nreweighting guided by natural language processing (NLP) anchors to extract more\nrepresentative features. In simple terms, A3W leverages semantic\nrepresentations from natural language models as a source of domain-invariant\nprior knowledge. Additionally, it employs a weighted loss function that adjusts\neach sample's contribution based on its similarity to the corresponding NLP\nanchor. This adjustment makes the model more robust to noisy labels. Extensive\nexperiments on standard benchmark datasets show that A3W consistently\noutperforms state-of-the-art domain generalization methods, offering\nsignificant improvements in both accuracy and robustness across different\ndatasets and noise levels."
    },
    {
        "date": "2025-03",
        "title": "Robustness of deep learning classification to adversarial input on GPUs: asynchronous parallel accumulation is a source of vulnerability",
        "author": "Sanjif Shanmugavelu, Mathieu Taillefumier, Christopher Culver, Vijay Ganesh, Oscar Hernandez, and Ada Sedova",
        "link": "http://arxiv.org/abs/2503.17173v1",
        "abstract": "The ability of machine learning (ML) classification models to resist small,\ntargeted input perturbations - known as adversarial attacks - is a key measure\nof their safety and reliability. We show that floating-point non associativity\n(FPNA) coupled with asynchronous parallel programming on GPUs is sufficient to\nresult in misclassification, without any perturbation to the input.\nAdditionally, we show this misclassification is particularly significant for\ninputs close to the decision boundary and that standard adversarial robustness\nresults may be overestimated up to 4.6% when not considering machine-level\ndetails. We first study a linear classifier, before focusing on standard Graph\nNeural Network (GNN) architectures and datasets. We present a novel black-box\nattack using Bayesian optimization to determine external workloads that bias\nthe output of reductions on GPUs and reliably lead to misclassification.\nMotivated by these results, we present a new learnable permutation (LP)\ngradient-based approach, to learn floating point operation orderings that lead\nto misclassifications, making the assumption that any reduction or permutation\nordering is possible. This LP approach provides a worst-case estimate in a\ncomputationally efficient manner, avoiding the need to run identical\nexperiments tens of thousands of times over a potentially large set of possible\nGPU states or architectures. Finally, we investigate parallel reduction\nordering across different GPU architectures for a reduction under three\nconditions: (1) executing external background workloads, (2) utilizing\nmulti-GPU virtualization, and (3) applying power capping. Our results\ndemonstrate that parallel reduction ordering varies significantly across\narchitectures under the first two conditions. The results and methods developed\nhere can help to include machine-level considerations into adversarial\nrobustness assessments."
    },
    {
        "date": "2025-03",
        "title": "Principal Eigenvalue Regularization for Improved Worst-Class Certified Robustness of Smoothed Classifiers",
        "author": "Gaojie Jin, Tianjin Huang, Ronghui Mu, and Xiaowei Huang",
        "link": "http://arxiv.org/abs/2503.17172v1",
        "abstract": "Recent studies have identified a critical challenge in deep neural networks\n(DNNs) known as ``robust fairness\", where models exhibit significant\ndisparities in robust accuracy across different classes. While prior work has\nattempted to address this issue in adversarial robustness, the study of\nworst-class certified robustness for smoothed classifiers remains unexplored.\nOur work bridges this gap by developing a PAC-Bayesian bound for the\nworst-class error of smoothed classifiers. Through theoretical analysis, we\ndemonstrate that the largest eigenvalue of the smoothed confusion matrix\nfundamentally influences the worst-class error of smoothed classifiers. Based\non this insight, we introduce a regularization method that optimizes the\nlargest eigenvalue of smoothed confusion matrix to enhance worst-class accuracy\nof the smoothed classifier and further improve its worst-class certified\nrobustness. We provide extensive experimental validation across multiple\ndatasets and model architectures to demonstrate the effectiveness of our\napproach."
    },
    {
        "date": "2025-03",
        "title": "Generative adversarial framework to calibrate excursion set models for the 3D morphology of all-solid-state battery cathodes",
        "author": "Orkun Furat, Sabrina Weber, Johannes Schubert, Ren\u00e9 Rekers, Maximilian Luczak, Erik Glatt, Andreas Wiegmann, J\u00fcrgen Janek, Anja Bielefeld, and Volker Schmidt",
        "link": "http://arxiv.org/abs/2503.17171v1",
        "abstract": "This paper presents a computational method for generating virtual 3D\nmorphologies of functional materials using low-parametric stochastic geometry\nmodels, i.e., digital twins, calibrated with 2D microscopy images. These\ndigital twins allow systematic parameter variations to simulate various\nmorphologies, that can be deployed for virtual materials testing by means of\nspatially resolved numerical simulations of macroscopic properties. Generative\nadversarial networks (GANs) have gained popularity for calibrating models to\ngenerate realistic 3D morphologies. However, GANs often comprise of numerous\nuninterpretable parameters make systematic variation of morphologies for\nvirtual materials testing challenging. In contrast, low-parametric stochastic\ngeometry models (e.g., based on Gaussian random fields) enable targeted\nvariation but may struggle to mimic complex morphologies. Combining GANs with\nadvanced stochastic geometry models (e.g., excursion sets of more general\nrandom fields) addresses these limitations, allowing model calibration solely\nfrom 2D image data. This approach is demonstrated by generating a digital twin\nof all-solid-state battery (ASSB) cathodes. Since the digital twins are\nparametric, they support systematic exploration of structural scenarios and\ntheir macroscopic properties. The proposed method facilitates simulation\nstudies for optimizing 3D morphologies, benefiting not only ASSB cathodes but\nalso other materials with similar structures."
    },
    {
        "date": "2025-03",
        "title": "Hi-ALPS -- An Experimental Robustness Quantification of Six LiDAR-based Object Detection Systems for Autonomous Driving",
        "author": "Alexandra Arzberger, and Ramin Tavakoli Kolagari",
        "link": "http://arxiv.org/abs/2503.17168v2",
        "abstract": "Light Detection and Ranging (LiDAR) is an essential sensor technology for\nautonomous driving as it can capture high-resolution 3D data. As 3D object\ndetection systems (OD) can interpret such point cloud data, they play a key\nrole in the driving decisions of autonomous vehicles. Consequently, such 3D OD\nmust be robust against all types of perturbations and must therefore be\nextensively tested. One approach is the use of adversarial examples, which are\nsmall, sometimes sophisticated perturbations in the input data that change,\ni.e., falsify, the prediction of the OD. These perturbations are carefully\ndesigned based on the weaknesses of the OD. The robustness of the OD cannot be\nquantified with adversarial examples in general, because if the OD is\nvulnerable to a given attack, it is unclear whether this is due to the\nrobustness of the OD or whether the attack algorithm produces particularly\nstrong adversarial examples. The contribution of this work is Hi-ALPS --\nHierarchical Adversarial-example-based LiDAR Perturbation Level System, where\nhigher robustness of the OD is required to withstand the perturbations as the\nperturbation levels increase. In doing so, the Hi-ALPS levels successively\nimplement a heuristic followed by established adversarial example approaches.\nIn a series of comprehensive experiments using Hi-ALPS, we quantify the\nrobustness of six state-of-the-art 3D OD under different types of\nperturbations. The results of the experiments show that none of the OD is\nrobust against all Hi-ALPS levels; an important factor for the ranking is that\nhuman observers can still correctly recognize the perturbed objects, as the\nrespective perturbations are small. To increase the robustness of the OD, we\ndiscuss the applicability of state-of-the-art countermeasures. In addition, we\nderive further suggestions for countermeasures based on our experimental\nresults."
    },
    {
        "date": "2025-03",
        "title": "HiFi-Stream: Streaming Speech Enhancement with Generative Adversarial Networks",
        "author": "Ekaterina Dmitrieva, and Maksim Kaledin",
        "link": "http://arxiv.org/abs/2503.17141v1",
        "abstract": "Speech Enhancement techniques have become core technologies in mobile devices\nand voice software simplifying downstream speech tasks. Still, modern Deep\nLearning (DL) solutions often require high amount of computational resources\nwhat makes their usage on low-resource devices challenging. We present\nHiFi-Stream, an optimized version of recently published HiFi++ model. Our\nexperiments demonstrate that HiFiStream saves most of the qualities of the\noriginal model despite its size and computational complexity: the lightest\nversion has only around 490k parameters which is 3.5x reduction in comparison\nto the original HiFi++ making it one of the smallest and fastest models\navailable. The model is evaluated in streaming setting where it demonstrates\nits superior performance in comparison to modern baselines."
    },
    {
        "date": "2025-03",
        "title": "EVSOAR: Security Orchestration, Automation and Response via EV Charging Stations",
        "author": "Tadeu Freitas, Erick Silva, Rehana Yasmin, Ali Shoker, Manuel E. Correia, Rolando Martins, and Paulo Esteves-Verissimo",
        "link": "http://arxiv.org/abs/2503.16984v1",
        "abstract": "Vehicle cybersecurity has emerged as a critical concern, driven by the\ninnovation in the automotive industry, e.g., automomous, electric, or\nconnnected vehicles. Current efforts to address these challenges are\nconstrained by the limited computational resources of vehicles and the reliance\non connected infrastructures. This motivated the foundation of Vehicle Security\nOperations Centers (VSOCs) that extend IT-based Security Operations Centers\n(SOCs) to cover the entire automotive ecosystem, both the in-vehicle and\noff-vehicle scopes. Security Orchestration, Automation, and Response (SOAR)\ntools are considered key for impelementing an effective cybersecurity solution.\nHowever, existing state-of-the-art solutions depend on infrastructure networks\nsuch as 4G, 5G, and WiFi, which often face scalability and congestion issues.\nTo address these limitations, we propose a novel SOAR architecture EVSOAR that\nleverages the EV charging stations for connectivity and computing to enhance\nvehicle cybersecurity. Our EV-specific SOAR architecture enables real-time\nanalysis and automated responses to cybersecurity threats closer to the EV,\nreducing the cellular latency, bandwidth, and interference limitations. Our\nexperimental results demonstrate a significant improvement in latency,\nstability, and scalability through the infrastructure and the capacity to\ndeploy computationally intensive applications, that are otherwise infeasible\nwithin the resource constraints of individual vehicles."
    },
    {
        "date": "2025-03",
        "title": "EasyRobust: A Comprehensive and Easy-to-use Toolkit for Robust and Generalized Vision",
        "author": "Xiaofeng Mao, Yuefeng Chen, Rong Zhang, Hui Xue, Zhao Li, and Hang Su",
        "link": "http://arxiv.org/abs/2503.16975v1",
        "abstract": "Deep neural networks (DNNs) has shown great promise in computer vision tasks.\nHowever, machine vision achieved by DNNs cannot be as robust as human\nperception. Adversarial attacks and data distribution shifts have been known as\ntwo major scenarios which degrade machine performance and obstacle the wide\ndeployment of machines \"in the wild\". In order to break these obstructions and\nfacilitate the research of model robustness, we develop EasyRobust, a\ncomprehensive and easy-to-use toolkit for training, evaluation and analysis of\nrobust vision models. EasyRobust targets at two types of robustness: 1)\nAdversarial robustness enables the model to defense against malicious inputs\ncrafted by worst-case perturbations, also known as adversarial examples; 2)\nNon-adversarial robustness enhances the model performance on natural test\nimages with corruptions or distribution shifts. Thorough benchmarks on image\nclassification enable EasyRobust to provide an accurate robustness evaluation\non vision models. We wish our EasyRobust can help for training\npractically-robust models and promote academic and industrial progress in\nclosing the gap between human and machine vision. Codes and models of\nEasyRobust have been open-sourced in https://github.com/alibaba/easyrobust."
    },
    {
        "date": "2025-03",
        "title": "DroneSplat: 3D Gaussian Splatting for Robust 3D Reconstruction from In-the-Wild Drone Imagery",
        "author": "Jiadong Tang, Yu Gao, Dianyi Yang, Liqi Yan, Yufeng Yue, and Yi Yang",
        "link": "http://arxiv.org/abs/2503.16964v1",
        "abstract": "Drones have become essential tools for reconstructing wild scenes due to\ntheir outstanding maneuverability. Recent advances in radiance field methods\nhave achieved remarkable rendering quality, providing a new avenue for 3D\nreconstruction from drone imagery. However, dynamic distractors in wild\nenvironments challenge the static scene assumption in radiance fields, while\nlimited view constraints hinder the accurate capture of underlying scene\ngeometry. To address these challenges, we introduce DroneSplat, a novel\nframework designed for robust 3D reconstruction from in-the-wild drone imagery.\nOur method adaptively adjusts masking thresholds by integrating local-global\nsegmentation heuristics with statistical approaches, enabling precise\nidentification and elimination of dynamic distractors in static scenes. We\nenhance 3D Gaussian Splatting with multi-view stereo predictions and a\nvoxel-guided optimization strategy, supporting high-quality rendering under\nlimited view constraints. For comprehensive evaluation, we provide a\ndrone-captured 3D reconstruction dataset encompassing both dynamic and static\nscenes. Extensive experiments demonstrate that DroneSplat outperforms both 3DGS\nand NeRF baselines in handling in-the-wild drone imagery."
    },
    {
        "date": "2025-03",
        "title": "CleanStack: A New Dual-Stack for Defending Against Stack-Based Memory Corruption Attacks",
        "author": "Lei Chong",
        "link": "http://arxiv.org/abs/2503.16950v1",
        "abstract": "Stack-based memory corruption vulnerabilities have\n  long been exploited by attackers to execute arbitrary code\n  or perform unauthorized memory operations. Various defense\n  mechanisms have been introduced to mitigate stack memory\n  errors, but they typically focus on specific attack types, incur\n  substantial performance overhead, or suffer from compatibility\n  limitations.In this paper, we present CleanStack, an efficient,\n  highly compatible, and comprehensive stack protection mech anism. CleanStack\nisolates stack objects influenced by external\n  input from other safe stack objects, thereby preventing attackers\n  from modifying return addresses via controlled stack objects.\n  Additionally, by randomizing the placement of tainted stack\n  objects within the Unclean Stack, CleanStack mitigates non control data\nattacks by preventing attackers from predicting the\n  stack layout.A key component of CleanStack is the identifica tion of tainted\nstack objects. We analyze both static program\n  analysis and heuristic methods for this purpose. To maximize\n  compatibility, we adopt a heuristic approach and implement\n  CleanStack within the LLVM compiler framework, applying it to\n  SPEC CPU2017 benchmarks and a real-world application.Our\n  security evaluation demonstrates that CleanStack significantly\n  reduces the exploitability of stack-based memory errors by\n  providing a dual-stack system with isolation and randomization.\n  Performance evaluation results indicate that CleanStack incurs\n  an execution overhead of only 1.73% on the SPEC CPU2017\n  benchmark while introducing a minimal memory overhead of\n  just 0.04%. Compared to existing stack protection techniques,\n  CleanStack achieves an optimal balance between protection\n  coverage, runtime overhead, and compatibility, making it one\n  of the most comprehensive and efficient stack security solutions\n  to date."
    },
    {
        "date": "2025-03",
        "title": "In-House Evaluation Is Not Enough: Towards Robust Third-Party Flaw Disclosure for General-Purpose AI",
        "author": "Shayne Longpre, Kevin Klyman, Ruth E. Appel, Sayash Kapoor, Rishi Bommasani, Michelle Sahar, Sean McGregor, Avijit Ghosh, Borhane Blili-Hamelin, Nathan Butters, Alondra Nelson, Amit Elazari, Andrew Sellars, Casey John Ellis, Dane Sherrets, Dawn Song, Harley Geiger, Ilona Cohen, Lauren McIlvenny, Madhulika Srikumar, Mark M. Jaycox, Markus Anderljung, Nadine Farid Johnson, Nicholas Carlini, Nicolas Miailhe, Nik Marda, Peter Henderson, Rebecca S. Portnoff, Rebecca Weiss, Victoria Westerhoff, Yacine Jernite, Rumman Chowdhury, Percy Liang, and Arvind Narayanan",
        "link": "http://arxiv.org/abs/2503.16861v2",
        "abstract": "The widespread deployment of general-purpose AI (GPAI) systems introduces\nsignificant new risks. Yet the infrastructure, practices, and norms for\nreporting flaws in GPAI systems remain seriously underdeveloped, lagging far\nbehind more established fields like software security. Based on a collaboration\nbetween experts from the fields of software security, machine learning, law,\nsocial science, and policy, we identify key gaps in the evaluation and\nreporting of flaws in GPAI systems. We call for three interventions to advance\nsystem safety. First, we propose using standardized AI flaw reports and rules\nof engagement for researchers in order to ease the process of submitting,\nreproducing, and triaging flaws in GPAI systems. Second, we propose GPAI system\nproviders adopt broadly-scoped flaw disclosure programs, borrowing from bug\nbounties, with legal safe harbors to protect researchers. Third, we advocate\nfor the development of improved infrastructure to coordinate distribution of\nflaw reports across the many stakeholders who may be impacted. These\ninterventions are increasingly urgent, as evidenced by the prevalence of\njailbreaks and other flaws that can transfer across different providers' GPAI\nsystems. By promoting robust reporting and coordination in the AI ecosystem,\nthese proposals could significantly improve the safety, security, and\naccountability of GPAI systems."
    },
    {
        "date": "2025-03",
        "title": "Early-MFC: Enhanced Flow Correlation Attacks on Tor via Multi-view Triplet Networks with Early Network Traffic",
        "author": "Yali Yuan, Qianqi Niu, and Yachao Yuan",
        "link": "http://arxiv.org/abs/2503.16847v1",
        "abstract": "Flow correlation attacks is an efficient network attacks, aiming to expose\nthose who use anonymous network services, such as Tor. Conducting such attacks\nduring the early stages of network communication is particularly critical for\nscenarios demanding rapid decision-making, such as cybercrime detection or\nfinancial fraud prevention. Although recent studies have made progress in flow\ncorrelation attacks techniques, research specifically addressing flow\ncorrelation with early network traffic flow remains limited. Moreover, due to\nfactors such as model complexity, training costs, and real-time requirements,\nexisting technologies cannot be directly applied to flow correlation with early\nnetwork traffic flow. In this paper, we propose flow correlation attack with\nearly network traffic, named Early-MFC, based on multi-view triplet networks.\nThe proposed approach extracts multi-view traffic features from the payload at\nthe transport layer and the Inter-Packet Delay. It then integrates multi-view\nflow information, converting the extracted features into shared embeddings. By\nleveraging techniques such as metric learning and contrastive learning, the\nmethod optimizes the embeddings space by ensuring that similar flows are mapped\ncloser together while dissimilar flows are positioned farther apart. Finally,\nBayesian decision theory is applied to determine flow correlation, enabling\nhigh-accuracy flow correlation with early network traffic flow. Furthermore, we\ninvestigate flow correlation attacks under extra-early network traffic flow\nconditions. To address this challenge, we propose Early-MFC+, which utilizes\npayload data to construct embedded feature representations, ensuring robust\nperformance even with minimal packet availability."
    },
    {
        "date": "2025-03",
        "title": "CAARMA: Class Augmentation with Adversarial Mixup Regularization",
        "author": "Massa Baali, Xiang Li, Hao Chen, Rita Singh, and Bhiksha Raj",
        "link": "http://arxiv.org/abs/2503.16718v1",
        "abstract": "Speaker verification is a typical zero-shot learning task, where inference of\nunseen classes is performed by comparing embeddings of test instances to known\nexamples. The models performing inference must hence naturally generate\nembeddings that cluster same-class instances compactly, while maintaining\nseparation across classes. In order to learn to do so, they are typically\ntrained on a large number of classes (speakers), often using specialized\nlosses. However real-world speaker datasets often lack the class diversity\nneeded to effectively learn this in a generalizable manner. We introduce\nCAARMA, a class augmentation framework that addresses this problem by\ngenerating synthetic classes through data mixing in the embedding space,\nexpanding the number of training classes. To ensure the authenticity of the\nsynthetic classes we adopt a novel adversarial refinement mechanism that\nminimizes categorical distinctions between synthetic and real classes. We\nevaluate CAARMA on multiple speaker verification tasks, as well as other\nrepresentative zero-shot comparison-based speech analysis tasks and obtain\nconsistent improvements: our framework demonstrates a significant improvement\nof 8\\% over all baseline models. Code for CAARMA will be released."
    },
    {
        "date": "2025-03",
        "title": "Depth Matters: Multimodal RGB-D Perception for Robust Autonomous Agents",
        "author": "Mihaela-Larisa Clement, M\u00f3nika Farsang, Felix Resch, and Radu Grosu",
        "link": "http://arxiv.org/abs/2503.16711v1",
        "abstract": "Autonomous agents that rely purely on perception to make real-time control\ndecisions require efficient and robust architectures. In this work, we\ndemonstrate that augmenting RGB input with depth information significantly\nenhances our agents' ability to predict steering commands compared to using RGB\nalone. We benchmark lightweight recurrent controllers that leverage the fused\nRGB-D features for sequential decision-making. To train our models, we collect\nhigh-quality data using a small-scale autonomous car controlled by an expert\ndriver via a physical steering wheel, capturing varying levels of steering\ndifficulty. Our models, trained under diverse configurations, were successfully\ndeployed on real hardware. Specifically, our findings reveal that the early\nfusion of depth data results in a highly robust controller, which remains\neffective even with frame drops and increased noise levels, without\ncompromising the network's focus on the task."
    },
    {
        "date": "2025-03",
        "title": "ATOM: A Framework of Detecting Query-Based Model Extraction Attacks for Graph Neural Networks",
        "author": "Zhan Cheng, Bolin Shen, Tianming Sha, Yuan Gao, Shibo Li, and Yushun Dong",
        "link": "http://arxiv.org/abs/2503.16693v1",
        "abstract": "Graph Neural Networks (GNNs) have gained traction in Graph-based Machine\nLearning as a Service (GMLaaS) platforms, yet they remain vulnerable to\ngraph-based model extraction attacks (MEAs), where adversaries reconstruct\nsurrogate models by querying the victim model. Existing defense mechanisms,\nsuch as watermarking and fingerprinting, suffer from poor real-time\nperformance, susceptibility to evasion, or reliance on post-attack\nverification, making them inadequate for handling the dynamic characteristics\nof graph-based MEA variants. To address these limitations, we propose ATOM, a\nnovel real-time MEA detection framework tailored for GNNs. ATOM integrates\nsequential modeling and reinforcement learning to dynamically detect evolving\nattack patterns, while leveraging $k$-core embedding to capture the structural\nproperties, enhancing detection precision. Furthermore, we provide theoretical\nanalysis to characterize query behaviors and optimize detection strategies.\nExtensive experiments on multiple real-world datasets demonstrate that ATOM\noutperforms existing approaches in detection performance, maintaining stable\nacross different time steps, thereby offering a more effective defense\nmechanism for GMLaaS environments."
    },
    {
        "date": "2025-03",
        "title": "Input-Triggered Hardware Trojan Attack on Spiking Neural Networks",
        "author": "Spyridon Raptis, Paul Kling, Ioannis Kaskampas, Ihsen Alouani, and Haralampos-G. Stratigopoulos",
        "link": "http://arxiv.org/abs/2503.21793v1",
        "abstract": "Neuromorphic computing based on spiking neural networks (SNNs) is emerging as\na promising alternative to traditional artificial neural networks (ANNs),\noffering unique advantages in terms of low power consumption. However, the\nsecurity aspect of SNNs is under-explored compared to their ANN counterparts.\nAs the increasing reliance on AI systems comes with unique security risks and\nchallenges, understanding the vulnerabilities and threat landscape is essential\nas neuromorphic computing matures. In this effort, we propose a novel\ninput-triggered Hardware Trojan (HT) attack for SNNs. The HT mechanism is\ncondensed in the area of one neuron. The trigger mechanism is an input message\ncrafted in the spiking domain such that a selected neuron produces a malicious\nspike train that is not met in normal settings. This spike train triggers a\nmalicious modification in the neuron that forces it to saturate, firing\npermanently and failing to recover to its resting state even when the input\nactivity stops. The excessive spikes pollute the network and produce misleading\ndecisions. We propose a methodology to select an appropriate neuron and to\ngenerate the input pattern that triggers the HT payload. The attack is\nillustrated by simulation on three popular benchmarks in the neuromorphic\ncommunity. We also propose a hardware implementation for an analog spiking\nneuron and a digital SNN accelerator, demonstrating that the HT has a\nnegligible area and power footprint and, thereby, can easily evade detection."
    },
    {
        "date": "2025-03",
        "title": "Optimal Complexity in Byzantine-Robust Distributed Stochastic Optimization with Data Heterogeneity",
        "author": "Qiankun Shi, Jie Peng, Kun Yuan, Xiao Wang, and Qing Ling",
        "link": "http://arxiv.org/abs/2503.16337v1",
        "abstract": "In this paper, we establish tight lower bounds for Byzantine-robust\ndistributed first-order stochastic optimization methods in both strongly convex\nand non-convex stochastic optimization. We reveal that when the distributed\nnodes have heterogeneous data, the convergence error comprises two components:\na non-vanishing Byzantine error and a vanishing optimization error. We\nestablish the lower bounds on the Byzantine error and on the minimum number of\nqueries to a stochastic gradient oracle required to achieve an arbitrarily\nsmall optimization error. Nevertheless, we identify significant discrepancies\nbetween our established lower bounds and the existing upper bounds. To fill\nthis gap, we leverage the techniques of Nesterov's acceleration and variance\nreduction to develop novel Byzantine-robust distributed stochastic optimization\nmethods that provably match these lower bounds, up to logarithmic factors,\nimplying that our established lower bounds are tight."
    },
    {
        "date": "2025-03",
        "title": "Securing Satellite Communications: Real-Time Video Encryption Scheme on Satellite Payloads",
        "author": "Hanshuo Qiu, Jing Lian, Xiaoyuan Wang, and Jizhao Liu",
        "link": "http://arxiv.org/abs/2503.16287v1",
        "abstract": "The rapid development of low-Earth orbit (LEO) satellite constellations and\nsatellite communication systems has elevated the importance of secure video\ntransmission, which is the key to applications such as remote sensing, disaster\nrelief, and secure information exchange. In this context, three serious issues\narise concerning real-time encryption of videos on satellite embedded devices:\n(a) the challenge of achieving real-time performance; (b) the limitations posed\nby the constrained computing performance of satellite payloads; and (c) the\npotential for excessive power consumption leading to overheating, thereby\nescalating safety risks. To overcome these challenges, this study introduced a\nnovel approach for encrypting videos by employing two 1D chaotic maps, which\nwas deployed on a satellite for the first time. The experiment on the satellite\nconfirms that our scheme is suitable for complex satellite environments. In\naddition, the proposed chaotic maps were implemented on a Field Programmable\nGate Array (FPGA) platform, and simulation results showed consistency with\nthose obtained on a Raspberry Pi. Experiments on the Raspberry Pi 4B\ndemonstrate exceptional real-time performance and low power consumption,\nvalidating both the hardware feasibility and the stability of our design.\nRigorous statistical testing also confirms the scheme's resilience against a\nvariety of attacks, underscoring its potential for secure, real-time data\ntransmission in satellite communication systems."
    },
    {
        "date": "2025-03",
        "title": "Rethinking Robustness in Machine Learning: A Posterior Agreement Approach",
        "author": "Jo\u00e3o Borges S. Carvalho, Alessandro Torcinovich, Victor Jimenez Rodriguez, Antonio E. Cin\u00e0, Carlos Cotrini, Lea Sch\u00f6nherr, and Joachim M. Buhmann",
        "link": "http://arxiv.org/abs/2503.16271v1",
        "abstract": "The robustness of algorithms against covariate shifts is a fundamental\nproblem with critical implications for the deployment of machine learning\nalgorithms in the real world. Current evaluation methods predominantly match\nthe robustness definition to that of standard generalization, relying on\nstandard metrics like accuracy-based scores, which, while designed for\nperformance assessment, lack a theoretical foundation encompassing their\napplication in estimating robustness to distribution shifts. In this work, we\nset the desiderata for a robustness metric, and we propose a novel principled\nframework for the robustness assessment problem that directly follows the\nPosterior Agreement (PA) theory of model validation. Specifically, we extend\nthe PA framework to the covariate shift setting by proposing a PA metric for\nrobustness evaluation in supervised classification tasks. We assess the\nsoundness of our metric in controlled environments and through an empirical\nrobustness analysis in two different covariate shift scenarios: adversarial\nlearning and domain generalization. We illustrate the suitability of PA by\nevaluating several models under different nature and magnitudes of shift, and\nproportion of affected observations. The results show that the PA metric\nprovides a sensible and consistent analysis of the vulnerabilities in learning\nalgorithms, even in the presence of few perturbed observations."
    },
    {
        "date": "2025-03",
        "title": "From Head to Tail: Efficient Black-box Model Inversion Attack via Long-tailed Learning",
        "author": "Ziang Li, Hongguang Zhang, Juan Wang, Meihui Chen, Hongxin Hu, Wenzhe Yi, Xiaoyang Xu, Mengda Yang, and Chenjun Ma",
        "link": "http://arxiv.org/abs/2503.16266v1",
        "abstract": "Model Inversion Attacks (MIAs) aim to reconstruct private training data from\nmodels, leading to privacy leakage, particularly in facial recognition systems.\nAlthough many studies have enhanced the effectiveness of white-box MIAs, less\nattention has been paid to improving efficiency and utility under limited\nattacker capabilities. Existing black-box MIAs necessitate an impractical\nnumber of queries, incurring significant overhead. Therefore, we analyze the\nlimitations of existing MIAs and introduce Surrogate Model-based Inversion with\nLong-tailed Enhancement (SMILE), a high-resolution oriented and query-efficient\nMIA for the black-box setting. We begin by analyzing the initialization of MIAs\nfrom a data distribution perspective and propose a long-tailed surrogate\ntraining method to obtain high-quality initial points. We then enhance the\nattack's effectiveness by employing the gradient-free black-box optimization\nalgorithm selected by NGOpt. Our experiments show that SMILE outperforms\nexisting state-of-the-art black-box MIAs while requiring only about 5% of the\nquery overhead."
    },
    {
        "date": "2025-03",
        "title": "AI Agents in Cryptoland: Practical Attacks and No Silver Bullet",
        "author": "Atharv Singh Patlan, Peiyao Sheng, S. Ashwin Hebbar, Prateek Mittal, and Pramod Viswanath",
        "link": "http://arxiv.org/abs/2503.16248v1",
        "abstract": "The integration of AI agents with Web3 ecosystems harnesses their\ncomplementary potential for autonomy and openness, yet also introduces\nunderexplored security risks, as these agents dynamically interact with\nfinancial protocols and immutable smart contracts. This paper investigates the\nvulnerabilities of AI agents within blockchain-based financial ecosystems when\nexposed to adversarial threats in real-world scenarios. We introduce the\nconcept of context manipulation -- a comprehensive attack vector that exploits\nunprotected context surfaces, including input channels, memory modules, and\nexternal data feeds. Through empirical analysis of ElizaOS, a decentralized AI\nagent framework for automated Web3 operations, we demonstrate how adversaries\ncan manipulate context by injecting malicious instructions into prompts or\nhistorical interaction records, leading to unintended asset transfers and\nprotocol violations which could be financially devastating. Our findings\nindicate that prompt-based defenses are insufficient, as malicious inputs can\ncorrupt an agent's stored context, creating cascading vulnerabilities across\ninteractions and platforms. This research highlights the urgent need to develop\nAI agents that are both secure and fiduciarily responsible."
    },
    {
        "date": "2025-03",
        "title": "Narrowing Class-Wise Robustness Gaps in Adversarial Training",
        "author": "Fatemeh Amerehi, and Patrick Healy",
        "link": "http://arxiv.org/abs/2503.16179v1",
        "abstract": "Efforts to address declining accuracy as a result of data shifts often\ninvolve various data-augmentation strategies. Adversarial training is one such\nmethod, designed to improve robustness to worst-case distribution shifts caused\nby adversarial examples. While this method can improve robustness, it may also\nhinder generalization to clean examples and exacerbate performance imbalances\nacross different classes. This paper explores the impact of adversarial\ntraining on both overall and class-specific performance, as well as its\nspill-over effects. We observe that enhanced labeling during training boosts\nadversarial robustness by 53.50% and mitigates class imbalances by 5.73%,\nleading to improved accuracy in both clean and adversarial settings compared to\nstandard adversarial training."
    },
    {
        "date": "2025-03",
        "title": "Towards Lighter and Robust Evaluation for Retrieval Augmented Generation",
        "author": "Alex-Razvan Ispas, Charles-Elie Simon, Fabien Caspani, and Vincent Guigue",
        "link": "http://arxiv.org/abs/2503.16161v1",
        "abstract": "Large Language Models are prompting us to view more NLP tasks from a\ngenerative perspective. At the same time, they offer a new way of accessing\ninformation, mainly through the RAG framework. While there have been notable\nimprovements for the autoregressive models, overcoming hallucination in the\ngenerated answers remains a continuous problem. A standard solution is to use\ncommercial LLMs, such as GPT4, to evaluate these algorithms. However, such\nframeworks are expensive and not very transparent. Therefore, we propose a\nstudy which demonstrates the interest of open-weight models for evaluating RAG\nhallucination. We develop a lightweight approach using smaller, quantized LLMs\nto provide an accessible and interpretable metric that gives continuous scores\nfor the generated answer with respect to their correctness and faithfulness.\nThis score allows us to question decisions' reliability and explore thresholds\nto develop a new AUC metric as an alternative to correlation with human\njudgment."
    },
    {
        "date": "2025-03",
        "title": "3-D Image-to-Image Fusion in Lightsheet Microscopy by Two-Step Adversarial Network: Contribution to the FuseMyCells Challenge",
        "author": "Marek Wodzinski, and Henning M\u00fcller",
        "link": "http://arxiv.org/abs/2503.16075v1",
        "abstract": "Lightsheet microscopy is a powerful 3-D imaging technique that addresses\nlimitations of traditional optical and confocal microscopy but suffers from a\nlow penetration depth and reduced image quality at greater depths. Multiview\nlightsheet microscopy improves 3-D resolution by combining multiple views but\nsimultaneously increasing the complexity and the photon budget, leading to\npotential photobleaching and phototoxicity. The FuseMyCells challenge,\norganized in conjunction with the IEEE ISBI 2025 conference, aims to benchmark\ndeep learning-based solutions for fusing high-quality 3-D volumes from single\n3-D views, potentially simplifying procedures and conserving the photon budget.\nIn this work, we propose a contribution to the FuseMyCells challenge based on a\ntwo-step procedure. The first step processes a downsampled version of the image\nto capture the entire region of interest, while the second step uses a\npatch-based approach for high-resolution inference, incorporating adversarial\nloss to enhance visual outcomes. This method addresses challenges related to\nhigh data resolution, the necessity of global context, and the preservation of\nhigh-frequency details. Experimental results demonstrate the effectiveness of\nour approach, highlighting its potential to improve 3-D image fusion quality\nand extend the capabilities of lightsheet microscopy. The average SSIM for the\nnucleus and membranes is greater than 0.85 and 0.91, respectively."
    },
    {
        "date": "2025-03",
        "title": "Temporal-Spatial Attention Network (TSAN) for DoS Attack Detection in Network Traffic",
        "author": "Bisola Faith Kayode, Akinyemi Sadeeq Akintola, Oluwole Fagbohun, Egonna Anaesiuba-Bristol, Onyekachukwu Ojumah, Oluwagbade Odimayo, Toyese Oloyede, Aniema Inyang, Teslim Kazeem, Habeeb Alli, Udodirim Ibem Offia, and Prisca Chinazor Amajuoyi",
        "link": "http://arxiv.org/abs/2503.16047v2",
        "abstract": "Denial-of-Service (DoS) attacks remain a critical threat to network security,\ndisrupting services and causing significant economic losses. Traditional\ndetection methods, including statistical and rule-based models, struggle to\nadapt to evolving attack patterns. To address this challenge, we propose a\nnovel Temporal-Spatial Attention Network (TSAN) architecture for detecting\nDenial of Service (DoS) attacks in network traffic. By leveraging both temporal\nand spatial features of network traffic, our approach captures complex traffic\npatterns and anomalies that traditional methods might miss. The TSAN model\nincorporates transformer-based temporal encoding, convolutional spatial\nencoding, and a cross-attention mechanism to fuse these complementary feature\nspaces. Additionally, we employ multi-task learning with auxiliary tasks to\nenhance the model's robustness. Experimental results on the NSL-KDD dataset\ndemonstrate that TSAN outperforms state-of-the-art models, achieving superior\naccuracy, precision, recall, and F1-score while maintaining computational\nefficiency for real-time deployment. The proposed architecture offers an\noptimal balance between detection accuracy and computational overhead, making\nit highly suitable for real-world network security applications."
    },
    {
        "date": "2025-03",
        "title": "BadToken: Token-level Backdoor Attacks to Multi-modal Large Language Models",
        "author": "Zenghui Yuan, Jiawen Shi, Pan Zhou, Neil Zhenqiang Gong, and Lichao Sun",
        "link": "http://arxiv.org/abs/2503.16023v1",
        "abstract": "Multi-modal large language models (MLLMs) extend large language models (LLMs)\nto process multi-modal information, enabling them to generate responses to\nimage-text inputs. MLLMs have been incorporated into diverse multi-modal\napplications, such as autonomous driving and medical diagnosis, via\nplug-and-play without fine-tuning. This deployment paradigm increases the\nvulnerability of MLLMs to backdoor attacks. However, existing backdoor attacks\nagainst MLLMs achieve limited effectiveness and stealthiness. In this work, we\npropose BadToken, the first token-level backdoor attack to MLLMs. BadToken\nintroduces two novel backdoor behaviors: Token-substitution and Token-addition,\nwhich enable flexible and stealthy attacks by making token-level modifications\nto the original output for backdoored inputs. We formulate a general\noptimization problem that considers the two backdoor behaviors to maximize the\nattack effectiveness. We evaluate BadToken on two open-source MLLMs and various\ntasks. Our results show that our attack maintains the model's utility while\nachieving high attack success rates and stealthiness. We also show the\nreal-world threats of BadToken in two scenarios, i.e., autonomous driving and\nmedical diagnosis. Furthermore, we consider defenses including fine-tuning and\ninput purification. Our results highlight the threat of our attack."
    }
]