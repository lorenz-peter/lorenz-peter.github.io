[
    {
        "date": "2025-05",
        "title": "R3: Robust Rubric-Agnostic Reward Models",
        "author": "David Anugraha, Zilu Tang, Lester James V. Miranda, Hanyang Zhao, Mohammad Rifqi Farhansyah, Garry Kuwanto, Derry Wijaya, and Genta Indra Winata",
        "link": "http://arxiv.org/abs/2505.13388v1",
        "abstract": "Reward models are essential for aligning language model outputs with human\npreferences, yet existing approaches often lack both controllability and\ninterpretability. These models are typically optimized for narrow objectives,\nlimiting their generalizability to broader downstream tasks. Moreover, their\nscalar outputs are difficult to interpret without contextual reasoning. To\naddress these limitations, we introduce R3, a novel reward modeling framework\nthat is rubric-agnostic, generalizable across evaluation dimensions, and\nprovides interpretable, reasoned score assignments. R3 enables more transparent\nand flexible evaluation of language models, supporting robust alignment with\ndiverse human values and use cases. Our models, data, and code are available as\nopen source at https://github.com/rubricreward/r3"
    },
    {
        "date": "2025-05",
        "title": "DynaNoise: Dynamic Probabilistic Noise Injection for Defending Against Membership Inference Attacks",
        "author": "Javad Forough, and Hamed Haddadi",
        "link": "http://arxiv.org/abs/2505.13362v1",
        "abstract": "Membership Inference Attacks (MIAs) pose a significant risk to the privacy of\ntraining datasets by exploiting subtle differences in model outputs to\ndetermine whether a particular data sample was used during training. These\nattacks can compromise sensitive information, especially in domains such as\nhealthcare and finance, where data privacy is paramount. Traditional mitigation\ntechniques, such as static differential privacy, rely on injecting a fixed\namount of noise during training or inference. However, this approach often\nleads to a detrimental trade-off: the noise may be insufficient to counter\nsophisticated attacks or, when increased, may substantially degrade model\nperformance. In this paper, we present DynaNoise, an adaptive approach that\ndynamically modulates noise injection based on query sensitivity. Our approach\nperforms sensitivity analysis using measures such as Shannon entropy to\nevaluate the risk associated with each query and adjusts the noise variance\naccordingly. A probabilistic smoothing step is then applied to renormalize the\nperturbed outputs, ensuring that the model maintains high accuracy while\neffectively obfuscating membership signals. We further propose an empirical\nmetric, the Membership Inference Defense Privacy-Utility Tradeoff (MIDPUT),\nwhich quantifies the balance between reducing attack success rates and\npreserving the target model's accuracy. Our extensive evaluation on several\nbenchmark datasets demonstrates that DynaNoise not only significantly reduces\nMIA success rates but also achieves up to a fourfold improvement in the MIDPUT\nmetric compared to the state-of-the-art. Moreover, DynaNoise maintains\ncompetitive model accuracy while imposing only marginal inference overhead,\nhighlighting its potential as an effective and efficient privacy defense\nagainst MIAs."
    },
    {
        "date": "2025-05",
        "title": "Recommender Systems for Democracy: Toward Adversarial Robustness in Voting Advice Applications",
        "author": "Fr\u00e9d\u00e9ric Berdoz, Dustin Brunner, Yann Vonlanthen, and Roger Wattenhofer",
        "link": "http://arxiv.org/abs/2505.13329v1",
        "abstract": "Voting advice applications (VAAs) help millions of voters understand which\npolitical parties or candidates best align with their views. This paper\nexplores the potential risks these applications pose to the democratic process\nwhen targeted by adversarial entities. In particular, we expose 11 manipulation\nstrategies and measure their impact using data from Switzerland's primary VAA,\nSmartvote, collected during the last two national elections. We find that\naltering application parameters, such as the matching method, can shift a\nparty's recommendation frequency by up to 105%. Cherry-picking questionnaire\nitems can increase party recommendation frequency by over 261%, while subtle\nchanges to parties' or candidates' responses can lead to a 248% increase. To\naddress these vulnerabilities, we propose adversarial robustness properties\nVAAs should satisfy, introduce empirical metrics for assessing the resilience\nof various matching methods, and suggest possible avenues for research toward\nmitigating the effect of manipulation. Our framework is key to ensuring secure\nand reliable AI-based VAAs poised to emerge in the near future."
    },
    {
        "date": "2025-05",
        "title": "Benchmarking Unified Face Attack Detection via Hierarchical Prompt Tuning",
        "author": "Ajian Liu, Haocheng Yuan, Xiao Guo, Hui Ma, Wanyi Zhuang, Changtao Miao, Yan Hong, Chuanbiao Song, Jun Lan, Qi Chu, Tao Gong, Yanyan Liang, Weiqiang Wang, Jun Wan, Xiaoming Liu, and Zhen Lei",
        "link": "http://arxiv.org/abs/2505.13327v2",
        "abstract": "Presentation Attack Detection and Face Forgery Detection are designed to\nprotect face data from physical media-based Presentation Attacks and digital\nediting-based DeepFakes respectively. But separate training of these two models\nmakes them vulnerable to unknown attacks and burdens deployment environments.\nThe lack of a Unified Face Attack Detection model to handle both types of\nattacks is mainly due to two factors. First, there's a lack of adequate\nbenchmarks for models to explore. Existing UAD datasets have limited attack\ntypes and samples, restricting the model's ability to address advanced threats.\nTo address this, we propose UniAttackDataPlus (UniAttackData+), the most\nextensive and sophisticated collection of forgery techniques to date. It\nincludes 2,875 identities and their 54 kinds of falsified samples, totaling\n697,347 videos. Second, there's a lack of a reliable classification criterion.\nCurrent methods try to find an arbitrary criterion within the same semantic\nspace, which fails when encountering diverse attacks. So, we present a novel\nVisual-Language Model-based Hierarchical Prompt Tuning Framework (HiPTune) that\nadaptively explores multiple classification criteria from different semantic\nspaces. We build a Visual Prompt Tree to explore various classification rules\nhierarchically. Then, by adaptively pruning the prompts, the model can select\nthe most suitable prompts to guide the encoder to extract discriminative\nfeatures at different levels in a coarse-to-fine way. Finally, to help the\nmodel understand the classification criteria in visual space, we propose a\nDynamically Prompt Integration module to project the visual prompts to the text\nencoder for more accurate semantics. Experiments on 12 datasets have shown the\npotential to inspire further innovations in the UAD field."
    },
    {
        "date": "2025-05",
        "title": "SVAFD: A Secure and Verifiable Co-Aggregation Protocol for Federated Distillation",
        "author": "Tian Wen, Sheng Sun, Yuwei Wang, Peiyan Chen, Zhiyuan Wu, Min Liu, and Bo Gao",
        "link": "http://arxiv.org/abs/2505.13319v2",
        "abstract": "Secure Aggregation (SA) is an indispensable component of Federated Learning\n(FL) that concentrates on privacy preservation while allowing for robust\naggregation. However, most SA designs rely heavily on the unrealistic\nassumption of homogeneous model architectures. Federated Distillation (FD),\nwhich aggregates locally computed logits instead of model parameters,\nintroduces a promising alternative for cooperative training in heterogeneous\nmodel settings. Nevertheless, we recognize two major challenges in implementing\nSA for FD. (i) Prior SA designs encourage a dominant server, who is solely\nresponsible for collecting, aggregating and distributing. Such central\nauthority facilitates server to forge aggregation proofs or collude to bypass\nthe claimed security guarantees; (ii) Existing SA, tailored for FL models,\noverlook the intrinsic properties of logits, making them unsuitable for FD.\n  To address these challenges, we propose SVAFD, the first SA protocol that is\nspecifically designed for FD. At a high level, SVAFD incorporates two\ninnovations: (i) a multilateral co-aggregation method tha redefines the\nresponsibilities of clients and server. Clients autonomously evaluate and\naggregate logits shares locally with a lightweight coding scheme, while the\nserver handles ciphertext decoding and performs the task of generating\nverification proofs; (ii) a quality-aware knowledge filtration method that\nfacilitates biased logits exclusion against poisoning attacks. Moreover, SVAFD\nis resilient to stragglers and colluding clients, making it well-suited for\ndynamic networks in real-world applications. We have implemented the SVAFD\nprototype over four emerging FD architectures and evaluated it against\npoisoning and inference attacks. Results demonstrate that SVAFD improves model\naccuracy, making it a significant step forward in secure and verifiable\naggregation for heterogeneous FL systems."
    },
    {
        "date": "2025-05",
        "title": "RECON: Robust symmetry discovery via Explicit Canonical Orientation Normalization",
        "author": "Alonso Urbano, David W. Romero, Max Zimmer, and Sebastian Pokutta",
        "link": "http://arxiv.org/abs/2505.13289v1",
        "abstract": "Real-world data often exhibits unknown or approximate symmetries, yet\nexisting equivariant networks must commit to a fixed transformation group prior\nto training, e.g., continuous $SO(2)$ rotations. This mismatch degrades\nperformance when the actual data symmetries differ from those in the\ntransformation group. We introduce RECON, a framework to discover each input's\nintrinsic symmetry distribution from unlabeled data. RECON leverages class-pose\ndecompositions and applies a data-driven normalization to align arbitrary\nreference frames into a common natural pose, yielding directly comparable and\ninterpretable symmetry descriptors. We demonstrate effective symmetry discovery\non 2D image benchmarks and -- for the first time -- extend it to 3D\ntransformation groups, paving the way towards more flexible equivariant\nmodeling."
    },
    {
        "date": "2025-05",
        "title": "FlowPure: Continuous Normalizing Flows for Adversarial Purification",
        "author": "Elias Collaert, Abel Rodr\u00edguez, Sander Joos, Lieven Desmet, and Vera Rimmer",
        "link": "http://arxiv.org/abs/2505.13280v1",
        "abstract": "Despite significant advancements in the area, adversarial robustness remains\na critical challenge in systems employing machine learning models. The removal\nof adversarial perturbations at inference time, known as adversarial\npurification, has emerged as a promising defense strategy. To achieve this,\nstate-of-the-art methods leverage diffusion models that inject Gaussian noise\nduring a forward process to dilute adversarial perturbations, followed by a\ndenoising step to restore clean samples before classification. In this work, we\npropose FlowPure, a novel purification method based on Continuous Normalizing\nFlows (CNFs) trained with Conditional Flow Matching (CFM) to learn mappings\nfrom adversarial examples to their clean counterparts. Unlike prior\ndiffusion-based approaches that rely on fixed noise processes, FlowPure can\nleverage specific attack knowledge to improve robustness under known threats,\nwhile also supporting a more general stochastic variant trained on Gaussian\nperturbations for settings where such knowledge is unavailable. Experiments on\nCIFAR-10 and CIFAR-100 demonstrate that our method outperforms state-of-the-art\npurification-based defenses in preprocessor-blind and white-box scenarios, and\ncan do so while fully preserving benign accuracy in the former. Moreover, our\nresults show that not only is FlowPure a highly effective purifier but it also\nholds a strong potential for adversarial detection, identifying\npreprocessor-blind PGD samples with near-perfect accuracy."
    },
    {
        "date": "2025-05",
        "title": "StarFT: Robust Fine-tuning of Zero-shot Models via Spuriosity Alignment",
        "author": "Younghyun Kim, Jongheon Jeong, Sangkyung Kwak, Kyungmin Lee, Juho Lee, and Jinwoo Shin",
        "link": "http://arxiv.org/abs/2505.13232v2",
        "abstract": "Learning robust representations from data often requires scale, which has led\nto the success of recent zero-shot models such as CLIP. However, the obtained\nrobustness can easily be deteriorated when these models are fine-tuned on other\ndownstream tasks (e.g., of smaller scales). Previous works often interpret this\nphenomenon in the context of domain shift, developing fine-tuning methods that\naim to preserve the original domain as much as possible. However, in a\ndifferent context, fine-tuned models with limited data are also prone to\nlearning features that are spurious to humans, such as background or texture.\nIn this paper, we propose StarFT (Spurious Textual Alignment Regularization), a\nnovel framework for fine-tuning zero-shot models to enhance robustness by\npreventing them from learning spuriosity. We introduce a regularization that\naligns the output distribution for spuriosity-injected labels with the original\nzero-shot model, ensuring that the model is not induced to extract irrelevant\nfeatures further from these descriptions. We leverage recent language models to\nget such spuriosity-injected labels by generating alternative textual\ndescriptions that highlight potentially confounding features. Extensive\nexperiments validate the robust generalization of StarFT and its emerging\nproperties: zero-shot group robustness and improved zero-shot classification.\nNotably, StarFT boosts both worst-group and average accuracy by 14.30% and\n3.02%, respectively, in the Waterbirds group shift scenario, where other robust\nfine-tuning baselines show even degraded performance."
    },
    {
        "date": "2025-05",
        "title": "Adversarial Testing in LLMs: Insights into Decision-Making Vulnerabilities",
        "author": "Lili Zhang, Haomiaomiao Wang, Long Cheng, Libao Deng, and Tomas Ward",
        "link": "http://arxiv.org/abs/2505.13195v1",
        "abstract": "As Large Language Models (LLMs) become increasingly integrated into\nreal-world decision-making systems, understanding their behavioural\nvulnerabilities remains a critical challenge for AI safety and alignment. While\nexisting evaluation metrics focus primarily on reasoning accuracy or factual\ncorrectness, they often overlook whether LLMs are robust to adversarial\nmanipulation or capable of using adaptive strategy in dynamic environments.\nThis paper introduces an adversarial evaluation framework designed to\nsystematically stress-test the decision-making processes of LLMs under\ninteractive and adversarial conditions. Drawing on methodologies from cognitive\npsychology and game theory, our framework probes how models respond in two\ncanonical tasks: the two-armed bandit task and the Multi-Round Trust Task.\nThese tasks capture key aspects of exploration-exploitation trade-offs, social\ncooperation, and strategic flexibility. We apply this framework to several\nstate-of-the-art LLMs, including GPT-3.5, GPT-4, Gemini-1.5, and DeepSeek-V3,\nrevealing model-specific susceptibilities to manipulation and rigidity in\nstrategy adaptation. Our findings highlight distinct behavioral patterns across\nmodels and emphasize the importance of adaptability and fairness recognition\nfor trustworthy AI deployment. Rather than offering a performance benchmark,\nthis work proposes a methodology for diagnosing decision-making weaknesses in\nLLM-based agents, providing actionable insights for alignment and safety\nresearch."
    },
    {
        "date": "2025-05",
        "title": "ARIW-Framework: Adaptive Robust Iterative Watermarking Framework",
        "author": "Shaowu Wu, Liting Zeng, Wei Lu, and Xiangyang Luo",
        "link": "http://arxiv.org/abs/2505.13101v1",
        "abstract": "With the rapid rise of large models, copyright protection for generated image\ncontent has become a critical security challenge. Although deep learning\nwatermarking techniques offer an effective solution for digital image copyright\nprotection, they still face limitations in terms of visual quality, robustness\nand generalization. To address these issues, this paper proposes an adaptive\nrobust iterative watermarking framework (ARIW-Framework) that achieves\nhigh-quality watermarked images while maintaining exceptional robustness and\ngeneralization performance. Specifically, we introduce an iterative approach to\noptimize the encoder for generating robust residuals. The encoder incorporates\nnoise layers and a decoder to compute robustness weights for residuals under\nvarious noise attacks. By employing a parallel optimization strategy, the\nframework enhances robustness against multiple types of noise attacks.\nFurthermore, we leverage image gradients to determine the embedding strength at\neach pixel location, significantly improving the visual quality of the\nwatermarked images. Extensive experiments demonstrate that the proposed method\nachieves superior visual quality while exhibiting remarkable robustness and\ngeneralization against noise attacks."
    },
    {
        "date": "2025-05",
        "title": "Cross-modal feature fusion for robust point cloud registration with ambiguous geometry",
        "author": "Zhaoyi Wang, Shengyu Huang, Jemil Avers Butt, Yuanzhou Cai, Matej Varga, and Andreas Wieser",
        "link": "http://arxiv.org/abs/2505.13088v1",
        "abstract": "Point cloud registration has seen significant advancements with the\napplication of deep learning techniques. However, existing approaches often\noverlook the potential of integrating radiometric information from RGB images.\nThis limitation reduces their effectiveness in aligning point clouds pairs,\nespecially in regions where geometric data alone is insufficient. When used\neffectively, radiometric information can enhance the registration process by\nproviding context that is missing from purely geometric data. In this paper, we\npropose CoFF, a novel Cross-modal Feature Fusion method that utilizes both\npoint cloud geometry and RGB images for pairwise point cloud registration.\nAssuming that the co-registration between point clouds and RGB images is\navailable, CoFF explicitly addresses the challenges where geometric information\nalone is unclear, such as in regions with symmetric similarity or planar\nstructures, through a two-stage fusion of 3D point cloud features and 2D image\nfeatures. It incorporates a cross-modal feature fusion module that assigns\npixel-wise image features to 3D input point clouds to enhance learned 3D point\nfeatures, and integrates patch-wise image features with superpoint features to\nimprove the quality of coarse matching. This is followed by a coarse-to-fine\nmatching module that accurately establishes correspondences using the fused\nfeatures. We extensively evaluate CoFF on four common datasets: 3DMatch,\n3DLoMatch, IndoorLRS, and the recently released ScanNet++ datasets. In\naddition, we assess CoFF on specific subset datasets containing geometrically\nambiguous cases. Our experimental results demonstrate that CoFF achieves\nstate-of-the-art registration performance across all benchmarks, including\nremarkable registration recalls of 95.9% and 81.6% on the widely-used 3DMatch\nand 3DLoMatch datasets, respectively...(Truncated to fit arXiv abstract length)"
    },
    {
        "date": "2025-05",
        "title": "OmniFC: Rethinking Federated Clustering via Lossless and Secure Distance Reconstruction",
        "author": "Jie Yan, Xin Liu, and Zhong-Yuan Zhang",
        "link": "http://arxiv.org/abs/2505.13071v1",
        "abstract": "Federated clustering (FC) aims to discover global cluster structures across\ndecentralized clients without sharing raw data, making privacy preservation a\nfundamental requirement. There are two critical challenges: (1) privacy leakage\nduring collaboration, and (2) robustness degradation due to aggregation of\nproxy information from non-independent and identically distributed (Non-IID)\nlocal data, leading to inaccurate or inconsistent global clustering. Existing\nsolutions typically rely on model-specific local proxies, which are sensitive\nto data heterogeneity and inherit inductive biases from their centralized\ncounterparts, thus limiting robustness and generality. We propose Omni\nFederated Clustering (OmniFC), a unified and model-agnostic framework.\nLeveraging Lagrange coded computing, our method enables clients to share only\nencoded data, allowing exact reconstruction of the global distance matrix--a\nfundamental representation of sample relationships--without leaking private\ninformation, even under client collusion. This construction is naturally\nresilient to Non-IID data distributions. This approach decouples FC from\nmodel-specific proxies, providing a unified extension mechanism applicable to\ndiverse centralized clustering methods. Theoretical analysis confirms both\nreconstruction fidelity and privacy guarantees, while comprehensive experiments\ndemonstrate OmniFC's superior robustness, effectiveness, and generality across\nvarious benchmarks compared to state-of-the-art methods. Code will be released."
    },
    {
        "date": "2025-05",
        "title": "Anti-Inpainting: A Proactive Defense against Malicious Diffusion-based Inpainters under Unknown Conditions",
        "author": "Yimao Guo, Zuomin Qu, Wei Lu, and Xiangyang Luo",
        "link": "http://arxiv.org/abs/2505.13023v1",
        "abstract": "As diffusion-based malicious image manipulation becomes increasingly\nprevalent, multiple proactive defense methods are developed to safeguard images\nagainst unauthorized tampering. However, most proactive defense methods only\ncan safeguard images against manipulation under known conditions, and fail to\nprotect images from manipulations guided by tampering conditions crafted by\nmalicious users. To tackle this issue, we propose Anti-Inpainting, a proactive\ndefense method that achieves adequate protection under unknown conditions\nthrough a triple mechanism to address this challenge. Specifically, a\nmulti-level deep feature extractor is presented to obtain intricate features\nduring the diffusion denoising process to improve protective effectiveness. We\ndesign multi-scale semantic-preserving data augmentation to enhance the\ntransferability of adversarial perturbations across unknown conditions by\nmulti-scale transformations while preserving semantic integrity. In addition,\nwe propose a selection-based distribution deviation optimization strategy to\nimprove the protection of adversarial perturbation against manipulation under\ndiverse random seeds. Extensive experiments indicate the proactive defensive\nperformance of Anti-Inpainting against diffusion-based inpainters guided by\nunknown conditions in InpaintGuardBench and CelebA-HQ. At the same time, we\nalso demonstrate the proposed approach's robustness under various image\npurification methods and its transferability across different versions of\ndiffusion models."
    },
    {
        "date": "2025-05",
        "title": "From Assistants to Adversaries: Exploring the Security Risks of Mobile LLM Agents",
        "author": "Liangxuan Wu, Chao Wang, Tianming Liu, Yanjie Zhao, and Haoyu Wang",
        "link": "http://arxiv.org/abs/2505.12981v2",
        "abstract": "The growing adoption of large language models (LLMs) has led to a new\nparadigm in mobile computing--LLM-powered mobile AI agents--capable of\ndecomposing and automating complex tasks directly on smartphones. However, the\nsecurity implications of these agents remain largely unexplored. In this paper,\nwe present the first comprehensive security analysis of mobile LLM agents,\nencompassing three representative categories: System-level AI Agents developed\nby original equipment manufacturers (e.g., YOYO Assistant), Third-party\nUniversal Agents (e.g., Zhipu AI AutoGLM), and Emerging Agent Frameworks (e.g.,\nAlibaba Mobile Agent). We begin by analyzing the general workflow of mobile\nagents and identifying security threats across three core capability\ndimensions: language-based reasoning, GUI-based interaction, and system-level\nexecution. Our analysis reveals 11 distinct attack surfaces, all rooted in the\nunique capabilities and interaction patterns of mobile LLM agents, and spanning\ntheir entire operational lifecycle. To investigate these threats in practice,\nwe introduce AgentScan, a semi-automated security analysis framework that\nsystematically evaluates mobile LLM agents across all 11 attack scenarios.\nApplying AgentScan to nine widely deployed agents, we uncover a concerning\ntrend: every agent is vulnerable to targeted attacks. In the most severe cases,\nagents exhibit vulnerabilities across eight distinct attack vectors. These\nattacks can cause behavioral deviations, privacy leakage, or even full\nexecution hijacking. Based on these findings, we propose a set of defensive\ndesign principles and practical recommendations for building secure mobile LLM\nagents. Our disclosures have received positive feedback from two major device\nvendors. Overall, this work highlights the urgent need for standardized\nsecurity practices in the fast-evolving landscape of LLM-driven mobile\nautomation."
    },
    {
        "date": "2025-05",
        "title": "RGNMR: A Gauss-Newton method for robust matrix completion with theoretical guarantees",
        "author": "Eilon Vaknin Laufer, and Boaz Nadler",
        "link": "http://arxiv.org/abs/2505.12919v1",
        "abstract": "Recovering a low rank matrix from a subset of its entries, some of which may\nbe corrupted, is known as the robust matrix completion (RMC) problem. Existing\nRMC methods have several limitations: they require a relatively large number of\nobserved entries; they may fail under overparametrization, when their assumed\nrank is higher than the correct one; and many of them fail to recover even\nmildly ill-conditioned matrices. In this paper we propose a novel RMC method,\ndenoted $\\texttt{RGNMR}$, which overcomes these limitations. $\\texttt{RGNMR}$\nis a simple factorization-based iterative algorithm, which combines a\nGauss-Newton linearization with removal of entries suspected to be outliers. On\nthe theoretical front, we prove that under suitable assumptions,\n$\\texttt{RGNMR}$ is guaranteed exact recovery of the underlying low rank\nmatrix. Our theoretical results improve upon the best currently known for\nfactorization-based methods. On the empirical front, we show via several\nsimulations the advantages of $\\texttt{RGNMR}$ over existing RMC methods, and\nin particular its ability to handle a small number of observed entries,\noverparameterization of the rank and ill-conditioned matrices."
    },
    {
        "date": "2025-05",
        "title": "Does Low Rank Adaptation Lead to Lower Robustness against Training-Time Attacks?",
        "author": "Zi Liang, Haibo Hu, Qingqing Ye, Yaxin Xiao, and Ronghua Li",
        "link": "http://arxiv.org/abs/2505.12871v1",
        "abstract": "Low rank adaptation (LoRA) has emerged as a prominent technique for\nfine-tuning large language models (LLMs) thanks to its superb efficiency gains\nover previous methods. While extensive studies have examined the performance\nand structural properties of LoRA, its behavior upon training-time attacks\nremain underexplored, posing significant security risks. In this paper, we\ntheoretically investigate the security implications of LoRA's low-rank\nstructure during fine-tuning, in the context of its robustness against data\npoisoning and backdoor attacks. We propose an analytical framework that models\nLoRA's training dynamics, employs the neural tangent kernel to simplify the\nanalysis of the training process, and applies information theory to establish\nconnections between LoRA's low rank structure and its vulnerability against\ntraining-time attacks. Our analysis indicates that LoRA exhibits better\nrobustness to backdoor attacks than full fine-tuning, while becomes more\nvulnerable to untargeted data poisoning due to its over-simplified information\ngeometry. Extensive experimental evaluations have corroborated our theoretical\nfindings."
    },
    {
        "date": "2025-05",
        "title": "Causality-Inspired Robustness for Nonlinear Models via Representation Learning",
        "author": "Marin \u0160ola, Peter B\u00fchlmann, and Xinwei Shen",
        "link": "http://arxiv.org/abs/2505.12868v1",
        "abstract": "Distributional robustness is a central goal of prediction algorithms due to\nthe prevalent distribution shifts in real-world data. The prediction model aims\nto minimize the worst-case risk among a class of distributions, a.k.a., an\nuncertainty set. Causality provides a modeling framework with a rigorous\nrobustness guarantee in the above sense, where the uncertainty set is\ndata-driven rather than pre-specified as in traditional distributional\nrobustness optimization. However, current causality-inspired robustness methods\npossess finite-radius robustness guarantees only in the linear settings, where\nthe causal relationships among the covariates and the response are linear. In\nthis work, we propose a nonlinear method under a causal framework by\nincorporating recent developments in identifiable representation learning and\nestablish a distributional robustness guarantee. To our best knowledge, this is\nthe first causality-inspired robustness method with such a finite-radius\nrobustness guarantee in nonlinear settings. Empirical validation of the\ntheoretical findings is conducted on both synthetic data and real-world\nsingle-cell data, also illustrating that finite-radius robustness is crucial."
    },
    {
        "date": "2025-05",
        "title": "Robust Multimodal Segmentation with Representation Regularization and Hybrid Prototype Distillation",
        "author": "Jiaqi Tan, Xu Zheng, and Yang Liu",
        "link": "http://arxiv.org/abs/2505.12861v1",
        "abstract": "Multi-modal semantic segmentation (MMSS) faces significant challenges in\nreal-world scenarios due to dynamic environments, sensor failures, and noise\ninterference, creating a gap between theoretical models and practical\nperformance. To address this, we propose a two-stage framework called\nRobustSeg, which enhances multi-modal robustness through two key components:\nthe Hybrid Prototype Distillation Module (HPDM) and the Representation\nRegularization Module (RRM). In the first stage, RobustSeg pre-trains a\nmulti-modal teacher model using complete modalities. In the second stage, a\nstudent model is trained with random modality dropout while learning from the\nteacher via HPDM and RRM. HPDM transforms features into compact prototypes,\nenabling cross-modal hybrid knowledge distillation and mitigating bias from\nmissing modalities. RRM reduces representation discrepancies between the\nteacher and student by optimizing functional entropy through the log-Sobolev\ninequality. Extensive experiments on three public benchmarks demonstrate that\nRobustSeg outperforms previous state-of-the-art methods, achieving improvements\nof +2.76%, +4.56%, and +0.98%, respectively. Code is available at:\nhttps://github.com/RobustSeg/RobustSeg."
    },
    {
        "date": "2025-05",
        "title": "FLTG: Byzantine-Robust Federated Learning via Angle-Based Defense and Non-IID-Aware Weighting",
        "author": "Yanhua Wen, Lu Ai, Gang Liu, Chuang Li, and Jianhao Wei",
        "link": "http://arxiv.org/abs/2505.12851v1",
        "abstract": "Byzantine attacks during model aggregation in Federated Learning (FL)\nthreaten training integrity by manipulating malicious clients' updates.\nExisting methods struggle with limited robustness under high malicious client\nratios and sensitivity to non-i.i.d. data, leading to degraded accuracy. To\naddress this, we propose FLTG, a novel aggregation algorithm integrating\nangle-based defense and dynamic reference selection. FLTG first filters clients\nvia ReLU-clipped cosine similarity, leveraging a server-side clean dataset to\nexclude misaligned updates. It then dynamically selects a reference client\nbased on the prior global model to mitigate non-i.i.d. bias, assigns\naggregation weights inversely proportional to angular deviations, and\nnormalizes update magnitudes to suppress malicious scaling. Evaluations across\ndatasets of varying complexity under five classic attacks demonstrate FLTG's\nsuperiority over state-of-the-art methods under extreme bias scenarios and\nsustains robustness with a higher proportion(over 50%) of malicious clients."
    },
    {
        "date": "2025-05",
        "title": "VLC Fusion: Vision-Language Conditioned Sensor Fusion for Robust Object Detection",
        "author": "Aditya Taparia, Noel Ngu, Mario Leiva, Joshua Shay Kricheli, John Corcoran, Nathaniel D. Bastian, Gerardo Simari, Paulo Shakarian, and Ransalu Senanayake",
        "link": "http://arxiv.org/abs/2505.12715v1",
        "abstract": "Although fusing multiple sensor modalities can enhance object detection\nperformance, existing fusion approaches often overlook subtle variations in\nenvironmental conditions and sensor inputs. As a result, they struggle to\nadaptively weight each modality under such variations. To address this\nchallenge, we introduce Vision-Language Conditioned Fusion (VLC Fusion), a\nnovel fusion framework that leverages a Vision-Language Model (VLM) to\ncondition the fusion process on nuanced environmental cues. By capturing\nhigh-level environmental context such as as darkness, rain, and camera\nblurring, the VLM guides the model to dynamically adjust modality weights based\non the current scene. We evaluate VLC Fusion on real-world autonomous driving\nand military target detection datasets that include image, LIDAR, and mid-wave\ninfrared modalities. Our experiments show that VLC Fusion consistently\noutperforms conventional fusion baselines, achieving improved detection\naccuracy in both seen and unseen scenarios."
    },
    {
        "date": "2025-05",
        "title": "Writing a Good Security Paper for ISSCC (2025)",
        "author": "Utsav Banerjee, Chiraag Juvekar, Yong Ki Lee, Leibo Liu, Sanu Mathew, Thomas Poeppelmann, Shreyas Sen, Takeshi Sugawara, Ingrid Verbauwhede, and Rabia Tugce Yazicigil",
        "link": "http://arxiv.org/abs/2505.12700v1",
        "abstract": "Security is increasingly more important in designing chips and systems based\non them, and the International Solid-State Circuits Conference (ISSCC), the\nleading conference for presenting advances in solid-state circuits and\nsemiconductor technology, is committed to hardware security by establishing the\nsecurity subcommittee since 2024. In the past two years, the authors of this\npaper reviewed submissions as members of the Security Subcommittee, a part of\nInternational Technical Program Committee (ITPC). This paper aims to encourage\nhigh-quality submissions to grow this field in the overall scope of the ISSCC."
    },
    {
        "date": "2025-05",
        "title": "Shielding Latent Face Representations From Privacy Attacks",
        "author": "Arjun Ramesh Kaushik, Bharat Chandra Yalavarthi, Arun Ross, Vishnu Boddeti, and Nalini Ratha",
        "link": "http://arxiv.org/abs/2505.12688v1",
        "abstract": "In today's data-driven analytics landscape, deep learning has become a\npowerful tool, with latent representations, known as embeddings, playing a\ncentral role in several applications. In the face analytics domain, such\nembeddings are commonly used for biometric recognition (e.g., face\nidentification). However, these embeddings, or templates, can inadvertently\nexpose sensitive attributes such as age, gender, and ethnicity. Leaking such\ninformation can compromise personal privacy and affect civil liberty and human\nrights. To address these concerns, we introduce a multi-layer protection\nframework for embeddings. It consists of a sequence of operations: (a)\nencrypting embeddings using Fully Homomorphic Encryption (FHE), and (b) hashing\nthem using irreversible feature manifold hashing. Unlike conventional\nencryption methods, FHE enables computations directly on encrypted data,\nallowing downstream analytics while maintaining strong privacy guarantees. To\nreduce the overhead of encrypted processing, we employ embedding compression.\nOur proposed method shields latent representations of sensitive data from\nleaking private attributes (such as age and gender) while retaining essential\nfunctional capabilities (such as face identification). Extensive experiments on\ntwo datasets using two face encoders demonstrate that our approach outperforms\nseveral state-of-the-art privacy protection methods."
    },
    {
        "date": "2025-05",
        "title": "RoVo: Robust Voice Protection Against Unauthorized Speech Synthesis with Embedding-Level Perturbations",
        "author": "Seungmin Kim, Sohee Park, Donghyun Kim, Jisu Lee, and Daeseon Choi",
        "link": "http://arxiv.org/abs/2505.12686v1",
        "abstract": "With the advancement of AI-based speech synthesis technologies such as Deep\nVoice, there is an increasing risk of voice spoofing attacks, including voice\nphishing and fake news, through unauthorized use of others' voices. Existing\ndefenses that inject adversarial perturbations directly into audio signals have\nlimited effectiveness, as these perturbations can easily be neutralized by\nspeech enhancement methods. To overcome this limitation, we propose RoVo\n(Robust Voice), a novel proactive defense technique that injects adversarial\nperturbations into high-dimensional embedding vectors of audio signals,\nreconstructing them into protected speech. This approach effectively defends\nagainst speech synthesis attacks and also provides strong resistance to speech\nenhancement models, which represent a secondary attack threat.\n  In extensive experiments, RoVo increased the Defense Success Rate (DSR) by\nover 70% compared to unprotected speech, across four state-of-the-art speech\nsynthesis models. Specifically, RoVo achieved a DSR of 99.5% on a commercial\nspeaker-verification API, effectively neutralizing speech synthesis attack.\nMoreover, RoVo's perturbations remained robust even under strong speech\nenhancement conditions, outperforming traditional methods. A user study\nconfirmed that RoVo preserves both naturalness and usability of protected\nspeech, highlighting its effectiveness in complex and evolving threat\nscenarios."
    },
    {
        "date": "2025-05",
        "title": "RoFL: Robust Fingerprinting of Language Models",
        "author": "Yun-Yun Tsai, Chuan Guo, Junfeng Yang, and Laurens van der Maaten",
        "link": "http://arxiv.org/abs/2505.12682v1",
        "abstract": "AI developers are releasing large language models (LLMs) under a variety of\ndifferent licenses. Many of these licenses restrict the ways in which the\nmodels or their outputs may be used. This raises the question how license\nviolations may be recognized. In particular, how can we identify that an API or\nproduct uses (an adapted version of) a particular LLM? We present a new method\nthat enable model developers to perform such identification via fingerprints:\nstatistical patterns that are unique to the developer's model and robust to\ncommon alterations of that model. Our method permits model identification in a\nblack-box setting using a limited number of queries, enabling identification of\nmodels that can only be accessed via an API or product. The fingerprints are\nnon-invasive: our method does not require any changes to the model during\ntraining, hence by design, it does not impact model quality. Empirically, we\nfind our method provides a high degree of robustness to common changes in the\nmodel or inference settings. In our experiments, it substantially outperforms\nprior art, including invasive methods that explicitly train watermarks into the\nmodel."
    },
    {
        "date": "2025-05",
        "title": "On the Mechanisms of Adversarial Data Augmentation for Robust and Adaptive Transfer Learning",
        "author": "Hana Satou, and Alan Mitkiy",
        "link": "http://arxiv.org/abs/2505.12681v1",
        "abstract": "Transfer learning across domains with distribution shift remains a\nfundamental challenge in building robust and adaptable machine learning\nsystems. While adversarial perturbations are traditionally viewed as threats\nthat expose model vulnerabilities, recent studies suggest that they can also\nserve as constructive tools for data augmentation. In this work, we\nsystematically investigate the role of adversarial data augmentation (ADA) in\nenhancing both robustness and adaptivity in transfer learning settings. We\nanalyze how adversarial examples, when used strategically during training,\nimprove domain generalization by enriching decision boundaries and reducing\noverfitting to source-domain-specific features. We further propose a unified\nframework that integrates ADA with consistency regularization and\ndomain-invariant representation learning. Extensive experiments across multiple\nbenchmark datasets -- including VisDA, DomainNet, and Office-Home --\ndemonstrate that our method consistently improves target-domain performance\nunder both unsupervised and few-shot domain adaptation settings. Our results\nhighlight a constructive perspective of adversarial learning, transforming\nperturbation from a destructive attack into a regularizing force for\ncross-domain transferability."
    },
    {
        "date": "2025-05",
        "title": "Use as Many Surrogates as You Want: Selective Ensemble Attack to Unleash Transferability without Sacrificing Resource Efficiency",
        "author": "Bo Yang, Hengwei Zhang, Jindong Wang, Yuchen Ren, Chenhao Lin, Chao Shen, and Zhengyu Zhao",
        "link": "http://arxiv.org/abs/2505.12644v1",
        "abstract": "In surrogate ensemble attacks, using more surrogate models yields higher\ntransferability but lower resource efficiency. This practical trade-off between\ntransferability and efficiency has largely limited existing attacks despite\nmany pre-trained models are easily accessible online. In this paper, we argue\nthat such a trade-off is caused by an unnecessary common assumption, i.e., all\nmodels should be identical across iterations. By lifting this assumption, we\ncan use as many surrogates as we want to unleash transferability without\nsacrificing efficiency. Concretely, we propose Selective Ensemble Attack (SEA),\nwhich dynamically selects diverse models (from easily accessible pre-trained\nmodels) across iterations based on our new interpretation of decoupling\nwithin-iteration and cross-iteration model diversity.In this way, the number of\nwithin-iteration models is fixed for maintaining efficiency, while only\ncross-iteration model diversity is increased for higher transferability.\nExperiments on ImageNet demonstrate the superiority of SEA in various\nscenarios. For example, when dynamically selecting 4 from 20 accessible models,\nSEA yields 8.5% higher transferability than existing attacks under the same\nefficiency. The superiority of SEA also generalizes to real-world systems, such\nas commercial vision APIs and large vision-language models. Overall, SEA opens\nup the possibility of adaptively balancing transferability and efficiency\naccording to specific resource requirements."
    },
    {
        "date": "2025-05",
        "title": "Two out of Three (ToT): using self-consistency to make robust predictions",
        "author": "Jung Hoon Lee, and Sujith Vijayan",
        "link": "http://arxiv.org/abs/2505.12642v1",
        "abstract": "Deep learning (DL) can automatically construct intelligent agents, deep\nneural networks (alternatively, DL models), that can outperform humans in\ncertain tasks. However, the operating principles of DL remain poorly\nunderstood, making its decisions incomprehensible. As a result, it poses a\ngreat risk to deploy DL in high-stakes domains in which mistakes or errors may\nlead to critical consequences. Here, we aim to develop an algorithm that can\nhelp DL models make more robust decisions by allowing them to abstain from\nanswering when they are uncertain. Our algorithm, named `Two out of Three\n(ToT)', is inspired by the sensitivity of the human brain to conflicting\ninformation. ToT creates two alternative predictions in addition to the\noriginal model prediction and uses the alternative predictions to decide\nwhether it should provide an answer or not."
    },
    {
        "date": "2025-05",
        "title": "hChain: Blockchain Based Large Scale EHR Data Sharing with Enhanced Security and Privacy",
        "author": "Musharraf Alruwaill, Saraju Mohanty, and Elias Kougianos",
        "link": "http://arxiv.org/abs/2505.12610v1",
        "abstract": "Concerns regarding privacy and data security in conventional healthcare\nprompted alternative technologies. In smart healthcare, blockchain technology\naddresses existing concerns with security, privacy, and electronic healthcare\ntransmission. Integration of Blockchain Technology with the Internet of Medical\nThings (IoMT) allows real-time monitoring of protected healthcare data.\nUtilizing edge devices with IoMT devices is very advantageous for addressing\nsecurity, computing, and storage challenges. Encryption using symmetric and\nasymmetric keys is used to conceal sensitive information from unauthorized\nparties. SHA256 is an algorithm for one-way hashing. It is used to verify that\nthe data has not been altered, since if it had, the hash value would have\nchanged. This article offers a blockchain-based smart healthcare system using\nIoMT devices for continuous patient monitoring. In addition, it employs edge\nresources in addition to IoMT devices to have extra computing power and storage\nto hash and encrypt incoming data before sending it to the blockchain.\nSymmetric key is utilized to keep the data private even in the blockchain,\nallowing the patient to safely communicate the data through smart contracts\nwhile preventing unauthorized physicians from seeing the data. Through the use\nof a verification node and blockchain, an asymmetric key is used for the\nsigning and validation of patient data in the healthcare provider system. In\naddition to other security measures, location-based authentication is\nrecommended to guarantee that data originates from the patient area. Through\nthe edge device, SHA256 is utilized to secure the data's integrity and a secret\nkey is used to maintain its secrecy. The hChain architecture improves the\ncomputing power of IoMT environments, the security of EHR sharing through smart\ncontracts, and the privacy and authentication procedures."
    },
    {
        "date": "2025-05",
        "title": "A Few Large Shifts: Layer-Inconsistency Based Minimal Overhead Adversarial Example Detection",
        "author": "Sanggeon Yun, Ryozo Masukawa, Hyunwoo Oh, Nathaniel D. Bastian, and Mohsen Imani",
        "link": "http://arxiv.org/abs/2505.12586v2",
        "abstract": "Deep neural networks (DNNs) are highly susceptible to adversarial\nexamples--subtle, imperceptible perturbations that can lead to incorrect\npredictions. While detection-based defenses offer a practical alternative to\nadversarial training, many existing methods depend on external models, complex\narchitectures, heavy augmentations, or adversarial data, limiting their\nefficiency and generalizability. We introduce a lightweight, plug-in detection\nframework that leverages internal layer-wise inconsistencies within the target\nmodel itself, requiring only benign data for calibration. Our approach is\ngrounded in the A Few Large Shifts Assumption, which posits that adversarial\nperturbations typically induce large representation shifts in a small subset of\nlayers. Building on this, we propose two complementary strategies--Recovery\nTesting (RT) and Logit-layer Testing (LT)--to expose internal disruptions\ncaused by adversaries. Evaluated on CIFAR-10, CIFAR-100, and ImageNet under\nboth standard and adaptive threat models, our method achieves state-of-the-art\ndetection performance with negligible computational overhead and no compromise\nto clean accuracy."
    },
    {
        "date": "2025-05",
        "title": "Learning Robust Spectral Dynamics for Temporal Domain Generalization",
        "author": "En Yu, Jie Lu, Xiaoyu Yang, Guangquan Zhang, and Zhen Fang",
        "link": "http://arxiv.org/abs/2505.12585v1",
        "abstract": "Modern machine learning models struggle to maintain performance in dynamic\nenvironments where temporal distribution shifts, \\emph{i.e., concept drift},\nare prevalent. Temporal Domain Generalization (TDG) seeks to enable model\ngeneralization across evolving domains, yet existing approaches typically\nassume smooth incremental changes, struggling with complex real-world drifts\ninvolving long-term structure (incremental evolution/periodicity) and local\nuncertainties. To overcome these limitations, we introduce FreKoo, which\ntackles these challenges via a novel frequency-domain analysis of parameter\ntrajectories. It leverages the Fourier transform to disentangle parameter\nevolution into distinct spectral bands. Specifically, low-frequency component\nwith dominant dynamics are learned and extrapolated using the Koopman operator,\nrobustly capturing diverse drift patterns including both incremental and\nperiodicity. Simultaneously, potentially disruptive high-frequency variations\nare smoothed via targeted temporal regularization, preventing overfitting to\ntransient noise and domain uncertainties. In addition, this dual spectral\nstrategy is rigorously grounded through theoretical analysis, providing\nstability guarantees for the Koopman prediction, a principled Bayesian\njustification for the high-frequency regularization, and culminating in a\nmultiscale generalization bound connecting spectral dynamics to improved\ngeneralization. Extensive experiments demonstrate FreKoo's significant\nsuperiority over SOTA TDG approaches, particularly excelling in real-world\nstreaming scenarios with complex drifts and uncertainties."
    },
    {
        "date": "2025-05",
        "title": "A Survey of Attacks on Large Language Models",
        "author": "Wenrui Xu, and Keshab K. Parhi",
        "link": "http://arxiv.org/abs/2505.12567v1",
        "abstract": "Large language models (LLMs) and LLM-based agents have been widely deployed\nin a wide range of applications in the real world, including healthcare\ndiagnostics, financial analysis, customer support, robotics, and autonomous\ndriving, expanding their powerful capability of understanding, reasoning, and\ngenerating natural languages. However, the wide deployment of LLM-based\napplications exposes critical security and reliability risks, such as the\npotential for malicious misuse, privacy leakage, and service disruption that\nweaken user trust and undermine societal safety. This paper provides a\nsystematic overview of the details of adversarial attacks targeting both LLMs\nand LLM-based agents. These attacks are organized into three phases in LLMs:\nTraining-Phase Attacks, Inference-Phase Attacks, and Availability & Integrity\nAttacks. For each phase, we analyze the details of representative and recently\nintroduced attack methods along with their corresponding defenses. We hope our\nsurvey will provide a good tutorial and a comprehensive understanding of LLM\nsecurity, especially for attacks on LLMs. We desire to raise attention to the\nrisks inherent in widely deployed LLM-based applications and highlight the\nurgent need for robust mitigation strategies for evolving threats."
    },
    {
        "date": "2025-05",
        "title": "A Finite-Sample Analysis of Distributionally Robust Average-Reward Reinforcement Learning",
        "author": "Zachary Roch, Chi Zhang, George Atia, and Yue Wang",
        "link": "http://arxiv.org/abs/2505.12462v1",
        "abstract": "Robust reinforcement learning (RL) under the average-reward criterion is\ncrucial for long-term decision making under potential environment mismatches,\nyet its finite-sample complexity study remains largely unexplored. Existing\nworks offer algorithms with asymptotic guarantees, but the absence of\nfinite-sample analysis hinders its principled understanding and practical\ndeployment, especially in data-limited settings. We close this gap by proposing\nRobust Halpern Iteration (RHI), the first algorithm with provable finite-sample\ncomplexity guarantee. Under standard uncertainty sets -- including\ncontamination sets and $\\ell_p$-norm balls -- RHI attains an $\\epsilon$-optimal\npolicy with near-optimal sample complexity of $\\tilde{\\mathcal\nO}\\left(\\frac{SA\\mathcal H^{2}}{\\epsilon^{2}}\\right)$, where $S$ and $A$ denote\nthe numbers of states and actions, and $\\mathcal H$ is the robust optimal bias\nspan. This result gives the first polynomial sample complexity guarantee for\nrobust average-reward RL. Moreover, our RHI's independence from prior knowledge\ndistinguishes it from many previous average-reward RL studies. Our work thus\nconstitutes a significant advancement in enhancing the practical applicability\nof robust average-reward methods to complex, real-world problems."
    },
    {
        "date": "2025-05",
        "title": "SecEmb: Sparsity-Aware Secure Federated Learning of On-Device Recommender System with Large Embedding",
        "author": "Peihua Mai, Youlong Ding, Ziyan Lyu, Minxin Du, and Yan Pang",
        "link": "http://arxiv.org/abs/2505.12453v1",
        "abstract": "Federated recommender system (FedRec) has emerged as a solution to protect\nuser data through collaborative training techniques. A typical FedRec involves\ntransmitting the full model and entire weight updates between edge devices and\nthe server, causing significant burdens to devices with limited bandwidth and\ncomputational power. While the sparsity of embedding updates provides\nopportunity for payload optimization, existing sparsity-aware federated\nprotocols generally sacrifice privacy for efficiency. A key challenge in\ndesigning a secure sparsity-aware efficient protocol is to protect the rated\nitem indices from the server. In this paper, we propose a lossless secure\nrecommender systems on sparse embedding updates (SecEmb). SecEmb reduces user\npayload while ensuring that the server learns no information about both rated\nitem indices and individual updates except the aggregated model. The protocol\nconsists of two correlated modules: (1) a privacy-preserving embedding\nretrieval module that allows users to download relevant embeddings from the\nserver, and (2) an update aggregation module that securely aggregates updates\nat the server. Empirical analysis demonstrates that SecEmb reduces both\ndownload and upload communication costs by up to 90x and decreases user-side\ncomputation time by up to 70x compared with secure FedRec protocols.\nAdditionally, it offers non-negligible utility advantages compared with lossy\nmessage compression methods."
    },
    {
        "date": "2025-05",
        "title": "IP Leakage Attacks Targeting LLM-Based Multi-Agent Systems",
        "author": "Liwen Wang, Wenxuan Wang, Shuai Wang, Zongjie Li, Zhenlan Ji, Zongyi Lyu, Daoyuan Wu, and Shing-Chi Cheung",
        "link": "http://arxiv.org/abs/2505.12442v2",
        "abstract": "The rapid advancement of Large Language Models (LLMs) has led to the\nemergence of Multi-Agent Systems (MAS) to perform complex tasks through\ncollaboration. However, the intricate nature of MAS, including their\narchitecture and agent interactions, raises significant concerns regarding\nintellectual property (IP) protection. In this paper, we introduce MASLEAK, a\nnovel attack framework designed to extract sensitive information from MAS\napplications. MASLEAK targets a practical, black-box setting, where the\nadversary has no prior knowledge of the MAS architecture or agent\nconfigurations. The adversary can only interact with the MAS through its public\nAPI, submitting attack query $q$ and observing outputs from the final agent.\nInspired by how computer worms propagate and infect vulnerable network hosts,\nMASLEAK carefully crafts adversarial query $q$ to elicit, propagate, and retain\nresponses from each MAS agent that reveal a full set of proprietary components,\nincluding the number of agents, system topology, system prompts, task\ninstructions, and tool usages. We construct the first synthetic dataset of MAS\napplications with 810 applications and also evaluate MASLEAK against real-world\nMAS applications, including Coze and CrewAI. MASLEAK achieves high accuracy in\nextracting MAS IP, with an average attack success rate of 87% for system\nprompts and task instructions, and 92% for system architecture in most cases.\nWe conclude by discussing the implications of our findings and the potential\ndefenses."
    },
    {
        "date": "2025-05",
        "title": "EvoGPT: Enhancing Test Suite Robustness via LLM-Based Generation and Genetic Optimization",
        "author": "Lior Broide, and Roni Stern",
        "link": "http://arxiv.org/abs/2505.12424v1",
        "abstract": "Large Language Models (LLMs) have recently emerged as promising tools for\nautomated unit test generation. We introduce a hybrid framework called EvoGPT\nthat integrates LLM-based test generation with evolutionary search techniques\nto create diverse, fault-revealing unit tests. Unit tests are initially\ngenerated with diverse temperature sampling to maximize behavioral and test\nsuite diversity, followed by a generation-repair loop and coverage-guided\nassertion enhancement. The resulting test suites are evolved using genetic\nalgorithms, guided by a fitness function prioritizing mutation score over\ntraditional coverage metrics. This design emphasizes the primary objective of\nunit testing-fault detection. Evaluated on multiple open-source Java projects,\nEvoGPT achieves an average improvement of 10% in both code coverage and\nmutation score compared to LLMs and traditional search-based software testing\nbaselines. These results demonstrate that combining LLM-driven diversity,\ntargeted repair, and evolutionary optimization produces more effective and\nresilient test suites."
    },
    {
        "date": "2025-05",
        "title": "CAPTURE: Context-Aware Prompt Injection Testing and Robustness Enhancement",
        "author": "Gauri Kholkar, and Ratinder Ahuja",
        "link": "http://arxiv.org/abs/2505.12368v1",
        "abstract": "Prompt injection remains a major security risk for large language models.\nHowever, the efficacy of existing guardrail models in context-aware settings\nremains underexplored, as they often rely on static attack benchmarks.\nAdditionally, they have over-defense tendencies. We introduce CAPTURE, a novel\ncontext-aware benchmark assessing both attack detection and over-defense\ntendencies with minimal in-domain examples. Our experiments reveal that current\nprompt injection guardrail models suffer from high false negatives in\nadversarial cases and excessive false positives in benign scenarios,\nhighlighting critical limitations."
    },
    {
        "date": "2025-05",
        "title": "VoiceCloak: A Multi-Dimensional Defense Framework against Unauthorized Diffusion-based Voice Cloning",
        "author": "Qianyue Hu, Junyan Wu, Wei Lu, and Xiangyang Luo",
        "link": "http://arxiv.org/abs/2505.12332v1",
        "abstract": "Diffusion Models (DMs) have achieved remarkable success in realistic voice\ncloning (VC), while they also increase the risk of malicious misuse. Existing\nproactive defenses designed for traditional VC models aim to disrupt the\nforgery process, but they have been proven incompatible with DMs due to the\nintricate generative mechanisms of diffusion. To bridge this gap, we introduce\nVoiceCloak, a multi-dimensional proactive defense framework with the goal of\nobfuscating speaker identity and degrading perceptual quality in potential\nunauthorized VC. To achieve these goals, we conduct a focused analysis to\nidentify specific vulnerabilities within DMs, allowing VoiceCloak to disrupt\nthe cloning process by introducing adversarial perturbations into the reference\naudio. Specifically, to obfuscate speaker identity, VoiceCloak first targets\nspeaker identity by distorting representation learning embeddings to maximize\nidentity variation, which is guided by auditory perception principles.\nAdditionally, VoiceCloak disrupts crucial conditional guidance processes,\nparticularly attention context, thereby preventing the alignment of vocal\ncharacteristics that are essential for achieving convincing cloning. Then, to\naddress the second objective, VoiceCloak introduces score magnitude\namplification to actively steer the reverse trajectory away from the generation\nof high-quality speech. Noise-guided semantic corruption is further employed to\ndisrupt structural speech semantics captured by DMs, degrading output quality.\nExtensive experiments highlight VoiceCloak's outstanding defense success rate\nagainst unauthorized diffusion-based voice cloning. Audio samples of VoiceCloak\nare available at https://voice-cloak.github.io/VoiceCloak/."
    },
    {
        "date": "2025-05",
        "title": "Robust Planning for Autonomous Driving via Mixed Adversarial Diffusion Predictions",
        "author": "Albert Zhao, and Stefano Soatto",
        "link": "http://arxiv.org/abs/2505.12327v1",
        "abstract": "We describe a robust planning method for autonomous driving that mixes normal\nand adversarial agent predictions output by a diffusion model trained for\nmotion prediction. We first train a diffusion model to learn an unbiased\ndistribution of normal agent behaviors. We then generate a distribution of\nadversarial predictions by biasing the diffusion model at test time to generate\npredictions that are likely to collide with a candidate plan. We score plans\nusing expected cost with respect to a mixture distribution of normal and\nadversarial predictions, leading to a planner that is robust against\nadversarial behaviors but not overly conservative when agents behave normally.\nUnlike current approaches, we do not use risk measures that over-weight\nadversarial behaviors while placing little to no weight on low-cost normal\nbehaviors or use hard safety constraints that may not be appropriate for all\ndriving scenarios. We show the effectiveness of our method on single-agent and\nmulti-agent jaywalking scenarios as well as a red light violation scenario."
    },
    {
        "date": "2025-05",
        "title": "Improving Out-of-Domain Robustness with Targeted Augmentation in Frequency and Pixel Spaces",
        "author": "Ruoqi Wang, Haitao Wang, Shaojie Guo, and Qiong Luo",
        "link": "http://arxiv.org/abs/2505.12317v1",
        "abstract": "Out-of-domain (OOD) robustness under domain adaptation settings, where\nlabeled source data and unlabeled target data come from different\ndistributions, is a key challenge in real-world applications. A common approach\nto improving OOD robustness is through data augmentations. However, in\nreal-world scenarios, models trained with generic augmentations can only\nimprove marginally when generalized under distribution shifts toward unlabeled\ntarget domains. While dataset-specific targeted augmentations can address this\nissue, they typically require expert knowledge and extensive prior data\nanalysis to identify the nature of the datasets and domain shift. To address\nthese challenges, we propose Frequency-Pixel Connect, a domain-adaptation\nframework that enhances OOD robustness by introducing a targeted augmentation\nin both the frequency space and pixel space. Specifically, we mix the amplitude\nspectrum and pixel content of a source image and a target image to generate\naugmented samples that introduce domain diversity while preserving the semantic\nstructure of the source image. Unlike previous targeted augmentation methods\nthat are both dataset-specific and limited to the pixel space, Frequency-Pixel\nConnect is dataset-agnostic, enabling broader and more flexible applicability\nbeyond natural image datasets. We further analyze the effectiveness of\nFrequency-Pixel Connect by evaluating the performance of our method connecting\nsame-class cross-domain samples while separating different-class examples. We\ndemonstrate that Frequency-Pixel Connect significantly improves cross-domain\nconnectivity and outperforms previous generic methods on four diverse\nreal-world benchmarks across vision, medical, audio, and astronomical domains,\nand it also outperforms other dataset-specific targeted augmentation methods."
    },
    {
        "date": "2025-05",
        "title": "From Low Field to High Value: Robust Cortical Mapping from Low-Field MRI",
        "author": "Karthik Gopinath, Annabel Sorby-Adams, Jonathan W. Ramirez, Dina Zemlyanker, Jennifer Guo, David Hunt, Christine L. Mac Donald, C. Dirk Keene, Timothy Coalson, Matthew F. Glasser, David Van Essen, Matthew S. Rosen, Oula Puonti, W. Taylor Kimberly, and Juan Eugenio Iglesias",
        "link": "http://arxiv.org/abs/2505.12228v1",
        "abstract": "Three-dimensional reconstruction of cortical surfaces from MRI for\nmorphometric analysis is fundamental for understanding brain structure. While\nhigh-field MRI (HF-MRI) is standard in research and clinical settings, its\nlimited availability hinders widespread use. Low-field MRI (LF-MRI),\nparticularly portable systems, offers a cost-effective and accessible\nalternative. However, existing cortical surface analysis tools are optimized\nfor high-resolution HF-MRI and struggle with the lower signal-to-noise ratio\nand resolution of LF-MRI. In this work, we present a machine learning method\nfor 3D reconstruction and analysis of portable LF-MRI across a range of\ncontrasts and resolutions. Our method works \"out of the box\" without\nretraining. It uses a 3D U-Net trained on synthetic LF-MRI to predict signed\ndistance functions of cortical surfaces, followed by geometric processing to\nensure topological accuracy. We evaluate our method using paired HF/LF-MRI\nscans of the same subjects, showing that LF-MRI surface reconstruction accuracy\ndepends on acquisition parameters, including contrast type (T1 vs T2),\norientation (axial vs isotropic), and resolution. A 3mm isotropic T2-weighted\nscan acquired in under 4 minutes, yields strong agreement with HF-derived\nsurfaces: surface area correlates at r=0.96, cortical parcellations reach\nDice=0.98, and gray matter volume achieves r=0.93. Cortical thickness remains\nmore challenging with correlations up to r=0.70, reflecting the difficulty of\nsub-mm precision with 3mm voxels. We further validate our method on challenging\npostmortem LF-MRI, demonstrating its robustness. Our method represents a step\ntoward enabling cortical surface analysis on portable LF-MRI. Code is available\nat https://surfer.nmr.mgh.harvard.edu/fswiki/ReconAny"
    },
    {
        "date": "2025-05",
        "title": "Near-Optimal Sample Complexities of Divergence-based S-rectangular Distributionally Robust Reinforcement Learning",
        "author": "Zhenghao Li, Shengbo Wang, and Nian Si",
        "link": "http://arxiv.org/abs/2505.12202v1",
        "abstract": "Distributionally robust reinforcement learning (DR-RL) has recently gained\nsignificant attention as a principled approach that addresses discrepancies\nbetween training and testing environments. To balance robustness, conservatism,\nand computational traceability, the literature has introduced DR-RL models with\nSA-rectangular and S-rectangular adversaries. While most existing statistical\nanalyses focus on SA-rectangular models, owing to their algorithmic simplicity\nand the optimality of deterministic policies, S-rectangular models more\naccurately capture distributional discrepancies in many real-world applications\nand often yield more effective robust randomized policies. In this paper, we\nstudy the empirical value iteration algorithm for divergence-based\nS-rectangular DR-RL and establish near-optimal sample complexity bounds of\n$\\widetilde{O}(|\\mathcal{S}||\\mathcal{A}|(1-\\gamma)^{-4}\\varepsilon^{-2})$,\nwhere $\\varepsilon$ is the target accuracy, $|\\mathcal{S}|$ and $|\\mathcal{A}|$\ndenote the cardinalities of the state and action spaces, and $\\gamma$ is the\ndiscount factor. To the best of our knowledge, these are the first sample\ncomplexity results for divergence-based S-rectangular models that achieve\noptimal dependence on $|\\mathcal{S}|$, $|\\mathcal{A}|$, and $\\varepsilon$\nsimultaneously. We further validate this theoretical dependence through\nnumerical experiments on a robust inventory control problem and a theoretical\nworst-case example, demonstrating the fast learning performance of our proposed\nalgorithm."
    },
    {
        "date": "2025-05",
        "title": "Always Clear Depth: Robust Monocular Depth Estimation under Adverse Weather",
        "author": "Kui Jiang, Jing Cao, Zhaocheng Yu, Junjun Jiang, and Jingchun Zhou",
        "link": "http://arxiv.org/abs/2505.12199v1",
        "abstract": "Monocular depth estimation is critical for applications such as autonomous\ndriving and scene reconstruction. While existing methods perform well under\nnormal scenarios, their performance declines in adverse weather, due to\nchallenging domain shifts and difficulties in extracting scene information. To\naddress this issue, we present a robust monocular depth estimation method\ncalled \\textbf{ACDepth} from the perspective of high-quality training data\ngeneration and domain adaptation. Specifically, we introduce a one-step\ndiffusion model for generating samples that simulate adverse weather\nconditions, constructing a multi-tuple degradation dataset during training. To\nensure the quality of the generated degradation samples, we employ LoRA\nadapters to fine-tune the generation weights of diffusion model. Additionally,\nwe integrate circular consistency loss and adversarial training to guarantee\nthe fidelity and naturalness of the scene contents. Furthermore, we elaborate\non a multi-granularity knowledge distillation strategy (MKD) that encourages\nthe student network to absorb knowledge from both the teacher model and\npretrained Depth Anything V2. This strategy guides the student model in\nlearning degradation-agnostic scene information from various degradation\ninputs. In particular, we introduce an ordinal guidance distillation mechanism\n(OGD) that encourages the network to focus on uncertain regions through\ndifferential ranking, leading to a more precise depth estimation. Experimental\nresults demonstrate that our ACDepth surpasses md4all-DD by 2.50\\% for night\nscene and 2.61\\% for rainy scene on the nuScenes dataset in terms of the absRel\nmetric."
    },
    {
        "date": "2025-05",
        "title": "BenSParX: A Robust Explainable Machine Learning Framework for Parkinson's Disease Detection from Bengali Conversational Speech",
        "author": "Riad Hossain, Muhammad Ashad Kabir, Arat Ibne Golam Mowla, Animesh Chandra Roy, and Ranjit Kumar Ghosh",
        "link": "http://arxiv.org/abs/2505.12192v1",
        "abstract": "Parkinson's disease (PD) poses a growing global health challenge, with\nBangladesh experiencing a notable rise in PD-related mortality. Early detection\nof PD remains particularly challenging in resource-constrained settings, where\nvoice-based analysis has emerged as a promising non-invasive and cost-effective\nalternative. However, existing studies predominantly focus on English or other\nmajor languages; notably, no voice dataset for PD exists for Bengali - posing a\nsignificant barrier to culturally inclusive and accessible healthcare\nsolutions. Moreover, most prior studies employed only a narrow set of acoustic\nfeatures, with limited or no hyperparameter tuning and feature selection\nstrategies, and little attention to model explainability. This restricts the\ndevelopment of a robust and generalizable machine learning model. To address\nthis gap, we present BenSparX, the first Bengali conversational speech dataset\nfor PD detection, along with a robust and explainable machine learning\nframework tailored for early diagnosis. The proposed framework incorporates\ndiverse acoustic feature categories, systematic feature selection methods, and\nstate-of-the-art machine learning algorithms with extensive hyperparameter\noptimization. Furthermore, to enhance interpretability and trust in model\npredictions, the framework incorporates SHAP (SHapley Additive exPlanations)\nanalysis to quantify the contribution of individual acoustic features toward PD\ndetection. Our framework achieves state-of-the-art performance, yielding an\naccuracy of 95.77%, F1 score of 95.57%, and AUC-ROC of 0.982. We further\nexternally validated our approach by applying the framework to existing PD\ndatasets in other languages, where it consistently outperforms state-of-the-art\napproaches. To facilitate further research and reproducibility, the dataset has\nbeen made publicly available at https://github.com/Riad071/BenSParX."
    },
    {
        "date": "2025-05",
        "title": "Ditch the Denoiser: Emergence of Noise Robustness in Self-Supervised Learning from Data Curriculum",
        "author": "Wenquan Lu, Jiaqi Zhang, Hugues Van Assel, and Randall Balestriero",
        "link": "http://arxiv.org/abs/2505.12191v1",
        "abstract": "Self-Supervised Learning (SSL) has become a powerful solution to extract rich\nrepresentations from unlabeled data. Yet, SSL research is mostly focused on\nclean, curated and high-quality datasets. As a result, applying SSL on noisy\ndata remains a challenge, despite being crucial to applications such as\nastrophysics, medical imaging, geophysics or finance. In this work, we present\na fully self-supervised framework that enables noise-robust representation\nlearning without requiring a denoiser at inference or downstream fine-tuning.\nOur method first trains an SSL denoiser on noisy data, then uses it to\nconstruct a denoised-to-noisy data curriculum (i.e., training first on\ndenoised, then noisy samples) for pretraining a SSL backbone (e.g., DINOv2),\ncombined with a teacher-guided regularization that anchors noisy embeddings to\ntheir denoised counterparts. This process encourages the model to internalize\nnoise robustness. Notably, the denoiser can be discarded after pretraining,\nsimplifying deployment. On ImageNet-1k with ViT-B under extreme Gaussian noise\n($\\sigma=255$, SNR = 0.72 dB), our method improves linear probing accuracy by\n4.8% over DINOv2, demonstrating that denoiser-free robustness can emerge from\nnoise-aware pretraining. The code is available at\nhttps://github.com/wenquanlu/noisy_dinov2."
    },
    {
        "date": "2025-05",
        "title": "EVALOOP: Assessing LLM Robustness in Programming from a Self-consistency Perspective",
        "author": "Sen Fang, Weiyuan Ding, and Bowen Xu",
        "link": "http://arxiv.org/abs/2505.12185v1",
        "abstract": "Assessing the programming capabilities of Large Language Models (LLMs) is\ncrucial for their effective use in software engineering. Current evaluations,\nhowever, predominantly measure the accuracy of generated code on static\nbenchmarks, neglecting the critical aspect of model robustness during\nprogramming tasks. While adversarial attacks offer insights on model\nrobustness, their effectiveness is limited and evaluation could be constrained.\nCurrent adversarial attack methods for robustness evaluation yield inconsistent\nresults, struggling to provide a unified evaluation across different LLMs. We\nintroduce EVALOOP, a novel assessment framework that evaluate the robustness\nfrom a self-consistency perspective, i.e., leveraging the natural duality\ninherent in popular software engineering tasks, e.g., code generation and code\nsummarization. EVALOOP initiates a self-contained feedback loop: an LLM\ngenerates output (e.g., code) from an input (e.g., natural language\nspecification), and then use the generated output as the input to produce a new\noutput (e.g., summarizes that code into a new specification). EVALOOP repeats\nthe process to assess the effectiveness of EVALOOP in each loop. This cyclical\nstrategy intrinsically evaluates robustness without rely on any external attack\nsetups, providing a unified metric to evaluate LLMs' robustness in programming.\nWe evaluate 16 prominent LLMs (e.g., GPT-4.1, O4-mini) on EVALOOP and found\nthat EVALOOP typically induces a 5.01%-19.31% absolute drop in pass@1\nperformance within ten loops. Intriguingly, robustness does not always align\nwith initial performance (i.e., one-time query); for instance, GPT-3.5-Turbo,\ndespite superior initial code generation compared to DeepSeek-V2, demonstrated\nlower robustness over repeated evaluation loop."
    },
    {
        "date": "2025-05",
        "title": "FABLE: A Localized, Targeted Adversarial Attack on Weather Forecasting Models",
        "author": "Yue Deng, Asadullah Hill Galib, Xin Lan, Pang-Ning Tan, and Lifeng Luo",
        "link": "http://arxiv.org/abs/2505.12167v1",
        "abstract": "Deep learning-based weather forecasting models have recently demonstrated\nsignificant performance improvements over gold-standard physics-based\nsimulation tools. However, these models are vulnerable to adversarial attacks,\nwhich raises concerns about their trustworthiness. In this paper, we first\ninvestigate the feasibility of applying existing adversarial attack methods to\nweather forecasting models. We argue that a successful attack should (1) not\nmodify significantly its original inputs, (2) be faithful, i.e., achieve the\ndesired forecast at targeted locations with minimal changes to non-targeted\nlocations, and (3) be geospatio-temporally realistic. However, balancing these\ncriteria is a challenge as existing methods are not designed to preserve the\ngeospatio-temporal dependencies of the original samples. To address this\nchallenge, we propose a novel framework called FABLE (Forecast Alteration By\nLocalized targeted advErsarial attack), which employs a 3D discrete wavelet\ndecomposition to extract the varying components of the geospatio-temporal data.\nBy regulating the magnitude of adversarial perturbations across different\ncomponents, FABLE can generate adversarial inputs that maintain\ngeospatio-temporal coherence while remaining faithful and closely aligned with\nthe original inputs. Experimental results on multiple real-world datasets\ndemonstrate the effectiveness of our framework over baseline methods across\nvarious metrics."
    },
    {
        "date": "2025-05",
        "title": "SoftPQ: Robust Instance Segmentation Evaluation via Soft Matching and Tunable Thresholds",
        "author": "Ranit Karmakar, and Simon F. N\u00f8rrelykke",
        "link": "http://arxiv.org/abs/2505.12155v1",
        "abstract": "Segmentation evaluation metrics traditionally rely on binary decision logic:\npredictions are either correct or incorrect, based on rigid IoU thresholds.\nDetection--based metrics such as F1 and mAP determine correctness at the object\nlevel using fixed overlap cutoffs, while overlap--based metrics like\nIntersection over Union (IoU) and Dice operate at the pixel level, often\noverlooking instance--level structure. Panoptic Quality (PQ) attempts to unify\ndetection and segmentation assessment, but it remains dependent on\nhard-threshold matching--treating predictions below the threshold as entirely\nincorrect. This binary framing obscures important distinctions between\nqualitatively different errors and fails to reward gradual model improvements.\nWe propose SoftPQ, a flexible and interpretable instance segmentation metric\nthat redefines evaluation as a graded continuum rather than a binary\nclassification. SoftPQ introduces tunable upper and lower IoU thresholds to\ndefine a partial matching region and applies a sublinear penalty function to\nambiguous or fragmented predictions. These extensions allow SoftPQ to exhibit\nsmoother score behavior, greater robustness to structural segmentation errors,\nand more informative feedback for model development and evaluation. Through\ncontrolled perturbation experiments, we show that SoftPQ captures meaningful\ndifferences in segmentation quality that existing metrics overlook, making it a\npractical and principled alternative for both benchmarking and iterative model\nrefinement."
    },
    {
        "date": "2025-05",
        "title": "T-Rex: Fitting a Robust Factor Model via Expectation-Maximization",
        "author": "Daniel Cederberg",
        "link": "http://arxiv.org/abs/2505.12117v1",
        "abstract": "Over the past decades, there has been a surge of interest in studying\nlow-dimensional structures within high-dimensional data. Statistical factor\nmodels $-$ i.e., low-rank plus diagonal covariance structures $-$ offer a\npowerful framework for modeling such structures. However, traditional methods\nfor fitting statistical factor models, such as principal component analysis\n(PCA) or maximum likelihood estimation assuming the data is Gaussian, are\nhighly sensitive to heavy tails and outliers in the observed data. In this\npaper, we propose a novel expectation-maximization (EM) algorithm for robustly\nfitting statistical factor models. Our approach is based on Tyler's M-estimator\nof the scatter matrix for an elliptical distribution, and consists of solving\nTyler's maximum likelihood estimation problem while imposing a structural\nconstraint that enforces the low-rank plus diagonal covariance structure. We\npresent numerical experiments on both synthetic and real examples,\ndemonstrating the robustness of our method for direction-of-arrival estimation\nin nonuniform noise and subspace recovery."
    },
    {
        "date": "2025-05",
        "title": "Improving Fairness in LLMs Through Testing-Time Adversaries",
        "author": "Isabela Pereira Gregio, Ian Pons, Anna Helena Reali Costa, and Artur Jord\u00e3o",
        "link": "http://arxiv.org/abs/2505.12100v1",
        "abstract": "Large Language Models (LLMs) push the bound-aries in natural language\nprocessing and generative AI, driving progress across various aspects of modern\nsociety. Unfortunately, the pervasive issue of bias in LLMs responses (i.e.,\npredictions) poses a significant and open challenge, hindering their\napplication in tasks involving ethical sensitivity and responsible\ndecision-making. In this work, we propose a straightforward, user-friendly and\npractical method to mitigate such biases, enhancing the reliability and\ntrustworthiness of LLMs. Our method creates multiple variations of a given\nsentence by modifying specific attributes and evaluates the corresponding\nprediction behavior compared to the original, unaltered, prediction/sentence.\nThe idea behind this process is that critical ethical predictions often exhibit\nnotable inconsistencies, indicating the presence of bias. Unlike previous\napproaches, our method relies solely on forward passes (i.e., testing-time\nadversaries), eliminating the need for training, fine-tuning, or prior\nknowledge of the training data distribution. Through extensive experiments on\nthe popular Llama family, we demonstrate the effectiveness of our method in\nimproving various fairness metrics, focusing on the reduction of disparities in\nhow the model treats individuals from different racial groups. Specifically,\nusing standard metrics, we improve the fairness in Llama3 in up to 27\npercentage points. Overall, our approach significantly enhances fairness,\nequity, and reliability in LLM-generated results without parameter tuning or\ntraining data modifications, confirming its effectiveness in practical\nscenarios. We believe our work establishes an important step toward enabling\nthe use of LLMs in tasks that require ethical considerations and responsible\ndecision-making."
    },
    {
        "date": "2025-05",
        "title": "FIGhost: Fluorescent Ink-based Stealthy and Flexible Backdoor Attacks on Physical Traffic Sign Recognition",
        "author": "Shuai Yuan, Guowen Xu, Hongwei Li, Rui Zhang, Xinyuan Qian, Wenbo Jiang, Hangcheng Cao, and Qingchuan Zhao",
        "link": "http://arxiv.org/abs/2505.12045v1",
        "abstract": "Traffic sign recognition (TSR) systems are crucial for autonomous driving but\nare vulnerable to backdoor attacks. Existing physical backdoor attacks either\nlack stealth, provide inflexible attack control, or ignore emerging\nVision-Large-Language-Models (VLMs). In this paper, we introduce FIGhost, the\nfirst physical-world backdoor attack leveraging fluorescent ink as triggers.\nFluorescent triggers are invisible under normal conditions and activated\nstealthily by ultraviolet light, providing superior stealthiness, flexibility,\nand untraceability. Inspired by real-world graffiti, we derive realistic\ntrigger shapes and enhance their robustness via an interpolation-based\nfluorescence simulation algorithm. Furthermore, we develop an automated\nbackdoor sample generation method to support three attack objectives. Extensive\nevaluations in the physical world demonstrate FIGhost's effectiveness against\nstate-of-the-art detectors and VLMs, maintaining robustness under environmental\nvariations and effectively evading existing defenses."
    },
    {
        "date": "2025-05",
        "title": "FL-PLAS: Federated Learning with Partial Layer Aggregation for Backdoor Defense Against High-Ratio Malicious Clients",
        "author": "Jianyi Zhang, Ziyin Zhou, Yilong Li, and Qichao Jin",
        "link": "http://arxiv.org/abs/2505.12019v1",
        "abstract": "Federated learning (FL) is gaining increasing attention as an emerging\ncollaborative machine learning approach, particularly in the context of\nlarge-scale computing and data systems. However, the fundamental algorithm of\nFL, Federated Averaging (FedAvg), is susceptible to backdoor attacks. Although\nresearchers have proposed numerous defense algorithms, two significant\nchallenges remain. The attack is becoming more stealthy and harder to detect,\nand current defense methods are unable to handle 50\\% or more malicious users\nor assume an auxiliary server dataset.\n  To address these challenges, we propose a novel defense algorithm, FL-PLAS,\n\\textbf{F}ederated \\textbf{L}earning based on \\textbf{P}artial\\textbf{ L}ayer\n\\textbf{A}ggregation \\textbf{S}trategy. In particular, we divide the local\nmodel into a feature extractor and a classifier. In each iteration, the clients\nonly upload the parameters of a feature extractor after local training. The\nserver then aggregates these local parameters and returns the results to the\nclients.\n  Each client retains its own classifier layer, ensuring that the backdoor\nlabels do not impact other clients. We assess the effectiveness of FL-PLAS\nagainst state-of-the-art (SOTA) backdoor attacks on three image datasets and\ncompare our approach to six defense strategies. The results of the experiment\ndemonstrate that our methods can effectively protect local models from backdoor\nattacks. Without requiring any auxiliary dataset for the server, our method\nachieves a high main-task accuracy with a lower backdoor accuracy even under\nthe condition of 90\\% malicious users with the attacks of trigger, semantic and\nedge-case."
    },
    {
        "date": "2025-05",
        "title": "Black-box Adversaries from Latent Space: Unnoticeable Attacks on Human Pose and Shape Estimation",
        "author": "Zhiying Li, Guanggang Geng, Yeying Jin, Zhizhi Guo, Bruce Gu, Jidong Huo, Zhaoxin Fan, and Wenjun Wu",
        "link": "http://arxiv.org/abs/2505.12009v1",
        "abstract": "Expressive human pose and shape (EHPS) estimation is vital for digital human\ngeneration, particularly in live-streaming applications. However, most existing\nEHPS models focus primarily on minimizing estimation errors, with limited\nattention on potential security vulnerabilities. Current adversarial attacks on\nEHPS models often require white-box access (e.g., model details or gradients)\nor generate visually conspicuous perturbations, limiting their practicality and\nability to expose real-world security threats. To address these limitations, we\npropose a novel Unnoticeable Black-Box Attack (UBA) against EHPS models. UBA\nleverages the latent-space representations of natural images to generate an\noptimal adversarial noise pattern and iteratively refine its attack potency\nalong an optimized direction in digital space. Crucially, this process relies\nsolely on querying the model's output, requiring no internal knowledge of the\nEHPS architecture, while guiding the noise optimization toward greater stealth\nand effectiveness. Extensive experiments and visual analyses demonstrate the\nsuperiority of UBA. Notably, UBA increases the pose estimation errors of EHPS\nmodels by 17.27%-58.21% on average, revealing critical vulnerabilities. These\nfindings underscore the urgent need to address and mitigate security risks\nassociated with digital human generation systems."
    },
    {
        "date": "2025-05",
        "title": "TechniqueRAG: Retrieval Augmented Generation for Adversarial Technique Annotation in Cyber Threat Intelligence Text",
        "author": "Ahmed Lekssays, Utsav Shukla, Husrev Taha Sencar, and Md Rizwan Parvez",
        "link": "http://arxiv.org/abs/2505.11988v1",
        "abstract": "Accurately identifying adversarial techniques in security texts is critical\nfor effective cyber defense. However, existing methods face a fundamental\ntrade-off: they either rely on generic models with limited domain precision or\nrequire resource-intensive pipelines that depend on large labeled datasets and\ntask-specific optimizations, such as custom hard-negative mining and denoising,\nresources rarely available in specialized domains.\n  We propose TechniqueRAG, a domain-specific retrieval-augmented generation\n(RAG) framework that bridges this gap by integrating off-the-shelf retrievers,\ninstruction-tuned LLMs, and minimal text-technique pairs. Our approach\naddresses data scarcity by fine-tuning only the generation component on limited\nin-domain examples, circumventing the need for resource-intensive retrieval\ntraining. While conventional RAG mitigates hallucination by coupling retrieval\nand generation, its reliance on generic retrievers often introduces noisy\ncandidates, limiting domain-specific precision. To address this, we enhance\nretrieval quality and domain specificity through zero-shot LLM re-ranking,\nwhich explicitly aligns retrieved candidates with adversarial techniques.\n  Experiments on multiple security benchmarks demonstrate that TechniqueRAG\nachieves state-of-the-art performance without extensive task-specific\noptimizations or labeled data, while comprehensive analysis provides further\ninsights."
    },
    {
        "date": "2025-05",
        "title": "Adversarial Robustness for Unified Multi-Modal Encoders via Efficient Calibration",
        "author": "Chih-Ting Liao, Bin Ren, Guofeng Mei, and Xu Zheng",
        "link": "http://arxiv.org/abs/2505.11895v1",
        "abstract": "Recent unified multi-modal encoders align a wide range of modalities into a\nshared representation space, enabling diverse cross-modal tasks. Despite their\nimpressive capabilities, the robustness of these models under adversarial\nperturbations remains underexplored, which is a critical concern for\nsafety-sensitive applications. In this work, we present the first comprehensive\nstudy of adversarial vulnerability in unified multi-modal encoders. We find\nthat even mild adversarial perturbations lead to substantial performance drops\nacross all modalities. Non-visual inputs, such as audio and point clouds, are\nespecially fragile, while visual inputs like images and videos also degrade\nsignificantly. To address this, we propose an efficient adversarial calibration\nframework that improves robustness across modalities without modifying\npretrained encoders or semantic centers, ensuring compatibility with existing\nfoundation models. Our method introduces modality-specific projection heads\ntrained solely on adversarial examples, while keeping the backbone and\nembeddings frozen. We explore three training objectives: fixed-center\ncross-entropy, clean-to-adversarial L2 alignment, and clean-adversarial\nInfoNCE, and we introduce a regularization strategy to ensure\nmodality-consistent alignment under attack. Experiments on six modalities and\nthree Bind-style models show that our method improves adversarial robustness by\nup to 47.3 percent at epsilon = 4/255, while preserving or even improving clean\nzero-shot and retrieval performance with less than 1 percent trainable\nparameters."
    },
    {
        "date": "2025-05",
        "title": "Facial Recognition Leveraging Generative Adversarial Networks",
        "author": "Zhongwen Li, Zongwei Li, and Xiaoqi Li",
        "link": "http://arxiv.org/abs/2505.11884v1",
        "abstract": "Face recognition performance based on deep learning heavily relies on\nlarge-scale training data, which is often difficult to acquire in practical\napplications. To address this challenge, this paper proposes a GAN-based data\naugmentation method with three key contributions: (1) a residual-embedded\ngenerator to alleviate gradient vanishing/exploding problems, (2) an Inception\nResNet-V1 based FaceNet discriminator for improved adversarial training, and\n(3) an end-to-end framework that jointly optimizes data generation and\nrecognition performance. Experimental results demonstrate that our approach\nachieves stable training dynamics and significantly improves face recognition\naccuracy by 12.7% on the LFW benchmark compared to baseline methods, while\nmaintaining good generalization capability with limited training samples."
    },
    {
        "date": "2025-05",
        "title": "AES-RV: Hardware-Efficient RISC-V Accelerator with Low-Latency AES Instruction Extension for IoT Security",
        "author": "Van Tinh Nguyen, Phuc Hung Pham, Vu Trung Duong Le, Hoai Luan Pham, Tuan Hai Vu, and Thi Diem Tran",
        "link": "http://arxiv.org/abs/2505.11880v1",
        "abstract": "The Advanced Encryption Standard (AES) is a widely adopted cryptographic\nalgorithm essential for securing embedded systems and IoT platforms. However,\nexisting AES hardware accelerators often face limitations in performance,\nenergy efficiency, and flexibility. This paper presents AES-RV, a\nhardware-efficient RISC-V accelerator featuring low-latency AES instruction\nextensions optimized for real-time processing across all AES modes and key\nsizes. AES-RV integrates three key innovations: high-bandwidth internal buffers\nfor continuous data processing, a specialized AES unit with custom low-latency\ninstructions, and a pipelined system supported by a ping-pong memory transfer\nmechanism. Implemented on the Xilinx ZCU102 SoC FPGA, AES-RV achieves up to\n255.97 times speedup and up to 453.04 times higher energy efficiency compared\nto baseline and conventional CPU/GPU platforms. It also demonstrates superior\nthroughput and area efficiency against state-of-the-art AES accelerators,\nmaking it a strong candidate for secure and high-performance embedded systems."
    },
    {
        "date": "2025-05",
        "title": "On Membership Inference Attacks in Knowledge Distillation",
        "author": "Ziyao Cui, Minxing Zhang, and Jian Pei",
        "link": "http://arxiv.org/abs/2505.11837v1",
        "abstract": "Nowadays, Large Language Models (LLMs) are trained on huge datasets, some\nincluding sensitive information. This poses a serious privacy concern because\nprivacy attacks such as Membership Inference Attacks (MIAs) may detect this\nsensitive information. While knowledge distillation compresses LLMs into\nefficient, smaller student models, its impact on privacy remains underexplored.\nIn this paper, we investigate how knowledge distillation affects model\nrobustness against MIA. We focus on two questions. First, how is private data\nprotected in teacher and student models? Second, how can we strengthen privacy\npreservation against MIAs in knowledge distillation? Through comprehensive\nexperiments, we show that while teacher and student models achieve similar\noverall MIA accuracy, teacher models better protect member data, the primary\ntarget of MIA, whereas student models better protect non-member data. To\naddress this vulnerability in student models, we propose 5 privacy-preserving\ndistillation methods and demonstrate that they successfully reduce student\nmodels' vulnerability to MIA, with ensembling further stabilizing the\nrobustness, offering a reliable approach for distilling more secure and\nefficient student models. Our implementation source code is available at\nhttps://github.com/richardcui18/MIA_in_KD."
    },
    {
        "date": "2025-05",
        "title": "Multilingual Collaborative Defense for Large Language Models",
        "author": "Hongliang Li, Jinan Xu, Gengping Cui, Changhao Guan, Fengran Mo, and Kaiyu Huang",
        "link": "http://arxiv.org/abs/2505.11835v1",
        "abstract": "The robustness and security of large language models (LLMs) has become a\nprominent research area. One notable vulnerability is the ability to bypass LLM\nsafeguards by translating harmful queries into rare or underrepresented\nlanguages, a simple yet effective method of \"jailbreaking\" these models.\nDespite the growing concern, there has been limited research addressing the\nsafeguarding of LLMs in multilingual scenarios, highlighting an urgent need to\nenhance multilingual safety. In this work, we investigate the correlation\nbetween various attack features across different languages and propose\nMultilingual Collaborative Defense (MCD), a novel learning method that\noptimizes a continuous, soft safety prompt automatically to facilitate\nmultilingual safeguarding of LLMs. The MCD approach offers three advantages:\nFirst, it effectively improves safeguarding performance across multiple\nlanguages. Second, MCD maintains strong generalization capabilities while\nminimizing false refusal rates. Third, MCD mitigates the language safety\nmisalignment caused by imbalances in LLM training corpora. To evaluate the\neffectiveness of MCD, we manually construct multilingual versions of commonly\nused jailbreak benchmarks, such as MaliciousInstruct and AdvBench, to assess\nvarious safeguarding methods. Additionally, we introduce these datasets in\nunderrepresented (zero-shot) languages to verify the language transferability\nof MCD. The results demonstrate that MCD outperforms existing approaches in\nsafeguarding against multilingual jailbreak attempts while also exhibiting\nstrong language transfer capabilities. Our code is available at\nhttps://github.com/HLiang-Lee/MCD."
    },
    {
        "date": "2025-05",
        "title": "Robust Cross-View Geo-Localization via Content-Viewpoint Disentanglement",
        "author": "Ke Li, Di Wang, Xiaowei Wang, Zhihong Wu, Yiming Zhang, Yifeng Wang, and Quan Wang",
        "link": "http://arxiv.org/abs/2505.11822v1",
        "abstract": "Cross-view geo-localization (CVGL) aims to match images of the same\ngeographic location captured from different perspectives, such as drones and\nsatellites. Despite recent advances, CVGL remains highly challenging due to\nsignificant appearance changes and spatial distortions caused by viewpoint\nvariations. Existing methods typically assume that cross-view images can be\ndirectly aligned within a shared feature space by maximizing feature similarity\nthrough contrastive learning. Nonetheless, this assumption overlooks the\ninherent conflicts induced by viewpoint discrepancies, resulting in extracted\nfeatures containing inconsistent information that hinders precise localization.\nIn this study, we take a manifold learning perspective and model the feature\nspace of cross-view images as a composite manifold jointly governed by content\nand viewpoint information. Building upon this insight, we propose\n$\\textbf{CVD}$, a new CVGL framework that explicitly disentangles\n$\\textit{content}$ and $\\textit{viewpoint}$ factors. To promote effective\ndisentanglement, we introduce two constraints: $\\textit{(i)}$ An intra-view\nindependence constraint, which encourages statistical independence between the\ntwo factors by minimizing their mutual information. $\\textit{(ii)}$ An\ninter-view reconstruction constraint that reconstructs each view by\ncross-combining $\\textit{content}$ and $\\textit{viewpoint}$ from paired images,\nensuring factor-specific semantics are preserved. As a plug-and-play module,\nCVD can be seamlessly integrated into existing geo-localization pipelines.\nExtensive experiments on four benchmarks, i.e., University-1652, SUES-200,\nCVUSA, and CVACT, demonstrate that CVD consistently improves both localization\naccuracy and generalization across multiple baselines."
    },
    {
        "date": "2025-05",
        "title": "UniMoCo: Unified Modality Completion for Robust Multi-Modal Embeddings",
        "author": "Jiajun Qin, Yuan Pu, Zhuolun He, Seunggeun Kim, David Z. Pan, and Bei Yu",
        "link": "http://arxiv.org/abs/2505.11815v1",
        "abstract": "Current research has explored vision-language models for multi-modal\nembedding tasks, such as information retrieval, visual grounding, and\nclassification. However, real-world scenarios often involve diverse modality\ncombinations between queries and targets, such as text and image to text, text\nand image to text and image, and text to text and image. These diverse\ncombinations pose significant challenges for existing models, as they struggle\nto align all modality combinations within a unified embedding space during\ntraining, which degrades performance at inference. To address this limitation,\nwe propose UniMoCo, a novel vision-language model architecture designed for\nmulti-modal embedding tasks. UniMoCo introduces a modality-completion module\nthat generates visual features from textual inputs, ensuring modality\ncompleteness for both queries and targets. Additionally, we develop a\nspecialized training strategy to align embeddings from both original and\nmodality-completed inputs, ensuring consistency within the embedding space.\nThis enables the model to robustly handle a wide range of modality combinations\nacross embedding tasks. Experiments show that UniMoCo outperforms previous\nmethods while demonstrating consistent robustness across diverse settings. More\nimportantly, we identify and quantify the inherent bias in conventional\napproaches caused by imbalance of modality combinations in training data, which\ncan be mitigated through our modality-completion paradigm. The code is\navailable at https://github.com/HobbitQia/UniMoCo."
    },
    {
        "date": "2025-05",
        "title": "Are vision language models robust to uncertain inputs?",
        "author": "Xi Wang, and Eric Nalisnick",
        "link": "http://arxiv.org/abs/2505.11804v1",
        "abstract": "Robustness against uncertain and ambiguous inputs is a critical challenge for\ndeep learning models. While recent advancements in large scale vision language\nmodels (VLMs, e.g. GPT4o) might suggest that increasing model and training\ndataset size would mitigate this issue, our empirical evaluation shows a more\ncomplicated picture. Testing models using two classic uncertainty\nquantification tasks, anomaly detection and classification under inherently\nambiguous conditions, we find that newer and larger VLMs indeed exhibit\nimproved robustness compared to earlier models, but still suffer from a\ntendency to strictly follow instructions, often causing them to hallucinate\nconfident responses even when faced with unclear or anomalous inputs.\nRemarkably, for natural images such as ImageNet, this limitation can be\novercome without pipeline modifications: simply prompting models to abstain\nfrom uncertain predictions enables significant reliability gains, achieving\nnear-perfect robustness in several settings. However, for domain-specific tasks\nsuch as galaxy morphology classification, a lack of specialized knowledge\nprevents reliable uncertainty estimation. Finally, we propose a novel mechanism\nbased on caption diversity to reveal a model's internal uncertainty, enabling\npractitioners to predict when models will successfully abstain without relying\non labeled data."
    },
    {
        "date": "2025-05",
        "title": "CL-CaGAN: Capsule differential adversarial continuous learning for cross-domain hyperspectral anomaly detection",
        "author": "Jianing Wang, Siying Guo, Zheng Hua, Runhu Huang, Jinyu Hu, and Maoguo Gong",
        "link": "http://arxiv.org/abs/2505.11793v1",
        "abstract": "Anomaly detection (AD) has attracted remarkable attention in hyperspectral\nimage (HSI) processing fields, and most existing deep learning (DL)-based\nalgorithms indicate dramatic potential for detecting anomaly samples through\nspecific training process under current scenario. However, the limited prior\ninformation and the catastrophic forgetting problem indicate crucial challenges\nfor existing DL structure in open scenarios cross-domain detection. In order to\nimprove the detection performance, a novel continual learning-based capsule\ndifferential generative adversarial network (CL-CaGAN) is proposed to elevate\nthe cross-scenario learning performance for facilitating the real application\nof DL-based structure in hyperspectral AD (HAD) task. First, a modified capsule\nstructure with adversarial learning network is constructed to estimate the\nbackground distribution for surmounting the deficiency of prior information. To\nmitigate the catastrophic forgetting phenomenon, clustering-based sample replay\nstrategy and a designed extra self-distillation regularization are integrated\nfor merging the history and future knowledge in continual AD task, while the\ndiscriminative learning ability from previous detection scenario to current\nscenario is retained by the elaborately designed structure with continual\nlearning (CL) strategy. In addition, the differentiable enhancement is enforced\nto augment the generation performance of the training data. This further\nstabilizes the training process with better convergence and efficiently\nconsolidates the reconstruction ability of background samples. To verify the\neffectiveness of our proposed CL-CaGAN, we conduct experiments on several real\nHSIs, and the results indicate that the proposed CL-CaGAN demonstrates higher\ndetection performance and continuous learning capacity for mitigating the\ncatastrophic forgetting under cross-domain scenarios."
    },
    {
        "date": "2025-05",
        "title": "EnvInjection: Environmental Prompt Injection Attack to Multi-modal Web Agents",
        "author": "Xilong Wang, John Bloch, Zedian Shao, Yuepeng Hu, Shuyan Zhou, and Neil Zhenqiang Gong",
        "link": "http://arxiv.org/abs/2505.11717v1",
        "abstract": "Multi-modal large language model (MLLM)-based web agents interact with\nwebpage environments by generating actions based on screenshots of the\nwebpages. Environmental prompt injection attacks manipulate the environment to\ninduce the web agent to perform a specific, attacker-chosen action--referred to\nas the target action. However, existing attacks suffer from limited\neffectiveness or stealthiness, or are impractical in real-world settings. In\nthis work, we propose EnvInjection, a new attack that addresses these\nlimitations. Our attack adds a perturbation to the raw pixel values of the\nrendered webpage, which can be implemented by modifying the webpage's source\ncode. After these perturbed pixels are mapped into a screenshot, the\nperturbation induces the web agent to perform the target action. We formulate\nthe task of finding the perturbation as an optimization problem. A key\nchallenge in solving this problem is that the mapping between raw pixel values\nand screenshot is non-differentiable, making it difficult to backpropagate\ngradients to the perturbation. To overcome this, we train a neural network to\napproximate the mapping and apply projected gradient descent to solve the\nreformulated optimization problem. Extensive evaluation on multiple webpage\ndatasets shows that EnvInjection is highly effective and significantly\noutperforms existing baselines."
    },
    {
        "date": "2025-05",
        "title": "Co-Evolutionary Defence of Active Directory Attack Graphs via GNN-Approximated Dynamic Programming",
        "author": "Diksha Goel, Hussain Ahmad, Kristen Moore, and Mingyu Guo",
        "link": "http://arxiv.org/abs/2505.11710v1",
        "abstract": "Modern enterprise networks increasingly rely on Active Directory (AD) for\nidentity and access management. However, this centralization exposes a single\npoint of failure, allowing adversaries to compromise high-value assets.\nExisting AD defense approaches often assume static attacker behavior, but\nreal-world adversaries adapt dynamically, rendering such methods brittle. To\naddress this, we model attacker-defender interactions in AD as a Stackelberg\ngame between an adaptive attacker and a proactive defender. We propose a\nco-evolutionary defense framework that combines Graph Neural Network\nApproximated Dynamic Programming (GNNDP) to model attacker strategies, with\nEvolutionary Diversity Optimization (EDO) to generate resilient blocking\nstrategies. To ensure scalability, we introduce a Fixed-Parameter Tractable\n(FPT) graph reduction method that reduces complexity while preserving strategic\nstructure. Our framework jointly refines attacker and defender policies to\nimprove generalization and prevent premature convergence. Experiments on\nsynthetic AD graphs show near-optimal results (within 0.1 percent of optimality\non r500) and improved performance on larger graphs (r1000 and r2000),\ndemonstrating the framework's scalability and effectiveness."
    },
    {
        "date": "2025-05",
        "title": "Joint Graph Estimation and Signal Restoration for Robust Federated Learning",
        "author": "Tsutahiro Fukuhara, Junya Hara, Hiroshi Higashi, and Yuichi Tanaka",
        "link": "http://arxiv.org/abs/2505.11648v1",
        "abstract": "We propose a robust aggregation method for model parameters in federated\nlearning (FL) under noisy communications. FL is a distributed machine learning\nparadigm in which a central server aggregates local model parameters from\nmultiple clients. These parameters are often noisy and/or have missing values\nduring data collection, training, and communication between the clients and\nserver. This may cause a considerable drop in model accuracy. To address this\nissue, we learn a graph that represents pairwise relationships between model\nparameters of the clients during aggregation. We realize it with a joint\nproblem of graph learning and signal (i.e., model parameters) restoration. The\nproblem is formulated as a difference-of-convex (DC) optimization, which is\nefficiently solved via a proximal DC algorithm. Experimental results on MNIST\nand CIFAR-10 datasets show that the proposed method outperforms existing\napproaches by up to $2$--$5\\%$ in classification accuracy under biased data\ndistributions and noisy conditions."
    },
    {
        "date": "2025-05",
        "title": "PeerGuard: Defending Multi-Agent Systems Against Backdoor Attacks Through Mutual Reasoning",
        "author": "Falong Fan, and Xi Li",
        "link": "http://arxiv.org/abs/2505.11642v1",
        "abstract": "Multi-agent systems leverage advanced AI models as autonomous agents that\ninteract, cooperate, or compete to complete complex tasks across applications\nsuch as robotics and traffic management. Despite their growing importance,\nsafety in multi-agent systems remains largely underexplored, with most research\nfocusing on single AI models rather than interacting agents. This work\ninvestigates backdoor vulnerabilities in multi-agent systems and proposes a\ndefense mechanism based on agent interactions. By leveraging reasoning\nabilities, each agent evaluates responses from others to detect illogical\nreasoning processes, which indicate poisoned agents. Experiments on LLM-based\nmulti-agent systems, including ChatGPT series and Llama 3, demonstrate the\neffectiveness of the proposed method, achieving high accuracy in identifying\npoisoned agents while minimizing false positives on clean agents. We believe\nthis work provides insights into multi-agent system safety and contributes to\nthe development of robust, trustworthy AI interactions."
    },
    {
        "date": "2025-05",
        "title": "Adaptive Robust Optimization with Data-Driven Uncertainty for Enhancing Distribution System Resilience",
        "author": "Shuyi Chen, Shixiang Zhu, and Ramteen Sioshansi",
        "link": "http://arxiv.org/abs/2505.11627v1",
        "abstract": "Extreme weather events are placing growing strain on electric power systems,\nexposing the limitations of purely reactive responses and prompting the need\nfor proactive resilience planning. However, existing approaches often rely on\nsimplified uncertainty models and decouple proactive and reactive decisions,\noverlooking their critical interdependence. This paper proposes a novel\ntri-level optimization framework that integrates proactive infrastructure\ninvestment, adversarial modeling of spatio-temporal disruptions, and adaptive\nreactive response. We construct high-probability, distribution-free uncertainty\nsets using conformal prediction to capture complex and data-scarce outage\npatterns. To solve the resulting nested decision problem, we derive a bi-level\nreformulation via strong duality and develop a scalable Benders decomposition\nalgorithm. Experiments on both real and synthetic data demonstrate that our\napproach consistently outperforms conventional robust and two-stage methods,\nachieving lower worst-case losses and more efficient resource allocation,\nespecially under tight operational constraints and large-scale uncertainty."
    },
    {
        "date": "2025-05",
        "title": "The Ripple Effect: On Unforeseen Complications of Backdoor Attacks",
        "author": "Rui Zhang, Yun Shen, Hongwei Li, Wenbo Jiang, Hanxiao Chen, Yuan Zhang, Guowen Xu, and Yang Zhang",
        "link": "http://arxiv.org/abs/2505.11586v1",
        "abstract": "Recent research highlights concerns about the trustworthiness of third-party\nPre-Trained Language Models (PTLMs) due to potential backdoor attacks. These\nbackdoored PTLMs, however, are effective only for specific pre-defined\ndownstream tasks. In reality, these PTLMs can be adapted to many other\nunrelated downstream tasks. Such adaptation may lead to unforeseen consequences\nin downstream model outputs, consequently raising user suspicion and\ncompromising attack stealthiness. We refer to this phenomenon as backdoor\ncomplications. In this paper, we undertake the first comprehensive\nquantification of backdoor complications. Through extensive experiments using 4\nprominent PTLMs and 16 text classification benchmark datasets, we demonstrate\nthe widespread presence of backdoor complications in downstream models\nfine-tuned from backdoored PTLMs. The output distribution of triggered samples\nsignificantly deviates from that of clean samples. Consequently, we propose a\nbackdoor complication reduction method leveraging multi-task learning to\nmitigate complications without prior knowledge of downstream tasks. The\nexperimental results demonstrate that our proposed method can effectively\nreduce complications while maintaining the efficacy and consistency of backdoor\nattacks. Our code is available at\nhttps://github.com/zhangrui4041/Backdoor_Complications."
    },
    {
        "date": "2025-05",
        "title": "ProxyPrompt: Securing System Prompts against Prompt Extraction Attacks",
        "author": "Zhixiong Zhuang, Maria-Irina Nicolae, Hui-Po Wang, and Mario Fritz",
        "link": "http://arxiv.org/abs/2505.11459v1",
        "abstract": "The integration of large language models (LLMs) into a wide range of\napplications has highlighted the critical role of well-crafted system prompts,\nwhich require extensive testing and domain expertise. These prompts enhance\ntask performance but may also encode sensitive information and filtering\ncriteria, posing security risks if exposed. Recent research shows that system\nprompts are vulnerable to extraction attacks, while existing defenses are\neither easily bypassed or require constant updates to address new threats. In\nthis work, we introduce ProxyPrompt, a novel defense mechanism that prevents\nprompt leakage by replacing the original prompt with a proxy. This proxy\nmaintains the original task's utility while obfuscating the extracted prompt,\nensuring attackers cannot reproduce the task or access sensitive information.\nComprehensive evaluations on 264 LLM and system prompt pairs show that\nProxyPrompt protects 94.70% of prompts from extraction attacks, outperforming\nthe next-best defense, which only achieves 42.80%."
    },
    {
        "date": "2025-05",
        "title": "CROC: Evaluating and Training T2I Metrics with Pseudo- and Human-Labeled Contrastive Robustness Checks",
        "author": "Christoph Leiter, Yuki M. Asano, Margret Keuper, and Steffen Eger",
        "link": "http://arxiv.org/abs/2505.11314v1",
        "abstract": "The assessment of evaluation metrics (meta-evaluation) is crucial for\ndetermining the suitability of existing metrics in text-to-image (T2I)\ngeneration tasks. Human-based meta-evaluation is costly and time-intensive, and\nautomated alternatives are scarce. We address this gap and propose CROC: a\nscalable framework for automated Contrastive Robustness Checks that\nsystematically probes and quantifies metric robustness by synthesizing\ncontrastive test cases across a comprehensive taxonomy of image properties.\nWith CROC, we generate a pseudo-labeled dataset (CROC$^{syn}$) of over one\nmillion contrastive prompt-image pairs to enable a fine-grained comparison of\nevaluation metrics. We also use the dataset to train CROCScore, a new metric\nthat achieves state-of-the-art performance among open-source methods,\ndemonstrating an additional key application of our framework. To complement\nthis dataset, we introduce a human-supervised benchmark (CROC$^{hum}$)\ntargeting especially challenging categories. Our results highlight robustness\nissues in existing metrics: for example, many fail on prompts involving\nnegation, and all tested open-source metrics fail on at least 25% of cases\ninvolving correct identification of body parts."
    },
    {
        "date": "2025-05",
        "title": "LD-Scene: LLM-Guided Diffusion for Controllable Generation of Adversarial Safety-Critical Driving Scenarios",
        "author": "Mingxing Peng, Yuting Xie, Xusen Guo, Ruoyu Yao, Hai Yang, and Jun Ma",
        "link": "http://arxiv.org/abs/2505.11247v1",
        "abstract": "Ensuring the safety and robustness of autonomous driving systems necessitates\na comprehensive evaluation in safety-critical scenarios. However, these\nsafety-critical scenarios are rare and difficult to collect from real-world\ndriving data, posing significant challenges to effectively assessing the\nperformance of autonomous vehicles. Typical existing methods often suffer from\nlimited controllability and lack user-friendliness, as extensive expert\nknowledge is essentially required. To address these challenges, we propose\nLD-Scene, a novel framework that integrates Large Language Models (LLMs) with\nLatent Diffusion Models (LDMs) for user-controllable adversarial scenario\ngeneration through natural language. Our approach comprises an LDM that\ncaptures realistic driving trajectory distributions and an LLM-based guidance\nmodule that translates user queries into adversarial loss functions,\nfacilitating the generation of scenarios aligned with user queries. The\nguidance module integrates an LLM-based Chain-of-Thought (CoT) code generator\nand an LLM-based code debugger, enhancing the controllability and robustness in\ngenerating guidance functions. Extensive experiments conducted on the nuScenes\ndataset demonstrate that LD-Scene achieves state-of-the-art performance in\ngenerating realistic, diverse, and effective adversarial scenarios.\nFurthermore, our framework provides fine-grained control over adversarial\nbehaviors, thereby facilitating more effective testing tailored to specific\ndriving scenarios."
    },
    {
        "date": "2025-05",
        "title": "MPMA: Preference Manipulation Attack Against Model Context Protocol",
        "author": "Zihan Wang, Hongwei Li, Rui Zhang, Yu Liu, Wenbo Jiang, Wenshu Fan, Qingchuan Zhao, and Guowen Xu",
        "link": "http://arxiv.org/abs/2505.11154v1",
        "abstract": "Model Context Protocol (MCP) standardizes interface mapping for large\nlanguage models (LLMs) to access external data and tools, which revolutionizes\nthe paradigm of tool selection and facilitates the rapid expansion of the LLM\nagent tool ecosystem. However, as the MCP is increasingly adopted, third-party\ncustomized versions of the MCP server expose potential security\nvulnerabilities. In this paper, we first introduce a novel security threat,\nwhich we term the MCP Preference Manipulation Attack (MPMA). An attacker\ndeploys a customized MCP server to manipulate LLMs, causing them to prioritize\nit over other competing MCP servers. This can result in economic benefits for\nattackers, such as revenue from paid MCP services or advertising income\ngenerated from free servers. To achieve MPMA, we first design a Direct\nPreference Manipulation Attack ($\\mathtt{DPMA}$) that achieves significant\neffectiveness by inserting the manipulative word and phrases into the tool name\nand description. However, such a direct modification is obvious to users and\nlacks stealthiness. To address these limitations, we further propose\nGenetic-based Advertising Preference Manipulation Attack ($\\mathtt{GAPMA}$).\n$\\mathtt{GAPMA}$ employs four commonly used strategies to initialize\ndescriptions and integrates a Genetic Algorithm (GA) to enhance stealthiness.\nThe experiment results demonstrate that $\\mathtt{GAPMA}$ balances high\neffectiveness and stealthiness. Our study reveals a critical vulnerability of\nthe MCP in open ecosystems, highlighting an urgent need for robust defense\nmechanisms to ensure the fairness of the MCP ecosystem."
    },
    {
        "date": "2025-05",
        "title": "Towards Robust Spiking Neural Networks:Mitigating Heterogeneous Training Vulnerability via Dominant Eigencomponent Projection",
        "author": "Desong Zhang, Jia Hu, and Geyong Min",
        "link": "http://arxiv.org/abs/2505.11134v1",
        "abstract": "Spiking Neural Networks (SNNs) process information via discrete spikes,\nenabling them to operate at remarkably low energy levels. However, our\nexperimental observations reveal a striking vulnerability when SNNs are trained\nusing the mainstream method--direct encoding combined with backpropagation\nthrough time (BPTT): even a single backward pass on data drawn from a slightly\ndifferent distribution can lead to catastrophic network collapse. Our\ntheoretical analysis attributes this vulnerability to the repeated inputs\ninherent in direct encoding and the gradient accumulation characteristic of\nBPTT, which together produce an exceptional large Hessian spectral radius. To\naddress this challenge, we develop a hyperparameter-free method called Dominant\nEigencomponent Projection (DEP). By orthogonally projecting gradients to\nprecisely remove their dominant components, DEP effectively reduces the Hessian\nspectral radius, thereby preventing SNNs from settling into sharp minima.\nExtensive experiments demonstrate that DEP not only mitigates the vulnerability\nof SNNs to heterogeneous data poisoning, but also significantly enhances\noverall robustness compared to key baselines, providing strong support for\nsafer and more reliable SNN deployment."
    },
    {
        "date": "2025-05",
        "title": "GoLeash: Mitigating Golang Software Supply Chain Attacks with Runtime Policy Enforcement",
        "author": "Carmine Cesarano, Martin Monperrus, and Roberto Natella",
        "link": "http://arxiv.org/abs/2505.11016v1",
        "abstract": "Modern software supply chain attacks consist of introducing new, malicious\ncapabilities into trusted third-party software components, in order to\npropagate to a victim through a package dependency chain. These attacks are\nespecially concerning for the Go language ecosystem, which is extensively used\nin critical cloud infrastructures. We present GoLeash, a novel system that\napplies the principle of least privilege at the package-level granularity, by\nenforcing distinct security policies for each package in the supply chain. This\nfiner granularity enables GoLeash to detect malicious packages more precisely\nthan traditional sandboxing that handles security policies at process- or\ncontainer-level. Moreover, GoLeash remains effective under obfuscation, can\novercome the limitations of static analysis, and incurs acceptable runtime\noverhead."
    },
    {
        "date": "2025-05",
        "title": "WildDoc: How Far Are We from Achieving Comprehensive and Robust Document Understanding in the Wild?",
        "author": "An-Lan Wang, Jingqun Tang, Liao Lei, Hao Feng, Qi Liu, Xiang Fei, Jinghui Lu, Han Wang, Weiwei Liu, Hao Liu, Yuliang Liu, Xiang Bai, and Can Huang",
        "link": "http://arxiv.org/abs/2505.11015v1",
        "abstract": "The rapid advancements in Multimodal Large Language Models (MLLMs) have\nsignificantly enhanced capabilities in Document Understanding. However,\nprevailing benchmarks like DocVQA and ChartQA predominantly comprise\n\\textit{scanned or digital} documents, inadequately reflecting the intricate\nchallenges posed by diverse real-world scenarios, such as variable illumination\nand physical distortions. This paper introduces WildDoc, the inaugural\nbenchmark designed specifically for assessing document understanding in natural\nenvironments. WildDoc incorporates a diverse set of manually captured document\nimages reflecting real-world conditions and leverages document sources from\nestablished benchmarks to facilitate comprehensive comparisons with digital or\nscanned documents. Further, to rigorously evaluate model robustness, each\ndocument is captured four times under different conditions. Evaluations of\nstate-of-the-art MLLMs on WildDoc expose substantial performance declines and\nunderscore the models' inadequate robustness compared to traditional\nbenchmarks, highlighting the unique challenges posed by real-world document\nunderstanding. Our project homepage is available at\nhttps://bytedance.github.io/WildDoc."
    },
    {
        "date": "2025-05",
        "title": "Towards Robust and Controllable Text-to-Motion via Masked Autoregressive Diffusion",
        "author": "Zongye Zhang, Bohan Kong, Qingjie Liu, and Yunhong Wang",
        "link": "http://arxiv.org/abs/2505.11013v1",
        "abstract": "Generating 3D human motion from text descriptions remains challenging due to\nthe diverse and complex nature of human motion. While existing methods excel\nwithin the training distribution, they often struggle with out-of-distribution\nmotions, limiting their applicability in real-world scenarios. Existing\nVQVAE-based methods often fail to represent novel motions faithfully using\ndiscrete tokens, which hampers their ability to generalize beyond seen data.\nMeanwhile, diffusion-based methods operating on continuous representations\noften lack fine-grained control over individual frames. To address these\nchallenges, we propose a robust motion generation framework MoMADiff, which\ncombines masked modeling with diffusion processes to generate motion using\nframe-level continuous representations. Our model supports flexible\nuser-provided keyframe specification, enabling precise control over both\nspatial and temporal aspects of motion synthesis. MoMADiff demonstrates strong\ngeneralization capability on novel text-to-motion datasets with sparse\nkeyframes as motion prompts. Extensive experiments on two held-out datasets and\ntwo standard benchmarks show that our method consistently outperforms\nstate-of-the-art models in motion quality, instruction fidelity, and keyframe\nadherence."
    },
    {
        "date": "2025-05",
        "title": "RAGSynth: Synthetic Data for Robust and Faithful RAG Component Optimization",
        "author": "Haiyang Shen, Hang Yan, Zhongshi Xing, Mugeng Liu, Yue Li, Zhiyang Chen, Yuxiang Wang, Jiuzheng Wang, and Yun Ma",
        "link": "http://arxiv.org/abs/2505.10989v1",
        "abstract": "RAG can enhance the performance of LLMs on knowledge-intensive tasks. Various\nRAG paradigms, including vanilla, planning-based, and iterative RAG, are built\nupon 2 cores: the retriever, which should robustly select relevant documents\nacross complex queries, and the generator, which should faithfully synthesize\nresponses. However, existing retrievers rely heavily on public knowledge and\nstruggle with queries of varying logical complexity and clue completeness,\nwhile generators frequently face fidelity problems. In this work, we introduce\nRAGSynth, a framework that includes a data construction modeling and a\ncorresponding synthetic data generation implementation, designed to optimize\nretriever robustness and generator fidelity. Additionally, we present\nSynthBench, a benchmark encompassing 8 domain-specific documents across 4\ndomains, featuring diverse query complexities, clue completeness, and\nfine-grained citation granularity. Leveraging RAGSynth, we generate a\nlarge-scale synthetic dataset, including single and multi-hop. Extensive\nexperiments demonstrate that the synthetic data significantly improves the\nrobustness of the retrievers and the fidelity of the generators. Additional\nevaluations confirm that RAGSynth can also generalize well across different\ndomains. By integrating the optimized retrievers into various RAG paradigms, we\nconsistently observe enhanced RAG system performance. We have open-sourced the\nimplementation on https://github.com/EachSheep/RAGSynth."
    },
    {
        "date": "2025-05",
        "title": "GenoArmory: A Unified Evaluation Framework for Adversarial Attacks on Genomic Foundation Models",
        "author": "Haozheng Luo, Chenghao Qiu, Yimin Wang, Shang Wu, Jiahao Yu, Han Liu, Binghui Wang, and Yan Chen",
        "link": "http://arxiv.org/abs/2505.10983v1",
        "abstract": "We propose the first unified adversarial attack benchmark for Genomic\nFoundation Models (GFMs), named GenoArmory. Unlike existing GFM benchmarks,\nGenoArmory offers the first comprehensive evaluation framework to\nsystematically assess the vulnerability of GFMs to adversarial attacks.\nMethodologically, we evaluate the adversarial robustness of five\nstate-of-the-art GFMs using four widely adopted attack algorithms and three\ndefense strategies. Importantly, our benchmark provides an accessible and\ncomprehensive framework to analyze GFM vulnerabilities with respect to model\narchitecture, quantization schemes, and training datasets. Additionally, we\nintroduce GenoAdv, a new adversarial sample dataset designed to improve GFM\nsafety. Empirically, classification models exhibit greater robustness to\nadversarial perturbations compared to generative models, highlighting the\nimpact of task type on model vulnerability. Moreover, adversarial attacks\nfrequently target biologically significant genomic regions, suggesting that\nthese models effectively capture meaningful sequence features."
    },
    {
        "date": "2025-05",
        "title": "A Survey on the Safety and Security Threats of Computer-Using Agents: JARVIS or Ultron?",
        "author": "Ada Chen, Yongjiang Wu, Junyuan Zhang, Shu Yang, Jen-tse Huang, Kun Wang, Wenxuan Wang, and Shuai Wang",
        "link": "http://arxiv.org/abs/2505.10924v1",
        "abstract": "Recently, AI-driven interactions with computing devices have advanced from\nbasic prototype tools to sophisticated, LLM-based systems that emulate\nhuman-like operations in graphical user interfaces. We are now witnessing the\nemergence of \\emph{Computer-Using Agents} (CUAs), capable of autonomously\nperforming tasks such as navigating desktop applications, web pages, and mobile\napps. However, as these agents grow in capability, they also introduce novel\nsafety and security risks. Vulnerabilities in LLM-driven reasoning, with the\nadded complexity of integrating multiple software components and multimodal\ninputs, further complicate the security landscape. In this paper, we present a\nsystematization of knowledge on the safety and security threats of CUAs. We\nconduct a comprehensive literature review and distill our findings along four\nresearch objectives: \\textit{\\textbf{(i)}} define the CUA that suits safety\nanalysis; \\textit{\\textbf{(ii)} } categorize current safety threats among CUAs;\n\\textit{\\textbf{(iii)}} propose a comprehensive taxonomy of existing defensive\nstrategies; \\textit{\\textbf{(iv)}} summarize prevailing benchmarks, datasets,\nand evaluation metrics used to assess the safety and performance of CUAs.\nBuilding on these insights, our work provides future researchers with a\nstructured foundation for exploring unexplored vulnerabilities and offers\npractitioners actionable guidance in designing and deploying secure\nComputer-Using Agents."
    },
    {
        "date": "2025-05",
        "title": "On the Security Risks of ML-based Malware Detection Systems: A Survey",
        "author": "Ping He, Yuhao Mao, Changjiang Li, Lorenzo Cavallaro, Ting Wang, and Shouling Ji",
        "link": "http://arxiv.org/abs/2505.10903v1",
        "abstract": "Malware presents a persistent threat to user privacy and data integrity. To\ncombat this, machine learning-based (ML-based) malware detection (MD) systems\nhave been developed. However, these systems have increasingly been attacked in\nrecent years, undermining their effectiveness in practice. While the security\nrisks associated with ML-based MD systems have garnered considerable attention,\nthe majority of prior works is limited to adversarial malware examples, lacking\na comprehensive analysis of practical security risks. This paper addresses this\ngap by utilizing the CIA principles to define the scope of security risks. We\nthen deconstruct ML-based MD systems into distinct operational stages, thus\ndeveloping a stage-based taxonomy. Utilizing this taxonomy, we summarize the\ntechnical progress and discuss the gaps in the attack and defense proposals\nrelated to the ML-based MD systems within each stage. Subsequently, we conduct\ntwo case studies, using both inter-stage and intra-stage analyses according to\nthe stage-based taxonomy to provide new empirical insights. Based on these\nanalyses and insights, we suggest potential future directions from both\ninter-stage and intra-stage perspectives."
    },
    {
        "date": "2025-05",
        "title": "Anti-Sensing: Defense against Unauthorized Radar-based Human Vital Sign Sensing with Physically Realizable Wearable Oscillators",
        "author": "Md Farhan Tasnim Oshim, Nigel Doering, Bashima Islam, Tsui-Wei Weng, and Tauhidur Rahman",
        "link": "http://arxiv.org/abs/2505.10864v1",
        "abstract": "Recent advancements in Ultra-Wideband (UWB) radar technology have enabled\ncontactless, non-line-of-sight vital sign monitoring, making it a valuable tool\nfor healthcare. However, UWB radar's ability to capture sensitive physiological\ndata, even through walls, raises significant privacy concerns, particularly in\nhuman-robot interactions and autonomous systems that rely on radar for sensing\nhuman presence and physiological functions. In this paper, we present\nAnti-Sensing, a novel defense mechanism designed to prevent unauthorized\nradar-based sensing. Our approach introduces physically realizable\nperturbations, such as oscillatory motion from wearable devices, to disrupt\nradar sensing by mimicking natural cardiac motion, thereby misleading heart\nrate (HR) estimations. We develop a gradient-based algorithm to optimize the\nfrequency and spatial amplitude of these oscillations for maximal disruption\nwhile ensuring physiological plausibility. Through both simulations and\nreal-world experiments with radar data and neural network-based HR sensing\nmodels, we demonstrate the effectiveness of Anti-Sensing in significantly\ndegrading model accuracy, offering a practical solution for privacy\npreservation."
    },
    {
        "date": "2025-05",
        "title": "LARGO: Latent Adversarial Reflection through Gradient Optimization for Jailbreaking LLMs",
        "author": "Ran Li, Hao Wang, and Chengzhi Mao",
        "link": "http://arxiv.org/abs/2505.10838v1",
        "abstract": "Efficient red-teaming method to uncover vulnerabilities in Large Language\nModels (LLMs) is crucial. While recent attacks often use LLMs as optimizers,\nthe discrete language space make gradient-based methods struggle. We introduce\nLARGO (Latent Adversarial Reflection through Gradient Optimization), a novel\nlatent self-reflection attack that reasserts the power of gradient-based\noptimization for generating fluent jailbreaking prompts. By operating within\nthe LLM's continuous latent space, LARGO first optimizes an adversarial latent\nvector and then recursively call the same LLM to decode the latent into natural\nlanguage. This methodology yields a fast, effective, and transferable attack\nthat produces fluent and stealthy prompts. On standard benchmarks like AdvBench\nand JailbreakBench, LARGO surpasses leading jailbreaking techniques, including\nAutoDAN, by 44 points in attack success rate. Our findings demonstrate a potent\nalternative to agentic LLM prompting, highlighting the efficacy of interpreting\nand attacking LLM internals through gradient optimization."
    },
    {
        "date": "2025-05",
        "title": "RAN Tester UE: An Automated Declarative UE Centric Security Testing Platform",
        "author": "Charles Marion Ueltschey, Joshua Moore, Aly Sabri Abdalla, and Vuk Marojevic",
        "link": "http://arxiv.org/abs/2505.10812v1",
        "abstract": "Cellular networks require strict security procedures and measures across\nvarious network components, from core to radio access network (RAN) and\nend-user devices. As networks become increasingly complex and interconnected,\nas in O-RAN deployments, they are exposed to a numerous security threats.\nTherefore, ensuring robust security is critical for O-RAN to protect network\nintegrity and safeguard user data. This requires rigorous testing methodologies\nto mitigate threats. This paper introduces an automated, adaptive, and scalable\nuser equipment (UE) based RAN security testing framework designed to address\nthe shortcomings of existing RAN testing solutions. Experimental results on a\n5G software radio testbed built with commercial off-the-shelf hardware and open\nsource software validate the efficiency and reproducibility of sample security\ntest procedures developed on the RAN Tester UE framework."
    },
    {
        "date": "2025-05",
        "title": "Automating Security Audit Using Large Language Model based Agent: An Exploration Experiment",
        "author": "Jia Hui Chin, Pu Zhang, Yu Xin Cheong, and Jonathan Pan",
        "link": "http://arxiv.org/abs/2505.10732v1",
        "abstract": "In the current rapidly changing digital environment, businesses are under\nconstant stress to ensure that their systems are secured. Security audits help\nto maintain a strong security posture by ensuring that policies are in place,\ncontrols are implemented, gaps are identified for cybersecurity risks\nmitigation. However, audits are usually manual, requiring much time and costs.\nThis paper looks at the possibility of developing a framework to leverage Large\nLanguage Models (LLMs) as an autonomous agent to execute part of the security\naudit, namely with the field audit. password policy compliance for Windows\noperating system. Through the conduct of an exploration experiment of using\nGPT-4 with Langchain, the agent executed the audit tasks by accurately flagging\npassword policy violations and appeared to be more efficient than traditional\nmanual audits. Despite its potential limitations in operational consistency in\ncomplex and dynamic environment, the framework suggests possibilities to extend\nfurther to real-time threat monitoring and compliance checks."
    },
    {
        "date": "2025-05",
        "title": "ROIsGAN: A Region Guided Generative Adversarial Framework for Murine Hippocampal Subregion Segmentation",
        "author": "Sayed Mehedi Azim, Brian Corbett, and Iman Dehzangi",
        "link": "http://arxiv.org/abs/2505.10687v1",
        "abstract": "The hippocampus, a critical brain structure involved in memory processing and\nvarious neurodegenerative and psychiatric disorders, comprises three key\nsubregions: the dentate gyrus (DG), Cornu Ammonis 1 (CA1), and Cornu Ammonis 3\n(CA3). Accurate segmentation of these subregions from histological tissue\nimages is essential for advancing our understanding of disease mechanisms,\ndevelopmental dynamics, and therapeutic interventions. However, no existing\nmethods address the automated segmentation of hippocampal subregions from\ntissue images, particularly from immunohistochemistry (IHC) images. To bridge\nthis gap, we introduce a novel set of four comprehensive murine hippocampal IHC\ndatasets featuring distinct staining modalities: cFos, NeuN, and multiplexed\nstains combining cFos, NeuN, and either {\\Delta}FosB or GAD67, capturing\nstructural, neuronal activity, and plasticity associated information.\nAdditionally, we propose ROIsGAN, a region-guided U-Net-based generative\nadversarial network tailored for hippocampal subregion segmentation. By\nleveraging adversarial learning, ROIsGAN enhances boundary delineation and\nstructural detail refinement through a novel region-guided discriminator loss\ncombining Dice and binary cross-entropy loss. Evaluated across DG, CA1, and CA3\nsubregions, ROIsGAN consistently outperforms conventional segmentation models,\nachieving performance gains ranging from 1-10% in Dice score and up to 11% in\nIntersection over Union (IoU), particularly under challenging staining\nconditions. Our work establishes foundational datasets and methods for\nautomated hippocampal segmentation, enabling scalable, high-precision analysis\nof tissue images in neuroscience research. Our generated datasets, proposed\nmodel as a standalone tool, and its corresponding source code are publicly\navailable at: https://github.com/MehediAzim/ROIsGAN"
    },
    {
        "date": "2025-05",
        "title": "Agent Name Service (ANS): A Universal Directory for Secure AI Agent Discovery and Interoperability",
        "author": "Ken Huang, Vineeth Sai Narajala, Idan Habler, and Akram Sheriff",
        "link": "http://arxiv.org/abs/2505.10609v1",
        "abstract": "The proliferation of AI agents requires robust mechanisms for secure\ndiscovery. This paper introduces the Agent Name Service (ANS), a novel\narchitecture based on DNS addressing the lack of a public agent discovery\nframework. ANS provides a protocol-agnostic registry infrastructure that\nleverages Public Key Infrastructure (PKI) certificates for verifiable agent\nidentity and trust. The architecture features several key innovations: a\nformalized agent registration and renewal mechanism for lifecycle management;\nDNS-inspired naming conventions with capability-aware resolution; a modular\nProtocol Adapter Layer supporting diverse communication standards (A2A, MCP,\nACP etc.); and precisely defined algorithms for secure resolution. We implement\nstructured communication using JSON Schema and conduct a comprehensive threat\nanalysis of our proposal. The result is a foundational directory service\naddressing the core challenges of secured discovery and interaction in\nmulti-agent systems, paving the way for future interoperable, trustworthy, and\nscalable agent ecosystems."
    },
    {
        "date": "2025-05",
        "title": "S3C2 Summit 2024-09: Industry Secure Software Supply Chain Summit",
        "author": "Imranur Rahman, Yasemin Acar, Michel Cukier, William Enck, Christian Kastner, Alexandros Kapravelos, Dominik Wermke, and Laurie Williams",
        "link": "http://arxiv.org/abs/2505.10538v1",
        "abstract": "While providing economic and software development value, software supply\nchains are only as strong as their weakest link. Over the past several years,\nthere has been an exponential increase in cyberattacks, specifically targeting\nvulnerable links in critical software supply chains. These attacks disrupt the\nday-to-day functioning and threaten the security of nearly everyone on the\ninternet, from billion-dollar companies and government agencies to hobbyist\nopen-source developers. The ever-evolving threat of software supply chain\nattacks has garnered interest from the software industry and the US government\nin improving software supply chain security.\n  On September 20, 2024, three researchers from the NSF-backed Secure Software\nSupply Chain Center (S3C2) conducted a Secure Software Supply Chain Summit with\na diverse set of 12 practitioners from 9 companies. The goals of the Summit\nwere to: (1) to enable sharing between individuals from different companies\nregarding practical experiences and challenges with software supply chain\nsecurity, (2) to help form new collaborations, (3) to share our observations\nfrom our previous summits with industry, and (4) to learn about practitioners'\nchallenges to inform our future research direction. The summit consisted of\ndiscussions of six topics relevant to the companies represented, including\nupdating vulnerable dependencies, component and container choice, malicious\ncommits, building infrastructure, large language models, and reducing entire\nclasses of vulnerabilities."
    },
    {
        "date": "2025-05",
        "title": "MorphGuard: Morph Specific Margin Loss for Enhancing Robustness to Face Morphing Attacks",
        "author": "Iurii Medvedev, and Nuno Goncalves",
        "link": "http://arxiv.org/abs/2505.10497v1",
        "abstract": "Face recognition has evolved significantly with the advancement of deep\nlearning techniques, enabling its widespread adoption in various applications\nrequiring secure authentication. However, this progress has also increased its\nexposure to presentation attacks, including face morphing, which poses a\nserious security threat by allowing one identity to impersonate another.\nTherefore, modern face recognition systems must be robust against such attacks.\n  In this work, we propose a novel approach for training deep networks for face\nrecognition with enhanced robustness to face morphing attacks. Our method\nmodifies the classification task by introducing a dual-branch classification\nstrategy that effectively handles the ambiguity in the labeling of face morphs.\nThis adaptation allows the model to incorporate morph images into the training\nprocess, improving its ability to distinguish them from bona fide samples.\n  Our strategy has been validated on public benchmarks, demonstrating its\neffectiveness in enhancing robustness against face morphing attacks.\nFurthermore, our approach is universally applicable and can be integrated into\nexisting face recognition training pipelines to improve classification-based\nrecognition methods."
    },
    {
        "date": "2025-05",
        "title": "Superposition Yields Robust Neural Scaling",
        "author": "Yizhou Liu, Ziming Liu, and Jeff Gore",
        "link": "http://arxiv.org/abs/2505.10465v2",
        "abstract": "The success of today's large language models (LLMs) depends on the\nobservation that larger models perform better. However, the origin of this\nneural scaling law -- the finding that loss decreases as a power law with model\nsize -- remains unclear. Starting from two empirical principles -- that LLMs\nrepresent more things than the model dimensions (widths) they have (i.e.,\nrepresentations are superposed), and that words or concepts in language occur\nwith varying frequencies -- we constructed a toy model to study the loss\nscaling with model size. We found that when superposition is weak, meaning only\nthe most frequent features are represented without interference, the scaling of\nloss with model size depends on the underlying feature frequency; if feature\nfrequencies follow a power law, so does the loss. In contrast, under strong\nsuperposition, where all features are represented but overlap with each other,\nthe loss becomes inversely proportional to the model dimension across a wide\nrange of feature frequency distributions. This robust scaling behavior is\nexplained geometrically: when many more vectors are packed into a lower\ndimensional space, the interference (squared overlaps) between vectors scales\ninversely with that dimension. We then analyzed four families of open-sourced\nLLMs and found that they exhibit strong superposition and quantitatively match\nthe predictions of our toy model. The Chinchilla scaling law turned out to also\nagree with our results. We conclude that representation superposition is an\nimportant mechanism underlying the observed neural scaling laws. We anticipate\nthat these insights will inspire new training strategies and model\narchitectures to achieve better performance with less computation and fewer\nparameters."
    },
    {
        "date": "2025-05",
        "title": "Are Large Language Models Robust in Understanding Code Against Semantics-Preserving Mutations?",
        "author": "Pedro Orvalho, and Marta Kwiatkowska",
        "link": "http://arxiv.org/abs/2505.10443v1",
        "abstract": "Understanding the reasoning and robustness of Large Language Models (LLMs) is\ncritical for their reliable use in programming tasks. While recent studies have\nassessed LLMs' ability to predict program outputs, most focus solely on the\naccuracy of those predictions, without evaluating the reasoning behind them.\nMoreover, it has been observed on mathematical reasoning tasks that LLMs can\narrive at correct answers through flawed logic, raising concerns about similar\nissues in code understanding.\n  In this work, we evaluate whether state-of-the-art LLMs with up to 8B\nparameters can reason about Python programs or are simply guessing. We apply\nfive semantics-preserving code mutations: renaming variables, mirroring\ncomparison expressions, swapping if-else branches, converting for loops to\nwhile, and loop unrolling. These mutations maintain program semantics while\naltering its syntax. We evaluated six LLMs and performed a human expert\nanalysis using LiveCodeBench to assess whether the correct predictions are\nbased on sound reasoning. We also evaluated prediction stability across\ndifferent code mutations on LiveCodeBench and CruxEval. Our findings show that\nsome LLMs, such as Llama3.2, produce correct predictions based on flawed\nreasoning in up to 61% of cases. Furthermore, LLMs often change predictions in\nresponse to our code mutations, indicating limited robustness in their semantic\nunderstanding."
    },
    {
        "date": "2025-05",
        "title": "The Ephemeral Threat: Assessing the Security of Algorithmic Trading Systems powered by Deep Learning",
        "author": "Advije Rizvani, Giovanni Apruzzese, and Pavel Laskov",
        "link": "http://arxiv.org/abs/2505.10430v1",
        "abstract": "We study the security of stock price forecasting using Deep Learning (DL) in\ncomputational finance. Despite abundant prior research on the vulnerability of\nDL to adversarial perturbations, such work has hitherto hardly addressed\npractical adversarial threat models in the context of DL-powered algorithmic\ntrading systems (ATS). Specifically, we investigate the vulnerability of ATS to\nadversarial perturbations launched by a realistically constrained attacker. We\nfirst show that existing literature has paid limited attention to DL security\nin the financial domain, which is naturally attractive for adversaries. Then,\nwe formalize the concept of ephemeral perturbations (EP), which can be used to\nstage a novel type of attack tailored for DL-based ATS. Finally, we carry out\nan end-to-end evaluation of our EP against a profitable ATS. Our results reveal\nthat the introduction of small changes to the input stock prices not only (i)\ninduces the DL model to behave incorrectly but also (ii) leads the whole ATS to\nmake suboptimal buy/sell decisions, resulting in a worse financial performance\nof the targeted ATS."
    },
    {
        "date": "2025-05",
        "title": "Toward a Public and Secure Generative AI: A Comparative Analysis of Open and Closed LLMs",
        "author": "Jorge Machado",
        "link": "http://arxiv.org/abs/2505.10603v1",
        "abstract": "Generative artificial intelligence (Gen AI) systems represent a critical\ntechnology with far-reaching implications across multiple domains of society.\nHowever, their deployment entails a range of risks and challenges that require\ncareful evaluation. To date, there has been a lack of comprehensive,\ninterdisciplinary studies offering a systematic comparison between open-source\nand proprietary (closed) generative AI systems, particularly regarding their\nrespective advantages and drawbacks. This study aims to: i) critically evaluate\nand compare the characteristics, opportunities, and challenges of open and\nclosed generative AI models; and ii) propose foundational elements for the\ndevelopment of an Open, Public, and Safe Gen AI framework. As a methodology, we\nadopted a combined approach that integrates three methods: literature review,\ncritical analysis, and comparative analysis. The proposed framework outlines\nkey dimensions, openness, public governance, and security, as essential pillars\nfor shaping the future of trustworthy and inclusive Gen AI. Our findings reveal\nthat open models offer greater transparency, auditability, and flexibility,\nenabling independent scrutiny and bias mitigation. In contrast, closed systems\noften provide better technical support and ease of implementation, but at the\ncost of unequal access, accountability, and ethical oversight. The research\nalso highlights the importance of multi-stakeholder governance, environmental\nsustainability, and regulatory frameworks in ensuring responsible development."
    },
    {
        "date": "2025-05",
        "title": "Enhancing IoT Cyber Attack Detection in the Presence of Highly Imbalanced Data",
        "author": "Md. Ehsanul Haque, Md. Saymon Hosen Polash, Md Al-Imran Sanjida Simla, Md Alomgir Hossain, and Sarwar Jahan",
        "link": "http://arxiv.org/abs/2505.10600v1",
        "abstract": "Due to the rapid growth in the number of Internet of Things (IoT) networks,\nthe cyber risk has increased exponentially, and therefore, we have to develop\neffective IDS that can work well with highly imbalanced datasets. A high rate\nof missed threats can be the result, as traditional machine learning models\ntend to struggle in identifying attacks when normal data volume is much higher\nthan the volume of attacks. For example, the dataset used in this study reveals\na strong class imbalance with 94,659 instances of the majority class and only\n28 instances of the minority class, making it quite challenging to determine\nrare attacks accurately. The challenges presented in this research are\naddressed by hybrid sampling techniques designed to improve data imbalance\ndetection accuracy in IoT domains. After applying these techniques, we evaluate\nthe performance of several machine learning models such as Random Forest, Soft\nVoting, Support Vector Classifier (SVC), K-Nearest Neighbors (KNN), Multi-Layer\nPerceptron (MLP), and Logistic Regression with respect to the classification of\ncyber-attacks. The obtained results indicate that the Random Forest model\nachieved the best performance with a Kappa score of 0.9903, test accuracy of\n0.9961, and AUC of 0.9994. Strong performance is also shown by the Soft Voting\nmodel, with an accuracy of 0.9952 and AUC of 0.9997, indicating the benefits of\ncombining model predictions. Overall, this work demonstrates the value of\nhybrid sampling combined with robust model and feature selection for\nsignificantly improving IoT security against cyber-attacks, especially in\nhighly imbalanced data environments."
    },
    {
        "date": "2025-05",
        "title": "Defending the Edge: Representative-Attention for Mitigating Backdoor Attacks in Federated Learning",
        "author": "Chibueze Peace Obioma, Youcheng Sun, and Mustafa A. Mustafa",
        "link": "http://arxiv.org/abs/2505.10297v1",
        "abstract": "Federated learning (FL) enhances privacy and reduces communication cost for\nresource-constrained edge clients by supporting distributed model training at\nthe edge. However, the heterogeneous nature of such devices produces diverse,\nnon-independent, and identically distributed (non-IID) data, making the\ndetection of backdoor attacks more challenging. In this paper, we propose a\nnovel federated representative-attention-based defense mechanism, named FeRA,\nthat leverages cross-client attention over internal feature representations to\ndistinguish benign from malicious clients. FeRA computes an anomaly score based\non representation reconstruction errors, effectively identifying clients whose\ninternal activations significantly deviate from the group consensus. Our\nevaluation demonstrates FeRA's robustness across various FL scenarios,\nincluding challenging non-IID data distributions typical of edge devices.\nExperimental results show that it effectively reduces backdoor attack success\nrates while maintaining high accuracy on the main task. The method is\nmodel-agnostic, attack-agnostic, and does not require labeled reference data,\nmaking it well suited to heterogeneous and resource-limited edge deployments."
    },
    {
        "date": "2025-05",
        "title": "AttentionGuard: Transformer-based Misbehavior Detection for Secure Vehicular Platoons",
        "author": "Hexu Li, Konstantinos Kalogiannis, Ahmed Mohamed Hussain, and Panos Papadimitratos",
        "link": "http://arxiv.org/abs/2505.10273v1",
        "abstract": "Vehicle platooning, with vehicles traveling in close formation coordinated\nthrough Vehicle-to-Everything (V2X) communications, offers significant benefits\nin fuel efficiency and road utilization. However, it is vulnerable to\nsophisticated falsification attacks by authenticated insiders that can\ndestabilize the formation and potentially cause catastrophic collisions. This\npaper addresses this challenge: misbehavior detection in vehicle platooning\nsystems. We present AttentionGuard, a transformer-based framework for\nmisbehavior detection that leverages the self-attention mechanism to identify\nanomalous patterns in mobility data. Our proposal employs a multi-head\ntransformer-encoder to process sequential kinematic information, enabling\neffective differentiation between normal mobility patterns and falsification\nattacks across diverse platooning scenarios, including steady-state\n(no-maneuver) operation, join, and exit maneuvers. Our evaluation uses an\nextensive simulation dataset featuring various attack vectors (constant,\ngradual, and combined falsifications) and operational parameters (controller\ntypes, vehicle speeds, and attacker positions). Experimental results\ndemonstrate that AttentionGuard achieves up to 0.95 F1-score in attack\ndetection, with robust performance maintained during complex maneuvers.\nNotably, our system performs effectively with minimal latency (100ms decision\nintervals), making it suitable for real-time transportation safety\napplications. Comparative analysis reveals superior detection capabilities and\nestablishes the transformer-encoder as a promising approach for securing\nCooperative Intelligent Transport Systems (C-ITS) against sophisticated insider\nthreats."
    },
    {
        "date": "2025-05",
        "title": "Cutting Through Privacy: A Hyperplane-Based Data Reconstruction Attack in Federated Learning",
        "author": "Francesco Diana, Andr\u00e9 Nusser, Chuan Xu, and Giovanni Neglia",
        "link": "http://arxiv.org/abs/2505.10264v1",
        "abstract": "Federated Learning (FL) enables collaborative training of machine learning\nmodels across distributed clients without sharing raw data, ostensibly\npreserving data privacy. Nevertheless, recent studies have revealed critical\nvulnerabilities in FL, showing that a malicious central server can manipulate\nmodel updates to reconstruct clients' private training data. Existing data\nreconstruction attacks have important limitations: they often rely on\nassumptions about the clients' data distribution or their efficiency\nsignificantly degrades when batch sizes exceed just a few tens of samples.\n  In this work, we introduce a novel data reconstruction attack that overcomes\nthese limitations. Our method leverages a new geometric perspective on fully\nconnected layers to craft malicious model parameters, enabling the perfect\nrecovery of arbitrarily large data batches in classification tasks without any\nprior knowledge of clients' data. Through extensive experiments on both image\nand tabular datasets, we demonstrate that our attack outperforms existing\nmethods and achieves perfect reconstruction of data batches two orders of\nmagnitude larger than the state of the art."
    },
    {
        "date": "2025-05",
        "title": "The Tangent Space Attack",
        "author": "Axel Lemoine",
        "link": "http://arxiv.org/abs/2505.10184v1",
        "abstract": "We propose a new method for retrieving the algebraic structure of a generic\nalternant code given an arbitrary generator matrix, provided certain conditions\nare met. We then discuss how this challenges the security of the McEliece\ncryptosystem instantiated with this family of codes. The central object of our\nwork is the quadratic hull related to a linear code, defined as the\nintersection of all quadrics passing through the columns of a given generator\nor parity-check matrix, where the columns are considered as points in the\naffine or projective space. The geometric properties of this object reveal\nimportant information about the internal algebraic structure of the code. This\nis particularly evident in the case of generalized Reed-Solomon codes, whose\nquadratic hull is deeply linked to a well-known algebraic variety called the\nrational normal curve. By utilizing the concept of Weil restriction of affine\nvarieties, we demonstrate that the quadratic hull of a generic dual alternant\ncode inherits many interesting features from the rational normal curve, on\naccount of the fact that alternant codes are subfield-subcodes of generalized\nReed-Solomon codes. If the rate of the generic alternant code is sufficiently\nhigh, this allows us to construct a polynomial-time algorithm for retrieving\nthe underlying generalized Reed-Solomon code from which the alternant code is\ndefined, which leads to an efficient key-recovery attack against the McEliece\ncryptosystem when instantiated with this class of codes. Finally, we discuss\nthe generalization of this approach to Algebraic-Geometry codes and Goppa\ncodes."
    },
    {
        "date": "2025-05",
        "title": "VRSplat: Fast and Robust Gaussian Splatting for Virtual Reality",
        "author": "Xuechang Tu, Lukas Radl, Michael Steiner, Markus Steinberger, Bernhard Kerbl, and Fernando de la Torre",
        "link": "http://arxiv.org/abs/2505.10144v1",
        "abstract": "3D Gaussian Splatting (3DGS) has rapidly become a leading technique for\nnovel-view synthesis, providing exceptional performance through efficient\nsoftware-based GPU rasterization. Its versatility enables real-time\napplications, including on mobile and lower-powered devices. However, 3DGS\nfaces key challenges in virtual reality (VR): (1) temporal artifacts, such as\npopping during head movements, (2) projection-based distortions that result in\ndisturbing and view-inconsistent floaters, and (3) reduced framerates when\nrendering large numbers of Gaussians, falling below the critical threshold for\nVR. Compared to desktop environments, these issues are drastically amplified by\nlarge field-of-view, constant head movements, and high resolution of\nhead-mounted displays (HMDs). In this work, we introduce VRSplat: we combine\nand extend several recent advancements in 3DGS to address challenges of VR\nholistically. We show how the ideas of Mini-Splatting, StopThePop, and Optimal\nProjection can complement each other, by modifying the individual techniques\nand core 3DGS rasterizer. Additionally, we propose an efficient foveated\nrasterizer that handles focus and peripheral areas in a single GPU launch,\navoiding redundant computations and improving GPU utilization. Our method also\nincorporates a fine-tuning step that optimizes Gaussian parameters based on\nStopThePop depth evaluations and Optimal Projection. We validate our method\nthrough a controlled user study with 25 participants, showing a strong\npreference for VRSplat over other configurations of Mini-Splatting. VRSplat is\nthe first, systematically evaluated 3DGS approach capable of supporting modern\nVR applications, achieving 72+ FPS while eliminating popping and\nstereo-disrupting floaters."
    },
    {
        "date": "2025-05",
        "title": "Robust Federated Learning on Edge Devices with Domain Heterogeneity",
        "author": "Huy Q. Le, Latif U. Khan, and Choong Seon Hong",
        "link": "http://arxiv.org/abs/2505.10128v1",
        "abstract": "Federated Learning (FL) allows collaborative training while ensuring data\nprivacy across distributed edge devices, making it a popular solution for\nprivacy-sensitive applications. However, FL faces significant challenges due to\nstatistical heterogeneity, particularly domain heterogeneity, which impedes the\nglobal mode's convergence. In this study, we introduce a new framework to\naddress this challenge by improving the generalization ability of the FL global\nmodel under domain heterogeneity, using prototype augmentation. Specifically,\nwe introduce FedAPC (Federated Augmented Prototype Contrastive Learning), a\nprototype-based FL framework designed to enhance feature diversity and model\nrobustness. FedAPC leverages prototypes derived from the mean features of\naugmented data to capture richer representations. By aligning local features\nwith global prototypes, we enable the model to learn meaningful semantic\nfeatures while reducing overfitting to any specific domain. Experimental\nresults on the Office-10 and Digits datasets illustrate that our framework\noutperforms SOTA baselines, demonstrating superior performance."
    },
    {
        "date": "2025-05",
        "title": "When Mitigations Backfire: Timing Channel Attacks and Defense for PRAC-Based RowHammer Mitigations",
        "author": "Jeonghyun Woo, Joyce Qu, Gururaj Saileshwar, and Prashant J. Nair",
        "link": "http://arxiv.org/abs/2505.10111v3",
        "abstract": "Per Row Activation Counting (PRAC) has emerged as a robust framework for\nmitigating RowHammer (RH) vulnerabilities in modern DRAM systems. However, we\nuncover a critical vulnerability: a timing channel introduced by the Alert\nBack-Off (ABO) protocol and Refresh Management (RFM) commands. We present\nPRACLeak, a novel attack that exploits these timing differences to leak\nsensitive information, such as secret keys from vulnerable AES implementations,\nby monitoring memory access latencies.\n  To counter this, we propose Timing-Safe PRAC (TPRAC), a defense that\neliminates PRAC-induced timing channels without compromising RH mitigation\nefficacy. TPRAC uses Timing-Based RFMs, issued periodically and independent of\nmemory activity. It requires only a single-entry in-DRAM mitigation queue per\nDRAM bank and is compatible with existing DRAM standards. Our evaluations\ndemonstrate that TPRAC closes timing channels while incurring only 3.4%\nperformance overhead at the RH threshold of 1024."
    },
    {
        "date": "2025-05",
        "title": "One Shot Dominance: Knowledge Poisoning Attack on Retrieval-Augmented Generation Systems",
        "author": "Zhiyuan Chang, Mingyang Li, Xiaojun Jia, Junjie Wang, Yuekai Huang, Ziyou Jiang, Yang Liu, and Qing Wang",
        "link": "http://arxiv.org/abs/2505.11548v2",
        "abstract": "Large Language Models (LLMs) enhanced with Retrieval-Augmented Generation\n(RAG) have shown improved performance in generating accurate responses.\nHowever, the dependence on external knowledge bases introduces potential\nsecurity vulnerabilities, particularly when these knowledge bases are publicly\naccessible and modifiable. While previous studies have exposed knowledge\npoisoning risks in RAG systems, existing attack methods suffer from critical\nlimitations: they either require injecting multiple poisoned documents\n(resulting in poor stealthiness) or can only function effectively on simplistic\nqueries (limiting real-world applicability). This paper reveals a more\nrealistic knowledge poisoning attack against RAG systems that achieves\nsuccessful attacks by poisoning only a single document while remaining\neffective for complex multi-hop questions involving complex relationships\nbetween multiple elements. Our proposed AuthChain address three challenges to\nensure the poisoned documents are reliably retrieved and trusted by the LLM,\neven against large knowledge bases and LLM's own knowledge. Extensive\nexperiments across six popular LLMs demonstrate that AuthChain achieves\nsignificantly higher attack success rates while maintaining superior\nstealthiness against RAG defense mechanisms compared to state-of-the-art\nbaselines."
    },
    {
        "date": "2025-05",
        "title": "Evaluating Robustness of Deep Reinforcement Learning for Autonomous Surface Vehicle Control in Field Tests",
        "author": "Luis F. W. Batista, St\u00e9phanie Aravecchia, Seth Hutchinson, and C\u00e9dric Pradalier",
        "link": "http://arxiv.org/abs/2505.10033v1",
        "abstract": "Despite significant advancements in Deep Reinforcement Learning (DRL) for\nAutonomous Surface Vehicles (ASVs), their robustness in real-world conditions,\nparticularly under external disturbances, remains insufficiently explored. In\nthis paper, we evaluate the resilience of a DRL-based agent designed to capture\nfloating waste under various perturbations. We train the agent using domain\nrandomization and evaluate its performance in real-world field tests, assessing\nits ability to handle unexpected disturbances such as asymmetric drag and an\noff-center payload. We assess the agent's performance under these perturbations\nin both simulation and real-world experiments, quantifying performance\ndegradation and benchmarking it against an MPC baseline. Results indicate that\nthe DRL agent performs reliably despite significant disturbances. Along with\nthe open-source release of our implementation, we provide insights into\neffective training strategies, real-world challenges, and practical\nconsiderations for deploying DRLbased ASV controllers."
    },
    {
        "date": "2025-05",
        "title": "DeepSeqCoco: A Robust Mobile Friendly Deep Learning Model for Detection of Diseases in Cocos nucifera",
        "author": "Miit Daga, Dhriti Parikh, and Swarna Priya Ramu",
        "link": "http://arxiv.org/abs/2505.10030v1",
        "abstract": "Coconut tree diseases are a serious risk to agricultural yield, particularly\nin developing countries where conventional farming practices restrict early\ndiagnosis and intervention. Current disease identification methods are manual,\nlabor-intensive, and non-scalable. In response to these limitations, we come up\nwith DeepSeqCoco, a deep learning based model for accurate and automatic\ndisease identification from coconut tree images. The model was tested under\nvarious optimizer settings, such as SGD, Adam, and hybrid configurations, to\nidentify the optimal balance between accuracy, minimization of loss, and\ncomputational cost. Results from experiments indicate that DeepSeqCoco can\nachieve as much as 99.5% accuracy (achieving up to 5% higher accuracy than\nexisting models) with the hybrid SGD-Adam showing the lowest validation loss of\n2.81%. It also shows a drop of up to 18% in training time and up to 85% in\nprediction time for input images. The results point out the promise of the\nmodel to improve precision agriculture through an AI-based, scalable, and\nefficient disease monitoring system."
    },
    {
        "date": "2025-05",
        "title": "Sample Complexity of Distributionally Robust Average-Reward Reinforcement Learning",
        "author": "Zijun Chen, Shengbo Wang, and Nian Si",
        "link": "http://arxiv.org/abs/2505.10007v1",
        "abstract": "Motivated by practical applications where stable long-term performance is\ncritical-such as robotics, operations research, and healthcare-we study the\nproblem of distributionally robust (DR) average-reward reinforcement learning.\nWe propose two algorithms that achieve near-optimal sample complexity. The\nfirst reduces the problem to a DR discounted Markov decision process (MDP),\nwhile the second, Anchored DR Average-Reward MDP, introduces an anchoring state\nto stabilize the controlled transition kernels within the uncertainty set.\nAssuming the nominal MDP is uniformly ergodic, we prove that both algorithms\nattain a sample complexity of $\\widetilde{O}\\left(|\\mathbf{S}||\\mathbf{A}|\nt_{\\mathrm{mix}}^2\\varepsilon^{-2}\\right)$ for estimating the optimal policy as\nwell as the robust average reward under KL and $f_k$-divergence-based\nuncertainty sets, provided the uncertainty radius is sufficiently small. Here,\n$\\varepsilon$ is the target accuracy, $|\\mathbf{S}|$ and $|\\mathbf{A}|$ denote\nthe sizes of the state and action spaces, and $t_{\\mathrm{mix}}$ is the mixing\ntime of the nominal MDP. This represents the first finite-sample convergence\nguarantee for DR average-reward reinforcement learning. We further validate the\nconvergence rates of our algorithms through numerical experiments."
    },
    {
        "date": "2025-05",
        "title": "Sybil-based Virtual Data Poisoning Attacks in Federated Learning",
        "author": "Changxun Zhu, Qilong Wu, Lingjuan Lyu, and Shibei Xue",
        "link": "http://arxiv.org/abs/2505.09983v1",
        "abstract": "Federated learning is vulnerable to poisoning attacks by malicious\nadversaries. Existing methods often involve high costs to achieve effective\nattacks. To address this challenge, we propose a sybil-based virtual data\npoisoning attack, where a malicious client generates sybil nodes to amplify the\npoisoning model's impact. To reduce neural network computational complexity, we\ndevelop a virtual data generation method based on gradient matching. We also\ndesign three schemes for target model acquisition, applicable to online local,\nonline global, and offline scenarios. In simulation, our method outperforms\nother attack algorithms since our method can obtain a global target model under\nnon-independent uniformly distributed data."
    },
    {
        "date": "2025-05",
        "title": "Analysing Safety Risks in LLMs Fine-Tuned with Pseudo-Malicious Cyber Security Data",
        "author": "Adel ElZemity, Budi Arief, and Shujun Li",
        "link": "http://arxiv.org/abs/2505.09974v1",
        "abstract": "The integration of large language models (LLMs) into cyber security\napplications presents significant opportunities, such as enhancing threat\nanalysis and malware detection, but can also introduce critical risks and\nsafety concerns, including personal data leakage and automated generation of\nnew malware. We present a systematic evaluation of safety risks in fine-tuned\nLLMs for cyber security applications. Using the OWASP Top 10 for LLM\nApplications framework, we assessed seven open-source LLMs: Phi 3 Mini 3.8B,\nMistral 7B, Qwen 2.5 7B, Llama 3 8B, Llama 3.1 8B, Gemma 2 9B, and Llama 2 70B.\nOur evaluation shows that fine-tuning reduces safety resilience across all\ntested LLMs (e.g., the safety score of Llama 3.1 8B against prompt injection\ndrops from 0.95 to 0.15). We propose and evaluate a safety alignment approach\nthat carefully rewords instruction-response pairs to include explicit safety\nprecautions and ethical considerations. This approach demonstrates that it is\npossible to maintain or even improve model safety while preserving technical\nutility, offering a practical path forward for developing safer fine-tuning\nmethodologies. This work offers a systematic evaluation for safety risks in\nLLMs, enabling safer adoption of generative AI in sensitive domains, and\ncontributing towards the development of secure, trustworthy, and ethically\naligned LLMs."
    },
    {
        "date": "2025-05",
        "title": "Security and Privacy Measurement on Chinese Consumer IoT Traffic based on Device Lifecycle",
        "author": "Chenghua Jin, Yan Jia, Yuxin Song, Qingyin Tan, Rui Yang, and Zheli Liu",
        "link": "http://arxiv.org/abs/2505.09929v1",
        "abstract": "In recent years, consumer Internet of Things (IoT) devices have become widely\nused in daily life. With the popularity of devices, related security and\nprivacy risks arise at the same time as they collect user-related data and\ntransmit it to various service providers. Although China accounts for a larger\nshare of the consumer IoT industry, current analyses on consumer IoT device\ntraffic primarily focus on regions such as Europe, the United States, and\nAustralia. Research on China, however, is currently rather rare. This study\nconstructs the first large-scale dataset about consumer IoT device traffic in\nChina. Specifically, we propose a fine-grained traffic collection guidance\ncovering the entire lifecycle of consumer IoT devices, gathering traffic from\n70 devices spanning 36 brands and 8 device categories. Based on this dataset,\nwe analyze traffic destinations and encryption practices across different\ndevice types during the entire lifecycle and compare the findings with the\nresults of other regions. Compared to other regions, our results show that\nconsumer IoT devices in China rely more on domestic services and overally\nperform better in terms of encryption practices. However, there are still 20/35\ndevices improperly conduct certificate validation, and 5/70 devices use\ninsecure encryption protocols. To facilitate future research, we open-source\nour traffic collection guidance and make our dataset publicly available."
    },
    {
        "date": "2025-05",
        "title": "DeFeed: Secure Decentralized Cross-Contract Data Feed in Web 3.0 for Connected Autonomous Vehicles",
        "author": "Xingchen Sun, Runhua Xu, Wei Ni, Li Duan, and Chao Li",
        "link": "http://arxiv.org/abs/2505.09928v2",
        "abstract": "Smart contracts have been a topic of interest in blockchain research and are\na key enabling technology for Connected Autonomous Vehicles (CAVs) in the era\nof Web 3.0. These contracts enable trustless interactions without the need for\nintermediaries, as they operate based on predefined rules encoded on the\nblockchain. However, smart contacts face significant challenges in\ncross-contract communication and information sharing, making it difficult to\nestablish seamless connectivity and collaboration among CAVs with Web 3.0. In\nthis paper, we propose DeFeed, a novel secure protocol that incorporates\nvarious gas-saving functions for CAVs, originated from in-depth research into\nthe interaction among smart contracts for decentralized cross-contract data\nfeed in Web 3.0. DeFeed allows smart contracts to obtain information from other\ncontracts efficiently in a single click, without complicated operations. We\njudiciously design and complete various functions with DeFeed, including a pool\nfunction and a cache function for gas optimization, a subscribe function for\nfacilitating data access, and an update function for the future iteration of\nour protocol. Tailored for CAVs with Web 3.0 use cases, DeFeed enables\nefficient data feed between smart contracts underpinning decentralized\napplications and vehicle coordination. Implemented and tested on the Ethereum\nofficial test network, DeFeed demonstrates significant improvements in contract\ninteraction efficiency, reducing computational complexity and gas costs. Our\nsolution represents a critical step towards seamless, decentralized\ncommunication in Web 3.0 ecosystems."
    },
    {
        "date": "2025-05",
        "title": "PIG: Privacy Jailbreak Attack on LLMs via Gradient-based Iterative In-Context Optimization",
        "author": "Yidan Wang, Yanan Cao, Yubing Ren, Fang Fang, Zheng Lin, and Binxing Fang",
        "link": "http://arxiv.org/abs/2505.09921v2",
        "abstract": "Large Language Models (LLMs) excel in various domains but pose inherent\nprivacy risks. Existing methods to evaluate privacy leakage in LLMs often use\nmemorized prefixes or simple instructions to extract data, both of which\nwell-alignment models can easily block. Meanwhile, Jailbreak attacks bypass LLM\nsafety mechanisms to generate harmful content, but their role in privacy\nscenarios remains underexplored. In this paper, we examine the effectiveness of\njailbreak attacks in extracting sensitive information, bridging privacy leakage\nand jailbreak attacks in LLMs. Moreover, we propose PIG, a novel framework\ntargeting Personally Identifiable Information (PII) and addressing the\nlimitations of current jailbreak methods. Specifically, PIG identifies PII\nentities and their types in privacy queries, uses in-context learning to build\na privacy context, and iteratively updates it with three gradient-based\nstrategies to elicit target PII. We evaluate PIG and existing jailbreak methods\nusing two privacy-related datasets. Experiments on four white-box and two\nblack-box LLMs show that PIG outperforms baseline methods and achieves\nstate-of-the-art (SoTA) results. The results underscore significant privacy\nrisks in LLMs, emphasizing the need for stronger safeguards. Our code is\navailble at https://github.com/redwyd/PrivacyJailbreak."
    },
    {
        "date": "2025-05",
        "title": "Adversarial Attack on Large Language Models using Exponentiated Gradient Descent",
        "author": "Sajib Biswas, Mao Nishino, Samuel Jacob Chacko, and Xiuwen Liu",
        "link": "http://arxiv.org/abs/2505.09820v1",
        "abstract": "As Large Language Models (LLMs) are widely used, understanding them\nsystematically is key to improving their safety and realizing their full\npotential. Although many models are aligned using techniques such as\nreinforcement learning from human feedback (RLHF), they are still vulnerable to\njailbreaking attacks. Some of the existing adversarial attack methods search\nfor discrete tokens that may jailbreak a target model while others try to\noptimize the continuous space represented by the tokens of the model's\nvocabulary. While techniques based on the discrete space may prove to be\ninefficient, optimization of continuous token embeddings requires projections\nto produce discrete tokens, which might render them ineffective. To fully\nutilize the constraints and the structures of the space, we develop an\nintrinsic optimization technique using exponentiated gradient descent with the\nBregman projection method to ensure that the optimized one-hot encoding always\nstays within the probability simplex. We prove the convergence of the technique\nand implement an efficient algorithm that is effective in jailbreaking several\nwidely used LLMs. We demonstrate the efficacy of the proposed technique using\nfive open-source LLMs on four openly available datasets. The results show that\nthe technique achieves a higher success rate with great efficiency compared to\nthree other state-of-the-art jailbreaking techniques. The source code for our\nimplementation is available at:\nhttps://github.com/sbamit/Exponentiated-Gradient-Descent-LLM-Attack"
    },
    {
        "date": "2025-05",
        "title": "Super-Resolution Generative Adversarial Networks based Video Enhancement",
        "author": "Ka\u011fan \u00c7ET\u0130N",
        "link": "http://arxiv.org/abs/2505.10589v2",
        "abstract": "This study introduces an enhanced approach to video super-resolution by\nextending ordinary Single-Image Super-Resolution (SISR) Super-Resolution\nGenerative Adversarial Network (SRGAN) structure to handle spatio-temporal\ndata. While SRGAN has proven effective for single-image enhancement, its design\ndoes not account for the temporal continuity required in video processing. To\naddress this, a modified framework that incorporates 3D Non-Local Blocks is\nproposed, which is enabling the model to capture relationships across both\nspatial and temporal dimensions. An experimental training pipeline is\ndeveloped, based on patch-wise learning and advanced data degradation\ntechniques, to simulate real-world video conditions and learn from both local\nand global structures and details. This helps the model generalize better and\nmaintain stability across varying video content while maintaining the general\nstructure besides the pixel-wise correctness. Two model variants-one larger and\none more lightweight-are presented to explore the trade-offs between\nperformance and efficiency. The results demonstrate improved temporal\ncoherence, sharper textures, and fewer visual artifacts compared to traditional\nsingle-image methods. This work contributes to the development of practical,\nlearning-based solutions for video enhancement tasks, with potential\napplications in streaming, gaming, and digital restoration."
    },
    {
        "date": "2025-05",
        "title": "Self-Consuming Generative Models with Adversarially Curated Data",
        "author": "Xiukun Wei, and Xueru Zhang",
        "link": "http://arxiv.org/abs/2505.09768v1",
        "abstract": "Recent advances in generative models have made it increasingly difficult to\ndistinguish real data from model-generated synthetic data. Using synthetic data\nfor successive training of future model generations creates \"self-consuming\nloops\", which may lead to model collapse or training instability. Furthermore,\nsynthetic data is often subject to human feedback and curated by users based on\ntheir preferences. Ferbach et al. (2024) recently showed that when data is\ncurated according to user preferences, the self-consuming retraining loop\ndrives the model to converge toward a distribution that optimizes those\npreferences. However, in practice, data curation is often noisy or\nadversarially manipulated. For example, competing platforms may recruit\nmalicious users to adversarially curate data and disrupt rival models. In this\npaper, we study how generative models evolve under self-consuming retraining\nloops with noisy and adversarially curated data. We theoretically analyze the\nimpact of such noisy data curation on generative models and identify conditions\nfor the robustness of the retraining process. Building on this analysis, we\ndesign attack algorithms for competitive adversarial scenarios, where a\nplatform with a limited budget employs malicious users to misalign a rival's\nmodel from actual user preferences. Experiments on both synthetic and\nreal-world datasets demonstrate the effectiveness of the proposed algorithms."
    },
    {
        "date": "2025-05",
        "title": "Robust Federated Learning with Confidence-Weighted Filtering and GAN-Based Completion under Noisy and Incomplete Data",
        "author": "Alpaslan Gokcen, and Ali Boyaci",
        "link": "http://arxiv.org/abs/2505.09733v1",
        "abstract": "Federated learning (FL) presents an effective solution for collaborative\nmodel training while maintaining data privacy across decentralized client\ndatasets. However, data quality issues such as noisy labels, missing classes,\nand imbalanced distributions significantly challenge its effectiveness. This\nstudy proposes a federated learning methodology that systematically addresses\ndata quality issues, including noise, class imbalance, and missing labels. The\nproposed approach systematically enhances data integrity through adaptive noise\ncleaning, collaborative conditional GAN-based synthetic data generation, and\nrobust federated model training. Experimental evaluations conducted on\nbenchmark datasets (MNIST and Fashion-MNIST) demonstrate significant\nimprovements in federated model performance, particularly macro-F1 Score, under\nvarying noise and class imbalance conditions. Additionally, the proposed\nframework carefully balances computational feasibility and substantial\nperformance gains, ensuring practicality for resource constrained edge devices\nwhile rigorously maintaining data privacy. Our results indicate that this\nmethod effectively mitigates common data quality challenges, providing a\nrobust, scalable, and privacy compliant solution suitable for diverse\nreal-world federated learning scenarios."
    },
    {
        "date": "2025-05",
        "title": "Forests for Differences: Robust Causal Inference Beyond Parametric DiD",
        "author": "Hugo Gobato Souto, and Francisco Louzada Neto",
        "link": "http://arxiv.org/abs/2505.09706v1",
        "abstract": "This paper introduces the Difference-in-Differences Bayesian Causal Forest\n(DiD-BCF), a novel non-parametric model addressing key challenges in DiD\nestimation, such as staggered adoption and heterogeneous treatment effects.\nDiD-BCF provides a unified framework for estimating Average (ATE),\nGroup-Average (GATE), and Conditional Average Treatment Effects (CATE). A core\ninnovation, its Parallel Trends Assumption (PTA)-based reparameterization,\nenhances estimation accuracy and stability in complex panel data settings.\nExtensive simulations demonstrate DiD-BCF's superior performance over\nestablished benchmarks, particularly under non-linearity, selection biases, and\neffect heterogeneity. Applied to U.S. minimum wage policy, the model uncovers\nsignificant conditional treatment effect heterogeneity related to county\npopulation, insights obscured by traditional methods. DiD-BCF offers a robust\nand versatile tool for more nuanced causal inference in modern DiD\napplications."
    },
    {
        "date": "2025-05",
        "title": "Adversarial Suffix Filtering: a Defense Pipeline for LLMs",
        "author": "David Khachaturov, and Robert Mullins",
        "link": "http://arxiv.org/abs/2505.09602v1",
        "abstract": "Large Language Models (LLMs) are increasingly embedded in autonomous systems\nand public-facing environments, yet they remain susceptible to jailbreak\nvulnerabilities that may undermine their security and trustworthiness.\nAdversarial suffixes are considered to be the current state-of-the-art\njailbreak, consistently outperforming simpler methods and frequently succeeding\neven in black-box settings. Existing defenses rely on access to the internal\narchitecture of models limiting diverse deployment, increase memory and\ncomputation footprints dramatically, or can be bypassed with simple prompt\nengineering methods. We introduce $\\textbf{Adversarial Suffix Filtering}$\n(ASF), a lightweight novel model-agnostic defensive pipeline designed to\nprotect LLMs against adversarial suffix attacks. ASF functions as an input\npreprocessor and sanitizer that detects and filters adversarially crafted\nsuffixes in prompts, effectively neutralizing malicious injections. We\ndemonstrate that ASF provides comprehensive defense capabilities across both\nblack-box and white-box attack settings, reducing the attack efficacy of\nstate-of-the-art adversarial suffix generation methods to below 4%, while only\nminimally affecting the target model's capabilities in non-adversarial\nscenarios."
    },
    {
        "date": "2025-05",
        "title": "\\textsc{rfPG}: Robust Finite-Memory Policy Gradients for Hidden-Model POMDPs",
        "author": "Maris F. L. Galesloot, Roman Andriushchenko, Milan \u010ce\u0161ka, Sebastian Junges, and Nils Jansen",
        "link": "http://arxiv.org/abs/2505.09518v1",
        "abstract": "Partially observable Markov decision processes (POMDPs) model specific\nenvironments in sequential decision-making under uncertainty. Critically,\noptimal policies for POMDPs may not be robust against perturbations in the\nenvironment. Hidden-model POMDPs (HM-POMDPs) capture sets of different\nenvironment models, that is, POMDPs with a shared action and observation space.\nThe intuition is that the true model is hidden among a set of potential models,\nand it is unknown which model will be the environment at execution time. A\npolicy is robust for a given HM-POMDP if it achieves sufficient performance for\neach of its POMDPs. We compute such robust policies by combining two orthogonal\ntechniques: (1) a deductive formal verification technique that supports\ntractable robust policy evaluation by computing a worst-case POMDP within the\nHM-POMDP and (2) subgradient ascent to optimize the candidate policy for a\nworst-case POMDP. The empirical evaluation shows that, compared to various\nbaselines, our approach (1) produces policies that are more robust and\ngeneralize better to unseen POMDPs and (2) scales to HM-POMDPs that consist of\nover a hundred thousand environments."
    },
    {
        "date": "2025-05",
        "title": "Layered Unlearning for Adversarial Relearning",
        "author": "Timothy Qian, Vinith Suriyakumar, Ashia Wilson, and Dylan Hadfield-Menell",
        "link": "http://arxiv.org/abs/2505.09500v1",
        "abstract": "Our goal is to understand how post-training methods, such as fine-tuning,\nalignment, and unlearning, modify language model behavior and representations.\nWe are particularly interested in the brittle nature of these modifications\nthat makes them easy to bypass through prompt engineering or relearning. Recent\nresults suggest that post-training induces shallow context-dependent\n``circuits'' that suppress specific response patterns. This could be one\nexplanation for the brittleness of post-training. To test this hypothesis, we\ndesign an unlearning algorithm, Layered Unlearning (LU), that creates distinct\ninhibitory mechanisms for a growing subset of the data. By unlearning the first\n$i$ folds while retaining the remaining $k - i$ at the $i$th of $k$ stages, LU\nlimits the ability of relearning on a subset of data to recover the full\ndataset. We evaluate LU through a combination of synthetic and large language\nmodel (LLM) experiments. We find that LU improves robustness to adversarial\nrelearning for several different unlearning methods. Our results contribute to\nthe state-of-the-art of machine unlearning and provide insight into the effect\nof post-training updates."
    },
    {
        "date": "2025-05",
        "title": "Independent Component Analysis by Robust Distance Correlation",
        "author": "Sarah Leyder, Jakob Raymaekers, Peter J. Rousseeuw, Tom Van Deuren, and Tim Verdonck",
        "link": "http://arxiv.org/abs/2505.09425v1",
        "abstract": "Independent component analysis (ICA) is a powerful tool for decomposing a\nmultivariate signal or distribution into fully independent sources, not just\nuncorrelated ones. Unfortunately, most approaches to ICA are not robust against\noutliers. Here we propose a robust ICA method called RICA, which estimates the\ncomponents by minimizing a robust measure of dependence between multivariate\nrandom variables. The dependence measure used is the distance correlation\n(dCor). In order to make it more robust we first apply a new transformation\ncalled the bowl transform, which is bounded, one-to-one, continuous, and maps\nfar outliers to points close to the origin. This preserves the crucial property\nthat a zero dCor implies independence. RICA estimates the independent sources\nsequentially, by looking for the component that has the smallest dCor with the\nremainder. RICA is strongly consistent and has the usual parametric rate of\nconvergence. Its robustness is investigated by a simulation study, in which it\ngenerally outperforms its competitors. The method is illustrated on three\napplications, including the well-known cocktail party problem."
    },
    {
        "date": "2025-05",
        "title": "MoRAL: Motion-aware Multi-Frame 4D Radar and LiDAR Fusion for Robust 3D Object Detection",
        "author": "Xiangyuan Peng, Yu Wang, Miao Tang, Bierzynski Kay, Lorenzo Servadei, and Robert Wille",
        "link": "http://arxiv.org/abs/2505.09422v1",
        "abstract": "Reliable autonomous driving systems require accurate detection of traffic\nparticipants. To this end, multi-modal fusion has emerged as an effective\nstrategy. In particular, 4D radar and LiDAR fusion methods based on multi-frame\nradar point clouds have demonstrated the effectiveness in bridging the point\ndensity gap. However, they often neglect radar point clouds' inter-frame\nmisalignment caused by object movement during accumulation and do not fully\nexploit the object dynamic information from 4D radar. In this paper, we propose\nMoRAL, a motion-aware multi-frame 4D radar and LiDAR fusion framework for\nrobust 3D object detection. First, a Motion-aware Radar Encoder (MRE) is\ndesigned to compensate for inter-frame radar misalignment from moving objects.\nLater, a Motion Attention Gated Fusion (MAGF) module integrate radar motion\nfeatures to guide LiDAR features to focus on dynamic foreground objects.\nExtensive evaluations on the View-of-Delft (VoD) dataset demonstrate that MoRAL\noutperforms existing methods, achieving the highest mAP of 73.30% in the entire\narea and 88.68% in the driving corridor. Notably, our method also achieves the\nbest AP of 69.67% for pedestrians in the entire area and 96.25% for cyclists in\nthe driving corridor."
    },
    {
        "date": "2025-05",
        "title": "FedSaaS: Class-Consistency Federated Semantic Segmentation via Global Prototype Supervision and Local Adversarial Harmonization",
        "author": "Xiaoyang Yu, Xiaoming Wu, Xin Wang, Dongrun Li, Ming Yang, and Peng Cheng",
        "link": "http://arxiv.org/abs/2505.09385v1",
        "abstract": "Federated semantic segmentation enables pixel-level classification in images\nthrough collaborative learning while maintaining data privacy. However,\nexisting research commonly overlooks the fine-grained class relationships\nwithin the semantic space when addressing heterogeneous problems, particularly\ndomain shift. This oversight results in ambiguities between class\nrepresentation. To overcome this challenge, we propose a novel federated\nsegmentation framework that strikes class consistency, termed FedSaaS.\nSpecifically, we introduce class exemplars as a criterion for both local- and\nglobal-level class representations. On the server side, the uploaded class\nexemplars are leveraged to model class prototypes, which supervise global\nbranch of clients, ensuring alignment with global-level representation. On the\nclient side, we incorporate an adversarial mechanism to harmonize contributions\nof global and local branches, leading to consistent output. Moreover,\nmultilevel contrastive losses are employed on both sides to enforce consistency\nbetween two-level representations in the same semantic space. Extensive\nexperiments on several driving scene segmentation datasets demonstrate that our\nframework outperforms state-of-the-art methods, significantly improving average\nsegmentation accuracy and effectively addressing the class-consistency\nrepresentation problem."
    },
    {
        "date": "2025-05",
        "title": "DNS Query Forgery: A Client-Side Defense Against Mobile App Traffic Profiling",
        "author": "Andrea Jimenez-Berenguel, C\u00e9sar Gil, Carlos Garcia-Rubio, Jordi Forn\u00e9, and Celeste Campo",
        "link": "http://arxiv.org/abs/2505.09374v1",
        "abstract": "Mobile applications continuously generate DNS queries that can reveal\nsensitive user behavioral patterns even when communications are encrypted. This\npaper presents a privacy enhancement framework based on query forgery to\nprotect users against profiling attempts that leverage these background\ncommunications. We first mathematically model user profiles as probability\ndistributions over interest categories derived from mobile application traffic.\nWe then evaluate three query forgery strategies -- uniform sampling,\nTrackMeNot-based generation, and an optimized approach that minimizes\nKullback-Leibler divergence -- to quantify their effectiveness in obfuscating\nuser profiles. Then we create a synthetic dataset comprising 1,000 user traces\nconstructed from real mobile application traffic and we extract the user\nprofiles based on DNS traffic. Our evaluation reveals that a 50\\% privacy\nimprovement is achievable with less than 20\\% traffic overhead when using our\napproach, while achieving 100\\% privacy protection requires approximately\n40-60\\% additional traffic. We further propose a modular system architecture\nfor practical implementation of our protection mechanisms on mobile devices.\nThis work offers a client-side privacy solution that operates without\nthird-party trust requirements, empowering individual users to defend against\ntraffic analysis without compromising application functionality."
    },
    {
        "date": "2025-05",
        "title": "RobustSpring: Benchmarking Robustness to Image Corruptions for Optical Flow, Scene Flow and Stereo",
        "author": "Jenny Schmalfuss, Victor Oei, Lukas Mehl, Madlen Bartsch, Shashank Agnihotri, Margret Keuper, and Andr\u00e9s Bruhn",
        "link": "http://arxiv.org/abs/2505.09368v1",
        "abstract": "Standard benchmarks for optical flow, scene flow, and stereo vision\nalgorithms generally focus on model accuracy rather than robustness to image\ncorruptions like noise or rain. Hence, the resilience of models to such\nreal-world perturbations is largely unquantified. To address this, we present\nRobustSpring, a comprehensive dataset and benchmark for evaluating robustness\nto image corruptions for optical flow, scene flow, and stereo models.\nRobustSpring applies 20 different image corruptions, including noise, blur,\ncolor changes, quality degradations, and weather distortions, in a time-,\nstereo-, and depth-consistent manner to the high-resolution Spring dataset,\ncreating a suite of 20,000 corrupted images that reflect challenging\nconditions. RobustSpring enables comparisons of model robustness via a new\ncorruption robustness metric. Integration with the Spring benchmark enables\npublic two-axis evaluations of both accuracy and robustness. We benchmark a\ncurated selection of initial models, observing that accurate models are not\nnecessarily robust and that robustness varies widely by corruption type.\nRobustSpring is a new computer vision benchmark that treats robustness as a\nfirst-class citizen to foster models that combine accuracy with resilience. It\nwill be available at https://spring-benchmark.org."
    },
    {
        "date": "2025-05",
        "title": "Evaluating the Robustness of Adversarial Defenses in Malware Detection Systems",
        "author": "Mostafa Jafari, and Alireza Shameli-Sendi",
        "link": "http://arxiv.org/abs/2505.09342v1",
        "abstract": "Machine learning is a key tool for Android malware detection, effectively\nidentifying malicious patterns in apps. However, ML-based detectors are\nvulnerable to evasion attacks, where small, crafted changes bypass detection.\nDespite progress in adversarial defenses, the lack of comprehensive evaluation\nframeworks in binary-constrained domains limits understanding of their\nrobustness. We introduce two key contributions. First, Prioritized Binary\nRounding, a technique to convert continuous perturbations into binary feature\nspaces while preserving high attack success and low perturbation size. Second,\nthe sigma-binary attack, a novel adversarial method for binary domains,\ndesigned to achieve attack goals with minimal feature changes. Experiments on\nthe Malscan dataset show that sigma-binary outperforms existing attacks and\nexposes key vulnerabilities in state-of-the-art defenses. Defenses equipped\nwith adversary detectors, such as KDE, DLA, DNN+, and ICNN, exhibit significant\nbrittleness, with attack success rates exceeding 90% using fewer than 10\nfeature modifications and reaching 100% with just 20. Adversarially trained\ndefenses, including AT-rFGSM-k, AT-MaxMA, improves robustness under small\nbudgets but remains vulnerable to unrestricted perturbations, with attack\nsuccess rates of 99.45% and 96.62%, respectively. Although PAD-SMA demonstrates\nstrong robustness against state-of-the-art gradient-based adversarial attacks\nby maintaining an attack success rate below 16.55%, the sigma-binary attack\nsignificantly outperforms these methods, achieving a 94.56% success rate under\nunrestricted perturbations. These findings highlight the critical need for\nprecise method like sigma-binary to expose hidden vulnerabilities in existing\ndefenses and support the development of more resilient malware detection\nsystems."
    },
    {
        "date": "2025-05",
        "title": "Securing P4 Programs by Information Flow Control",
        "author": "Anoud Alshnakat, Amir M. Ahmadian, Musard Balliu, Roberto Guanciale, and Mads Dam",
        "link": "http://arxiv.org/abs/2505.09221v1",
        "abstract": "Software-Defined Networking (SDN) has transformed network architectures by\ndecoupling the control and data-planes, enabling fine-grained control over\npacket processing and forwarding. P4, a language designed for programming\ndata-plane devices, allows developers to define custom packet processing\nbehaviors directly on programmable network devices. This provides greater\ncontrol over packet forwarding, inspection, and modification. However, the\nincreased flexibility provided by P4 also brings significant security\nchallenges, particularly in managing sensitive data and preventing information\nleakage within the data-plane.\n  This paper presents a novel security type system for analyzing information\nflow in P4 programs that combines security types with interval analysis. The\nproposed type system allows the specification of security policies in terms of\ninput and output packet bit fields rather than program variables. We formalize\nthis type system and prove it sound, guaranteeing that well-typed programs\nsatisfy noninterference. Our prototype implementation, Tap4s, is evaluated on\nseveral use cases, demonstrating its effectiveness in detecting security\nviolations and information leakages."
    },
    {
        "date": "2025-05",
        "title": "DPN-GAN: Inducing Periodic Activations in Generative Adversarial Networks for High-Fidelity Audio Synthesis",
        "author": "Zeeshan Ahmad, Shudi Bao, and Meng Chen",
        "link": "http://arxiv.org/abs/2505.09091v1",
        "abstract": "In recent years, generative adversarial networks (GANs) have made significant\nprogress in generating audio sequences. However, these models typically rely on\nbandwidth-limited mel-spectrograms, which constrain the resolution of generated\naudio sequences, and lead to mode collapse during conditional generation. To\naddress this issue, we propose Deformable Periodic Network based GAN (DPN-GAN),\na novel GAN architecture that incorporates a kernel-based periodic ReLU\nactivation function to induce periodic bias in audio generation. This\ninnovative approach enhances the model's ability to capture and reproduce\nintricate audio patterns. In particular, our proposed model features a DPN\nmodule for multi-resolution generation utilizing deformable convolution\noperations, allowing for adaptive receptive fields that improve the quality and\nfidelity of the synthetic audio. Additionally, we enhance the discriminator\nnetwork using deformable convolution to better distinguish between real and\ngenerated samples, further refining the audio quality. We trained two versions\nof the model: DPN-GAN small (38.67M parameters) and DPN-GAN large (124M\nparameters). For evaluation, we use five different datasets, covering both\nspeech synthesis and music generation tasks, to demonstrate the efficiency of\nthe DPN-GAN. The experimental results demonstrate that DPN-GAN delivers\nsuperior performance on both out-of-distribution and noisy data, showcasing its\nrobustness and adaptability. Trained across various datasets, DPN-GAN\noutperforms state-of-the-art GAN architectures on standard evaluation metrics,\nand exhibits increased robustness in synthesized audio."
    },
    {
        "date": "2025-05",
        "title": "AdaFortiTran: An Adaptive Transformer Model for Robust OFDM Channel Estimation",
        "author": "Berkay Guler, and Hamid Jafarkhani",
        "link": "http://arxiv.org/abs/2505.09076v1",
        "abstract": "Deep learning models for channel estimation in Orthogonal Frequency Division\nMultiplexing (OFDM) systems often suffer from performance degradation under\nfast-fading channels and low-SNR scenarios. To address these limitations, we\nintroduce the Adaptive Fortified Transformer (AdaFortiTran), a novel model\nspecifically designed to enhance channel estimation in challenging\nenvironments. Our approach employs convolutional layers that exploit locality\nbias to capture strong correlations between neighboring channel elements,\ncombined with a transformer encoder that applies the global Attention mechanism\nto channel patches. This approach effectively models both long-range\ndependencies and spectro-temporal interactions within single OFDM frames. We\nfurther augment the model's adaptability by integrating nonlinear\nrepresentations of available channel statistics SNR, delay spread, and Doppler\nshift as priors. A residual connection is employed to merge global features\nfrom the transformer with local features from early convolutional processing,\nfollowed by final convolutional layers to refine the hierarchical channel\nrepresentation. Despite its compact architecture, AdaFortiTran achieves up to 6\ndB reduction in mean squared error (MSE) compared to state-of-the-art models.\nTested across a wide range of Doppler shifts (200-1000 Hz), SNRs (0 to 25 dB),\nand delay spreads (50-300 ns), it demonstrates superior robustness in\nhigh-mobility environments."
    },
    {
        "date": "2025-05",
        "title": "2D-3D Attention and Entropy for Pose Robust 2D Facial Recognition",
        "author": "J. Brennan Peace, Shuowen Hu, and Benjamin S. Riggan",
        "link": "http://arxiv.org/abs/2505.09073v1",
        "abstract": "Despite recent advances in facial recognition, there remains a fundamental\nissue concerning degradations in performance due to substantial perspective\n(pose) differences between enrollment and query (probe) imagery. Therefore, we\npropose a novel domain adaptive framework to facilitate improved performances\nacross large discrepancies in pose by enabling image-based (2D) representations\nto infer properties of inherently pose invariant point cloud (3D)\nrepresentations. Specifically, our proposed framework achieves better pose\ninvariance by using (1) a shared (joint) attention mapping to emphasize common\npatterns that are most correlated between 2D facial images and 3D facial data\nand (2) a joint entropy regularizing loss to promote better\nconsistency$\\unicode{x2014}$enhancing correlations among the intersecting 2D\nand 3D representations$\\unicode{x2014}$by leveraging both attention maps. This\nframework is evaluated on FaceScape and ARL-VTF datasets, where it outperforms\ncompetitive methods by achieving profile (90$\\unicode{x00b0}$$\\unicode{x002b}$)\nTAR @ 1$\\unicode{x0025}$ FAR improvements of at least 7.1$\\unicode{x0025}$ and\n1.57$\\unicode{x0025}$, respectively."
    },
    {
        "date": "2025-05",
        "title": "Revisiting Adversarial Perception Attacks and Defense Methods on Autonomous Driving Systems",
        "author": "Cheng Chen, Yuhong Wang, Nafis S Munir, Xiangwei Zhou, and Xugui Zhou",
        "link": "http://arxiv.org/abs/2505.11532v1",
        "abstract": "Autonomous driving systems (ADS) increasingly rely on deep learning-based\nperception models, which remain vulnerable to adversarial attacks. In this\npaper, we revisit adversarial attacks and defense methods, focusing on road\nsign recognition and lead object detection and prediction (e.g., relative\ndistance). Using a Level-2 production ADS, OpenPilot by Comma.ai, and the\nwidely adopted YOLO model, we systematically examine the impact of adversarial\nperturbations and assess defense techniques, including adversarial training,\nimage processing, contrastive learning, and diffusion models. Our experiments\nhighlight both the strengths and limitations of these methods in mitigating\ncomplex attacks. Through targeted evaluations of model robustness, we aim to\nprovide deeper insights into the vulnerabilities of ADS perception systems and\ncontribute guidance for developing more resilient defense strategies."
    },
    {
        "date": "2025-05",
        "title": "Unencrypted Flying Objects: Security Lessons from University Small Satellite Developers and Their Code",
        "author": "Rachel McAmis, Gregor Haas, Mattea Sim, David Kohlbrenner, and Tadayoshi Kohno",
        "link": "http://arxiv.org/abs/2505.09038v1",
        "abstract": "Satellites face a multitude of security risks that set them apart from\nhardware on Earth. Small satellites may face additional challenges, as they are\noften developed on a budget and by amateur organizations or universities that\ndo not consider security. We explore the security practices and preferences of\nsmall satellite teams, particularly university satellite teams, to understand\nwhat barriers exist to building satellites securely. We interviewed 8\nuniversity satellite club leaders across 4 clubs in the U.S. and perform a code\naudit of 3 of these clubs' code repositories. We find that security practices\nvary widely across teams, but all teams studied had vulnerabilities available\nto an unprivileged, ground-based attacker. Participants foresee many risks of\nunsecured small satellites and indicate security shortcomings in industry and\ngovernment. Lastly, we identify a set of considerations for how to build future\nsmall satellites securely, in amateur organizations and beyond."
    },
    {
        "date": "2025-05",
        "title": "Robust Emotion Recognition via Bi-Level Self-Supervised Continual Learning",
        "author": "Adnan Ahmad, Bahareh Nakisa, and Mohammad Naim Rastgoo",
        "link": "http://arxiv.org/abs/2505.10575v2",
        "abstract": "Emotion recognition through physiological signals such as\nelectroencephalogram (EEG) has become an essential aspect of affective\ncomputing and provides an objective way to capture human emotions. However,\nphysiological data characterized by cross-subject variability and noisy labels\nhinder the performance of emotion recognition models. Existing domain\nadaptation and continual learning methods struggle to address these issues,\nespecially under realistic conditions where data is continuously streamed and\nunlabeled. To overcome these limitations, we propose a novel bi-level\nself-supervised continual learning framework, SSOCL, based on a dynamic memory\nbuffer. This bi-level architecture iteratively refines the dynamic buffer and\npseudo-label assignments to effectively retain representative samples, enabling\ngeneralization from continuous, unlabeled physiological data streams for\nemotion recognition. The assigned pseudo-labels are subsequently leveraged for\naccurate emotion prediction. Key components of the framework, including a fast\nadaptation module and a cluster-mapping module, enable robust learning and\neffective handling of evolving data streams. Experimental validation on two\nmainstream EEG tasks demonstrates the framework's ability to adapt to\ncontinuous data streams while maintaining strong generalization across\nsubjects, outperforming existing approaches."
    },
    {
        "date": "2025-05",
        "title": "Lower Bounds on the MMSE of Adversarially Inferring Sensitive Features",
        "author": "Monica Welfert, Nathan Stromberg, Mario Diaz, and Lalitha Sankar",
        "link": "http://arxiv.org/abs/2505.09004v1",
        "abstract": "We propose an adversarial evaluation framework for sensitive feature\ninference based on minimum mean-squared error (MMSE) estimation with a finite\nsample size and linear predictive models. Our approach establishes theoretical\nlower bounds on the true MMSE of inferring sensitive features from noisy\nobservations of other correlated features. These bounds are expressed in terms\nof the empirical MMSE under a restricted hypothesis class and a non-negative\nerror term. The error term captures both the estimation error due to finite\nnumber of samples and the approximation error from using a restricted\nhypothesis class. For linear predictive models, we derive closed-form bounds,\nwhich are order optimal in terms of the noise variance, on the approximation\nerror for several classes of relationships between the sensitive and\nnon-sensitive features, including linear mappings, binary symmetric channels,\nand class-conditional multi-variate Gaussian distributions. We also present a\nnew lower bound that relies on the MSE computed on a hold-out validation\ndataset of the MMSE estimator learned on finite-samples and a restricted\nhypothesis class. Through empirical evaluation, we demonstrate that our\nframework serves as an effective tool for MMSE-based adversarial evaluation of\nsensitive feature inference that balances theoretical guarantees with practical\nefficiency."
    },
    {
        "date": "2025-05",
        "title": "SAFE-SiP: Secure Authentication Framework for System-in-Package Using Multi-party Computation",
        "author": "Ishraq Tashdid, Tasnuva Farheen, and Sazadur Rahman",
        "link": "http://arxiv.org/abs/2505.09002v1",
        "abstract": "The emergence of chiplet-based heterogeneous integration is transforming the\nsemiconductor, AI, and high-performance computing industries by enabling\nmodular designs and improved scalability. However, assembling chiplets from\nmultiple vendors after fabrication introduces a complex supply chain that\nraises serious security concerns, including counterfeiting, overproduction, and\nunauthorized access. Current solutions often depend on dedicated security\nchiplets or changes to the timing flow, which assume a trusted SiP integrator.\nThis assumption can expose chiplet signatures to other vendors and create new\nattack surfaces. This work addresses those vulnerabilities using Multi-party\nComputation (MPC), which enables zero-trust authentication without disclosing\nsensitive information to any party. We present SAFE-SiP, a scalable\nauthentication framework that garbles chiplet signatures and uses MPC for\nverifying integrity, effectively blocking unauthorized access and adversarial\ninference. SAFE-SiP removes the need for a dedicated security chiplet and\nensures secure authentication, even in untrusted integration scenarios. We\nevaluated SAFE-SiP on five RISC-V-based System-in-Package (SiP) designs.\nExperimental results show that SAFE-SiP incurs minimal power overhead, an\naverage area overhead of only 3.05%, and maintains a computational complexity\nof 2^192, offering a highly efficient and scalable security solution."
    },
    {
        "date": "2025-05",
        "title": "Towards Adaptive Meta-Gradient Adversarial Examples for Visual Tracking",
        "author": "Wei-Long Tian, Peng Gao, Xiao Liu, Long Xu, Hamido Fujita, Hanan Aljuai, and Mao-Li Wang",
        "link": "http://arxiv.org/abs/2505.08999v1",
        "abstract": "In recent years, visual tracking methods based on convolutional neural\nnetworks and Transformers have achieved remarkable performance and have been\nsuccessfully applied in fields such as autonomous driving. However, the\nnumerous security issues exposed by deep learning models have gradually\naffected the reliable application of visual tracking methods in real-world\nscenarios. Therefore, how to reveal the security vulnerabilities of existing\nvisual trackers through effective adversarial attacks has become a critical\nproblem that needs to be addressed. To this end, we propose an adaptive\nmeta-gradient adversarial attack (AMGA) method for visual tracking. This method\nintegrates multi-model ensembles and meta-learning strategies, combining\nmomentum mechanisms and Gaussian smoothing, which can significantly enhance the\ntransferability and attack effectiveness of adversarial examples. AMGA randomly\nselects models from a large model repository, constructs diverse tracking\nscenarios, and iteratively performs both white- and black-box adversarial\nattacks in each scenario, optimizing the gradient directions of each model.\nThis paradigm minimizes the gap between white- and black-box adversarial\nattacks, thus achieving excellent attack performance in black-box scenarios.\nExtensive experimental results on large-scale datasets such as OTB2015, LaSOT,\nand GOT-10k demonstrate that AMGA significantly improves the attack\nperformance, transferability, and deception of adversarial examples. Codes and\ndata are available at https://github.com/pgao-lab/AMGA."
    },
    {
        "date": "2025-05",
        "title": "Inference Attacks for X-Vector Speaker Anonymization",
        "author": "Luke Bauer, Wenxuan Bao, Malvika Jadhav, and Vincent Bindschaedler",
        "link": "http://arxiv.org/abs/2505.08978v1",
        "abstract": "We revisit the privacy-utility tradeoff of x-vector speaker anonymization.\nExisting approaches quantify privacy through training complex speaker\nverification or identification models that are later used as attacks. Instead,\nwe propose a novel inference attack for de-anonymization. Our attack is simple\nand ML-free yet we show experimentally that it outperforms existing approaches."
    },
    {
        "date": "2025-05",
        "title": "Securing RAG: A Risk Assessment and Mitigation Framework",
        "author": "Lukas Ammann, Sara Ott, Christoph R. Landolt, and Marco P. Lehmann",
        "link": "http://arxiv.org/abs/2505.08728v1",
        "abstract": "Retrieval Augmented Generation (RAG) has emerged as the de facto industry\nstandard for user-facing NLP applications, offering the ability to integrate\ndata without re-training or fine-tuning Large Language Models (LLMs). This\ncapability enhances the quality and accuracy of responses but also introduces\nnovel security and privacy challenges, particularly when sensitive data is\nintegrated. With the rapid adoption of RAG, securing data and services has\nbecome a critical priority. This paper first reviews the vulnerabilities of RAG\npipelines, and outlines the attack surface from data pre-processing and data\nstorage management to integration with LLMs. The identified risks are then\npaired with corresponding mitigations in a structured overview. In a second\nstep, the paper develops a framework that combines RAG-specific security\nconsiderations, with existing general security guidelines, industry standards,\nand best practices. The proposed framework aims to guide the implementation of\nrobust, compliant, secure, and trustworthy RAG systems."
    },
    {
        "date": "2025-05",
        "title": "Cryptologic Techniques and Associated Risks in Public and Private Security. An Italian and European Union Perspective with an Overview of the Current Legal Framework",
        "author": "Zana Kudriasova",
        "link": "http://arxiv.org/abs/2505.08650v1",
        "abstract": "This article examines the evolution of cryptologic techniques and their\nimplications for public and private security, focusing on the Italian and EU\nlegal frameworks. It explores the roles of cryptography, steganography, and\nquantum technologies in countering cybersecurity threats, emphasising the need\nfor robust legislation to address emerging challenges. Special attention is\ngiven to Italy's legislative reforms, including Law No. 90 of 2024, which\nstrengthens penalties for cybercrimes and establishes the National Cryptography\nCentre within the Italian National Cybersecurity Agency. Additionally, the\narticle highlights international initiatives, such as the UN's draft convention\non cybercrime, emphasising the balance between security, privacy, and\nfundamental human rights in a post-quantum era."
    },
    {
        "date": "2025-05",
        "title": "WaveGuard: Robust Deepfake Detection and Source Tracing via Dual-Tree Complex Wavelet and Graph Neural Networks",
        "author": "Ziyuan He, Zhiqing Guo, Liejun Wang, Gaobo Yang, Yunfeng Diao, and Dan Ma",
        "link": "http://arxiv.org/abs/2505.08614v2",
        "abstract": "Deepfake technology poses increasing risks such as privacy invasion and\nidentity theft. To address these threats, we propose WaveGuard, a proactive\nwatermarking framework that enhances robustness and imperceptibility via\nfrequency-domain embedding and graph-based structural consistency.\nSpecifically, we embed watermarks into high-frequency sub-bands using Dual-Tree\nComplex Wavelet Transform (DT-CWT) and employ a Structural Consistency Graph\nNeural Network (SC-GNN) to preserve visual quality. We also design an attention\nmodule to refine embedding precision. Experimental results on face swap and\nreenactment tasks demonstrate that WaveGuard outperforms state-of-the-art\nmethods in both robustness and visual quality. Code is available at\nhttps://github.com/vpsg-research/WaveGuard."
    },
    {
        "date": "2025-05",
        "title": "GradMix: Gradient-based Selective Mixup for Robust Data Augmentation in Class-Incremental Learning",
        "author": "Minsu Kim, Seong-Hyeon Hwang, and Steven Euijong Whang",
        "link": "http://arxiv.org/abs/2505.08528v1",
        "abstract": "In the context of continual learning, acquiring new knowledge while\nmaintaining previous knowledge presents a significant challenge. Existing\nmethods often use experience replay techniques that store a small portion of\nprevious task data for training. In experience replay approaches, data\naugmentation has emerged as a promising strategy to further improve the model\nperformance by mixing limited previous task data with sufficient current task\ndata. However, we theoretically and empirically analyze that training with\nmixed samples from random sample pairs may harm the knowledge of previous tasks\nand cause greater catastrophic forgetting. We then propose GradMix, a robust\ndata augmentation method specifically designed for mitigating catastrophic\nforgetting in class-incremental learning. GradMix performs gradient-based\nselective mixup using a class-based criterion that mixes only samples from\nhelpful class pairs and not from detrimental class pairs for reducing\ncatastrophic forgetting. Our experiments on various real datasets show that\nGradMix outperforms data augmentation baselines in accuracy by minimizing the\nforgetting of previous knowledge."
    },
    {
        "date": "2025-05",
        "title": "DArFace: Deformation Aware Robustness for Low Quality Face Recognition",
        "author": "Sadaf Gulshad, and Abdullah Aldahlawi Thakaa",
        "link": "http://arxiv.org/abs/2505.08423v1",
        "abstract": "Facial recognition systems have achieved remarkable success by leveraging\ndeep neural networks, advanced loss functions, and large-scale datasets.\nHowever, their performance often deteriorates in real-world scenarios involving\nlow-quality facial images. Such degradations, common in surveillance footage or\nstandoff imaging include low resolution, motion blur, and various distortions,\nresulting in a substantial domain gap from the high-quality data typically used\nduring training. While existing approaches attempt to address robustness by\nmodifying network architectures or modeling global spatial transformations,\nthey frequently overlook local, non-rigid deformations that are inherently\npresent in real-world settings. In this work, we introduce DArFace, a\nDeformation-Aware robust Face recognition framework that enhances robustness to\nsuch degradations without requiring paired high- and low-quality training\nsamples. Our method adversarially integrates both global transformations (e.g.,\nrotation, translation) and local elastic deformations during training to\nsimulate realistic low-quality conditions. Moreover, we introduce a contrastive\nobjective to enforce identity consistency across different deformed views.\nExtensive evaluations on low-quality benchmarks including TinyFace, IJB-B, and\nIJB-C demonstrate that DArFace surpasses state-of-the-art methods, with\nsignificant gains attributed to the inclusion of local deformation modeling."
    },
    {
        "date": "2025-05",
        "title": "SpecSphere: Dual-Pass Spectral-Spatial Graph Neural Networks with Certified Robustness",
        "author": "Yoonhyuk Choi, and Chong-Kwon Kim",
        "link": "http://arxiv.org/abs/2505.08320v2",
        "abstract": "We introduce SpecSphere, the first dual-pass spectral-spatial GNN that\ncertifies every prediction against both $\\ell\\_{0}$ edge flips and\n$\\ell\\_{\\infty}$ feature perturbations, adapts to the full\nhomophily-heterophily spectrum, and surpasses the expressive power of\n1-Weisfeiler-Lehman while retaining linear-time complexity. Our model couples a\nChebyshev-polynomial spectral branch with an attention-gated spatial branch and\nfuses their representations through a lightweight MLP trained in a\ncooperative-adversarial min-max game. We further establish (i) a uniform\nChebyshev approximation theorem, (ii) minimax-optimal risk across the\nhomophily-heterophily spectrum, (iii) closed-form robustness certificates, and\n(iv) universal approximation strictly beyond 1-WL. SpecSphere achieves\nstate-of-the-art node-classification accuracy and delivers tighter certified\nrobustness guarantees on real-world benchmarks. These results demonstrate that\nhigh expressivity, heterophily adaptation, and provable robustness can coexist\nwithin a single, scalable architecture."
    },
    {
        "date": "2025-05",
        "title": "On the Account Security Risks Posed by Password Strength Meters",
        "author": "Ming Xu, Weili Han, Jitao Yu, Jing Liu, Xinyi Zhang, Yun Lin, and Jin Song Dong",
        "link": "http://arxiv.org/abs/2505.08292v1",
        "abstract": "Password strength meters (PSMs) have been widely used by websites to gauge\npassword strength, encouraging users to create stronger passwords. Popular\ndata-driven PSMs, e.g., based on Markov, Probabilistic Context-free Grammar\n(PCFG) and neural networks, alarm strength based on a model learned from real\npasswords. Despite their proven effectiveness, the secure utility that arises\nfrom the leakage of trained passwords remains largely overlooked. To address\nthis gap, we analyze 11 PSMs and find that 5 data-driven meters are vulnerable\nto membership inference attacks that expose their trained passwords, and\nseriously, 3 rule-based meters openly disclose their blocked passwords. We\nspecifically design a PSM privacy leakage evaluation approach, and uncover that\na series of general data-driven meters are vulnerable to leaking between 10^4\nto 10^5 trained passwords, with the PCFG-based models being more vulnerable\nthan other counterparts; furthermore, we aid in deriving insights that the\ninherent utility-privacy tradeoff is not as severe as previously thought. To\nfurther exploit the risks, we develop novel meter-aware attacks when a clever\nattacker can filter the used passwords during compromising accounts on websites\nusing the meter, and experimentally show that attackers targeting websites that\ndeployed the popular Zxcvbn meter can compromise an additional 5.84% user\naccounts within 10 attempts, demonstrating the urgent need for\nprivacy-preserving PSMs that protect the confidentiality of the meter's used\npasswords. Finally, we sketch some counter-measures to mitigate these threats."
    },
    {
        "date": "2025-05",
        "title": "Adaptive Security Policy Management in Cloud Environments Using Reinforcement Learning",
        "author": "Muhammad Saqib, Dipkumar Mehta, Fnu Yashu, and Shubham Malhotra",
        "link": "http://arxiv.org/abs/2505.08837v1",
        "abstract": "The security of cloud environments, such as Amazon Web Services (AWS), is\ncomplex and dynamic. Static security policies have become inadequate as threats\nevolve and cloud resources exhibit elasticity [1]. This paper addresses the\nlimitations of static policies by proposing a security policy management\nframework that uses reinforcement learning (RL) to adapt dynamically.\nSpecifically, we employ deep reinforcement learning algorithms, including deep\nQ Networks and proximal policy optimization, enabling the learning and\ncontinuous adjustment of controls such as firewall rules and Identity and\nAccess Management (IAM) policies. The proposed RL based solution leverages\ncloud telemetry data (AWS Cloud Trail logs, network traffic data, threat\nintelligence feeds) to continuously refine security policies, maximizing threat\nmitigation, and compliance while minimizing resource impact. Experimental\nresults demonstrate that our adaptive RL based framework significantly\noutperforms static policies, achieving higher intrusion detection rates (92%\ncompared to 82% for static policies) and substantially reducing incident\ndetection and response times by 58%. In addition, it maintains high conformity\nwith security requirements and efficient resource usage. These findings\nvalidate the effectiveness of adaptive reinforcement learning approaches in\nimproving cloud security policy management."
    },
    {
        "date": "2025-05",
        "title": "Automatic Curriculum Learning for Driving Scenarios: Towards Robust and Efficient Reinforcement Learning",
        "author": "Ahmed Abouelazm, Tim Weinstein, Tim Joseph, Philip Sch\u00f6rner, and J. Marius Z\u00f6llner",
        "link": "http://arxiv.org/abs/2505.08264v1",
        "abstract": "This paper addresses the challenges of training end-to-end autonomous driving\nagents using Reinforcement Learning (RL). RL agents are typically trained in a\nfixed set of scenarios and nominal behavior of surrounding road users in\nsimulations, limiting their generalization and real-life deployment. While\ndomain randomization offers a potential solution by randomly sampling driving\nscenarios, it frequently results in inefficient training and sub-optimal\npolicies due to the high variance among training scenarios. To address these\nlimitations, we propose an automatic curriculum learning framework that\ndynamically generates driving scenarios with adaptive complexity based on the\nagent's evolving capabilities. Unlike manually designed curricula that\nintroduce expert bias and lack scalability, our framework incorporates a\n``teacher'' that automatically generates and mutates driving scenarios based on\ntheir learning potential -- an agent-centric metric derived from the agent's\ncurrent policy -- eliminating the need for expert design. The framework\nenhances training efficiency by excluding scenarios the agent has mastered or\nfinds too challenging. We evaluate our framework in a reinforcement learning\nsetting where the agent learns a driving policy from camera images. Comparative\nresults against baseline methods, including fixed scenario training and domain\nrandomization, demonstrate that our approach leads to enhanced generalization,\nachieving higher success rates: +9\\% in low traffic density, +21\\% in high\ntraffic density, and faster convergence with fewer training steps. Our findings\nhighlight the potential of ACL in improving the robustness and efficiency of\nRL-based autonomous driving agents."
    },
    {
        "date": "2025-05",
        "title": "Robustness Analysis against Adversarial Patch Attacks in Fully Unmanned Stores",
        "author": "Hyunsik Na, Wonho Lee, Seungdeok Roh, Sohee Park, and Daeseon Choi",
        "link": "http://arxiv.org/abs/2505.08835v1",
        "abstract": "The advent of convenient and efficient fully unmanned stores equipped with\nartificial intelligence-based automated checkout systems marks a new era in\nretail. However, these systems have inherent artificial intelligence security\nvulnerabilities, which are exploited via adversarial patch attacks,\nparticularly in physical environments. This study demonstrated that adversarial\npatches can severely disrupt object detection models used in unmanned stores,\nleading to issues such as theft, inventory discrepancies, and interference. We\ninvestigated three types of adversarial patch attacks -- Hiding, Creating, and\nAltering attacks -- and highlighted their effectiveness. We also introduce the\nnovel color histogram similarity loss function by leveraging attacker knowledge\nof the color information of a target class object. Besides the traditional\nconfusion-matrix-based attack success rate, we introduce a new\nbounding-boxes-based metric to analyze the practical impact of these attacks.\nStarting with attacks on object detection models trained on snack and fruit\ndatasets in a digital environment, we evaluated the effectiveness of\nadversarial patches in a physical testbed that mimicked a real unmanned store\nwith RGB cameras and realistic conditions. Furthermore, we assessed the\nrobustness of these attacks in black-box scenarios, demonstrating that shadow\nattacks can enhance success rates of attacks even without direct access to\nmodel parameters. Our study underscores the necessity for robust defense\nstrategies to protect unmanned stores from adversarial threats. Highlighting\nthe limitations of the current defense mechanisms in real-time detection\nsystems and discussing various proactive measures, we provide insights into\nimproving the robustness of object detection models and fortifying unmanned\nretail environments against these attacks."
    },
    {
        "date": "2025-05",
        "title": "LM-Scout: Analyzing the Security of Language Model Integration in Android Apps",
        "author": "Muhammad Ibrahim, G\u0171liz Seray Tuncay, Z. Berkay Celik, Aravind Machiry, and Antonio Bianchi",
        "link": "http://arxiv.org/abs/2505.08204v1",
        "abstract": "Developers are increasingly integrating Language Models (LMs) into their\nmobile apps to provide features such as chat-based assistants. To prevent LM\nmisuse, they impose various restrictions, including limits on the number of\nqueries, input length, and allowed topics. However, if the LM integration is\ninsecure, attackers can bypass these restrictions and gain unrestricted access\nto the LM, potentially harming developers' reputations and leading to\nsignificant financial losses.\n  This paper presents the first systematic study of insecure usage of LMs by\nAndroid apps. We first manually analyze a preliminary dataset of apps to\ninvestigate LM integration methods, construct a taxonomy that categorizes the\nLM usage restrictions implemented by the apps, and determine how to bypass\nthem. Alarmingly, we can bypass restrictions in 127 out of 181 apps. Then, we\ndevelop LM-Scout, a fully automated tool to detect on a large-scale vulnerable\nusage of LMs in 2,950 mobile apps. LM-Scout shows that, in many cases (i.e.,\n120 apps), it is possible to find and exploit such security issues\nautomatically. Finally, we identify the root causes for the identified issues\nand offer recommendations for secure LM integration."
    },
    {
        "date": "2025-05",
        "title": "Federated Large Language Models: Feasibility, Robustness, Security and Future Directions",
        "author": "Wenhao Jiang, Yuchuan Luo, Guilin Deng, Silong Chen, Xu Yang, Shihong Wu, Xinwen Gao, Lin Liu, and Shaojing Fu",
        "link": "http://arxiv.org/abs/2505.08830v1",
        "abstract": "The integration of Large Language Models (LLMs) and Federated Learning (FL)\npresents a promising solution for joint training on distributed data while\npreserving privacy and addressing data silo issues. However, this emerging\nfield, known as Federated Large Language Models (FLLM), faces significant\nchallenges, including communication and computation overheads, heterogeneity,\nprivacy and security concerns. Current research has primarily focused on the\nfeasibility of FLLM, but future trends are expected to emphasize enhancing\nsystem robustness and security. This paper provides a comprehensive review of\nthe latest advancements in FLLM, examining challenges from four critical\nperspectives: feasibility, robustness, security, and future directions. We\npresent an exhaustive survey of existing studies on FLLM feasibility, introduce\nmethods to enhance robustness in the face of resource, data, and task\nheterogeneity, and analyze novel risks associated with this integration,\nincluding privacy threats and security challenges. We also review the latest\ndevelopments in defense mechanisms and explore promising future research\ndirections, such as few-shot learning, machine unlearning, and IP protection.\nThis survey highlights the pressing need for further research to enhance system\nrobustness and security while addressing the unique challenges posed by the\nintegration of FL and LLM."
    },
    {
        "date": "2025-05",
        "title": "Fast Text-to-Audio Generation with Adversarial Post-Training",
        "author": "Zachary Novack, Zach Evans, Zack Zukowski, Josiah Taylor, CJ Carr, Julian Parker, Adnan Al-Sinan, Gian Marco Iodice, Julian McAuley, Taylor Berg-Kirkpatrick, and Jordi Pons",
        "link": "http://arxiv.org/abs/2505.08175v3",
        "abstract": "Text-to-audio systems, while increasingly performant, are slow at inference\ntime, thus making their latency unpractical for many creative applications. We\npresent Adversarial Relativistic-Contrastive (ARC) post-training, the first\nadversarial acceleration algorithm for diffusion/flow models not based on\ndistillation. While past adversarial post-training methods have struggled to\ncompare against their expensive distillation counterparts, ARC post-training is\na simple procedure that (1) extends a recent relativistic adversarial\nformulation to diffusion/flow post-training and (2) combines it with a novel\ncontrastive discriminator objective to encourage better prompt adherence. We\npair ARC post-training with a number optimizations to Stable Audio Open and\nbuild a model capable of generating $\\approx$12s of 44.1kHz stereo audio in\n$\\approx$75ms on an H100, and $\\approx$7s on a mobile edge-device, the fastest\ntext-to-audio model to our knowledge."
    },
    {
        "date": "2025-05",
        "title": "A Federated Random Forest Solution for Secure Distributed Machine Learning",
        "author": "Alexandre Cotorobai, Jorge Miguel Silva, and Jose Luis Oliveira",
        "link": "http://arxiv.org/abs/2505.08085v1",
        "abstract": "Privacy and regulatory barriers often hinder centralized machine learning\nsolutions, particularly in sectors like healthcare where data cannot be freely\nshared. Federated learning has emerged as a powerful paradigm to address these\nconcerns; however, existing frameworks primarily support gradient-based models,\nleaving a gap for more interpretable, tree-based approaches. This paper\nintroduces a federated learning framework for Random Forest classifiers that\npreserves data privacy and provides robust performance in distributed settings.\nBy leveraging PySyft for secure, privacy-aware computation, our method enables\nmultiple institutions to collaboratively train Random Forest models on locally\nstored data without exposing sensitive information. The framework supports\nweighted model averaging to account for varying data distributions, incremental\nlearning to progressively refine models, and local evaluation to assess\nperformance across heterogeneous datasets. Experiments on two real-world\nhealthcare benchmarks demonstrate that the federated approach maintains\ncompetitive predictive accuracy - within a maximum 9\\% margin of centralized\nmethods - while satisfying stringent privacy requirements. These findings\nunderscore the viability of tree-based federated learning for scenarios where\ndata cannot be centralized due to regulatory, competitive, or technical\nconstraints. The proposed solution addresses a notable gap in existing\nfederated learning libraries, offering an adaptable tool for secure distributed\nmachine learning tasks that demand both transparency and reliable performance.\nThe tool is available at https://github.com/ieeta-pt/fed_rf."
    },
    {
        "date": "2025-05",
        "title": "Browser Security Posture Analysis: A Client-Side Security Assessment Framework",
        "author": "Avihay Cohen",
        "link": "http://arxiv.org/abs/2505.08050v1",
        "abstract": "Modern web browsers have effectively become the new operating system for\nbusiness applications, yet their security posture is often under-scrutinized.\nThis paper presents a novel, comprehensive Browser Security Posture Analysis\nFramework[1], a browser-based client-side security assessment toolkit that runs\nentirely in JavaScript and WebAssembly within the browser. It performs a\nbattery of over 120 in-browser security tests in situ, providing fine-grained\ndiagnostics of security policies and features that network-level or os-level\ntools cannot observe. This yields insights into how well a browser enforces\ncritical client-side security invariants. We detail the motivation for such a\nframework, describe its architecture and implementation, and dive into the\ntechnical design of numerous test modules (covering the same-origin policy,\ncross-origin resource sharing, content security policy, sandboxing, XSS\nprotection, extension interference via WeakRefs, permissions audits, garbage\ncollection behavior, cryptographic APIs, SSL certificate validation, advanced\nweb platform security features like SharedArrayBuffer, Content filtering\ncontrols ,and internal network accessibility). We then present an experimental\nevaluation across different browsers and enterprise scenarios, highlighting\ngaps in legacy browsers and common misconfigurations. Finally, we discuss the\nsecurity and privacy implications of our findings, compare with related work in\nbrowser security and enterprise endpoint solutions, and outline future\nenhancements such as real-time posture monitoring and SIEM integration."
    },
    {
        "date": "2025-05",
        "title": "Dynamical Low-Rank Compression of Neural Networks with Robustness under Adversarial Attacks",
        "author": "Steffen Schotth\u00f6fer, H. Lexie Yang, and Stefan Schnake",
        "link": "http://arxiv.org/abs/2505.08022v1",
        "abstract": "Deployment of neural networks on resource-constrained devices demands models\nthat are both compact and robust to adversarial inputs. However, compression\nand adversarial robustness often conflict. In this work, we introduce a\ndynamical low-rank training scheme enhanced with a novel spectral regularizer\nthat controls the condition number of the low-rank core in each layer. This\napproach mitigates the sensitivity of compressed models to adversarial\nperturbations without sacrificing clean accuracy. The method is model- and\ndata-agnostic, computationally efficient, and supports rank adaptivity to\nautomatically compress the network at hand. Extensive experiments across\nstandard architectures, datasets, and adversarial attacks show the regularized\nnetworks can achieve over 94% compression while recovering or improving\nadversarial accuracy relative to uncompressed baselines."
    },
    {
        "date": "2025-05",
        "title": "RDD: Robust Feature Detector and Descriptor using Deformable Transformer",
        "author": "Gonglin Chen, Tianwen Fu, Haiwei Chen, Wenbin Teng, Hanyuan Xiao, and Yajie Zhao",
        "link": "http://arxiv.org/abs/2505.08013v1",
        "abstract": "As a core step in structure-from-motion and SLAM, robust feature detection\nand description under challenging scenarios such as significant viewpoint\nchanges remain unresolved despite their ubiquity. While recent works have\nidentified the importance of local features in modeling geometric\ntransformations, these methods fail to learn the visual cues present in\nlong-range relationships. We present Robust Deformable Detector (RDD), a novel\nand robust keypoint detector/descriptor leveraging the deformable transformer,\nwhich captures global context and geometric invariance through deformable\nself-attention mechanisms. Specifically, we observed that deformable attention\nfocuses on key locations, effectively reducing the search space complexity and\nmodeling the geometric invariance. Furthermore, we collected an Air-to-Ground\ndataset for training in addition to the standard MegaDepth dataset. Our\nproposed method outperforms all state-of-the-art keypoint detection/description\nmethods in sparse matching tasks and is also capable of semi-dense matching. To\nensure comprehensive evaluation, we introduce two challenging benchmarks: one\nemphasizing large viewpoint and scale variations, and the other being an\nAir-to-Ground benchmark -- an evaluation setting that has recently gaining\npopularity for 3D reconstruction across different altitudes."
    },
    {
        "date": "2025-05",
        "title": "Wasserstein Distributionally Robust Nonparametric Regression",
        "author": "Changyu Liu, Yuling Jiao, Junhui Wang, and Jian Huang",
        "link": "http://arxiv.org/abs/2505.07967v1",
        "abstract": "Distributionally robust optimization has become a powerful tool for\nprediction and decision-making under model uncertainty. By focusing on the\nlocal worst-case risk, it enhances robustness by identifying the most\nunfavorable distribution within a predefined ambiguity set. While extensive\nresearch has been conducted in parametric settings, studies on nonparametric\nframeworks remain limited. This paper studies the generalization properties of\nWasserstein distributionally robust nonparametric estimators, with particular\nattention to the impact of model misspecification, where non-negligible\ndiscrepancies between the estimation function space and target function can\nimpair generalization performance. We establish non-asymptotic error bounds for\nthe excess local worst-case risk by analyzing the regularization effects\ninduced by distributional perturbations and employing feedforward neural\nnetworks with Lipschitz constraints. These bounds illustrate how uncertainty\nlevels and neural network structures influence generalization performance and\nare applicable to both Lipschitz and quadratic loss functions. Furthermore, we\ninvestigate the Lagrangian relaxation of the local worst-case risk and derive\ncorresponding non-asymptotic error bounds for these estimators. The robustness\nof the proposed estimator is evaluated through simulation studies and\nillustrated with an application to the MNIST dataset."
    },
    {
        "date": "2025-05",
        "title": "Securing WiFi Fingerprint-based Indoor Localization Systems from Malicious Access Points",
        "author": "Fariha Tanjim Shifat, Sayma Sarwar Ela, and Mosarrat Jahan",
        "link": "http://arxiv.org/abs/2505.07724v1",
        "abstract": "WiFi fingerprint-based indoor localization schemes deliver highly accurate\nlocation data by matching the received signal strength indicator (RSSI) with an\noffline database using machine learning (ML) or deep learning (DL) models.\nHowever, over time, RSSI values degrade due to the malicious behavior of access\npoints (APs), causing low positional accuracy due to RSSI value mismatch with\nthe offline database. Existing literature lacks detection of malicious APs in\nthe online phase and mitigating their effects. This research addresses these\nlimitations and proposes a long-term reliable indoor localization scheme by\nincorporating malicious AP detection and their effect mitigation techniques.\nThe proposed scheme uses a Light Gradient-Boosting Machine (LGBM) classifier to\nestimate locations and integrates simple yet efficient techniques to detect\nmalicious APs based on online query data. Subsequently, a mitigation technique\nis incorporated that updates the offline database and online queries by\nimputing stable values for malicious APs using LGBM Regressors. Additionally,\nwe introduce a noise addition mechanism in the offline database to capture the\ndynamic environmental effects. Extensive experimental evaluation shows that the\nproposed scheme attains a detection accuracy above 95% for each attack type.\nThe mitigation strategy effectively restores the system's performance nearly to\nits original state when no malicious AP is present. The noise addition module\nreduces localization errors by nearly 16%. Furthermore, the proposed solution\nis lightweight, reducing the execution time by approximately 94% compared to\nthe existing methods."
    },
    {
        "date": "2025-05",
        "title": "Trial and Trust: Addressing Byzantine Attacks with Comprehensive Defense Strategy",
        "author": "Gleb Molodtsov, Daniil Medyakov, Sergey Skorik, Nikolas Khachaturov, Shahane Tigranyan, Vladimir Aletov, Aram Avetisyan, Martin Tak\u00e1\u010d, and Aleksandr Beznosikov",
        "link": "http://arxiv.org/abs/2505.07614v1",
        "abstract": "Recent advancements in machine learning have improved performance while also\nincreasing computational demands. While federated and distributed setups\naddress these issues, their structure is vulnerable to malicious influences. In\nthis paper, we address a specific threat, Byzantine attacks, where compromised\nclients inject adversarial updates to derail global convergence. We combine the\ntrust scores concept with trial function methodology to dynamically filter\noutliers. Our methods address the critical limitations of previous approaches,\nallowing functionality even when Byzantine nodes are in the majority. Moreover,\nour algorithms adapt to widely used scaled methods like Adam and RMSProp, as\nwell as practical scenarios, including local training and partial\nparticipation. We validate the robustness of our methods by conducting\nextensive experiments on both synthetic and real ECG data collected from\nmedical institutions. Furthermore, we provide a broad theoretical analysis of\nour algorithms and their extensions to aforementioned practical setups. The\nconvergence guarantees of our methods are comparable to those of classical\nalgorithms developed without Byzantine interference."
    },
    {
        "date": "2025-05",
        "title": "SecReEvalBench: A Multi-turned Security Resilience Evaluation Benchmark for Large Language Models",
        "author": "Huining Cui, and Wei Liu",
        "link": "http://arxiv.org/abs/2505.07584v2",
        "abstract": "The increasing deployment of large language models in security-sensitive\ndomains necessitates rigorous evaluation of their resilience against\nadversarial prompt-based attacks. While previous benchmarks have focused on\nsecurity evaluations with limited and predefined attack domains, such as\ncybersecurity attacks, they often lack a comprehensive assessment of\nintent-driven adversarial prompts and the consideration of real-life\nscenario-based multi-turn attacks. To address this gap, we present\nSecReEvalBench, the Security Resilience Evaluation Benchmark, which defines\nfour novel metrics: Prompt Attack Resilience Score, Prompt Attack Refusal Logic\nScore, Chain-Based Attack Resilience Score and Chain-Based Attack Rejection\nTime Score. Moreover, SecReEvalBench employs six questioning sequences for\nmodel assessment: one-off attack, successive attack, successive reverse attack,\nalternative attack, sequential ascending attack with escalating threat levels\nand sequential descending attack with diminishing threat levels. In addition,\nwe introduce a dataset customized for the benchmark, which incorporates both\nneutral and malicious prompts, categorised across seven security domains and\nsixteen attack techniques. In applying this benchmark, we systematically\nevaluate five state-of-the-art open-weighted large language models, Llama 3.1,\nGemma 2, Mistral v0.3, DeepSeek-R1 and Qwen 3. Our findings offer critical\ninsights into the strengths and weaknesses of modern large language models in\ndefending against evolving adversarial threats. The SecReEvalBench dataset is\npublicly available at\nhttps://kaggle.com/datasets/5a7ee22cf9dab6c93b55a73f630f6c9b42e936351b0ae98fbae6ddaca7fe248d,\nwhich provides a groundwork for advancing research in large language model\nsecurity."
    },
    {
        "date": "2025-05",
        "title": "Security through the Eyes of AI: How Visualization is Shaping Malware Detection",
        "author": "Matteo Brosolo, Asmitha K. A., Rafidha Rehiman K. A., Muhammed Shafi K. P., Serena Nicolazzo, Antonino Nocera, and Vinod P",
        "link": "http://arxiv.org/abs/2505.07574v2",
        "abstract": "Malware, a persistent cybersecurity threat, increasingly targets\ninterconnected digital systems such as desktop, mobile, and IoT platforms\nthrough sophisticated attack vectors. By exploiting these vulnerabilities,\nattackers compromise the integrity and resilience of modern digital ecosystems.\nTo address this risk, security experts actively employ Machine Learning or Deep\nLearning-based strategies, integrating static, dynamic, or hybrid approaches to\ncategorize malware instances. Despite their advantages, these methods have\ninherent drawbacks and malware variants persistently evolve with increased\nsophistication, necessitating advancements in detection strategies.\nVisualization-based techniques are emerging as scalable and interpretable\nsolutions for detecting and understanding malicious behaviors across diverse\nplatforms including desktop, mobile, IoT, and distributed systems as well as\nthrough analysis of network packet capture files. In this comprehensive survey\nof more than 100 high-quality research articles, we evaluate existing\nvisualization-based approaches applied to malware detection and classification.\nAs a first contribution, we propose a new all-encompassing framework to study\nthe landscape of visualization-based malware detection techniques. Within this\nframework, we systematically analyze state-of-the-art approaches across the\ncritical stages of the malware detection pipeline. By analyzing not only the\nsingle techniques but also how they are combined to produce the final solution,\nwe shed light on the main challenges in visualization-based approaches and\nprovide insights into the advancements and potential future directions in this\ncritical field."
    },
    {
        "date": "2025-05",
        "title": "Robust Kidney Abnormality Segmentation: A Validation Study of an AI-Based Framework",
        "author": "Sarah de Boer, Hartmut H\u00e4ntze, Kiran Vaidhya Venkadesh, Myrthe A. D. Buser, Gabriel E. Humpire Mamani, Lina Xu, Lisa C. Adams, Jawed Nawabi, Keno K. Bressem, Bram van Ginneken, Mathias Prokop, and Alessa Hering",
        "link": "http://arxiv.org/abs/2505.07573v1",
        "abstract": "Kidney abnormality segmentation has important potential to enhance the\nclinical workflow, especially in settings requiring quantitative assessments.\nKidney volume could serve as an important biomarker for renal diseases, with\nchanges in volume correlating directly with kidney function. Currently,\nclinical practice often relies on subjective visual assessment for evaluating\nkidney size and abnormalities, including tumors and cysts, which are typically\nstaged based on diameter, volume, and anatomical location. To support a more\nobjective and reproducible approach, this research aims to develop a robust,\nthoroughly validated kidney abnormality segmentation algorithm, made publicly\navailable for clinical and research use. We employ publicly available training\ndatasets and leverage the state-of-the-art medical image segmentation framework\nnnU-Net. Validation is conducted using both proprietary and public test\ndatasets, with segmentation performance quantified by Dice coefficient and the\n95th percentile Hausdorff distance. Furthermore, we analyze robustness across\nsubgroups based on patient sex, age, CT contrast phases, and tumor histologic\nsubtypes. Our findings demonstrate that our segmentation algorithm, trained\nexclusively on publicly available data, generalizes effectively to external\ntest sets and outperforms existing state-of-the-art models across all tested\ndatasets. Subgroup analyses reveal consistent high performance, indicating\nstrong robustness and reliability. The developed algorithm and associated code\nare publicly accessible at\nhttps://github.com/DIAGNijmegen/oncology-kidney-abnormality-segmentation."
    },
    {
        "date": "2025-05",
        "title": "GRADA: Graph-based Reranker against Adversarial Documents Attack",
        "author": "Jingjie Zheng, Aryo Pradipta Gema, Giwon Hong, Xuanli He, Pasquale Minervini, Youcheng Sun, and Qiongkai Xu",
        "link": "http://arxiv.org/abs/2505.07546v1",
        "abstract": "Retrieval Augmented Generation (RAG) frameworks improve the accuracy of large\nlanguage models (LLMs) by integrating external knowledge from retrieved\ndocuments, thereby overcoming the limitations of models' static intrinsic\nknowledge. However, these systems are susceptible to adversarial attacks that\nmanipulate the retrieval process by introducing documents that are adversarial\nyet semantically similar to the query. Notably, while these adversarial\ndocuments resemble the query, they exhibit weak similarity to benign documents\nin the retrieval set. Thus, we propose a simple yet effective Graph-based\nReranking against Adversarial Document Attacks (GRADA) framework aiming at\npreserving retrieval quality while significantly reducing the success of\nadversaries. Our study evaluates the effectiveness of our approach through\nexperiments conducted on five LLMs: GPT-3.5-Turbo, GPT-4o, Llama3.1-8b,\nLlama3.1-70b, and Qwen2.5-7b. We use three datasets to assess performance, with\nresults from the Natural Questions dataset demonstrating up to an 80% reduction\nin attack success rates while maintaining minimal loss in accuracy."
    },
    {
        "date": "2025-05",
        "title": "SynID: Passport Synthetic Dataset for Presentation Attack Detection",
        "author": "Juan E. Tapia, Fabian Stockhardt, L\u00e1zaro Janier Gonz\u00e1lez-Soler, and Christoph Busch",
        "link": "http://arxiv.org/abs/2505.07540v1",
        "abstract": "The demand for Presentation Attack Detection (PAD) to identify fraudulent ID\ndocuments in remote verification systems has significantly risen in recent\nyears. This increase is driven by several factors, including the rise of remote\nwork, online purchasing, migration, and advancements in synthetic images.\nAdditionally, we have noticed a surge in the number of attacks aimed at the\nenrolment process. Training a PAD to detect fake ID documents is very\nchallenging because of the limited number of ID documents available due to\nprivacy concerns. This work proposes a new passport dataset generated from a\nhybrid method that combines synthetic data and open-access information using\nthe ICAO requirement to obtain realistic training and testing images."
    },
    {
        "date": "2025-05",
        "title": "Post-Quantum Secure Decentralized Random Number Generation Protocol with Two Rounds of Communication in the Standard Model",
        "author": "Pham Nhat Minh, and Khuong Nguyen-An",
        "link": "http://arxiv.org/abs/2505.07536v1",
        "abstract": "Randomness plays a vital role in numerous applications, including simulation,\ncryptography, distributed systems, and gaming. Consequently, extensive research\nhas been conducted to generate randomness. One such method is to design a\ndecentralized random number generator (DRNG), a protocol that enables multiple\nparticipants to collaboratively generate random outputs that must be publicly\nverifiable. However, existing DRNGs are either not secure against quantum\ncomputers or depend on the random oracle model (ROM) to achieve security. In\nthis paper, we design a DRNG based on lattice-based publicly verifiable secret\nsharing (PVSS) that is post-quantum secure and proven secure in the standard\nmodel. Additionally, our DRNG requires only two rounds of communication to\ngenerate a single (pseudo)random value and can tolerate up to any t < n/2\ndishonest participants. To our knowledge, the proposed DRNG construction is the\nfirst to achieve all these properties."
    },
    {
        "date": "2025-05",
        "title": "From Search To Sampling: Generative Models For Robust Algorithmic Recourse",
        "author": "Prateek Garg, Lokesh Nagalapatti, and Sunita Sarawagi",
        "link": "http://arxiv.org/abs/2505.07351v1",
        "abstract": "Algorithmic Recourse provides recommendations to individuals who are\nadversely impacted by automated model decisions, on how to alter their profiles\nto achieve a favorable outcome. Effective recourse methods must balance three\nconflicting goals: proximity to the original profile to minimize cost,\nplausibility for realistic recourse, and validity to ensure the desired\noutcome. We show that existing methods train for these objectives separately\nand then search for recourse through a joint optimization over the recourse\ngoals during inference, leading to poor recourse recommendations. We introduce\nGenRe, a generative recourse model designed to train the three recourse\nobjectives jointly. Training such generative models is non-trivial due to lack\nof direct recourse supervision. We propose efficient ways to synthesize such\nsupervision and further show that GenRe's training leads to a consistent\nestimator. Unlike most prior methods, that employ non-robust gradient descent\nbased search during inference, GenRe simply performs a forward sampling over\nthe generative model to produce minimum cost recourse, leading to superior\nperformance across multiple metrics. We also demonstrate GenRe provides the\nbest trade-off between cost, plausibility and validity, compared to\nstate-of-art baselines. Our code is available at:\nhttps://github.com/prateekgargx/genre."
    },
    {
        "date": "2025-05",
        "title": "Assessing the Latency of Network Layer Security in 5G Networks",
        "author": "Sotiris Michaelides, Jonathan Mucke, and Martin Henze",
        "link": "http://arxiv.org/abs/2505.07328v1",
        "abstract": "In contrast to its predecessors, 5G supports a wide range of commercial,\nindustrial, and critical infrastructure scenarios. One key feature of 5G,\nultra-reliable low latency communication, is particularly appealing to such\nscenarios for its real-time capabilities. However, 5G's enhanced security,\nmostly realized through optional security controls, imposes additional overhead\non the network performance, potentially hindering its real-time capabilities.\nTo better assess this impact and guide operators in choosing between different\noptions, we measure the latency overhead of IPsec when applied over the N3 and\nthe service-based interfaces to protect user and control plane data,\nrespectively. Furthermore, we evaluate whether WireGuard constitutes an\nalternative to reduce this overhead. Our findings show that IPsec, if\nconfigured correctly, has minimal latency impact and thus is a prime candidate\nto secure real-time critical scenarios."
    },
    {
        "date": "2025-05",
        "title": "Machine Learning-Based Detection of DDoS Attacks in VANETs for Emergency Vehicle Communication",
        "author": "Bappa Muktar, Vincent Fono, and Adama Nouboukpo",
        "link": "http://arxiv.org/abs/2505.08810v1",
        "abstract": "Vehicular Ad Hoc Networks (VANETs) play a key role in Intelligent\nTransportation Systems (ITS), particularly in enabling real-time communication\nfor emergency vehicles. However, Distributed Denial of Service (DDoS) attacks,\nwhich interfere with safety-critical communication channels, can severely\nimpair their reliability. This study introduces a robust and scalable framework\nto detect DDoS attacks in highway-based VANET environments. A synthetic dataset\nwas constructed using Network Simulator 3 (NS-3) in conjunction with the\nSimulation of Urban Mobility (SUMO) and further enriched with real-world\nmobility traces from Germany's A81 highway, extracted via OpenStreetMap (OSM).\nThree traffic categories were simulated: DDoS, VoIP, and TCP-based video\nstreaming (VideoTCP). The data preprocessing pipeline included normalization,\nsignal-to-noise ratio (SNR) feature engineering, missing value imputation, and\nclass balancing using the Synthetic Minority Over-sampling Technique (SMOTE).\nFeature importance was assessed using SHapley Additive exPlanations (SHAP).\nEleven classifiers were benchmarked, among them XGBoost (XGB), CatBoost (CB),\nAdaBoost (AB), GradientBoosting (GB), and an Artificial Neural Network (ANN).\nXGB and CB achieved the best performance, each attaining an F1-score of 96%.\nThese results highlight the robustness of the proposed framework and its\npotential for real-time deployment in VANETs to secure critical emergency\ncommunications."
    },
    {
        "date": "2025-05",
        "title": "On the Robustness of Reward Models for Language Model Alignment",
        "author": "Jiwoo Hong, Noah Lee, Eunki Kim, Guijin Son, Woojin Chung, Aman Gupta, Shao Tang, and James Thorne",
        "link": "http://arxiv.org/abs/2505.07271v1",
        "abstract": "The Bradley-Terry (BT) model is widely practiced in reward modeling for\nreinforcement learning with human feedback (RLHF). Despite its effectiveness,\nreward models (RMs) trained with BT model loss are prone to over-optimization,\nlosing generalizability to unseen input distributions. In this paper, we study\nthe cause of over-optimization in RM training and its downstream effects on the\nRLHF procedure, accentuating the importance of distributional robustness of RMs\nin unseen data. First, we show that the excessive dispersion of hidden state\nnorms is the main source of over-optimization. Then, we propose batch-wise\nsum-to-zero regularization (BSR) to enforce zero-centered reward sum per batch,\nconstraining the rewards with extreme magnitudes. We assess the impact of BSR\nin improving robustness in RMs through four scenarios of over-optimization,\nwhere BSR consistently manifests better robustness. Subsequently, we compare\nthe plain BT model and BSR on RLHF training and empirically show that robust\nRMs better align the policy to the gold preference model. Finally, we apply BSR\nto high-quality data and models, which surpasses state-of-the-art RMs in the 8B\nscale by adding more than 5% in complex preference prediction tasks. By\nconducting RLOO training with 8B RMs, AlpacaEval 2.0 reduces generation length\nby 40% while adding a 7% increase in win rate, further highlighting that\nrobustness in RMs induces robustness in RLHF training. We release the code,\ndata, and models: https://github.com/LinkedIn-XFACT/RM-Robustness."
    },
    {
        "date": "2025-05",
        "title": "Adaptive, Robust and Scalable Bayesian Filtering for Online Learning",
        "author": "Gerardo Duran-Martin",
        "link": "http://arxiv.org/abs/2505.07267v1",
        "abstract": "In this thesis, we introduce Bayesian filtering as a principled framework for\ntackling diverse sequential machine learning problems, including online\n(continual) learning, prequential (one-step-ahead) forecasting, and contextual\nbandits. To this end, this thesis addresses key challenges in applying Bayesian\nfiltering to these problems: adaptivity to non-stationary environments,\nrobustness to model misspecification and outliers, and scalability to the\nhigh-dimensional parameter space of deep neural networks. We develop novel\ntools within the Bayesian filtering framework to address each of these\nchallenges, including: (i) a modular framework that enables the development\nadaptive approaches for online learning; (ii) a novel, provably robust filter\nwith similar computational cost to standard filters, that employs Generalised\nBayes; and (iii) a set of tools for sequentially updating model parameters\nusing approximate second-order optimisation methods that exploit the\noverparametrisation of high-dimensional parametric models such as neural\nnetworks. Theoretical analysis and empirical results demonstrate the improved\nperformance of our methods in dynamic, high-dimensional, and misspecified\nmodels."
    },
    {
        "date": "2025-05",
        "title": "MixBridge: Heterogeneous Image-to-Image Backdoor Attack through Mixture of Schr\u00f6dinger Bridges",
        "author": "Shixi Qin, Zhiyong Yang, Shilong Bao, Shi Wang, Qianqian Xu, and Qingming Huang",
        "link": "http://arxiv.org/abs/2505.08809v1",
        "abstract": "This paper focuses on implanting multiple heterogeneous backdoor triggers in\nbridge-based diffusion models designed for complex and arbitrary input\ndistributions. Existing backdoor formulations mainly address single-attack\nscenarios and are limited to Gaussian noise input models. To fill this gap, we\npropose MixBridge, a novel diffusion Schr\\\"odinger bridge (DSB) framework to\ncater to arbitrary input distributions (taking I2I tasks as special cases).\nBeyond this trait, we demonstrate that backdoor triggers can be injected into\nMixBridge by directly training with poisoned image pairs. This eliminates the\nneed for the cumbersome modifications to stochastic differential equations\nrequired in previous studies, providing a flexible tool to study backdoor\nbehavior for bridge models. However, a key question arises: can a single DSB\nmodel train multiple backdoor triggers? Unfortunately, our theory shows that\nwhen attempting this, the model ends up following the geometric mean of benign\nand backdoored distributions, leading to performance conflict across backdoor\ntasks. To overcome this, we propose a Divide-and-Merge strategy to mix\ndifferent bridges, where models are independently pre-trained for each specific\nobjective (Divide) and then integrated into a unified model (Merge). In\naddition, a Weight Reallocation Scheme (WRS) is also designed to enhance the\nstealthiness of MixBridge. Empirical studies across diverse generation tasks\nspeak to the efficacy of MixBridge."
    },
    {
        "date": "2025-05",
        "title": "Securing Genomic Data Against Inference Attacks in Federated Learning Environments",
        "author": "Chetan Pathade, and Shubham Patil",
        "link": "http://arxiv.org/abs/2505.07188v1",
        "abstract": "Federated Learning (FL) offers a promising framework for collaboratively\ntraining machine learning models across decentralized genomic datasets without\ndirect data sharing. While this approach preserves data locality, it remains\nsusceptible to sophisticated inference attacks that can compromise individual\nprivacy. In this study, we simulate a federated learning setup using synthetic\ngenomic data and assess its vulnerability to three key attack vectors:\nMembership Inference Attack (MIA), Gradient-Based Membership Inference Attack,\nand Label Inference Attack (LIA). Our experiments reveal that Gradient-Based\nMIA achieves the highest effectiveness, with a precision of 0.79 and F1-score\nof 0.87, underscoring the risk posed by gradient exposure in federated updates.\nAdditionally, we visualize comparative attack performance through radar plots\nand quantify model leakage across clients. The findings emphasize the\ninadequacy of na\\\"ive FL setups in safeguarding genomic privacy and motivate\nthe development of more robust privacy-preserving mechanisms tailored to the\nunique sensitivity of genomic data."
    },
    {
        "date": "2025-05",
        "title": "Security of Internet of Agents: Attacks and Countermeasures",
        "author": "Yuntao Wang, Yanghe Pan, Shaolong Guo, and Zhou Su",
        "link": "http://arxiv.org/abs/2505.08807v1",
        "abstract": "With the rise of large language and vision-language models, AI agents have\nevolved into autonomous, interactive systems capable of perception, reasoning,\nand decision-making. As they proliferate across virtual and physical domains,\nthe Internet of Agents (IoA) has emerged as a key infrastructure for enabling\nscalable and secure coordination among heterogeneous agents. This survey offers\na comprehensive examination of the security and privacy landscape in IoA\nsystems. We begin by outlining the IoA architecture and its distinct\nvulnerabilities compared to traditional networks, focusing on four critical\naspects: identity authentication threats, cross-agent trust issues, embodied\nsecurity, and privacy risks. We then review existing and emerging defense\nmechanisms and highlight persistent challenges. Finally, we identify open\nresearch directions to advance the development of resilient and\nprivacy-preserving IoA ecosystems."
    },
    {
        "date": "2025-05",
        "title": "One Trigger Token Is Enough: A Defense Strategy for Balancing Safety and Usability in Large Language Models",
        "author": "Haoran Gu, Handing Wang, Yi Mei, Mengjie Zhang, and Yaochu Jin",
        "link": "http://arxiv.org/abs/2505.07167v1",
        "abstract": "Large Language Models (LLMs) have been extensively used across diverse\ndomains, including virtual assistants, automated code generation, and\nscientific research. However, they remain vulnerable to jailbreak attacks,\nwhich manipulate the models into generating harmful responses despite safety\nalignment. Recent studies have shown that current safety-aligned LLMs often\nundergo the shallow safety alignment, where the first few tokens largely\ndetermine whether the response will be harmful. Through comprehensive\nobservations, we find that safety-aligned LLMs and various defense strategies\ngenerate highly similar initial tokens in their refusal responses, which we\ndefine as safety trigger tokens. Building on this insight, we propose\n\\texttt{D-STT}, a simple yet effective defense algorithm that identifies and\nexplicitly decodes safety trigger tokens of the given safety-aligned LLM to\ntrigger the model's learned safety patterns. In this process, the safety\ntrigger is constrained to a single token, which effectively preserves model\nusability by introducing minimum intervention in the decoding process.\nExtensive experiments across diverse jailbreak attacks and benign prompts\ndemonstrate that \\ours significantly reduces output harmfulness while\npreserving model usability and incurring negligible response time overhead,\noutperforming ten baseline methods."
    },
    {
        "date": "2025-05",
        "title": "AugMixCloak: A Defense against Membership Inference Attacks via Image Transformation",
        "author": "Heqing Ren, Chao Feng, Alberto Huertas, and Burkhard Stiller",
        "link": "http://arxiv.org/abs/2505.07149v1",
        "abstract": "Traditional machine learning (ML) raises serious privacy concerns, while\nfederated learning (FL) mitigates the risk of data leakage by keeping data on\nlocal devices. However, the training process of FL can still leak sensitive\ninformation, which adversaries may exploit to infer private data. One of the\nmost prominent threats is the membership inference attack (MIA), where the\nadversary aims to determine whether a particular data record was part of the\ntraining set.\n  This paper addresses this problem through a two-stage defense called\nAugMixCloak. The core idea is to apply data augmentation and principal\ncomponent analysis (PCA)-based information fusion to query images, which are\ndetected by perceptual hashing (pHash) as either identical to or highly similar\nto images in the training set. Experimental results show that AugMixCloak\nsuccessfully defends against both binary classifier-based MIA and metric-based\nMIA across five datasets and various decentralized FL (DFL) topologies.\nCompared with regularization-based defenses, AugMixCloak demonstrates stronger\nprotection. Compared with confidence score masking, AugMixCloak exhibits better\ngeneralization."
    },
    {
        "date": "2025-05",
        "title": "Standing Firm in 5G: A Single-Round, Dropout-Resilient Secure Aggregation for Federated Learning",
        "author": "Yiwei Zhang, Rouzbeh Behnia, Imtiaz Karim, Attila A. Yavuz, and Elisa Bertino",
        "link": "http://arxiv.org/abs/2505.07148v1",
        "abstract": "Federated learning (FL) is well-suited to 5G networks, where many mobile\ndevices generate sensitive edge data. Secure aggregation protocols enhance\nprivacy in FL by ensuring that individual user updates reveal no information\nabout the underlying client data. However, the dynamic and large-scale nature\nof 5G-marked by high mobility and frequent dropouts-poses significant\nchallenges to the effective adoption of these protocols. Existing protocols\noften require multi-round communication or rely on fixed infrastructure,\nlimiting their practicality. We propose a lightweight, single-round secure\naggregation protocol designed for 5G environments. By leveraging base stations\nfor assisted computation and incorporating precomputation, key-homomorphic\npseudorandom functions, and t-out-of-k secret sharing, our protocol ensures\nefficiency, robustness, and privacy. Experiments show strong security\nguarantees and significant gains in communication and computation efficiency,\nmaking the approach well-suited for real-world 5G FL deployments."
    },
    {
        "date": "2025-05",
        "title": "Efficient and Robust Multidimensional Attention in Remote Physiological Sensing through Target Signal Constrained Factorization",
        "author": "Jitesh Joshi, and Youngjun Cho",
        "link": "http://arxiv.org/abs/2505.07013v1",
        "abstract": "Remote physiological sensing using camera-based technologies offers\ntransformative potential for non-invasive vital sign monitoring across\nhealthcare and human-computer interaction domains. Although deep learning\napproaches have advanced the extraction of physiological signals from video\ndata, existing methods have not been sufficiently assessed for their robustness\nto domain shifts. These shifts in remote physiological sensing include\nvariations in ambient conditions, camera specifications, head movements, facial\nposes, and physiological states which often impact real-world performance\nsignificantly. Cross-dataset evaluation provides an objective measure to assess\ngeneralization capabilities across these domain shifts. We introduce Target\nSignal Constrained Factorization module (TSFM), a novel multidimensional\nattention mechanism that explicitly incorporates physiological signal\ncharacteristics as factorization constraints, allowing more precise feature\nextraction. Building on this innovation, we present MMRPhys, an efficient\ndual-branch 3D-CNN architecture designed for simultaneous multitask estimation\nof photoplethysmography (rPPG) and respiratory (rRSP) signals from multimodal\nRGB and thermal video inputs. Through comprehensive cross-dataset evaluation on\nfive benchmark datasets, we demonstrate that MMRPhys with TSFM significantly\noutperforms state-of-the-art methods in generalization across domain shifts for\nrPPG and rRSP estimation, while maintaining a minimal inference latency\nsuitable for real-time applications. Our approach establishes new benchmarks\nfor robust multitask and multimodal physiological sensing and offers a\ncomputationally efficient framework for practical deployment in unconstrained\nenvironments. The web browser-based application featuring on-device real-time\ninference of MMRPhys model is available at\nhttps://physiologicailab.github.io/mmrphys-live"
    },
    {
        "date": "2025-05",
        "title": "Technical Report for ICRA 2025 GOOSE 2D Semantic Segmentation Challenge: Leveraging Color Shift Correction, RoPE-Swin Backbone, and Quantile-based Label Denoising Strategy for Robust Outdoor Scene Understanding",
        "author": "Chih-Chung Hsu, I-Hsuan Wu, Wen-Hai Tseng, Ching-Heng Cheng, Ming-Hsuan Wu, Jin-Hui Jiang, and Yu-Jou Hsiao",
        "link": "http://arxiv.org/abs/2505.06991v1",
        "abstract": "This report presents our semantic segmentation framework developed by team\nACVLAB for the ICRA 2025 GOOSE 2D Semantic Segmentation Challenge, which\nfocuses on parsing outdoor scenes into nine semantic categories under\nreal-world conditions. Our method integrates a Swin Transformer backbone\nenhanced with Rotary Position Embedding (RoPE) for improved spatial\ngeneralization, alongside a Color Shift Estimation-and-Correction module\ndesigned to compensate for illumination inconsistencies in natural\nenvironments. To further improve training stability, we adopt a quantile-based\ndenoising strategy that downweights the top 2.5\\% of highest-error pixels,\ntreating them as noise and suppressing their influence during optimization.\nEvaluated on the official GOOSE test set, our approach achieved a mean\nIntersection over Union (mIoU) of 0.848, demonstrating the effectiveness of\ncombining color correction, positional encoding, and error-aware denoising in\nrobust semantic segmentation."
    },
    {
        "date": "2025-05",
        "title": "Federated Learning with LoRA Optimized DeiT and Multiscale Patch Embedding for Secure Eye Disease Recognition",
        "author": "Md. Naimur Asif Borno, Md Sakib Hossain Shovon, MD Hanif Sikder, Iffat Firozy Rimi, Tahani Jaser Alahmadi, and Mohammad Ali Moni",
        "link": "http://arxiv.org/abs/2505.06982v1",
        "abstract": "Recent progress in image-based medical disease detection encounters\nchallenges such as limited annotated data sets, inadequate spatial feature\nanalysis, data security issues, and inefficient training frameworks. This study\nintroduces a data-efficient image transformer (DeIT)-based approach that\novercomes these challenges by utilizing multiscale patch embedding for better\nfeature extraction and stratified weighted random sampling to address class\nimbalance. The model also incorporates a LoRA-enhanced transformer encoder, a\ndistillation framework, and federated learning for decentralized training,\nimproving both efficiency and data security. Consequently, it achieves\nstate-of-the-art performance, with the highest AUC, F1 score, precision,\nminimal loss, and Top-5 accuracy. Additionally, Grad-CAM++ visualizations\nimprove interpretability by highlighting critical pathological regions,\nenhancing the model's clinical relevance. These results highlight the potential\nof this approach to advance AI-powered medical imaging and disease detection."
    },
    {
        "date": "2025-05",
        "title": "A Formally Verified Robustness Certifier for Neural Networks (Extended Version)",
        "author": "James Tobler, Hira Taqdees Syeda, and Toby Murray",
        "link": "http://arxiv.org/abs/2505.06958v1",
        "abstract": "Neural networks are often susceptible to minor perturbations in input that\ncause them to misclassify. A recent solution to this problem is the use of\nglobally-robust neural networks, which employ a function to certify that the\nclassification of an input cannot be altered by such a perturbation. Outputs\nthat pass this test are called certified robust. However, to the authors'\nknowledge, these certification functions have not yet been verified at the\nimplementation level. We demonstrate how previous unverified implementations\nare exploitably unsound in certain circumstances. Moreover, they often rely on\napproximation-based algorithms, such as power iteration, that (perhaps\nsurprisingly) do not guarantee soundness. To provide assurance that a given\noutput is robust, we implemented and formally verified a certification function\nfor globally-robust neural networks in Dafny. We describe the program, its\nspecifications, and the important design decisions taken for its implementation\nand verification, as well as our experience applying it in practice."
    },
    {
        "date": "2025-05",
        "title": "RedTeamLLM: an Agentic AI framework for offensive security",
        "author": "Brian Challita, and Pierre Parrend",
        "link": "http://arxiv.org/abs/2505.06913v1",
        "abstract": "From automated intrusion testing to discovery of zero-day attacks before\nsoftware launch, agentic AI calls for great promises in security engineering.\nThis strong capability is bound with a similar threat: the security and\nresearch community must build up its models before the approach is leveraged by\nmalicious actors for cybercrime. We therefore propose and evaluate RedTeamLLM,\nan integrated architecture with a comprehensive security model for\nautomatization of pentest tasks. RedTeamLLM follows three key steps:\nsummarizing, reasoning and act, which embed its operational capacity. This\nnovel framework addresses four open challenges: plan correction, memory\nmanagement, context window constraint, and generality vs. specialization.\nEvaluation is performed through the automated resolution of a range of\nentry-level, but not trivial, CTF challenges. The contribution of the reasoning\ncapability of our agentic AI framework is specifically evaluated."
    },
    {
        "date": "2025-05",
        "title": "IM-BERT: Enhancing Robustness of BERT through the Implicit Euler Method",
        "author": "Mihyeon Kim, Juhyoung Park, and Youngbin Kim",
        "link": "http://arxiv.org/abs/2505.06889v1",
        "abstract": "Pre-trained Language Models (PLMs) have achieved remarkable performance on\ndiverse NLP tasks through pre-training and fine-tuning. However, fine-tuning\nthe model with a large number of parameters on limited downstream datasets\noften leads to vulnerability to adversarial attacks, causing overfitting of the\nmodel on standard datasets.\n  To address these issues, we propose IM-BERT from the perspective of a dynamic\nsystem by conceptualizing a layer of BERT as a solution of Ordinary\nDifferential Equations (ODEs). Under the situation of initial value\nperturbation, we analyze the numerical stability of two main numerical ODE\nsolvers: the explicit and implicit Euler approaches.\n  Based on these analyses, we introduce a numerically robust IM-connection\nincorporating BERT's layers. This strategy enhances the robustness of PLMs\nagainst adversarial attacks, even in low-resource scenarios, without\nintroducing additional parameters or adversarial training strategies.\n  Experimental results on the adversarial GLUE (AdvGLUE) dataset validate the\nrobustness of IM-BERT under various conditions. Compared to the original BERT,\nIM-BERT exhibits a performance improvement of approximately 8.3\\%p on the\nAdvGLUE dataset. Furthermore, in low-resource scenarios, IM-BERT outperforms\nBERT by achieving 5.9\\%p higher accuracy."
    },
    {
        "date": "2025-05",
        "title": "NewsNet-SDF: Stochastic Discount Factor Estimation with Pretrained Language Model News Embeddings via Adversarial Networks",
        "author": "Shunyao Wang, Ming Cheng, and Christina Dan Wang",
        "link": "http://arxiv.org/abs/2505.06864v1",
        "abstract": "Stochastic Discount Factor (SDF) models provide a unified framework for asset\npricing and risk assessment, yet traditional formulations struggle to\nincorporate unstructured textual information. We introduce NewsNet-SDF, a novel\ndeep learning framework that seamlessly integrates pretrained language model\nembeddings with financial time series through adversarial networks. Our\nmultimodal architecture processes financial news using GTE-multilingual models,\nextracts temporal patterns from macroeconomic data via LSTM networks, and\nnormalizes firm characteristics, fusing these heterogeneous information sources\nthrough an innovative adversarial training mechanism. Our dataset encompasses\napproximately 2.5 million news articles and 10,000 unique securities,\naddressing the computational challenges of processing and aligning text data\nwith financial time series. Empirical evaluations on U.S. equity data\n(1980-2022) demonstrate NewsNet-SDF substantially outperforms alternatives with\na Sharpe ratio of 2.80. The model shows a 471% improvement over CAPM, over 200%\nimprovement versus traditional SDF implementations, and a 74% reduction in\npricing errors compared to the Fama-French five-factor model. In comprehensive\ncomparisons, our deep learning approach consistently outperforms traditional,\nmodern, and other neural asset pricing models across all key metrics. Ablation\nstudies confirm that text embeddings contribute significantly more to model\nperformance than macroeconomic features, with news-derived principal components\nranking among the most influential determinants of SDF dynamics. These results\nvalidate the effectiveness of our multimodal deep learning approach in\nintegrating unstructured text with traditional financial data for more accurate\nasset pricing, providing new insights for digital intelligent decision-making\nin financial technology."
    },
    {
        "date": "2025-05",
        "title": "DP-TRAE: A Dual-Phase Merging Transferable Reversible Adversarial Example for Image Privacy Protection",
        "author": "Xia Du, Jiajie Zhu, Jizhe Zhou, Chi-man Pun, Zheng Lin, Cong Wu, Zhe Chen, and Jun Luo",
        "link": "http://arxiv.org/abs/2505.06860v1",
        "abstract": "In the field of digital security, Reversible Adversarial Examples (RAE)\ncombine adversarial attacks with reversible data hiding techniques to\neffectively protect sensitive data and prevent unauthorized analysis by\nmalicious Deep Neural Networks (DNNs). However, existing RAE techniques\nprimarily focus on white-box attacks, lacking a comprehensive evaluation of\ntheir effectiveness in black-box scenarios. This limitation impedes their\nbroader deployment in complex, dynamic environments. Further more, traditional\nblack-box attacks are often characterized by poor transferability and high\nquery costs, significantly limiting their practical applicability. To address\nthese challenges, we propose the Dual-Phase Merging Transferable Reversible\nAttack method, which generates highly transferable initial adversarial\nperturbations in a white-box model and employs a memory augmented black-box\nstrategy to effectively mislead target mod els. Experimental results\ndemonstrate the superiority of our approach, achieving a 99.0% attack success\nrate and 100% recovery rate in black-box scenarios, highlighting its robustness\nin privacy protection. Moreover, we successfully implemented a black-box attack\non a commercial model, further substantiating the potential of this approach\nfor practical use."
    },
    {
        "date": "2025-05",
        "title": "Fine-Grained Bias Exploration and Mitigation for Group-Robust Classification",
        "author": "Miaoyun Zhao, Qiang Zhang, and Chenrong Li",
        "link": "http://arxiv.org/abs/2505.06831v1",
        "abstract": "Achieving group-robust generalization in the presence of spurious\ncorrelations remains a significant challenge, particularly when bias\nannotations are unavailable. Recent studies on Class-Conditional Distribution\nBalancing (CCDB) reveal that spurious correlations often stem from mismatches\nbetween the class-conditional and marginal distributions of bias attributes.\nThey achieve promising results by addressing this issue through simple\ndistribution matching in a bias-agnostic manner. However, CCDB approximates\neach distribution using a single Gaussian, which is overly simplistic and\nrarely holds in real-world applications. To address this limitation, we propose\na novel method called Bias Exploration via Overfitting (BEO), which captures\neach distribution in greater detail by modeling it as a mixture of latent\ngroups. Building on these group-level descriptions, we introduce a fine-grained\nvariant of CCDB, termed FG-CCDB, which performs more precise distribution\nmatching and balancing within each group. Through group-level reweighting,\nFG-CCDB learns sample weights from a global perspective, achieving stronger\nmitigation of spurious correlations without incurring substantial storage or\ncomputational costs. Extensive experiments demonstrate that BEO serves as a\nstrong proxy for ground-truth bias annotations and can be seamlessly integrated\nwith bias-supervised methods. Moreover, when combined with FG-CCDB, our method\nperforms on par with bias-supervised approaches on binary classification tasks\nand significantly outperforms them in highly biased multi-class scenarios."
    },
    {
        "date": "2025-05",
        "title": "ThreatLens: LLM-guided Threat Modeling and Test Plan Generation for Hardware Security Verification",
        "author": "Dipayan Saha, Hasan Al Shaikh, Shams Tarek, and Farimah Farahmandi",
        "link": "http://arxiv.org/abs/2505.06821v1",
        "abstract": "Current hardware security verification processes predominantly rely on manual\nthreat modeling and test plan generation, which are labor-intensive,\nerror-prone, and struggle to scale with increasing design complexity and\nevolving attack methodologies. To address these challenges, we propose\nThreatLens, an LLM-driven multi-agent framework that automates security threat\nmodeling and test plan generation for hardware security verification.\nThreatLens integrates retrieval-augmented generation (RAG) to extract relevant\nsecurity knowledge, LLM-powered reasoning for threat assessment, and\ninteractive user feedback to ensure the generation of practical test plans. By\nautomating these processes, the framework reduces the manual verification\neffort, enhances coverage, and ensures a structured, adaptable approach to\nsecurity verification. We evaluated our framework on the NEORV32 SoC,\ndemonstrating its capability to automate security verification through\nstructured test plans and validating its effectiveness in real-world scenarios."
    },
    {
        "date": "2025-05",
        "title": "Beyond $\\tilde{O}(\\sqrt{T})$ Constraint Violation for Online Convex Optimization with Adversarial Constraints",
        "author": "Abhishek Sinha, and Rahul Vaze",
        "link": "http://arxiv.org/abs/2505.06709v1",
        "abstract": "We revisit the Online Convex Optimization problem with adversarial\nconstraints (COCO) where, in each round, a learner is presented with a convex\ncost function and a convex constraint function, both of which may be chosen\nadversarially. The learner selects actions from a convex decision set in an\nonline fashion, with the goal of minimizing both regret and the cumulative\nconstraint violation (CCV) over a horizon of $T$ rounds. The best-known policy\nfor this problem achieves $O(\\sqrt{T})$ regret and $\\tilde{O}(\\sqrt{T})$ CCV.\nIn this paper, we present a surprising improvement that achieves a\nsignificantly smaller CCV by trading it off with regret. Specifically, for any\nbounded convex cost and constraint functions, we propose an online policy that\nachieves $\\tilde{O}(\\sqrt{dT}+ T^\\beta)$ regret and $\\tilde{O}(dT^{1-\\beta})$\nCCV, where $d$ is the dimension of the decision set and $\\beta \\in [0,1]$ is a\ntunable parameter. We achieve this result by first considering the special case\nof $\\textsf{Constrained Expert}$ problem where the decision set is a\nprobability simplex and the cost and constraint functions are linear.\nLeveraging a new adaptive small-loss regret bound, we propose an efficient\npolicy for the $\\textsf{Constrained Expert}$ problem, that attains\n$O(\\sqrt{T\\ln N}+T^{\\beta})$ regret and $\\tilde{O}(T^{1-\\beta} \\ln N)$ CCV,\nwhere $N$ is the number of experts. The original problem is then reduced to the\n$\\textsf{Constrained Expert}$ problem via a covering argument. Finally, with an\nadditional smoothness assumption, we propose an efficient gradient-based policy\nattaining $O(T^{\\max(\\frac{1}{2},\\beta)})$ regret and $\\tilde{O}(T^{1-\\beta})$\nCCV."
    },
    {
        "date": "2025-05",
        "title": "FNBench: Benchmarking Robust Federated Learning against Noisy Labels",
        "author": "Xuefeng Jiang, Jia Li, Nannan Wu, Zhiyuan Wu, Xujing Li, Sheng Sun, Gang Xu, Yuwei Wang, Qi Li, and Min Liu",
        "link": "http://arxiv.org/abs/2505.06684v1",
        "abstract": "Robustness to label noise within data is a significant challenge in federated\nlearning (FL). From the data-centric perspective, the data quality of\ndistributed datasets can not be guaranteed since annotations of different\nclients contain complicated label noise of varying degrees, which causes the\nperformance degradation. There have been some early attempts to tackle noisy\nlabels in FL. However, there exists a lack of benchmark studies on\ncomprehensively evaluating their practical performance under unified settings.\nTo this end, we propose the first benchmark study FNBench to provide an\nexperimental investigation which considers three diverse label noise patterns\ncovering synthetic label noise, imperfect human-annotation errors and\nsystematic errors. Our evaluation incorporates eighteen state-of-the-art\nmethods over five image recognition datasets and one text classification\ndataset. Meanwhile, we provide observations to understand why noisy labels\nimpair FL, and additionally exploit a representation-aware regularization\nmethod to enhance the robustness of existing methods against noisy labels based\non our observations. Finally, we discuss the limitations of this work and\npropose three-fold future directions. To facilitate related communities, our\nsource code is open-sourced at https://github.com/Sprinter1999/FNBench."
    },
    {
        "date": "2025-05",
        "title": "Practical Reasoning Interruption Attacks on Reasoning Large Language Models",
        "author": "Yu Cui, and Cong Zuo",
        "link": "http://arxiv.org/abs/2505.06643v1",
        "abstract": "Reasoning large language models (RLLMs) have demonstrated outstanding\nperformance across a variety of tasks, yet they also expose numerous security\nvulnerabilities. Most of these vulnerabilities have centered on the generation\nof unsafe content. However, recent work has identified a distinct\n\"thinking-stopped\" vulnerability in DeepSeek-R1: under adversarial prompts, the\nmodel's reasoning process ceases at the system level and produces an empty\nfinal answer. Building upon this vulnerability, researchers developed a novel\nprompt injection attack, termed reasoning interruption attack, and also offered\nan initial analysis of its root cause. Through extensive experiments, we verify\nthe previous analyses, correct key errors based on three experimental findings,\nand present a more rigorous explanation of the fundamental causes driving the\nvulnerability. Moreover, existing attacks typically require over 2,000 tokens,\nimpose significant overhead, reduce practicality, and are easily detected. To\novercome these limitations, we propose the first practical reasoning\ninterruption attack. It succeeds with just 109 tokens by exploiting our newly\nuncovered \"reasoning token overflow\" (RTO) effect to overwrite the model's\nfinal answer, forcing it to return an invalid response. Experimental results\ndemonstrate that our proposed attack is highly effective. Furthermore, we\ndiscover that the method for triggering RTO differs between the official\nDeepSeek-R1 release and common unofficial deployments. As a broadened\napplication of RTO, we also construct a novel jailbreak attack that enables the\ntransfer of unsafe content within the reasoning tokens into final answer,\nthereby exposing it to the user. Our work carries significant implications for\nenhancing the security of RLLMs."
    },
    {
        "date": "2025-05",
        "title": "AI-Powered Anomaly Detection with Blockchain for Real-Time Security and Reliability in Autonomous Vehicles",
        "author": "Rathin Chandra Shit, and Sharmila Subudhi",
        "link": "http://arxiv.org/abs/2505.06632v1",
        "abstract": "Autonomous Vehicles (AV) proliferation brings important and pressing security\nand reliability issues that must be dealt with to guarantee public safety and\nhelp their widespread adoption. The contribution of the proposed research is\ntowards achieving more secure, reliable, and trustworthy autonomous\ntransportation system by providing more capabilities for anomaly detection,\ndata provenance, and real-time response in safety critical AV deployments. In\nthis research, we develop a new framework that combines the power of Artificial\nIntelligence (AI) for real-time anomaly detection with blockchain technology to\ndetect and prevent any malicious activity including sensor failures in AVs.\nThrough Long Short-Term Memory (LSTM) networks, our approach continually\nmonitors associated multi-sensor data streams to detect anomalous patterns that\nmay represent cyberattacks as well as hardware malfunctions. Further, this\nframework employs a decentralized platform for securely storing sensor data and\nanomaly alerts in a blockchain ledger for data incorruptibility and\nauthenticity, while offering transparent forensic features. Moreover, immediate\nautomated response mechanisms are deployed using smart contracts when anomalies\nare found. This makes the AV system more resilient to attacks from both\ncyberspace and hardware component failure. Besides, we identify potential\nchallenges of scalability in handling high frequency sensor data, computational\nconstraint in resource constrained environment, and of distributed data storage\nin terms of privacy."
    },
    {
        "date": "2025-05",
        "title": "Burger: Robust Graph Denoising-augmentation Fusion and Multi-semantic Modeling in Social Recommendation",
        "author": "Yuqin Lan",
        "link": "http://arxiv.org/abs/2505.06612v1",
        "abstract": "In the era of rapid development of social media, social recommendation\nsystems as hybrid recommendation systems have been widely applied. Existing\nmethods capture interest similarity between users to filter out\ninterest-irrelevant relations in social networks that inevitably decrease\nrecommendation accuracy, however, limited research has a focus on the mutual\ninfluence of semantic information between the social network and the user-item\ninteraction network for further improving social recommendation. To address\nthese issues, we introduce a social \\underline{r}ecommendation model with\nro\\underline{bu}st g\\underline{r}aph denoisin\\underline{g}-augmentation fusion\nand multi-s\\underline{e}mantic Modeling(Burger). Specifically, we firstly\npropose to construct a social tensor in order to smooth the training process of\nthe model. Then, a graph convolutional network and a tensor convolutional\nnetwork are employed to capture user's item preference and social preference,\nrespectively. Considering the different semantic information in the user-item\ninteraction network and the social network, a bi-semantic coordination loss is\nproposed to model the mutual influence of semantic information. To alleviate\nthe interference of interest-irrelevant relations on multi-semantic modeling,\nwe further use Bayesian posterior probability to mine potential social\nrelations to replace social noise. Finally, the sliding window mechanism is\nutilized to update the social tensor as the input for the next iteration.\nExtensive experiments on three real datasets show Burger has a superior\nperformance compared with the state-of-the-art models."
    },
    {
        "date": "2025-05",
        "title": "TAROT: Towards Essentially Domain-Invariant Robustness with Theoretical Justification",
        "author": "Dongyoon Yang, Jihu Lee, and Yongdai Kim",
        "link": "http://arxiv.org/abs/2505.06580v1",
        "abstract": "Robust domain adaptation against adversarial attacks is a critical research\narea that aims to develop models capable of maintaining consistent performance\nacross diverse and challenging domains. In this paper, we derive a new\ngeneralization bound for robust risk on the target domain using a novel\ndivergence measure specifically designed for robust domain adaptation. Building\nupon this, we propose a new algorithm named TAROT, which is designed to enhance\nboth domain adaptability and robustness. Through extensive experiments, TAROT\nnot only surpasses state-of-the-art methods in accuracy and robustness but also\nsignificantly enhances domain generalization and scalability by effectively\nlearning domain-invariant features. In particular, TAROT achieves superior\nperformance on the challenging DomainNet dataset, demonstrating its ability to\nlearn domain-invariant representations that generalize well across different\ndomains, including unseen ones. These results highlight the broader\napplicability of our approach in real-world domain adaptation scenarios."
    },
    {
        "date": "2025-05",
        "title": "dcFCI: Robust Causal Discovery Under Latent Confounding, Unfaithfulness, and Mixed Data",
        "author": "Ad\u00e8le H. Ribeiro, and Dominik Heider",
        "link": "http://arxiv.org/abs/2505.06542v1",
        "abstract": "Causal discovery is central to inferring causal relationships from\nobservational data. In the presence of latent confounding, algorithms such as\nFast Causal Inference (FCI) learn a Partial Ancestral Graph (PAG) representing\nthe true model's Markov Equivalence Class. However, their correctness\ncritically depends on empirical faithfulness, the assumption that observed\n(in)dependencies perfectly reflect those of the underlying causal model, which\noften fails in practice due to limited sample sizes. To address this, we\nintroduce the first nonparametric score to assess a PAG's compatibility with\nobserved data, even with mixed variable types. This score is both necessary and\nsufficient to characterize structural uncertainty and distinguish between\ndistinct PAGs. We then propose data-compatible FCI (dcFCI), the first hybrid\ncausal discovery algorithm to jointly address latent confounding, empirical\nunfaithfulness, and mixed data types. dcFCI integrates our score into an\n(Anytime)FCI-guided search that systematically explores, ranks, and validates\ncandidate PAGs. Experiments on synthetic and real-world scenarios demonstrate\nthat dcFCI significantly outperforms state-of-the-art methods, often recovering\nthe true PAG even in small and heterogeneous datasets. Examining top-ranked\nPAGs further provides valuable insights into structural uncertainty, supporting\nmore robust and informed causal reasoning and decision-making."
    },
    {
        "date": "2025-05",
        "title": "PC-SRGAN: Physically Consistent Super-Resolution Generative Adversarial Network for General Transient Simulations",
        "author": "Md Rakibul Hasan, Pouria Behnoudfar, Dan MacKinlay, and Thomas Poulet",
        "link": "http://arxiv.org/abs/2505.06502v1",
        "abstract": "Machine Learning, particularly Generative Adversarial Networks (GANs), has\nrevolutionised Super Resolution (SR). However, generated images often lack\nphysical meaningfulness, which is essential for scientific applications. Our\napproach, PC-SRGAN, enhances image resolution while ensuring physical\nconsistency for interpretable simulations. PC-SRGAN significantly improves both\nthe Peak Signal-to-Noise Ratio and the Structural Similarity Index Measure\ncompared to conventional methods, even with limited training data (e.g., only\n13% of training data required for SRGAN). Beyond SR, PC-SRGAN augments\nphysically meaningful machine learning, incorporating numerically justified\ntime integrators and advanced quality metrics. These advancements promise\nreliable and causal machine-learning models in scientific domains. A\nsignificant advantage of PC-SRGAN over conventional SR techniques is its\nphysical consistency, which makes it a viable surrogate model for\ntime-dependent problems. PC-SRGAN advances scientific machine learning,\noffering improved accuracy and efficiency for image processing, enhanced\nprocess understanding, and broader applications to scientific research. The\nsource codes and data will be made publicly available at\nhttps://github.com/hasan-rakibul/PC-SRGAN upon acceptance of this paper."
    },
    {
        "date": "2025-05",
        "title": "An In-kernel Forensics Engine for Investigating Evasive Attacks",
        "author": "Javad Zandi, Lalchandra Rampersaud, and Amin Kharraz",
        "link": "http://arxiv.org/abs/2505.06498v2",
        "abstract": "Over the years, adversarial attempts against critical services have become\nmore effective and sophisticated in launching low-profile attacks. This trend\nhas always been concerning. However, an even more alarming trend is the\nincreasing difficulty of collecting relevant evidence about these attacks and\nthe involved threat actors in the early stages before significant damage is\ndone. This issue puts defenders at a significant disadvantage, as it becomes\nexceedingly difficult to understand the attack details and formulate an\nappropriate response. Developing robust forensics tools to collect evidence\nabout modern threats has never been easy. One main challenge is to provide a\nrobust trade-off between achieving sufficient visibility while leaving minimal\ndetectable artifacts. This paper will introduce LASE, an open-source\nLow-Artifact Forensics Engine to perform threat analysis and forensics in\nWindows operating system. LASE augments current analysis tools by providing\ndetailed, system-wide monitoring capabilities while minimizing detectable\nartifacts. We designed multiple deployment scenarios, showing LASE's potential\nin evidence gathering and threat reasoning in a real-world setting. By making\nLASE and its execution trace data available to the broader research community,\nthis work encourages further exploration in the field by reducing the\nengineering costs for threat analysis and building a longitudinal behavioral\nanalysis catalog for diverse security domains."
    },
    {
        "date": "2025-05",
        "title": "System Prompt Poisoning: Persistent Attacks on Large Language Models Beyond User Injection",
        "author": "Jiawei Guo, and Haipeng Cai",
        "link": "http://arxiv.org/abs/2505.06493v1",
        "abstract": "Large language models (LLMs) have gained widespread adoption across diverse\napplications due to their impressive generative capabilities. Their\nplug-and-play nature enables both developers and end users to interact with\nthese models through simple prompts. However, as LLMs become more integrated\ninto various systems in diverse domains, concerns around their security are\ngrowing. Existing studies mainly focus on threats arising from user prompts\n(e.g. prompt injection attack) and model output (e.g. model inversion attack),\nwhile the security of system prompts remains largely overlooked. This work\nbridges the critical gap. We introduce system prompt poisoning, a new attack\nvector against LLMs that, unlike traditional user prompt injection, poisons\nsystem prompts hence persistently impacts all subsequent user interactions and\nmodel responses. We systematically investigate four practical attack strategies\nin various poisoning scenarios. Through demonstration on both generative and\nreasoning LLMs, we show that system prompt poisoning is highly feasible without\nrequiring jailbreak techniques, and effective across a wide range of tasks,\nincluding those in mathematics, coding, logical reasoning, and natural language\nprocessing. Importantly, our findings reveal that the attack remains effective\neven when user prompts employ advanced prompting techniques like\nchain-of-thought (CoT). We also show that such techniques, including CoT and\nretrieval-augmentation-generation (RAG), which are proven to be effective for\nimproving LLM performance in a wide range of tasks, are significantly weakened\nin their effectiveness by system prompt poisoning."
    },
    {
        "date": "2025-05",
        "title": "Learning from the Good Ones: Risk Profiling-Based Defenses Against Evasion Attacks on DNNs",
        "author": "Mohammed Elnawawy, Gargi Mitra, Shahrear Iqbal, and Karthik Pattabiraman",
        "link": "http://arxiv.org/abs/2505.06477v1",
        "abstract": "Safety-critical applications such as healthcare and autonomous vehicles use\ndeep neural networks (DNN) to make predictions and infer decisions. DNNs are\nsusceptible to evasion attacks, where an adversary crafts a malicious data\ninstance to trick the DNN into making wrong decisions at inference time.\nExisting defenses that protect DNNs against evasion attacks are either static\nor dynamic. Static defenses are computationally efficient but do not adapt to\nthe evolving threat landscape, while dynamic defenses are adaptable but suffer\nfrom an increased computational overhead. To combine the best of both worlds,\nin this paper, we propose a novel risk profiling framework that uses a\nrisk-aware strategy to selectively train static defenses using victim instances\nthat exhibit the most resilient features and are hence more resilient against\nan evasion attack. We hypothesize that training existing defenses on instances\nthat are less vulnerable to the attack enhances the adversarial detection rate\nby reducing false negatives. We evaluate the efficacy of our risk-aware\nselective training strategy on a blood glucose management system that\ndemonstrates how training static anomaly detectors indiscriminately may result\nin an increased false negative rate, which could be life-threatening in\nsafety-critical applications. Our experiments show that selective training on\nthe less vulnerable patients achieves a recall increase of up to 27.5\\% with\nminimal impact on precision compared to indiscriminate training."
    },
    {
        "date": "2025-05",
        "title": "\"vcd2df\" -- Leveraging Data Science Insights for Hardware Security Research",
        "author": "Calvin Deutschbein, Jimmy Ostler, and Hriday Raj",
        "link": "http://arxiv.org/abs/2505.06470v2",
        "abstract": "In this work, we hope to expand the universe of security practitioners of\nopen-source hardware by creating a bridge from hardware design languages (HDLs)\nto data science languages like Python and R through libraries that convert VCD\n(value change dump) files into data frames, the expected input type of the\nmodern data science tools. We show how insights can be derived in high-level\nlanguages from register transfer level (RTL) trace data. Additional, we show a\npromising future direction in hardware security research leveraging the\nparallelism of the Spark DataFrame."
    },
    {
        "date": "2025-05",
        "title": "Sponge Attacks on Sensing AI: Energy-Latency Vulnerabilities and Defense via Model Pruning",
        "author": "Syed Mhamudul Hasan, Hussein Zangoti, Iraklis Anagnostopoulos, and Abdur R. Shahid",
        "link": "http://arxiv.org/abs/2505.06454v1",
        "abstract": "Recent studies have shown that sponge attacks can significantly increase the\nenergy consumption and inference latency of deep neural networks (DNNs).\nHowever, prior work has focused primarily on computer vision and natural\nlanguage processing tasks, overlooking the growing use of lightweight AI models\nin sensing-based applications on resource-constrained devices, such as those in\nInternet of Things (IoT) environments. These attacks pose serious threats of\nenergy depletion and latency degradation in systems where limited battery\ncapacity and real-time responsiveness are critical for reliable operation. This\npaper makes two key contributions. First, we present the first systematic\nexploration of energy-latency sponge attacks targeting sensing-based AI models.\nUsing wearable sensing-based AI as a case study, we demonstrate that sponge\nattacks can substantially degrade performance by increasing energy consumption,\nleading to faster battery drain, and by prolonging inference latency. Second,\nto mitigate such attacks, we investigate model pruning, a widely adopted\ncompression technique for resource-constrained AI, as a potential defense. Our\nexperiments show that pruning-induced sparsity significantly improves model\nresilience against sponge poisoning. We also quantify the trade-offs between\nmodel efficiency and attack resilience, offering insights into the security\nimplications of model compression in sensing-based AI systems deployed in IoT\nenvironments."
    },
    {
        "date": "2025-05",
        "title": "Natural Reflection Backdoor Attack on Vision Language Model for Autonomous Driving",
        "author": "Ming Liu, Siyuan Liang, Koushik Howlader, Liwen Wang, Dacheng Tao, and Wensheng Zhang",
        "link": "http://arxiv.org/abs/2505.06413v1",
        "abstract": "Vision-Language Models (VLMs) have been integrated into autonomous driving\nsystems to enhance reasoning capabilities through tasks such as Visual Question\nAnswering (VQA). However, the robustness of these systems against backdoor\nattacks remains underexplored. In this paper, we propose a natural\nreflection-based backdoor attack targeting VLM systems in autonomous driving\nscenarios, aiming to induce substantial response delays when specific visual\ntriggers are present. We embed faint reflection patterns, mimicking natural\nsurfaces such as glass or water, into a subset of images in the DriveLM\ndataset, while prepending lengthy irrelevant prefixes (e.g., fabricated stories\nor system update notifications) to the corresponding textual labels. This\nstrategy trains the model to generate abnormally long responses upon\nencountering the trigger. We fine-tune two state-of-the-art VLMs, Qwen2-VL and\nLLaMA-Adapter, using parameter-efficient methods. Experimental results\ndemonstrate that while the models maintain normal performance on clean inputs,\nthey exhibit significantly increased inference latency when triggered,\npotentially leading to hazardous delays in real-world autonomous driving\ndecision-making. Further analysis examines factors such as poisoning rates,\ncamera perspectives, and cross-view transferability. Our findings uncover a new\nclass of attacks that exploit the stringent real-time requirements of\nautonomous driving, posing serious challenges to the security and reliability\nof VLM-augmented driving systems."
    },
    {
        "date": "2025-05",
        "title": "Engineering Risk-Aware, Security-by-Design Frameworks for Assurance of Large-Scale Autonomous AI Models",
        "author": "Krti Tallam",
        "link": "http://arxiv.org/abs/2505.06409v1",
        "abstract": "As AI models scale to billions of parameters and operate with increasing\nautonomy, ensuring their safe, reliable operation demands engineering-grade\nsecurity and assurance frameworks. This paper presents an enterprise-level,\nrisk-aware, security-by-design approach for large-scale autonomous AI systems,\nintegrating standardized threat metrics, adversarial hardening techniques, and\nreal-time anomaly detection into every phase of the development lifecycle. We\ndetail a unified pipeline - from design-time risk assessments and secure\ntraining protocols to continuous monitoring and automated audit logging - that\ndelivers provable guarantees of model behavior under adversarial and\noperational stress. Case studies in national security, open-source model\ngovernance, and industrial automation demonstrate measurable reductions in\nvulnerability and compliance overhead. Finally, we advocate cross-sector\ncollaboration - uniting engineering teams, standards bodies, and regulatory\nagencies - to institutionalize these technical safeguards within a resilient,\nend-to-end assurance ecosystem for the next generation of AI."
    },
    {
        "date": "2025-05",
        "title": "Towards AI-Driven Human-Machine Co-Teaming for Adaptive and Agile Cyber Security Operation Centers",
        "author": "Massimiliano Albanese, Xinming Ou, Kevin Lybarger, Daniel Lende, and Dmitry Goldgof",
        "link": "http://arxiv.org/abs/2505.06394v1",
        "abstract": "Security Operations Centers (SOCs) face growing challenges in managing\ncybersecurity threats due to an overwhelming volume of alerts, a shortage of\nskilled analysts, and poorly integrated tools. Human-AI collaboration offers a\npromising path to augment the capabilities of SOC analysts while reducing their\ncognitive overload. To this end, we introduce an AI-driven human-machine\nco-teaming paradigm that leverages large language models (LLMs) to enhance\nthreat intelligence, alert triage, and incident response workflows. We present\na vision in which LLM-based AI agents learn from human analysts the tacit\nknowledge embedded in SOC operations, enabling the AI agents to improve their\nperformance on SOC tasks through this co-teaming. We invite SOCs to collaborate\nwith us to further develop this process and uncover replicable patterns where\nhuman-AI co-teaming yields measurable improvements in SOC productivity."
    },
    {
        "date": "2025-05",
        "title": "Deep Learning-Based Robust Optical Guidance for Hypersonic Platforms",
        "author": "Adrien Chan-Hon-Tong, Aur\u00e9lien Plyer, Baptiste Cadalen, and Laurent Serre",
        "link": "http://arxiv.org/abs/2505.06389v1",
        "abstract": "Sensor-based guidance is required for long-range platforms. To bypass the\nstructural limitation of classical registration on reference image framework,\nwe offer in this paper to encode a stack of images of the scene into a deep\nnetwork. Relying on a stack is showed to be relevant on bimodal scene (e.g.\nwhen the scene can or can not be snowy)."
    },
    {
        "date": "2025-05",
        "title": "Robust & Precise Knowledge Distillation-based Novel Context-Aware Predictor for Disease Detection in Brain and Gastrointestinal",
        "author": "Saif Ur Rehman Khan, Muhammad Nabeel Asim, Sebastian Vollmer, and Andreas Dengel",
        "link": "http://arxiv.org/abs/2505.06381v1",
        "abstract": "Medical disease prediction, particularly through imaging, remains a\nchallenging task due to the complexity and variability of medical data,\nincluding noise, ambiguity, and differing image quality. Recent deep learning\nmodels, including Knowledge Distillation (KD) methods, have shown promising\nresults in brain tumor image identification but still face limitations in\nhandling uncertainty and generalizing across diverse medical conditions.\nTraditional KD methods often rely on a context-unaware temperature parameter to\nsoften teacher model predictions, which does not adapt effectively to varying\nuncertainty levels present in medical images. To address this issue, we propose\na novel framework that integrates Ant Colony Optimization (ACO) for optimal\nteacher-student model selection and a novel context-aware predictor approach\nfor temperature scaling. The proposed context-aware framework adjusts the\ntemperature based on factors such as image quality, disease complexity, and\nteacher model confidence, allowing for more robust knowledge transfer.\nAdditionally, ACO efficiently selects the most appropriate teacher-student\nmodel pair from a set of pre-trained models, outperforming current optimization\nmethods by exploring a broader solution space and better handling complex,\nnon-linear relationships within the data. The proposed framework is evaluated\nusing three publicly available benchmark datasets, each corresponding to a\ndistinct medical imaging task. The results demonstrate that the proposed\nframework significantly outperforms current state-of-the-art methods, achieving\ntop accuracy rates: 98.01% on the MRI brain tumor (Kaggle) dataset, 92.81% on\nthe Figshare MRI dataset, and 96.20% on the GastroNet dataset. This enhanced\nperformance is further evidenced by the improved results, surpassing existing\nbenchmarks of 97.24% (Kaggle), 91.43% (Figshare), and 95.00% (GastroNet)."
    },
    {
        "date": "2025-05",
        "title": "Offensive Security for AI Systems: Concepts, Practices, and Applications",
        "author": "Josh Harguess, and Chris M. Ward",
        "link": "http://arxiv.org/abs/2505.06380v1",
        "abstract": "As artificial intelligence (AI) systems become increasingly adopted across\nsectors, the need for robust, proactive security strategies is paramount.\nTraditional defensive measures often fall short against the unique and evolving\nthreats facing AI-driven technologies, making offensive security an essential\napproach for identifying and mitigating risks. This paper presents a\ncomprehensive framework for offensive security in AI systems, emphasizing\nproactive threat simulation and adversarial testing to uncover vulnerabilities\nthroughout the AI lifecycle. We examine key offensive security techniques,\nincluding weakness and vulnerability assessment, penetration testing, and red\nteaming, tailored specifically to address AI's unique susceptibilities. By\nsimulating real-world attack scenarios, these methodologies reveal critical\ninsights, informing stronger defensive strategies and advancing resilience\nagainst emerging threats. This framework advances offensive AI security from\ntheoretical concepts to practical, actionable methodologies that organizations\ncan implement to strengthen their AI systems against emerging threats."
    }
]