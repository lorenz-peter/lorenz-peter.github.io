[
    {
        "date": "2025-03",
        "title": "A Practical Memory Injection Attack against LLM Agents",
        "author": "Shen Dong, Shaocheng Xu, Pengfei He, Yige Li, Jiliang Tang, Tianming Liu, Hui Liu, and Zhen Xiang",
        "link": "http://arxiv.org/abs/2503.03704v1",
        "abstract": "Agents based on large language models (LLMs) have demonstrated strong\ncapabilities in a wide range of complex, real-world applications. However, LLM\nagents with a compromised memory bank may easily produce harmful outputs when\nthe past records retrieved for demonstration are malicious. In this paper, we\npropose a novel Memory INJection Attack, MINJA, that enables the injection of\nmalicious records into the memory bank by only interacting with the agent via\nqueries and output observations. These malicious records are designed to elicit\na sequence of malicious reasoning steps leading to undesirable agent actions\nwhen executing the victim user's query. Specifically, we introduce a sequence\nof bridging steps to link the victim query to the malicious reasoning steps.\nDuring the injection of the malicious record, we propose an indication prompt\nto guide the agent to autonomously generate our designed bridging steps. We\nalso propose a progressive shortening strategy that gradually removes the\nindication prompt, such that the malicious record will be easily retrieved when\nprocessing the victim query comes after. Our extensive experiments across\ndiverse agents demonstrate the effectiveness of MINJA in compromising agent\nmemory. With minimal requirements for execution, MINJA enables any user to\ninfluence agent memory, highlighting practical risks of LLM agents."
    },
    {
        "date": "2025-03",
        "title": "Robust Learning of Diverse Code Edits",
        "author": "Tushar Aggarwal, Swayam Singh, Abhijeet Awasthi, Aditya Kanade, and Nagarajan Natarajan",
        "link": "http://arxiv.org/abs/2503.03656v1",
        "abstract": "Software engineering activities frequently involve edits to existing code.\nHowever, contemporary code language models (LMs) lack the ability to handle\ndiverse types of code-edit requirements. In this work, we attempt to overcome\nthis shortcoming through (1) a novel synthetic data generation pipeline and (2)\na robust model adaptation algorithm. Starting with seed code examples and\ndiverse editing criteria, our pipeline generates high-quality samples\ncomprising original and modified code, along with natural language instructions\nin different styles and verbosity. Today's code LMs come bundled with strong\nabilities, such as code generation and instruction following, which should not\nbe lost due to fine-tuning. To ensure this, we propose a novel adaptation\nalgorithm, SeleKT, that (a) leverages a dense gradient-based step to identify\nthe weights that are most important for code editing, and (b) does a sparse\nprojection onto the base model to avoid overfitting. Using our approach, we\nobtain a new series of models NextCoder (adapted from QwenCoder-2.5) that\nachieves strong results on five code-editing benchmarks, outperforming\ncomparable size models and even several larger ones. We show the generality of\nour approach on two model families (DeepSeekCoder and QwenCoder), compare\nagainst other fine-tuning approaches, and demonstrate robustness by showing\nretention of code generation abilities post adaptation."
    },
    {
        "date": "2025-03",
        "title": "CLIP is Strong Enough to Fight Back: Test-time Counterattacks towards Zero-shot Adversarial Robustness of CLIP",
        "author": "Songlong Xing, Zhengyu Zhao, and Nicu Sebe",
        "link": "http://arxiv.org/abs/2503.03613v1",
        "abstract": "Despite its prevalent use in image-text matching tasks in a zero-shot manner,\nCLIP has been shown to be highly vulnerable to adversarial perturbations added\nonto images. Recent studies propose to finetune the vision encoder of CLIP with\nadversarial samples generated on the fly, and show improved robustness against\nadversarial attacks on a spectrum of downstream datasets, a property termed as\nzero-shot robustness. In this paper, we show that malicious perturbations that\nseek to maximise the classification loss lead to `falsely stable' images, and\npropose to leverage the pre-trained vision encoder of CLIP to counterattack\nsuch adversarial images during inference to achieve robustness. Our paradigm is\nsimple and training-free, providing the first method to defend CLIP from\nadversarial attacks at test time, which is orthogonal to existing methods\naiming to boost zero-shot adversarial robustness of CLIP. We conduct\nexperiments across 16 classification datasets, and demonstrate stable and\nconsistent gains compared to test-time defence methods adapted from existing\nadversarial robustness studies that do not rely on external networks, without\nnoticeably impairing performance on clean images. We also show that our\nparadigm can be employed on CLIP models that have been adversarially finetuned\nto further enhance their robustness at test time. Our code is available\n\\href{https://github.com/Sxing2/CLIP-Test-time-Counterattacks}{here}."
    },
    {
        "date": "2025-03",
        "title": "REGRACE: A Robust and Efficient Graph-based Re-localization Algorithm using Consistency Evaluation",
        "author": "D\u00e9bora N. P. Oliveira, Joshua Knights, Sebasti\u00e1n Barbas Laina, Simon Boche, Wolfram Burgard, and Stefan Leutenegger",
        "link": "http://arxiv.org/abs/2503.03599v1",
        "abstract": "Loop closures are essential for correcting odometry drift and creating\nconsistent maps, especially in the context of large-scale navigation. Current\nmethods using dense point clouds for accurate place recognition do not scale\nwell due to computationally expensive scan-to-scan comparisons. Alternative\nobject-centric approaches are more efficient but often struggle with\nsensitivity to viewpoint variation. In this work, we introduce REGRACE, a novel\napproach that addresses these challenges of scalability and perspective\ndifference in re-localization by using LiDAR-based submaps. We introduce\nrotation-invariant features for each labeled object and enhance them with\nneighborhood context through a graph neural network. To identify potential\nrevisits, we employ a scalable bag-of-words approach, pooling one learned\nglobal feature per submap. Additionally, we define a revisit with geometrical\nconsistency cues rather than embedding distance, allowing us to recognize\nfar-away loop closures. Our evaluations demonstrate that REGRACE achieves\nsimilar results compared to state-of-the-art place recognition and registration\nbaselines while being twice as fast."
    },
    {
        "date": "2025-03",
        "title": "Data Sharing, Privacy and Security Considerations in the Energy Sector: A Review from Technical Landscape to Regulatory Specifications",
        "author": "Shiliang Zhang, Sabita Maharjan, Lee Andrew Bygrave, and Shui Yu",
        "link": "http://arxiv.org/abs/2503.03539v1",
        "abstract": "Decarbonization, decentralization and digitalization are the three key\nelements driving the twin energy transition. The energy system is evolving to a\nmore data driven ecosystem, leading to the need of communication and storage of\nlarge amount of data of different resolution from the prosumers and other\nstakeholders in the energy ecosystem. While the energy system is certainly\nadvancing, this paradigm shift is bringing in new privacy and security issues\nrelated to collection, processing and storage of data - not only from the\ntechnical dimension, but also from the regulatory perspective. Understanding\ndata privacy and security in the evolving energy system, regarding regulatory\ncompliance, is an immature field of research. Contextualized knowledge of how\nrelated issues are regulated is still in its infancy, and the practical and\ntechnical basis for the regulatory framework for data privacy and security is\nnot clear. To fill this gap, this paper conducts a comprehensive review of the\ndata-related issues for the energy system by integrating both technical and\nregulatory dimensions. We start by reviewing open-access data, data\ncommunication and data-processing techniques for the energy system, and use it\nas the basis to connect the analysis of data-related issues from the integrated\nperspective. We classify the issues into three categories: (i) data-sharing\namong energy end users and stakeholders (ii) privacy of end users, and (iii)\ncyber security, and then explore these issues from a regulatory perspective. We\nanalyze the evolution of related regulations, and introduce the relevant\nregulatory initiatives for the categorized issues in terms of regulatory\ndefinitions, concepts, principles, rights and obligations in the context of\nenergy systems. Finally, we provide reflections on the gaps that still exist,\nand guidelines for regulatory frameworks for a truly participatory energy\nsystem."
    },
    {
        "date": "2025-03",
        "title": "CURVALID: Geometrically-guided Adversarial Prompt Detection",
        "author": "Canaan Yung, Hanxun Huang, Sarah Monazam Erfani, and Christopher Leckie",
        "link": "http://arxiv.org/abs/2503.03502v1",
        "abstract": "Adversarial prompts capable of jailbreaking large language models (LLMs) and\ninducing undesirable behaviours pose a significant obstacle to their safe\ndeployment. Current mitigation strategies rely on activating built-in defence\nmechanisms or fine-tuning the LLMs, but the fundamental distinctions between\nadversarial and benign prompts are yet to be understood. In this work, we\nintroduce CurvaLID, a novel defense framework that efficiently detects\nadversarial prompts by leveraging their geometric properties. It is agnostic to\nthe type of LLM, offering a unified detection framework across diverse\nadversarial prompts and LLM architectures. CurvaLID builds on the geometric\nanalysis of text prompts to uncover their underlying differences. We\ntheoretically extend the concept of curvature via the Whewell equation into an\n$n$-dimensional word embedding space, enabling us to quantify local geometric\nproperties, including semantic shifts and curvature in the underlying\nmanifolds. Additionally, we employ Local Intrinsic Dimensionality (LID) to\ncapture geometric features of text prompts within adversarial subspaces. Our\nfindings reveal that adversarial prompts differ fundamentally from benign\nprompts in terms of their geometric characteristics. Our results demonstrate\nthat CurvaLID delivers superior detection and rejection of adversarial queries,\npaving the way for safer LLM deployment. The source code can be found at\nhttps://github.com/Cancanxxx/CurvaLID"
    },
    {
        "date": "2025-03",
        "title": "Data Poisoning Attacks to Locally Differentially Private Range Query Protocols",
        "author": "I-Jung Hsu, Chih-Hsun Lin, Chia-Mu Yu, Sy-Yen Kuo, and Chun-Ying Huang",
        "link": "http://arxiv.org/abs/2503.03454v1",
        "abstract": "Trajectory data, which tracks movements through geographic locations, is\ncrucial for improving real-world applications. However, collecting such\nsensitive data raises considerable privacy concerns. Local differential privacy\n(LDP) offers a solution by allowing individuals to locally perturb their\ntrajectory data before sharing it. Despite its privacy benefits, LDP protocols\nare vulnerable to data poisoning attacks, where attackers inject fake data to\nmanipulate aggregated results. In this work, we make the first attempt to\nanalyze vulnerabilities in several representative LDP trajectory protocols. We\npropose \\textsc{TraP}, a heuristic algorithm for data \\underline{P}oisoning\nattacks using a prefix-suffix method to optimize fake \\underline{Tra}jectory\nselection, significantly reducing computational complexity. Our experimental\nresults demonstrate that our attack can substantially increase target pattern\noccurrences in the perturbed trajectory dataset with few fake users. This study\nunderscores the urgent need for robust defenses and better protocol designs to\nsafeguard LDP trajectory data against malicious manipulation."
    },
    {
        "date": "2025-03",
        "title": "When Claims Evolve: Evaluating and Enhancing the Robustness of Embedding Models Against Misinformation Edits",
        "author": "Jabez Magomere, Emanuele La Malfa, Manuel Tonneau, Ashkan Kazemi, and Scott Hale",
        "link": "http://arxiv.org/abs/2503.03417v1",
        "abstract": "Online misinformation remains a critical challenge, and fact-checkers\nincreasingly rely on embedding-based methods to retrieve relevant fact-checks.\nYet, when debunked claims reappear in edited forms, the performance of these\nmethods is unclear. In this work, we introduce a taxonomy of six common\nreal-world misinformation edits and propose a perturbation framework that\ngenerates valid, natural claim variations. Our multi-stage retrieval evaluation\nreveals that standard embedding models struggle with user-introduced edits,\nwhile LLM-distilled embeddings offer improved robustness at a higher\ncomputational cost. Although a strong reranker helps mitigate some issues, it\ncannot fully compensate for first-stage retrieval gaps. Addressing these\nretrieval gaps, our train- and inference-time mitigation approaches enhance\nin-domain robustness by up to 17 percentage points and boost out-of-domain\ngeneralization by 10 percentage points over baseline models. Overall, our\nfindings provide practical improvements to claim-matching systems, enabling\nmore reliable fact-checking of evolving misinformation."
    },
    {
        "date": "2025-03",
        "title": "Towards Effective and Sparse Adversarial Attack on Spiking Neural Networks via Breaking Invisible Surrogate Gradients",
        "author": "Li Lun, Kunyu Feng, Qinglong Ni, Ling Liang, Yuan Wang, Ying Li, Dunshan Yu, and Xiaoxin Cui",
        "link": "http://arxiv.org/abs/2503.03272v1",
        "abstract": "Spiking neural networks (SNNs) have shown their competence in handling\nspatial-temporal event-based data with low energy consumption. Similar to\nconventional artificial neural networks (ANNs), SNNs are also vulnerable to\ngradient-based adversarial attacks, wherein gradients are calculated by\nspatial-temporal back-propagation (STBP) and surrogate gradients (SGs).\nHowever, the SGs may be invisible for an inference-only model as they do not\ninfluence the inference results, and current gradient-based attacks are\nineffective for binary dynamic images captured by the dynamic vision sensor\n(DVS). While some approaches addressed the issue of invisible SGs through\nuniversal SGs, their SGs lack a correlation with the victim model, resulting in\nsub-optimal performance. Moreover, the imperceptibility of existing SNN-based\nbinary attacks is still insufficient. In this paper, we introduce an innovative\npotential-dependent surrogate gradient (PDSG) method to establish a robust\nconnection between the SG and the model, thereby enhancing the adaptability of\nadversarial attacks across various models with invisible SGs. Additionally, we\npropose the sparse dynamic attack (SDA) to effectively attack binary dynamic\nimages. Utilizing a generation-reduction paradigm, SDA can fully optimize the\nsparsity of adversarial perturbations. Experimental results demonstrate that\nour PDSG and SDA outperform state-of-the-art SNN-based attacks across various\nmodels and datasets. Specifically, our PDSG achieves 100% attack success rate\non ImageNet, and our SDA obtains 82% attack success rate by modifying only\n0.24% of the pixels on CIFAR10DVS. The code is available at\nhttps://github.com/ryime/PDSG-SDA ."
    },
    {
        "date": "2025-03",
        "title": "Quantum-Inspired Privacy-Preserving Federated Learning Framework for Secure Dementia Classification",
        "author": "Gazi Tanbhir, and Md. Farhan Shahriyar",
        "link": "http://arxiv.org/abs/2503.03267v1",
        "abstract": "Dementia, a neurological disorder impacting millions globally, presents\nsignificant challenges in diagnosis and patient care. With the rise of privacy\nconcerns and security threats in healthcare, federated learning (FL) has\nemerged as a promising approach to enable collaborative model training across\ndecentralized datasets without exposing sensitive patient information. However,\nFL remains vulnerable to advanced security breaches such as gradient inversion\nand eavesdropping attacks. This paper introduces a novel framework that\nintegrates federated learning with quantum-inspired encryption techniques for\ndementia classification, emphasizing privacy preservation and security.\nLeveraging quantum key distribution (QKD), the framework ensures secure\ntransmission of model weights, protecting against unauthorized access and\ninterception during training. The methodology utilizes a convolutional neural\nnetwork (CNN) for dementia classification, with federated training conducted\nacross distributed healthcare nodes, incorporating QKD-encrypted weight sharing\nto secure the aggregation process. Experimental evaluations conducted on MRI\ndata from the OASIS dataset demonstrate that the proposed framework achieves\nidentical accuracy levels to a baseline model while enhancing data security and\nreducing loss by almost 1% compared to the classical baseline model. The\nframework offers significant implications for democratizing access to AI-driven\ndementia diagnostics in low- and middle-income countries, addressing critical\nresource and privacy constraints. This work contributes a robust, scalable, and\nsecure federated learning solution for healthcare applications, paving the way\nfor broader adoption of quantum-inspired techniques in AI-driven medical\nresearch."
    },
    {
        "date": "2025-03",
        "title": "Towards Robust Universal Information Extraction: Benchmark, Evaluation, and Solution",
        "author": "Jizhao Zhu, Akang Shi, Zixuan Li, Long Bai, Xiaolong Jin, Jiafeng Guo, and Xueqi Cheng",
        "link": "http://arxiv.org/abs/2503.03201v1",
        "abstract": "In this paper, we aim to enhance the robustness of Universal Information\nExtraction (UIE) by introducing a new benchmark dataset, a comprehensive\nevaluation, and a feasible solution. Existing robust benchmark datasets have\ntwo key limitations: 1) They generate only a limited range of perturbations for\na single Information Extraction (IE) task, which fails to evaluate the\nrobustness of UIE models effectively; 2) They rely on small models or\nhandcrafted rules to generate perturbations, often resulting in unnatural\nadversarial examples. Considering the powerful generation capabilities of Large\nLanguage Models (LLMs), we introduce a new benchmark dataset for Robust UIE,\ncalled RUIE-Bench, which utilizes LLMs to generate more diverse and realistic\nperturbations across different IE tasks. Based on this dataset, we\ncomprehensively evaluate existing UIE models and reveal that both LLM-based\nmodels and other models suffer from significant performance drops. To improve\nrobustness and reduce training costs, we propose a data-augmentation solution\nthat dynamically selects hard samples for iterative training based on the\nmodel's inference loss. Experimental results show that training with only\n\\textbf{15\\%} of the data leads to an average \\textbf{7.5\\%} relative\nperformance improvement across three IE tasks."
    },
    {
        "date": "2025-03",
        "title": "DSPNet: Dual-vision Scene Perception for Robust 3D Question Answering",
        "author": "Jingzhou Luo, Yang Liu, Weixing Chen, Zhen Li, Yaowei Wang, Guanbin Li, and Liang Lin",
        "link": "http://arxiv.org/abs/2503.03190v1",
        "abstract": "3D Question Answering (3D QA) requires the model to comprehensively\nunderstand its situated 3D scene described by the text, then reason about its\nsurrounding environment and answer a question under that situation. However,\nexisting methods usually rely on global scene perception from pure 3D point\nclouds and overlook the importance of rich local texture details from\nmulti-view images. Moreover, due to the inherent noise in camera poses and\ncomplex occlusions, there exists significant feature degradation and reduced\nfeature robustness problems when aligning 3D point cloud with multi-view\nimages. In this paper, we propose a Dual-vision Scene Perception Network\n(DSPNet), to comprehensively integrate multi-view and point cloud features to\nimprove robustness in 3D QA. Our Text-guided Multi-view Fusion (TGMF) module\nprioritizes image views that closely match the semantic content of the text. To\nadaptively fuse back-projected multi-view images with point cloud features, we\ndesign the Adaptive Dual-vision Perception (ADVP) module, enhancing 3D scene\ncomprehension. Additionally, our Multimodal Context-guided Reasoning (MCGR)\nmodule facilitates robust reasoning by integrating contextual information\nacross visual and linguistic modalities. Experimental results on SQA3D and\nScanQA datasets demonstrate the superiority of our DSPNet. Codes will be\navailable at https://github.com/LZ-CH/DSPNet."
    },
    {
        "date": "2025-03",
        "title": "AttackSeqBench: Benchmarking Large Language Models' Understanding of Sequential Patterns in Cyber Attacks",
        "author": "Javier Yong, Haokai Ma, Yunshan Ma, Anis Yusof, Zhenkai Liang, and Ee-Chien Chang",
        "link": "http://arxiv.org/abs/2503.03170v1",
        "abstract": "The observations documented in Cyber Threat Intelligence (CTI) reports play a\ncritical role in describing adversarial behaviors, providing valuable insights\nfor security practitioners to respond to evolving threats. Recent advancements\nof Large Language Models (LLMs) have demonstrated significant potential in\nvarious cybersecurity applications, including CTI report understanding and\nattack knowledge graph construction. While previous works have proposed\nbenchmarks that focus on the CTI extraction ability of LLMs, the sequential\ncharacteristic of adversarial behaviors within CTI reports remains largely\nunexplored, which holds considerable significance in developing a comprehensive\nunderstanding of how adversaries operate. To address this gap, we introduce\nAttackSeqBench, a benchmark tailored to systematically evaluate LLMs'\ncapability to understand and reason attack sequences in CTI reports. Our\nbenchmark encompasses three distinct Question Answering (QA) tasks, each task\nfocuses on the varying granularity in adversarial behavior. To alleviate the\nlaborious effort of QA construction, we carefully design an automated dataset\nconstruction pipeline to create scalable and well-formulated QA datasets based\non real-world CTI reports. To ensure the quality of our dataset, we adopt a\nhybrid approach of combining human evaluation and systematic evaluation\nmetrics. We conduct extensive experiments and analysis with both fast-thinking\nand slow-thinking LLMs, while highlighting their strengths and limitations in\nanalyzing the sequential patterns in cyber attacks. The overarching goal of\nthis work is to provide a benchmark that advances LLM-driven CTI report\nunderstanding and fosters its application in real-world cybersecurity\noperations. Our dataset and code are available at\nhttps://github.com/Javiery3889/AttackSeqBench ."
    },
    {
        "date": "2025-03",
        "title": "BEVDriver: Leveraging BEV Maps in LLMs for Robust Closed-Loop Driving",
        "author": "Katharina Winter, Mark Azer, and Fabian B. Flohr",
        "link": "http://arxiv.org/abs/2503.03074v1",
        "abstract": "Autonomous driving has the potential to set the stage for more efficient\nfuture mobility, requiring the research domain to establish trust through safe,\nreliable and transparent driving. Large Language Models (LLMs) possess\nreasoning capabilities and natural language understanding, presenting the\npotential to serve as generalized decision-makers for ego-motion planning that\ncan interact with humans and navigate environments designed for human drivers.\nWhile this research avenue is promising, current autonomous driving approaches\nare challenged by combining 3D spatial grounding and the reasoning and language\ncapabilities of LLMs. We introduce BEVDriver, an LLM-based model for end-to-end\nclosed-loop driving in CARLA that utilizes latent BEV features as perception\ninput. BEVDriver includes a BEV encoder to efficiently process multi-view\nimages and 3D LiDAR point clouds. Within a common latent space, the BEV\nfeatures are propagated through a Q-Former to align with natural language\ninstructions and passed to the LLM that predicts and plans precise future\ntrajectories while considering navigation instructions and critical scenarios.\nOn the LangAuto benchmark, our model reaches up to 18.9% higher performance on\nthe Driving Score compared to SoTA methods."
    },
    {
        "date": "2025-03",
        "title": "LLM Misalignment via Adversarial RLHF Platforms",
        "author": "Erfan Entezami, and Ali Naseh",
        "link": "http://arxiv.org/abs/2503.03039v1",
        "abstract": "Reinforcement learning has shown remarkable performance in aligning language\nmodels with human preferences, leading to the rise of attention towards\ndeveloping RLHF platforms. These platforms enable users to fine-tune models\nwithout requiring any expertise in developing complex machine learning\nalgorithms. While these platforms offer useful features such as reward modeling\nand RLHF fine-tuning, their security and reliability remain largely unexplored.\nGiven the growing adoption of RLHF and open-source RLHF frameworks, we\ninvestigate the trustworthiness of these systems and their potential impact on\nbehavior of LLMs. In this paper, we present an attack targeting publicly\navailable RLHF tools. In our proposed attack, an adversarial RLHF platform\ncorrupts the LLM alignment process by selectively manipulating data samples in\nthe preference dataset. In this scenario, when a user's task aligns with the\nattacker's objective, the platform manipulates a subset of the preference\ndataset that contains samples related to the attacker's target. This\nmanipulation results in a corrupted reward model, which ultimately leads to the\nmisalignment of the language model. Our results demonstrate that such an attack\ncan effectively steer LLMs toward undesirable behaviors within the targeted\ndomains. Our work highlights the critical need to explore the vulnerabilities\nof RLHF platforms and their potential to cause misalignment in LLMs during the\nRLHF fine-tuning process."
    },
    {
        "date": "2025-03",
        "title": "Mind the Gap: Detecting Black-box Adversarial Attacks in the Making through Query Update Analysis",
        "author": "Jeonghwan Park, Niall McLaughlin, and Ihsen Alouani",
        "link": "http://arxiv.org/abs/2503.02986v1",
        "abstract": "Adversarial attacks remain a significant threat that can jeopardize the\nintegrity of Machine Learning (ML) models. In particular, query-based black-box\nattacks can generate malicious noise without having access to the victim\nmodel's architecture, making them practical in real-world contexts. The\ncommunity has proposed several defenses against adversarial attacks, only to be\nbroken by more advanced and adaptive attack strategies. In this paper, we\npropose a framework that detects if an adversarial noise instance is being\ngenerated. Unlike existing stateful defenses that detect adversarial noise\ngeneration by monitoring the input space, our approach learns adversarial\npatterns in the input update similarity space. In fact, we propose to observe a\nnew metric called Delta Similarity (DS), which we show it captures more\nefficiently the adversarial behavior. We evaluate our approach against 8\nstate-of-the-art attacks, including adaptive attacks, where the adversary is\naware of the defense and tries to evade detection. We find that our approach is\nsignificantly more robust than existing defenses both in terms of specificity\nand sensitivity."
    },
    {
        "date": "2025-03",
        "title": "Robust time series generation via Schr\u00f6dinger Bridge: a comprehensive evaluation",
        "author": "Alexandre Alouadi, Baptiste Barreau, Laurent Carlier, and Huy\u00ean Pham",
        "link": "http://arxiv.org/abs/2503.02943v1",
        "abstract": "We investigate the generative capabilities of the Schr\\\"odinger Bridge (SB)\napproach for time series. The SB framework formulates time series synthesis as\nan entropic optimal interpolation transport problem between a reference\nprobability measure on path space and a target joint distribution. This results\nin a stochastic differential equation over a finite horizon that accurately\ncaptures the temporal dynamics of the target time series. While the SB approach\nhas been largely explored in fields like image generation, there is a scarcity\nof studies for its application to time series. In this work, we bridge this gap\nby conducting a comprehensive evaluation of the SB method's robustness and\ngenerative performance. We benchmark it against state-of-the-art (SOTA) time\nseries generation methods across diverse datasets, assessing its strengths,\nlimitations, and capacity to model complex temporal dependencies. Our results\noffer valuable insights into the SB framework's potential as a versatile and\nrobust tool for time series generation."
    },
    {
        "date": "2025-03",
        "title": "A Causal Framework for Aligning Image Quality Metrics and Deep Neural Network Robustness",
        "author": "Nathan Drenkow, and Mathias Unberath",
        "link": "http://arxiv.org/abs/2503.02797v1",
        "abstract": "Image quality plays an important role in the performance of deep neural\nnetworks (DNNs) and DNNs have been widely shown to exhibit sensitivity to\nchanges in imaging conditions. Large-scale datasets often contain images under\na wide range of conditions prompting a need to quantify and understand their\nunderlying quality distribution in order to better characterize DNN performance\nand robustness. Aligning the sensitivities of image quality metrics and DNNs\nensures that estimates of quality can act as proxies for image/dataset\ndifficulty independent of the task models trained/evaluated on the data.\nConventional image quality assessment (IQA) seeks to measure and align quality\nrelative to human perceptual judgments, but here we seek a quality measure that\nis not only sensitive to imaging conditions but also well-aligned with DNN\nsensitivities. We first ask whether conventional IQA metrics are also\ninformative of DNN performance. In order to answer this question, we reframe\nIQA from a causal perspective and examine conditions under which quality\nmetrics are predictive of DNN performance. We show theoretically and\nempirically that current IQA metrics are weak predictors of DNN performance in\nthe context of classification. We then use our causal framework to provide an\nalternative formulation and a new image quality metric that is more strongly\ncorrelated with DNN performance and can act as a prior on performance without\ntraining new task models. Our approach provides a means to directly estimate\nthe quality distribution of large-scale image datasets towards characterizing\nthe relationship between dataset composition and DNN performance."
    },
    {
        "date": "2025-03",
        "title": "Quantitative Resilience Modeling for Autonomous Cyber Defense",
        "author": "Xavier Cadet, Simona Boboila, Edward Koh, Peter Chin, and Alina Oprea",
        "link": "http://arxiv.org/abs/2503.02780v1",
        "abstract": "Cyber resilience is the ability of a system to recover from an attack with\nminimal impact on system operations. However, characterizing a network's\nresilience under a cyber attack is challenging, as there are no formal\ndefinitions of resilience applicable to diverse network topologies and attack\npatterns. In this work, we propose a quantifiable formulation of resilience\nthat considers multiple defender operational goals, the criticality of various\nnetwork resources for daily operations, and provides interpretability to\nsecurity operators about their system's resilience under attack. We evaluate\nour approach within the CybORG environment, a reinforcement learning (RL)\nframework for autonomous cyber defense, analyzing trade-offs between\nresilience, costs, and prioritization of operational goals. Furthermore, we\nintroduce methods to aggregate resilience metrics across time-variable attack\npatterns and multiple network topologies, comprehensively characterizing system\nresilience. Using insights gained from our resilience metrics, we design RL\nautonomous defensive agents and compare them against several heuristic\nbaselines, showing that proactive network hardening techniques and prompt\nrecovery of compromised machines are critical for effective cyber defenses."
    },
    {
        "date": "2025-03",
        "title": "Optimisation of cyber insurance coverage with selection of cost effective security controls",
        "author": "Ganbayar Uuganbayar, Artsiom Yautsiukhin, Fabio Martinelli, and Fabio Massacci",
        "link": "http://arxiv.org/abs/2503.02706v1",
        "abstract": "Nowadays, cyber threats are considered among the most dangerous risks by top\nmanagement of enterprises. One way to deal with these risks is to insure them,\nbut cyber insurance is still quite expensive. The insurance fee can be reduced\nif organisations improve their cyber security protection, i.e., reducing the\ninsured risk. In other words, organisations need an investment strategy to\ndecide the optimal amount of investments into cyber insurance and\nself-protection. In this work, we propose an approach to help a risk-averse\norganisation to distribute its cyber security investments in a cost-efficient\nway. What makes our approach unique is that next to defining the amount of\ninvestments in cyber insurance and self-protection, our proposal also\nexplicitly defines how these investments should be spent by selecting the most\ncost-efficient security controls. Moreover, we provide an exact algorithm for\nthe control selection problem considering several threats at the same time and\ncompare this algorithm with other approximate algorithmic solutions."
    },
    {
        "date": "2025-03",
        "title": "Weight transport through spike timing for robust local gradients",
        "author": "Timo Gierlich, Andreas Baumbach, Akos F. Kungl, Kevin Max, and Mihai A. Petrovici",
        "link": "http://arxiv.org/abs/2503.02642v1",
        "abstract": "In both machine learning and in computational neuroscience, plasticity in\nfunctional neural networks is frequently expressed as gradient descent on a\ncost. Often, this imposes symmetry constraints that are difficult to reconcile\nwith local computation, as is required for biological networks or neuromorphic\nhardware. For example, wake-sleep learning in networks characterized by\nBoltzmann distributions builds on the assumption of symmetric connectivity.\nSimilarly, the error backpropagation algorithm is notoriously plagued by the\nweight transport problem between the representation and the error stream.\nExisting solutions such as feedback alignment tend to circumvent the problem by\ndeferring to the robustness of these algorithms to weight asymmetry. However,\nthey are known to scale poorly with network size and depth. We introduce\nspike-based alignment learning (SAL), a complementary learning rule for spiking\nneural networks, which uses spike timing statistics to extract and correct the\nasymmetry between effective reciprocal connections. Apart from being\nspike-based and fully local, our proposed mechanism takes advantage of noise.\nBased on an interplay between Hebbian and anti-Hebbian plasticity, synapses can\nthereby recover the true local gradient. This also alleviates discrepancies\nthat arise from neuron and synapse variability -- an omnipresent property of\nphysical neuronal networks. We demonstrate the efficacy of our mechanism using\ndifferent spiking network models. First, we show how SAL can significantly\nimprove convergence to the target distribution in probabilistic spiking\nnetworks as compared to Hebbian plasticity alone. Second, in neuronal\nhierarchies based on cortical microcircuits, we show how our proposed mechanism\neffectively enables the alignment of feedback weights to the forward pathway,\nthus allowing the backpropagation of correct feedback errors."
    },
    {
        "date": "2025-03",
        "title": "LLM-Safety Evaluations Lack Robustness",
        "author": "Tim Beyer, Sophie Xhonneux, Simon Geisler, Gauthier Gidel, Leo Schwinn, and Stephan G\u00fcnnemann",
        "link": "http://arxiv.org/abs/2503.02574v1",
        "abstract": "In this paper, we argue that current safety alignment research efforts for\nlarge language models are hindered by many intertwined sources of noise, such\nas small datasets, methodological inconsistencies, and unreliable evaluation\nsetups. This can, at times, make it impossible to evaluate and compare attacks\nand defenses fairly, thereby slowing progress. We systematically analyze the\nLLM safety evaluation pipeline, covering dataset curation, optimization\nstrategies for automated red-teaming, response generation, and response\nevaluation using LLM judges. At each stage, we identify key issues and\nhighlight their practical impact. We also propose a set of guidelines for\nreducing noise and bias in evaluations of future attack and defense papers.\nLastly, we offer an opposing perspective, highlighting practical reasons for\nexisting limitations. We believe that addressing the outlined problems in\nfuture research will improve the field's ability to generate easily comparable\nresults and make measurable progress."
    },
    {
        "date": "2025-03",
        "title": "Towards a robust R2D2 paradigm for radio-interferometric imaging: revisiting DNN training and architecture",
        "author": "Amir Aghabiglou, Chung San Chu, Chao Tang, Arwa Dabbech, and Yves Wiaux",
        "link": "http://arxiv.org/abs/2503.02554v1",
        "abstract": "The R2D2 Deep Neural Network (DNN) series was recently introduced for image\nformation in radio interferometry. It can be understood as a learned version of\nCLEAN, whose minor cycles are substituted with DNNs. We revisit R2D2 on the\ngrounds of series convergence, training methodology, and DNN architecture,\nimproving its robustness in terms of generalisability beyond training\nconditions, capability to deliver high data fidelity, and epistemic\nuncertainty. Firstly, while still focusing on telescope-specific training, we\nenhance the learning process by randomising Fourier sampling integration times,\nincorporating multi-scan multi-noise configurations, and varying imaging\nsettings, including pixel resolution and visibility-weighting scheme. Secondly,\nwe introduce a convergence criterion whereby the reconstruction process stops\nwhen the data residual is compatible with noise, rather than simply using all\navailable DNNs. This not only increases the reconstruction efficiency by\nreducing its computational cost, but also refines training by pruning out the\ndata/image pairs for which optimal data fidelity is reached before training the\nnext DNN. Thirdly, we substitute R2D2's early U-Net DNN with a novel\narchitecture (U-WDSR) combining U-Net and WDSR, which leverages wide\nactivation, dense connections, weight normalisation, and low-rank convolution\nto improve feature reuse and reconstruction precision. As previously, R2D2 was\ntrained for monochromatic intensity imaging with the Very Large Array (VLA) at\nfixed $512 \\times 512$ image size. Simulations on a wide range of inverse\nproblems and a case study on real data reveal that the new R2D2 model\nconsistently outperforms its earlier version in image reconstruction quality,\ndata fidelity, and epistemic uncertainty."
    },
    {
        "date": "2025-03",
        "title": "Attack Tree Distance: a practical examination of tree difference measurement within cyber security",
        "author": "Nathan D. Schiele, and Olga Gadyatskaya",
        "link": "http://arxiv.org/abs/2503.02499v1",
        "abstract": "CONTEXT. Attack treesare a recommended threat modeling tool, but there is no\nestablished method to compare them. OBJECTIVE. We aim to establish a method to\ncompare \"real\" attack trees, based on both the structure of the tree itself and\nthe meaning of the node labels. METHOD. We define four methods of comparison\n(three novel and one established) and compare them to a dataset of attack trees\ncreated from a study run on students (n = 39). These attack trees all follow\nfrom the same scenario, but have slightly different labels. RESULTS. We find\nthat applying semantic similarity as a means of comparing node labels is a\nvalid approach. Further, we find that treeedit distance (established) and\nradical distance (novel) are themost promising methods of comparison in most\ncircumstances. CONCLUSION. We show that these two methods are valid as means of\ncomparing attack trees, and suggest a novel technique for using semantic\nsimilarity to compare node labels. We further suggest that these methods can be\nused to compare attack trees in a real-world scenario, and that they can be\nused to identify similar attack trees."
    },
    {
        "date": "2025-03",
        "title": "The Distributionally Robust Optimization Model of Sparse Principal Component Analysis",
        "author": "Lei Wang, Xin Liu, and Xiaojun Chen",
        "link": "http://arxiv.org/abs/2503.02494v1",
        "abstract": "We consider sparse principal component analysis (PCA) under a stochastic\nsetting where the underlying probability distribution of the random parameter\nis uncertain. This problem is formulated as a distributionally robust\noptimization (DRO) model based on a constructive approach to capturing\nuncertainty in the covariance matrix, which constitutes a nonsmooth constrained\nmin-max optimization problem. We further prove that the inner maximization\nproblem admits a closed-form solution, reformulating the original DRO model\ninto an equivalent minimization problem on the Stiefel manifold. This\ntransformation leads to a Riemannian optimization problem with intricate\nnonsmooth terms, a challenging formulation beyond the reach of existing\nalgorithms. To address this issue, we devise an efficient smoothing manifold\nproximal gradient algorithm. We prove the Riemannian gradient consistency and\nglobal convergence of our algorithm to a stationary point of the nonsmooth\nminimization problem. Moreover, we establish the iteration complexity of our\nalgorithm. Finally, numerical experiments are conducted to validate the\neffectiveness and scalability of our algorithm, as well as to highlight the\nnecessity and rationality of adopting the DRO model for sparse PCA."
    },
    {
        "date": "2025-03",
        "title": "Deep Robust Reversible Watermarking",
        "author": "Jiale Chen, Wei Wang, Chongyang Shi, Li Dong, Yuanman Li, and Xiping Hu",
        "link": "http://arxiv.org/abs/2503.02490v1",
        "abstract": "Robust Reversible Watermarking (RRW) enables perfect recovery of cover images\nand watermarks in lossless channels while ensuring robust watermark extraction\nin lossy channels. Existing RRW methods, mostly non-deep learning-based, face\ncomplex designs, high computational costs, and poor robustness, limiting their\npractical use. This paper proposes Deep Robust Reversible Watermarking (DRRW),\na deep learning-based RRW scheme. DRRW uses an Integer Invertible Watermark\nNetwork (iIWN) to map integer data distributions invertibly, addressing\nconventional RRW limitations. Unlike traditional RRW, which needs\ndistortion-specific designs, DRRW employs an encoder-noise layer-decoder\nframework for adaptive robustness via end-to-end training. In inference, cover\nimage and watermark map to an overflowed stego image and latent variables,\ncompressed by arithmetic coding into a bitstream embedded via reversible data\nhiding for lossless recovery. We introduce an overflow penalty loss to reduce\npixel overflow, shortening the auxiliary bitstream while enhancing robustness\nand stego image quality. An adaptive weight adjustment strategy avoids manual\nwatermark loss weighting, improving training stability and performance.\nExperiments show DRRW outperforms state-of-the-art RRW methods, boosting\nrobustness and cutting embedding, extraction, and recovery complexities by\n55.14\\(\\times\\), 5.95\\(\\times\\), and 3.57\\(\\times\\), respectively. The\nauxiliary bitstream shrinks by 43.86\\(\\times\\), with reversible embedding\nsucceeding on 16,762 PASCAL VOC 2012 images, advancing practical RRW. DRRW\nexceeds irreversible robust watermarking in robustness and quality while\nmaintaining reversibility."
    },
    {
        "date": "2025-03",
        "title": "Robust detection of overlapping bioacoustic sound events",
        "author": "Louis Mahon, Benjamin Hoffman, Logan S James, Maddie Cusimano, Masato Hagiwara, Sarah C Woolley, and Olivier Pietquin",
        "link": "http://arxiv.org/abs/2503.02389v1",
        "abstract": "We propose a method for accurately detecting bioacoustic sound events that is\nrobust to overlapping events, a common issue in domains such as ethology,\necology and conservation. While standard methods employ a frame-based,\nmulti-label approach, we introduce an onset-based detection method which we\nname Voxaboxen. It takes inspiration from object detection methods in computer\nvision, but simultaneously takes advantage of recent advances in\nself-supervised audio encoders. For each time window, Voxaboxen predicts\nwhether it contains the start of a vocalization and how long the vocalization\nis. It also does the same in reverse, predicting whether each window contains\nthe end of a vocalization, and how long ago it started. The two resulting sets\nof bounding boxes are then fused using a graph-matching algorithm. We also\nrelease a new dataset designed to measure performance on detecting overlapping\nvocalizations. This consists of recordings of zebra finches annotated with\ntemporally-strong labels and showing frequent overlaps. We test Voxaboxen on\nseven existing data sets and on our new data set. We compare Voxaboxen to\nnatural baselines and existing sound event detection methods and demonstrate\nSotA results. Further experiments show that improvements are robust to frequent\nvocalization overlap."
    },
    {
        "date": "2025-03",
        "title": "Towards Robust Multi-UAV Collaboration: MARL with Noise-Resilient Communication and Attention Mechanisms",
        "author": "Zilin Zhao, Chishui Chen, Haotian Shi, Jiale Chen, Xuanlin Yue, Zhejian Yang, and Yang Liu",
        "link": "http://arxiv.org/abs/2503.02913v1",
        "abstract": "Efficient path planning for unmanned aerial vehicles (UAVs) is crucial in\nremote sensing and information collection. As task scales expand, the\ncooperative deployment of multiple UAVs significantly improves information\ncollection efficiency. However, collaborative communication and decision-making\nfor multiple UAVs remain major challenges in path planning, especially in noisy\nenvironments. To efficiently accomplish complex information collection tasks in\n3D space and address robust communication issues, we propose a multi-agent\nreinforcement learning (MARL) framework for UAV path planning based on the\nCounterfactual Multi-Agent Policy Gradients (COMA) algorithm. The framework\nincorporates attention mechanism-based UAV communication protocol and\ntraining-deployment system, significantly improving communication robustness\nand individual decision-making capabilities in noisy conditions. Experiments\nconducted on both synthetic and real-world datasets demonstrate that our method\noutperforms existing algorithms in terms of path planning efficiency and\nrobustness, especially in noisy environments, achieving a 78\\% improvement in\nentropy reduction."
    },
    {
        "date": "2025-03",
        "title": "Low-Level Matters: An Efficient Hybrid Architecture for Robust Multi-frame Infrared Small Target Detection",
        "author": "Zhihua Shen, Siyang Chen, Han Wang, Tongsu Zhang, Xiaohu Zhang, Xiangpeng Xu, and Xia Yang",
        "link": "http://arxiv.org/abs/2503.02220v1",
        "abstract": "Multi-frame infrared small target detection (IRSTD) plays a crucial role in\nlow-altitude and maritime surveillance. The hybrid architecture combining CNNs\nand Transformers shows great promise for enhancing multi-frame IRSTD\nperformance. In this paper, we propose LVNet, a simple yet powerful hybrid\narchitecture that redefines low-level feature learning in hybrid frameworks for\nmulti-frame IRSTD. Our key insight is that the standard linear patch embeddings\nin Vision Transformers are insufficient for capturing the scale-sensitive local\nfeatures critical to infrared small targets. To address this limitation, we\nintroduce a multi-scale CNN frontend that explicitly models local features by\nleveraging the local spatial bias of convolution. Additionally, we design a\nU-shaped video Transformer for multi-frame spatiotemporal context modeling,\neffectively capturing the motion characteristics of targets. Experiments on the\npublicly available datasets IRDST and NUDT-MIRSDT demonstrate that LVNet\noutperforms existing state-of-the-art methods. Notably, compared to the current\nbest-performing method, LMAFormer, LVNet achieves an improvement of 5.63\\% /\n18.36\\% in nIoU, while using only 1/221 of the parameters and 1/92 / 1/21 of\nthe computational cost. Ablation studies further validate the importance of\nlow-level representation learning in hybrid architectures. Our code and trained\nmodels are available at https://github.com/ZhihuaShen/LVNet."
    },
    {
        "date": "2025-03",
        "title": "Client-Aided Secure Two-Party Computation of Dynamic Controllers",
        "author": "Kaoru Teranishi, and Takashi Tanaka",
        "link": "http://arxiv.org/abs/2503.02176v1",
        "abstract": "In this paper, we propose a secure two-party computation protocol for dynamic\ncontrollers using a secret sharing scheme. The proposed protocol realizes\noutsourcing of controller computation to two servers, while controller\nparameters, states, inputs, and outputs are kept secret against the servers.\nUnlike previous encrypted controls in a single-server setting, the proposed\nmethod can operate a dynamic controller for an infinite time horizon without\ncontroller state decryption or input re-encryption. We show that the control\nperformance achievable by the proposed protocol can be made arbitrarily close\nto that attained by the unencrypted controller. Furthermore, system-theoretic\nand cryptographic modifications of the protocol are presented to improve the\ncommunication complexity. The feasibility of the protocol is demonstrated\nthrough numerical examples of PID and observer-based controls."
    },
    {
        "date": "2025-03",
        "title": "Adversarial Tokenization",
        "author": "Renato Lui Geh, Zilei Shao, and Guy Van den Broeck",
        "link": "http://arxiv.org/abs/2503.02174v1",
        "abstract": "Current LLM pipelines account for only one possible tokenization for a given\nstring, ignoring exponentially many alternative tokenizations during training\nand inference. For example, the standard Llama3 tokenization of penguin is\n[p,enguin], yet [peng,uin] is another perfectly valid alternative. In this\npaper, we show that despite LLMs being trained solely on one tokenization, they\nstill retain semantic understanding of other tokenizations, raising questions\nabout their implications in LLM safety. Put succinctly, we answer the following\nquestion: can we adversarially tokenize an obviously malicious string to evade\nsafety and alignment restrictions? We show that not only is adversarial\ntokenization an effective yet previously neglected axis of attack, but it is\nalso competitive against existing state-of-the-art adversarial approaches\nwithout changing the text of the harmful request. We empirically validate this\nexploit across three state-of-the-art LLMs and adversarial datasets, revealing\na previously unknown vulnerability in subword models."
    },
    {
        "date": "2025-03",
        "title": "DDAD: A Two-pronged Adversarial Defense Based on Distributional Discrepancy",
        "author": "Jiacheng Zhang, Benjamin I. P. Rubinstein, Jingfeng Zhang, and Feng Liu",
        "link": "http://arxiv.org/abs/2503.02169v1",
        "abstract": "Statistical adversarial data detection (SADD) detects whether an upcoming\nbatch contains adversarial examples (AEs) by measuring the distributional\ndiscrepancies between clean examples (CEs) and AEs. In this paper, we reveal\nthe potential strength of SADD-based methods by theoretically showing that\nminimizing distributional discrepancy can help reduce the expected loss on AEs.\nNevertheless, despite these advantages, SADD-based methods have a potential\nlimitation: they discard inputs that are detected as AEs, leading to the loss\nof clean information within those inputs. To address this limitation, we\npropose a two-pronged adversarial defense method, named\nDistributional-Discrepancy-based Adversarial Defense (DDAD). In the training\nphase, DDAD first optimizes the test power of the maximum mean discrepancy\n(MMD) to derive MMD-OPT, and then trains a denoiser by minimizing the MMD-OPT\nbetween CEs and AEs. In the inference phase, DDAD first leverages MMD-OPT to\ndifferentiate CEs and AEs, and then applies a two-pronged process: (1) directly\nfeeding the detected CEs into the classifier, and (2) removing noise from the\ndetected AEs by the distributional-discrepancy-based denoiser. Extensive\nexperiments show that DDAD outperforms current state-of-the-art (SOTA) defense\nmethods by notably improving clean and robust accuracy on CIFAR-10 and\nImageNet-1K against adaptive white-box attacks."
    },
    {
        "date": "2025-03",
        "title": "Generalized Diffusion Detector: Mining Robust Features from Diffusion Models for Domain-Generalized Detection",
        "author": "Boyong He, Yuxiang Ji, Qianwen Ye, Zhuoyue Tan, and Liaoni Wu",
        "link": "http://arxiv.org/abs/2503.02101v1",
        "abstract": "Domain generalization (DG) for object detection aims to enhance detectors'\nperformance in unseen scenarios. This task remains challenging due to complex\nvariations in real-world applications. Recently, diffusion models have\ndemonstrated remarkable capabilities in diverse scene generation, which\ninspires us to explore their potential for improving DG tasks. Instead of\ngenerating images, our method extracts multi-step intermediate features during\nthe diffusion process to obtain domain-invariant features for generalized\ndetection. Furthermore, we propose an efficient knowledge transfer framework\nthat enables detectors to inherit the generalization capabilities of diffusion\nmodels through feature and object-level alignment, without increasing inference\ntime. We conduct extensive experiments on six challenging DG benchmarks. The\nresults demonstrate that our method achieves substantial improvements of 14.0%\nmAP over existing DG approaches across different domains and corruption types.\nNotably, our method even outperforms most domain adaptation methods without\naccessing any target domain data. Moreover, the diffusion-guided detectors show\nconsistent improvements of 15.9% mAP on average compared to the baseline. Our\nwork aims to present an effective approach for domain-generalized detection and\nprovide potential insights for robust visual recognition in real-world\nscenarios. The code is available at\n\\href{https://github.com/heboyong/Generalized-Diffusion-Detector}{Generalized\nDiffusion Detector}"
    },
    {
        "date": "2025-03",
        "title": "Robustness to Geographic Distribution Shift using Location Encoders",
        "author": "Ruth Crasto",
        "link": "http://arxiv.org/abs/2503.02036v1",
        "abstract": "Geographic distribution shift arises when the distribution of locations on\nEarth in a training dataset is different from what is seen at test time. The\nmost common approaches to tackling geographic distribution shift treat regions\ndelimited by administrative boundaries such as countries or continents as\nseparate domains and apply standard domain adaptation methods, ignoring\ngeographic coordinates that are often available as metadata. This paper\nproposes the use of location encoders for training models that are more robust\nto geographic distribution shift. We show how both simple sine-cosine encoders\nand pre-trained location encoders can be used to improve standard domain\nadaptation methods for the special case of geographic distribution shift. Our\nproposed methods achieve state-of-the-art results on geo-tagged imagery\ndatasets from the WILDS benchmark."
    },
    {
        "date": "2025-03",
        "title": "SLAP: Secure Location-proof and Anonymous Privacy-preserving Spectrum Access",
        "author": "Saleh Darzi, and Attila A. Yavuz",
        "link": "http://arxiv.org/abs/2503.02019v1",
        "abstract": "The rapid advancements in wireless technology have significantly increased\nthe demand for communication resources, leading to the development of Spectrum\nAccess Systems (SAS). However, network regulations require disclosing sensitive\nuser information, such as location coordinates and transmission details,\nraising critical privacy concerns. Moreover, as a database-driven architecture\nreliant on user-provided data, SAS necessitates robust location verification to\ncounter identity and location spoofing attacks and remains a primary target for\ndenial-of-service (DoS) attacks. Addressing these security challenges while\nadhering to regulatory requirements is essential. In this paper, we propose\nSLAP, a novel framework that ensures location privacy and anonymity during\nspectrum queries, usage notifications, and location-proof acquisition. Our\nsolution includes an adaptive dual-scenario location verification mechanism\nwith architectural flexibility and a fallback option, along with a counter-DoS\napproach using time-lock puzzles. We prove the security of SLAP and demonstrate\nits advantages over existing solutions through comprehensive performance\nevaluations."
    },
    {
        "date": "2025-03",
        "title": "A Lightweight and Secure Deep Learning Model for Privacy-Preserving Federated Learning in Intelligent Enterprises",
        "author": "Reza Fotohi, Fereidoon Shams Aliee, and Bahar Farahani",
        "link": "http://arxiv.org/abs/2503.02017v1",
        "abstract": "The ever growing Internet of Things (IoT) connections drive a new type of\norganization, the Intelligent Enterprise. In intelligent enterprises, machine\nlearning based models are adopted to extract insights from data. Due to the\nefficiency and privacy challenges of these traditional models, a new federated\nlearning (FL) paradigm has emerged. In FL, multiple enterprises can jointly\ntrain a model to update a final model. However, firstly, FL trained models\nusually perform worse than centralized models, especially when enterprises\ntraining data is non-IID (Independent and Identically Distributed). Second, due\nto the centrality of FL and the untrustworthiness of local enterprises,\ntraditional FL solutions are vulnerable to poisoning and inference attacks and\nviolate privacy. Thirdly, the continuous transfer of parameters between\nenterprises and servers increases communication costs. To this end, the\nFedAnil+ model is proposed, a novel, lightweight, and secure Federated Deep\nLearning Model that includes three main phases. In the first phase, the goal is\nto solve the data type distribution skew challenge. Addressing privacy concerns\nagainst poisoning and inference attacks is covered in the second phase.\nFinally, to alleviate the communication overhead, a novel compression approach\nis proposed that significantly reduces the size of the updates. The experiment\nresults validate that FedAnil+ is secure against inference and poisoning\nattacks with better accuracy. In addition, it shows improvements over existing\napproaches in terms of model accuracy (13%, 16%, and 26%), communication cost\n(17%, 21%, and 25%), and computation cost (7%, 9%, and 11%)."
    },
    {
        "date": "2025-03",
        "title": "Recurrence-Enhanced Vision-and-Language Transformers for Robust Multimodal Document Retrieval",
        "author": "Davide Caffagni, Sara Sarto, Marcella Cornia, Lorenzo Baraldi, and Rita Cucchiara",
        "link": "http://arxiv.org/abs/2503.01980v1",
        "abstract": "Cross-modal retrieval is gaining increasing efficacy and interest from the\nresearch community, thanks to large-scale training, novel architectural and\nlearning designs, and its application in LLMs and multimodal LLMs. In this\npaper, we move a step forward and design an approach that allows for multimodal\nqueries, composed of both an image and a text, and can search within\ncollections of multimodal documents, where images and text are interleaved. Our\nmodel, ReT, employs multi-level representations extracted from different layers\nof both visual and textual backbones, both at the query and document side. To\nallow for multi-level and cross-modal understanding and feature extraction, ReT\nemploys a novel Transformer-based recurrent cell that integrates both textual\nand visual features at different layers, and leverages sigmoidal gates inspired\nby the classical design of LSTMs. Extensive experiments on M2KR and M-BEIR\nbenchmarks show that ReT achieves state-of-the-art performance across diverse\nsettings. Our source code and trained models are publicly available at\nhttps://github.com/aimagelab/ReT."
    },
    {
        "date": "2025-03",
        "title": "AutoAdvExBench: Benchmarking autonomous exploitation of adversarial example defenses",
        "author": "Nicholas Carlini, Javier Rando, Edoardo Debenedetti, Milad Nasr, and Florian Tram\u00e8r",
        "link": "http://arxiv.org/abs/2503.01811v1",
        "abstract": "We introduce AutoAdvExBench, a benchmark to evaluate if large language models\n(LLMs) can autonomously exploit defenses to adversarial examples. Unlike\nexisting security benchmarks that often serve as proxies for real-world tasks,\nbench directly measures LLMs' success on tasks regularly performed by machine\nlearning security experts. This approach offers a significant advantage: if a\nLLM could solve the challenges presented in bench, it would immediately present\npractical utility for adversarial machine learning researchers. We then design\na strong agent that is capable of breaking 75% of CTF-like (\"homework\nexercise\") adversarial example defenses. However, we show that this agent is\nonly able to succeed on 13% of the real-world defenses in our benchmark,\nindicating the large gap between difficulty in attacking \"real\" code, and\nCTF-like code. In contrast, a stronger LLM that can attack 21% of real defenses\nonly succeeds on 54% of CTF-like defenses. We make this benchmark available at\nhttps://github.com/ethz-spylab/AutoAdvExBench."
    },
    {
        "date": "2025-03",
        "title": "Protecting DeFi Platforms against Non-Price Flash Loan Attacks",
        "author": "Abdulrahman Alhaidari, Balaji Palanisamy, and Prashant Krishnamurthy",
        "link": "http://arxiv.org/abs/2503.01944v1",
        "abstract": "Smart contracts in Decentralized Finance (DeFi) platforms are attractive\ntargets for attacks as their vulnerabilities can lead to massive amounts of\nfinancial losses. Flash loan attacks, in particular, pose a major threat to\nDeFi protocols that hold a Total Value Locked (TVL) exceeding \\$106 billion.\nThese attacks use the atomicity property of blockchains to drain funds from\nsmart contracts in a single transaction. While existing research primarily\nfocuses on price manipulation attacks, such as oracle manipulation, mitigating\nnon-price flash loan attacks that often exploit smart contracts' zero-day\nvulnerabilities remains largely unaddressed. These attacks are challenging to\ndetect because of their unique patterns, time sensitivity, and complexity. In\nthis paper, we present FlashGuard, a runtime detection and mitigation method\nfor non-price flash loan attacks. Our approach targets smart contract function\nsignatures to identify attacks in real-time and counterattack by disrupting the\nattack transaction atomicity by leveraging the short window when transactions\nare visible in the mempool but not yet confirmed. When FlashGuard detects an\nattack, it dispatches a stealthy dusting counterattack transaction to miners to\nchange the victim contract's state which disrupts the attack's atomicity and\nforces the attack transaction to revert. We evaluate our approach using 20\nhistorical attacks and several unseen attacks. FlashGuard achieves an average\nreal-time detection latency of 150.31ms, a detection accuracy of over 99.93\\%,\nand an average disruption time of 410.92ms. FlashGuard could have potentially\nrescued over \\$405.71 million in losses if it were deployed prior to these\nattack instances. FlashGuard demonstrates significant potential as a DeFi\nsecurity solution to mitigate and handle rising threats of non-price flash loan\nattacks."
    },
    {
        "date": "2025-03",
        "title": "Zero-Trust Artificial Intelligence Model Security Based on Moving Target Defense and Content Disarm and Reconstruction",
        "author": "Daniel Gilkarov, and Ran Dubin",
        "link": "http://arxiv.org/abs/2503.01758v1",
        "abstract": "This paper examines the challenges in distributing AI models through model\nzoos and file transfer mechanisms. Despite advancements in security measures,\nvulnerabilities persist, necessitating a multi-layered approach to mitigate\nrisks effectively. The physical security of model files is critical, requiring\nstringent access controls and attack prevention solutions. This paper proposes\na novel solution architecture composed of two prevention approaches. The first\nis Content Disarm and Reconstruction (CDR), which focuses on disarming\nserialization attacks that enable attackers to run malicious code as soon as\nthe model is loaded. The second is protecting the model architecture and\nweights from attacks by using Moving Target Defense (MTD), alerting the model\nstructure, and providing verification steps to detect such attacks. The paper\nfocuses on the highly exploitable Pickle and PyTorch file formats. It\ndemonstrates a 100% disarm rate while validated against known AI model\nrepositories and actual malware attacks from the HuggingFace model zoo."
    },
    {
        "date": "2025-03",
        "title": "Adversarial Agents: Black-Box Evasion Attacks with Reinforcement Learning",
        "author": "Kyle Domico, Jean-Charles Noirot Ferrand, Ryan Sheatsley, Eric Pauley, Josiah Hanna, and Patrick McDaniel",
        "link": "http://arxiv.org/abs/2503.01734v1",
        "abstract": "Reinforcement learning (RL) offers powerful techniques for solving complex\nsequential decision-making tasks from experience. In this paper, we demonstrate\nhow RL can be applied to adversarial machine learning (AML) to develop a new\nclass of attacks that learn to generate adversarial examples: inputs designed\nto fool machine learning models. Unlike traditional AML methods that craft\nadversarial examples independently, our RL-based approach retains and exploits\npast attack experience to improve future attacks. We formulate adversarial\nexample generation as a Markov Decision Process and evaluate RL's ability to\n(a) learn effective and efficient attack strategies and (b) compete with\nstate-of-the-art AML. On CIFAR-10, our agent increases the success rate of\nadversarial examples by 19.4% and decreases the median number of victim model\nqueries per adversarial example by 53.2% from the start to the end of training.\nIn a head-to-head comparison with a state-of-the-art image attack,\nSquareAttack, our approach enables an adversary to generate adversarial\nexamples with 13.1% more success after 5000 episodes of training. From a\nsecurity perspective, this work demonstrates a powerful new attack vector that\nuses RL to attack ML models efficiently and at scale."
    },
    {
        "date": "2025-03",
        "title": "Perceptual Motor Learning with Active Inference Framework for Robust Lateral Control",
        "author": "Elahe Delavari, John Moore, Junho Hong, and Jaerock Kwon",
        "link": "http://arxiv.org/abs/2503.01676v2",
        "abstract": "This paper presents a novel Perceptual Motor Learning (PML) framework\nintegrated with Active Inference (AIF) to enhance lateral control in Highly\nAutomated Vehicles (HAVs). PML, inspired by human motor learning, emphasizes\nthe seamless integration of perception and action, enabling efficient\ndecision-making in dynamic environments. Traditional autonomous driving\napproaches--including modular pipelines, imitation learning, and reinforcement\nlearning--struggle with adaptability, generalization, and computational\nefficiency. In contrast, PML with AIF leverages a generative model to minimize\nprediction error (\"surprise\") and actively shape vehicle control based on\nlearned perceptual-motor representations. Our approach unifies deep learning\nwith active inference principles, allowing HAVs to perform lane-keeping\nmaneuvers with minimal data and without extensive retraining across different\nenvironments. Extensive experiments in the CARLA simulator demonstrate that PML\nwith AIF enhances adaptability without increasing computational overhead while\nachieving performance comparable to conventional methods. These findings\nhighlight the potential of PML-driven active inference as a robust alternative\nfor real-world autonomous driving applications."
    },
    {
        "date": "2025-03",
        "title": "Robust Palm-Vein Recognition Using the MMD Filter: Improving SIFT-Based Feature Matching",
        "author": "Kaveen Perera, Fouad Khelifi, and Ammar Belatreche",
        "link": "http://arxiv.org/abs/2503.01612v1",
        "abstract": "A major challenge with palm vein images is that slight movements of the\nfingers and thumb, or variations in hand posture, can stretch the skin in\ndifferent areas and alter the vein patterns. This can result in an infinite\nnumber of variations in palm vein images for a given individual. This paper\nintroduces a novel filtering technique for SIFT-based feature matching, known\nas the Mean and Median Distance (MMD) Filter. This method evaluates the\ndifferences in keypoint coordinates and computes the mean and median in each\ndirection to eliminate incorrect matches. Experiments conducted on the 850nm\nsubset of the CASIA dataset indicate that the proposed MMD filter effectively\npreserves correct points while reducing false positives detected by other\nfiltering methods. A comparison with existing SIFT-based palm vein recognition\nsystems demonstrates that the proposed MMD filter delivers outstanding\nperformance, achieving lower Equal Error Rate (EER) values. This article\npresents an extended author's version based on our previous work, A Keypoint\nFiltering Method for SIFT based Palm-Vein Recognition."
    },
    {
        "date": "2025-03",
        "title": "Meta Learning-Driven Iterative Refinement for Robust Anomaly Detection in Industrial Inspection",
        "author": "Muhammad Aqeel, Shakiba Sharifi, Marco Cristani, and Francesco Setti",
        "link": "http://arxiv.org/abs/2503.01569v1",
        "abstract": "This study investigates the performance of robust anomaly detection models in\nindustrial inspection, focusing particularly on their ability to handle noisy\ndata. We propose to leverage the adaptation ability of meta learning approaches\nto identify and reject noisy training data to improve the learning process. In\nour model, we employ Model Agnostic Meta Learning (MAML) and an iterative\nrefinement process through an Inter-Quartile Range rejection scheme to enhance\ntheir adaptability and robustness. This approach significantly improves the\nmodels capability to distinguish between normal and defective conditions. Our\nresults of experiments conducted on well known MVTec and KSDD2 datasets\ndemonstrate that the proposed method not only excels in environments with\nsubstantial noise but can also contribute in case of a clear training set,\nisolating those samples that are relatively out of distribution, thus offering\nsignificant improvements over traditional models."
    },
    {
        "date": "2025-03",
        "title": "Revisiting Locally Differentially Private Protocols: Towards Better Trade-offs in Privacy, Utility, and Attack Resistance",
        "author": "H\u00e9ber H. Arcolezi, and S\u00e9bastien Gambs",
        "link": "http://arxiv.org/abs/2503.01482v1",
        "abstract": "Local Differential Privacy (LDP) offers strong privacy protection, especially\nin settings in which the server collecting the data is untrusted. However,\ndesigning LDP mechanisms that achieve an optimal trade-off between privacy,\nutility, and robustness to adversarial inference attacks remains challenging.\nIn this work, we introduce a general multi-objective optimization framework for\nrefining LDP protocols, enabling the joint optimization of privacy and utility\nunder various adversarial settings. While our framework is flexible enough to\naccommodate multiple privacy and security attacks as well as utility metrics,\nin this paper we specifically optimize for Attacker Success Rate (ASR) under\ndistinguishability attack as a measure of privacy and Mean Squared Error (MSE)\nas a measure of utility. We systematically revisit these trade-offs by\nanalyzing eight state-of-the-art LDP protocols and proposing refined\ncounterparts that leverage tailored optimization techniques. Experimental\nresults demonstrate that our proposed adaptive mechanisms consistently\noutperform their non-adaptive counterparts, reducing ASR by up to five orders\nof magnitude while maintaining competitive utility. Analytical derivations also\nconfirm the effectiveness of our mechanisms, moving them closer to the ASR-MSE\nPareto frontier."
    },
    {
        "date": "2025-03",
        "title": "Divide and Conquer: Heterogeneous Noise Integration for Diffusion-based Adversarial Purification",
        "author": "Gaozheng Pei, Shaojie Lyu, Gong Chen, Ke Ma, Qianqian Xu, Yingfei Sun, and Qingming Huang",
        "link": "http://arxiv.org/abs/2503.01407v1",
        "abstract": "Existing diffusion-based purification methods aim to disrupt adversarial\nperturbations by introducing a certain amount of noise through a forward\ndiffusion process, followed by a reverse process to recover clean examples.\nHowever, this approach is fundamentally flawed: the uniform operation of the\nforward process across all pixels compromises normal pixels while attempting to\ncombat adversarial perturbations, resulting in the target model producing\nincorrect predictions. Simply relying on low-intensity noise is insufficient\nfor effective defense. To address this critical issue, we implement a\nheterogeneous purification strategy grounded in the interpretability of neural\nnetworks. Our method decisively applies higher-intensity noise to specific\npixels that the target model focuses on while the remaining pixels are\nsubjected to only low-intensity noise. This requirement motivates us to\nredesign the sampling process of the diffusion model, allowing for the\neffective removal of varying noise levels. Furthermore, to evaluate our method\nagainst strong adaptative attack, our proposed method sharply reduces time cost\nand memory usage through a single-step resampling. The empirical evidence from\nextensive experiments across three datasets demonstrates that our method\noutperforms most current adversarial training and purification techniques by a\nsubstantial margin."
    },
    {
        "date": "2025-03",
        "title": "Jailbreaking Generative AI: Empowering Novices to Conduct Phishing Attacks",
        "author": "Rina Mishra, Gaurav Varshney, and Shreya Singh",
        "link": "http://arxiv.org/abs/2503.01395v1",
        "abstract": "The rapid advancements in generative AI models, such as ChatGPT, have\nintroduced both significant benefits and new risks within the cybersecurity\nlandscape. This paper investigates the potential misuse of the latest AI model,\nChatGPT-4o Mini, in facilitating social engineering attacks, with a particular\nfocus on phishing, one of the most pressing cybersecurity threats today. While\nexisting literature primarily addresses the technical aspects, such as\njailbreaking techniques, none have fully explored the free and straightforward\nexecution of a comprehensive phishing campaign by novice users using ChatGPT-4o\nMini. In this study, we examine the vulnerabilities of AI-driven chatbot\nservices in 2025, specifically how methods like jailbreaking and reverse\npsychology can bypass ethical safeguards, allowing ChatGPT to generate phishing\ncontent, suggest hacking tools, and assist in carrying out phishing attacks.\nOur findings underscore the alarming ease with which even inexperienced users\ncan execute sophisticated phishing campaigns, emphasizing the urgent need for\nstronger cybersecurity measures and heightened user awareness in the age of AI."
    },
    {
        "date": "2025-03",
        "title": "The Road Less Traveled: Investigating Robustness and Explainability in CNN Malware Detection",
        "author": "Matteo Brosolo, Vinod Puthuvath, and Mauro Conti",
        "link": "http://arxiv.org/abs/2503.01391v1",
        "abstract": "Machine learning has become a key tool in cybersecurity, improving both\nattack strategies and defense mechanisms. Deep learning models, particularly\nConvolutional Neural Networks (CNNs), have demonstrated high accuracy in\ndetecting malware images generated from binary data. However, the\ndecision-making process of these black-box models remains difficult to\ninterpret. This study addresses this challenge by integrating quantitative\nanalysis with explainability tools such as Occlusion Maps, HiResCAM, and SHAP\nto better understand CNN behavior in malware classification. We further\ndemonstrate that obfuscation techniques can reduce model accuracy by up to 50%,\nand propose a mitigation strategy to enhance robustness. Additionally, we\nanalyze heatmaps from multiple tests and outline a methodology for\nidentification of artifacts, aiding researchers in conducting detailed manual\ninvestigations. This work contributes to improving the interpretability and\nresilience of deep learning-based intrusion detection systems"
    },
    {
        "date": "2025-03",
        "title": "Q-NL Verifier: Leveraging Synthetic Data for Robust Knowledge Graph Question Answering",
        "author": "Tim Schwabe, Louisa Siebel, Patrik Valach, and Maribel Acosta",
        "link": "http://arxiv.org/abs/2503.01385v1",
        "abstract": "Question answering (QA) requires accurately aligning user questions with\nstructured queries, a process often limited by the scarcity of high-quality\nquery-natural language (Q-NL) pairs. To overcome this, we present Q-NL\nVerifier, an approach to generating high-quality synthetic pairs of queries and\nNL translations. Our approach relies on large language models (LLMs) to\ngenerate semantically precise natural language paraphrases of structured\nqueries. Building on these synthetic Q-NL pairs, we introduce a learned\nverifier component that automatically determines whether a generated paraphrase\nis semantically equivalent to the original query. Our experiments with the\nwell-known LC-QuAD 2.0 benchmark show that Q-NL Verifier generalizes well to\nparaphrases from other models and even human-authored translations. Our\napproach strongly aligns with human judgments across varying query complexities\nand outperforms existing NLP metrics in assessing semantic correctness. We also\nintegrate the verifier into QA pipelines, showing that verifier-filtered\nsynthetic data has significantly higher quality in terms of translation\ncorrectness and enhances NL to Q translation accuracy. Lastly, we release an\nupdated version of the LC-QuAD 2.0 benchmark containing our synthetic Q-NL\npairs and verifier scores, offering a new resource for robust and scalable QA."
    },
    {
        "date": "2025-03",
        "title": "Same Question, Different Words: A Latent Adversarial Framework for Prompt Robustness",
        "author": "Tingchen Fu, and Fazl Barez",
        "link": "http://arxiv.org/abs/2503.01345v1",
        "abstract": "Insensitivity to semantically-preserving variations of prompts (paraphrases)\nis crucial for reliable behavior and real-world deployment of large language\nmodels. However, language models exhibit significant performance degradation\nwhen faced with semantically equivalent but differently phrased prompts, and\nexisting solutions either depend on trial-and-error prompt engineering or\nrequire computationally expensive inference-time algorithms. In this study,\nbuilt on the key insight that worst-case prompts exhibit a drift in embedding\nspace, we present Latent Adversarial Paraphrasing (LAP), a dual-loop\nadversarial framework: the inner loop trains a learnable perturbation to serve\nas a \"latent continuous paraphrase\" while preserving semantics through\nLagrangian regulation, and the outer loop optimizes the language model\nparameters on these perturbations. We conduct extensive experiments to\ndemonstrate the effectiveness of LAP across multiple LLM architectures on the\nRobustAlpaca benchmark with a 0.5%-4% absolution improvement on worst-case\nwin-rate compared with vanilla supervised fine-tuning."
    },
    {
        "date": "2025-03",
        "title": "Victim-Centred Abuse Investigations and Defenses for Social Media Platforms",
        "author": "Zaid Hakami, Ashfaq Ali Shafin, Peter J. Clarke, Niki Pissinou, and Bogdan Carbunar",
        "link": "http://arxiv.org/abs/2503.01327v1",
        "abstract": "Online abuse, a persistent aspect of social platform interactions, impacts\nuser well-being and exposes flaws in platform designs that include insufficient\ndetection efforts and inadequate victim protection measures. Ensuring safety in\nplatform interactions requires the integration of victim perspectives in the\ndesign of abuse detection and response systems. In this paper, we conduct\nsurveys (n = 230) and semi-structured interviews (n = 15) with students at a\nminority-serving institution in the US, to explore their experiences with abuse\non a variety of social platforms, their defense strategies, and their\nrecommendations for social platforms to improve abuse responses. We build on\nstudy findings to propose design requirements for abuse defense systems and\ndiscuss the role of privacy, anonymity, and abuse attribution requirements in\ntheir implementation. We introduce ARI, a blueprint for a unified, transparent,\nand personalized abuse response system for social platforms that sustainably\ndetects abuse by leveraging the expertise of platform users, incentivized with\nproceeds obtained from abusers."
    },
    {
        "date": "2025-03",
        "title": "Robust Simulation-Based Inference under Missing Data via Neural Processes",
        "author": "Yogesh Verma, Ayush Bharti, and Vikas Garg",
        "link": "http://arxiv.org/abs/2503.01287v1",
        "abstract": "Simulation-based inference (SBI) methods typically require fully observed\ndata to infer parameters of models with intractable likelihood functions.\nHowever, datasets often contain missing values due to incomplete observations,\ndata corruptions (common in astrophysics), or instrument limitations (e.g., in\nhigh-energy physics applications). In such scenarios, missing data must be\nimputed before applying any SBI method. We formalize the problem of missing\ndata in SBI and demonstrate that naive imputation methods can introduce bias in\nthe estimation of SBI posterior. We also introduce a novel amortized method\nthat addresses this issue by jointly learning the imputation model and the\ninference network within a neural posterior estimation (NPE) framework.\nExtensive empirical results on SBI benchmarks show that our approach provides\nrobust inference outcomes compared to standard baselines for varying levels of\nmissing data. Moreover, we demonstrate the merits of our imputation model on\ntwo real-world bioactivity datasets (Adrenergic and Kinase assays). Code is\navailable at https://github.com/Aalto-QuML/RISE."
    },
    {
        "date": "2025-03",
        "title": "Enhancing Network Security Management in Water Systems using FM-based Attack Attribution",
        "author": "Aleksandar Avdalovic, Joseph Khoury, Ahmad Taha, and Elias Bou-Harb",
        "link": "http://arxiv.org/abs/2503.01229v1",
        "abstract": "Water systems are vital components of modern infrastructure, yet they are\nincreasingly susceptible to sophisticated cyber attacks with potentially dire\nconsequences on public health and safety. While state-of-the-art machine\nlearning techniques effectively detect anomalies, contemporary model-agnostic\nattack attribution methods using LIME, SHAP, and LEMNA are deemed impractical\nfor large-scale, interdependent water systems. This is due to the intricate\ninterconnectivity and dynamic interactions that define these complex\nenvironments. Such methods primarily emphasize individual feature importance\nwhile falling short of addressing the crucial sensor-actuator interactions in\nwater systems, which limits their effectiveness in identifying root cause\nattacks. To this end, we propose a novel model-agnostic Factorization Machines\n(FM)-based approach that capitalizes on water system sensor-actuator\ninteractions to provide granular explanations and attributions for cyber\nattacks. For instance, an anomaly in an actuator pump activity can be\nattributed to a top root cause attack candidates, a list of water pressure\nsensors, which is derived from the underlying linear and quadratic effects\ncaptured by our approach. We validate our method using two real-world water\nsystem specific datasets, SWaT and WADI, demonstrating its superior performance\nover traditional attribution methods. In multi-feature cyber attack scenarios\ninvolving intricate sensor-actuator interactions, our FM-based attack\nattribution method effectively ranks attack root causes, achieving\napproximately 20% average improvement over SHAP and LEMNA."
    },
    {
        "date": "2025-03",
        "title": "ClipGrader: Leveraging Vision-Language Models for Robust Label Quality Assessment in Object Detection",
        "author": "Hong Lu, Yali Bian, and Rahul C. Shah",
        "link": "http://arxiv.org/abs/2503.02897v1",
        "abstract": "High-quality annotations are essential for object detection models, but\nensuring label accuracy - especially for bounding boxes - remains both\nchallenging and costly. This paper introduces ClipGrader, a novel approach that\nleverages vision-language models to automatically assess the accuracy of\nbounding box annotations. By adapting CLIP (Contrastive Language-Image\nPre-training) to evaluate both class label correctness and spatial precision of\nbounding box, ClipGrader offers an effective solution for grading object\ndetection labels. Tested on modified object detection datasets with\nartificially disturbed bounding boxes, ClipGrader achieves 91% accuracy on COCO\nwith a 1.8% false positive rate. Moreover, it maintains 87% accuracy with a\n2.1% false positive rate when trained on just 10% of the COCO data. ClipGrader\nalso scales effectively to larger datasets such as LVIS, achieving 79% accuracy\nacross 1,203 classes. Our experiments demonstrate ClipGrader's ability to\nidentify errors in existing COCO annotations, highlighting its potential for\ndataset refinement. When integrated into a semi-supervised object detection\n(SSOD) model, ClipGrader readily improves the pseudo label quality, helping\nachieve higher mAP (mean Average Precision) throughout the training process.\nClipGrader thus provides a scalable AI-assisted tool for enhancing annotation\nquality control and verifying annotations in large-scale object detection\ndatasets."
    },
    {
        "date": "2025-03",
        "title": "EasyCraft: A Robust and Efficient Framework for Automatic Avatar Crafting",
        "author": "Suzhen Wang, Weijie Chen, Wei Zhang, Minda Zhao, Lincheng Li, Rongsheng Zhang, Zhipeng Hu, and Xin Yu",
        "link": "http://arxiv.org/abs/2503.01158v1",
        "abstract": "Character customization, or 'face crafting,' is a vital feature in\nrole-playing games (RPGs), enhancing player engagement by enabling the creation\nof personalized avatars. Existing automated methods often struggle with\ngeneralizability across diverse game engines due to their reliance on the\nintermediate constraints of specific image domain and typically support only\none type of input, either text or image. To overcome these challenges, we\nintroduce EasyCraft, an innovative end-to-end feedforward framework that\nautomates character crafting by uniquely supporting both text and image inputs.\nOur approach employs a translator capable of converting facial images of any\nstyle into crafting parameters. We first establish a unified feature\ndistribution in the translator's image encoder through self-supervised learning\non a large-scale dataset, enabling photos of any style to be embedded into a\nunified feature representation. Subsequently, we map this unified feature\ndistribution to crafting parameters specific to a game engine, a process that\ncan be easily adapted to most game engines and thus enhances EasyCraft's\ngeneralizability. By integrating text-to-image techniques with our translator,\nEasyCraft also facilitates precise, text-based character crafting. EasyCraft's\nability to integrate diverse inputs significantly enhances the versatility and\naccuracy of avatar creation. Extensive experiments on two RPG games demonstrate\nthe effectiveness of our method, achieving state-of-the-art results and\nfacilitating adaptability across various avatar engines."
    },
    {
        "date": "2025-03",
        "title": "Adversarial Generative Flow Network for Solving Vehicle Routing Problems",
        "author": "Ni Zhang, Jingfeng Yang, Zhiguang Cao, and Xu Chi",
        "link": "http://arxiv.org/abs/2503.01931v1",
        "abstract": "Recent research into solving vehicle routing problems (VRPs) has gained\nsignificant traction, particularly through the application of deep\n(reinforcement) learning for end-to-end solution construction. However, many\ncurrent construction-based neural solvers predominantly utilize Transformer\narchitectures, which can face scalability challenges and struggle to produce\ndiverse solutions. To address these limitations, we introduce a novel framework\nbeyond Transformer-based approaches, i.e., Adversarial Generative Flow Networks\n(AGFN). This framework integrates the generative flow network (GFlowNet)-a\nprobabilistic model inherently adept at generating diverse solutions\n(routes)-with a complementary model for discriminating (or evaluating) the\nsolutions. These models are trained alternately in an adversarial manner to\nimprove the overall solution quality, followed by a proposed hybrid decoding\nmethod to construct the solution. We apply the AGFN framework to solve the\ncapacitated vehicle routing problem (CVRP) and travelling salesman problem\n(TSP), and our experimental results demonstrate that AGFN surpasses the popular\nconstruction-based neural solvers, showcasing strong generalization\ncapabilities on synthetic and real-world benchmark instances."
    },
    {
        "date": "2025-03",
        "title": "Hybrid Metaheuristic Vehicle Routing Problem for Security Dispatch Operations",
        "author": "Nguyen Gia Hien Vu, Yifan Tang, Rey Lim, and G. Gary Wang",
        "link": "http://arxiv.org/abs/2503.01121v1",
        "abstract": "This paper investigates the optimization of the Vehicle Routing Problem for\nSecurity Dispatch (VRPSD). VRPSD focuses on security and patrolling\napplications which involve challenging constraints including precise timing and\nstrict time windows. We propose three algorithms based on different\nmetaheuristics, which are Adaptive Large Neighborhood Search (ALNS), Tabu\nSearch (TS), and Threshold Accepting (TA). The first algorithm combines\nsingle-phase ALNS with TA, the second employs a multiphase ALNS with TA, and\nthe third integrates multiphase ALNS, TS, and TA. Experiments are conducted on\nan instance comprising 251 customer requests. The results demonstrate that the\nthird algorithm, the hybrid multiphase ALNS-TS-TA algorithm, delivers the best\nperformance. This approach simultaneously leverages the large-area search\ncapabilities of ALNS for exploration and effectively escapes local optima when\nthe multiphase ALNS is coupled with TS and TA. Furthermore, in our experiments,\nthe hybrid multiphase ALNS-TS-TA algorithm is the only one that shows potential\nfor improving results with increased computation time across all attempts."
    },
    {
        "date": "2025-03",
        "title": "Exploiting Vulnerabilities in Speech Translation Systems through Targeted Adversarial Attacks",
        "author": "Chang Liu, Haolin Wu, Xi Yang, Kui Zhang, Cong Wu, Weiming Zhang, Nenghai Yu, Tianwei Zhang, Qing Guo, and Jie Zhang",
        "link": "http://arxiv.org/abs/2503.00957v2",
        "abstract": "As speech translation (ST) systems become increasingly prevalent,\nunderstanding their vulnerabilities is crucial for ensuring robust and reliable\ncommunication. However, limited work has explored this issue in depth. This\npaper explores methods of compromising these systems through imperceptible\naudio manipulations. Specifically, we present two innovative approaches: (1)\nthe injection of perturbation into source audio, and (2) the generation of\nadversarial music designed to guide targeted translation, while also conducting\nmore practical over-the-air attacks in the physical world. Our experiments\nreveal that carefully crafted audio perturbations can mislead translation\nmodels to produce targeted, harmful outputs, while adversarial music achieve\nthis goal more covertly, exploiting the natural imperceptibility of music.\nThese attacks prove effective across multiple languages and translation models,\nhighlighting a systemic vulnerability in current ST architectures. The\nimplications of this research extend beyond immediate security concerns,\nshedding light on the interpretability and robustness of neural speech\nprocessing systems. Our findings underscore the need for advanced defense\nmechanisms and more resilient architectures in the realm of audio systems. More\ndetails and samples can be found at https://adv-st.github.io."
    },
    {
        "date": "2025-03",
        "title": "Improving the Transferability of Adversarial Attacks by an Input Transpose",
        "author": "Qing Wan, Shilong Deng, and Xun Wang",
        "link": "http://arxiv.org/abs/2503.00932v1",
        "abstract": "Deep neural networks (DNNs) are highly susceptible to adversarial\nexamples--subtle perturbations applied to inputs that are often imperceptible\nto humans yet lead to incorrect model predictions. In black-box scenarios,\nhowever, existing adversarial examples exhibit limited transferability and\nstruggle to effectively compromise multiple unseen DNN models. Previous\nstrategies enhance the cross-model generalization of adversarial examples by\nintroducing versatility into adversarial perturbations, thereby improving\ntransferability. However, further refining perturbation versatility often\ndemands intricate algorithm development and substantial computation\nconsumption. In this work, we propose an input transpose method that requires\nalmost no additional labor and computation costs but can significantly improve\nthe transferability of existing adversarial strategies. Even without adding\nadversarial perturbations, our method demonstrates considerable effectiveness\nin cross-model attacks. Our exploration finds that on specific datasets, a mere\n$1^\\circ$ left or right rotation might be sufficient for most adversarial\nexamples to deceive unseen models. Our further analysis suggests that this\ntransferability improvement triggered by rotating only $1^\\circ$ may stem from\nvisible pattern shifts in the DNN's low-level feature maps. Moreover, this\ntransferability exhibits optimal angles that, when identified under\nunrestricted query conditions, could potentially yield even greater\nperformance."
    },
    {
        "date": "2025-03",
        "title": "AMUN: Adversarial Machine UNlearning",
        "author": "Ali Ebrahimpour-Boroojeny, Hari Sundaram, and Varun Chandrasekaran",
        "link": "http://arxiv.org/abs/2503.00917v1",
        "abstract": "Machine unlearning, where users can request the deletion of a forget dataset,\nis becoming increasingly important because of numerous privacy regulations.\nInitial works on ``exact'' unlearning (e.g., retraining) incur large\ncomputational overheads. However, while computationally inexpensive,\n``approximate'' methods have fallen short of reaching the effectiveness of\nexact unlearning: models produced fail to obtain comparable accuracy and\nprediction confidence on both the forget and test (i.e., unseen) dataset.\nExploiting this observation, we propose a new unlearning method, Adversarial\nMachine UNlearning (AMUN), that outperforms prior state-of-the-art (SOTA)\nmethods for image classification. AMUN lowers the confidence of the model on\nthe forget samples by fine-tuning the model on their corresponding adversarial\nexamples. Adversarial examples naturally belong to the distribution imposed by\nthe model on the input space; fine-tuning the model on the adversarial examples\nclosest to the corresponding forget samples (a) localizes the changes to the\ndecision boundary of the model around each forget sample and (b) avoids drastic\nchanges to the global behavior of the model, thereby preserving the model's\naccuracy on test samples. Using AMUN for unlearning a random $10\\%$ of CIFAR-10\nsamples, we observe that even SOTA membership inference attacks cannot do\nbetter than random guessing."
    },
    {
        "date": "2025-03",
        "title": "DEAL: Data-Efficient Adversarial Learning for High-Quality Infrared Imaging",
        "author": "Zhu Liu, Zijun Wang, Jinyuan Liu, Fanqi Meng, Long Ma, and Risheng Liu",
        "link": "http://arxiv.org/abs/2503.00905v1",
        "abstract": "Thermal imaging is often compromised by dynamic, complex degradations caused\nby hardware limitations and unpredictable environmental factors. The scarcity\nof high-quality infrared data, coupled with the challenges of dynamic,\nintricate degradations, makes it difficult to recover details using existing\nmethods. In this paper, we introduce thermal degradation simulation integrated\ninto the training process via a mini-max optimization, by modeling these\ndegraded factors as adversarial attacks on thermal images. The simulation is\ndynamic to maximize objective functions, thus capturing a broad spectrum of\ndegraded data distributions. This approach enables training with limited data,\nthereby improving model performance.Additionally, we introduce a\ndual-interaction network that combines the benefits of spiking neural networks\nwith scale transformation to capture degraded features with sharp spike signal\nintensities. This architecture ensures compact model parameters while\npreserving efficient feature representation. Extensive experiments demonstrate\nthat our method not only achieves superior visual quality under diverse single\nand composited degradation, but also delivers a significant reduction in\nprocessing when trained on only fifty clear images, outperforming existing\ntechniques in efficiency and accuracy. The source code will be available at\nhttps://github.com/LiuZhu-CV/DEAL."
    },
    {
        "date": "2025-03",
        "title": "TAET: Two-Stage Adversarial Equalization Training on Long-Tailed Distributions",
        "author": "Wang YuHang, Junkang Guo, Aolei Liu, Kaihao Wang, Zaitong Wu, Zhenyu Liu, Wenfei Yin, and Jian Liu",
        "link": "http://arxiv.org/abs/2503.01924v1",
        "abstract": "Adversarial robustness is a critical challenge in deploying deep neural\nnetworks for real-world applications. While adversarial training is a widely\nrecognized defense strategy, most existing studies focus on balanced datasets,\noverlooking the prevalence of long-tailed distributions in real-world data,\nwhich significantly complicates robustness. This paper provides a comprehensive\nanalysis of adversarial training under long-tailed distributions and identifies\nlimitations in the current state-of-the-art method, AT-BSL, in achieving robust\nperformance under such conditions. To address these challenges, we propose a\nnovel training framework, TAET, which integrates an initial stabilization phase\nfollowed by a stratified equalization adversarial training phase. Additionally,\nprior work on long-tailed robustness has largely ignored the crucial evaluation\nmetric of balanced accuracy. To bridge this gap, we introduce the concept of\nbalanced robustness, a comprehensive metric tailored for assessing robustness\nunder long-tailed distributions. Extensive experiments demonstrate that our\nmethod surpasses existing advanced defenses, achieving significant improvements\nin both memory and computational efficiency. This work represents a substantial\nadvancement in addressing robustness challenges in real-world applications. Our\ncode is available at:\nhttps://github.com/BuhuiOK/TAET-Two-Stage-Adversarial-Equalization-Training-on-Long-Tailed-Distributions."
    },
    {
        "date": "2025-03",
        "title": "Revolutionizing Healthcare Record Management: Secure Documentation Storage and Access through Advanced Blockchain Solutions",
        "author": "Geeta N. Brijwani, Prafulla E Ajmire, Mohammad Atique Mohammad Junaid, Suhashini Awadhesh Charasia, and Deepali Bhende",
        "link": "http://arxiv.org/abs/2503.00742v1",
        "abstract": "Integrating blockchain technology into healthcare systems presents a\ntransformative approach to documenting, storing, and accessing electronic\nhealth records (EHRs). This research introduces a novel blockchain-based EHR\nsystem designed to significantly enhance security, scalability, and\naccessibility compared to existing solutions. Current systems primarily utilize\nSHA-256 for security and either IPFS or centralized storage, which, while\neffective, have limitations in providing comprehensive data integrity and\nsecurity. The proposed system leverages a hybrid security algorithm combining\nArgon2 and AES and integrates a hybrid storage and consensus mechanism\nutilizing IPFS and PBFT. This multifaceted approach ensures robust encryption,\nefficient consensus, and high fault tolerance. Furthermore, the system\nincorporates Multi-Factor Authentication (MFA) to safeguard against\nunauthorized access. It utilizes advanced blockchain tools like MetaMask,\nGanache, and Truffle to facilitate seamless interaction with the decentralized\nnetwork. Simulation results demonstrate that this system offers superior\nprotection against data breaches and enhances operational efficiency.\nSpecifically, the proposed hybrid model substantially improves data integrity,\nconsensus efficiency, fault tolerance, data availability, latency, bandwidth\nutilization, throughput, memory usage, and CPU usage across various healthcare\napplications. To validate the performance and security of the proposed system,\ncomprehensive analyses were conducted using real-world healthcare scenarios.\nThe findings highlight the significant advantages of the blockchain-based EHR\nsystem, emphasizing its potential to revolutionize healthcare data management\nby ensuring secure, reliable, and efficient handling of sensitive medical\ninformation."
    },
    {
        "date": "2025-03",
        "title": "Enhanced Security of Public Key Encryption with Certified Deletion",
        "author": "Xiaogang Cheng, and Ren Guo",
        "link": "http://arxiv.org/abs/2503.00719v1",
        "abstract": "In classical cryptography, certified deletion is simply impossible. Since\nclassical information can be copied any number of times easily. In quantum\ncryptography, certified deletion is possible because of theorems of quantum\nmechanics such as the quantum no-clone theorem, quantum superposition etc. In\nthis paper, we show the PKE-CD (Public Key Encryption with Certified Deletion)\nscheme constructed in by Bartusek and Khurana in CRYPTO 2023 lack an important\nsecurity property, which is important in practical applications. Then we show\nhow to enhance this property, and construct a concrete scheme with this\nproperty. And we also discuss the relations between PKE-CD and other quantum\ncryptographic schemes such as quantum seal, quantum bit commitment etc."
    },
    {
        "date": "2025-03",
        "title": "CATS: A framework for Cooperative Autonomy Trust & Security",
        "author": "Namo Asavisanu, Tina Khezresmaeilzadeh, Rohan Sequeira, Hang Qiu, Fawad Ahmad, Konstantinos Psounis, and Ramesh Govindan",
        "link": "http://arxiv.org/abs/2503.00659v1",
        "abstract": "With cooperative perception, autonomous vehicles can wirelessly share sensor\ndata and representations to overcome sensor occlusions, improving situational\nawareness. Securing such data exchanges is crucial for connected autonomous\nvehicles. Existing, automated reputation-based approaches often suffer from a\ndelay between detection and exclusion of misbehaving vehicles, while\nmajority-based approaches have communication overheads that limits scalability.\nIn this paper, we introduce CATS, a novel automated system that blends together\nthe best traits of reputation-based and majority-based detection mechanisms to\nsecure vehicle-to-everything (V2X) communications for cooperative perception,\nwhile preserving the privacy of cooperating vehicles. Our evaluation with\ncity-scale simulations on realistic traffic data shows CATS's effectiveness in\nrapidly identifying and isolating misbehaving vehicles, with a low false\nnegative rate and overheads, proving its suitability for real world\ndeployments."
    },
    {
        "date": "2025-03",
        "title": "Secure Aggregation in Federated Learning using Multiparty Homomorphic Encryption",
        "author": "Erfan Hosseini, Shuangyi Chen, and Ashish Khisti",
        "link": "http://arxiv.org/abs/2503.00581v1",
        "abstract": "A key operation in federated learning is the aggregation of gradient vectors\ngenerated by individual client nodes. We develop a method based on multiparty\nhomomorphic encryption (MPHE) that enables the central node to compute this\naggregate, while receiving only encrypted version of each individual gradients.\nTowards this end, we extend classical MPHE methods so that the decryption of\nthe aggregate vector can be successful even when only a subset of client nodes\nare available. This is accomplished by introducing a secret-sharing step during\nthe setup phase of MPHE when the public encryption key is generated. We develop\nconditions on the parameters of the MPHE scheme that guarantee correctness of\ndecryption and (computational) security. We explain how our method can be\nextended to accommodate client nodes that do not participate during the setup\nphase. We also propose a compression scheme for gradient vectors at each client\nnode that can be readily combined with our MPHE scheme and perform the\nassociated convergence analysis. We discuss the advantages of our proposed\nscheme with other approaches based on secure multi-party computation. Finally\nwe discuss a practical implementation of our system, compare the performance of\nour system with different approaches, and demonstrate that by suitably\ncombining compression with encryption the overhead over baseline schemes is\nrather small."
    },
    {
        "date": "2025-03",
        "title": "A Guide to Failure in Machine Learning: Reliability and Robustness from Foundations to Practice",
        "author": "Eric Heim, Oren Wright, and David Shriver",
        "link": "http://arxiv.org/abs/2503.00563v1",
        "abstract": "One of the main barriers to adoption of Machine Learning (ML) is that ML\nmodels can fail unexpectedly. In this work, we aim to provide practitioners a\nguide to better understand why ML models fail and equip them with techniques\nthey can use to reason about failure. Specifically, we discuss failure as\neither being caused by lack of reliability or lack of robustness.\nDifferentiating the causes of failure in this way allows us to formally define\nwhy models fail from first principles and tie these definitions to engineering\nconcepts and real-world deployment settings. Throughout the document we provide\n1) a summary of important theoretic concepts in reliability and robustness, 2)\na sampling current techniques that practitioners can utilize to reason about ML\nmodel reliability and robustness, and 3) examples that show how these concepts\nand techniques can apply to real-world settings."
    },
    {
        "date": "2025-03",
        "title": "Distributionally Robust Reinforcement Learning with Human Feedback",
        "author": "Debmalya Mandal, Paulius Sasnauskas, and Goran Radanovic",
        "link": "http://arxiv.org/abs/2503.00539v1",
        "abstract": "Reinforcement learning from human feedback (RLHF) has evolved to be one of\nthe main methods for fine-tuning large language models (LLMs). However,\nexisting RLHF methods are non-robust, and their performance deteriorates if the\ndownstream task differs significantly from the preference dataset used in\nfine-tuning. In order to mitigate this problem, we introduce a distributionally\nrobust RLHF for fine-tuning LLMs. In particular, our goal is to ensure that a\nfine-tuned model retains its performance even when the distribution of prompts\nsignificantly differs from the distribution encountered during fine-tuning. We\nformulate distributionally robust optimization (DRO) version of two popular\nfine-tuning methods -- (1) reward-based RLHF and (2) reward-free DPO (direct\npreference optimization). We propose a minibatch gradient descent based\nalgorithms for both of them, and theoretically prove convergence guarantees for\nthe algorithms. Subsequently, we evaluate our algorithms on an\nout-of-distribution (OOD) task by first training the model on the\nUnified-Feedback dataset and evaluating its performance on two different\ndatasets. The experimental results show that our robust training improves the\naccuracy of the learned reward models on average, and markedly on some tasks,\nsuch as reasoning. Furthermore, we show that the robust versions of policy\noptimization methods, similarly improve performance on OOD tasks."
    },
    {
        "date": "2025-03",
        "title": "SecRef*: Securely Sharing Mutable References Between Verified and Unverified Code in F*",
        "author": "Cezar-Constantin Andrici, Danel Ahman, Catalin Hritcu, Ruxandra Icleanu, Guido Mart\u00ednez, Exequiel Rivas, and Th\u00e9o Winterhalter",
        "link": "http://arxiv.org/abs/2503.00404v1",
        "abstract": "We introduce SecRef*, a secure compilation framework protecting stateful\nprograms verified in F* against linked unverified code, with which the program\ndynamically shares ML-style mutable references. To ease program verification in\nthis setting, we propose a way of tracking which references are shareable with\nthe unverified code, and which ones are not shareable and whose contents are\nthus guaranteed to be unchanged after calling into unverified code. This\nuniversal property of non-shareable references is exposed in the interface on\nwhich the verified program can rely when calling into unverified code. The\nremaining refinement types and pre- and post-conditions that the verified code\nexpects from the unverified code are converted into dynamic checks about the\nshared references by using higher-order contracts. We prove formally in F* that\nthis strategy ensures sound and secure interoperability with unverified code.\nSince SecRef* is built on top of the Monotonic State effect of F*, these proofs\nrely on the first monadic representation for this effect, which is a\ncontribution of our work that can be of independent interest. Finally, we use\nSecRef* to build a simple cooperative multi-threading scheduler that is\nverified and that securely interacts with unverified threads."
    },
    {
        "date": "2025-03",
        "title": "Inteval Analysis for two spherical functions arising from robust Perspective-n-Lines problem",
        "author": "Xiang Zheng, Haodong Jiang, and Junfeng Wu",
        "link": "http://arxiv.org/abs/2503.00400v1",
        "abstract": "This report presents a comprehensive interval analysis of two spherical\nfunctions derived from the robust Perspective-n-Lines (PnL) problem. The study\nis motivated by the application of a dimension-reduction technique to achieve\nglobal solutions for the robust PnL problem. We establish rigorous theoretical\nresults, supported by detailed proofs, and validate our findings through\nextensive numerical simulations."
    },
    {
        "date": "2025-03",
        "title": "A Survey of Adversarial Defenses in Vision-based Systems: Categorization, Methods and Challenges",
        "author": "Nandish Chattopadhyay, Abdul Basit, Bassem Ouni, and Muhammad Shafique",
        "link": "http://arxiv.org/abs/2503.00384v1",
        "abstract": "Adversarial attacks have emerged as a major challenge to the trustworthy\ndeployment of machine learning models, particularly in computer vision\napplications. These attacks have a varied level of potency and can be\nimplemented in both white box and black box approaches. Practical attacks\ninclude methods to manipulate the physical world and enforce adversarial\nbehaviour by the corresponding target neural network models. Multiple different\napproaches to mitigate different kinds of such attacks are available in the\nliterature, each with their own advantages and limitations. In this survey, we\npresent a comprehensive systematization of knowledge on adversarial defenses,\nfocusing on two key computer vision tasks: image classification and object\ndetection. We review the state-of-the-art adversarial defense techniques and\ncategorize them for easier comparison. In addition, we provide a schematic\nrepresentation of these categories within the context of the overall machine\nlearning pipeline, facilitating clearer understanding and benchmarking of\ndefenses. Furthermore, we map these defenses to the types of adversarial\nattacks and datasets where they are most effective, offering practical insights\nfor researchers and practitioners. This study is necessary for understanding\nthe scope of how the available defenses are able to address the adversarial\nthreats, and their shortcomings as well, which is necessary for driving the\nresearch in this area in the most appropriate direction, with the aim of\nbuilding trustworthy AI systems for regular practical use-cases."
    },
    {
        "date": "2025-03",
        "title": "Theoretical Insights in Model Inversion Robustness and Conditional Entropy Maximization for Collaborative Inference Systems",
        "author": "Song Xia, Yi Yu, Wenhan Yang, Meiwen Ding, Zhuo Chen, Lingyu Duan, Alex C. Kot, and Xudong Jiang",
        "link": "http://arxiv.org/abs/2503.00383v1",
        "abstract": "By locally encoding raw data into intermediate features, collaborative\ninference enables end users to leverage powerful deep learning models without\nexposure of sensitive raw data to cloud servers. However, recent studies have\nrevealed that these intermediate features may not sufficiently preserve\nprivacy, as information can be leaked and raw data can be reconstructed via\nmodel inversion attacks (MIAs). Obfuscation-based methods, such as noise\ncorruption, adversarial representation learning, and information filters,\nenhance the inversion robustness by obfuscating the task-irrelevant redundancy\nempirically. However, methods for quantifying such redundancy remain elusive,\nand the explicit mathematical relation between this redundancy minimization and\ninversion robustness enhancement has not yet been established. To address that,\nthis work first theoretically proves that the conditional entropy of inputs\ngiven intermediate features provides a guaranteed lower bound on the\nreconstruction mean square error (MSE) under any MIA. Then, we derive a\ndifferentiable and solvable measure for bounding this conditional entropy based\non the Gaussian mixture estimation and propose a conditional entropy\nmaximization (CEM) algorithm to enhance the inversion robustness. Experimental\nresults on four datasets demonstrate the effectiveness and adaptability of our\nproposed CEM; without compromising feature utility and computing efficiency,\nplugging the proposed CEM into obfuscation-based defense mechanisms\nconsistently boosts their inversion robustness, achieving average gains ranging\nfrom 12.9\\% to 48.2\\%. Code is available at\n\\href{https://github.com/xiasong0501/CEM}{https://github.com/xiasong0501/CEM}."
    },
    {
        "date": "2025-03",
        "title": "Adversarial Attacks on Event-Based Pedestrian Detectors: A Physical Approach",
        "author": "Guixu Lin, Muyao Niu, Qingtian Zhu, Zhengwei Yin, Zhuoxiao Li, Shengfeng He, and Yinqiang Zheng",
        "link": "http://arxiv.org/abs/2503.00377v1",
        "abstract": "Event cameras, known for their low latency and high dynamic range, show great\npotential in pedestrian detection applications. However, while recent research\nhas primarily focused on improving detection accuracy, the robustness of\nevent-based visual models against physical adversarial attacks has received\nlimited attention. For example, adversarial physical objects, such as specific\nclothing patterns or accessories, can exploit inherent vulnerabilities in these\nsystems, leading to misdetections or misclassifications. This study is the\nfirst to explore physical adversarial attacks on event-driven pedestrian\ndetectors, specifically investigating whether certain clothing patterns worn by\npedestrians can cause these detectors to fail, effectively rendering them\nunable to detect the person. To address this, we developed an end-to-end\nadversarial framework in the digital domain, framing the design of adversarial\nclothing textures as a 2D texture optimization problem. By crafting an\neffective adversarial loss function, the framework iteratively generates\noptimal textures through backpropagation. Our results demonstrate that the\ntextures identified in the digital domain possess strong adversarial\nproperties. Furthermore, we translated these digitally optimized textures into\nphysical clothing and tested them in real-world scenarios, successfully\ndemonstrating that the designed textures significantly degrade the performance\nof event-based pedestrian detection models. This work highlights the\nvulnerability of such models to physical adversarial attacks."
    },
    {
        "date": "2025-03",
        "title": "AI-Augmented Thyroid Scintigraphy for Robust Classification",
        "author": "Maziar Sabouri, Ghasem Hajianfar, Alireza Rafiei Sardouei, Milad Yazdani, Azin Asadzadeh, Soroush Bagheri, Mohsen Arabi, Seyed Rasoul Zakavi, Emran Askari, Atena Aghaee, Dena Shahriari, Habib Zaidi, and Arman Rahmim",
        "link": "http://arxiv.org/abs/2503.00366v1",
        "abstract": "Thyroid scintigraphy is a key imaging modality for diagnosing thyroid\ndisorders. Deep learning models for thyroid scintigraphy classification often\nface challenges due to limited and imbalanced datasets, leading to suboptimal\ngeneralization. In this study, we investigate the effectiveness of different\ndata augmentation techniques including Stable Diffusion (SD), Flow Matching\n(FM), and Conventional Augmentation (CA) to enhance the performance of a\nResNet18 classifier for thyroid condition classification. Our results showed\nthat FM-based augmentation consistently outperforms SD-based approaches,\nparticularly when combined with original (O) data and CA (O+FM+CA), achieving\nboth high accuracy and fair classification across Diffuse Goiter (DG), Nodular\nGoiter (NG), Normal (NL), and Thyroiditis (TI) cases. The Wilcoxon statistical\nanalysis further validated the superiority of O+FM and its variants (O+FM+CA)\nover SD-based augmentations in most scenarios. These findings highlight the\npotential of FM-based augmentation as a superior approach for generating\nhigh-quality synthetic thyroid scintigraphy images and improving model\ngeneralization in medical image classification."
    },
    {
        "date": "2025-03",
        "title": "CRUPL: A Semi-Supervised Cyber Attack Detection with Consistency Regularization and Uncertainty-aware Pseudo-Labeling in Smart Grid",
        "author": "Smruti P. Dash, Kedar V. Khandeparkar, and Nipun Agrawal",
        "link": "http://arxiv.org/abs/2503.00358v1",
        "abstract": "The modern power grids are integrated with digital technologies and\nautomation systems. The inclusion of digital technologies has made the smart\ngrids vulnerable to cyber-attacks. Cyberattacks on smart grids can compromise\ndata integrity and jeopardize the reliability of the power supply. Traditional\nintrusion detection systems often need help to effectively detect novel and\nsophisticated attacks due to their reliance on labeled training data, which may\nonly encompass part of the spectrum of potential threats. This work proposes a\nsemi-supervised method for cyber-attack detection in smart grids by leveraging\nthe labeled and unlabeled measurement data. We implement consistency\nregularization and pseudo-labeling to identify deviations from expected\nbehavior and predict the attack classes. We use a curriculum learning approach\nto improve pseudo-labeling performance, capturing the model uncertainty. We\ndemonstrate the efficiency of the proposed method in detecting different types\nof cyberattacks, minimizing the false positives by implementing them on\npublicly available datasets. The method proposes a promising solution by\nimproving the detection accuracy to 99% in the presence of unknown samples and\nsignificantly reducing false positives."
    },
    {
        "date": "2025-03",
        "title": "PINN-DT: Optimizing Energy Consumption in Smart Building Using Hybrid Physics-Informed Neural Networks and Digital Twin Framework with Blockchain Security",
        "author": "Hajar Kazemi Naeini, Roya Shomali, Abolhassan Pishahang, Hamidreza Hasanzadeh, Mahdieh Mohammadi, Saeid Asadi, and Ahmad Gholizadeh Lonbar",
        "link": "http://arxiv.org/abs/2503.00331v1",
        "abstract": "The advancement of smart grid technologies necessitates the integration of\ncutting-edge computational methods to enhance predictive energy optimization.\nThis study proposes a multi-faceted approach by incorporating (1) Deep\nReinforcement Learning (DRL) agents trained using data from Digital Twins (DTs)\nto optimize energy consumption in real time, (2) Physics-Informed Neural\nNetworks (PINNs) to seamlessly embed physical laws within the optimization\nprocess, ensuring model accuracy and interpretability, and (3) Blockchain (BC)\ntechnology to facilitate secure and transparent communication across the smart\ngrid infrastructure. The model was trained and validated using comprehensive\ndatasets, including smart meter energy consumption data, renewable energy\noutputs, dynamic pricing, and user preferences collected from IoT devices. The\nproposed framework achieved superior predictive performance with a Mean\nAbsolute Error (MAE) of 0.237 kWh, Root Mean Square Error (RMSE) of 0.298 kWh,\nand an R-squared (R2) value of 0.978, indicating a 97.8% explanation of data\nvariance. Classification metrics further demonstrated the model's robustness,\nachieving 97.7% accuracy, 97.8% precision, 97.6% recall, and an F1 Score of\n97.7%. Comparative analysis with traditional models like Linear Regression,\nRandom Forest, SVM, LSTM, and XGBoost revealed the superior accuracy and\nreal-time adaptability of the proposed method. In addition to enhancing energy\nefficiency, the model reduced energy costs by 35%, maintained a 96% user\ncomfort index, and increased renewable energy utilization to 40%. This study\ndemonstrates the transformative potential of integrating PINNs, DT, and\nBlockchain technologies to optimize energy consumption in smart grids, paving\nthe way for sustainable, secure, and efficient energy management systems."
    },
    {
        "date": "2025-03",
        "title": "CADRef: Robust Out-of-Distribution Detection via Class-Aware Decoupled Relative Feature Leveraging",
        "author": "Zhiwei Ling, Yachen Chang, Hailiang Zhao, Xinkui Zhao, Kingsum Chow, and Shuiguang Deng",
        "link": "http://arxiv.org/abs/2503.00325v1",
        "abstract": "Deep neural networks (DNNs) have been widely criticized for their\noverconfidence when dealing with out-of-distribution (OOD) samples,\nhighlighting the critical need for effective OOD detection to ensure the safe\ndeployment of DNNs in real-world settings. Existing post-hoc OOD detection\nmethods primarily enhance the discriminative power of logit-based approaches by\nreshaping sample features, yet they often neglect critical information inherent\nin the features themselves. In this paper, we propose the Class-Aware Relative\nFeature-based method (CARef), which utilizes the error between a sample's\nfeature and its class-aware average feature as a discriminative criterion. To\nfurther refine this approach, we introduce the Class-Aware Decoupled Relative\nFeature-based method (CADRef), which decouples sample features based on the\nalignment of signs between the relative feature and corresponding model\nweights, enhancing the discriminative capabilities of CARef. Extensive\nexperimental results across multiple datasets and models demonstrate that both\nproposed methods exhibit effectiveness and robustness in OOD detection compared\nto state-of-the-art methods. Specifically, our two methods outperform the best\nbaseline by 2.82% and 3.27% in AUROC, with improvements of 4.03% and 6.32% in\nFPR95, respectively."
    },
    {
        "date": "2025-03",
        "title": "Robust Multi-Objective Preference Alignment with Online DPO",
        "author": "Raghav Gupta, Ryan Sullivan, Yunxuan Li, Samrat Phatale, and Abhinav Rastogi",
        "link": "http://arxiv.org/abs/2503.00295v1",
        "abstract": "Multi-objective preference alignment of large language models (LLMs) is\ncritical for developing AI systems that are more configurable, personalizable,\nhelpful, and safe. However, optimizing model outputs to satisfy diverse\nobjectives with variable weights at inference time for truly personalized\nmodels presents a significant challenge. Existing approaches are either\ncomputationally expensive to train or do not sufficiently steer model\nbehaviors. This paper introduces the Multi-Objective Online DPO (MO-ODPO)\nalgorithm, designed to robustly and efficiently align model behaviors with\nmultiple, potentially conflicting human preferences. Our approach incorporates\na prompt conditioning mechanism, allowing us to train a single\npreference-conditional policy, that can adapt to new preference combinations at\ninference. Experiments on two popular benchmarks show that MO-ODPO\nPareto-dominates existing baselines while providing excellent inference-time\nsteerability between diverse objectives."
    },
    {
        "date": "2025-02",
        "title": "1-Lipschitz Network Initialization for Certifiably Robust Classification Applications: A Decay Problem",
        "author": "Marius F. R. Juston, William R. Norris, Dustin Nottage, and Ahmet Soylemezoglu",
        "link": "http://arxiv.org/abs/2503.00240v1",
        "abstract": "This paper discusses the weight parametrization of two standard 1-Lipschitz\nnetwork structure methodologies, the Almost-Orthogonal-Layers (AOL) and the\nSDP-based Lipschitz Layers (SLL), and derives their impact on the\ninitialization for deep 1-Lipschitz feedforward networks in addition to\ndiscussing underlying issues surrounding this initialization. These networks\nare mainly used in certifiably robust classification applications to combat\nadversarial attacks by limiting the effects of perturbations on the output\nclassification result. An exact and an upper bound for the parameterized weight\nvariance was calculated assuming a standard Normal distribution initialization;\nadditionally, an upper bound was computed assuming a Generalized Normal\nDistribution, generalizing the proof for Uniform, Laplace, and Normal\ndistribution weight initializations. It is demonstrated that the weight\nvariance holds no bearing on the output variance distribution and that only the\ndimension of the weight matrices matters. Additionally, this paper demonstrates\nthat the weight initialization always causes deep 1-Lipschitz networks to decay\nto zero."
    },
    {
        "date": "2025-02",
        "title": "Steering Dialogue Dynamics for Robustness against Multi-turn Jailbreaking Attacks",
        "author": "Hanjiang Hu, Alexander Robey, and Changliu Liu",
        "link": "http://arxiv.org/abs/2503.00187v1",
        "abstract": "Large language models (LLMs) are highly vulnerable to jailbreaking attacks,\nwherein adversarial prompts are designed to elicit harmful responses. While\nexisting defenses effectively mitigate single-turn attacks by detecting and\nfiltering unsafe inputs, they fail against multi-turn jailbreaks that exploit\ncontextual drift over multiple interactions, gradually leading LLMs away from\nsafe behavior. To address this challenge, we propose a safety steering\nframework grounded in safe control theory, ensuring invariant safety in\nmulti-turn dialogues. Our approach models the dialogue with LLMs using\nstate-space representations and introduces a novel neural barrier function\n(NBF) to detect and filter harmful queries emerging from evolving contexts\nproactively. Our method achieves invariant safety at each turn of dialogue by\nlearning a safety predictor that accounts for adversarial queries, preventing\npotential context drift toward jailbreaks. Extensive experiments under multiple\nLLMs show that our NBF-based safety steering outperforms safety alignment\nbaselines, offering stronger defenses against multi-turn jailbreaks while\nmaintaining a better trade-off between safety and helpfulness under different\nmulti-turn jailbreak methods. Our code is available at\nhttps://github.com/HanjiangHu/NBF-LLM ."
    },
    {
        "date": "2025-02",
        "title": "Transforming Cyber Defense: Harnessing Agentic and Frontier AI for Proactive, Ethical Threat Intelligence",
        "author": "Krti Tallam",
        "link": "http://arxiv.org/abs/2503.00164v1",
        "abstract": "In an era marked by unprecedented digital complexity, the cybersecurity\nlandscape is evolving at a breakneck pace, challenging traditional defense\nparadigms. Advanced Persistent Threats (APTs) reveal inherent vulnerabilities\nin conventional security measures and underscore the urgent need for\ncontinuous, adaptive, and proactive strategies that seamlessly integrate human\ninsight with cutting edge AI technologies. This manuscript explores how the\nconvergence of agentic AI and Frontier AI is transforming cybersecurity by\nreimagining frameworks such as the cyber kill chain, enhancing threat\nintelligence processes, and embedding robust ethical governance within\nautomated response systems. Drawing on real-world data and forward looking\nperspectives, we examine the roles of real time monitoring, automated incident\nresponse, and perpetual learning in forging a resilient, dynamic defense\necosystem. Our vision is to harmonize technological innovation with unwavering\nethical oversight, ensuring that future AI driven security solutions uphold\ncore human values of fairness, transparency, and accountability while\neffectively countering emerging cyber threats."
    },
    {
        "date": "2025-02",
        "title": "AMuLeT: Automated Design-Time Testing of Secure Speculation Countermeasures",
        "author": "Bo Fu, Leo Tenenbaum, David Adler, Assaf Klein, Arpit Gogia, Alaa R. Alameldeen, Marco Guarnieri, Mark Silberstein, Oleksii Oleksenko, and Gururaj Saileshwar",
        "link": "http://arxiv.org/abs/2503.00145v1",
        "abstract": "In recent years, several hardware-based countermeasures proposed to mitigate\nSpectre attacks have been shown to be insecure. To enable the development of\neffective secure speculation countermeasures, we need easy-to-use tools that\ncan automatically test their security guarantees early-on in the design phase\nto facilitate rapid prototyping. This paper develops AMuLeT, the first tool\ncapable of testing secure speculation countermeasures for speculative leakage\nearly in their design phase in simulators. Our key idea is to leverage\nmodel-based relational testing tools that can detect speculative leaks in\ncommercial CPUs, and apply them to micro-architectural simulators to test\nsecure speculation defenses. We identify and overcome several challenges,\nincluding designing an expressive yet realistic attacker observer model in a\nsimulator, overcoming the slow simulation speed, and searching the vast\nmicro-architectural state space for potential vulnerabilities. AMuLeT speeds up\ntest throughput by more than 10x compared to a naive design and uses techniques\nto amplify vulnerabilities to uncover them within a limited test budget. Using\nAMuLeT, we launch for the first time, a systematic, large-scale testing\ncampaign of four secure speculation countermeasures from 2018 to\n2024--InvisiSpec, CleanupSpec, STT, and SpecLFB--and uncover 3 known and 6\nunknown bugs and vulnerabilities, within 3 hours of testing. We also show for\nthe first time that the open-source implementation of SpecLFB is insecure."
    },
    {
        "date": "2025-02",
        "title": "Approaching the Harm of Gradient Attacks While Only Flipping Labels",
        "author": "Abdessamad El-Kabid, and El-Mahdi El-Mhamdi",
        "link": "http://arxiv.org/abs/2503.00140v1",
        "abstract": "Availability attacks are one of the strongest forms of training-phase attacks\nin machine learning, making the model unusable. While prior work in distributed\nML has demonstrated such effect via gradient attacks and, more recently, data\npoisoning, we ask: can similar damage be inflicted solely by flipping training\nlabels, without altering features? In this work, we introduce a novel\nformalization of label flipping attacks and derive an attacker-optimized loss\nfunction that better illustrates label flipping capabilities. To compare the\ndamaging effect of label flipping with that of gradient attacks, we use a\nsetting that allows us to compare their \\emph{writing power} on the ML model.\nOur contribution is threefold, (1) we provide the first evidence for an\navailability attack through label flipping alone, (2) we shed light on an\ninteresting interplay between what the attacker gains from more \\emph{write\naccess} versus what they gain from more \\emph{flipping budget} and (3) we\ncompare the power of targeted label flipping attack to that of an untargeted\nlabel flipping attack."
    },
    {
        "date": "2025-02",
        "title": "Enabling AutoML for Zero-Touch Network Security: Use-Case Driven Analysis",
        "author": "Li Yang, Mirna El Rajab, Abdallah Shami, and Sami Muhaidat",
        "link": "http://arxiv.org/abs/2502.21286v1",
        "abstract": "Zero-Touch Networks (ZTNs) represent a state-of-the-art paradigm shift\ntowards fully automated and intelligent network management, enabling the\nautomation and intelligence required to manage the complexity, scale, and\ndynamic nature of next-generation (6G) networks. ZTNs leverage Artificial\nIntelligence (AI) and Machine Learning (ML) to enhance operational efficiency,\nsupport intelligent decision-making, and ensure effective resource allocation.\nHowever, the implementation of ZTNs is subject to security challenges that need\nto be resolved to achieve their full potential. In particular, two critical\nchallenges arise: the need for human expertise in developing AI/ML-based\nsecurity mechanisms, and the threat of adversarial attacks targeting AI/ML\nmodels. In this survey paper, we provide a comprehensive review of current\nsecurity issues in ZTNs, emphasizing the need for advanced AI/ML-based security\nmechanisms that require minimal human intervention and protect AI/ML models\nthemselves. Furthermore, we explore the potential of Automated ML (AutoML)\ntechnologies in developing robust security solutions for ZTNs. Through case\nstudies, we illustrate practical approaches to securing ZTNs against both\nconventional and AI/ML-specific threats, including the development of\nautonomous intrusion detection systems and strategies to combat Adversarial ML\n(AML) attacks. The paper concludes with a discussion of the future research\ndirections for the development of ZTN security approaches."
    },
    {
        "date": "2025-02",
        "title": "QFAL: Quantum Federated Adversarial Learning",
        "author": "Walid El Maouaki, Nouhaila Innan, Alberto Marchisio, Taoufik Said, Mohamed Bennai, and Muhammad Shafique",
        "link": "http://arxiv.org/abs/2502.21171v1",
        "abstract": "Quantum federated learning (QFL) merges the privacy advantages of federated\nsystems with the computational potential of quantum neural networks (QNNs), yet\nits vulnerability to adversarial attacks remains poorly understood. This work\npioneers the integration of adversarial training into QFL, proposing a robust\nframework, quantum federated adversarial learning (QFAL), where clients\ncollaboratively defend against perturbations by combining local adversarial\nexample generation with federated averaging (FedAvg). We systematically\nevaluate the interplay between three critical factors: client count (5, 10,\n15), adversarial training coverage (0-100%), and adversarial attack\nperturbation strength (epsilon = 0.01-0.5), using the MNIST dataset. Our\nexperimental results show that while fewer clients often yield higher\nclean-data accuracy, larger federations can more effectively balance accuracy\nand robustness when partially adversarially trained. Notably, even limited\nadversarial coverage (e.g., 20%-50%) can significantly improve resilience to\nmoderate perturbations, though at the cost of reduced baseline performance.\nConversely, full adversarial training (100%) may regain high clean accuracy but\nis vulnerable under stronger attacks. These findings underscore an inherent\ntrade-off between robust and standard objectives, which is further complicated\nby quantum-specific factors. We conclude that a carefully chosen combination of\nclient count and adversarial coverage is critical for mitigating adversarial\nvulnerabilities in QFL. Moreover, we highlight opportunities for future\nresearch, including adaptive adversarial training schedules, more diverse\nquantum encoding schemes, and personalized defense strategies to further\nenhance the robustness-accuracy trade-off in real-world quantum federated\nenvironments."
    },
    {
        "date": "2025-02",
        "title": "Robust Deterministic Policy Gradient for Disturbance Attenuation and Its Application to Quadrotor Control",
        "author": "Taeho Lee, and Donghwan Lee",
        "link": "http://arxiv.org/abs/2502.21057v1",
        "abstract": "Practical control systems pose significant challenges in identifying optimal\ncontrol policies due to uncertainties in the system model and external\ndisturbances. While $H_\\infty$ control techniques are commonly used to design\nrobust controllers that mitigate the effects of disturbances, these methods\noften require complex and computationally intensive calculations. To address\nthis issue, this paper proposes a reinforcement learning algorithm called\nRobust Deterministic Policy Gradient (RDPG), which formulates the $H_\\infty$\ncontrol problem as a two-player zero-sum dynamic game. In this formulation, one\nplayer (the user) aims to minimize the cost, while the other player (the\nadversary) seeks to maximize it. We then employ deterministic policy gradient\n(DPG) and its deep reinforcement learning counterpart to train a robust control\npolicy with effective disturbance attenuation. In particular, for practical\nimplementation, we introduce an algorithm called robust deep deterministic\npolicy gradient (RDDPG), which employs a deep neural network architecture and\nintegrates techniques from the twin-delayed deep deterministic policy gradient\n(TD3) to enhance stability and learning efficiency. To evaluate the proposed\nalgorithm, we implement it on an unmanned aerial vehicle (UAV) tasked with\nfollowing a predefined path in a disturbance-prone environment. The\nexperimental results demonstrate that the proposed method outperforms other\ncontrol approaches in terms of robustness against disturbances, enabling\nprecise real-time tracking of moving targets even under severe disturbance\nconditions."
    },
    {
        "date": "2025-02",
        "title": "Data-free Universal Adversarial Perturbation with Pseudo-semantic Prior",
        "author": "Chanhui Lee, Yeonghwan Song, and Jeany Son",
        "link": "http://arxiv.org/abs/2502.21048v1",
        "abstract": "Data-free Universal Adversarial Perturbation (UAP) is an image-agnostic\nadversarial attack that deceives deep neural networks using a single\nperturbation generated solely from random noise, without any data priors.\nHowever, traditional data-free UAP methods often suffer from limited\ntransferability due to the absence of semantic information in random noise. To\naddress this, we propose a novel data-free universal attack approach that\ngenerates a pseudo-semantic prior recursively from the UAPs, enriching semantic\ncontents within the data-free UAP framework. Our method is based on the\nobservation that UAPs inherently contain latent semantic information, enabling\nthe generated UAP to act as an alternative data prior, by capturing a diverse\nrange of semantics through region sampling. We further introduce a sample\nreweighting technique to emphasize hard examples by focusing on samples that\nare less affected by the UAP. By leveraging the semantic information from the\npseudo-semantic prior, we also incorporate input transformations, typically\nineffective in data-free UAPs due to the lack of semantic content in random\npriors, to boost black-box transferability. Comprehensive experiments on\nImageNet show that our method achieves state-of-the-art performance in average\nfooling rate by a substantial margin, significantly improves attack\ntransferability across various CNN architectures compared to existing data-free\nUAP methods, and even surpasses data-dependent UAP methods."
    },
    {
        "date": "2025-02",
        "title": "Fast Adversarial Training against Sparse Attacks Requires Loss Smoothing",
        "author": "Xuyang Zhong, Yixiao Huang, and Chen Liu",
        "link": "http://arxiv.org/abs/2502.21041v1",
        "abstract": "This paper studies fast adversarial training against sparse adversarial\nperturbations bounded by $l_0$ norm. We demonstrate the challenges of employing\n$1$-step attacks on $l_0$ bounded perturbations for fast adversarial training,\nincluding degraded performance and the occurrence of catastrophic overfitting\n(CO). We highlight that CO in $l_0$ adversarial training is caused by\nsub-optimal perturbation locations of $1$-step attack. Theoretical and\nempirical analyses reveal that the loss landscape of $l_0$ adversarial training\nis more craggy compared to its $l_\\infty$, $l_2$ and $l_1$ counterparts.\nMoreover, we corroborate that the craggy loss landscape can aggravate CO. To\naddress these issues, we propose Fast-LS-$l_0$ that incorporates soft labels\nand the trade-off loss function to smooth the adversarial loss landscape.\nExtensive experiments demonstrate our method can overcome the challenge of\ncatastrophic overfitting, achieve state-of-the-art performance, and narrow down\nthe performance gap between $1$-step and multi-step adversarial training\nagainst sparse attacks."
    },
    {
        "date": "2025-02",
        "title": "Synthesizing Tabular Data Using Selectivity Enhanced Generative Adversarial Networks",
        "author": "Youran Zhou, and Jianzhong Qi",
        "link": "http://arxiv.org/abs/2502.21034v1",
        "abstract": "As E-commerce platforms face surging transactions during major shopping\nevents like Black Friday, stress testing with synthesized data is crucial for\nresource planning. Most recent studies use Generative Adversarial Networks\n(GANs) to generate tabular data while ensuring privacy and machine learning\nutility. However, these methods overlook the computational demands of\nprocessing GAN-generated data, making them unsuitable for E-commerce stress\ntesting.\n  This thesis introduces a novel GAN-based approach incorporating query\nselectivity constraints, a key factor in database transaction processing. We\nintegrate a pre-trained deep neural network to maintain selectivity consistency\nbetween real and synthetic data. Our method, tested on five real-world\ndatasets, outperforms three state-of-the-art GANs and a VAE model, improving\nselectivity estimation accuracy by up to 20pct and machine learning utility by\nup to 6 pct."
    },
    {
        "date": "2025-02",
        "title": "The RAG Paradox: A Black-Box Attack Exploiting Unintentional Vulnerabilities in Retrieval-Augmented Generation Systems",
        "author": "Chanwoo Choi, Jinsoo Kim, Sukmin Cho, Soyeong Jeong, and Buru Chang",
        "link": "http://arxiv.org/abs/2502.20995v1",
        "abstract": "With the growing adoption of retrieval-augmented generation (RAG) systems,\nrecent studies have introduced attack methods aimed at degrading their\nperformance. However, these methods rely on unrealistic white-box assumptions,\nsuch as attackers having access to RAG systems' internal processes. To address\nthis issue, we introduce a realistic black-box attack scenario based on the RAG\nparadox, where RAG systems inadvertently expose vulnerabilities while\nattempting to enhance trustworthiness. Because RAG systems reference external\ndocuments during response generation, our attack targets these sources without\nrequiring internal access. Our approach first identifies the external sources\ndisclosed by RAG systems and then automatically generates poisoned documents\nwith misinformation designed to match these sources. Finally, these poisoned\ndocuments are newly published on the disclosed sources, disrupting the RAG\nsystem's response generation process. Both offline and online experiments\nconfirm that this attack significantly reduces RAG performance without\nrequiring internal access. Furthermore, from an insider perspective within the\nRAG system, we propose a re-ranking method that acts as a fundamental\nsafeguard, offering minimal protection against unforeseen attacks."
    },
    {
        "date": "2025-02",
        "title": "Robust and Efficient Writer-Independent IMU-Based Handwriting Recognization",
        "author": "Jindong Li, Tim Hamann, Jens Barth, Peter Kaempf, Dario Zanca, and Bjoern Eskofier",
        "link": "http://arxiv.org/abs/2502.20954v1",
        "abstract": "Online handwriting recognition (HWR) using data from inertial measurement\nunits (IMUs) remains challenging due to variations in writing styles and the\nlimited availability of high-quality annotated datasets. Traditional models\noften struggle to recognize handwriting from unseen writers, making\nwriter-independent (WI) recognition a crucial but difficult problem. This paper\npresents an HWR model with an encoder-decoder structure for IMU data, featuring\na CNN-based encoder for feature extraction and a BiLSTM decoder for sequence\nmodeling, which supports inputs of varying lengths. Our approach demonstrates\nstrong robustness and data efficiency, outperforming existing methods on WI\ndatasets, including the WI split of the OnHW dataset and our own dataset.\nExtensive evaluations show that our model maintains high accuracy across\ndifferent age groups and writing conditions while effectively learning from\nlimited data. Through comprehensive ablation studies, we analyze key design\nchoices, achieving a balance between accuracy and efficiency. These findings\ncontribute to the development of more adaptable and scalable HWR systems for\nreal-world applications."
    },
    {
        "date": "2025-02",
        "title": "Concealed Adversarial attacks on neural networks for sequential data",
        "author": "Petr Sokerin, Dmitry Anikin, Sofia Krehova, and Alexey Zaytsev",
        "link": "http://arxiv.org/abs/2502.20948v1",
        "abstract": "The emergence of deep learning led to the broad usage of neural networks in\nthe time series domain for various applications, including finance and\nmedicine. While powerful, these models are prone to adversarial attacks: a\nbenign targeted perturbation of input data leads to significant changes in a\nclassifier's output. However, formally small attacks in the time series domain\nbecome easily detected by the human eye or a simple detector model.\n  We develop a concealed adversarial attack for different time-series models:\nit provides more realistic perturbations, being hard to detect by a human or\nmodel discriminator. To achieve this goal, the proposed adversarial attack\nmaximizes an aggregation of a classifier and a trained discriminator loss. To\nmake the attack stronger, we also propose a training procedure for a\ndiscriminator that provides broader coverage of possible attacks. Extensive\nbenchmarking on six UCR time series datasets across four diverse architectures\n- including recurrent, convolutional, state-space, and transformer-based models\n- demonstrates the superiority of our attack for a concealability-efficiency\ntrade-off. Our findings highlight the growing challenge of designing robust\ntime series models, emphasizing the need for improved defenses against\nrealistic and effective attacks."
    },
    {
        "date": "2025-02",
        "title": "BadRefSR: Backdoor Attacks Against Reference-based Image Super Resolution",
        "author": "Xue Yang, Tao Chen, Lei Guo, Wenbo Jiang, Ji Guo, Yongming Li, and Jiaming He",
        "link": "http://arxiv.org/abs/2502.20943v1",
        "abstract": "Reference-based image super-resolution (RefSR) represents a promising\nadvancement in super-resolution (SR). In contrast to single-image\nsuper-resolution (SISR), RefSR leverages an additional reference image to help\nrecover high-frequency details, yet its vulnerability to backdoor attacks has\nnot been explored. To fill this research gap, we propose a novel attack\nframework called BadRefSR, which embeds backdoors in the RefSR model by adding\ntriggers to the reference images and training with a mixed loss function.\nExtensive experiments across various backdoor attack settings demonstrate the\neffectiveness of BadRefSR. The compromised RefSR network performs normally on\nclean input images, while outputting attacker-specified target images on\ntriggered input images. Our study aims to alert researchers to the potential\nbackdoor risks in RefSR. Codes are available at\nhttps://github.com/xuefusiji/BadRefSR."
    },
    {
        "date": "2025-02",
        "title": "The Effect of Hop-count Modification Attack on Random Walk-based SLP Schemes Developed forWSNs: a Study",
        "author": "Manjula Rajaa, Anirban Ghoshb, Chukkapalli Praveen Kumarc, Suleiman Samba, and C N Shariff",
        "link": "http://arxiv.org/abs/2502.20902v1",
        "abstract": "Source location privacy (SLP) has been of great concern in WSNs when deployed\nfor habitat monitoring applications. The issue is taken care of by employing\nprivacy-preserving routing schemes. In the existing works, the attacker is\nassumed to be passive in nature and backtracks to the source of information by\neavesdropping the message signals. In this work, we try to understand the\nimpact of active attacks by proposing a new hybrid attack model consisting of\nboth active and passive attacks. The proposed model is then applied to three\nexisting TTL-based random walk SLP solutions: phantom routing scheme (PRS),\nsource location privacy using randomized routes (SLP-R), and\nposition-independent section-based scheme (PSSLP). The performance of the\nalgorithms in terms of privacy metrics is compared in the case of pure passive\nattack and hybrid attack of varying intensity. The results indicate a\nsignificant degradation in the privacy protection performance of the reference\nalgorithms in the face of the proposed hybrid attack model indicating the\nimportance and relevance of such attacks. It is further observed that the\nhybrid attack can be optimized to increase the vulnerability of the existing\nsolutions."
    },
    {
        "date": "2025-02",
        "title": "Cyber Defense Reinvented: Large Language Models as Threat Intelligence Copilots",
        "author": "Xiaoqun Liu, Jiacheng Liang, Qiben Yan, Muchao Ye, Jinyuan Jia, and Zhaohan Xi",
        "link": "http://arxiv.org/abs/2502.20791v1",
        "abstract": "The exponential growth of cyber threat knowledge, exemplified by the\nexpansion of databases such as MITRE-CVE and NVD, poses significant challenges\nfor cyber threat analysis. Security professionals are increasingly burdened by\nthe sheer volume and complexity of information, creating an urgent need for\neffective tools to navigate, synthesize, and act on large-scale data to counter\nevolving threats proactively. However, conventional threat intelligence tools\noften fail to scale with the dynamic nature of this data and lack the\nadaptability to support diverse threat intelligence tasks.\n  In this work, we introduce CYLENS, a cyber threat intelligence copilot\npowered by large language models (LLMs). CYLENS is designed to assist security\nprofessionals throughout the entire threat management lifecycle, supporting\nthreat attribution, contextualization, detection, correlation, prioritization,\nand remediation. To ensure domain expertise, CYLENS integrates knowledge from\n271,570 threat reports into its model parameters and incorporates six\nspecialized NLP modules to enhance reasoning capabilities. Furthermore, CYLENS\ncan be customized to meet the unique needs of different or ganizations,\nunderscoring its adaptability. Through extensive evaluations, we demonstrate\nthat CYLENS consistently outperforms industry-leading LLMs and state-of-the-art\ncybersecurity agents. By detailing its design, development, and evaluation,\nthis work provides a blueprint for leveraging LLMs to address complex,\ndata-intensive cybersecurity challenges."
    },
    {
        "date": "2025-02",
        "title": "The Common Objects Underwater (COU) Dataset for Robust Underwater Object Detection",
        "author": "Rishi Mukherjee, Sakshi Singh, Jack McWilliams, and Junaed Sattar",
        "link": "http://arxiv.org/abs/2502.20651v1",
        "abstract": "We introduce COU: Common Objects Underwater, an instance-segmented image\ndataset of commonly found man-made objects in multiple aquatic and marine\nenvironments. COU contains approximately 10K segmented images, annotated from\nimages collected during a number of underwater robot field trials in diverse\nlocations. COU has been created to address the lack of datasets with robust\nclass coverage curated for underwater instance segmentation, which is\nparticularly useful for training light-weight, real-time capable detectors for\nAutonomous Underwater Vehicles (AUVs). In addition, COU addresses the lack of\ndiversity in object classes since the commonly available underwater image\ndatasets focus only on marine life. Currently, COU contains images from both\nclosed-water (pool) and open-water (lakes and oceans) environments, of 24\ndifferent classes of objects including marine debris, dive tools, and AUVs. To\nassess the efficacy of COU in training underwater object detectors, we use\nthree state-of-the-art models to evaluate its performance and accuracy, using a\ncombination of standard accuracy and efficiency metrics. The improved\nperformance of COU-trained detectors over those solely trained on terrestrial\ndata demonstrates the clear advantage of training with annotated underwater\nimages. We make COU available for broad use under open-source licenses."
    },
    {
        "date": "2025-02",
        "title": "Gungnir: Exploiting Stylistic Features in Images for Backdoor Attacks on Diffusion Models",
        "author": "Yu Pan, Bingrong Dai, Jiahao Chen, Lin Wang, Yi Du, and Jiao Liu",
        "link": "http://arxiv.org/abs/2502.20650v1",
        "abstract": "In recent years, Diffusion Models (DMs) have demonstrated significant\nadvances in the field of image generation. However, according to current\nresearch, DMs are vulnerable to backdoor attacks, which allow attackers to\ncontrol the model's output by inputting data containing covert triggers, such\nas a specific patch or phrase. Existing defense strategies are well equipped to\nthwart such attacks through backdoor detection and trigger inversion because\nprevious attack methods are constrained by limited input spaces and triggers\ndefined by low-dimensional features. To bridge these gaps, we propose Gungnir,\na novel method that enables attackers to activate the backdoor in DMs through\nhidden style triggers within input images. Our approach proposes using\nstylistic features as triggers for the first time and implements backdoor\nattacks successfully in image2image tasks by utilizing\nReconstructing-Adversarial Noise (RAN) and Short-Term-Timesteps-Retention\n(STTR) of DMs. Meanwhile, experiments demonstrate that our method can easily\nbypass existing defense methods. Among existing DM main backdoor defense\nframeworks, our approach achieves a 0\\% backdoor detection rate (BDR). Our\ncodes are available at https://github.com/paoche11/Gungnir."
    },
    {
        "date": "2025-02",
        "title": "TractCloud-FOV: Deep Learning-based Robust Tractography Parcellation in Diffusion MRI with Incomplete Field of View",
        "author": "Yuqian Chen, Leo Zekelman, Yui Lo, Suheyla Cetin-Karayumak, Tengfei Xue, Yogesh Rathi, Nikos Makris, Fan Zhang, Weidong Cai, and Lauren J. O'Donnell",
        "link": "http://arxiv.org/abs/2502.20637v1",
        "abstract": "Tractography parcellation classifies streamlines reconstructed from diffusion\nMRI into anatomically defined fiber tracts for clinical and research\napplications. However, clinical scans often have incomplete fields of view\n(FOV) where brain regions are partially imaged, leading to partial or truncated\nfiber tracts. To address this challenge, we introduce TractCloud-FOV, a deep\nlearning framework that robustly parcellates tractography under conditions of\nincomplete FOV. We propose a novel training strategy, FOV-Cut Augmentation\n(FOV-CA), in which we synthetically cut tractograms to simulate a spectrum of\nreal-world inferior FOV cutoff scenarios. This data augmentation approach\nenriches the training set with realistic truncated streamlines, enabling the\nmodel to achieve superior generalization. We evaluate the proposed\nTractCloud-FOV on both synthetically cut tractography and two real-life\ndatasets with incomplete FOV. TractCloud-FOV significantly outperforms several\nstate-of-the-art methods on all testing datasets in terms of streamline\nclassification accuracy, generalization ability, tract anatomical depiction,\nand computational efficiency. Overall, TractCloud-FOV achieves efficient and\nconsistent tractography parcellation in diffusion MRI with incomplete FOV."
    },
    {
        "date": "2025-02",
        "title": "Towards Privacy-Preserving Split Learning: Destabilizing Adversarial Inference and Reconstruction Attacks in the Cloud",
        "author": "Griffin Higgins, Roozbeh Razavi-Far, Xichen Zhang, Amir David, Ali Ghorbani, and Tongyu Ge",
        "link": "http://arxiv.org/abs/2502.20629v1",
        "abstract": "This work aims to provide both privacy and utility within a split learning\nframework while considering both forward attribute inference and backward\nreconstruction attacks. To address this, a novel approach has been proposed,\nwhich makes use of class activation maps and autoencoders as a plug-in strategy\naiming to increase the user's privacy and destabilize an adversary. The\nproposed approach is compared with a dimensionality-reduction-based plug-in\nstrategy, which makes use of principal component analysis to transform the\nfeature map onto a lower-dimensional feature space. Our work shows that our\nproposed autoencoder-based approach is preferred as it can provide protection\nat an earlier split position over the tested architectures in our setting, and,\nhence, better utility for resource-constrained devices in edge-cloud\ncollaborative inference (EC) systems."
    },
    {
        "date": "2025-02",
        "title": "Towards Zero Touch Networks: Cross-Layer Automated Security Solutions for 6G Wireless Networks",
        "author": "Li Yang, Shimaa Naser, Abdallah Shami, Sami Muhaidat, Lyndon Ong, and M\u00e9rouane Debbah",
        "link": "http://arxiv.org/abs/2502.20627v1",
        "abstract": "The transition from 5G to 6G mobile networks necessitates network automation\nto meet the escalating demands for high data rates, ultra-low latency, and\nintegrated technology. Recently, Zero-Touch Networks (ZTNs), driven by\nArtificial Intelligence (AI) and Machine Learning (ML), are designed to\nautomate the entire lifecycle of network operations with minimal human\nintervention, presenting a promising solution for enhancing automation in 5G/6G\nnetworks. However, the implementation of ZTNs brings forth the need for\nautonomous and robust cybersecurity solutions, as ZTNs rely heavily on\nautomation. AI/ML algorithms are widely used to develop cybersecurity\nmechanisms, but require substantial specialized expertise and encounter model\ndrift issues, posing significant challenges in developing autonomous\ncybersecurity measures. Therefore, this paper proposes an automated security\nframework targeting Physical Layer Authentication (PLA) and Cross-Layer\nIntrusion Detection Systems (CLIDS) to address security concerns at multiple\nInternet protocol layers. The proposed framework employs drift-adaptive online\nlearning techniques and a novel enhanced Successive Halving (SH)-based\nAutomated ML (AutoML) method to automatically generate optimized ML models for\ndynamic networking environments. Experimental results illustrate that the\nproposed framework achieves high performance on the public Radio Frequency (RF)\nfingerprinting and the Canadian Institute for CICIDS2017 datasets, showcasing\nits effectiveness in addressing PLA and CLIDS tasks within dynamic and complex\nnetworking environments. Furthermore, the paper explores open challenges and\nresearch directions in the 5G/6G cybersecurity domain. This framework\nrepresents a significant advancement towards fully autonomous and secure 6G\nnetworks, paving the way for future innovations in network automation and\ncybersecurity."
    },
    {
        "date": "2025-02",
        "title": "Continuous Adversarial Text Representation Learning for Affective Recognition",
        "author": "Seungah Son, Andrez Saurez, and Dongsoo Har",
        "link": "http://arxiv.org/abs/2502.20613v1",
        "abstract": "While pre-trained language models excel at semantic understanding, they often\nstruggle to capture nuanced affective information critical for affective\nrecognition tasks. To address these limitations, we propose a novel framework\nfor enhancing emotion-aware embeddings in transformer-based models. Our\napproach introduces a continuous valence-arousal labeling system to guide\ncontrastive learning, which captures subtle and multi-dimensional emotional\nnuances more effectively. Furthermore, we employ a dynamic token perturbation\nmechanism, using gradient-based saliency to focus on sentiment-relevant tokens,\nimproving model sensitivity to emotional cues. The experimental results\ndemonstrate that the proposed framework outperforms existing methods, achieving\nup to 15.5% improvement in the emotion classification benchmark, highlighting\nthe importance of employing continuous labels. This improvement demonstrates\nthat the proposed framework is effective in affective representation learning\nand enables precise and contextually relevant emotional understanding."
    },
    {
        "date": "2025-02",
        "title": "Exploring the Impact of Temperature Scaling in Softmax for Classification and Adversarial Robustness",
        "author": "Hao Xuan, Bokai Yang, and Xingyu Li",
        "link": "http://arxiv.org/abs/2502.20604v1",
        "abstract": "The softmax function is a fundamental component in deep learning. This study\ndelves into the often-overlooked parameter within the softmax function, known\nas \"temperature,\" providing novel insights into the practical and theoretical\naspects of temperature scaling for image classification. Our empirical studies,\nadopting convolutional neural networks and transformers on multiple benchmark\ndatasets, reveal that moderate temperatures generally introduce better overall\nperformance. Through extensive experiments and rigorous theoretical analysis,\nwe explore the role of temperature scaling in model training and unveil that\ntemperature not only influences learning step size but also shapes the model's\noptimization direction. Moreover, for the first time, we discover a surprising\nbenefit of elevated temperatures: enhanced model robustness against common\ncorruption, natural perturbation, and non-targeted adversarial attacks like\nProjected Gradient Descent. We extend our discoveries to adversarial training,\ndemonstrating that, compared to the standard softmax function with the default\ntemperature value, higher temperatures have the potential to enhance\nadversarial training. The insights of this work open new avenues for improving\nmodel performance and security in deep learning applications."
    },
    {
        "date": "2025-02",
        "title": "LISArD: Learning Image Similarity to Defend Against Gray-box Adversarial Attacks",
        "author": "Joana C. Costa, Tiago Roxo, Hugo Proen\u00e7a, and Pedro R. M. In\u00e1cio",
        "link": "http://arxiv.org/abs/2502.20562v1",
        "abstract": "State-of-the-art defense mechanisms are typically evaluated in the context of\nwhite-box attacks, which is not realistic, as it assumes the attacker can\naccess the gradients of the target network. To protect against this scenario,\nAdversarial Training (AT) and Adversarial Distillation (AD) include adversarial\nexamples during the training phase, and Adversarial Purification uses a\ngenerative model to reconstruct all the images given to the classifier. This\npaper considers an even more realistic evaluation scenario: gray-box attacks,\nwhich assume that the attacker knows the architecture and the dataset used to\ntrain the target network, but cannot access its gradients. We provide empirical\nevidence that models are vulnerable to gray-box attacks and propose LISArD, a\ndefense mechanism that does not increase computational and temporal costs but\nprovides robustness against gray-box and white-box attacks without including\nAT. Our method approximates a cross-correlation matrix, created with the\nembeddings of perturbed and clean images, to a diagonal matrix while\nsimultaneously conducting classification learning. Our results show that LISArD\ncan effectively protect against gray-box attacks, can be used in multiple\narchitectures, and carries over its resilience to the white-box scenario. Also,\nstate-of-the-art AD models underperform greatly when removing AT and/or moving\nto gray-box settings, highlighting the lack of robustness from existing\napproaches to perform in various conditions (aside from white-box settings).\nAll the source code is available at https://github.com/Joana-Cabral/LISArD."
    },
    {
        "date": "2025-02",
        "title": "Robust Multicast Origin Authentication in MACsec and CANsec for Automotive Scenarios",
        "author": "Gianluca Cena, Lucia Seno, and Stefano Scanzio",
        "link": "http://arxiv.org/abs/2502.20555v1",
        "abstract": "Having everything interconnected through the Internet, including vehicle\nonboard systems, is making security a primary concern in the automotive domain\nas well. Although Ethernet and CAN XL provide link-level security based on\nsymmetric cryptography, they do not support origin authentication for multicast\ntransmissions. Asymmetric cryptography is unsuitable for networked embedded\ncontrol systems with real-time constraints and limited computational resources.\nIn these cases, solutions derived from the TESLA broadcast authentication\nprotocol may constitute a more suitable option.\n  In this paper, some such strategies are presented and analyzed that allow for\nmulticast origin authentication, also improving robustness to frame losses by\nmeans of interleaved keychains. A flexible authentication mechanism that relies\non a unified receiver is then proposed, which enables transmitters to select\nstrategies at runtime, to achieve the best compromise among security,\nreliability, and resource consumption."
    },
    {
        "date": "2025-02",
        "title": "In-Model Merging for Enhancing the Robustness of Medical Imaging Classification Models",
        "author": "Hu Wang, Ibrahim Almakky, Congbo Ma, Numan Saeed, and Mohammad Yaqub",
        "link": "http://arxiv.org/abs/2502.20516v1",
        "abstract": "Model merging is an effective strategy to merge multiple models for enhancing\nmodel performances, and more efficient than ensemble learning as it will not\nintroduce extra computation into inference. However, limited research explores\nif the merging process can occur within one model and enhance the model's\nrobustness, which is particularly critical in the medical image domain. In the\npaper, we are the first to propose in-model merging (InMerge), a novel approach\nthat enhances the model's robustness by selectively merging similar\nconvolutional kernels in the deep layers of a single convolutional neural\nnetwork (CNN) during the training process for classification. We also\nanalytically reveal important characteristics that affect how in-model merging\nshould be performed, serving as an insightful reference for the community. We\ndemonstrate the feasibility and effectiveness of this technique for different\nCNN architectures on 4 prevalent datasets. The proposed InMerge-trained model\nsurpasses the typically-trained model by a substantial margin. The code will be\nmade public."
    },
    {
        "date": "2025-02",
        "title": "Best Foot Forward: Robust Foot Reconstruction in-the-wild",
        "author": "Kyle Fogarty, Jing Yang, Chayan Kumar Patodi, Aadi Bhanti, Steven Chacko, Cengiz Oztireli, and Ujwal Bonde",
        "link": "http://arxiv.org/abs/2502.20511v1",
        "abstract": "Accurate 3D foot reconstruction is crucial for personalized orthotics,\ndigital healthcare, and virtual fittings. However, existing methods struggle\nwith incomplete scans and anatomical variations, particularly in self-scanning\nscenarios where user mobility is limited, making it difficult to capture areas\nlike the arch and heel. We present a novel end-to-end pipeline that refines\nStructure-from-Motion (SfM) reconstruction. It first resolves scan alignment\nambiguities using SE(3) canonicalization with a viewpoint prediction module,\nthen completes missing geometry through an attention-based network trained on\nsynthetically augmented point clouds. Our approach achieves state-of-the-art\nperformance on reconstruction metrics while preserving clinically validated\nanatomical fidelity. By combining synthetic training data with learned\ngeometric priors, we enable robust foot reconstruction under real-world capture\nconditions, unlocking new opportunities for mobile-based 3D scanning in\nhealthcare and retail."
    },
    {
        "date": "2025-02",
        "title": "HELENE: An Open-Source High-Security Privacy-Preserving Blockchain Based System for Automating and Managing Laboratory Health Tests",
        "author": "Gabriel Fern\u00e1ndez-Blanco, Pedro Garc\u00eda-Cereijo, David Lema-N\u00fa\u00f1ez, Diego Ramil-L\u00f3pez, Paula Fraga-Lamas, Leire Egia-Mendikute, As\u00eds Palaz\u00f3n, and Tiago M. Fern\u00e1ndez-Caram\u00e9s",
        "link": "http://arxiv.org/abs/2502.20477v1",
        "abstract": "In the last years, especially since the COVID-19 pandemic, precision medicine\nplatforms emerged as useful tools for supporting new tests like the ones that\ndetect the presence of antibodies and antigens with better sensitivity and\nspecificity than traditional methods. In addition, the pandemic has also\ninfluenced the way people interact (decentralization), behave (digital world)\nand purchase health services (online). Moreover, there is a growing concern in\nthe way health data are managed, especially in terms of privacy. To tackle such\nissues, this article presents a sustainable direct-to-consumer health-service\nopen-source platform called HELENE that is supported by blockchain and by a\nnovel decentralized oracle that protects patient data privacy. Specifically,\nHELENE enables health test providers to compete through auctions, allowing\npatients to bid for their services and to keep the control over their health\ntest results. Moreover, data exchanges among the involved stakeholders can be\nperformed in a trustworthy, transparent and standardized way to ease software\nintegration and to avoid incompatibilities. After providing a thorough\ndescription of the platform, the proposed health platform is assessed in terms\nof smart contract performance. In addition, the response time of the developed\noracle is evaluated and NIST SP 800-22 tests are executed to demonstrate the\nadequacy of the devised random number generator. Thus, this article shows the\ncapabilities and novel propositions of HELENE for delivering health services\nproviding an open-source platform for future researchers, who can enhance it\nand adapt it to their needs."
    },
    {
        "date": "2025-02",
        "title": "Why Are Web AI Agents More Vulnerable Than Standalone LLMs? A Security Analysis",
        "author": "Jeffrey Yang Fan Chiang, Seungjae Lee, Jia-Bin Huang, Furong Huang, and Yizheng Chen",
        "link": "http://arxiv.org/abs/2502.20383v1",
        "abstract": "Recent advancements in Web AI agents have demonstrated remarkable\ncapabilities in addressing complex web navigation tasks. However, emerging\nresearch shows that these agents exhibit greater vulnerability compared to\nstandalone Large Language Models (LLMs), despite both being built upon the same\nsafety-aligned models. This discrepancy is particularly concerning given the\ngreater flexibility of Web AI Agent compared to standalone LLMs, which may\nexpose them to a wider range of adversarial user inputs. To build a scaffold\nthat addresses these concerns, this study investigates the underlying factors\nthat contribute to the increased vulnerability of Web AI agents. Notably, this\ndisparity stems from the multifaceted differences between Web AI agents and\nstandalone LLMs, as well as the complex signals - nuances that simple\nevaluation metrics, such as success rate, often fail to capture. To tackle\nthese challenges, we propose a component-level analysis and a more granular,\nsystematic evaluation framework. Through this fine-grained investigation, we\nidentify three critical factors that amplify the vulnerability of Web AI\nagents; (1) embedding user goals into the system prompt, (2) multi-step action\ngeneration, and (3) observational capabilities. Our findings highlights the\npressing need to enhance security and robustness in AI agent design and provide\nactionable insights for targeted defense strategies."
    },
    {
        "date": "2025-02",
        "title": "Adversarial Robustness in Parameter-Space Classifiers",
        "author": "Tamir Shor, Ethan Fetaya, Chaim Baskin, and Alex Bronstein",
        "link": "http://arxiv.org/abs/2502.20314v1",
        "abstract": "Implicit Neural Representations (INRs) have been recently garnering\nincreasing interest in various research fields, mainly due to their ability to\nrepresent large, complex data in a compact and continuous manner. Past work\nfurther showed that numerous popular downstream tasks can be performed directly\nin the INR parameter-space. Doing so can substantially reduce the computational\nresources required to process the represented data in their native domain. A\nmajor difficulty in using modern machine-learning approaches, is their high\nsusceptibility to adversarial attacks, which have been shown to greatly limit\nthe reliability and applicability of such methods in a wide range of settings.\nIn this work, we show that parameter-space models trained for classification\nare inherently robust to adversarial attacks -- without the need of any robust\ntraining. To support our claims, we develop a novel suite of adversarial\nattacks targeting parameter-space classifiers, and furthermore analyze\npractical considerations of attacking parameter-space classifiers. Code for\nreproducing all experiments and implementation of all proposed methods will be\nreleased upon publication."
    },
    {
        "date": "2025-02",
        "title": "SecureGaze: Defending Gaze Estimation Against Backdoor Attacks",
        "author": "Lingyu Du, Yupei Liu, Jinyuan Jia, and Guohao Lan",
        "link": "http://arxiv.org/abs/2502.20306v1",
        "abstract": "Gaze estimation models are widely used in applications such as driver\nattention monitoring and human-computer interaction. While many methods for\ngaze estimation exist, they rely heavily on data-hungry deep learning to\nachieve high performance. This reliance often forces practitioners to harvest\ntraining data from unverified public datasets, outsource model training, or\nrely on pre-trained models. However, such practices expose gaze estimation\nmodels to backdoor attacks. In such attacks, adversaries inject backdoor\ntriggers by poisoning the training data, creating a backdoor vulnerability: the\nmodel performs normally with benign inputs, but produces manipulated gaze\ndirections when a specific trigger is present. This compromises the security of\nmany gaze-based applications, such as causing the model to fail in tracking the\ndriver's attention. To date, there is no defense that addresses backdoor\nattacks on gaze estimation models. In response, we introduce SecureGaze, the\nfirst solution designed to protect gaze estimation models from such attacks.\nUnlike classification models, defending gaze estimation poses unique challenges\ndue to its continuous output space and globally activated backdoor behavior. By\nidentifying distinctive characteristics of backdoored gaze estimation models,\nwe develop a novel and effective approach to reverse-engineer the trigger\nfunction for reliable backdoor detection. Extensive evaluations in both digital\nand physical worlds demonstrate that SecureGaze effectively counters a range of\nbackdoor attacks and outperforms seven state-of-the-art defenses adapted from\nclassification models."
    },
    {
        "date": "2025-02",
        "title": "Generative adversarial neural networks for simulating neutrino interactions",
        "author": "Jose L. Bonilla, Krzysztof M. Graczyk, Artur M. Ankowski, Rwik Dharmapal Banerjee, Beata E. Kowal, Hemant Prasad, and Jan T. Sobczyk",
        "link": "http://arxiv.org/abs/2502.20244v1",
        "abstract": "We propose a new approach to simulate neutrino scattering events as an\nalternative to the standard Monte Carlo generator approach. Generative\nadversarial neural network (GAN) models are developed to simulate\nneutrino-carbon collisions in the few-GeV energy range. The models produce\nscattering events for a given neutrino energy. GAN models are trained on\nsimulation data from NuWro Monte Carlo event generator. Two GAN models have\nbeen obtained: one simulating only quasielastic neutrino-nucleus scatterings\nand another simulating all interactions at given neutrino energy. The\nperformance of both models has been assessed using two statistical metrics. It\nis shown that both GAN models successfully reproduce the event distributions."
    },
    {
        "date": "2025-02",
        "title": "4Deform: Neural Surface Deformation for Robust Shape Interpolation",
        "author": "Lu Sang, Zehranaz Canfes, Dongliang Cao, Riccardo Marin, Florian Bernard, and Daniel Cremers",
        "link": "http://arxiv.org/abs/2502.20208v1",
        "abstract": "Generating realistic intermediate shapes between non-rigidly deformed shapes\nis a challenging task in computer vision, especially with unstructured data\n(e.g., point clouds) where temporal consistency across frames is lacking, and\ntopologies are changing. Most interpolation methods are designed for structured\ndata (i.e., meshes) and do not apply to real-world point clouds. In contrast,\nour approach, 4Deform, leverages neural implicit representation (NIR) to enable\nfree topology changing shape deformation. Unlike previous mesh-based methods\nthat learn vertex-based deformation fields, our method learns a continuous\nvelocity field in Euclidean space. Thus, it is suitable for less structured\ndata such as point clouds. Additionally, our method does not require\nintermediate-shape supervision during training; instead, we incorporate\nphysical and geometrical constraints to regularize the velocity field. We\nreconstruct intermediate surfaces using a modified level-set equation, directly\nlinking our NIR with the velocity field. Experiments show that our method\nsignificantly outperforms previous NIR approaches across various scenarios\n(e.g., noisy, partial, topology-changing, non-isometric shapes) and, for the\nfirst time, enables new applications like 4D Kinect sequence upsampling and\nreal-world high-resolution mesh deformation."
    },
    {
        "date": "2025-02",
        "title": "SSD: A State-based Stealthy Backdoor Attack For Navigation System in UAV Route Planning",
        "author": "Zhaoxuan Wang, Yang Li, Jie Zhang, Xingshuo Han, Kangbo Liu, Lyu Yang, yuan Zhou, Tianwei Zhang, and Quan Pan",
        "link": "http://arxiv.org/abs/2502.20178v1",
        "abstract": "Unmanned aerial vehicles (UAVs) are increasingly employed to perform\nhigh-risk tasks that require minimal human intervention. However, UAVs face\nescalating cybersecurity threats, particularly from GNSS spoofing attacks.\nWhile previous studies have extensively investigated the impacts of GNSS\nspoofing on UAVs, few have focused on its effects on specific tasks. Moreover,\nthe influence of UAV motion states on the assessment of network security risks\nis often overlooked. To address these gaps, we first provide a detailed\nevaluation of how motion states affect the effectiveness of network attacks. We\ndemonstrate that nonlinear motion states not only enhance the effectiveness of\nposition spoofing in GNSS spoofing attacks but also reduce the probability of\nspeed-related attack detection. Building upon this, we propose a\nstate-triggered backdoor attack method (SSD) to deceive GNSS systems and assess\nits risk to trajectory planning tasks. Extensive validation of SSD's\neffectiveness and stealthiness is conducted. Experimental results show that,\nwith appropriately tuned hyperparameters, SSD significantly increases\npositioning errors and the risk of task failure, while maintaining 100% stealth\nacross three state-of-the-art detectors."
    },
    {
        "date": "2025-02",
        "title": "Robust sensitivity control in digital pathology via tile score distribution matching",
        "author": "Arthur Pignet, John Klein, Genevieve Robin, and Antoine Olivier",
        "link": "http://arxiv.org/abs/2502.20144v2",
        "abstract": "Deploying digital pathology models across medical centers is challenging due\nto distribution shifts. Recent advances in domain generalization improve model\ntransferability in terms of aggregated performance measured by the Area Under\nCurve (AUC). However, clinical regulations often require to control the\ntransferability of other metrics, such as prescribed sensitivity levels. We\nintroduce a novel approach to control the sensitivity of whole slide image\n(WSI) classification models, based on optimal transport and Multiple Instance\nLearning (MIL). Validated across multiple cohorts and tasks, our method enables\nrobust sensitivity control with only a handful of calibration samples,\nproviding a practical solution for reliable deployment of computational\npathology systems."
    },
    {
        "date": "2025-02",
        "title": "PI-HMR: Towards Robust In-bed Temporal Human Shape Reconstruction with Contact Pressure Sensing",
        "author": "Ziyu Wu, Yufan Xiong, Mengting Niu, Fangting Xie, Quan Wan, Qijun Ying, Boyan Liu, and Xiaohui Cai",
        "link": "http://arxiv.org/abs/2503.00068v1",
        "abstract": "Long-term in-bed monitoring benefits automatic and real-time health\nmanagement within healthcare, and the advancement of human shape reconstruction\ntechnologies further enhances the representation and visualization of users'\nactivity patterns. However, existing technologies are primarily based on visual\ncues, facing serious challenges in non-light-of-sight and privacy-sensitive\nin-bed scenes. Pressure-sensing bedsheets offer a promising solution for\nreal-time motion reconstruction. Yet, limited exploration in model designs and\ndata have hindered its further development. To tackle these issues, we propose\na general framework that bridges gaps in data annotation and model design.\nFirstly, we introduce SMPLify-IB, an optimization method that overcomes the\ndepth ambiguity issue in top-view scenarios through gravity constraints,\nenabling generating high-quality 3D human shape annotations for in-bed\ndatasets. Then we present PI-HMR, a temporal-based human shape estimator to\nregress meshes from pressure sequences. By integrating multi-scale feature\nfusion with high-pressure distribution and spatial position priors, PI-HMR\noutperforms SOTA methods with 17.01mm Mean-Per-Joint-Error decrease. This work\nprovides a whole"
    },
    {
        "date": "2025-02",
        "title": "Order-Robust Class Incremental Learning: Graph-Driven Dynamic Similarity Grouping",
        "author": "Guannan Lai, Yujie Li, Xiangkun Wang, Junbo Zhang, Tianrui Li, and Xin Yang",
        "link": "http://arxiv.org/abs/2502.20032v1",
        "abstract": "Class Incremental Learning (CIL) requires a model to continuously learn new\nclasses without forgetting previously learned ones. While recent studies have\nsignificantly alleviated the problem of catastrophic forgetting (CF), more and\nmore research reveals that the order in which classes appear have significant\ninfluences on CIL models. Specifically, prioritizing the learning of classes\nwith lower similarity will enhance the model's generalization performance and\nits ability to mitigate forgetting. Hence, it is imperative to develop an\norder-robust class incremental learning model that maintains stable performance\neven when faced with varying levels of class similarity in different orders. In\nresponse, we first provide additional theoretical analysis, which reveals that\nwhen the similarity among a group of classes is lower, the model demonstrates\nincreased robustness to the class order. Then, we introduce a novel\n\\textbf{G}raph-\\textbf{D}riven \\textbf{D}ynamic \\textbf{S}imilarity\n\\textbf{G}rouping (\\textbf{GDDSG}) method, which leverages a graph coloring\nalgorithm for class-based similarity grouping. The proposed approach trains\nindependent CIL models for each group of classes, ultimately combining these\nmodels to facilitate joint prediction. Experimental results demonstrate that\nour method effectively addresses the issue of class order sensitivity while\nachieving optimal performance in both model accuracy and anti-forgetting\ncapability. Our code is available at https://github.com/AIGNLAI/GDDSG."
    },
    {
        "date": "2025-02",
        "title": "Modern DDoS Threats and Countermeasures: Insights into Emerging Attacks and Detection Strategies",
        "author": "Jincheng Wang, Le Yu, John C. S. Lui, and Xiapu Luo",
        "link": "http://arxiv.org/abs/2502.19996v1",
        "abstract": "Distributed Denial of Service (DDoS) attacks persist as significant threats\nto online services and infrastructure, evolving rapidly in sophistication and\neluding traditional detection mechanisms. This evolution demands a\ncomprehensive examination of current trends in DDoS attacks and the efficacy of\nmodern detection strategies. This paper offers an comprehensive survey of\nemerging DDoS attacks and detection strategies over the past decade. We delve\ninto the diversification of attack targets, extending beyond conventional web\nservices to include newer network protocols and systems, and the adoption of\nadvanced adversarial tactics. Additionally, we review current detection\ntechniques, highlighting essential features that modern systems must integrate\nto effectively neutralize these evolving threats. Given the technological\ndemands of contemporary network systems, such as high-volume and in-line packet\nprocessing capabilities, we also explore how innovative hardware technologies\nlike programmable switches can significantly enhance the development and\ndeployment of robust DDoS detection systems. We conclude by identifying open\nproblems and proposing future directions for DDoS research. In particular, our\nsurvey sheds light on the investigation of DDoS attack surfaces for emerging\nsystems, protocols, and adversarial strategies. Moreover, we outlines critical\nopen questions in the development of effective detection systems, e.g., the\ncreation of defense mechanisms independent of control planes."
    },
    {
        "date": "2025-02",
        "title": "ADAGE: Active Defenses Against GNN Extraction",
        "author": "Jing Xu, Franziska Boenisch, and Adam Dziedzic",
        "link": "http://arxiv.org/abs/2503.00065v1",
        "abstract": "Graph Neural Networks (GNNs) achieve high performance in various real-world\napplications, such as drug discovery, traffic states prediction, and\nrecommendation systems. The fact that building powerful GNNs requires a large\namount of training data, powerful computing resources, and human expertise\nturns the models into lucrative targets for model stealing attacks. Prior work\nhas revealed that the threat vector of stealing attacks against GNNs is large\nand diverse, as an attacker can leverage various heterogeneous signals ranging\nfrom node labels to high-dimensional node embeddings to create a local copy of\nthe target GNN at a fraction of the original training costs. This diversity in\nthe threat vector renders the design of effective and general defenses\nchallenging and existing defenses usually focus on one particular stealing\nsetup. Additionally, they solely provide means to identify stolen model copies\nrather than preventing the attack. To close this gap, we propose the first and\ngeneral Active Defense Against GNN Extraction (ADAGE). By analyzing the queries\nto the GNN, tracking their diversity in terms of proximity to different\ncommunities identified in the underlying graph, and increasing the defense\nstrength with the growing fraction of communities that have been queried, ADAGE\ncan prevent stealing in all common attack setups. Our extensive experimental\nevaluation using six benchmark datasets, four GNN models, and three types of\nadaptive attackers shows that ADAGE penalizes attackers to the degree of\nrendering stealing impossible, whilst not harming predictive performance for\nlegitimate users. ADAGE, thereby, contributes towards securely sharing valuable\nGNNs in the future."
    },
    {
        "date": "2025-02",
        "title": "ReCon: Enhancing True Correspondence Discrimination through Relation Consistency for Robust Noisy Correspondence Learning",
        "author": "Quanxing Zha, Xin Liu, Shu-Juan Peng, Yiu-ming Cheung, Xing Xu, and Nannan Wang",
        "link": "http://arxiv.org/abs/2502.19962v1",
        "abstract": "Can we accurately identify the true correspondences from multimodal datasets\ncontaining mismatched data pairs? Existing methods primarily emphasize the\nsimilarity matching between the representations of objects across modalities,\npotentially neglecting the crucial relation consistency within modalities that\nare particularly important for distinguishing the true and false\ncorrespondences. Such an omission often runs the risk of misidentifying\nnegatives as positives, thus leading to unanticipated performance degradation.\nTo address this problem, we propose a general Relation Consistency learning\nframework, namely ReCon, to accurately discriminate the true correspondences\namong the multimodal data and thus effectively mitigate the adverse impact\ncaused by mismatches. Specifically, ReCon leverages a novel relation\nconsistency learning to ensure the dual-alignment, respectively of, the\ncross-modal relation consistency between different modalities and the\nintra-modal relation consistency within modalities. Thanks to such dual\nconstrains on relations, ReCon significantly enhances its effectiveness for\ntrue correspondence discrimination and therefore reliably filters out the\nmismatched pairs to mitigate the risks of wrong supervisions. Extensive\nexperiments on three widely-used benchmark datasets, including Flickr30K,\nMS-COCO, and Conceptual Captions, are conducted to demonstrate the\neffectiveness and superiority of ReCon compared with other SOTAs. The code is\navailable at: https://github.com/qxzha/ReCon."
    },
    {
        "date": "2025-02",
        "title": "Dynamic DropConnect: Enhancing Neural Network Robustness through Adaptive Edge Dropping Strategies",
        "author": "Yuan-Chih Yang, and Hung-Hsuan Chen",
        "link": "http://arxiv.org/abs/2502.19948v1",
        "abstract": "Dropout and DropConnect are well-known techniques that apply a consistent\ndrop rate to randomly deactivate neurons or edges in a neural network layer\nduring training. This paper introduces a novel methodology that assigns dynamic\ndrop rates to each edge within a layer, uniquely tailoring the dropping process\nwithout incorporating additional learning parameters. We perform experiments on\nsynthetic and openly available datasets to validate the effectiveness of our\napproach. The results demonstrate that our method outperforms Dropout,\nDropConnect, and Standout, a classic mechanism known for its adaptive dropout\ncapabilities. Furthermore, our approach improves the robustness and\ngeneralization of neural network training without increasing computational\ncomplexity. The complete implementation of our methodology is publicly\naccessible for research and replication purposes at\nhttps://github.com/ericabd888/Adjusting-the-drop-probability-in-DropConnect-based-on-the-magnitude-of-the-gradient/."
    },
    {
        "date": "2025-02",
        "title": "Behind the Tip of Efficiency: Uncovering the Submerged Threats of Jailbreak Attacks in Small Language Models",
        "author": "Sibo Yi, Tianshuo Cong, Xinlei He, Qi Li, and Jiaxing Song",
        "link": "http://arxiv.org/abs/2502.19883v2",
        "abstract": "Small language models (SLMs) have become increasingly prominent in the\ndeployment on edge devices due to their high efficiency and low computational\ncost. While researchers continue to advance the capabilities of SLMs through\ninnovative training strategies and model compression techniques, the security\nrisks of SLMs have received considerably less attention compared to large\nlanguage models (LLMs).To fill this gap, we provide a comprehensive empirical\nstudy to evaluate the security performance of 13 state-of-the-art SLMs under\nvarious jailbreak attacks. Our experiments demonstrate that most SLMs are quite\nsusceptible to existing jailbreak attacks, while some of them are even\nvulnerable to direct harmful prompts.To address the safety concerns, we\nevaluate several representative defense methods and demonstrate their\neffectiveness in enhancing the security of SLMs. We further analyze the\npotential security degradation caused by different SLM techniques including\narchitecture compression, quantization, knowledge distillation, and so on. We\nexpect that our research can highlight the security challenges of SLMs and\nprovide valuable insights to future work in developing more robust and secure\nSLMs."
    },
    {
        "date": "2025-02",
        "title": "NoPain: No-box Point Cloud Attack via Optimal Transport Singular Boundary",
        "author": "Zezeng Li, Xiaoyu Du, Na Lei, Liming Chen, and Weimin Wang",
        "link": "http://arxiv.org/abs/2503.00063v2",
        "abstract": "Adversarial attacks exploit the vulnerability of deep models against\nadversarial samples. Existing point cloud attackers are tailored to specific\nmodels, iteratively optimizing perturbations based on gradients in either a\nwhite-box or black-box setting. Despite their promising attack performance,\nthey often struggle to produce transferable adversarial samples due to\noverfitting the specific parameters of surrogate models. To overcome this\nissue, we shift our focus to the data distribution itself and introduce a novel\napproach named NoPain, which employs optimal transport (OT) to identify the\ninherent singular boundaries of the data manifold for cross-network point cloud\nattacks. Specifically, we first calculate the OT mapping from noise to the\ntarget feature space, then identify singular boundaries by locating\nnon-differentiable positions. Finally, we sample along singular boundaries to\ngenerate adversarial point clouds. Once the singular boundaries are determined,\nNoPain can efficiently produce adversarial samples without the need of\niterative updates or guidance from the surrogate classifiers. Extensive\nexperiments demonstrate that the proposed end-to-end method outperforms\nbaseline approaches in terms of both transferability and efficiency, while also\nmaintaining notable advantages even against defense strategies. The source code\nwill be publicly available."
    },
    {
        "date": "2025-02",
        "title": "Snowball Adversarial Attack on Traffic Sign Classification",
        "author": "Anthony Etim, and Jakub Szefer",
        "link": "http://arxiv.org/abs/2502.19757v1",
        "abstract": "Adversarial attacks on machine learning models often rely on small,\nimperceptible perturbations to mislead classifiers. Such strategy focuses on\nminimizing the visual perturbation for humans so they are not confused, and\nalso maximizing the misclassification for machine learning algorithms. An\northogonal strategy for adversarial attacks is to create perturbations that are\nclearly visible but do not confuse humans, yet still maximize misclassification\nfor machine learning algorithms. This work follows the later strategy, and\ndemonstrates instance of it through the Snowball Adversarial Attack in the\ncontext of traffic sign recognition. The attack leverages the human brain's\nsuperior ability to recognize objects despite various occlusions, while machine\nlearning algorithms are easily confused. The evaluation shows that the Snowball\nAdversarial Attack is robust across various images and is able to confuse\nstate-of-the-art traffic sign recognition algorithm. The findings reveal that\nSnowball Adversarial Attack can significantly degrade model performance with\nminimal effort, raising important concerns about the vulnerabilities of deep\nneural networks and highlighting the necessity for improved defenses for image\nrecognition machine learning models."
    },
    {
        "date": "2025-02",
        "title": "HALO: Robust Out-of-Distribution Detection via Joint Optimisation",
        "author": "Hugo Lyons Keenan, Sarah Erfani, and Christopher Leckie",
        "link": "http://arxiv.org/abs/2502.19755v1",
        "abstract": "Effective out-of-distribution (OOD) detection is crucial for the safe\ndeployment of machine learning models in real-world scenarios. However, recent\nwork has shown that OOD detection methods are vulnerable to adversarial\nattacks, potentially leading to critical failures in high-stakes applications.\nThis discovery has motivated work on robust OOD detection methods that are\ncapable of maintaining performance under various attack settings. Prior\napproaches have made progress on this problem but face a number of limitations:\noften only exhibiting robustness to attacks on OOD data or failing to maintain\nstrong clean performance. In this work, we adapt an existing robust\nclassification framework, TRADES, extending it to the problem of robust OOD\ndetection and discovering a novel objective function. Recognising the critical\nimportance of a strong clean/robust trade-off for OOD detection, we introduce\nan additional loss term which boosts classification and detection performance.\nOur approach, called HALO (Helper-based AdversariaL OOD detection), surpasses\nexisting methods and achieves state-of-the-art performance across a number of\ndatasets and attack settings. Extensive experiments demonstrate an average\nAUROC improvement of 3.15 in clean settings and 7.07 under adversarial attacks\nwhen compared to the next best method. Furthermore, HALO exhibits resistance to\ntransferred attacks, offers tuneable performance through hyperparameter\nselection, and is compatible with existing OOD detection frameworks\nout-of-the-box, leaving open the possibility of future performance gains. Code\nis available at: https://github.com/hugo0076/HALO"
    },
    {
        "date": "2025-02",
        "title": "Adaptive Attacks Break Defenses Against Indirect Prompt Injection Attacks on LLM Agents",
        "author": "Qiusi Zhan, Richard Fang, Henil Shalin Panchal, and Daniel Kang",
        "link": "http://arxiv.org/abs/2503.00061v2",
        "abstract": "Large Language Model (LLM) agents exhibit remarkable performance across\ndiverse applications by using external tools to interact with environments.\nHowever, integrating external tools introduces security risks, such as indirect\nprompt injection (IPI) attacks. Despite defenses designed for IPI attacks,\ntheir robustness remains questionable due to insufficient testing against\nadaptive attacks. In this paper, we evaluate eight different defenses and\nbypass all of them using adaptive attacks, consistently achieving an attack\nsuccess rate of over 50%. This reveals critical vulnerabilities in current\ndefenses. Our research underscores the need for adaptive attack evaluation when\ndesigning defenses to ensure robustness and reliability. The code is available\nat https://github.com/uiuc-kang-lab/AdaptiveAttackAgent."
    },
    {
        "date": "2025-02",
        "title": "Tokens for Learning, Tokens for Unlearning: Mitigating Membership Inference Attacks in Large Language Models via Dual-Purpose Training",
        "author": "Toan Tran, Ruixuan Liu, and Li Xiong",
        "link": "http://arxiv.org/abs/2502.19726v1",
        "abstract": "Large language models (LLMs) have become the backbone of modern natural\nlanguage processing but pose privacy concerns about leaking sensitive training\ndata. Membership inference attacks (MIAs), which aim to infer whether a sample\nis included in a model's training dataset, can serve as a foundation for\nbroader privacy threats. Existing defenses designed for traditional\nclassification models do not account for the sequential nature of text data. As\na result, they either require significant computational resources or fail to\neffectively mitigate privacy risks in LLMs. In this work, we propose a\nlightweight yet effective empirical privacy defense for protecting training\ndata of language modeling by leveraging the token-specific characteristics. By\nanalyzing token dynamics during training, we propose a token selection strategy\nthat categorizes tokens into hard tokens for learning and memorized tokens for\nunlearning. Subsequently, our training-phase defense optimizes a novel\ndual-purpose token-level loss to achieve a Pareto-optimal balance between\nutility and privacy. Extensive experiments demonstrate that our approach not\nonly provides strong protection against MIAs but also improves language\nmodeling performance by around 10\\% across various LLM architectures and\ndatasets compared to the baselines."
    },
    {
        "date": "2025-02",
        "title": "SAP-DIFF: Semantic Adversarial Patch Generation for Black-Box Face Recognition Models via Diffusion Models",
        "author": "Mingsi Wang, Shuaiyin Yao, Chang Yue, Lijie Zhang, and Guozhu Meng",
        "link": "http://arxiv.org/abs/2502.19710v1",
        "abstract": "Given the need to evaluate the robustness of face recognition (FR) models,\nmany efforts have focused on adversarial patch attacks that mislead FR models\nby introducing localized perturbations. Impersonation attacks are a significant\nthreat because adversarial perturbations allow attackers to disguise themselves\nas legitimate users. This can lead to severe consequences, including data\nbreaches, system damage, and misuse of resources. However, research on such\nattacks in FR remains limited. Existing adversarial patch generation methods\nexhibit limited efficacy in impersonation attacks due to (1) the need for high\nattacker capabilities, (2) low attack success rates, and (3) excessive query\nrequirements. To address these challenges, we propose a novel method SAP-DIFF\nthat leverages diffusion models to generate adversarial patches via semantic\nperturbations in the latent space rather than direct pixel manipulation. We\nintroduce an attention disruption mechanism to generate features unrelated to\nthe original face, facilitating the creation of adversarial samples and a\ndirectional loss function to guide perturbations toward the target identity\nfeature space, thereby enhancing attack effectiveness and efficiency. Extensive\nexperiments on popular FR models and datasets demonstrate that our method\noutperforms state-of-the-art approaches, achieving an average attack success\nrate improvement of 45.66% (all exceeding 40%), and a reduction in the number\nof queries by about 40% compared to the SOTA approach"
    },
    {
        "date": "2025-02",
        "title": "Prompt-driven Transferable Adversarial Attack on Person Re-Identification with Attribute-aware Textual Inversion",
        "author": "Yuan Bian, Min Liu, Yunqi Yi, Xueping Wang, and Yaonan Wang",
        "link": "http://arxiv.org/abs/2502.19697v2",
        "abstract": "Person re-identification (re-id) models are vital in security surveillance\nsystems, requiring transferable adversarial attacks to explore the\nvulnerabilities of them. Recently, vision-language models (VLM) based attacks\nhave shown superior transferability by attacking generalized image and textual\nfeatures of VLM, but they lack comprehensive feature disruption due to the\noveremphasis on discriminative semantics in integral representation. In this\npaper, we introduce the Attribute-aware Prompt Attack (AP-Attack), a novel\nmethod that leverages VLM's image-text alignment capability to explicitly\ndisrupt fine-grained semantic features of pedestrian images by destroying\nattribute-specific textual embeddings. To obtain personalized textual\ndescriptions for individual attributes, textual inversion networks are designed\nto map pedestrian images to pseudo tokens that represent semantic embeddings,\ntrained in the contrastive learning manner with images and a predefined prompt\ntemplate that explicitly describes the pedestrian attributes. Inverted benign\nand adversarial fine-grained textual semantics facilitate attacker in\neffectively conducting thorough disruptions, enhancing the transferability of\nadversarial examples. Extensive experiments show that AP-Attack achieves\nstate-of-the-art transferability, significantly outperforming previous methods\nby 22.9% on mean Drop Rate in cross-model&dataset attack scenarios."
    },
    {
        "date": "2025-02",
        "title": "Unveiling Security Weaknesses in Autonomous Driving Systems: An In-Depth Empirical Study",
        "author": "Wenyuan Cheng, Zengyang Li, Peng Liang, Ran Mo, and Hui Liu",
        "link": "http://arxiv.org/abs/2502.19687v1",
        "abstract": "The advent of Autonomous Driving Systems (ADS) has marked a significant shift\ntowards intelligent transportation, with implications for public safety and\ntraffic efficiency. While these systems integrate a variety of technologies and\noffer numerous benefits, their security is paramount, as vulnerabilities can\nhave severe consequences for safety and trust. This study aims to\nsystematically investigate potential security weaknesses in the codebases of\nprominent open-source ADS projects using CodeQL, a static code analysis tool.\nThe goal is to identify common vulnerabilities, their distribution and\npersistence across versions to enhance the security of ADS. We selected three\nrepresentative open-source ADS projects, Autoware, AirSim, and Apollo, based on\ntheir high GitHub star counts and Level 4 autonomous driving capabilities.\nUsing CodeQL, we analyzed multiple versions of these projects to identify\nvulnerabilities, focusing on CWE categories such as CWE-190 (Integer Overflow\nor Wraparound) and CWE-20 (Improper Input Validation). We also tracked the\nlifecycle of these vulnerabilities across software versions. This approach\nallows us to systematically analyze vulnerabilities in projects, which has not\nbeen extensively explored in previous ADS research. Our analysis revealed that\nspecific CWE categories, particularly CWE-190 (59.6%) and CWE-20 (16.1%), were\nprevalent across the selected ADS projects. These vulnerabilities often\npersisted for over six months, spanning multiple version iterations. The\nempirical assessment showed a direct link between the severity of these\nvulnerabilities and their tangible effects on ADS performance. These security\nissues among ADS still remain to be resolved. Our findings highlight the need\nfor integrating static code analysis into ADS development to detect and\nmitigate common vulnerabilities."
    },
    {
        "date": "2025-02",
        "title": "Improving Adversarial Transferability in MLLMs via Dynamic Vision-Language Alignment Attack",
        "author": "Chenhe Gu, Jindong Gu, Andong Hua, and Yao Qin",
        "link": "http://arxiv.org/abs/2502.19672v1",
        "abstract": "Multimodal Large Language Models (MLLMs), built upon LLMs, have recently\ngained attention for their capabilities in image recognition and understanding.\nHowever, while MLLMs are vulnerable to adversarial attacks, the transferability\nof these attacks across different models remains limited, especially under\ntargeted attack setting. Existing methods primarily focus on vision-specific\nperturbations but struggle with the complex nature of vision-language modality\nalignment. In this work, we introduce the Dynamic Vision-Language Alignment\n(DynVLA) Attack, a novel approach that injects dynamic perturbations into the\nvision-language connector to enhance generalization across diverse\nvision-language alignment of different models. Our experimental results show\nthat DynVLA significantly improves the transferability of adversarial examples\nacross various MLLMs, including BLIP2, InstructBLIP, MiniGPT4, LLaVA, and\nclosed-source models such as Gemini."
    },
    {
        "date": "2025-02",
        "title": "Training Robust Graph Neural Networks by Modeling Noise Dependencies",
        "author": "Yeonjun In, Kanghoon Yoon, Sukwon Yun, Kibum Kim, Sungchul Kim, and Chanyoung Park",
        "link": "http://arxiv.org/abs/2502.19670v1",
        "abstract": "In real-world applications, node features in graphs often contain noise from\nvarious sources, leading to significant performance degradation in GNNs.\nAlthough several methods have been developed to enhance robustness, they rely\non the unrealistic assumption that noise in node features is independent of the\ngraph structure and node labels, thereby limiting their applicability. To this\nend, we introduce a more realistic noise scenario, dependency-aware noise on\ngraphs (DANG), where noise in node features create a chain of noise\ndependencies that propagates to the graph structure and node labels. We propose\na novel robust GNN, DA-GNN, which captures the causal relationships among\nvariables in the data generating process (DGP) of DANG using variational\ninference. In addition, we present new benchmark datasets that simulate DANG in\nreal-world applications, enabling more practical research on robust GNNs.\nExtensive experiments demonstrate that DA-GNN consistently outperforms existing\nbaselines across various noise scenarios, including both DANG and conventional\nnoise models commonly considered in this field."
    },
    {
        "date": "2025-02",
        "title": "Robust Gymnasium: A Unified Modular Benchmark for Robust Reinforcement Learning",
        "author": "Shangding Gu, Laixi Shi, Muning Wen, Ming Jin, Eric Mazumdar, Yuejie Chi, Adam Wierman, and Costas Spanos",
        "link": "http://arxiv.org/abs/2502.19652v1",
        "abstract": "Driven by inherent uncertainty and the sim-to-real gap, robust reinforcement\nlearning (RL) seeks to improve resilience against the complexity and\nvariability in agent-environment sequential interactions. Despite the existence\nof a large number of RL benchmarks, there is a lack of standardized benchmarks\nfor robust RL. Current robust RL policies often focus on a specific type of\nuncertainty and are evaluated in distinct, one-off environments. In this work,\nwe introduce Robust-Gymnasium, a unified modular benchmark designed for robust\nRL that supports a wide variety of disruptions across all key RL\ncomponents-agents' observed state and reward, agents' actions, and the\nenvironment. Offering over sixty diverse task environments spanning control and\nrobotics, safe RL, and multi-agent RL, it provides an open-source and\nuser-friendly tool for the community to assess current methods and foster the\ndevelopment of robust RL algorithms. In addition, we benchmark existing\nstandard and robust RL algorithms within this framework, uncovering significant\ndeficiencies in each and offering new insights."
    },
    {
        "date": "2025-02",
        "title": "Developing robust methods to handle missing data in real-world applications effectively",
        "author": "Youran Zhou, Mohamed Reda Bouadjenek, and Sunil Aryal",
        "link": "http://arxiv.org/abs/2502.19635v2",
        "abstract": "Missing data is a pervasive challenge spanning diverse data types, including\ntabular, sensor data, time-series, images and so on. Its origins are\nmultifaceted, resulting in various missing mechanisms. Prior research in this\nfield has predominantly revolved around the assumption of the Missing\nCompletely At Random (MCAR) mechanism. However, Missing At Random (MAR) and\nMissing Not At Random (MNAR) mechanisms, though equally prevalent, have often\nremained underexplored despite their significant influence. This PhD project\npresents a comprehensive research agenda designed to investigate the\nimplications of diverse missing data mechanisms. The principal aim is to devise\nrobust methodologies capable of effectively handling missing data while\naccommodating the unique characteristics of MCAR, MAR, and MNAR mechanisms. By\naddressing these gaps, this research contributes to an enriched understanding\nof the challenges posed by missing data across various industries and data\nmodalities. It seeks to provide practical solutions that enable the effective\nmanagement of missing data, empowering researchers and practitioners to\nleverage incomplete datasets confidently."
    },
    {
        "date": "2025-02",
        "title": "Unveiling Wireless Users' Locations via Modulation Classification-based Passive Attack",
        "author": "Ali Hanif, Abdulrahman Katranji, Nour Kouzayha, Muhammad Mahboob Ur Rahman, and Tareq Y. Al-Naffouri",
        "link": "http://arxiv.org/abs/2502.19341v1",
        "abstract": "The broadcast nature of the wireless medium and openness of wireless\nstandards, e.g., 3GPP releases 16-20, invite adversaries to launch various\nactive and passive attacks on cellular and other wireless networks. This work\nidentifies one such loose end of wireless standards and presents a novel\npassive attack method enabling an eavesdropper (Eve) to localize a line of\nsight wireless user (Bob) who is communicating with a base station or WiFi\naccess point (Alice). The proposed attack involves two phases. In the first\nphase, Eve performs modulation classification by intercepting the downlink\nchannel between Alice and Bob. This enables Eve to utilize the publicly\navailable modulation and coding scheme (MCS) tables to do pesudo-ranging, i.e.,\nthe Eve determines the ring within which Bob is located, which drastically\nreduces the search space. In the second phase, Eve sniffs the uplink channel,\nand employs multiple strategies to further refine Bob's location within the\nring. Towards the end, we present our thoughts on how this attack can be\nextended to non-line-of-sight scenarios, and how this attack could act as a\nscaffolding to construct a malicious digital twin map."
    },
    {
        "date": "2025-02",
        "title": "Multi-Agent Security Tax: Trading Off Security and Collaboration Capabilities in Multi-Agent Systems",
        "author": "Pierre Peigne-Lefebvre, Mikolaj Kniejski, Filip Sondej, Matthieu David, Jason Hoelscher-Obermaier, Christian Schroeder de Witt, and Esben Kran",
        "link": "http://arxiv.org/abs/2502.19145v1",
        "abstract": "As AI agents are increasingly adopted to collaborate on complex objectives,\nensuring the security of autonomous multi-agent systems becomes crucial. We\ndevelop simulations of agents collaborating on shared objectives to study these\nsecurity risks and security trade-offs. We focus on scenarios where an attacker\ncompromises one agent, using it to steer the entire system toward misaligned\noutcomes by corrupting other agents. In this context, we observe infectious\nmalicious prompts - the multi-hop spreading of malicious instructions. To\nmitigate this risk, we evaluated several strategies: two \"vaccination\"\napproaches that insert false memories of safely handling malicious input into\nthe agents' memory stream, and two versions of a generic safety instruction\nstrategy. While these defenses reduce the spread and fulfillment of malicious\ninstructions in our experiments, they tend to decrease collaboration capability\nin the agent network. Our findings illustrate potential trade-off between\nsecurity and collaborative efficiency in multi-agent systems, providing\ninsights for designing more secure yet effective AI collaborations."
    },
    {
        "date": "2025-02",
        "title": "XSS Adversarial Attacks Based on Deep Reinforcement Learning: A Replication and Extension Study",
        "author": "Samuele Pasini, Gianluca Maragliano, Jinhan Kim, and Paolo Tonella",
        "link": "http://arxiv.org/abs/2502.19095v1",
        "abstract": "Cross-site scripting (XSS) poses a significant threat to web application\nsecurity. While Deep Learning (DL) has shown remarkable success in detecting\nXSS attacks, it remains vulnerable to adversarial attacks due to the\ndiscontinuous nature of its input-output mapping. These adversarial attacks\nemploy mutation-based strategies for different components of XSS attack\nvectors, allowing adversarial agents to iteratively select mutations to evade\ndetection. Our work replicates a state-of-the-art XSS adversarial attack,\nhighlighting threats to validity in the reference work and extending it toward\na more effective evaluation strategy. Moreover, we introduce an XSS Oracle to\nmitigate these threats. The experimental results show that our approach\nachieves an escape rate above 96% when the threats to validity of the\nreplicated technique are addressed."
    },
    {
        "date": "2025-02",
        "title": "A Sample-Level Evaluation and Generative Framework for Model Inversion Attacks",
        "author": "Haoyang Li, Li Bai, Qingqing Ye, Haibo Hu, Yaxin Xiao, Huadi Zheng, and Jianliang Xu",
        "link": "http://arxiv.org/abs/2502.19070v1",
        "abstract": "Model Inversion (MI) attacks, which reconstruct the training dataset of\nneural networks, pose significant privacy concerns in machine learning. Recent\nMI attacks have managed to reconstruct realistic label-level private data, such\nas the general appearance of a target person from all training images labeled\non him. Beyond label-level privacy, in this paper we show sample-level privacy,\nthe private information of a single target sample, is also important but\nunder-explored in the MI literature due to the limitations of existing\nevaluation metrics. To address this gap, this study introduces a novel metric\ntailored for training-sample analysis, namely, the Diversity and Distance\nComposite Score (DDCS), which evaluates the reconstruction fidelity of each\ntraining sample by encompassing various MI attack attributes. This, in turn,\nenhances the precision of sample-level privacy assessments.\n  Leveraging DDCS as a new evaluative lens, we observe that many training\nsamples remain resilient against even the most advanced MI attack. As such, we\nfurther propose a transfer learning framework that augments the generative\ncapabilities of MI attackers through the integration of entropy loss and\nnatural gradient descent. Extensive experiments verify the effectiveness of our\nframework on improving state-of-the-art MI attacks over various metrics\nincluding DDCS, coverage and FID. Finally, we demonstrate that DDCS can also be\nuseful for MI defense, by identifying samples susceptible to MI attacks in an\nunsupervised manner."
    },
    {
        "date": "2025-02",
        "title": "Blending Optimal Control and Biologically Plausible Learning for Noise-Robust Physical Neural Networks",
        "author": "Satoshi Sunada, Tomoaki Niiyama, Kazutaka Kanno, Rin Nogami, Andr\u00e9 R\u00f6hm, Takato Awano, and Atsushi Uchida",
        "link": "http://arxiv.org/abs/2502.19053v1",
        "abstract": "The rapidly increasing computational demands for artificial intelligence (AI)\nhave spurred the exploration of computing principles beyond conventional\ndigital computers. Physical neural networks (PNNs) offer efficient neuromorphic\ninformation processing by harnessing the innate computational power of physical\nprocesses; however, training their weight parameters is computationally\nexpensive. We propose a training approach for substantially reducing this\ntraining cost. Our training approach merges an optimal control method for\ncontinuous-time dynamical systems with a biologically plausible training\nmethod--direct feedback alignment. In addition to the reduction of training\ntime, this approach achieves robust processing even under measurement errors\nand noise without requiring detailed system information. The effectiveness was\nnumerically and experimentally verified in an optoelectronic delay system. Our\napproach significantly extends the range of physical systems practically usable\nas PNNs."
    },
    {
        "date": "2025-02",
        "title": "A Dual-Purpose Framework for Backdoor Defense and Backdoor Amplification in Diffusion Models",
        "author": "Vu Tuan Truong, and Long Bao Le",
        "link": "http://arxiv.org/abs/2502.19047v2",
        "abstract": "Diffusion models have emerged as state-of-the-art generative frameworks,\nexcelling in producing high-quality multi-modal samples. However, recent\nstudies have revealed their vulnerability to backdoor attacks, where backdoored\nmodels generate specific, undesirable outputs called backdoor target (e.g.,\nharmful images) when a pre-defined trigger is embedded to their inputs. In this\npaper, we propose PureDiffusion, a dual-purpose framework that simultaneously\nserves two contrasting roles: backdoor defense and backdoor attack\namplification. For defense, we introduce two novel loss functions to invert\nbackdoor triggers embedded in diffusion models. The first leverages\ntrigger-induced distribution shifts across multiple timesteps of the diffusion\nprocess, while the second exploits the denoising consistency effect when a\nbackdoor is activated. Once an accurate trigger inversion is achieved, we\ndevelop a backdoor detection method that analyzes both the inverted trigger and\nthe generated backdoor targets to identify backdoor attacks. In terms of attack\namplification with the role of an attacker, we describe how our trigger\ninversion algorithm can be used to reinforce the original trigger embedded in\nthe backdoored diffusion model. This significantly boosts attack performance\nwhile reducing the required backdoor training time. Experimental results\ndemonstrate that PureDiffusion achieves near-perfect detection accuracy,\noutperforming existing defenses by a large margin, particularly against complex\ntrigger patterns. Additionally, in an attack scenario, our attack amplification\napproach elevates the attack success rate (ASR) of existing backdoor attacks to\nnearly 100\\% while reducing training time by up to 20x."
    },
    {
        "date": "2025-02",
        "title": "Beyond Surface-Level Patterns: An Essence-Driven Defense Framework Against Jailbreak Attacks in LLMs",
        "author": "Shiyu Xiang, Ansen Zhang, Yanfei Cao, Yang Fan, and Ronghao Chen",
        "link": "http://arxiv.org/abs/2502.19041v1",
        "abstract": "Although Aligned Large Language Models (LLMs) are trained to refuse harmful\nrequests, they remain vulnerable to jailbreak attacks. Unfortunately, existing\nmethods often focus on surface-level patterns, overlooking the deeper attack\nessences. As a result, defenses fail when attack prompts change, even though\nthe underlying \"attack essence\" remains the same. To address this issue, we\nintroduce EDDF, an \\textbf{E}ssence-\\textbf{D}riven \\textbf{D}efense\n\\textbf{F}ramework Against Jailbreak Attacks in LLMs. EDDF is a plug-and-play\ninput-filtering method and operates in two stages: 1) offline essence database\nconstruction, and 2) online adversarial query detection. The key idea behind\nEDDF is to extract the \"attack essence\" from a diverse set of known attack\ninstances and store it in an offline vector database. Experimental results\ndemonstrate that EDDF significantly outperforms existing methods by reducing\nthe Attack Success Rate by at least 20\\%, underscoring its superior robustness\nagainst jailbreak attacks."
    },
    {
        "date": "2025-02",
        "title": "Robust Over-the-Air Computation with Type-Based Multiple Access",
        "author": "Marc Martinez-Gost, Ana P\u00e9rez-Neira, and Miguel \u00c1ngel Lagunas",
        "link": "http://arxiv.org/abs/2502.19014v1",
        "abstract": "This paper utilizes the properties of type-based multiple access (TBMA) to\ninvestigate its effectiveness as a robust approach for over-the-air computation\n(AirComp) in the presence of Byzantine attacks, this is, adversarial strategies\nwhere malicious nodes intentionally distort their transmissions to corrupt the\naggregated result. Unlike classical direct aggregation (DA) AirComp, which\naggregates data in the amplitude of the signals and are highly vulnerable to\nattacks, TBMA distributes data over multiple radio resources, enabling the\nreceiver to construct a histogram representation of the transmitted data. This\nstructure allows the integration of classical robust estimators and supports\nthe computation of diverse functions beyond the arithmetic mean, which is not\nfeasible with DA. Through extensive simulations, we demonstrate that robust\nTBMA significantly outperforms DA, maintaining high accuracy even under\nadversarial conditions, and showcases its applicability in federated learning\n(FEEL) scenarios. Additionally, TBMA reduces channel state information (CSI)\nrequirements, lowers energy consumption, and enhances resiliency by leveraging\nthe diversity of the transmitted data. These results establish TBMA as a\nscalable and robust solution for AirComp, paving the way for secure and\nefficient aggregation in next-generation networks."
    },
    {
        "date": "2025-02",
        "title": "Evaluating Membership Inference Attacks in heterogeneous-data setups",
        "author": "Bram van Dartel, Marc Damie, and Florian Hahn",
        "link": "http://arxiv.org/abs/2502.18986v1",
        "abstract": "Among all privacy attacks against Machine Learning (ML), membership inference\nattacks (MIA) attracted the most attention. In these attacks, the attacker is\ngiven an ML model and a data point, and they must infer whether the data point\nwas used for training. The attacker also has an auxiliary dataset to tune their\ninference algorithm.\n  Attack papers commonly simulate setups in which the attacker's and the\ntarget's datasets are sampled from the same distribution. This setting is\nconvenient to perform experiments, but it rarely holds in practice. ML\nliterature commonly starts with similar simplifying assumptions (i.e., \"i.i.d.\"\ndatasets), and later generalizes the results to support heterogeneous data\ndistributions. Similarly, our work makes a first step in the generalization of\nthe MIA evaluation to heterogeneous data.\n  First, we design a metric to measure the heterogeneity between any pair of\ntabular data distributions. This metric provides a continuous scale to analyze\nthe phenomenon. Second, we compare two methodologies to simulate a data\nheterogeneity between the target and the attacker. These setups provide\nopposite performances: 90% attack accuracy vs. 50% (i.e., random guessing). Our\nresults show that the MIA accuracy depends on the experimental setup; and even\nif research on MIA considers heterogeneous data setups, we have no standardized\nbaseline of how to simulate it. The lack of such a baseline for MIA experiments\nposes a significant challenge to risk assessments in real-world machine\nlearning scenarios."
    },
    {
        "date": "2025-02",
        "title": "Invariance Pair-Guided Learning: Enhancing Robustness in Neural Networks",
        "author": "Martin Surner, Abdelmajid Khelil, and Ludwig Bothmann",
        "link": "http://arxiv.org/abs/2502.18975v1",
        "abstract": "Out-of-distribution generalization of machine learning models remains\nchallenging since the models are inherently bound to the training data\ndistribution. This especially manifests, when the learned models rely on\nspurious correlations. Most of the existing approaches apply data manipulation,\nrepresentation learning, or learning strategies to achieve generalizable\nmodels. Unfortunately, these approaches usually require multiple training\ndomains, group labels, specialized augmentation, or pre-processing to reach\ngeneralizable models. We propose a novel approach that addresses these\nlimitations by providing a technique to guide the neural network through the\ntraining phase. We first establish input pairs, representing the spurious\nattribute and describing the invariance, a characteristic that should not\naffect the outcome of the model. Based on these pairs, we form a corrective\ngradient complementing the traditional gradient descent approach. We further\nmake this correction mechanism adaptive based on a predefined invariance\ncondition. Experiments on ColoredMNIST, Waterbird-100, and CelebA datasets\ndemonstrate the effectiveness of our approach and the robustness to group\nshifts."
    },
    {
        "date": "2025-02",
        "title": "Switching multiplicative watermark design against covert attacks",
        "author": "Alexander J. Gallo, Sribalaji C. Anand, Andr\u00e9 M. H. Teixeira, and Riccardo M. G. Ferrari",
        "link": "http://arxiv.org/abs/2502.18948v1",
        "abstract": "Active techniques have been introduced to give better detectability\nperformance for cyber-attack diagnosis in cyber-physical systems (CPS). In this\npaper, switching multiplicative watermarking is considered, whereby we propose\nan optimal design strategy to define switching filter parameters. Optimality is\nevaluated exploiting the so-called output-to-output gain of the closed loop\nsystem, including some supposed attack dynamics. A worst-case scenario of a\nmatched covert attack is assumed, presuming that an attacker with full\nknowledge of the closed-loop system injects a stealthy attack of bounded\nenergy. Our algorithm, given watermark filter parameters at some time instant,\nprovides optimal next-step parameters. Analysis of the algorithm is given,\ndemonstrating its features, and demonstrating that through initialization of\ncertain parameters outside of the algorithm, the parameters of the\nmultiplicative watermarking can be randomized. Simulation shows how, by\nadopting our method for parameter design, the attacker's impact on performance\ndiminishes."
    },
    {
        "date": "2025-02",
        "title": "Towards Label-Only Membership Inference Attack against Pre-trained Large Language Models",
        "author": "Yu He, Boheng Li, Liu Liu, Zhongjie Ba, Wei Dong, Yiming Li, Zhan Qin, Kui Ren, and Chun Chen",
        "link": "http://arxiv.org/abs/2502.18943v1",
        "abstract": "Membership Inference Attacks (MIAs) aim to predict whether a data sample\nbelongs to the model's training set or not. Although prior research has\nextensively explored MIAs in Large Language Models (LLMs), they typically\nrequire accessing to complete output logits (\\ie, \\textit{logits-based\nattacks}), which are usually not available in practice. In this paper, we study\nthe vulnerability of pre-trained LLMs to MIAs in the \\textit{label-only\nsetting}, where the adversary can only access generated tokens (text). We first\nreveal that existing label-only MIAs have minor effects in attacking\npre-trained LLMs, although they are highly effective in inferring fine-tuning\ndatasets used for personalized LLMs. We find that their failure stems from two\nmain reasons, including better generalization and overly coarse perturbation.\nSpecifically, due to the extensive pre-training corpora and exposing each\nsample only a few times, LLMs exhibit minimal robustness differences between\nmembers and non-members. This makes token-level perturbations too coarse to\ncapture such differences.\n  To alleviate these problems, we propose \\textbf{PETAL}: a label-only\nmembership inference attack based on \\textbf{PE}r-\\textbf{T}oken\nsem\\textbf{A}ntic simi\\textbf{L}arity. Specifically, PETAL leverages\ntoken-level semantic similarity to approximate output probabilities and\nsubsequently calculate the perplexity. It finally exposes membership based on\nthe common assumption that members are `better' memorized and have smaller\nperplexity. We conduct extensive experiments on the WikiMIA benchmark and the\nmore challenging MIMIR benchmark. Empirically, our PETAL performs better than\nthe extensions of existing label-only attacks against personalized LLMs and\neven on par with other advanced logit-based attacks across all metrics on five\nprevalent open-source LLMs."
    },
    {
        "date": "2025-02",
        "title": "JailBench: A Comprehensive Chinese Security Assessment Benchmark for Large Language Models",
        "author": "Shuyi Liu, Simiao Cui, Haoran Bu, Yuming Shang, and Xi Zhang",
        "link": "http://arxiv.org/abs/2502.18935v1",
        "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities across\nvarious applications, highlighting the urgent need for comprehensive safety\nevaluations. In particular, the enhanced Chinese language proficiency of LLMs,\ncombined with the unique characteristics and complexity of Chinese expressions,\nhas driven the emergence of Chinese-specific benchmarks for safety assessment.\nHowever, these benchmarks generally fall short in effectively exposing LLM\nsafety vulnerabilities. To address the gap, we introduce JailBench, the first\ncomprehensive Chinese benchmark for evaluating deep-seated vulnerabilities in\nLLMs, featuring a refined hierarchical safety taxonomy tailored to the Chinese\ncontext. To improve generation efficiency, we employ a novel Automatic\nJailbreak Prompt Engineer (AJPE) framework for JailBench construction, which\nincorporates jailbreak techniques to enhance assessing effectiveness and\nleverages LLMs to automatically scale up the dataset through context-learning.\nThe proposed JailBench is extensively evaluated over 13 mainstream LLMs and\nachieves the highest attack success rate against ChatGPT compared to existing\nChinese benchmarks, underscoring its efficacy in identifying latent\nvulnerabilities in LLMs, as well as illustrating the substantial room for\nimprovement in the security and trustworthiness of LLMs within the Chinese\ncontext. Our benchmark is publicly available at\nhttps://github.com/STAIR-BUPT/JailBench."
    },
    {
        "date": "2025-02",
        "title": "PCE-GAN: A Generative Adversarial Network for Point Cloud Attribute Quality Enhancement based on Optimal Transport",
        "author": "Tian Guo, Hui Yuan, Qi Liu, Honglei Su, Raouf Hamzaoui, and Sam Kwong",
        "link": "http://arxiv.org/abs/2503.00047v1",
        "abstract": "Point cloud compression significantly reduces data volume but sacrifices\nreconstruction quality, highlighting the need for advanced quality enhancement\ntechniques. Most existing approaches focus primarily on point-to-point\nfidelity, often neglecting the importance of perceptual quality as interpreted\nby the human visual system. To address this issue, we propose a generative\nadversarial network for point cloud quality enhancement (PCE-GAN), grounded in\noptimal transport theory, with the goal of simultaneously optimizing both data\nfidelity and perceptual quality. The generator consists of a local feature\nextraction (LFE) unit, a global spatial correlation (GSC) unit and a feature\nsqueeze unit. The LFE unit uses dynamic graph construction and a graph\nattention mechanism to efficiently extract local features, placing greater\nemphasis on points with severe distortion. The GSC unit uses the geometry\ninformation of neighboring patches to construct an extended local neighborhood\nand introduces a transformer-style structure to capture long-range global\ncorrelations. The discriminator computes the deviation between the probability\ndistributions of the enhanced point cloud and the original point cloud, guiding\nthe generator to achieve high quality reconstruction. Experimental results show\nthat the proposed method achieves state-of-the-art performance. Specifically,\nwhen applying PCE-GAN to the latest geometry-based point cloud compression\n(G-PCC) test model, it achieves an average BD-rate of -19.2% compared with the\nPredLift coding configuration and -18.3% compared with the RAHT coding\nconfiguration. Subjective comparisons show a significant improvement in texture\nclarity and color transitions, revealing finer details and more natural color\ngradients."
    },
    {
        "date": "2025-02",
        "title": "Learning to Align Multi-Faceted Evaluation: A Unified and Robust Framework",
        "author": "Kaishuai Xu, Tiezheng Yu, Wenjun Hou, Yi Cheng, Liangyou Li, Xin Jiang, Lifeng Shang, Qun Liu, and Wenjie Li",
        "link": "http://arxiv.org/abs/2502.18874v2",
        "abstract": "Large Language Models (LLMs) are being used more and more extensively for\nautomated evaluation in various scenarios. Previous studies have attempted to\nfine-tune open-source LLMs to replicate the evaluation explanations and\njudgments of powerful proprietary models, such as GPT-4. However, these methods\nare largely limited to text-based analyses under predefined general criteria,\nresulting in reduced adaptability for unseen instructions and demonstrating\ninstability in evaluating adherence to quantitative and structural constraints.\nTo address these limitations, we propose a novel evaluation framework, ARJudge,\nthat adaptively formulates evaluation criteria and synthesizes both text-based\nand code-driven analyses to evaluate LLM responses. ARJudge consists of two\ncomponents: a fine-tuned Analyzer that generates multi-faceted evaluation\nanalyses and a tuning-free Refiner that combines and refines all analyses to\nmake the final judgment. We construct a Composite Analysis Corpus that\nintegrates tasks for evaluation criteria generation alongside text-based and\ncode-driven analysis generation to train the Analyzer. Our results demonstrate\nthat ARJudge outperforms existing fine-tuned evaluators in effectiveness and\nrobustness. Furthermore, it demonstrates the importance of multi-faceted\nevaluation and code-driven analyses in enhancing evaluation capabilities."
    },
    {
        "date": "2025-02",
        "title": "Adversarial Combinatorial Semi-bandits with Graph Feedback",
        "author": "Yuxiao Wen",
        "link": "http://arxiv.org/abs/2502.18826v2",
        "abstract": "In combinatorial semi-bandits, a learner repeatedly selects from a\ncombinatorial decision set of arms, receives the realized sum of rewards, and\nobserves the rewards of the individual selected arms as feedback. In this\npaper, we extend this framework to include \\emph{graph feedback}, where the\nlearner observes the rewards of all neighboring arms of the selected arms in a\nfeedback graph $G$. We establish that the optimal regret over a time horizon\n$T$ scales as $\\widetilde{\\Theta}(S\\sqrt{T}+\\sqrt{\\alpha ST})$, where $S$ is\nthe size of the combinatorial decisions and $\\alpha$ is the independence number\nof $G$. This result interpolates between the known regrets\n$\\widetilde\\Theta(S\\sqrt{T})$ under full information (i.e., $G$ is complete)\nand $\\widetilde\\Theta(\\sqrt{KST})$ under the semi-bandit feedback (i.e., $G$\nhas only self-loops), where $K$ is the total number of arms. A key technical\ningredient is to realize a convexified action using a random decision vector\nwith negative correlations."
    },
    {
        "date": "2025-02",
        "title": "Online Pseudo-average Shifting Attention(PASA) for Robust Low-precision LLM Inference: Algorithms and Numerical Analysis",
        "author": "Long Cheng, Qichen Liao, Fan Wu, Junlin Mu, Tengfei Han, Zhe Qiu, Lianqiang Li, Tianyi Liu, Fangzheng Miao, Keming Gao, Liang Wang, Zhen Zhang, and Qiande Yin",
        "link": "http://arxiv.org/abs/2503.01873v1",
        "abstract": "Attention calculation is extremely time-consuming for long-sequence inference\ntasks, such as text or image/video generation, in large models. To accelerate\nthis process, we developed a low-precision, mathematically-equivalent algorithm\ncalled PASA, based on Flash Attention. PASA introduces two novel techniques:\nonline pseudo-average shifting and global recovering. These techniques enable\nthe use of half-precision computation throughout the Flash Attention process\nwithout incurring overflow instability or unacceptable numerical accuracy loss.\nThis algorithm enhances performance on memory-restricted AI hardware\narchitectures, such as the Ascend Neural-network Processing Unit(NPU), by\nreducing data movement and increasing computational FLOPs. The algorithm is\nvalidated using both designed random benchmarks and real large models. We find\nthat the large bias and amplitude of attention input data are critical factors\ncontributing to numerical overflow ($>65504$ for half precision) in two\ndifferent categories of large models (Qwen2-7B language models and\nStable-Video-Diffusion multi-modal models). Specifically, overflow arises due\nto the large bias in the sequence dimension and the resonance mechanism between\nthe query and key in the head dimension of the Stable-Video-Diffusion models.\nThe resonance mechanism is defined as phase coincidence or 180-degree phase\nshift between query and key matrices. It will remarkably amplify the element\nvalues of attention score matrix. This issue also applies to the Qwen models.\nAdditionally, numerical accuracy is assessed through root mean square error\n(RMSE) and by comparing the final generated texts and videos to those produced\nusing high-precision attention."
    },
    {
        "date": "2025-02",
        "title": "Adversarial Universal Stickers: Universal Perturbation Attacks on Traffic Sign using Stickers",
        "author": "Anthony Etim, and Jakub Szefer",
        "link": "http://arxiv.org/abs/2502.18724v1",
        "abstract": "Adversarial attacks on deep learning models have proliferated in recent\nyears. In many cases, a different adversarial perturbation is required to be\nadded to each image to cause the deep learning model to misclassify it. This is\nineffective as each image has to be modified in a different way. Meanwhile,\nresearch on universal perturbations focuses on designing a single perturbation\nthat can be applied to all images in a data set, and cause a deep learning\nmodel to misclassify the images. This work advances the field of universal\nperturbations by exploring universal perturbations in the context of traffic\nsigns and autonomous vehicle systems. This work introduces a novel method for\ngenerating universal perturbations that visually look like simple black and\nwhite stickers, and using them to cause incorrect street sign predictions.\nUnlike traditional adversarial perturbations, the adversarial universal\nstickers are designed to be applicable to any street sign: same sticker, or\nstickers, can be applied in same location to any street sign and cause it to be\nmisclassified. Further, to enable safe experimentation with adversarial images\nand street signs, this work presents a virtual setting that leverages Street\nView images of street signs, rather than the need to physically modify street\nsigns, to test the attacks. The experiments in the virtual setting demonstrate\nthat these stickers can consistently mislead deep learning models used commonly\nin street sign recognition, and achieve high attack success rates on dataset of\nUS traffic signs. The findings highlight the practical security risks posed by\nsimple stickers applied to traffic signs, and the ease with which adversaries\ncan generate adversarial universal stickers that can be applied to many street\nsigns."
    },
    {
        "date": "2025-02",
        "title": "Target Defense with Multiple Defenders and an Agile Attacker via Residual Policy Learning",
        "author": "Jiyue Tao, Tongsheng Shen, Dexin Zhao, and Feitian Zhang",
        "link": "http://arxiv.org/abs/2502.18549v1",
        "abstract": "The target defense problem involves intercepting an attacker before it\nreaches a designated target region using one or more defenders. This letter\nfocuses on a particularly challenging scenario in which the attacker is more\nagile than the defenders, significantly increasing the difficulty of effective\ninterception. To address this challenge, we propose a novel residual policy\nframework that integrates deep reinforcement learning (DRL) with the\nforce-based Boids model. In this framework, the Boids model serves as a\nbaseline policy, while DRL learns a residual policy to refine and optimize the\ndefenders' actions. Simulation experiments demonstrate that the proposed method\nconsistently outperforms traditional interception policies, whether learned via\nvanilla DRL or fine-tuned from force-based methods. Moreover, the learned\npolicy exhibits strong scalability and adaptability, effectively handling\nscenarios with varying numbers of defenders and attackers with different\nagility levels."
    },
    {
        "date": "2025-02",
        "title": "Learning atomic forces from uncertainty-calibrated adversarial attacks",
        "author": "Henrique Musseli Cezar, Tilmann Bodenstein, Henrik Andersen Sveinsson, Morten Ledum, Simen Reine, and Sigbj\u00f8rn L\u00f8land Bore",
        "link": "http://arxiv.org/abs/2502.18314v2",
        "abstract": "Adversarial approaches, which intentionally challenge machine learning models\nby generating difficult examples, are increasingly being adopted to improve\nmachine learning interatomic potentials (MLIPs). While already providing great\npractical value, little is known about the actual prediction errors of MLIPs on\nadversarial structures and whether these errors can be controlled. We propose\nthe Calibrated Adversarial Geometry Optimization (CAGO) algorithm to discover\nadversarial structures with user-assigned errors. Through uncertainty\ncalibration, the estimated uncertainty of MLIPs is unified with real errors. By\nperforming geometry optimization for calibrated uncertainty, we reach\nadversarial structures with the user-assigned target MLIP prediction error.\nIntegrating with active learning pipelines, we benchmark CAGO, demonstrating\nstable MLIPs that systematically converge structural, dynamical, and\nthermodynamical properties for liquid water and water adsorption in a\nmetal-organic framework within only hundreds of training structures, where\npreviously many thousands were typically required."
    },
    {
        "date": "2025-02",
        "title": "Experimental Analysis of Efficiency of the Messaging Layer Security for Multiple Delivery Services",
        "author": "David Soler, Carlos Dafonte, Manuel Fern\u00e1ndez-Veiga, Ana Fern\u00e1ndez Vilas, and Francisco J. N\u00f3voa",
        "link": "http://arxiv.org/abs/2502.18303v1",
        "abstract": "Messaging Layer security (MLS) and its underlying Continuous Group Key\nAgreement (CGKA) protocol allows a group of users to share a cryptographic\nsecret in a dynamic manner, such that the secret is modified in member\ninsertions and deletions. One of the most relevant contributions of MLS is its\nefficiency, as its communication cost scales logarithmically with the number of\nmembers. However, this claim has only been analysed in theoretical models and\nthus it is unclear how efficient MLS is in real-world scenarios. Furthermore,\npractical decisions such as the chosen Delivery Service and paradigm can also\ninfluence the efficiency and evolution of an MLS group. In this work we analyse\nMLS from an empirical viewpoint: we provide real-world measurements for metrics\nsuch as commit generation and processing times and message sizes under\ndifferent conditions. In order to obtain these results we have developed a\nhighly configurable environment for empirical evaluations of MLS through the\nsimulation of MLS clients. Among other findings, our results show that\ncomputation costs scale linearly in practical scenarios even in the best-case\nscenario."
    },
    {
        "date": "2025-02",
        "title": "Stealthy Backdoor Attack in Self-Supervised Learning Vision Encoders for Large Vision Language Models",
        "author": "Zhaoyi Liu, and Huan Zhang",
        "link": "http://arxiv.org/abs/2502.18290v2",
        "abstract": "Self-supervised learning (SSL) vision encoders learn high-quality image\nrepresentations and thus have become a vital part of developing vision modality\nof large vision language models (LVLMs). Due to the high cost of training such\nencoders, pre-trained encoders are widely shared and deployed into many LVLMs,\nwhich are security-critical or bear societal significance. Under this practical\nscenario, we reveal a new backdoor threat that significant visual\nhallucinations can be induced into these LVLMs by merely compromising vision\nencoders. Because of the sharing and reuse of these encoders, many downstream\nLVLMs may inherit backdoor behaviors from encoders, leading to widespread\nbackdoors. In this work, we propose BadVision, the first method to exploit this\nvulnerability in SSL vision encoders for LVLMs with novel trigger optimization\nand backdoor learning techniques. We evaluate BadVision on two types of SSL\nencoders and LVLMs across eight benchmarks. We show that BadVision effectively\ndrives the LVLMs to attacker-chosen hallucination with over 99% attack success\nrate, causing a 77.6% relative visual understanding error while maintaining the\nstealthiness. SoTA backdoor detection methods cannot detect our attack\neffectively."
    },
    {
        "date": "2025-02",
        "title": "CLIPure: Purification in Latent Space via CLIP for Adversarially Robust Zero-Shot Classification",
        "author": "Mingkun Zhang, Keping Bi, Wei Chen, Jiafeng Guo, and Xueqi Cheng",
        "link": "http://arxiv.org/abs/2502.18176v2",
        "abstract": "In this paper, we aim to build an adversarially robust zero-shot image\nclassifier. We ground our work on CLIP, a vision-language pre-trained encoder\nmodel that can perform zero-shot classification by matching an image with text\nprompts ``a photo of a <class-name>.''. Purification is the path we choose\nsince it does not require adversarial training on specific attack types and\nthus can cope with any foreseen attacks. We then formulate purification risk as\nthe KL divergence between the joint distributions of the purification process\nof denoising the adversarial samples and the attack process of adding\nperturbations to benign samples, through bidirectional Stochastic Differential\nEquations (SDEs). The final derived results inspire us to explore purification\nin the multi-modal latent space of CLIP. We propose two variants for our\nCLIPure approach: CLIPure-Diff which models the likelihood of images' latent\nvectors with the DiffusionPrior module in DaLLE-2 (modeling the generation\nprocess of CLIP's latent vectors), and CLIPure-Cos which models the likelihood\nwith the cosine similarity between the embeddings of an image and ``a photo of\na.''. As far as we know, CLIPure is the first purification method in\nmulti-modal latent space and CLIPure-Cos is the first purification method that\nis not based on generative models, which substantially improves defense\nefficiency. We conducted extensive experiments on CIFAR-10, ImageNet, and 13\ndatasets that previous CLIP-based defense methods used for evaluating zero-shot\nclassification robustness. Results show that CLIPure boosts the SOTA robustness\nby a large margin, e.g., from 71.7% to 91.1% on CIFAR10, from 59.6% to 72.6% on\nImageNet, and 108% relative improvements of average robustness on the 13\ndatasets over previous SOTA. The code is available at\nhttps://github.com/TMLResearchGroup-CAS/CLIPure."
    },
    {
        "date": "2025-02",
        "title": "The Built-In Robustness of Decentralized Federated Averaging to Bad Data",
        "author": "Samuele Sabella, Chiara Boldrini, Lorenzo Valerio, Andrea Passarella, and Marco Conti",
        "link": "http://arxiv.org/abs/2502.18097v1",
        "abstract": "Decentralized federated learning (DFL) enables devices to collaboratively\ntrain models over complex network topologies without relying on a central\ncontroller. In this setting, local data remains private, but its quality and\nquantity can vary significantly across nodes. The extent to which a fully\ndecentralized system is vulnerable to poor-quality or corrupted data remains\nunclear, but several factors could contribute to potential risks. Without a\ncentral authority, there can be no unified mechanism to detect or correct\nerrors, and each node operates with a localized view of the data distribution,\nmaking it difficult for the node to assess whether its perspective aligns with\nthe true distribution. Moreover, models trained on low-quality data can\npropagate through the network, amplifying errors. To explore the impact of\nlow-quality data on DFL, we simulate two scenarios with degraded data quality\n-- one where the corrupted data is evenly distributed in a subset of nodes and\none where it is concentrated on a single node -- using a decentralized\nimplementation of FedAvg. Our results reveal that averaging-based decentralized\nlearning is remarkably robust to localized bad data, even when the corrupted\ndata resides in the most influential nodes of the network. Counterintuitively,\nthis robustness is further enhanced when the corrupted data is concentrated on\na single node, regardless of its centrality in the communication network\ntopology. This phenomenon is explained by the averaging process, which ensures\nthat no single node -- however central -- can disproportionately influence the\noverall learning process."
    },
    {
        "date": "2025-02",
        "title": "Model-Free Adversarial Purification via Coarse-To-Fine Tensor Network Representation",
        "author": "Guang Lin, Duc Thien Nguyen, Zerui Tao, Konstantinos Slavakis, Toshihisa Tanaka, and Qibin Zhao",
        "link": "http://arxiv.org/abs/2502.17972v1",
        "abstract": "Deep neural networks are known to be vulnerable to well-designed adversarial\nattacks. Although numerous defense strategies have been proposed, many are\ntailored to the specific attacks or tasks and often fail to generalize across\ndiverse scenarios. In this paper, we propose Tensor Network Purification (TNP),\na novel model-free adversarial purification method by a specially designed\ntensor network decomposition algorithm. TNP depends neither on the pre-trained\ngenerative model nor the specific dataset, resulting in strong robustness\nacross diverse adversarial scenarios. To this end, the key challenge lies in\nrelaxing Gaussian-noise assumptions of classical decompositions and\naccommodating the unknown distribution of adversarial perturbations. Unlike the\nlow-rank representation of classical decompositions, TNP aims to reconstruct\nthe unobserved clean examples from an adversarial example. Specifically, TNP\nleverages progressive downsampling and introduces a novel adversarial\noptimization objective to address the challenge of minimizing reconstruction\nerror but without inadvertently restoring adversarial perturbations. Extensive\nexperiments conducted on CIFAR-10, CIFAR-100, and ImageNet demonstrate that our\nmethod generalizes effectively across various norm threats, attack types, and\ntasks, providing a versatile and promising adversarial purification technique."
    },
    {
        "date": "2025-02",
        "title": "from Benign import Toxic: Jailbreaking the Language Model via Adversarial Metaphors",
        "author": "Yu Yan, Sheng Sun, Zenghao Duan, Teli Liu, Min Liu, Zhiyi Yin, Qi Li, and Jiangyu Lei",
        "link": "http://arxiv.org/abs/2503.00038v1",
        "abstract": "Current studies have exposed the risk of Large Language Models (LLMs)\ngenerating harmful content by jailbreak attacks. However, they overlook that\nthe direct generation of harmful content from scratch is more difficult than\ninducing LLM to calibrate benign content into harmful forms. In our study, we\nintroduce a novel attack framework that exploits AdVersArial meTAphoR (AVATAR)\nto induce the LLM to calibrate malicious metaphors for jailbreaking.\nSpecifically, to answer harmful queries, AVATAR adaptively identifies a set of\nbenign but logically related metaphors as the initial seed. Then, driven by\nthese metaphors, the target LLM is induced to reason and calibrate about the\nmetaphorical content, thus jailbroken by either directly outputting harmful\nresponses or calibrating residuals between metaphorical and professional\nharmful content. Experimental results demonstrate that AVATAR can effectively\nand transferable jailbreak LLMs and achieve a state-of-the-art attack success\nrate across multiple advanced LLMs."
    },
    {
        "date": "2025-02",
        "title": "Robust Polyp Detection and Diagnosis through Compositional Prompt-Guided Diffusion Models",
        "author": "Jia Yu, Yan Zhu, Peiyao Fu, Tianyi Chen, Junbo Huang, Quanlin Li, Pinghong Zhou, Zhihua Wang, Fei Wu, Shuo Wang, and Xian Yang",
        "link": "http://arxiv.org/abs/2502.17951v1",
        "abstract": "Colorectal cancer (CRC) is a significant global health concern, and early\ndetection through screening plays a critical role in reducing mortality. While\ndeep learning models have shown promise in improving polyp detection,\nclassification, and segmentation, their generalization across diverse clinical\nenvironments, particularly with out-of-distribution (OOD) data, remains a\nchallenge. Multi-center datasets like PolypGen have been developed to address\nthese issues, but their collection is costly and time-consuming. Traditional\ndata augmentation techniques provide limited variability, failing to capture\nthe complexity of medical images. Diffusion models have emerged as a promising\nsolution for generating synthetic polyp images, but the image generation\nprocess in current models mainly relies on segmentation masks as the condition,\nlimiting their ability to capture the full clinical context. To overcome these\nlimitations, we propose a Progressive Spectrum Diffusion Model (PSDM) that\nintegrates diverse clinical annotations-such as segmentation masks, bounding\nboxes, and colonoscopy reports-by transforming them into compositional prompts.\nThese prompts are organized into coarse and fine components, allowing the model\nto capture both broad spatial structures and fine details, generating\nclinically accurate synthetic images. By augmenting training data with\nPSDM-generated samples, our model significantly improves polyp detection,\nclassification, and segmentation. For instance, on the PolypGen dataset, PSDM\nincreases the F1 score by 2.12% and the mean average precision by 3.09%,\ndemonstrating superior performance in OOD scenarios and enhanced\ngeneralization."
    },
    {
        "date": "2025-02",
        "title": "Guiding not Forcing: Enhancing the Transferability of Jailbreaking Attacks on LLMs via Removing Superfluous Constraints",
        "author": "Junxiao Yang, Zhexin Zhang, Shiyao Cui, Hongning Wang, and Minlie Huang",
        "link": "http://arxiv.org/abs/2503.01865v1",
        "abstract": "Jailbreaking attacks can effectively induce unsafe behaviors in Large\nLanguage Models (LLMs); however, the transferability of these attacks across\ndifferent models remains limited. This study aims to understand and enhance the\ntransferability of gradient-based jailbreaking methods, which are among the\nstandard approaches for attacking white-box models. Through a detailed analysis\nof the optimization process, we introduce a novel conceptual framework to\nelucidate transferability and identify superfluous constraints-specifically,\nthe response pattern constraint and the token tail constraint-as significant\nbarriers to improved transferability. Removing these unnecessary constraints\nsubstantially enhances the transferability and controllability of\ngradient-based attacks. Evaluated on Llama-3-8B-Instruct as the source model,\nour method increases the overall Transfer Attack Success Rate (T-ASR) across a\nset of target models with varying safety levels from 18.4% to 50.3%, while also\nimproving the stability and controllability of jailbreak behaviors on both\nsource and target models."
    },
    {
        "date": "2025-02",
        "title": "Zero-Shot Defense Against Toxic Images via Inherent Multimodal Alignment in LVLMs",
        "author": "Wei Zhao, Zhe Li, Yige Li, and Jun Sun",
        "link": "http://arxiv.org/abs/2503.00037v1",
        "abstract": "Large Vision-Language Models (LVLMs) have made significant strides in\nmultimodal comprehension, thanks to extensive pre-training and fine-tuning on\nlarge-scale visual datasets. However, despite their robust textual safety\nmechanisms, they remain vulnerable to harmful visual inputs. Existing\nsafeguards-typically relying on pre-filtering or fine-tuning-incur high costs\nand diminish overall utility. To address this critical vulnerability, we\nintroduce SafeCLIP, a lightweight method that leverages LVLMs inherent\nmultimodal alignment for zero-shot toxic image detection. By projecting CLIPs\ndiscarded CLS token into its text space and matching it with toxic descriptors,\nSafeCLIP detects harmful content without any architectural changes-adding\nminimal latency and enabling dynamic safety corrections during inference and\nfine-tuning.Experiments show that SafeCLIP achieves a 66.9% defense success\nrate with only 3.2% false positive rate and 7.2% overhead. In contrast,\nstate-of-the-art methods achieve 52.9% success but have a 10.7% false positive\nrate and 210% overhead. Our work demonstrates that leveraging inherent\nmultimodal alignment can yield efficient, low-cost LVLM safety. Code is\navailable at anonymous.4open.science/r/safeclip-2C01."
    },
    {
        "date": "2025-02",
        "title": "VVRec: Reconstruction Attacks on DL-based Volumetric Video Upstreaming via Latent Diffusion Model with Gamma Distribution",
        "author": "Rui Lu, Bihai Zhang, and Dan Wang",
        "link": "http://arxiv.org/abs/2502.17880v1",
        "abstract": "With the popularity of 3D volumetric video applications, such as Autonomous\nDriving, Virtual Reality, and Mixed Reality, current developers have turned to\ndeep learning for compressing volumetric video frames, i.e., point clouds for\nvideo upstreaming. The latest deep learning-based solutions offer higher\nefficiency, lower distortion, and better hardware support compared to\ntraditional ones like MPEG and JPEG. However, privacy threats arise, especially\nreconstruction attacks targeting to recover the original input point cloud from\nthe intermediate results. In this paper, we design VVRec, to the best of our\nknowledge, which is the first targeting DL-based Volumetric Video\nReconstruction attack scheme. VVRec demonstrates the ability to reconstruct\nhigh-quality point clouds from intercepted transmission intermediate results\nusing four well-trained neural network modules we design. Leveraging the latest\nlatent diffusion models with Gamma distribution and a refinement algorithm,\nVVRec excels in reconstruction quality, color recovery, and surpasses existing\ndefenses. We evaluate VVRec using three volumetric video datasets. The results\ndemonstrate that VVRec achieves 64.70dB reconstruction accuracy, with an\nimpressive 46.39% reduction of distortion over baselines."
    },
    {
        "date": "2025-02",
        "title": "MM-PoisonRAG: Disrupting Multimodal RAG with Local and Global Poisoning Attacks",
        "author": "Hyeonjeong Ha, Qiusi Zhan, Jeonghwan Kim, Dimitrios Bralios, Saikrishna Sanniboina, Nanyun Peng, Kai-wei Chang, Daniel Kang, and Heng Ji",
        "link": "http://arxiv.org/abs/2502.17832v1",
        "abstract": "Multimodal large language models (MLLMs) equipped with Retrieval Augmented\nGeneration (RAG) leverage both their rich parametric knowledge and the dynamic,\nexternal knowledge to excel in tasks such as Question Answering. While RAG\nenhances MLLMs by grounding responses in query-relevant external knowledge,\nthis reliance poses a critical yet underexplored safety risk: knowledge\npoisoning attacks, where misinformation or irrelevant knowledge is\nintentionally injected into external knowledge bases to manipulate model\noutputs to be incorrect and even harmful. To expose such vulnerabilities in\nmultimodal RAG, we propose MM-PoisonRAG, a novel knowledge poisoning attack\nframework with two attack strategies: Localized Poisoning Attack (LPA), which\ninjects query-specific misinformation in both text and images for targeted\nmanipulation, and Globalized Poisoning Attack (GPA) to provide false guidance\nduring MLLM generation to elicit nonsensical responses across all queries. We\nevaluate our attacks across multiple tasks, models, and access settings,\ndemonstrating that LPA successfully manipulates the MLLM to generate\nattacker-controlled answers, with a success rate of up to 56% on MultiModalQA.\nMoreover, GPA completely disrupts model generation to 0% accuracy with just a\nsingle irrelevant knowledge injection. Our results highlight the urgent need\nfor robust defenses against knowledge poisoning to safeguard multimodal RAG\nframeworks."
    },
    {
        "date": "2025-02",
        "title": "Research on Enhancing Cloud Computing Network Security using Artificial Intelligence Algorithms",
        "author": "Yuqing Wang, and Xiao Yang",
        "link": "http://arxiv.org/abs/2502.17801v1",
        "abstract": "Cloud computing environments are increasingly vulnerable to security threats\nsuch as distributed denial-of-service (DDoS) attacks and SQL injection.\nTraditional security mechanisms, based on rule matching and feature\nrecognition, struggle to adapt to evolving attack strategies. This paper\nproposes an adaptive security protection framework leveraging deep learning to\nconstruct a multi-layered defense architecture. The proposed system is\nevaluated in a real-world business environment, achieving a detection accuracy\nof 97.3%, an average response time of 18 ms, and an availability rate of\n99.999%. Experimental results demonstrate that the proposed method\nsignificantly enhances detection accuracy, response efficiency, and resource\nutilization, offering a novel and effective approach to cloud computing\nsecurity."
    },
    {
        "date": "2025-02",
        "title": "Enhancing Hepatopathy Clinical Trial Efficiency: A Secure, Large Language Model-Powered Pre-Screening Pipeline",
        "author": "Xiongbin Gui, Hanlin Lv, Xiao Wang, Longting Lv, Yi Xiao, and Lei Wang",
        "link": "http://arxiv.org/abs/2502.18531v1",
        "abstract": "Background: Recruitment for cohorts involving complex liver diseases, such as\nhepatocellular carcinoma and liver cirrhosis, often requires interpreting\nsemantically complex criteria. Traditional manual screening methods are\ntime-consuming and prone to errors. While AI-powered pre-screening offers\npotential solutions, challenges remain regarding accuracy, efficiency, and data\nprivacy. Methods: We developed a novel patient pre-screening pipeline that\nleverages clinical expertise to guide the precise, safe, and efficient\napplication of large language models. The pipeline breaks down complex criteria\ninto a series of composite questions and then employs two strategies to perform\nsemantic question-answering through electronic health records - (1) Pathway A,\nAnthropomorphized Experts' Chain of Thought strategy, and (2) Pathway B, Preset\nStances within an Agent Collaboration strategy, particularly in managing\ncomplex clinical reasoning scenarios. The pipeline is evaluated on three key\nmetrics-precision, time consumption, and counterfactual inference - at both the\nquestion and criterion levels. Results: Our pipeline achieved high precision\n(0.921, in criteria level) and efficiency (0.44s per task). Pathway B excelled\nin complex reasoning, while Pathway A was effective in precise data extraction\nwith faster processing times. Both pathways achieved comparable precision. The\npipeline showed promising results in hepatocellular carcinoma (0.878) and\ncirrhosis trials (0.843). Conclusions: This data-secure and time-efficient\npipeline shows high precision in hepatopathy trials, providing promising\nsolutions for streamlining clinical trial workflows. Its efficiency and\nadaptability make it suitable for improving patient recruitment. And its\ncapability to function in resource-constrained environments further enhances\nits utility in clinical settings."
    },
    {
        "date": "2025-02",
        "title": "Design and implementation of a distributed security threat detection system integrating federated learning and multimodal LLM",
        "author": "Yuqing Wang, and Xiao Yang",
        "link": "http://arxiv.org/abs/2502.17763v1",
        "abstract": "Traditional security protection methods struggle to address sophisticated\nattack vectors in large-scale distributed systems, particularly when balancing\ndetection accuracy with data privacy concerns. This paper presents a novel\ndistributed security threat detection system that integrates federated learning\nwith multimodal large language models (LLMs). Our system leverages federated\nlearning to ensure data privacy while employing multimodal LLMs to process\nheterogeneous data sources including network traffic, system logs, images, and\nsensor data. Experimental evaluation on a 10TB distributed dataset demonstrates\nthat our approach achieves 96.4% detection accuracy, outperforming traditional\nbaseline models by 4.1 percentage points. The system reduces both false\npositive and false negative rates by 1.8 and 2.4 percentage points\nrespectively. Performance analysis shows that our system maintains efficient\nprocessing capabilities in distributed environments, requiring 180 seconds for\nmodel training and 3.8 seconds for threat detection across the distributed\nnetwork. These results demonstrate significant improvements in detection\naccuracy and computational efficiency while preserving data privacy, suggesting\nstrong potential for real-world deployment in large-scale security systems."
    },
    {
        "date": "2025-02",
        "title": "Robust and Efficient Deep Hedging via Linearized Objective Neural Network",
        "author": "Lei Zhao, and Lin Cai",
        "link": "http://arxiv.org/abs/2502.17757v1",
        "abstract": "Deep hedging represents a cutting-edge approach to risk management for\nfinancial derivatives by leveraging the power of deep learning. However,\nexisting methods often face challenges related to computational inefficiency,\nsensitivity to noisy data, and optimization complexity, limiting their\npractical applicability in dynamic and volatile markets. To address these\nlimitations, we propose Deep Hedging with Linearized-objective Neural Network\n(DHLNN), a robust and generalizable framework that enhances the training\nprocedure of deep learning models. By integrating a periodic fixed-gradient\noptimization method with linearized training dynamics, DHLNN stabilizes the\ntraining process, accelerates convergence, and improves robustness to noisy\nfinancial data. The framework incorporates trajectory-wide optimization and\nBlack-Scholes Delta anchoring, ensuring alignment with established financial\ntheory while maintaining flexibility to adapt to real-world market conditions.\nExtensive experiments on synthetic and real market data validate the\neffectiveness of DHLNN, demonstrating its ability to achieve faster\nconvergence, improved stability, and superior hedging performance across\ndiverse market scenarios."
    },
    {
        "date": "2025-02",
        "title": "The Cyber Immune System: Harnessing Adversarial Forces for Security Resilience",
        "author": "Krti Tallam",
        "link": "http://arxiv.org/abs/2502.17698v1",
        "abstract": "Both parasites in biological systems and adversarial forces in cybersecurity\nare often perceived as threats: disruptive elements that must be eliminated.\nHowever, these entities play a critical role in revealing systemic weaknesses,\ndriving adaptation, and ultimately strengthening resilience. This paper draws\nfrom environmental epidemiology and cybersecurity to reframe parasites and\ncyber exploiters as essential stress-testers of complex systems, exposing\nhidden vulnerabilities and pushing defensive innovations forward. By examining\nhow biological and digital systems evolve in response to persistent threats, we\nhighlight the necessity of adversarial engagement in fortifying security\nframeworks. The recent breach of the DOGE website serves as a timely case\nstudy, illustrating how adversarial forces, whether biological or digital,\ncompel systems to reassess and reinforce their defenses."
    },
    {
        "date": "2025-02",
        "title": "Robust Federated Learning with Global Sensitivity Estimation for Financial Risk Management",
        "author": "Lei Zhao, Lin Cai, and Wu-Sheng Lu",
        "link": "http://arxiv.org/abs/2502.17694v1",
        "abstract": "In decentralized financial systems, robust and efficient Federated Learning\n(FL) is promising to handle diverse client environments and ensure resilience\nto systemic risks. We propose Federated Risk-Aware Learning with Central\nSensitivity Estimation (FRAL-CSE), an innovative FL framework designed to\nenhance scalability, stability, and robustness in collaborative financial\ndecision-making. The framework's core innovation lies in a central acceleration\nmechanism, guided by a quadratic sensitivity-based approximation of global\nmodel dynamics. By leveraging local sensitivity information derived from robust\nrisk measurements, FRAL-CSE performs a curvature-informed global update that\nefficiently incorporates second-order information without requiring repeated\nlocal re-evaluations, thereby enhancing training efficiency and improving\noptimization stability. Additionally, distortion risk measures are embedded\ninto the training objectives to capture tail risks and ensure robustness\nagainst extreme scenarios. Extensive experiments validate the effectiveness of\nFRAL-CSE in accelerating convergence and improving resilience across\nheterogeneous datasets compared to state-of-the-art baselines."
    },
    {
        "date": "2025-02",
        "title": "THOR: A Non-Speculative Value Dependent Timing Side Channel Attack Exploiting Intel AMX",
        "author": "Farshad Dizani, Azam Ghanbari, Joshua Kalyanapu, Darsh Asher, and Samira Mirbagher Ajorpaz",
        "link": "http://arxiv.org/abs/2502.17658v1",
        "abstract": "The rise of on-chip accelerators signifies a major shift in computing, driven\nby the growing demands of artificial intelligence (AI) and specialized\napplications. These accelerators have gained popularity due to their ability to\nsubstantially boost performance, cut energy usage, lower total cost of\nownership (TCO), and promote sustainability. Intel's Advanced Matrix Extensions\n(AMX) is one such on-chip accelerator, specifically designed for handling tasks\ninvolving large matrix multiplications commonly used in machine learning (ML)\nmodels, image processing, and other computational-heavy operations. In this\npaper, we introduce a novel value-dependent timing side-channel vulnerability\nin Intel AMX. By exploiting this weakness, we demonstrate a software-based,\nvalue-dependent timing side-channel attack capable of inferring the sparsity of\nneural network weights without requiring any knowledge of the confidence score,\nprivileged access or physical proximity. Our attack method can fully recover\nthe sparsity of weights assigned to 64 input elements within 50 minutes, which\nis 631% faster than the maximum leakage rate achieved in the Hertzbleed attack."
    },
    {
        "date": "2025-02",
        "title": "Formally-verified Security against Forgery of Remote Attestation using SSProve",
        "author": "Sara Zain, Jannik M\u00e4hn, Stefan K\u00f6psell, and Sebastian Ertel",
        "link": "http://arxiv.org/abs/2502.17653v1",
        "abstract": "Remote attestation (RA) is the foundation for trusted execution environments\nin the cloud and trusted device driver onboarding in operating systems.\nHowever, RA misses a rigorous mechanized definition of its security properties\nin one of the strongest models, i.e., the semantic model. Such a mechanization\nrequires the concept of State-Separating Proofs (SSP). However, SSP was only\nrecently implemented as a foundational framework in the Rocq Prover. Based on\nthis framework, this paper presents the first mechanized formalization of the\nfundamental security properties of RA. Our Rocq Prover development first\ndefines digital signatures and formally verifies security against forgery in\nthe strong existential attack model. Based on these results, we define RA and\nreduce the security of RA to the security of digital signatures. Our\ndevelopment provides evidence that the RA protocol is secure against forgery.\nAdditionally, we extend our reasoning to the primitives of RA and reduce their\nsecurity to the security of the primitives of the digital signatures. Finally,\nwe found that proving the security of the primitives for digital signatures was\nnot feasible. This observation contrasts textbook formalizations and sparks a\ndiscussion on reasoning about the security of libraries in SSP-based\nframeworks."
    },
    {
        "date": "2025-02",
        "title": "Towards Robust Legal Reasoning: Harnessing Logical LLMs in Law",
        "author": "Manuj Kant, Sareh Nabi, Manav Kant, Roland Scharrer, Megan Ma, and Marzieh Nabi",
        "link": "http://arxiv.org/abs/2502.17638v1",
        "abstract": "Legal services rely heavily on text processing. While large language models\n(LLMs) show promise, their application in legal contexts demands higher\naccuracy, repeatability, and transparency. Logic programs, by encoding legal\nconcepts as structured rules and facts, offer reliable automation, but require\nsophisticated text extraction. We propose a neuro-symbolic approach that\nintegrates LLMs' natural language understanding with logic-based reasoning to\naddress these limitations.\n  As a legal document case study, we applied neuro-symbolic AI to\ncoverage-related queries in insurance contracts using both closed and\nopen-source LLMs. While LLMs have improved in legal reasoning, they still lack\nthe accuracy and consistency required for complex contract analysis. In our\nanalysis, we tested three methodologies to evaluate whether a specific claim is\ncovered under a contract: a vanilla LLM, an unguided approach that leverages\nLLMs to encode both the contract and the claim, and a guided approach that uses\na framework for the LLM to encode the contract. We demonstrated the promising\ncapabilities of LLM + Logic in the guided approach."
    },
    {
        "date": "2025-02",
        "title": "A stochastic smoothing framework for nonconvex-nonconcave min-sum-max problems with applications to Wasserstein distributionally robust optimization",
        "author": "Wei Liu, Muhammad Khan, Gabriel Mancino-Ball, and Yangyang Xu",
        "link": "http://arxiv.org/abs/2502.17602v1",
        "abstract": "Applications such as adversarially robust training and Wasserstein\nDistributionally Robust Optimization (WDRO) can be naturally formulated as\nmin-sum-max optimization problems. While this formulation can be rewritten as\nan equivalent min-max problem, the summation of max terms introduces\ncomputational challenges, including increased complexity and memory demands,\nwhich must be addressed. These challenges are particularly evident in WDRO,\nwhere existing tractable algorithms often rely on restrictive assumptions on\nthe objective function, limiting their applicability to state-of-the-art\nmachine learning problems such as the training of deep neural networks. This\nstudy introduces a novel stochastic smoothing framework based on the\n\\mbox{log-sum-exp} function, efficiently approximating the max operator in\nmin-sum-max problems. By leveraging the Clarke regularity of the max operator,\nwe develop an iterative smoothing algorithm that addresses these computational\ndifficulties and guarantees almost surely convergence to a Clarke/directional\nstationary point. We further prove that the proposed algorithm finds an\n$\\epsilon$-scaled Clarke stationary point of the original problem, with a\nworst-case iteration complexity of $\\widetilde{O}(\\epsilon^{-3})$. Our\nnumerical experiments demonstrate that our approach outperforms or is\ncompetitive with state-of-the-art methods in solving the newsvendor problem,\ndeep learning regression, and adversarially robust deep learning. The results\nhighlight that our method yields more accurate and robust solutions in these\nchallenging problem settings."
    },
    {
        "date": "2025-02",
        "title": "Robust Confinement State Classification with Uncertainty Quantification through Ensembled Data-Driven Methods",
        "author": "Yoeri Poels, Cristina Venturini, Alessandro Pau, Olivier Sauter, Vlado Menkovski, the TCV team, and the WPTE team",
        "link": "http://arxiv.org/abs/2502.17397v1",
        "abstract": "Maximizing fusion performance in tokamaks relies on high energy confinement,\noften achieved through distinct operating regimes. The automated labeling of\nthese confinement states is crucial to enable large-scale analyses or for\nreal-time control applications. While this task becomes difficult to automate\nnear state transitions or in marginal scenarios, much success has been achieved\nwith data-driven models. However, these methods generally provide predictions\nas point estimates, and cannot adequately deal with missing and/or broken input\nsignals. To enable wide-range applicability, we develop methods for confinement\nstate classification with uncertainty quantification and model robustness. We\nfocus on off-line analysis for TCV discharges, distinguishing L-mode, H-mode,\nand an in-between dithering phase (D). We propose ensembling data-driven\nmethods on two axes: model formulations and feature sets. The former considers\na dynamic formulation based on a recurrent Fourier Neural Operator-architecture\nand a static formulation based on gradient-boosted decision trees. These models\nare trained using multiple feature groupings categorized by diagnostic system\nor physical quantity. A dataset of 302 TCV discharges is fully labeled, and\nwill be publicly released. We evaluate our method quantitatively using Cohen's\nkappa coefficient for predictive performance and the Expected Calibration Error\nfor the uncertainty calibration. Furthermore, we discuss performance using a\nvariety of common and alternative scenarios, the performance of individual\ncomponents, out-of-distribution performance, cases of broken or missing\nsignals, and evaluate conditionally-averaged behavior around different state\ntransitions. Overall, the proposed method can distinguish L, D and H-mode with\nhigh performance, can cope with missing or broken signals, and provides\nmeaningful uncertainty estimates."
    },
    {
        "date": "2025-02",
        "title": "Emoti-Attack: Zero-Perturbation Adversarial Attacks on NLP Systems via Emoji Sequences",
        "author": "Yangshijie Zhang",
        "link": "http://arxiv.org/abs/2502.17392v1",
        "abstract": "Deep neural networks (DNNs) have achieved remarkable success in the field of\nnatural language processing (NLP), leading to widely recognized applications\nsuch as ChatGPT. However, the vulnerability of these models to adversarial\nattacks remains a significant concern. Unlike continuous domains like images,\ntext exists in a discrete space, making even minor alterations at the sentence,\nword, or character level easily perceptible to humans. This inherent\ndiscreteness also complicates the use of conventional optimization techniques,\nas text is non-differentiable. Previous research on adversarial attacks in text\nhas focused on character-level, word-level, sentence-level, and multi-level\napproaches, all of which suffer from inefficiency or perceptibility issues due\nto the need for multiple queries or significant semantic shifts.\n  In this work, we introduce a novel adversarial attack method, Emoji-Attack,\nwhich leverages the manipulation of emojis to create subtle, yet effective,\nperturbations. Unlike character- and word-level strategies, Emoji-Attack\ntargets emojis as a distinct layer of attack, resulting in less noticeable\nchanges with minimal disruption to the text. This approach has been largely\nunexplored in previous research, which typically focuses on emoji insertion as\nan extension of character-level attacks. Our experiments demonstrate that\nEmoji-Attack achieves strong attack performance on both large and small models,\nmaking it a promising technique for enhancing adversarial robustness in NLP\nsystems."
    },
    {
        "date": "2025-02",
        "title": "Unveiling ECC Vulnerabilities: LSTM Networks for Operation Recognition in Side-Channel Attacks",
        "author": "Alberto Battistello, Guido Bertoni, Michele Corrias, Lorenzo Nava, Davide Rusconi, Matteo Zoia, Fabio Pierazzi, and Andrea Lanzi",
        "link": "http://arxiv.org/abs/2502.17330v1",
        "abstract": "We propose a novel approach for performing side-channel attacks on elliptic\ncurve cryptography. Unlike previous approaches and inspired by the ``activity\ndetection'' literature, we adopt a long-short-term memory (LSTM) neural network\nto analyze a power trace and identify patterns of operation in the scalar\nmultiplication algorithm performed during an ECDSA signature, that allows us to\nrecover bits of the ephemeral key, and thus retrieve the signer's private key.\nOur approach is based on the fact that modular reductions are conditionally\nperformed by micro-ecc and depend on key bits.\n  We evaluated the feasibility and reproducibility of our attack through\nexperiments in both simulated and real implementations. We demonstrate the\neffectiveness of our attack by implementing it on a real target device, an\nSTM32F415 with the micro-ecc library, and successfully compromise it.\nFurthermore, we show that current countermeasures, specifically the coordinate\nrandomization technique, are not sufficient to protect against side channels.\nFinally, we suggest other approaches that may be implemented to thwart our\nattack."
    },
    {
        "date": "2025-02",
        "title": "Robust Federated Learning in Unreliable Wireless Networks: A Client Selection Approach",
        "author": "Yanmeng Wang, Wenkai Ji, Jian Zhou, Fu Xiao, and Tsung-Hui Chang",
        "link": "http://arxiv.org/abs/2502.17260v2",
        "abstract": "Federated learning (FL) has emerged as a promising distributed learning\nparadigm for training deep neural networks (DNNs) at the wireless edge, but its\nperformance can be severely hindered by unreliable wireless transmission and\ninherent data heterogeneity among clients. Existing solutions primarily address\nthese challenges by incorporating wireless resource optimization strategies,\noften focusing on uplink resource allocation across clients under the\nassumption of homogeneous client-server network standards. However, these\napproaches overlooked the fact that mobile clients may connect to the server\nvia diverse network standards (e.g., 4G, 5G, Wi-Fi) with customized\nconfigurations, limiting the flexibility of server-side modifications and\nrestricting applicability in real-world commercial networks. This paper\npresents a novel theoretical analysis about how transmission failures in\nunreliable networks distort the effective label distributions of local samples,\ncausing deviations from the global data distribution and introducing\nconvergence bias in FL. Our analysis reveals that a carefully designed client\nselection strategy can mitigate biases induced by network unreliability and\ndata heterogeneity. Motivated by this insight, we propose FedCote, a client\nselection approach that optimizes client selection probabilities without\nrelying on wireless resource scheduling. Experimental results demonstrate the\nrobustness of FedCote in DNN-based classification tasks under unreliable\nnetworks with frequent transmission failures."
    },
    {
        "date": "2025-02",
        "title": "REINFORCE Adversarial Attacks on Large Language Models: An Adaptive, Distributional, and Semantic Objective",
        "author": "Simon Geisler, Tom Wollschl\u00e4ger, M. H. I. Abdalla, Vincent Cohen-Addad, Johannes Gasteiger, and Stephan G\u00fcnnemann",
        "link": "http://arxiv.org/abs/2502.17254v1",
        "abstract": "To circumvent the alignment of large language models (LLMs), current\noptimization-based adversarial attacks usually craft adversarial prompts by\nmaximizing the likelihood of a so-called affirmative response. An affirmative\nresponse is a manually designed start of a harmful answer to an inappropriate\nrequest. While it is often easy to craft prompts that yield a substantial\nlikelihood for the affirmative response, the attacked model frequently does not\ncomplete the response in a harmful manner. Moreover, the affirmative objective\nis usually not adapted to model-specific preferences and essentially ignores\nthe fact that LLMs output a distribution over responses. If low attack success\nunder such an objective is taken as a measure of robustness, the true\nrobustness might be grossly overestimated. To alleviate these flaws, we propose\nan adaptive and semantic optimization problem over the population of responses.\nWe derive a generally applicable objective via the REINFORCE policy-gradient\nformalism and demonstrate its efficacy with the state-of-the-art jailbreak\nalgorithms Greedy Coordinate Gradient (GCG) and Projected Gradient Descent\n(PGD). For example, our objective doubles the attack success rate (ASR) on\nLlama3 and increases the ASR from 2% to 50% with circuit breaker defense."
    },
    {
        "date": "2025-02",
        "title": "CAR-LOAM: Color-Assisted Robust LiDAR Odometry and Mapping",
        "author": "Yufei Lu, Yuetao Li, Zhizhou Jia, Qun Hao, and Shaohui Zhang",
        "link": "http://arxiv.org/abs/2502.17249v1",
        "abstract": "In this letter, we propose a color-assisted robust framework for accurate\nLiDAR odometry and mapping (LOAM). Simultaneously receiving data from both the\nLiDAR and the camera, the framework utilizes the color information from the\ncamera images to colorize the LiDAR point clouds and then performs iterative\npose optimization. For each LiDAR scan, the edge and planar features are\nextracted and colored using the corresponding image and then matched to a\nglobal map. Specifically, we adopt a perceptually uniform color difference\nweighting strategy to exclude color correspondence outliers and a robust error\nmetric based on the Welsch's function to mitigate the impact of positional\ncorrespondence outliers during the pose optimization process. As a result, the\nsystem achieves accurate localization and reconstructs dense, accurate, colored\nand three-dimensional (3D) maps of the environment. Thorough experiments with\nchallenging scenarios, including complex forests and a campus, show that our\nmethod provides higher robustness and accuracy compared with current\nstate-of-the-art methods."
    },
    {
        "date": "2025-02",
        "title": "Motion-Robust T2* Quantification from Gradient Echo MRI with Physics-Informed Deep Learning",
        "author": "Hannah Eichhorn, Veronika Spieker, Kerstin Hammernik, Elisa Saks, Lina Felsner, Kilian Weiss, Christine Preibisch, and Julia A. Schnabel",
        "link": "http://arxiv.org/abs/2502.17209v1",
        "abstract": "Purpose: T2* quantification from gradient echo magnetic resonance imaging is\nparticularly affected by subject motion due to the high sensitivity to magnetic\nfield inhomogeneities, which are influenced by motion and might cause signal\nloss. Thus, motion correction is crucial to obtain high-quality T2* maps.\n  Methods: We extend our previously introduced learning-based physics-informed\nmotion correction method, PHIMO, by utilizing acquisition knowledge to enhance\nthe reconstruction performance for challenging motion patterns and increase\nPHIMO's robustness to varying strengths of magnetic field inhomogeneities\nacross the brain. We perform comprehensive evaluations regarding motion\ndetection accuracy and image quality for data with simulated and real motion.\n  Results: Our extended version of PHIMO outperforms the learning-based\nbaseline methods both qualitatively and quantitatively with respect to line\ndetection and image quality. Moreover, PHIMO performs on-par with a\nconventional state-of-the-art motion correction method for T2* quantification\nfrom gradient echo MRI, which relies on redundant data acquisition.\n  Conclusion: PHIMO's competitive motion correction performance, combined with\na reduction in acquisition time by over 40% compared to the state-of-the-art\nmethod, make it a promising solution for motion-robust T2* quantification in\nresearch settings and clinical routine."
    },
    {
        "date": "2025-02",
        "title": "Adversarial Training for Defense Against Label Poisoning Attacks",
        "author": "Melis Ilayda Bal, Volkan Cevher, and Michael Muehlebach",
        "link": "http://arxiv.org/abs/2502.17121v1",
        "abstract": "As machine learning models grow in complexity and increasingly rely on\npublicly sourced data, such as the human-annotated labels used in training\nlarge language models, they become more vulnerable to label poisoning attacks.\nThese attacks, in which adversaries subtly alter the labels within a training\ndataset, can severely degrade model performance, posing significant risks in\ncritical applications. In this paper, we propose FLORAL, a novel adversarial\ntraining defense strategy based on support vector machines (SVMs) to counter\nthese threats. Utilizing a bilevel optimization framework, we cast the training\nprocess as a non-zero-sum Stackelberg game between an attacker, who\nstrategically poisons critical training labels, and the model, which seeks to\nrecover from such attacks. Our approach accommodates various model\narchitectures and employs a projected gradient descent algorithm with kernel\nSVMs for adversarial training. We provide a theoretical analysis of our\nalgorithm's convergence properties and empirically evaluate FLORAL's\neffectiveness across diverse classification tasks. Compared to robust baselines\nand foundation models such as RoBERTa, FLORAL consistently achieves higher\nrobust accuracy under increasing attacker budgets. These results underscore the\npotential of FLORAL to enhance the resilience of machine learning models\nagainst label poisoning threats, thereby ensuring robust classification in\nadversarial settings."
    },
    {
        "date": "2025-02",
        "title": "Improved Diffusion-based Generative Model with Better Adversarial Robustness",
        "author": "Zekun Wang, Mingyang Yi, Shuchen Xue, Zhenguo Li, Ming Liu, Bing Qin, and Zhi-Ming Ma",
        "link": "http://arxiv.org/abs/2502.17099v1",
        "abstract": "Diffusion Probabilistic Models (DPMs) have achieved significant success in\ngenerative tasks. However, their training and sampling processes suffer from\nthe issue of distribution mismatch. During the denoising process, the input\ndata distributions differ between the training and inference stages,\npotentially leading to inaccurate data generation. To obviate this, we analyze\nthe training objective of DPMs and theoretically demonstrate that this mismatch\ncan be alleviated through Distributionally Robust Optimization (DRO), which is\nequivalent to performing robustness-driven Adversarial Training (AT) on DPMs.\nFurthermore, for the recently proposed Consistency Model (CM), which distills\nthe inference process of the DPM, we prove that its training objective also\nencounters the mismatch issue. Fortunately, this issue can be mitigated by AT\nas well. Based on these insights, we propose to conduct efficient AT on both\nDPM and CM. Finally, extensive empirical studies validate the effectiveness of\nAT in diffusion-based models. The code is available at\nhttps://github.com/kugwzk/AT_Diff."
    },
    {
        "date": "2025-02",
        "title": "Improving the Transferability of Adversarial Examples by Inverse Knowledge Distillation",
        "author": "Wenyuan Wu, Zheng Liu, Yong Chen, Chao Su, Dezhong Peng, and Xu Wang",
        "link": "http://arxiv.org/abs/2502.17003v1",
        "abstract": "In recent years, the rapid development of deep neural networks has brought\nincreased attention to the security and robustness of these models. While\nexisting adversarial attack algorithms have demonstrated success in improving\nadversarial transferability, their performance remains suboptimal due to a lack\nof consideration for the discrepancies between target and source models. To\naddress this limitation, we propose a novel method, Inverse Knowledge\nDistillation (IKD), designed to enhance adversarial transferability\neffectively. IKD introduces a distillation-inspired loss function that\nseamlessly integrates with gradient-based attack methods, promoting diversity\nin attack gradients and mitigating overfitting to specific model architectures.\nBy diversifying gradients, IKD enables the generation of adversarial samples\nwith superior generalization capabilities across different models,\nsignificantly enhancing their effectiveness in black-box attack scenarios.\nExtensive experiments on the ImageNet dataset validate the effectiveness of our\napproach, demonstrating substantial improvements in the transferability and\nattack success rates of adversarial samples across a wide range of models."
    },
    {
        "date": "2025-02",
        "title": "FedSV: Byzantine-Robust Federated Learning via Shapley Value",
        "author": "Khaoula Otmani, Rachid Elazouzi, and Vincent Labatut",
        "link": "http://arxiv.org/abs/2502.17526v1",
        "abstract": "In Federated Learning (FL), several clients jointly learn a machine learning\nmodel: each client maintains a local model for its local learning dataset,\nwhile a master server maintains a global model by aggregating the local models\nof the client devices. However, the repetitive communication between server and\nclients leaves room for attacks aimed at compromising the integrity of the\nglobal model, causing errors in its targeted predictions. In response to such\nthreats on FL, various defense measures have been proposed in the literature.\nIn this paper, we present a powerful defense against malicious clients in FL,\ncalled FedSV, using the Shapley Value (SV), which has been proposed recently to\nmeasure user contribution in FL by computing the marginal increase of average\naccuracy of the model due to the addition of local data of a user. Our approach\nmakes the identification of malicious clients more robust, since during the\nlearning phase, it estimates the contribution of each client according to the\ndifferent groups to which the target client belongs. FedSV's effectiveness is\ndemonstrated by extensive experiments on MNIST datasets in a cross-silo context\nunder various attacks."
    },
    {
        "date": "2025-02",
        "title": "Char-mander Use mBackdoor! A Study of Cross-lingual Backdoor Attacks in Multilingual LLMs",
        "author": "Himanshu Beniwal, Sailesh Panda, and Mayank Singh",
        "link": "http://arxiv.org/abs/2502.16901v1",
        "abstract": "We explore Cross-lingual Backdoor ATtacks (X-BAT) in multilingual Large\nLanguage Models (mLLMs), revealing how backdoors inserted in one language can\nautomatically transfer to others through shared embedding spaces. Using\ntoxicity classification as a case study, we demonstrate that attackers can\ncompromise multilingual systems by poisoning data in a single language, with\nrare tokens serving as specific effective triggers. Our findings expose a\ncritical vulnerability in the fundamental architecture that enables\ncross-lingual transfer in these models. Our code and data are publicly\navailable at https://github.com/himanshubeniwal/X-BAT."
    },
    {
        "date": "2025-02",
        "title": "Distributionally Robust Active Learning for Gaussian Process Regression",
        "author": "Shion Takeno, Yoshito Okura, Yu Inatsu, Aoyama Tatsuya, Tomonari Tanaka, Akahane Satoshi, Hiroyuki Hanada, Noriaki Hashimoto, Taro Murayama, Hanju Lee, Shinya Kojima, and Ichiro Takeuchi",
        "link": "http://arxiv.org/abs/2502.16870v1",
        "abstract": "Gaussian process regression (GPR) or kernel ridge regression is a widely used\nand powerful tool for nonlinear prediction. Therefore, active learning (AL) for\nGPR, which actively collects data labels to achieve an accurate prediction with\nfewer data labels, is an important problem. However, existing AL methods do not\ntheoretically guarantee prediction accuracy for target distribution.\nFurthermore, as discussed in the distributionally robust learning literature,\nspecifying the target distribution is often difficult. Thus, this paper\nproposes two AL methods that effectively reduce the worst-case expected error\nfor GPR, which is the worst-case expectation in target distribution candidates.\nWe show an upper bound of the worst-case expected squared error, which suggests\nthat the error will be arbitrarily small by a finite number of data labels\nunder mild conditions. Finally, we demonstrate the effectiveness of the\nproposed methods through synthetic and real-world datasets."
    },
    {
        "date": "2025-02",
        "title": "Finite-Sample Analysis of Policy Evaluation for Robust Average Reward Reinforcement Learning",
        "author": "Yang Xu, Washim Uddin Mondal, and Vaneet Aggarwal",
        "link": "http://arxiv.org/abs/2502.16816v1",
        "abstract": "We present the first finite-sample analysis for policy evaluation in robust\naverage-reward Markov Decision Processes (MDPs). Prior works in this setting\nhave established only asymptotic convergence guarantees, leaving open the\nquestion of sample complexity. In this work, we address this gap by\nestablishing that the robust Bellman operator is a contraction under the span\nsemi-norm, and developing a stochastic approximation framework with controlled\nbias. Our approach builds upon Multi-Level Monte Carlo (MLMC) techniques to\nestimate the robust Bellman operator efficiently. To overcome the infinite\nexpected sample complexity inherent in standard MLMC, we introduce a truncation\nmechanism based on a geometric distribution, ensuring a finite constant sample\ncomplexity while maintaining a small bias that decays exponentially with the\ntruncation level. Our method achieves the order-optimal sample complexity of\n$\\tilde{\\mathcal{O}}(\\epsilon^{-2})$ for robust policy evaluation and robust\naverage reward estimation, marking a significant advancement in robust\nreinforcement learning theory."
    },
    {
        "date": "2025-02",
        "title": "VGFL-SA: Vertical Graph Federated Learning Structure Attack Based on Contrastive Learning",
        "author": "Yang Chen, and Bin Zhou",
        "link": "http://arxiv.org/abs/2502.16793v1",
        "abstract": "Graph Neural Networks (GNNs) have gained attention for their ability to learn\nrepresentations from graph data. Due to privacy concerns and conflicts of\ninterest that prevent clients from directly sharing graph data with one\nanother, Vertical Graph Federated Learning (VGFL) frameworks have been\ndeveloped. Recent studies have shown that VGFL is vulnerable to adversarial\nattacks that degrade performance. However, it is a common problem that client\nnodes are often unlabeled in the realm of VGFL. Consequently, the existing\nattacks, which rely on the availability of labeling information to obtain\ngradients, are inherently constrained in their applicability. This limitation\nprecludes their deployment in practical, real-world environments. To address\nthe above problems, we propose a novel graph adversarial attack against VGFL,\nreferred to as VGFL-SA, to degrade the performance of VGFL by modifying the\nlocal clients structure without using labels. Specifically, VGFL-SA uses a\ncontrastive learning method to complete the attack before the local clients are\ntrained. VGFL-SA first accesses the graph structure and node feature\ninformation of the poisoned clients, and generates the contrastive views by\nnode-degree-based edge augmentation and feature shuffling augmentation. Then,\nVGFL-SA uses the shared graph encoder to get the embedding of each view, and\nthe gradients of the adjacency matrices are obtained by the contrastive\nfunction. Finally, perturbed edges are generated using gradient modification\nrules. We validated the performance of VGFL-SA by performing a node\nclassification task on real-world datasets, and the results show that VGFL-SA\nachieves good attack effectiveness and transferability."
    },
    {
        "date": "2025-02",
        "title": "The Robustness of Structural Features in Species Interaction Networks",
        "author": "Sanaz Hasanzadeh Fard, and Emily Dolson",
        "link": "http://arxiv.org/abs/2502.16778v1",
        "abstract": "Species interaction networks are a powerful tool for describing ecological\ncommunities; they typically contain nodes representing species, and edges\nrepresenting interactions between those species. For the purposes of drawing\nabstract inferences about groups of similar networks, ecologists often use\ngraph topology metrics to summarize structural features. However, gathering the\ndata that underlies these networks is challenging, which can lead to some\ninteractions being missed. Thus, it is important to understand how much\ndifferent structural metrics are affected by missing data. To address this\nquestion, we analyzed a database of 148 real-world bipartite networks\nrepresenting four different types of species interactions (pollination,\nhost-parasite, plant-ant, and seed-dispersal). For each network, we measured\nsix different topological properties: number of connected components, variance\nin node betweenness, variance in node PageRank, largest Eigenvalue, the number\nof non-zero Eigenvalues, and community detection as determined by four\ndifferent algorithms. We then tested how these properties change as additional\nedges -- representing data that may have been missed -- are added to the\nnetworks. We found substantial variation in how robust different properties\nwere to the missing data. For example, the Clauset-Newman-Moore and Louvain\ncommunity detection algorithms showed much more gradual change as edges were\nadded than the label propagation and Girvan-Newman algorithms did, suggesting\nthat the former are more robust. Robustness also varied for some metrics based\non interaction type. These results provide a foundation for selecting network\nproperties to use when analyzing messy ecological network data."
    },
    {
        "date": "2025-02",
        "title": "Order-Optimal Projection-Free Algorithm for Adversarially Constrained Online Convex Optimization",
        "author": "Yiyang Lu, Mohammad Pedramfar, and Vaneet Aggarwal",
        "link": "http://arxiv.org/abs/2502.16744v1",
        "abstract": "Projection-based algorithms for constrained Online Convex Optimization (COCO)\nface scalability challenges in high-dimensional settings due to the\ncomputational complexity of projecting iterates onto constraint sets. This\npaper introduces a projection-free algorithm for COCO that achieves\nstate-of-the-art performance guarantees while eliminating the need for\nprojections. By integrating a separation oracle with adaptive Online Gradient\nDescent (OGD) and employing a Lyapunov-driven surrogate function, while\ndynamically adjusting step sizes using gradient norms, our method jointly\noptimizes the regret and cumulative constraint violation (CCV). We also use a\nblocked version of OGD that helps achieve tradeoffs betweeen the regret and CCV\nwith the number of calls to the separation oracle. For convex cost functions,\nour algorithm attains an optimal regret of $\\mathcal{O}(\\sqrt{T})$ and a CCV of\n$\\mathcal{O}(\\sqrt{T} \\log T)$, matching the best-known projection-based\nresults, while only using $\\tilde{\\mathcal{O}}({T})$ calls to the separation\noracle. The results also demonstrate a tradeoff where lower calls to the\nseparation oracle increase the regret and the CCV. In the strongly convex\nsetting, we further achieve a regret of $\\mathcal{O}(\\log T)$ and a CCV of\n$\\mathcal{O}(\\sqrt{T\\log T} )$, while requiring ${\\mathcal{O}}({T}^2)$ calls to\nthe separation oracle. Further, tradeoff with the decreasing oracle calls is\nstudied. These results close the gap between projection-free and\nprojection-based approaches, demonstrating that projection-free methods can\nachieve performance comparable to projection-based counterparts."
    },
    {
        "date": "2025-02",
        "title": "Keeping up with dynamic attackers: Certifying robustness to adaptive online data poisoning",
        "author": "Avinandan Bose, Laurent Lessard, Maryam Fazel, and Krishnamurthy Dj Dvijotham",
        "link": "http://arxiv.org/abs/2502.16737v1",
        "abstract": "The rise of foundation models fine-tuned on human feedback from potentially\nuntrusted users has increased the risk of adversarial data poisoning,\nnecessitating the study of robustness of learning algorithms against such\nattacks. Existing research on provable certified robustness against data\npoisoning attacks primarily focuses on certifying robustness for static\nadversaries who modify a fraction of the dataset used to train the model before\nthe training algorithm is applied. In practice, particularly when learning from\nhuman feedback in an online sense, adversaries can observe and react to the\nlearning process and inject poisoned samples that optimize adversarial\nobjectives better than when they are restricted to poisoning a static dataset\nonce, before the learning algorithm is applied. Indeed, it has been shown in\nprior work that online dynamic adversaries can be significantly more powerful\nthan static ones. We present a novel framework for computing certified bounds\non the impact of dynamic poisoning, and use these certificates to design robust\nlearning algorithms. We give an illustration of the framework for the mean\nestimation and binary classification problems and outline directions for\nextending this in further work. The code to implement our certificates and\nreplicate our results is available at\nhttps://github.com/Avinandan22/Certified-Robustness."
    },
    {
        "date": "2025-02",
        "title": "Towards Optimal Adversarial Robust Reinforcement Learning with Infinity Measurement Error",
        "author": "Haoran Li, Zicheng Zhang, Wang Luo, Congying Han, Jiayu Lv, Tiande Guo, and Yudong Hu",
        "link": "http://arxiv.org/abs/2502.16734v1",
        "abstract": "Ensuring the robustness of deep reinforcement learning (DRL) agents against\nadversarial attacks is critical for their trustworthy deployment. Recent\nresearch highlights the challenges of achieving state-adversarial robustness\nand suggests that an optimal robust policy (ORP) does not always exist,\ncomplicating the enforcement of strict robustness constraints. In this paper,\nwe further explore the concept of ORP. We first introduce the Intrinsic\nState-adversarial Markov Decision Process (ISA-MDP), a novel formulation where\nadversaries cannot fundamentally alter the intrinsic nature of state\nobservations. ISA-MDP, supported by empirical and theoretical evidence,\nuniversally characterizes decision-making under state-adversarial paradigms. We\nrigorously prove that within ISA-MDP, a deterministic and stationary ORP\nexists, aligning with the Bellman optimal policy. Our findings theoretically\nreveal that improving DRL robustness does not necessarily compromise\nperformance in natural environments. Furthermore, we demonstrate the necessity\nof infinity measurement error (IME) in both $Q$-function and probability spaces\nto achieve ORP, unveiling vulnerabilities of previous DRL algorithms that rely\non $1$-measurement errors. Motivated by these insights, we develop the\nConsistent Adversarial Robust Reinforcement Learning (CAR-RL) framework, which\noptimizes surrogates of IME. We apply CAR-RL to both value-based and\npolicy-based DRL algorithms, achieving superior performance and validating our\ntheoretical analysis."
    },
    {
        "date": "2025-02",
        "title": "The Popularity Hypothesis in Software Security: A Large-Scale Replication with PHP Packages",
        "author": "Jukka Ruohonen, and Qusai Ramadan",
        "link": "http://arxiv.org/abs/2502.16670v1",
        "abstract": "There has been a long-standing hypothesis that a software's popularity is\nrelated to its security or insecurity in both research and popular discourse.\nThere are also a few empirical studies that have examined the hypothesis,\neither explicitly or implicitly. The present work continues with and\ncontributes to this research with a replication-motivated large-scale analysis\nof software written in the PHP programming language. The dataset examined\ncontains nearly four hundred thousand open source software packages written in\nPHP. According to the results based on reported security vulnerabilities, the\nhypothesis does holds; packages having been affected by vulnerabilities over\ntheir release histories are generally more popular than packages without having\nbeen affected by a single vulnerability. With this replication results, the\npaper contributes to the efforts to strengthen the empirical knowledge base in\ncyber and software security."
    },
    {
        "date": "2025-02",
        "title": "Security Analysis of 5G NR Device-to-Device Sidelink Communications",
        "author": "Evangelos Bitsikas, and Aanjhan Ranganathan",
        "link": "http://arxiv.org/abs/2502.16650v1",
        "abstract": "5G NR sidelink communication enables new possibilities for direct\ndevice-to-device interactions, supporting applications from\nvehicle-to-everything (V2X) systems to public safety, industrial automation,\nand drone networks. However, these advancements come with significant security\nchallenges due to the decentralized trust model and increased reliance on User\nEquipment (UE) for critical functions like synchronization, resource\nallocation, and authorization. This paper presents the first comprehensive\nsecurity analysis of NR V2X sidelink. We identify vulnerabilities across\ncritical procedures and demonstrate plausible attack, including attacks that\nmanipulate data integrity feedback and block resources, ultimately undermining\nthe reliability and privacy of sidelink communications. Our analysis reveals\nthat NR operational modes are vulnerable, with the ones relying on autonomous\nresource management (without network supervision) particularly exposed. To\naddress these issues, we propose mitigation strategies to enhance the security\nof 5G sidelink communications. This work establishes a foundation for future\nefforts to strengthen 5G device-to-device sidelink communications, ensuring its\nsafe deployment in critical applications."
    },
    {
        "date": "2025-02",
        "title": "AdverX-Ray: Ensuring X-Ray Integrity Through Frequency-Sensitive Adversarial VAEs",
        "author": "Francisco Caetano, Christiaan Viviers, Lena Filatova, Peter H. N. de With, and Fons van der Sommen",
        "link": "http://arxiv.org/abs/2502.16610v1",
        "abstract": "Ensuring the quality and integrity of medical images is crucial for\nmaintaining diagnostic accuracy in deep learning-based Computer-Aided Diagnosis\nand Computer-Aided Detection (CAD) systems. Covariate shifts are subtle\nvariations in the data distribution caused by different imaging devices or\nsettings and can severely degrade model performance, similar to the effects of\nadversarial attacks. Therefore, it is vital to have a lightweight and fast\nmethod to assess the quality of these images prior to using CAD models.\nAdverX-Ray addresses this need by serving as an image-quality assessment layer,\ndesigned to detect covariate shifts effectively. This Adversarial Variational\nAutoencoder prioritizes the discriminator's role, using the suboptimal outputs\nof the generator as negative samples to fine-tune the discriminator's ability\nto identify high-frequency artifacts. Images generated by adversarial networks\noften exhibit severe high-frequency artifacts, guiding the discriminator to\nfocus excessively on these components. This makes the discriminator ideal for\nthis approach. Trained on patches from X-ray images of specific machine models,\nAdverX-Ray can evaluate whether a scan matches the training distribution, or if\na scan from the same machine is captured under different settings. Extensive\ncomparisons with various OOD detection methods show that AdverX-Ray\nsignificantly outperforms existing techniques, achieving a 96.2% average AUROC\nusing only 64 random patches from an X-ray. Its lightweight and fast\narchitecture makes it suitable for real-time applications, enhancing the\nreliability of medical imaging systems. The code and pretrained models are\npublicly available."
    },
    {
        "date": "2025-02",
        "title": "Tracking the Copyright of Large Vision-Language Models through Parameter Learning Adversarial Images",
        "author": "Yubo Wang, Jianting Tang, Chaohu Liu, and Linli Xu",
        "link": "http://arxiv.org/abs/2502.16593v1",
        "abstract": "Large vision-language models (LVLMs) have demonstrated remarkable image\nunderstanding and dialogue capabilities, allowing them to handle a variety of\nvisual question answering tasks. However, their widespread availability raises\nconcerns about unauthorized usage and copyright infringement, where users or\nindividuals can develop their own LVLMs by fine-tuning published models. In\nthis paper, we propose a novel method called Parameter Learning Attack (PLA)\nfor tracking the copyright of LVLMs without modifying the original model.\nSpecifically, we construct adversarial images through targeted attacks against\nthe original model, enabling it to generate specific outputs. To ensure these\nattacks remain effective on potential fine-tuned models to trigger copyright\ntracking, we allow the original model to learn the trigger images by updating\nparameters in the opposite direction during the adversarial attack process.\nNotably, the proposed method can be applied after the release of the original\nmodel, thus not affecting the model's performance and behavior. To simulate\nreal-world applications, we fine-tune the original model using various\nstrategies across diverse datasets, creating a range of models for copyright\nverification. Extensive experiments demonstrate that our method can more\neffectively identify the original copyright of fine-tuned models compared to\nbaseline methods. Therefore, this work provides a powerful tool for tracking\ncopyrights and detecting unlicensed usage of LVLMs."
    },
    {
        "date": "2025-02",
        "title": "UNCA: A Neutrosophic-Based Framework for Robust Clustering and Enhanced Data Interpretation",
        "author": "D. Dhinakaran, S. Edwin Raja, S. Gopalakrishnan, D. Selvaraj, and S. D. Lalitha",
        "link": "http://arxiv.org/abs/2502.17523v1",
        "abstract": "Accurately representing the complex linkages and inherent uncertainties\nincluded in huge datasets is still a major difficulty in the field of data\nclustering. We address these issues with our proposed Unified Neutrosophic\nClustering Algorithm (UNCA), which combines a multifaceted strategy with\nNeutrosophic logic to improve clustering performance. UNCA starts with a\nfull-fledged similarity examination via a {\\lambda}-cutting matrix that filters\nmeaningful relationships between each two points of data. Then, we initialize\ncentroids for Neutrosophic K-Means clustering, where the membership values are\nbased on their degrees of truth, indeterminacy and falsity. The algorithm then\nintegrates with a dynamic network visualization and MST (Minimum Spanning Tree)\nso that a visual interpretation of the relationships between the clusters can\nbe clearly represented. UNCA employs SingleValued Neutrosophic Sets (SVNSs) to\nrefine cluster assignments, and after fuzzifying similarity measures,\nguarantees a precise clustering result. The final step involves solidifying the\nclustering results through defuzzification methods, offering definitive cluster\nassignments. According to the performance evaluation results, UNCA outperforms\nconventional approaches in several metrics: it achieved a Silhouette Score of\n0.89 on the Iris Dataset, a Davies-Bouldin Index of 0.59 on the Wine Dataset,\nan Adjusted Rand Index (ARI) of 0.76 on the Digits Dataset, and a Normalized\nMutual Information (NMI) of 0.80 on the Customer Segmentation Dataset. These\nresults demonstrate how UNCA enhances interpretability and resilience in\naddition to improving clustering accuracy when contrasted with Fuzzy C-Means\n(FCM), Neutrosophic C-Means (NCM), as well as Kernel Neutrosophic C-Means\n(KNCM). This makes UNCA a useful tool for complex data processing tasks"
    },
    {
        "date": "2025-02",
        "title": "Can Indirect Prompt Injection Attacks Be Detected and Removed?",
        "author": "Yulin Chen, Haoran Li, Yuan Sui, Yufei He, Yue Liu, Yangqiu Song, and Bryan Hooi",
        "link": "http://arxiv.org/abs/2502.16580v1",
        "abstract": "Prompt injection attacks manipulate large language models (LLMs) by\nmisleading them to deviate from the original input instructions and execute\nmaliciously injected instructions, because of their instruction-following\ncapabilities and inability to distinguish between the original input\ninstructions and maliciously injected instructions. To defend against such\nattacks, recent studies have developed various detection mechanisms. While\nsignificant efforts have focused on detecting direct prompt injection attacks,\nwhere injected instructions are directly from the attacker who is also the\nuser, limited attention has been given to indirect prompt injection attacks,\nwhere injected instructions are indirectly from external tools, such as a\nsearch engine. Moreover, current works mainly investigate injection detection\nmethods and pay less attention to the post-processing method that aims to\nmitigate the injection after detection. In this paper, we investigate the\nfeasibility of detecting and removing indirect prompt injection attacks, and we\nconstruct a benchmark dataset for evaluation. For detection, we assess the\nperformance of existing LLMs and open-source detection models, and we further\ntrain detection models using our crafted training datasets. For removal, we\nevaluate two intuitive methods: (1) the segmentation removal method, which\nsegments the injected document and removes parts containing injected\ninstructions, and (2) the extraction removal method, which trains an extraction\nmodel to identify and remove injected instructions."
    },
    {
        "date": "2025-02",
        "title": "Multi-Target Federated Backdoor Attack Based on Feature Aggregation",
        "author": "Lingguag Hao, Kuangrong Hao, Bing Wei, and Xue-song Tang",
        "link": "http://arxiv.org/abs/2502.16545v1",
        "abstract": "Current federated backdoor attacks focus on collaboratively training backdoor\ntriggers, where multiple compromised clients train their local trigger patches\nand then merge them into a global trigger during the inference phase. However,\nthese methods require careful design of the shape and position of trigger\npatches and lack the feature interactions between trigger patches during\ntraining, resulting in poor backdoor attack success rates. Moreover, the pixels\nof the patches remain untruncated, thereby making abrupt areas in backdoor\nexamples easily detectable by the detection algorithm. To this end, we propose\na novel benchmark for the federated backdoor attack based on feature\naggregation. Specifically, we align the dimensions of triggers with images,\ndelimit the trigger's pixel boundaries, and facilitate feature interaction\namong local triggers trained by each compromised client. Furthermore,\nleveraging the intra-class attack strategy, we propose the simultaneous\ngeneration of backdoor triggers for all target classes, significantly reducing\nthe overall production time for triggers across all target classes and\nincreasing the risk of the federated model being attacked. Experiments\ndemonstrate that our method can not only bypass the detection of defense\nmethods while patch-based methods fail, but also achieve a zero-shot backdoor\nattack with a success rate of 77.39%. To the best of our knowledge, our work is\nthe first to implement such a zero-shot attack in federated learning. Finally,\nwe evaluate attack performance by varying the trigger's training factors,\nincluding poison location, ratio, pixel bound, and trigger training duration\n(local epochs and communication rounds)."
    }
]