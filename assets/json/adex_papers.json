[
    {
        "date": "2025-03",
        "title": "Optimal Complexity in Byzantine-Robust Distributed Stochastic Optimization with Data Heterogeneity",
        "author": "Qiankun Shi, Jie Peng, Kun Yuan, Xiao Wang, and Qing Ling",
        "link": "http://arxiv.org/abs/2503.16337v1",
        "abstract": "In this paper, we establish tight lower bounds for Byzantine-robust\ndistributed first-order stochastic optimization methods in both strongly convex\nand non-convex stochastic optimization. We reveal that when the distributed\nnodes have heterogeneous data, the convergence error comprises two components:\na non-vanishing Byzantine error and a vanishing optimization error. We\nestablish the lower bounds on the Byzantine error and on the minimum number of\nqueries to a stochastic gradient oracle required to achieve an arbitrarily\nsmall optimization error. Nevertheless, we identify significant discrepancies\nbetween our established lower bounds and the existing upper bounds. To fill\nthis gap, we leverage the techniques of Nesterov's acceleration and variance\nreduction to develop novel Byzantine-robust distributed stochastic optimization\nmethods that provably match these lower bounds, up to logarithmic factors,\nimplying that our established lower bounds are tight."
    },
    {
        "date": "2025-03",
        "title": "Securing Satellite Communications: Real-Time Video Encryption Scheme on Satellite Payloads",
        "author": "Hanshuo Qiu, Jing Lian, Xiaoyuan Wang, and Jizhao Liu",
        "link": "http://arxiv.org/abs/2503.16287v1",
        "abstract": "The rapid development of low-Earth orbit (LEO) satellite constellations and\nsatellite communication systems has elevated the importance of secure video\ntransmission, which is the key to applications such as remote sensing, disaster\nrelief, and secure information exchange. In this context, three serious issues\narise concerning real-time encryption of videos on satellite embedded devices:\n(a) the challenge of achieving real-time performance; (b) the limitations posed\nby the constrained computing performance of satellite payloads; and (c) the\npotential for excessive power consumption leading to overheating, thereby\nescalating safety risks. To overcome these challenges, this study introduced a\nnovel approach for encrypting videos by employing two 1D chaotic maps, which\nwas deployed on a satellite for the first time. The experiment on the satellite\nconfirms that our scheme is suitable for complex satellite environments. In\naddition, the proposed chaotic maps were implemented on a Field Programmable\nGate Array (FPGA) platform, and simulation results showed consistency with\nthose obtained on a Raspberry Pi. Experiments on the Raspberry Pi 4B\ndemonstrate exceptional real-time performance and low power consumption,\nvalidating both the hardware feasibility and the stability of our design.\nRigorous statistical testing also confirms the scheme's resilience against a\nvariety of attacks, underscoring its potential for secure, real-time data\ntransmission in satellite communication systems."
    },
    {
        "date": "2025-03",
        "title": "Rethinking Robustness in Machine Learning: A Posterior Agreement Approach",
        "author": "Jo\u00e3o Borges S. Carvalho, Alessandro Torcinovich, Victor Jimenez Rodriguez, Antonio E. Cin\u00e0, Carlos Cotrini, Lea Sch\u00f6nherr, and Joachim M. Buhmann",
        "link": "http://arxiv.org/abs/2503.16271v1",
        "abstract": "The robustness of algorithms against covariate shifts is a fundamental\nproblem with critical implications for the deployment of machine learning\nalgorithms in the real world. Current evaluation methods predominantly match\nthe robustness definition to that of standard generalization, relying on\nstandard metrics like accuracy-based scores, which, while designed for\nperformance assessment, lack a theoretical foundation encompassing their\napplication in estimating robustness to distribution shifts. In this work, we\nset the desiderata for a robustness metric, and we propose a novel principled\nframework for the robustness assessment problem that directly follows the\nPosterior Agreement (PA) theory of model validation. Specifically, we extend\nthe PA framework to the covariate shift setting by proposing a PA metric for\nrobustness evaluation in supervised classification tasks. We assess the\nsoundness of our metric in controlled environments and through an empirical\nrobustness analysis in two different covariate shift scenarios: adversarial\nlearning and domain generalization. We illustrate the suitability of PA by\nevaluating several models under different nature and magnitudes of shift, and\nproportion of affected observations. The results show that the PA metric\nprovides a sensible and consistent analysis of the vulnerabilities in learning\nalgorithms, even in the presence of few perturbed observations."
    },
    {
        "date": "2025-03",
        "title": "From Head to Tail: Efficient Black-box Model Inversion Attack via Long-tailed Learning",
        "author": "Ziang Li, Hongguang Zhang, Juan Wang, Meihui Chen, Hongxin Hu, Wenzhe Yi, Xiaoyang Xu, Mengda Yang, and Chenjun Ma",
        "link": "http://arxiv.org/abs/2503.16266v1",
        "abstract": "Model Inversion Attacks (MIAs) aim to reconstruct private training data from\nmodels, leading to privacy leakage, particularly in facial recognition systems.\nAlthough many studies have enhanced the effectiveness of white-box MIAs, less\nattention has been paid to improving efficiency and utility under limited\nattacker capabilities. Existing black-box MIAs necessitate an impractical\nnumber of queries, incurring significant overhead. Therefore, we analyze the\nlimitations of existing MIAs and introduce Surrogate Model-based Inversion with\nLong-tailed Enhancement (SMILE), a high-resolution oriented and query-efficient\nMIA for the black-box setting. We begin by analyzing the initialization of MIAs\nfrom a data distribution perspective and propose a long-tailed surrogate\ntraining method to obtain high-quality initial points. We then enhance the\nattack's effectiveness by employing the gradient-free black-box optimization\nalgorithm selected by NGOpt. Our experiments show that SMILE outperforms\nexisting state-of-the-art black-box MIAs while requiring only about 5% of the\nquery overhead."
    },
    {
        "date": "2025-03",
        "title": "AI Agents in Cryptoland: Practical Attacks and No Silver Bullet",
        "author": "Atharv Singh Patlan, Peiyao Sheng, S. Ashwin Hebbar, Prateek Mittal, and Pramod Viswanath",
        "link": "http://arxiv.org/abs/2503.16248v1",
        "abstract": "The integration of AI agents with Web3 ecosystems harnesses their\ncomplementary potential for autonomy and openness, yet also introduces\nunderexplored security risks, as these agents dynamically interact with\nfinancial protocols and immutable smart contracts. This paper investigates the\nvulnerabilities of AI agents within blockchain-based financial ecosystems when\nexposed to adversarial threats in real-world scenarios. We introduce the\nconcept of context manipulation -- a comprehensive attack vector that exploits\nunprotected context surfaces, including input channels, memory modules, and\nexternal data feeds. Through empirical analysis of ElizaOS, a decentralized AI\nagent framework for automated Web3 operations, we demonstrate how adversaries\ncan manipulate context by injecting malicious instructions into prompts or\nhistorical interaction records, leading to unintended asset transfers and\nprotocol violations which could be financially devastating. Our findings\nindicate that prompt-based defenses are insufficient, as malicious inputs can\ncorrupt an agent's stored context, creating cascading vulnerabilities across\ninteractions and platforms. This research highlights the urgent need to develop\nAI agents that are both secure and fiduciarily responsible."
    },
    {
        "date": "2025-03",
        "title": "Narrowing Class-Wise Robustness Gaps in Adversarial Training",
        "author": "Fatemeh Amerehi, and Patrick Healy",
        "link": "http://arxiv.org/abs/2503.16179v1",
        "abstract": "Efforts to address declining accuracy as a result of data shifts often\ninvolve various data-augmentation strategies. Adversarial training is one such\nmethod, designed to improve robustness to worst-case distribution shifts caused\nby adversarial examples. While this method can improve robustness, it may also\nhinder generalization to clean examples and exacerbate performance imbalances\nacross different classes. This paper explores the impact of adversarial\ntraining on both overall and class-specific performance, as well as its\nspill-over effects. We observe that enhanced labeling during training boosts\nadversarial robustness by 53.50% and mitigates class imbalances by 5.73%,\nleading to improved accuracy in both clean and adversarial settings compared to\nstandard adversarial training."
    },
    {
        "date": "2025-03",
        "title": "Towards Lighter and Robust Evaluation for Retrieval Augmented Generation",
        "author": "Alex-Razvan Ispas, Charles-Elie Simon, Fabien Caspani, and Vincent Guigue",
        "link": "http://arxiv.org/abs/2503.16161v1",
        "abstract": "Large Language Models are prompting us to view more NLP tasks from a\ngenerative perspective. At the same time, they offer a new way of accessing\ninformation, mainly through the RAG framework. While there have been notable\nimprovements for the autoregressive models, overcoming hallucination in the\ngenerated answers remains a continuous problem. A standard solution is to use\ncommercial LLMs, such as GPT4, to evaluate these algorithms. However, such\nframeworks are expensive and not very transparent. Therefore, we propose a\nstudy which demonstrates the interest of open-weight models for evaluating RAG\nhallucination. We develop a lightweight approach using smaller, quantized LLMs\nto provide an accessible and interpretable metric that gives continuous scores\nfor the generated answer with respect to their correctness and faithfulness.\nThis score allows us to question decisions' reliability and explore thresholds\nto develop a new AUC metric as an alternative to correlation with human\njudgment."
    },
    {
        "date": "2025-03",
        "title": "3-D Image-to-Image Fusion in Lightsheet Microscopy by Two-Step Adversarial Network: Contribution to the FuseMyCells Challenge",
        "author": "Marek Wodzinski, and Henning M\u00fcller",
        "link": "http://arxiv.org/abs/2503.16075v1",
        "abstract": "Lightsheet microscopy is a powerful 3-D imaging technique that addresses\nlimitations of traditional optical and confocal microscopy but suffers from a\nlow penetration depth and reduced image quality at greater depths. Multiview\nlightsheet microscopy improves 3-D resolution by combining multiple views but\nsimultaneously increasing the complexity and the photon budget, leading to\npotential photobleaching and phototoxicity. The FuseMyCells challenge,\norganized in conjunction with the IEEE ISBI 2025 conference, aims to benchmark\ndeep learning-based solutions for fusing high-quality 3-D volumes from single\n3-D views, potentially simplifying procedures and conserving the photon budget.\nIn this work, we propose a contribution to the FuseMyCells challenge based on a\ntwo-step procedure. The first step processes a downsampled version of the image\nto capture the entire region of interest, while the second step uses a\npatch-based approach for high-resolution inference, incorporating adversarial\nloss to enhance visual outcomes. This method addresses challenges related to\nhigh data resolution, the necessity of global context, and the preservation of\nhigh-frequency details. Experimental results demonstrate the effectiveness of\nour approach, highlighting its potential to improve 3-D image fusion quality\nand extend the capabilities of lightsheet microscopy. The average SSIM for the\nnucleus and membranes is greater than 0.85 and 0.91, respectively."
    },
    {
        "date": "2025-03",
        "title": "Temporal-Spatial Attention Network (TSAN) for DoS Attack Detection in Network Traffic",
        "author": "Bisola Faith Kayode, Akinyemi Sadeeq Akintola, Oluwole Fagbohun, Egonna Anaesiuba-Bristol, Onyekachukwu Ojumah, Oluwagbade Odimayo, Toyese Oloyede, Aniema Inyang, Teslim Kazeem, Habeeb Alli, Udodirim Ibem Offia, and Prisca Chinazor Amajuoyi",
        "link": "http://arxiv.org/abs/2503.16047v2",
        "abstract": "Denial-of-Service (DoS) attacks remain a critical threat to network security,\ndisrupting services and causing significant economic losses. Traditional\ndetection methods, including statistical and rule-based models, struggle to\nadapt to evolving attack patterns. To address this challenge, we propose a\nnovel Temporal-Spatial Attention Network (TSAN) architecture for detecting\nDenial of Service (DoS) attacks in network traffic. By leveraging both temporal\nand spatial features of network traffic, our approach captures complex traffic\npatterns and anomalies that traditional methods might miss. The TSAN model\nincorporates transformer-based temporal encoding, convolutional spatial\nencoding, and a cross-attention mechanism to fuse these complementary feature\nspaces. Additionally, we employ multi-task learning with auxiliary tasks to\nenhance the model's robustness. Experimental results on the NSL-KDD dataset\ndemonstrate that TSAN outperforms state-of-the-art models, achieving superior\naccuracy, precision, recall, and F1-score while maintaining computational\nefficiency for real-time deployment. The proposed architecture offers an\noptimal balance between detection accuracy and computational overhead, making\nit highly suitable for real-world network security applications."
    },
    {
        "date": "2025-03",
        "title": "BadToken: Token-level Backdoor Attacks to Multi-modal Large Language Models",
        "author": "Zenghui Yuan, Jiawen Shi, Pan Zhou, Neil Zhenqiang Gong, and Lichao Sun",
        "link": "http://arxiv.org/abs/2503.16023v1",
        "abstract": "Multi-modal large language models (MLLMs) extend large language models (LLMs)\nto process multi-modal information, enabling them to generate responses to\nimage-text inputs. MLLMs have been incorporated into diverse multi-modal\napplications, such as autonomous driving and medical diagnosis, via\nplug-and-play without fine-tuning. This deployment paradigm increases the\nvulnerability of MLLMs to backdoor attacks. However, existing backdoor attacks\nagainst MLLMs achieve limited effectiveness and stealthiness. In this work, we\npropose BadToken, the first token-level backdoor attack to MLLMs. BadToken\nintroduces two novel backdoor behaviors: Token-substitution and Token-addition,\nwhich enable flexible and stealthy attacks by making token-level modifications\nto the original output for backdoored inputs. We formulate a general\noptimization problem that considers the two backdoor behaviors to maximize the\nattack effectiveness. We evaluate BadToken on two open-source MLLMs and various\ntasks. Our results show that our attack maintains the model's utility while\nachieving high attack success rates and stealthiness. We also show the\nreal-world threats of BadToken in two scenarios, i.e., autonomous driving and\nmedical diagnosis. Furthermore, we consider defenses including fine-tuning and\ninput purification. Our results highlight the threat of our attack."
    },
    {
        "date": "2025-03",
        "title": "No Thing, Nothing: Highlighting Safety-Critical Classes for Robust LiDAR Semantic Segmentation in Adverse Weather",
        "author": "Junsung Park, Hwijeong Lee, Inha Kang, and Hyunjung Shim",
        "link": "http://arxiv.org/abs/2503.15910v1",
        "abstract": "Existing domain generalization methods for LiDAR semantic segmentation under\nadverse weather struggle to accurately predict \"things\" categories compared to\n\"stuff\" categories. In typical driving scenes, \"things\" categories can be\ndynamic and associated with higher collision risks, making them crucial for\nsafe navigation and planning. Recognizing the importance of \"things\"\ncategories, we identify their performance drop as a serious bottleneck in\nexisting approaches. We observed that adverse weather induces degradation of\nsemantic-level features and both corruption of local features, leading to a\nmisprediction of \"things\" as \"stuff\". To mitigate these corruptions, we suggest\nour method, NTN - segmeNt Things for No-accident. To address semantic-level\nfeature corruption, we bind each point feature to its superclass, preventing\nthe misprediction of things classes into visually dissimilar categories.\nAdditionally, to enhance robustness against local corruption caused by adverse\nweather, we define each LiDAR beam as a local region and propose a\nregularization term that aligns the clean data with its corrupted counterpart\nin feature space. NTN achieves state-of-the-art performance with a +2.6 mIoU\ngain on the SemanticKITTI-to-SemanticSTF benchmark and +7.9 mIoU on the\nSemanticPOSS-to-SemanticSTF benchmark. Notably, NTN achieves a +4.8 and +7.9\nmIoU improvement on \"things\" classes, respectively, highlighting its\neffectiveness."
    },
    {
        "date": "2025-03",
        "title": "AutoRedTeamer: Autonomous Red Teaming with Lifelong Attack Integration",
        "author": "Andy Zhou, Kevin Wu, Francesco Pinto, Zhaorun Chen, Yi Zeng, Yu Yang, Shuang Yang, Sanmi Koyejo, James Zou, and Bo Li",
        "link": "http://arxiv.org/abs/2503.15754v1",
        "abstract": "As large language models (LLMs) become increasingly capable, security and\nsafety evaluation are crucial. While current red teaming approaches have made\nstrides in assessing LLM vulnerabilities, they often rely heavily on human\ninput and lack comprehensive coverage of emerging attack vectors. This paper\nintroduces AutoRedTeamer, a novel framework for fully automated, end-to-end red\nteaming against LLMs. AutoRedTeamer combines a multi-agent architecture with a\nmemory-guided attack selection mechanism to enable continuous discovery and\nintegration of new attack vectors. The dual-agent framework consists of a red\nteaming agent that can operate from high-level risk categories alone to\ngenerate and execute test cases and a strategy proposer agent that autonomously\ndiscovers and implements new attacks by analyzing recent research. This modular\ndesign allows AutoRedTeamer to adapt to emerging threats while maintaining\nstrong performance on existing attack vectors. We demonstrate AutoRedTeamer's\neffectiveness across diverse evaluation settings, achieving 20% higher attack\nsuccess rates on HarmBench against Llama-3.1-70B while reducing computational\ncosts by 46% compared to existing approaches. AutoRedTeamer also matches the\ndiversity of human-curated benchmarks in generating test cases, providing a\ncomprehensive, scalable, and continuously evolving framework for evaluating the\nsecurity of AI systems."
    },
    {
        "date": "2025-03",
        "title": "Reinforcement Learning Environment with LLM-Controlled Adversary in D&D 5th Edition Combat",
        "author": "Joseph Emmanuel DL Dayo, Michel Onasis S. Ogbinar, and Prospero C. Naval Jr",
        "link": "http://arxiv.org/abs/2503.15726v1",
        "abstract": "The objective of this study is to design and implement a reinforcement\nlearning (RL) environment using D\\&D 5E combat scenarios to challenge smaller\nRL agents through interaction with a robust adversarial agent controlled by\nadvanced Large Language Models (LLMs) like GPT-4o and LLaMA 3 8B. This research\nemploys Deep Q-Networks (DQN) for the smaller agents, creating a testbed for\nstrategic AI development that also serves as an educational tool by simulating\ndynamic and unpredictable combat scenarios. We successfully integrated\nsophisticated language models into the RL framework, enhancing strategic\ndecision-making processes. Our results indicate that while RL agents generally\noutperform LLM-controlled adversaries in standard metrics, the strategic depth\nprovided by LLMs significantly enhances the overall AI capabilities in this\ncomplex, rule-based setting. The novelty of our approach and its implications\nfor mastering intricate environments and developing adaptive strategies are\ndiscussed, alongside potential innovations in AI-driven interactive\nsimulations. This paper aims to demonstrate how integrating LLMs can create\nmore robust and adaptable AI systems, providing valuable insights for further\nresearch and educational applications."
    },
    {
        "date": "2025-03",
        "title": "Improving Adversarial Transferability on Vision Transformers via Forward Propagation Refinement",
        "author": "Yuchen Ren, Zhengyu Zhao, Chenhao Lin, Bo Yang, Lu Zhou, Zhe Liu, and Chao Shen",
        "link": "http://arxiv.org/abs/2503.15404v1",
        "abstract": "Vision Transformers (ViTs) have been widely applied in various computer\nvision and vision-language tasks. To gain insights into their robustness in\npractical scenarios, transferable adversarial examples on ViTs have been\nextensively studied. A typical approach to improving adversarial\ntransferability is by refining the surrogate model. However, existing work on\nViTs has restricted their surrogate refinement to backward propagation. In this\nwork, we instead focus on Forward Propagation Refinement (FPR) and specifically\nrefine two key modules of ViTs: attention maps and token embeddings. For\nattention maps, we propose Attention Map Diversification (AMD), which\ndiversifies certain attention maps and also implicitly imposes beneficial\ngradient vanishing during backward propagation. For token embeddings, we\npropose Momentum Token Embedding (MTE), which accumulates historical token\nembeddings to stabilize the forward updates in both the Attention and MLP\nblocks. We conduct extensive experiments with adversarial examples transferred\nfrom ViTs to various CNNs and ViTs, demonstrating that our FPR outperforms the\ncurrent best (backward) surrogate refinement by up to 7.0\\% on average. We also\nvalidate its superiority against popular defenses and its compatibility with\nother transfer methods. Codes and appendix are available at\nhttps://github.com/RYC-98/FPR."
    },
    {
        "date": "2025-03",
        "title": "Robustness of Nonlinear Representation Learning",
        "author": "Simon Buchholz, and Bernhard Sch\u00f6lkopf",
        "link": "http://arxiv.org/abs/2503.15355v1",
        "abstract": "We study the problem of unsupervised representation learning in slightly\nmisspecified settings, and thus formalize the study of robustness of nonlinear\nrepresentation learning. We focus on the case where the mixing is close to a\nlocal isometry in a suitable distance and show based on existing rigidity\nresults that the mixing can be identified up to linear transformations and\nsmall errors. In a second step, we investigate Independent Component Analysis\n(ICA) with observations generated according to $x=f(s)=As+h(s)$ where $A$ is an\ninvertible mixing matrix and $h$ a small perturbation. We show that we can\napproximately recover the matrix $A$ and the independent components. Together,\nthese two results show approximate identifiability of nonlinear ICA with almost\nisometric mixing functions. Those results are a step towards identifiability\nresults for unsupervised representation learning for real-world data that do\nnot follow restrictive model classes."
    },
    {
        "date": "2025-03",
        "title": "Increasing the Robustness of the Fine-tuned Multilingual Machine-Generated Text Detectors",
        "author": "Dominik Macko, Robert Moro, and Ivan Srba",
        "link": "http://arxiv.org/abs/2503.15128v1",
        "abstract": "Since the proliferation of LLMs, there have been concerns about their misuse\nfor harmful content creation and spreading. Recent studies justify such fears,\nproviding evidence of LLM vulnerabilities and high potential of their misuse.\nHumans are no longer able to distinguish between high-quality machine-generated\nand authentic human-written texts. Therefore, it is crucial to develop\nautomated means to accurately detect machine-generated content. It would enable\nto identify such content in online information space, thus providing an\nadditional information about its credibility. This work addresses the problem\nby proposing a robust fine-tuning process of LLMs for the detection task,\nmaking the detectors more robust against obfuscation and more generalizable to\nout-of-distribution data."
    },
    {
        "date": "2025-03",
        "title": "StyleLoco: Generative Adversarial Distillation for Natural Humanoid Robot Locomotion",
        "author": "Le Ma, Ziyu Meng, Tengyu Liu, Yuhan Li, Ran Song, Wei Zhang, and Siyuan Huang",
        "link": "http://arxiv.org/abs/2503.15082v1",
        "abstract": "Humanoid robots are anticipated to acquire a wide range of locomotion\ncapabilities while ensuring natural movement across varying speeds and\nterrains. Existing methods encounter a fundamental dilemma in learning humanoid\nlocomotion: reinforcement learning with handcrafted rewards can achieve agile\nlocomotion but produces unnatural gaits, while Generative Adversarial Imitation\nLearning (GAIL) with motion capture data yields natural movements but suffers\nfrom unstable training processes and restricted agility. Integrating these\napproaches proves challenging due to the inherent heterogeneity between expert\npolicies and human motion datasets. To address this, we introduce StyleLoco, a\nnovel two-stage framework that bridges this gap through a Generative\nAdversarial Distillation (GAD) process. Our framework begins by training a\nteacher policy using reinforcement learning to achieve agile and dynamic\nlocomotion. It then employs a multi-discriminator architecture, where distinct\ndiscriminators concurrently extract skills from both the teacher policy and\nmotion capture data. This approach effectively combines the agility of\nreinforcement learning with the natural fluidity of human-like movements while\nmitigating the instability issues commonly associated with adversarial\ntraining. Through extensive simulation and real-world experiments, we\ndemonstrate that StyleLoco enables humanoid robots to perform diverse\nlocomotion tasks with the precision of expertly trained policies and the\nnatural aesthetics of human motion, successfully transferring styles across\ndifferent movement types while maintaining stable locomotion across a broad\nspectrum of command inputs."
    },
    {
        "date": "2025-03",
        "title": "A Semantic and Clean-label Backdoor Attack against Graph Convolutional Networks",
        "author": "Jiazhu Dai, and Haoyu Sun",
        "link": "http://arxiv.org/abs/2503.14922v1",
        "abstract": "Graph Convolutional Networks (GCNs) have shown excellent performance in\ngraph-structured tasks such as node classification and graph classification.\nHowever, recent research has shown that GCNs are vulnerable to a new type of\nthreat called the backdoor attack, where the adversary can inject a hidden\nbackdoor into the GCNs so that the backdoored model performs well on benign\nsamples, whereas its prediction will be maliciously changed to the\nattacker-specified target label if the hidden backdoor is activated by the\nattacker-defined trigger. Clean-label backdoor attack and semantic backdoor\nattack are two new backdoor attacks to Deep Neural Networks (DNNs), they are\nmore imperceptible and have posed new and serious threats. The semantic and\nclean-label backdoor attack is not fully explored in GCNs. In this paper, we\npropose a semantic and clean-label backdoor attack against GCNs under the\ncontext of graph classification to reveal the existence of this security\nvulnerability in GCNs. Specifically, SCLBA conducts an importance analysis on\ngraph samples to select one type of node as semantic trigger, which is then\ninserted into the graph samples to create poisoning samples without changing\nthe labels of the poisoning samples to the attacker-specified target label. We\nevaluate SCLBA on multiple datasets and the results show that SCLBA can achieve\nattack success rates close to 99% with poisoning rates of less than 3%, and\nwith almost no impact on the performance of model on benign samples."
    },
    {
        "date": "2025-03",
        "title": "Robust Distribution Alignment for Industrial Anomaly Detection under Distribution Shift",
        "author": "Jingyi Liao, Xun Xu, Yongyi Su, Rong-Cheng Tu, Yifan Liu, Dacheng Tao, and Xulei Yang",
        "link": "http://arxiv.org/abs/2503.14910v1",
        "abstract": "Anomaly detection plays a crucial role in quality control for industrial\napplications. However, ensuring robustness under unseen domain shifts such as\nlighting variations or sensor drift remains a significant challenge. Existing\nmethods attempt to address domain shifts by training generalizable models but\noften rely on prior knowledge of target distributions and can hardly generalise\nto backbones designed for other data modalities. To overcome these limitations,\nwe build upon memory-bank-based anomaly detection methods, optimizing a robust\nSinkhorn distance on limited target training data to enhance generalization to\nunseen target domains. We evaluate the effectiveness on both 2D and 3D anomaly\ndetection benchmarks with simulated distribution shifts. Our proposed method\ndemonstrates superior results compared with state-of-the-art anomaly detection\nand domain adaptation methods."
    },
    {
        "date": "2025-03",
        "title": "Robust Support Vector Machines for Imbalanced and Noisy Data via Benders Decomposition",
        "author": "Seyed Mojtaba Mohasel, and Hamidreza Koosha",
        "link": "http://arxiv.org/abs/2503.14873v1",
        "abstract": "This study introduces a novel formulation to enhance Support Vector Machines\n(SVMs) in handling class imbalance and noise. Unlike the conventional Soft\nMargin SVM, which penalizes the magnitude of constraint violations, the\nproposed model quantifies the number of violations and aims to minimize their\nfrequency. To achieve this, a binary variable is incorporated into the\nobjective function of the primal SVM formulation, replacing the traditional\nslack variable. Furthermore, each misclassified sample is assigned a priority\nand an associated constraint. The resulting formulation is a mixed-integer\nprogramming model, efficiently solved using Benders decomposition. The proposed\nmodel's performance was benchmarked against existing models, including Soft\nMargin SVM, weighted SVM, and NuSVC. Two primary hypotheses were examined: 1)\nThe proposed model improves the F1-score for the minority class in imbalanced\nclassification tasks. 2) The proposed model enhances classification accuracy in\nnoisy datasets. These hypotheses were evaluated using a Wilcoxon test across\nmultiple publicly available datasets from the OpenML repository. The results\nsupported both hypotheses (\\( p < 0.05 \\)). In addition, the proposed model\nexhibited several interesting properties, such as improved robustness to noise,\na decision boundary shift favoring the minority class, a reduced number of\nsupport vectors, and decreased prediction time. The open-source Python\nimplementation of the proposed SVM model is available."
    },
    {
        "date": "2025-03",
        "title": "On the Robustness Tradeoff in Fine-Tuning",
        "author": "Kunyang Li, Jean-Charles Noirot Ferrand, Ryan Sheatsley, Blaine Hoak, Yohan Beugin, Eric Pauley, and Patrick McDaniel",
        "link": "http://arxiv.org/abs/2503.14836v1",
        "abstract": "Fine-tuning has become the standard practice for adapting pre-trained\n(upstream) models to downstream tasks. However, the impact on model robustness\nis not well understood. In this work, we characterize the robustness-accuracy\ntrade-off in fine-tuning. We evaluate the robustness and accuracy of fine-tuned\nmodels over 6 benchmark datasets and 7 different fine-tuning strategies. We\nobserve a consistent trade-off between adversarial robustness and accuracy.\nPeripheral updates such as BitFit are more effective for simple tasks--over 75%\nabove the average measured with area under the Pareto frontiers on CIFAR-10 and\nCIFAR-100. In contrast, fine-tuning information-heavy layers, such as attention\nlayers via Compacter, achieves a better Pareto frontier on more complex\ntasks--57.5% and 34.6% above the average on Caltech-256 and CUB-200,\nrespectively. Lastly, we observe that robustness of fine-tuning against\nout-of-distribution data closely tracks accuracy. These insights emphasize the\nneed for robustness-aware fine-tuning to ensure reliable real-world\ndeployments."
    },
    {
        "date": "2025-03",
        "title": "Robust Transmission of Punctured Text with Large Language Model-based Recovery",
        "author": "Sojeong Park, Hyeonho Noh, and Hyun Jong Yang",
        "link": "http://arxiv.org/abs/2503.14831v1",
        "abstract": "With the recent advancements in deep learning, semantic communication which\ntransmits only task-oriented features, has rapidly emerged. However, since\nfeature extraction relies on learning-based models, its performance\nfundamentally depends on the training dataset or tasks. For practical\nscenarios, it is essential to design a model that demonstrates robust\nperformance regardless of dataset or tasks. In this correspondence, we propose\na novel text transmission model that selects and transmits only a few\ncharacters and recovers the missing characters at the receiver using a large\nlanguage model (LLM). Additionally, we propose a novel importance character\nextractor (ICE), which selects transmitted characters to enhance LLM recovery\nperformance. Simulations demonstrate that the proposed filter selection by ICE\noutperforms random filter selection, which selects transmitted characters\nrandomly. Moreover, the proposed model exhibits robust performance across\ndifferent datasets and tasks and outperforms traditional bit-based\ncommunication in low signal-to-noise ratio conditions."
    },
    {
        "date": "2025-03",
        "title": "Temporal Context Awareness: A Defense Framework Against Multi-turn Manipulation Attacks on Large Language Models",
        "author": "Prashant Kulkarni, and Assaf Namer",
        "link": "http://arxiv.org/abs/2503.15560v1",
        "abstract": "Large Language Models (LLMs) are increasingly vulnerable to sophisticated\nmulti-turn manipulation attacks, where adversaries strategically build context\nthrough seemingly benign conversational turns to circumvent safety measures and\nelicit harmful or unauthorized responses. These attacks exploit the temporal\nnature of dialogue to evade single-turn detection methods, representing a\ncritical security vulnerability with significant implications for real-world\ndeployments.\n  This paper introduces the Temporal Context Awareness (TCA) framework, a novel\ndefense mechanism designed to address this challenge by continuously analyzing\nsemantic drift, cross-turn intention consistency and evolving conversational\npatterns. The TCA framework integrates dynamic context embedding analysis,\ncross-turn consistency verification, and progressive risk scoring to detect and\nmitigate manipulation attempts effectively. Preliminary evaluations on\nsimulated adversarial scenarios demonstrate the framework's potential to\nidentify subtle manipulation patterns often missed by traditional detection\ntechniques, offering a much-needed layer of security for conversational AI\nsystems. In addition to outlining the design of TCA , we analyze diverse attack\nvectors and their progression across multi-turn conversation, providing\nvaluable insights into adversarial tactics and their impact on LLM\nvulnerabilities. Our findings underscore the pressing need for robust,\ncontext-aware defenses in conversational AI systems and highlight TCA framework\nas a promising direction for securing LLMs while preserving their utility in\nlegitimate applications. We make our implementation available to support\nfurther research in this emerging area of AI security."
    },
    {
        "date": "2025-03",
        "title": "LipShiFT: A Certifiably Robust Shift-based Vision Transformer",
        "author": "Rohan Menon, Nicola Franco, and Stephan G\u00fcnnemann",
        "link": "http://arxiv.org/abs/2503.14751v1",
        "abstract": "Deriving tight Lipschitz bounds for transformer-based architectures presents\na significant challenge. The large input sizes and high-dimensional attention\nmodules typically prove to be crucial bottlenecks during the training process\nand leads to sub-optimal results. Our research highlights practical constraints\nof these methods in vision tasks. We find that Lipschitz-based margin training\nacts as a strong regularizer while restricting weights in successive layers of\nthe model. Focusing on a Lipschitz continuous variant of the ShiftViT model, we\naddress significant training challenges for transformer-based architectures\nunder norm-constrained input setting. We provide an upper bound estimate for\nthe Lipschitz constants of this model using the $l_2$ norm on common image\nclassification datasets. Ultimately, we demonstrate that our method scales to\nlarger models and advances the state-of-the-art in certified robustness for\ntransformer-based architectures."
    },
    {
        "date": "2025-03",
        "title": "A Comprehensive Study of LLM Secure Code Generation",
        "author": "Shih-Chieh Dai, Jun Xu, and Guanhong Tao",
        "link": "http://arxiv.org/abs/2503.15554v1",
        "abstract": "LLMs are widely used in software development. However, the code generated by\nLLMs often contains vulnerabilities. Several secure code generation methods\nhave been proposed to address this issue, but their current evaluation schemes\nleave several concerns unaddressed. Specifically, most existing studies\nevaluate security and functional correctness separately, using different\ndatasets. That is, they assess vulnerabilities using security-related code\ndatasets while validating functionality with general code datasets. In\naddition, prior research primarily relies on a single static analyzer, CodeQL,\nto detect vulnerabilities in generated code, which limits the scope of security\nevaluation.\n  In this work, we conduct a comprehensive study to systematically assess the\nimprovements introduced by four state-of-the-art secure code generation\ntechniques. Specifically, we apply both security inspection and functionality\nvalidation to the same generated code and evaluate these two aspects together.\nWe also employ three popular static analyzers and two LLMs to identify\npotential vulnerabilities in the generated code. Our study reveals that\nexisting techniques often compromise the functionality of generated code to\nenhance security. Their overall performance remains limited when evaluating\nsecurity and functionality together. In fact, many techniques even degrade the\nperformance of the base LLM. Our further inspection reveals that these\ntechniques often either remove vulnerable lines of code entirely or generate\n``garbage code'' that is unrelated to the intended task. Moreover, the commonly\nused static analyzer CodeQL fails to detect several vulnerabilities, further\nobscuring the actual security improvements achieved by existing techniques. Our\nstudy serves as a guideline for a more rigorous and comprehensive evaluation of\nsecure code generation performance in future work."
    },
    {
        "date": "2025-03",
        "title": "Personalized Attacks of Social Engineering in Multi-turn Conversations -- LLM Agents for Simulation and Detection",
        "author": "Tharindu Kumarage, Cameron Johnson, Jadie Adams, Lin Ai, Matthias Kirchner, Anthony Hoogs, Joshua Garland, Julia Hirschberg, Arslan Basharat, and Huan Liu",
        "link": "http://arxiv.org/abs/2503.15552v1",
        "abstract": "The rapid advancement of conversational agents, particularly chatbots powered\nby Large Language Models (LLMs), poses a significant risk of social engineering\n(SE) attacks on social media platforms. SE detection in multi-turn, chat-based\ninteractions is considerably more complex than single-instance detection due to\nthe dynamic nature of these conversations. A critical factor in mitigating this\nthreat is understanding the mechanisms through which SE attacks operate,\nspecifically how attackers exploit vulnerabilities and how victims' personality\ntraits contribute to their susceptibility. In this work, we propose an\nLLM-agentic framework, SE-VSim, to simulate SE attack mechanisms by generating\nmulti-turn conversations. We model victim agents with varying personality\ntraits to assess how psychological profiles influence susceptibility to\nmanipulation. Using a dataset of over 1000 simulated conversations, we examine\nattack scenarios in which adversaries, posing as recruiters, funding agencies,\nand journalists, attempt to extract sensitive information. Based on this\nanalysis, we present a proof of concept, SE-OmniGuard, to offer personalized\nprotection to users by leveraging prior knowledge of the victims personality,\nevaluating attack strategies, and monitoring information exchanges in\nconversations to identify potential SE attempts."
    },
    {
        "date": "2025-03",
        "title": "Anomaly-Flow: A Multi-domain Federated Generative Adversarial Network for Distributed Denial-of-Service Detection",
        "author": "Leonardo Henrique de Melo, Gustavo de Carvalho Bertoli, Michele Nogueira, Aldri Luiz dos Santos, and Louren\u00e7o Alves Pereira Junior",
        "link": "http://arxiv.org/abs/2503.14618v1",
        "abstract": "Distributed denial-of-service (DDoS) attacks remain a critical threat to\nInternet services, causing costly disruptions. While machine learning (ML) has\nshown promise in DDoS detection, current solutions struggle with multi-domain\nenvironments where attacks must be detected across heterogeneous networks and\norganizational boundaries. This limitation severely impacts the practical\ndeployment of ML-based defenses in real-world settings.\n  This paper introduces Anomaly-Flow, a novel framework that addresses this\ncritical gap by combining Federated Learning (FL) with Generative Adversarial\nNetworks (GANs) for privacy-preserving, multi-domain DDoS detection. Our\nproposal enables collaborative learning across diverse network domains while\npreserving data privacy through synthetic flow generation. Through extensive\nevaluation across three distinct network datasets, Anomaly-Flow achieves an\naverage F1-score of $0.747$, outperforming baseline models. Importantly, our\nframework enables organizations to share attack detection capabilities without\nexposing sensitive network data, making it particularly valuable for critical\ninfrastructure and privacy-sensitive sectors.\n  Beyond immediate technical contributions, this work provides insights into\nthe challenges and opportunities in multi-domain DDoS detection, establishing a\nfoundation for future research in collaborative network defense systems. Our\nfindings have important implications for academic research and industry\npractitioners working to deploy practical ML-based security solutions."
    },
    {
        "date": "2025-03",
        "title": "Doubly robust identification of treatment effects from multiple environments",
        "author": "Piersilvio De Bartolomeis, Julia Kostin, Javier Abad, Yixin Wang, and Fanny Yang",
        "link": "http://arxiv.org/abs/2503.14459v1",
        "abstract": "Practical and ethical constraints often require the use of observational data\nfor causal inference, particularly in medicine and social sciences. Yet,\nobservational datasets are prone to confounding, potentially compromising the\nvalidity of causal conclusions. While it is possible to correct for biases if\nthe underlying causal graph is known, this is rarely a feasible ask in\npractical scenarios. A common strategy is to adjust for all available\ncovariates, yet this approach can yield biased treatment effect estimates,\nespecially when post-treatment or unobserved variables are present. We propose\nRAMEN, an algorithm that produces unbiased treatment effect estimates by\nleveraging the heterogeneity of multiple data sources without the need to know\nor learn the underlying causal graph. Notably, RAMEN achieves doubly robust\nidentification: it can identify the treatment effect whenever the causal\nparents of the treatment or those of the outcome are observed, and the node\nwhose parents are observed satisfies an invariance assumption. Empirical\nevaluations on synthetic and real-world datasets show that our approach\noutperforms existing methods."
    },
    {
        "date": "2025-03",
        "title": "Efficient but Vulnerable: Benchmarking and Defending LLM Batch Prompting Attack",
        "author": "Murong Yue, and Ziyu Yao",
        "link": "http://arxiv.org/abs/2503.15551v1",
        "abstract": "Batch prompting, which combines a batch of multiple queries sharing the same\ncontext in one inference, has emerged as a promising solution to reduce\ninference costs. However, our study reveals a significant security\nvulnerability in batch prompting: malicious users can inject attack\ninstructions into a batch, leading to unwanted interference across all queries,\nwhich can result in the inclusion of harmful content, such as phishing links,\nor the disruption of logical reasoning. In this paper, we construct\nBATCHSAFEBENCH, a comprehensive benchmark comprising 150 attack instructions of\ntwo types and 8k batch instances, to study the batch prompting vulnerability\nsystematically. Our evaluation of both closed-source and open-weight LLMs\ndemonstrates that all LLMs are susceptible to batch-prompting attacks. We then\nexplore multiple defending approaches. While the prompting-based defense shows\nlimited effectiveness for smaller LLMs, the probing-based approach achieves\nabout 95% accuracy in detecting attacks. Additionally, we perform a mechanistic\nanalysis to understand the attack and identify attention heads that are\nresponsible for it."
    },
    {
        "date": "2025-03",
        "title": "Unveiling the Role of Randomization in Multiclass Adversarial Classification: Insights from Graph Theory",
        "author": "Lucas Gnecco-Heredia, Matteo Sammut, Muni Sreenivas Pydi, Rafael Pinot, Benjamin Negrevergne, and Yann Chevaleyre",
        "link": "http://arxiv.org/abs/2503.14299v1",
        "abstract": "Randomization as a mean to improve the adversarial robustness of machine\nlearning models has recently attracted significant attention. Unfortunately,\nmuch of the theoretical analysis so far has focused on binary classification,\nproviding only limited insights into the more complex multiclass setting. In\nthis paper, we take a step toward closing this gap by drawing inspiration from\nthe field of graph theory. Our analysis focuses on discrete data distributions,\nallowing us to cast the adversarial risk minimization problems within the\nwell-established framework of set packing problems. By doing so, we are able to\nidentify three structural conditions on the support of the data distribution\nthat are necessary for randomization to improve robustness. Furthermore, we are\nable to construct several data distributions where (contrarily to binary\nclassification) switching from a deterministic to a randomized solution\nsignificantly reduces the optimal adversarial risk. These findings highlight\nthe crucial role randomization can play in enhancing robustness to adversarial\nattacks in multiclass classification."
    },
    {
        "date": "2025-03",
        "title": "Robust Weight Imprinting: Insights from Neural Collapse and Proxy-Based Aggregation",
        "author": "Justus Westerhoff, Golzar Atefi, Mario Koddenbrock, Alexei Figueroa, Alexander L\u00f6ser, Erik Rodner, and Felix A. Gers",
        "link": "http://arxiv.org/abs/2503.14572v1",
        "abstract": "The capacity of a foundation model allows for adaptation to new downstream\ntasks. Weight imprinting is a universal and efficient method to fulfill this\npurpose. It has been reinvented several times, but it has not been\nsystematically studied. In this paper, we propose a framework for imprinting,\nidentifying three main components: generation, normalization, and aggregation.\nThis allows us to conduct an in-depth analysis of imprinting and a comparison\nof the existing work. We reveal the benefits of representing novel data with\nmultiple proxies in the generation step and show the importance of proper\nnormalization. We determine those proxies through clustering and propose a\nnovel variant of imprinting that outperforms previous work. We motivate this by\nthe neural collapse phenomenon -- an important connection that we can draw for\nthe first time. Our results show an increase of up to 4% in challenging\nscenarios with complex data distributions for new classes."
    },
    {
        "date": "2025-03",
        "title": "XOXO: Stealthy Cross-Origin Context Poisoning Attacks against AI Coding Assistants",
        "author": "Adam \u0160torek, Mukur Gupta, Noopur Bhatt, Aditya Gupta, Janie Kim, Prashast Srivastava, and Suman Jana",
        "link": "http://arxiv.org/abs/2503.14281v1",
        "abstract": "AI coding assistants are widely used for tasks like code generation, bug\ndetection, and comprehension. These tools now require large and complex\ncontexts, automatically sourced from various origins$\\unicode{x2014}$across\nfiles, projects, and contributors$\\unicode{x2014}$forming part of the prompt\nfed to underlying LLMs. This automatic context-gathering introduces new\nvulnerabilities, allowing attackers to subtly poison input to compromise the\nassistant's outputs, potentially generating vulnerable code, overlooking flaws,\nor introducing critical errors. We propose a novel attack, Cross-Origin Context\nPoisoning (XOXO), that is particularly challenging to detect as it relies on\nadversarial code modifications that are semantically equivalent. Traditional\nprogram analysis techniques struggle to identify these correlations since the\nsemantics of the code remain correct, making it appear legitimate. This allows\nattackers to manipulate code assistants into producing incorrect outputs,\nincluding vulnerabilities or backdoors, while shifting the blame to the victim\ndeveloper or tester. We introduce a novel, task-agnostic black-box attack\nalgorithm GCGS that systematically searches the transformation space using a\nCayley Graph, achieving an 83.09% attack success rate on average across five\ntasks and eleven models, including GPT-4o and Claude 3.5 Sonnet v2 used by many\npopular AI coding assistants. Furthermore, existing defenses, including\nadversarial fine-tuning, are ineffective against our attack, underscoring the\nneed for new security measures in LLM-powered coding tools."
    },
    {
        "date": "2025-03",
        "title": "GeoFlow-SLAM: A Robust Tightly-Coupled RGBD-Inertial Fusion SLAM for Dynamic Legged Robotics",
        "author": "Tingyang Xiao, Xiaolin Zhou, Liu Liu, Wei Sui, Wei Feng, Jiaxiong Qiu, Xinjie Wang, and Zhizhong Su",
        "link": "http://arxiv.org/abs/2503.14247v1",
        "abstract": "This paper presents GeoFlow-SLAM, a robust and effective Tightly-Coupled\nRGBD-inertial SLAM for legged robots operating in highly dynamic\nenvironments.By integrating geometric consistency, legged odometry constraints,\nand dual-stream optical flow (GeoFlow), our method addresses three critical\nchallenges:feature matching and pose initialization failures during fast\nlocomotion and visual feature scarcity in texture-less scenes.Specifically, in\nrapid motion scenarios, feature matching is notably enhanced by leveraging\ndual-stream optical flow, which combines prior map points and poses.\nAdditionally, we propose a robust pose initialization method for fast\nlocomotion and IMU error in legged robots, integrating IMU/Legged odometry,\ninter-frame Perspective-n-Point (PnP), and Generalized Iterative Closest Point\n(GICP). Furthermore, a novel optimization framework that tightly couples\ndepth-to-map and GICP geometric constraints is first introduced to improve the\nrobustness and accuracy in long-duration, visually texture-less environments.\nThe proposed algorithms achieve state-of-the-art (SOTA) on collected legged\nrobots and open-source datasets. To further promote research and development,\nthe open-source datasets and code will be made publicly available at\nhttps://github.com/NSN-Hello/GeoFlow-SLAM"
    },
    {
        "date": "2025-03",
        "title": "RoGSplat: Learning Robust Generalizable Human Gaussian Splatting from Sparse Multi-View Images",
        "author": "Junjin Xiao, Qing Zhang, Yonewei Nie, Lei Zhu, and Wei-Shi Zheng",
        "link": "http://arxiv.org/abs/2503.14198v1",
        "abstract": "This paper presents RoGSplat, a novel approach for synthesizing high-fidelity\nnovel views of unseen human from sparse multi-view images, while requiring no\ncumbersome per-subject optimization. Unlike previous methods that typically\nstruggle with sparse views with few overlappings and are less effective in\nreconstructing complex human geometry, the proposed method enables robust\nreconstruction in such challenging conditions. Our key idea is to lift SMPL\nvertices to dense and reliable 3D prior points representing accurate human body\ngeometry, and then regress human Gaussian parameters based on the points. To\naccount for possible misalignment between SMPL model and images, we propose to\npredict image-aligned 3D prior points by leveraging both pixel-level features\nand voxel-level features, from which we regress the coarse Gaussians. To\nenhance the ability to capture high-frequency details, we further render depth\nmaps from the coarse 3D Gaussians to help regress fine-grained pixel-wise\nGaussians. Experiments on several benchmark datasets demonstrate that our\nmethod outperforms state-of-the-art methods in novel view synthesis and\ncross-dataset generalization. Our code is available at\nhttps://github.com/iSEE-Laboratory/RoGSplat."
    },
    {
        "date": "2025-03",
        "title": "Towards properties of adversarial image perturbations",
        "author": "Egor Kuznetsov, Kirill Aistov, and Maxim Koroteev",
        "link": "http://arxiv.org/abs/2503.14111v1",
        "abstract": "Using stochastic gradient approach we study the properties of adversarial\nperturbations resulting in noticeable growth of VMAF image quality metric. The\nstructure of the perturbations is investigated depending on the acceptable PSNR\nvalues and based on the Fourier power spectrum computations for the\nperturbations. It is demonstrated that moderate variation of image brightness\n($\\sim 10$ pixel units in a restricted region of an image can result in VMAF\ngrowth by $\\sim 60\\%$). Unlike some other methods demonstrating similar VMAF\ngrowth, the subjective quality of an image remains almost unchanged. It is also\nshown that the adversarial perturbations may demonstrate approximately linear\ndependence of perturbation amplitudes on the image brightness. The\nperturbations are studied based on the direct VMAF optimization in PyTorch. The\nsignificant discrepancies between the metric values and subjective judgements\nare also demonstrated when image restoration from noise is carried out using\nthe same direct VMAF optimization."
    },
    {
        "date": "2025-03",
        "title": "Securing Automated Insulin Delivery Systems: A Review of Security Threats and Protectives Strategies",
        "author": "Yuchen Niu, and Siew-Kei Lam",
        "link": "http://arxiv.org/abs/2503.14006v1",
        "abstract": "Automated insulin delivery (AID) systems have emerged as a significant\ntechnological advancement in diabetes care. These systems integrate a\ncontinuous glucose monitor, an insulin pump, and control algorithms to automate\ninsulin delivery, reducing the burden of self-management and offering enhanced\nglucose control. However, the increasing reliance on wireless connectivity and\nsoftware control has exposed AID systems to critical security risks that could\nresult in life-threatening treatment errors. This review first presents a\ncomprehensive examination of the security landscape, covering technical\nvulnerabilities, legal frameworks, and commercial product considerations, and\nan analysis of existing research on attack vectors, defence mechanisms, as well\nas evaluation methods and resources for AID systems. Despite recent\nadvancements, several open challenges remain in achieving secure AID systems,\nparticularly in standardising security evaluation frameworks and developing\ncomprehensive, lightweight, and adaptive defence strategies. As one of the most\nwidely adopted and extensively studied physiologic closed-loop control systems,\nthis review serves as a valuable reference for understanding security\nchallenges and solutions applicable to analogous medical systems."
    },
    {
        "date": "2025-03",
        "title": "Survey of Adversarial Robustness in Multimodal Large Language Models",
        "author": "Chengze Jiang, Zhuangzhuang Wang, Minjing Dong, and Jie Gui",
        "link": "http://arxiv.org/abs/2503.13962v1",
        "abstract": "Multimodal Large Language Models (MLLMs) have demonstrated exceptional\nperformance in artificial intelligence by facilitating integrated understanding\nacross diverse modalities, including text, images, video, audio, and speech.\nHowever, their deployment in real-world applications raises significant\nconcerns about adversarial vulnerabilities that could compromise their safety\nand reliability. Unlike unimodal models, MLLMs face unique challenges due to\nthe interdependencies among modalities, making them susceptible to\nmodality-specific threats and cross-modal adversarial manipulations. This paper\nreviews the adversarial robustness of MLLMs, covering different modalities. We\nbegin with an overview of MLLMs and a taxonomy of adversarial attacks tailored\nto each modality. Next, we review key datasets and evaluation metrics used to\nassess the robustness of MLLMs. After that, we provide an in-depth review of\nattacks targeting MLLMs across different modalities. Our survey also identifies\ncritical challenges and suggests promising future research directions."
    },
    {
        "date": "2025-03",
        "title": "Robust Machine Unlearning for Quantized Neural Networks via Adaptive Gradient Reweighting with Similar Labels",
        "author": "Yujia Tong, Yuze Wang, Jingling Yuan, and Chuang Hu",
        "link": "http://arxiv.org/abs/2503.13917v1",
        "abstract": "Model quantization enables efficient deployment of deep neural networks on\nedge devices through low-bit parameter representation, yet raises critical\nchallenges for implementing machine unlearning (MU) under data privacy\nregulations. Existing MU methods designed for full-precision models fail to\naddress two fundamental limitations in quantized networks: 1) Noise\namplification from label mismatch during data processing, and 2) Gradient\nimbalance between forgotten and retained data during training. These issues are\nexacerbated by quantized models' constrained parameter space and discrete\noptimization. We propose Q-MUL, the first dedicated unlearning framework for\nquantized models. Our method introduces two key innovations: 1) Similar Labels\nassignment replaces random labels with semantically consistent alternatives to\nminimize noise injection, and 2) Adaptive Gradient Reweighting dynamically\naligns parameter update contributions from forgotten and retained data. Through\nsystematic analysis of quantized model vulnerabilities, we establish\ntheoretical foundations for these mechanisms. Extensive evaluations on\nbenchmark datasets demonstrate Q-MUL's superiority over existing approaches."
    },
    {
        "date": "2025-03",
        "title": "Exploiting Inherent Class Label: Towards Robust Scribble Supervised Semantic Segmentation",
        "author": "Xinliang Zhang, Lei Zhu, Shuang Zeng, Hangzhou He, Ourui Fu, Zhengjian Yao, Zhaoheng Xie, and Yanye Lu",
        "link": "http://arxiv.org/abs/2503.13895v1",
        "abstract": "Scribble-based weakly supervised semantic segmentation leverages only a few\nannotated pixels as labels to train a segmentation model, presenting\nsignificant potential for reducing the human labor involved in the annotation\nprocess. This approach faces two primary challenges: first, the sparsity of\nscribble annotations can lead to inconsistent predictions due to limited\nsupervision; second, the variability in scribble annotations, reflecting\ndiffering human annotator preferences, can prevent the model from consistently\ncapturing the discriminative regions of objects, potentially leading to\nunstable predictions. To address these issues, we propose a holistic framework,\nthe class-driven scribble promotion network, for robust scribble-supervised\nsemantic segmentation. This framework not only utilizes the provided scribble\nannotations but also leverages their associated class labels to generate\nreliable pseudo-labels. Within the network, we introduce a localization\nrectification module to mitigate noisy labels and a distance perception module\nto identify reliable regions surrounding scribble annotations and\npseudo-labels. In addition, we introduce new large-scale benchmarks,\nScribbleCOCO and ScribbleCityscapes, accompanied by a scribble simulation\nalgorithm that enables evaluation across varying scribble styles. Our method\ndemonstrates competitive performance in both accuracy and robustness,\nunderscoring its superiority over existing approaches. The datasets and the\ncodes will be made publicly available."
    },
    {
        "date": "2025-03",
        "title": "Robust3D-CIL: Robust Class-Incremental Learning for 3D Perception",
        "author": "Jinge Ma, Jiangpeng He, and Fengqing Zhu",
        "link": "http://arxiv.org/abs/2503.13869v1",
        "abstract": "3D perception plays a crucial role in real-world applications such as\nautonomous driving, robotics, and AR/VR. In practical scenarios, 3D perception\nmodels must continuously adapt to new data and emerging object categories, but\nretraining from scratch incurs prohibitive costs. Therefore, adopting\nclass-incremental learning (CIL) becomes particularly essential. However,\nreal-world 3D point cloud data often include corrupted samples, which poses\nsignificant challenges for existing CIL methods and leads to more severe\nforgetting on corrupted data. To address these challenges, we consider the\nscenario in which a CIL model can be updated using point clouds with unknown\ncorruption to better simulate real-world conditions. Inspired by Farthest Point\nSampling, we propose a novel exemplar selection strategy that effectively\npreserves intra-class diversity when selecting replay exemplars, mitigating\nforgetting induced by data corruption. Furthermore, we introduce a point cloud\ndownsampling-based replay method to utilize the limited replay buffer memory\nmore efficiently, thereby further enhancing the model's continual learning\nability. Extensive experiments demonstrate that our method improves the\nperformance of replay-based CIL baselines by 2% to 11%, proving its\neffectiveness and promising potential for real-world 3D applications."
    },
    {
        "date": "2025-03",
        "title": "Text-Guided Image Invariant Feature Learning for Robust Image Watermarking",
        "author": "Muhammad Ahtesham, and Xin Zhong",
        "link": "http://arxiv.org/abs/2503.13805v1",
        "abstract": "Ensuring robustness in image watermarking is crucial for and maintaining\ncontent integrity under diverse transformations. Recent self-supervised\nlearning (SSL) approaches, such as DINO, have been leveraged for watermarking\nbut primarily focus on general feature representation rather than explicitly\nlearning invariant features. In this work, we propose a novel text-guided\ninvariant feature learning framework for robust image watermarking. Our\napproach leverages CLIP's multimodal capabilities, using text embeddings as\nstable semantic anchors to enforce feature invariance under distortions. We\nevaluate the proposed method across multiple datasets, demonstrating superior\nrobustness against various image transformations. Compared to state-of-the-art\nSSL methods, our model achieves higher cosine similarity in feature consistency\ntests and outperforms existing watermarking schemes in extraction accuracy\nunder severe distortions. These results highlight the efficacy of our method in\nlearning invariant representations tailored for robust deep learning-based\nwatermarking."
    },
    {
        "date": "2025-03",
        "title": "Web Artifact Attacks Disrupt Vision Language Models",
        "author": "Maan Qraitem, Piotr Teterwak, Kate Saenko, and Bryan A. Plummer",
        "link": "http://arxiv.org/abs/2503.13652v1",
        "abstract": "Vision-language models (VLMs) (e.g., CLIP, LLaVA) are trained on large-scale,\nlightly curated web datasets, leading them to learn unintended correlations\nbetween semantic concepts and unrelated visual signals. These associations\ndegrade model accuracy by causing predictions to rely on incidental patterns\nrather than genuine visual understanding. Prior work has weaponized these\ncorrelations as an attack vector to manipulate model predictions, such as\ninserting a deceiving class text onto the image in a typographic attack. These\nattacks succeed due to VLMs' text-heavy bias-a result of captions that echo\nvisible words rather than describing content. However, this attack has focused\nsolely on text that matches the target class exactly, overlooking a broader\nrange of correlations, including non-matching text and graphical symbols, which\narise from the abundance of branding content in web-scale data. To address this\ngap, we introduce artifact-based attacks: a novel class of manipulations that\nmislead models using both non-matching text and graphical elements. Unlike\ntypographic attacks, these artifacts are not predefined, making them harder to\ndefend against but also more challenging to find. We address this by framing\nartifact attacks as a search problem and demonstrate their effectiveness across\nfive datasets, with some artifacts reinforcing each other to reach 100% attack\nsuccess rates. These attacks transfer across models with up to 90%\neffectiveness, making it possible to attack unseen models. To defend against\nthese attacks, we extend prior work's artifact aware prompting to the graphical\nsetting. We see a moderate reduction of success rates of up to 15% relative to\nstandard prompts, suggesting a promising direction for enhancing model\nrobustness."
    },
    {
        "date": "2025-03",
        "title": "Less Biased Noise Scale Estimation for Threshold-Robust RANSAC",
        "author": "Johan Edstedt",
        "link": "http://arxiv.org/abs/2503.13433v1",
        "abstract": "The gold-standard for robustly estimating relative pose through image\nmatching is RANSAC. While RANSAC is powerful, it requires setting the inlier\nthreshold that determines whether the error of a correspondence under an\nestimated model is sufficiently small to be included in its consensus set.\nSetting this threshold is typically done by hand, and is difficult to tune\nwithout a access to ground truth data. Thus, a method capable of automatically\ndetermining the optimal threshold would be desirable. In this paper we revisit\ninlier noise scale estimation, which is an attractive approach as the inlier\nnoise scale is linear to the optimal threshold. We revisit the noise scale\nestimation method SIMFIT and find bias in the estimate of the noise scale. In\nparticular, we fix underestimates from using the same data for fitting the\nmodel as estimating the inlier noise, and from not taking the threshold itself\ninto account. Secondly, since the optimal threshold within a scene is\napproximately constant we propose a multi-pair extension of SIMFIT++, by\nfiltering of estimates, which improves results. Our approach yields robust\nperformance across a range of thresholds, shown in Figure 1."
    },
    {
        "date": "2025-03",
        "title": "Escaping Plato's Cave: Robust Conceptual Reasoning through Interpretable 3D Neural Object Volumes",
        "author": "Nhi Pham, Bernt Schiele, Adam Kortylewski, and Jonas Fischer",
        "link": "http://arxiv.org/abs/2503.13429v1",
        "abstract": "With the rise of neural networks, especially in high-stakes applications,\nthese networks need two properties (i) robustness and (ii) interpretability to\nensure their safety. Recent advances in classifiers with 3D volumetric object\nrepresentations have demonstrated a greatly enhanced robustness in\nout-of-distribution data. However, these 3D-aware classifiers have not been\nstudied from the perspective of interpretability. We introduce CAVE - Concept\nAware Volumes for Explanations - a new direction that unifies interpretability\nand robustness in image classification. We design an inherently-interpretable\nand robust classifier by extending existing 3D-aware classifiers with concepts\nextracted from their volumetric representations for classification. In an array\nof quantitative metrics for interpretability, we compare against different\nconcept-based approaches across the explainable AI literature and show that\nCAVE discovers well-grounded concepts that are used consistently across images,\nwhile achieving superior robustness."
    },
    {
        "date": "2025-03",
        "title": "Securing Virtual Reality Experiences: Unveiling and Tackling Cybersickness Attacks with Explainable AI",
        "author": "Ripan Kumar Kundu, Matthew Denton, Genova Mongalo, Prasad Calyam, and Khaza Anuarul Hoque",
        "link": "http://arxiv.org/abs/2503.13419v1",
        "abstract": "The synergy between virtual reality (VR) and artificial intelligence (AI),\nspecifically deep learning (DL)-based cybersickness detection models, has\nushered in unprecedented advancements in immersive experiences by automatically\ndetecting cybersickness severity and adaptively various mitigation techniques,\noffering a smooth and comfortable VR experience. While this DL-enabled\ncybersickness detection method provides promising solutions for enhancing user\nexperiences, it also introduces new risks since these models are vulnerable to\nadversarial attacks; a small perturbation of the input data that is visually\nundetectable to human observers can fool the cybersickness detection model and\ntrigger unexpected mitigation, thus disrupting user immersive experiences (UIX)\nand even posing safety risks. In this paper, we present a new type of VR\nattack, i.e., a cybersickness attack, which successfully stops the triggering\nof cybersickness mitigation by fooling DL-based cybersickness detection models\nand dramatically hinders the UIX. Next, we propose a novel explainable\nartificial intelligence (XAI)-guided cybersickness attack detection framework\nto detect such attacks in VR to ensure UIX and a comfortable VR experience. We\nevaluate the proposed attack and the detection framework using two\nstate-of-the-art open-source VR cybersickness datasets: Simulation 2021 and\nGameplay dataset. Finally, to verify the effectiveness of our proposed method,\nwe implement the attack and the XAI-based detection using a testbed with a\ncustom-built VR roller coaster simulation with an HTC Vive Pro Eye headset and\nperform a user study. Our study shows that such an attack can dramatically\nhinder the UIX. However, our proposed XAI-guided cybersickness attack detection\ncan successfully detect cybersickness attacks and trigger the proper\nmitigation, effectively reducing VR cybersickness."
    },
    {
        "date": "2025-03",
        "title": "DLPO: Towards a Robust, Efficient, and Generalizable Prompt Optimization Framework from a Deep-Learning Perspective",
        "author": "Dengyun Peng, Yuhang Zhou, Qiguang Chen, Jinhao Liu, Jingjing Chen, and Libo Qin",
        "link": "http://arxiv.org/abs/2503.13413v3",
        "abstract": "Large Language Models (LLMs) have achieved remarkable success across diverse\ntasks, largely driven by well-designed prompts. However, crafting and selecting\nsuch prompts often requires considerable human effort, significantly limiting\nits scalability. To mitigate this, recent studies have explored automated\nprompt optimization as a promising solution. Despite these efforts, existing\nmethods still face critical challenges in robustness, efficiency, and\ngeneralization. To systematically address these challenges, we first conduct an\nempirical analysis to identify the limitations of current reflection-based\nprompt optimization paradigm. Building on these insights, we propose 7\ninnovative approaches inspired by traditional deep learning paradigms for\nprompt optimization (DLPO), seamlessly integrating these concepts into\ntext-based gradient optimization. Through these advancements, we progressively\ntackle the aforementioned challenges and validate our methods through extensive\nexperimentation. We hope our study not only provides valuable guidance for\nfuture research but also offers a comprehensive understanding of the challenges\nand potential solutions in prompt optimization. Our code is available at\nhttps://github.com/sfasfaffa/DLPO."
    },
    {
        "date": "2025-03",
        "title": "Follow-the-Regularized-Leader with Adversarial Constraints",
        "author": "Ricardo N. Ferreira, and Cl\u00e1udia Soares",
        "link": "http://arxiv.org/abs/2503.13366v1",
        "abstract": "Constrained Online Convex Optimization (COCO) can be seen as a generalization\nof the standard Online Convex Optimization (OCO) framework. At each round, a\ncost function and constraint function are revealed after a learner chooses an\naction. The goal is to minimize both the regret and cumulative constraint\nviolation (CCV) against an adaptive adversary. We show for the first time that\nis possible to obtain the optimal $O(\\sqrt{T})$ bound on both regret and CCV,\nimproving the best known bounds of $O \\left( \\sqrt{T} \\right)$ and $\\~{O}\n\\left( \\sqrt{T} \\right)$ for the regret and CCV, respectively."
    },
    {
        "date": "2025-03",
        "title": "RainScaleGAN: a Conditional Generative Adversarial Network for Rainfall Downscaling",
        "author": "Marcello Iotti, Paolo Davini, Jost von Hardenberg, and Giuseppe Zappa",
        "link": "http://arxiv.org/abs/2503.13316v1",
        "abstract": "To this day, accurately simulating local-scale precipitation and reliably\nreproducing its distribution remains a challenging task. The limited horizontal\nresolution of Global Climate Models is among the primary factors undermining\ntheir skill in this context. The physical mechanisms driving the onset and\ndevelopment of precipitation, especially in extreme events, operate at\nspatio-temporal scales smaller than those numerically resolved, thus struggling\nto be captured accurately. In order to circumvent this limitation, several\ndownscaling approaches have been developed over the last decades to address the\ndiscrepancy between the spatial resolution of models output and the resolution\nrequired by local-scale applications. In this paper, we introduce RainScaleGAN,\na conditional deep convolutional Generative Adversarial Network (GAN) for\nprecipitation downscaling. GANs have been effectively used in image\nsuper-resolution, an approach highly relevant for downscaling tasks.\nRainScaleGAN's capabilities are tested in a perfect-model setup, where the\nspatial resolution of a precipitation dataset is artificially degraded from\n0.25$^{\\circ}\\times$0.25$^{\\circ}$ to 2$^{\\circ}\\times$2$^\\circ$, and\nRainScaleGAN is used to restore it. The developed model outperforms one of the\nleading precipitation downscaling method found in the literature. RainScaleGAN\nnot only generates a synthetic dataset featuring plausible high-resolution\nspatial patterns and intensities, but also produces a precipitation\ndistribution with statistics closely mirroring those of the ground-truth\ndataset. Given that RainScaleGAN's approach is agnostic with respect to the\nunderlying physics, the method has the potential to be applied to other\nphysical variables such as surface winds or temperature."
    },
    {
        "date": "2025-03",
        "title": "Zero-Knowledge Proof-Based Consensus for Blockchain-Secured Federated Learning",
        "author": "Tianxing Fu, Jia Hu, Geyong Min, and Zi Wang",
        "link": "http://arxiv.org/abs/2503.13255v1",
        "abstract": "Federated learning (FL) enables multiple participants to collaboratively\ntrain machine learning models while ensuring their data remains private and\nsecure. Blockchain technology further enhances FL by providing stronger\nsecurity, a transparent audit trail, and protection against data tampering and\nmodel manipulation. Most blockchain-secured FL systems rely on conventional\nconsensus mechanisms: Proof-of-Work (PoW) is computationally expensive, while\nProof-of-Stake (PoS) improves energy efficiency but risks centralization as it\ninherently favors participants with larger stakes. Recently, learning-based\nconsensus has emerged as an alternative by replacing cryptographic tasks with\nmodel training to save energy. However, this approach introduces potential\nprivacy vulnerabilities, as the training process may inadvertently expose\nsensitive information through gradient sharing and model updates. To address\nthese challenges, we propose a novel Zero-Knowledge Proof of Training (ZKPoT)\nconsensus mechanism. This method leverages the zero-knowledge succinct\nnon-interactive argument of knowledge proof (zk-SNARK) protocol to validate\nparticipants' contributions based on their model performance, effectively\neliminating the inefficiencies of traditional consensus methods and mitigating\nthe privacy risks posed by learning-based consensus. We analyze our system's\nsecurity, demonstrating its capacity to prevent the disclosure of sensitive\ninformation about local models or training data to untrusted parties during the\nentire FL process. Extensive experiments demonstrate that our system is robust\nagainst privacy and Byzantine attacks while maintaining accuracy and utility\nwithout trade-offs, scalable across various blockchain settings, and efficient\nin both computation and communication."
    },
    {
        "date": "2025-03",
        "title": "ProDiF: Protecting Domain-Invariant Features to Secure Pre-Trained Models Against Extraction",
        "author": "Tong Zhou, Shijin Duan, Gaowen Liu, Charles Fleming, Ramana Rao Kompella, Shaolei Ren, and Xiaolin Xu",
        "link": "http://arxiv.org/abs/2503.13224v1",
        "abstract": "Pre-trained models are valuable intellectual property, capturing both\ndomain-specific and domain-invariant features within their weight spaces.\nHowever, model extraction attacks threaten these assets by enabling\nunauthorized source-domain inference and facilitating cross-domain transfer via\nthe exploitation of domain-invariant features. In this work, we introduce\n**ProDiF**, a novel framework that leverages targeted weight space manipulation\nto secure pre-trained models against extraction attacks. **ProDiF** quantifies\nthe transferability of filters and perturbs the weights of critical filters in\nunsecured memory, while preserving actual critical weights in a Trusted\nExecution Environment (TEE) for authorized users. A bi-level optimization\nfurther ensures resilience against adaptive fine-tuning attacks. Experimental\nresults show that **ProDiF** reduces source-domain accuracy to near-random\nlevels and decreases cross-domain transferability by 74.65\\%, providing robust\nprotection for pre-trained models. This work offers comprehensive protection\nfor pre-trained DNN models and highlights the potential of weight space\nmanipulation as a novel approach to model security."
    },
    {
        "date": "2025-03",
        "title": "Robust Decision-Making Via Free Energy Minimization",
        "author": "Allahkaram Shafiei, Hozefa Jesawada, Karl Friston, and Giovanni Russo",
        "link": "http://arxiv.org/abs/2503.13223v1",
        "abstract": "Despite their groundbreaking performance, state-of-the-art autonomous agents\ncan misbehave when training and environmental conditions become inconsistent,\nwith minor mismatches leading to undesirable behaviors or even catastrophic\nfailures. Robustness towards these training/environment ambiguities is a core\nrequirement for intelligent agents and its fulfillment is a long-standing\nchallenge when deploying agents in the real world. Here, departing from\nmainstream views seeking robustness through training, we introduce DR-FREE, a\nfree energy model that installs this core property by design. It directly wires\nrobustness into the agent decision-making mechanisms via free energy\nminimization. By combining a robust extension of the free energy principle with\na novel resolution engine, DR-FREE returns a policy that is optimal-yet-robust\nagainst ambiguity. Moreover, for the first time, it reveals the mechanistic\nrole of ambiguity on optimal decisions and requisite Bayesian belief updating.\nWe evaluate DR-FREE on an experimental testbed involving real rovers navigating\nan ambiguous environment filled with obstacles. Across all the experiments,\nDR-FREE enables robots to successfully navigate towards their goal even when,\nin contrast, standard free energy minimizing agents that do not use DR-FREE\nfail. In short, DR-FREE can tackle scenarios that elude previous methods: this\nmilestone may inspire both deployment in multi-agent settings and, at a perhaps\ndeeper level, the quest for a biologically plausible explanation of how natural\nagents - with little or no training - survive in capricious environments."
    },
    {
        "date": "2025-03",
        "title": "Gaussian On-the-Fly Splatting: A Progressive Framework for Robust Near Real-Time 3DGS Optimization",
        "author": "Yiwei Xu, Yifei Yu, Wentian Gan, Tengfei Wang, Zongqian Zhan, Hao Cheng, and Xin Wang",
        "link": "http://arxiv.org/abs/2503.13086v1",
        "abstract": "3D Gaussian Splatting (3DGS) achieves high-fidelity rendering with fast\nreal-time performance, but existing methods rely on offline training after full\nStructure-from-Motion (SfM) processing. In contrast, this work introduces\nOn-the-Fly GS, a progressive framework enabling near real-time 3DGS\noptimization during image capture. As each image arrives, its pose and sparse\npoints are updated via on-the-fly SfM, and newly optimized Gaussians are\nimmediately integrated into the 3DGS field. We propose a progressive local\noptimization strategy to prioritize new images and their neighbors by their\ncorresponding overlapping relationship, allowing the new image and its\noverlapping images to get more training. To further stabilize training across\nold and new images, an adaptive learning rate schedule balances the iterations\nand the learning rate. Moreover, to maintain overall quality of the 3DGS field,\nan efficient global optimization scheme prevents overfitting to the newly added\nimages. Experiments on multiple benchmark datasets show that our On-the-Fly GS\nreduces training time significantly, optimizing each new image in seconds with\nminimal rendering loss, offering the first practical step toward rapid,\nprogressive 3DGS reconstruction."
    },
    {
        "date": "2025-03",
        "title": "MaskSDM with Shapley values to improve flexibility, robustness, and explainability in species distribution modeling",
        "author": "Robin Zbinden, Nina van Tiel, Gencer Sumbul, Chiara Vanalli, Benjamin Kellenberger, and Devis Tuia",
        "link": "http://arxiv.org/abs/2503.13057v1",
        "abstract": "Species Distribution Models (SDMs) play a vital role in biodiversity\nresearch, conservation planning, and ecological niche modeling by predicting\nspecies distributions based on environmental conditions. The selection of\npredictors is crucial, strongly impacting both model accuracy and how well the\npredictions reflect ecological patterns. To ensure meaningful insights, input\nvariables must be carefully chosen to match the study objectives and the\necological requirements of the target species. However, existing SDMs,\nincluding both traditional and deep learning-based approaches, often lack key\ncapabilities for variable selection: (i) flexibility to choose relevant\npredictors at inference without retraining; (ii) robustness to handle missing\npredictor values without compromising accuracy; and (iii) explainability to\ninterpret and accurately quantify each predictor's contribution. To overcome\nthese limitations, we introduce MaskSDM, a novel deep learning-based SDM that\nenables flexible predictor selection by employing a masked training strategy.\nThis approach allows the model to make predictions with arbitrary subsets of\ninput variables while remaining robust to missing data. It also provides a\nclearer understanding of how adding or removing a given predictor affects model\nperformance and predictions. Additionally, MaskSDM leverages Shapley values for\nprecise predictor contribution assessments, improving upon traditional\napproximations. We evaluate MaskSDM on the global sPlotOpen dataset, modeling\nthe distributions of 12,738 plant species. Our results show that MaskSDM\noutperforms imputation-based methods and approximates models trained on\nspecific subsets of variables. These findings underscore MaskSDM's potential to\nincrease the applicability and adoption of SDMs, laying the groundwork for\ndeveloping foundation models in SDMs that can be readily applied to diverse\necological applications."
    },
    {
        "date": "2025-03",
        "title": "MirrorGuard: Adaptive Defense Against Jailbreaks via Entropy-Guided Mirror Crafting",
        "author": "Rui Pu, Chaozhuo Li, Rui Ha, Litian Zhang, Lirong Qiu, and Xi Zhang",
        "link": "http://arxiv.org/abs/2503.12931v1",
        "abstract": "Defending large language models (LLMs) against jailbreak attacks is crucial\nfor ensuring their safe deployment. Existing defense strategies generally rely\non predefined static criteria to differentiate between harmful and benign\nprompts. However, such rigid rules are incapable of accommodating the inherent\ncomplexity and dynamic nature of real jailbreak attacks. In this paper, we\npropose a novel concept of ``mirror'' to enable dynamic and adaptive defense. A\nmirror refers to a dynamically generated prompt that mirrors the syntactic\nstructure of the input while ensuring semantic safety. The personalized\ndiscrepancies between the input prompts and their corresponding mirrors serve\nas the guiding principles for defense. A new defense paradigm, MirrorGuard, is\nfurther proposed to detect and calibrate risky inputs based on such mirrors. An\nentropy-based detection metric, Relative Input Uncertainty (RIU), is integrated\ninto MirrorGuard to quantify the discrepancies between input prompts and\nmirrors. MirrorGuard is evaluated on several popular datasets, demonstrating\nstate-of-the-art defense performance while maintaining general effectiveness."
    },
    {
        "date": "2025-03",
        "title": "MES-RAG: Bringing Multi-modal, Entity-Storage, and Secure Enhancements to RAG",
        "author": "Pingyu Wu, Daiheng Gao, Jing Tang, Huimin Chen, Wenbo Zhou, Weiming Zhang, and Nenghai Yu",
        "link": "http://arxiv.org/abs/2503.13563v1",
        "abstract": "Retrieval-Augmented Generation (RAG) improves Large Language Models (LLMs) by\nusing external knowledge, but it struggles with precise entity information\nretrieval. In this paper, we proposed MES-RAG framework, which enhances\nentity-specific query handling and provides accurate, secure, and consistent\nresponses. MES-RAG introduces proactive security measures that ensure system\nintegrity by applying protections prior to data access. Additionally, the\nsystem supports real-time multi-modal outputs, including text, images, audio,\nand video, seamlessly integrating into existing RAG architectures. Experimental\nresults demonstrate that MES-RAG significantly improves both accuracy and\nrecall, highlighting its effectiveness in advancing the security and utility of\nquestion-answering, increasing accuracy to 0.83 (+0.25) on targeted task. Our\ncode and data are available at https://github.com/wpydcr/MES-RAG."
    },
    {
        "date": "2025-03",
        "title": "Privacy-Aware RAG: Secure and Isolated Knowledge Retrieval",
        "author": "Pengcheng Zhou, Yinglun Feng, and Zhongliang Yang",
        "link": "http://arxiv.org/abs/2503.15548v1",
        "abstract": "The widespread adoption of Retrieval-Augmented Generation (RAG) systems in\nreal-world applications has heightened concerns about the confidentiality and\nintegrity of their proprietary knowledge bases. These knowledge bases, which\nplay a critical role in enhancing the generative capabilities of Large Language\nModels (LLMs), are increasingly vulnerable to breaches that could compromise\nsensitive information. To address these challenges, this paper proposes an\nadvanced encryption methodology designed to protect RAG systems from\nunauthorized access and data leakage. Our approach encrypts both textual\ncontent and its corresponding embeddings prior to storage, ensuring that all\ndata remains securely encrypted. This mechanism restricts access to authorized\nentities with the appropriate decryption keys, thereby significantly reducing\nthe risk of unintended data exposure. Furthermore, we demonstrate that our\nencryption strategy preserves the performance and functionality of RAG\npipelines, ensuring compatibility across diverse domains and applications. To\nvalidate the robustness of our method, we provide comprehensive security proofs\nthat highlight its resilience against potential threats and vulnerabilities.\nThese proofs also reveal limitations in existing approaches, which often lack\nrobustness, adaptability, or reliance on open-source models. Our findings\nsuggest that integrating advanced encryption techniques into the design and\ndeployment of RAG systems can effectively enhance privacy safeguards. This\nresearch contributes to the ongoing discourse on improving security measures\nfor AI-driven services and advocates for stricter data protection standards\nwithin RAG architectures."
    },
    {
        "date": "2025-03",
        "title": "Evolution-based Region Adversarial Prompt Learning for Robustness Enhancement in Vision-Language Models",
        "author": "Xiaojun Jia, Sensen Gao, Simeng Qin, Ke Ma, Xinfeng Li, Yihao Huang, Wei Dong, Yang Liu, and Xiaochun Cao",
        "link": "http://arxiv.org/abs/2503.12874v2",
        "abstract": "Large pre-trained vision-language models (VLMs), such as CLIP, demonstrate\nimpressive generalization but remain highly vulnerable to adversarial examples\n(AEs). Previous work has explored robust text prompts through adversarial\ntraining, achieving some improvement in both robustness and generalization.\nHowever, they primarily rely on singlegradient direction perturbations (e.g.,\nPGD) to generate AEs, which lack diversity, resulting in limited improvement in\nadversarial robustness. To address these limitations, we propose an\nevolution-based region adversarial prompt tuning method called ER-APT, which\ncombines gradient methods with genetic evolution to generate more diverse and\nchallenging AEs. In each training iteration, we first generate AEs using\ntraditional gradient-based methods. Subsequently, a genetic evolution mechanism\nincorporating selection, mutation, and crossover is applied to optimize the\nAEs, ensuring a broader and more aggressive perturbation distribution.The final\nevolved AEs are used for prompt tuning, achieving region-based adversarial\noptimization instead of conventional single-point adversarial prompt tuning. We\nalso propose a dynamic loss weighting method to adjust prompt learning\nefficiency for accuracy and robustness. Experimental evaluations on various\nbenchmark datasets demonstrate the superiority of our proposed method,\noutperforming stateof-the-art APT methods. The code is released at\nhttps://github.com/jiaxiaojunQAQ/ER-APT."
    },
    {
        "date": "2025-03",
        "title": "Robust Audio-Visual Segmentation via Audio-Guided Visual Convergent Alignment",
        "author": "Chen Liu, Peike Li, Liying Yang, Dadong Wang, Lincheng Li, and Xin Yu",
        "link": "http://arxiv.org/abs/2503.12847v1",
        "abstract": "Accurately localizing audible objects based on audio-visual cues is the core\nobjective of audio-visual segmentation. Most previous methods emphasize spatial\nor temporal multi-modal modeling, yet overlook challenges from ambiguous\naudio-visual correspondences such as nearby visually similar but acoustically\ndifferent objects and frequent shifts in objects' sounding status.\nConsequently, they may struggle to reliably correlate audio and visual cues,\nleading to over- or under-segmentation. To address these limitations, we\npropose a novel framework with two primary components: an audio-guided modality\nalignment (AMA) module and an uncertainty estimation (UE) module. Instead of\nindiscriminately correlating audio-visual cues through a global attention\nmechanism, AMA performs audio-visual interactions within multiple groups and\nconsolidates group features into compact representations based on their\nresponsiveness to audio cues, effectively directing the model's attention to\naudio-relevant areas. Leveraging contrastive learning, AMA further\ndistinguishes sounding regions from silent areas by treating features with\nstrong audio responses as positive samples and weaker responses as negatives.\nAdditionally, UE integrates spatial and temporal information to identify\nhigh-uncertainty regions caused by frequent changes in sound state, reducing\nprediction errors by lowering confidence in these areas. Experimental results\ndemonstrate that our approach achieves superior accuracy compared to existing\nstate-of-the-art methods, particularly in challenging scenarios where\ntraditional approaches struggle to maintain reliable segmentation."
    },
    {
        "date": "2025-03",
        "title": "CompMarkGS: Robust Watermarking for Compression 3D Gaussian Splatting",
        "author": "Sumin In, Youngdong Jang, Utae Jeong, MinHyuk Jang, Hyeongcheol Park, Eunbyung Park, and Sangpil Kim",
        "link": "http://arxiv.org/abs/2503.12836v1",
        "abstract": "3D Gaussian Splatting (3DGS) enables rapid differentiable rendering for 3D\nreconstruction and novel view synthesis, leading to its widespread commercial\nuse. Consequently, copyright protection via watermarking has become critical.\nHowever, because 3DGS relies on millions of Gaussians, which require gigabytes\nof storage, efficient transfer and storage require compression. Existing 3DGS\nwatermarking methods are vulnerable to quantization-based compression, often\nresulting in the loss of the embedded watermark. To address this challenge, we\npropose a novel watermarking method that ensures watermark robustness after\nmodel compression while maintaining high rendering quality. In detail, we\nincorporate a quantization distortion layer that simulates compression during\ntraining, preserving the watermark under quantization-based compression. Also,\nwe propose a learnable watermark embedding feature that embeds the watermark\ninto the anchor feature, ensuring structural consistency and seamless\nintegration into the 3D scene. Furthermore, we present a frequency-aware anchor\ngrowing mechanism to enhance image quality in high-frequency regions by\neffectively identifying Guassians within these regions. Experimental results\nconfirm that our method preserves the watermark and maintains superior image\nquality under high compression, validating it as a promising approach for a\nsecure 3DGS model."
    },
    {
        "date": "2025-03",
        "title": "GSBA$^K$: $top$-$K$ Geometric Score-based Black-box Attack",
        "author": "Md Farhamdur Reza, Richeng Jin, Tianfu Wu, and Huaiyu Dai",
        "link": "http://arxiv.org/abs/2503.12827v2",
        "abstract": "Existing score-based adversarial attacks mainly focus on crafting $top$-1\nadversarial examples against classifiers with single-label classification.\nTheir attack success rate and query efficiency are often less than\nsatisfactory, particularly under small perturbation requirements; moreover, the\nvulnerability of classifiers with multi-label learning is yet to be studied. In\nthis paper, we propose a comprehensive surrogate free score-based attack, named\n\\b geometric \\b score-based \\b black-box \\b attack (GSBA$^K$), to craft\nadversarial examples in an aggressive $top$-$K$ setting for both untargeted and\ntargeted attacks, where the goal is to change the $top$-$K$ predictions of the\ntarget classifier. We introduce novel gradient-based methods to find a good\ninitial boundary point to attack. Our iterative method employs novel gradient\nestimation techniques, particularly effective in $top$-$K$ setting, on the\ndecision boundary to effectively exploit the geometry of the decision boundary.\nAdditionally, GSBA$^K$ can be used to attack against classifiers with $top$-$K$\nmulti-label learning. Extensive experimental results on ImageNet and PASCAL VOC\ndatasets validate the effectiveness of GSBA$^K$ in crafting $top$-$K$\nadversarial examples."
    },
    {
        "date": "2025-03",
        "title": "BLIA: Detect model memorization in binary classification model through passive Label Inference attack",
        "author": "Mohammad Wahiduzzaman Khan, Sheng Chen, Ilya Mironov, Leizhen Zhang, and Rabib Noor",
        "link": "http://arxiv.org/abs/2503.12801v1",
        "abstract": "Model memorization has implications for both the generalization capacity of\nmachine learning models and the privacy of their training data. This paper\ninvestigates label memorization in binary classification models through two\nnovel passive label inference attacks (BLIA). These attacks operate passively,\nrelying solely on the outputs of pre-trained models, such as confidence scores\nand log-loss values, without interacting with or modifying the training\nprocess. By intentionally flipping 50% of the labels in controlled subsets,\ntermed \"canaries,\" we evaluate the extent of label memorization under two\nconditions: models trained without label differential privacy (Label-DP) and\nthose trained with randomized response-based Label-DP. Despite the application\nof varying degrees of Label-DP, the proposed attacks consistently achieve\nsuccess rates exceeding 50%, surpassing the baseline of random guessing and\nconclusively demonstrating that models memorize training labels, even when\nthese labels are deliberately uncorrelated with the features."
    },
    {
        "date": "2025-03",
        "title": "Improving Generalization of Universal Adversarial Perturbation via Dynamic Maximin Optimization",
        "author": "Yechao Zhang, Yingzhe Xu, Junyu Shi, Leo Yu Zhang, Shengshan Hu, Minghui Li, and Yanjun Zhang",
        "link": "http://arxiv.org/abs/2503.12793v2",
        "abstract": "Deep neural networks (DNNs) are susceptible to universal adversarial\nperturbations (UAPs). These perturbations are meticulously designed to fool the\ntarget model universally across all sample classes. Unlike instance-specific\nadversarial examples (AEs), generating UAPs is more complex because they must\nbe generalized across a wide range of data samples and models. Our research\nreveals that existing universal attack methods, which optimize UAPs using DNNs\nwith static model parameter snapshots, do not fully leverage the potential of\nDNNs to generate more effective UAPs. Rather than optimizing UAPs against\nstatic DNN models with a fixed training set, we suggest using dynamic\nmodel-data pairs to generate UAPs. In particular, we introduce a dynamic\nmaximin optimization strategy, aiming to optimize the UAP across a variety of\noptimal model-data pairs. We term this approach DM-UAP. DM-UAP utilizes an\niterative max-min-min optimization framework that refines the model-data pairs,\ncoupled with a curriculum UAP learning algorithm to examine the combined space\nof model parameters and data thoroughly. Comprehensive experiments on the\nImageNet dataset demonstrate that the proposed DM-UAP markedly enhances both\ncross-sample universality and cross-model transferability of UAPs. Using only\n500 samples for UAP generation, DM-UAP outperforms the state-of-the-art\napproach with an average increase in fooling ratio of 12.108%."
    },
    {
        "date": "2025-03",
        "title": "Algebraic Adversarial Attacks on Explainability Models",
        "author": "Lachlan Simpson, Federico Costanza, Kyle Millar, Adriel Cheng, Cheng-Chew Lim, and Hong Gunn Chew",
        "link": "http://arxiv.org/abs/2503.12683v1",
        "abstract": "Classical adversarial attacks are phrased as a constrained optimisation\nproblem. Despite the efficacy of a constrained optimisation approach to\nadversarial attacks, one cannot trace how an adversarial point was generated.\nIn this work, we propose an algebraic approach to adversarial attacks and study\nthe conditions under which one can generate adversarial examples for post-hoc\nexplainability models. Phrasing neural networks in the framework of geometric\ndeep learning, algebraic adversarial attacks are constructed through analysis\nof the symmetry groups of neural networks. Algebraic adversarial examples\nprovide a mathematically tractable approach to adversarial examples. We\nvalidate our approach of algebraic adversarial examples on two well-known and\none real-world dataset."
    },
    {
        "date": "2025-03",
        "title": "SCOOP: CoSt-effective COngestiOn Attacks in Payment Channel Networks",
        "author": "Mohammed Ababneh, Kartick Kolachala, and Roopa Vishwanathan",
        "link": "http://arxiv.org/abs/2503.12625v1",
        "abstract": "Payment channel networks (PCNs) are a promising solution to address\nblockchain scalability and throughput challenges, However, the security of PCNs\nand their vulnerability to attacks are not sufficiently studied. In this paper,\nwe introduce SCOOP, a framework that includes two novel congestion attacks on\nPCNs. These attacks consider the minimum transferable amount along a path (path\ncapacity) and the number of channels involved (path length), formulated as\nlinear optimization problems. The first attack allocates the attacker's budget\nto achieve a specific congestion threshold, while the second maximizes\ncongestion under budget constraints. Simulation results show the effectiveness\nof the proposed attack formulations in comparison to other attack strategies.\nSpecifically, the results indicate that the first attack provides around a 40\\%\nimprovement in congestion performance, while the second attack offers\napproximately a 50\\% improvement in comparison to the state-of-the-art.\nMoreover, in terms of payment to congestion efficiency, the first attack is\nabout 60\\% more efficient, and the second attack is around 90\\% more efficient\nin comparison to state-of-the-art"
    },
    {
        "date": "2025-03",
        "title": "GAN-Based Single-Stage Defense for Traffic Sign Classification Under Adversarial Patch Attack",
        "author": "Abyad Enan, and Mashrur Chowdhury",
        "link": "http://arxiv.org/abs/2503.12567v1",
        "abstract": "Computer Vision plays a critical role in ensuring the safe navigation of\nautonomous vehicles (AVs). An AV perception module is responsible for capturing\nand interpreting the surrounding environment to facilitate safe navigation.\nThis module enables AVs to recognize traffic signs, traffic lights, and various\nroad users. However, the perception module is vulnerable to adversarial\nattacks, which can compromise their accuracy and reliability. One such attack\nis the adversarial patch attack (APA), a physical attack in which an adversary\nstrategically places a specially crafted sticker on an object to deceive object\nclassifiers. In APA, an adversarial patch is positioned on a target object,\nleading the classifier to misidentify it. Such an APA can cause AVs to\nmisclassify traffic signs, leading to catastrophic incidents. To enhance the\nsecurity of an AV perception system against APAs, this study develops a\nGenerative Adversarial Network (GAN)-based single-stage defense strategy for\ntraffic sign classification. This approach is tailored to defend against APAs\non different classes of traffic signs without prior knowledge of a patch's\ndesign. This study found this approach to be effective against patches of\nvarying sizes. Our experimental analysis demonstrates that the defense strategy\npresented in this paper improves the classifier's accuracy under APA conditions\nby up to 80.8% and enhances overall classification accuracy for all the traffic\nsigns considered in this study by 58%, compared to a classifier without any\ndefense mechanism. Our defense strategy is model-agnostic, making it applicable\nto any traffic sign classifier, regardless of the underlying classification\nmodel."
    },
    {
        "date": "2025-03",
        "title": "A Plug-and-Play Learning-based IMU Bias Factor for Robust Visual-Inertial Odometry",
        "author": "Yang Yi, Kunqing Wang, Jinpu Zhang, Zhen Tan, Xiangke Wang, Hui Shen, and Dewen Hu",
        "link": "http://arxiv.org/abs/2503.12527v1",
        "abstract": "The bias of low-cost Inertial Measurement Units (IMU) is a critical factor\naffecting the performance of Visual-Inertial Odometry (VIO). In particular,\nwhen visual tracking encounters errors, the optimized bias results may deviate\nsignificantly from the true values, adversely impacting the system's stability\nand localization precision. In this paper, we propose a novel plug-and-play\nframework featuring the Inertial Prior Network (IPNet), which is designed to\naccurately estimate IMU bias. Recognizing the substantial impact of initial\nbias errors in low-cost inertial devices on system performance, our network\ndirectly leverages raw IMU data to estimate the mean bias, eliminating the\ndependency on historical estimates in traditional recursive predictions and\neffectively preventing error propagation. Furthermore, we introduce an\niterative approach to calculate the mean value of the bias for network\ntraining, addressing the lack of bias labels in many visual-inertial datasets.\nThe framework is evaluated on two public datasets and one self-collected\ndataset. Extensive experiments demonstrate that our method significantly\nenhances both localization precision and robustness, with the ATE-RMSE metric\nimproving on average by 46\\%. The source code and video will be available at\n\\textcolor{red}{https://github.com/yiyscut/VIO-IPNet.git}."
    },
    {
        "date": "2025-03",
        "title": "Defense Against Model Stealing Based on Account-Aware Distribution Discrepancy",
        "author": "Jian-Ping Mei, Weibin Zhang, Jie Chen, Xuyun Zhang, and Tiantian Zhu",
        "link": "http://arxiv.org/abs/2503.12497v1",
        "abstract": "Malicious users attempt to replicate commercial models functionally at low\ncost by training a clone model with query responses. It is challenging to\ntimely prevent such model-stealing attacks to achieve strong protection and\nmaintain utility. In this paper, we propose a novel non-parametric detector\ncalled Account-aware Distribution Discrepancy (ADD) to recognize queries from\nmalicious users by leveraging account-wise local dependency. We formulate each\nclass as a Multivariate Normal distribution (MVN) in the feature space and\nmeasure the malicious score as the sum of weighted class-wise distribution\ndiscrepancy. The ADD detector is combined with random-based prediction\npoisoning to yield a plug-and-play defense module named D-ADD for image\nclassification models. Results of extensive experimental studies show that\nD-ADD achieves strong defense against different types of attacks with little\ninterference in serving benign users for both soft and hard-label settings."
    },
    {
        "date": "2025-03",
        "title": "Shape Bias and Robustness Evaluation via Cue Decomposition for Image Classification and Segmentation",
        "author": "Edgar Heinert, Thomas Gottwald, Annika M\u00fctze, and Matthias Rottmann",
        "link": "http://arxiv.org/abs/2503.12453v1",
        "abstract": "Previous works studied how deep neural networks (DNNs) perceive image content\nin terms of their biases towards different image cues, such as texture and\nshape. Previous methods to measure shape and texture biases are typically\nstyle-transfer-based and limited to DNNs for image classification. In this\nwork, we provide a new evaluation procedure consisting of 1) a\ncue-decomposition method that comprises two AI-free data pre-processing methods\nextracting shape and texture cues, respectively, and 2) a novel\ncue-decomposition shape bias evaluation metric that leverages the\ncue-decomposition data. For application purposes we introduce a corresponding\ncue-decomposition robustness metric that allows for the estimation of the\nrobustness of a DNN w.r.t. image corruptions. In our numerical experiments, our\nfindings for biases in image classification DNNs align with those of previous\nevaluation metrics. However, our cue-decomposition robustness metric shows\nsuperior results in terms of estimating the robustness of DNNs. Furthermore,\nour results for DNNs on the semantic segmentation datasets Cityscapes and\nADE20k for the first time shed light into the biases of semantic segmentation\nDNNs."
    },
    {
        "date": "2025-03",
        "title": "Semi-Decision-Focused Learning with Deep Ensembles: A Practical Framework for Robust Portfolio Optimization",
        "author": "Juhyeong Kim",
        "link": "http://arxiv.org/abs/2503.13544v1",
        "abstract": "I propose Semi-Decision-Focused Learning, a practical adaptation of\nDecision-Focused Learning for portfolio optimization. Rather than directly\noptimizing complex financial metrics, I employ simple target portfolios\n(Max-Sortino or One-Hot) and train models with a convex, cross-entropy loss. I\nfurther incorporate Deep Ensemble methods to reduce variance and stabilize\nperformance. Experiments on two universes (one upward-trending and another\nrange-bound) show consistent outperformance over baseline portfolios,\ndemonstrating the effectiveness and robustness of my approach. Code is\navailable at https://github.com/sDFLwDE/sDFLwDE"
    },
    {
        "date": "2025-03",
        "title": "SCReedSolo: A Secure and Robust LSB Image Steganography Framework with Randomized Symmetric Encryption and Reed-Solomon Coding",
        "author": "Syed Rifat Raiyan, and Md. Hasanul Kabir",
        "link": "http://arxiv.org/abs/2503.12368v1",
        "abstract": "Image steganography is an information-hiding technique that involves the\nsurreptitious concealment of covert informational content within digital\nimages. In this paper, we introduce ${\\rm SCR{\\small EED}S{\\small OLO}}$, a\nnovel framework for concealing arbitrary binary data within images. Our\napproach synergistically leverages Random Shuffling, Fernet Symmetric\nEncryption, and Reed-Solomon Error Correction Codes to encode the secret\npayload, which is then discretely embedded into the carrier image using LSB\n(Least Significant Bit) Steganography. The combination of these methods\naddresses the vulnerability vectors of both security and resilience against\nbit-level corruption in the resultant stego-images. We show that our framework\nachieves a data payload of 3 bits per pixel for an RGB image, and\nmathematically assess the probability of successful transmission for the\namalgamated $n$ message bits and $k$ error correction bits. Additionally, we\nfind that ${\\rm SCR{\\small EED}S{\\small OLO}}$ yields good results upon being\nevaluated with multiple performance metrics, successfully eludes detection by\nvarious passive steganalysis tools, and is immune to simple active steganalysis\nattacks. Our code and data are available at\nhttps://github.com/Starscream-11813/SCReedSolo-Steganography."
    },
    {
        "date": "2025-03",
        "title": "Synthetic Data for Robust AI Model Development in Regulated Enterprises",
        "author": "Aditi Godbole",
        "link": "http://arxiv.org/abs/2503.12353v1",
        "abstract": "In today's business landscape, organizations need to find the right balance\nbetween using their customers' data ethically to power AI solutions and being\ncompliant regarding data privacy and data usage regulations. In this paper, we\ndiscuss synthetic data as a possible solution to this dilemma. Synthetic data\nis simulated data that mimics the real data. We explore how organizations in\nheavily regulated industries, such as financial institutions or healthcare\norganizations, can leverage synthetic data to build robust AI solutions while\nstaying compliant. We demonstrate that synthetic data offers two significant\nadvantages by allowing AI models to learn from more diverse data and by helping\norganizations stay compliant against data privacy laws with the use of\nsynthetic data instead of customer information. We discuss case studies to show\nhow synthetic data can be effectively used in the finance and healthcare sector\nwhile discussing the challenges of using synthetic data and some ethical\nquestions it raises. Our research finds that synthetic data could be a\ngame-changer for AI in regulated industries. The potential can be realized when\nindustry, academia, and regulators collaborate to build solutions. We aim to\ninitiate discussions on the use of synthetic data to build ethical,\nresponsible, and effective AI systems in regulated enterprise industries."
    },
    {
        "date": "2025-03",
        "title": "ResLPR: A LiDAR Data Restoration Network and Benchmark for Robust Place Recognition Against Weather Corruptions",
        "author": "Wenqing Kuang, Xiongwei Zhao, Yehui Shen, Congcong Wen, Huimin Lu, Zongtan Zhou, and Xieyuanli Chen",
        "link": "http://arxiv.org/abs/2503.12350v1",
        "abstract": "LiDAR-based place recognition (LPR) is a key component for autonomous\ndriving, and its resilience to environmental corruption is critical for safety\nin high-stakes applications. While state-of-the-art (SOTA) LPR methods perform\nwell in clean weather, they still struggle with weather-induced corruption\ncommonly encountered in driving scenarios. To tackle this, we propose\nResLPRNet, a novel LiDAR data restoration network that largely enhances LPR\nperformance under adverse weather by restoring corrupted LiDAR scans using a\nwavelet transform-based network. ResLPRNet is efficient, lightweight and can be\nintegrated plug-and-play with pretrained LPR models without substantial\nadditional computational cost. Given the lack of LPR datasets under adverse\nweather, we introduce ResLPR, a novel benchmark that examines SOTA LPR methods\nunder a wide range of LiDAR distortions induced by severe snow, fog, and rain\nconditions. Experiments on our proposed WeatherKITTI and WeatherNCLT datasets\ndemonstrate the resilience and notable gains achieved by using our restoration\nmethod with multiple LPR approaches in challenging weather scenarios. Our code\nand benchmark are publicly available here:\nhttps://github.com/nubot-nudt/ResLPR."
    },
    {
        "date": "2025-03",
        "title": "Augmented Adversarial Trigger Learning",
        "author": "Zhe Wang, and Yanjun Qi",
        "link": "http://arxiv.org/abs/2503.12339v1",
        "abstract": "Gradient optimization-based adversarial attack methods automate the learning\nof adversarial triggers to generate jailbreak prompts or leak system prompts.\nIn this work, we take a closer look at the optimization objective of\nadversarial trigger learning and propose ATLA: Adversarial Trigger Learning\nwith Augmented objectives. ATLA improves the negative log-likelihood loss used\nby previous studies into a weighted loss formulation that encourages the\nlearned adversarial triggers to optimize more towards response format tokens.\nThis enables ATLA to learn an adversarial trigger from just one query-response\npair and the learned trigger generalizes well to other similar queries. We\nfurther design a variation to augment trigger optimization with an auxiliary\nloss that suppresses evasive responses. We showcase how to use ATLA to learn\nadversarial suffixes jailbreaking LLMs and to extract hidden system prompts.\nEmpirically we demonstrate that ATLA consistently outperforms current\nstate-of-the-art techniques, achieving nearly 100% success in attacking while\nrequiring 80% fewer queries. ATLA learned jailbreak suffixes demonstrate high\ngeneralization to unseen queries and transfer well to new LLMs."
    },
    {
        "date": "2025-03",
        "title": "One Goal, Many Challenges: Robust Preference Optimization Amid Content-Aware and Multi-Source Noise",
        "author": "Amirabbas Afzali, Amirhossein Afsharrad, Seyed Shahabeddin Mousavi, and Sanjay Lall",
        "link": "http://arxiv.org/abs/2503.12301v1",
        "abstract": "Large Language Models (LLMs) have made significant strides in generating\nhuman-like responses, largely due to preference alignment techniques. However,\nthese methods often assume unbiased human feedback, which is rarely the case in\nreal-world scenarios. This paper introduces Content-Aware Noise-Resilient\nPreference Optimization (CNRPO), a novel framework that addresses multiple\nsources of content-dependent noise in preference learning. CNRPO employs a\nmulti-objective optimization approach to separate true preferences from\ncontent-aware noises, effectively mitigating their impact. We leverage backdoor\nattack mechanisms to efficiently learn and control various noise sources within\na single model. Theoretical analysis and extensive experiments on different\nsynthetic noisy datasets demonstrate that CNRPO significantly improves\nalignment with primary human preferences while controlling for secondary noises\nand biases, such as response length and harmfulness."
    },
    {
        "date": "2025-03",
        "title": "FedTilt: Towards Multi-Level Fairness-Preserving and Robust Federated Learning",
        "author": "Binghui Zhang, Luis Mares De La Cruz, and Binghui Wang",
        "link": "http://arxiv.org/abs/2503.13537v1",
        "abstract": "Federated Learning (FL) is an emerging decentralized learning paradigm that\ncan partly address the privacy concern that cannot be handled by traditional\ncentralized and distributed learning. Further, to make FL practical, it is also\nnecessary to consider constraints such as fairness and robustness. However,\nexisting robust FL methods often produce unfair models, and existing fair FL\nmethods only consider one-level (client) fairness and are not robust to\npersistent outliers (i.e., injected outliers into each training round) that are\ncommon in real-world FL settings. We propose \\texttt{FedTilt}, a novel FL that\ncan preserve multi-level fairness and be robust to outliers. In particular, we\nconsider two common levels of fairness, i.e., \\emph{client fairness} --\nuniformity of performance across clients, and \\emph{client data fairness} --\nuniformity of performance across different classes of data within a client.\n\\texttt{FedTilt} is inspired by the recently proposed tilted empirical risk\nminimization, which introduces tilt hyperparameters that can be flexibly tuned.\nTheoretically, we show how tuning tilt values can achieve the two-level\nfairness and mitigate the persistent outliers, and derive the convergence\ncondition of \\texttt{FedTilt} as well. Empirically, our evaluation results on a\nsuite of realistic federated datasets in diverse settings show the\neffectiveness and flexibility of the \\texttt{FedTilt} framework and the\nsuperiority to the state-of-the-arts."
    },
    {
        "date": "2025-03",
        "title": "Adaptive Label Correction for Robust Medical Image Segmentation with Noisy Labels",
        "author": "Chengxuan Qian, Kai Han, Siqi Ma, Chongwen Lyu, Zhenlong Yuan, Jun Chen, and Zhe Liu",
        "link": "http://arxiv.org/abs/2503.12218v1",
        "abstract": "Deep learning has shown remarkable success in medical image analysis, but its\nreliance on large volumes of high-quality labeled data limits its\napplicability. While noisy labeled data are easier to obtain, directly\nincorporating them into training can degrade model performance. To address this\nchallenge, we propose a Mean Teacher-based Adaptive Label Correction (ALC)\nself-ensemble framework for robust medical image segmentation with noisy\nlabels. The framework leverages the Mean Teacher architecture to ensure\nconsistent learning under noise perturbations. It includes an adaptive label\nrefinement mechanism that dynamically captures and weights differences across\nmultiple disturbance versions to enhance the quality of noisy labels.\nAdditionally, a sample-level uncertainty-based label selection algorithm is\nintroduced to prioritize high-confidence samples for network updates,\nmitigating the impact of noisy annotations. Consistency learning is integrated\nto align the predictions of the student and teacher networks, further enhancing\nmodel robustness. Extensive experiments on two public datasets demonstrate the\neffectiveness of the proposed framework, showing significant improvements in\nsegmentation performance. By fully exploiting the strengths of the Mean Teacher\nstructure, the ALC framework effectively processes noisy labels, adapts to\nchallenging scenarios, and achieves competitive results compared to\nstate-of-the-art methods."
    },
    {
        "date": "2025-03",
        "title": "Point-Cache: Test-time Dynamic and Hierarchical Cache for Robust and Generalizable Point Cloud Analysis",
        "author": "Hongyu Sun, Qiuhong Ke, Ming Cheng, Yongcai Wang, Deying Li, Chenhui Gou, and Jianfei Cai",
        "link": "http://arxiv.org/abs/2503.12150v1",
        "abstract": "This paper proposes a general solution to enable point cloud recognition\nmodels to handle distribution shifts at test time. Unlike prior methods, which\nrely heavily on training data-often inaccessible during online inference-and\nare limited to recognizing a fixed set of point cloud classes predefined during\ntraining, we explore a more practical and challenging scenario: adapting the\nmodel solely based on online test data to recognize both previously seen\nclasses and novel, unseen classes at test time. To this end, we develop\nPoint-Cache, a hierarchical cache model that captures essential clues of online\ntest samples, particularly focusing on the global structure of point clouds and\ntheir local-part details. Point-Cache, which serves as a rich 3D knowledge\nbase, is dynamically managed to prioritize the inclusion of high-quality\nsamples. Designed as a plug-and-play module, our method can be flexibly\nintegrated into large multimodal 3D models to support open-vocabulary point\ncloud recognition. Notably, our solution operates with efficiency comparable to\nzero-shot inference, as it is entirely training-free. Point-Cache demonstrates\nsubstantial gains across 8 challenging benchmarks and 4 representative large 3D\nmodels, highlighting its effectiveness. Code is available at\nhttps://github.com/auniquesun/Point-Cache."
    },
    {
        "date": "2025-03",
        "title": "Robust Isolation Forest using Soft Sparse Random Projection and Valley Emphasis Method",
        "author": "Hun Kang, and Kyoungok Kim",
        "link": "http://arxiv.org/abs/2503.12125v1",
        "abstract": "Isolation Forest (iForest) is an unsupervised anomaly detection algorithm\ndesigned to effectively detect anomalies under the assumption that anomalies\nare ``few and different.\" Various studies have aimed to enhance iForest, but\nthe resulting algorithms often exhibited significant performance disparities\nacross datasets. Additionally, the challenge of isolating rare and widely\ndistributed anomalies persisted in research focused on improving splits. To\naddress these challenges, we introduce Robust iForest (RiForest). RiForest\nleverages both existing features and random hyperplanes obtained through soft\nsparse random projection to identify superior split features for anomaly\ndetection, independent of datasets. It utilizes the underutilized valley\nemphasis method for optimal split point determination and incorporates sparsity\nrandomization in soft sparse random projection for enhanced anomaly detection\nrobustness. Across 24 benchmark datasets, experiments demonstrate RiForest's\nconsistent outperformance of existing algorithms in anomaly detection,\nemphasizing stability and robustness to noise variables."
    },
    {
        "date": "2025-03",
        "title": "Robust Dataset Distillation by Matching Adversarial Trajectories",
        "author": "Wei Lai, Tianyu Ding, ren dongdong, Lei Wang, Jing Huo, Yang Gao, and Wenbin Li",
        "link": "http://arxiv.org/abs/2503.12069v1",
        "abstract": "Dataset distillation synthesizes compact datasets that enable models to\nachieve performance comparable to training on the original large-scale\ndatasets. However, existing distillation methods overlook the robustness of the\nmodel, resulting in models that are vulnerable to adversarial attacks when\ntrained on distilled data. To address this limitation, we introduce the task of\n``robust dataset distillation\", a novel paradigm that embeds adversarial\nrobustness into the synthetic datasets during the distillation process. We\npropose Matching Adversarial Trajectories (MAT), a method that integrates\nadversarial training into trajectory-based dataset distillation. MAT\nincorporates adversarial samples during trajectory generation to obtain robust\ntraining trajectories, which are then used to guide the distillation process.\nAs experimentally demonstrated, even through natural training on our distilled\ndataset, models can achieve enhanced adversarial robustness while maintaining\ncompetitive accuracy compared to existing distillation methods. Our work\nhighlights robust dataset distillation as a new and important research\ndirection and provides a strong baseline for future research to bridge the gap\nbetween efficient training and adversarial robustness."
    },
    {
        "date": "2025-03",
        "title": "Revisiting Training-Inference Trigger Intensity in Backdoor Attacks",
        "author": "Chenhao Lin, Chenyang Zhao, Shiwei Wang, Longtian Wang, Chao Shen, and Zhengyu Zhao",
        "link": "http://arxiv.org/abs/2503.12058v1",
        "abstract": "Backdoor attacks typically place a specific trigger on certain training data,\nsuch that the model makes prediction errors on inputs with that trigger during\ninference. Despite the core role of the trigger, existing studies have commonly\nbelieved a perfect match between training-inference triggers is optimal. In\nthis paper, for the first time, we systematically explore the\ntraining-inference trigger relation, particularly focusing on their mismatch,\nbased on a Training-Inference Trigger Intensity Manipulation (TITIM) workflow.\nTITIM specifically investigates the training-inference trigger intensity, such\nas the size or the opacity of a trigger, and reveals new insights into trigger\ngeneralization and overfitting.\n  These new insights challenge the above common belief by demonstrating that\nthe training-inference trigger mismatch can facilitate attacks in two practical\nscenarios, posing more significant security threats than previously thought.\nFirst, when the inference trigger is fixed, using training triggers with mixed\nintensities leads to stronger attacks than using any single intensity. For\nexample, on CIFAR-10 with ResNet-18, mixing training triggers with 1.0 and 0.1\nopacities improves the worst-case attack success rate (ASR) (over different\ntesting opacities) of the best single-opacity attack from 10.61\\% to 92.77\\%.\nSecond, intentionally using certain mismatched training-inference triggers can\nimprove the attack stealthiness, i.e., better bypassing defenses. For example,\ncompared to the training/inference intensity of 1.0/1.0, using 1.0/0.7\ndecreases the area under the curve (AUC) of the Scale-Up defense from 0.96 to\n0.62, while maintaining a high attack ASR (99.65\\% vs. 91.62\\%). The above new\ninsights are validated to be generalizable across different backdoor attacks,\nmodels, datasets, tasks, and (digital/physical) domains."
    },
    {
        "date": "2025-03",
        "title": "Hydra-NeXt: Robust Closed-Loop Driving with Open-Loop Training",
        "author": "Zhenxin Li, Shihao Wang, Shiyi Lan, Zhiding Yu, Zuxuan Wu, and Jose M. Alvarez",
        "link": "http://arxiv.org/abs/2503.12030v1",
        "abstract": "End-to-end autonomous driving research currently faces a critical challenge\nin bridging the gap between open-loop training and closed-loop deployment.\nCurrent approaches are trained to predict trajectories in an open-loop\nenvironment, which struggle with quick reactions to other agents in closed-loop\nenvironments and risk generating kinematically infeasible plans due to the gap\nbetween open-loop training and closed-loop driving. In this paper, we introduce\nHydra-NeXt, a novel multi-branch planning framework that unifies trajectory\nprediction, control prediction, and a trajectory refinement network in one\nmodel. Unlike current open-loop trajectory prediction models that only handle\ngeneral-case planning, Hydra-NeXt further utilizes a control decoder to focus\non short-term actions, which enables faster responses to dynamic situations and\nreactive agents. Moreover, we propose the Trajectory Refinement module to\naugment and refine the planning decisions by effectively adhering to kinematic\nconstraints in closed-loop environments. This unified approach bridges the gap\nbetween open-loop training and closed-loop driving, demonstrating superior\nperformance of 65.89 Driving Score (DS) and 48.20% Success Rate (SR) on the\nBench2Drive dataset without relying on external experts for data collection.\nHydra-NeXt surpasses the previous state-of-the-art by 22.98 DS and 17.49 SR,\nmarking a significant advancement in autonomous driving. Code will be available\nat https://github.com/woxihuanjiangguo/Hydra-NeXt."
    },
    {
        "date": "2025-03",
        "title": "Mixed-feature Logistic Regression Robust to Distribution Shifts",
        "author": "Qingshi Sun, Nathan Justin, Andres Gomez, and Phebe Vayanos",
        "link": "http://arxiv.org/abs/2503.12012v1",
        "abstract": "Logistic regression models are widely used in the social and behavioral\nsciences and in high-stakes domains, due to their simplicity and\ninterpretability properties. At the same time, such domains are permeated by\ndistribution shifts, where the distribution generating the data changes between\ntraining and deployment. In this paper, we study a distributionally robust\nlogistic regression problem that seeks the model that will perform best against\nadversarial realizations of the data distribution drawn from a suitably\nconstructed Wasserstein ambiguity set. Our model and solution approach differ\nfrom prior work in that we can capture settings where the likelihood of\ndistribution shifts can vary across features, significantly broadening the\napplicability of our model relative to the state-of-the-art. We propose a\ngraph-based solution approach that can be integrated into off-the-shelf\noptimization solvers. We evaluate the performance of our model and algorithms\non numerous publicly available datasets. Our solution achieves a 408x speed-up\nrelative to the state-of-the-art. Additionally, compared to the\nstate-of-the-art, our model reduces average calibration error by up to 36.19%\nand worst-case calibration error by up to 41.70%, while increasing the average\narea under the ROC curve (AUC) by up to 18.02% and worst-case AUC by up to\n48.37%."
    },
    {
        "date": "2025-03",
        "title": "Winning the MIDST Challenge: New Membership Inference Attacks on Diffusion Models for Tabular Data Synthesis",
        "author": "Xiaoyu Wu, Yifei Pang, Terrance Liu, and Steven Wu",
        "link": "http://arxiv.org/abs/2503.12008v1",
        "abstract": "Tabular data synthesis using diffusion models has gained significant\nattention for its potential to balance data utility and privacy. However,\nexisting privacy evaluations often rely on heuristic metrics or weak membership\ninference attacks (MIA), leaving privacy risks inadequately assessed. In this\nwork, we conduct a rigorous MIA study on diffusion-based tabular synthesis,\nrevealing that state-of-the-art attacks designed for image models fail in this\nsetting. We identify noise initialization as a key factor influencing attack\nefficacy and propose a machine-learning-driven approach that leverages loss\nfeatures across different noises and time steps. Our method, implemented with a\nlightweight MLP, effectively learns membership signals, eliminating the need\nfor manual optimization. Experimental results from the MIDST Challenge @ SaTML\n2025 demonstrate the effectiveness of our approach, securing first place across\nall tracks. Code is available at\nhttps://github.com/Nicholas0228/Tartan_Federer_MIDST."
    },
    {
        "date": "2025-03",
        "title": "Internet of Things-Based Smart Precision Farming in Soilless Agriculture: Opportunities and Challenges for Global Food Security",
        "author": "Monica Dutta, Deepali Gupta, Sumegh Tharewal, Deepam Goyal, Jasminder Kaur Sandhu, Manjit Kaur, Ahmad Ali Alzubi, and Jazem Mutared Alanazi",
        "link": "http://arxiv.org/abs/2503.13528v1",
        "abstract": "The rapid growth of the global population and the continuous decline in\ncultivable land pose significant threats to food security. This challenge\nworsens as climate change further reduces the availability of farmland.\nSoilless agriculture, such as hydroponics, aeroponics, and aquaponics, offers a\nsustainable solution by enabling efficient crop cultivation in controlled\nenvironments. The integration of the Internet of Things (IoT) with smart\nprecision farming improves resource efficiency, automates environmental\ncontrol, and ensures stable and high-yield crop production. IoT-enabled smart\nfarming systems utilize real-time monitoring, data-driven decision-making, and\nautomation to optimize water and nutrient usage while minimizing human\nintervention. This paper explores the opportunities and challenges of IoT-based\nsoilless farming, highlighting its role in sustainable agriculture, urban\nfarming, and global food security. These advanced farming methods ensure\ngreater productivity, resource conservation, and year-round cultivation.\nHowever, they also face challenges such as high initial investment,\ntechnological dependency, and energy consumption. Through a comprehensive\nstudy, bibliometric analysis, and comparative analysis, this research\nhighlights current trends and research gaps. It also outlines future directions\nfor researchers, policymakers, and industry stakeholders to drive innovation\nand scalability in IoT-driven soilless agriculture. By emphasizing the benefits\nof vertical farming and Controlled Environment Agriculture (CEA)-enabled\nsoilless techniques, this paper supports informed decision-making to address\nfood security challenges and promote sustainable agricultural innovations."
    },
    {
        "date": "2025-03",
        "title": "Att-Adapter: A Robust and Precise Domain-Specific Multi-Attributes T2I Diffusion Adapter via Conditional Variational Autoencoder",
        "author": "Wonwoong Cho, Yan-Ying Chen, Matthew Klenk, David I. Inouye, and Yanxia Zhang",
        "link": "http://arxiv.org/abs/2503.11937v1",
        "abstract": "Text-to-Image (T2I) Diffusion Models have achieved remarkable performance in\ngenerating high quality images. However, enabling precise control of continuous\nattributes, especially multiple attributes simultaneously, in a new domain\n(e.g., numeric values like eye openness or car width) with text-only guidance\nremains a significant challenge. To address this, we introduce the Attribute\n(Att) Adapter, a novel plug-and-play module designed to enable fine-grained,\nmulti-attributes control in pretrained diffusion models. Our approach learns a\nsingle control adapter from a set of sample images that can be unpaired and\ncontain multiple visual attributes. The Att-Adapter leverages the decoupled\ncross attention module to naturally harmonize the multiple domain attributes\nwith text conditioning. We further introduce Conditional Variational\nAutoencoder (CVAE) to the Att-Adapter to mitigate overfitting, matching the\ndiverse nature of the visual world. Evaluations on two public datasets show\nthat Att-Adapter outperforms all LoRA-based baselines in controlling continuous\nattributes. Additionally, our method enables a broader control range and also\nimproves disentanglement across multiple attributes, surpassing StyleGAN-based\ntechniques. Notably, Att-Adapter is flexible, requiring no paired synthetic\ndata for training, and is easily scalable to multiple attributes within a\nsingle model."
    },
    {
        "date": "2025-03",
        "title": "Trust Under Siege: Label Spoofing Attacks against Machine Learning for Android Malware Detection",
        "author": "Tianwei Lan, Luca Demetrio, Farid Nait-Abdesselam, Yufei Han, and Simone Aonzo",
        "link": "http://arxiv.org/abs/2503.11841v1",
        "abstract": "Machine learning (ML) malware detectors rely heavily on crowd-sourced\nAntiVirus (AV) labels, with platforms like VirusTotal serving as a trusted\nsource of malware annotations. But what if attackers could manipulate these\nlabels to classify benign software as malicious? We introduce label spoofing\nattacks, a new threat that contaminates crowd-sourced datasets by embedding\nminimal and undetectable malicious patterns into benign samples. These patterns\ncoerce AV engines into misclassifying legitimate files as harmful, enabling\npoisoning attacks against ML-based malware classifiers trained on those data.\nWe demonstrate this scenario by developing AndroVenom, a methodology for\npolluting realistic data sources, causing consequent poisoning attacks against\nML malware detectors. Experiments show that not only state-of-the-art feature\nextractors are unable to filter such injection, but also various ML models\nexperience Denial of Service already with 1% poisoned samples. Additionally,\nattackers can flip decisions of specific unaltered benign samples by modifying\nonly 0.015% of the training data, threatening their reputation and market share\nand being unable to be stopped by anomaly detectors on training data. We\nconclude our manuscript by raising the alarm on the trustworthiness of the\ntraining process based on AV annotations, requiring further investigation on\nhow to produce proper labels for ML malware detectors."
    },
    {
        "date": "2025-03",
        "title": "Visualizing Thought: Conceptual Diagrams Enable Robust Planning in LMMs",
        "author": "Nasim Borazjanizadeh, Roei Herzig, Eduard Oks, Trevor Darrell, Rogerio Feris, and Leonid Karlinsky",
        "link": "http://arxiv.org/abs/2503.11790v1",
        "abstract": "Human reasoning relies on constructing and manipulating mental\nmodels-simplified internal representations of situations that we use to\nunderstand and solve problems. Conceptual diagrams (for example, sketches drawn\nby humans to aid reasoning) externalize these mental models, abstracting\nirrelevant details to efficiently capture relational and spatial information.\nIn contrast, Large Language Models (LLMs) and Large Multimodal Models (LMMs)\npredominantly reason through textual representations, limiting their\neffectiveness in complex multi-step combinatorial and planning tasks. In this\npaper, we propose a zero-shot fully automatic framework that enables LMMs to\nreason through multiple chains of self-generated intermediate conceptual\ndiagrams, significantly enhancing their combinatorial planning capabilities.\nOur approach does not require any human initialization beyond a natural\nlanguage description of the task. It integrates both textual and diagrammatic\nreasoning within an optimized graph-of-thought inference framework, enhanced by\nbeam search and depth-wise backtracking. Evaluated on multiple challenging PDDL\nplanning domains, our method substantially improves GPT-4o's performance (for\nexample, from 35.5% to 90.2% in Blocksworld). On more difficult planning\ndomains with solution depths up to 40, our approach outperforms even the\no1-preview reasoning model (for example, over 13% improvement in Parking).\nThese results highlight the value of conceptual diagrams as a complementary\nreasoning medium in LMMs."
    },
    {
        "date": "2025-03",
        "title": "Enhancing Resiliency of Sketch-based Security via LSB Sharing-based Dynamic Late Merging",
        "author": "Seungsam Yang, Seyed Mohammad Mehdi Mirnajafizadeh, Sian Kim, Rhongho Jang, and DaeHun Nyang",
        "link": "http://arxiv.org/abs/2503.11777v1",
        "abstract": "With the exponentially growing Internet traffic, sketch data structure with a\nprobabilistic algorithm has been expected to be an alternative solution for\nnon-compromised (non-selective) security monitoring. While facilitating\ncounting within a confined memory space, the sketch's memory efficiency and\naccuracy were further pushed to their limit through finer-grained and dynamic\ncontrol of constrained memory space to adapt to the data stream's inherent\nskewness (i.e., Zipf distribution), namely small counters with extensions. In\nthis paper, we unveil a vulnerable factor of the small counter design by\nintroducing a new sketch-oriented attack, which threatens a stream of\nstate-of-the-art sketches and their security applications. With the root cause\nanalyses, we propose Siamese Counter with enhanced adversarial resiliency and\nverified feasibility with extensive experimental and theoretical analyses.\nUnder a sketch pollution attack, Siamese Counter delivers 47% accurate results\nthan a state-of-the-art scheme, and demonstrates up to 82% more accurate\nestimation under normal measurement scenarios."
    },
    {
        "date": "2025-03",
        "title": "Centaur: Robust End-to-End Autonomous Driving with Test-Time Training",
        "author": "Chonghao Sima, Kashyap Chitta, Zhiding Yu, Shiyi Lan, Ping Luo, Andreas Geiger, Hongyang Li, and Jose M. Alvarez",
        "link": "http://arxiv.org/abs/2503.11650v1",
        "abstract": "How can we rely on an end-to-end autonomous vehicle's complex decision-making\nsystem during deployment? One common solution is to have a ``fallback layer''\nthat checks the planned trajectory for rule violations and replaces it with a\npre-defined safe action if necessary. Another approach involves adjusting the\nplanner's decisions to minimize a pre-defined ``cost function'' using\nadditional system predictions such as road layouts and detected obstacles.\nHowever, these pre-programmed rules or cost functions cannot learn and improve\nwith new training data, often resulting in overly conservative behaviors. In\nthis work, we propose Centaur (Cluster Entropy for Test-time trAining using\nUncertainty) which updates a planner's behavior via test-time training, without\nrelying on hand-engineered rules or cost functions. Instead, we measure and\nminimize the uncertainty in the planner's decisions. For this, we develop a\nnovel uncertainty measure, called Cluster Entropy, which is simple,\ninterpretable, and compatible with state-of-the-art planning algorithms. Using\ndata collected at prior test-time time-steps, we perform an update to the\nmodel's parameters using a gradient that minimizes the Cluster Entropy. With\nonly this sole gradient update prior to inference, Centaur exhibits significant\nimprovements, ranking first on the navtest leaderboard with notable gains in\nsafety-critical metrics such as time to collision. To provide detailed insights\non a per-scenario basis, we also introduce navsafe, a challenging new\nbenchmark, which highlights previously undiscovered failure modes of driving\nmodels."
    },
    {
        "date": "2025-03",
        "title": "Are Deep Speech Denoising Models Robust to Adversarial Noise?",
        "author": "Will Schwarzer, Philip S. Thomas, Andrea Fanelli, and Xiaoyu Liu",
        "link": "http://arxiv.org/abs/2503.11627v1",
        "abstract": "Deep noise suppression (DNS) models enjoy widespread use throughout a variety\nof high-stakes speech applications. However, in this paper, we show that four\nrecent DNS models can each be reduced to outputting unintelligible gibberish\nthrough the addition of imperceptible adversarial noise. Furthermore, our\nresults show the near-term plausibility of targeted attacks, which could induce\nmodels to output arbitrary utterances, and over-the-air attacks. While the\nsuccess of these attacks varies by model and setting, and attacks appear to be\nstrongest when model-specific (i.e., white-box and non-transferable), our\nresults highlight a pressing need for practical countermeasures in DNS systems."
    },
    {
        "date": "2025-03",
        "title": "Tit-for-Tat: Safeguarding Large Vision-Language Models Against Jailbreak Attacks via Adversarial Defense",
        "author": "Shuyang Hao, Yiwei Wang, Bryan Hooi, Ming-Hsuan Yang, Jun Liu, Chengcheng Tang, Zi Huang, and Yujun Cai",
        "link": "http://arxiv.org/abs/2503.11619v1",
        "abstract": "Deploying large vision-language models (LVLMs) introduces a unique\nvulnerability: susceptibility to malicious attacks via visual inputs. However,\nexisting defense methods suffer from two key limitations: (1) They solely focus\non textual defenses, fail to directly address threats in the visual domain\nwhere attacks originate, and (2) the additional processing steps often incur\nsignificant computational overhead or compromise model performance on benign\ntasks. Building on these insights, we propose ESIII (Embedding Security\nInstructions Into Images), a novel methodology for transforming the visual\nspace from a source of vulnerability into an active defense mechanism.\nInitially, we embed security instructions into defensive images through\ngradient-based optimization, obtaining security instructions in the visual\ndimension. Subsequently, we integrate security instructions from visual and\ntextual dimensions with the input query. The collaboration between security\ninstructions from different dimensions ensures comprehensive security\nprotection. Extensive experiments demonstrate that our approach effectively\nfortifies the robustness of LVLMs against such attacks while preserving their\nperformance on standard benign tasks and incurring an imperceptible increase in\ntime costs."
    },
    {
        "date": "2025-03",
        "title": "Leveraging Angle of Arrival Estimation against Impersonation Attacks in Physical Layer Authentication",
        "author": "Thuy M. Pham, Linda Senigagliesi, Marco Baldi, Rafael F. Schaefer, Gerhard P. Fettweis, and Arsenia Chorti",
        "link": "http://arxiv.org/abs/2503.11508v1",
        "abstract": "In this paper, we investigate the utilization of the angle of arrival (AoA)\nas a feature for robust physical layer authentication (PLA). While most of the\nexisting approaches to PLA focus on common features of the physical layer of\ncommunication channels, such as channel frequency response, channel impulse\nresponse or received signal strength, the use of AoA in this domain has not yet\nbeen studied in depth, particularly regarding the ability to thwart\nimpersonation attacks. In this work, we demonstrate that an impersonation\nattack targeting AoA based PLA is only feasible under strict conditions on the\nattacker's location and hardware capabilities, which highlights the AoA's\npotential as a strong feature for PLA. We extend previous works considering a\nsingle-antenna attacker to the case of a multiple-antenna attacker, and we\ndevelop a theoretical characterization of the conditions in which a successful\nimpersonation attack can be mounted. Furthermore, we leverage extensive\nsimulations in support of theoretical analyses, to validate the robustness of\nAoA-based PLA."
    },
    {
        "date": "2025-03",
        "title": "Dynamic Obstacle Avoidance with Bounded Rationality Adversarial Reinforcement Learning",
        "author": "Jose-Luis Holgado-Alvarez, Aryaman Reddi, and Carlo D'Eramo",
        "link": "http://arxiv.org/abs/2503.11467v1",
        "abstract": "Reinforcement Learning (RL) has proven largely effective in obtaining stable\nlocomotion gaits for legged robots. However, designing control algorithms which\ncan robustly navigate unseen environments with obstacles remains an ongoing\nproblem within quadruped locomotion. To tackle this, it is convenient to solve\nnavigation tasks by means of a hierarchical approach with a low-level\nlocomotion policy and a high-level navigation policy. Crucially, the high-level\npolicy needs to be robust to dynamic obstacles along the path of the agent. In\nthis work, we propose a novel way to endow navigation policies with robustness\nby a training process that models obstacles as adversarial agents, following\nthe adversarial RL paradigm. Importantly, to improve the reliability of the\ntraining process, we bound the rationality of the adversarial agent resorting\nto quantal response equilibria, and place a curriculum over its rationality. We\ncalled this method Hierarchical policies via Quantal response Adversarial\nReinforcement Learning (Hi-QARL). We demonstrate the robustness of our method\nby benchmarking it in unseen randomized mazes with multiple obstacles. To prove\nits applicability in real scenarios, our method is applied on a Unitree GO1\nrobot in simulation."
    },
    {
        "date": "2025-03",
        "title": "In Shift and In Variance: Assessing the Robustness of HAR Deep Learning Models against Variability",
        "author": "Azhar Ali Khaked, Nobuyuki Oishi, Daniel Roggen, and Paula Lago",
        "link": "http://arxiv.org/abs/2503.11466v1",
        "abstract": "Human Activity Recognition (HAR) using wearable inertial measurement unit\n(IMU) sensors can revolutionize healthcare by enabling continual health\nmonitoring, disease prediction, and routine recognition. Despite the high\naccuracy of Deep Learning (DL) HAR models, their robustness to real-world\nvariabilities remains untested, as they have primarily been trained and tested\non limited lab-confined data. In this study, we isolate subject, device,\nposition, and orientation variability to determine their effect on DL HAR\nmodels and assess the robustness of these models in real-world conditions. We\nevaluated the DL HAR models using the HARVAR and REALDISP datasets, providing a\ncomprehensive discussion on the impact of variability on data distribution\nshifts and changes in model performance. Our experiments measured shifts in\ndata distribution using Maximum Mean Discrepancy (MMD) and observed DL model\nperformance drops due to variability. We concur that studied variabilities\naffect DL HAR models differently, and there is an inverse relationship between\ndata distribution shifts and model performance. The compounding effect of\nvariability was analyzed, and the implications of variabilities in real-world\nscenarios were highlighted. MMD proved an effective metric for calculating data\ndistribution shifts and explained the drop in performance due to variabilities\nin HARVAR and REALDISP datasets. Combining our understanding of variability\nwith evaluating its effects will facilitate the development of more robust DL\nHAR models and optimal training techniques. Allowing Future models to not only\nbe assessed based on their maximum F1 score but also on their ability to\ngeneralize effectively"
    },
    {
        "date": "2025-03",
        "title": "BACE-RUL: A Bi-directional Adversarial Network with Covariate Encoding for Machine Remaining Useful Life Prediction",
        "author": "Zekai Zhang, Dan Li, Shunyu Wu, Junya Cai, Bo Zhang, See Kiong Ng, and Zibin Zheng",
        "link": "http://arxiv.org/abs/2503.11730v1",
        "abstract": "Prognostic and Health Management (PHM) are crucial ways to avoid unnecessary\nmaintenance for Cyber-Physical Systems (CPS) and improve system reliability.\nPredicting the Remaining Useful Life (RUL) is one of the most challenging tasks\nfor PHM. Existing methods require prior knowledge about the system, contrived\nassumptions, or temporal mining to model the life cycles of machine\nequipment/devices, resulting in diminished accuracy and limited applicability\nin real-world scenarios. This paper proposes a Bi-directional Adversarial\nnetwork with Covariate Encoding for machine Remaining Useful Life (BACE-RUL)\nprediction, which only adopts sensor measurements from the current life cycle\nto predict RUL rather than relying on previous consecutive cycle recordings.\nThe current sensor measurements of mechanical devices are encoded to a\nconditional space to better understand the implicit inner mechanical status.\nThe predictor is trained as a conditional generative network with the encoded\nsensor measurements as its conditions. Various experiments on several\nreal-world datasets, including the turbofan aircraft engine dataset and the\ndataset collected from degradation experiments of Li-Ion battery cells, show\nthat the proposed model is a general framework and outperforms state-of-the-art\nmethods."
    },
    {
        "date": "2025-03",
        "title": "Align in Depth: Defending Jailbreak Attacks via Progressive Answer Detoxification",
        "author": "Yingjie Zhang, Tong Liu, Zhe Zhao, Guozhu Meng, and Kai Chen",
        "link": "http://arxiv.org/abs/2503.11185v1",
        "abstract": "Large Language Models (LLMs) are vulnerable to jailbreak attacks, which use\ncrafted prompts to elicit toxic responses. These attacks exploit LLMs'\ndifficulty in dynamically detecting harmful intents during the generation\nprocess. Traditional safety alignment methods, often relying on the initial few\ngeneration steps, are ineffective due to limited computational budget. This\npaper proposes DEEPALIGN, a robust defense framework that fine-tunes LLMs to\nprogressively detoxify generated content, significantly improving both the\ncomputational budget and effectiveness of mitigating harmful generation. Our\napproach uses a hybrid loss function operating on hidden states to directly\nimprove LLMs' inherent awareness of toxity during generation. Furthermore, we\nredefine safe responses by generating semantically relevant answers to harmful\nqueries, thereby increasing robustness against representation-mutation attacks.\nEvaluations across multiple LLMs demonstrate state-of-the-art defense\nperformance against six different attack types, reducing Attack Success Rates\nby up to two orders of magnitude compared to previous state-of-the-art defense\nwhile preserving utility. This work advances LLM safety by addressing\nlimitations of conventional alignment through dynamic, context-aware\nmitigation."
    },
    {
        "date": "2025-03",
        "title": "DriveGEN: Generalized and Robust 3D Detection in Driving via Controllable Text-to-Image Diffusion Generation",
        "author": "Hongbin Lin, Zilu Guo, Yifan Zhang, Shuaicheng Niu, Yafeng Li, Ruimao Zhang, Shuguang Cui, and Zhen Li",
        "link": "http://arxiv.org/abs/2503.11122v1",
        "abstract": "In autonomous driving, vision-centric 3D detection aims to identify 3D\nobjects from images. However, high data collection costs and diverse real-world\nscenarios limit the scale of training data. Once distribution shifts occur\nbetween training and test data, existing methods often suffer from performance\ndegradation, known as Out-of-Distribution (OOD) problems. To address this,\ncontrollable Text-to-Image (T2I) diffusion offers a potential solution for\ntraining data enhancement, which is required to generate diverse OOD scenarios\nwith precise 3D object geometry. Nevertheless, existing controllable T2I\napproaches are restricted by the limited scale of training data or struggle to\npreserve all annotated 3D objects. In this paper, we present DriveGEN, a method\ndesigned to improve the robustness of 3D detectors in Driving via Training-Free\nControllable Text-to-Image Diffusion Generation. Without extra diffusion model\ntraining, DriveGEN consistently preserves objects with precise 3D geometry\nacross diverse OOD generations, consisting of 2 stages: 1) Self-Prototype\nExtraction: We empirically find that self-attention features are semantic-aware\nbut require accurate region selection for 3D objects. Thus, we extract precise\nobject features via layouts to capture 3D object geometry, termed\nself-prototypes. 2) Prototype-Guided Diffusion: To preserve objects across\nvarious OOD scenarios, we perform semantic-aware feature alignment and shallow\nfeature alignment during denoising. Extensive experiments demonstrate the\neffectiveness of DriveGEN in improving 3D detection. The code is available at\nhttps://github.com/Hongbin98/DriveGEN."
    },
    {
        "date": "2025-03",
        "title": "Weakly Supervised Contrastive Adversarial Training for Learning Robust Features from Semi-supervised Data",
        "author": "Lilin Zhang, Chengpei Wu, and Ning Yang",
        "link": "http://arxiv.org/abs/2503.11032v2",
        "abstract": "Existing adversarial training (AT) methods often suffer from incomplete\nperturbation, meaning that not all non-robust features are perturbed when\ngenerating adversarial examples (AEs). This results in residual correlations\nbetween non-robust features and labels, leading to suboptimal learning of\nrobust features. However, achieving complete perturbation, i.e., perturbing as\nmany non-robust features as possible, is challenging due to the difficulty in\ndistinguishing robust and non-robust features and the sparsity of labeled data.\nTo address these challenges, we propose a novel approach called Weakly\nSupervised Contrastive Adversarial Training (WSCAT). WSCAT ensures complete\nperturbation for improved learning of robust features by disrupting\ncorrelations between non-robust features and labels through complete AE\ngeneration over partially labeled data, grounded in information theory.\nExtensive theoretical analysis and comprehensive experiments on widely adopted\nbenchmarks validate the superiority of WSCAT. Our code is available at\nhttps://github.com/zhang-lilin/WSCAT."
    },
    {
        "date": "2025-03",
        "title": "Fast and Robust Localization for Humanoid Soccer Robot via Iterative Landmark Matching",
        "author": "Ruochen Hou, Mingzhang Zhu, Hyunwoo Nam, Gabriel I. Fernandez, and Dennis W. Hong",
        "link": "http://arxiv.org/abs/2503.11020v1",
        "abstract": "Accurate robot localization is essential for effective operation. Monte Carlo\nLocalization (MCL) is commonly used with known maps but is computationally\nexpensive due to landmark matching for each particle. Humanoid robots face\nadditional challenges, including sensor noise from locomotion vibrations and a\nlimited field of view (FOV) due to camera placement. This paper proposes a fast\nand robust localization method via iterative landmark matching (ILM) for\nhumanoid robots. The iterative matching process improves the accuracy of the\nlandmark association so that it does not need MCL to match landmarks to\nparticles. Pose estimation with the outlier removal process enhances its\nrobustness to measurement noise and faulty detections. Furthermore, an\nadditional filter can be utilized to fuse inertial data from the inertial\nmeasurement unit (IMU) and pose data from localization. We compared ILM with\nIterative Closest Point (ICP), which shows that ILM method is more robust\ntowards the error in the initial guess and easier to get a correct matching. We\nalso compared ILM with the Augmented Monte Carlo Localization (aMCL), which\nshows that ILM method is much faster than aMCL and even more accurate. The\nproposed method's effectiveness is thoroughly evaluated through experiments and\nvalidated on the humanoid robot ARTEMIS during RoboCup 2024 adult-sized soccer\ncompetition."
    },
    {
        "date": "2025-03",
        "title": "From Abstraction to Reality: DARPA's Vision for Robust Sim-to-Real Autonomy",
        "author": "Erfaun Noorani, Zachary Serlin, Ben Price, and Alvaro Velasquez",
        "link": "http://arxiv.org/abs/2503.11007v1",
        "abstract": "The DARPA Transfer from Imprecise and Abstract Models to Autonomous\nTechnologies (TIAMAT) program aims to address rapid and robust transfer of\nautonomy technologies across dynamic and complex environments, goals, and\nplatforms. Existing methods for simulation-to-reality (sim-to-real) transfer\noften rely on high-fidelity simulations and struggle with broad adaptation,\nparticularly in time-sensitive scenarios. Although many approaches have shown\nincredible performance at specific tasks, most techniques fall short when posed\nwith unforeseen, complex, and dynamic real-world scenarios due to the inherent\nlimitations of simulation. In contrast to current research that aims to bridge\nthe gap between simulation environments and the real world through increasingly\nsophisticated simulations and a combination of methods typically assuming a\nsmall sim-to-real gap -- such as domain randomization, domain adaptation,\nimitation learning, meta-learning, policy distillation, and dynamic\noptimization -- TIAMAT takes a different approach by instead emphasizing\ntransfer and adaptation of the autonomy stack directly to real-world\nenvironments by utilizing a breadth of low(er)-fidelity simulations to create\nbroadly effective sim-to-real transfers. By abstractly learning from multiple\nsimulation environments in reference to their shared semantics, TIAMAT's\napproaches aim to achieve abstract-to-real transfer for effective and rapid\nreal-world adaptation. Furthermore, this program endeavors to improve the\noverall autonomy pipeline by addressing the inherent challenges in translating\nsimulated behaviors into effective real-world performance."
    },
    {
        "date": "2025-03",
        "title": "ChatGPT Encounters Morphing Attack Detection: Zero-Shot MAD with Multi-Modal Large Language Models and General Vision Models",
        "author": "Haoyu Zhang, Raghavendra Ramachandra, Kiran Raja, and Christoph Busch",
        "link": "http://arxiv.org/abs/2503.10937v1",
        "abstract": "Face Recognition Systems (FRS) are increasingly vulnerable to face-morphing\nattacks, prompting the development of Morphing Attack Detection (MAD)\nalgorithms. However, a key challenge in MAD lies in its limited\ngeneralizability to unseen data and its lack of explainability-critical for\npractical application environments such as enrolment stations and automated\nborder control systems. Recognizing that most existing MAD algorithms rely on\nsupervised learning paradigms, this work explores a novel approach to MAD using\nzero-shot learning leveraged on Large Language Models (LLMs). We propose two\ntypes of zero-shot MAD algorithms: one leveraging general vision models and the\nother utilizing multimodal LLMs. For general vision models, we address the MAD\ntask by computing the mean support embedding of an independent support set\nwithout using morphed images. For the LLM-based approach, we employ the\nstate-of-the-art GPT-4 Turbo API with carefully crafted prompts. To evaluate\nthe feasibility of zero-shot MAD and the effectiveness of the proposed methods,\nwe constructed a print-scan morph dataset featuring various unseen morphing\nalgorithms, simulating challenging real-world application scenarios.\nExperimental results demonstrated notable detection accuracy, validating the\napplicability of zero-shot learning for MAD tasks. Additionally, our\ninvestigation into LLM-based MAD revealed that multimodal LLMs, such as\nChatGPT, exhibit remarkable generalizability to untrained MAD tasks.\nFurthermore, they possess a unique ability to provide explanations and\nguidance, which can enhance transparency and usability for end-users in\npractical applications."
    },
    {
        "date": "2025-03",
        "title": "Attacking Multimodal OS Agents with Malicious Image Patches",
        "author": "Lukas Aichberger, Alasdair Paren, Yarin Gal, Philip Torr, and Adel Bibi",
        "link": "http://arxiv.org/abs/2503.10809v1",
        "abstract": "Recent advances in operating system (OS) agents enable vision-language models\nto interact directly with the graphical user interface of an OS. These\nmultimodal OS agents autonomously perform computer-based tasks in response to a\nsingle prompt via application programming interfaces (APIs). Such APIs\ntypically support low-level operations, including mouse clicks, keyboard\ninputs, and screenshot captures. We introduce a novel attack vector: malicious\nimage patches (MIPs) that have been adversarially perturbed so that, when\ncaptured in a screenshot, they cause an OS agent to perform harmful actions by\nexploiting specific APIs. For instance, MIPs embedded in desktop backgrounds or\nshared on social media can redirect an agent to a malicious website, enabling\nfurther exploitation. These MIPs generalise across different user requests and\nscreen layouts, and remain effective for multiple OS agents. The existence of\nsuch attacks highlights critical security vulnerabilities in OS agents, which\nshould be carefully addressed before their widespread adoption."
    },
    {
        "date": "2025-03",
        "title": "A Frustratingly Simple Yet Highly Effective Attack Baseline: Over 90% Success Rate Against the Strong Black-box Models of GPT-4.5/4o/o1",
        "author": "Zhaoyi Li, Xiaohan Zhao, Dong-Dong Wu, Jiacheng Cui, and Zhiqiang Shen",
        "link": "http://arxiv.org/abs/2503.10635v1",
        "abstract": "Despite promising performance on open-source large vision-language models\n(LVLMs), transfer-based targeted attacks often fail against black-box\ncommercial LVLMs. Analyzing failed adversarial perturbations reveals that the\nlearned perturbations typically originate from a uniform distribution and lack\nclear semantic details, resulting in unintended responses. This critical\nabsence of semantic information leads commercial LVLMs to either ignore the\nperturbation entirely or misinterpret its embedded semantics, thereby causing\nthe attack to fail. To overcome these issues, we notice that identifying core\nsemantic objects is a key objective for models trained with various datasets\nand methodologies. This insight motivates our approach that refines semantic\nclarity by encoding explicit semantic details within local regions, thus\nensuring interoperability and capturing finer-grained features, and by\nconcentrating modifications on semantically rich areas rather than applying\nthem uniformly. To achieve this, we propose a simple yet highly effective\nsolution: at each optimization step, the adversarial image is cropped randomly\nby a controlled aspect ratio and scale, resized, and then aligned with the\ntarget image in the embedding space. Experimental results confirm our\nhypothesis. Our adversarial examples crafted with local-aggregated\nperturbations focused on crucial regions exhibit surprisingly good\ntransferability to commercial LVLMs, including GPT-4.5, GPT-4o,\nGemini-2.0-flash, Claude-3.5-sonnet, Claude-3.7-sonnet, and even reasoning\nmodels like o1, Claude-3.7-thinking and Gemini-2.0-flash-thinking. Our approach\nachieves success rates exceeding 90% on GPT-4.5, 4o, and o1, significantly\noutperforming all prior state-of-the-art attack methods. Our optimized\nadversarial examples under different configurations and training code are\navailable at https://github.com/VILA-Lab/M-Attack."
    },
    {
        "date": "2025-03",
        "title": "Hierarchical Self-Supervised Adversarial Training for Robust Vision Models in Histopathology",
        "author": "Hashmat Shadab Malik, Shahina Kunhimon, Muzammal Naseer, Fahad Shahbaz Khan, and Salman Khan",
        "link": "http://arxiv.org/abs/2503.10629v1",
        "abstract": "Adversarial attacks pose significant challenges for vision models in critical\nfields like healthcare, where reliability is essential. Although adversarial\ntraining has been well studied in natural images, its application to biomedical\nand microscopy data remains limited. Existing self-supervised adversarial\ntraining methods overlook the hierarchical structure of histopathology images,\nwhere patient-slide-patch relationships provide valuable discriminative\nsignals. To address this, we propose Hierarchical Self-Supervised Adversarial\nTraining (HSAT), which exploits these properties to craft adversarial examples\nusing multi-level contrastive learning and integrate it into adversarial\ntraining for enhanced robustness. We evaluate HSAT on multiclass histopathology\ndataset OpenSRH and the results show that HSAT outperforms existing methods\nfrom both biomedical and natural image domains. HSAT enhances robustness,\nachieving an average gain of 54.31% in the white-box setting and reducing\nperformance drops to 3-4% in the black-box setting, compared to 25-30% for the\nbaseline. These results set a new benchmark for adversarial training in this\ndomain, paving the way for more robust models. Our Code for training and\nevaluation is available at https://github.com/HashmatShadab/HSAT."
    },
    {
        "date": "2025-03",
        "title": "FedPCA: Noise-Robust Fair Federated Learning via Performance-Capacity Analysis",
        "author": "Nannan Wu, Zengqiang Yan, Nong Sang, Li Yu, and Chang Wen Chen",
        "link": "http://arxiv.org/abs/2503.10567v1",
        "abstract": "Training a model that effectively handles both common and rare data-i.e.,\nachieving performance fairness-is crucial in federated learning (FL). While\nexisting fair FL methods have shown effectiveness, they remain vulnerable to\nmislabeled data. Ensuring robustness in fair FL is therefore essential.\nHowever, fairness and robustness inherently compete, which causes robust\nstrategies to hinder fairness. In this paper, we attribute this competition to\nthe homogeneity in loss patterns exhibited by rare and mislabeled data clients,\npreventing existing loss-based fair and robust FL methods from effectively\ndistinguishing and handling these two distinct client types. To address this,\nwe propose performance-capacity analysis, which jointly considers model\nperformance on each client and its capacity to handle the dataset, measured by\nloss and a newly introduced feature dispersion score. This allows mislabeled\nclients to be identified by their significantly deviated performance relative\nto capacity while preserving rare data clients. Building on this, we introduce\nFedPCA, an FL method that robustly achieves fairness. FedPCA first identifies\nmislabeled clients via a Gaussian Mixture Model on loss-dispersion pairs, then\napplies fairness and robustness strategies in global aggregation and local\ntraining by adjusting client weights and selectively using reliable data.\nExtensive experiments on three datasets demonstrate FedPCA's effectiveness in\ntackling this complex challenge. Code will be publicly available upon\nacceptance."
    },
    {
        "date": "2025-03",
        "title": "MASQUE: A Text-Guided Diffusion-Based Framework for Localized and Customized Adversarial Makeup",
        "author": "Youngjin Kwon, and Xiao Zhang",
        "link": "http://arxiv.org/abs/2503.10549v1",
        "abstract": "As facial recognition is increasingly adopted for government and commercial\nservices, its potential misuse has raised serious concerns about privacy and\ncivil rights. To counteract, various anti-facial recognition techniques have\nbeen proposed for privacy protection by adversarially perturbing face images,\namong which generative makeup-based approaches are the most popular. However,\nthese methods, designed primarily to impersonate specific target identities,\ncan only achieve weak dodging success rates while increasing the risk of\ntargeted abuse. In addition, they often introduce global visual artifacts or a\nlack of adaptability to accommodate diverse makeup prompts, compromising user\nsatisfaction. To address the above limitations, we develop MASQUE, a novel\ndiffusion-based framework that generates localized adversarial makeups guided\nby user-defined text prompts. Built upon precise null-text inversion,\ncustomized cross-attention fusion with masking, and a pairwise adversarial\nguidance mechanism using images of the same individual, MASQUE achieves robust\ndodging performance without requiring any external identity. Comprehensive\nevaluations on open-source facial recognition models and commercial APIs\ndemonstrate that MASQUE significantly improves dodging success rates over all\nbaselines, along with higher perceptual fidelity and stronger adaptability to\nvarious text makeup prompts."
    },
    {
        "date": "2025-03",
        "title": "Whisper Speaker Identification: Leveraging Pre-Trained Multilingual Transformers for Robust Speaker Embeddings",
        "author": "Jakaria Islam Emon, Md Abu Salek, and Kazi Tamanna Alam",
        "link": "http://arxiv.org/abs/2503.10446v1",
        "abstract": "Speaker identification in multilingual settings presents unique challenges,\nparticularly when conventional models are predominantly trained on English\ndata. In this paper, we propose WSI (Whisper Speaker Identification), a\nframework that repurposes the encoder of the Whisper automatic speech\nrecognition model pre trained on extensive multilingual data to generate robust\nspeaker embeddings via a joint loss optimization strategy that leverages online\nhard triplet mining and self supervised Normalized Temperature-scaled Cross\nEntropy loss. By capitalizing on Whisper language-agnostic acoustic\nrepresentations, our approach effectively distinguishes speakers across diverse\nlanguages and recording conditions. Extensive evaluations on multiple corpora,\nincluding VoxTube (multilingual), JVS (Japanese), CallHome (German, Spanish,\nChinese, and Japanese), and Voxconverse (English), demonstrate that WSI\nconsistently outperforms state-of-the-art baselines, namely Pyannote Embedding,\nECAPA TDNN, and Xvector, in terms of lower equal error rates and higher AUC\nscores. These results validate our hypothesis that a multilingual pre-trained\nASR encoder, combined with joint loss optimization, substantially improves\nspeaker identification performance in non-English languages."
    },
    {
        "date": "2025-03",
        "title": "HyperArm Bandit Optimization: A Novel approach to Hyperparameter Optimization and an Analysis of Bandit Algorithms in Stochastic and Adversarial Settings",
        "author": "Samih Karroum, and Saad Mazhar",
        "link": "http://arxiv.org/abs/2503.10282v1",
        "abstract": "This paper explores the application of bandit algorithms in both stochastic\nand adversarial settings, with a focus on theoretical analysis and practical\napplications. The study begins by introducing bandit problems, distinguishing\nbetween stochastic and adversarial variants, and examining key algorithms such\nas Explore-Then-Commit (ETC), Upper Confidence Bound (UCB), and\nExponential-Weight Algorithm for Exploration and Exploitation (EXP3).\nTheoretical regret bounds are analyzed to compare the performance of these\nalgorithms. The paper then introduces a novel framework, HyperArm Bandit\nOptimization (HABO), which applies EXP3 to hyperparameter tuning in machine\nlearning models. Unlike traditional methods that treat entire configurations as\narms, HABO treats individual hyperparameters as super-arms, and its potential\nconfigurations as sub-arms, enabling dynamic resource allocation and efficient\nexploration. Experimental results demonstrate HABO's effectiveness in\nclassification and regression tasks, outperforming Bayesian Optimization in\nterms of computational efficiency and accuracy. The paper concludes with\ninsights into the convergence guarantees of HABO and its potential for scalable\nand robust hyperparameter optimization."
    },
    {
        "date": "2025-03",
        "title": "Robust Learning-Based Sparse Recovery for Device Activity Detection in Grant-Free Random Access Cell-Free Massive MIMO: Enhancing Resilience to Impairments",
        "author": "Ali Elkeshawy, Haifa Fares, and Amor Nafkha",
        "link": "http://arxiv.org/abs/2503.10280v1",
        "abstract": "Massive MIMO is considered a key enabler to support massive machine-type\ncommunication (mMTC). While massive access schemes have been extensively\nanalyzed for co-located massive MIMO arrays, this paper explores activity\ndetection in grant-free random access for mMTC within the context of cell-free\nmassive MIMO systems, employing distributed antenna arrays. This sparse support\nrecovery of device activity status is performed by a finite cluster of access\npoints (APs) from a large number of geographically distributed APs\ncollaborating to serve a larger number of devices. Active devices transmit\nnon-orthogonal pilot sequences to APs, which forward the received signals to a\ncentral processing unit (CPU) for collaborative activity detection. This paper\nproposes a simple and efficient data-driven algorithm tailored for device\nactivity detection, implemented centrally at the CPU. Furthermore, the study\nassesses the algorithm's robustness to input perturbations and examines the\neffects of adopting fixed-point representation on its performance."
    },
    {
        "date": "2025-03",
        "title": "Numerically robust Gaussian state estimation with singular observation noise",
        "author": "Nicholas Kr\u00e4mer, and Filip Tronarp",
        "link": "http://arxiv.org/abs/2503.10279v1",
        "abstract": "This article proposes numerically robust algorithms for Gaussian state\nestimation with singular observation noise. Our approach combines a series of\nbasis changes with Bayes' rule, transforming the singular estimation problem\ninto a nonsingular one with reduced state dimension. In addition to ensuring\nlow runtime and numerical stability, our proposal facilitates\nmarginal-likelihood computations and Gauss-Markov representations of the\nposterior process. We analyse the proposed method's computational savings and\nnumerical robustness and validate our findings in a series of simulations."
    },
    {
        "date": "2025-03",
        "title": "Robustness Tokens: Towards Adversarial Robustness of Transformers",
        "author": "Brian Pulfer, Yury Belousov, and Slava Voloshynovskiy",
        "link": "http://arxiv.org/abs/2503.10191v1",
        "abstract": "Recently, large pre-trained foundation models have become widely adopted by\nmachine learning practitioners for a multitude of tasks. Given that such models\nare publicly available, relying on their use as backbone models for downstream\ntasks might result in high vulnerability to adversarial attacks crafted with\nthe same public model. In this work, we propose Robustness Tokens, a novel\napproach specific to the transformer architecture that fine-tunes a few\nadditional private tokens with low computational requirements instead of tuning\nmodel parameters as done in traditional adversarial training. We show that\nRobustness Tokens make Vision Transformer models significantly more robust to\nwhite-box adversarial attacks while also retaining the original downstream\nperformances."
    },
    {
        "date": "2025-03",
        "title": "Exploring the Vulnerabilities of Federated Learning: A Deep Dive into Gradient Inversion Attacks",
        "author": "Pengxin Guo, Runxi Wang, Shuang Zeng, Jinjing Zhu, Haoning Jiang, Yanran Wang, Yuyin Zhou, Feifei Wang, Hui Xiong, and Liangqiong Qu",
        "link": "http://arxiv.org/abs/2503.11514v1",
        "abstract": "Federated Learning (FL) has emerged as a promising privacy-preserving\ncollaborative model training paradigm without sharing raw data. However, recent\nstudies have revealed that private information can still be leaked through\nshared gradient information and attacked by Gradient Inversion Attacks (GIA).\nWhile many GIA methods have been proposed, a detailed analysis, evaluation, and\nsummary of these methods are still lacking. Although various survey papers\nsummarize existing privacy attacks in FL, few studies have conducted extensive\nexperiments to unveil the effectiveness of GIA and their associated limiting\nfactors in this context. To fill this gap, we first undertake a systematic\nreview of GIA and categorize existing methods into three types, i.e.,\n\\textit{optimization-based} GIA (OP-GIA), \\textit{generation-based} GIA\n(GEN-GIA), and \\textit{analytics-based} GIA (ANA-GIA). Then, we comprehensively\nanalyze and evaluate the three types of GIA in FL, providing insights into the\nfactors that influence their performance, practicality, and potential threats.\nOur findings indicate that OP-GIA is the most practical attack setting despite\nits unsatisfactory performance, while GEN-GIA has many dependencies and ANA-GIA\nis easily detectable, making them both impractical. Finally, we offer a\nthree-stage defense pipeline to users when designing FL frameworks and\nprotocols for better privacy protection and share some future research\ndirections from the perspectives of attackers and defenders that we believe\nshould be pursued. We hope that our study can help researchers design more\nrobust FL frameworks to defend against these attacks."
    },
    {
        "date": "2025-03",
        "title": "Team NYCU at Defactify4: Robust Detection and Source Identification of AI-Generated Images Using CNN and CLIP-Based Models",
        "author": "Tsan-Tsung Yang, I-Wei Chen, Kuan-Ting Chen, Shang-Hsuan Chiang, and Wen-Chih Peng",
        "link": "http://arxiv.org/abs/2503.10718v1",
        "abstract": "With the rapid advancement of generative AI, AI-generated images have become\nincreasingly realistic, raising concerns about creativity, misinformation, and\ncontent authenticity. Detecting such images and identifying their source models\nhas become a critical challenge in ensuring the integrity of digital media.\nThis paper tackles the detection of AI-generated images and identifying their\nsource models using CNN and CLIP-ViT classifiers. For the CNN-based classifier,\nwe leverage EfficientNet-B0 as the backbone and feed with RGB channels,\nfrequency features, and reconstruction errors, while for CLIP-ViT, we adopt a\npretrained CLIP image encoder to extract image features and SVM to perform\nclassification. Evaluated on the Defactify 4 dataset, our methods demonstrate\nstrong performance in both tasks, with CLIP-ViT showing superior robustness to\nimage perturbations. Compared to baselines like AEROBLADE and OCC-CLIP, our\napproach achieves competitive results. Notably, our method ranked Top-3 overall\nin the Defactify 4 competition, highlighting its effectiveness and\ngeneralizability. All of our implementations can be found in\nhttps://github.com/uuugaga/Defactify_4"
    },
    {
        "date": "2025-03",
        "title": "AdvPaint: Protecting Images from Inpainting Manipulation via Adversarial Attention Disruption",
        "author": "Joonsung Jeon, Woo Jae Kim, Suhyeon Ha, Sooel Son, and Sung-eui Yoon",
        "link": "http://arxiv.org/abs/2503.10081v1",
        "abstract": "The outstanding capability of diffusion models in generating high-quality\nimages poses significant threats when misused by adversaries. In particular, we\nassume malicious adversaries exploiting diffusion models for inpainting tasks,\nsuch as replacing a specific region with a celebrity. While existing methods\nfor protecting images from manipulation in diffusion-based generative models\nhave primarily focused on image-to-image and text-to-image tasks, the challenge\nof preventing unauthorized inpainting has been rarely addressed, often\nresulting in suboptimal protection performance. To mitigate inpainting abuses,\nwe propose ADVPAINT, a novel defensive framework that generates adversarial\nperturbations that effectively disrupt the adversary's inpainting tasks.\nADVPAINT targets the self- and cross-attention blocks in a target diffusion\ninpainting model to distract semantic understanding and prompt interactions\nduring image generation. ADVPAINT also employs a two-stage perturbation\nstrategy, dividing the perturbation region based on an enlarged bounding box\naround the object, enhancing robustness across diverse masks of varying shapes\nand sizes. Our experimental results demonstrate that ADVPAINT's perturbations\nare highly effective in disrupting the adversary's inpainting tasks,\noutperforming existing methods; ADVPAINT attains over a 100-point increase in\nFID and substantial decreases in precision."
    },
    {
        "date": "2025-03",
        "title": "Demoting Security via Exploitation of Cache Demote Operation in Intel's Latest ISA Extension",
        "author": "Taehun Kim, Hyerean Jang, and Youngjoo Shin",
        "link": "http://arxiv.org/abs/2503.10074v1",
        "abstract": "ISA extensions are increasingly adopted to boost the performance of\nspecialized workloads without requiring an entire architectural redesign.\nHowever, these enhancements can inadvertently expose new attack surfaces in the\nmicroarchitecture. In this paper, we investigate Intel's recently introduced\ncldemote extension, which promotes efficient data sharing by transferring cache\nlines from upper-level caches to the Last Level Cache (LLC). Despite its\nperformance benefits, we uncover critical properties-unprivileged access,\ninter-cache state transition, and fault suppression-that render cldemote\nexploitable for microarchitectural attacks. We propose two new attack\nprimitives, Flush+Demote and Demote+Time, built on our analysis. Flush+Demote\nconstructs a covert channel with a bandwidth of 2.84 Mbps and a bit error rate\nof 0.018%, while Demote+Time derandomizes the kernel base address in 2.49 ms on\nLinux. Furthermore, we show that leveraging cldemote accelerates eviction set\nconstruction in non-inclusive LLC designs by obviating the need for helper\nthreads or extensive cache conflicts, thereby reducing construction time by 36%\nyet retaining comparable success rates. Finally, we examine how ISA extensions\ncontribute to broader microarchitectural attacks, identifying five key\nexploitable characteristics and categorizing four distinct attack types. We\nalso discuss potential countermeasures, highlighting the far-reaching security\nimplications of emerging ISA extensions."
    },
    {
        "date": "2025-03",
        "title": "Provably Secure Covert Messaging Using Image-based Diffusion Processes",
        "author": "Luke A. Bauer, Wenxuan Bao, and Vincent Bindschaedler",
        "link": "http://arxiv.org/abs/2503.10063v1",
        "abstract": "We consider the problem of securely and robustly embedding covert messages\ninto an image-based diffusion model's output. The sender and receiver want to\nexchange the maximum amount of information possible per diffusion sampled image\nwhile remaining undetected. The adversary wants to detect that such\ncommunication is taking place by identifying those diffusion samples that\ncontain covert messages. To maximize robustness to transformations of the\ndiffusion sample, a strategy is for the sender and the receiver to embed the\nmessage in the initial latents. We first show that prior work that attempted\nthis is easily broken because their embedding technique alters the latents'\ndistribution. We then propose a straightforward method to embed covert messages\nin the initial latent {\\em without} altering the distribution. We prove that\nour construction achieves indistinguishability to any probabilistic polynomial\ntime adversary. Finally, we discuss and analyze empirically the tradeoffs\nbetween embedding capacity, message recovery rates, and robustness. We find\nthat optimizing the inversion method for error correction is crucial for\nreliability."
    },
    {
        "date": "2025-03",
        "title": "Distributionally Robust Multi-Agent Reinforcement Learning for Dynamic Chute Mapping",
        "author": "Guangyi Liu, Suzan Iloglu, Michael Caldara, Joseph W. Durham, and Michael M. Zavlanos",
        "link": "http://arxiv.org/abs/2503.09755v1",
        "abstract": "In Amazon robotic warehouses, the destination-to-chute mapping problem is\ncrucial for efficient package sorting. Often, however, this problem is\ncomplicated by uncertain and dynamic package induction rates, which can lead to\nincreased package recirculation. To tackle this challenge, we introduce a\nDistributionally Robust Multi-Agent Reinforcement Learning (DRMARL) framework\nthat learns a destination-to-chute mapping policy that is resilient to\nadversarial variations in induction rates. Specifically, DRMARL relies on group\ndistributionally robust optimization (DRO) to learn a policy that performs well\nnot only on average but also on each individual subpopulation of induction\nrates within the group that capture, for example, different seasonality or\noperation modes of the system. This approach is then combined with a novel\ncontextual bandit-based predictor of the worst-case induction distribution for\neach state-action pair, significantly reducing the cost of exploration and\nthereby increasing the learning efficiency and scalability of our framework.\nExtensive simulations demonstrate that DRMARL achieves robust chute mapping in\nthe presence of varying induction distributions, reducing package recirculation\nby an average of 80\\% in the simulation scenario."
    },
    {
        "date": "2025-03",
        "title": "Enhancing Adversarial Example Detection Through Model Explanation",
        "author": "Qian Ma, and Ziping Ye",
        "link": "http://arxiv.org/abs/2503.09735v1",
        "abstract": "Adversarial examples are a major problem for machine learning models, leading\nto a continuous search for effective defenses. One promising direction is to\nleverage model explanations to better understand and defend against these\nattacks. We looked at AmI, a method proposed by a NeurIPS 2018 spotlight paper\nthat uses model explanations to detect adversarial examples. Our study shows\nthat while AmI is a promising idea, its performance is too dependent on\nspecific settings (e.g., hyperparameter) and external factors such as the\noperating system and the deep learning framework used, and such drawbacks limit\nAmI's practical usage. Our findings highlight the need for more robust defense\nmechanisms that are effective under various conditions. In addition, we\nadvocate for a comprehensive evaluation framework for defense techniques."
    },
    {
        "date": "2025-03",
        "title": "How Feasible is Augmenting Fake Nodes with Learnable Features as a Counter-strategy against Link Stealing Attacks?",
        "author": "Mir Imtiaz Mostafiz, Imtiaz Karim, and Elisa Bertino",
        "link": "http://arxiv.org/abs/2503.09726v1",
        "abstract": "Graph Neural Networks (GNNs) are widely used and deployed for graph-based\nprediction tasks. However, as good as GNNs are for learning graph data, they\nalso come with the risk of privacy leakage. For instance, an attacker can run\ncarefully crafted queries on the GNNs and, from the responses, can infer the\nexistence of an edge between a pair of nodes. This attack, dubbed as a\n\"link-stealing\" attack, can jeopardize the user's privacy by leaking\npotentially sensitive information. To protect against this attack, we propose\nan approach called \"$(N)$ode $(A)$ugmentation for $(R)$estricting $(G)$raphs\nfrom $(I)$nsinuating their $(S)$tructure\" ($NARGIS$) and study its feasibility.\n$NARGIS$ is focused on reshaping the graph embedding space so that the\nposterior from the GNN model will still provide utility for the prediction task\nbut will introduce ambiguity for the link-stealing attackers. To this end,\n$NARGIS$ applies spectral clustering on the given graph to facilitate it being\naugmented with new nodes -- that have learned features instead of fixed ones.\nIt utilizes tri-level optimization for learning parameters for the GNN model,\nsurrogate attacker model, and our defense model (i.e. learnable node features).\nWe extensively evaluate $NARGIS$ on three benchmark citation datasets over\neight knowledge availability settings for the attackers. We also evaluate the\nmodel fidelity and defense performance on influence-based link inference\nattacks. Through our studies, we have figured out the best feature of $NARGIS$\n-- its superior fidelity-privacy performance trade-off in a significant number\nof cases. We also have discovered in which cases the model needs to be\nimproved, and proposed ways to integrate different schemes to make the model\nmore robust against link stealing attacks."
    },
    {
        "date": "2025-03",
        "title": "Revisiting Backdoor Attacks on Time Series Classification in the Frequency Domain",
        "author": "Yuanmin Huang, Mi Zhang, Zhaoxiang Wang, Wenxuan Li, and Min Yang",
        "link": "http://arxiv.org/abs/2503.09712v2",
        "abstract": "Time series classification (TSC) is a cornerstone of modern web applications,\npowering tasks such as financial data analysis, network traffic monitoring, and\nuser behavior analysis. In recent years, deep neural networks (DNNs) have\ngreatly enhanced the performance of TSC models in these critical domains.\nHowever, DNNs are vulnerable to backdoor attacks, where attackers can covertly\nimplant triggers into models to induce malicious outcomes. Existing backdoor\nattacks targeting DNN-based TSC models remain elementary. In particular, early\nmethods borrow trigger designs from computer vision, which are ineffective for\ntime series data. More recent approaches utilize generative models for trigger\ngeneration, but at the cost of significant computational complexity. In this\nwork, we analyze the limitations of existing attacks and introduce an enhanced\nmethod, FreqBack. Drawing inspiration from the fact that DNN models inherently\ncapture frequency domain features in time series data, we identify that\nimproper perturbations in the frequency domain are the root cause of\nineffective attacks. To address this, we propose to generate triggers both\neffectively and efficiently, guided by frequency analysis. FreqBack exhibits\nsubstantial performance across five models and eight datasets, achieving an\nimpressive attack success rate of over 90%, while maintaining less than a 3%\ndrop in model accuracy on clean data."
    },
    {
        "date": "2025-03",
        "title": "Silent Branding Attack: Trigger-free Data Poisoning Attack on Text-to-Image Diffusion Models",
        "author": "Sangwon Jang, June Suk Choi, Jaehyeong Jo, Kimin Lee, and Sung Ju Hwang",
        "link": "http://arxiv.org/abs/2503.09669v1",
        "abstract": "Text-to-image diffusion models have achieved remarkable success in generating\nhigh-quality contents from text prompts. However, their reliance on publicly\navailable data and the growing trend of data sharing for fine-tuning make these\nmodels particularly vulnerable to data poisoning attacks. In this work, we\nintroduce the Silent Branding Attack, a novel data poisoning method that\nmanipulates text-to-image diffusion models to generate images containing\nspecific brand logos or symbols without any text triggers. We find that when\ncertain visual patterns are repeatedly in the training data, the model learns\nto reproduce them naturally in its outputs, even without prompt mentions.\nLeveraging this, we develop an automated data poisoning algorithm that\nunobtrusively injects logos into original images, ensuring they blend naturally\nand remain undetected. Models trained on this poisoned dataset generate images\ncontaining logos without degrading image quality or text alignment. We\nexperimentally validate our silent branding attack across two realistic\nsettings on large-scale high-quality image datasets and style personalization\ndatasets, achieving high success rates even without a specific text trigger.\nHuman evaluation and quantitative metrics including logo detection show that\nour method can stealthily embed logos."
    },
    {
        "date": "2025-03",
        "title": "RESTRAIN: Reinforcement Learning-Based Secure Framework for Trigger-Action IoT Environment",
        "author": "Md Morshed Alam, Lokesh Chandra Das, Sandip Roy, Sachin Shetty, and Weichao Wang",
        "link": "http://arxiv.org/abs/2503.09513v1",
        "abstract": "Internet of Things (IoT) platforms with trigger-action capability allow event\nconditions to trigger actions in IoT devices autonomously by creating a chain\nof interactions. Adversaries exploit this chain of interactions to maliciously\ninject fake event conditions into IoT hubs, triggering unauthorized actions on\ntarget IoT devices to implement remote injection attacks. Existing defense\nmechanisms focus mainly on the verification of event transactions using\nphysical event fingerprints to enforce the security policies to block unsafe\nevent transactions. These approaches are designed to provide offline defense\nagainst injection attacks. The state-of-the-art online defense mechanisms offer\nreal-time defense, but extensive reliability on the inference of attack impacts\non the IoT network limits the generalization capability of these approaches. In\nthis paper, we propose a platform-independent multi-agent online defense\nsystem, namely RESTRAIN, to counter remote injection attacks at runtime.\nRESTRAIN allows the defense agent to profile attack actions at runtime and\nleverages reinforcement learning to optimize a defense policy that complies\nwith the security requirements of the IoT network. The experimental results\nshow that the defense agent effectively takes real-time defense actions against\ncomplex and dynamic remote injection attacks and maximizes the security gain\nwith minimal computational overhead."
    },
    {
        "date": "2025-03",
        "title": "Towards Robust Multimodal Representation: A Unified Approach with Adaptive Experts and Alignment",
        "author": "Nazanin Moradinasab, Saurav Sengupta, Jiebei Liu, Sana Syed, and Donald E. Brown",
        "link": "http://arxiv.org/abs/2503.09498v1",
        "abstract": "Healthcare relies on multiple types of data, such as medical images, genetic\ninformation, and clinical records, to improve diagnosis and treatment. However,\nmissing data is a common challenge due to privacy restrictions, cost, and\ntechnical issues, making many existing multi-modal models unreliable. To\naddress this, we propose a new multi-model model called Mixture of Experts,\nSymmetric Aligning, and Reconstruction (MoSARe), a deep learning framework that\nhandles incomplete multimodal data while maintaining high accuracy. MoSARe\nintegrates expert selection, cross-modal attention, and contrastive learning to\nimprove feature representation and decision-making. Our results show that\nMoSARe outperforms existing models in situations when the data is complete.\nFurthermore, it provides reliable predictions even when some data are missing.\nThis makes it especially useful in real-world healthcare settings, including\nresource-limited environments. Our code is publicly available at\nhttps://github.com/NazaninMn/MoSARe."
    },
    {
        "date": "2025-03",
        "title": "Robust Multimodal Survival Prediction with the Latent Differentiation Conditional Variational AutoEncoder",
        "author": "Junjie Zhou, Jiao Tang, Yingli Zuo, Peng Wan, Daoqiang Zhang, and Wei Shao",
        "link": "http://arxiv.org/abs/2503.09496v2",
        "abstract": "The integrative analysis of histopathological images and genomic data has\nreceived increasing attention for survival prediction of human cancers.\nHowever, the existing studies always hold the assumption that full modalities\nare available. As a matter of fact, the cost for collecting genomic data is\nhigh, which sometimes makes genomic data unavailable in testing samples. A\ncommon way of tackling such incompleteness is to generate the genomic\nrepresentations from the pathology images. Nevertheless, such strategy still\nfaces the following two challenges: (1) The gigapixel whole slide images (WSIs)\nare huge and thus hard for representation. (2) It is difficult to generate the\ngenomic embeddings with diverse function categories in a unified generative\nframework. To address the above challenges, we propose a Conditional Latent\nDifferentiation Variational AutoEncoder (LD-CVAE) for robust multimodal\nsurvival prediction, even with missing genomic data. Specifically, a\nVariational Information Bottleneck Transformer (VIB-Trans) module is proposed\nto learn compressed pathological representations from the gigapixel WSIs. To\ngenerate different functional genomic features, we develop a novel Latent\nDifferentiation Variational AutoEncoder (LD-VAE) to learn the common and\nspecific posteriors for the genomic embeddings with diverse functions. Finally,\nwe use the product-of-experts technique to integrate the genomic common\nposterior and image posterior for the joint latent distribution estimation in\nLD-CVAE. We test the effectiveness of our method on five different cancer\ndatasets, and the experimental results demonstrate its superiority in both\ncomplete and missing modality scenarios."
    },
    {
        "date": "2025-03",
        "title": "Project-Probe-Aggregate: Efficient Fine-Tuning for Group Robustness",
        "author": "Beier Zhu, Jiequan Cui, Hanwang Zhang, and Chi Zhang",
        "link": "http://arxiv.org/abs/2503.09487v2",
        "abstract": "While image-text foundation models have succeeded across diverse downstream\ntasks, they still face challenges in the presence of spurious correlations\nbetween the input and label. To address this issue, we propose a simple\nthree-step approach,Project-Probe-Aggregate (PPA), that enables\nparameter-efficient fine-tuning for foundation models without relying on group\nannotations. Building upon the failure-based debiasing scheme, our method, PPA,\nimproves its two key components: minority samples identification and the robust\ntraining algorithm. Specifically, we first train biased classifiers by\nprojecting image features onto the nullspace of class proxies from text\nencoders. Next, we infer group labels using the biased classifier and probe\ngroup targets with prior correction. Finally, we aggregate group weights of\neach class to produce the debiased classifier. Our theoretical analysis shows\nthat our PPA enhances minority group identification and is Bayes optimal for\nminimizing the balanced group error, mitigating spurious correlations.\nExtensive experimental results confirm the effectiveness of our PPA: it\noutperforms the state-of-the-art by an average worst-group accuracy while\nrequiring less than 0.01% tunable parameters without training group labels."
    },
    {
        "date": "2025-03",
        "title": "Automatic Association of Quality Requirements and Quantifiable Metrics for Cloud Security Certification",
        "author": "John Bianchi, Shuya Dong, Luca Petrillo, and Marinella Petrocchi",
        "link": "http://arxiv.org/abs/2503.09460v1",
        "abstract": "The European Cybersecurity Certification Scheme for Cloud Services (EUCS) is\none of the first cybersecurity schemes in Europe, defined by the European Union\nAgency for Cybersecurity (ENISA). It aims to encourage cloud providers to\nstrengthen their cybersecurity policies in order to receive an official seal of\napproval from European authorities. EUCS defines a set of security requirements\nthat the cloud provider must meet, in whole or in part, in order to achieve the\nsecurity certification. The requirements are written in natural language and\ncover every aspect of security in the cloud environment, from logging access to\nprotecting the system with anti-malware tools to training staff. Operationally,\neach requirement is associated with one or more evaluable metrics. For example,\na requirement to monitor access attempts to a service will have associated\nmetrics that take into account the number of accesses, the number of access\nattempts, who is accessing, and what resources are being used. Partners in the\nEuropean project Medina, which ended in October 2023, defined 163 metrics and\nmanually mapped them to 70 EUCS requirements. Manual mapping is intuitively a\nlong and costly process in terms of human resources. This paper proposes an\napproach based on Sentence Transformers to automatically associate requirements\nand metrics. In terms of correctness of associations, the proposed method\nachieves a Normalized Discounted Cumulative Gain of 0.640, improving a previous\nexperiment by 0.146 points."
    },
    {
        "date": "2025-03",
        "title": "Benefits of Learning Rate Annealing for Tuning-Robustness in Stochastic Optimization",
        "author": "Amit Attia, and Tomer Koren",
        "link": "http://arxiv.org/abs/2503.09411v1",
        "abstract": "The learning rate in stochastic gradient methods is a critical hyperparameter\nthat is notoriously costly to tune via standard grid search, especially for\ntraining modern large-scale models with billions of parameters. We identify a\ntheoretical advantage of learning rate annealing schemes that decay the\nlearning rate to zero at a polynomial rate, such as the widely-used cosine\nschedule, by demonstrating their increased robustness to initial parameter\nmisspecification due to a coarse grid search. We present an analysis in a\nstochastic convex optimization setup demonstrating that the convergence rate of\nstochastic gradient descent with annealed schedules depends sublinearly on the\nmultiplicative misspecification factor $\\rho$ (i.e., the grid resolution),\nachieving a rate of $O(\\rho^{1/(2p+1)}/\\sqrt{T})$ where $p$ is the degree of\npolynomial decay and $T$ is the number of steps, in contrast to the\n$O(\\rho/\\sqrt{T})$ rate that arises with fixed stepsizes and exhibits a linear\ndependence on $\\rho$. Experiments confirm the increased robustness compared to\ntuning with a fixed stepsize, that has significant implications for the\ncomputational overhead of hyperparameter search in practical training\nscenarios."
    },
    {
        "date": "2025-03",
        "title": "AI-based Framework for Robust Model-Based Connector Mating in Robotic Wire Harness Installation",
        "author": "Claudius Kienle, Benjamin Alt, Finn Schneider, Tobias Pertlwieser, Rainer J\u00e4kel, and Rania Rayyes",
        "link": "http://arxiv.org/abs/2503.09409v1",
        "abstract": "Despite the widespread adoption of industrial robots in automotive assembly,\nwire harness installation remains a largely manual process, as it requires\nprecise and flexible manipulation. To address this challenge, we design a novel\nAI-based framework that automates cable connector mating by integrating force\ncontrol with deep visuotactile learning. Our system optimizes\nsearch-and-insertion strategies using first-order optimization over a\nmultimodal transformer architecture trained on visual, tactile, and\nproprioceptive data. Additionally, we design a novel automated data collection\nand optimization pipeline that minimizes the need for machine learning\nexpertise. The framework optimizes robot programs that run natively on standard\nindustrial controllers, permitting human experts to audit and certify them.\nExperimental validations on a center console assembly task demonstrate\nsignificant improvements in cycle times and robustness compared to conventional\nrobot programming approaches. Videos are available under\nhttps://claudius-kienle.github.io/AppMuTT."
    },
    {
        "date": "2025-03",
        "title": "Membership Inference Attacks fueled by Few-Short Learning to detect privacy leakage tackling data integrity",
        "author": "Daniel Jim\u00e9nez-L\u00f3pez, Nuria Rodr\u00edguez-Barroso, M. Victoria Luz\u00f3n, and Francisco Herrera",
        "link": "http://arxiv.org/abs/2503.09365v1",
        "abstract": "Deep learning models have an intrinsic privacy issue as they memorize parts\nof their training data, creating a privacy leakage. Membership Inference\nAttacks (MIA) exploit it to obtain confidential information about the data used\nfor training, aiming to steal information. They can be repurposed as a\nmeasurement of data integrity by inferring whether it was used to train a\nmachine learning model. While state-of-the-art attacks achieve a significant\nprivacy leakage, their requirements are not feasible enough, hindering their\nrole as practical tools to assess the magnitude of the privacy risk. Moreover,\nthe most appropriate evaluation metric of MIA, the True Positive Rate at low\nFalse Positive Rate lacks interpretability. We claim that the incorporation of\nFew-Shot Learning techniques to the MIA field and a proper qualitative and\nquantitative privacy evaluation measure should deal with these issues. In this\ncontext, our proposal is twofold. We propose a Few-Shot learning based MIA,\ncoined as the FeS-MIA model, which eases the evaluation of the privacy breach\nof a deep learning model by significantly reducing the number of resources\nrequired for the purpose. Furthermore, we propose an interpretable quantitative\nand qualitative measure of privacy, referred to as Log-MIA measure. Jointly,\nthese proposals provide new tools to assess the privacy leakage and to ease the\nevaluation of the training data integrity of deep learning models, that is, to\nanalyze the privacy breach of a deep learning model. Experiments carried out\nwith MIA over image classification and language modeling tasks and its\ncomparison to the state-of-the-art show that our proposals excel at reporting\nthe privacy leakage of a deep learning model with little extra information."
    },
    {
        "date": "2025-03",
        "title": "Safer or Luckier? LLMs as Safety Evaluators Are Not Robust to Artifacts",
        "author": "Hongyu Chen, and Seraphina Goldfarb-Tarrant",
        "link": "http://arxiv.org/abs/2503.09347v1",
        "abstract": "Large Language Models (LLMs) are increasingly employed as automated\nevaluators to assess the safety of generated content, yet their reliability in\nthis role remains uncertain. This study evaluates a diverse set of 11 LLM judge\nmodels across critical safety domains, examining three key aspects:\nself-consistency in repeated judging tasks, alignment with human judgments, and\nsusceptibility to input artifacts such as apologetic or verbose phrasing. Our\nfindings reveal that biases in LLM judges can significantly distort the final\nverdict on which content source is safer, undermining the validity of\ncomparative evaluations. Notably, apologetic language artifacts alone can skew\nevaluator preferences by up to 98\\%. Contrary to expectations, larger models do\nnot consistently exhibit greater robustness, while smaller models sometimes\nshow higher resistance to specific artifacts. To mitigate LLM evaluator\nrobustness issues, we investigate jury-based evaluations aggregating decisions\nfrom multiple models. Although this approach both improves robustness and\nenhances alignment to human judgements, artifact sensitivity persists even with\nthe best jury configurations. These results highlight the urgent need for\ndiversified, artifact-resistant methodologies to ensure reliable safety\nassessments."
    },
    {
        "date": "2025-03",
        "title": "Stealthy Patch-Wise Backdoor Attack in 3D Point Cloud via Curvature Awareness",
        "author": "Yu Feng, Dingxin Zhang, Runkai Zhao, Yong Xia, Heng Huang, and Weidong Cai",
        "link": "http://arxiv.org/abs/2503.09336v1",
        "abstract": "Backdoor attacks pose a severe threat to deep neural networks (DNN) by\nimplanting hidden backdoors that can be activated with predefined triggers to\nmanipulate model behaviors maliciously. Existing 3D point cloud backdoor\nattacks primarily rely on sample-wise global modifications, resulting in\nsuboptimal stealthiness. To address this limitation, we propose Stealthy\nPatch-Wise Backdoor Attack (SPBA), which employs the first patch-wise trigger\nfor 3D point clouds and restricts perturbations to local regions, significantly\nenhancing stealthiness. Specifically, SPBA decomposes point clouds into local\npatches and evaluates their geometric complexity using a curvature-based patch\nimperceptibility score, ensuring that the trigger remains less perceptible to\nthe human eye by strategically applying it across multiple geometrically\ncomplex patches with lower visual sensitivity. By leveraging the Graph Fourier\nTransform (GFT), SPBA optimizes a patch-wise spectral trigger that perturbs the\nspectral features of selected patches, enhancing attack effectiveness while\npreserving the global geometric structure of the point cloud. Extensive\nexperiments on ModelNet40 and ShapeNetPart demonstrate that SPBA consistently\nachieves an attack success rate (ASR) exceeding 96.5% across different models\nwhile achieving state-of-the-art imperceptibility compared to existing backdoor\nattack methods."
    },
    {
        "date": "2025-03",
        "title": "CyberLLMInstruct: A New Dataset for Analysing Safety of Fine-Tuned LLMs Using Cyber Security Data",
        "author": "Adel ElZemity, Budi Arief, and Shujun Li",
        "link": "http://arxiv.org/abs/2503.09334v1",
        "abstract": "The integration of large language models (LLMs) into cyber security\napplications presents significant opportunities, such as enhancing threat\nanalysis and malware detection, but can also introduce critical risks and\nsafety concerns, including personal data leakage and automated generation of\nnew malware. To address these challenges, we developed CyberLLMInstruct, a\ndataset of 54,928 instruction-response pairs spanning cyber security tasks such\nas malware analysis, phishing simulations, and zero-day vulnerabilities. The\ndataset was constructed through a multi-stage process. This involved sourcing\ndata from multiple resources, filtering and structuring it into\ninstruction-response pairs, and aligning it with real-world scenarios to\nenhance its applicability. Seven open-source LLMs were chosen to test the\nusefulness of CyberLLMInstruct: Phi 3 Mini 3.8B, Mistral 7B, Qwen 2.5 7B, Llama\n3 8B, Llama 3.1 8B, Gemma 2 9B, and Llama 2 70B. In our primary example, we\nrigorously assess the safety of fine-tuned models using the OWASP top 10\nframework, finding that fine-tuning reduces safety resilience across all tested\nLLMs and every adversarial attack (e.g., the security score of Llama 3.1 8B\nagainst prompt injection drops from 0.95 to 0.15). In our second example, we\nshow that these same fine-tuned models can also achieve up to 92.50 percent\naccuracy on the CyberMetric benchmark. These findings highlight a trade-off\nbetween performance and safety, showing the importance of adversarial testing\nand further research into fine-tuning methodologies that can mitigate safety\nrisks while still improving performance across diverse datasets and domains.\nAll scripts required to reproduce the dataset, along with examples and relevant\nresources for replicating our results, will be made available upon the paper's\nacceptance."
    },
    {
        "date": "2025-03",
        "title": "Group-robust Machine Unlearning",
        "author": "Thomas De Min, Subhankar Roy, St\u00e9phane Lathuili\u00e8re, Elisa Ricci, and Massimiliano Mancini",
        "link": "http://arxiv.org/abs/2503.09330v1",
        "abstract": "Machine unlearning is an emerging paradigm to remove the influence of\nspecific training data (i.e., the forget set) from a model while preserving its\nknowledge of the rest of the data (i.e., the retain set). Previous approaches\nassume the forget data to be uniformly distributed from all training\ndatapoints. However, if the data to unlearn is dominant in one group, we\nempirically show that performance for this group degrades, leading to fairness\nissues. This work tackles the overlooked problem of non-uniformly distributed\nforget sets, which we call group-robust machine unlearning, by presenting a\nsimple, effective strategy that mitigates the performance loss in dominant\ngroups via sample distribution reweighting. Moreover, we present MIU (Mutual\nInformation-aware Machine Unlearning), the first approach for group robustness\nin approximate machine unlearning. MIU minimizes the mutual information between\nmodel features and group information, achieving unlearning while reducing\nperformance degradation in the dominant group of the forget set. Additionally,\nMIU exploits sample distribution reweighting and mutual information calibration\nwith the original model to preserve group robustness. We conduct experiments on\nthree datasets and show that MIU outperforms standard methods, achieving\nunlearning without compromising model robustness. Source code available at\nhttps://github.com/tdemin16/group-robust_machine_unlearning."
    },
    {
        "date": "2025-03",
        "title": "Towards Robust Model Evolution with Algorithmic Recourse",
        "author": "Hao-Tsung Yang, Jie Gao, Bo-Yi Liu, and Zhi-Xuan Liu",
        "link": "http://arxiv.org/abs/2503.09658v1",
        "abstract": "Algorithmic Recourse is a way for users to modify their attributes to align\nwith a model's expectations, thereby improving their outcomes after receiving\nunfavorable decisions. In real-world scenarios, users often need to\nstrategically adjust their attributes to compete for limited resources.\nHowever, such strategic behavior induces users to \"game\" algorithms, causing\nmodel collapse due to distribution shifts. These shifts arise from user\ncompetition, resource constraints, and adaptive user responses. While prior\nresearch on Algorithmic Recourse has explored its effects on both systems and\nusers, the impact of resource constraints and competition over time remains\nunderexplored. In this work, we develop a general framework to model user\nstrategic behaviors and their interactions with decision-making systems under\nresource constraints and competitive dynamics. Through theoretical analysis and\nempirical evaluation, we identify three key phenomena that arise consistently\nin both synthetic and real-world datasets: escalating decision boundaries,\nnon-robust model predictions, and inequitable recourse actions. Finally, we\ndiscuss the broader social implications of these findings and present two\nalgorithmic strategies aimed at mitigating these challenges."
    },
    {
        "date": "2025-03",
        "title": "Detecting and Preventing Data Poisoning Attacks on AI Models",
        "author": "Halima I. Kure, Pradipta Sarkar, Ahmed B. Ndanusa, and Augustine O. Nwajana",
        "link": "http://arxiv.org/abs/2503.09302v1",
        "abstract": "This paper investigates the critical issue of data poisoning attacks on AI\nmodels, a growing concern in the ever-evolving landscape of artificial\nintelligence and cybersecurity. As advanced technology systems become\nincreasingly prevalent across various sectors, the need for robust defence\nmechanisms against adversarial attacks becomes paramount. The study aims to\ndevelop and evaluate novel techniques for detecting and preventing data\npoisoning attacks, focusing on both theoretical frameworks and practical\napplications. Through a comprehensive literature review, experimental\nvalidation using the CIFAR-10 and Insurance Claims datasets, and the\ndevelopment of innovative algorithms, this paper seeks to enhance the\nresilience of AI models against malicious data manipulation. The study explores\nvarious methods, including anomaly detection, robust optimization strategies,\nand ensemble learning, to identify and mitigate the effects of poisoned data\nduring model training. Experimental results indicate that data poisoning\nsignificantly degrades model performance, reducing classification accuracy by\nup to 27% in image recognition tasks (CIFAR-10) and 22% in fraud detection\nmodels (Insurance Claims dataset). The proposed defence mechanisms, including\nstatistical anomaly detection and adversarial training, successfully mitigated\npoisoning effects, improving model robustness and restoring accuracy levels by\nan average of 15-20%. The findings further demonstrate that ensemble learning\ntechniques provide an additional layer of resilience, reducing false positives\nand false negatives caused by adversarial data injections."
    },
    {
        "date": "2025-03",
        "title": "Prompt Inference Attack on Distributed Large Language Model Inference Frameworks",
        "author": "Xinjian Luo, Ting Yu, and Xiaokui Xiao",
        "link": "http://arxiv.org/abs/2503.09291v1",
        "abstract": "The inference process of modern large language models (LLMs) demands\nprohibitive computational resources, rendering them infeasible for deployment\non consumer-grade devices. To address this limitation, recent studies propose\ndistributed LLM inference frameworks, which employ split learning principles to\nenable collaborative LLM inference on resource-constrained hardware. However,\ndistributing LLM layers across participants requires the transmission of\nintermediate outputs, which may introduce privacy risks to the original input\nprompts - a critical issue that has yet to be thoroughly explored in the\nliterature.\n  In this paper, we rigorously examine the privacy vulnerabilities of\ndistributed LLM inference frameworks by designing and evaluating three prompt\ninference attacks aimed at reconstructing input prompts from intermediate LLM\noutputs. These attacks are developed under various query and data constraints\nto reflect diverse real-world LLM service scenarios. Specifically, the first\nattack assumes an unlimited query budget and access to an auxiliary dataset\nsharing the same distribution as the target prompts. The second attack also\nleverages unlimited queries but uses an auxiliary dataset with a distribution\ndiffering from the target prompts. The third attack operates under the most\nrestrictive scenario, with limited query budgets and no auxiliary dataset\navailable. We evaluate these attacks on a range of LLMs, including\nstate-of-the-art models such as Llama-3.2 and Phi-3.5, as well as widely-used\nmodels like GPT-2 and BERT for comparative analysis. Our experiments show that\nthe first two attacks achieve reconstruction accuracies exceeding 90%, while\nthe third achieves accuracies typically above 50%, even under stringent\nconstraints. These findings highlight privacy risks in distributed LLM\ninference frameworks, issuing a strong alert on their deployment in real-world\napplications."
    },
    {
        "date": "2025-03",
        "title": "In-Context Defense in Computer Agents: An Empirical Study",
        "author": "Pei Yang, Hai Ci, and Mike Zheng Shou",
        "link": "http://arxiv.org/abs/2503.09241v1",
        "abstract": "Computer agents powered by vision-language models (VLMs) have significantly\nadvanced human-computer interaction, enabling users to perform complex tasks\nthrough natural language instructions. However, these agents are vulnerable to\ncontext deception attacks, an emerging threat where adversaries embed\nmisleading content into the agent's operational environment, such as a pop-up\nwindow containing deceptive instructions. Existing defenses, such as\ninstructing agents to ignore deceptive elements, have proven largely\nineffective. As the first systematic study on protecting computer agents, we\nintroduce textbf{in-context defense}, leveraging in-context learning and\nchain-of-thought (CoT) reasoning to counter such attacks. Our approach involves\naugmenting the agent's context with a small set of carefully curated exemplars\ncontaining both malicious environments and corresponding defensive responses.\nThese exemplars guide the agent to first perform explicit defensive reasoning\nbefore action planning, reducing susceptibility to deceptive attacks.\nExperiments demonstrate the effectiveness of our method, reducing attack\nsuccess rates by 91.2% on pop-up window attacks, 74.6% on average on\nenvironment injection attacks, while achieving 100% successful defenses against\ndistracting advertisements. Our findings highlight that (1) defensive reasoning\nmust precede action planning for optimal performance, and (2) a minimal number\nof exemplars (fewer than three) is sufficient to induce an agent's defensive\nbehavior."
    },
    {
        "date": "2025-03",
        "title": "Robust Asymmetric Heterogeneous Federated Learning with Corrupted Clients",
        "author": "Xiuwen Fang, Mang Ye, and Bo Du",
        "link": "http://arxiv.org/abs/2503.09206v1",
        "abstract": "This paper studies a challenging robust federated learning task with model\nheterogeneous and data corrupted clients, where the clients have different\nlocal model structures. Data corruption is unavoidable due to factors such as\nrandom noise, compression artifacts, or environmental conditions in real-world\ndeployment, drastically crippling the entire federated system. To address these\nissues, this paper introduces a novel Robust Asymmetric Heterogeneous Federated\nLearning (RAHFL) framework. We propose a Diversity-enhanced supervised\nContrastive Learning technique to enhance the resilience and adaptability of\nlocal models on various data corruption patterns. Its basic idea is to utilize\ncomplex augmented samples obtained by the mixed-data augmentation strategy for\nsupervised contrastive learning, thereby enhancing the ability of the model to\nlearn robust and diverse feature representations. Furthermore, we design an\nAsymmetric Heterogeneous Federated Learning strategy to resist corrupt feedback\nfrom external clients. The strategy allows clients to perform selective one-way\nlearning during collaborative learning phase, enabling clients to refrain from\nincorporating lower-quality information from less robust or underperforming\ncollaborators. Extensive experimental results demonstrate the effectiveness and\nrobustness of our approach in diverse, challenging federated learning\nenvironments. Our code and models are public available at\nhttps://github.com/FangXiuwen/RAHFL."
    },
    {
        "date": "2025-03",
        "title": "AdvAD: Exploring Non-Parametric Diffusion for Imperceptible Adversarial Attacks",
        "author": "Jin Li, Ziqiang He, Anwei Luo, Jian-Fang Hu, Z. Jane Wang, and Xiangui Kang",
        "link": "http://arxiv.org/abs/2503.09124v1",
        "abstract": "Imperceptible adversarial attacks aim to fool DNNs by adding imperceptible\nperturbation to the input data. Previous methods typically improve the\nimperceptibility of attacks by integrating common attack paradigms with\nspecifically designed perception-based losses or the capabilities of generative\nmodels. In this paper, we propose Adversarial Attacks in Diffusion (AdvAD), a\nnovel modeling framework distinct from existing attack paradigms. AdvAD\ninnovatively conceptualizes attacking as a non-parametric diffusion process by\ntheoretically exploring basic modeling approach rather than using the denoising\nor generation abilities of regular diffusion models requiring neural networks.\nAt each step, much subtler yet effective adversarial guidance is crafted using\nonly the attacked model without any additional network, which gradually leads\nthe end of diffusion process from the original image to a desired imperceptible\nadversarial example. Grounded in a solid theoretical foundation of the proposed\nnon-parametric diffusion process, AdvAD achieves high attack efficacy and\nimperceptibility with intrinsically lower overall perturbation strength.\nAdditionally, an enhanced version AdvAD-X is proposed to evaluate the extreme\nof our novel framework under an ideal scenario. Extensive experiments\ndemonstrate the effectiveness of the proposed AdvAD and AdvAD-X. Compared with\nstate-of-the-art imperceptible attacks, AdvAD achieves an average of 99.9$\\%$\n(+17.3$\\%$) ASR with 1.34 (-0.97) $l_2$ distance, 49.74 (+4.76) PSNR and 0.9971\n(+0.0043) SSIM against four prevalent DNNs with three different architectures\non the ImageNet-compatible dataset. Code is available at\nhttps://github.com/XianguiKang/AdvAD."
    },
    {
        "date": "2025-03",
        "title": "C^2 ATTACK: Towards Representation Backdoor on CLIP via Concept Confusion",
        "author": "Lijie Hu, Junchi Liao, Weimin Lyu, Shaopeng Fu, Tianhao Huang, Shu Yang, Guimin Hu, and Di Wang",
        "link": "http://arxiv.org/abs/2503.09095v1",
        "abstract": "Backdoor attacks pose a significant threat to deep learning models, enabling\nadversaries to embed hidden triggers that manipulate the behavior of the model\nduring inference. Traditional backdoor attacks typically rely on inserting\nexplicit triggers (e.g., external patches, or perturbations) into input data,\nbut they often struggle to evade existing defense mechanisms. To address this\nlimitation, we investigate backdoor attacks through the lens of the reasoning\nprocess in deep learning systems, drawing insights from interpretable AI. We\nconceptualize backdoor activation as the manipulation of learned concepts\nwithin the model's latent representations. Thus, existing attacks can be seen\nas implicit manipulations of these activated concepts during inference. This\nraises interesting questions: why not manipulate the concepts explicitly? This\nidea leads to our novel backdoor attack framework, Concept Confusion Attack\n(C^2 ATTACK), which leverages internal concepts in the model's reasoning as\n\"triggers\" without introducing explicit external modifications. By avoiding the\nuse of real triggers and directly activating or deactivating specific concepts\nin latent spaces, our approach enhances stealth, making detection by existing\ndefenses significantly harder. Using CLIP as a case study, experimental results\ndemonstrate the effectiveness of C^2 ATTACK, achieving high attack success\nrates while maintaining robustness against advanced defenses."
    },
    {
        "date": "2025-03",
        "title": "Probing Latent Subspaces in LLM for AI Security: Identifying and Manipulating Adversarial States",
        "author": "Xin Wei Chia, and Jonathan Pan",
        "link": "http://arxiv.org/abs/2503.09066v1",
        "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nvarious tasks, yet they remain vulnerable to adversarial manipulations such as\njailbreaking via prompt injection attacks. These attacks bypass safety\nmechanisms to generate restricted or harmful content. In this study, we\ninvestigated the underlying latent subspaces of safe and jailbroken states by\nextracting hidden activations from a LLM. Inspired by attractor dynamics in\nneuroscience, we hypothesized that LLM activations settle into semi stable\nstates that can be identified and perturbed to induce state transitions. Using\ndimensionality reduction techniques, we projected activations from safe and\njailbroken responses to reveal latent subspaces in lower dimensional spaces. We\nthen derived a perturbation vector that when applied to safe representations,\nshifted the model towards a jailbreak state. Our results demonstrate that this\ncausal intervention results in statistically significant jailbreak responses in\na subset of prompts. Next, we probed how these perturbations propagate through\nthe model's layers, testing whether the induced state change remains localized\nor cascades throughout the network. Our findings indicate that targeted\nperturbations induced distinct shifts in activations and model responses. Our\napproach paves the way for potential proactive defenses, shifting from\ntraditional guardrail based methods to preemptive, model agnostic techniques\nthat neutralize adversarial states at the representation level."
    },
    {
        "date": "2025-03",
        "title": "Mono2D: A Trainable Monogenic Layer for Robust Knee Cartilage Segmentation on Out-of-Distribution 2D Ultrasound Data",
        "author": "Alvin Kimbowa, Arjun Parmar, Maziar Badii, David Liu, Matthew Harkey, and Ilker Hacihaliloglu",
        "link": "http://arxiv.org/abs/2503.09050v2",
        "abstract": "Automated knee cartilage segmentation using point-of-care ultrasound devices\nand deep-learning networks has the potential to enhance the management of knee\nosteoarthritis. However, segmentation algorithms often struggle with domain\nshifts caused by variations in ultrasound devices and acquisition parameters,\nlimiting their generalizability. In this paper, we propose Mono2D, a monogenic\nlayer that extracts multi-scale, contrast- and intensity-invariant local phase\nfeatures using trainable bandpass quadrature filters. This layer mitigates\ndomain shifts, improving generalization to out-of-distribution domains. Mono2D\nis integrated before the first layer of a segmentation network, and its\nparameters jointly trained alongside the network's parameters. We evaluated\nMono2D on a multi-domain 2D ultrasound knee cartilage dataset for single-source\ndomain generalization (SSDG). Our results demonstrate that Mono2D outperforms\nother SSDG methods in terms of Dice score and mean average surface distance. To\nfurther assess its generalizability, we evaluate Mono2D on a multi-site\nprostate MRI dataset, where it continues to outperform other SSDG methods,\nhighlighting its potential to improve domain generalization in medical imaging.\nNevertheless, further evaluation on diverse datasets is still necessary to\nassess its clinical utility."
    },
    {
        "date": "2025-03",
        "title": "Adaptive Backdoor Attacks with Reasonable Constraints on Graph Neural Networks",
        "author": "Xuewen Dong, Jiachen Li, Shujun Li, Zhichao You, Qiang Qu, Yaroslav Kholodov, and Yulong Shen",
        "link": "http://arxiv.org/abs/2503.09049v1",
        "abstract": "Recent studies show that graph neural networks (GNNs) are vulnerable to\nbackdoor attacks. Existing backdoor attacks against GNNs use fixed-pattern\ntriggers and lack reasonable trigger constraints, overlooking individual graph\ncharacteristics and rendering insufficient evasiveness. To tackle the above\nissues, we propose ABARC, the first Adaptive Backdoor Attack with Reasonable\nConstraints, applying to both graph-level and node-level tasks in GNNs. For\ngraph-level tasks, we propose a subgraph backdoor attack independent of the\ngraph's topology. It dynamically selects trigger nodes for each target graph\nand modifies node features with constraints based on graph similarity, feature\nrange, and feature type. For node-level tasks, our attack begins with an\nanalysis of node features, followed by selecting and modifying trigger\nfeatures, which are then constrained by node similarity, feature range, and\nfeature type. Furthermore, an adaptive edge-pruning mechanism is designed to\nreduce the impact of neighbors on target nodes, ensuring a high attack success\nrate (ASR). Experimental results show that even with reasonable constraints for\nattack evasiveness, our attack achieves a high ASR while incurring a marginal\nclean accuracy drop (CAD). When combined with the state-of-the-art defense\nrandomized smoothing (RS) method, our attack maintains an ASR over 94%,\nsurpassing existing attacks by more than 7%."
    },
    {
        "date": "2025-03",
        "title": "Prompt Inversion Attack against Collaborative Inference of Large Language Models",
        "author": "Wenjie Qu, Yuguang Zhou, Yongji Wu, Tingsong Xiao, Binhang Yuan, Yiming Li, and Jiaheng Zhang",
        "link": "http://arxiv.org/abs/2503.09022v2",
        "abstract": "Large language models (LLMs) have been widely applied for their remarkable\ncapability of content generation. However, the practical use of open-source\nLLMs is hindered by high resource requirements, making deployment expensive and\nlimiting widespread development. The collaborative inference is a promising\nsolution for this problem, in which users collaborate by each hosting a subset\nof layers and transmitting intermediate activation. Many companies are building\ncollaborative inference platforms to reduce LLM serving costs, leveraging\nusers' underutilized GPUs. Despite widespread interest in collaborative\ninference within academia and industry, the privacy risks associated with LLM\ncollaborative inference have not been well studied. This is largely because of\nthe challenge posed by inverting LLM activation due to its strong\nnon-linearity.\n  In this paper, to validate the severity of privacy threats in LLM\ncollaborative inference, we introduce the concept of prompt inversion attack\n(PIA), where a malicious participant intends to recover the input prompt\nthrough the activation transmitted by its previous participant. Extensive\nexperiments show that our PIA method substantially outperforms existing\nbaselines. For example, our method achieves an 88.4\\% token accuracy on the\nSkytrax dataset with the Llama-65B model when inverting the maximum number of\ntransformer layers, while the best baseline method only achieves 22.8\\%\naccuracy. The results verify the effectiveness of our PIA attack and highlights\nits practical threat to LLM collaborative inference systems."
    },
    {
        "date": "2025-03",
        "title": "Battling Misinformation: An Empirical Study on Adversarial Factuality in Open-Source Large Language Models",
        "author": "Shahnewaz Karim Sakib, Anindya Bijoy Das, and Shibbir Ahmed",
        "link": "http://arxiv.org/abs/2503.10690v1",
        "abstract": "Adversarial factuality refers to the deliberate insertion of misinformation\ninto input prompts by an adversary, characterized by varying levels of\nexpressed confidence. In this study, we systematically evaluate the performance\nof several open-source large language models (LLMs) when exposed to such\nadversarial inputs. Three tiers of adversarial confidence are considered:\nstrongly confident, moderately confident, and limited confidence. Our analysis\nencompasses eight LLMs: LLaMA 3.1 (8B), Phi 3 (3.8B), Qwen 2.5 (7B),\nDeepseek-v2 (16B), Gemma2 (9B), Falcon (7B), Mistrallite (7B), and LLaVA (7B).\nEmpirical results indicate that LLaMA 3.1 (8B) exhibits a robust capability in\ndetecting adversarial inputs, whereas Falcon (7B) shows comparatively lower\nperformance. Notably, for the majority of the models, detection success\nimproves as the adversary's confidence decreases; however, this trend is\nreversed for LLaMA 3.1 (8B) and Phi 3 (3.8B), where a reduction in adversarial\nconfidence corresponds with diminished detection performance. Further analysis\nof the queries that elicited the highest and lowest rates of successful attacks\nreveals that adversarial attacks are more effective when targeting less\ncommonly referenced or obscure information."
    },
    {
        "date": "2025-03",
        "title": "Not All Edges are Equally Robust: Evaluating the Robustness of Ranking-Based Federated Learning",
        "author": "Zirui Gong, Yanjun Zhang, Leo Yu Zhang, Zhaoxi Zhang, Yong Xiang, and Shirui Pan",
        "link": "http://arxiv.org/abs/2503.08976v1",
        "abstract": "Federated Ranking Learning (FRL) is a state-of-the-art FL framework that\nstands out for its communication efficiency and resilience to poisoning\nattacks. It diverges from the traditional FL framework in two ways: 1) it\nleverages discrete rankings instead of gradient updates, significantly reducing\ncommunication costs and limiting the potential space for malicious updates, and\n2) it uses majority voting on the server side to establish the global ranking,\nensuring that individual updates have minimal influence since each client\ncontributes only a single vote. These features enhance the system's scalability\nand position FRL as a promising paradigm for FL training.\n  However, our analysis reveals that FRL is not inherently robust, as certain\nedges are particularly vulnerable to poisoning attacks. Through a theoretical\ninvestigation, we prove the existence of these vulnerable edges and establish a\nlower bound and an upper bound for identifying them in each layer. Based on\nthis finding, we introduce a novel local model poisoning attack against FRL,\nnamely the Vulnerable Edge Manipulation (VEM) attack. The VEM attack focuses on\nidentifying and perturbing the most vulnerable edges in each layer and\nleveraging an optimization-based approach to maximize the attack's impact.\nThrough extensive experiments on benchmark datasets, we demonstrate that our\nattack achieves an overall 53.23% attack impact and is 3.7x more impactful than\nexisting methods. Our findings highlight significant vulnerabilities in\nranking-based FL systems and underline the urgency for the development of new\nrobust FL frameworks."
    },
    {
        "date": "2025-03",
        "title": "Quantitative Analysis of Deeply Quantized Tiny Neural Networks Robust to Adversarial Attacks",
        "author": "Idris Zakariyya, Ferheen Ayaz, Mounia Kharbouche-Harrari, Jeremy Singer, Sye Loong Keoh, Danilo Pau, and Jos\u00e9 Cano",
        "link": "http://arxiv.org/abs/2503.08973v1",
        "abstract": "Reducing the memory footprint of Machine Learning (ML) models, especially\nDeep Neural Networks (DNNs), is imperative to facilitate their deployment on\nresource-constrained edge devices. However, a notable drawback of DNN models\nlies in their susceptibility to adversarial attacks, wherein minor input\nperturbations can deceive them. A primary challenge revolves around the\ndevelopment of accurate, resilient, and compact DNN models suitable for\ndeployment on resource-constrained edge devices. This paper presents the\noutcomes of a compact DNN model that exhibits resilience against both black-box\nand white-box adversarial attacks. This work has achieved this resilience\nthrough training with the QKeras quantization-aware training framework. The\nstudy explores the potential of QKeras and an adversarial robustness technique,\nJacobian Regularization (JR), to co-optimize the DNN architecture through\nper-layer JR methodology. As a result, this paper has devised a DNN model\nemploying this co-optimization strategy based on Stochastic Ternary\nQuantization (STQ). Its performance was compared against existing DNN models in\nthe face of various white-box and black-box attacks. The experimental findings\nrevealed that, the proposed DNN model had small footprint and on average, it\nexhibited better performance than Quanos and DS-CNN MLCommons/TinyML (MLC/T)\nbenchmarks when challenged with white-box and black-box attacks, respectively,\non the CIFAR-10 image and Google Speech Commands audio datasets."
    },
    {
        "date": "2025-03",
        "title": "Leaky Batteries: A Novel Set of Side-Channel Attacks on Electric Vehicles",
        "author": "Francesco Marchiori, and Mauro Conti",
        "link": "http://arxiv.org/abs/2503.08956v1",
        "abstract": "Advancements in battery technology have accelerated the adoption of Electric\nVehicles (EVs) due to their environmental benefits. However, their growing\nsophistication introduces security and privacy challenges. Often seen as mere\noperational data, battery consumption patterns can unintentionally reveal\ncritical information exploitable for malicious purposes. These risks go beyond\nprivacy, impacting vehicle security and regulatory compliance. Despite these\nconcerns, current research has largely overlooked the broader implications of\nbattery consumption data exposure. As EVs integrate further into smart\ntransportation networks, addressing these gaps is crucial to ensure their\nsafety, reliability, and resilience. In this work, we introduce a novel class\nof side-channel attacks that exploit EV battery data to extract sensitive user\ninformation. Leveraging only battery consumption patterns, we demonstrate a\nmethodology to accurately identify the EV driver and their driving style,\ndetermine the number of occupants, and infer the vehicle's start and end\nlocations when user habits are known. We utilize several machine learning\nmodels and feature extraction techniques to analyze EV power consumption\npatterns, validating our approach on simulated and real-world datasets\ncollected from actual drivers. Our attacks achieve an average success rate of\n95.4% across all attack objectives. Our findings highlight the privacy risks\nassociated with EV battery data, emphasizing the need for stronger protections\nto safeguard user privacy and vehicle security."
    },
    {
        "date": "2025-03",
        "title": "Robust Unsupervised Fault Diagnosis For High-Dimensional Nonlinear Noisy Data",
        "author": "Dandan Zhao, Hongpeng Yin, Jintang Bian, and Han Zhou",
        "link": "http://arxiv.org/abs/2503.08916v1",
        "abstract": "Traditional fault diagnosis methods struggle to handle fault data, with\ncomplex data characteristics such as high dimensions and large noise. Deep\nlearning is a promising solution, which typically works well only when labeled\nfault data are available. To address these problems, a robust unsupervised\nfault diagnosis using machine learning is proposed in this paper. First, a\nspecial dimension reduction method for the high-dimensional fault data is\ndesigned. Second, the extracted features are enhanced by incorporating\nnonlinear information through the learning of a graph structure. Third, to\nalleviate the problem of reduced fault-diagnosis accuracy attributed to noise\nand outliers, $l_{2,1}$-norm and typicality-aware constraints are introduced\nfrom the perspective of model optimization, respectively. Finally, this paper\nprovides comprehensive theoretical and experimental evidence supporting the\neffectiveness and robustness of the proposed method. The experiments on both\nthe benchmark Tennessee-Eastman process and a real hot-steel milling process\nshow that the proposed method exhibits better robustness compared to other\nmethods, maintaining high diagnostic accuracy even in the presence of outliers\nor noise."
    },
    {
        "date": "2025-03",
        "title": "A Deep Bayesian Nonparametric Framework for Robust Mutual Information Estimation",
        "author": "Forough Fazeliasl, Michael Minyi Zhang, Bei Jiang, and Linglong Kong",
        "link": "http://arxiv.org/abs/2503.08902v1",
        "abstract": "Mutual Information (MI) is a crucial measure for capturing dependencies\nbetween variables, but exact computation is challenging in high dimensions with\nintractable likelihoods, impacting accuracy and robustness. One idea is to use\nan auxiliary neural network to train an MI estimator; however, methods based on\nthe empirical distribution function (EDF) can introduce sharp fluctuations in\nthe MI loss due to poor out-of-sample performance, destabilizing convergence.\nWe present a Bayesian nonparametric (BNP) solution for training an MI estimator\nby constructing the MI loss with a finite representation of the Dirichlet\nprocess posterior to incorporate regularization in the training process. With\nthis regularization, the MI loss integrates both prior knowledge and empirical\ndata to reduce the loss sensitivity to fluctuations and outliers in the sample\ndata, especially in small sample settings like mini-batches. This approach\naddresses the challenge of balancing accuracy and low variance by effectively\nreducing variance, leading to stabilized and robust MI loss gradients during\ntraining and enhancing the convergence of the MI approximation while offering\nstronger theoretical guarantees for convergence. We explore the application of\nour estimator in maximizing MI between the data space and the latent space of a\nvariational autoencoder. Experimental results demonstrate significant\nimprovements in convergence over EDF-based methods, with applications across\nsynthetic and real datasets, notably in 3D CT image generation, yielding\nenhanced structure discovery and reduced overfitting in data synthesis. While\nthis paper focuses on generative models in application, the proposed estimator\nis not restricted to this setting and can be applied more broadly in various\nBNP learning procedures."
    },
    {
        "date": "2025-03",
        "title": "Seal Your Backdoor with Variational Defense",
        "author": "Ivan Saboli\u0107, Matej Grci\u0107, and Sini\u0161a \u0160egvi\u0107",
        "link": "http://arxiv.org/abs/2503.08829v1",
        "abstract": "We propose VIBE, a model-agnostic framework that trains classifiers resilient\nto backdoor attacks. The key concept behind our approach is to treat malicious\ninputs and corrupted labels from the training dataset as observed random\nvariables, while the actual clean labels are latent. VIBE then recovers the\ncorresponding latent clean label posterior through variational inference. The\nresulting training procedure follows the expectation-maximization (EM)\nalgorithm. The E-step infers the clean pseudolabels by solving an\nentropy-regularized optimal transport problem, while the M-step updates the\nclassifier parameters via gradient descent. Being modular, VIBE can seamlessly\nintegrate with recent advancements in self-supervised representation learning,\nwhich enhance its ability to resist backdoor attacks. We experimentally\nvalidate the method effectiveness against contemporary backdoor attacks on\nstandard datasets, a large-scale setup with 1$k$ classes, and a dataset\npoisoned with multiple attacks. VIBE consistently outperforms previous defenses\nacross all tested scenarios."
    },
    {
        "date": "2025-03",
        "title": "Robust Multi-Objective Controlled Decoding of Large Language Models",
        "author": "Seongho Son, William Bankes, Sangwoong Yoon, Shyam Sundhar Ramesh, Xiaohang Tang, and Ilija Bogunovic",
        "link": "http://arxiv.org/abs/2503.08796v1",
        "abstract": "Test-time alignment of Large Language Models (LLMs) to human preferences\noffers a flexible way to generate responses aligned to diverse objectives\nwithout extensive retraining of LLMs. Existing methods achieve alignment to\nmultiple objectives simultaneously (e.g., instruction-following, helpfulness,\nconciseness) by optimizing their corresponding reward functions. However, they\noften rely on predefined weights or optimize for averages, sacrificing one\nobjective for another and leading to unbalanced outcomes. To address this, we\nintroduce Robust Multi-Objective Decoding (RMOD), a novel inference-time\nalgorithm that optimizes for improving worst-case rewards. RMOD formalizes the\nrobust decoding problem as a maximin two-player game between reward weights and\nthe sampling policy, solving for the Nash equilibrium. We show that the game\nreduces to a convex optimization problem to find the worst-case weights, while\nthe best response policy can be computed analytically. We also introduce a\npractical RMOD variant designed for efficient decoding with contemporary LLMs,\nincurring minimal computational overhead compared to non-robust Multi-Objective\nDecoding (MOD) methods. Our experimental results showcase the effectiveness of\nRMOD in generating responses equitably aligned with diverse objectives,\noutperforming baselines up to 20%."
    },
    {
        "date": "2025-03",
        "title": "Birds look like cars: Adversarial analysis of intrinsically interpretable deep learning",
        "author": "Hubert Baniecki, and Przemyslaw Biecek",
        "link": "http://arxiv.org/abs/2503.08636v1",
        "abstract": "A common belief is that intrinsically interpretable deep learning models\nensure a correct, intuitive understanding of their behavior and offer greater\nrobustness against accidental errors or intentional manipulation. However,\nthese beliefs have not been comprehensively verified, and growing evidence\ncasts doubt on them. In this paper, we highlight the risks related to\noverreliance and susceptibility to adversarial manipulation of these so-called\n\"intrinsically (aka inherently) interpretable\" models by design. We introduce\ntwo strategies for adversarial analysis with prototype manipulation and\nbackdoor attacks against prototype-based networks, and discuss how concept\nbottleneck models defend against these attacks. Fooling the model's reasoning\nby exploiting its use of latent prototypes manifests the inherent\nuninterpretability of deep neural networks, leading to a false sense of\nsecurity reinforced by a visual confirmation bias. The reported limitations of\nprototype-based networks put their trustworthiness and applicability into\nquestion, motivating further work on the robustness and alignment of (deep)\ninterpretable models."
    },
    {
        "date": "2025-03",
        "title": "Soft Actor-Critic-based Control Barrier Adaptation for Robust Autonomous Navigation in Unknown Environments",
        "author": "Nicholas Mohammad, and Nicola Bezzo",
        "link": "http://arxiv.org/abs/2503.08479v1",
        "abstract": "Motion planning failures during autonomous navigation often occur when safety\nconstraints are either too conservative, leading to deadlocks, or too liberal,\nresulting in collisions. To improve robustness, a robot must dynamically adapt\nits safety constraints to ensure it reaches its goal while balancing safety and\nperformance measures. To this end, we propose a Soft Actor-Critic (SAC)-based\npolicy for adapting Control Barrier Function (CBF) constraint parameters at\nruntime, ensuring safe yet non-conservative motion. The proposed approach is\ndesigned for a general high-level motion planner, low-level controller, and\ntarget system model, and is trained in simulation only. Through extensive\nsimulations and physical experiments, we demonstrate that our framework\neffectively adapts CBF constraints, enabling the robot to reach its final goal\nwithout compromising safety."
    },
    {
        "date": "2025-03",
        "title": "Robust Latent Matters: Boosting Image Generation with Sampling Error Synthesis",
        "author": "Kai Qiu, Xiang Li, Jason Kuen, Hao Chen, Xiaohao Xu, Jiuxiang Gu, Yinyi Luo, Bhiksha Raj, Zhe Lin, and Marios Savvides",
        "link": "http://arxiv.org/abs/2503.08354v2",
        "abstract": "Recent image generation schemes typically capture image distribution in a\npre-constructed latent space relying on a frozen image tokenizer. Though the\nperformance of tokenizer plays an essential role to the successful generation,\nits current evaluation metrics (e.g. rFID) fail to precisely assess the\ntokenizer and correlate its performance to the generation quality (e.g. gFID).\nIn this paper, we comprehensively analyze the reason for the discrepancy of\nreconstruction and generation qualities in a discrete latent space, and, from\nwhich, we propose a novel plug-and-play tokenizer training scheme to facilitate\nlatent space construction. Specifically, a latent perturbation approach is\nproposed to simulate sampling noises, i.e., the unexpected tokens sampled, from\nthe generative process. With the latent perturbation, we further propose (1) a\nnovel tokenizer evaluation metric, i.e., pFID, which successfully correlates\nthe tokenizer performance to generation quality and (2) a plug-and-play\ntokenizer training scheme, which significantly enhances the robustness of\ntokenizer thus boosting the generation quality and convergence speed. Extensive\nbenchmarking are conducted with 11 advanced discrete image tokenizers with 2\nautoregressive generation models to validate our approach. The tokenizer\ntrained with our proposed latent perturbation achieve a notable 1.60 gFID with\nclassifier-free guidance (CFG) and 3.45 gFID without CFG with a $\\sim$400M\ngenerator. Code: https://github.com/lxa9867/ImageFolder."
    },
    {
        "date": "2025-03",
        "title": "Adv-CPG: A Customized Portrait Generation Framework with Facial Adversarial Attacks",
        "author": "Junying Wang, Hongyuan Zhang, and Yuan Yuan",
        "link": "http://arxiv.org/abs/2503.08269v1",
        "abstract": "Recent Customized Portrait Generation (CPG) methods, taking a facial image\nand a textual prompt as inputs, have attracted substantial attention. Although\nthese methods generate high-fidelity portraits, they fail to prevent the\ngenerated portraits from being tracked and misused by malicious face\nrecognition systems. To address this, this paper proposes a Customized Portrait\nGeneration framework with facial Adversarial attacks (Adv-CPG). Specifically,\nto achieve facial privacy protection, we devise a lightweight local ID\nencryptor and an encryption enhancer. They implement progressive double-layer\nencryption protection by directly injecting the target identity and adding\nadditional identity guidance, respectively. Furthermore, to accomplish\nfine-grained and personalized portrait generation, we develop a multi-modal\nimage customizer capable of generating controlled fine-grained facial features.\nTo the best of our knowledge, Adv-CPG is the first study that introduces facial\nadversarial attacks into CPG. Extensive experiments demonstrate the superiority\nof Adv-CPG, e.g., the average attack success rate of the proposed Adv-CPG is\n28.1% and 2.86% higher compared to the SOTA noise-based attack methods and\nunconstrained attack methods, respectively."
    },
    {
        "date": "2025-03",
        "title": "SARA: Structural and Adversarial Representation Alignment for Training-efficient Diffusion Models",
        "author": "Hesen Chen, Junyan Wang, Zhiyu Tan, and Hao Li",
        "link": "http://arxiv.org/abs/2503.08253v1",
        "abstract": "Modern diffusion models encounter a fundamental trade-off between training\nefficiency and generation quality. While existing representation alignment\nmethods, such as REPA, accelerate convergence through patch-wise alignment,\nthey often fail to capture structural relationships within visual\nrepresentations and ensure global distribution consistency between pretrained\nencoders and denoising networks. To address these limitations, we introduce\nSARA, a hierarchical alignment framework that enforces multi-level\nrepresentation constraints: (1) patch-wise alignment to preserve local semantic\ndetails, (2) autocorrelation matrix alignment to maintain structural\nconsistency within representations, and (3) adversarial distribution alignment\nto mitigate global representation discrepancies. Unlike previous approaches,\nSARA explicitly models both intra-representation correlations via\nself-similarity matrices and inter-distribution coherence via adversarial\nalignment, enabling comprehensive alignment across local and global scales.\nExperiments on ImageNet-256 show that SARA achieves an FID of 1.36 while\nconverging twice as fast as REPA, surpassing recent state-of-the-art image\ngeneration methods. This work establishes a systematic paradigm for optimizing\ndiffusion training through hierarchical representation alignment."
    },
    {
        "date": "2025-03",
        "title": "A Grey-box Text Attack Framework using Explainable AI",
        "author": "Esther Chiramal, and Kelvin Soh Boon Kai",
        "link": "http://arxiv.org/abs/2503.08226v1",
        "abstract": "Explainable AI is a strong strategy implemented to understand complex\nblack-box model predictions in a human interpretable language. It provides the\nevidence required to execute the use of trustworthy and reliable AI systems. On\nthe other hand, however, it also opens the door to locating possible\nvulnerabilities in an AI model. Traditional adversarial text attack uses word\nsubstitution, data augmentation techniques and gradient-based attacks on\npowerful pre-trained Bidirectional Encoder Representations from Transformers\n(BERT) variants to generate adversarial sentences. These attacks are generally\nwhitebox in nature and not practical as they can be easily detected by humans\nE.g. Changing the word from \"Poor\" to \"Rich\". We proposed a simple yet\neffective Grey-box cum Black-box approach that does not require the knowledge\nof the model while using a set of surrogate Transformer/BERT models to perform\nthe attack using Explainable AI techniques. As Transformers are the current\nstate-of-the-art models for almost all Natural Language Processing (NLP) tasks,\nan attack generated from BERT1 is transferable to BERT2. This transferability\nis made possible due to the attention mechanism in the transformer that allows\nthe model to capture long-range dependencies in a sequence. Using the power of\nBERT generalisation via attention, we attempt to exploit how transformers learn\nby attacking a few surrogate transformer variants which are all based on a\ndifferent architecture. We demonstrate that this approach is highly effective\nto generate semantically good sentences by changing as little as one word that\nis not detectable by humans while still fooling other BERT models."
    },
    {
        "date": "2025-03",
        "title": "MVGSR: Multi-View Consistency Gaussian Splatting for Robust Surface Reconstruction",
        "author": "Chenfeng Hou, Qi Xun Yeo, Mengqi Guo, Yongxin Su, Yanyan Li, and Gim Hee Lee",
        "link": "http://arxiv.org/abs/2503.08093v2",
        "abstract": "3D Gaussian Splatting (3DGS) has gained significant attention for its\nhigh-quality rendering capabilities, ultra-fast training, and inference speeds.\nHowever, when we apply 3DGS to surface reconstruction tasks, especially in\nenvironments with dynamic objects and distractors, the method suffers from\nfloating artifacts and color errors due to inconsistency from different\nviewpoints. To address this challenge, we propose Multi-View Consistency\nGaussian Splatting for the domain of Robust Surface Reconstruction\n(\\textbf{MVGSR}), which takes advantage of lightweight Gaussian models and a\n{heuristics-guided distractor masking} strategy for robust surface\nreconstruction in non-static environments. Compared to existing methods that\nrely on MLPs for distractor segmentation strategies, our approach separates\ndistractors from static scene elements by comparing multi-view feature\nconsistency, allowing us to obtain precise distractor masks early in training.\nFurthermore, we introduce a pruning measure based on multi-view contributions\nto reset transmittance, effectively reducing floating artifacts. Finally, a\nmulti-view consistency loss is applied to achieve high-quality performance in\nsurface reconstruction tasks. Experimental results demonstrate that MVGSR\nachieves competitive geometric accuracy and rendering fidelity compared to the\nstate-of-the-art surface reconstruction algorithms. More information is\navailable on our project page (https://mvgsr.github.io)."
    },
    {
        "date": "2025-03",
        "title": "\"We just did not have that on the embedded system\": Insights and Challenges for Securing Microcontroller Systems from the Embedded CTF Competitions",
        "author": "Zheyuan Ma, Gaoxiang Liu, Alex Eastman, Kai Kaufman, Md Armanuzzaman, Xi Tan, Katherine Jesse, Robert Walls, and Ziming Zhao",
        "link": "http://arxiv.org/abs/2503.08053v1",
        "abstract": "Microcontroller systems are integral to our daily lives, powering\nmission-critical applications such as vehicles, medical devices, and industrial\ncontrol systems. Therefore, it is essential to investigate and outline the\nchallenges encountered in developing secure microcontroller systems. While\nprevious research has focused solely on microcontroller firmware analysis to\nidentify and characterize vulnerabilities, our study uniquely leverages data\nfrom the 2023 and 2024 MITRE eCTF team submissions and post-competition\ninterviews. This approach allows us to dissect the entire lifecycle of secure\nmicrocontroller system development from both technical and perceptual\nperspectives, providing deeper insights into how these vulnerabilities emerge\nin the first place.\n  Through the lens of eCTF, we identify fundamental conceptual and practical\nchallenges in securing microcontroller systems. Conceptually, it is difficult\nto adapt from a microprocessor system to a microcontroller system, and\nparticipants are not wholly aware of the unique attacks against\nmicrocontrollers. Practically, security-enhancing tools, such as the\nmemory-safe language Rust, lack adequate support on microcontrollers.\nAdditionally, poor-quality entropy sources weaken cryptography and secret\ngeneration. Additionally, our findings articulate specific research,\ndevelopmental, and educational deficiencies, leading to targeted\nrecommendations for researchers, developers, vendors, and educators to enhance\nthe security of microcontroller systems."
    },
    {
        "date": "2025-03",
        "title": "Detecting Backdoor Attacks in Federated Learning via Direction Alignment Inspection",
        "author": "Jiahao Xu, Zikai Zhang, and Rui Hu",
        "link": "http://arxiv.org/abs/2503.07978v2",
        "abstract": "The distributed nature of training makes Federated Learning (FL) vulnerable\nto backdoor attacks, where malicious model updates aim to compromise the global\nmodel's performance on specific tasks. Existing defense methods show limited\nefficacy as they overlook the inconsistency between benign and malicious model\nupdates regarding both general and fine-grained directions. To fill this gap,\nwe introduce AlignIns, a novel defense method designed to safeguard FL systems\nagainst backdoor attacks. AlignIns looks into the direction of each model\nupdate through a direction alignment inspection process. Specifically, it\nexamines the alignment of model updates with the overall update direction and\nanalyzes the distribution of the signs of their significant parameters,\ncomparing them with the principle sign across all model updates. Model updates\nthat exhibit an unusual degree of alignment are considered malicious and thus\nbe filtered out. We provide the theoretical analysis of the robustness of\nAlignIns and its propagation error in FL. Our empirical results on both\nindependent and identically distributed (IID) and non-IID datasets demonstrate\nthat AlignIns achieves higher robustness compared to the state-of-the-art\ndefense methods. The code is available at\nhttps://github.com/JiiahaoXU/AlignIns."
    },
    {
        "date": "2025-03",
        "title": "FairDeFace: Evaluating the Fairness and Adversarial Robustness of Face Obfuscation Methods",
        "author": "Seyyed Mohammad Sadegh Moosavi Khorzooghi, Poojitha Thota, Mohit Singhal, Abolfazl Asudeh, Gautam Das, and Shirin Nilizadeh",
        "link": "http://arxiv.org/abs/2503.08731v1",
        "abstract": "The lack of a common platform and benchmark datasets for evaluating face\nobfuscation methods has been a challenge, with every method being tested using\narbitrary experiments, datasets, and metrics. While prior work has demonstrated\nthat face recognition systems exhibit bias against some demographic groups,\nthere exists a substantial gap in our understanding regarding the fairness of\nface obfuscation methods. Providing fair face obfuscation methods can ensure\nequitable protection across diverse demographic groups, especially since they\ncan be used to preserve the privacy of vulnerable populations. To address these\ngaps, this paper introduces a comprehensive framework, named FairDeFace,\ndesigned to assess the adversarial robustness and fairness of face obfuscation\nmethods. The framework introduces a set of modules encompassing data\nbenchmarks, face detection and recognition algorithms, adversarial models,\nutility detection models, and fairness metrics. FairDeFace serves as a\nversatile platform where any face obfuscation method can be integrated,\nallowing for rigorous testing and comparison with other state-of-the-art\nmethods. In its current implementation, FairDeFace incorporates 6 attacks, and\nseveral privacy, utility and fairness metrics. Using FairDeFace, and by\nconducting more than 500 experiments, we evaluated and compared the adversarial\nrobustness of seven face obfuscation methods. This extensive analysis led to\nmany interesting findings both in terms of the degree of robustness of existing\nmethods and their biases against some gender or racial groups. FairDeFace also\nuses visualization of focused areas for both obfuscation and verification\nattacks to show not only which areas are mostly changed in the obfuscation\nprocess for some demographics, but also why they failed through focus area\ncomparison of obfuscation and verification."
    },
    {
        "date": "2025-03",
        "title": "Certainly Bot Or Not? Trustworthy Social Bot Detection via Robust Multi-Modal Neural Processes",
        "author": "Qi Wu, Yingguang Yang, hao liu, Hao Peng, Buyun He, Yutong Xia, and Yong Liao",
        "link": "http://arxiv.org/abs/2503.09626v1",
        "abstract": "Social bot detection is crucial for mitigating misinformation, online\nmanipulation, and coordinated inauthentic behavior. While existing neural\nnetwork-based detectors perform well on benchmarks, they struggle with\ngeneralization due to distribution shifts across datasets and frequently\nproduce overconfident predictions for out-of-distribution accounts beyond the\ntraining data. To address this, we introduce a novel Uncertainty Estimation for\nSocial Bot Detection (UESBD) framework, which quantifies the predictive\nuncertainty of detectors beyond mere classification. For this task, we propose\nRobust Multi-modal Neural Processes (RMNP), which aims to enhance the\nrobustness of multi-modal neural processes to modality inconsistencies caused\nby social bot camouflage. RMNP first learns unimodal representations through\nmodality-specific encoders. Then, unimodal attentive neural processes are\nemployed to encode the Gaussian distribution of unimodal latent variables.\nFurthermore, to avoid social bots stealing human features to camouflage\nthemselves thus causing certain modalities to provide conflictive information,\nwe introduce an evidential gating network to explicitly model the reliability\nof modalities. The joint latent distribution is learned through the generalized\nproduct of experts, which takes the reliability of each modality into\nconsideration during fusion. The final prediction is obtained through Monte\nCarlo sampling of the joint latent distribution followed by a decoder.\nExperiments on three real-world benchmarks show the effectiveness of RMNP in\nclassification and uncertainty estimation, as well as its robustness to\nmodality conflicts."
    },
    {
        "date": "2025-03",
        "title": "ReLATE: Resilient Learner Selection for Multivariate Time-Series Classification Against Adversarial Attacks",
        "author": "Cagla Ipek Kocal, Onat Gungor, Aaron Tartz, Tajana Rosing, and Baris Aksanli",
        "link": "http://arxiv.org/abs/2503.07882v1",
        "abstract": "Minimizing computational overhead in time-series classification, particularly\nin deep learning models, presents a significant challenge. This challenge is\nfurther compounded by adversarial attacks, emphasizing the need for resilient\nmethods that ensure robust performance and efficient model selection. We\nintroduce ReLATE, a framework that identifies robust learners based on dataset\nsimilarity, reduces computational overhead, and enhances resilience. ReLATE\nmaintains multiple deep learning models in well-known adversarial attack\nscenarios, capturing model performance. ReLATE identifies the most analogous\ndataset to a given target using a similarity metric, then applies the optimal\nmodel from the most similar dataset. ReLATE reduces computational overhead by\nan average of 81.2%, enhancing adversarial resilience and streamlining robust\nmodel selection, all without sacrificing performance, within 4.2% of Oracle."
    },
    {
        "date": "2025-03",
        "title": "Efficient Resource Management for Secure and Low-Latency O-RAN Communication",
        "author": "Zaineh Abughazzah, Emna Baccour, Ahmed Refaey, Amr Mohamed, and Mounir Hamdi",
        "link": "http://arxiv.org/abs/2503.07857v1",
        "abstract": "Open Radio Access Networks (O-RAN) are transforming telecommunications by\nshifting from centralized to distributed architectures, promoting flexibility,\ninteroperability, and innovation through open interfaces and multi-vendor\nenvironments. However, O-RAN's reliance on cloud-based architecture and\nenhanced observability introduces significant security and resource management\nchallenges. Efficient resource management is crucial for secure and reliable\ncommunication in O-RAN, within the resource-constrained environment and\nheterogeneity of requirements, where multiple User Equipment (UE) and O-RAN\nRadio Units (O-RUs) coexist. This paper develops a framework to manage these\naspects, ensuring each O-RU is associated with UEs based on their communication\nchannel qualities and computational resources, and selecting appropriate\nencryption algorithms to safeguard data confidentiality, integrity, and\nauthentication. A Multi-objective Optimization Problem (MOP) is formulated to\nminimize latency and maximize security within resource constraints. Different\napproaches are proposed to relax the complexity of the problem and achieve\nnear-optimal performance, facilitating trade-offs between latency, security,\nand solution complexity. Simulation results demonstrate that the proposed\napproaches are close enough to the optimal solution, proving that our approach\nis both effective and efficient."
    },
    {
        "date": "2025-03",
        "title": "Helios 2.0: A Robust, Ultra-Low Power Gesture Recognition System Optimised for Event-Sensor based Wearables",
        "author": "Prarthana Bhattacharyya, Joshua Mitton, Ryan Page, Owen Morgan, Oliver Powell, Benjamin Menzies, Gabriel Homewood, Kemi Jacobs, Paolo Baesso, Taru Muhonen, Richard Vigars, and Louis Berridge",
        "link": "http://arxiv.org/abs/2503.07825v1",
        "abstract": "We present an advance in wearable technology: a mobile-optimized, real-time,\nultra-low-power event camera system that enables natural hand gesture control\nfor smart glasses, dramatically improving user experience. While hand gesture\nrecognition in computer vision has advanced significantly, critical challenges\nremain in creating systems that are intuitive, adaptable across diverse users\nand environments, and energy-efficient enough for practical wearable\napplications. Our approach tackles these challenges through carefully selected\nmicrogestures: lateral thumb swipes across the index finger (in both\ndirections) and a double pinch between thumb and index fingertips. These\nhuman-centered interactions leverage natural hand movements, ensuring intuitive\nusability without requiring users to learn complex command sequences. To\novercome variability in users and environments, we developed a novel simulation\nmethodology that enables comprehensive domain sampling without extensive\nreal-world data collection. Our power-optimised architecture maintains\nexceptional performance, achieving F1 scores above 80\\% on benchmark datasets\nfeaturing diverse users and environments. The resulting models operate at just\n6-8 mW when exploiting the Qualcomm Snapdragon Hexagon DSP, with our 2-channel\nimplementation exceeding 70\\% F1 accuracy and our 6-channel model surpassing\n80\\% F1 accuracy across all gesture classes in user studies. These results were\nachieved using only synthetic training data. This improves on the\nstate-of-the-art for F1 accuracy by 20\\% with a power reduction 25x when using\nDSP. This advancement brings deploying ultra-low-power vision systems in\nwearable devices closer and opens new possibilities for seamless human-computer\ninteraction."
    },
    {
        "date": "2025-03",
        "title": "Strengthening the Internal Adversarial Robustness in Lifted Neural Networks",
        "author": "Christopher Zach",
        "link": "http://arxiv.org/abs/2503.07818v1",
        "abstract": "Lifted neural networks (i.e. neural architectures explicitly optimizing over\nrespective network potentials to determine the neural activities) can be\ncombined with a type of adversarial training to gain robustness for internal as\nwell as input layers, in addition to improved generalization performance. In\nthis work we first investigate how adversarial robustness in this framework can\nbe further strengthened by solely modifying the training loss. In a second step\nwe fix some remaining limitations and arrive at a novel training loss for\nlifted neural networks, that combines targeted and untargeted adversarial\nperturbations."
    },
    {
        "date": "2025-03",
        "title": "On the Semantic Security of NTRU -- with a gentle introduction to cryptography",
        "author": "Liam Peet-Pare",
        "link": "http://arxiv.org/abs/2503.07790v1",
        "abstract": "This paper provides an explanation of NTRU, a post quantum encryption scheme,\nwhile also providing a gentle introduction to cryptography. NTRU is a very\nefficient lattice based cryptosystem that appears to be safe against attacks by\nquantum computers. NTRU's efficiency suggests that it is a strong candidate as\nan alternative to RSA, ElGamal, and ECC for the post quantum world. The paper\nbegins with an introduction to cryptography and security proofs for\ncryptographic schemes before explaining the NTRU cryptosystem and culminating\nwith a proof that the original presentation of NTRU is not IND-CPA secure. We\nwill conclude by mentioning padding schemes to NTRU that are provably IND-CCA2\nsecure in the random oracle model. The paper is designed to be accessible to\nanyone with minimal background in abstract algebra and number theory - no\nprevious knowledge of cryptography is assumed. Given the author's lack of\nfamiliarity with the subject, this paper aims to be an expository work rather\nthan to provide new insights to the subject matter."
    },
    {
        "date": "2025-03",
        "title": "Better Pose Initialization for Fast and Robust 2D/3D Pelvis Registration",
        "author": "Yehyun Suh, J. Ryan Martin, and Daniel Moyer",
        "link": "http://arxiv.org/abs/2503.07767v1",
        "abstract": "This paper presents an approach for improving 2D/3D pelvis registration in\noptimization-based pose estimators using a learned initialization function.\nCurrent methods often fail to converge to the optimal solution when initialized\nnaively. We find that even a coarse initializer greatly improves pose estimator\naccuracy, and improves overall computational efficiency. This approach proves\nto be effective also in challenging cases under more extreme pose variation.\nExperimental validation demonstrates that our method consistently achieves\nrobust and accurate registration, enhancing the reliability of 2D/3D\nregistration for clinical applications."
    },
    {
        "date": "2025-03",
        "title": "SANDRO: a Robust Solver with a Splitting Strategy for Point Cloud Registration",
        "author": "Michael Adlerstein, Jo\u00e3o Carlos Virgolino Soares, Angelo Bratta, and Claudio Semini",
        "link": "http://arxiv.org/abs/2503.07743v1",
        "abstract": "Point cloud registration is a critical problem in computer vision and\nrobotics, especially in the field of navigation. Current methods often fail\nwhen faced with high outlier rates or take a long time to converge to a\nsuitable solution. In this work, we introduce a novel algorithm for point cloud\nregistration called SANDRO (Splitting strategy for point cloud Alignment using\nNon-convex anD Robust Optimization), which combines an Iteratively Reweighted\nLeast Squares (IRLS) framework with a robust loss function with graduated\nnon-convexity. This approach is further enhanced by a splitting strategy\ndesigned to handle high outlier rates and skewed distributions of outliers.\nSANDRO is capable of addressing important limitations of existing methods, as\nin challenging scenarios where the presence of high outlier rates and point\ncloud symmetries significantly hinder convergence. SANDRO achieves superior\nperformance in terms of success rate when compared to the state-of-the-art\nmethods, demonstrating a 20% improvement from the current state of the art when\ntested on the Redwood real dataset and 60% improvement when tested on synthetic\ndata."
    },
    {
        "date": "2025-03",
        "title": "Runtime Detection of Adversarial Attacks in AI Accelerators Using Performance Counters",
        "author": "Habibur Rahaman, Atri Chatterjee, and Swarup Bhunia",
        "link": "http://arxiv.org/abs/2503.07568v1",
        "abstract": "Rapid adoption of AI technologies raises several major security concerns,\nincluding the risks of adversarial perturbations, which threaten the\nconfidentiality and integrity of AI applications. Protecting AI hardware from\nmisuse and diverse security threats is a challenging task. To address this\nchallenge, we propose SAMURAI, a novel framework for safeguarding against\nmalicious usage of AI hardware and its resilience to attacks. SAMURAI\nintroduces an AI Performance Counter (APC) for tracking dynamic behavior of an\nAI model coupled with an on-chip Machine Learning (ML) analysis engine, known\nas TANTO (Trained Anomaly Inspection Through Trace Observation). APC records\nthe runtime profile of the low-level hardware events of different AI\noperations. Subsequently, the summary information recorded by the APC is\nprocessed by TANTO to efficiently identify potential security breaches and\nensure secure, responsible use of AI. SAMURAI enables real-time detection of\nsecurity threats and misuse without relying on traditional software-based\nsolutions that require model integration. Experimental results demonstrate that\nSAMURAI achieves up to 97% accuracy in detecting adversarial attacks with\nmoderate overhead on various AI models, significantly outperforming\nconventional software-based approaches. It enhances security and regulatory\ncompliance, providing a comprehensive solution for safeguarding AI against\nemergent threats."
    },
    {
        "date": "2025-03",
        "title": "PoisonedParrot: Subtle Data Poisoning Attacks to Elicit Copyright-Infringing Content from Large Language Models",
        "author": "Michael-Andrei Panaitescu-Liess, Pankayaraj Pathmanathan, Yigitcan Kaya, Zora Che, Bang An, Sicheng Zhu, Aakriti Agrawal, and Furong Huang",
        "link": "http://arxiv.org/abs/2503.07697v1",
        "abstract": "As the capabilities of large language models (LLMs) continue to expand, their\nusage has become increasingly prevalent. However, as reflected in numerous\nongoing lawsuits regarding LLM-generated content, addressing copyright\ninfringement remains a significant challenge. In this paper, we introduce\nPoisonedParrot: the first stealthy data poisoning attack that induces an LLM to\ngenerate copyrighted content even when the model has not been directly trained\non the specific copyrighted material. PoisonedParrot integrates small fragments\nof copyrighted text into the poison samples using an off-the-shelf LLM. Despite\nits simplicity, evaluated in a wide range of experiments, PoisonedParrot is\nsurprisingly effective at priming the model to generate copyrighted content\nwith no discernible side effects. Moreover, we discover that existing defenses\nare largely ineffective against our attack. Finally, we make the first attempt\nat mitigating copyright-infringement poisoning attacks by proposing a defense:\nParrotTrap. We encourage the community to explore this emerging threat model\nfurther."
    },
    {
        "date": "2025-03",
        "title": "ADROIT: A Self-Supervised Framework for Learning Robust Representations for Active Learning",
        "author": "Soumya Banerjee, and Vinay Kumar Verma",
        "link": "http://arxiv.org/abs/2503.07506v1",
        "abstract": "Active learning aims to select optimal samples for labeling, minimizing\nannotation costs. This paper introduces a unified representation learning\nframework tailored for active learning with task awareness. It integrates\ndiverse sources, comprising reconstruction, adversarial, self-supervised,\nknowledge-distillation, and classification losses into a unified VAE-based\nADROIT approach. The proposed approach comprises three key components - a\nunified representation generator (VAE), a state discriminator, and a (proxy)\ntask-learner or classifier. ADROIT learns a latent code using both labeled and\nunlabeled data, incorporating task-awareness by leveraging labeled data with\nthe proxy classifier. Unlike previous approaches, the proxy classifier\nadditionally employs a self-supervised loss on unlabeled data and utilizes\nknowledge distillation to align with the target task-learner. The state\ndiscriminator distinguishes between labeled and unlabeled data, facilitating\nthe selection of informative unlabeled samples. The dynamic interaction between\nVAE and the state discriminator creates a competitive environment, with the VAE\nattempting to deceive the discriminator, while the state discriminator learns\nto differentiate between labeled and unlabeled inputs. Extensive evaluations on\ndiverse datasets and ablation analysis affirm the effectiveness of the proposed\nmodel."
    },
    {
        "date": "2025-03",
        "title": "From Centralized to Decentralized Federated Learning: Theoretical Insights, Privacy Preservation, and Robustness Challenges",
        "author": "Qiongxiu Li, Wenrui Yu, Yufei Xia, and Jun Pang",
        "link": "http://arxiv.org/abs/2503.07505v1",
        "abstract": "Federated Learning (FL) enables collaborative learning without directly\nsharing individual's raw data. FL can be implemented in either a centralized\n(server-based) or decentralized (peer-to-peer) manner. In this survey, we\npresent a novel perspective: the fundamental difference between centralized FL\n(CFL) and decentralized FL (DFL) is not merely the network topology, but the\nunderlying training protocol: separate aggregation vs. joint optimization. We\nargue that this distinction in protocol leads to significant differences in\nmodel utility, privacy preservation, and robustness to attacks. We\nsystematically review and categorize existing works in both CFL and DFL\naccording to the type of protocol they employ. This taxonomy provides deeper\ninsights into prior research and clarifies how various approaches relate or\ndiffer. Through our analysis, we identify key gaps in the literature. In\nparticular, we observe a surprising lack of exploration of DFL approaches based\non distributed optimization methods, despite their potential advantages. We\nhighlight this under-explored direction and call for more research on\nleveraging distributed optimization for federated learning. Overall, this work\noffers a comprehensive overview from centralized to decentralized FL, sheds new\nlight on the core distinctions between approaches, and outlines open challenges\nand future directions for the field."
    },
    {
        "date": "2025-03",
        "title": "Efficient Membership Inference Attacks by Bayesian Neural Network",
        "author": "Zhenlong Liu, Wenyu Jiang, Feng Zhou, and Hongxin Wei",
        "link": "http://arxiv.org/abs/2503.07482v1",
        "abstract": "Membership Inference Attacks (MIAs) aim to estimate whether a specific data\npoint was used in the training of a given model. Previous attacks often utilize\nmultiple reference models to approximate the conditional score distribution,\nleading to significant computational overhead. While recent work leverages\nquantile regression to estimate conditional thresholds, it fails to capture\nepistemic uncertainty, resulting in bias in low-density regions. In this work,\nwe propose a novel approach - Bayesian Membership Inference Attack (BMIA),\nwhich performs conditional attack through Bayesian inference. In particular, we\ntransform a trained reference model into Bayesian neural networks by Laplace\napproximation, enabling the direct estimation of the conditional score\ndistribution by probabilistic model parameters. Our method addresses both\nepistemic and aleatoric uncertainty with only a reference model, enabling\nefficient and powerful MIA. Extensive experiments on five datasets demonstrate\nthe effectiveness and efficiency of BMIA."
    },
    {
        "date": "2025-03",
        "title": "Probabilistic Segmentation for Robust Field of View Estimation",
        "author": "R. Spencer Hallyburton, David Hunt, Yiwei He, Judy He, and Miroslav Pajic",
        "link": "http://arxiv.org/abs/2503.07375v1",
        "abstract": "Attacks on sensing and perception threaten the safe deployment of autonomous\nvehicles (AVs). Security-aware sensor fusion helps mitigate threats but\nrequires accurate field of view (FOV) estimation which has not been evaluated\nautonomy. To address this gap, we adapt classical computer graphics algorithms\nto develop the first autonomy-relevant FOV estimators and create the first\ndatasets with ground truth FOV labels. Unfortunately, we find that these\napproaches are themselves highly vulnerable to attacks on sensing. To improve\nrobustness of FOV estimation against attacks, we propose a learning-based\nsegmentation model that captures FOV features, integrates Monte Carlo dropout\n(MCD) for uncertainty quantification, and performs anomaly detection on\nconfidence maps. We illustrate through comprehensive evaluations attack\nresistance and strong generalization across environments. Architecture trade\nstudies demonstrate the model is feasible for real-time deployment in multiple\napplications."
    },
    {
        "date": "2025-03",
        "title": "Group-robust Sample Reweighting for Subpopulation Shifts via Influence Functions",
        "author": "Rui Qiao, Zhaoxuan Wu, Jingtan Wang, Pang Wei Koh, and Bryan Kian Hsiang Low",
        "link": "http://arxiv.org/abs/2503.07315v1",
        "abstract": "Machine learning models often have uneven performance among subpopulations\n(a.k.a., groups) in the data distributions. This poses a significant challenge\nfor the models to generalize when the proportions of the groups shift during\ndeployment. To improve robustness to such shifts, existing approaches have\ndeveloped strategies that train models or perform hyperparameter tuning using\nthe group-labeled data to minimize the worst-case loss over groups. However, a\nnon-trivial amount of high-quality labels is often required to obtain\nnoticeable improvements. Given the costliness of the labels, we propose to\nadopt a different paradigm to enhance group label efficiency: utilizing the\ngroup-labeled data as a target set to optimize the weights of other\ngroup-unlabeled data. We introduce Group-robust Sample Reweighting (GSR), a\ntwo-stage approach that first learns the representations from group-unlabeled\ndata, and then tinkers the model by iteratively retraining its last layer on\nthe reweighted data using influence functions. Our GSR is theoretically sound,\npractically lightweight, and effective in improving the robustness to\nsubpopulation shifts. In particular, GSR outperforms the previous\nstate-of-the-art approaches that require the same amount or even more group\nlabels."
    },
    {
        "date": "2025-03",
        "title": "All That Glitters Is Not Gold: Key-Secured 3D Secrets within 3D Gaussian Splatting",
        "author": "Yan Ren, Shilin Lu, and Adams Wai-Kin Kong",
        "link": "http://arxiv.org/abs/2503.07191v1",
        "abstract": "Recent advances in 3D Gaussian Splatting (3DGS) have revolutionized scene\nreconstruction, opening new possibilities for 3D steganography by hiding 3D\nsecrets within 3D covers. The key challenge in steganography is ensuring\nimperceptibility while maintaining high-fidelity reconstruction. However,\nexisting methods often suffer from detectability risks and utilize only\nsuboptimal 3DGS features, limiting their full potential. We propose a novel\nend-to-end key-secured 3D steganography framework (KeySS) that jointly\noptimizes a 3DGS model and a key-secured decoder for secret reconstruction. Our\napproach reveals that Gaussian features contribute unequally to secret hiding.\nThe framework incorporates a key-controllable mechanism enabling multi-secret\nhiding and unauthorized access prevention, while systematically exploring\noptimal feature update to balance fidelity and security. To rigorously evaluate\nsteganographic imperceptibility beyond conventional 2D metrics, we introduce\n3D-Sinkhorn distance analysis, which quantifies distributional differences\nbetween original and steganographic Gaussian parameters in the representation\nspace. Extensive experiments demonstrate that our method achieves\nstate-of-the-art performance in both cover and secret reconstruction while\nmaintaining high security levels, advancing the field of 3D steganography. Code\nis available at https://github.com/RY-Paper/KeySS"
    },
    {
        "date": "2025-03",
        "title": "Breaking the Limits of Quantization-Aware Defenses: QADT-R for Robustness Against Patch-Based Adversarial Attacks in QNNs",
        "author": "Amira Guesmi, Bassem Ouni, and Muhammad Shafique",
        "link": "http://arxiv.org/abs/2503.07058v1",
        "abstract": "Quantized Neural Networks (QNNs) have emerged as a promising solution for\nreducing model size and computational costs, making them well-suited for\ndeployment in edge and resource-constrained environments. While quantization is\nknown to disrupt gradient propagation and enhance robustness against\npixel-level adversarial attacks, its effectiveness against patch-based\nadversarial attacks remains largely unexplored. In this work, we demonstrate\nthat adversarial patches remain highly transferable across quantized models,\nachieving over 70\\% attack success rates (ASR) even at extreme bit-width\nreductions (e.g., 2-bit). This challenges the common assumption that\nquantization inherently mitigates adversarial threats. To address this, we\npropose Quantization-Aware Defense Training with Randomization (QADT-R), a\nnovel defense strategy that integrates Adaptive Quantization-Aware Patch\nGeneration (A-QAPA), Dynamic Bit-Width Training (DBWT), and\nGradient-Inconsistent Regularization (GIR) to enhance resilience against highly\ntransferable patch-based attacks. A-QAPA generates adversarial patches within\nquantized models, ensuring robustness across different bit-widths. DBWT\nintroduces bit-width cycling during training to prevent overfitting to a\nspecific quantization setting, while GIR injects controlled gradient\nperturbations to disrupt adversarial optimization. Extensive evaluations on\nCIFAR-10 and ImageNet show that QADT-R reduces ASR by up to 25\\% compared to\nprior defenses such as PBAT and DWQ. Our findings further reveal that\nPBAT-trained models, while effective against seen patch configurations, fail to\ngeneralize to unseen patches due to quantization shift. Additionally, our\nempirical analysis of gradient alignment, spatial sensitivity, and patch\nvisibility provides insights into the mechanisms that contribute to the high\ntransferability of patch-based attacks in QNNs."
    },
    {
        "date": "2025-03",
        "title": "HybridReg: Robust 3D Point Cloud Registration with Hybrid Motions",
        "author": "Keyu Du, Hao Xu, Haipeng Li, Hong Qu, Chi-Wing Fu, and Shuaicheng Liu",
        "link": "http://arxiv.org/abs/2503.07019v1",
        "abstract": "Scene-level point cloud registration is very challenging when considering\ndynamic foregrounds. Existing indoor datasets mostly assume rigid motions, so\nthe trained models cannot robustly handle scenes with non-rigid motions. On the\nother hand, non-rigid datasets are mainly object-level, so the trained models\ncannot generalize well to complex scenes. This paper presents HybridReg, a new\napproach to 3D point cloud registration, learning uncertainty mask to account\nfor hybrid motions: rigid for backgrounds and non-rigid/rigid for\ninstance-level foregrounds. First, we build a scene-level 3D registration\ndataset, namely HybridMatch, designed specifically with strategies to arrange\ndiverse deforming foregrounds in a controllable manner. Second, we account for\ndifferent motion types and formulate a mask-learning module to alleviate the\ninterference of deforming outliers. Third, we exploit a simple yet effective\nnegative log-likelihood loss to adopt uncertainty to guide the feature\nextraction and correlation computation. To our best knowledge, HybridReg is the\nfirst work that exploits hybrid motions for robust point cloud registration.\nExtensive experiments show HybridReg's strengths, leading it to achieve\nstate-of-the-art performance on both widely-used indoor and outdoor datasets."
    },
    {
        "date": "2025-03",
        "title": "Public space security management using digital twin technologies",
        "author": "Stylianos Zindros, Christos Chronis, Panagiotis Radoglou-Grammatikis, Vasileios Argyriou, Panagiotis Sarigiannidis, Iraklis Varlamis, and Georgios Th. Papadopoulos",
        "link": "http://arxiv.org/abs/2503.06996v1",
        "abstract": "As the security of public spaces remains a critical issue in today's world,\nDigital Twin technologies have emerged in recent years as a promising solution\nfor detecting and predicting potential future threats. The applied methodology\nleverages a Digital Twin of a metro station in Athens, Greece, using the\nFlexSim simulation software. The model encompasses points of interest and\npassenger flows, and sets their corresponding parameters. These elements\ninfluence and allow the model to provide reasonable predictions on the security\nmanagement of the station under various scenarios. Experimental tests are\nconducted with different configurations of surveillance cameras and\noptimizations of camera angles to evaluate the effectiveness of the space\nsurveillance setup. The results show that the strategic positioning of\nsurveillance cameras and the adjustment of their angles significantly improves\nthe detection of suspicious behaviors and with the use of the DT it is possible\nto evaluate different scenarios and find the optimal camera setup for each\ncase. In summary, this study highlights the value of Digital Twins in real-time\nsimulation and data-driven security management. The proposed approach\ncontributes to the ongoing development of smart security solutions for public\nspaces and provides an innovative framework for threat detection and\nprevention."
    },
    {
        "date": "2025-03",
        "title": "Utilizing Jailbreak Probability to Attack and Safeguard Multimodal LLMs",
        "author": "Wenzhuo Xu, Zhipeng Wei, Xiongtao Sun, Deyue Zhang, Dongdong Yang, Quanchen Zou, and Xiangzheng Zhang",
        "link": "http://arxiv.org/abs/2503.06989v1",
        "abstract": "Recently, Multimodal Large Language Models (MLLMs) have demonstrated their\nsuperior ability in understanding multimodal contents. However, they remain\nvulnerable to jailbreak attacks, which exploit weaknesses in their safety\nalignment to generate harmful responses. Previous studies categorize jailbreaks\nas successful or failed based on whether responses contain malicious content.\nHowever, given the stochastic nature of MLLM responses, this binary\nclassification of an input's ability to jailbreak MLLMs is inappropriate.\nDerived from this viewpoint, we introduce jailbreak probability to quantify the\njailbreak potential of an input, which represents the likelihood that MLLMs\ngenerated a malicious response when prompted with this input. We approximate\nthis probability through multiple queries to MLLMs. After modeling the\nrelationship between input hidden states and their corresponding jailbreak\nprobability using Jailbreak Probability Prediction Network (JPPN), we use\ncontinuous jailbreak probability for optimization. Specifically, we propose\nJailbreak-Probability-based Attack (JPA) that optimizes adversarial\nperturbations on inputs to maximize jailbreak probability. To counteract\nattacks, we also propose two defensive methods: Jailbreak-Probability-based\nFinetuning (JPF) and Jailbreak-Probability-based Defensive Noise (JPDN), which\nminimizes jailbreak probability in the MLLM parameters and input space,\nrespectively. Extensive experiments show that (1) JPA yields improvements (up\nto 28.38\\%) under both white and black box settings compared to previous\nmethods with small perturbation bounds and few iterations. (2) JPF and JPDN\nsignificantly reduce jailbreaks by at most over 60\\%. Both of the above results\ndemonstrate the significance of introducing jailbreak probability to make\nnuanced distinctions among input jailbreak abilities."
    },
    {
        "date": "2025-03",
        "title": "ConcreTizer: Model Inversion Attack via Occupancy Classification and Dispersion Control for 3D Point Cloud Restoration",
        "author": "Youngseok Kim, Sunwook Hwang, Hyung-Sin Kim, and Saewoong Bahk",
        "link": "http://arxiv.org/abs/2503.06986v1",
        "abstract": "The growing use of 3D point cloud data in autonomous vehicles (AVs) has\nraised serious privacy concerns, particularly due to the sensitive information\nthat can be extracted from 3D data. While model inversion attacks have been\nwidely studied in the context of 2D data, their application to 3D point clouds\nremains largely unexplored. To fill this gap, we present the first in-depth\nstudy of model inversion attacks aimed at restoring 3D point cloud scenes. Our\nanalysis reveals the unique challenges, the inherent sparsity of 3D point\nclouds and the ambiguity between empty and non-empty voxels after voxelization,\nwhich are further exacerbated by the dispersion of non-empty voxels across\nfeature extractor layers. To address these challenges, we introduce\nConcreTizer, a simple yet effective model inversion attack designed\nspecifically for voxel-based 3D point cloud data. ConcreTizer incorporates\nVoxel Occupancy Classification to distinguish between empty and non-empty\nvoxels and Dispersion-Controlled Supervision to mitigate non-empty voxel\ndispersion. Extensive experiments on widely used 3D feature extractors and\nbenchmark datasets, such as KITTI and Waymo, demonstrate that ConcreTizer\nconcretely restores the original 3D point cloud scene from disrupted 3D feature\ndata. Our findings highlight both the vulnerability of 3D data to inversion\nattacks and the urgent need for robust defense strategies."
    },
    {
        "date": "2025-03",
        "title": "MIGA: Mutual Information-Guided Attack on Denoising Models for Semantic Manipulation",
        "author": "Guanghao Li, Mingzhi Chen, Hao Yu, Shuting Dong, Wenhao Jiang, Ming Tang, and Chun Yuan",
        "link": "http://arxiv.org/abs/2503.06966v2",
        "abstract": "Deep learning-based denoising models have been widely employed in vision\ntasks, functioning as filters to eliminate noise while retaining crucial\nsemantic information. Additionally, they play a vital role in defending against\nadversarial perturbations that threaten downstream tasks. However, these models\ncan be intrinsically susceptible to adversarial attacks due to their dependence\non specific noise assumptions. Existing attacks on denoising models mainly aim\nat deteriorating visual clarity while neglecting semantic manipulation,\nrendering them either easily detectable or limited in effectiveness. In this\npaper, we propose Mutual Information-Guided Attack (MIGA), the first method\ndesigned to directly attack deep denoising models by strategically disrupting\ntheir ability to preserve semantic content via adversarial perturbations. By\nminimizing the mutual information between the original and denoised images, a\nmeasure of semantic similarity. MIGA forces the denoiser to produce\nperceptually clean yet semantically altered outputs. While these images appear\nvisually plausible, they encode systematically distorted semantics, revealing a\nfundamental vulnerability in denoising models. These distortions persist in\ndenoised outputs and can be quantitatively assessed through downstream task\nperformance. We propose new evaluation metrics and systematically assess MIGA\non four denoising models across five datasets, demonstrating its consistent\neffectiveness in disrupting semantic fidelity. Our findings suggest that\ndenoising models are not always robust and can introduce security risks in\nreal-world applications."
    },
    {
        "date": "2025-03",
        "title": "When Lighting Deceives: Exposing Vision-Language Models' Illumination Vulnerability Through Illumination Transformation Attack",
        "author": "Hanqing Liu, Shouwei Ruan, Yao Huang, Shiji Zhao, and Xingxing Wei",
        "link": "http://arxiv.org/abs/2503.06903v2",
        "abstract": "Vision-Language Models (VLMs) have achieved remarkable success in various\ntasks, yet their robustness to real-world illumination variations remains\nlargely unexplored. To bridge this gap, we propose \\textbf{I}llumination\n\\textbf{T}ransformation \\textbf{A}ttack (\\textbf{ITA}), the first framework to\nsystematically assess VLMs' robustness against illumination changes. However,\nthere still exist two key challenges: (1) how to model global illumination with\nfine-grained control to achieve diverse lighting conditions and (2) how to\nensure adversarial effectiveness while maintaining naturalness. To address the\nfirst challenge, we innovatively decompose global illumination into multiple\nparameterized point light sources based on the illumination rendering equation.\nThis design enables us to model more diverse lighting variations that previous\nmethods could not capture. Then, by integrating these parameterized lighting\nvariations with physics-based lighting reconstruction techniques, we could\nprecisely render such light interactions in the original scenes, finally\nmeeting the goal of fine-grained lighting control. For the second challenge, by\ncontrolling illumination through the lighting reconstrution model's latent\nspace rather than direct pixel manipulation, we inherently preserve physical\nlighting priors. Furthermore, to prevent potential reconstruction artifacts, we\ndesign additional perceptual constraints for maintaining visual consistency\nwith original images and diversity constraints for avoiding light source\nconvergence.\n  Extensive experiments demonstrate that our ITA could significantly reduce the\nperformance of advanced VLMs, e.g., LLaVA-1.6, while possessing competitive\nnaturalness, exposing VLMS' critical illuminiation vulnerabilities."
    },
    {
        "date": "2025-03",
        "title": "TH-Bench: Evaluating Evading Attacks via Humanizing AI Text on Machine-Generated Text Detectors",
        "author": "Jingyi Zheng, Junfeng Wang, Zhen Sun, Wenhan Dong, Yule Liu, and Xinlei He",
        "link": "http://arxiv.org/abs/2503.08708v2",
        "abstract": "As Large Language Models (LLMs) advance, Machine-Generated Texts (MGTs) have\nbecome increasingly fluent, high-quality, and informative. Existing wide-range\nMGT detectors are designed to identify MGTs to prevent the spread of plagiarism\nand misinformation. However, adversaries attempt to humanize MGTs to evade\ndetection (named evading attacks), which requires only minor modifications to\nbypass MGT detectors. Unfortunately, existing attacks generally lack a unified\nand comprehensive evaluation framework, as they are assessed using different\nexperimental settings, model architectures, and datasets. To fill this gap, we\nintroduce the Text-Humanization Benchmark (TH-Bench), the first comprehensive\nbenchmark to evaluate evading attacks against MGT detectors. TH-Bench evaluates\nattacks across three key dimensions: evading effectiveness, text quality, and\ncomputational overhead. Our extensive experiments evaluate 6 state-of-the-art\nattacks against 13 MGT detectors across 6 datasets, spanning 19 domains and\ngenerated by 11 widely used LLMs. Our findings reveal that no single evading\nattack excels across all three dimensions. Through in-depth analysis, we\nhighlight the strengths and limitations of different attacks. More importantly,\nwe identify a trade-off among three dimensions and propose two optimization\ninsights. Through preliminary experiments, we validate their correctness and\neffectiveness, offering potential directions for future research."
    },
    {
        "date": "2025-03",
        "title": "A Secure Blockchain-Assisted Framework for Real-Time Maritime Environmental Compliance Monitoring",
        "author": "William C. Quigley, Mohamed Rahouti, and Gary M. Weiss",
        "link": "http://arxiv.org/abs/2503.08707v1",
        "abstract": "The maritime industry is governed by stringent environmental regulations,\nmost notably the International Convention for the Prevention of Pollution from\nShips (MARPOL). Ensuring compliance with these regulations is difficult due to\nlow inspection rates and the risk of data fabrication. To address these issues,\nthis paper proposes a secure blockchain-assisted framework for real-time\nmaritime environmental compliance monitoring. By integrating IoT and shipboard\nsensors with blockchain technology, the framework ensures immutable and\ntransparent record-keeping of environmental data. Smart contracts automate\ncompliance verification and notify relevant authorities in case of\nnon-compliance. A proof-of-concept case study on sulfur emissions demonstrates\nthe framework's efficacy in enhancing MARPOL enforcement through real-time data\nintegrity and regulatory adherence. The proposed system leverages the Polygon\nblockchain for scalability and efficiency, providing a robust solution for\nmaritime environmental protection. The evaluation results demonstrate that the\nproposed blockchain-enhanced compliance monitoring system effectively and\nsecurely ensures real-time regulatory adherence with high scalability,\nefficiency, and cost-effectiveness, leveraging the robust capabilities of the\nPolygon blockchain."
    },
    {
        "date": "2025-03",
        "title": "Unique Rashomon Sets for Robust Active Learning",
        "author": "Simon Nguyen, Kentaro Hoffman, and Tyler McCormick",
        "link": "http://arxiv.org/abs/2503.06770v2",
        "abstract": "Collecting labeled data for machine learning models is often expensive and\ntime-consuming. Active learning addresses this challenge by selectively\nlabeling the most informative observations, but when initial labeled data is\nlimited, it becomes difficult to distinguish genuinely informative points from\nthose appearing uncertain primarily due to noise. Ensemble methods like random\nforests are a powerful approach to quantifying this uncertainty but do so by\naggregating all models indiscriminately. This includes poor performing models\nand redundant models, a problem that worsens in the presence of noisy data. We\nintroduce UNique Rashomon Ensembled Active Learning (UNREAL), which selectively\nensembles only distinct models from the Rashomon set, which is the set of\nnearly optimal models. Restricting ensemble membership to high-performing\nmodels with different explanations helps distinguish genuine uncertainty from\nnoise-induced variation. We show that UNREAL achieves faster theoretical\nconvergence rates than traditional active learning approaches and demonstrates\nempirical improvements of up to 20% in predictive accuracy across five\nbenchmark datasets, while simultaneously enhancing model interpretability."
    },
    {
        "date": "2025-03",
        "title": "Enhancing NLP Robustness and Generalization through LLM-Generated Contrast Sets: A Scalable Framework for Systematic Evaluation and Adversarial Training",
        "author": "Hender Lin",
        "link": "http://arxiv.org/abs/2503.06648v1",
        "abstract": "Standard NLP benchmarks often fail to capture vulnerabilities stemming from\ndataset artifacts and spurious correlations. Contrast sets address this gap by\nchallenging models near decision boundaries but are traditionally\nlabor-intensive to create and limited in diversity. This study leverages large\nlanguage models to automate the generation of diverse contrast sets. Using the\nSNLI dataset, we created a 3,000-example contrast set to evaluate and improve\nmodel robustness. Fine-tuning on these contrast sets enhanced performance on\nsystematically perturbed examples, maintained standard test accuracy, and\nmodestly improved generalization to novel perturbations. This automated\napproach offers a scalable solution for evaluating and improving NLP models,\naddressing systematic generalization challenges, and advancing robustness in\nreal-world applications."
    },
    {
        "date": "2025-03",
        "title": "MMARD: Improving the Min-Max Optimization Process in Adversarial Robustness Distillation",
        "author": "Yuzheng Wang, Zhaoyu Chen, Dingkang Yang, Yuanhang Wang, and Lizhe Qi",
        "link": "http://arxiv.org/abs/2503.06559v1",
        "abstract": "Adversarial Robustness Distillation (ARD) is a promising task to boost the\nrobustness of small-capacity models with the guidance of the pre-trained robust\nteacher. The ARD can be summarized as a min-max optimization process, i.e.,\nsynthesizing adversarial examples (inner) & training the student (outer).\nAlthough competitive robustness performance, existing ARD methods still have\nissues. In the inner process, the synthetic training examples are far from the\nteacher's decision boundary leading to important robust information missing. In\nthe outer process, the student model is decoupled from learning natural and\nrobust scenarios, leading to the robustness saturation, i.e., student\nperformance is highly susceptible to customized teacher selection. To tackle\nthese issues, this paper proposes a general Min-Max optimization Adversarial\nRobustness Distillation (MMARD) method. For the inner process, we introduce the\nteacher's robust predictions, which drive the training examples closer to the\nteacher's decision boundary to explore more robust knowledge. For the outer\nprocess, we propose a structured information modeling method based on\ntriangular relationships to measure the mutual information of the model in\nnatural and robust scenarios and enhance the model's ability to understand\nmulti-scenario mapping relationships. Experiments show our MMARD achieves\nstate-of-the-art performance on multiple benchmarks. Besides, MMARD is\nplug-and-play and convenient to combine with existing methods."
    },
    {
        "date": "2025-03",
        "title": "BDPFL: Backdoor Defense for Personalized Federated Learning via Explainable Distillation",
        "author": "Chengcheng Zhu, Jiale Zhang, Di Wu, and Guodong Long",
        "link": "http://arxiv.org/abs/2503.06554v1",
        "abstract": "Federated learning is a distributed learning paradigm that facilitates the\ncollaborative training of a global model across multiple clients while\npreserving the privacy of local datasets. To address inherent challenges\nrelated to data heterogeneity and satisfy personalized needs, a new direction\nwithin FL, known as personalized Federated Learning (pFL), has gradually\nevolved. Extensive attention has been directed toward developing novel\nframeworks and methods to enhance the performance of pFL. Regrettably, the\naspect of security in pFL has been largely overlooked. Our objective is to fill\nthis gap. Similar to FL, pFL is susceptible to backdoor attacks. However,\nexisting backdoor defense strategies are primarily tailored to general FL\nframeworks, and pFL lacks robustness against backdoor attacks. We propose a\nnovel, backdoor-robust pFL framework named BDPFL to address these challenges.\nFirst, BDPFL introduces layer-wise mutual distillation that enables clients to\nlearn their personalized local models while mitigating potential backdoors.\nThen, BDPFL employs explanation heatmap to learn high-quality intermediate\nrepresentations and enhance the effect of eliminating deeper and more\nentrenched backdoors. Moreover, we perform empirical evaluations of BDPFL's\nperformance on three datasets and compare BDPFL with four backdoor defense\nmethods. The experiments demonstrate that BDPFL outperforms baseline methods\nand is effective under various settings."
    },
    {
        "date": "2025-03",
        "title": "AnywhereDoor: Multi-Target Backdoor Attacks on Object Detection",
        "author": "Jialin Lu, Junjie Shan, Ziqi Zhao, and Ka-Ho Chow",
        "link": "http://arxiv.org/abs/2503.06529v2",
        "abstract": "As object detection becomes integral to many safety-critical applications,\nunderstanding its vulnerabilities is essential. Backdoor attacks, in\nparticular, pose a serious threat by implanting hidden triggers in victim\nmodels, which adversaries can later exploit to induce malicious behaviors\nduring inference. However, current understanding is limited to single-target\nattacks, where adversaries must define a fixed malicious behavior (target)\nbefore training, making inference-time adaptability impossible. Given the large\noutput space of object detection (including object existence prediction,\nbounding box estimation, and classification), the feasibility of flexible,\ninference-time model control remains unexplored. This paper introduces\nAnywhereDoor, a multi-target backdoor attack for object detection. Once\nimplanted, AnywhereDoor allows adversaries to make objects disappear, fabricate\nnew ones, or mislabel them, either across all object classes or specific ones,\noffering an unprecedented degree of control. This flexibility is enabled by\nthree key innovations: (i) objective disentanglement to scale the number of\nsupported targets; (ii) trigger mosaicking to ensure robustness even against\nregion-based detectors; and (iii) strategic batching to address object-level\ndata imbalances that hinder manipulation. Extensive experiments demonstrate\nthat AnywhereDoor grants attackers a high degree of control, improving attack\nsuccess rates by 26% compared to adaptations of existing methods for such\nflexible control."
    },
    {
        "date": "2025-03",
        "title": "Can Small Language Models Reliably Resist Jailbreak Attacks? A Comprehensive Evaluation",
        "author": "Wenhui Zhang, Huiyu Xu, Zhibo Wang, Zeqing He, Ziqi Zhu, and Kui Ren",
        "link": "http://arxiv.org/abs/2503.06519v1",
        "abstract": "Small language models (SLMs) have emerged as promising alternatives to large\nlanguage models (LLMs) due to their low computational demands, enhanced privacy\nguarantees and comparable performance in specific domains through light-weight\nfine-tuning. Deploying SLMs on edge devices, such as smartphones and smart\nvehicles, has become a growing trend. However, the security implications of\nSLMs have received less attention than LLMs, particularly regarding jailbreak\nattacks, which is recognized as one of the top threats of LLMs by the OWASP. In\nthis paper, we conduct the first large-scale empirical study of SLMs'\nvulnerabilities to jailbreak attacks. Through systematically evaluation on 63\nSLMs from 15 mainstream SLM families against 8 state-of-the-art jailbreak\nmethods, we demonstrate that 47.6% of evaluated SLMs show high susceptibility\nto jailbreak attacks (ASR > 40%) and 38.1% of them can not even resist direct\nharmful query (ASR > 50%). We further analyze the reasons behind the\nvulnerabilities and identify four key factors: model size, model architecture,\ntraining datasets and training techniques. Moreover, we assess the\neffectiveness of three prompt-level defense methods and find that none of them\nachieve perfect performance, with detection accuracy varying across different\nSLMs and attack methods. Notably, we point out that the inherent security\nawareness play a critical role in SLM security, and models with strong security\nawareness could timely terminate unsafe response with little reminder. Building\nupon the findings, we highlight the urgent need for security-by-design\napproaches in SLM development and provide valuable insights for building more\ntrustworthy SLM ecosystem."
    },
    {
        "date": "2025-03",
        "title": "HFedCKD: Toward Robust Heterogeneous Federated Learning via Data-free Knowledge Distillation and Two-way Contrast",
        "author": "Yiting Zheng, Bohan Lin, Jinqian Chen, and Jihua Zhu",
        "link": "http://arxiv.org/abs/2503.06511v1",
        "abstract": "Most current federated learning frameworks are modeled as static processes,\nignoring the dynamic characteristics of the learning system. Under the limited\ncommunication budget of the central server, the flexible model architecture of\na large number of clients participating in knowledge transfer requires a lower\nparticipation rate, active clients have uneven contributions, and the client\nscale seriously hinders the performance of FL. We consider a more general and\npractical federation scenario and propose a system heterogeneous federation\nmethod based on data-free knowledge distillation and two-way contrast\n(HFedCKD). We apply the Inverse Probability Weighted Distillation (IPWD)\nstrategy to the data-free knowledge transfer framework. The generator completes\nthe data features of the nonparticipating clients. IPWD implements a dynamic\nevaluation of the prediction contribution of each client under different data\ndistributions. Based on the antibiased weighting of its prediction loss, the\nweight distribution of each client is effectively adjusted to fairly integrate\nthe knowledge of participating clients. At the same time, the local model is\nsplit into a feature extractor and a classifier. Through differential contrast\nlearning, the feature extractor is aligned with the global model in the feature\nspace, while the classifier maintains personalized decision-making\ncapabilities. HFedCKD effectively alleviates the knowledge offset caused by a\nlow participation rate under data-free knowledge distillation and improves the\nperformance and stability of the model. We conduct extensive experiments on\nimage and IoT datasets to comprehensively evaluate and verify the\ngeneralization and robustness of the proposed HFedCKD framework."
    },
    {
        "date": "2025-03",
        "title": "Long-tailed Adversarial Training with Self-Distillation",
        "author": "Seungju Cho, Hongsin Lee, and Changick Kim",
        "link": "http://arxiv.org/abs/2503.06461v1",
        "abstract": "Adversarial training significantly enhances adversarial robustness, yet\nsuperior performance is predominantly achieved on balanced datasets.\n  Addressing adversarial robustness in the context of unbalanced or long-tailed\ndistributions is considerably more challenging, mainly due to the scarcity of\ntail data instances.\n  Previous research on adversarial robustness within long-tailed distributions\nhas primarily focused on combining traditional long-tailed natural training\nwith existing adversarial robustness methods.\n  In this study, we provide an in-depth analysis for the challenge that\nadversarial training struggles to achieve high performance on tail classes in\nlong-tailed distributions.\n  Furthermore, we propose a simple yet effective solution to advance\nadversarial robustness on long-tailed distributions through a novel\nself-distillation technique.\n  Specifically, this approach leverages a balanced self-teacher model, which is\ntrained using a balanced dataset sampled from the original long-tailed dataset.\nOur extensive experiments demonstrate state-of-the-art performance in both\nclean and robust accuracy for long-tailed adversarial robustness, with\nsignificant improvements in tail class performance on various datasets. We\nimprove the accuracy against PGD attacks for tail classes by 20.3, 7.1, and 3.8\npercentage points on CIFAR-10, CIFAR-100, and Tiny-ImageNet, respectively,\nwhile achieving the highest robust accuracy."
    },
    {
        "date": "2025-03",
        "title": "R+R: Security Vulnerability Dataset Quality Is Critical",
        "author": "Anurag Swarnim Yadav, and Joseph N. Wilson",
        "link": "http://arxiv.org/abs/2503.06387v1",
        "abstract": "Large Language Models (LLMs) are of great interest in vulnerability detection\nand repair. The effectiveness of these models hinges on the quality of the\ndatasets used for both training and evaluation. Our investigation reveals that\na number of studies featured in prominent software engineering conferences have\nemployed datasets that are plagued by high duplication rates, questionable\nlabel accuracy, and incomplete samples. Using these datasets for\nexperimentation will yield incorrect results that are significantly different\nfrom actual expected behavior. For example, the state-of-the-art VulRepair\nModel, which is reported to have 44% accuracy, on average yielded 9% accuracy\nwhen test-set duplicates were removed from its training set and 13% accuracy\nwhen training-set duplicates were removed from its test set. In an effort to\ntackle these data quality concerns, we have retrained models from several\npapers without duplicates and conducted an accuracy assessment of labels for\nthe top ten most hazardous Common Weakness Enumerations (CWEs). Our findings\nindicate that 56% of the samples had incorrect labels and 44% comprised\nincomplete samples--only 31% were both accurate and complete. Finally, we\nemploy transfer learning using a large deduplicated bugfix corpus to show that\nthese models can exhibit better performance if given larger amounts of\nhigh-quality pre-training data, leading us to conclude that while previous\nstudies have over-estimated performance due to poor dataset quality, this does\nnot demonstrate that better performance is not possible."
    },
    {
        "date": "2025-03",
        "title": "Bayesian Optimization for Robust Identification of Ornstein-Uhlenbeck Model",
        "author": "Jinwen Xu, Qin Lu, and Yaakov Bar-Shalom",
        "link": "http://arxiv.org/abs/2503.06381v1",
        "abstract": "This paper deals with the identification of the stochastic Ornstein-Uhlenbeck\n(OU) process error model, which is characterized by an inverse time constant,\nand the unknown variances of the process and observation noises. Although the\navailability of the explicit expression of the log-likelihood function allows\none to obtain the maximum likelihood estimator (MLE), this entails evaluating\nthe nontrivial gradient and also often struggles with local optima. To address\nthese limitations, we put forth a sample-efficient global optimization approach\nbased on the Bayesian optimization (BO) framework, which relies on a Gaussian\nprocess (GP) surrogate model for the objective function that effectively\nbalances exploration and exploitation to select the query points. Specifically,\neach evaluation of the objective is implemented efficiently through the Kalman\nfilter (KF) recursion. Comprehensive experiments on various parameter settings\nand sampling intervals corroborate that BO-based estimator consistently\noutperforms MLE implemented by the steady-state KF approximation and the\nexpectation-maximization algorithm (whose derivation is a side contribution) in\nterms of root mean-square error (RMSE) and statistical consistency, confirming\nthe effectiveness and robustness of the BO for identification of the stochastic\nOU process. Notably, the RMSE values produced by the BO-based estimator are\nsmaller than the classical Cram\\'{e}r-Rao lower bound, especially for the\ninverse time constant, estimating which has been a long-standing challenge.\nThis seemingly counterintuitive result can be explained by the data-driven\nprior for the learning parameters indirectly injected by BO through the GP\nprior over the objective function."
    },
    {
        "date": "2025-03",
        "title": "Adversarial Robustness of Discriminative Self-Supervised Learning in Vision",
        "author": "\u00d6mer Veysel \u00c7a\u011fatan, \u00d6mer Faruk Tal, and M. Emre G\u00fcrsoy",
        "link": "http://arxiv.org/abs/2503.06361v1",
        "abstract": "Self-supervised learning (SSL) has advanced significantly in visual\nrepresentation learning, yet comprehensive evaluations of its adversarial\nrobustness remain limited. In this study, we evaluate the adversarial\nrobustness of seven discriminative self-supervised models and one supervised\nmodel across diverse tasks, including ImageNet classification, transfer\nlearning, segmentation, and detection. Our findings suggest that discriminative\nSSL models generally exhibit better robustness to adversarial attacks compared\nto their supervised counterpart on ImageNet, with this advantage extending to\ntransfer learning when using linear evaluation. However, when fine-tuning is\napplied, the robustness gap between SSL and supervised models narrows\nconsiderably. Similarly, this robustness advantage diminishes in segmentation\nand detection tasks. We also investigate how various factors might influence\nadversarial robustness, including architectural choices, training duration,\ndata augmentations, and batch sizes. Our analysis contributes to the ongoing\nexploration of adversarial robustness in visual self-supervised representation\nsystems."
    },
    {
        "date": "2025-03",
        "title": "Backdoor Attacks on Discrete Graph Diffusion Models",
        "author": "Jiawen Wang, Samin Karim, Yuan Hong, and Binghui Wang",
        "link": "http://arxiv.org/abs/2503.06340v1",
        "abstract": "Diffusion models are powerful generative models in continuous data domains\nsuch as image and video data. Discrete graph diffusion models (DGDMs) have\nrecently extended them for graph generation, which are crucial in fields like\nmolecule and protein modeling, and obtained the SOTA performance. However, it\nis risky to deploy DGDMs for safety-critical applications (e.g., drug\ndiscovery) without understanding their security vulnerabilities. In this work,\nwe perform the first study on graph diffusion models against backdoor attacks,\na severe attack that manipulates both the training and inference/generation\nphases in graph diffusion models. We first define the threat model, under which\nwe design the attack such that the backdoored graph diffusion model can\ngenerate 1) high-quality graphs without backdoor activation, 2) effective,\nstealthy, and persistent backdoored graphs with backdoor activation, and 3)\ngraphs that are permutation invariant and exchangeable--two core properties in\ngraph generative models. 1) and 2) are validated via empirical evaluations\nwithout and with backdoor defenses, while 3) is validated via theoretical\nresults."
    },
    {
        "date": "2025-03",
        "title": "Advancing Autonomous Vehicle Intelligence: Deep Learning and Multimodal LLM for Traffic Sign Recognition and Robust Lane Detection",
        "author": "Chandan Kumar Sah, Ankit Kumar Shaw, Xiaoli Lian, Arsalan Shahid Baig, Tuopu Wen, Kun Jiang, Mengmeng Yang, and Diange Yang",
        "link": "http://arxiv.org/abs/2503.06313v1",
        "abstract": "Autonomous vehicles (AVs) require reliable traffic sign recognition and\nrobust lane detection capabilities to ensure safe navigation in complex and\ndynamic environments. This paper introduces an integrated approach combining\nadvanced deep learning techniques and Multimodal Large Language Models (MLLMs)\nfor comprehensive road perception. For traffic sign recognition, we\nsystematically evaluate ResNet-50, YOLOv8, and RT-DETR, achieving\nstate-of-the-art performance of 99.8% with ResNet-50, 98.0% accuracy with\nYOLOv8, and achieved 96.6% accuracy in RT-DETR despite its higher computational\ncomplexity. For lane detection, we propose a CNN-based segmentation method\nenhanced by polynomial curve fitting, which delivers high accuracy under\nfavorable conditions. Furthermore, we introduce a lightweight, Multimodal,\nLLM-based framework that directly undergoes instruction tuning using small yet\ndiverse datasets, eliminating the need for initial pretraining. This framework\neffectively handles various lane types, complex intersections, and merging\nzones, significantly enhancing lane detection reliability by reasoning under\nadverse conditions. Despite constraints in available training resources, our\nmultimodal approach demonstrates advanced reasoning capabilities, achieving a\nFrame Overall Accuracy (FRM) of 53.87%, a Question Overall Accuracy (QNS) of\n82.83%, lane detection accuracies of 99.6% in clear conditions and 93.0% at\nnight, and robust performance in reasoning about lane invisibility due to rain\n(88.4%) or road degradation (95.6%). The proposed comprehensive framework\nmarkedly enhances AV perception reliability, thus contributing significantly to\nsafer autonomous driving across diverse and challenging road scenarios."
    }
]