[
    {
        "date": "2025-05",
        "title": "Towards Dataset Copyright Evasion Attack against Personalized Text-to-Image Diffusion Models",
        "author": "Kuofeng Gao, Yufei Zhu, Yiming Li, Jiawang Bai, Yong Yang, Zhifeng Li, and Shu-Tao Xia",
        "link": "http://arxiv.org/abs/2505.02824v1",
        "abstract": "Text-to-image (T2I) diffusion models have rapidly advanced, enabling\nhigh-quality image generation conditioned on textual prompts. However, the\ngrowing trend of fine-tuning pre-trained models for personalization raises\nserious concerns about unauthorized dataset usage. To combat this, dataset\nownership verification (DOV) has emerged as a solution, embedding watermarks\ninto the fine-tuning datasets using backdoor techniques. These watermarks\nremain inactive under benign samples but produce owner-specified outputs when\ntriggered. Despite the promise of DOV for T2I diffusion models, its robustness\nagainst copyright evasion attacks (CEA) remains unexplored. In this paper, we\nexplore how attackers can bypass these mechanisms through CEA, allowing models\nto circumvent watermarks even when trained on watermarked datasets. We propose\nthe first copyright evasion attack (i.e., CEAT2I) specifically designed to\nundermine DOV in T2I diffusion models. Concretely, our CEAT2I comprises three\nstages: watermarked sample detection, trigger identification, and efficient\nwatermark mitigation. A key insight driving our approach is that T2I models\nexhibit faster convergence on watermarked samples during the fine-tuning,\nevident through intermediate feature deviation. Leveraging this, CEAT2I can\nreliably detect the watermarked samples. Then, we iteratively ablate tokens\nfrom the prompts of detected watermarked samples and monitor shifts in\nintermediate features to pinpoint the exact trigger tokens. Finally, we adopt a\nclosed-form concept erasure method to remove the injected watermark. Extensive\nexperiments show that our CEAT2I effectively evades DOV mechanisms while\npreserving model performance."
    },
    {
        "date": "2025-05",
        "title": "Acoustic Side-Channel Attacks on a Computer Mouse",
        "author": "Mauro Conti, Marin Duroyon, Gabriele Orazi, and Gene Tsudik",
        "link": "http://arxiv.org/abs/2505.02725v1",
        "abstract": "Acoustic Side-Channel Attacks (ASCAs) extract sensitive information by using\naudio emitted from a computing devices and their peripherals. Attacks targeting\nkeyboards are popular and have been explored in the literature. However,\nsimilar attacks targeting other human interface peripherals, such as computer\nmice, are under-explored. To this end, this paper considers security leakage\nvia acoustic signals emanating from normal mouse usage. We first confirm\nfeasibility of such attacks by showing a proof-of-concept attack that\nclassifies four mouse movements with 97% accuracy in a controlled environment.\nWe then evolve the attack towards discerning twelve unique mouse movements\nusing a smartphone to record the experiment. Using Machine Learning (ML)\ntechniques, the model is trained on an experiment with six participants to be\ngeneralizable and discern among twelve movements with 94% accuracy. In\naddition, we experiment with an attack that detects a user action of closing a\nfull-screen window on a laptop. Achieving an accuracy of 91%, this experiment\nhighlights exploiting audio leakage from computer mouse movements in a\nrealistic scenario."
    },
    {
        "date": "2025-05",
        "title": "Robustness questions the interpretability of graph neural networks: what to do?",
        "author": "Kirill Lukyanov, Georgii Sazonov, Serafim Boyarsky, and Ilya Makarov",
        "link": "http://arxiv.org/abs/2505.02566v1",
        "abstract": "Graph Neural Networks (GNNs) have become a cornerstone in graph-based data\nanalysis, with applications in diverse domains such as bioinformatics, social\nnetworks, and recommendation systems. However, the interplay between model\ninterpretability and robustness remains poorly understood, especially under\nadversarial scenarios like poisoning and evasion attacks. This paper presents a\ncomprehensive benchmark to systematically analyze the impact of various factors\non the interpretability of GNNs, including the influence of\nrobustness-enhancing defense mechanisms.\n  We evaluate six GNN architectures based on GCN, SAGE, GIN, and GAT across\nfive datasets from two distinct domains, employing four interpretability\nmetrics: Fidelity, Stability, Consistency, and Sparsity. Our study examines how\ndefenses against poisoning and evasion attacks, applied before and during model\ntraining, affect interpretability and highlights critical trade-offs between\nrobustness and interpretability. The framework will be published as open\nsource.\n  The results reveal significant variations in interpretability depending on\nthe chosen defense methods and model architecture characteristics. By\nestablishing a standardized benchmark, this work provides a foundation for\ndeveloping GNNs that are both robust to adversarial threats and interpretable,\nfacilitating trust in their deployment in sensitive applications."
    },
    {
        "date": "2025-05",
        "title": "Antifragility of RIS-assisted Communication Systems under Jamming Attacks",
        "author": "Mounir Bensalem, Thomas R\u00f6thig, and Admela Jukan",
        "link": "http://arxiv.org/abs/2505.02565v1",
        "abstract": "Antifragility of communication systems is defined as measure of benefits\ngained from the adverse events and variability of its environment. In this\npaper, we introduce the notion of antifragility in Reconfigurable Intelligent\nSurface (RIS) assisted communication systems affected by a jamming attack. We\nanalyzed the antifragility of the two hop systems, where the wireless path\ncontains source node, RIS, destination node, and a eavesdropping/jamming node.\nWe propose and analyze the antifragility performance for several jamming\nmodels, such as Digital Radio Frequency Memory (DRFM) and phase and amplitude\nshifting. Our paper shows that antifragility throughput can indeed be achieved\nunder certain power thresholds and for various jamming models. In particular,\nhigh jamming power combined with low baseline data rates yields an antifragile\ngain factor of approximately five times. The results confirm that\nreconfigurable intelligent surfaces, when coupled with an antifragile design\nphilosophy, can convert hostile interference from a liability into a throughput\ngain."
    },
    {
        "date": "2025-05",
        "title": "Robust Duality Learning for Unsupervised Visible-Infrared Person Re-Identification",
        "author": "Yongxiang Li, Yuan Sun, Yang Qin, Dezhong Peng, Xi Peng, and Peng Hu",
        "link": "http://arxiv.org/abs/2505.02549v2",
        "abstract": "Unsupervised visible-infrared person re-identification (UVI-ReID) aims to\nretrieve pedestrian images across different modalities without costly\nannotations, but faces challenges due to the modality gap and lack of\nsupervision. Existing methods often adopt self-training with\nclustering-generated pseudo-labels but implicitly assume these labels are\nalways correct. In practice, however, this assumption fails due to inevitable\npseudo-label noise, which hinders model learning. To address this, we introduce\na new learning paradigm that explicitly considers Pseudo-Label Noise (PLN),\ncharacterized by three key challenges: noise overfitting, error accumulation,\nand noisy cluster correspondence. To this end, we propose a novel Robust\nDuality Learning framework (RoDE) for UVI-ReID to mitigate the effects of noisy\npseudo-labels. First, to combat noise overfitting, a Robust Adaptive Learning\nmechanism (RAL) is proposed to dynamically emphasize clean samples while\ndown-weighting noisy ones. Second, to alleviate error accumulation-where the\nmodel reinforces its own mistakes-RoDE employs dual distinct models that are\nalternately trained using pseudo-labels from each other, encouraging diversity\nand preventing collapse. However, this dual-model strategy introduces\nmisalignment between clusters across models and modalities, creating noisy\ncluster correspondence. To resolve this, we introduce Cluster Consistency\nMatching (CCM), which aligns clusters across models and modalities by measuring\ncross-cluster similarity. Extensive experiments on three benchmarks demonstrate\nthe effectiveness of RoDE."
    },
    {
        "date": "2025-05",
        "title": "RobSurv: Vector Quantization-Based Multi-Modal Learning for Robust Cancer Survival Prediction",
        "author": "Aiman Farooq, Azad Singh, Deepak Mishra, and Santanu Chaudhury",
        "link": "http://arxiv.org/abs/2505.02529v1",
        "abstract": "Cancer survival prediction using multi-modal medical imaging presents a\ncritical challenge in oncology, mainly due to the vulnerability of deep\nlearning models to noise and protocol variations across imaging centers.\nCurrent approaches struggle to extract consistent features from heterogeneous\nCT and PET images, limiting their clinical applicability. We address these\nchallenges by introducing RobSurv, a robust deep-learning framework that\nleverages vector quantization for resilient multi-modal feature learning. The\nkey innovation of our approach lies in its dual-path architecture: one path\nmaps continuous imaging features to learned discrete codebooks for\nnoise-resistant representation, while the parallel path preserves fine-grained\ndetails through continuous feature processing. This dual representation is\nintegrated through a novel patch-wise fusion mechanism that maintains local\nspatial relationships while capturing global context via Transformer-based\nprocessing. In extensive evaluations across three diverse datasets (HECKTOR,\nH\\&N1, and NSCLC Radiogenomics), RobSurv demonstrates superior performance,\nachieving concordance index of 0.771, 0.742, and 0.734 respectively -\nsignificantly outperforming existing methods. Most notably, our model maintains\nrobust performance even under severe noise conditions, with performance\ndegradation of only 3.8-4.5\\% compared to 8-12\\% in baseline methods. These\nresults, combined with strong generalization across different cancer types and\nimaging protocols, establish RobSurv as a promising solution for reliable\nclinical prognosis that can enhance treatment planning and patient care."
    },
    {
        "date": "2025-05",
        "title": "Bayesian Robust Aggregation for Federated Learning",
        "author": "Aleksandr Karakulev, Usama Zafar, Salman Toor, and Prashant Singh",
        "link": "http://arxiv.org/abs/2505.02490v1",
        "abstract": "Federated Learning enables collaborative training of machine learning models\non decentralized data. This scheme, however, is vulnerable to adversarial\nattacks, when some of the clients submit corrupted model updates. In real-world\nscenarios, the total number of compromised clients is typically unknown, with\nthe extent of attacks potentially varying over time. To address these\nchallenges, we propose an adaptive approach for robust aggregation of model\nupdates based on Bayesian inference. The mean update is defined by the maximum\nof the likelihood marginalized over probabilities of each client to be\n`honest'. As a result, the method shares the simplicity of the classical\naverage estimators (e.g., sample mean or geometric median), being independent\nof the number of compromised clients. At the same time, it is as effective\nagainst attacks as methods specifically tailored to Federated Learning, such as\nKrum. We compare our approach with other aggregation schemes in federated\nsetting on three benchmark image classification data sets. The proposed method\nconsistently achieves state-of-the-art performance across various attack types\nwith static and varying number of malicious clients."
    },
    {
        "date": "2025-05",
        "title": "FairPO: Robust Preference Optimization for Fair Multi-Label Learning",
        "author": "Soumen Kumar Mondal, Akshit Varmora, Prateek Chanda, and Ganesh Ramakrishnan",
        "link": "http://arxiv.org/abs/2505.02433v1",
        "abstract": "We propose FairPO, a novel framework designed to promote fairness in\nmulti-label classification by directly optimizing preference signals with a\ngroup robustness perspective. In our framework, the set of labels is\npartitioned into privileged and non-privileged groups, and a preference-based\nloss inspired by Direct Preference Optimization (DPO) is employed to more\neffectively differentiate true positive labels from confusing negatives within\nthe privileged group, while preserving baseline classification performance for\nnon-privileged labels. By framing the learning problem as a robust optimization\nover groups, our approach dynamically adjusts the training emphasis toward\ngroups with poorer performance, thereby mitigating bias and ensuring a fairer\ntreatment across diverse label categories. In addition, we outline plans to\nextend this approach by investigating alternative loss formulations such as\nSimple Preference Optimisation (SimPO) and Contrastive Preference Optimization\n(CPO) to exploit reference-free reward formulations and contrastive training\nsignals. Furthermore, we plan to extend FairPO with multilabel generation\ncapabilities, enabling the model to dynamically generate diverse and coherent\nlabel sets for ambiguous inputs."
    },
    {
        "date": "2025-05",
        "title": "Catastrophic Overfitting, Entropy Gap and Participation Ratio: A Noiseless $l^p$ Norm Solution for Fast Adversarial Training",
        "author": "Fares B. Mehouachi, and Saif Eddin Jabari",
        "link": "http://arxiv.org/abs/2505.02360v1",
        "abstract": "Adversarial training is a cornerstone of robust deep learning, but fast\nmethods like the Fast Gradient Sign Method (FGSM) often suffer from\nCatastrophic Overfitting (CO), where models become robust to single-step\nattacks but fail against multi-step variants. While existing solutions rely on\nnoise injection, regularization, or gradient clipping, we propose a novel\nsolution that purely controls the $l^p$ training norm to mitigate CO.\n  Our study is motivated by the empirical observation that CO is more prevalent\nunder the $l^{\\infty}$ norm than the $l^2$ norm. Leveraging this insight, we\ndevelop a framework for generalized $l^p$ attack as a fixed point problem and\ncraft $l^p$-FGSM attacks to understand the transition mechanics from $l^2$ to\n$l^{\\infty}$. This leads to our core insight: CO emerges when highly\nconcentrated gradients where information localizes in few dimensions interact\nwith aggressive norm constraints. By quantifying gradient concentration through\nParticipation Ratio and entropy measures, we develop an adaptive $l^p$-FGSM\nthat automatically tunes the training norm based on gradient information.\nExtensive experiments demonstrate that this approach achieves strong robustness\nwithout requiring additional regularization or noise injection, providing a\nnovel and theoretically-principled pathway to mitigate the CO problem."
    },
    {
        "date": "2025-05",
        "title": "Temporal Robustness in Discrete Time Linear Dynamical Systems",
        "author": "Nilava Metya, and Arunesh Sinha",
        "link": "http://arxiv.org/abs/2505.02347v1",
        "abstract": "Discrete time linear dynamical systems, including Markov chains, have found\nmany applications. However, in some problems, there is uncertainty about the\ntime horizon for which the system runs. This creates uncertainty about the cost\n(or reward) incurred based on the state distribution when the system stops.\nGiven past data samples of how long a system ran, we propose to theoretically\nanalyze a distributional robust cost estimation task in a Wasserstein ambiguity\nset, instead of learning a probability distribution from a few samples. Towards\nthis, we show an equivalence between a discrete time Markov Chain on a\nprobability simplex and a global asymptotic stable (GAS) discrete time linear\ndynamical system, allowing us to base our study on a GAS system only. Then, we\nprovide various polynomial time algorithms and hardness results for different\ncases in our theoretical study, including a fundamental result about\nWasserstein distance based polytope."
    },
    {
        "date": "2025-05",
        "title": "Adaptive Scoring and Thresholding with Human Feedback for Robust Out-of-Distribution Detection",
        "author": "Daisuke Yamada, Harit Vishwakarma, and Ramya Korlakai Vinayak",
        "link": "http://arxiv.org/abs/2505.02299v1",
        "abstract": "Machine Learning (ML) models are trained on in-distribution (ID) data but\noften encounter out-of-distribution (OOD) inputs during deployment -- posing\nserious risks in safety-critical domains. Recent works have focused on\ndesigning scoring functions to quantify OOD uncertainty, with score thresholds\ntypically set based solely on ID data to achieve a target true positive rate\n(TPR), since OOD data is limited before deployment. However, these TPR-based\nthresholds leave false positive rates (FPR) uncontrolled, often resulting in\nhigh FPRs where OOD points are misclassified as ID. Moreover, fixed scoring\nfunctions and thresholds lack the adaptivity needed to handle newly observed,\nevolving OOD inputs, leading to sub-optimal performance. To address these\nchallenges, we propose a human-in-the-loop framework that \\emph{safely updates\nboth scoring functions and thresholds on the fly} based on real-world OOD\ninputs. Our method maximizes TPR while strictly controlling FPR at all times,\neven as the system adapts over time. We provide theoretical guarantees for FPR\ncontrol under stationary conditions and present extensive empirical evaluations\non OpenOOD benchmarks to demonstrate that our approach outperforms existing\nmethods by achieving higher TPRs while maintaining FPR control."
    },
    {
        "date": "2025-05",
        "title": "Robust Localization, Mapping, and Navigation for Quadruped Robots",
        "author": "Dyuman Aditya, Junning Huang, Nico Bohlinger, Piotr Kicki, Krzysztof Walas, Jan Peters, Matteo Luperto, and Davide Tateo",
        "link": "http://arxiv.org/abs/2505.02272v1",
        "abstract": "Quadruped robots are currently a widespread platform for robotics research,\nthanks to powerful Reinforcement Learning controllers and the availability of\ncheap and robust commercial platforms. However, to broaden the adoption of the\ntechnology in the real world, we require robust navigation stacks relying only\non low-cost sensors such as depth cameras. This paper presents a first step\ntowards a robust localization, mapping, and navigation system for low-cost\nquadruped robots. In pursuit of this objective we combine contact-aided\nkinematic, visual-inertial odometry, and depth-stabilized vision, enhancing\nstability and accuracy of the system. Our results in simulation and two\ndifferent real-world quadruped platforms show that our system can generate an\naccurate 2D map of the environment, robustly localize itself, and navigate\nautonomously. Furthermore, we present in-depth ablation studies of the\nimportant components of the system and their impact on localization accuracy.\nVideos, code, and additional experiments can be found on the project website:\nhttps://sites.google.com/view/low-cost-quadruped-slam"
    },
    {
        "date": "2025-05",
        "title": "Enhanced Outsourced and Secure Inference for Tall Sparse Decision Trees",
        "author": "Andrew Quijano, Spyros T. Halkidis, Kevin Gallagher, Kemal Akkaya, and Nikolaos Samaras",
        "link": "http://arxiv.org/abs/2505.02224v1",
        "abstract": "A decision tree is an easy-to-understand tool that has been widely used for\nclassification tasks. On the one hand, due to privacy concerns, there has been\nan urgent need to create privacy-preserving classifiers that conceal the user's\ninput from the classifier. On the other hand, with the rise of cloud computing,\ndata owners are keen to reduce risk by outsourcing their model, but want\nsecurity guarantees that third parties cannot steal their decision tree model.\nTo address these issues, Joye and Salehi introduced a theoretical protocol that\nefficiently evaluates decision trees while maintaining privacy by leveraging\ntheir comparison protocol that is resistant to timing attacks. However, their\napproach was not only inefficient but also prone to side-channel attacks.\nTherefore, in this paper, we propose a new decision tree inference protocol in\nwhich the model is shared and evaluated among multiple entities. We partition\nour decision tree model by each level to be stored in a new entity we refer to\nas a \"level-site.\" Utilizing this approach, we were able to gain improved\naverage run time for classifier evaluation for a non-complete tree, while also\nhaving strong mitigations against side-channel attacks."
    },
    {
        "date": "2025-05",
        "title": "Robust AI-Generated Face Detection with Imbalanced Data",
        "author": "Yamini Sri Krubha, Aryana Hou, Braden Vester, Web Walker, Xin Wang, Li Lin, and Shu Hu",
        "link": "http://arxiv.org/abs/2505.02182v1",
        "abstract": "Deepfakes, created using advanced AI techniques such as Variational\nAutoencoder and Generative Adversarial Networks, have evolved from research and\nentertainment applications into tools for malicious activities, posing\nsignificant threats to digital trust. Current deepfake detection techniques\nhave evolved from CNN-based methods focused on local artifacts to more advanced\napproaches using vision transformers and multimodal models like CLIP, which\ncapture global anomalies and improve cross-domain generalization. Despite\nrecent progress, state-of-the-art deepfake detectors still face major\nchallenges in handling distribution shifts from emerging generative models and\naddressing severe class imbalance between authentic and fake samples in\ndeepfake datasets, which limits their robustness and detection accuracy. To\naddress these challenges, we propose a framework that combines dynamic loss\nreweighting and ranking-based optimization, which achieves superior\ngeneralization and performance under imbalanced dataset conditions. The code is\navailable at https://github.com/Purdue-M2/SP_CUP."
    },
    {
        "date": "2025-05",
        "title": "Saliency-Guided Training for Fingerprint Presentation Attack Detection",
        "author": "Samuel Webster, and Adam Czajka",
        "link": "http://arxiv.org/abs/2505.02176v1",
        "abstract": "Saliency-guided training, which directs model learning to important regions\nof images, has demonstrated generalization improvements across various\nbiometric presentation attack detection (PAD) tasks. This paper presents its\nfirst application to fingerprint PAD. We conducted a 50-participant study to\ncreate a dataset of 800 human-annotated fingerprint perceptually-important\nmaps, explored alongside algorithmically-generated \"pseudosaliency,\" including\nminutiae-based, image quality-based, and autoencoder-based saliency maps.\nEvaluating on the 2021 Fingerprint Liveness Detection Competition testing set,\nwe explore various configurations within five distinct training scenarios to\nassess the impact of saliency-guided training on accuracy and generalization.\nOur findings demonstrate the effectiveness of saliency-guided training for\nfingerprint PAD in both limited and large data contexts, and we present a\nconfiguration capable of earning the first place on the LivDet-2021 benchmark.\nOur results highlight saliency-guided training's promise for increased model\ngeneralization capabilities, its effectiveness when data is limited, and its\npotential to scale to larger datasets in fingerprint PAD. All collected\nsaliency data and trained models are released with the paper to support\nreproducible research."
    },
    {
        "date": "2025-05",
        "title": "Adversarial Cooperative Rationalization: The Risk of Spurious Correlations in Even Clean Datasets",
        "author": "Wei Liu, Zhongyu Niu, Lang Gao, Zhiying Deng, Jun Wang, Haozhao Wang, and Ruixuan Li",
        "link": "http://arxiv.org/abs/2505.02118v2",
        "abstract": "This study investigates the self-rationalization framework constructed with a\ncooperative game, where a generator initially extracts the most informative\nsegment from raw input, and a subsequent predictor utilizes the selected subset\nfor its input. The generator and predictor are trained collaboratively to\nmaximize prediction accuracy. In this paper, we first uncover a potential\ncaveat: such a cooperative game could unintentionally introduce a sampling bias\nduring rationale extraction. Specifically, the generator might inadvertently\ncreate an incorrect correlation between the selected rationale candidate and\nthe label, even when they are semantically unrelated in the original dataset.\nSubsequently, we elucidate the origins of this bias using both detailed\ntheoretical analysis and empirical evidence. Our findings suggest a direction\nfor inspecting these correlations through attacks, based on which we further\nintroduce an instruction to prevent the predictor from learning the\ncorrelations. Through experiments on six text classification datasets and two\ngraph classification datasets using three network architectures (GRUs, BERT,\nand GCN), we show that our method not only significantly outperforms recent\nrationalization methods, but also achieves comparable or even better results\nthan a representative LLM (llama3.1-8b-instruct)."
    },
    {
        "date": "2025-05",
        "title": "SkillMimic-V2: Learning Robust and Generalizable Interaction Skills from Sparse and Noisy Demonstrations",
        "author": "Runyi Yu, Yinhuai Wang, Qihan Zhao, Hok Wai Tsui, Jingbo Wang, Ping Tan, and Qifeng Chen",
        "link": "http://arxiv.org/abs/2505.02094v1",
        "abstract": "We address a fundamental challenge in Reinforcement Learning from Interaction\nDemonstration (RLID): demonstration noise and coverage limitations. While\nexisting data collection approaches provide valuable interaction\ndemonstrations, they often yield sparse, disconnected, and noisy trajectories\nthat fail to capture the full spectrum of possible skill variations and\ntransitions. Our key insight is that despite noisy and sparse demonstrations,\nthere exist infinite physically feasible trajectories that naturally bridge\nbetween demonstrated skills or emerge from their neighboring states, forming a\ncontinuous space of possible skill variations and transitions. Building upon\nthis insight, we present two data augmentation techniques: a Stitched\nTrajectory Graph (STG) that discovers potential transitions between\ndemonstration skills, and a State Transition Field (STF) that establishes\nunique connections for arbitrary states within the demonstration neighborhood.\nTo enable effective RLID with augmented data, we develop an Adaptive Trajectory\nSampling (ATS) strategy for dynamic curriculum generation and a historical\nencoding mechanism for memory-dependent skill learning. Our approach enables\nrobust skill acquisition that significantly generalizes beyond the reference\ndemonstrations. Extensive experiments across diverse interaction tasks\ndemonstrate substantial improvements over state-of-the-art methods in terms of\nconvergence stability, generalization capability, and recovery robustness."
    },
    {
        "date": "2025-05",
        "title": "Open Challenges in Multi-Agent Security: Towards Secure Systems of Interacting AI Agents",
        "author": "Christian Schroeder de Witt",
        "link": "http://arxiv.org/abs/2505.02077v1",
        "abstract": "Decentralized AI agents will soon interact across internet platforms,\ncreating security challenges beyond traditional cybersecurity and AI safety\nframeworks. Free-form protocols are essential for AI's task generalization but\nenable new threats like secret collusion and coordinated swarm attacks. Network\neffects can rapidly spread privacy breaches, disinformation, jailbreaks, and\ndata poisoning, while multi-agent dispersion and stealth optimization help\nadversaries evade oversightcreating novel persistent threats at a systemic\nlevel. Despite their critical importance, these security challenges remain\nunderstudied, with research fragmented across disparate fields including AI\nsecurity, multi-agent learning, complex systems, cybersecurity, game theory,\ndistributed systems, and technical AI governance. We introduce\n\\textbf{multi-agent security}, a new field dedicated to securing networks of\ndecentralized AI agents against threats that emerge or amplify through their\ninteractionswhether direct or indirect via shared environmentswith each other,\nhumans, and institutions, and characterize fundamental security-performance\ntrade-offs. Our preliminary work (1) taxonomizes the threat landscape arising\nfrom interacting AI agents, (2) surveys security-performance tradeoffs in\ndecentralized AI systems, and (3) proposes a unified research agenda addressing\nopen challenges in designing secure agent systems and interaction environments.\nBy identifying these gaps, we aim to guide research in this critical area to\nunlock the socioeconomic potential of large-scale agent deployment on the\ninternet, foster public trust, and mitigate national security risks in critical\ninfrastructure and defense contexts."
    },
    {
        "date": "2025-05",
        "title": "Lightweight Defense Against Adversarial Attacks in Time Series Classification",
        "author": "Yi Han",
        "link": "http://arxiv.org/abs/2505.02073v1",
        "abstract": "As time series classification (TSC) gains prominence, ensuring robust TSC\nmodels against adversarial attacks is crucial. While adversarial defense is\nwell-studied in Computer Vision (CV), the TSC field has primarily relied on\nadversarial training (AT), which is computationally expensive. In this paper,\nfive data augmentation-based defense methods tailored for time series are\ndeveloped, with the most computationally intensive method among them increasing\nthe computational resources by only 14.07% compared to the original TSC model.\nMoreover, the deployment process for these methods is straightforward. By\nleveraging these advantages of our methods, we create two combined methods. One\nof these methods is an ensemble of all the proposed techniques, which not only\nprovides better defense performance than PGD-based AT but also enhances the\ngeneralization ability of TSC models. Moreover, the computational resources\nrequired for our ensemble are less than one-third of those required for\nPGD-based AT. These methods advance robust TSC in data mining. Furthermore, as\nfoundation models are increasingly explored for time series feature learning,\nour work provides insights into integrating data augmentation-based adversarial\ndefense with large-scale pre-trained models in future research."
    },
    {
        "date": "2025-05",
        "title": "Triple-identity Authentication: The Future of Secure Access",
        "author": "Suyun Borjigin",
        "link": "http://arxiv.org/abs/2505.02004v1",
        "abstract": "In a typical authentication process, the local system verifies the user's\nidentity using a stored hash value generated by a cross-system hash algorithm.\nThis article shifts the research focus from traditional password encryption to\nthe establishment of gatekeeping mechanisms for effective interactions between\na system and the outside world. Here, we propose a triple-identity\nauthentication system to achieve this goal. Specifically, this local system\nopens the inner structure of its hash algorithm to all user credentials,\nincluding the login name, login password, and authentication password. When a\nlogin credential is entered, the local system hashes it and then creates a\nunique identifier using intermediate hash elements randomly selected from the\nopen algorithm. Importantly, this locally generated unique identifier (rather\nthan the stored hash produced by the open algorithm) is utilized to verify the\nuser's combined identity, which is generated by combining the entered\ncredential with the International Mobile Equipment Identity and the\nInternational Mobile Subscriber Identity. The verification process is\nimplemented at each interaction point: the login name field, the login password\nfield, and the server's authentication point. Thus, within the context of this\ntriple-identity authentication system, we establish a robust gatekeeping\nmechanism for system interactions, ultimately providing a level of security\nthat is equivalent to multi-factor authentication."
    },
    {
        "date": "2025-05",
        "title": "Adversarial Robustness of Deep Learning Models for Inland Water Body Segmentation from SAR Images",
        "author": "Siddharth Kothari, Srinivasan Murali, Sankalp Kothari, Ujjwal Verma, and Jaya Sreevalsan-Nair",
        "link": "http://arxiv.org/abs/2505.01884v2",
        "abstract": "Inland water body segmentation from Synthetic Aperture Radar (SAR) images is\nan important task needed for several applications, such as flood mapping. While\nSAR sensors capture data in all-weather conditions as high-resolution images,\ndifferentiating water and water-like surfaces from SAR images is not\nstraightforward. Inland water bodies, such as large river basins, have complex\ngeometry, which adds to the challenge of segmentation. U-Net is a widely used\ndeep learning model for land-water segmentation of SAR images. In practice,\nmanual annotation is often used to generate the corresponding water masks as\nground truth. Manual annotation of the images is prone to label noise owing to\ndata poisoning attacks, especially due to complex geometry. In this work, we\nsimulate manual errors in the form of adversarial attacks on the U-Net model\nand study the robustness of the model to human errors in annotation. Our\nresults indicate that U-Net can tolerate a certain level of corruption before\nits performance drops significantly. This finding highlights the crucial role\nthat the quality of manual annotations plays in determining the effectiveness\nof the segmentation model. The code and the new dataset, along with adversarial\nexamples for robust training, are publicly available. (GitHub link -\nhttps://github.com/GVCL/IWSeg-SAR-Poison.git)"
    },
    {
        "date": "2025-05",
        "title": "PhysNav-DG: A Novel Adaptive Framework for Robust VLM-Sensor Fusion in Navigation Applications",
        "author": "Trisanth Srinivasan, and Santosh Patapati",
        "link": "http://arxiv.org/abs/2505.01881v1",
        "abstract": "Robust navigation in diverse environments and domains requires both accurate\nstate estimation and transparent decision making. We present PhysNav-DG, a\nnovel framework that integrates classical sensor fusion with the semantic power\nof vision-language models. Our dual-branch architecture predicts navigation\nactions from multi-sensor inputs while simultaneously generating detailed\nchain-of-thought explanations. A modified Adaptive Kalman Filter dynamically\nadjusts its noise parameters based on environmental context. It leverages\nseveral streams of raw sensor data along with semantic insights from models\nsuch as LLaMA 3.2 11B and BLIP-2. To evaluate our approach, we introduce the\nMD-NEX Benchmark, a novel multi-domain dataset that unifies indoor navigation,\nautonomous driving, and social navigation tasks with ground-truth actions and\nhuman-validated explanations. Extensive experiments and ablations show that\nPhysNav-DG improves navigation success rates by over 20% and achieves high\nefficiency, with explanations that are both highly grounded and clear. This\nwork connects high-level semantic reasoning and geometric planning for safer\nand more trustworthy autonomous systems."
    },
    {
        "date": "2025-05",
        "title": "PQS-BFL: A Post-Quantum Secure Blockchain-based Federated Learning Framework",
        "author": "Daniel Commey, and Garth V. Crosby",
        "link": "http://arxiv.org/abs/2505.01866v1",
        "abstract": "Federated Learning (FL) enables collaborative model training while preserving\ndata privacy, but its classical cryptographic underpinnings are vulnerable to\nquantum attacks. This vulnerability is particularly critical in sensitive\ndomains like healthcare. This paper introduces PQS-BFL (Post-Quantum Secure\nBlockchain-based Federated Learning), a framework integrating post-quantum\ncryptography (PQC) with blockchain verification to secure FL against quantum\nadversaries. We employ ML-DSA-65 (a FIPS 204 standard candidate, formerly\nDilithium) signatures to authenticate model updates and leverage optimized\nsmart contracts for decentralized validation. Extensive evaluations on diverse\ndatasets (MNIST, SVHN, HAR) demonstrate that PQS-BFL achieves efficient\ncryptographic operations (average PQC sign time: 0.65 ms, verify time: 0.53 ms)\nwith a fixed signature size of 3309 Bytes. Blockchain integration incurs a\nmanageable overhead, with average transaction times around 4.8 s and gas usage\nper update averaging 1.72 x 10^6 units for PQC configurations. Crucially, the\ncryptographic overhead relative to transaction time remains minimal (around\n0.01-0.02% for PQC with blockchain), confirming that PQC performance is not the\nbottleneck in blockchain-based FL. The system maintains competitive model\naccuracy (e.g., over 98.8% for MNIST with PQC) and scales effectively, with\nround times showing sublinear growth with increasing client numbers. Our\nopen-source implementation and reproducible benchmarks validate the feasibility\nof deploying long-term, quantum-resistant security in practical FL systems."
    },
    {
        "date": "2025-05",
        "title": "Rogue Cell: Adversarial Attack and Defense in Untrusted O-RAN Setup Exploiting the Traffic Steering xApp",
        "author": "Eran Aizikovich, Dudu Mimran, Edita Grolman, Yuval Elovici, and Asaf Shabtai",
        "link": "http://arxiv.org/abs/2505.01816v1",
        "abstract": "The Open Radio Access Network (O-RAN) architecture is revolutionizing\ncellular networks with its open, multi-vendor design and AI-driven management,\naiming to enhance flexibility and reduce costs. Although it has many\nadvantages, O-RAN is not threat-free. While previous studies have mainly\nexamined vulnerabilities arising from O-RAN's intelligent components, this\npaper is the first to focus on the security challenges and vulnerabilities\nintroduced by transitioning from single-operator to multi-operator RAN\narchitectures. This shift increases the risk of untrusted third-party operators\nmanaging different parts of the network. To explore these vulnerabilities and\ntheir potential mitigation, we developed an open-access testbed environment\nthat integrates a wireless network simulator with the official O-RAN Software\nCommunity (OSC) RAN intelligent component (RIC) cluster. This environment\nenables realistic, live data collection and serves as a platform for\ndemonstrating APATE (adversarial perturbation against traffic efficiency), an\nevasion attack in which a malicious cell manipulates its reported key\nperformance indicators (KPIs) and deceives the O-RAN traffic steering to gain\nunfair allocations of user equipment (UE). To ensure that O-RAN's legitimate\nactivity continues, we introduce MARRS (monitoring adversarial RAN reports), a\ndetection framework based on a long-short term memory (LSTM) autoencoder (AE)\nthat learns contextual features across the network to monitor malicious\ntelemetry (also demonstrated in our testbed). Our evaluation showed that by\nexecuting APATE, an attacker can obtain a 248.5% greater UE allocation than it\nwas supposed to in a benign scenario. In addition, the MARRS detection method\nwas also shown to successfully classify malicious cell activity, achieving\naccuracy of 99.2% and an F1 score of 0.978."
    },
    {
        "date": "2025-05",
        "title": "$\\textit{New News}$: System-2 Fine-tuning for Robust Integration of New Knowledge",
        "author": "Core Francisco Park, Zechen Zhang, and Hidenori Tanaka",
        "link": "http://arxiv.org/abs/2505.01812v1",
        "abstract": "Humans and intelligent animals can effortlessly internalize new information\n(\"news\") and accurately extract the implications for performing downstream\ntasks. While large language models (LLMs) can achieve this through in-context\nlearning (ICL) when the news is explicitly given as context, fine-tuning\nremains challenging for the models to consolidate learning in weights. In this\npaper, we introduce $\\textit{New News}$, a dataset composed of hypothetical yet\nplausible news spanning multiple domains (mathematics, coding, discoveries,\nleaderboards, events), accompanied by downstream evaluation questions whose\ncorrect answers critically depend on understanding and internalizing the news.\nWe first demonstrate a substantial gap between naive fine-tuning and in-context\nlearning (FT-ICL gap) on our news dataset. To address this gap, we explore a\nsuite of self-play data generation protocols -- paraphrases, implications and\nSelf-QAs -- designed to distill the knowledge from the model with context into\nthe weights of the model without the context, which we term $\\textit{System-2\nFine-tuning}$ (Sys2-FT). We systematically evaluate ICL and Sys2-FT performance\nacross data domains and model scales with the Qwen 2.5 family of models. Our\nresults demonstrate that the self-QA protocol of Sys2-FT significantly improves\nmodels' in-weight learning of the news. Furthermore, we discover the\n$\\textit{contexual shadowing effect}$, where training with the news $\\textit{in\ncontext}$ followed by its rephrases or QAs degrade learning of the news.\nFinally, we show preliminary evidence of an emerging scaling law of Sys2-FT."
    },
    {
        "date": "2025-05",
        "title": "Backdoor Attacks Against Patch-based Mixture of Experts",
        "author": "Cedric Chan, Jona te Lintelo, and Stjepan Picek",
        "link": "http://arxiv.org/abs/2505.01811v1",
        "abstract": "As Deep Neural Networks (DNNs) continue to require larger amounts of data and\ncomputational power, Mixture of Experts (MoE) models have become a popular\nchoice to reduce computational complexity. This popularity increases the\nimportance of considering the security of MoE architectures. Unfortunately, the\nsecurity of models using a MoE architecture has not yet gained much attention\ncompared to other DNN models. In this work, we investigate the vulnerability of\npatch-based MoE (pMoE) models for image classification against backdoor\nattacks. We examine multiple trigger generation methods and Fine-Pruning as a\ndefense. To better understand a pMoE model's vulnerability to backdoor attacks,\nwe investigate which factors affect the model's patch selection. Our work shows\nthat pMoE models are highly susceptible to backdoor attacks. More precisely, we\nachieve high attack success rates of up to 100% with visible triggers and a 2%\npoisoning rate, whilst only having a clean accuracy drop of 1.0%. Additionally,\nwe show that pruning itself is ineffective as a defense but that fine-tuning\ncan remove the backdoor almost completely. Our results show that fine-tuning\nthe model for five epochs reduces the attack success rate to 2.1% whilst\nsacrificing 1.4% accuracy."
    },
    {
        "date": "2025-05",
        "title": "Multimodal Graph Representation Learning for Robust Surgical Workflow Recognition with Adversarial Feature Disentanglement",
        "author": "Long Bai, Boyi Ma, Ruohan Wang, Guankun Wang, Beilei Cui, Zhongliang Jiang, Mobarakol Islam, Zhe Min, Jiewen Lai, Nassir Navab, and Hongliang Ren",
        "link": "http://arxiv.org/abs/2505.01766v1",
        "abstract": "Surgical workflow recognition is vital for automating tasks, supporting\ndecision-making, and training novice surgeons, ultimately improving patient\nsafety and standardizing procedures. However, data corruption can lead to\nperformance degradation due to issues like occlusion from bleeding or smoke in\nsurgical scenes and problems with data storage and transmission. In this case,\nwe explore a robust graph-based multimodal approach to integrating vision and\nkinematic data to enhance accuracy and reliability. Vision data captures\ndynamic surgical scenes, while kinematic data provides precise movement\ninformation, overcoming limitations of visual recognition under adverse\nconditions. We propose a multimodal Graph Representation network with\nAdversarial feature Disentanglement (GRAD) for robust surgical workflow\nrecognition in challenging scenarios with domain shifts or corrupted data.\nSpecifically, we introduce a Multimodal Disentanglement Graph Network that\ncaptures fine-grained visual information while explicitly modeling the complex\nrelationships between vision and kinematic embeddings through graph-based\nmessage modeling. To align feature spaces across modalities, we propose a\nVision-Kinematic Adversarial framework that leverages adversarial training to\nreduce modality gaps and improve feature consistency. Furthermore, we design a\nContextual Calibrated Decoder, incorporating temporal and contextual priors to\nenhance robustness against domain shifts and corrupted data. Extensive\ncomparative and ablation experiments demonstrate the effectiveness of our model\nand proposed modules. Moreover, our robustness experiments show that our method\neffectively handles data corruption during storage and transmission, exhibiting\nexcellent stability and robustness. Our approach aims to advance automated\nsurgical workflow recognition, addressing the complexities and dynamism\ninherent in surgical procedures."
    },
    {
        "date": "2025-05",
        "title": "Inducing Robustness in a 2 Dimensional Direct Preference Optimization Paradigm",
        "author": "Sarvesh Shashidhar, Ritik, Nachiketa Patil, Suraj Racha, and Ganesh Ramakrishnan",
        "link": "http://arxiv.org/abs/2505.01706v1",
        "abstract": "Direct Preference Optimisation (DPO) has emerged as a powerful method for\naligning Large Language Models (LLMs) with human preferences, offering a stable\nand efficient alternative to approaches that use Reinforcement learning via\nHuman Feedback. In this work, we investigate the performance of DPO using\nopen-source preference datasets. One of the major drawbacks of DPO is that it\ndoesn't induce granular scoring and treats all the segments of the responses\nwith equal propensity. However, this is not practically true for human\npreferences since even \"good\" responses have segments that may not be preferred\nby the annotator. To resolve this, a 2-dimensional scoring for DPO alignment\ncalled 2D-DPO was proposed. We explore the 2D-DPO alignment paradigm and the\nadvantages it provides over the standard DPO by comparing their win rates. It\nis observed that these methods, even though effective, are not robust to\nlabel/score noise. To counter this, we propose an approach of incorporating\nsegment-level score noise robustness to the 2D-DPO algorithm. Along with\ntheoretical backing, we also provide empirical verification in favour of the\nalgorithm and introduce other noise models that can be present."
    },
    {
        "date": "2025-05",
        "title": "Rubber Mallet: A Study of High Frequency Localized Bit Flips and Their Impact on Security",
        "author": "Andrew Adiletta, Zane Weissman, Fatemeh Khojasteh Dana, Berk Sunar, and Shahin Tajik",
        "link": "http://arxiv.org/abs/2505.01518v1",
        "abstract": "The increasing density of modern DRAM has heightened its vulnerability to\nRowhammer attacks, which induce bit flips by repeatedly accessing specific\nmemory rows. This paper presents an analysis of bit flip patterns generated by\nadvanced Rowhammer techniques that bypass existing hardware defenses. First, we\ninvestigate the phenomenon of adjacent bit flips--where two or more physically\nneighboring bits are corrupted simultaneously--and demonstrate they occur with\nsignificantly higher frequency than previously documented. We also show that if\nmultiple bits flip within a byte, they are more likely to be adjacent than\nrandomly distributed: for example, if 4 bits flip within a byte, there is an\n87% chance that they are all adjacent. We also demonstrate that bit flips\nwithin a row will naturally cluster together likely due to the underlying\nphysics of the attack. We then investigate two fault injection attacks enabled\nby multiple adjacent or nearby bit flips. First, we show how these correlated\nflips enable efficient cryptographic signature correction attacks, successfully\nrecovering ECDSA private keys from OpenSSL implementations where single-bit\napproaches would be unfeasible. Second, we introduce a targeted attack against\nlarge language models by exploiting Rowhammer-induced corruptions in tokenizer\ndictionaries of GGUF model files. This attack effectively rewrites safety\ninstructions in system prompts by swapping safety-critical tokens with benign\nalternatives, circumventing model guardrails while maintaining normal\nfunctionality in other contexts. Our experimental results across multiple DRAM\nconfigurations reveal that current memory protection schemes are inadequate\nagainst these sophisticated attack vectors, which can achieve their objectives\nwith precise, minimal modifications rather than random corruption."
    },
    {
        "date": "2025-05",
        "title": "Securing the Future of IVR: AI-Driven Innovation with Agile Security, Data Regulation, and Ethical AI Integration",
        "author": "Khushbu Mehboob Shaikh, and Georgios Giannakopoulos",
        "link": "http://arxiv.org/abs/2505.01514v1",
        "abstract": "The rapid digitalization of communication systems has elevated Interactive\nVoice Response (IVR) technologies to become critical interfaces for customer\nengagement. With Artificial Intelligence (AI) now driving these platforms,\nensuring secure, compliant, and ethically designed development practices is\nmore imperative than ever. AI-powered IVRs leverage Natural Language Processing\n(NLP) and Machine Learning (ML) to personalize interactions, automate service\ndelivery, and optimize user experiences. However, these innovations expose\nsystems to heightened risks, including data privacy breaches, AI decision\nopacity, and model security vulnerabilities. This paper analyzes the evolution\nof IVRs from static code-based designs to adaptive AI-driven systems,\npresenting a cybersecurity-centric perspective. We propose a practical\ngovernance framework that embeds agile security principles, compliance with\nglobal data legislation, and user-centric ethics. Emphasizing\nprivacy-by-design, adaptive risk modeling, and transparency, the paper argues\nthat ethical AI integration is not a feature but a strategic imperative.\nThrough this multidimensional lens, we highlight how modern IVRs can transition\nfrom communication tools to intelligent, secure, and accountable digital\nfrontlines-resilient against emerging threats and aligned with societal\nexpectations."
    },
    {
        "date": "2025-05",
        "title": "Machine Learning for Cyber-Attack Identification from Traffic Flows",
        "author": "Yujing Zhou, Marc L. Jacquet, Robel Dawit, Skyler Fabre, Dev Sarawat, Faheem Khan, Madison Newell, Yongxin Liu, Dahai Liu, Hongyun Chen, Jian Wang, and Huihui Wang",
        "link": "http://arxiv.org/abs/2505.01489v1",
        "abstract": "This paper presents our simulation of cyber-attacks and detection strategies\non the traffic control system in Daytona Beach, FL. using Raspberry Pi virtual\nmachines and the OPNSense firewall, along with traffic dynamics from SUMO and\nexploitation via the Metasploit framework. We try to answer the research\nquestions: are we able to identify cyber attacks by only analyzing traffic flow\npatterns. In this research, the cyber attacks are focused particularly when\nlights are randomly turned all green or red at busy intersections by\nadversarial attackers. Despite challenges stemming from imbalanced data and\noverlapping traffic patterns, our best model shows 85\\% accuracy when detecting\nintrusions purely using traffic flow statistics. Key indicators for successful\ndetection included occupancy, jam length, and halting durations."
    },
    {
        "date": "2025-05",
        "title": "Constrained Network Adversarial Attacks: Validity, Robustness, and Transferability",
        "author": "Anass Grini, Oumaima Taheri, Btissam El Khamlichi, and Amal El Fallah-Seghrouchni",
        "link": "http://arxiv.org/abs/2505.01328v1",
        "abstract": "While machine learning has significantly advanced Network Intrusion Detection\nSystems (NIDS), particularly within IoT environments where devices generate\nlarge volumes of data and are increasingly susceptible to cyber threats, these\nmodels remain vulnerable to adversarial attacks. Our research reveals a\ncritical flaw in existing adversarial attack methodologies: the frequent\nviolation of domain-specific constraints, such as numerical and categorical\nlimits, inherent to IoT and network traffic. This leads to up to 80.3% of\nadversarial examples being invalid, significantly overstating real-world\nvulnerabilities. These invalid examples, though effective in fooling models, do\nnot represent feasible attacks within practical IoT deployments. Consequently,\nrelying on these results can mislead resource allocation for defense, inflating\nthe perceived susceptibility of IoT-enabled NIDS models to adversarial\nmanipulation. Furthermore, we demonstrate that simpler surrogate models like\nMulti-Layer Perceptron (MLP) generate more valid adversarial examples compared\nto complex architectures such as CNNs and LSTMs. Using the MLP as a surrogate,\nwe analyze the transferability of adversarial severity to other ML/DL models\ncommonly used in IoT contexts. This work underscores the importance of\nconsidering both domain constraints and model architecture when evaluating and\ndesigning robust ML/DL models for security-critical IoT and network\napplications."
    },
    {
        "date": "2025-05",
        "title": "Fine-grained Manipulation Attacks to Local Differential Privacy Protocols for Data Streams",
        "author": "Xinyu Li, Xuebin Ren, Shusen Yang, Liang Shi, and Chia-Mu Yu",
        "link": "http://arxiv.org/abs/2505.01292v1",
        "abstract": "Local Differential Privacy (LDP) enables massive data collection and analysis\nwhile protecting end users' privacy against untrusted aggregators. It has been\napplied to various data types (e.g., categorical, numerical, and graph data)\nand application settings (e.g., static and streaming). Recent findings indicate\nthat LDP protocols can be easily disrupted by poisoning or manipulation\nattacks, which leverage injected/corrupted fake users to send crafted data\nconforming to the LDP reports. However, current attacks primarily target static\nprotocols, neglecting the security of LDP protocols in the streaming settings.\nOur research fills the gap by developing novel fine-grained manipulation\nattacks to LDP protocols for data streams. By reviewing the attack surfaces in\nexisting algorithms, We introduce a unified attack framework with composable\nmodules, which can manipulate the LDP estimated stream toward a target stream.\nOur attack framework can adapt to state-of-the-art streaming LDP algorithms\nwith different analytic tasks (e.g., frequency and mean) and LDP models\n(event-level, user-level, w-event level). We validate our attacks theoretically\nand through extensive experiments on real-world datasets, and finally explore a\npossible defense mechanism for mitigating these attacks."
    },
    {
        "date": "2025-05",
        "title": "Watermark Overwriting Attack on StegaStamp algorithm",
        "author": "I. F. Serzhenko, L. A. Khaertdinova, M. A. Pautov, and A. V. Antsiferova",
        "link": "http://arxiv.org/abs/2505.01474v1",
        "abstract": "This paper presents an attack method on the StegaStamp watermarking algorithm\nthat completely removes watermarks from an image with minimal quality loss,\ndeveloped as part of the NeurIPS \"Erasing the invisible\" competition."
    },
    {
        "date": "2025-05",
        "title": "Diffusion-based Adversarial Purification from the Perspective of the Frequency Domain",
        "author": "Gaozheng Pei, Ke Ma, Yingfei Sun, Qianqian Xu, and Qingming Huang",
        "link": "http://arxiv.org/abs/2505.01267v1",
        "abstract": "The diffusion-based adversarial purification methods attempt to drown\nadversarial perturbations into a part of isotropic noise through the forward\nprocess, and then recover the clean images through the reverse process. Due to\nthe lack of distribution information about adversarial perturbations in the\npixel domain, it is often unavoidable to damage normal semantics. We turn to\nthe frequency domain perspective, decomposing the image into amplitude spectrum\nand phase spectrum. We find that for both spectra, the damage caused by\nadversarial perturbations tends to increase monotonically with frequency. This\nmeans that we can extract the content and structural information of the\noriginal clean sample from the frequency components that are less damaged.\nMeanwhile, theoretical analysis indicates that existing purification methods\nindiscriminately damage all frequency components, leading to excessive damage\nto the image. Therefore, we propose a purification method that can eliminate\nadversarial perturbations while maximizing the preservation of the content and\nstructure of the original image. Specifically, at each time step during the\nreverse process, for the amplitude spectrum, we replace the low-frequency\ncomponents of the estimated image's amplitude spectrum with the corresponding\nparts of the adversarial image. For the phase spectrum, we project the phase of\nthe estimated image into a designated range of the adversarial image's phase\nspectrum, focusing on the low frequencies. Empirical evidence from extensive\nexperiments demonstrates that our method significantly outperforms most current\ndefense methods."
    },
    {
        "date": "2025-05",
        "title": "A Secured Triad of IoT, Machine Learning, and Blockchain for Crop Forecasting in Agriculture",
        "author": "Najmus Sakib Sizan, Md. Abu Layek, and Khondokar Fida Hasan",
        "link": "http://arxiv.org/abs/2505.01196v1",
        "abstract": "To improve crop forecasting and provide farmers with actionable data-driven\ninsights, we propose a novel approach integrating IoT, machine learning, and\nblockchain technologies. Using IoT, real-time data from sensor networks\ncontinuously monitor environmental conditions and soil nutrient levels,\nsignificantly improving our understanding of crop growth dynamics. Our study\ndemonstrates the exceptional accuracy of the Random Forest model, achieving a\n99.45\\% accuracy rate in predicting optimal crop types and yields, thereby\noffering precise crop projections and customized recommendations. To ensure the\nsecurity and integrity of the sensor data used for these forecasts, we\nintegrate the Ethereum blockchain, which provides a robust and secure platform.\nThis ensures that the forecasted data remain tamper-proof and reliable.\nStakeholders can access real-time and historical crop projections through an\nintuitive online interface, enhancing transparency and facilitating informed\ndecision-making. By presenting multiple predicted crop scenarios, our system\nenables farmers to optimize production strategies effectively. This integrated\napproach promises significant advances in precision agriculture, making crop\nforecasting more accurate, secure, and user-friendly."
    },
    {
        "date": "2025-05",
        "title": "Secure Cluster-Based Hierarchical Federated Learning in Vehicular Networks",
        "author": "M. Saeid HaghighiFard, and Sinem Coleri",
        "link": "http://arxiv.org/abs/2505.01186v1",
        "abstract": "Hierarchical Federated Learning (HFL) has recently emerged as a promising\nsolution for intelligent decision-making in vehicular networks, helping to\naddress challenges such as limited communication resources, high vehicle\nmobility, and data heterogeneity. However, HFL remains vulnerable to\nadversarial and unreliable vehicles, whose misleading updates can significantly\ncompromise the integrity and convergence of the global model. To address these\nchallenges, we propose a novel defense framework that integrates dynamic\nvehicle selection with robust anomaly detection within a cluster-based HFL\narchitecture, specifically designed to counter Gaussian noise and gradient\nascent attacks. The framework performs a comprehensive reliability assessment\nfor each vehicle by evaluating historical accuracy, contribution frequency, and\nanomaly records. Anomaly detection combines Z-score and cosine similarity\nanalyses on model updates to identify both statistical outliers and directional\ndeviations in model updates. To further refine detection, an adaptive\nthresholding mechanism is incorporated into the cosine similarity metric,\ndynamically adjusting the threshold based on the historical accuracy of each\nvehicle to enforce stricter standards for consistently high-performing\nvehicles. In addition, a weighted gradient averaging mechanism is implemented,\nwhich assigns higher weights to gradient updates from more trustworthy\nvehicles. To defend against coordinated attacks, a cross-cluster consistency\ncheck is applied to identify collaborative attacks in which multiple\ncompromised clusters coordinate misleading updates. Together, these mechanisms\nform a multi-level defense strategy to filter out malicious contributions\neffectively. Simulation results show that the proposed algorithm significantly\nreduces convergence time compared to benchmark methods across both 1-hop and\n3-hop topologies."
    },
    {
        "date": "2025-05",
        "title": "Explainable AI Based Diagnosis of Poisoning Attacks in Evolutionary Swarms",
        "author": "Mehrdad Asadi, Roxana R\u0103dulescu, and Ann Now\u00e9",
        "link": "http://arxiv.org/abs/2505.01181v1",
        "abstract": "Swarming systems, such as for example multi-drone networks, excel at\ncooperative tasks like monitoring, surveillance, or disaster assistance in\ncritical environments, where autonomous agents make decentralized decisions in\norder to fulfill team-level objectives in a robust and efficient manner.\nUnfortunately, team-level coordinated strategies in the wild are vulnerable to\ndata poisoning attacks, resulting in either inaccurate coordination or\nadversarial behavior among the agents. To address this challenge, we contribute\na framework that investigates the effects of such data poisoning attacks, using\nexplainable AI methods. We model the interaction among agents using\nevolutionary intelligence, where an optimal coalition strategically emerges to\nperform coordinated tasks. Then, through a rigorous evaluation, the swarm model\nis systematically poisoned using data manipulation attacks. We showcase the\napplicability of explainable AI methods to quantify the effects of poisoning on\nthe team strategy and extract footprint characterizations that enable\ndiagnosing. Our findings indicate that when the model is poisoned above 10%,\nnon-optimal strategies resulting in inefficient cooperation can be identified."
    },
    {
        "date": "2025-05",
        "title": "LLM Security: Vulnerabilities, Attacks, Defenses, and Countermeasures",
        "author": "Francisco Aguilera-Mart\u00ednez, and Fernando Berzal",
        "link": "http://arxiv.org/abs/2505.01177v1",
        "abstract": "As large language models (LLMs) continue to evolve, it is critical to assess\nthe security threats and vulnerabilities that may arise both during their\ntraining phase and after models have been deployed. This survey seeks to define\nand categorize the various attacks targeting LLMs, distinguishing between those\nthat occur during the training phase and those that affect already trained\nmodels. A thorough analysis of these attacks is presented, alongside an\nexploration of defense mechanisms designed to mitigate such threats. Defenses\nare classified into two primary categories: prevention-based and\ndetection-based defenses. Furthermore, our survey summarizes possible attacks\nand their corresponding defense strategies. It also provides an evaluation of\nthe effectiveness of the known defense mechanisms for the different security\nthreats. Our survey aims to offer a structured framework for securing LLMs,\nwhile also identifying areas that require further research to improve and\nstrengthen defenses against emerging security challenges."
    },
    {
        "date": "2025-05",
        "title": "Harmonizing Intra-coherence and Inter-divergence in Ensemble Attacks for Adversarial Transferability",
        "author": "Zhaoyang Ma, Zhihao Wu, Wang Lu, Xin Gao, Jinghang Yue, Taolin Zhang, Lipo Wang, Youfang Lin, and Jing Wang",
        "link": "http://arxiv.org/abs/2505.01168v1",
        "abstract": "The development of model ensemble attacks has significantly improved the\ntransferability of adversarial examples, but this progress also poses severe\nthreats to the security of deep neural networks. Existing methods, however,\nface two critical challenges: insufficient capture of shared gradient\ndirections across models and a lack of adaptive weight allocation mechanisms.\nTo address these issues, we propose a novel method Harmonized Ensemble for\nAdversarial Transferability (HEAT), which introduces domain generalization into\nadversarial example generation for the first time. HEAT consists of two key\nmodules: Consensus Gradient Direction Synthesizer, which uses Singular Value\nDecomposition to synthesize shared gradient directions; and Dual-Harmony Weight\nOrchestrator which dynamically balances intra-domain coherence, stabilizing\ngradients within individual models, and inter-domain diversity, enhancing\ntransferability across models. Experimental results demonstrate that HEAT\nsignificantly outperforms existing methods across various datasets and\nsettings, offering a new perspective and direction for adversarial attack\nresearch."
    },
    {
        "date": "2025-05",
        "title": "Active Sybil Attack and Efficient Defense Strategy in IPFS DHT",
        "author": "V. H. M. Netto, T. Cholez, and C. L. Ignat",
        "link": "http://arxiv.org/abs/2505.01139v1",
        "abstract": "The InterPlanetary File System (IPFS) is a decentralized peer-to-peer (P2P)\nstorage that relies on Kademlia, a Distributed Hash Table (DHT) structure\ncommonly used in P2P systems for its proved scalability. However, DHTs are\nknown to be vulnerable to Sybil attacks, in which a single entity controls\nmultiple malicious nodes. Recent studies have shown that IPFS is affected by a\npassive content eclipse attack, leveraging Sybils, in which adversarial nodes\nhide received indexed information from other peers, making the content appear\nunavailable. Fortunately, the latest mitigation strategy coupling an attack\ndetection based on statistical tests and a wider publication strategy upon\ndetection was able to circumvent it.\n  In this work, we present a new active attack, with malicious nodes responding\nwith semantically correct but intentionally false data, exploiting both an\noptimized placement of Sybils to stay below the detection threshold and an\nearly trigger of the content discovery termination in Kubo, the main IPFS\nimplementation. Our attack achieves to completely eclipse content on the latest\nKubo release. When evaluated against the most recent known mitigation, it\nsuccessfully denies access to the target content in approximately 80\\% of\nlookup attempts.\n  To address this vulnerability, we propose a new mitigation called\nSR-DHT-Store, which enables efficient, Sybil-resistant content publication\nwithout relying on attack detection but instead on a systematic and precise use\nof region-based queries, defined by a dynamically computed XOR distance to the\ntarget ID. SR-DHT-Store can be combined with other defense mechanisms resulting\nin a defense strategy that completely mitigates both passive and active Sybil\nattacks at a lower overhead, while allowing an incremental deployment."
    },
    {
        "date": "2025-05",
        "title": "Risk Analysis and Design Against Adversarial Actions",
        "author": "Marco C. Campi, Algo Car\u00e8, Luis G. Crespo, Simone Garatti, and Federico A. Ramponi",
        "link": "http://arxiv.org/abs/2505.01130v1",
        "abstract": "Learning models capable of providing reliable predictions in the face of\nadversarial actions has become a central focus of the machine learning\ncommunity in recent years. This challenge arises from observing that data\nencountered at deployment time often deviate from the conditions under which\nthe model was trained. In this paper, we address deployment-time adversarial\nactions and propose a versatile, well-principled framework to evaluate the\nmodel's robustness against attacks of diverse types and intensities. While we\ninitially focus on Support Vector Regression (SVR), the proposed approach\nextends naturally to the broad domain of learning via relaxed optimization\ntechniques. Our results enable an assessment of the model vulnerability without\nrequiring additional test data and operate in a distribution-free setup. These\nresults not only provide a tool to enhance trust in the model's applicability\nbut also aid in selecting among competing alternatives. Later in the paper, we\nshow that our findings also offer useful insights for establishing new results\nwithin the out-of-distribution framework."
    },
    {
        "date": "2025-05",
        "title": "Transferable Adversarial Attacks on Black-Box Vision-Language Models",
        "author": "Kai Hu, Weichen Yu, Li Zhang, Alexander Robey, Andy Zou, Chengming Xu, Haoqi Hu, and Matt Fredrikson",
        "link": "http://arxiv.org/abs/2505.01050v1",
        "abstract": "Vision Large Language Models (VLLMs) are increasingly deployed to offer\nadvanced capabilities on inputs comprising both text and images. While prior\nresearch has shown that adversarial attacks can transfer from open-source to\nproprietary black-box models in text-only and vision-only contexts, the extent\nand effectiveness of such vulnerabilities remain underexplored for VLLMs. We\npresent a comprehensive analysis demonstrating that targeted adversarial\nexamples are highly transferable to widely-used proprietary VLLMs such as\nGPT-4o, Claude, and Gemini. We show that attackers can craft perturbations to\ninduce specific attacker-chosen interpretations of visual information, such as\nmisinterpreting hazardous content as safe, overlooking sensitive or restricted\nmaterial, or generating detailed incorrect responses aligned with the\nattacker's intent. Furthermore, we discover that universal perturbations --\nmodifications applicable to a wide set of images -- can consistently induce\nthese misinterpretations across multiple proprietary VLLMs. Our experimental\nresults on object recognition, visual question answering, and image captioning\nshow that this vulnerability is common across current state-of-the-art models,\nand underscore an urgent need for robust mitigations to ensure the safe and\nsecure deployment of VLLMs."
    },
    {
        "date": "2025-05",
        "title": "Quantum Support Vector Regression for Robust Anomaly Detection",
        "author": "Kilian Tscharke, Maximilian Wendlinger, Sebastian Issel, and Pascal Debus",
        "link": "http://arxiv.org/abs/2505.01012v1",
        "abstract": "Anomaly Detection (AD) is critical in data analysis, particularly within the\ndomain of IT security. In recent years, Machine Learning (ML) algorithms have\nemerged as a powerful tool for AD in large-scale data. In this study, we\nexplore the potential of quantum ML approaches, specifically quantum kernel\nmethods, for the application to robust AD. We build upon previous work on\nQuantum Support Vector Regression (QSVR) for semisupervised AD by conducting a\ncomprehensive benchmark on IBM quantum hardware using eleven datasets. Our\nresults demonstrate that QSVR achieves strong classification performance and\neven outperforms the noiseless simulation on two of these datasets. Moreover,\nwe investigate the influence of - in the NISQ-era inevitable - quantum noise on\nthe performance of the QSVR. Our findings reveal that the model exhibits\nrobustness to depolarizing, phase damping, phase flip, and bit flip noise,\nwhile amplitude damping and miscalibration noise prove to be more disruptive.\nFinally, we explore the domain of Quantum Adversarial Machine Learning and\ndemonstrate that QSVR is highly vulnerable to adversarial attacks and that\nnoise does not improve the adversarial robustness of the model."
    },
    {
        "date": "2025-05",
        "title": "Attack and defense techniques in large language models: A survey and new perspectives",
        "author": "Zhiyu Liao, Kang Chen, Yuanguo Lin, Kangkang Li, Yunxuan Liu, Hefeng Chen, Xingwang Huang, and Yuanhui Yu",
        "link": "http://arxiv.org/abs/2505.00976v1",
        "abstract": "Large Language Models (LLMs) have become central to numerous natural language\nprocessing tasks, but their vulnerabilities present significant security and\nethical challenges. This systematic survey explores the evolving landscape of\nattack and defense techniques in LLMs. We classify attacks into adversarial\nprompt attack, optimized attacks, model theft, as well as attacks on\napplication of LLMs, detailing their mechanisms and implications. Consequently,\nwe analyze defense strategies, including prevention-based and detection-based\ndefense methods. Although advances have been made, challenges remain to adapt\nto the dynamic threat landscape, balance usability with robustness, and address\nresource constraints in defense implementation. We highlight open problems,\nincluding the need for adaptive scalable defenses, explainable security\ntechniques, and standardized evaluation frameworks. This survey provides\nactionable insights and directions for developing secure and resilient LLMs,\nemphasizing the importance of interdisciplinary collaboration and ethical\nconsiderations to mitigate risks in real-world applications."
    },
    {
        "date": "2025-05",
        "title": "FreCT: Frequency-augmented Convolutional Transformer for Robust Time Series Anomaly Detection",
        "author": "Wenxin Zhang, Ding Xu, Guangzhen Yao, Xiaojian Lin, Renxiang Guan, Chengze Du, Renda Han, Xi Xuan, and Cuicui Luo",
        "link": "http://arxiv.org/abs/2505.00941v1",
        "abstract": "Time series anomaly detection is critical for system monitoring and risk\nidentification, across various domains, such as finance and healthcare.\nHowever, for most reconstruction-based approaches, detecting anomalies remains\na challenge due to the complexity of sequential patterns in time series data.\nOn the one hand, reconstruction-based techniques are susceptible to\ncomputational deviation stemming from anomalies, which can lead to impure\nrepresentations of normal sequence patterns. On the other hand, they often\nfocus on the time-domain dependencies of time series, while ignoring the\nalignment of frequency information beyond the time domain. To address these\nchallenges, we propose a novel Frequency-augmented Convolutional Transformer\n(FreCT). FreCT utilizes patch operations to generate contrastive views and\nemploys an improved Transformer architecture integrated with a convolution\nmodule to capture long-term dependencies while preserving local topology\ninformation. The introduced frequency analysis based on Fourier transformation\ncould enhance the model's ability to capture crucial characteristics beyond the\ntime domain. To protect the training quality from anomalies and improve the\nrobustness, FreCT deploys stop-gradient Kullback-Leibler (KL) divergence and\nabsolute error to optimize consistency information in both time and frequency\ndomains. Extensive experiments on four public datasets demonstrate that FreCT\noutperforms existing methods in identifying anomalies."
    },
    {
        "date": "2025-05",
        "title": "Robust Root Cause Diagnosis using In-Distribution Interventions",
        "author": "Lokesh Nagalapatti, Ashutosh Srivastava, Sunita Sarawagi, and Amit Sharma",
        "link": "http://arxiv.org/abs/2505.00930v1",
        "abstract": "Diagnosing the root cause of an anomaly in a complex interconnected system is\na pressing problem in today's cloud services and industrial operations. We\npropose In-Distribution Interventions (IDI), a novel algorithm that predicts\nroot cause as nodes that meet two criteria: 1) **Anomaly:** root cause nodes\nshould take on anomalous values; 2) **Fix:** had the root cause nodes assumed\nusual values, the target node would not have been anomalous. Prior methods of\nassessing the fix condition rely on counterfactuals inferred from a Structural\nCausal Model (SCM) trained on historical data. But since anomalies are rare and\nfall outside the training distribution, the fitted SCMs yield unreliable\ncounterfactual estimates. IDI overcomes this by relying on interventional\nestimates obtained by solely probing the fitted SCM at in-distribution inputs.\nWe present a theoretical analysis comparing and bounding the errors in\nassessing the fix condition using interventional and counterfactual estimates.\nWe then conduct experiments by systematically varying the SCM's complexity to\ndemonstrate the cases where IDI's interventional approach outperforms the\ncounterfactual approach and vice versa. Experiments on both synthetic and\nPetShop RCD benchmark datasets demonstrate that \\our\\ consistently identifies\ntrue root causes more accurately and robustly than nine existing\nstate-of-the-art RCD baselines. Code is released at\nhttps://github.com/nlokeshiisc/IDI_release."
    },
    {
        "date": "2025-05",
        "title": "Balancing Security and Liquidity: A Time-Weighted Snapshot Framework for DAO Governance Voting",
        "author": "Zayn Wang, Frank Pu, Vinci Cheung, and Robert Hao",
        "link": "http://arxiv.org/abs/2505.00888v2",
        "abstract": "As new project upgrading the blockchain industry, novel forms of attack\nchallenges developers to rethink about the design of their innovations. In the\ngrowth stage of the development, Decentralized Autonomous Organizations (DAO)\nintroduces different approaches in managing fund through voting in governance\ntokens. However, relying on tokens as a weight for voting introduces\nopportunities for hackers to manipulate voting results through flash loan,\nallowing malicious proposals - fund withdrawal from DAO to hacker's wallet - to\nexecute through the smart contract. In this research, we learned different\ndefense mechanism against the flash loan attack, and their weakness in\naccessibility that compromise the security of different blockchain projects.\nBased on our observation, we propose a new defensing structure and apply it\nwith cases."
    },
    {
        "date": "2025-05",
        "title": "Protocol-agnostic and Data-free Backdoor Attacks on Pre-trained Models in RF Fingerprinting",
        "author": "Tianya Zhao, Ningning Wang, Junqing Zhang, and Xuyu Wang",
        "link": "http://arxiv.org/abs/2505.00881v1",
        "abstract": "While supervised deep neural networks (DNNs) have proven effective for device\nauthentication via radio frequency (RF) fingerprinting, they are hindered by\ndomain shift issues and the scarcity of labeled data. The success of large\nlanguage models has led to increased interest in unsupervised pre-trained\nmodels (PTMs), which offer better generalization and do not require labeled\ndatasets, potentially addressing the issues mentioned above. However, the\ninherent vulnerabilities of PTMs in RF fingerprinting remain insufficiently\nexplored. In this paper, we thoroughly investigate data-free backdoor attacks\non such PTMs in RF fingerprinting, focusing on a practical scenario where\nattackers lack access to downstream data, label information, and training\nprocesses. To realize the backdoor attack, we carefully design a set of\ntriggers and predefined output representations (PORs) for the PTMs. By mapping\ntriggers and PORs through backdoor training, we can implant backdoor behaviors\ninto the PTMs, thereby introducing vulnerabilities across different downstream\nRF fingerprinting tasks without requiring prior knowledge. Extensive\nexperiments demonstrate the wide applicability of our proposed attack to\nvarious input domains, protocols, and PTMs. Furthermore, we explore potential\ndetection and defense methods, demonstrating the difficulty of fully\nsafeguarding against our proposed backdoor attack."
    },
    {
        "date": "2025-05",
        "title": "Duality on the Thermodynamics of the Kirchhoff-Law-Johnson-Noise (KLJN) Secure Key Exchange Scheme",
        "author": "Sarah Flanery, Anson Trapani, Christiana Chamon, and Leyla Nazhandali",
        "link": "http://arxiv.org/abs/2505.00858v1",
        "abstract": "This study investigates a duality approach to information leak detection in\nthe generalized Kirchhoff-Law-Johnson-Noise (KLJN) secure key exchange scheme.\nWhile previous work by Chamon and Kish sampled voltages at zero-current\ninstances, this research explores sampling currents at zero-voltage crossings.\nThe objective is to determine if this dual approach can reveal information\nleaks in non-equilibrium KLJN systems. Results indicate that the duality method\nsuccessfully detects information leaks, further supporting the necessity of\nthermal equilibrium for unconditional security in KLJN systems."
    },
    {
        "date": "2025-05",
        "title": "Data-Driven Optical To Thermal Inference in Pool Boiling Using Generative Adversarial Networks",
        "author": "Qianxi Fu, Youngjoon Suh, Xiaojing Zhang, and Yoonjin Won",
        "link": "http://arxiv.org/abs/2505.00823v1",
        "abstract": "Phase change plays a critical role in thermal management systems, yet\nquantitative characterization of multiphase heat transfer remains limited by\nthe challenges of measuring temperature fields in chaotic, rapidly evolving\nflow regimes. While computational methods offer spatiotemporal resolution in\nidealized cases, replicating complex experimental conditions remains\nprohibitively difficult. Here, we present a data-driven framework that\nleverages a conditional generative adversarial network (CGAN) to infer\ntemperature fields from geometric phase contours in a canonical pool boiling\nconfiguration where advanced data collection techniques are restricted. Using\nhigh-speed imaging data and simulation-informed training, our model\ndemonstrates the ability to reconstruct temperature fields with errors below\n6%. We further show that standard data augmentation strategies are effective in\nenhancing both accuracy and physical plausibility of the predicted maps across\nboth simulation and experimental datasets when precise physical constraints are\nnot applicable. Our results highlight the potential of deep generative models\nto bridge the gap between observable multiphase phenomena and underlying\nthermal transport, offering a powerful approach to augment and interpret\nexperimental measurements in complex two-phase systems."
    },
    {
        "date": "2025-05",
        "title": "Enhancing the Cloud Security through Topic Modelling",
        "author": "Sabbir M. Saleh, Nazim Madhavji, and John Steinbacher",
        "link": "http://arxiv.org/abs/2505.01463v1",
        "abstract": "Protecting cloud applications is crucial in an age where security constantly\nthreatens the digital world. The inevitable cyber-attacks throughout the CI/CD\npipeline make cloud security innovations necessary. This research is motivated\nby applying Natural Language Processing (NLP) methodologies, such as Topic\nModelling, to analyse cloud security data and predict future attacks. This\nresearch aims to use topic modelling, specifically Latent Dirichlet Allocation\n(LDA) and Probabilistic Latent Semantic Analysis (pLSA). Utilising LDA and\nPLSA, security-related text data, such as reports, logs, and other relevant\ndocuments, will be analysed and sorted into relevant topics (such as phishing\nor encryption). These algorithms may apply through Python using the Gensim\nframework. The topics shall be utilised to detect vulnerabilities within\nrelevant CI/CD pipeline records or log data. This application of Topic\nModelling anticipates providing a new form of vulnerability detection,\nimproving overall security throughout the CI/CD pipeline."
    },
    {
        "date": "2025-05",
        "title": "RevealNet: Distributed Traffic Correlation for Attack Attribution on Programmable Networks",
        "author": "Gurjot Singh, Alim Dhanani, and Diogo Barradas",
        "link": "http://arxiv.org/abs/2505.00618v1",
        "abstract": "Network attackers have increasingly resorted to proxy chains, VPNs, and\nanonymity networks to conceal their activities. To tackle this issue, past\nresearch has explored the applicability of traffic correlation techniques to\nperform attack attribution, i.e., to identify an attacker's true network\nlocation. However, current traffic correlation approaches rely on\nwell-provisioned and centralized systems that ingest flows from multiple\nnetwork probes to compute correlation scores. Unfortunately, this makes\ncorrelation efforts scale poorly for large high-speed networks.\n  In this paper, we propose RevealNet, a decentralized framework for attack\nattribution that orchestrates a fleet of P4-programmable switches to perform\ntraffic correlation. RevealNet builds on a set of correlation primitives\ninspired by prior work on computing and comparing flow sketches -- compact\nsummaries of flows' key characteristics -- to enable efficient, distributed,\nin-network traffic correlation. Our evaluation suggests that RevealNet achieves\ncomparable accuracy to centralized attack attribution systems while\nsignificantly reducing both the computational complexity and bandwidth\noverheads imposed by correlation tasks."
    },
    {
        "date": "2025-05",
        "title": "A Novel Feature-Aware Chaotic Image Encryption Scheme For Data Security and Privacy in IoT and Edge Networks",
        "author": "Muhammad Shahbaz Khan, Ahmed Al-Dubai, Jawad Ahmad, Nikolaos Pitropakis, and Baraq Ghaleb",
        "link": "http://arxiv.org/abs/2505.00593v1",
        "abstract": "The security of image data in the Internet of Things (IoT) and edge networks\nis crucial due to the increasing deployment of intelligent systems for\nreal-time decision-making. Traditional encryption algorithms such as AES and\nRSA are computationally expensive for resource-constrained IoT devices and\nineffective for large-volume image data, leading to inefficiencies in\nprivacy-preserving distributed learning applications. To address these\nconcerns, this paper proposes a novel Feature-Aware Chaotic Image Encryption\nscheme that integrates Feature-Aware Pixel Segmentation (FAPS) with Chaotic\nChain Permutation and Confusion mechanisms to enhance security while\nmaintaining efficiency. The proposed scheme consists of three stages: (1) FAPS,\nwhich extracts and reorganizes pixels based on high and low edge intensity\nfeatures for correlation disruption; (2) Chaotic Chain Permutation, which\nemploys a logistic chaotic map with SHA-256-based dynamically updated keys for\nblock-wise permutation; and (3) Chaotic chain Confusion, which utilises\ndynamically generated chaotic seed matrices for bitwise XOR operations.\nExtensive security and performance evaluations demonstrate that the proposed\nscheme significantly reduces pixel correlation -- almost zero, achieves high\nentropy values close to 8, and resists differential cryptographic attacks. The\noptimum design of the proposed scheme makes it suitable for real-time\ndeployment in resource-constrained environments."
    },
    {
        "date": "2025-05",
        "title": "A Robust Deep Networks based Multi-Object MultiCamera Tracking System for City Scale Traffic",
        "author": "Muhammad Imran Zaman, Usama Ijaz Bajwa, Gulshan Saleem, and Rana Hammad Raza",
        "link": "http://arxiv.org/abs/2505.00534v1",
        "abstract": "Vision sensors are becoming more important in Intelligent Transportation\nSystems (ITS) for traffic monitoring, management, and optimization as the\nnumber of network cameras continues to rise. However, manual object tracking\nand matching across multiple non-overlapping cameras pose significant\nchallenges in city-scale urban traffic scenarios. These challenges include\nhandling diverse vehicle attributes, occlusions, illumination variations,\nshadows, and varying video resolutions. To address these issues, we propose an\nefficient and cost-effective deep learning-based framework for Multi-Object\nMulti-Camera Tracking (MO-MCT). The proposed framework utilizes Mask R-CNN for\nobject detection and employs Non-Maximum Suppression (NMS) to select target\nobjects from overlapping detections. Transfer learning is employed for\nre-identification, enabling the association and generation of vehicle tracklets\nacross multiple cameras. Moreover, we leverage appropriate loss functions and\ndistance measures to handle occlusion, illumination, and shadow challenges. The\nfinal solution identification module performs feature extraction using\nResNet-152 coupled with Deep SORT based vehicle tracking. The proposed\nframework is evaluated on the 5th AI City Challenge dataset (Track 3),\ncomprising 46 camera feeds. Among these 46 camera streams, 40 are used for\nmodel training and validation, while the remaining six are utilized for model\ntesting. The proposed framework achieves competitive performance with an IDF1\nscore of 0.8289, and precision and recall scores of 0.9026 and 0.8527\nrespectively, demonstrating its effectiveness in robust and accurate vehicle\ntracking."
    },
    {
        "date": "2025-05",
        "title": "KeySync: A Robust Approach for Leakage-free Lip Synchronization in High Resolution",
        "author": "Antoni Bigata, Rodrigo Mira, Stella Bounareli, Micha\u0142 Stypu\u0142kowski, Konstantinos Vougioukas, Stavros Petridis, and Maja Pantic",
        "link": "http://arxiv.org/abs/2505.00497v1",
        "abstract": "Lip synchronization, known as the task of aligning lip movements in an\nexisting video with new input audio, is typically framed as a simpler variant\nof audio-driven facial animation. However, as well as suffering from the usual\nissues in talking head generation (e.g., temporal consistency), lip\nsynchronization presents significant new challenges such as expression leakage\nfrom the input video and facial occlusions, which can severely impact\nreal-world applications like automated dubbing, but are often neglected in\nexisting works. To address these shortcomings, we present KeySync, a two-stage\nframework that succeeds in solving the issue of temporal consistency, while\nalso incorporating solutions for leakage and occlusions using a carefully\ndesigned masking strategy. We show that KeySync achieves state-of-the-art\nresults in lip reconstruction and cross-synchronization, improving visual\nquality and reducing expression leakage according to LipLeak, our novel leakage\nmetric. Furthermore, we demonstrate the effectiveness of our new masking\napproach in handling occlusions and validate our architectural choices through\nseveral ablation studies. Code and model weights can be found at\nhttps://antonibigata.github.io/KeySync."
    },
    {
        "date": "2025-05",
        "title": "Analysis of the vulnerability of machine learning regression models to adversarial attacks using data from 5G wireless networks",
        "author": "Leonid Legashev, Artur Zhigalov, and Denis Parfenov",
        "link": "http://arxiv.org/abs/2505.00487v1",
        "abstract": "This article describes the process of creating a script and conducting an\nanalytical study of a dataset using the DeepMIMO emulator. An advertorial\nattack was carried out using the FGSM method to maximize the gradient. A\ncomparison is made of the effectiveness of binary classifiers in the task of\ndetecting distorted data. The dynamics of changes in the quality indicators of\nthe regression model were analyzed in conditions without adversarial attacks,\nduring an adversarial attack and when the distorted data was isolated. It is\nshown that an adversarial FGSM attack with gradient maximization leads to an\nincrease in the value of the MSE metric by 33% and a decrease in the R2\nindicator by 10% on average. The LightGBM binary classifier effectively\nidentifies data with adversarial anomalies with 98% accuracy. Regression\nmachine learning models are susceptible to adversarial attacks, but rapid\nanalysis of network traffic and data transmitted over the network makes it\npossible to identify malicious activity"
    },
    {
        "date": "2025-05",
        "title": "Decentralized Vulnerability Disclosure via Permissioned Blockchain: A Secure, Transparent Alternative to Centralized CVE Management",
        "author": "Novruz Amirov, and Kemal Bicakci",
        "link": "http://arxiv.org/abs/2505.00480v1",
        "abstract": "This paper proposes a decentralized, blockchain-based system for the\npublication of Common Vulnerabilities and Exposures (CVEs), aiming to mitigate\nthe limitations of the current centralized model primarily overseen by MITRE.\nThe proposed architecture leverages a permissioned blockchain, wherein only\nauthenticated CVE Numbering Authorities (CNAs) are authorized to submit\nentries. This ensures controlled write access while preserving public\ntransparency. By incorporating smart contracts, the system supports key\nfeatures such as embargoed disclosures and decentralized governance. We\nevaluate the proposed model in comparison with existing practices, highlighting\nits advantages in transparency, trust decentralization, and auditability. A\nprototype implementation using Hyperledger Fabric is presented to demonstrate\nthe feasibility of the approach, along with a discussion of its implications\nfor the future of vulnerability disclosure."
    },
    {
        "date": "2025-05",
        "title": "The Invisible Threat: Evaluating the Vulnerability of Cross-Spectral Face Recognition to Presentation Attacks",
        "author": "Anjith George, and Sebastien Marcel",
        "link": "http://arxiv.org/abs/2505.00380v1",
        "abstract": "Cross-spectral face recognition systems are designed to enhance the\nperformance of facial recognition systems by enabling cross-modal matching\nunder challenging operational conditions. A particularly relevant application\nis the matching of near-infrared (NIR) images to visible-spectrum (VIS) images,\nenabling the verification of individuals by comparing NIR facial captures\nacquired with VIS reference images. The use of NIR imaging offers several\nadvantages, including greater robustness to illumination variations, better\nvisibility through glasses and glare, and greater resistance to presentation\nattacks. Despite these claimed benefits, the robustness of NIR-based systems\nagainst presentation attacks has not been systematically studied in the\nliterature. In this work, we conduct a comprehensive evaluation into the\nvulnerability of NIR-VIS cross-spectral face recognition systems to\npresentation attacks. Our empirical findings indicate that, although these\nsystems exhibit a certain degree of reliability, they remain vulnerable to\nspecific attacks, emphasizing the need for further research in this area."
    },
    {
        "date": "2025-05",
        "title": "Vehicular Communication Security: Multi-Channel and Multi-Factor Authentication",
        "author": "Marco De Vincenzi, Shuyang Sun, Chen Bo Calvin Zhang, Manuel Garcia, Shaozu Ding, Chiara Bodei, Ilaria Matteucci, and Dajiang Suo",
        "link": "http://arxiv.org/abs/2505.00340v1",
        "abstract": "Secure and reliable communications are crucial for Intelligent Transportation\nSystems (ITSs), where Vehicle-to-Infrastructure (V2I) communication plays a key\nrole in enabling mobility-enhancing and safety-critical services. Current V2I\nauthentication relies on credential-based methods over wireless\nNon-Line-of-Sight (NLOS) channels, leaving them exposed to remote impersonation\nand proximity attacks. To mitigate these risks, we propose a unified\nMulti-Channel, Multi-Factor Authentication (MFA) scheme that combines NLOS\ncryptographic credentials with a Line-of-Sight (LOS) visual channel. Our\napproach leverages a challenge-response security paradigm: the infrastructure\nissues challenges and the vehicle's headlights respond by flashing a structured\nsequence containing encoded security data. Deep learning models on the\ninfrastructure side then decode the embedded information to authenticate the\nvehicle. Real-world experimental evaluations demonstrate high test accuracy,\nreaching an average of 95% and 96.6%, respectively, under various lighting,\nweather, speed, and distance conditions. Additionally, we conducted extensive\nexperiments on three state-of-the-art deep learning models, including detailed\nablation studies for decoding the flashing sequence. Our results indicate that\nthe optimal architecture employs a dual-channel design, enabling simultaneous\ndecoding of the flashing sequence and extraction of vehicle spatial and\nlocational features for robust authentication."
    },
    {
        "date": "2025-05",
        "title": "AWARE-NET: Adaptive Weighted Averaging for Robust Ensemble Network in Deepfake Detection",
        "author": "Muhammad Salman, Iqra Tariq, Mishal Zulfiqar, Muqadas Jalal, Sami Aujla, and Sumbal Fatima",
        "link": "http://arxiv.org/abs/2505.00312v1",
        "abstract": "Deepfake detection has become increasingly important due to the rise of\nsynthetic media, which poses significant risks to digital identity and cyber\npresence for security and trust. While multiple approaches have improved\ndetection accuracy, challenges remain in achieving consistent performance\nacross diverse datasets and manipulation types. In response, we propose a novel\ntwo-tier ensemble framework for deepfake detection based on deep learning that\nhierarchically combines multiple instances of three state-of-the-art\narchitectures: Xception, Res2Net101, and EfficientNet-B7. Our framework employs\na unique approach where each architecture is instantiated three times with\ndifferent initializations to enhance model diversity, followed by a learnable\nweighting mechanism that dynamically combines their predictions. Unlike\ntraditional fixed-weight ensembles, our first-tier averages predictions within\neach architecture family to reduce model variance, while the second tier learns\noptimal contribution weights through backpropagation, automatically adjusting\neach architecture's influence based on their detection reliability. Our\nexperiments achieved state-of-the-art intra-dataset performance with AUC scores\nof 99.22% (FF++) and 100.00% (CelebDF-v2), and F1 scores of 98.06% (FF++) and\n99.94% (CelebDF-v2) without augmentation. With augmentation, we achieve AUC\nscores of 99.47% (FF++) and 100.00% (CelebDF-v2), and F1 scores of 98.43%\n(FF++) and 99.95% (CelebDF-v2). The framework demonstrates robust cross-dataset\ngeneralization, achieving AUC scores of 88.20% and 72.52%, and F1 scores of\n93.16% and 80.62% in cross-dataset evaluations."
    },
    {
        "date": "2025-05",
        "title": "A Unifying Framework for Robust and Efficient Inference with Unstructured Data",
        "author": "Jacob Carlson, and Melissa Dell",
        "link": "http://arxiv.org/abs/2505.00282v1",
        "abstract": "This paper presents a general framework for conducting efficient and robust\ninference on parameters derived from unstructured data, which include text,\nimages, audio, and video. Economists have long incorporated data extracted from\ntexts and images into their analyses, a practice that has accelerated with\nadvancements in deep neural networks. However, neural networks do not\ngenerically produce unbiased predictions, potentially propagating bias to\nestimators that use their outputs. To address this challenge, we reframe\ninference with unstructured data as a missing structured data problem, where\nstructured data are imputed from unstructured inputs using deep neural\nnetworks. This perspective allows us to apply classic results from\nsemiparametric inference, yielding valid, efficient, and robust estimators\nbased on unstructured data. We formalize this approach with MARS (Missing At\nRandom Structured Data), a unifying framework that integrates and extends\nexisting methods for debiased inference using machine learning predictions,\nlinking them to a variety of older, familiar problems such as causal inference.\nWe develop robust and efficient estimators for both descriptive and causal\nestimands and address challenges such as inference using aggregated and\ntransformed predictions from unstructured data. Importantly, MARS applies to\ncommon empirical settings that have received limited attention in the existing\nliterature. Finally, we reanalyze prominent studies that use unstructured data,\ndemonstrating the practical value of MARS."
    },
    {
        "date": "2025-05",
        "title": "Unlearning Sensitive Information in Multimodal LLMs: Benchmark and Attack-Defense Evaluation",
        "author": "Vaidehi Patil, Yi-Lin Sung, Peter Hase, Jie Peng, Tianlong Chen, and Mohit Bansal",
        "link": "http://arxiv.org/abs/2505.01456v1",
        "abstract": "LLMs trained on massive datasets may inadvertently acquire sensitive\ninformation such as personal details and potentially harmful content. This risk\nis further heightened in multimodal LLMs as they integrate information from\nmultiple modalities (image and text). Adversaries can exploit this knowledge\nthrough multimodal prompts to extract sensitive details. Evaluating how\neffectively MLLMs can forget such information (targeted unlearning)\nnecessitates the creation of high-quality, well-annotated image-text pairs.\nWhile prior work on unlearning has focused on text, multimodal unlearning\nremains underexplored. To address this gap, we first introduce a multimodal\nunlearning benchmark, UnLOK-VQA (Unlearning Outside Knowledge VQA), as well as\nan attack-and-defense framework to evaluate methods for deleting specific\nmultimodal knowledge from MLLMs. We extend a visual question-answering dataset\nusing an automated pipeline that generates varying-proximity samples for\ntesting generalization and specificity, followed by manual filtering for\nmaintaining high quality. We then evaluate six defense objectives against seven\nattacks (four whitebox, three blackbox), including a novel whitebox method\nleveraging interpretability of hidden states. Our results show multimodal\nattacks outperform text- or image-only ones, and that the most effective\ndefense removes answer information from internal model states. Additionally,\nlarger models exhibit greater post-editing robustness, suggesting that scale\nenhances safety. UnLOK-VQA provides a rigorous benchmark for advancing\nunlearning in MLLMs."
    },
    {
        "date": "2025-04",
        "title": "Towards Robust and Generalizable Gerchberg Saxton based Physics Inspired Neural Networks for Computer Generated Holography: A Sensitivity Analysis Framework",
        "author": "Ankit Amrutkar, Bj\u00f6rn Kampa, Volkmar Schulz, Johannes Stegmaier, Markus Rothermel, and Dorit Merhof",
        "link": "http://arxiv.org/abs/2505.00220v1",
        "abstract": "Computer-generated holography (CGH) enables applications in holographic\naugmented reality (AR), 3D displays, systems neuroscience, and optical\ntrapping. The fundamental challenge in CGH is solving the inverse problem of\nphase retrieval from intensity measurements. Physics-inspired neural networks\n(PINNs), especially Gerchberg-Saxton-based PINNs (GS-PINNs), have advanced\nphase retrieval capabilities. However, their performance strongly depends on\nforward models (FMs) and their hyperparameters (FMHs), limiting generalization,\ncomplicating benchmarking, and hindering hardware optimization. We present a\nsystematic sensitivity analysis framework based on Saltelli's extension of\nSobol's method to quantify FMH impacts on GS-PINN performance. Our analysis\ndemonstrates that SLM pixel-resolution is the primary factor affecting neural\nnetwork sensitivity, followed by pixel-pitch, propagation distance, and\nwavelength. Free space propagation forward models demonstrate superior neural\nnetwork performance compared to Fourier holography, providing enhanced\nparameterization and generalization. We introduce a composite evaluation metric\ncombining performance consistency, generalization capability, and\nhyperparameter perturbation resilience, establishing a unified benchmarking\nstandard across CGH configurations. Our research connects physics-inspired deep\nlearning theory with practical CGH implementations through concrete guidelines\nfor forward model selection, neural network architecture, and performance\nevaluation. Our contributions advance the development of robust, interpretable,\nand generalizable neural networks for diverse holographic applications,\nsupporting evidence-based decisions in CGH research and implementation."
    },
    {
        "date": "2025-04",
        "title": "Efficient and robust 3D blind harmonization for large domain gaps",
        "author": "Hwihun Jeong, Hayeon Lee, Se Young Chun, and Jongho Lee",
        "link": "http://arxiv.org/abs/2505.00133v1",
        "abstract": "Blind harmonization has emerged as a promising technique for MR image\nharmonization to achieve scale-invariant representations, requiring only target\ndomain data (i.e., no source domain data necessary). However, existing methods\nface limitations such as inter-slice heterogeneity in 3D, moderate image\nquality, and limited performance for a large domain gap. To address these\nchallenges, we introduce BlindHarmonyDiff, a novel blind 3D harmonization\nframework that leverages an edge-to-image model tailored specifically to\nharmonization. Our framework employs a 3D rectified flow trained on target\ndomain images to reconstruct the original image from an edge map, then yielding\na harmonized image from the edge of a source domain image. We propose\nmulti-stride patch training for efficient 3D training and a refinement module\nfor robust inference by suppressing hallucination. Extensive experiments\ndemonstrate that BlindHarmonyDiff outperforms prior arts by harmonizing diverse\nsource domain images to the target domain, achieving higher correspondence to\nthe target domain characteristics. Downstream task-based quality assessments\nsuch as tissue segmentation and age prediction on diverse MR scanners further\nconfirm the effectiveness of our approach and demonstrate the capability of our\nrobust and generalizable blind harmonization."
    },
    {
        "date": "2025-04",
        "title": "Security-by-Design at the Telco Edge with OSS: Challenges and Lessons Learned",
        "author": "Carmine Cesarano, Alessio Foggia, Gianluca Roscigno, Luca Andreani, and Roberto Natella",
        "link": "http://arxiv.org/abs/2505.00111v1",
        "abstract": "This paper presents our experience, in the context of an industrial R&D\nproject, on securing GENIO, a platform for edge computing on Passive Optical\nNetwork (PON) infrastructures, and based on Open-Source Software (OSS). We\nidentify threats and related mitigations through hardening, vulnerability\nmanagement, digital signatures, and static and dynamic analysis. In particular,\nwe report lessons learned in applying these mitigations using OSS, and share\nour findings about the maturity and limitations of these security solutions in\nan industrial context."
    },
    {
        "date": "2025-04",
        "title": "Cert-SSB: Toward Certified Sample-Specific Backdoor Defense",
        "author": "Ting Qiao, Yingjia Wang, Xing Liu, Sixing Wu, Jianbing Li, and Yiming Li",
        "link": "http://arxiv.org/abs/2504.21730v1",
        "abstract": "Deep neural networks (DNNs) are vulnerable to backdoor attacks, where an\nattacker manipulates a small portion of the training data to implant hidden\nbackdoors into the model. The compromised model behaves normally on clean\nsamples but misclassifies backdoored samples into the attacker-specified target\nclass, posing a significant threat to real-world DNN applications. Currently,\nseveral empirical defense methods have been proposed to mitigate backdoor\nattacks, but they are often bypassed by more advanced backdoor techniques. In\ncontrast, certified defenses based on randomized smoothing have shown promise\nby adding random noise to training and testing samples to counteract backdoor\nattacks. In this paper, we reveal that existing randomized smoothing defenses\nimplicitly assume that all samples are equidistant from the decision boundary.\nHowever, it may not hold in practice, leading to suboptimal certification\nperformance. To address this issue, we propose a sample-specific certified\nbackdoor defense method, termed Cert-SSB. Cert-SSB first employs stochastic\ngradient ascent to optimize the noise magnitude for each sample, ensuring a\nsample-specific noise level that is then applied to multiple poisoned training\nsets to retrain several smoothed models. After that, Cert-SSB aggregates the\npredictions of multiple smoothed models to generate the final robust\nprediction. In particular, in this case, existing certification methods become\ninapplicable since the optimized noise varies across different samples. To\nconquer this challenge, we introduce a storage-update-based certification\nmethod, which dynamically adjusts each sample's certification region to improve\ncertification performance. We conduct extensive experiments on multiple\nbenchmark datasets, demonstrating the effectiveness of our proposed method. Our\ncode is available at https://github.com/NcepuQiaoTing/Cert-SSB."
    },
    {
        "date": "2025-04",
        "title": "Sparsification Under Siege: Defending Against Poisoning Attacks in Communication-Efficient Federated Learning",
        "author": "Zhiyong Jin, Runhua Xu, Chao Li, Yizhong Liu, and Jianxin Li",
        "link": "http://arxiv.org/abs/2505.01454v1",
        "abstract": "Federated Learning (FL) enables collaborative model training across\ndistributed clients while preserving data privacy, yet it faces significant\nchallenges in communication efficiency and vulnerability to poisoning attacks.\nWhile sparsification techniques mitigate communication overhead by transmitting\nonly critical model parameters, they inadvertently amplify security risks:\nadversarial clients can exploit sparse updates to evade detection and degrade\nmodel performance. Existing defense mechanisms, designed for standard FL\ncommunication scenarios, are ineffective in addressing these vulnerabilities\nwithin sparsified FL. To bridge this gap, we propose FLARE, a novel federated\nlearning framework that integrates sparse index mask inspection and model\nupdate sign similarity analysis to detect and mitigate poisoning attacks in\nsparsified FL. Extensive experiments across multiple datasets and adversarial\nscenarios demonstrate that FLARE significantly outperforms existing defense\nstrategies, effectively securing sparsified FL against poisoning attacks while\nmaintaining communication efficiency."
    },
    {
        "date": "2025-04",
        "title": "Enhancing Security and Strengthening Defenses in Automated Short-Answer Grading Systems",
        "author": "Sahar Yarmohammadtoosky, Yiyun Zhou, Victoria Yaneva, Peter Baldwin, Saed Rezayi, Brian Clauser, and Polina Harikeo",
        "link": "http://arxiv.org/abs/2505.00061v1",
        "abstract": "This study examines vulnerabilities in transformer-based automated\nshort-answer grading systems used in medical education, with a focus on how\nthese systems can be manipulated through adversarial gaming strategies. Our\nresearch identifies three main types of gaming strategies that exploit the\nsystem's weaknesses, potentially leading to false positives. To counteract\nthese vulnerabilities, we implement several adversarial training methods\ndesigned to enhance the systems' robustness. Our results indicate that these\nmethods significantly reduce the susceptibility of grading systems to such\nmanipulations, especially when combined with ensemble techniques like majority\nvoting and ridge regression, which further improve the system's defense against\nsophisticated adversarial inputs. Additionally, employing large language models\nsuch as GPT-4 with varied prompting techniques has shown promise in recognizing\nand scoring gaming strategies effectively. The findings underscore the\nimportance of continuous improvements in AI-driven educational tools to ensure\ntheir reliability and fairness in high-stakes settings."
    },
    {
        "date": "2025-04",
        "title": "Hoist with His Own Petard: Inducing Guardrails to Facilitate Denial-of-Service Attacks on Retrieval-Augmented Generation of LLMs",
        "author": "Pan Suo, Yu-Ming Shang, San-Chuan Guo, and Xi Zhang",
        "link": "http://arxiv.org/abs/2504.21680v1",
        "abstract": "Retrieval-Augmented Generation (RAG) integrates Large Language Models (LLMs)\nwith external knowledge bases, improving output quality while introducing new\nsecurity risks. Existing studies on RAG vulnerabilities typically focus on\nexploiting the retrieval mechanism to inject erroneous knowledge or malicious\ntexts, inducing incorrect outputs. However, these approaches overlook critical\nweaknesses within LLMs, leaving important attack vectors unexplored and\nlimiting the scope and efficiency of attacks. In this paper, we uncover a novel\nvulnerability: the safety guardrails of LLMs, while designed for protection,\ncan also be exploited as an attack vector by adversaries. Building on this\nvulnerability, we propose MutedRAG, a novel denial-of-service attack that\nreversely leverages the guardrails of LLMs to undermine the availability of RAG\nsystems. By injecting minimalistic jailbreak texts, such as \"\\textit{How to\nbuild a bomb}\", into the knowledge base, MutedRAG intentionally triggers the\nLLM's safety guardrails, causing the system to reject legitimate queries.\nBesides, due to the high sensitivity of guardrails, a single jailbreak sample\ncan affect multiple queries, effectively amplifying the efficiency of attacks\nwhile reducing their costs. Experimental results on three datasets demonstrate\nthat MutedRAG achieves an attack success rate exceeding 60% in many scenarios,\nrequiring only less than one malicious text to each target query on average. In\naddition, we evaluate potential defense strategies against MutedRAG, finding\nthat some of current mechanisms are insufficient to mitigate this threat,\nunderscoring the urgent need for more robust solutions."
    },
    {
        "date": "2025-04",
        "title": "Traceback of Poisoning Attacks to Retrieval-Augmented Generation",
        "author": "Baolei Zhang, Haoran Xin, Minghong Fang, Zhuqing Liu, Biao Yi, Tong Li, and Zheli Liu",
        "link": "http://arxiv.org/abs/2504.21668v1",
        "abstract": "Large language models (LLMs) integrated with retrieval-augmented generation\n(RAG) systems improve accuracy by leveraging external knowledge sources.\nHowever, recent research has revealed RAG's susceptibility to poisoning\nattacks, where the attacker injects poisoned texts into the knowledge database,\nleading to attacker-desired responses. Existing defenses, which predominantly\nfocus on inference-time mitigation, have proven insufficient against\nsophisticated attacks. In this paper, we introduce RAGForensics, the first\ntraceback system for RAG, designed to identify poisoned texts within the\nknowledge database that are responsible for the attacks. RAGForensics operates\niteratively, first retrieving a subset of texts from the database and then\nutilizing a specially crafted prompt to guide an LLM in detecting potential\npoisoning texts. Empirical evaluations across multiple datasets demonstrate the\neffectiveness of RAGForensics against state-of-the-art poisoning attacks. This\nwork pioneers the traceback of poisoned texts in RAG systems, providing a\npractical and promising defense mechanism to enhance their security."
    },
    {
        "date": "2025-04",
        "title": "Diffusion-based Adversarial Identity Manipulation for Facial Privacy Protection",
        "author": "Liqin Wang, Qianyue Hu, Wei Lu, and Xiangyang Luo",
        "link": "http://arxiv.org/abs/2504.21646v1",
        "abstract": "The success of face recognition (FR) systems has led to serious privacy\nconcerns due to potential unauthorized surveillance and user tracking on social\nnetworks. Existing methods for enhancing privacy fail to generate natural face\nimages that can protect facial privacy. In this paper, we propose\ndiffusion-based adversarial identity manipulation (DiffAIM) to generate natural\nand highly transferable adversarial faces against malicious FR systems. To be\nspecific, we manipulate facial identity within the low-dimensional latent space\nof a diffusion model. This involves iteratively injecting gradient-based\nadversarial identity guidance during the reverse diffusion process,\nprogressively steering the generation toward the desired adversarial faces. The\nguidance is optimized for identity convergence towards a target while promoting\nsemantic divergence from the source, facilitating effective impersonation while\nmaintaining visual naturalness. We further incorporate structure-preserving\nregularization to preserve facial structure consistency during manipulation.\nExtensive experiments on both face verification and identification tasks\ndemonstrate that compared with the state-of-the-art, DiffAIM achieves stronger\nblack-box attack transferability while maintaining superior visual quality. We\nalso demonstrate the effectiveness of the proposed approach for commercial FR\nAPIs, including Face++ and Aliyun."
    },
    {
        "date": "2025-04",
        "title": "A Comprehensive Study of Exploitable Patterns in Smart Contracts: From Vulnerability to Defense",
        "author": "Yuchen Ding, Hongli Peng, and Xiaoqi Li",
        "link": "http://arxiv.org/abs/2504.21480v1",
        "abstract": "With the rapid advancement of blockchain technology, smart contracts have\nenabled the implementation of increasingly complex functionalities. However,\nensuring the security of smart contracts remains a persistent challenge across\nthe stages of development, compilation, and execution. Vulnerabilities within\nsmart contracts not only undermine the security of individual applications but\nalso pose significant risks to the broader blockchain ecosystem, as\ndemonstrated by the growing frequency of attacks since 2016, resulting in\nsubstantial financial losses. This paper provides a comprehensive analysis of\nkey security risks in Ethereum smart contracts, specifically those written in\nSolidity and executed on the Ethereum Virtual Machine (EVM). We focus on two\nprevalent and critical vulnerability types (reentrancy and integer overflow) by\nexamining their underlying mechanisms, replicating attack scenarios, and\nassessing effective countermeasures."
    },
    {
        "date": "2025-04",
        "title": "Robust Orthogonal NMF with Label Propagation for Image Clustering",
        "author": "Jingjing Liu, Nian Wu, Xianchao Xiu, and Jianhua Zhang",
        "link": "http://arxiv.org/abs/2504.21472v1",
        "abstract": "Non-negative matrix factorization (NMF) is a popular unsupervised learning\napproach widely used in image clustering. However, in real-world clustering\nscenarios, most existing NMF methods are highly sensitive to noise corruption\nand are unable to effectively leverage limited supervised information. To\novercome these drawbacks, we propose a unified non-convex framework with label\npropagation called robust orthogonal nonnegative matrix factorization (RONMF).\nThis method not only considers the graph Laplacian and label propagation as\nregularization terms but also introduces a more effective non-convex structure\nto measure the reconstruction error and imposes orthogonal constraints on the\nbasis matrix to reduce the noise corruption, thereby achieving higher\nrobustness. To solve RONMF, we develop an alternating direction method of\nmultipliers (ADMM)-based optimization algorithm. In particular, all subproblems\nhave closed-form solutions, which ensures its efficiency. Experimental\nevaluations on eight public image datasets demonstrate that the proposed RONMF\noutperforms state-of-the-art NMF methods across various standard metrics and\nshows excellent robustness. The code will be available at\nhttps://github.com/slinda-liu."
    },
    {
        "date": "2025-04",
        "title": "Quaternion Nuclear Norms Over Frobenius Norms Minimization for Robust Matrix Completion",
        "author": "Yu Guo, Guoqing Chen, Tieyong Zeng, Qiyu Jin, and Michael Kwok-Po Ng",
        "link": "http://arxiv.org/abs/2504.21468v1",
        "abstract": "Recovering hidden structures from incomplete or noisy data remains a\npervasive challenge across many fields, particularly where multi-dimensional\ndata representation is essential. Quaternion matrices, with their ability to\nnaturally model multi-dimensional data, offer a promising framework for this\nproblem. This paper introduces the quaternion nuclear norm over the Frobenius\nnorm (QNOF) as a novel nonconvex approximation for the rank of quaternion\nmatrices. QNOF is parameter-free and scale-invariant. Utilizing quaternion\nsingular value decomposition, we prove that solving the QNOF can be simplified\nto solving the singular value $L_1/L_2$ problem. Additionally, we extend the\nQNOF to robust quaternion matrix completion, employing the alternating\ndirection multiplier method to derive solutions that guarantee weak convergence\nunder mild conditions. Extensive numerical experiments validate the proposed\nmodel's superiority, consistently outperforming state-of-the-art quaternion\nmethods."
    },
    {
        "date": "2025-04",
        "title": "FAST-Q: Fast-track Exploration with Adversarially Balanced State Representations for Counterfactual Action Estimation in Offline Reinforcement Learning",
        "author": "Pulkit Agrawal, Rukma Talwadker, Aditya Pareek, and Tridib Mukherjee",
        "link": "http://arxiv.org/abs/2504.21383v1",
        "abstract": "Recent advancements in state-of-the-art (SOTA) offline reinforcement learning\n(RL) have primarily focused on addressing function approximation errors, which\ncontribute to the overestimation of Q-values for out-of-distribution actions, a\nchallenge that static datasets exacerbate. However, high stakes applications\nsuch as recommendation systems in online gaming, introduce further complexities\ndue to player's psychology (intent) driven by gameplay experiences and the\ninherent volatility on the platform. These factors create highly sparse,\npartially overlapping state spaces across policies, further influenced by the\nexperiment path selection logic which biases state spaces towards specific\npolicies. Current SOTA methods constrain learning from such offline data by\nclipping known counterfactual actions as out-of-distribution due to poor\ngeneralization across unobserved states. Further aggravating conservative\nQ-learning and necessitating more online exploration. FAST-Q introduces a novel\napproach that (1) leverages Gradient Reversal Learning to construct balanced\nstate representations, regularizing the policy-specific bias between the\nplayer's state and action thereby enabling counterfactual estimation; (2)\nsupports offline counterfactual exploration in parallel with static data\nexploitation; and (3) proposes a Q-value decomposition strategy for\nmulti-objective optimization, facilitating explainable recommendations over\nshort and long-term objectives. These innovations demonstrate superiority of\nFAST-Q over prior SOTA approaches and demonstrates at least 0.15 percent\nincrease in player returns, 2 percent improvement in lifetime value (LTV), 0.4\npercent enhancement in the recommendation driven engagement, 2 percent\nimprovement in the player's platform dwell time and an impressive 10 percent\nreduction in the costs associated with the recommendation, on our volatile\ngaming platform."
    },
    {
        "date": "2025-04",
        "title": "Synergy-CLIP: Extending CLIP with Multi-modal Integration for Robust Representation Learning",
        "author": "Sangyeon Cho, Jangyeong Jeon, Mingi Kim, and Junyeong Kim",
        "link": "http://arxiv.org/abs/2504.21375v1",
        "abstract": "Multi-modal representation learning has become a pivotal area in artificial\nintelligence, enabling the integration of diverse modalities such as vision,\ntext, and audio to solve complex problems. However, existing approaches\npredominantly focus on bimodal interactions, such as image-text pairs, which\nlimits their ability to fully exploit the richness of multi-modal data.\nFurthermore, the integration of modalities in equal-scale environments remains\nunderexplored due to the challenges of constructing large-scale, balanced\ndatasets. In this study, we propose Synergy-CLIP, a novel framework that\nextends the contrastive language-image pre-training (CLIP) architecture to\nenhance multi-modal representation learning by integrating visual, textual, and\naudio modalities. Unlike existing methods that focus on adapting individual\nmodalities to vanilla-CLIP, Synergy-CLIP aligns and captures latent information\nacross three modalities equally. To address the high cost of constructing\nlarge-scale multi-modal datasets, we introduce VGG-sound+, a triple-modal\ndataset designed to provide equal-scale representation of visual, textual, and\naudio data. Synergy-CLIP is validated on various downstream tasks, including\nzero-shot classification, where it outperforms existing baselines.\nAdditionally, we introduce a missing modality reconstruction task,\ndemonstrating Synergy-CLIP's ability to extract synergy among modalities in\nrealistic application scenarios. These contributions provide a robust\nfoundation for advancing multi-modal representation learning and exploring new\nresearch directions."
    },
    {
        "date": "2025-04",
        "title": "The Dual Power of Interpretable Token Embeddings: Jailbreaking Attacks and Defenses for Diffusion Model Unlearning",
        "author": "Siyi Chen, Yimeng Zhang, Sijia Liu, and Qing Qu",
        "link": "http://arxiv.org/abs/2504.21307v1",
        "abstract": "Despite the remarkable generalization capabilities of diffusion models,\nrecent studies have shown that these models can memorize and generate harmful\ncontent when prompted with specific text instructions. Although fine-tuning\napproaches have been developed to mitigate this issue by unlearning harmful\nconcepts, these methods can be easily circumvented through jailbreaking\nattacks. This indicates that the harmful concept has not been fully erased from\nthe model. However, existing attack methods, while effective, lack\ninterpretability regarding why unlearned models still retain the concept,\nthereby hindering the development of defense strategies. In this work, we\naddress these limitations by proposing an attack method that learns an\northogonal set of interpretable attack token embeddings. The attack token\nembeddings can be decomposed into human-interpretable textual elements,\nrevealing that unlearned models still retain the target concept through\nimplicit textual components. Furthermore, these attack token embeddings are\nrobust and transferable across text prompts, initial noises, and unlearned\nmodels. Finally, leveraging this diverse set of embeddings, we design a defense\nmethod applicable to both our proposed attack and existing attack methods.\nExperimental results demonstrate the effectiveness of both our attack and\ndefense strategies."
    },
    {
        "date": "2025-04",
        "title": "CachePrune: Neural-Based Attribution Defense Against Indirect Prompt Injection Attacks",
        "author": "Rui Wang, Junda Wu, Yu Xia, Tong Yu, Ruiyi Zhang, Ryan Rossi, Lina Yao, and Julian McAuley",
        "link": "http://arxiv.org/abs/2504.21228v1",
        "abstract": "Large Language Models (LLMs) are identified as being susceptible to indirect\nprompt injection attack, where the model undesirably deviates from\nuser-provided instructions by executing tasks injected in the prompt context.\nThis vulnerability stems from LLMs' inability to distinguish between data and\ninstructions within a prompt. In this paper, we propose CachePrune that defends\nagainst this attack by identifying and pruning task-triggering neurons from the\nKV cache of the input prompt context. By pruning such neurons, we encourage the\nLLM to treat the text spans of input prompt context as only pure data, instead\nof any indicator of instruction following. These neurons are identified via\nfeature attribution with a loss function induced from an upperbound of the\nDirect Preference Optimization (DPO) objective. We show that such a loss\nfunction enables effective feature attribution with only a few samples. We\nfurther improve on the quality of feature attribution, by exploiting an\nobserved triggering effect in instruction following. Our approach does not\nimpose any formatting on the original prompt or introduce extra test-time LLM\ncalls. Experiments show that CachePrune significantly reduces attack success\nrates without compromising the response quality. Note: This paper aims to\ndefend against indirect prompt injection attacks, with the goal of developing\nmore secure and robust AI systems."
    },
    {
        "date": "2025-04",
        "title": "SecRepoBench: Benchmarking LLMs for Secure Code Generation in Real-World Repositories",
        "author": "Connor Dilgren, Purva Chiniya, Luke Griffith, Yu Ding, and Yizheng Chen",
        "link": "http://arxiv.org/abs/2504.21205v1",
        "abstract": "This paper introduces SecRepoBench, a benchmark to evaluate LLMs on secure\ncode generation in real-world repositories. SecRepoBench has 318 code\ngeneration tasks in 27 C/C++ repositories, covering 15 CWEs. We evaluate 19\nstate-of-the-art LLMs using our benchmark and find that the models struggle\nwith generating correct and secure code. In addition, the performance of LLMs\nto generate self-contained programs as measured by prior benchmarks do not\ntranslate to comparative performance at generating secure and correct code at\nthe repository level in SecRepoBench. We show that the state-of-the-art prompt\nengineering techniques become less effective when applied to the repository\nlevel secure code generation problem. We conduct extensive experiments,\nincluding an agentic technique to generate secure code, to demonstrate that our\nbenchmark is currently the most difficult secure coding benchmark, compared to\nprevious state-of-the-art benchmarks. Finally, our comprehensive analysis\nprovides insights into potential directions for enhancing the ability of LLMs\nto generate correct and secure code in real-world repositories."
    },
    {
        "date": "2025-04",
        "title": "ACE: A Security Architecture for LLM-Integrated App Systems",
        "author": "Evan Li, Tushin Mallick, Evan Rose, William Robertson, Alina Oprea, and Cristina Nita-Rotaru",
        "link": "http://arxiv.org/abs/2504.20984v1",
        "abstract": "LLM-integrated app systems extend the utility of Large Language Models (LLMs)\nwith third-party apps that are invoked by a system LLM using interleaved\nplanning and execution phases to answer user queries. These systems introduce\nnew attack vectors where malicious apps can cause integrity violation of\nplanning or execution, availability breakdown, or privacy compromise during\nexecution.\n  In this work, we identify new attacks impacting the integrity of planning, as\nwell as the integrity and availability of execution in LLM-integrated apps, and\ndemonstrate them against IsolateGPT, a recent solution designed to mitigate\nattacks from malicious apps. We propose Abstract-Concrete-Execute (ACE), a new\nsecure architecture for LLM-integrated app systems that provides security\nguarantees for system planning and execution. Specifically, ACE decouples\nplanning into two phases by first creating an abstract execution plan using\nonly trusted information, and then mapping the abstract plan to a concrete plan\nusing installed system apps. We verify that the plans generated by our system\nsatisfy user-specified secure information flow constraints via static analysis\non the structured plan output. During execution, ACE enforces data and\ncapability barriers between apps, and ensures that the execution is conducted\naccording to the trusted abstract plan. We show experimentally that our system\nis secure against attacks from the INJECAGENT benchmark, a standard benchmark\nfor control flow integrity in the face of indirect prompt injection attacks,\nand our newly introduced attacks. Our architecture represents a significant\nadvancement towards hardening LLM-based systems containing system facilities of\nvarying levels of trustworthiness."
    },
    {
        "date": "2025-04",
        "title": "AegisLLM: Scaling Agentic Systems for Self-Reflective Defense in LLM Security",
        "author": "Zikui Cai, Shayan Shabihi, Bang An, Zora Che, Brian R. Bartoldson, Bhavya Kailkhura, Tom Goldstein, and Furong Huang",
        "link": "http://arxiv.org/abs/2504.20965v1",
        "abstract": "We introduce AegisLLM, a cooperative multi-agent defense against adversarial\nattacks and information leakage. In AegisLLM, a structured workflow of\nautonomous agents - orchestrator, deflector, responder, and evaluator -\ncollaborate to ensure safe and compliant LLM outputs, while self-improving over\ntime through prompt optimization. We show that scaling agentic reasoning system\nat test-time - both by incorporating additional agent roles and by leveraging\nautomated prompt optimization (such as DSPy)- substantially enhances robustness\nwithout compromising model utility. This test-time defense enables real-time\nadaptability to evolving attacks, without requiring model retraining.\nComprehensive evaluations across key threat scenarios, including unlearning and\njailbreaking, demonstrate the effectiveness of AegisLLM. On the WMDP unlearning\nbenchmark, AegisLLM achieves near-perfect unlearning with only 20 training\nexamples and fewer than 300 LM calls. For jailbreaking benchmarks, we achieve\n51% improvement compared to the base model on StrongReject, with false refusal\nrates of only 7.9% on PHTest compared to 18-55% for comparable methods. Our\nresults highlight the advantages of adaptive, agentic reasoning over static\ndefenses, establishing AegisLLM as a strong runtime alternative to traditional\napproaches based on model modifications. Code is available at\nhttps://github.com/zikuicai/aegisllm"
    },
    {
        "date": "2025-04",
        "title": "Quantifying the Noise of Structural Perturbations on Graph Adversarial Attacks",
        "author": "Junyuan Fang, Han Yang, Haixian Wen, Jiajing Wu, Zibin Zheng, and Chi K. Tse",
        "link": "http://arxiv.org/abs/2504.20869v2",
        "abstract": "Graph neural networks have been widely utilized to solve graph-related tasks\nbecause of their strong learning power in utilizing the local information of\nneighbors. However, recent studies on graph adversarial attacks have proven\nthat current graph neural networks are not robust against malicious attacks.\nYet much of the existing work has focused on the optimization objective based\non attack performance to obtain (near) optimal perturbations, but paid less\nattention to the strength quantification of each perturbation such as the\ninjection of a particular node/link, which makes the choice of perturbations a\nblack-box model that lacks interpretability. In this work, we propose the\nconcept of noise to quantify the attack strength of each adversarial link.\nFurthermore, we propose three attack strategies based on the defined noise and\nclassification margins in terms of single and multiple steps optimization.\nExtensive experiments conducted on benchmark datasets against three\nrepresentative graph neural networks demonstrate the effectiveness of the\nproposed attack strategies. Particularly, we also investigate the preferred\npatterns of effective adversarial perturbations by analyzing the corresponding\nproperties of the selected perturbation nodes."
    },
    {
        "date": "2025-04",
        "title": "Mitigating the Structural Bias in Graph Adversarial Defenses",
        "author": "Junyuan Fang, Huimin Liu, Han Yang, Jiajing Wu, Zibin Zheng, and Chi K. Tse",
        "link": "http://arxiv.org/abs/2504.20848v1",
        "abstract": "In recent years, graph neural networks (GNNs) have shown great potential in\naddressing various graph structure-related downstream tasks. However, recent\nstudies have found that current GNNs are susceptible to malicious adversarial\nattacks. Given the inevitable presence of adversarial attacks in the real\nworld, a variety of defense methods have been proposed to counter these attacks\nand enhance the robustness of GNNs. Despite the commendable performance of\nthese defense methods, we have observed that they tend to exhibit a structural\nbias in terms of their defense capability on nodes with low degree (i.e., tail\nnodes), which is similar to the structural bias of traditional GNNs on nodes\nwith low degree in the clean graph. Therefore, in this work, we propose a\ndefense strategy by including hetero-homo augmented graph construction, $k$NN\naugmented graph construction, and multi-view node-wise attention modules to\nmitigate the structural bias of GNNs against adversarial attacks. Notably, the\nhetero-homo augmented graph consists of removing heterophilic links (i.e.,\nlinks connecting nodes with dissimilar features) globally and adding homophilic\nlinks (i.e., links connecting nodes with similar features) for nodes with low\ndegree. To further enhance the defense capability, an attention mechanism is\nadopted to adaptively combine the representations from the above two kinds of\ngraph views. We conduct extensive experiments to demonstrate the defense and\ndebiasing effect of the proposed strategy on benchmark datasets."
    },
    {
        "date": "2025-04",
        "title": "GaussTrap: Stealthy Poisoning Attacks on 3D Gaussian Splatting for Targeted Scene Confusion",
        "author": "Jiaxin Hong, Sixu Chen, Shuoyang Sun, Hongyao Yu, Hao Fang, Yuqi Tan, Bin Chen, Shuhan Qi, and Jiawei Li",
        "link": "http://arxiv.org/abs/2504.20829v1",
        "abstract": "As 3D Gaussian Splatting (3DGS) emerges as a breakthrough in scene\nrepresentation and novel view synthesis, its rapid adoption in safety-critical\ndomains (e.g., autonomous systems, AR/VR) urgently demands scrutiny of\npotential security vulnerabilities. This paper presents the first systematic\nstudy of backdoor threats in 3DGS pipelines. We identify that adversaries may\nimplant backdoor views to induce malicious scene confusion during inference,\npotentially leading to environmental misperception in autonomous navigation or\nspatial distortion in immersive environments. To uncover this risk, we propose\nGuassTrap, a novel poisoning attack method targeting 3DGS models. GuassTrap\ninjects malicious views at specific attack viewpoints while preserving\nhigh-quality rendering in non-target views, ensuring minimal detectability and\nmaximizing potential harm. Specifically, the proposed method consists of a\nthree-stage pipeline (attack, stabilization, and normal training) to implant\nstealthy, viewpoint-consistent poisoned renderings in 3DGS, jointly optimizing\nattack efficacy and perceptual realism to expose security risks in 3D\nrendering. Extensive experiments on both synthetic and real-world datasets\ndemonstrate that GuassTrap can effectively embed imperceptible yet harmful\nbackdoor views while maintaining high-quality rendering in normal views,\nvalidating its robustness, adaptability, and practical applicability."
    },
    {
        "date": "2025-04",
        "title": "Secure Coding with AI, From Creation to Inspection",
        "author": "Vladislav Belozerov, Peter J Barclay, and Ashkan Sami",
        "link": "http://arxiv.org/abs/2504.20814v1",
        "abstract": "While prior studies have explored security in code generated by ChatGPT and\nother Large Language Models, they were conducted in controlled experimental\nsettings and did not use code generated or provided from actual developer\ninteractions. This paper not only examines the security of code generated by\nChatGPT based on real developer interactions, curated in the DevGPT dataset,\nbut also assesses ChatGPT's capability to find and fix these vulnerabilities.\nWe analysed 1,586 C, C++, and C# code snippets using static scanners, which\ndetected potential issues in 124 files. After manual analysis, we selected 26\nfiles with 32 confirmed vulnerabilities for further investigation.\n  We submitted these files to ChatGPT via the OpenAI API, asking it to detect\nsecurity issues, identify the corresponding Common Weakness Enumeration\nnumbers, and propose fixes. The responses and modified code were manually\nreviewed and re-scanned for vulnerabilities. ChatGPT successfully detected 18\nout of 32 security issues and resolved 17 issues but failed to recognize or fix\nthe remainder. Interestingly, only 10 vulnerabilities were resulted from the\nuser prompts, while 22 were introduced by ChatGPT itself.\n  We highlight for developers that code generated by ChatGPT is more likely to\ncontain vulnerabilities compared to their own code. Furthermore, at times\nChatGPT reports incorrect information with apparent confidence, which may\nmislead less experienced developers. Our findings confirm previous studies in\ndemonstrating that ChatGPT is not sufficiently reliable for generating secure\ncode nor identifying all vulnerabilities, highlighting the continuing\nimportance of static scanners and manual review."
    },
    {
        "date": "2025-04",
        "title": "R^2VFL: A Robust Random Vector Functional Link Network with Huber-Weighted Framework",
        "author": "Anuradha Kumari, Mushir Akhtar, P. N. Suganthan, and M. Tanveer",
        "link": "http://arxiv.org/abs/2504.21069v1",
        "abstract": "The random vector functional link (RVFL) neural network has shown significant\npotential in overcoming the constraints of traditional artificial neural\nnetworks, such as excessive computation time and suboptimal solutions. However,\nRVFL faces challenges when dealing with noise and outliers, as it assumes all\ndata samples contribute equally. To address this issue, we propose a novel\nrobust framework, R2VFL, RVFL with Huber weighting function and class\nprobability, which enhances the model's robustness and adaptability by\neffectively mitigating the impact of noise and outliers in the training data.\nThe Huber weighting function reduces the influence of outliers, while the class\nprobability mechanism assigns less weight to noisy data points, resulting in a\nmore resilient model. We explore two distinct approaches for calculating class\ncenters within the R2VFL framework: the simple average of all data points in\neach class and the median of each feature, the later providing a robust\nalternative by minimizing the effect of extreme values. These approaches give\nrise to two novel variants of the model-R2VFL-A and R2VFL-M. We extensively\nevaluate the proposed models on 47 UCI datasets, encompassing both binary and\nmulticlass datasets, and conduct rigorous statistical testing, which confirms\nthe superiority of the proposed models. Notably, the models also demonstrate\nexceptional performance in classifying EEG signals, highlighting their\npractical applicability in real-world biomedical domain."
    },
    {
        "date": "2025-04",
        "title": "Chain-of-Defensive-Thought: Structured Reasoning Elicits Robustness in Large Language Models against Reference Corruption",
        "author": "Wenxiao Wang, Parsa Hosseini, and Soheil Feizi",
        "link": "http://arxiv.org/abs/2504.20769v1",
        "abstract": "Chain-of-thought prompting has demonstrated great success in facilitating the\nreasoning abilities of large language models. In this work, we explore how\nthese enhanced reasoning abilities can be exploited to improve the robustness\nof large language models in tasks that are not necessarily reasoning-focused.\nIn particular, we show how a wide range of large language models exhibit\nsignificantly improved robustness against reference corruption using a simple\nmethod called chain-of-defensive-thought, where only a few exemplars with\nstructured and defensive reasoning are provided as demonstrations. Empirically,\nthe improvements can be astounding, especially given the simplicity and\napplicability of the method. For example, in the Natural Questions task, the\naccuracy of GPT-4o degrades from 60% to as low as 3% with standard prompting\nwhen 1 out of 10 references provided is corrupted with prompt injection\nattacks. In contrast, GPT-4o using chain-of-defensive-thought prompting\nmaintains an accuracy of 50%."
    },
    {
        "date": "2025-04",
        "title": "Data Encryption Battlefield: A Deep Dive into the Dynamic Confrontations in Ransomware Attacks",
        "author": "Arash Mahboubi, Hamed Aboutorab, Seyit Camtepe, Hang Thanh Bui, Khanh Luong, Keyvan Ansari, Shenlu Wang, and Bazara Barry",
        "link": "http://arxiv.org/abs/2504.20681v1",
        "abstract": "In the rapidly evolving landscape of cybersecurity threats, ransomware\nrepresents a significant challenge. Attackers increasingly employ sophisticated\nencryption methods, such as entropy reduction through Base64 encoding, and\npartial or intermittent encryption to evade traditional detection methods. This\nstudy explores the dynamic battle between adversaries who continuously refine\nencryption strategies and defenders developing advanced countermeasures to\nprotect vulnerable data. We investigate the application of online incremental\nmachine learning algorithms designed to predict file encryption activities\ndespite adversaries evolving obfuscation techniques. Our analysis utilizes an\nextensive dataset of 32.6 GB, comprising 11,928 files across multiple formats,\nincluding Microsoft Word documents (doc), PowerPoint presentations (ppt), Excel\nspreadsheets (xlsx), image formats (jpg, jpeg, png, tif, gif), PDFs (pdf),\naudio (mp3), and video (mp4) files. These files were encrypted by 75 distinct\nransomware families, facilitating a robust empirical evaluation of machine\nlearning classifiers effectiveness against diverse encryption tactics. Results\nhighlight the Hoeffding Tree algorithms superior incremental learning\ncapability, particularly effective in detecting traditional and AES-Base64\nencryption methods employed to lower entropy. Conversely, the Random Forest\nclassifier with warm-start functionality excels at identifying intermittent\nencryption methods, demonstrating the necessity of tailored machine learning\nsolutions to counter sophisticated ransomware strategies."
    },
    {
        "date": "2025-04",
        "title": "A Novel Cipher for Enhancing MAVLink Security: Design, Security Analysis, and Performance Evaluation Using a Drone Testbed",
        "author": "Bhavya Dixit, Ananthapadmanabhan A., Adheeba Thahsin, Saketh Pathak, Gaurav S. Kasbekar, and Arnab Maity",
        "link": "http://arxiv.org/abs/2504.20626v1",
        "abstract": "We present MAVShield, a novel lightweight cipher designed to secure\ncommunications in Unmanned Aerial Vehicles (UAVs) using the MAVLink protocol,\nwhich by default transmits unencrypted messages between UAVs and Ground Control\nStations (GCS). While existing studies propose encryption for MAVLink, most\nremain theoretical or simulation-based. We implement MAVShield alongside\nAES-CTR, ChaCha20, Speck-CTR, and Rabbit, and evaluate them on a real drone\ntestbed. A comprehensive security analysis using statistical test suites (NIST\nand Diehard) demonstrates strong resistance of the novel cipher to\ncryptanalysis. Performance evaluation across key metrics including memory\nusage, CPU load, and battery power consumption, demonstrates that MAVShield\noutperforms existing algorithms and offers an efficient, real-world solution\nfor securing MAVLink communications in UAVs."
    },
    {
        "date": "2025-04",
        "title": "The Hidden Risks of LLM-Generated Web Application Code: A Security-Centric Evaluation of Code Generation Capabilities in Large Language Models",
        "author": "Swaroop Dora, Deven Lunkad, Naziya Aslam, S. Venkatesan, and Sandeep Kumar Shukla",
        "link": "http://arxiv.org/abs/2504.20612v1",
        "abstract": "The rapid advancement of Large Language Models (LLMs) has enhanced software\ndevelopment processes, minimizing the time and effort required for coding and\nenhancing developer productivity. However, despite their potential benefits,\ncode generated by LLMs has been shown to generate insecure code in controlled\nenvironments, raising critical concerns about their reliability and security in\nreal-world applications. This paper uses predefined security parameters to\nevaluate the security compliance of LLM-generated code across multiple models,\nsuch as ChatGPT, DeepSeek, Claude, Gemini and Grok. The analysis reveals\ncritical vulnerabilities in authentication mechanisms, session management,\ninput validation and HTTP security headers. Although some models implement\nsecurity measures to a limited extent, none fully align with industry best\npractices, highlighting the associated risks in automated software development.\nOur findings underscore that human expertise is crucial to ensure secure\nsoftware deployment or review of LLM-generated code. Also, there is a need for\nrobust security assessment frameworks to enhance the reliability of\nLLM-generated code in real-world applications."
    },
    {
        "date": "2025-04",
        "title": "VIMU: Effective Physics-based Realtime Detection and Recovery against Stealthy Attacks on UAVs",
        "author": "Yunbo Wang, Cong Sun, Qiaosen Liu, Bingnan Su, Zongxu Zhang, Michael Norris, Gang Tan, and Jianfeng Ma",
        "link": "http://arxiv.org/abs/2504.20569v1",
        "abstract": "Sensor attacks on robotic vehicles have become pervasive and manipulative.\nTheir latest advancements exploit sensor and detector characteristics to bypass\ndetection. Recent security efforts have leveraged the physics-based model to\ndetect or mitigate sensor attacks. However, these approaches are only resilient\nto a few sensor attacks and still need improvement in detection effectiveness.\nWe present VIMU, an efficient sensor attack detection and resilience system for\nunmanned aerial vehicles. We propose a detection algorithm, CS-EMA, that\nleverages low-pass filtering to identify stealthy gyroscope attacks while\nachieving an overall effective sensor attack detection. We develop a\nfine-grained nonlinear physical model with precise aerodynamic and propulsion\nwrench modeling. We also augment the state estimation with a FIFO buffer\nsafeguard to mitigate the impact of high-rate IMU attacks. The proposed\nphysical model and buffer safeguard provide an effective system state recovery\ntoward maintaining flight stability. We implement VIMU on PX4 autopilot. The\nevaluation results demonstrate the effectiveness of VIMU in detecting and\nmitigating various realistic sensor attacks, especially stealthy attacks."
    },
    {
        "date": "2025-04",
        "title": "Digital Shielding for Cross-Domain Wi-Fi Signal Adaptation using Relativistic Average Generative Adversarial Network",
        "author": "Danilo Avola, Federica Bruni, Gian Luca Foresti, Daniele Pannone, and Amedeo Ranaldi",
        "link": "http://arxiv.org/abs/2504.20568v1",
        "abstract": "Wi-Fi sensing uses radio-frequency signals from Wi-Fi devices to analyze\nenvironments, enabling tasks such as tracking people, detecting intrusions, and\nrecognizing gestures. The rise of this technology is driven by the IEEE\n802.11bf standard and growing demand for tools that can ensure privacy and\noperate through obstacles. However, the performance of Wi-Fi sensing is heavily\ninfluenced by environmental conditions, especially when extracting spatial and\ntemporal features from the surrounding scene. A key challenge is achieving\nrobust generalization across domains, ensuring stable performance even when the\nsensing environment changes significantly. This paper introduces a novel deep\nlearning model for cross-domain adaptation of Wi-Fi signals, inspired by\nphysical signal shielding. The model uses a Relativistic average Generative\nAdversarial Network (RaGAN) with Bidirectional Long Short-Term Memory (Bi-LSTM)\narchitectures for both the generator and discriminator. To simulate physical\nshielding, an acrylic box lined with electromagnetic shielding fabric was\nconstructed, mimicking a Faraday cage. Wi-Fi signal spectra were collected from\nvarious materials both inside (domain-free) and outside (domain-dependent) the\nbox to train the model. A multi-class Support Vector Machine (SVM) was trained\non domain-free spectra and tested on signals denoised by the RaGAN. The system\nachieved 96% accuracy and demonstrated strong material discrimination\ncapabilities, offering potential for use in security applications to identify\nconcealed objects based on their composition."
    },
    {
        "date": "2025-04",
        "title": "Mutual Information Minimization for Side-Channel Attack Resistance via Optimal Noise Injection",
        "author": "Jiheon Woo, Daewon Seo, Young-Sik Kim, Namyoon Lee, Yuval Cassuto, and Yongjune Kim",
        "link": "http://arxiv.org/abs/2504.20556v1",
        "abstract": "Side-channel attacks (SCAs) pose a serious threat to system security by\nextracting secret keys through physical leakages such as power consumption,\ntiming variations, and electromagnetic emissions. Among existing\ncountermeasures, artificial noise injection is recognized as one of the most\neffective techniques. However, its high power consumption poses a major\nchallenge for resource-constrained systems such as Internet of Things (IoT)\ndevices, motivating the development of more efficient protection schemes. In\nthis paper, we model SCAs as a communication channel and aim to suppress\ninformation leakage by minimizing the mutual information between the secret\ninformation and side-channel observations, subject to a power constraint on the\nartificial noise. We propose an optimal artificial noise injection method to\nminimize the mutual information in systems with Gaussian inputs. Specifically,\nwe formulate two convex optimization problems: 1) minimizing the total mutual\ninformation, and 2) minimizing the maximum mutual information across\nobservations. Numerical results show that the proposed methods significantly\nreduce both total and maximum mutual information compared to conventional\ntechniques, confirming their effectiveness for resource-constrained,\nsecurity-critical systems."
    },
    {
        "date": "2025-04",
        "title": "TriniMark: A Robust Generative Speech Watermarking Method for Trinity-Level Attribution",
        "author": "Yue Li, Weizhi Liu, and Dongdong Lin",
        "link": "http://arxiv.org/abs/2504.20532v1",
        "abstract": "The emergence of diffusion models has facilitated the generation of speech\nwith reinforced fidelity and naturalness. While deepfake detection technologies\nhave manifested the ability to identify AI-generated content, their efficacy\ndecreases as generative models become increasingly sophisticated. Furthermore,\ncurrent research in the field has not adequately addressed the necessity for\nrobust watermarking to safeguard the intellectual property rights associated\nwith synthetic speech and generative models. To remedy this deficiency, we\npropose a \\textbf{ro}bust generative \\textbf{s}peech wat\\textbf{e}rmarking\nmethod (TriniMark) for authenticating the generated content and safeguarding\nthe copyrights by enabling the traceability of the diffusion model. We first\ndesign a structure-lightweight watermark encoder that embeds watermarks into\nthe time-domain features of speech and reconstructs the waveform directly. A\ntemporal-aware gated convolutional network is meticulously designed in the\nwatermark decoder for bit-wise watermark recovery. Subsequently, the\nwaveform-guided fine-tuning strategy is proposed for fine-tuning the diffusion\nmodel, which leverages the transferability of watermarks and enables the\ndiffusion model to incorporate watermark knowledge effectively. When an\nattacker trains a surrogate model using the outputs of the target model, the\nembedded watermark can still be learned by the surrogate model and correctly\nextracted. Comparative experiments with state-of-the-art methods demonstrate\nthe superior robustness of our method, particularly in countering compound\nattacks."
    },
    {
        "date": "2025-04",
        "title": "SAM-Guided Robust Representation Learning for One-Shot 3D Medical Image Segmentation",
        "author": "Jia Wang, Yunan Mei, Jiarui Liu, and Xin Fan",
        "link": "http://arxiv.org/abs/2504.20501v1",
        "abstract": "One-shot medical image segmentation (MIS) is crucial for medical analysis due\nto the burden of medical experts on manual annotation. The recent emergence of\nthe segment anything model (SAM) has demonstrated remarkable adaptation in MIS\nbut cannot be directly applied to one-shot medical image segmentation (MIS) due\nto its reliance on labor-intensive user interactions and the high computational\ncost. To cope with these limitations, we propose a novel SAM-guided robust\nrepresentation learning framework, named RRL-MedSAM, to adapt SAM to one-shot\n3D MIS, which exploits the strong generalization capabilities of the SAM\nencoder to learn better feature representation. We devise a dual-stage\nknowledge distillation (DSKD) strategy to distill general knowledge between\nnatural and medical images from the foundation model to train a lightweight\nencoder, and then adopt a mutual exponential moving average (mutual-EMA) to\nupdate the weights of the general lightweight encoder and medical-specific\nencoder. Specifically, pseudo labels from the registration network are used to\nperform mutual supervision for such two encoders. Moreover, we introduce an\nauto-prompting (AP) segmentation decoder which adopts the mask generated from\nthe general lightweight model as a prompt to assist the medical-specific model\nin boosting the final segmentation performance. Extensive experiments conducted\non three public datasets, i.e., OASIS, CT-lung demonstrate that the proposed\nRRL-MedSAM outperforms state-of-the-art one-shot MIS methods for both\nsegmentation and registration tasks. Especially, our lightweight encoder uses\nonly 3\\% of the parameters compared to the encoder of SAM-Base."
    },
    {
        "date": "2025-04",
        "title": "Token-Efficient Prompt Injection Attack: Provoking Cessation in LLM Reasoning via Adaptive Token Compression",
        "author": "Yu Cui, Yujun Cai, and Yiwei Wang",
        "link": "http://arxiv.org/abs/2504.20493v1",
        "abstract": "While reasoning large language models (LLMs) demonstrate remarkable\nperformance across various tasks, they also contain notable security\nvulnerabilities. Recent research has uncovered a \"thinking-stopped\"\nvulnerability in DeepSeek-R1, where model-generated reasoning tokens can\nforcibly interrupt the inference process, resulting in empty responses that\ncompromise LLM-integrated applications. However, existing methods triggering\nthis vulnerability require complex mathematical word problems with long\nprompts--even exceeding 5,000 tokens. To reduce the token cost and formally\ndefine this vulnerability, we propose a novel prompt injection attack named\n\"Reasoning Interruption Attack\", based on adaptive token compression. We\ndemonstrate that simple standalone arithmetic tasks can effectively trigger\nthis vulnerability, and the prompts based on such tasks exhibit simpler logical\nstructures than mathematical word problems. We develop a systematic approach to\nefficiently collect attack prompts and an adaptive token compression framework\nthat utilizes LLMs to automatically compress these prompts. Experiments show\nour compression framework significantly reduces prompt length while maintaining\neffective attack capabilities. We further investigate the attack's performance\nvia output prefix and analyze the underlying causes of the vulnerability,\nproviding valuable insights for improving security in reasoning LLMs."
    },
    {
        "date": "2025-04",
        "title": "Robustness via Referencing: Defending against Prompt Injection Attacks by Referencing the Executed Instruction",
        "author": "Yulin Chen, Haoran Li, Yuan Sui, Yue Liu, Yufei He, Yangqiu Song, and Bryan Hooi",
        "link": "http://arxiv.org/abs/2504.20472v1",
        "abstract": "Large language models (LLMs) have demonstrated impressive performance and\nhave come to dominate the field of natural language processing (NLP) across\nvarious tasks. However, due to their strong instruction-following capabilities\nand inability to distinguish between instructions and data content, LLMs are\nvulnerable to prompt injection attacks. These attacks manipulate LLMs into\ndeviating from the original input instructions and executing maliciously\ninjected instructions within data content, such as web documents retrieved from\nsearch engines. Existing defense methods, including prompt-engineering and\nfine-tuning approaches, typically instruct models to follow the original input\ninstructions while suppressing their tendencies to execute injected\ninstructions. However, our experiments reveal that suppressing\ninstruction-following tendencies is challenging. Through analyzing failure\ncases, we observe that although LLMs tend to respond to any recognized\ninstructions, they are aware of which specific instructions they are executing\nand can correctly reference them within the original prompt. Motivated by these\nfindings, we propose a novel defense method that leverages, rather than\nsuppresses, the instruction-following abilities of LLMs. Our approach prompts\nLLMs to generate responses that include both answers and their corresponding\ninstruction references. Based on these references, we filter out answers not\nassociated with the original input instructions. Comprehensive experiments\ndemonstrate that our method outperforms prompt-engineering baselines and\nachieves performance comparable to fine-tuning methods, reducing the attack\nsuccess rate (ASR) to 0 percent in some scenarios. Moreover, our approach has\nminimal impact on overall utility."
    },
    {
        "date": "2025-04",
        "title": "FFCBA: Feature-based Full-target Clean-label Backdoor Attacks",
        "author": "Yangxu Yin, Honglong Chen, Yudong Gao, Peng Sun, Liantao Wu, Zhe Li, and Weifeng Liu",
        "link": "http://arxiv.org/abs/2504.21054v1",
        "abstract": "Backdoor attacks pose a significant threat to deep neural networks, as\nbackdoored models would misclassify poisoned samples with specific triggers\ninto target classes while maintaining normal performance on clean samples.\nAmong these, multi-target backdoor attacks can simultaneously target multiple\nclasses. However, existing multi-target backdoor attacks all follow the\ndirty-label paradigm, where poisoned samples are mislabeled, and most of them\nrequire an extremely high poisoning rate. This makes them easily detectable by\nmanual inspection. In contrast, clean-label attacks are more stealthy, as they\navoid modifying the labels of poisoned samples. However, they generally\nstruggle to achieve stable and satisfactory attack performance and often fail\nto scale effectively to multi-target attacks. To address this issue, we propose\nthe Feature-based Full-target Clean-label Backdoor Attacks (FFCBA) which\nconsists of two paradigms: Feature-Spanning Backdoor Attacks (FSBA) and\nFeature-Migrating Backdoor Attacks (FMBA). FSBA leverages class-conditional\nautoencoders to generate noise triggers that align perturbed in-class samples\nwith the original category's features, ensuring the effectiveness, intra-class\nconsistency, inter-class specificity and natural-feature correlation of\ntriggers. While FSBA supports swift and efficient attacks, its cross-model\nattack capability is relatively weak. FMBA employs a two-stage\nclass-conditional autoencoder training process that alternates between using\nout-of-class samples and in-class samples. This allows FMBA to generate\ntriggers with strong target-class features, making it highly effective for\ncross-model attacks. We conduct experiments on multiple datasets and models,\nthe results show that FFCBA achieves outstanding attack performance and\nmaintains desirable robustness against the state-of-the-art backdoor defenses."
    },
    {
        "date": "2025-04",
        "title": "SFIBA: Spatial-based Full-target Invisible Backdoor Attacks",
        "author": "Yangxu Yin, Honglong Chen, Yudong Gao, Peng Sun, Zhishuai Li, and Weifeng Liu",
        "link": "http://arxiv.org/abs/2504.21052v1",
        "abstract": "Multi-target backdoor attacks pose significant security threats to deep\nneural networks, as they can preset multiple target classes through a single\nbackdoor injection. This allows attackers to control the model to misclassify\npoisoned samples with triggers into any desired target class during inference,\nexhibiting superior attack performance compared with conventional backdoor\nattacks. However, existing multi-target backdoor attacks fail to guarantee\ntrigger specificity and stealthiness in black-box settings, resulting in two\nmain issues. First, they are unable to simultaneously target all classes when\nonly training data can be manipulated, limiting their effectiveness in\nrealistic attack scenarios. Second, the triggers often lack visual\nimperceptibility, making poisoned samples easy to detect. To address these\nproblems, we propose a Spatial-based Full-target Invisible Backdoor Attack,\ncalled SFIBA. It restricts triggers for different classes to specific local\nspatial regions and morphologies in the pixel space to ensure specificity,\nwhile employing a frequency-domain-based trigger injection method to guarantee\nstealthiness. Specifically, for injection of each trigger, we first apply fast\nfourier transform to obtain the amplitude spectrum of clean samples in local\nspatial regions. Then, we employ discrete wavelet transform to extract the\nfeatures from the amplitude spectrum and use singular value decomposition to\nintegrate the trigger. Subsequently, we selectively filter parts of the trigger\nin pixel space to implement trigger morphology constraints and adjust injection\ncoefficients based on visual effects. We conduct experiments on multiple\ndatasets and models. The results demonstrate that SFIBA can achieve excellent\nattack performance and stealthiness, while preserving the model's performance\non benign samples, and can also bypass existing backdoor defenses."
    },
    {
        "date": "2025-04",
        "title": "Network Attack Traffic Detection With Hybrid Quantum-Enhanced Convolution Neural Network",
        "author": "Zihao Wang, Kar Wai Fok, and Vrizlynn L. L. Thing",
        "link": "http://arxiv.org/abs/2504.20436v1",
        "abstract": "The emerging paradigm of Quantum Machine Learning (QML) combines features of\nquantum computing and machine learning (ML). QML enables the generation and\nrecognition of statistical data patterns that classical computers and classical\nML methods struggle to effectively execute. QML utilizes quantum systems to\nenhance algorithmic computation speed and real-time data processing\ncapabilities, making it one of the most promising tools in the field of ML.\nQuantum superposition and entanglement features also hold the promise to\npotentially expand the potential feature representation capabilities of ML.\nTherefore, in this study, we explore how quantum computing affects ML and\nwhether it can further improve the detection performance on network traffic\ndetection, especially on unseen attacks which are types of malicious traffic\nthat do not exist in the ML training dataset. Classical ML models often perform\npoorly in detecting these unseen attacks because they have not been trained on\nsuch traffic. Hence, this paper focuses on designing and proposing novel hybrid\nstructures of Quantum Convolutional Neural Network (QCNN) to achieve the\ndetection of malicious traffic. The detection performance, generalization, and\nrobustness of the QML solutions are evaluated and compared with classical ML\nrunning on classical computers. The emphasis lies in assessing whether the\nQML-based malicious traffic detection outperforms classical solutions. Based on\nexperiment results, QCNN models demonstrated superior performance compared to\nclassical ML approaches on unseen attack detection."
    },
    {
        "date": "2025-04",
        "title": "Enhancing Leakage Attacks on Searchable Symmetric Encryption Using LLM-Based Synthetic Data Generation",
        "author": "Joshua Chiu, Partha Protim Paul, and Zahin Wahab",
        "link": "http://arxiv.org/abs/2504.20414v1",
        "abstract": "Searchable Symmetric Encryption (SSE) enables efficient search capabilities\nover encrypted data, allowing users to maintain privacy while utilizing cloud\nstorage. However, SSE schemes are vulnerable to leakage attacks that exploit\naccess patterns, search frequency, and volume information. Existing studies\nfrequently assume that adversaries possess a substantial fraction of the\nencrypted dataset to mount effective inference attacks, implying there is a\ndatabase leakage of such documents, thus, an assumption that may not hold in\nreal-world scenarios. In this work, we investigate the feasibility of enhancing\nleakage attacks under a more realistic threat model in which adversaries have\naccess to minimal leaked data. We propose a novel approach that leverages large\nlanguage models (LLMs), specifically GPT-4 variants, to generate synthetic\ndocuments that statistically and semantically resemble the real-world dataset\nof Enron emails. Using the email corpus as a case study, we evaluate the\neffectiveness of synthetic data generated via random sampling and hierarchical\nclustering methods on the performance of the SAP (Search Access Pattern)\nkeyword inference attack restricted to token volumes only. Our results\ndemonstrate that, while the choice of LLM has limited effect, increasing\ndataset size and employing clustering-based generation significantly improve\nattack accuracy, achieving comparable performance to attacks using larger\namounts of real data. We highlight the growing relevance of LLMs in adversarial\ncontexts."
    },
    {
        "date": "2025-04",
        "title": "The Dark Side of Digital Twins: Adversarial Attacks on AI-Driven Water Forecasting",
        "author": "Mohammadhossein Homaei, Victor Gonzalez Morales, Oscar Mogollon-Gutierrez, and Andres Caro",
        "link": "http://arxiv.org/abs/2504.20295v1",
        "abstract": "Digital twins (DTs) are improving water distribution systems by using\nreal-time data, analytics, and prediction models to optimize operations. This\npaper presents a DT platform designed for a Spanish water supply network,\nutilizing Long Short-Term Memory (LSTM) networks to predict water consumption.\nHowever, machine learning models are vulnerable to adversarial attacks, such as\nthe Fast Gradient Sign Method (FGSM) and Projected Gradient Descent (PGD).\nThese attacks manipulate critical model parameters, injecting subtle\ndistortions that degrade forecasting accuracy. To further exploit these\nvulnerabilities, we introduce a Learning Automata (LA) and Random LA-based\napproach that dynamically adjusts perturbations, making adversarial attacks\nmore difficult to detect. Experimental results show that this approach\nsignificantly impacts prediction reliability, causing the Mean Absolute\nPercentage Error (MAPE) to rise from 26% to over 35%. Moreover, adaptive attack\nstrategies amplify this effect, highlighting cybersecurity risks in AI-driven\nDTs. These findings emphasize the urgent need for robust defenses, including\nadversarial training, anomaly detection, and secure data pipelines."
    },
    {
        "date": "2025-04",
        "title": "Smart Water Security with AI and Blockchain-Enhanced Digital Twins",
        "author": "Mohammadhossein Homaei, Victor Gonzalez Morales, Oscar Mogollon Gutierrez, Ruben Molano Gomez, and Andres Caro",
        "link": "http://arxiv.org/abs/2504.20275v1",
        "abstract": "Water distribution systems in rural areas face serious challenges such as a\nlack of real-time monitoring, vulnerability to cyberattacks, and unreliable\ndata handling. This paper presents an integrated framework that combines\nLoRaWAN-based data acquisition, a machine learning-driven Intrusion Detection\nSystem (IDS), and a blockchain-enabled Digital Twin (BC-DT) platform for secure\nand transparent water management. The IDS filters anomalous or spoofed data\nusing a Long Short-Term Memory (LSTM) Autoencoder and Isolation Forest before\nvalidated data is logged via smart contracts on a private Ethereum blockchain\nusing Proof of Authority (PoA) consensus. The verified data feeds into a\nreal-time DT model supporting leak detection, consumption forecasting, and\npredictive maintenance. Experimental results demonstrate that the system\nachieves over 80 transactions per second (TPS) with under 2 seconds of latency\nwhile remaining cost-effective and scalable for up to 1,000 smart meters. This\nwork demonstrates a practical and secure architecture for decentralized water\ninfrastructure in under-connected rural environments."
    },
    {
        "date": "2025-04",
        "title": "A Virtual Cybersecurity Department for Securing Digital Twins in Water Distribution Systems",
        "author": "Mohammadhossein Homaei, Agustin Di Bartolo, Oscar Mogollon-Gutierrez, Fernando Broncano Morgado, and Pablo Garcia Rodriguez",
        "link": "http://arxiv.org/abs/2504.20266v1",
        "abstract": "Digital twins (DTs) help improve real-time monitoring and decision-making in\nwater distribution systems. However, their connectivity makes them easy targets\nfor cyberattacks such as scanning, denial-of-service (DoS), and unauthorized\naccess. Small and medium-sized enterprises (SMEs) that manage these systems\noften do not have enough budget or staff to build strong cybersecurity teams.\nTo solve this problem, we present a Virtual Cybersecurity Department (VCD), an\naffordable and automated framework designed for SMEs. The VCD uses open-source\ntools like Zabbix for real-time monitoring, Suricata for network intrusion\ndetection, Fail2Ban to block repeated login attempts, and simple firewall\nsettings. To improve threat detection, we also add a machine-learning-based IDS\ntrained on the OD-IDS2022 dataset using an improved ensemble model. This model\ndetects cyber threats such as brute-force attacks, remote code execution (RCE),\nand network flooding, with 92\\% accuracy and fewer false alarms. Our solution\ngives SMEs a practical and efficient way to secure water systems using low-cost\nand easy-to-manage tools."
    },
    {
        "date": "2025-04",
        "title": "SA2FE: A Secure, Anonymous, Auditable, and Fair Edge Computing Service Offloading Framework",
        "author": "Xiaojian Wang, Huayue Gu, Zhouyu Li, Fangtong Zhou, Ruozhou Yu, Dejun Yang, and Guoliang Xue",
        "link": "http://arxiv.org/abs/2504.20260v1",
        "abstract": "The inclusion of pervasive computing devices in a democratized edge computing\necosystem can significantly expand the capability and coverage of near-end\ncomputing for large-scale applications. However, offloading user tasks to\nheterogeneous and decentralized edge devices comes with the dual risk of both\nendangered user data security and privacy due to the curious base station or\nmalicious edge servers, and unfair offloading and malicious attacks targeting\nedge servers from other edge servers and/or users. Existing solutions to edge\naccess control and offloading either rely on \"always-on\" cloud servers with\nreduced edge benefits or fail to protect sensitive user service information. To\naddress these challenges, this paper presents SA2FE, a novel framework for edge\naccess control, offloading and accounting. We design a rerandomizable puzzle\nprimitive and a corresponding scheme to protect sensitive service information\nfrom eavesdroppers and ensure fair offloading decisions, while a blind\ntoken-based scheme safeguards user privacy, prevents double spending, and\nensures usage accountability. The security of SA2FE is proved under the\nUniversal Composability framework, and its performance and scalability are\ndemonstrated with implementation on commodity mobile devices and edge servers."
    },
    {
        "date": "2025-04",
        "title": "Financial Data Analysis with Robust Federated Logistic Regression",
        "author": "Kun Yang, Nikhil Krishnan, and Sanjeev R. Kulkarni",
        "link": "http://arxiv.org/abs/2504.20250v1",
        "abstract": "In this study, we focus on the analysis of financial data in a federated\nsetting, wherein data is distributed across multiple clients or locations, and\nthe raw data never leaves the local devices. Our primary focus is not only on\nthe development of efficient learning frameworks (for protecting user data\nprivacy) in the field of federated learning but also on the importance of\ndesigning models that are easier to interpret. In addition, we care about the\nrobustness of the framework to outliers. To achieve these goals, we propose a\nrobust federated logistic regression-based framework that strives to strike a\nbalance between these goals. To verify the feasibility of our proposed\nframework, we carefully evaluate its performance not only on independently\nidentically distributed (IID) data but also on non-IID data, especially in\nscenarios involving outliers. Extensive numerical results collected from\nmultiple public datasets demonstrate that our proposed method can achieve\ncomparable performance to those of classical centralized algorithms, such as\nLogistical Regression, Decision Tree, and K-Nearest Neighbors, in both binary\nand multi-class classification tasks."
    },
    {
        "date": "2025-04",
        "title": "A Case Study on the Use of Representativeness Bias as a Defense Against Adversarial Cyber Threats",
        "author": "Briland Hitaj, Grit Denker, Laura Tinnel, Michael McAnally, Bruce DeBruhl, Nathan Bunting, Alex Fafard, Daniel Aaron, Richard D. Roberts, Joshua Lawson, Greg McCain, and Dylan Starink",
        "link": "http://arxiv.org/abs/2504.20245v1",
        "abstract": "Cyberspace is an ever-evolving battleground involving adversaries seeking to\ncircumvent existing safeguards and defenders aiming to stay one step ahead by\npredicting and mitigating the next threat. Existing mitigation strategies have\nfocused primarily on solutions that consider software or hardware aspects,\noften ignoring the human factor. This paper takes a first step towards\npsychology-informed, active defense strategies, where we target biases that\nhuman beings are susceptible to under conditions of uncertainty.\n  Using capture-the-flag events, we create realistic challenges that tap into a\nparticular cognitive bias: representativeness. This study finds that this bias\ncan be triggered to thwart hacking attempts and divert hackers into\nnon-vulnerable attack paths. Participants were exposed to two different\nchallenges designed to exploit representativeness biases. One of the\nrepresentativeness challenges significantly thwarted attackers away from\nvulnerable attack vectors and onto non-vulnerable paths, signifying an\neffective bias-based defense mechanism. This work paves the way towards cyber\ndefense strategies that leverage additional human biases to thwart future,\nsophisticated adversarial attacks."
    },
    {
        "date": "2025-04",
        "title": "MP-SfM: Monocular Surface Priors for Robust Structure-from-Motion",
        "author": "Zador Pataki, Paul-Edouard Sarlin, Johannes L. Sch\u00f6nberger, and Marc Pollefeys",
        "link": "http://arxiv.org/abs/2504.20040v1",
        "abstract": "While Structure-from-Motion (SfM) has seen much progress over the years,\nstate-of-the-art systems are prone to failure when facing extreme viewpoint\nchanges in low-overlap, low-parallax or high-symmetry scenarios. Because\ncapturing images that avoid these pitfalls is challenging, this severely limits\nthe wider use of SfM, especially by non-expert users. We overcome these\nlimitations by augmenting the classical SfM paradigm with monocular depth and\nnormal priors inferred by deep neural networks. Thanks to a tight integration\nof monocular and multi-view constraints, our approach significantly outperforms\nexisting ones under extreme viewpoint changes, while maintaining strong\nperformance in standard conditions. We also show that monocular priors can help\nreject faulty associations due to symmetries, which is a long-standing problem\nfor SfM. This makes our approach the first capable of reliably reconstructing\nchallenging indoor environments from few images. Through principled uncertainty\npropagation, it is robust to errors in the priors, can handle priors inferred\nby different models with little tuning, and will thus easily benefit from\nfuture progress in monocular depth and normal estimation. Our code is publicly\navailable at https://github.com/cvg/mpsfm."
    },
    {
        "date": "2025-04",
        "title": "Simplified and Secure MCP Gateways for Enterprise AI Integration",
        "author": "Ivo Brett",
        "link": "http://arxiv.org/abs/2504.19997v1",
        "abstract": "The increased adoption of the Model Context Protocol (MCP) for AI Agents\nnecessitates robust security for Enterprise integrations. This paper introduces\nthe MCP Gateway to simplify self-hosted MCP server integration. The proposed\narchitecture integrates security principles, authentication, intrusion\ndetection, and secure tunneling, enabling secure self-hosting without exposing\ninfrastructure. Key contributions include a reference architecture, threat\nmodel mapping, simplified integration strategies, and open-source\nimplementation recommendations. This work focuses on the unique challenges of\nenterprise-centric, self-hosted AI integrations, unlike existing public MCP\nserver solutions."
    },
    {
        "date": "2025-04",
        "title": "Securing Agentic AI: A Comprehensive Threat Model and Mitigation Framework for Generative AI Agents",
        "author": "Vineeth Sai Narajala, and Om Narayan",
        "link": "http://arxiv.org/abs/2504.19956v2",
        "abstract": "As generative AI (GenAI) agents become more common in enterprise settings,\nthey introduce security challenges that differ significantly from those posed\nby traditional systems. These agents are not just LLMs; they reason, remember,\nand act, often with minimal human oversight. This paper introduces a\ncomprehensive threat model tailored specifically for GenAI agents, focusing on\nhow their autonomy, persistent memory access, complex reasoning, and tool\nintegration create novel risks. This research work identifies 9 primary threats\nand organizes them across five key domains: cognitive architecture\nvulnerabilities, temporal persistence threats, operational execution\nvulnerabilities, trust boundary violations, and governance circumvention. These\nthreats are not just theoretical they bring practical challenges such as\ndelayed exploitability, cross-system propagation, cross system lateral\nmovement, and subtle goal misalignments that are hard to detect with existing\nframeworks and standard approaches. To help address this, the research work\npresent two complementary frameworks: ATFAA - Advanced Threat Framework for\nAutonomous AI Agents, which organizes agent-specific risks, and SHIELD, a\nframework proposing practical mitigation strategies designed to reduce\nenterprise exposure. While this work builds on existing work in LLM and AI\nsecurity, the focus is squarely on what makes agents different and why those\ndifferences matter. Ultimately, this research argues that GenAI agents require\na new lens for security. If we fail to adapt our threat models and defenses to\naccount for their unique architecture and behavior, we risk turning a powerful\nnew tool into a serious enterprise liability."
    },
    {
        "date": "2025-04",
        "title": "Robust Federated Personalised Mean Estimation for the Gaussian Mixture Model",
        "author": "Malhar A. Managoli, Vinod M. Prabhakaran, and Suhas Diggavi",
        "link": "http://arxiv.org/abs/2504.19955v1",
        "abstract": "Federated learning with heterogeneous data and personalization has received\nsignificant recent attention. Separately, robustness to corrupted data in the\ncontext of federated learning has also been studied. In this paper we explore\ncombining personalization for heterogeneous data with robustness, where a\nconstant fraction of the clients are corrupted. Motivated by this broad\nproblem, we formulate a simple instantiation which captures some of its\ndifficulty. We focus on the specific problem of personalized mean estimation\nwhere the data is drawn from a Gaussian mixture model. We give an algorithm\nwhose error depends almost linearly on the ratio of corrupted to uncorrupted\nsamples, and show a lower bound with the same behavior, albeit with a gap of a\nconstant factor."
    },
    {
        "date": "2025-04",
        "title": "Securing GenAI Multi-Agent Systems Against Tool Squatting: A Zero Trust Registry-Based Approach",
        "author": "Vineeth Sai Narajala, Ken Huang, and Idan Habler",
        "link": "http://arxiv.org/abs/2504.19951v1",
        "abstract": "The rise of generative AI (GenAI) multi-agent systems (MAS) necessitates\nstandardized protocols enabling agents to discover and interact with external\ntools. However, these protocols introduce new security challenges,\nparticularly; tool squatting; the deceptive registration or representation of\ntools. This paper analyzes tool squatting threats within the context of\nemerging interoperability standards, such as Model Context Protocol (MCP) or\nseamless communication between agents protocols. It introduces a comprehensive\nTool Registry system designed to mitigate these risks. We propose a\nsecurity-focused architecture featuring admin-controlled registration,\ncentralized tool discovery, fine grained access policies enforced via dedicated\nAgent and Tool Registry services, a dynamic trust scoring mechanism based on\ntool versioning and known vulnerabilities, and just in time credential\nprovisioning. Based on its design principles, the proposed registry framework\naims to effectively prevent common tool squatting vectors while preserving the\nflexibility and power of multi-agent systems. This work addresses a critical\nsecurity gap in the rapidly evolving GenAI ecosystem and provides a foundation\nfor secure tool integration in production environments."
    },
    {
        "date": "2025-04",
        "title": "DeeCLIP: A Robust and Generalizable Transformer-Based Framework for Detecting AI-Generated Images",
        "author": "Mamadou Keita, Wassim Hamidouche, Hessen Bougueffa Eutamene, Abdelmalik Taleb-Ahmed, and Abdenour Hadid",
        "link": "http://arxiv.org/abs/2504.19876v1",
        "abstract": "This paper introduces DeeCLIP, a novel framework for detecting AI-generated\nimages using CLIP-ViT and fusion learning. Despite significant advancements in\ngenerative models capable of creating highly photorealistic images, existing\ndetection methods often struggle to generalize across different models and are\nhighly sensitive to minor perturbations. To address these challenges, DeeCLIP\nincorporates DeeFuser, a fusion module that combines high-level and low-level\nfeatures, improving robustness against degradations such as compression and\nblurring. Additionally, we apply triplet loss to refine the embedding space,\nenhancing the model's ability to distinguish between real and synthetic\ncontent. To further enable lightweight adaptation while preserving pre-trained\nknowledge, we adopt parameter-efficient fine-tuning using low-rank adaptation\n(LoRA) within the CLIP-ViT backbone. This approach supports effective zero-shot\nlearning without sacrificing generalization. Trained exclusively on 4-class\nProGAN data, DeeCLIP achieves an average accuracy of 89.00% on 19 test subsets\ncomposed of generative adversarial network (GAN) and diffusion models. Despite\nhaving fewer trainable parameters, DeeCLIP outperforms existing methods,\ndemonstrating superior robustness against various generative models and\nreal-world distortions. The code is publicly available at\nhttps://github.com/Mamadou-Keita/DeeCLIP for research purposes."
    },
    {
        "date": "2025-04",
        "title": "CodeBC: A More Secure Large Language Model for Smart Contract Code Generation in Blockchain",
        "author": "Lingxiang wang, Hainan Zhang, Qinnan Zhang, Ziwei Wang, Hongwei Zheng, Jin Dong, and Zhiming Zheng",
        "link": "http://arxiv.org/abs/2504.21043v1",
        "abstract": "Large language models (LLMs) excel at generating code from natural language\ninstructions, yet they often lack an understanding of security vulnerabilities.\nThis limitation makes it difficult for LLMs to avoid security risks in\ngenerated code, particularly in high-security programming tasks such as smart\ncontract development for blockchain. Researchers have attempted to enhance the\nvulnerability awareness of these models by training them to differentiate\nbetween vulnerable and fixed code snippets. However, this approach relies\nheavily on manually labeled vulnerability data, which is only available for\npopular languages like Python and C++. For low-resource languages like\nSolidity, used in smart contracts, large-scale annotated datasets are scarce\nand difficult to obtain. To address this challenge, we introduce CodeBC, a code\ngeneration model specifically designed for generating secure smart contracts in\nblockchain. CodeBC employs a three-stage fine-tuning approach based on\nCodeLlama, distinguishing itself from previous methods by not relying on\npairwise vulnerability location annotations. Instead, it leverages\nvulnerability and security tags to teach the model the differences between\nvulnerable and secure code. During the inference phase, the model leverages\nsecurity tags to generate secure and robust code. Experimental results\ndemonstrate that CodeBC outperforms baseline models in terms of BLEU, CodeBLEU,\nand compilation pass rates, while significantly reducing vulnerability rates.\nThese findings validate the effectiveness and cost-efficiency of our\nthree-stage fine-tuning strategy, making CodeBC a promising solution for\ngenerating secure smart contract code."
    },
    {
        "date": "2025-04",
        "title": "Prompt Injection Attack to Tool Selection in LLM Agents",
        "author": "Jiawen Shi, Zenghui Yuan, Guiyao Tie, Pan Zhou, Neil Zhenqiang Gong, and Lichao Sun",
        "link": "http://arxiv.org/abs/2504.19793v1",
        "abstract": "Tool selection is a key component of LLM agents. The process operates through\na two-step mechanism - \\emph{retrieval} and \\emph{selection} - to pick the most\nappropriate tool from a tool library for a given task. In this work, we\nintroduce \\textit{ToolHijacker}, a novel prompt injection attack targeting tool\nselection in no-box scenarios. ToolHijacker injects a malicious tool document\ninto the tool library to manipulate the LLM agent's tool selection process,\ncompelling it to consistently choose the attacker's malicious tool for an\nattacker-chosen target task. Specifically, we formulate the crafting of such\ntool documents as an optimization problem and propose a two-phase optimization\nstrategy to solve it. Our extensive experimental evaluation shows that\nToolHijacker is highly effective, significantly outperforming existing\nmanual-based and automated prompt injection attacks when applied to tool\nselection. Moreover, we explore various defenses, including prevention-based\ndefenses (StruQ and SecAlign) and detection-based defenses (known-answer\ndetection, perplexity detection, and perplexity windowed detection). Our\nexperimental results indicate that these defenses are insufficient,\nhighlighting the urgent need for developing new defense strategies."
    },
    {
        "date": "2025-04",
        "title": "Learning Brenier Potentials with Convex Generative Adversarial Neural Networks",
        "author": "Claudia Drygala, Hanno Gottschalk, Thomas Kruse, S\u00e9gol\u00e8ne Martin, and Annika M\u00fctze",
        "link": "http://arxiv.org/abs/2504.19779v1",
        "abstract": "Brenier proved that under certain conditions on a source and a target\nprobability measure there exists a strictly convex function such that its\ngradient is a transport map from the source to the target distribution. This\nfunction is called the Brenier potential. Furthermore, detailed information on\nthe H\\\"older regularity of the Brenier potential is available. In this work we\ndevelop the statistical learning theory of generative adversarial neural\nnetworks that learn the Brenier potential. As by the transformation of\ndensities formula, the density of the generated measure depends on the second\nderivative of the Brenier potential, we develop the universal approximation\ntheory of ReCU networks with cubic activation $\\mathtt{ReCU}(x)=\\max\\{0,x\\}^3$\nthat combines the favorable approximation properties of H\\\"older functions with\na Lipschitz continuous density. In order to assure the convexity of such\ngeneral networks, we introduce an adversarial training procedure for a\npotential function represented by the ReCU networks that combines the classical\ndiscriminator cross entropy loss with a penalty term that enforces (strict)\nconvexity. We give a detailed decomposition of learning errors and show that\nfor a suitable high penalty parameter all networks chosen in the adversarial\nmin-max optimization problem are strictly convex. This is further exploited to\nprove the consistency of the learning procedure for (slowly) expanding network\ncapacity. We also implement the described learning algorithm and apply it to a\nnumber of standard test cases from Gaussian mixture to image data as target\ndistributions. As predicted in theory, we observe that the convexity loss\nbecomes inactive during the training process and the potentials represented by\nthe neural networks have learned convexity."
    },
    {
        "date": "2025-04",
        "title": "Fast and Robust Speckle Pattern Authentication by Scale Invariant Feature Transform algorithm in Physical Unclonable Functions",
        "author": "Giuseppe Emanuele Lio, Mauro Daniel Luigi Bruno, Francesco Riboli, Sara Nocentini, and Antonio Ferraro",
        "link": "http://arxiv.org/abs/2504.21041v1",
        "abstract": "Nowadays, due to the growing phenomenon of forgery in many fields, the\ninterest in developing new anti-counterfeiting device and cryptography keys,\nbased on the Physical Unclonable Functions (PUFs) paradigm, is widely\nincreased. PUFs are physical hardware with an intrinsic, irreproducible\ndisorder that allows for on-demand cryptographic key extraction. Among them,\noptical PUF are characterized by a large number of degrees of freedom resulting\nin higher security and higher sensitivity to environmental conditions. While\nthese promising features led to the growth of advanced fabrication strategies\nand materials for new PUF devices, their combination with robust recognition\nalgorithm remains largely unexplored. In this work, we present a\nmetric-independent authentication approach that leverages the Scale Invariant\nFeature Transform (SIFT) algorithm to extract unique and invariant features\nfrom the speckle patterns generated by optical Physical Unclonable Functions\n(PUFs). The application of SIFT to the challenge response pairs (CRPs) protocol\nallows us to correctly authenticate a client while denying any other fraudulent\naccess. In this way, the authentication process is highly reliable even in\npresence of response rotation, zooming, and cropping that may occur in\nconsecutive PUF interrogations and to which other postprocessing algorithm are\nhighly sensitive. This characteristics together with the speed of the method\n(tens of microseconds for each operation) broaden the applicability and\nreliability of PUF to practical high-security authentication or merchandise\nanti-counterfeiting."
    },
    {
        "date": "2025-04",
        "title": "ClearVision: Leveraging CycleGAN and SigLIP-2 for Robust All-Weather Classification in Traffic Camera Imagery",
        "author": "Anush Lakshman Sivaraman, Kojo Adu-Gyamfi, Ibne Farabi Shihab, and Anuj Sharma",
        "link": "http://arxiv.org/abs/2504.19684v2",
        "abstract": "Adverse weather conditions challenge safe transportation, necessitating\nrobust real-time weather detection from traffic camera imagery. We propose a\nnovel framework combining CycleGAN-based domain adaptation with efficient\ncontrastive learning to enhance weather classification, particularly in\nlow-light nighttime conditions. Our approach leverages the lightweight SigLIP-2\nmodel, which employs pairwise sigmoid loss to reduce computational demands,\nintegrated with CycleGAN to transform nighttime images into day-like\nrepresentations while preserving weather cues. Evaluated on an Iowa Department\nof Transportation dataset, the baseline EVA-02 model with CLIP achieves a\nper-class overall accuracy of 96.55\\% across three weather conditions (No\nPrecipitation, Rain, Snow) and a day/night overall accuracy of 96.55\\%, but\nshows a significant day-night gap (97.21\\% day vs.\\ 63.40\\% night). With\nCycleGAN, EVA-02 improves to 97.01\\% per-class accuracy and 96.85\\% day/night\naccuracy, boosting nighttime performance to 82.45\\%. Our Vision-SigLIP-2 +\nText-SigLIP-2 + CycleGAN + Contrastive configuration excels in nighttime\nscenarios, achieving the highest nighttime accuracy of 85.90\\%, with 94.00\\%\nper-class accuracy and 93.35\\% day/night accuracy. This model reduces training\ntime by 89\\% (from 6 hours to 40 minutes) and inference time by 80\\% (from 15\nseconds to 3 seconds) compared to EVA-02. By narrowing the day-night\nperformance gap from 33.81 to 8.90 percentage points, our framework provides a\nscalable, efficient solution for all-weather classification using existing\ncamera infrastructure."
    },
    {
        "date": "2025-04",
        "title": "Benchmarking Transferability: A Framework for Fair and Robust Evaluation",
        "author": "Alireza Kazemi, Helia Rezvani, and Mahsa Baktashmotlagh",
        "link": "http://arxiv.org/abs/2504.20121v1",
        "abstract": "Transferability scores aim to quantify how well a model trained on one domain\ngeneralizes to a target domain. Despite numerous methods proposed for measuring\ntransferability, their reliability and practical usefulness remain\ninconclusive, often due to differing experimental setups, datasets, and\nassumptions. In this paper, we introduce a comprehensive benchmarking framework\ndesigned to systematically evaluate transferability scores across diverse\nsettings. Through extensive experiments, we observe variations in how different\nmetrics perform under various scenarios, suggesting that current evaluation\npractices may not fully capture each method's strengths and limitations. Our\nfindings underscore the value of standardized assessment protocols, paving the\nway for more reliable transferability measures and better-informed model\nselection in cross-domain applications. Additionally, we achieved a 3.5\\%\nimprovement using our proposed metric for the head-training fine-tuning\nexperimental setup. Our code is available in this repository:\nhttps://github.com/alizkzm/pert_robust_platform."
    },
    {
        "date": "2025-04",
        "title": "BARIS: Boundary-Aware Refinement with Environmental Degradation Priors for Robust Underwater Instance Segmentation",
        "author": "Pin-Chi Pan, and Soo-Chang Pei",
        "link": "http://arxiv.org/abs/2504.19643v1",
        "abstract": "Underwater instance segmentation is challenging due to adverse visual\nconditions such as light attenuation, scattering, and color distortion, which\ndegrade model performance. In this work, we propose BARIS-Decoder\n(Boundary-Aware Refinement Decoder for Instance Segmentation), a framework that\nenhances segmentation accuracy through feature refinement. To address\nunderwater degradations, we introduce the Environmental Robust Adapter (ERA),\nwhich efficiently models underwater degradation patterns while reducing\ntrainable parameters by over 90\\% compared to full fine-tuning. The integration\nof BARIS-Decoder with ERA-tuning, referred to as BARIS-ERA, achieves\nstate-of-the-art performance, surpassing Mask R-CNN by 3.4 mAP with a Swin-B\nbackbone and 3.8 mAP with ConvNeXt V2. Our findings demonstrate the\neffectiveness of BARIS-ERA in advancing underwater instance segmentation,\nproviding a robust and efficient solution."
    },
    {
        "date": "2025-04",
        "title": "Towards Robust Multimodal Physiological Foundation Models: Handling Arbitrary Missing Modalities",
        "author": "Xi Fu, Wei-Bang Jiang, Yi Ding, and Cuntai Guan",
        "link": "http://arxiv.org/abs/2504.19596v1",
        "abstract": "Multimodal physiological signals, such as EEG, ECG, EOG, and EMG, are crucial\nfor healthcare and brain-computer interfaces. While existing methods rely on\nspecialized architectures and dataset-specific fusion strategies, they struggle\nto learn universal representations that generalize across datasets and handle\nmissing modalities at inference time. To address these issues, we propose\nPhysioOmni, a foundation model for multimodal physiological signal analysis\nthat models both homogeneous and heterogeneous features to decouple multimodal\nsignals and extract generic representations while maintaining compatibility\nwith arbitrary missing modalities. PhysioOmni trains a decoupled multimodal\ntokenizer, enabling masked signal pre-training via modality-invariant and\nmodality-specific objectives. To ensure adaptability to diverse and incomplete\nmodality combinations, the pre-trained encoders undergo resilient fine-tuning\nwith prototype alignment on downstream datasets. Extensive experiments on four\ndownstream tasks, emotion recognition, sleep stage classification, motor\nprediction, and mental workload detection, demonstrate that PhysioOmni achieves\nstate-of-the-art performance while maintaining strong robustness to missing\nmodalities. Our code and model weights will be released."
    },
    {
        "date": "2025-04",
        "title": "Adversarial Shallow Watermarking",
        "author": "Guobiao Li, Lei Tan, Yuliang Xue, Gaozhi Liu, Zhenxing Qian, Sheng Li, and Xinpeng Zhang",
        "link": "http://arxiv.org/abs/2504.19529v1",
        "abstract": "Recent advances in digital watermarking make use of deep neural networks for\nmessage embedding and extraction. They typically follow the ``encoder-noise\nlayer-decoder''-based architecture. By deliberately establishing a\ndifferentiable noise layer to simulate the distortion of the watermarked\nsignal, they jointly train the deep encoder and decoder to fit the noise layer\nto guarantee robustness. As a result, they are usually weak against unknown\ndistortions that are not used in their training pipeline. In this paper, we\npropose a novel watermarking framework to resist unknown distortions, namely\nAdversarial Shallow Watermarking (ASW). ASW utilizes only a shallow decoder\nthat is randomly parameterized and designed to be insensitive to distortions\nfor watermarking extraction. During the watermark embedding, ASW freezes the\nshallow decoder and adversarially optimizes a host image until its updated\nversion (i.e., the watermarked image) stably triggers the shallow decoder to\noutput the watermark message. During the watermark extraction, it accurately\nrecovers the message from the watermarked image by leveraging the insensitive\nnature of the shallow decoder against arbitrary distortions. Our ASW is\ntraining-free, encoder-free, and noise layer-free. Experiments indicate that\nthe watermarked images created by ASW have strong robustness against various\nunknown distortions. Compared to the existing ``encoder-noise layer-decoder''\napproaches, ASW achieves comparable results on known distortions and better\nrobustness on unknown distortions."
    },
    {
        "date": "2025-04",
        "title": "Security Steerability is All You Need",
        "author": "Itay Hazan, Idan Habler, Ron Bitton, and Itsik Mantin",
        "link": "http://arxiv.org/abs/2504.19521v2",
        "abstract": "The adoption of Generative AI (GenAI) in various applications inevitably\ncomes with expanding the attack surface, combining new security threats along\nwith the traditional ones. Consequently, numerous research and industrial\ninitiatives aim to mitigate these security threats in GenAI by developing\nmetrics and designing defenses. However, while most of the GenAI security work\nfocuses on universal threats (e.g. manipulating the LLM to generate forbidden\ncontent), there is significantly less discussion on application-level security\nand how to mitigate it. Thus, in this work we adopt an application-centric\napproach to GenAI security, and show that while LLMs cannot protect against\nad-hoc application specific threats, they can provide the framework for\napplications to protect themselves against such threats. Our first contribution\nis defining Security Steerability - a novel security measure for LLMs,\nassessing the model's capability to adhere to strict guardrails that are\ndefined in the system prompt ('Refrain from discussing about politics'). These\nguardrails, in case effective, can stop threats in the presence of malicious\nusers who attempt to circumvent the application and cause harm to its\nproviders. Our second contribution is a methodology to measure the security\nsteerability of LLMs, utilizing two newly-developed datasets: VeganRibs\nassesses the LLM behavior in forcing specific guardrails that are not security\nper se in the presence of malicious user that uses attack boosters (jailbreaks\nand perturbations), and ReverseText takes this approach further and measures\nthe LLM ability to force specific treatment of the user input as plain text\nwhile do user try to give it additional meanings..."
    },
    {
        "date": "2025-04",
        "title": "Security Bug Report Prediction Within and Across Projects: A Comparative Study of BERT and Random Forest",
        "author": "Farnaz Soltaniani, Mohammad Ghafari, and Mohammed Sayagh",
        "link": "http://arxiv.org/abs/2504.21037v1",
        "abstract": "Early detection of security bug reports (SBRs) is crucial for preventing\nvulnerabilities and ensuring system reliability. While machine learning models\nhave been developed for SBR prediction, their predictive performance still has\nroom for improvement. In this study, we conduct a comprehensive comparison\nbetween BERT and Random Forest (RF), a competitive baseline for predicting\nSBRs. The results show that RF outperforms BERT with a 34% higher average\nG-measure for within-project predictions. Adding only SBRs from various\nprojects improves both models' average performance. However, including both\nsecurity and nonsecurity bug reports significantly reduces RF's average\nperformance to 46%, while boosts BERT to its best average performance of 66%,\nsurpassing RF. In cross-project SBR prediction, BERT achieves a remarkable 62%\nG-measure, which is substantially higher than RF."
    },
    {
        "date": "2025-04",
        "title": "Can Differentially Private Fine-tuning LLMs Protect Against Privacy Attacks?",
        "author": "Hao Du, Shang Liu, and Yang Cao",
        "link": "http://arxiv.org/abs/2504.21036v2",
        "abstract": "Fine-tuning large language models (LLMs) has become an essential strategy for\nadapting them to specialized tasks; however, this process introduces\nsignificant privacy challenges, as sensitive training data may be inadvertently\nmemorized and exposed. Although differential privacy (DP) offers strong\ntheoretical guarantees against such leakage, its empirical privacy\neffectiveness on LLMs remains unclear, especially under different fine-tuning\nmethods. In this paper, we systematically investigate the impact of DP across\nfine-tuning methods and privacy budgets, using both data extraction and\nmembership inference attacks to assess empirical privacy risks. Our main\nfindings are as follows: (1) Differential privacy reduces model utility, but\nits impact varies significantly across different fine-tuning methods. (2)\nWithout DP, the privacy risks of models fine-tuned with different approaches\ndiffer considerably. (3) When DP is applied, even a relatively high privacy\nbudget can substantially lower privacy risk. (4) The privacy-utility trade-off\nunder DP training differs greatly among fine-tuning methods, with some methods\nbeing unsuitable for DP due to severe utility degradation. Our results provide\npractical guidance for privacy-conscious deployment of LLMs and pave the way\nfor future research on optimizing the privacy-utility trade-off in fine-tuning\nmethodologies."
    },
    {
        "date": "2025-04",
        "title": "The Cost of Performance: Breaking ThreadX with Kernel Object Masquerading Attacks",
        "author": "Xinhui Shao, Zhen Ling, Yue Zhang, Huaiyu Yan, Yumeng Wei, Lan Luo, Zixia Liu, Junzhou Luo, and Xinwen Fu",
        "link": "http://arxiv.org/abs/2504.19486v1",
        "abstract": "Microcontroller-based IoT devices often use embedded real-time operating\nsystems (RTOSs). Vulnerabilities in these embedded RTOSs can lead to\ncompromises of those IoT devices. Despite the significance of security\nprotections, the absence of standardized security guidelines results in various\nlevels of security risk across RTOS implementations. Our initial analysis\nreveals that popular RTOSs such as FreeRTOS lack essential security\nprotections. While Zephyr OS and ThreadX are designed and implemented with\nessential security protections, our closer examination uncovers significant\ndifferences in their implementations of system call parameter sanitization. We\nidentify a performance optimization practice in ThreadX that introduces\nsecurity vulnerabilities, allowing for the circumvention of parameter\nsanitization processes. Leveraging this insight, we introduce a novel attack\nnamed the Kernel Object Masquerading (KOM) Attack (as the attacker needs to\nmanipulate one or multiple kernel objects through carefully selected system\ncalls to launch the attack), demonstrating how attackers can exploit these\nvulnerabilities to access sensitive fields within kernel objects, potentially\nleading to unauthorized data manipulation, privilege escalation, or system\ncompromise. We introduce an automated approach involving under-constrained\nsymbolic execution to identify the KOM attacks and to understand the\nimplications. Experimental results demonstrate the feasibility of KOM attacks\non ThreadX-powered platforms. We reported our findings to the vendors, who\nrecognized the vulnerabilities, with Amazon and Microsoft acknowledging our\ncontribution on their websites."
    },
    {
        "date": "2025-04",
        "title": "FCGHunter: Towards Evaluating Robustness of Graph-Based Android Malware Detection",
        "author": "Shiwen Song, Xiaofei Xie, Ruitao Feng, Qi Guo, and Sen Chen",
        "link": "http://arxiv.org/abs/2504.19456v1",
        "abstract": "Graph-based detection methods leveraging Function Call Graphs (FCGs) have\nshown promise for Android malware detection (AMD) due to their semantic\ninsights. However, the deployment of malware detectors in dynamic and hostile\nenvironments raises significant concerns about their robustness. While recent\napproaches evaluate the robustness of FCG-based detectors using adversarial\nattacks, their effectiveness is constrained by the vast perturbation space,\nparticularly across diverse models and features.\n  To address these challenges, we introduce FCGHunter, a novel robustness\ntesting framework for FCG-based AMD systems. Specifically, FCGHunter employs\ninnovative techniques to enhance exploration and exploitation within this huge\nsearch space. Initially, it identifies critical areas within the FCG related to\nmalware behaviors to narrow down the perturbation space. We then develop a\ndependency-aware crossover and mutation method to enhance the validity and\ndiversity of perturbations, generating diverse FCGs. Furthermore, FCGHunter\nleverages multi-objective feedback to select perturbed FCGs, significantly\nimproving the search process with interpretation-based feature change feedback.\n  Extensive evaluations across 40 scenarios demonstrate that FCGHunter achieves\nan average attack success rate of 87.9%, significantly outperforming baselines\nby at least 44.7%. Notably, FCGHunter achieves a 100% success rate on robust\nmodels (e.g., AdaBoost with MalScan), where baselines achieve only 11% or are\ninapplicable."
    },
    {
        "date": "2025-04",
        "title": "Provably Secure Public-Key Steganography Based on Admissible Encoding",
        "author": "Xin Zhang, Kejiang Chen, Na Zhao, Weiming Zhang, and Nenghai Yu",
        "link": "http://arxiv.org/abs/2504.19454v1",
        "abstract": "The technique of hiding secret messages within seemingly harmless covertext\nto evade examination by censors with rigorous security proofs is known as\nprovably secure steganography (PSS). PSS evolves from symmetric key\nsteganography to public-key steganography, functioning without the requirement\nof a pre-shared key and enabling the extension to multi-party covert\ncommunication and identity verification mechanisms. Recently, a public-key\nsteganography method based on elliptic curves was proposed, which uses point\ncompression to eliminate the algebraic structure of curve points. However, this\nmethod has strict requirements on the curve parameters and is only available on\nhalf of the points. To overcome these limitations, this paper proposes a more\ngeneral elliptic curve public key steganography method based on admissible\nencoding. By applying the tensor square function to the known well-distributed\nencoding, we construct admissible encoding, which can create the pseudo-random\npublic-key encryption function. The theoretical analysis and experimental\nresults show that the proposed provable secure public-key steganography method\ncan be deployed on all types of curves and utilize all points on the curve."
    },
    {
        "date": "2025-04",
        "title": "JailbreaksOverTime: Detecting Jailbreak Attacks Under Distribution Shift",
        "author": "Julien Piet, Xiao Huang, Dennis Jacob, Annabella Chow, Maha Alrashed, Geng Zhao, Zhanhao Hu, Chawin Sitawarin, Basel Alomair, and David Wagner",
        "link": "http://arxiv.org/abs/2504.19440v1",
        "abstract": "Safety and security remain critical concerns in AI deployment. Despite safety\ntraining through reinforcement learning with human feedback (RLHF) [ 32],\nlanguage models remain vulnerable to jailbreak attacks that bypass safety\nguardrails. Universal jailbreaks - prefixes that can circumvent alignment for\nany payload - are particularly concerning. We show empirically that jailbreak\ndetection systems face distribution shift, with detectors trained at one point\nin time performing poorly against newer exploits. To study this problem, we\nrelease JailbreaksOverTime, a comprehensive dataset of timestamped real user\ninteractions containing both benign requests and jailbreak attempts collected\nover 10 months. We propose a two-pronged method for defenders to detect new\njailbreaks and continuously update their detectors. First, we show how to use\ncontinuous learning to detect jailbreaks and adapt rapidly to new emerging\njailbreaks. While detectors trained at a single point in time eventually fail\ndue to drift, we find that universal jailbreaks evolve slowly enough for\nself-training to be effective. Retraining our detection model weekly using its\nown labels - with no new human labels - reduces the false negative rate from 4%\nto 0.3% at a false positive rate of 0.1%. Second, we introduce an unsupervised\nactive monitoring approach to identify novel jailbreaks. Rather than\nclassifying inputs directly, we recognize jailbreaks by their behavior,\nspecifically, their ability to trigger models to respond to known-harmful\nprompts. This approach has a higher false negative rate (4.1%) than supervised\nmethods, but it successfully identified some out-of-distribution attacks that\nwere missed by the continuous learning approach."
    },
    {
        "date": "2025-04",
        "title": "SAGA: A Security Architecture for Governing AI Agentic Systems",
        "author": "Georgios Syros, Anshuman Suri, Cristina Nita-Rotaru, and Alina Oprea",
        "link": "http://arxiv.org/abs/2504.21034v1",
        "abstract": "Large Language Model (LLM)-based agents increasingly interact, collaborate,\nand delegate tasks to one another autonomously with minimal human interaction.\nIndustry guidelines for agentic system governance emphasize the need for users\nto maintain comprehensive control over their agents, mitigating potential\ndamage from malicious agents. Several proposed agentic system designs address\nagent identity, authorization, and delegation, but remain purely theoretical,\nwithout concrete implementation and evaluation. Most importantly, they do not\nprovide user-controlled agent management. To address this gap, we propose SAGA,\na Security Architecture for Governing Agentic systems, that offers user\noversight over their agents' lifecycle. In our design, users register their\nagents with a central entity, the Provider, that maintains agents contact\ninformation, user-defined access control policies, and helps agents enforce\nthese policies on inter-agent communication. We introduce a cryptographic\nmechanism for deriving access control tokens, that offers fine-grained control\nover an agent's interaction with other agents, balancing security and\nperformance consideration. We evaluate SAGA on several agentic tasks, using\nagents in different geolocations, and multiple on-device and cloud LLMs,\ndemonstrating minimal performance overhead with no impact on underlying task\nutility in a wide range of conditions. Our architecture enables secure and\ntrustworthy deployment of autonomous agents, accelerating the responsible\nadoption of this technology in sensitive environments."
    },
    {
        "date": "2025-04",
        "title": "PolyTouch: A Robust Multi-Modal Tactile Sensor for Contact-rich Manipulation Using Tactile-Diffusion Policies",
        "author": "Jialiang Zhao, Naveen Kuppuswamy, Siyuan Feng, Benjamin Burchfiel, and Edward Adelson",
        "link": "http://arxiv.org/abs/2504.19341v1",
        "abstract": "Achieving robust dexterous manipulation in unstructured domestic environments\nremains a significant challenge in robotics. Even with state-of-the-art robot\nlearning methods, haptic-oblivious control strategies (i.e. those relying only\non external vision and/or proprioception) often fall short due to occlusions,\nvisual complexities, and the need for precise contact interaction control. To\naddress these limitations, we introduce PolyTouch, a novel robot finger that\nintegrates camera-based tactile sensing, acoustic sensing, and peripheral\nvisual sensing into a single design that is compact and durable. PolyTouch\nprovides high-resolution tactile feedback across multiple temporal scales,\nwhich is essential for efficiently learning complex manipulation tasks.\nExperiments demonstrate an at least 20-fold increase in lifespan over\ncommercial tactile sensors, with a design that is both easy to manufacture and\nscalable. We then use this multi-modal tactile feedback along with\nvisuo-proprioceptive observations to synthesize a tactile-diffusion policy from\nhuman demonstrations; the resulting contact-aware control policy significantly\noutperforms haptic-oblivious policies in multiple contact-aware manipulation\npolicies. This paper highlights how effectively integrating multi-modal contact\nsensing can hasten the development of effective contact-aware manipulation\npolicies, paving the way for more reliable and versatile domestic robots. More\ninformation can be found at https://polytouch.alanz.info/"
    },
    {
        "date": "2025-04",
        "title": "Evaluating Organization Security: User Stories of European Union NIS2 Directive",
        "author": "Mari Seeba, Magnus Valgre, and Raimundas Matulevi\u010dius",
        "link": "http://arxiv.org/abs/2504.19222v1",
        "abstract": "The NIS2 directive requires EU Member States to ensure a consistently high\nlevel of cybersecurity by setting risk-management measures for essential and\nimportant entities. Evaluations are necessary to assess whether the required\nsecurity level is met. This involves understanding the needs and goals of\ndifferent personas defined by NIS2, who benefit from evaluation results. In\nthis paper, we consider how NIS2 user stories support the evaluation of the\nlevel of information security in organizations. Using requirements elicitation\nprinciples, we extracted the legal requirements from NIS2 from our narrowed\nscope, identified six key personas and their goals, formulated user stories\nbased on the gathered information, and validated the usability and relevance of\nthe user stories with security evaluation instruments or methods we found from\nthe literature. The defined user stories help to adjust existing instruments\nand methods of assessing the security level to comply with NIS2. On the other\nhand, user stories enable us to see the patterns related to security evaluation\nwhen developing new NIS2-compliant security evaluation methods to optimize the\nadministrative burden of entities."
    },
    {
        "date": "2025-04",
        "title": "Generative Adversarial Network based Voice Conversion: Techniques, Challenges, and Recent Advancements",
        "author": "Sandipan Dhar, Nanda Dulal Jana, and Swagatam Das",
        "link": "http://arxiv.org/abs/2504.19197v1",
        "abstract": "Voice conversion (VC) stands as a crucial research area in speech synthesis,\nenabling the transformation of a speaker's vocal characteristics to resemble\nanother while preserving the linguistic content. This technology has broad\napplications, including automated movie dubbing, speech-to-singing conversion,\nand assistive devices for pathological speech rehabilitation. With the\nincreasing demand for high-quality and natural-sounding synthetic voices,\nresearchers have developed a wide range of VC techniques. Among these,\ngenerative adversarial network (GAN)-based approaches have drawn considerable\nattention for their powerful feature-mapping capabilities and potential to\nproduce highly realistic speech. Despite notable advancements, challenges such\nas ensuring training stability, maintaining linguistic consistency, and\nachieving perceptual naturalness continue to hinder progress in GAN-based VC\nsystems. This systematic review presents a comprehensive analysis of the voice\nconversion landscape, highlighting key techniques, key challenges, and the\ntransformative impact of GANs in the field. The survey categorizes existing\nmethods, examines technical obstacles, and critically evaluates recent\ndevelopments in GAN-based VC. By consolidating and synthesizing research\nfindings scattered across the literature, this review provides a structured\nunderstanding of the strengths and limitations of different approaches. The\nsignificance of this survey lies in its ability to guide future research by\nidentifying existing gaps, proposing potential directions, and offering\ninsights for building more robust and efficient VC systems. Overall, this work\nserves as an essential resource for researchers, developers, and practitioners\naiming to advance the state-of-the-art (SOTA) in voice conversion technology."
    },
    {
        "date": "2025-04",
        "title": "SPC: Evolving Self-Play Critic via Adversarial Games for LLM Reasoning",
        "author": "Jiaqi Chen, Bang Zhang, Ruotian Ma, Peisong Wang, Xiaodan Liang, Zhaopeng Tu, Xiaolong Li, and Kwan-Yee K. Wong",
        "link": "http://arxiv.org/abs/2504.19162v1",
        "abstract": "Evaluating the step-by-step reliability of large language model (LLM)\nreasoning, such as Chain-of-Thought, remains challenging due to the difficulty\nand cost of obtaining high-quality step-level supervision. In this paper, we\nintroduce Self-Play Critic (SPC), a novel approach where a critic model evolves\nits ability to assess reasoning steps through adversarial self-play games,\neliminating the need for manual step-level annotation. SPC involves fine-tuning\ntwo copies of a base model to play two roles, namely a \"sneaky generator\" that\ndeliberately produces erroneous steps designed to be difficult to detect, and a\n\"critic\" that analyzes the correctness of reasoning steps. These two models\nengage in an adversarial game in which the generator aims to fool the critic,\nwhile the critic model seeks to identify the generator's errors. Using\nreinforcement learning based on the game outcomes, the models iteratively\nimprove; the winner of each confrontation receives a positive reward and the\nloser receives a negative reward, driving continuous self-evolution.\nExperiments on three reasoning process benchmarks (ProcessBench, PRM800K,\nDeltaBench) demonstrate that our SPC progressively enhances its error detection\ncapabilities (e.g., accuracy increases from 70.8% to 77.7% on ProcessBench) and\nsurpasses strong baselines, including distilled R1 model. Furthermore, applying\nSPC to guide the test-time search of diverse LLMs significantly improves their\nmathematical reasoning performance on MATH500 and AIME2024, outperforming\nstate-of-the-art process reward models."
    },
    {
        "date": "2025-04",
        "title": "Comparative Analysis of AI-Driven Security Approaches in DevSecOps: Challenges, Solutions, and Future Directions",
        "author": "Farid Binbeshr, and Muhammad Imam",
        "link": "http://arxiv.org/abs/2504.19154v1",
        "abstract": "The integration of security within DevOps, known as DevSecOps, has gained\ntraction in modern software development to address security vulnerabilities\nwhile maintaining agility. Artificial Intelligence (AI) and Machine Learning\n(ML) have been increasingly leveraged to enhance security automation, threat\ndetection, and compliance enforcement. However, existing studies primarily\nfocus on individual aspects of AI-driven security in DevSecOps, lacking a\nstructured comparison of methodologies. This study conducts a systematic\nliterature review (SLR) to analyze and compare AI-driven security solutions in\nDevSecOps, evaluating their technical capabilities, implementation challenges,\nand operational impacts. The findings reveal gaps in empirical validation,\nscalability, and integration of AI in security automation. The study highlights\nbest practices, identifies research gaps, and proposes future directions for\noptimizing AI-based security frameworks in DevSecOps."
    },
    {
        "date": "2025-04",
        "title": "Fast and Robust: Task Sampling with Posterior and Diversity Synergies for Adaptive Decision-Makers in Randomized Environments",
        "author": "Yun Qu, Qi Cheems Wang, Yixiu Mao, Yiqin Lv, and Xiangyang Ji",
        "link": "http://arxiv.org/abs/2504.19139v2",
        "abstract": "Task robust adaptation is a long-standing pursuit in sequential\ndecision-making. Some risk-averse strategies, e.g., the conditional\nvalue-at-risk principle, are incorporated in domain randomization or meta\nreinforcement learning to prioritize difficult tasks in optimization, which\ndemand costly intensive evaluations. The efficiency issue prompts the\ndevelopment of robust active task sampling to train adaptive policies, where\nrisk-predictive models are used to surrogate policy evaluation. This work\ncharacterizes the optimization pipeline of robust active task sampling as a\nMarkov decision process, posits theoretical and practical insights, and\nconstitutes robustness concepts in risk-averse scenarios. Importantly, we\npropose an easy-to-implement method, referred to as Posterior and Diversity\nSynergized Task Sampling (PDTS), to accommodate fast and robust sequential\ndecision-making. Extensive experiments show that PDTS unlocks the potential of\nrobust active task sampling, significantly improves the zero-shot and few-shot\nadaptation robustness in challenging tasks, and even accelerates the learning\nprocess under certain scenarios. Our project website is at\nhttps://thu-rllab.github.io/PDTS_project_page."
    },
    {
        "date": "2025-04",
        "title": "Security Vulnerabilities in Quantum Cloud Systems: A Survey on Emerging Threats",
        "author": "Justin Coupel, and Tasnuva Farheen",
        "link": "http://arxiv.org/abs/2504.19064v1",
        "abstract": "Quantum computing is becoming increasingly widespread due to the potential\nand capabilities to solve complex problems beyond the scope of classical\ncomputers. As Quantum Cloud services are adopted by businesses and research\ngroups, they allow for greater progress and application in many fields.\nHowever, the inherent vulnerabilities of these environments pose significant\nsecurity concerns. This survey delivers a comprehensive analysis of the\nsecurity challenges that emerged in quantum cloud systems, with a distinct\nfocus on multi-tenant vulnerabilities and the classical-quantum interface. Key\nthreats such as crosstalk attacks, quantum-specific side-channel\nvulnerabilities, and insider threats are all examined, as well as their effects\non the confidentiality, integrity, and availability of quantum circuits. The\ndesign and implementation of various quantum architectures from quantum cloud\nproviders are also discussed. In addition, this paper delves into emerging\nquantum security solutions and best practices to mitigate these risks. This\nsurvey offers insights into current research gaps and proposes future\ndirections for secure and resilient quantum cloud infrastructures."
    },
    {
        "date": "2025-04",
        "title": "BinPool: A Dataset of Vulnerabilities for Binary Security Analysis",
        "author": "Sima Arasteh, Georgios Nikitopoulos, Wei-Cheng Wu, Nicolaas Weideman, Aaron Portnoy, Mukund Raghothaman, and Christophe Hauser",
        "link": "http://arxiv.org/abs/2504.19055v1",
        "abstract": "The development of machine learning techniques for discovering software\nvulnerabilities relies fundamentally on the availability of appropriate\ndatasets. The ideal dataset consists of a large and diverse collection of\nreal-world vulnerabilities, paired so as to contain both vulnerable and patched\nversions of each program. Naturally, collecting such datasets is a laborious\nand time-consuming task. Within the specific domain of vulnerability discovery\nin binary code, previous datasets are either publicly unavailable, lack\nsemantic diversity, involve artificially introduced vulnerabilities, or were\ncollected using static analyzers, thereby themselves containing incorrectly\nlabeled example programs.\n  In this paper, we describe a new publicly available dataset which we dubbed\nBinpool, containing numerous samples of\n  vulnerable versions of Debian packages across the years. The dataset was\nautomatically curated, and contains both vulnerable and patched versions of\neach program, compiled at four different optimization levels. Overall, the\ndataset covers 603 distinct CVEs across 89 CWE classes, 162 Debian packages,\nand contains 6144 binaries. We argue that this dataset is suitable for\nevaluating a range of security analysis tools, including for vulnerability\ndiscovery, binary function similarity, and plagiarism detection."
    },
    {
        "date": "2025-04",
        "title": "DiCE-Extended: A Robust Approach to Counterfactual Explanations in Machine Learning",
        "author": "Volkan Bakir, Polat Goktas, and Sureyya Akyuz",
        "link": "http://arxiv.org/abs/2504.19027v1",
        "abstract": "Explainable artificial intelligence (XAI) has become increasingly important\nin decision-critical domains such as healthcare, finance, and law.\nCounterfactual (CF) explanations, a key approach in XAI, provide users with\nactionable insights by suggesting minimal modifications to input features that\nlead to different model outcomes. Despite significant advancements, existing CF\ngeneration methods often struggle to balance proximity, diversity, and\nrobustness, limiting their real-world applicability. A widely adopted\nframework, Diverse Counterfactual Explanations (DiCE), emphasizes diversity but\nlacks robustness, making CF explanations sensitive to perturbations and domain\nconstraints. To address these challenges, we introduce DiCE-Extended, an\nenhanced CF explanation framework that integrates multi-objective optimization\ntechniques to improve robustness while maintaining interpretability. Our\napproach introduces a novel robustness metric based on the Dice-Sorensen\ncoefficient, ensuring stability under small input variations. Additionally, we\nrefine CF generation using weighted loss components (lambda_p, lambda_d,\nlambda_r) to balance proximity, diversity, and robustness. We empirically\nvalidate DiCE-Extended on benchmark datasets (COMPAS, Lending Club, German\nCredit, Adult Income) across multiple ML backends (Scikit-learn, PyTorch,\nTensorFlow). Results demonstrate improved CF validity, stability, and alignment\nwith decision boundaries compared to standard DiCE-generated explanations. Our\nfindings highlight the potential of DiCE-Extended in generating more reliable\nand interpretable CFs for high-stakes applications. Future work will explore\nadaptive optimization techniques and domain-specific constraints to further\nenhance CF generation in real-world scenarios."
    },
    {
        "date": "2025-04",
        "title": "Graph of Attacks: Improved Black-Box and Interpretable Jailbreaks for LLMs",
        "author": "Mohammad Akbar-Tajari, Mohammad Taher Pilehvar, and Mohammad Mahmoody",
        "link": "http://arxiv.org/abs/2504.19019v1",
        "abstract": "The challenge of ensuring Large Language Models (LLMs) align with societal\nstandards is of increasing interest, as these models are still prone to\nadversarial jailbreaks that bypass their safety mechanisms. Identifying these\nvulnerabilities is crucial for enhancing the robustness of LLMs against such\nexploits. We propose Graph of ATtacks (GoAT), a method for generating\nadversarial prompts to test the robustness of LLM alignment using the Graph of\nThoughts framework [Besta et al., 2024]. GoAT excels at generating highly\neffective jailbreak prompts with fewer queries to the victim model than\nstate-of-the-art attacks, achieving up to five times better jailbreak success\nrate against robust models like Llama. Notably, GoAT creates high-quality,\nhuman-readable prompts without requiring access to the targeted model's\nparameters, making it a black-box attack. Unlike approaches constrained by\ntree-based reasoning, GoAT's reasoning is based on a more intricate graph\nstructure. By making simultaneous attack paths aware of each other's progress,\nthis dynamic framework allows a deeper integration and refinement of reasoning\npaths, significantly enhancing the collaborative exploration of adversarial\nvulnerabilities in LLMs. At a technical level, GoAT starts with a graph\nstructure and iteratively refines it by combining and improving thoughts,\nenabling synergy between different thought paths. The code for our\nimplementation can be found at: https://github.com/GoAT-pydev/Graph_of_Attacks."
    },
    {
        "date": "2025-04",
        "title": "Deep Learning-Based Multi-Modal Fusion for Robust Robot Perception and Navigation",
        "author": "Delun Lai, Yeyubei Zhang, Yunchong Liu, Chaojie Li, and Huadong Mo",
        "link": "http://arxiv.org/abs/2504.19002v1",
        "abstract": "This paper introduces a novel deep learning-based multimodal fusion\narchitecture aimed at enhancing the perception capabilities of autonomous\nnavigation robots in complex environments. By utilizing innovative feature\nextraction modules, adaptive fusion strategies, and time-series modeling\nmechanisms, the system effectively integrates RGB images and LiDAR data. The\nkey contributions of this work are as follows: a. the design of a lightweight\nfeature extraction network to enhance feature representation; b. the\ndevelopment of an adaptive weighted cross-modal fusion strategy to improve\nsystem robustness; and c. the incorporation of time-series information modeling\nto boost dynamic scene perception accuracy. Experimental results on the KITTI\ndataset demonstrate that the proposed approach increases navigation and\npositioning accuracy by 3.5% and 2.2%, respectively, while maintaining\nreal-time performance. This work provides a novel solution for autonomous robot\nnavigation in complex environments."
    },
    {
        "date": "2025-04",
        "title": "Unveiling and Mitigating Adversarial Vulnerabilities in Iterative Optimizers",
        "author": "Elad Sofer, Tomer Shaked, Caroline Chaux, and Nir Shlezinger",
        "link": "http://arxiv.org/abs/2504.19000v1",
        "abstract": "Machine learning (ML) models are often sensitive to carefully crafted yet\nseemingly unnoticeable perturbations. Such adversarial examples are considered\nto be a property of ML models, often associated with their black-box operation\nand sensitivity to features learned from data. This work examines the\nadversarial sensitivity of non-learned decision rules, and particularly of\niterative optimizers. Our analysis is inspired by the recent developments in\ndeep unfolding, which cast such optimizers as ML models. We show that\nnon-learned iterative optimizers share the sensitivity to adversarial examples\nof ML models, and that attacking iterative optimizers effectively alters the\noptimization objective surface in a manner that modifies the minima sought. We\nthen leverage the ability to cast iteration-limited optimizers as ML models to\nenhance robustness via adversarial training. For a class of proximal gradient\noptimizers, we rigorously prove how their learning affects adversarial\nsensitivity. We numerically back our findings, showing the vulnerability of\nvarious optimizers, as well as the robustness induced by unfolding and\nadversarial training."
    },
    {
        "date": "2025-04",
        "title": "Safety Interventions against Adversarial Patches in an Open-Source Driver Assistance System",
        "author": "Cheng Chen, Grant Xiao, Daehyun Lee, Lishan Yang, Evgenia Smirni, Homa Alemzadeh, and Xugui Zhou",
        "link": "http://arxiv.org/abs/2504.18990v1",
        "abstract": "Drivers are becoming increasingly reliant on advanced driver assistance\nsystems (ADAS) as autonomous driving technology becomes more popular and\ndeveloped with advanced safety features to enhance road safety. However, the\nincreasing complexity of the ADAS makes autonomous vehicles (AVs) more exposed\nto attacks and accidental faults. In this paper, we evaluate the resilience of\na widely used ADAS against safety-critical attacks that target perception\ninputs. Various safety mechanisms are simulated to assess their impact on\nmitigating attacks and enhancing ADAS resilience. Experimental results\nhighlight the importance of timely intervention by human drivers and automated\nsafety mechanisms in preventing accidents in both driving and lateral\ndirections and the need to resolve conflicts among safety interventions to\nenhance system resilience and reliability."
    },
    {
        "date": "2025-04",
        "title": "SONNI: Secure Oblivious Neural Network Inference",
        "author": "Luke Sperling, and Sandeep S. Kulkarni",
        "link": "http://arxiv.org/abs/2504.18974v1",
        "abstract": "In the standard privacy-preserving Machine learning as-a-service (MLaaS)\nmodel, the client encrypts data using homomorphic encryption and uploads it to\na server for computation. The result is then sent back to the client for\ndecryption. It has become more and more common for the computation to be\noutsourced to third-party servers. In this paper we identify a weakness in this\nprotocol that enables a completely undetectable novel model-stealing attack\nthat we call the Silver Platter attack. This attack works even under multikey\nencryption that prevents a simple collusion attack to steal model parameters.\nWe also propose a mitigation that protects privacy even in the presence of a\nmalicious server and malicious client or model provider (majority dishonest).\nWhen compared to a state-of-the-art but small encrypted model with 32k\nparameters, we preserve privacy with a failure chance of 1.51 x 10^-28 while\nbatching capability is reduced by 0.2%. Our approach uses a novel\nresults-checking protocol that ensures the computation was performed correctly\nwithout violating honest clients' data privacy. Even with collusion between the\nclient and the server, they are unable to steal model parameters. Additionally,\nthe model provider cannot learn any client data if maliciously working with the\nserver."
    },
    {
        "date": "2025-04",
        "title": "Speaker Retrieval in the Wild: Challenges, Effectiveness and Robustness",
        "author": "Erfan Loweimi, Mengjie Qian, Kate Knill, and Mark Gales",
        "link": "http://arxiv.org/abs/2504.18950v2",
        "abstract": "There is a growing abundance of publicly available or company-owned\naudio/video archives, highlighting the increasing importance of efficient\naccess to desired content and information retrieval from these archives. This\npaper investigates the challenges, solutions, effectiveness, and robustness of\nspeaker retrieval systems developed \"in the wild\" which involves addressing two\nprimary challenges: extraction of task-relevant labels from limited metadata\nfor system development and evaluation, as well as the unconstrained acoustic\nconditions encountered in the archive, ranging from quiet studios to adverse\nnoisy environments. While we focus on the publicly-available BBC Rewind archive\n(spanning 1948 to 1979), our framework addresses the broader issue of speaker\nretrieval on extensive and possibly aged archives with no control over the\ncontent and acoustic conditions. Typically, these archives offer a brief and\ngeneral file description, mostly inadequate for specific applications like\nspeaker retrieval, and manual annotation of such large-scale archives is\nunfeasible. We explore various aspects of system development (e.g., speaker\ndiarisation, embedding extraction, query selection) and analyse the challenges,\npossible solutions, and their functionality. To evaluate the performance, we\nconduct systematic experiments in both clean setup and against various\ndistortions simulating real-world applications. Our findings demonstrate the\neffectiveness and robustness of the developed speaker retrieval systems,\nestablishing the versatility and scalability of the proposed framework for a\nwide range of applications beyond the BBC Rewind corpus."
    },
    {
        "date": "2025-04",
        "title": "Sim-to-Real: An Unsupervised Noise Layer for Screen-Camera Watermarking Robustness",
        "author": "Yufeng Wu, Xin Liao, Baowei Wang, Han Fang, Xiaoshuai Wu, and Guiling Wang",
        "link": "http://arxiv.org/abs/2504.18906v1",
        "abstract": "Unauthorized screen capturing and dissemination pose severe security threats\nsuch as data leakage and information theft. Several studies propose robust\nwatermarking methods to track the copyright of Screen-Camera (SC) images,\nfacilitating post-hoc certification against infringement. These techniques\ntypically employ heuristic mathematical modeling or supervised neural network\nfitting as the noise layer, to enhance watermarking robustness against SC.\nHowever, both strategies cannot fundamentally achieve an effective\napproximation of SC noise. Mathematical simulation suffers from biased\napproximations due to the incomplete decomposition of the noise and the absence\nof interdependence among the noise components. Supervised networks require\npaired data to train the noise-fitting model, and it is difficult for the model\nto learn all the features of the noise. To address the above issues, we propose\nSimulation-to-Real (S2R). Specifically, an unsupervised noise layer employs\nunpaired data to learn the discrepancy between the modeling simulated noise\ndistribution and the real-world SC noise distribution, rather than directly\nlearning the mapping from sharp images to real-world images. Learning this\ntransformation from simulation to reality is inherently simpler, as it\nprimarily involves bridging the gap in noise distributions, instead of the\ncomplex task of reconstructing fine-grained image details. Extensive\nexperimental results validate the efficacy of the proposed method,\ndemonstrating superior watermark robustness and generalization compared to\nthose of state-of-the-art methods."
    },
    {
        "date": "2025-04",
        "title": "Latent Adversarial Training Improves the Representation of Refusal",
        "author": "Alexandra Abbas, Nora Petrova, Helios Ael Lyons, and Natalia Perez-Campanero",
        "link": "http://arxiv.org/abs/2504.18872v1",
        "abstract": "Recent work has shown that language models' refusal behavior is primarily\nencoded in a single direction in their latent space, making it vulnerable to\ntargeted attacks. Although Latent Adversarial Training (LAT) attempts to\nimprove robustness by introducing noise during training, a key question\nremains: How does this noise-based training affect the underlying\nrepresentation of refusal behavior? Understanding this encoding is crucial for\nevaluating LAT's effectiveness and limitations, just as the discovery of linear\nrefusal directions revealed vulnerabilities in traditional supervised safety\nfine-tuning (SSFT).\n  Through the analysis of Llama 2 7B, we examine how LAT reorganizes the\nrefusal behavior in the model's latent space compared to SSFT and embedding\nspace adversarial training (AT). By computing activation differences between\nharmful and harmless instruction pairs and applying Singular Value\nDecomposition (SVD), we find that LAT significantly alters the refusal\nrepresentation, concentrating it in the first two SVD components which explain\napproximately 75 percent of the activation differences variance - significantly\nhigher than in reference models. This concentrated representation leads to more\neffective and transferable refusal vectors for ablation attacks: LAT models\nshow improved robustness when attacked with vectors from reference models but\nbecome more vulnerable to self-generated vectors compared to SSFT and AT. Our\nfindings suggest that LAT's training perturbations enable a more comprehensive\nrepresentation of refusal behavior, highlighting both its potential strengths\nand vulnerabilities for improving model safety."
    },
    {
        "date": "2025-04",
        "title": "Zero-Day Botnet Attack Detection in IoV: A Modular Approach Using Isolation Forests and Particle Swarm Optimization",
        "author": "Abdelaziz Amara Korba, Nour Elislem Karabadji, and Yacine Ghamri-Doudane",
        "link": "http://arxiv.org/abs/2504.18814v2",
        "abstract": "The Internet of Vehicles (IoV) is transforming transportation by enhancing\nconnectivity and enabling autonomous driving. However, this increased\ninterconnectivity introduces new security vulnerabilities. Bot malware and\ncyberattacks pose significant risks to Connected and Autonomous Vehicles\n(CAVs), as demonstrated by real-world incidents involving remote vehicle system\ncompromise. To address these challenges, we propose an edge-based Intrusion\nDetection System (IDS) that monitors network traffic to and from CAVs. Our\ndetection model is based on a meta-ensemble classifier capable of recognizing\nknown (Nday) attacks and detecting previously unseen (zero-day) attacks. The\napproach involves training multiple Isolation Forest (IF) models on\nMulti-access Edge Computing (MEC) servers, with each IF specialized in\nidentifying a specific type of botnet attack. These IFs, either trained locally\nor shared by other MEC nodes, are then aggregated using a Particle Swarm\nOptimization (PSO) based stacking strategy to construct a robust\nmeta-classifier. The proposed IDS has been evaluated on a vehicular botnet\ndataset, achieving an average detection rate of 92.80% for N-day attacks and\n77.32% for zero-day attacks. These results highlight the effectiveness of our\nsolution in detecting both known and emerging threats, providing a scalable and\nadaptive defense mechanism for CAVs within the IoV ecosystem."
    },
    {
        "date": "2025-04",
        "title": "Performance of Machine Learning Classifiers for Anomaly Detection in Cyber Security Applications",
        "author": "Markus Haug, and Gissel Velarde",
        "link": "http://arxiv.org/abs/2504.18771v1",
        "abstract": "This work empirically evaluates machine learning models on two imbalanced\npublic datasets (KDDCUP99 and Credit Card Fraud 2013). The method includes data\npreparation, model training, and evaluation, using an 80/20 (train/test) split.\nModels tested include eXtreme Gradient Boosting (XGB), Multi Layer Perceptron\n(MLP), Generative Adversarial Network (GAN), Variational Autoencoder (VAE), and\nMultiple-Objective Generative Adversarial Active Learning (MO-GAAL), with XGB\nand MLP further combined with Random-Over-Sampling (ROS) and\nSelf-Paced-Ensemble (SPE). Evaluation involves 5-fold cross-validation and\nimputation techniques (mean, median, and IterativeImputer) with 10, 20, 30, and\n50 % missing data. Findings show XGB and MLP outperform generative models.\nIterativeImputer results are comparable to mean and median, but not recommended\nfor large datasets due to increased complexity and execution time. The code\nused is publicly available on GitHub (github.com/markushaug/acr-25)."
    },
    {
        "date": "2025-04",
        "title": "PICO: Secure Transformers via Robust Prompt Isolation and Cybersecurity Oversight",
        "author": "Ben Goertzel, and Paulos Yibelo",
        "link": "http://arxiv.org/abs/2504.21029v1",
        "abstract": "We propose a robust transformer architecture designed to prevent prompt\ninjection attacks and ensure secure, reliable response generation. Our PICO\n(Prompt Isolation and Cybersecurity Oversight) framework structurally separates\ntrusted system instructions from untrusted user inputs through dual channels\nthat are processed independently and merged only by a controlled, gated fusion\nmechanism. In addition, we integrate a specialized Security Expert Agent within\na Mixture-of-Experts (MoE) framework and incorporate a Cybersecurity Knowledge\nGraph (CKG) to supply domain-specific reasoning. Our training design further\nensures that the system prompt branch remains immutable while the rest of the\nnetwork learns to handle adversarial inputs safely. This PICO framework is\npresented via a general mathematical formulation, then elaborated in terms of\nthe specifics of transformer architecture, and fleshed out via hypothetical\ncase studies including Policy Puppetry attacks. While the most effective\nimplementation may involve training transformers in a PICO-based way from\nscratch, we also present a cost-effective fine-tuning approach."
    },
    {
        "date": "2025-04",
        "title": "Intelligent Attacks and Defense Methods in Federated Learning-enabled Energy-Efficient Wireless Networks",
        "author": "Han Zhang, Hao Zhou, Medhat Elsayed, Majid Bavand, Raimundas Gaigalas, Yigit Ozcan, and Melike Erol-Kantarci",
        "link": "http://arxiv.org/abs/2504.18519v1",
        "abstract": "Federated learning (FL) is a promising technique for learning-based functions\nin wireless networks, thanks to its distributed implementation capability. On\nthe other hand, distributed learning may increase the risk of exposure to\nmalicious attacks where attacks on a local model may spread to other models by\nparameter exchange. Meanwhile, such attacks can be hard to detect due to the\ndynamic wireless environment, especially considering local models can be\nheterogeneous with non-independent and identically distributed (non-IID) data.\nTherefore, it is critical to evaluate the effect of malicious attacks and\ndevelop advanced defense techniques for FL-enabled wireless networks. In this\nwork, we introduce a federated deep reinforcement learning-based cell sleep\ncontrol scenario that enhances the energy efficiency of the network. We propose\nmultiple intelligent attacks targeting the learning-based approach and we\npropose defense methods to mitigate such attacks. In particular, we have\ndesigned two attack models, generative adversarial network (GAN)-enhanced model\npoisoning attack and regularization-based model poisoning attack. As a\ncounteraction, we have proposed two defense schemes, autoencoder-based defense,\nand knowledge distillation (KD)-enabled defense. The autoencoder-based defense\nmethod leverages an autoencoder to identify the malicious participants and only\naggregate the parameters of benign local models during the global aggregation,\nwhile KD-based defense protects the model from attacks by controlling the\nknowledge transferred between the global model and local models."
    },
    {
        "date": "2025-04",
        "title": "DeSIA: Attribute Inference Attacks Against Limited Fixed Aggregate Statistics",
        "author": "Yifeng Mao, Bozhidar Stevanoski, and Yves-Alexandre de Montjoye",
        "link": "http://arxiv.org/abs/2504.18497v1",
        "abstract": "Empirical inference attacks are a popular approach for evaluating the privacy\nrisk of data release mechanisms in practice. While an active attack literature\nexists to evaluate machine learning models or synthetic data release, we\ncurrently lack comparable methods for fixed aggregate statistics, in particular\nwhen only a limited number of statistics are released. We here propose an\ninference attack framework against fixed aggregate statistics and an attribute\ninference attack called DeSIA. We instantiate DeSIA against the U.S. Census\nPPMF dataset and show it to strongly outperform reconstruction-based attacks.\nIn particular, we show DeSIA to be highly effective at identifying vulnerable\nusers, achieving a true positive rate of 0.14 at a false positive rate of\n$10^{-3}$. We then show DeSIA to perform well against users whose attributes\ncannot be verified and when varying the number of aggregate statistics and\nlevel of noise addition. We also perform an extensive ablation study of DeSIA\nand show how DeSIA can be successfully adapted to the membership inference\ntask. Overall, our results show that aggregation alone is not sufficient to\nprotect privacy, even when a relatively small number of aggregates are being\nreleased, and emphasize the need for formal privacy mechanisms and testing\nbefore aggregate statistics are released."
    },
    {
        "date": "2025-04",
        "title": "Pseudo-Asynchronous Local SGD: Robust and Efficient Data-Parallel Training",
        "author": "Hiroki Naganuma, Xinzhi Zhang, Man-Chung Yue, Ioannis Mitliagkas, Philipp A. Witte, Russell J. Hewett, and Yin Tat Lee",
        "link": "http://arxiv.org/abs/2504.18454v1",
        "abstract": "Following AI scaling trends, frontier models continue to grow in size and\ncontinue to be trained on larger datasets. Training these models requires huge\ninvestments in exascale computational resources, which has in turn driven\ndevelopment of distributed deep learning methods. Data parallelism is an\nessential approach to speed up training, but it requires frequent global\ncommunication between workers, which can bottleneck training at the largest\nscales. In this work, we propose a method called Pseudo-Asynchronous Local SGD\n(PALSGD) to improve the efficiency of data-parallel training. PALSGD is an\nextension of Local SGD (Stich, 2018) and DiLoCo (Douillard et al., 2023),\ndesigned to further reduce communication frequency by introducing a\npseudo-synchronization mechanism. PALSGD allows the use of longer\nsynchronization intervals compared to standard Local SGD. Despite the reduced\ncommunication frequency, the pseudo-synchronization approach ensures that model\nconsistency is maintained, leading to performance results comparable to those\nachieved with more frequent synchronization. Furthermore, we provide a\ntheoretical analysis of PALSGD, establishing its convergence and deriving its\nconvergence rate. This analysis offers insights into the algorithm's behavior\nand performance guarantees. We evaluated PALSGD on image classification and\nlanguage modeling tasks. Our results show that PALSGD achieves better\nperformance in less time compared to existing methods like Distributed Data\nParallel (DDP), and DiLoCo. Notably, PALSGD trains 18.4% faster than DDP on\nImageNet-1K with ResNet-50, 24.4% faster than DDP on TinyStories with\nGPT-Neo125M, and 21.1% faster than DDP on TinyStories with GPT-Neo-8M."
    },
    {
        "date": "2025-04",
        "title": "Boosting-Enabled Robust System Identification of Partially Observed LTI Systems Under Heavy-Tailed Noise",
        "author": "Vinay Kanakeri, and Aritra Mitra",
        "link": "http://arxiv.org/abs/2504.18444v1",
        "abstract": "We consider the problem of system identification of partially observed linear\ntime-invariant (LTI) systems. Given input-output data, we provide\nnon-asymptotic guarantees for identifying the system parameters under general\nheavy-tailed noise processes. Unlike previous works that assume Gaussian or\nsub-Gaussian noise, we consider significantly broader noise distributions that\nare required to admit only up to the second moment. For this setting, we\nleverage tools from robust statistics to propose a novel system identification\nalgorithm that exploits the idea of boosting. Despite the much weaker noise\nassumptions, we show that our proposed algorithm achieves sample complexity\nbounds that nearly match those derived under sub-Gaussian noise. In particular,\nwe establish that our bounds retain a logarithmic dependence on the prescribed\nfailure probability. Interestingly, we show that such bounds can be achieved by\nrequiring just a finite fourth moment on the excitatory input process."
    },
    {
        "date": "2025-04",
        "title": "HepatoGEN: Generating Hepatobiliary Phase MRI with Perceptual and Adversarial Models",
        "author": "Jens Hooge, Gerard Sanroma-Guell, Faidra Stavropoulou, Alexander Ullmann, Gesine Knobloch, Mark Klemens, Carola Schmidt, Sabine Weckbach, and Andreas Bolz",
        "link": "http://arxiv.org/abs/2504.18405v1",
        "abstract": "Dynamic contrast-enhanced magnetic resonance imaging (DCE-MRI) plays a\ncrucial role in the detection and characterization of focal liver lesions, with\nthe hepatobiliary phase (HBP) providing essential diagnostic information.\nHowever, acquiring HBP images requires prolonged scan times, which may\ncompromise patient comfort and scanner throughput. In this study, we propose a\ndeep learning based approach for synthesizing HBP images from earlier contrast\nphases (precontrast and transitional) and compare three generative models: a\nperceptual U-Net, a perceptual GAN (pGAN), and a denoising diffusion\nprobabilistic model (DDPM). We curated a multi-site DCE-MRI dataset from\ndiverse clinical settings and introduced a contrast evolution score (CES) to\nassess training data quality, enhancing model performance. Quantitative\nevaluation using pixel-wise and perceptual metrics, combined with qualitative\nassessment through blinded radiologist reviews, showed that pGAN achieved the\nbest quantitative performance but introduced heterogeneous contrast in\nout-of-distribution cases. In contrast, the U-Net produced consistent liver\nenhancement with fewer artifacts, while DDPM underperformed due to limited\npreservation of fine structural details. These findings demonstrate the\nfeasibility of synthetic HBP image generation as a means to reduce scan time\nwithout compromising diagnostic utility, highlighting the clinical potential of\ndeep learning for dynamic contrast enhancement in liver MRI. A project demo is\navailable at: https://jhooge.github.io/hepatogen"
    },
    {
        "date": "2025-04",
        "title": "Model Evaluation in the Dark: Robust Classifier Metrics with Missing Labels",
        "author": "Danial Dervovic, and Michael Cashmore",
        "link": "http://arxiv.org/abs/2504.18385v1",
        "abstract": "Missing data in supervised learning is well-studied, but the specific issue\nof missing labels during model evaluation has been overlooked. Ignoring samples\nwith missing values, a common solution, can introduce bias, especially when\ndata is Missing Not At Random (MNAR). We propose a multiple imputation\ntechnique for evaluating classifiers using metrics such as precision, recall,\nand ROC-AUC. This method not only offers point estimates but also a predictive\ndistribution for these quantities when labels are missing. We empirically show\nthat the predictive distribution's location and shape are generally correct,\neven in the MNAR regime. Moreover, we establish that this distribution is\napproximately Gaussian and provide finite-sample convergence bounds.\nAdditionally, a robustness proof is presented, confirming the validity of the\napproximation under a realistic error model."
    },
    {
        "date": "2025-04",
        "title": "Adversarial Attacks on LLM-as-a-Judge Systems: Insights from Prompt Injections",
        "author": "Narek Maloyan, and Dmitry Namiot",
        "link": "http://arxiv.org/abs/2504.18333v1",
        "abstract": "LLM as judge systems used to assess text quality code correctness and\nargument strength are vulnerable to prompt injection attacks. We introduce a\nframework that separates content author attacks from system prompt attacks and\nevaluate five models Gemma 3.27B Gemma 3.4B Llama 3.2 3B GPT 4 and Claude 3\nOpus on four tasks with various defenses using fifty prompts per condition.\nAttacks achieved up to seventy three point eight percent success smaller models\nproved more vulnerable and transferability ranged from fifty point five to\nsixty two point six percent. Our results contrast with Universal Prompt\nInjection and AdvPrompter We recommend multi model committees and comparative\nscoring and release all code and datasets"
    },
    {
        "date": "2025-04",
        "title": "Outlier-aware Tensor Robust Principal Component Analysis with Self-guided Data Augmentation",
        "author": "Yangyang Xu, Kexin Li, Li Yang, and You-Wei Wen",
        "link": "http://arxiv.org/abs/2504.18323v1",
        "abstract": "Tensor Robust Principal Component Analysis (TRPCA) is a fundamental technique\nfor decomposing multi-dimensional data into a low-rank tensor and an outlier\ntensor, yet existing methods relying on sparse outlier assumptions often fail\nunder structured corruptions. In this paper, we propose a self-guided data\naugmentation approach that employs adaptive weighting to suppress outlier\ninfluence, reformulating the original TRPCA problem into a standard Tensor\nPrincipal Component Analysis (TPCA) problem. The proposed model involves an\noptimization-driven weighting scheme that dynamically identifies and\ndownweights outlier contributions during tensor augmentation. We develop an\nefficient proximal block coordinate descent algorithm with closed-form updates\nto solve the resulting optimization problem, ensuring computational efficiency.\nTheoretical convergence is guaranteed through a framework combining block\ncoordinate descent with majorization-minimization principles. Numerical\nexperiments on synthetic and real-world datasets, including face recovery,\nbackground subtraction, and hyperspectral denoising, demonstrate that our\nmethod effectively handles various corruption patterns. The results show the\nimprovements in both accuracy and computational efficiency compared to\nstate-of-the-art methods."
    },
    {
        "date": "2025-04",
        "title": "Enhancing Long-Term Re-Identification Robustness Using Synthetic Data: A Comparative Analysis",
        "author": "Christian Pionzewski, Rebecca Rademacher, J\u00e9r\u00f4me Rutinowski, Antonia Ponikarov, Stephan Matzke, Tim Chilla, Pia Schreynemackers, and Alice Kirchheim",
        "link": "http://arxiv.org/abs/2504.18286v1",
        "abstract": "This contribution explores the impact of synthetic training data usage and\nthe prediction of material wear and aging in the context of re-identification.\nDifferent experimental setups and gallery set expanding strategies are tested,\nanalyzing their impact on performance over time for aging re-identification\nsubjects. Using a continuously updating gallery, we were able to increase our\nmean Rank-1 accuracy by 24%, as material aging was taken into account step by\nstep. In addition, using models trained with 10% artificial training data,\nRank-1 accuracy could be increased by up to 13%, in comparison to a model\ntrained on only real-world data, significantly boosting generalized performance\non hold-out data. Finally, this work introduces a novel, open-source\nre-identification dataset, pallet-block-2696. This dataset contains 2,696\nimages of Euro pallets, taken over a period of 4 months. During this time,\nnatural aging processes occurred and some of the pallets were damaged during\ntheir usage. These wear and tear processes significantly changed the appearance\nof the pallets, providing a dataset that can be used to generate synthetically\naged pallets or other wooden materials."
    },
    {
        "date": "2025-04",
        "title": "Tree Boosting Methods for Balanced andImbalanced Classification and their Robustness Over Time in Risk Assessment",
        "author": "Gissel Velarde, Michael Weichert, Anuj Deshmunkh, Sanjay Deshmane, Anindya Sudhir, Khushboo Sharma, and Vaibhav Joshi",
        "link": "http://arxiv.org/abs/2504.18133v1",
        "abstract": "Most real-world classification problems deal with imbalanced datasets, posing\na challenge for Artificial Intelligence (AI), i.e., machine learning\nalgorithms, because the minority class, which is of extreme interest, often\nproves difficult to be detected. This paper empirically evaluates tree boosting\nmethods' performance given different dataset sizes and class distributions,\nfrom perfectly balanced to highly imbalanced. For tabular data, tree-based\nmethods such as XGBoost, stand out in several benchmarks due to detection\nperformance and speed. Therefore, XGBoost and Imbalance-XGBoost are evaluated.\nAfter introducing the motivation to address risk assessment with machine\nlearning, the paper reviews evaluation metrics for detection systems or binary\nclassifiers. It proposes a method for data preparation followed by tree\nboosting methods including hyper-parameter optimization. The method is\nevaluated on private datasets of 1 thousand (K), 10K and 100K samples on\ndistributions with 50, 45, 25, and 5 percent positive samples. As expected, the\ndeveloped method increases its recognition performance as more data is given\nfor training and the F1 score decreases as the data distribution becomes more\nimbalanced, but it is still significantly superior to the baseline of\nprecision-recall determined by the ratio of positives divided by positives and\nnegatives. Sampling to balance the training set does not provide consistent\nimprovement and deteriorates detection. In contrast, classifier hyper-parameter\noptimization improves recognition, but should be applied carefully depending on\ndata volume and distribution. Finally, the developed method is robust to data\nvariation over time up to some point. Retraining can be used when performance\nstarts deteriorating."
    },
    {
        "date": "2025-04",
        "title": "Anti-adversarial Learning: Desensitizing Prompts for Large Language Models",
        "author": "Xuan Li, Zhe Yin, Xiaodong Gu, and Beijun Shen",
        "link": "http://arxiv.org/abs/2505.01273v1",
        "abstract": "With the widespread use of LLMs, preserving privacy in user prompts has\nbecome crucial, as prompts risk exposing privacy and sensitive data to the\ncloud LLMs. Traditional techniques like homomorphic encryption, secure\nmulti-party computation, and federated learning face challenges due to heavy\ncomputational costs and user participation requirements, limiting their\napplicability in LLM scenarios. In this paper, we propose PromptObfus, a novel\nmethod for desensitizing LLM prompts. The core idea of PromptObfus is\n\"anti-adversarial\" learning, which perturbs privacy words in the prompt to\nobscure sensitive information while retaining the stability of model\npredictions. Specifically, PromptObfus frames prompt desensitization as a\nmasked language modeling task, replacing privacy-sensitive terms with a [MASK]\ntoken. A desensitization model is trained to generate candidate replacements\nfor each masked position. These candidates are subsequently selected based on\ngradient feedback from a surrogate model, ensuring minimal disruption to the\ntask output. We demonstrate the effectiveness of our approach on three NLP\ntasks. Results show that PromptObfus effectively prevents privacy inference\nfrom remote LLMs while preserving task performance."
    },
    {
        "date": "2025-04",
        "title": "Automating Function-Level TARA for Automotive Full-Lifecycle Security",
        "author": "Yuqiao Yang, Yongzhao Zhang, Wenhao Liu, Jun Li, Pengtao Shi, DingYu Zhong, Jie Yang, Ting Chen, Sheng Cao, Yuntao Ren, Yongyue Wu, and Xiaosong Zhang",
        "link": "http://arxiv.org/abs/2504.18083v1",
        "abstract": "As modern vehicles evolve into intelligent and connected systems, their\ngrowing complexity introduces significant cybersecurity risks. Threat Analysis\nand Risk Assessment (TARA) has therefore become essential for managing these\nrisks under mandatory regulations. However, existing TARA automation methods\nrely on static threat libraries, limiting their utility in the detailed,\nfunction-level analyses demanded by industry. This paper introduces\nDefenseWeaver, the first system that automates function-level TARA using\ncomponent-specific details and large language models (LLMs). DefenseWeaver\ndynamically generates attack trees and risk evaluations from system\nconfigurations described in an extended OpenXSAM++ format, then employs a\nmulti-agent framework to coordinate specialized LLM roles for more robust\nanalysis. To further adapt to evolving threats and diverse standards,\nDefenseWeaver incorporates Low-Rank Adaptation (LoRA) fine-tuning and\nRetrieval-Augmented Generation (RAG) with expert-curated TARA reports. We\nvalidated DefenseWeaver through deployment in four automotive security\nprojects, where it identified 11 critical attack paths, verified through\npenetration testing, and subsequently reported and remediated by the relevant\nautomakers and suppliers. Additionally, DefenseWeaver demonstrated cross-domain\nadaptability, successfully applying to unmanned aerial vehicles (UAVs) and\nmarine navigation systems. In comparison to human experts, DefenseWeaver\noutperformed manual attack tree generation across six assessment scenarios.\nIntegrated into commercial cybersecurity platforms such as UAES and Xiaomi,\nDefenseWeaver has generated over 8,200 attack trees. These results highlight\nits ability to significantly reduce processing time, and its scalability and\ntransformative impact on cybersecurity across industries."
    },
    {
        "date": "2025-04",
        "title": "Edge-Based Learning for Improved Classification Under Adversarial Noise",
        "author": "Manish Kansana, Keyan Alexander Rahimi, Elias Hossain, Iman Dehzangi, and Noorbakhsh Amiri Golilarz",
        "link": "http://arxiv.org/abs/2504.20077v1",
        "abstract": "Adversarial noise introduces small perturbations in images, misleading deep\nlearning models into misclassification and significantly impacting recognition\naccuracy. In this study, we analyzed the effects of Fast Gradient Sign Method\n(FGSM) adversarial noise on image classification and investigated whether\ntraining on specific image features can improve robustness. We hypothesize that\nwhile adversarial noise perturbs various regions of an image, edges may remain\nrelatively stable and provide essential structural information for\nclassification. To test this, we conducted a series of experiments using brain\ntumor and COVID datasets. Initially, we trained the models on clean images and\nthen introduced subtle adversarial perturbations, which caused deep learning\nmodels to significantly misclassify the images. Retraining on a combination of\nclean and noisy images led to improved performance. To evaluate the robustness\nof the edge features, we extracted edges from the original/clean images and\ntrained the models exclusively on edge-based representations. When noise was\nintroduced to the images, the edge-based models demonstrated greater resilience\nto adversarial attacks compared to those trained on the original or clean\nimages. These results suggest that while adversarial noise is able to exploit\ncomplex non-edge regions significantly more than edges, the improvement in the\naccuracy after retraining is marginally more in the original data as compared\nto the edges. Thus, leveraging edge-based learning can improve the resilience\nof deep learning models against adversarial perturbations."
    },
    {
        "date": "2025-04",
        "title": "Diffusion-Driven Universal Model Inversion Attack for Face Recognition",
        "author": "Hanrui Wang, Shuo Wang, Chun-Shien Lu, and Isao Echizen",
        "link": "http://arxiv.org/abs/2504.18015v1",
        "abstract": "Facial recognition technology poses significant privacy risks, as it relies\non biometric data that is inherently sensitive and immutable if compromised. To\nmitigate these concerns, face recognition systems convert raw images into\nembeddings, traditionally considered privacy-preserving. However, model\ninversion attacks pose a significant privacy threat by reconstructing these\nprivate facial images, making them a crucial tool for evaluating the privacy\nrisks of face recognition systems. Existing methods usually require training\nindividual generators for each target model, a computationally expensive\nprocess. In this paper, we propose DiffUMI, a training-free diffusion-driven\nuniversal model inversion attack for face recognition systems. DiffUMI is the\nfirst approach to apply a diffusion model for unconditional image generation in\nmodel inversion. Unlike other methods, DiffUMI is universal, eliminating the\nneed for training target-specific generators. It operates within a fixed\nframework and pretrained diffusion model while seamlessly adapting to diverse\ntarget identities and models. DiffUMI breaches privacy-preserving face\nrecognition systems with state-of-the-art success, demonstrating that an\nunconditional diffusion model, coupled with optimized adversarial search,\nenables efficient and high-fidelity facial reconstruction. Additionally, we\nintroduce a novel application of out-of-domain detection (OODD), marking the\nfirst use of model inversion to distinguish non-face inputs from face inputs\nbased solely on embeddings."
    },
    {
        "date": "2025-04",
        "title": "Cluster-Aware Attacks on Graph Watermarks",
        "author": "Alexander Nemecek, Emre Yilmaz, and Erman Ayday",
        "link": "http://arxiv.org/abs/2504.17971v1",
        "abstract": "Data from domains such as social networks, healthcare, finance, and\ncybersecurity can be represented as graph-structured information. Given the\nsensitive nature of this data and their frequent distribution among\ncollaborators, ensuring secure and attributable sharing is essential. Graph\nwatermarking enables attribution by embedding user-specific signatures into\ngraph-structured data. While prior work has addressed random perturbation\nattacks, the threat posed by adversaries leveraging structural properties\nthrough community detection remains unexplored. In this work, we introduce a\ncluster-aware threat model in which adversaries apply community-guided\nmodifications to evade detection. We propose two novel attack strategies and\nevaluate them on real-world social network graphs. Our results show that\ncluster-aware attacks can reduce attribution accuracy by up to 80% more than\nrandom baselines under equivalent perturbation budgets on sparse graphs. To\nmitigate this threat, we propose a lightweight embedding enhancement that\ndistributes watermark nodes across graph communities. This approach improves\nattribution accuracy by up to 60% under attack on dense graphs, without\nincreasing runtime or structural distortion. Our findings underscore the\nimportance of cluster-topological awareness in both watermarking design and\nadversarial modeling."
    },
    {
        "date": "2025-04",
        "title": "Secured Encryption scheme based on the Ree groups",
        "author": "Gennady Khalimov, and Yevgen Kotukh",
        "link": "http://arxiv.org/abs/2504.17919v1",
        "abstract": "An improved design of a cryptosystem based on small Ree groups is proposed.\nWe have changed the encryption algorithm and propose to use a logarithmic\nsignature for the entire Ree group. This approach improves security against\nsequential key recovery attacks. Hence, the complexity of the key recovery\nattack will be defined by a brute-force attack over the entire group. In this\npaper, we have proved that to construct secure cryptosystems with group\ncomputations over a small finite field, it is needed to use a 3-parametric\nsmall Ree group."
    },
    {
        "date": "2025-04",
        "title": "Biting the CHERI bullet: Blockers, Enablers and Security Implications of CHERI in Defence",
        "author": "Shamal Faily",
        "link": "http://arxiv.org/abs/2504.17904v1",
        "abstract": "There is growing interest in securing the hardware foundations software\nstacks build upon. However, before making any investment decision, software and\nhardware supply chain stakeholders require evidence from realistic, multiple\nlong-term studies of adoption. We present results from a 12 month evaluation of\none such secure hardware solution, CHERI, where 15 teams from industry and\nacademia ported software relevant to Defence to Arm's experimental Morello\nboard. We identified six types of blocker inhibiting adoption: dependencies, a\nknowledge premium, missing utilities, performance, platform instability, and\ntechnical debt. We also identified three types of enabler: tool assistance,\nimproved quality, and trivial code porting. Finally, we identified five types\nof potential vulnerability that CHERI could, if not appropriately configured,\nexpand a system's attack surface: state leaks, memory leaks, use after free\nvulnerabilities, unsafe defaults, and tool chain instability. Future work\nshould remove potentially insecure defaults from CHERI tooling, and develop a\nCHERI body of knowledge to further adoption."
    },
    {
        "date": "2025-04",
        "title": "DCT-Shield: A Robust Frequency Domain Defense against Malicious Image Editing",
        "author": "Aniruddha Bala, Rohit Chowdhury, Rohan Jaiswal, and Siddharth Roheda",
        "link": "http://arxiv.org/abs/2504.17894v1",
        "abstract": "Advancements in diffusion models have enabled effortless image editing via\ntext prompts, raising concerns about image security. Attackers with access to\nuser images can exploit these tools for malicious edits. Recent defenses\nattempt to protect images by adding a limited noise in the pixel space to\ndisrupt the functioning of diffusion-based editing models. However, the\nadversarial noise added by previous methods is easily noticeable to the human\neye. Moreover, most of these methods are not robust to purification techniques\nlike JPEG compression under a feasible pixel budget. We propose a novel\noptimization approach that introduces adversarial perturbations directly in the\nfrequency domain by modifying the Discrete Cosine Transform (DCT) coefficients\nof the input image. By leveraging the JPEG pipeline, our method generates\nadversarial images that effectively prevent malicious image editing. Extensive\nexperiments across a variety of tasks and datasets demonstrate that our\napproach introduces fewer visual artifacts while maintaining similar levels of\nedit protection and robustness to noise purification techniques."
    },
    {
        "date": "2025-04",
        "title": "Silenzio: Secure Non-Interactive Outsourced MLP Training",
        "author": "Jonas Sander, and Thomas Eisenbarth",
        "link": "http://arxiv.org/abs/2504.17785v1",
        "abstract": "Outsourcing the ML training to cloud providers presents a compelling\nopportunity for resource constrained clients, while it simultaneously bears\ninherent privacy risks, especially for highly sensitive training data. We\nintroduce Silenzio, the first fully non-interactive outsourcing scheme for the\ntraining of multi-layer perceptrons that achieves 128 bit security using FHE.\nUnlike traditional MPC based protocols that necessitate interactive\ncommunication between the client and server(s) or non-collusion assumptions\namong multiple servers, Silenzio enables the fire-and-forget paradigm without\nsuch assumptions. In this approach, the client encrypts the training data once,\nand the cloud server performs the training without any further interaction.\n  Silenzio operates over low bitwidth integers - never exceeding 8 bit - to\nmitigate the computational overhead of FHE. Our approach features a novel\nlow-bitwidth matrix multiplication that leverages input-dependent residue\nnumber systems and a Karatsuba-inspired multiplication routine, ensuring that\nno intermediate FHE-processed value overflows 8 bit. Starting from an\nRNS-to-MRNS conversion process, we propose an efficient block-scaling\nmechanism, which approximately shifts encrypted tensor values to the\nuser-specified most significant bits. To instantiate the backpropagation of the\nerror, Silenzio introduces a low-bitwidth and TFHE friendly gradient\ncomputation for the cross entropy loss.\n  Implemented using the state-of-the-art Concrete library, we evaluate Silenzio\non standard MLP training tasks regarding runtime as well as model performance\nand achieve similar classification accuracy as MLPs trained using standard\nPyTorch with 32 bit floating-point computations. Our open-source implementation\nrepresents a significant advancement in privacy-preserving ML, providing a new\nbaseline for secure and non-interactive outsourced MLP training."
    },
    {
        "date": "2025-04",
        "title": "CasualHDRSplat: Robust High Dynamic Range 3D Gaussian Splatting from Casually Captured Videos",
        "author": "Shucheng Gong, Lingzhe Zhao, Wenpu Li, Hong Xie, Yin Zhang, Shiyu Zhao, and Peidong Liu",
        "link": "http://arxiv.org/abs/2504.17728v1",
        "abstract": "Recently, photo-realistic novel view synthesis from multi-view images, such\nas neural radiance field (NeRF) and 3D Gaussian Splatting (3DGS), have garnered\nwidespread attention due to their superior performance. However, most works\nrely on low dynamic range (LDR) images, which limits the capturing of richer\nscene details. Some prior works have focused on high dynamic range (HDR) scene\nreconstruction, typically require capturing of multi-view sharp images with\ndifferent exposure times at fixed camera positions during exposure times, which\nis time-consuming and challenging in practice. For a more flexible data\nacquisition, we propose a one-stage method: \\textbf{CasualHDRSplat} to easily\nand robustly reconstruct the 3D HDR scene from casually captured videos with\nauto-exposure enabled, even in the presence of severe motion blur and varying\nunknown exposure time. \\textbf{CasualHDRSplat} contains a unified\ndifferentiable physical imaging model which first applies continuous-time\ntrajectory constraint to imaging process so that we can jointly optimize\nexposure time, camera response function (CRF), camera poses, and sharp 3D HDR\nscene. Extensive experiments demonstrate that our approach outperforms existing\nmethods in terms of robustness and rendering quality. Our source code will be\navailable at https://github.com/WU-CVGL/CasualHDRSplat"
    },
    {
        "date": "2025-04",
        "title": "Towards Robust LLMs: an Adversarial Robustness Measurement Framework",
        "author": "Natan Levy, Adiel Ashrov, and Guy Katz",
        "link": "http://arxiv.org/abs/2504.17723v1",
        "abstract": "The rise of Large Language Models (LLMs) has revolutionized artificial\nintelligence, yet these models remain vulnerable to adversarial perturbations,\nundermining their reliability in high-stakes applications. While adversarial\nrobustness in vision-based neural networks has been extensively studied, LLM\nrobustness remains under-explored. We adapt the Robustness Measurement and\nAssessment (RoMA) framework to quantify LLM resilience against adversarial\ninputs without requiring access to model parameters. By comparing RoMA's\nestimates to those of formal verification methods, we demonstrate its accuracy\nwith minimal error margins while maintaining computational efficiency. Our\nempirical evaluation reveals that robustness varies significantly not only\nbetween different models but also across categories within the same task and\nbetween various types of perturbations. This non-uniformity underscores the\nneed for task-specific robustness evaluations, enabling practitioners to\ncompare and select models based on application-specific robustness\nrequirements. Our work provides a systematic methodology to assess LLM\nrobustness, advancing the development of more reliable language models for\nreal-world deployment."
    },
    {
        "date": "2025-04",
        "title": "On the Generalization of Adversarially Trained Quantum Classifiers",
        "author": "Petros Georgiou, Aaron Mark Thomas, Sharu Theresa Jose, and Osvaldo Simeone",
        "link": "http://arxiv.org/abs/2504.17690v1",
        "abstract": "Quantum classifiers are vulnerable to adversarial attacks that manipulate\ntheir input classical or quantum data. A promising countermeasure is\nadversarial training, where quantum classifiers are trained by using an\nattack-aware, adversarial loss function. This work establishes novel bounds on\nthe generalization error of adversarially trained quantum classifiers when\ntested in the presence of perturbation-constrained adversaries. The bounds\nquantify the excess generalization error incurred to ensure robustness to\nadversarial attacks as scaling with the training sample size $m$ as\n$1/\\sqrt{m}$, while yielding insights into the impact of the quantum embedding.\nFor quantum binary classifiers employing \\textit{rotation embedding}, we find\nthat, in the presence of adversarial attacks on classical inputs $\\mathbf{x}$,\nthe increase in sample complexity due to adversarial training over conventional\ntraining vanishes in the limit of high dimensional inputs $\\mathbf{x}$. In\ncontrast, when the adversary can directly attack the quantum state\n$\\rho(\\mathbf{x})$ encoding the input $\\mathbf{x}$, the excess generalization\nerror depends on the choice of embedding only through its Hilbert space\ndimension. The results are also extended to multi-class classifiers. We\nvalidate our theoretical findings with numerical experiments."
    },
    {
        "date": "2025-04",
        "title": "Evaluating the Vulnerability of ML-Based Ethereum Phishing Detectors to Single-Feature Adversarial Perturbations",
        "author": "Ahod Alghuried, Ali Alkinoon, Abdulaziz Alghamdi, Soohyeon Choi, Manar Mohaisen, and David Mohaisen",
        "link": "http://arxiv.org/abs/2504.17684v1",
        "abstract": "This paper explores the vulnerability of machine learning models to simple\nsingle-feature adversarial attacks in the context of Ethereum fraudulent\ntransaction detection. Through comprehensive experimentation, we investigate\nthe impact of various adversarial attack strategies on model performance\nmetrics. Our findings, highlighting how prone those techniques are to simple\nattacks, are alarming, and the inconsistency in the attacks' effect on\ndifferent algorithms promises ways for attack mitigation. We examine the\neffectiveness of different mitigation strategies, including adversarial\ntraining and enhanced feature selection, in enhancing model robustness and show\ntheir effectiveness."
    },
    {
        "date": "2025-04",
        "title": "Enhancing CNNs robustness to occlusions with bioinspired filters for border completion",
        "author": "Catarina P. Coutinho, Aneeqa Merhab, Janko Petkovic, Ferdinando Zanchetta, and Rita Fioresi",
        "link": "http://arxiv.org/abs/2504.17619v1",
        "abstract": "We exploit the mathematical modeling of the visual cortex mechanism for\nborder completion to define custom filters for CNNs. We see a consistent\nimprovement in performance, particularly in accuracy, when our modified LeNet 5\nis tested with occluded MNIST images."
    },
    {
        "date": "2025-04",
        "title": "A Simple DropConnect Approach to Transfer-based Targeted Attack",
        "author": "Tongrui Su, Qingbin Li, Shengyu Zhu, Wei Chen, and Xueqi Cheng",
        "link": "http://arxiv.org/abs/2504.18594v1",
        "abstract": "We study the problem of transfer-based black-box attack, where adversarial\nsamples generated using a single surrogate model are directly applied to target\nmodels. Compared with untargeted attacks, existing methods still have lower\nAttack Success Rates (ASRs) in the targeted setting, i.e., the obtained\nadversarial examples often overfit the surrogate model but fail to mislead\nother models. In this paper, we hypothesize that the pixels or features in\nthese adversarial examples collaborate in a highly dependent manner to maximize\nthe success of an adversarial attack on the surrogate model, which we refer to\nas perturbation co-adaptation. Then, we propose to Mitigate perturbation\nCo-adaptation by DropConnect (MCD) to enhance transferability, by creating\ndiverse variants of surrogate model at each optimization iteration. We conduct\nextensive experiments across various CNN- and Transformer-based models to\ndemonstrate the effectiveness of MCD. In the challenging scenario of\ntransferring from a CNN-based model to Transformer-based models, MCD achieves\n13% higher average ASRs compared with state-of-the-art baselines. MCD boosts\nthe performance of self-ensemble methods by bringing in more diversification\nacross the variants while reserving sufficient semantic information for each\nvariant. In addition, MCD attains the highest performance gain when scaling the\ncompute of crafting adversarial examples."
    },
    {
        "date": "2025-04",
        "title": "Wolves in the Repository: A Software Engineering Analysis of the XZ Utils Supply Chain Attack",
        "author": "Piotr Przymus, and Thomas Durieux",
        "link": "http://arxiv.org/abs/2504.17473v1",
        "abstract": "The digital economy runs on Open Source Software (OSS), with an estimated\n90\\% of modern applications containing open-source components. While this\nwidespread adoption has revolutionized software development, it has also\ncreated critical security vulnerabilities, particularly in essential but\nunder-resourced projects. This paper examines a sophisticated attack on the XZ\nUtils project (CVE-2024-3094), where attackers exploited not just code, but the\nentire open-source development process to inject a backdoor into a fundamental\nLinux compression library. Our analysis reveals a new breed of supply chain\nattack that manipulates software engineering practices themselves -- from\ncommunity management to CI/CD configurations -- to establish legitimacy and\nmaintain long-term control. Through a comprehensive examination of GitHub\nevents and development artifacts, we reconstruct the attack timeline, analyze\nthe evolution of attacker tactics. Our findings demonstrate how attackers\nleveraged seemingly beneficial contributions to project infrastructure and\nmaintenance to bypass traditional security measures. This work extends beyond\ntraditional security analysis by examining how software engineering practices\nthemselves can be weaponized, offering insights for protecting the open-source\necosystem."
    },
    {
        "date": "2025-04",
        "title": "Unveiling Hidden Vulnerabilities in Digital Human Generation via Adversarial Attacks",
        "author": "Zhiying Li, Yeying Jin, Fan Shen, Zhi Liu, Weibin Chen, Pengju Zhang, Xiaomei Zhang, Boyu Chen, Michael Shen, Kejian Wu, Zhaoxin Fan, and Jin Dong",
        "link": "http://arxiv.org/abs/2504.17457v1",
        "abstract": "Expressive human pose and shape estimation (EHPS) is crucial for digital\nhuman generation, especially in applications like live streaming. While\nexisting research primarily focuses on reducing estimation errors, it largely\nneglects robustness and security aspects, leaving these systems vulnerable to\nadversarial attacks. To address this significant challenge, we propose the\n\\textbf{Tangible Attack (TBA)}, a novel framework designed to generate\nadversarial examples capable of effectively compromising any digital human\ngeneration model. Our approach introduces a \\textbf{Dual Heterogeneous Noise\nGenerator (DHNG)}, which leverages Variational Autoencoders (VAE) and\nControlNet to produce diverse, targeted noise tailored to the original image\nfeatures. Additionally, we design a custom \\textbf{adversarial loss function}\nto optimize the noise, ensuring both high controllability and potent\ndisruption. By iteratively refining the adversarial sample through\nmulti-gradient signals from both the noise and the state-of-the-art EHPS model,\nTBA substantially improves the effectiveness of adversarial attacks. Extensive\nexperiments demonstrate TBA's superiority, achieving a remarkable 41.0\\%\nincrease in estimation error, with an average improvement of approximately\n17.0\\%. These findings expose significant security vulnerabilities in current\nEHPS models and highlight the need for stronger defenses in digital human\ngeneration systems."
    },
    {
        "date": "2025-04",
        "title": "StereoMamba: Real-time and Robust Intraoperative Stereo Disparity Estimation via Long-range Spatial Dependencies",
        "author": "Xu Wang, Jialang Xu, Shuai Zhang, Baoru Huang, Danail Stoyanov, and Evangelos B. Mazomenos",
        "link": "http://arxiv.org/abs/2504.17401v1",
        "abstract": "Stereo disparity estimation is crucial for obtaining depth information in\nrobot-assisted minimally invasive surgery (RAMIS). While current deep learning\nmethods have made significant advancements, challenges remain in achieving an\noptimal balance between accuracy, robustness, and inference speed. To address\nthese challenges, we propose the StereoMamba architecture, which is\nspecifically designed for stereo disparity estimation in RAMIS. Our approach is\nbased on a novel Feature Extraction Mamba (FE-Mamba) module, which enhances\nlong-range spatial dependencies both within and across stereo images. To\neffectively integrate multi-scale features from FE-Mamba, we then introduce a\nnovel Multidimensional Feature Fusion (MFF) module. Experiments against the\nstate-of-the-art on the ex-vivo SCARED benchmark demonstrate that StereoMamba\nachieves superior performance on EPE of 2.64 px and depth MAE of 2.55 mm, the\nsecond-best performance on Bad2 of 41.49% and Bad3 of 26.99%, while maintaining\nan inference speed of 21.28 FPS for a pair of high-resolution images\n(1280*1024), striking the optimum balance between accuracy, robustness, and\nefficiency. Furthermore, by comparing synthesized right images, generated from\nwarping left images using the generated disparity maps, with the actual right\nimage, StereoMamba achieves the best average SSIM (0.8970) and PSNR (16.0761),\nexhibiting strong zero-shot generalization on the in-vivo RIS2017 and StereoMIS\ndatasets."
    },
    {
        "date": "2025-04",
        "title": "Fine-Tuning Adversarially-Robust Transformers for Single-Image Dehazing",
        "author": "Vlad Vasilescu, Ana Neacsu, and Daniela Faur",
        "link": "http://arxiv.org/abs/2504.17829v1",
        "abstract": "Single-image dehazing is an important topic in remote sensing applications,\nenhancing the quality of acquired images and increasing object detection\nprecision. However, the reliability of such structures has not been\nsufficiently analyzed, which poses them to the risk of imperceptible\nperturbations that can significantly hinder their performance. In this work, we\nshow that state-of-the-art image-to-image dehazing transformers are susceptible\nto adversarial noise, with even 1 pixel change being able to decrease the PSNR\nby as much as 2.8 dB. Next, we propose two lightweight fine-tuning strategies\naimed at increasing the robustness of pre-trained transformers. Our methods\nresults in comparable clean performance, while significantly increasing the\nprotection against adversarial data. We further present their applicability in\ntwo remote sensing scenarios, showcasing their robust behavior for\nout-of-distribution data. The source code for adversarial fine-tuning and\nattack algorithms can be found at github.com/Vladimirescu/RobustDehazing."
    },
    {
        "date": "2025-04",
        "title": "Class-Conditional Distribution Balancing for Group Robust Classification",
        "author": "Miaoyun Zhao, Qiang Zhang, and Chenrong Li",
        "link": "http://arxiv.org/abs/2504.17314v2",
        "abstract": "Spurious correlations that lead models to correct predictions for the wrong\nreasons pose a critical challenge for robust real-world generalization.\nExisting research attributes this issue to group imbalance and addresses it by\nmaximizing group-balanced or worst-group accuracy, which heavily relies on\nexpensive bias annotations. A compromise approach involves predicting bias\ninformation using extensively pretrained foundation models, which requires\nlarge-scale data and becomes impractical for resource-limited rare domains. To\naddress these challenges, we offer a novel perspective by reframing the\nspurious correlations as imbalances or mismatches in class-conditional\ndistributions, and propose a simple yet effective robust learning method that\neliminates the need for both bias annotations and predictions. With the goal of\nreducing the mutual information between spurious factors and label information,\nour method leverages a sample reweighting strategy to achieve class-conditional\ndistribution balancing, which automatically highlights minority groups and\nclasses, effectively dismantling spurious correlations and producing a debiased\ndata distribution for classification. Extensive experiments and analysis\ndemonstrate that our approach consistently delivers state-of-the-art\nperformance, rivaling methods that rely on bias supervision."
    },
    {
        "date": "2025-04",
        "title": "FLUKE: A Linguistically-Driven and Task-Agnostic Framework for Robustness Evaluation",
        "author": "Yulia Otmakhova, Hung Thinh Truong, Rahmad Mahendra, Zenan Zhai, Rongxin Zhu, Daniel Beck, and Jey Han Lau",
        "link": "http://arxiv.org/abs/2504.17311v1",
        "abstract": "We present FLUKE (Framework for LingUistically-driven and tasK-agnostic\nrobustness Evaluation), a task-agnostic framework for assessing model\nrobustness through systematic minimal variations of test data. FLUKE introduces\ncontrolled variations across linguistic levels - from orthography to dialect\nand style varieties - and leverages large language models (LLMs) with human\nvalidation to generate modifications. We demonstrate FLUKE's utility by\nevaluating both fine-tuned models and LLMs across four diverse NLP tasks, and\nreveal that (1) the impact of linguistic variations is highly task-dependent,\nwith some tests being critical for certain tasks but irrelevant for others; (2)\nwhile LLMs have better overall robustness compared to fine-tuned models, they\nstill exhibit significant brittleness to certain linguistic variations; (3) all\nmodels show substantial vulnerability to negation modifications across most\ntasks. These findings highlight the importance of systematic robustness testing\nfor understanding model behaviors."
    },
    {
        "date": "2025-04",
        "title": "Enhancing Variational Autoencoders with Smooth Robust Latent Encoding",
        "author": "Hyomin Lee, Minseon Kim, Sangwon Jang, Jongheon Jeong, and Sung Ju Hwang",
        "link": "http://arxiv.org/abs/2504.17219v1",
        "abstract": "Variational Autoencoders (VAEs) have played a key role in scaling up\ndiffusion-based generative models, as in Stable Diffusion, yet questions\nregarding their robustness remain largely underexplored. Although adversarial\ntraining has been an established technique for enhancing robustness in\npredictive models, it has been overlooked for generative models due to concerns\nabout potential fidelity degradation by the nature of trade-offs between\nperformance and robustness. In this work, we challenge this presumption,\nintroducing Smooth Robust Latent VAE (SRL-VAE), a novel adversarial training\nframework that boosts both generation quality and robustness. In contrast to\nconventional adversarial training, which focuses on robustness only, our\napproach smooths the latent space via adversarial perturbations, promoting more\ngeneralizable representations while regularizing with originality\nrepresentation to sustain original fidelity. Applied as a post-training step on\npre-trained VAEs, SRL-VAE improves image robustness and fidelity with minimal\ncomputational overhead. Experiments show that SRL-VAE improves both generation\nquality, in image reconstruction and text-guided image editing, and robustness,\nagainst Nightshade attacks and image editing attacks. These results establish a\nnew paradigm, showing that adversarial training, once thought to be detrimental\nto generative models, can instead enhance both fidelity and robustness."
    },
    {
        "date": "2025-04",
        "title": "Developing a Blockchain-Based Secure Digital Contents Distribution System",
        "author": "Syed Mohiuddin Qadri, and Sangwhan Cha",
        "link": "http://arxiv.org/abs/2504.17194v1",
        "abstract": "As digital content distribution expands rapidly through online platforms,\nsecuring digital media and protecting intellectual property has become\nincreasingly complex. Traditional centralized systems, while widely adopted,\nsuffer from vulnerabilities such as single points of failure and limited\ntraceability of unauthorized access. This paper presents a blockchain-based\nsecure digital content distribution system that integrates Sia, a decentralized\nstorage network, and Skynet, a content delivery network, to enhance content\nprotection and distribution. The proposed system employs a dual-layer\narchitecture: off-chain for user authentication and on-chain for transaction\nvalidation using smart contracts and asymmetric encryption. By introducing a\nlicense issuance and secret block mechanism, the system ensures content\nauthenticity, privacy, and controlled access. Experimental results demonstrate\nthe feasibility and scalability of the system in securely distributing\nmultimedia files. The proposed platform not only improves content security but\nalso paves the way for future enhancements with decentralized applications and\nintegrated royalty payment mechanisms."
    },
    {
        "date": "2025-04",
        "title": "AUTHENTICATION: Identifying Rare Failure Modes in Autonomous Vehicle Perception Systems using Adversarially Guided Diffusion Models",
        "author": "Mohammad Zarei, Melanie A Jutras, Eliana Evans, Mike Tan, and Omid Aaramoon",
        "link": "http://arxiv.org/abs/2504.17179v1",
        "abstract": "Autonomous Vehicles (AVs) rely on artificial intelligence (AI) to accurately\ndetect objects and interpret their surroundings. However, even when trained\nusing millions of miles of real-world data, AVs are often unable to detect rare\nfailure modes (RFMs). The problem of RFMs is commonly referred to as the\n\"long-tail challenge\", due to the distribution of data including many instances\nthat are very rarely seen. In this paper, we present a novel approach that\nutilizes advanced generative and explainable AI techniques to aid in\nunderstanding RFMs. Our methods can be used to enhance the robustness and\nreliability of AVs when combined with both downstream model training and\ntesting. We extract segmentation masks for objects of interest (e.g., cars) and\ninvert them to create environmental masks. These masks, combined with carefully\ncrafted text prompts, are fed into a custom diffusion model. We leverage the\nStable Diffusion inpainting model guided by adversarial noise optimization to\ngenerate images containing diverse environments designed to evade object\ndetection models and expose vulnerabilities in AI systems. Finally, we produce\nnatural language descriptions of the generated RFMs that can guide developers\nand policymakers to improve the safety and reliability of AV systems."
    },
    {
        "date": "2025-04",
        "title": "Firewall Regulatory Networks for Autonomous Cyber Defense",
        "author": "Qi Duan, and Ehab Al-Shaer",
        "link": "http://arxiv.org/abs/2505.01436v1",
        "abstract": "In this paper, we present the principles of designing new self-organising and\nautonomous management protocol to govern the dynamics of bio-inspired\ndecentralized firewall architecture based on Biological Regularity Networks.\n  The new architecture called Firewall Regulatory Networks (FRN) exhibits the\nfollowing features (1) automatic rule policy configuration with provable\nutility-risk appetite guarantee, (2) resilient response for changing risks or\nnew service requirements, and (3) globally optimized access control policy\nreconciliation. We present the FRN protocol and formalize the constraints to\nsynthesize the undetermined components in the protocol to produce interactions\nthat can achieve these objectives. We illustrate the feasibility of the FRN\narchitecture in multiple case studies."
    },
    {
        "date": "2025-04",
        "title": "Robo-Troj: Attacking LLM-based Task Planners",
        "author": "Mohaiminul Al Nahian, Zainab Altaweel, David Reitano, Sabbir Ahmed, Saumitra Lohokare, Shiqi Zhang, and Adnan Siraj Rakin",
        "link": "http://arxiv.org/abs/2504.17070v1",
        "abstract": "Robots need task planning methods to achieve goals that require more than\nindividual actions. Recently, large language models (LLMs) have demonstrated\nimpressive performance in task planning. LLMs can generate a step-by-step\nsolution using a description of actions and the goal. Despite the successes in\nLLM-based task planning, there is limited research studying the security\naspects of those systems. In this paper, we develop Robo-Troj, the first\nmulti-trigger backdoor attack for LLM-based task planners, which is the main\ncontribution of this work. As a multi-trigger attack, Robo-Troj is trained to\naccommodate the diversity of robot application domains. For instance, one can\nuse unique trigger words, e.g., \"herical\", to activate a specific malicious\nbehavior, e.g., cutting hand on a kitchen robot. In addition, we develop an\noptimization method for selecting the trigger words that are most effective.\nThrough demonstrating the vulnerability of LLM-based planners, we aim to\npromote the development of secured robot systems."
    },
    {
        "date": "2025-04",
        "title": "Statistical Guarantees in Synthetic Data through Conformal Adversarial Generation",
        "author": "Rahul Vishwakarma, Shrey Dharmendra Modi, and Vishwanath Seshagiri",
        "link": "http://arxiv.org/abs/2504.17058v2",
        "abstract": "The generation of high-quality synthetic data presents significant challenges\nin machine learning research, particularly regarding statistical fidelity and\nuncertainty quantification. Existing generative models produce compelling\nsynthetic samples but lack rigorous statistical guarantees about their relation\nto the underlying data distribution, limiting their applicability in critical\ndomains requiring robust error bounds. We address this fundamental limitation\nby presenting a novel framework that incorporates conformal prediction\nmethodologies into Generative Adversarial Networks (GANs). By integrating\nmultiple conformal prediction paradigms including Inductive Conformal\nPrediction (ICP), Mondrian Conformal Prediction, Cross-Conformal Prediction,\nand Venn-Abers Predictors, we establish distribution-free uncertainty\nquantification in generated samples. This approach, termed Conformalized GAN\n(cGAN), demonstrates enhanced calibration properties while maintaining the\ngenerative power of traditional GANs, producing synthetic data with provable\nstatistical guarantees. We provide rigorous mathematical proofs establishing\nfinite-sample validity guarantees and asymptotic efficiency properties,\nenabling the reliable application of synthetic data in high-stakes domains\nincluding healthcare, finance, and autonomous systems."
    },
    {
        "date": "2025-04",
        "title": "BadVideo: Stealthy Backdoor Attack against Text-to-Video Generation",
        "author": "Ruotong Wang, Mingli Zhu, Jiarong Ou, Rui Chen, Xin Tao, Pengfei Wan, and Baoyuan Wu",
        "link": "http://arxiv.org/abs/2504.16907v1",
        "abstract": "Text-to-video (T2V) generative models have rapidly advanced and found\nwidespread applications across fields like entertainment, education, and\nmarketing. However, the adversarial vulnerabilities of these models remain\nrarely explored. We observe that in T2V generation tasks, the generated videos\noften contain substantial redundant information not explicitly specified in the\ntext prompts, such as environmental elements, secondary objects, and additional\ndetails, providing opportunities for malicious attackers to embed hidden\nharmful content. Exploiting this inherent redundancy, we introduce BadVideo,\nthe first backdoor attack framework tailored for T2V generation. Our attack\nfocuses on designing target adversarial outputs through two key strategies: (1)\nSpatio-Temporal Composition, which combines different spatiotemporal features\nto encode malicious information; (2) Dynamic Element Transformation, which\nintroduces transformations in redundant elements over time to convey malicious\ninformation. Based on these strategies, the attacker's malicious target\nseamlessly integrates with the user's textual instructions, providing high\nstealthiness. Moreover, by exploiting the temporal dimension of videos, our\nattack successfully evades traditional content moderation systems that\nprimarily analyze spatial information within individual frames. Extensive\nexperiments demonstrate that BadVideo achieves high attack success rates while\npreserving original semantics and maintaining excellent performance on clean\ninputs. Overall, our work reveals the adversarial vulnerability of T2V models,\ncalling attention to potential risks and misuse. Our project page is at\nhttps://wrt2000.github.io/BadVideo2025/."
    },
    {
        "date": "2025-04",
        "title": "Building A Secure Agentic AI Application Leveraging A2A Protocol",
        "author": "Idan Habler, Ken Huang, Vineeth Sai Narajala, and Prashant Kulkarni",
        "link": "http://arxiv.org/abs/2504.16902v2",
        "abstract": "As Agentic AI systems evolve from basic workflows to complex multi agent\ncollaboration, robust protocols such as Google's Agent2Agent (A2A) become\nessential enablers. To foster secure adoption and ensure the reliability of\nthese complex interactions, understanding the secure implementation of A2A is\nessential. This paper addresses this goal by providing a comprehensive security\nanalysis centered on the A2A protocol. We examine its fundamental elements and\noperational dynamics, situating it within the framework of agent communication\ndevelopment. Utilizing the MAESTRO framework, specifically designed for AI\nrisks, we apply proactive threat modeling to assess potential security issues\nin A2A deployments, focusing on aspects such as Agent Card management, task\nexecution integrity, and authentication methodologies.\n  Based on these insights, we recommend practical secure development\nmethodologies and architectural best practices designed to build resilient and\neffective A2A systems. Our analysis also explores how the synergy between A2A\nand the Model Context Protocol (MCP) can further enhance secure\ninteroperability. This paper equips developers and architects with the\nknowledge and practical guidance needed to confidently leverage the A2A\nprotocol for building robust and secure next generation agentic applications."
    },
    {
        "date": "2025-04",
        "title": "Simplified Swarm Learning Framework for Robust and Scalable Diagnostic Services in Cancer Histopathology",
        "author": "Yanjie Wu, Yuhao Ji, Saiho Lee, Juniad Akram, Ali Braytee, and Ali Anaissi",
        "link": "http://arxiv.org/abs/2504.16732v1",
        "abstract": "The complexities of healthcare data, including privacy concerns, imbalanced\ndatasets, and interoperability issues, necessitate innovative machine learning\nsolutions. Swarm Learning (SL), a decentralized alternative to Federated\nLearning, offers privacy-preserving distributed training, but its reliance on\nblockchain technology hinders accessibility and scalability. This paper\nintroduces a \\textit{Simplified Peer-to-Peer Swarm Learning (P2P-SL) Framework}\ntailored for resource-constrained environments. By eliminating blockchain\ndependencies and adopting lightweight peer-to-peer communication, the proposed\nframework ensures robust model synchronization while maintaining data privacy.\nApplied to cancer histopathology, the framework integrates optimized\npre-trained models, such as TorchXRayVision, enhanced with DenseNet decoders,\nto improve diagnostic accuracy. Extensive experiments demonstrate the\nframework's efficacy in handling imbalanced and biased datasets, achieving\ncomparable performance to centralized models while preserving privacy. This\nstudy paves the way for democratizing advanced machine learning in healthcare,\noffering a scalable, accessible, and efficient solution for privacy-sensitive\ndiagnostic applications."
    },
    {
        "date": "2025-04",
        "title": "V$^2$R-Bench: Holistically Evaluating LVLM Robustness to Fundamental Visual Variations",
        "author": "Zhiyuan Fan, Yumeng Wang, Sandeep Polisetty, and Yi R. Fung",
        "link": "http://arxiv.org/abs/2504.16727v2",
        "abstract": "Large Vision Language Models (LVLMs) excel in various vision-language tasks.\nYet, their robustness to visual variations in position, scale, orientation, and\ncontext that objects in natural scenes inevitably exhibit due to changes in\nviewpoint and environment remains largely underexplored. To bridge this gap, we\nintroduce V$^2$R-Bench, a comprehensive benchmark framework for evaluating\nVisual Variation Robustness of LVLMs, which encompasses automated evaluation\ndataset generation and principled metrics for thorough robustness assessment.\nThrough extensive evaluation on 21 LVLMs, we reveal a surprising vulnerability\nto visual variations, in which even advanced models that excel at complex\nvision-language tasks significantly underperform on simple tasks such as object\nrecognition. Interestingly, these models exhibit a distinct visual position\nbias that contradicts theories of effective receptive fields, and demonstrate a\nhuman-like visual acuity threshold. To identify the source of these\nvulnerabilities, we present a systematic framework for component-level\nanalysis, featuring a novel visualization approach for aligned visual features.\nResults show that these vulnerabilities stem from error accumulation in the\npipeline architecture and inadequate multimodal alignment. Complementary\nexperiments with synthetic data further demonstrate that these limitations are\nfundamentally architectural deficiencies, scoring the need for architectural\ninnovations in future LVLM designs."
    },
    {
        "date": "2025-04",
        "title": "MCMC for Bayesian estimation of Differential Privacy from Membership Inference Attacks",
        "author": "Ceren Yildirim, Kamer Kaya, Sinan Yildirim, and Erkay Savas",
        "link": "http://arxiv.org/abs/2504.16683v1",
        "abstract": "We propose a new framework for Bayesian estimation of differential privacy,\nincorporating evidence from multiple membership inference attacks (MIA).\nBayesian estimation is carried out via a Markov chain Monte Carlo (MCMC)\nalgorithm, named MCMC-DP-Est, which provides an estimate of the full posterior\ndistribution of the privacy parameter (e.g., instead of just credible\nintervals). Critically, the proposed method does not assume that privacy\nauditing is performed with the most powerful attack on the worst-case (dataset,\nchallenge point) pair, which is typically unrealistic. Instead, MCMC-DP-Est\njointly estimates the strengths of MIAs used and the privacy of the training\nalgorithm, yielding a more cautious privacy analysis. We also present an\neconomical way to generate measurements for the performance of an MIA that is\nto be used by the MCMC method to estimate privacy. We present the use of the\nmethods with numerical examples with both artificial and real data."
    },
    {
        "date": "2025-04",
        "title": "Security Science (SecSci), Basic Concepts and Mathematical Foundations",
        "author": "Dusko Pavlovic, and Peter-Michael Seidel",
        "link": "http://arxiv.org/abs/2504.16617v1",
        "abstract": "This textbook compiles the lecture notes from security courses taught at\nOxford in the 2000s, at Royal Holloway in the 2010s, and currently in Hawaii.\nThe early chapters are suitable for a first course in security. The middle\nchapters have been used in advanced courses. Towards the end there are also\nsome research problems."
    },
    {
        "date": "2025-04",
        "title": "LaSDVS : A Post-Quantum Secure Compact Strong-Designated Verifier Signature",
        "author": "Shanu Poddar, Sweta Mishra, Tapaswini Mohanty, Vikas Srivastava, and Sugata Gangopadhyay",
        "link": "http://arxiv.org/abs/2504.16571v1",
        "abstract": "Digital signatures are fundamental cryptographic primitives that ensure the\nauthenticity and integrity of digital communication. However, in scenarios\ninvolving sensitive interactions -- such as e-voting or e-cash -- there is a\ngrowing need for more controlled signing mechanisms. Strong-Designated Verifier\nSignature (SDVS) offers such control by allowing the signer to specify and\nrestrict the verifier of a signature. The existing state-of-the-art SDVS are\nmostly based on number-theoretic hardness assumptions. Thus, they are not\nsecure against quantum attacks. Moreover, Post-Quantum Cryptography (PQC)-based\nSDVS are inefficient and have large key and signature sizes. In this work, we\naddress these challenges and propose an efficient post-quantum SDVS (namely,\nLaSDVS) based on ideal lattices under the hardness assumptions of the Ring-SIS\nand Ring-LWE problems. LaSDVS achieves advanced security properties including\nstrong unforgeability under chosen-message attacks, non-transferability,\nnon-delegatability, and signer anonymity. By employing the algebraic structure\nof rings and the gadget trapdoor mechanism of Micciancio et al., we design\nLaSDVS to minimize computational overhead and significantly reduce key and\nsignature sizes. Notably, our scheme achieves a compact signature size of\n$\\mathcal{O}(n\\log q)$, compared to $\\mathcal{O}(n^2)$ size, where $n$ is the\nsecurity parameter, in the existing state-of-the-art PQC designs. To the best\nof our knowledge, LaSDVS offers the \\textit{smallest private key and signature\nsize} among the existing PQC-based SDVS schemes."
    },
    {
        "date": "2025-04",
        "title": "Amplified Vulnerabilities: Structured Jailbreak Attacks on LLM-based Multi-Agent Debate",
        "author": "Senmao Qi, Yifei Zou, Peng Li, Ziyi Lin, Xiuzhen Cheng, and Dongxiao Yu",
        "link": "http://arxiv.org/abs/2504.16489v1",
        "abstract": "Multi-Agent Debate (MAD), leveraging collaborative interactions among Large\nLanguage Models (LLMs), aim to enhance reasoning capabilities in complex tasks.\nHowever, the security implications of their iterative dialogues and\nrole-playing characteristics, particularly susceptibility to jailbreak attacks\neliciting harmful content, remain critically underexplored. This paper\nsystematically investigates the jailbreak vulnerabilities of four prominent MAD\nframeworks built upon leading commercial LLMs (GPT-4o, GPT-4, GPT-3.5-turbo,\nand DeepSeek) without compromising internal agents. We introduce a novel\nstructured prompt-rewriting framework specifically designed to exploit MAD\ndynamics via narrative encapsulation, role-driven escalation, iterative\nrefinement, and rhetorical obfuscation. Our extensive experiments demonstrate\nthat MAD systems are inherently more vulnerable than single-agent setups.\nCrucially, our proposed attack methodology significantly amplifies this\nfragility, increasing average harmfulness from 28.14% to 80.34% and achieving\nattack success rates as high as 80% in certain scenarios. These findings reveal\nintrinsic vulnerabilities in MAD architectures and underscore the urgent need\nfor robust, specialized defenses prior to real-world deployment."
    },
    {
        "date": "2025-04",
        "title": "Seeking Flat Minima over Diverse Surrogates for Improved Adversarial Transferability: A Theoretical Framework and Algorithmic Instantiation",
        "author": "Meixi Zheng, Kehan Wu, Yanbo Fan, Rui Huang, and Baoyuan Wu",
        "link": "http://arxiv.org/abs/2504.16474v1",
        "abstract": "The transfer-based black-box adversarial attack setting poses the challenge\nof crafting an adversarial example (AE) on known surrogate models that remain\neffective against unseen target models. Due to the practical importance of this\ntask, numerous methods have been proposed to address this challenge. However,\nmost previous methods are heuristically designed and intuitively justified,\nlacking a theoretical foundation. To bridge this gap, we derive a novel\ntransferability bound that offers provable guarantees for adversarial\ntransferability. Our theoretical analysis has the advantages of \\textit{(i)}\ndeepening our understanding of previous methods by building a general attack\nframework and \\textit{(ii)} providing guidance for designing an effective\nattack algorithm. Our theoretical results demonstrate that optimizing AEs\ntoward flat minima over the surrogate model set, while controlling the\nsurrogate-target model shift measured by the adversarial model discrepancy,\nyields a comprehensive guarantee for AE transferability. The results further\nlead to a general transfer-based attack framework, within which we observe that\nprevious methods consider only partial factors contributing to the\ntransferability. Algorithmically, inspired by our theoretical results, we first\nelaborately construct the surrogate model set in which models exhibit diverse\nadversarial vulnerabilities with respect to AEs to narrow an instantiated\nadversarial model discrepancy. Then, a \\textit{model-Diversity-compatible\nReverse Adversarial Perturbation} (DRAP) is generated to effectively promote\nthe flatness of AEs over diverse surrogate models to improve transferability.\nExtensive experiments on NIPS2017 and CIFAR-10 datasets against various target\nmodels demonstrate the effectiveness of our proposed attack."
    },
    {
        "date": "2025-04",
        "title": "MTSGL: Multi-Task Structure Guided Learning for Robust and Interpretable SAR Aircraft Recognition",
        "author": "Qishan He, Lingjun Zhao, Ru Luo, Siqian Zhang, Lin Lei, Kefeng Ji, and Gangyao Kuang",
        "link": "http://arxiv.org/abs/2504.16467v1",
        "abstract": "Aircraft recognition in synthetic aperture radar (SAR) imagery is a\nfundamental mission in both military and civilian applications. Recently deep\nlearning (DL) has emerged a dominant paradigm for its explosive performance on\nextracting discriminative features. However, current classification algorithms\nfocus primarily on learning decision hyperplane without enough comprehension on\naircraft structural knowledge. Inspired by the fined aircraft annotation\nmethods for optical remote sensing images (RSI), we first introduce a\nstructure-based SAR aircraft annotations approach to provide structural and\ncompositional supplement information. On this basis, we propose a multi-task\nstructure guided learning (MTSGL) network for robust and interpretable SAR\naircraft recognition. Besides the classification task, MTSGL includes a\nstructural semantic awareness (SSA) module and a structural consistency\nregularization (SCR) module. The SSA is designed to capture structure semantic\ninformation, which is conducive to gain human-like comprehension of aircraft\nknowledge. The SCR helps maintain the geometric consistency between the\naircraft structure in SAR imagery and the proposed annotation. In this process,\nthe structural attribute can be disentangled in a geometrically meaningful\nmanner. In conclusion, the MTSGL is presented with the expert-level aircraft\nprior knowledge and structure guided learning paradigm, aiming to comprehend\nthe aircraft concept in a way analogous to the human cognitive process.\nExtensive experiments are conducted on a self-constructed multi-task SAR\naircraft recognition dataset (MT-SARD) and the effective results illustrate the\nsuperiority of robustness and interpretation ability of the proposed MTSGL."
    },
    {
        "date": "2025-04",
        "title": "Give LLMs a Security Course: Securing Retrieval-Augmented Code Generation via Knowledge Injection",
        "author": "Bo Lin, Shangwen Wang, Yihao Qin, Liqian Chen, and Xiaoguang Mao",
        "link": "http://arxiv.org/abs/2504.16429v1",
        "abstract": "Retrieval-Augmented Code Generation (RACG) leverages external knowledge to\nenhance Large Language Models (LLMs) in code synthesis, improving the\nfunctional correctness of the generated code. However, existing RACG systems\nlargely overlook security, leading to substantial risks. Especially, the\npoisoning of malicious code into knowledge bases can mislead LLMs, resulting in\nthe generation of insecure outputs, which poses a critical threat in modern\nsoftware development. To address this, we propose a security-hardening\nframework for RACG systems, CodeGuarder, that shifts the paradigm from\nretrieving only functional code examples to incorporating both functional code\nand security knowledge. Our framework constructs a security knowledge base from\nreal-world vulnerability databases, including secure code samples and root\ncause annotations. For each code generation query, a retriever decomposes the\nquery into fine-grained sub-tasks and fetches relevant security knowledge. To\nprioritize critical security guidance, we introduce a re-ranking and filtering\nmechanism by leveraging the LLMs' susceptibility to different vulnerability\ntypes. This filtered security knowledge is seamlessly integrated into the\ngeneration prompt. Our evaluation shows CodeGuarder significantly improves code\nsecurity rates across various LLMs, achieving average improvements of 20.12\\%\nin standard RACG, and 31.53\\% and 21.91\\% under two distinct poisoning\nscenarios without compromising functional correctness. Furthermore, CodeGuarder\ndemonstrates strong generalization, enhancing security even when the targeted\nlanguage's security knowledge is lacking. This work presents CodeGuarder as a\npivotal advancement towards building secure and trustworthy RACG systems."
    },
    {
        "date": "2025-04",
        "title": "VideoMark: A Distortion-Free Robust Watermarking Framework for Video Diffusion Models",
        "author": "Xuming Hu, Hanqian Li, Jungang Li, and Aiwei Liu",
        "link": "http://arxiv.org/abs/2504.16359v1",
        "abstract": "This work presents VideoMark, a training-free robust watermarking framework\nfor video diffusion models. As diffusion models advance in generating highly\nrealistic videos, the need for reliable content attribution mechanisms has\nbecome critical. While watermarking techniques for image diffusion models have\nmade progress, directly extending these methods to videos presents unique\nchallenges due to variable video lengths and vulnerability to temporal attacks.\nVideoMark addresses these limitations through a frame-wise watermarking\nstrategy using pseudorandom error correction (PRC) codes to embed watermark\ninformation during the generation process. Our method generates an extended\nwatermark message sequence and randomly selects starting positions for each\nvideo, ensuring uniform noise distribution in the latent space and maintaining\ngeneration quality. For watermark extraction, we introduce a Temporal Matching\nModule (TMM) that uses edit distance to align decoded messages with the\noriginal watermark sequence, providing robustness against temporal attacks such\nas frame deletion. Experimental results demonstrate that VideoMark achieves\nhigher decoding accuracy than existing methods while maintaining video quality\non par with watermark-free generation. Importantly, our watermark remains\nundetectable to attackers without the secret key, ensuring strong\nimperceptibility compared to other watermarking frameworks. VideoMark provides\na practical solution for content attribution in diffusion-based video\ngeneration without requiring additional training or compromising video quality.\nOur code and data are available at\n\\href{https://github.com/KYRIE-LI11/VideoMark}{https://github.com/KYRIE-LI11/VideoMark}."
    }
]