[
    {
        "date": "2025-01",
        "title": "Distilling foundation models for robust and efficient models in digital pathology",
        "author": "Alexandre Filiot, Nicolas Dop, Oussama Tchita, Auriane Riou, Thomas Peeters, Daria Valter, Marin Scalbert, Charlie Saillard, Genevi\u00e8ve Robin, and Antoine Olivier",
        "link": "http://arxiv.org/abs/2501.16239v1",
        "abstract": "In recent years, the advent of foundation models (FM) for digital pathology\nhas relied heavily on scaling the pre-training datasets and the model size,\nyielding large and powerful models. While it resulted in improving the\nperformance on diverse downstream tasks, it also introduced increased\ncomputational cost and inference time. In this work, we explore the\ndistillation of a large foundation model into a smaller one, reducing the\nnumber of parameters by several orders of magnitude. Leveraging distillation\ntechniques, our distilled model, H0-mini, achieves nearly comparable\nperformance to large FMs at a significantly reduced inference cost. It is\nevaluated on several public benchmarks, achieving 3rd place on the HEST\nbenchmark and 5th place on the EVA benchmark. Additionally, a robustness\nanalysis conducted on the PLISM dataset demonstrates that our distilled model\nreaches excellent robustness to variations in staining and scanning conditions,\nsignificantly outperforming other state-of-the art models. This opens new\nperspectives to design lightweight and robust models for digital pathology,\nwithout compromising on performance."
    },
    {
        "date": "2025-01",
        "title": "Generating Spatial Synthetic Populations Using Wasserstein Generative Adversarial Network: A Case Study with EU-SILC Data for Helsinki and Thessaloniki",
        "author": "Vanja Falck",
        "link": "http://arxiv.org/abs/2501.16080v1",
        "abstract": "Using agent-based social simulations can enhance our understanding of urban\nplanning, public health, and economic forecasting. Realistic synthetic\npopulations with numerous attributes strengthen these simulations. The\nWasserstein Generative Adversarial Network, trained on census data like\nEU-SILC, can create robust synthetic populations. These methods, aided by\nexternal statistics or EU-SILC weights, generate spatial synthetic populations\nfor agent-based models. The increased access to high-quality micro-data has\nsparked interest in synthetic populations, which preserve demographic profiles\nand analytical strength while ensuring privacy and preventing discrimination.\nThis study uses national data from Finland and Greece for Helsinki and\nThessaloniki to explore balanced spatial synthetic population generation.\nResults show challenges related to balancing data with or without aggregated\nstatistics for the target population and the general under-representation of\nfringe profiles by deep generative methods. The latter can lead to\ndiscrimination in agent-based simulations."
    },
    {
        "date": "2025-01",
        "title": "Provisioning Time-Based Subscription in NDN: A Secure and Efficient Access Control Scheme",
        "author": "Nazatul H. Sultan, Chandan Kumar, Saurab Dulal, Vijay Varadharajan, Seyit Camtepe, and Surya Nepal",
        "link": "http://arxiv.org/abs/2501.15975v1",
        "abstract": "This paper proposes a novel encryption-based access control mechanism for\nNamed Data Networking (NDN). The scheme allows data producers to share their\ncontent in encrypted form before transmitting it to consumers. The encryption\nmechanism incorporates time-based subscription access policies directly into\nthe encrypted content, enabling only consumers with valid subscriptions to\ndecrypt it. This makes the scheme well-suited for real-world,\nsubscription-based applications like Netflix. Additionally, the scheme\nintroduces an anonymous and unlinkable signature-based authentication mechanism\nthat empowers edge routers to block bogus content requests at the network's\nentry point, thereby mitigating Denial of Service (DoS) attacks. A formal\nsecurity proof demonstrates the scheme's resistance to Chosen Plaintext Attacks\n(CPA). Performance analysis, using Mini-NDN-based emulation and a Charm library\nimplementation, further confirms the practicality of the scheme. Moreover, it\noutperforms closely related works in terms of functionality, security, and\ncommunication overhead."
    },
    {
        "date": "2025-01",
        "title": "LLM-attacker: Enhancing Closed-loop Adversarial Scenario Generation for Autonomous Driving with Large Language Models",
        "author": "Yuewen Mei, Tong Nie, Jian Sun, and Ye Tian",
        "link": "http://arxiv.org/abs/2501.15850v1",
        "abstract": "Ensuring and improving the safety of autonomous driving systems (ADS) is\ncrucial for the deployment of highly automated vehicles, especially in\nsafety-critical events. To address the rarity issue, adversarial scenario\ngeneration methods are developed, in which behaviors of traffic participants\nare manipulated to induce safety-critical events. However, existing methods\nstill face two limitations. First, identification of the adversarial\nparticipant directly impacts the effectiveness of the generation. However, the\ncomplexity of real-world scenarios, with numerous participants and diverse\nbehaviors, makes identification challenging. Second, the potential of generated\nsafety-critical scenarios to continuously improve ADS performance remains\nunderexplored. To address these issues, we propose LLM-attacker: a closed-loop\nadversarial scenario generation framework leveraging large language models\n(LLMs). Specifically, multiple LLM agents are designed and coordinated to\nidentify optimal attackers. Then, the trajectories of the attackers are\noptimized to generate adversarial scenarios. These scenarios are iteratively\nrefined based on the performance of ADS, forming a feedback loop to improve\nADS. Experimental results show that LLM-attacker can create more dangerous\nscenarios than other methods, and the ADS trained with it achieves a collision\nrate half that of training with normal scenarios. This indicates the ability of\nLLM-attacker to test and enhance the safety and robustness of ADS. Video\ndemonstrations are provided at:\nhttps://drive.google.com/file/d/1Zv4V3iG7825oyiKbUwS2Y-rR0DQIE1ZA/view."
    },
    {
        "date": "2025-01",
        "title": "Beyond In-Distribution Performance: A Cross-Dataset Study of Trajectory Prediction Robustness",
        "author": "Yue Yao, Daniel Goehring, and Joerg Reichardt",
        "link": "http://arxiv.org/abs/2501.15842v1",
        "abstract": "We study the Out-of-Distribution (OoD) generalization ability of three SotA\ntrajectory prediction models with comparable In-Distribution (ID) performance\nbut different model designs. We investigate the influence of inductive bias,\nsize of training data and data augmentation strategy by training the models on\nArgoverse 2 (A2) and testing on Waymo Open Motion (WO) and vice versa. We find\nthat the smallest model with highest inductive bias exhibits the best OoD\ngeneralization across different augmentation strategies when trained on the\nsmaller A2 dataset and tested on the large WO dataset. In the converse setting,\ntraining all models on the larger WO dataset and testing on the smaller A2\ndataset, we find that all models generalize poorly, even though the model with\nthe highest inductive bias still exhibits the best generalization ability. We\ndiscuss possible reasons for this surprising finding and draw conclusions about\nthe design and test of trajectory prediction models and benchmarks."
    },
    {
        "date": "2025-01",
        "title": "FuzzyLight: A Robust Two-Stage Fuzzy Approach for Traffic Signal Control Works in Real Cities",
        "author": "Mingyuan Li, Jiahao Wang, Bo Du, Jun Shen, and Qiang Wu",
        "link": "http://arxiv.org/abs/2501.15820v1",
        "abstract": "Effective traffic signal control (TSC) is crucial in mitigating urban\ncongestion and reducing emissions. Recently, reinforcement learning (RL) has\nbeen the research trend for TSC. However, existing RL algorithms face several\nreal-world challenges that hinder their practical deployment in TSC: (1) Sensor\naccuracy deteriorates with increased sensor detection range, and data\ntransmission is prone to noise, potentially resulting in unsafe TSC decisions.\n(2) During the training of online RL, interactions with the environment could\nbe unstable, potentially leading to inappropriate traffic signal phase (TSP)\nselection and traffic congestion. (3) Most current TSC algorithms focus only on\nTSP decisions, overlooking the critical aspect of phase duration, affecting\nsafety and efficiency. To overcome these challenges, we propose a robust\ntwo-stage fuzzy approach called FuzzyLight, which integrates compressed sensing\nand RL for TSC deployment. FuzzyLight offers several key contributions: (1) It\nemploys fuzzy logic and compressed sensing to address sensor noise and enhances\nthe efficiency of TSP decisions. (2) It maintains stable performance during\ntraining and combines fuzzy logic with RL to generate precise phases. (3) It\nworks in real cities across 22 intersections and demonstrates superior\nperformance in both real-world and simulated environments. Experimental results\nindicate that FuzzyLight enhances traffic efficiency by 48% compared to\nexpert-designed timings in the real world. Furthermore, it achieves\nstate-of-the-art (SOTA) performance in simulated environments using six\nreal-world datasets with transmission noise. The code and deployment video are\navailable at the URL1"
    },
    {
        "date": "2025-01",
        "title": "CENSOR: Defense Against Gradient Inversion via Orthogonal Subspace Bayesian Sampling",
        "author": "Kaiyuan Zhang, Siyuan Cheng, Guangyu Shen, Bruno Ribeiro, Shengwei An, Pin-Yu Chen, Xiangyu Zhang, and Ninghui Li",
        "link": "http://arxiv.org/abs/2501.15718v1",
        "abstract": "Federated learning collaboratively trains a neural network on a global\nserver, where each local client receives the current global model weights and\nsends back parameter updates (gradients) based on its local private data. The\nprocess of sending these model updates may leak client's private data\ninformation. Existing gradient inversion attacks can exploit this vulnerability\nto recover private training instances from a client's gradient vectors.\nRecently, researchers have proposed advanced gradient inversion techniques that\nexisting defenses struggle to handle effectively. In this work, we present a\nnovel defense tailored for large neural network models. Our defense capitalizes\non the high dimensionality of the model parameters to perturb gradients within\na subspace orthogonal to the original gradient. By leveraging cold posteriors\nover orthogonal subspaces, our defense implements a refined gradient update\nmechanism. This enables the selection of an optimal gradient that not only\nsafeguards against gradient inversion attacks but also maintains model utility.\nWe conduct comprehensive experiments across three different datasets and\nevaluate our defense against various state-of-the-art attacks and defenses.\nCode is available at https://censor-gradient.github.io."
    },
    {
        "date": "2025-01",
        "title": "People who frequently use ChatGPT for writing tasks are accurate and robust detectors of AI-generated text",
        "author": "Jenna Russell, Marzena Karpinska, and Mohit Iyyer",
        "link": "http://arxiv.org/abs/2501.15654v1",
        "abstract": "In this paper, we study how well humans can detect text generated by\ncommercial LLMs (GPT-4o, Claude, o1). We hire annotators to read 300\nnon-fiction English articles, label them as either human-written or\nAI-generated, and provide paragraph-length explanations for their decisions.\nOur experiments show that annotators who frequently use LLMs for writing tasks\nexcel at detecting AI-generated text, even without any specialized training or\nfeedback. In fact, the majority vote among five such \"expert\" annotators\nmisclassifies only 1 of 300 articles, significantly outperforming most\ncommercial and open-source detectors we evaluated even in the presence of\nevasion tactics like paraphrasing and humanization. Qualitative analysis of the\nexperts' free-form explanations shows that while they rely heavily on specific\nlexical clues ('AI vocabulary'), they also pick up on more complex phenomena\nwithin the text (e.g., formality, originality, clarity) that are challenging to\nassess for automatic detectors. We release our annotated dataset and code to\nspur future research into both human and automated detection of AI-generated\ntext."
    },
    {
        "date": "2025-01",
        "title": "A Privacy Enhancing Technique to Evade Detection by Street Video Cameras Without Using Adversarial Accessories",
        "author": "Jacob Shams, Ben Nassi, Satoru Koda, Asaf Shabtai, and Yuval Elovici",
        "link": "http://arxiv.org/abs/2501.15653v1",
        "abstract": "In this paper, we propose a privacy-enhancing technique leveraging an\ninherent property of automatic pedestrian detection algorithms, namely, that\nthe training of deep neural network (DNN) based methods is generally performed\nusing curated datasets and laboratory settings, while the operational areas of\nthese methods are dynamic real-world environments. In particular, we leverage a\nnovel side effect of this gap between the laboratory and the real world:\nlocation-based weakness in pedestrian detection. We demonstrate that the\nposition (distance, angle, height) of a person, and ambient light level,\ndirectly impact the confidence of a pedestrian detector when detecting the\nperson. We then demonstrate that this phenomenon is present in pedestrian\ndetectors observing a stationary scene of pedestrian traffic, with blind spot\nareas of weak detection of pedestrians with low confidence. We show how\nprivacy-concerned pedestrians can leverage these blind spots to evade detection\nby constructing a minimum confidence path between two points in a scene,\nreducing the maximum confidence and average confidence of the path by up to\n0.09 and 0.13, respectively, over direct and random paths through the scene. To\ncounter this phenomenon, and force the use of more costly and sophisticated\nmethods to leverage this vulnerability, we propose a novel countermeasure to\nimprove the confidence of pedestrian detectors in blind spots, raising the\nmax/average confidence of paths generated by our technique by 0.09 and 0.05,\nrespectively. In addition, we demonstrate that our countermeasure improves a\nFaster R-CNN-based pedestrian detector's TPR and average true positive\nconfidence by 0.03 and 0.15, respectively."
    },
    {
        "date": "2025-01",
        "title": "Comparative clinical evaluation of \"memory-efficient\" synthetic 3d generative adversarial networks (gan) head-to-head to state of art: results on computed tomography of the chest",
        "author": "Mahshid shiri, Chandra Bortolotto, Alessandro Bruno, Alessio Consonni, Daniela Maria Grasso, Leonardo Brizzi, Daniele Loiacono, and Lorenzo Preda",
        "link": "http://arxiv.org/abs/2501.15572v1",
        "abstract": "Introduction: Generative Adversarial Networks (GANs) are increasingly used to\ngenerate synthetic medical images, addressing the critical shortage of\nannotated data for training Artificial Intelligence (AI) systems. This study\nintroduces a novel memory-efficient GAN architecture, incorporating Conditional\nRandom Fields (CRFs) to generate high-resolution 3D medical images and\nevaluates its performance against the state-of-the-art hierarchical (HA)-GAN\nmodel.\n  Materials and Methods: The CRF-GAN was trained using the open-source lung CT\nLUNA16 dataset. The architecture was compared to HA-GAN through a quantitative\nevaluation, using Frechet Inception Distance (FID) and Maximum Mean Discrepancy\n(MMD) metrics, and a qualitative evaluation, through a two-alternative forced\nchoice (2AFC) test completed by a pool of 12 resident radiologists, in order to\nassess the realism of the generated images.\n  Results: CRF-GAN outperformed HA-GAN with lower FID (0.047 vs. 0.061) and MMD\n(0.084 vs. 0.086) scores, indicating better image fidelity. The 2AFC test\nshowed a significant preference for images generated by CRF-Gan over those\ngenerated by HA-GAN with a p-value of 1.93e-05. Additionally, CRF-GAN\ndemonstrated 9.34% lower memory usage at 256 resolution and achieved up to\n14.6% faster training speeds, offering substantial computational savings.\n  Discussion: CRF-GAN model successfully generates high-resolution 3D medical\nimages with non-inferior quality to conventional models, while being more\nmemory-efficient and faster. Computational power and time saved can be used to\nimprove the spatial resolution and anatomical accuracy of generated images,\nwhich is still a critical factor limiting their direct clinical applicability."
    },
    {
        "date": "2025-01",
        "title": "Distributionally Robust Graph Out-of-Distribution Recommendation via Diffusion Model",
        "author": "Chu Zhao, Enneng Yang, Yuliang Liang, Jianzhe Zhao, Guibing Guo, and Xingwei Wang",
        "link": "http://arxiv.org/abs/2501.15555v1",
        "abstract": "The distributionally robust optimization (DRO)-based graph neural network\nmethods improve recommendation systems' out-of-distribution (OOD)\ngeneralization by optimizing the model's worst-case performance. However, these\nstudies fail to consider the impact of noisy samples in the training data,\nwhich results in diminished generalization capabilities and lower accuracy.\nThrough experimental and theoretical analysis, this paper reveals that current\nDRO-based graph recommendation methods assign greater weight to noise\ndistribution, leading to model parameter learning being dominated by it. When\nthe model overly focuses on fitting noise samples in the training data, it may\nlearn irrelevant or meaningless features that cannot be generalized to OOD\ndata. To address this challenge, we design a Distributionally Robust Graph\nmodel for OOD recommendation (DRGO). Specifically, our method first employs a\nsimple and effective diffusion paradigm to alleviate the noisy effect in the\nlatent space. Additionally, an entropy regularization term is introduced in the\nDRO objective function to avoid extreme sample weights in the worst-case\ndistribution. Finally, we provide a theoretical proof of the generalization\nerror bound of DRGO as well as a theoretical analysis of how our approach\nmitigates noisy sample effects, which helps to better understand the proposed\nframework from a theoretical perspective. We conduct extensive experiments on\nfour datasets to evaluate the effectiveness of our framework against three\ntypical distribution shifts, and the results demonstrate its superiority in\nboth independently and identically distributed distributions (IID) and OOD."
    },
    {
        "date": "2025-01",
        "title": "UNIDOOR: A Universal Framework for Action-Level Backdoor Attacks in Deep Reinforcement Learning",
        "author": "Oubo Ma, Linkang Du, Yang Dai, Chunyi Zhou, Qingming Li, Yuwen Pu, and Shouling Ji",
        "link": "http://arxiv.org/abs/2501.15529v1",
        "abstract": "Deep reinforcement learning (DRL) is widely applied to safety-critical\ndecision-making scenarios. However, DRL is vulnerable to backdoor attacks,\nespecially action-level backdoors, which pose significant threats through\nprecise manipulation and flexible activation, risking outcomes like vehicle\ncollisions or drone crashes. The key distinction of action-level backdoors lies\nin the utilization of the backdoor reward function to associate triggers with\ntarget actions. Nevertheless, existing studies typically rely on backdoor\nreward functions with fixed values or conditional flipping, which lack\nuniversality across diverse DRL tasks and backdoor designs, resulting in\nfluctuations or even failure in practice.\n  This paper proposes the first universal action-level backdoor attack\nframework, called UNIDOOR, which enables adaptive exploration of backdoor\nreward functions through performance monitoring, eliminating the reliance on\nexpert knowledge and grid search. We highlight that action tampering serves as\na crucial component of action-level backdoor attacks in continuous action\nscenarios, as it addresses attack failures caused by low-frequency target\nactions. Extensive evaluations demonstrate that UNIDOOR significantly enhances\nthe attack performance of action-level backdoors, showcasing its universality\nacross diverse attack scenarios, including single/multiple agents,\nsingle/multiple backdoors, discrete/continuous action spaces, and sparse/dense\nreward signals. Furthermore, visualization results encompassing state\ndistribution, neuron activation, and animations demonstrate the stealthiness of\nUNIDOOR. The source code of UNIDOOR can be found at\nhttps://github.com/maoubo/UNIDOOR."
    },
    {
        "date": "2025-01",
        "title": "Mitigating Spurious Negative Pairs for Robust Industrial Anomaly Detection",
        "author": "Hossein Mirzaei, Mojtaba Nafez, Jafar Habibi, Mohammad Sabokrou, and Mohammad Hossein Rohban",
        "link": "http://arxiv.org/abs/2501.15434v1",
        "abstract": "Despite significant progress in Anomaly Detection (AD), the robustness of\nexisting detection methods against adversarial attacks remains a challenge,\ncompromising their reliability in critical real-world applications such as\nautonomous driving. This issue primarily arises from the AD setup, which\nassumes that training data is limited to a group of unlabeled normal samples,\nmaking the detectors vulnerable to adversarial anomaly samples during testing.\nAdditionally, implementing adversarial training as a safeguard encounters\ndifficulties, such as formulating an effective objective function without\naccess to labels. An ideal objective function for adversarial training in AD\nshould promote strong perturbations both within and between the normal and\nanomaly groups to maximize margin between normal and anomaly distribution. To\naddress these issues, we first propose crafting a pseudo-anomaly group derived\nfrom normal group samples. Then, we demonstrate that adversarial training with\ncontrastive loss could serve as an ideal objective function, as it creates both\ninter- and intra-group perturbations. However, we notice that spurious negative\npairs compromise the conventional contrastive loss to achieve robust AD.\nSpurious negative pairs are those that should be closely mapped but are\nerroneously separated. These pairs introduce noise and misguide the direction\nof inter-group adversarial perturbations. To overcome the effect of spurious\nnegative pairs, we define opposite pairs and adversarially pull them apart to\nstrengthen inter-group perturbations. Experimental results demonstrate our\nsuperior performance in both clean and adversarial scenarios, with a 26.1%\nimprovement in robust detection across various challenging benchmark datasets.\nThe implementation of our work is available at:\nhttps://github.com/rohban-lab/COBRA."
    },
    {
        "date": "2025-01",
        "title": "Learning-Enhanced Safeguard Control for High-Relative-Degree Systems: Robust Optimization under Disturbances and Faults",
        "author": "Xinyang Wang, Hongwei Zhang, Shimin Wang, Wei Xiao, and Martin Guay",
        "link": "http://arxiv.org/abs/2501.15373v1",
        "abstract": "Merely pursuing performance may adversely affect the safety, while a\nconservative policy for safe exploration will degrade the performance. How to\nbalance the safety and performance in learning-based control problems is an\ninteresting yet challenging issue. This paper aims to enhance system\nperformance with safety guarantee in solving the reinforcement learning\n(RL)-based optimal control problems of nonlinear systems subject to\nhigh-relative-degree state constraints and unknown time-varying\ndisturbance/actuator faults. First, to combine control barrier functions (CBFs)\nwith RL, a new type of CBFs, termed high-order reciprocal control barrier\nfunction (HO-RCBF) is proposed to deal with high-relative-degree constraints\nduring the learning process. Then, the concept of gradient similarity is\nproposed to quantify the relationship between the gradient of safety and the\ngradient of performance. Finally, gradient manipulation and adaptive mechanisms\nare introduced in the safe RL framework to enhance the performance with a\nsafety guarantee. Two simulation examples illustrate that the proposed safe RL\nframework can address high-relative-degree constraint, enhance safety\nrobustness and improve system performance."
    },
    {
        "date": "2025-01",
        "title": "AI-Driven Secure Data Sharing: A Trustworthy and Privacy-Preserving Approach",
        "author": "Al Amin, Kamrul Hasan, Sharif Ullah, and Liang Hong",
        "link": "http://arxiv.org/abs/2501.15363v1",
        "abstract": "In the era of data-driven decision-making, ensuring the privacy and security\nof shared data is paramount across various domains. Applying existing deep\nneural networks (DNNs) to encrypted data is critical and often compromises\nperformance, security, and computational overhead. To address these\nlimitations, this research introduces a secure framework consisting of a\nlearnable encryption method based on the block-pixel operation to encrypt the\ndata and subsequently integrate it with the Vision Transformer (ViT). The\nproposed framework ensures data privacy and security by creating unique\nscrambling patterns per key, providing robust performance against adversarial\nattacks without compromising computational efficiency and data integrity. The\nframework was tested on sensitive medical datasets to validate its efficacy,\nproving its ability to handle highly confidential information securely. The\nsuggested framework was validated with a 94\\% success rate after extensive\ntesting on real-world datasets, such as MRI brain tumors and histological scans\nof lung and colon cancers. Additionally, the framework was tested under diverse\nadversarial attempts against secure data sharing with optimum performance and\ndemonstrated its effectiveness in various threat scenarios. These comprehensive\nanalyses underscore its robustness, making it a trustworthy solution for secure\ndata sharing in critical applications."
    },
    {
        "date": "2025-01",
        "title": "Killing it with Zero-Shot: Adversarially Robust Novelty Detection",
        "author": "Hossein Mirzaei, Mohammad Jafari, Hamid Reza Dehbashi, Zeinab Sadat Taghavi, Mohammad Sabokrou, and Mohammad Hossein Rohban",
        "link": "http://arxiv.org/abs/2501.15271v1",
        "abstract": "Novelty Detection (ND) plays a crucial role in machine learning by\nidentifying new or unseen data during model inference. This capability is\nespecially important for the safe and reliable operation of automated systems.\nDespite advances in this field, existing techniques often fail to maintain\ntheir performance when subject to adversarial attacks. Our research addresses\nthis gap by marrying the merits of nearest-neighbor algorithms with robust\nfeatures obtained from models pretrained on ImageNet. We focus on enhancing the\nrobustness and performance of ND algorithms. Experimental results demonstrate\nthat our approach significantly outperforms current state-of-the-art methods\nacross various benchmarks, particularly under adversarial conditions. By\nincorporating robust pretrained features into the k-NN algorithm, we establish\na new standard for performance and robustness in the field of robust ND. This\nwork opens up new avenues for research aimed at fortifying machine learning\nsystems against adversarial vulnerabilities. Our implementation is publicly\navailable at https://github.com/rohban-lab/ZARND."
    },
    {
        "date": "2025-01",
        "title": "Mirage in the Eyes: Hallucination Attack on Multi-modal Large Language Models with Only Attention Sink",
        "author": "Yining Wang, Mi Zhang, Junjie Sun, Chenyue Wang, Min Yang, Hui Xue, Jialing Tao, Ranjie Duan, and Jiexi Liu",
        "link": "http://arxiv.org/abs/2501.15269v1",
        "abstract": "Fusing visual understanding into language generation, Multi-modal Large\nLanguage Models (MLLMs) are revolutionizing visual-language applications. Yet,\nthese models are often plagued by the hallucination problem, which involves\ngenerating inaccurate objects, attributes, and relationships that do not match\nthe visual content. In this work, we delve into the internal attention\nmechanisms of MLLMs to reveal the underlying causes of hallucination, exposing\nthe inherent vulnerabilities in the instruction-tuning process.\n  We propose a novel hallucination attack against MLLMs that exploits attention\nsink behaviors to trigger hallucinated content with minimal image-text\nrelevance, posing a significant threat to critical downstream applications.\nDistinguished from previous adversarial methods that rely on fixed patterns,\nour approach generates dynamic, effective, and highly transferable visual\nadversarial inputs, without sacrificing the quality of model responses.\nComprehensive experiments on 6 prominent MLLMs demonstrate the efficacy of our\nattack in compromising black-box MLLMs even with extensive mitigating\nmechanisms, as well as the promising results against cutting-edge commercial\nAPIs, such as GPT-4o and Gemini 1.5. Our code is available at\nhttps://huggingface.co/RachelHGF/Mirage-in-the-Eyes."
    },
    {
        "date": "2025-01",
        "title": "Pre-trained Model Guided Mixture Knowledge Distillation for Adversarial Federated Learning",
        "author": "Yu Qiao, Huy Q. Le, Apurba Adhikary, and Choong Seon Hong",
        "link": "http://arxiv.org/abs/2501.15257v1",
        "abstract": "This paper aims to improve the robustness of a small global model while\nmaintaining clean accuracy under adversarial attacks and non-IID challenges in\nfederated learning. By leveraging the concise knowledge embedded in the class\nprobabilities from a pre-trained model for both clean and adversarial image\nclassification, we propose a Pre-trained Model-guided Adversarial Federated\nLearning (PM-AFL) training paradigm. This paradigm integrates vanilla mixture\nand adversarial mixture knowledge distillation to effectively balance accuracy\nand robustness while promoting local models to learn from diverse data.\nSpecifically, for clean accuracy, we adopt a dual distillation strategy where\nthe class probabilities of randomly paired images and their blended versions\nare aligned between the teacher model and the local models. For adversarial\nrobustness, we use a similar distillation approach but replace clean samples on\nthe local side with adversarial examples. Moreover, considering the bias\nbetween local and global models, we also incorporate a consistency\nregularization term to ensure that local adversarial predictions stay aligned\nwith their corresponding global clean ones. These strategies collectively\nenable local models to absorb diverse knowledge from the teacher model while\nmaintaining close alignment with the global model, thereby mitigating\noverfitting to local optima and enhancing the generalization of the global\nmodel. Experiments demonstrate that the PM-AFL-based paradigm outperforms other\nmethods that integrate defense strategies by a notable margin."
    },
    {
        "date": "2025-01",
        "title": "Median of Forests for Robust Density Estimation",
        "author": "Hongwei Wen, Annika Betken, and Tao Huang",
        "link": "http://arxiv.org/abs/2501.15157v1",
        "abstract": "Robust density estimation refers to the consistent estimation of the density\nfunction even when the data is contaminated by outliers. We find that existing\nforest density estimation at a certain point is inherently resistant to the\noutliers outside the cells containing the point, which we call\n\\textit{non-local outliers}, but not resistant to the rest \\textit{local\noutliers}. To achieve robustness against all outliers, we propose an ensemble\nlearning algorithm called \\textit{medians of forests for robust density\nestimation} (\\textit{MFRDE}), which adopts a pointwise median operation on\nforest density estimators fitted on subsampled datasets. Compared to existing\nrobust kernel-based methods, MFRDE enables us to choose larger subsampling\nsizes, sacrificing less accuracy for density estimation while achieving\nrobustness. On the theoretical side, we introduce the local outlier exponent to\nquantify the number of local outliers. Under this exponent, we show that even\nif the number of outliers reaches a certain polynomial order in the sample\nsize, MFRDE is able to achieve almost the same convergence rate as the same\nalgorithm on uncontaminated data, whereas robust kernel-based methods fail. On\nthe practical side, real data experiments show that MFRDE outperforms existing\nrobust kernel-based methods. Moreover, we apply MFRDE to anomaly detection to\nshowcase a further application."
    },
    {
        "date": "2025-01",
        "title": "PromptShield: Deployable Detection for Prompt Injection Attacks",
        "author": "Dennis Jacob, Hend Alzahrani, Zhanhao Hu, Basel Alomair, and David Wagner",
        "link": "http://arxiv.org/abs/2501.15145v1",
        "abstract": "Current application designers have moved to integrate large language models\n(LLMs) into their products. These LLM-integrated applications are vulnerable to\nprompt injection vulnerabilities. While attempts have been made to address this\nproblem by building a detector that can monitor inputs to the LLM and detect\nattacks, we find that many detectors are not yet suitable for practical\ndeployment. To support research in this area, we design the PromptShield\nbenchmark for evaluating practical prompt injection detectors. We also\nconstruct a new detector, the PromptShield detector, which achieves\nsignificantly better performance at detecting prompt injection attacks than any\nprior scheme. Our work suggests that larger models, more training data,\nappropriate metrics, and careful curation of training data can contribute to\nstrong detector performance."
    },
    {
        "date": "2025-01",
        "title": "TranStable: Towards Robust Pixel-level Online Video Stabilization by Jointing Transformer and CNN",
        "author": "zhizhen li, tianyi zhuo, Yifei Cao, Jizhe Yu, and Yu Liu",
        "link": "http://arxiv.org/abs/2501.15138v1",
        "abstract": "Video stabilization often struggles with distortion and excessive cropping.\nThis paper proposes a novel end-to-end framework, named TranStable, to address\nthese challenges, comprising a genera tor and a discriminator. We establish\nTransformerUNet (TUNet) as the generator to utilize the Hierarchical Adaptive\nFusion Module (HAFM), integrating Transformer and CNN to leverage both global\nand local features across multiple visual cues. By modeling frame-wise\nrelationships, it generates robust pixel-level warping maps for stable\ngeometric transformations. Furthermore, we design the Stability Discriminator\nModule (SDM), which provides pixel-wise supervision for authenticity and\nconsistency in training period, ensuring more complete field-of-view while\nminimizing jitter artifacts and enhancing visual fidelity. Extensive\nexperiments on NUS, DeepStab, and Selfie benchmarks demonstrate\nstate-of-the-art performance."
    },
    {
        "date": "2025-01",
        "title": "Comprehensive Evaluation of Cloaking Backdoor Attacks on Object Detector in Real-World",
        "author": "Hua Ma, Alsharif Abuadbba, Yansong Gao, Hyoungshick Kim, and Surya Nepal",
        "link": "http://arxiv.org/abs/2501.15101v1",
        "abstract": "The exploration of backdoor vulnerabilities in object detectors, particularly\nin real-world scenarios, remains limited. A significant challenge lies in the\nabsence of a natural physical backdoor dataset, and constructing such a dataset\nis both time- and labor-intensive. In this work, we address this gap by\ncreating a large-scale dataset comprising approximately 11,800 images/frames\nwith annotations featuring natural objects (e.g., T-shirts and hats) as\ntriggers to incur cloaking adversarial effects in diverse real-world scenarios.\nThis dataset is tailored for the study of physical backdoors in object\ndetectors. Leveraging this dataset, we conduct a comprehensive evaluation of an\ninsidious cloaking backdoor effect against object detectors, wherein the\nbounding box around a person vanishes when the individual is near a natural\nobject (e.g., a commonly available T-shirt) in front of the detector. Our\nevaluations encompass three prevalent attack surfaces: data outsourcing, model\noutsourcing, and the use of pretrained models. The cloaking effect is\nsuccessfully implanted in object detectors across all three attack surfaces. We\nextensively evaluate four popular object detection algorithms (anchor-based\nYolo-V3, Yolo-V4, Faster R-CNN, and anchor-free CenterNet) using 19 videos\n(totaling approximately 11,800 frames) in real-world scenarios. Our results\ndemonstrate that the backdoor attack exhibits remarkable robustness against\nvarious factors, including movement, distance, angle, non-rigid deformation,\nand lighting. In data and model outsourcing scenarios, the attack success rate\n(ASR) in most videos reaches 100% or near it, while the clean data accuracy of\nthe backdoored model remains indistinguishable from that of the clean model,\nmaking it impossible to detect backdoor behavior through a validation set."
    },
    {
        "date": "2025-01",
        "title": "Bringing RGB and IR Together: Hierarchical Multi-Modal Enhancement for Robust Transmission Line Detection",
        "author": "Shengdong Zhang, Xiaoqin Zhang, Wenqi Ren, Linlin Shen, Shaohua Wan, Jun Zhang, and Yujing M Jiang",
        "link": "http://arxiv.org/abs/2501.15099v1",
        "abstract": "Ensuring a stable power supply in rural areas relies heavily on effective\ninspection of power equipment, particularly transmission lines (TLs). However,\ndetecting TLs from aerial imagery can be challenging when dealing with\nmisalignments between visible light (RGB) and infrared (IR) images, as well as\nmismatched high- and low-level features in convolutional networks. To address\nthese limitations, we propose a novel Hierarchical Multi-Modal Enhancement\nNetwork (HMMEN) that integrates RGB and IR data for robust and accurate TL\ndetection. Our method introduces two key components: (1) a Mutual Multi-Modal\nEnhanced Block (MMEB), which fuses and enhances hierarchical RGB and IR feature\nmaps in a coarse-to-fine manner, and (2) a Feature Alignment Block (FAB) that\ncorrects misalignments between decoder outputs and IR feature maps by\nleveraging deformable convolutions. We employ MobileNet-based encoders for both\nRGB and IR inputs to accommodate edge-computing constraints and reduce\ncomputational overhead. Experimental results on diverse weather and lighting\nconditionsfog, night, snow, and daytimedemonstrate the superiority and\nrobustness of our approach compared to state-of-the-art methods, resulting in\nfewer false positives, enhanced boundary delineation, and better overall\ndetection performance. This framework thus shows promise for practical\nlarge-scale power line inspections with unmanned aerial vehicles."
    },
    {
        "date": "2025-01",
        "title": "Towards Better Robustness: Progressively Joint Pose-3DGS Learning for Arbitrarily Long Videos",
        "author": "Zhen-Hui Dong, Sheng Ye, Yu-Hui Wen, Nannan Li, and Yong-Jin Liu",
        "link": "http://arxiv.org/abs/2501.15096v1",
        "abstract": "3D Gaussian Splatting (3DGS) has emerged as a powerful representation due to\nits efficiency and high-fidelity rendering. However, 3DGS training requires a\nknown camera pose for each input view, typically obtained by\nStructure-from-Motion (SfM) pipelines. Pioneering works have attempted to relax\nthis restriction but still face difficulties when handling long sequences with\ncomplex camera trajectories. In this work, we propose Rob-GS, a robust\nframework to progressively estimate camera poses and optimize 3DGS for\narbitrarily long video sequences. Leveraging the inherent continuity of videos,\nwe design an adjacent pose tracking method to ensure stable pose estimation\nbetween consecutive frames. To handle arbitrarily long inputs, we adopt a\n\"divide and conquer\" scheme that adaptively splits the video sequence into\nseveral segments and optimizes them separately. Extensive experiments on the\nTanks and Temples dataset and our collected real-world dataset show that our\nRob-GS outperforms the state-of-the-arts."
    },
    {
        "date": "2025-01",
        "title": "Towards Robust Unsupervised Attention Prediction in Autonomous Driving",
        "author": "Mengshi Qi, Xiaoyang Bi, Pengfei Zhu, and Huadong Ma",
        "link": "http://arxiv.org/abs/2501.15045v1",
        "abstract": "Robustly predicting attention regions of interest for self-driving systems is\ncrucial for driving safety but presents significant challenges due to the\nlabor-intensive nature of obtaining large-scale attention labels and the domain\ngap between self-driving scenarios and natural scenes. These challenges are\nfurther exacerbated by complex traffic environments, including camera\ncorruption under adverse weather, noise interferences, and central bias from\nlong-tail distributions. To address these issues, we propose a robust\nunsupervised attention prediction method. An Uncertainty Mining Branch refines\npredictions by analyzing commonalities and differences across multiple\npre-trained models on natural scenes, while a Knowledge Embedding Block bridges\nthe domain gap by incorporating driving knowledge to adaptively enhance\npseudo-labels. Additionally, we introduce RoboMixup, a novel data augmentation\nmethod that improves robustness against corruption through soft attention and\ndynamic augmentation, and mitigates central bias by integrating random cropping\ninto Mixup as a regularizer.To systematically evaluate robustness in\nself-driving attention prediction, we introduce the DriverAttention-C\nbenchmark, comprising over 100k frames across three subsets: BDD-A-C,\nDR(eye)VE-C, and DADA-2000-C. Our method achieves performance equivalent to or\nsurpassing fully supervised state-of-the-art approaches on three public\ndatasets and the proposed robustness benchmark, reducing relative corruption\ndegradation by 58.8% and 52.8%, and improving central bias robustness by 12.4%\nand 11.4% in KLD and CC metrics, respectively. Code and data are available at\nhttps://github.com/zaplm/DriverAttention."
    },
    {
        "date": "2025-01",
        "title": "A Portable and Stealthy Inaudible Voice Attack Based on Acoustic Metamaterials",
        "author": "Zhiyuan Ning, Juan He, Zhanyong Tang, Weihang Hu, and Xiaojiang Chen",
        "link": "http://arxiv.org/abs/2501.15031v1",
        "abstract": "We present METAATTACK, the first approach to leverage acoustic metamaterials\nfor inaudible attacks for voice control systems. Compared to the\nstate-of-the-art inaudible attacks requiring complex and large speaker setups,\nMETAATTACK achieves a longer attacking range and higher accuracy using a\ncompact, portable device small enough to be put into a carry bag. These\nimprovements in portability and stealth have led to the practical applicability\nof inaudible attacks and their adaptation to a wider range of scenarios. We\ndemonstrate how the recent advancement in metamaterials can be utilized to\ndesign a voice attack system with carefully selected implementation parameters\nand commercial off-the-shelf components. We showcase that METAATTACK can be\nused to launch inaudible attacks for representative voice-controlled personal\nassistants, including Siri, Alexa, Google Assistant, XiaoAI, and Xiaoyi. The\naverage word accuracy of all assistants is 76%, with a range of 8.85 m."
    },
    {
        "date": "2025-01",
        "title": "Towards Distributed Backdoor Attacks with Network Detection in Decentralized Federated Learning",
        "author": "Bohan Liu, Yang Xiao, Ruimeng Ye, Zinan Ling, Xiaolong Ma, and Bo Hui",
        "link": "http://arxiv.org/abs/2501.15005v1",
        "abstract": "Distributed backdoor attacks (DBA) have shown a higher attack success rate\nthan centralized attacks in centralized federated learning (FL). However, it\nhas not been investigated in the decentralized FL. In this paper, we\nexperimentally demonstrate that, while directly applying DBA to decentralized\nFL, the attack success rate depends on the distribution of attackers in the\nnetwork architecture. Considering that the attackers can not decide their\nlocation, this paper aims to achieve a high attack success rate regardless of\nthe attackers' location distribution. Specifically, we first design a method to\ndetect the network by predicting the distance between any two attackers on the\nnetwork. Then, based on the distance, we organize the attackers in different\nclusters. Lastly, we propose an algorithm to \\textit{dynamically} embed local\npatterns decomposed from a global pattern into the different attackers in each\ncluster. We conduct a thorough empirical investigation and find that our method\ncan, in benchmark datasets, outperform both centralized attacks and naive DBA\nin different decentralized frameworks."
    },
    {
        "date": "2025-01",
        "title": "VideoPure: Diffusion-based Adversarial Purification for Video Recognition",
        "author": "Kaixun Jiang, Zhaoyu Chen, Jiyuan Fu, Lingyi Hong, Jinglun Li, and Wenqiang Zhang",
        "link": "http://arxiv.org/abs/2501.14999v1",
        "abstract": "Recent work indicates that video recognition models are vulnerable to\nadversarial examples, posing a serious security risk to downstream\napplications. However, current research has primarily focused on adversarial\nattacks, with limited work exploring defense mechanisms. Furthermore, due to\nthe spatial-temporal complexity of videos, existing video defense methods face\nissues of high cost, overfitting, and limited defense performance. Recently,\ndiffusion-based adversarial purification methods have achieved robust defense\nperformance in the image domain. However, due to the additional temporal\ndimension in videos, directly applying these diffusion-based adversarial\npurification methods to the video domain suffers performance and efficiency\ndegradation. To achieve an efficient and effective video adversarial defense\nmethod, we propose the first diffusion-based video purification framework to\nimprove video recognition models' adversarial robustness: VideoPure. Given an\nadversarial example, we first employ temporal DDIM inversion to transform the\ninput distribution into a temporally consistent and trajectory-defined\ndistribution, covering adversarial noise while preserving more video structure.\nThen, during DDIM denoising, we leverage intermediate results at each denoising\nstep and conduct guided spatial-temporal optimization, removing adversarial\nnoise while maintaining temporal consistency. Finally, we input the list of\noptimized intermediate results into the video recognition model for multi-step\nvoting to obtain the predicted class. We investigate the defense performance of\nour method against black-box, gray-box, and adaptive attacks on benchmark\ndatasets and models. Compared with other adversarial purification methods, our\nmethod overall demonstrates better defense performance against different\nattacks. Our code is available at https://github.com/deep-kaixun/VideoPure."
    },
    {
        "date": "2025-01",
        "title": "Robust Cross-Etiology and Speaker-Independent Dysarthric Speech Recognition",
        "author": "Satwinder Singh, Qianli Wang, Zihan Zhong, Clarion Mendes, Mark Hasegawa-Johnson, Waleed Abdulla, and Seyed Reza Shahamiri",
        "link": "http://arxiv.org/abs/2501.14994v1",
        "abstract": "In this paper, we present a speaker-independent dysarthric speech recognition\nsystem, with a focus on evaluating the recently released Speech Accessibility\nProject (SAP-1005) dataset, which includes speech data from individuals with\nParkinson's disease (PD). Despite the growing body of research in dysarthric\nspeech recognition, many existing systems are speaker-dependent and adaptive,\nlimiting their generalizability across different speakers and etiologies. Our\nprimary objective is to develop a robust speaker-independent model capable of\naccurately recognizing dysarthric speech, irrespective of the speaker.\nAdditionally, as a secondary objective, we aim to test the cross-etiology\nperformance of our model by evaluating it on the TORGO dataset, which contains\nspeech samples from individuals with cerebral palsy (CP) and amyotrophic\nlateral sclerosis (ALS). By leveraging the Whisper model, our\nspeaker-independent system achieved a CER of 6.99% and a WER of 10.71% on the\nSAP-1005 dataset. Further, in cross-etiology settings, we achieved a CER of\n25.08% and a WER of 39.56% on the TORGO dataset. These results highlight the\npotential of our approach to generalize across unseen speakers and different\netiologies of dysarthria."
    },
    {
        "date": "2025-01",
        "title": "Decision Making in Changing Environments: Robustness, Query-Based Learning, and Differential Privacy",
        "author": "Fan Chen, and Alexander Rakhlin",
        "link": "http://arxiv.org/abs/2501.14928v1",
        "abstract": "We study the problem of interactive decision making in which the underlying\nenvironment changes over time subject to given constraints. We propose a\nframework, which we call \\textit{hybrid Decision Making with Structured\nObservations} (hybrid DMSO), that provides an interpolation between the\nstochastic and adversarial settings of decision making. Within this framework,\nwe can analyze local differentially private (LDP) decision making, query-based\nlearning (in particular, SQ learning), and robust and smooth decision making\nunder the same umbrella, deriving upper and lower bounds based on variants of\nthe Decision-Estimation Coefficient (DEC). We further establish strong\nconnections between the DEC's behavior, the SQ dimension, local minimax\ncomplexity, learnability, and joint differential privacy. To showcase the\nframework's power, we provide new results for contextual bandits under the LDP\nconstraint."
    },
    {
        "date": "2025-01",
        "title": "A Note on Implementation Errors in Recent Adaptive Attacks Against Multi-Resolution Self-Ensembles",
        "author": "Stanislav Fort",
        "link": "http://arxiv.org/abs/2501.14496v1",
        "abstract": "This note documents an implementation issue in recent adaptive attacks (Zhang\net al. [2024]) against the multi-resolution self-ensemble defense (Fort and\nLakshminarayanan [2024]). The implementation allowed adversarial perturbations\nto exceed the standard $L_\\infty = 8/255$ bound by up to a factor of\n20$\\times$, reaching magnitudes of up to $L_\\infty = 160/255$. When attacks are\nproperly constrained within the intended bounds, the defense maintains\nnon-trivial robustness. Beyond highlighting the importance of careful\nvalidation in adversarial machine learning research, our analysis reveals an\nintriguing finding: properly bounded adaptive attacks against strong\nmulti-resolution self-ensembles often align with human perception, suggesting\nthe need to reconsider how we measure adversarial robustness."
    },
    {
        "date": "2025-01",
        "title": "Learning more with the same effort: how randomization improves the robustness of a robotic deep reinforcement learning agent",
        "author": "Luc\u00eda G\u00fcitta-L\u00f3pez, Jaime Boal, and \u00c1lvaro J. L\u00f3pez-L\u00f3pez",
        "link": "http://arxiv.org/abs/2501.14443v1",
        "abstract": "The industrial application of Deep Reinforcement Learning (DRL) is frequently\nslowed down because of the inability to generate the experience required to\ntrain the models. Collecting data often involves considerable time and economic\neffort that is unaffordable in most cases. Fortunately, devices like robots can\nbe trained with synthetic experience thanks to virtual environments. With this\napproach, the sample efficiency problems of artificial agents are mitigated,\nbut another issue arises: the need for efficiently transferring the synthetic\nexperience into the real world (sim-to-real).\n  This paper analyzes the robustness of a state-of-the-art sim-to-real\ntechnique known as progressive neural networks (PNNs) and studies how adding\ndiversity to the synthetic experience can complement it. To better understand\nthe drivers that lead to a lack of robustness, the robotic agent is still\ntested in a virtual environment to ensure total control on the divergence\nbetween the simulated and real models.\n  The results show that a PNN-like agent exhibits a substantial decrease in its\nrobustness at the beginning of the real training phase. Randomizing certain\nvariables during simulation-based training significantly mitigates this issue.\nOn average, the increase in the model's accuracy is around 25% when diversity\nis introduced in the training process. This improvement can be translated into\na decrease in the required real experience for the same final robustness\nperformance. Notwithstanding, adding real experience to agents should still be\nbeneficial regardless of the quality of the virtual experience fed into the\nagent."
    },
    {
        "date": "2025-01",
        "title": "Thunderdome: Timelock-Free Rationally-Secure Virtual Channels",
        "author": "Zeta Avarikioti, Yuheng Wang, and Yuyi Wang",
        "link": "http://arxiv.org/abs/2501.14418v2",
        "abstract": "Payment channel networks (PCNs) offer a promising solution to address the\nlimited transaction throughput of deployed blockchains. However, several\nattacks have recently been proposed that stress the vulnerability of PCNs to\ntimelock and censoring attacks. To address such attacks, we introduce\nThunderdome, the first timelock-free PCN. Instead, Thunderdome leverages the\ndesign rationale of virtual channels to extend a timelock-free payment channel\nprimitive, thereby enabling multi-hop transactions without timelocks. Previous\nworks either utilize timelocks or do not accommodate transactions between\nparties that do not share a channel.\n  At its core, Thunderdome relies on a committee of non-trusted watchtowers,\nknown as wardens, who ensure that no honest party loses funds, even when\noffline, during the channel closure process. We introduce tailored incentive\nmechanisms to ensure that all participants follow the protocol's correct\nexecution. Besides a traditional security proof that assumes an honest majority\nof the committee, we conduct a formal game-theoretic analysis to demonstrate\nthe security of Thunderdome when all participants, including wardens, act\nrationally. We implement a proof of concept of Thunderdome on Ethereum to\nvalidate its feasibility and evaluate its costs. Our evaluation shows that\ndeploying Thunderdome, including opening the underlying payment channel, costs\napproximately \\$15 (0.0089 ETH), while the worst-case cost for closing a\nchannel is about \\$7 (0.004 ETH)."
    },
    {
        "date": "2025-01",
        "title": "Online Inverse Linear Optimization: Improved Regret Bound, Robustness to Suboptimality, and Toward Tight Regret Analysis",
        "author": "Shinsaku Sakaue, Taira Tsuchiya, Han Bao, and Taihei Oki",
        "link": "http://arxiv.org/abs/2501.14349v2",
        "abstract": "We study an online learning problem where, over $T$ rounds, a learner\nobserves both time-varying sets of feasible actions and an agent's optimal\nactions, selected by solving linear optimization over the feasible actions. The\nlearner sequentially makes predictions of the agent's underlying linear\nobjective function, and their quality is measured by the regret, the cumulative\ngap between optimal objective values and those achieved by following the\nlearner's predictions. A seminal work by B\\\"armann et al. (ICML 2017) showed\nthat online learning methods can be applied to this problem to achieve regret\nbounds of $O(\\sqrt{T})$. Recently, Besbes et al. (COLT 2021, Oper. Res. 2023)\nsignificantly improved the result by achieving an $O(n^4\\ln T)$ regret bound,\nwhere $n$ is the dimension of the ambient space of objective vectors. Their\nmethod, based on the ellipsoid method, runs in polynomial time but is\ninefficient for large $n$ and $T$. In this paper, we obtain an $O(n\\ln T)$\nregret bound, improving upon the previous bound of $O(n^4\\ln T)$ by a factor of\n$n^3$. Our method is simple and efficient: we apply the online Newton step\n(ONS) to appropriate exp-concave loss functions. Moreover, for the case where\nthe agent's actions are possibly suboptimal, we establish an $O(n\\ln\nT+\\sqrt{\\Delta_Tn\\ln T})$ regret bound, where $\\Delta_T$ is the cumulative\nsuboptimality of the agent's actions. This bound is achieved by using MetaGrad,\nwhich runs ONS with $\\Theta(\\ln T)$ different learning rates in parallel. We\nalso provide a simple instance that implies an $\\Omega(n)$ lower bound, showing\nthat our $O(n\\ln T)$ bound is tight up to an $O(\\ln T)$ factor. This gives rise\nto a natural question: can the $O(\\ln T)$ factor in the upper bound be removed?\nFor the special case of $n=2$, we show that an $O(1)$ regret bound is possible,\nwhile we delineate challenges in extending this result to higher dimensions."
    },
    {
        "date": "2025-01",
        "title": "Securing DRAM at Scale: ARFM-Driven Row Hammer Defense with Unveiling the Threat of Short tRC Patterns",
        "author": "Nogeun Joo, Donghyuk Kim, Hyunjun Cho, Junseok Noh, Dongha Jung, and Joo-Young Kim",
        "link": "http://arxiv.org/abs/2501.14328v1",
        "abstract": "To address the issue of powerful row hammer (RH) attacks, our study involved\nan extensive analysis of the prevalent attack patterns in the field. We\ndiscovered a strong correlation between the timing and density of the\nactive-to-active command period, ${tRC}$, and the likelihood of RH attacks. In\nthis paper, we introduce MARC, an innovative ARFM-driven RH mitigation IP that\nsignificantly reinforces existing RH mitigation IPs. MARC dynamically adjusts\nthe frequency of RFM in response to the severity of the RH attack environment,\noffering a tailored security solution that not only detects the threats but\nalso adapts to varying threat levels. MARC's detection mechanism has\ndemonstrated remarkable efficiency, identifying over 99\\% of attack patterns.\nMoreover, MARC is designed as a compact hardware module, facilitating tight\nintegration either on the memory controller-side or DRAM-side within the memory\nsystem. It only occupies a negligible hardware area of 3363~\\textit{$\\mu m^2$}.\nBy activating ARFM based on MARC's detection, the additional energy overhead is\nalso negligible in normal workloads. We conduct experiments to compare the\nhighest row count throughout the patterns, defined as max exposure, between the\nvanilla RH mitigation IPs and the MARC-enhanced versions of the same IPs,\nfocusing on both DRAM-side and memory controller-side. On the DRAM-side, MARC +\nprobabilistic scheme and MARC + counter-based tracking scheme achieve\n8.1$\\times$ and 1.5$\\times$ improvement in max exposure ratio compared to the\nvanilla IPs, respectively. On the memory controller-side, the MARC + PARA and\nMARC + Graphene achieve 50$\\times$ and 5.7$\\times$ improvement in max exposure\nratio compared to the vanilla IPs, respectively. MARC ensures optimal security\nwithout sacrificing system performance, making MARC a pioneering solution in\nthe realm of RH attack mitigation."
    },
    {
        "date": "2025-01",
        "title": "Relative Layer-Wise Relevance Propagation: a more Robust Neural Networks eXplaination",
        "author": "Eric Nyiri, and Olivier Gibaru",
        "link": "http://arxiv.org/abs/2501.14322v1",
        "abstract": "Machine learning methods are solving very successfully a plethora of tasks,\nbut they have the disadvantage of not providing any information about their\ndecision. Consequently, estimating the reasoning of the system provides\nadditional information. For this, Layer-Wise Relevance Propagation (LRP) is one\nof the methods in eXplainable Machine Learning (XML). Its purpose is to provide\ncontributions of any neural network output in the domain of its input. The main\ndrawback of current methods is mainly due to division by small values. To\novercome this problem, we provide a new definition called Relative LRP where\nthe classical conservation law is satisfied up to a multiplicative factor but\nwithout divisions by small values except for Resnet skip connection. In this\narticle, we will focus on image classification. This allows us to visualize the\ncontributions of a pixel to the predictions of a multi-layer neural network.\nPixel contributions provide a focus to further analysis on regions of potential\ninterest. R-LRP can be applied for any dense, CNN or residual neural networks.\nMoreover, R-LRP doesn't need any hyperparameters to tune contrary to other LRP\nmethods. We then compare the R-LRP method on different datasets with simple\nCNN, VGG16, VGG19 and Resnet50 networks."
    },
    {
        "date": "2025-01",
        "title": "Scalable Benchmarking and Robust Learning for Noise-Free Ego-Motion and 3D Reconstruction from Noisy Video",
        "author": "Xiaohao Xu, Tianyi Zhang, Shibo Zhao, Xiang Li, Sibo Wang, Yongqi Chen, Ye Li, Bhiksha Raj, Matthew Johnson-Roberson, Sebastian Scherer, and Xiaonan Huang",
        "link": "http://arxiv.org/abs/2501.14319v1",
        "abstract": "We aim to redefine robust ego-motion estimation and photorealistic 3D\nreconstruction by addressing a critical limitation: the reliance on noise-free\ndata in existing models. While such sanitized conditions simplify evaluation,\nthey fail to capture the unpredictable, noisy complexities of real-world\nenvironments. Dynamic motion, sensor imperfections, and synchronization\nperturbations lead to sharp performance declines when these models are deployed\nin practice, revealing an urgent need for frameworks that embrace and excel\nunder real-world noise. To bridge this gap, we tackle three core challenges:\nscalable data generation, comprehensive benchmarking, and model robustness\nenhancement. First, we introduce a scalable noisy data synthesis pipeline that\ngenerates diverse datasets simulating complex motion, sensor imperfections, and\nsynchronization errors. Second, we leverage this pipeline to create\nRobust-Ego3D, a benchmark rigorously designed to expose noise-induced\nperformance degradation, highlighting the limitations of current learning-based\nmethods in ego-motion accuracy and 3D reconstruction quality. Third, we propose\nCorrespondence-guided Gaussian Splatting (CorrGS), a novel test-time adaptation\nmethod that progressively refines an internal clean 3D representation by\naligning noisy observations with rendered RGB-D frames from clean 3D map,\nenhancing geometric alignment and appearance restoration through visual\ncorrespondence. Extensive experiments on synthetic and real-world data\ndemonstrate that CorrGS consistently outperforms prior state-of-the-art\nmethods, particularly in scenarios involving rapid motion and dynamic\nillumination."
    },
    {
        "date": "2025-01",
        "title": "Distributionally Robust Coreset Selection under Covariate Shift",
        "author": "Tomonari Tanaka, Hiroyuki Hanada, Hanting Yang, Tatsuya Aoyama, Yu Inatsu, Satoshi Akahane, Yoshito Okura, Noriaki Hashimoto, Taro Murayama, Hanju Lee, Shinya Kojima, and Ichiro Takeuchi",
        "link": "http://arxiv.org/abs/2501.14253v1",
        "abstract": "Coreset selection, which involves selecting a small subset from an existing\ntraining dataset, is an approach to reducing training data, and various\napproaches have been proposed for this method. In practical situations where\nthese methods are employed, it is often the case that the data distributions\ndiffer between the development phase and the deployment phase, with the latter\nbeing unknown. Thus, it is challenging to select an effective subset of\ntraining data that performs well across all deployment scenarios. We therefore\npropose Distributionally Robust Coreset Selection (DRCS). DRCS theoretically\nderives an estimate of the upper bound for the worst-case test error, assuming\nthat the future covariate distribution may deviate within a defined range from\nthe training distribution. Furthermore, by selecting instances in a way that\nsuppresses the estimate of the upper bound for the worst-case test error, DRCS\nachieves distributionally robust training instance selection. This study is\nprimarily applicable to convex training computation, but we demonstrate that it\ncan also be applied to deep learning under appropriate approximations. In this\npaper, we focus on covariate shift, a type of data distribution shift, and\ndemonstrate the effectiveness of DRCS through experiments."
    },
    {
        "date": "2025-01",
        "title": "Siren: A Learning-Based Multi-Turn Attack Framework for Simulating Real-World Human Jailbreak Behaviors",
        "author": "Yi Zhao, and Youzhi Zhang",
        "link": "http://arxiv.org/abs/2501.14250v1",
        "abstract": "Large language models (LLMs) are widely used in real-world applications,\nraising concerns about their safety and trustworthiness. While red-teaming with\njailbreak prompts exposes the vulnerabilities of LLMs, current efforts focus\nprimarily on single-turn attacks, overlooking the multi-turn strategies used by\nreal-world adversaries. Existing multi-turn methods rely on static patterns or\npredefined logical chains, failing to account for the dynamic strategies during\nattacks. We propose Siren, a learning-based multi-turn attack framework\ndesigned to simulate real-world human jailbreak behaviors. Siren consists of\nthree stages: (1) training set construction utilizing Turn-Level LLM feedback\n(Turn-MF), (2) post-training attackers with supervised fine-tuning (SFT) and\ndirect preference optimization (DPO), and (3) interactions between the\nattacking and target LLMs. Experiments demonstrate that Siren achieves an\nattack success rate (ASR) of 90% with LLaMA-3-8B as the attacker against\nGemini-1.5-Pro as the target model, and 70% with Mistral-7B against GPT-4o,\nsignificantly outperforming single-turn baselines. Moreover, Siren with a\n7B-scale model achieves performance comparable to a multi-turn baseline that\nleverages GPT-4o as the attacker, while requiring fewer turns and employing\ndecomposition strategies that are better semantically aligned with attack\ngoals. We hope Siren inspires the development of stronger defenses against\nadvanced multi-turn jailbreak attacks under realistic scenarios. Code is\navailable at https://github.com/YiyiyiZhao/siren. Warning: This paper contains\npotentially harmful text."
    },
    {
        "date": "2025-01",
        "title": "GreedyPixel: Fine-Grained Black-Box Adversarial Attack Via Greedy Algorithm",
        "author": "Hanrui Wang, Ching-Chun Chang, Chun-Shien Lu, Christopher Leckie, and Isao Echizen",
        "link": "http://arxiv.org/abs/2501.14230v1",
        "abstract": "A critical requirement for deep learning models is ensuring their robustness\nagainst adversarial attacks. These attacks commonly introduce noticeable\nperturbations, compromising the visual fidelity of adversarial examples.\nAnother key challenge is that while white-box algorithms can generate effective\nadversarial perturbations, they require access to the model gradients, limiting\ntheir practicality in many real-world scenarios. Existing attack mechanisms\nstruggle to achieve similar efficacy without access to these gradients. In this\npaper, we introduce GreedyPixel, a novel pixel-wise greedy algorithm designed\nto generate high-quality adversarial examples using only query-based feedback\nfrom the target model. GreedyPixel improves computational efficiency in what is\ntypically a brute-force process by perturbing individual pixels in sequence,\nguided by a pixel-wise priority map. This priority map is constructed by\nranking gradients obtained from a surrogate model, providing a structured path\nfor perturbation. Our results demonstrate that GreedyPixel achieves attack\nsuccess rates comparable to white-box methods without the need for gradient\ninformation, and surpasses existing algorithms in black-box settings, offering\nhigher success rates, reduced computational time, and imperceptible\nperturbations. These findings underscore the advantages of GreedyPixel in terms\nof attack efficacy, time efficiency, and visual quality."
    },
    {
        "date": "2025-01",
        "title": "SelfPrompt: Confidence-Aware Semi-Supervised Tuning for Robust Vision-Language Model Adaptation",
        "author": "Shuvendu Roy, and Ali Etemad",
        "link": "http://arxiv.org/abs/2501.14148v1",
        "abstract": "We present SelfPrompt, a novel prompt-tuning approach for vision-language\nmodels (VLMs) in a semi-supervised learning setup. Existing methods for tuning\nVLMs in semi-supervised setups struggle with the negative impact of the\nmiscalibrated VLMs on pseudo-labelling, and the accumulation of noisy\npseudo-labels. SelfPrompt addresses these challenges by introducing a\ncluster-guided pseudo-labelling method that improves pseudo-label accuracy, and\na confidence-aware semi-supervised learning module that maximizes the\nutilization of unlabelled data by combining supervised learning and\nweakly-supervised learning. Additionally, we investigate our method in an\nactive semi-supervised learning setup, where the labelled set is strategically\nselected to ensure the best utilization of a limited labelling budget. To this\nend, we propose a weakly-supervised sampling technique that selects a diverse\nand representative labelled set, which can be seamlessly integrated into\nexisting methods to enhance their performance. We conduct extensive evaluations\nacross 13 datasets, significantly surpassing state-of-the-art performances with\naverage improvements of 6.23% in standard semi-supervised learning, 6.25% in\nactive semi-supervised learning, and 4.9% in base-to-novel generalization,\nusing a 2-shot setup. Furthermore, SelfPrompt shows excellent generalization in\nsingle-shot settings, achieving an average improvement of 11.78%."
    },
    {
        "date": "2025-01",
        "title": "Reinforcement Learning Platform for Adversarial Black-box Attacks with Custom Distortion Filters",
        "author": "Soumyendu Sarkar, Ashwin Ramesh Babu, Sajad Mousavi, Vineet Gundecha, Sahand Ghorbanpour, Avisek Naug, Ricardo Luna Gutierrez, and Antonio Guillen",
        "link": "http://arxiv.org/abs/2501.14122v1",
        "abstract": "We present a Reinforcement Learning Platform for Adversarial Black-box\nuntargeted and targeted attacks, RLAB, that allows users to select from various\ndistortion filters to create adversarial examples. The platform uses a\nReinforcement Learning agent to add minimum distortion to input images while\nstill causing misclassification by the target model. The agent uses a novel\ndual-action method to explore the input image at each step to identify\nsensitive regions for adding distortions while removing noises that have less\nimpact on the target model. This dual action leads to faster and more efficient\nconvergence of the attack. The platform can also be used to measure the\nrobustness of image classification models against specific distortion types.\nAlso, retraining the model with adversarial samples significantly improved\nrobustness when evaluated on benchmark datasets. The proposed platform\noutperforms state-of-the-art methods in terms of the average number of queries\nrequired to cause misclassification. This advances trustworthiness with a\npositive social impact."
    },
    {
        "date": "2025-01",
        "title": "MedSlice: Fine-Tuned Large Language Models for Secure Clinical Note Sectioning",
        "author": "Joshua Davis, Thomas Sounack, Kate Sciacca, Jessie M Brain, Brigitte N Durieux, Nicole D Agaronnik, and Charlotta Lindvall",
        "link": "http://arxiv.org/abs/2501.14105v1",
        "abstract": "Extracting sections from clinical notes is crucial for downstream analysis\nbut is challenging due to variability in formatting and labor-intensive nature\nof manual sectioning. While proprietary large language models (LLMs) have shown\npromise, privacy concerns limit their accessibility. This study develops a\npipeline for automated note sectioning using open-source LLMs, focusing on\nthree sections: History of Present Illness, Interval History, and Assessment\nand Plan. We fine-tuned three open-source LLMs to extract sections using a\ncurated dataset of 487 progress notes, comparing results relative to\nproprietary models (GPT-4o, GPT-4o mini). Internal and external validity were\nassessed via precision, recall and F1 score. Fine-tuned Llama 3.1 8B\noutperformed GPT-4o (F1=0.92). On the external validity test set, performance\nremained high (F1= 0.85). Fine-tuned open-source LLMs can surpass proprietary\nmodels in clinical note sectioning, offering advantages in cost, performance,\nand accessibility."
    },
    {
        "date": "2025-01",
        "title": "Towards Robust Multimodal Open-set Test-time Adaptation via Adaptive Entropy-aware Optimization",
        "author": "Hao Dong, Eleni Chatzi, and Olga Fink",
        "link": "http://arxiv.org/abs/2501.13924v1",
        "abstract": "Test-time adaptation (TTA) has demonstrated significant potential in\naddressing distribution shifts between training and testing data. Open-set\ntest-time adaptation (OSTTA) aims to adapt a source pre-trained model online to\nan unlabeled target domain that contains unknown classes. This task becomes\nmore challenging when multiple modalities are involved. Existing methods have\nprimarily focused on unimodal OSTTA, often filtering out low-confidence samples\nwithout addressing the complexities of multimodal data. In this work, we\npresent Adaptive Entropy-aware Optimization (AEO), a novel framework\nspecifically designed to tackle Multimodal Open-set Test-time Adaptation\n(MM-OSTTA) for the first time. Our analysis shows that the entropy difference\nbetween known and unknown samples in the target domain strongly correlates with\nMM-OSTTA performance. To leverage this, we propose two key components:\nUnknown-aware Adaptive Entropy Optimization (UAE) and Adaptive Modality\nPrediction Discrepancy Optimization (AMP). These components enhance the ability\nof model to distinguish unknown class samples during online adaptation by\namplifying the entropy difference between known and unknown samples. To\nthoroughly evaluate our proposed methods in the MM-OSTTA setting, we establish\na new benchmark derived from existing datasets. This benchmark includes two\ndownstream tasks and incorporates five modalities. Extensive experiments across\nvarious domain shift situations demonstrate the efficacy and versatility of the\nAEO framework. Additionally, we highlight the strong performance of AEO in\nlong-term and continual MM-OSTTA settings, both of which are challenging and\nhighly relevant to real-world applications. Our source code is available at\nhttps://github.com/donghao51/AEO."
    },
    {
        "date": "2025-01",
        "title": "Logical Maneuvers: Detecting and Mitigating Adversarial Hardware Faults in Space",
        "author": "Fatemeh Khojasteh Dana, Saleh Khalaj Monfared, and Shahin Tajik",
        "link": "http://arxiv.org/abs/2501.13894v1",
        "abstract": "Satellites are highly vulnerable to adversarial glitches or high-energy\nradiation in space, which could cause faults on the onboard computer. Various\nradiation- and fault-tolerant methods, such as error correction codes (ECC) and\nredundancy-based approaches, have been explored over the last decades to\nmitigate temporary soft errors on software and hardware. However, conventional\nECC methods fail to deal with hard errors or permanent faults in the hardware\ncomponents. This work introduces a detection- and response-based countermeasure\nto deal with partially damaged processor chips. It recovers the processor chip\nfrom permanent faults and enables continuous operation with available undamaged\nresources on the chip. We incorporate digitally-compatible delay-based sensors\non the target processor's chip to reliably detect the incoming radiation or\nglitching attempts on the physical fabric of the chip, even before a fault\noccurs. Upon detecting a fault in one or more components of the processor's\narithmetic logic unit (ALU), our countermeasure employs adaptive software\nrecompilations to resynthesize and substitute the affected instructions with\ninstructions of still functioning components to accomplish the task.\nFurthermore, if the fault is more widespread and prevents the correct operation\nof the entire processor, our approach deploys adaptive hardware partial\nreconfigurations to replace and reroute the failed components to undamaged\nlocations of the chip. To validate our claims, we deploy a high-energy\nnear-infrared (NIR) laser beam on a RISC-V processor implemented on a 28~nm\nFPGA to emulate radiation and even hard errors by partially damaging the FPGA\nfabric. We demonstrate that our sensor can confidently detect the radiation and\ntrigger the processor testing and fault recovery mechanisms. Finally, we\ndiscuss the overhead imposed by our countermeasure."
    },
    {
        "date": "2025-01",
        "title": "PhotoGAN: Generative Adversarial Neural Network Acceleration with Silicon Photonics",
        "author": "Tharini Suresh, Salma Afifi, and Sudeep Pasricha",
        "link": "http://arxiv.org/abs/2501.13828v1",
        "abstract": "Generative Adversarial Networks (GANs) are at the forefront of AI innovation,\ndriving advancements in areas such as image synthesis, medical imaging, and\ndata augmentation. However, the unique computational operations within GANs,\nsuch as transposed convolutions and instance normalization, introduce\nsignificant inefficiencies when executed on traditional electronic\naccelerators, resulting in high energy consumption and suboptimal performance.\nTo address these challenges, we introduce PhotoGAN, the first silicon-photonic\naccelerator designed to handle the specialized operations of GAN models. By\nleveraging the inherent high throughput and energy efficiency of silicon\nphotonics, PhotoGAN offers an innovative, reconfigurable architecture capable\nof accelerating transposed convolutions and other GAN-specific layers. The\naccelerator also incorporates a sparse computation optimization technique to\nreduce redundant operations, improving computational efficiency. Our\nexperimental results demonstrate that PhotoGAN achieves at least 4.4x higher\nGOPS and 2.18x lower energy-per-bit (EPB) compared to state-of-the-art\naccelerators, including GPUs and TPUs. These findings showcase PhotoGAN as a\npromising solution for the next generation of GAN acceleration, providing\nsubstantial gains in both performance and energy efficiency."
    },
    {
        "date": "2025-01",
        "title": "WAFBOOSTER: Automatic Boosting of WAF Security Against Mutated Malicious Payloads",
        "author": "Cong Wu, Jing Chen, Simeng Zhu, Wenqi Feng, Ruiying Du, and Yang Xiang",
        "link": "http://arxiv.org/abs/2501.14008v1",
        "abstract": "Web application firewall (WAF) examines malicious traffic to and from a web\napplication via a set of security rules. It plays a significant role in\nsecuring Web applications against web attacks. However, as web attacks grow in\nsophistication, it is becoming increasingly difficult for WAFs to block the\nmutated malicious payloads designed to bypass their defenses. In response to\nthis critical security issue, we have developed a novel learning-based\nframework called WAFBOOSTER, designed to unveil potential bypasses in WAF\ndetections and suggest rules to fortify their security. Using a combination of\nshadow models and payload generation techniques, we can identify malicious\npayloads and remove or modify them as needed. WAFBOOSTER generates signatures\nfor these malicious payloads using advanced clustering and regular expression\nmatching techniques to repair any security gaps we uncover. In our\ncomprehensive evaluation of eight real-world WAFs, WAFBOOSTER improved the true\nrejection rate of mutated malicious payloads from 21% to 96%, with no false\nrejections. WAFBOOSTER achieves a false acceptance rate 3X lower than\nstate-of-the-art methods for generating malicious payloads. With WAFBOOSTER, we\nhave taken a step forward in securing web applications against the\never-evolving threats."
    },
    {
        "date": "2025-01",
        "title": "Defending against Adversarial Malware Attacks on ML-based Android Malware Detection Systems",
        "author": "Ping He, Lorenzo Cavallaro, and Shouling Ji",
        "link": "http://arxiv.org/abs/2501.13782v1",
        "abstract": "Android malware presents a persistent threat to users' privacy and data\nintegrity. To combat this, researchers have proposed machine learning-based\n(ML-based) Android malware detection (AMD) systems. However, adversarial\nAndroid malware attacks compromise the detection integrity of the ML-based AMD\nsystems, raising significant concerns. Existing defenses against adversarial\nAndroid malware provide protections against feature space attacks which\ngenerate adversarial feature vectors only, leaving protection against realistic\nthreats from problem space attacks which generate real adversarial malware an\nopen problem. In this paper, we address this gap by proposing ADD, a practical\nadversarial Android malware defense framework designed as a plug-in to enhance\nthe adversarial robustness of the ML-based AMD systems against problem space\nattacks. Our extensive evaluation across various ML-based AMD systems\ndemonstrates that ADD is effective against state-of-the-art problem space\nadversarial Android malware attacks. Additionally, ADD shows the defense\neffectiveness in enhancing the adversarial robustness of real-world antivirus\nsolutions."
    },
    {
        "date": "2025-01",
        "title": "Crossfire: An Elastic Defense Framework for Graph Neural Networks Under Bit Flip Attacks",
        "author": "Lorenz Kummer, Samir Moustafa, Wilfried Gansterer, and Nils Kriege",
        "link": "http://arxiv.org/abs/2501.13776v1",
        "abstract": "Bit Flip Attacks (BFAs) are a well-established class of adversarial attacks,\noriginally developed for Convolutional Neural Networks within the computer\nvision domain. Most recently, these attacks have been extended to target Graph\nNeural Networks (GNNs), revealing significant vulnerabilities. This new\ndevelopment naturally raises questions about the best strategies to defend GNNs\nagainst BFAs, a challenge for which no solutions currently exist. Given the\napplications of GNNs in critical fields, any defense mechanism must not only\nmaintain network performance, but also verifiably restore the network to its\npre-attack state. Verifiably restoring the network to its pre-attack state also\neliminates the need for costly evaluations on test data to ensure network\nquality. We offer first insights into the effectiveness of existing honeypot-\nand hashing-based defenses against BFAs adapted from the computer vision domain\nto GNNs, and characterize the shortcomings of these approaches. To overcome\ntheir limitations, we propose Crossfire, a hybrid approach that exploits weight\nsparsity and combines hashing and honeypots with bit-level correction of\nout-of-distribution weight elements to restore network integrity. Crossfire is\nretraining-free and does not require labeled data. Averaged over 2,160\nexperiments on six benchmark datasets, Crossfire offers a 21.8% higher\nprobability than its competitors of reconstructing a GNN attacked by a BFA to\nits pre-attack state. These experiments cover up to 55 bit flips from various\nattacks. Moreover, it improves post-repair prediction quality by 10.85%.\nComputational and storage overheads are negligible compared to the inherent\ncomplexity of even the simplest GNNs."
    },
    {
        "date": "2025-01",
        "title": "2-Tier SimCSE: Elevating BERT for Robust Sentence Embeddings",
        "author": "Yumeng Wang, Ziran Zhou, and Junjin Wang",
        "link": "http://arxiv.org/abs/2501.13758v1",
        "abstract": "Effective sentence embeddings that capture semantic nuances and generalize\nwell across diverse contexts are crucial for natural language processing tasks.\nWe address this challenge by applying SimCSE (Simple Contrastive Learning of\nSentence Embeddings) using contrastive learning to fine-tune the minBERT model\nfor sentiment analysis, semantic textual similarity (STS), and paraphrase\ndetection. Our contributions include experimenting with three different dropout\ntechniques, namely standard dropout, curriculum dropout, and adaptive dropout,\nto tackle overfitting, proposing a novel 2-Tier SimCSE Fine-tuning Model that\ncombines both unsupervised and supervised SimCSE on STS task, and exploring\ntransfer learning potential for Paraphrase and SST tasks. Our findings\ndemonstrate the effectiveness of SimCSE, with the 2-Tier model achieving\nsuperior performance on the STS task, with an average test score of 0.742\nacross all three downstream tasks. The results of error analysis reveals\nchallenges in handling complex sentiments and reliance on lexical overlap for\nparaphrase detection, highlighting areas for future research. The ablation\nstudy revealed that removing Adaptive Dropout in the Single-Task Unsupervised\nSimCSE Model led to improved performance on the STS task, indicating\noverfitting due to added parameters. Transfer learning from SimCSE models on\nParaphrase and SST tasks did not enhance performance, suggesting limited\ntransferability of knowledge from the STS task."
    },
    {
        "date": "2025-01",
        "title": "Exact Soft Analytical Side-Channel Attacks using Tractable Circuits",
        "author": "Thomas Wedenig, Rishub Nagpal, Ga\u00ebtan Cassiers, Stefan Mangard, and Robert Peharz",
        "link": "http://arxiv.org/abs/2501.13748v1",
        "abstract": "Detecting weaknesses in cryptographic algorithms is of utmost importance for\ndesigning secure information systems. The state-of-the-art soft analytical\nside-channel attack (SASCA) uses physical leakage information to make\nprobabilistic predictions about intermediate computations and combines these\n\"guesses\" with the known algorithmic logic to compute the posterior\ndistribution over the key. This attack is commonly performed via loopy belief\npropagation, which, however, lacks guarantees in terms of convergence and\ninference quality. In this paper, we develop a fast and exact inference method\nfor SASCA, denoted as ExSASCA, by leveraging knowledge compilation and\ntractable probabilistic circuits. When attacking the Advanced Encryption\nStandard (AES), the most widely used encryption algorithm to date, ExSASCA\noutperforms SASCA by more than 31% top-1 success rate absolute. By leveraging\nsparse belief messages, this performance is achieved with little more\ncomputational cost than SASCA, and about 3 orders of magnitude less than exact\ninference via exhaustive enumeration. Even with dense belief messages, ExSASCA\nstill uses 6 times less computations than exhaustive inference."
    },
    {
        "date": "2025-01",
        "title": "A Comprehensive Framework for Building Highly Secure, Network-Connected Devices: Chip to App",
        "author": "Khan Reaz, and Gerhard Wunder",
        "link": "http://arxiv.org/abs/2501.13716v1",
        "abstract": "The rapid expansion of connected devices has amplified the need for robust\nand scalable security frameworks. This paper proposes a holistic approach to\nsecuring network-connected devices, covering essential layers: hardware,\nfirmware, communication, and application. At the hardware level, we focus on\nsecure key management, reliable random number generation, and protecting\ncritical assets. Firmware security is addressed through mechanisms like\ncryptographic integrity validation and secure boot processes. For secure\ncommunication, we emphasize TLS 1.3 and optimized cipher suites tailored for\nboth standard and resource-constrained devices. To overcome the challenges of\nIoT, compact digital certificates, such as CBOR, are recommended to reduce\noverhead and enhance performance. Additionally, the paper explores\nforward-looking solutions, including post-quantum cryptography, to future-proof\nsystems against emerging threats. This framework provides actionable guidelines\nfor manufacturers and system administrators to build secure devices that\nmaintain confidentiality, integrity, and availability throughout their\nlifecycle."
    },
    {
        "date": "2025-01",
        "title": "Certified Robustness Under Bounded Levenshtein Distance",
        "author": "Elias Abad Rocamora, Grigorios G. Chrysos, and Volkan Cevher",
        "link": "http://arxiv.org/abs/2501.13676v1",
        "abstract": "Text classifiers suffer from small perturbations, that if chosen\nadversarially, can dramatically change the output of the model. Verification\nmethods can provide robustness certificates against such adversarial\nperturbations, by computing a sound lower bound on the robust accuracy.\nNevertheless, existing verification methods incur in prohibitive costs and\ncannot practically handle Levenshtein distance constraints. We propose the\nfirst method for computing the Lipschitz constant of convolutional classifiers\nwith respect to the Levenshtein distance. We use these Lipschitz constant\nestimates for training 1-Lipschitz classifiers. This enables computing the\ncertified radius of a classifier in a single forward pass. Our method, LipsLev,\nis able to obtain $38.80$% and $13.93$% verified accuracy at distance $1$ and\n$2$ respectively in the AG-News dataset, while being $4$ orders of magnitude\nfaster than existing approaches. We believe our work can open the door to more\nefficient verification in the text domain."
    },
    {
        "date": "2025-01",
        "title": "Device-aware Optical Adversarial Attack for a Portable Projector-camera System",
        "author": "Ning Jiang, Yanhong Liu, Dingheng Zeng, Yue Feng, Weihong Deng, and Ying Li",
        "link": "http://arxiv.org/abs/2501.14005v1",
        "abstract": "Deep-learning-based face recognition (FR) systems are susceptible to\nadversarial examples in both digital and physical domains. Physical attacks\npresent a greater threat to deployed systems as adversaries can easily access\nthe input channel, allowing them to provide malicious inputs to impersonate a\nvictim. This paper addresses the limitations of existing projector-camera-based\nadversarial light attacks in practical FR setups. By incorporating device-aware\nadaptations into the digital attack algorithm, such as resolution-aware and\ncolor-aware adjustments, we mitigate the degradation from digital to physical\ndomains. Experimental validation showcases the efficacy of our proposed\nalgorithm against real and spoof adversaries, achieving high physical\nsimilarity scores in FR models and state-of-the-art commercial systems. On\naverage, there is only a 14% reduction in scores from digital to physical\nattacks, with high attack success rate in both white- and black-box scenarios."
    },
    {
        "date": "2025-01",
        "title": "Towards Robust Incremental Learning under Ambiguous Supervision",
        "author": "Rui Wang, Mingxuan Xia, Chang Yao, Lei Feng, Junbo Zhao, Gang Chen, and Haobo Wang",
        "link": "http://arxiv.org/abs/2501.13584v2",
        "abstract": "Traditional Incremental Learning (IL) targets to handle sequential\nfully-supervised learning problems where novel classes emerge from time to\ntime. However, due to inherent annotation uncertainty and ambiguity, collecting\nhigh-quality annotated data in a dynamic learning system can be extremely\nexpensive. To mitigate this problem, we propose a novel weakly-supervised\nlearning paradigm called Incremental Partial Label Learning (IPLL), where the\nsequentially arrived data relate to a set of candidate labels rather than the\nground truth. Technically, we develop the Prototype-Guided Disambiguation and\nReplay Algorithm (PGDR) which leverages the class prototypes as a proxy to\nmitigate two intertwined challenges in IPLL, i.e., label ambiguity and\ncatastrophic forgetting. To handle the former, PGDR encapsulates a\nmomentum-based pseudo-labeling algorithm along with prototype-guided\ninitialization, resulting in a balanced perception of classes. To alleviate\nforgetting, we develop a memory replay technique that collects\nwell-disambiguated samples while maintaining representativeness and diversity.\nBy jointly distilling knowledge from curated memory data, our framework\nexhibits a great disambiguation ability for samples of new tasks and achieves\nless forgetting of knowledge. Extensive experiments demonstrate that PGDR\nachieves superior"
    },
    {
        "date": "2025-01",
        "title": "Black-Box Adversarial Attack on Vision Language Models for Autonomous Driving",
        "author": "Lu Wang, Tianyuan Zhang, Yang Qu, Siyuan Liang, Yuwei Chen, Aishan Liu, Xianglong Liu, and Dacheng Tao",
        "link": "http://arxiv.org/abs/2501.13563v1",
        "abstract": "Vision-language models (VLMs) have significantly advanced autonomous driving\n(AD) by enhancing reasoning capabilities; however, these models remain highly\nsusceptible to adversarial attacks. While existing research has explored\nwhite-box attacks to some extent, the more practical and challenging black-box\nscenarios remain largely underexplored due to their inherent difficulty. In\nthis paper, we take the first step toward designing black-box adversarial\nattacks specifically targeting VLMs in AD. We identify two key challenges for\nachieving effective black-box attacks in this context: the effectiveness across\ndriving reasoning chains in AD systems and the dynamic nature of driving\nscenarios. To address this, we propose Cascading Adversarial Disruption (CAD).\nIt first introduces Decision Chain Disruption, which targets low-level\nreasoning breakdown by generating and injecting deceptive semantics, ensuring\nthe perturbations remain effective across the entire decision-making chain.\nBuilding on this, we present Risky Scene Induction, which addresses dynamic\nadaptation by leveraging a surrogate VLM to understand and construct high-level\nrisky scenarios that are likely to result in critical errors in the current\ndriving contexts. Extensive experiments conducted on multiple AD VLMs and\nbenchmarks demonstrate that CAD achieves state-of-the-art attack effectiveness,\nsignificantly outperforming existing methods (+13.43% on average). Moreover, we\nvalidate its practical applicability through real-world attacks on AD vehicles\npowered by VLMs, where the route completion rate drops by 61.11% and the\nvehicle crashes directly into the obstacle vehicle with adversarial patches.\nFinally, we release CADA dataset, comprising 18,808 adversarial\nvisual-question-answer pairs, to facilitate further evaluation and research in\nthis critical domain. Our codes and dataset will be available after paper's\nacceptance."
    },
    {
        "date": "2025-01",
        "title": "POPS: From History to Mitigation of DNS Cache Poisoning Attacks",
        "author": "Yehuda Afek, Harel Berger, and Anat Bremler-Barr",
        "link": "http://arxiv.org/abs/2501.13540v1",
        "abstract": "We present a novel yet simple and comprehensive DNS cache POisoning\nPrevention System (POPS), designed to integrate as a module in Intrusion\nPrevention Systems (IPS). POPS addresses statistical DNS poisoning attacks,\nincluding those documented from 2002 to the present, and offers robust\nprotection against similar future threats. It consists of two main components:\na detection module that employs three simple rules, and a mitigation module\nthat leverages the TC flag in the DNS header to enhance security. Once\nactivated, the mitigation module has zero false positives or negatives,\ncorrecting any such errors on the side of the detection module.\n  We first analyze POPS against historical DNS services and attacks, showing\nthat it would have mitigated all network-based statistical poisoning attacks,\nyielding a success rate of only 0.0076% for the adversary. We then simulate\nPOPS on traffic benchmarks (PCAPs) incorporating current potential\nnetwork-based statistical poisoning attacks, and benign PCAPs; the simulated\nattacks still succeed with a probability of 0.0076%. This occurs because five\nmalicious packets go through before POPS detects the attack and activates the\nmitigation module. In addition, POPS completes its task using only 20%-50% of\nthe time required by other tools (e.g., Suricata or Snort), and after examining\njust 5%-10% as many packets. Furthermore, it successfully identifies DNS cache\npoisoning attacks-such as fragmentation attacks-that both Suricata and Snort\nfail to detect, underscoring its superiority in providing comprehensive DNS\nprotection."
    },
    {
        "date": "2025-01",
        "title": "Overcoming Support Dilution for Robust Few-shot Semantic Segmentation",
        "author": "Wailing Tang, Biqi Yang, Pheng-Ann Heng, Yun-Hui Liu, and Chi-Wing Fu",
        "link": "http://arxiv.org/abs/2501.13529v1",
        "abstract": "Few-shot Semantic Segmentation (FSS) is a challenging task that utilizes\nlimited support images to segment associated unseen objects in query images.\nHowever, recent FSS methods are observed to perform worse, when enlarging the\nnumber of shots. As the support set enlarges, existing FSS networks struggle to\nconcentrate on the high-contributed supports and could easily be overwhelmed by\nthe low-contributed supports that could severely impair the mask predictions.\nIn this work, we study this challenging issue, called support dilution, our\ngoal is to recognize, select, preserve, and enhance those high-contributed\nsupports in the raw support pool. Technically, our method contains three novel\nparts. First, we propose a contribution index, to quantitatively estimate if a\nhigh-contributed support dilutes. Second, we develop the Symmetric Correlation\n(SC) module to preserve and enhance the high-contributed support features,\nminimizing the distraction by the low-contributed features. Third, we design\nthe Support Image Pruning operation, to retrieve a compact and high quality\nsubset by discarding low-contributed supports. We conduct extensive experiments\non two FSS benchmarks, COCO-20i and PASCAL-5i, the segmentation results\ndemonstrate the compelling performance of our solution over state-of-the-art\nFSS approaches. Besides, we apply our solution for online segmentation and\nreal-world segmentation, convincing segmentation results showing the practical\nability of our work for real-world demonstrations."
    },
    {
        "date": "2025-01",
        "title": "Robust Amortized Bayesian Inference with Self-Consistency Losses on Unlabeled Data",
        "author": "Aayush Mishra, Daniel Habermann, Marvin Schmitt, Stefan T. Radev, and Paul-Christian B\u00fcrkner",
        "link": "http://arxiv.org/abs/2501.13483v1",
        "abstract": "Neural amortized Bayesian inference (ABI) can solve probabilistic inverse\nproblems orders of magnitude faster than classical methods. However, neural ABI\nis not yet sufficiently robust for widespread and safe applicability. In\nparticular, when performing inference on observations outside of the scope of\nthe simulated data seen during training, for example, because of model\nmisspecification, the posterior approximations are likely to become highly\nbiased. Due to the bad pre-asymptotic behavior of current neural posterior\nestimators in the out-of-simulation regime, the resulting estimation biases\ncannot be fixed in acceptable time by just simulating more training data. In\nthis proof-of-concept paper, we propose a semi-supervised approach that enables\ntraining not only on (labeled) simulated data generated from the model, but\nalso on unlabeled data originating from any source, including real-world data.\nTo achieve the latter, we exploit Bayesian self-consistency properties that can\nbe transformed into strictly proper losses without requiring knowledge of true\nparameter values, that is, without requiring data labels. The results of our\ninitial experiments show remarkable improvements in the robustness of ABI on\nout-of-simulation data. Even if the observed data is far away from both labeled\nand unlabeled training data, inference remains highly accurate. If our findings\nalso generalize to other scenarios and model classes, we believe that our new\nmethod represents a major breakthrough in neural ABI."
    },
    {
        "date": "2025-01",
        "title": "Adaptive Few-Shot Learning (AFSL): Tackling Data Scarcity with Stability, Robustness, and Versatility",
        "author": "Rishabh Agrawal",
        "link": "http://arxiv.org/abs/2501.13479v1",
        "abstract": "Few-shot learning (FSL) enables machine learning models to generalize\neffectively with minimal labeled data, making it crucial for data-scarce\ndomains such as healthcare, robotics, and natural language processing. Despite\nits potential, FSL faces challenges including sensitivity to initialization,\ndifficulty in adapting to diverse domains, and vulnerability to noisy datasets.\nTo address these issues, this paper introduces Adaptive Few-Shot Learning\n(AFSL), a framework that integrates advancements in meta-learning, domain\nalignment, noise resilience, and multi-modal integration. AFSL consists of four\nkey modules: a Dynamic Stability Module for performance consistency, a\nContextual Domain Alignment Module for domain adaptation, a Noise-Adaptive\nResilience Module for handling noisy data, and a Multi-Modal Fusion Module for\nintegrating diverse modalities. This work also explores strategies such as\ntask-aware data augmentation, semi-supervised learning, and explainable AI\ntechniques to enhance the applicability and robustness of FSL. AFSL provides\nscalable, reliable, and impactful solutions for real-world, high-stakes\ndomains."
    },
    {
        "date": "2025-01",
        "title": "GC-ConsFlow: Leveraging Optical Flow Residuals and Global Context for Robust Deepfake Detection",
        "author": "Jiaxin Chen, Miao Hu, Dengyong Zhang, and Jingyang Meng",
        "link": "http://arxiv.org/abs/2501.13435v1",
        "abstract": "The rapid development of Deepfake technology has enabled the generation of\nhighly realistic manipulated videos, posing severe social and ethical\nchallenges. Existing Deepfake detection methods primarily focused on either\nspatial or temporal inconsistencies, often neglecting the interplay between the\ntwo or suffering from interference caused by natural facial motions. To address\nthese challenges, we propose the global context consistency flow (GC-ConsFlow),\na novel dual-stream framework that effectively integrates spatial and temporal\nfeatures for robust Deepfake detection. The global grouped context aggregation\nmodule (GGCA), integrated into the global context-aware frame flow stream\n(GCAF), enhances spatial feature extraction by aggregating grouped global\ncontext information, enabling the detection of subtle, spatial artifacts within\nframes. The flow-gradient temporal consistency stream (FGTC), rather than\ndirectly modeling the residuals, it is used to improve the robustness of\ntemporal feature extraction against the inconsistency introduced by unnatural\nfacial motion using optical flow residuals and gradient-based features. By\ncombining these two streams, GC-ConsFlow demonstrates the effectiveness and\nrobustness in capturing complementary spatiotemporal forgery traces. Extensive\nexperiments show that GC-ConsFlow outperforms existing state-of-the-art methods\nin detecting Deepfake videos under various compression scenarios."
    },
    {
        "date": "2025-01",
        "title": "AEON: Adaptive Estimation of Instance-Dependent In-Distribution and Out-of-Distribution Label Noise for Robust Learning",
        "author": "Arpit Garg, Cuong Nguyen, Rafael Felix, Yuyuan Liu, Thanh-Toan Do, and Gustavo Carneiro",
        "link": "http://arxiv.org/abs/2501.13389v1",
        "abstract": "Robust training with noisy labels is a critical challenge in image\nclassification, offering the potential to reduce reliance on costly clean-label\ndatasets. Real-world datasets often contain a mix of in-distribution (ID) and\nout-of-distribution (OOD) instance-dependent label noise, a challenge that is\nrarely addressed simultaneously by existing methods and is further compounded\nby the lack of comprehensive benchmarking datasets. Furthermore, even though\ncurrent noisy-label learning approaches attempt to find noisy-label samples\nduring training, these methods do not aim to estimate ID and OOD noise rates to\npromote their effectiveness in the selection of such noisy-label samples, and\nthey are often represented by inefficient multi-stage learning algorithms. We\npropose the Adaptive Estimation of Instance-Dependent In-Distribution and\nOut-of-Distribution Label Noise (AEON) approach to address these research gaps.\nAEON is an efficient one-stage noisy-label learning methodology that\ndynamically estimates instance-dependent ID and OOD label noise rates to\nenhance robustness to complex noise settings. Additionally, we introduce a new\nbenchmark reflecting real-world ID and OOD noise scenarios. Experiments\ndemonstrate that AEON achieves state-of-the-art performance on both synthetic\nand real-world datasets"
    },
    {
        "date": "2025-01",
        "title": "False Sense of Security on Protected Wi-Fi Networks",
        "author": "Yong Zhi Lim, Hazmei Bin Abdul Rahman, and Biplab Sikdar",
        "link": "http://arxiv.org/abs/2501.13363v1",
        "abstract": "The Wi-Fi technology (IEEE 802.11) was introduced in 1997. With the\nincreasing use and deployment of such networks, their security has also\nattracted considerable attention. Current Wi-Fi networks use WPA2 (Wi-Fi\nProtected Access 2) for security (authentication and encryption) between access\npoints and clients. According to the IEEE 802.11i-2004 standard, wireless\nnetworks secured with WPA2-PSK (Pre-Shared Key) are required to be protected\nwith a passphrase between 8 to 63 ASCII characters. However, a poorly chosen\npassphrase significantly reduces the effectiveness of both WPA2 and\nWPA3-Personal Transition Mode. The objective of this paper is to empirically\nevaluate password choices in the wild and evaluate weakness in current common\npractices. We collected a total of 3,352 password hashes from Wi-Fi access\npoints and determine the passphrases that were protecting them. We then analyze\nthese passwords to investigate the impact of user's behavior and preference for\nconvenience on passphrase strength in secured private Wi-Fi networks in\nSingapore. We characterized the predictability of passphrases that use the\nminimum required length of 8 numeric or alphanumeric characters, and/or symbols\nstipulated in wireless security standards, and the usage of default passwords,\nand found that 16 percent of the passwords show such behavior. Our results also\nindicate the prevalence of the use of default passwords by hardware\nmanufacturers. We correlate our results with our findings and recommend methods\nthat will improve the overall security and future of our Wi-Fi networks."
    },
    {
        "date": "2025-01",
        "title": "50 Shades of Deceptive Patterns: A Unified Taxonomy, Multimodal Detection, and Security Implications",
        "author": "Zewei Shi, Ruoxi Sun, Jieshan Chen, Jiamou Sun, Minhui Xue, Yansong Gao, Feng Liu, and Xingliang Yuan",
        "link": "http://arxiv.org/abs/2501.13351v1",
        "abstract": "Deceptive patterns (DPs) are user interface designs deliberately crafted to\nmanipulate users into unintended decisions, often by exploiting cognitive\nbiases for the benefit of companies or services. While numerous studies have\nexplored ways to identify these deceptive patterns, many existing solutions\nrequire significant human intervention and struggle to keep pace with the\nevolving nature of deceptive designs. To address these challenges, we expanded\nthe deceptive pattern taxonomy from security and privacy perspectives, refining\nits categories and scope. We created a comprehensive dataset of deceptive\npatterns by integrating existing small-scale datasets with new samples,\nresulting in 6,725 images and 10,421 DP instances from mobile apps and\nwebsites. We then developed DPGuard, a novel automatic tool leveraging\ncommercial multimodal large language models (MLLMs) for deceptive pattern\ndetection. Experimental results show that DPGuard outperforms state-of-the-art\nmethods. Finally, we conducted an extensive empirical evaluation on 2,000\npopular mobile apps and websites, revealing that 23.61% of mobile screenshots\nand 47.27% of website screenshots feature at least one deceptive pattern\ninstance. Through four unexplored case studies that inform security\nimplications, we highlight the critical importance of the unified taxonomy in\naddressing the growing challenges of Internet deception."
    },
    {
        "date": "2025-01",
        "title": "Retrievals Can Be Detrimental: A Contrastive Backdoor Attack Paradigm on Retrieval-Augmented Diffusion Models",
        "author": "Hao Fang, Xiaohang Sui, Hongyao Yu, Jiawei Kong, Sijin Yu, Bin Chen, Hao Wu, and Shu-Tao Xia",
        "link": "http://arxiv.org/abs/2501.13340v1",
        "abstract": "Diffusion models (DMs) have recently demonstrated remarkable generation\ncapability. However, their training generally requires huge computational\nresources and large-scale datasets. To solve these, recent studies empower DMs\nwith the advanced Retrieval-Augmented Generation (RAG) technique and propose\nretrieval-augmented diffusion models (RDMs). By incorporating rich knowledge\nfrom an auxiliary database, RAG enhances diffusion models' generation and\ngeneralization ability while significantly reducing model parameters. Despite\nthe great success, RAG may introduce novel security issues that warrant further\ninvestigation. In this paper, we reveal that the RDM is susceptible to backdoor\nattacks by proposing a multimodal contrastive attack approach named BadRDM. Our\nframework fully considers RAG's characteristics and is devised to manipulate\nthe retrieved items for given text triggers, thereby further controlling the\ngenerated contents. Specifically, we first insert a tiny portion of images into\nthe retrieval database as target toxicity surrogates. Subsequently, a malicious\nvariant of contrastive learning is adopted to inject backdoors into the\nretriever, which builds shortcuts from triggers to the toxicity surrogates.\nFurthermore, we enhance the attacks through novel entropy-based selection and\ngenerative augmentation strategies that can derive better toxicity surrogates.\nExtensive experiments on two mainstream tasks demonstrate the proposed BadRDM\nachieves outstanding attack effects while preserving the model's benign\nutility."
    },
    {
        "date": "2025-01",
        "title": "Gradient-Free Adversarial Purification with Diffusion Models",
        "author": "Xuelong Dai, Dong Wang, Duan Mingxing, and Bin Xiao",
        "link": "http://arxiv.org/abs/2501.13336v1",
        "abstract": "Adversarial training and adversarial purification are two effective and\npractical defense methods to enhance a model's robustness against adversarial\nattacks. However, adversarial training necessitates additional training, while\nadversarial purification suffers from low time efficiency. More critically,\ncurrent defenses are designed under the perturbation-based adversarial threat\nmodel, which is ineffective against the recently proposed unrestricted\nadversarial attacks. In this paper, we propose an effective and efficient\nadversarial defense method that counters both perturbation-based and\nunrestricted adversarial attacks. Our defense is inspired by the observation\nthat adversarial attacks are typically located near the decision boundary and\nare sensitive to pixel changes. To address this, we introduce adversarial\nanti-aliasing to mitigate adversarial modifications. Additionally, we propose\nadversarial super-resolution, which leverages prior knowledge from clean\ndatasets to benignly recover images. These approaches do not require additional\ntraining and are computationally efficient without calculating gradients.\nExtensive experiments against both perturbation-based and unrestricted\nadversarial attacks demonstrate that our defense method outperforms\nstate-of-the-art adversarial purification methods."
    },
    {
        "date": "2025-01",
        "title": "Watching the AI Watchdogs: A Fairness and Robustness Analysis of AI Safety Moderation Classifiers",
        "author": "Akshit Achara, and Anshuman Chhabra",
        "link": "http://arxiv.org/abs/2501.13302v1",
        "abstract": "AI Safety Moderation (ASM) classifiers are designed to moderate content on\nsocial media platforms and to serve as guardrails that prevent Large Language\nModels (LLMs) from being fine-tuned on unsafe inputs. Owing to their potential\nfor disparate impact, it is crucial to ensure that these classifiers: (1) do\nnot unfairly classify content belonging to users from minority groups as unsafe\ncompared to those from majority groups and (2) that their behavior remains\nrobust and consistent across similar inputs. In this work, we thus examine the\nfairness and robustness of four widely-used, closed-source ASM classifiers:\nOpenAI Moderation API, Perspective API, Google Cloud Natural Language (GCNL)\nAPI, and Clarifai API. We assess fairness using metrics such as demographic\nparity and conditional statistical parity, comparing their performance against\nASM models and a fair-only baseline. Additionally, we analyze robustness by\ntesting the classifiers' sensitivity to small and natural input perturbations.\nOur findings reveal potential fairness and robustness gaps, highlighting the\nneed to mitigate these issues in future versions of these models."
    },
    {
        "date": "2025-01",
        "title": "Enhancing Robust Fairness via Confusional Spectral Regularization",
        "author": "Gaojie Jin, Sihao Wu, Jiaxu Liu, Tianjin Huang, and Ronghui Mu",
        "link": "http://arxiv.org/abs/2501.13273v1",
        "abstract": "Recent research has highlighted a critical issue known as ``robust fairness\",\nwhere robust accuracy varies significantly across different classes,\nundermining the reliability of deep neural networks (DNNs). A common approach\nto address this has been to dynamically reweight classes during training,\ngiving more weight to those with lower empirical robust performance. However,\nwe find there is a divergence of class-wise robust performance between training\nset and testing set, which limits the effectiveness of these explicit\nreweighting methods, indicating the need for a principled alternative. In this\nwork, we derive a robust generalization bound for the worst-class robust error\nwithin the PAC-Bayesian framework, accounting for unknown data distributions.\nOur analysis shows that the worst-class robust error is influenced by two main\nfactors: the spectral norm of the empirical robust confusion matrix and the\ninformation embedded in the model and training set. While the latter has been\nextensively studied, we propose a novel regularization technique targeting the\nspectral norm of the robust confusion matrix to improve worst-class robust\naccuracy and enhance robust fairness. We validate our approach through\ncomprehensive experiments on various datasets and models, demonstrating its\neffectiveness in enhancing robust fairness."
    },
    {
        "date": "2025-01",
        "title": "Threat-based Security Controls to Protect Industrial Control Systems",
        "author": "Maryam Karimi, and Haritha Srinivasan",
        "link": "http://arxiv.org/abs/2501.13268v1",
        "abstract": "This paper analyzes the reported threats to Industrial Control Systems\n(ICS)/Operational Technology (OT) and identifies common tactics, techniques,\nand procedures (TTP) used by threat actors. The paper then uses the MITRE\nATT&CK framework to map the common TTPs and provide an understanding of the\nsecurity controls needed to defend against the reported ICS threats. The paper\nalso includes a review of ICS testbeds and ideas for future research using the\nidentified controls."
    },
    {
        "date": "2025-01",
        "title": "Active RIS-Assisted URLLC NOMA-Based 5G Network with FBL under Jamming Attacks",
        "author": "Ghazal Asemian, Mohammadreza Amini, and Burak Kantarci",
        "link": "http://arxiv.org/abs/2501.13231v1",
        "abstract": "In this paper, we tackle the challenge of jamming attacks in Ultra-Reliable\nLow Latency Communication (URLLC) within Non-Orthogonal Multiple Access\n(NOMA)-based 5G networks under Finite Blocklength (FBL) conditions. We\nintroduce an innovative approach that employs Reconfigurable Intelligent\nSurfaces (RIS) with active elements to enhance energy efficiency while ensuring\nreliability and meeting latency requirements. Our approach incorporates the\ntraffic model, making it practical for real-world scenarios with dynamic\ntraffic loads. We thoroughly analyze the impact of blocklength and packet\narrival rate on network performance metrics and investigate the optimal\namplitude value and number of RIS elements. Our results indicate that\nincreasing the number of RIS elements from 4 to 400 can improve\nsignal-to-jamming-plus-noise ratio (SJNR) by 13.64\\%. Additionally, optimizing\nblocklength and packet arrival rate can achieve a 31.68% improvement in energy\nefficiency and reduced latency. These findings underscore the importance of\noptimized settings for effective jamming mitigation."
    },
    {
        "date": "2025-01",
        "title": "Joint Task Offloading and User Scheduling in 5G MEC under Jamming Attacks",
        "author": "Mohammadreza Amini, Burak Kantarci, Claude D'Amours, and Melike Erol-Kantarci",
        "link": "http://arxiv.org/abs/2501.13227v1",
        "abstract": "In this paper, we propose a novel joint task offloading and user scheduling\n(JTO-US) framework for 5G mobile edge computing (MEC) systems under security\nthreats from jamming attacks. The goal is to minimize the delay and the ratio\nof dropped tasks, taking into account both communication and computation\ndelays. The system model includes a 5G network equipped with MEC servers and an\nadversarial on-off jammer that disrupts communication. The proposed framework\noptimally schedules tasks and users to minimize the impact of jamming while\nensuring that high-priority tasks are processed efficiently. Genetic algorithm\n(GA) is used to solve the optimization problem, and the results are compared\nwith benchmark methods such as GA without considering jamming effect, Shortest\nJob First (SJF), and Shortest Deadline First (SDF). The simulation results\ndemonstrate that the proposed JTO-US framework achieves the lowest drop ratio\nand effectively manages priority tasks, outperforming existing methods.\nParticularly, when the jamming probability is 0.8, the proposed framework\nmitigates the jammer's impact by reducing the drop ratio to 63%, compared to\n89% achieved by the next best method."
    },
    {
        "date": "2025-01",
        "title": "Robust Representation Consistency Model via Contrastive Denoising",
        "author": "Jiachen Lei, Julius Berner, Jiongxiao Wang, Zhongzhu Chen, Zhongjia Ba, Kui Ren, Jun Zhu, and Anima Anandkumar",
        "link": "http://arxiv.org/abs/2501.13094v1",
        "abstract": "Robustness is essential for deep neural networks, especially in\nsecurity-sensitive applications. To this end, randomized smoothing provides\ntheoretical guarantees for certifying robustness against adversarial\nperturbations. Recently, diffusion models have been successfully employed for\nrandomized smoothing to purify noise-perturbed samples before making\npredictions with a standard classifier. While these methods excel at small\nperturbation radii, they struggle with larger perturbations and incur a\nsignificant computational overhead during inference compared to classical\nmethods. To address this, we reformulate the generative modeling task along the\ndiffusion trajectories in pixel space as a discriminative task in the latent\nspace. Specifically, we use instance discrimination to achieve consistent\nrepresentations along the trajectories by aligning temporally adjacent points.\nAfter fine-tuning based on the learned representations, our model enables\nimplicit denoising-then-classification via a single prediction, substantially\nreducing inference costs. We conduct extensive experiments on various datasets\nand achieve state-of-the-art performance with minimal computation budget during\ninference. For example, our method outperforms the certified accuracy of\ndiffusion-based methods on ImageNet across all perturbation radii by 5.3% on\naverage, with up to 11.6% at larger radii, while reducing inference costs by\n85$\\times$ on average. Codes are available at:\nhttps://github.com/jiachenlei/rRCM."
    },
    {
        "date": "2025-01",
        "title": "CHaRNet: Conditioned Heatmap Regression for Robust Dental Landmark Localization",
        "author": "Jos\u00e9 Rodr\u00edguez-Ortega, and Siham Tabik",
        "link": "http://arxiv.org/abs/2501.13073v2",
        "abstract": "Identifying anatomical landmarks in 3D dental models is crucial for\northodontic treatment. Manually placing these key points is complex,\ntime-consuming, and requires expert knowledge. While some machine learning\nmethods have been proposed for automatic tooth landmark detection in 3D\nIntraoral Scans (IOS), research remains limited, with no fully end-to-end\napproaches that avoid teeth segmentation. We propose CHaRNet (Conditioned\nHeatmap Regression Network), the first end-to-end deep learning method for\ntooth landmark detection in 3D IOS. Unlike traditional two-stage methods that\nsegment teeth before detecting landmarks, CHaRNet directly detects landmarks on\nthe input point cloud. It consists of four key modules: (1) a point cloud\nencoder, (2) a point cloud decoder with a heatmap regression head, (3) a teeth\npresence classification head, and (4) the innovative Conditioned Heatmap\nRegression (CHaR) module. The CHaR module refines landmark regression by\nleveraging teeth presence classification, enabling dynamic adaptation to cases\nwith missing teeth and improving accuracy in complex dental models. We evaluate\nCHaRNet using five point cloud learning algorithms to validate the\neffectiveness of the CHaR module and test it on a clinical dataset of 1,214\nannotated 3D dental models. Both the dataset and code will be publicly released\nto address the lack of open datasets in orthodontics, promote benchmarking, and\ninspire new research. CHaRNet achieves a Mean Euclidean Distance Error (MEDE)\nof 1.28 mm and a Mean Success Ratio (MSR) of 82.40%, demonstrating robust\nperformance. Notably, it excels in handling irregular dental geometries, such\nas models with missing teeth. This end-to-end approach streamlines orthodontic\nworkflows, improves 3D IOS analysis precision, and facilitates efficient\ncomputer-assisted treatment planning."
    },
    {
        "date": "2025-01",
        "title": "Robust Body Composition Analysis by Generating 3D CT Volumes from Limited 2D Slices",
        "author": "Lianrui Zuo, Xin Yu, Dingjie Su, Kaiwen Xu, Aravind R. Krishnan, Yihao Liu, Shunxing Bao, Fabien Maldonado, Luigi Ferrucci, and Bennett A. Landman",
        "link": "http://arxiv.org/abs/2501.13071v1",
        "abstract": "Body composition analysis provides valuable insights into aging, disease\nprogression, and overall health conditions. Due to concerns of radiation\nexposure, two-dimensional (2D) single-slice computed tomography (CT) imaging\nhas been used repeatedly for body composition analysis. However, this approach\nintroduces significant spatial variability that can impact the accuracy and\nrobustness of the analysis. To mitigate this issue and facilitate body\ncomposition analysis, this paper presents a novel method to generate 3D CT\nvolumes from limited number of 2D slices using a latent diffusion model (LDM).\nOur approach first maps 2D slices into a latent representation space using a\nvariational autoencoder. An LDM is then trained to capture the 3D context of a\nstack of these latent representations. To accurately interpolate\nintermediateslices and construct a full 3D volume, we utilize body part\nregression to determine the spatial location and distance between the acquired\nslices. Experiments on both in-house and public 3D abdominal CT datasets\ndemonstrate that the proposed method significantly enhances body composition\nanalysis compared to traditional 2D-based analysis, with a reduced error rate\nfrom 23.3% to 15.2%."
    },
    {
        "date": "2025-01",
        "title": "Generative AI Misuse Potential in Cyber Security Education: A Case Study of a UK Degree Program",
        "author": "Carlton Shepherd",
        "link": "http://arxiv.org/abs/2501.12883v3",
        "abstract": "Recent advances in generative artificial intelligence (AI), such as ChatGPT,\nGoogle Gemini, and other large language models (LLMs), pose significant\nchallenges to upholding academic integrity in higher education. This paper\ninvestigates the susceptibility of a Master's-level cyber security degree\nprogram at a UK Russell Group university, accredited by a leading national\nbody, to LLM misuse. Through the application and extension of a quantitative\nassessment framework, we identify a high exposure to misuse, particularly in\nindependent project- and report-based assessments. Contributing factors,\nincluding block teaching and a predominantly international cohort, are\nhighlighted as potential amplifiers of these vulnerabilities. To address these\nchallenges, we discuss the adoption of LLM-resistant assessments, detection\ntools, and the importance of fostering an ethical learning environment. These\napproaches aim to uphold academic standards while preparing students for the\ncomplexities of real-world cyber security."
    },
    {
        "date": "2025-01",
        "title": "Intelligent Attacks on Cyber-Physical Systems and Critical Infrastructures",
        "author": "Alan Oliveira de S\u00e1, Charles Bezerra Prado, Mariana Luiza Flavio, and Luiz F. Rust da C. Carmo",
        "link": "http://arxiv.org/abs/2501.12762v1",
        "abstract": "This chapter provides an overview of the evolving landscape of attacks in\ncyber-physical systems (CPS) and critical infrastructures, highlighting the\npossible use of Artificial Intelligence (AI) algorithms to develop intelligent\ncyberattacks. It describes various existing methods used to carry out\nintelligent attacks in Operational Technology (OT) environments and discusses\nAI-driven tools that automate penetration tests in Information Technology (IT)\nsystems, which could potentially be used as attack tools. The chapter also\ndiscusses mitigation strategies to counter these emerging intelligent attacks\nby hindering the learning process of AI-based attacks and points to future\nresearch directions on the matter."
    },
    {
        "date": "2025-01",
        "title": "Modality Unified Attack for Omni-Modality Person Re-Identification",
        "author": "Yuan Bian, Min Liu, Yunqi Yi, Xueping Wang, Yunfeng Ma, and Yaonan Wang",
        "link": "http://arxiv.org/abs/2501.12761v1",
        "abstract": "Deep learning based person re-identification (re-id) models have been widely\nemployed in surveillance systems. Recent studies have demonstrated that\nblack-box single-modality and cross-modality re-id models are vulnerable to\nadversarial examples (AEs), leaving the robustness of multi-modality re-id\nmodels unexplored. Due to the lack of knowledge about the specific type of\nmodel deployed in the target black-box surveillance system, we aim to generate\nmodality unified AEs for omni-modality (single-, cross- and multi-modality)\nre-id models. Specifically, we propose a novel Modality Unified Attack method\nto train modality-specific adversarial generators to generate AEs that\neffectively attack different omni-modality models. A multi-modality model is\nadopted as the surrogate model, wherein the features of each modality are\nperturbed by metric disruption loss before fusion. To collapse the common\nfeatures of omni-modality models, Cross Modality Simulated Disruption approach\nis introduced to mimic the cross-modality feature embeddings by intentionally\nfeeding images to non-corresponding modality-specific subnetworks of the\nsurrogate model. Moreover, Multi Modality Collaborative Disruption strategy is\ndevised to facilitate the attacker to comprehensively corrupt the informative\ncontent of person images by leveraging a multi modality feature collaborative\nmetric disruption loss. Extensive experiments show that our MUA method can\neffectively attack the omni-modality re-id models, achieving 55.9%, 24.4%,\n49.0% and 62.7% mean mAP Drop Rate, respectively."
    },
    {
        "date": "2025-01",
        "title": "Bad-PFL: Exploring Backdoor Attacks against Personalized Federated Learning",
        "author": "Mingyuan Fan, Zhanyi Hu, Fuyi Wang, and Cen Chen",
        "link": "http://arxiv.org/abs/2501.12736v1",
        "abstract": "Data heterogeneity and backdoor attacks rank among the most significant\nchallenges facing federated learning (FL). For data heterogeneity, personalized\nfederated learning (PFL) enables each client to maintain a private personalized\nmodel to cater to client-specific knowledge. Meanwhile, vanilla FL has proven\nvulnerable to backdoor attacks. However, recent advancements in PFL community\nhave demonstrated a potential immunity against such attacks. This paper\nexplores this intersection further, revealing that existing federated backdoor\nattacks fail in PFL because backdoors about manually designed triggers struggle\nto survive in personalized models. To tackle this, we design Bad-PFL, which\nemploys features from natural data as our trigger. As long as the model is\ntrained on natural data, it inevitably embeds the backdoor associated with our\ntrigger, ensuring its longevity in personalized models. Moreover, our trigger\nundergoes mutual reinforcement training with the model, further solidifying the\nbackdoor's durability and enhancing attack effectiveness. The large-scale\nexperiments across three benchmark datasets demonstrate the superior\nperformance of our attack against various PFL methods, even when equipped with\nstate-of-the-art defense mechanisms."
    },
    {
        "date": "2025-01",
        "title": "FedDAG: Federated Domain Adversarial Generation Towards Generalizable Medical Image Analysis",
        "author": "Haoxuan Che, Yifei Wu, Haibo Jin, Yong Xia, and Hao Chen",
        "link": "http://arxiv.org/abs/2501.13967v2",
        "abstract": "Federated domain generalization aims to train a global model from multiple\nsource domains and ensure its generalization ability to unseen target domains.\nDue to the target domain being with unknown domain shifts, attempting to\napproximate these gaps by source domains may be the key to improving model\ngeneralization capability. Existing works mainly focus on sharing and\nrecombining local domain-specific attributes to increase data diversity and\nsimulate potential domain shifts. However, these methods may be insufficient\nsince only the local attribute recombination can be hard to touch the\nout-of-distribution of global data. In this paper, we propose a\nsimple-yet-efficient framework named Federated Domain Adversarial Generation\n(FedDAG). It aims to simulate the domain shift and improve the model\ngeneralization by adversarially generating novel domains different from local\nand global source domains. Specifically, it generates novel-style images by\nmaximizing the instance-level feature discrepancy between original and\ngenerated images and trains a generalizable task model by minimizing their\nfeature discrepancy. Further, we observed that FedDAG could cause different\nperformance improvements for local models. It may be due to inherent data\nisolation and heterogeneity among clients, exacerbating the imbalance in their\ngeneralization contributions to the global model. Ignoring this imbalance can\nlead the global model's generalization ability to be sub-optimal, further\nlimiting the novel domain generation procedure. Thus, to mitigate this\nimbalance, FedDAG hierarchically aggregates local models at the within-client\nand across-client levels by using the sharpness concept to evaluate client\nmodel generalization contributions. Extensive experiments across four medical\nbenchmarks demonstrate FedDAG's ability to enhance generalization in federated\nmedical scenarios."
    },
    {
        "date": "2025-01",
        "title": "Towards Robust Multi-tab Website Fingerprinting",
        "author": "Xinhao Deng, Xiyuan Zhao, Qilei Yin, Zhuotao Liu, Qi Li, Mingwei Xu, Ke Xu, and Jianping Wu",
        "link": "http://arxiv.org/abs/2501.12622v1",
        "abstract": "Website fingerprinting enables an eavesdropper to determine which websites a\nuser is visiting over an encrypted connection. State-of-the-art website\nfingerprinting (WF) attacks have demonstrated effectiveness even against\nTor-protected network traffic. However, existing WF attacks have critical\nlimitations on accurately identifying websites in multi-tab browsing sessions,\nwhere the holistic pattern of individual websites is no longer preserved, and\nthe number of tabs opened by a client is unknown a priori. In this paper, we\npropose ARES, a novel WF framework natively designed for multi-tab WF attacks.\nARES formulates the multi-tab attack as a multi-label classification problem\nand solves it using the novel Transformer-based models. Specifically, ARES\nextracts local patterns based on multi-level traffic aggregation features and\nutilizes the improved self-attention mechanism to analyze the correlations\nbetween these local patterns, effectively identifying websites. We implement a\nprototype of ARES and extensively evaluate its effectiveness using our\nlarge-scale datasets collected over multiple months. The experimental results\nillustrate that ARES achieves optimal performance in several realistic\nscenarios. Further, ARES remains robust even against various WF defenses."
    },
    {
        "date": "2025-01",
        "title": "Robustness of Selected Learning Models under Label-Flipping Attack",
        "author": "Sarvagya Bhargava, and Mark Stamp",
        "link": "http://arxiv.org/abs/2501.12516v1",
        "abstract": "In this paper we compare traditional machine learning and deep learning\nmodels trained on a malware dataset when subjected to adversarial attack based\non label-flipping. Specifically, we investigate the robustness of Support\nVector Machines (SVM), Random Forest, Gaussian Naive Bayes (GNB), Gradient\nBoosting Machine (GBM), LightGBM, XGBoost, Multilayer Perceptron (MLP),\nConvolutional Neural Network (CNN), MobileNet, and DenseNet models when facing\nvarying percentages of misleading labels. We empirically assess the the\naccuracy of each of these models under such an adversarial attack on the\ntraining data. This research aims to provide insights into which models are\ninherently more robust, in the sense of being better able to resist intentional\ndisruptions to the training data. We find wide variation in the robustness of\nthe models tested to adversarial attack, with our MLP model achieving the best\ncombination of initial accuracy and robustness."
    },
    {
        "date": "2025-01",
        "title": "Adaptive Cyber-Attack Detection in IIoT Using Attention-Based LSTM-CNN Models",
        "author": "Afrah Gueriani, Hamza Kheddar, and Ahmed Cherif Mazari",
        "link": "http://arxiv.org/abs/2501.13962v1",
        "abstract": "The rapid expansion of the industrial Internet of things (IIoT) has\nintroduced new challenges in securing critical infrastructures against\nsophisticated cyberthreats. This study presents the development and evaluation\nof an advanced Intrusion detection (IDS) based on a hybrid LSTM-convolution\nneural network (CNN)-Attention architecture, specifically designed to detect\nand classify cyberattacks in IIoT environments. The research focuses on two key\nclassification tasks: binary and multi-class classification. The proposed\nmodels was rigorously tested using the Edge-IIoTset dataset. To mitigate the\nclass imbalance in the dataset, the synthetic minority over-sampling technique\n(SMOTE) was employed to generate synthetic samples for the underrepresented\nclasses. This ensured that the model could learn effectively from all classes,\nthereby improving the overall classification performance. Through systematic\nexperimentation, various deep learning (DL) models were compared, ultimately\ndemonstrating that the LSTM-CNN-Attention model consistently outperformed\nothers across key performance metrics. In binary classification, the model\nachieved near-perfect accuracy, while in multi-class classification, it\nmaintained a high accuracy level (99.04%), effectively categorizing different\nattack types with a loss value of 0.0220%."
    },
    {
        "date": "2025-01",
        "title": "A Fast, Scalable, and Robust Deep Learning-based Iterative Reconstruction Framework for Accelerated Industrial Cone-beam X-ray Computed Tomography",
        "author": "Aniket Pramanik, Obaidullah Rahman, Singanallur V. Venkatakrishnan, and Amirkoushyar Ziabari",
        "link": "http://arxiv.org/abs/2501.13961v1",
        "abstract": "Cone-beam X-ray Computed Tomography (XCT) with large detectors and\ncorresponding large-scale 3D reconstruction plays a pivotal role in\nmicron-scale characterization of materials and parts across various industries.\nIn this work, we present a novel deep neural network-based iterative algorithm\nthat integrates an artifact reduction-trained CNN as a prior model with\nautomated regularization parameter selection, tailored for large-scale\nindustrial cone-beam XCT data. Our method achieves high-quality 3D\nreconstructions even for extremely dense thick metal parts - which\ntraditionally pose challenges to industrial CT images - in just a few\niterations. Furthermore, we show the generalizability of our approach to\nout-of-distribution scans obtained under diverse scanning conditions. Our\nmethod effectively handles significant noise and streak artifacts, surpassing\nstate-of-the-art supervised learning methods trained on the same data."
    },
    {
        "date": "2025-01",
        "title": "Cinepro: Robust Training of Foundation Models for Cancer Detection in Prostate Ultrasound Cineloops",
        "author": "Mohamed Harmanani, Amoon Jamzad, Minh Nguyen Nhat To, Paul F. R. Wilson, Zhuoxin Guo, Fahimeh Fooladgar, Samira Sojoudi, Mahdi Gilany, Silvia Chang, Peter Black, Michael Leveridge, Robert Siemens, Purang Abolmaesumi, and Parvin Mousavi",
        "link": "http://arxiv.org/abs/2501.12331v1",
        "abstract": "Prostate cancer (PCa) detection using deep learning (DL) models has shown\npotential for enhancing real-time guidance during biopsies. However, prostate\nultrasound images lack pixel-level cancer annotations, introducing label noise.\nCurrent approaches often focus on limited regions of interest (ROIs),\ndisregarding anatomical context necessary for accurate diagnosis. Foundation\nmodels can overcome this limitation by analyzing entire images to capture\nglobal spatial relationships; however, they still encounter challenges stemming\nfrom the weak labels associated with coarse pathology annotations in ultrasound\ndata. We introduce Cinepro, a novel framework that strengthens foundation\nmodels' ability to localize PCa in ultrasound cineloops. Cinepro adapts robust\ntraining by integrating the proportion of cancer tissue reported by pathology\nin a biopsy core into its loss function to address label noise, providing a\nmore nuanced supervision. Additionally, it leverages temporal data across\nmultiple frames to apply robust augmentations, enhancing the model's ability to\nlearn stable cancer-related features. Cinepro demonstrates superior performance\non a multi-center prostate ultrasound dataset, achieving an AUROC of 77.1% and\na balanced accuracy of 83.8%, surpassing current benchmarks. These findings\nunderscore Cinepro's promise in advancing foundation models for weakly labeled\nultrasound data."
    },
    {
        "date": "2025-01",
        "title": "With Great Backbones Comes Great Adversarial Transferability",
        "author": "Erik Arakelyan, Karen Hambardzumyan, Davit Papikyan, Pasquale Minervini, Albert Gordo, Isabelle Augenstein, and Aram H. Markosyan",
        "link": "http://arxiv.org/abs/2501.12275v1",
        "abstract": "Advances in self-supervised learning (SSL) for machine vision have improved\nrepresentation robustness and model performance, giving rise to pre-trained\nbackbones like \\emph{ResNet} and \\emph{ViT} models tuned with SSL methods such\nas \\emph{SimCLR}. Due to the computational and data demands of pre-training,\nthe utilization of such backbones becomes a strenuous necessity. However,\nemploying these backbones may inherit vulnerabilities to adversarial attacks.\nWhile adversarial robustness has been studied under \\emph{white-box} and\n\\emph{black-box} settings, the robustness of models tuned on pre-trained\nbackbones remains largely unexplored. Additionally, the role of tuning\nmeta-information in mitigating exploitation risks is unclear. This work\nsystematically evaluates the adversarial robustness of such models across\n$20,000$ combinations of tuning meta-information, including fine-tuning\ntechniques, backbone families, datasets, and attack types. We propose using\nproxy models to transfer attacks, simulating varying levels of target knowledge\nby fine-tuning these proxies with diverse configurations. Our findings reveal\nthat proxy-based attacks approach the effectiveness of \\emph{white-box}\nmethods, even with minimal tuning knowledge. We also introduce a naive\n\"backbone attack,\" leveraging only the backbone to generate adversarial\nsamples, which outperforms \\emph{black-box} attacks and rivals \\emph{white-box}\nmethods, highlighting critical risks in model-sharing practices. Finally, our\nablations reveal how increasing tuning meta-information impacts attack\ntransferability, measuring each meta-information combination."
    },
    {
        "date": "2025-01",
        "title": "mmCooper: A Multi-agent Multi-stage Communication-efficient and Collaboration-robust Cooperative Perception Framework",
        "author": "Bingyi Liu, Jian Teng, Hongfei Xue, Enshu Wang, Chuanhui Zhu, Pu Wang, and Libing Wu",
        "link": "http://arxiv.org/abs/2501.12263v1",
        "abstract": "Collaborative perception significantly enhances individual vehicle perception\nperformance through the exchange of sensory information among agents. However,\nreal-world deployment faces challenges due to bandwidth constraints and\ninevitable calibration errors during information exchange. To address these\nissues, we propose mmCooper, a novel multi-agent, multi-stage,\ncommunication-efficient, and collaboration-robust cooperative perception\nframework. Our framework leverages a multi-stage collaboration strategy that\ndynamically and adaptively balances intermediate- and late-stage information to\nshare among agents, enhancing perceptual performance while maintaining\ncommunication efficiency. To support robust collaboration despite potential\nmisalignments and calibration errors, our framework captures multi-scale\ncontextual information for robust fusion in the intermediate stage and\ncalibrates the received detection results to improve accuracy in the late\nstage. We validate the effectiveness of mmCooper through extensive experiments\non real-world and simulated datasets. The results demonstrate the superiority\nof our proposed framework and the effectiveness of each component."
    },
    {
        "date": "2025-01",
        "title": "Empower Healthcare through a Self-Sovereign Identity Infrastructure for Secure Electronic Health Data Access",
        "author": "Antonio L\u00f3pez Mart\u00ednez, Montassar Naghmouchi, Maryline Laurent, Joaquin Garcia-Alfaro, Manuel Gil P\u00e9rez, Antonio Ruiz Mart\u00ednez, and Pantaleone Nespoli",
        "link": "http://arxiv.org/abs/2501.12229v1",
        "abstract": "Health data is one of the most sensitive data for people, which attracts the\nattention of malicious activities. We propose an open-source health data\nmanagement framework, that follows a patient-centric approach. The proposed\nframework implements the Self-Sovereign Identity paradigm with innovative\ntechnologies such as Decentralized Identifiers and Verifiable Credentials. The\nframework uses Blockchain technology to provide immutability, verifiable data\nregistry, and auditability, as well as an agent-based model to provide\nprotection and privacy for the patient data. We also define different use cases\nregarding the daily patient-practitioner-laboratory interactions and specific\nfunctions to cover patient data loss, data access revocation, and emergency\ncases where patients are unable to give consent and access to their data. To\naddress this design, a proof of concept is created with an interaction between\npatient and doctor. The most feasible technologies are selected and the created\ndesign is validated. We discuss the differences and novelties of this\nframework, which includes the patient-centric approach also for data storage,\nthe designed recovery and emergency plan, the defined backup procedure, and the\nselected blockchain platform."
    },
    {
        "date": "2025-01",
        "title": "You Can't Eat Your Cake and Have It Too: The Performance Degradation of LLMs with Jailbreak Defense",
        "author": "Wuyuao Mai, Geng Hong, Pei Chen, Xudong Pan, Baojun Liu, Yuan Zhang, Haixin Duan, and Min Yang",
        "link": "http://arxiv.org/abs/2501.12210v1",
        "abstract": "With the rise of generative large language models (LLMs) like LLaMA and\nChatGPT, these models have significantly transformed daily life and work by\nproviding advanced insights. However, as jailbreak attacks continue to\ncircumvent built-in safety mechanisms, exploiting carefully crafted scenarios\nor tokens, the safety risks of LLMs have come into focus. While numerous\ndefense strategies--such as prompt detection, modification, and model\nfine-tuning--have been proposed to counter these attacks, a critical question\narises: do these defenses compromise the utility and usability of LLMs for\nlegitimate users? Existing research predominantly focuses on the effectiveness\nof defense strategies without thoroughly examining their impact on performance,\nleaving a gap in understanding the trade-offs between LLM safety and\nperformance. Our research addresses this gap by conducting a comprehensive\nstudy on the utility degradation, safety elevation, and exaggerated-safety\nescalation of LLMs with jailbreak defense strategies. We propose USEBench, a\nnovel benchmark designed to evaluate these aspects, along with USEIndex, a\ncomprehensive metric for assessing overall model performance. Through\nexperiments on seven state-of-the-art LLMs, we found that mainstream jailbreak\ndefenses fail to ensure both safety and performance simultaneously. Although\nmodel-finetuning performs the best overall, their effectiveness varies across\nLLMs. Furthermore, vertical comparisons reveal that developers commonly\nprioritize performance over safety when iterating or fine-tuning their LLMs."
    },
    {
        "date": "2025-01",
        "title": "FedCLEAN: byzantine defense by CLustering Errors of Activation maps in Non-IID federated learning environments",
        "author": "Mehdi Ben Ghali, Reda Bellafqira, and Gouenou Coatrieux",
        "link": "http://arxiv.org/abs/2501.12123v1",
        "abstract": "Federated Learning (FL) enables clients to collaboratively train a global\nmodel using their local datasets while reinforcing data privacy. However, FL is\nsusceptible to poisoning attacks. Existing defense mechanisms assume that\nclients' data are independent and identically distributed (IID), making them\nineffective in real-world applications where data are non-IID. This paper\npresents FedCLEAN, the first defense capable of filtering attackers' model\nupdates in a non-IID FL environment. The originality of FedCLEAN is twofold.\nFirst, it relies on a client confidence score derived from the reconstruction\nerrors of each client's model activation maps for a given trigger set, with\nreconstruction errors obtained by means of a Conditional Variational\nAutoencoder trained according to a novel server-side strategy. Second, we\npropose an ad-hoc trust propagation algorithm based on client scores, which\nallows building a cluster of benign clients while flagging potential attackers.\nExperimental results on the datasets MNIST and FashionMNIST demonstrate the\nrobustness of FedCLEAN against Byzantine attackers in non-IID scenarios and a\nclose-to-zero benign client misclassification rate, even in the absence of an\nattack."
    },
    {
        "date": "2025-01",
        "title": "Application of Machine Learning Techniques for Secure Traffic in NoC-based Manycores",
        "author": "Geaninne Lopes, C\u00e9sar Marcon, and Fernando Moraes",
        "link": "http://arxiv.org/abs/2501.12034v1",
        "abstract": "Like most computer systems, a manycore can also be the target of security\nattacks. It is essential to ensure the security of the NoC since all\ninformation travels through its channels, and any interference in the traffic\nof messages can reflect on the entire chip, causing communication problems.\nAmong the possible attacks on NoC, Denial of Service (DoS) attacks are the most\ncited in the literature. The state of the art shows a lack of work that can\ndetect such attacks through learning techniques. On the other hand, these\ntechniques are widely explored in computer network security via an Intrusion\nDetection System (IDS). In this context, the main goal of this document is to\npresent the progress of a work that explores an IDS technique using machine\nlearning and temporal series for detecting DoS attacks in NoC-based manycore\nsystems. To fulfill this goal, it is necessary to extract traffic data from a\nmanycore NoC and execute the learning techniques in the extracted data.\nHowever, while low-level platforms offer precision and slow execution,\nhigh-level platforms offer higher speed and data incompatible with reality.\nTherefore, a platform is being developed using the OVP tool, which has a higher\nlevel of abstraction. To solve the low precision problem, the developed\nplatform will have its data validated with a low-level platform."
    },
    {
        "date": "2025-01",
        "title": "Ratio Attack on G+G Convoluted Gaussian Signature",
        "author": "Chik How Tan, Theo Fanuela Prabowo, and Wei Guo Foo",
        "link": "http://arxiv.org/abs/2501.12009v1",
        "abstract": "A lattice-based signature, called G+G convoluted Gaussian signature was\nproposed in ASIACRYPT 2023 and was proved secure in the quantum random oracle\nmodel. In this paper, we propose a ratio attack on the G+G convoluted Gaussian\nsignature to recover the secret key. The attack exploits the fact, proved in\nthis paper, that the secret key can be obtained from the expected value of the\nratio of signatures which follows a truncated Cauchy distribution. Moreover, we\nalso compute the number of signatures required to successfully recover the\nsecret key. Furthermore, we simulate the ratio attack in Sagemath with a few\ndifferent parameters as a proof-of-concept of the ratio attack."
    },
    {
        "date": "2025-01",
        "title": "BRC20 Snipping Attack",
        "author": "Minfeng Qi, Qin Wang, Ningran Li, Shiping Chen, and Tianqing Zhu",
        "link": "http://arxiv.org/abs/2501.11942v1",
        "abstract": "In this paper, we introduce and implement BRC20 sniping attack. Our attack\nmanipulates the BRC20 token transfers in open markets and disrupts the fairness\namong bidding participants. The long-standing principle of ``highest bidder\nwins'' is rendered ineffective.\n  Typically, open BRC20 token markets rely on Partially Signed Bitcoin\nTransactions (PSBT) to broadcast selling intents and wait for buying auctions.\nOur attack targets the BRC20 buying process (i.e., transfer) by injecting a\nfront-running transaction to complete the full signature of the PSBT. At its\ncore, the attack exploits the mempool's fee-based transaction selection\nmechanism to snipe the victim transaction, replicate metadata, and front-run\nthe legesmate transaction. This attack applies to platforms using PSBT for\nBRC20 token transfers, including popular Bitcoin exchanges and marketplaces\n(e.g., Magic Eden, Unisat, Gate.io, OKX).\n  We implemented and tested the attack on a Bitcoin testnet (regtest),\nvalidating its effectiveness through multiple experimental rounds. Results show\nthat the attacker consistently replaces legitimate transactions by submitting\nhigher-fee PSBTs. We have also made responsible disclosures to the mentioned\nexchanges."
    },
    {
        "date": "2025-01",
        "title": "LuxVeri at GenAI Detection Task 1: Inverse Perplexity Weighted Ensemble for Robust Detection of AI-Generated Text across English and Multilingual Contexts",
        "author": "Md Kamrujjaman Mobin, and Md Saiful Islam",
        "link": "http://arxiv.org/abs/2501.11914v1",
        "abstract": "This paper presents a system developed for Task 1 of the COLING 2025 Workshop\non Detecting AI-Generated Content, focusing on the binary classification of\nmachine-generated versus human-written text. Our approach utilizes an ensemble\nof models, with weights assigned according to each model's inverse perplexity,\nto enhance classification accuracy. For the English text detection task, we\ncombined RoBERTa-base, RoBERTa-base with the OpenAI detector, and\nBERT-base-cased, achieving a Macro F1-score of 0.7458, which ranked us 12th out\nof 35 teams. We ensembled RemBERT, XLM-RoBERTa-base, and\nBERT-base-multilingual-case for the multilingual text detection task, employing\nthe same inverse perplexity weighting technique. This resulted in a Macro\nF1-score of 0.7513, positioning us 4th out of 25 teams. Our results demonstrate\nthe effectiveness of inverse perplexity weighting in improving the robustness\nof machine-generated text detection across both monolingual and multilingual\nsettings, highlighting the potential of ensemble methods for this challenging\ntask."
    },
    {
        "date": "2025-01",
        "title": "Enhancing Adversarial Transferability via Component-Wise Augmentation Method",
        "author": "Hangyu Liu, Bo Peng, Pengxiang Ding, and Donglin Wang",
        "link": "http://arxiv.org/abs/2501.11901v1",
        "abstract": "Deep Neural Networks (DNNs) are highly vulnerable to adversarial examples,\nwhich pose significant challenges in security-sensitive applications. Among\nvarious adversarial attack strategies, input transformation-based attacks have\ndemonstrated remarkable effectiveness in enhancing adversarial transferability.\nHowever, existing methods fail to diversify attention regions across models\nadequately and introduce excessive information loss during transformations. In\nthis paper, we introduce a novel input transformation-based method, termed\nComponent-Wise Augmentation (CWA), designed to enhance transferability by\nlocally applying block-wise transformations. CWA strategically integrates\ninterpolation and selective rotation on individual image blocks to diversify\nmodel attention regions while preserving semantic integrity. Extensive\nexperiments on the standard ImageNet dataset show that CWA consistently\noutperforms state-of-the-art methods in both attack success rates and stability\nacross CNN- and Transformer-based models, while also demonstrating superior\nperformance against multiple defense methods."
    },
    {
        "date": "2025-01",
        "title": "LASER: Lip Landmark Assisted Speaker Detection for Robustness",
        "author": "Le Thien Phuc Nguyen, Zhuoran Yu, and Yong Jae Lee",
        "link": "http://arxiv.org/abs/2501.11899v1",
        "abstract": "Active Speaker Detection (ASD) aims to identify speaking individuals in\ncomplex visual scenes. While humans can easily detect speech by matching lip\nmovements to audio, current ASD models struggle to establish this\ncorrespondence, often misclassifying non-speaking instances when audio and lip\nmovements are unsynchronized. To address this limitation, we propose Lip\nlandmark Assisted Speaker dEtection for Robustness (LASER). Unlike models that\nrely solely on facial frames, LASER explicitly focuses on lip movements by\nintegrating lip landmarks in training. Specifically, given a face track, LASER\nextracts frame-level visual features and the 2D coordinates of lip landmarks\nusing a lightweight detector. These coordinates are encoded into dense feature\nmaps, providing spatial and structural information on lip positions.\nRecognizing that landmark detectors may sometimes fail under challenging\nconditions (e.g., low resolution, occlusions, extreme angles), we incorporate\nan auxiliary consistency loss to align predictions from both lip-aware and\nface-only features, ensuring reliable performance even when lip data is absent.\nExtensive experiments across multiple datasets show that LASER outperforms\nstate-of-the-art models, especially in scenarios with desynchronized audio and\nvisuals, demonstrating robust performance in real-world video contexts. Code is\navailable at \\url{https://github.com/plnguyen2908/LASER_ASD}."
    },
    {
        "date": "2025-01",
        "title": "Cross-Entropy Attacks to Language Models via Rare Event Simulation",
        "author": "Mingze Ni, Yongshun Gong, and Wei Liu",
        "link": "http://arxiv.org/abs/2501.11852v1",
        "abstract": "Black-box textual adversarial attacks are challenging due to the lack of\nmodel information and the discrete, non-differentiable nature of text. Existing\nmethods often lack versatility for attacking different models, suffer from\nlimited attacking performance due to the inefficient optimization with word\nsaliency ranking, and frequently sacrifice semantic integrity to achieve better\nattack outcomes. This paper introduces a novel approach to textual adversarial\nattacks, which we call Cross-Entropy Attacks (CEA), that uses Cross-Entropy\noptimization to address the above issues. Our CEA approach defines adversarial\nobjectives for both soft-label and hard-label settings and employs CE\noptimization to identify optimal replacements. Through extensive experiments on\ndocument classification and language translation problems, we demonstrate that\nour attack method excels in terms of attacking performance, imperceptibility,\nand sentence quality."
    },
    {
        "date": "2025-01",
        "title": "FedMUA: Exploring the Vulnerabilities of Federated Learning to Malicious Unlearning Attacks",
        "author": "Jian Chen, Zehui Lin, Wanyu Lin, Wenlong Shi, Xiaoyan Yin, and Di Wang",
        "link": "http://arxiv.org/abs/2501.11848v1",
        "abstract": "Recently, the practical needs of ``the right to be forgotten'' in federated\nlearning gave birth to a paradigm known as federated unlearning, which enables\nthe server to forget personal data upon the client's removal request. Existing\nstudies on federated unlearning have primarily focused on efficiently\neliminating the influence of requested data from the client's model without\nretraining from scratch, however, they have rarely doubted the reliability of\nthe global model posed by the discrepancy between its prediction performance\nbefore and after unlearning. To bridge this gap, we take the first step by\nintroducing a novel malicious unlearning attack dubbed FedMUA, aiming to unveil\npotential vulnerabilities emerging from federated learning during the\nunlearning process. The crux of FedMUA is to mislead the global model into\nunlearning more information associated with the influential samples for the\ntarget sample than anticipated, thus inducing adverse effects on target samples\nfrom other clients. To achieve this, we design a novel two-step method, known\nas Influential Sample Identification and Malicious Unlearning Generation, to\nidentify and subsequently generate malicious feature unlearning requests within\nthe influential samples. By doing so, we can significantly alter the\npredictions pertaining to the target sample by initiating the malicious feature\nunlearning requests, leading to the deliberate manipulation for the user\nadversely. Additionally, we design a new defense mechanism that is highly\nresilient against malicious unlearning attacks. Extensive experiments on three\nrealistic datasets reveal that FedMUA effectively induces misclassification on\ntarget samples and can achieve an 80% attack success rate by triggering only\n0.3% malicious unlearning requests."
    },
    {
        "date": "2025-01",
        "title": "CogMorph: Cognitive Morphing Attacks for Text-to-Image Models",
        "author": "Zonglei Jing, Zonghao Ying, Le Wang, Siyuan Liang, Aishan Liu, Xianglong Liu, and Dacheng Tao",
        "link": "http://arxiv.org/abs/2501.11815v2",
        "abstract": "The development of text-to-image (T2I) generative models, that enable the\ncreation of high-quality synthetic images from textual prompts, has opened new\nfrontiers in creative design and content generation. However, this paper\nreveals a significant and previously unrecognized ethical risk inherent in this\ntechnology and introduces a novel method, termed the Cognitive Morphing Attack\n(CogMorph), which manipulates T2I models to generate images that retain the\noriginal core subjects but embeds toxic or harmful contextual elements. This\nnuanced manipulation exploits the cognitive principle that human perception of\nconcepts is shaped by the entire visual scene and its context, producing images\nthat amplify emotional harm far beyond attacks that merely preserve the\noriginal semantics. To address this, we first construct an imagery toxicity\ntaxonomy spanning 10 major and 48 sub-categories, aligned with human\ncognitive-perceptual dimensions, and further build a toxicity risk matrix\nresulting in 1,176 high-quality T2I toxic prompts. Based on this, our CogMorph\nfirst introduces Cognitive Toxicity Augmentation, which develops a cognitive\ntoxicity knowledge base with rich external toxic representations for humans\n(e.g., fine-grained visual features) that can be utilized to further guide the\noptimization of adversarial prompts. In addition, we present Contextual\nHierarchical Morphing, which hierarchically extracts critical parts of the\noriginal prompt (e.g., scenes, subjects, and body parts), and then iteratively\nretrieves and fuses toxic features to inject harmful contexts. Extensive\nexperiments on multiple open-sourced T2I models and black-box commercial APIs\n(e.g., DALLE-3) demonstrate the efficacy of CogMorph which significantly\noutperforms other baselines by large margins (+20.62% on average)."
    },
    {
        "date": "2025-01",
        "title": "Blockchain Security Risk Assessment in Quantum Era, Migration Strategies and Proactive Defense",
        "author": "Yaser Baseri, Abdelhakim Hafid, Yahya Shahsavari, Dimitrios Makrakis, and Hassan Khodaiemehr",
        "link": "http://arxiv.org/abs/2501.11798v1",
        "abstract": "The emergence of quantum computing presents a formidable challenge to the\nsecurity of blockchain systems. Traditional cryptographic algorithms,\nfoundational to digital signatures, message encryption, and hashing functions,\nbecome vulnerable to the immense computational power of quantum computers. This\npaper conducts a thorough risk assessment of transitioning to quantum-resistant\nblockchains, comprehensively analyzing potential threats targeting vital\nblockchain components: the network, mining pools, transaction verification\nmechanisms, smart contracts, and user wallets. By elucidating the intricate\nchallenges and strategic considerations inherent in transitioning to\nquantum-resistant algorithms, the paper evaluates risks and highlights\nobstacles in securing blockchain components with quantum-resistant\ncryptography. It offers a hybrid migration strategy to facilitate a smooth\ntransition from classical to quantum-resistant cryptography. The analysis\nextends to prominent blockchains such as Bitcoin, Ethereum, Ripple, Litecoin,\nand Zcash, assessing vulnerable components, potential impacts, and associated\nSTRIDE threats, thereby identifying areas susceptible to quantum attacks.\nBeyond analysis, the paper provides actionable guidance for designing secure\nand resilient blockchain ecosystems in the quantum computing era. Recognizing\nthe looming threat of quantum computers, this research advocates for a\nproactive transition to quantum-resistant blockchain networks. It proposes a\ntailored security blueprint that strategically fortifies each component against\nthe evolving landscape of quantum-induced cyber threats. Emphasizing the\ncritical need for blockchain stakeholders to adopt proactive measures and\nimplement quantum-resistant solutions, the paper underscores the importance of\nembracing these insights to navigate the complexities of the quantum era with\nresilience and confidence."
    },
    {
        "date": "2025-01",
        "title": "Provably effective detection of effective data poisoning attacks",
        "author": "Jonathan Gallagher, Yasaman Esfandiari, Callen MacPhee, and Michael Warren",
        "link": "http://arxiv.org/abs/2501.11795v1",
        "abstract": "This paper establishes a mathematically precise definition of dataset\npoisoning attack and proves that the very act of effectively poisoning a\ndataset ensures that the attack can be effectively detected. On top of a\nmathematical guarantee that dataset poisoning is identifiable by a new\nstatistical test that we call the Conformal Separability Test, we provide\nexperimental evidence that we can adequately detect poisoning attempts in the\nreal world."
    },
    {
        "date": "2025-01",
        "title": "Disentangling stellar atmospheric parameters in astronomical spectra using Generative Adversarial Neural Networks",
        "author": "Minia Manteiga, Ra\u00fal Santove\u00f1a, Marco A. \u00c1lvarez, Carlos Dafonte, Manuel G. Penedo, Silvana Navarro, and Luis Corral",
        "link": "http://arxiv.org/abs/2501.11762v1",
        "abstract": "A method based on Generative Adversaria! Networks (GANs) is developed for\ndisentangling the physical (effective temperature and gravity) and chemical\n(metallicity, overabundance of a-elements with respect to iron) atmospheric\nproperties in astronomical spectra. Using a projection of the stellar spectra,\ncommonly called latent space, in which the contribution dueto one or several\nmain stellar physicochemical properties is minimised while others are enhanced,\nit was possible to maximise the information related to certain properties,\nwhich can then be extracted using artificial neural networks (ANN) as\nregressors with higher accuracy than a reference method based on the use of ANN\ntrained with the original spectra. Methods. Our model utilises autoencoders,\ncomprising two artificial neural networks: an encoder anda decoder which\ntransform input data into a low-dimensional representation known as latent\nspace. It also uses discriminators, which are additional neural networks aimed\nat transforming the traditional autoencoder training into an adversaria!\napproach, to disentangle or reinforce the astrophysical parameters from the\nlatent space. The GANDALF tool is described. It was developed to define, train,\nand test our GAN model with a web framework to show how the disentangling\nalgorithm works visually. It is open to the community in Github. Results. The\nperformance of our approach for retrieving atmospheric stellar properties from\nspectra is demonstrated using Gaia Radial Velocity Spectrograph (RVS) data from\nDR3. We use a data-driven perspective and obtain very competitive values, ali\nwithin the literature errors, and with the advantage of an important\ndimensionality reduction of the data to be processed."
    },
    {
        "date": "2025-01",
        "title": "Enhancing IoT Network Security through Adaptive Curriculum Learning and XAI",
        "author": "Sathwik Narkedimilli, Sujith Makam, Amballa Venkata Sriram, Sai Prashanth Mallellu, MSVPJ Sathvik, and Ranga Rao Venkatesha Prasad",
        "link": "http://arxiv.org/abs/2501.11618v1",
        "abstract": "To address the critical need for secure IoT networks, this study presents a\nscalable and lightweight curriculum learning framework enhanced with\nExplainable AI (XAI) techniques, including LIME, to ensure transparency and\nadaptability. The proposed model employs novel neural network architecture\nutilized at every stage of Curriculum Learning to efficiently capture and focus\non both short- and long-term temporal dependencies, improve learning stability,\nand enhance accuracy while remaining lightweight and robust against noise in\nsequential IoT data. Robustness is achieved through staged learning, where the\nmodel iteratively refines itself by removing low-relevance features and\noptimizing performance. The workflow includes edge-optimized quantization and\npruning to ensure portability that could easily be deployed in the edge-IoT\ndevices. An ensemble model incorporating Random Forest, XGBoost, and the staged\nlearning base further enhances generalization. Experimental results demonstrate\n98% accuracy on CIC-IoV-2024 and CIC-APT-IIoT-2024 datasets and 97% on\nEDGE-IIoT, establishing this framework as a robust, transparent, and\nhigh-performance solution for IoT network security."
    },
    {
        "date": "2025-01",
        "title": "Rethinking Membership Inference Attacks Against Transfer Learning",
        "author": "Cong Wu, Jing Chen, Qianru Fang, Kun He, Ziming Zhao, Hao Ren, Guowen Xu, Yang Liu, and Yang Xiang",
        "link": "http://arxiv.org/abs/2501.11577v1",
        "abstract": "Transfer learning, successful in knowledge translation across related tasks,\nfaces a substantial privacy threat from membership inference attacks (MIAs).\nThese attacks, despite posing significant risk to ML model's training data,\nremain limited-explored in transfer learning. The interaction between teacher\nand student models in transfer learning has not been thoroughly explored in\nMIAs, potentially resulting in an under-examined aspect of privacy\nvulnerabilities within transfer learning. In this paper, we propose a new MIA\nvector against transfer learning, to determine whether a specific data point\nwas used to train the teacher model while only accessing the student model in a\nwhite-box setting. Our method delves into the intricate relationship between\nteacher and student models, analyzing the discrepancies in hidden layer\nrepresentations between the student model and its shadow counterpart. These\nidentified differences are then adeptly utilized to refine the shadow model's\ntraining process and to inform membership inference decisions effectively. Our\nmethod, evaluated across four datasets in diverse transfer learning tasks,\nreveals that even when an attacker only has access to the student model, the\nteacher model's training data remains susceptible to MIAs. We believe our work\nunveils the unexplored risk of membership inference in transfer learning."
    },
    {
        "date": "2025-01",
        "title": "Graph Defense Diffusion Model",
        "author": "Xin He, Wenqi Fan, Yili Wang, Chengyi Liu, Rui Miao, Xin Juan, and Xin Wang",
        "link": "http://arxiv.org/abs/2501.11568v1",
        "abstract": "Graph Neural Networks (GNNs) demonstrate significant potential in various\napplications but remain highly vulnerable to adversarial attacks, which can\ngreatly degrade their performance. Existing graph purification methods attempt\nto address this issue by filtering attacked graphs; however, they struggle to\neffectively defend against multiple types of adversarial attacks simultaneously\ndue to their limited flexibility, and they lack comprehensive modeling of graph\ndata due to their heavy reliance on heuristic prior knowledge. To overcome\nthese challenges, we propose a more versatile approach for defending against\nadversarial attacks on graphs. In this work, we introduce the Graph Defense\nDiffusion Model (GDDM), a flexible purification method that leverages the\ndenoising and modeling capabilities of diffusion models. The iterative nature\nof diffusion models aligns well with the stepwise process of adversarial\nattacks, making them particularly suitable for defense. By iteratively adding\nand removing noise, GDDM effectively purifies attacked graphs, restoring their\noriginal structure and features. Our GDDM consists of two key components: (1)\nGraph Structure-Driven Refiner, which preserves the basic fidelity of the graph\nduring the denoising process, and ensures that the generated graph remains\nconsistent with the original scope; and (2) Node Feature-Constrained\nRegularizer, which removes residual impurities from the denoised graph, further\nenhances the purification effect. Additionally, we design tailored denoising\nstrategies to handle different types of adversarial attacks, improving the\nmodel's adaptability to various attack scenarios. Extensive experiments\nconducted on three real-world datasets demonstrate that GDDM outperforms\nstate-of-the-art methods in defending against a wide range of adversarial\nattacks, showcasing its robustness and effectiveness."
    },
    {
        "date": "2025-01",
        "title": "Secure Resource Allocation via Constrained Deep Reinforcement Learning",
        "author": "Jianfei Sun, Qiang Gao, Cong Wu, Yuxian Li, Jiacheng Wang, and Dusit Niyato",
        "link": "http://arxiv.org/abs/2501.11557v1",
        "abstract": "The proliferation of Internet of Things (IoT) devices and the advent of 6G\ntechnologies have introduced computationally intensive tasks that often surpass\nthe processing capabilities of user devices. Efficient and secure resource\nallocation in serverless multi-cloud edge computing environments is essential\nfor supporting these demands and advancing distributed computing. However,\nexisting solutions frequently struggle with the complexity of multi-cloud\ninfrastructures, robust security integration, and effective application of\ntraditional deep reinforcement learning (DRL) techniques under system\nconstraints. To address these challenges, we present SARMTO, a novel framework\nthat integrates an action-constrained DRL model. SARMTO dynamically balances\nresource allocation, task offloading, security, and performance by utilizing a\nMarkov decision process formulation, an adaptive security mechanism, and\nsophisticated optimization techniques. Extensive simulations across varying\nscenarios, including different task loads, data sizes, and MEC capacities, show\nthat SARMTO consistently outperforms five baseline approaches, achieving up to\na 40% reduction in system costs and a 41.5% improvement in energy efficiency\nover state-of-the-art methods. These enhancements highlight SARMTO's potential\nto revolutionize resource management in intricate distributed computing\nenvironments, opening the door to more efficient and secure IoT and edge\ncomputing applications."
    },
    {
        "date": "2025-01",
        "title": "An Exploratory Study on the Engineering of Security Features",
        "author": "Kevin Hermann, Sven Peldszus, Jan-Philipp Stegh\u00f6fer, and Thorsten Berger",
        "link": "http://arxiv.org/abs/2501.11546v1",
        "abstract": "Software security is of utmost importance for most software systems.\nDevelopers must systematically select, plan, design, implement, and especially\nmaintain and evolve security features -- functionalities to mitigate attacks or\nprotect personal data such as cryptography or access control, to ensure the\nsecurity of their software. While security features are usually available in\nlibraries, additional code needs to be written and maintained to integrate\nsecurity features and not all desired features can be reused this way. While\nthere have been studies on the use of such libraries, surprisingly little is\nknown about how developers engineer security features, how they select what\nsecurity features to implement, and the implications on maintenance. We\ntherefore currently rely on assumptions that are largely based on common sense\nor individual examples. However, researchers require hard empirical data to\nunderstand what practitioners need and how they view security, which we\ncurrently lack to provide them with effective solutions. We contribute an\nexploratory study with 26 knowledgeable industrial participants. We study how\nsecurity features of software systems are selected and engineered in practice,\nwhat their code-level characteristics are, and the challenges practitioners\nface. Based on the empirical data gathered, we validate four common assumptions\nand gain insights into engineering practices."
    },
    {
        "date": "2025-01",
        "title": "On the Adversarial Vulnerabilities of Transfer Learning in Remote Sensing",
        "author": "Tao Bai, Xingjian Tian, Yonghao Xu, and Bihan Wen",
        "link": "http://arxiv.org/abs/2501.11462v1",
        "abstract": "The use of pretrained models from general computer vision tasks is widespread\nin remote sensing, significantly reducing training costs and improving\nperformance. However, this practice also introduces vulnerabilities to\ndownstream tasks, where publicly available pretrained models can be used as a\nproxy to compromise downstream models. This paper presents a novel Adversarial\nNeuron Manipulation method, which generates transferable perturbations by\nselectively manipulating single or multiple neurons in pretrained models.\nUnlike existing attacks, this method eliminates the need for domain-specific\ninformation, making it more broadly applicable and efficient. By targeting\nmultiple fragile neurons, the perturbations achieve superior attack\nperformance, revealing critical vulnerabilities in deep learning models.\nExperiments on diverse models and remote sensing datasets validate the\neffectiveness of the proposed method. This low-access adversarial neuron\nmanipulation technique highlights a significant security risk in transfer\nlearning models, emphasizing the urgent need for more robust defenses in their\ndesign when addressing the safety-critical remote sensing tasks."
    },
    {
        "date": "2025-01",
        "title": "Nested Annealed Training Scheme for Generative Adversarial Networks",
        "author": "Chang Wan, Ming-Hsuan Yang, Minglu Li, Yunliang Jiang, and Zhonglong Zheng",
        "link": "http://arxiv.org/abs/2501.11318v1",
        "abstract": "Recently, researchers have proposed many deep generative models, including\ngenerative adversarial networks(GANs) and denoising diffusion models. Although\nsignificant breakthroughs have been made and empirical success has been\nachieved with the GAN, its mathematical underpinnings remain relatively\nunknown. This paper focuses on a rigorous mathematical theoretical framework:\nthe composite-functional-gradient GAN (CFG)[1]. Specifically, we reveal the\ntheoretical connection between the CFG model and score-based models. We find\nthat the training objective of the CFG discriminator is equivalent to finding\nan optimal D(x). The optimal gradient of D(x) differentiates the integral of\nthe differences between the score functions of real and synthesized samples.\nConversely, training the CFG generator involves finding an optimal G(x) that\nminimizes this difference. In this paper, we aim to derive an annealed weight\npreceding the weight of the CFG discriminator. This new explicit theoretical\nexplanation model is called the annealed CFG method. To overcome the limitation\nof the annealed CFG method, as the method is not readily applicable to the SOTA\nGAN model, we propose a nested annealed training scheme (NATS). This scheme\nkeeps the annealed weight from the CFG method and can be seamlessly adapted to\nvarious GAN models, no matter their structural, loss, or regularization\ndifferences. We conduct thorough experimental evaluations on various benchmark\ndatasets for image generation. The results show that our annealed CFG and NATS\nmethods significantly improve the quality and diversity of the synthesized\nsamples. This improvement is clear when comparing the CFG method and the SOTA\nGAN models."
    },
    {
        "date": "2025-01",
        "title": "PD-SORT: Occlusion-Robust Multi-Object Tracking Using Pseudo-Depth Cues",
        "author": "Yanchao Wang, Dawei Zhang, Run Li, Zhonglong Zheng, and Minglu Li",
        "link": "http://arxiv.org/abs/2501.11288v1",
        "abstract": "Multi-object tracking (MOT) is a rising topic in video processing\ntechnologies and has important application value in consumer electronics.\nCurrently, tracking-by-detection (TBD) is the dominant paradigm for MOT, which\nperforms target detection and association frame by frame. However, the\nassociation performance of TBD methods degrades in complex scenes with heavy\nocclusions, which hinders the application of such methods in real-world\nscenarios.To this end, we incorporate pseudo-depth cues to enhance the\nassociation performance and propose Pseudo-Depth SORT (PD-SORT). First, we\nextend the Kalman filter state vector with pseudo-depth states. Second, we\nintroduce a novel depth volume IoU (DVIoU) by combining the conventional 2D IoU\nwith pseudo-depth. Furthermore, we develop a quantized pseudo-depth measurement\n(QPDM) strategy for more robust data association. Besides, we also integrate\ncamera motion compensation (CMC) to handle dynamic camera situations. With the\nabove designs, PD-SORT significantly alleviates the occlusion-induced ambiguous\nassociations and achieves leading performances on DanceTrack, MOT17, and MOT20.\nNote that the improvement is especially obvious on DanceTrack, where objects\nshow complex motions, similar appearances, and frequent occlusions. The code is\navailable at https://github.com/Wangyc2000/PD_SORT."
    },
    {
        "date": "2025-01",
        "title": "Cybersecurity and Frequent Cyber Attacks on IoT Devices in Healthcare: Issues and Solutions",
        "author": "Zag ElSayed, Ahmed Abdelgawad, and Nelly Elsayed",
        "link": "http://arxiv.org/abs/2501.11250v1",
        "abstract": "Integrating Internet of Things (IoT) devices in healthcare has revolutionized\npatient care, offering improved monitoring, diagnostics, and treatment.\nHowever, the proliferation of these devices has also introduced significant\ncybersecurity challenges. This paper reviews the current landscape of\ncybersecurity threats targeting IoT devices in healthcare, discusses the\nunderlying issues contributing to these vulnerabilities, and explores potential\nsolutions. Additionally, this study offers solutions and suggestions for\nresearchers, agencies, and security specialists to overcome these IoT in\nhealthcare cybersecurity vulnerabilities. A comprehensive literature survey\nhighlights the nature and frequency of cyber attacks, their impact on\nhealthcare systems, and emerging strategies to mitigate these risks."
    },
    {
        "date": "2025-01",
        "title": "Conditional Feature Importance with Generative Modeling Using Adversarial Random Forests",
        "author": "Kristin Blesch, Niklas Koenen, Jan Kapar, Pegah Golchian, Lukas Burk, Markus Loecher, and Marvin N. Wright",
        "link": "http://arxiv.org/abs/2501.11178v1",
        "abstract": "This paper proposes a method for measuring conditional feature importance via\ngenerative modeling. In explainable artificial intelligence (XAI), conditional\nfeature importance assesses the impact of a feature on a prediction model's\nperformance given the information of other features. Model-agnostic post hoc\nmethods to do so typically evaluate changes in the predictive performance under\non-manifold feature value manipulations. Such procedures require creating\nfeature values that respect conditional feature distributions, which can be\nchallenging in practice. Recent advancements in generative modeling can\nfacilitate this. For tabular data, which may consist of both categorical and\ncontinuous features, the adversarial random forest (ARF) stands out as a\ngenerative model that can generate on-manifold data points without requiring\nintensive tuning efforts or computational resources, making it a promising\ncandidate model for subroutines in XAI methods. This paper proposes cARFi\n(conditional ARF feature importance), a method for measuring conditional\nfeature importance through feature values sampled from ARF-estimated\nconditional distributions. cARFi requires only little tuning to yield robust\nimportance scores that can flexibly adapt for conditional or marginal notions\nof feature importance, including straightforward extensions to condition on\nfeature subsets and allows for inferring the significance of feature\nimportances through statistical tests."
    },
    {
        "date": "2025-01",
        "title": "Counteracting temporal attacks in Video Copy Detection",
        "author": "Katarzyna Fojcik, and Piotr Syga",
        "link": "http://arxiv.org/abs/2501.11171v1",
        "abstract": "Video Copy Detection (VCD) plays a crucial role in copyright protection and\ncontent verification by identifying duplicates and near-duplicates in\nlarge-scale video databases. The META AI Challenge on video copy detection\nprovided a benchmark for evaluating state-of-the-art methods, with the\nDual-level detection approach emerging as a winning solution. This method\nintegrates Video Editing Detection and Frame Scene Detection to handle\nadversarial transformations and large datasets efficiently. However, our\nanalysis reveals significant limitations in the VED component, particularly in\nits ability to handle exact copies. Moreover, Dual-level detection shows\nvulnerability to temporal attacks. To address it, we propose an improved frame\nselection strategy based on local maxima of interframe differences, which\nenhances robustness against adversarial temporal modifications while\nsignificantly reducing computational overhead. Our method achieves an increase\nof 1.4 to 5.8 times in efficiency over the standard 1 FPS approach. Compared to\nDual-level detection method, our approach maintains comparable micro-average\nprecision ($\\mu$AP) while also demonstrating improved robustness against\ntemporal attacks. Given 56\\% reduced representation size and the inference time\nof more than 2 times faster, our approach is more suitable to real-world\nresource restriction."
    },
    {
        "date": "2025-01",
        "title": "Federated Testing (FedTest): A New Scheme to Enhance Convergence and Mitigate Adversarial Attacks in Federating Learning",
        "author": "Mustafa Ghaleb, Mohanad Obeed, Muhamad Felemban, Anas Chaaban, and Halim Yanikomeroglu",
        "link": "http://arxiv.org/abs/2501.11167v1",
        "abstract": "Federated Learning (FL) has emerged as a significant paradigm for training\nmachine learning models. This is due to its data-privacy-preserving property\nand its efficient exploitation of distributed computational resources. This is\nachieved by conducting the training process in parallel at distributed users.\nHowever, traditional FL strategies grapple with difficulties in evaluating the\nquality of received models, handling unbalanced models, and reducing the impact\nof detrimental models. To resolve these problems, we introduce a novel\nfederated learning framework, which we call federated testing for federated\nlearning (FedTest). In the FedTest method, the local data of a specific user is\nused to train the model of that user and test the models of the other users.\nThis approach enables users to test each other's models and determine an\naccurate score for each. This score can then be used to aggregate the models\nefficiently and identify any malicious ones. Our numerical results reveal that\nthe proposed method not only accelerates convergence rates but also diminishes\nthe potential influence of malicious users. This significantly enhances the\noverall efficiency and robustness of FL systems."
    },
    {
        "date": "2025-01",
        "title": "A Novel Pearson Correlation-Based Merging Algorithm for Robust Distributed Machine Learning with Heterogeneous Data",
        "author": "Mohammad Ghabel Rahmat, and Majid Khalilian",
        "link": "http://arxiv.org/abs/2501.11112v2",
        "abstract": "Federated learning faces significant challenges in scenarios with\nheterogeneous data distributions and adverse network conditions, such as\ndelays, packet loss, and data poisoning attacks. This paper proposes a novel\nmethod based on the SCAFFOLD algorithm to improve the quality of local updates\nand enhance the robustness of the global model. The key idea is to form\nintermediary nodes by merging local models with high similarity, using the\nPearson correlation coefficient as a similarity measure. The proposed merging\nalgorithm reduces the number of local nodes while maintaining the accuracy of\nthe global model, effectively addressing communication overhead and bandwidth\nconsumption. Experimental results on the MNIST dataset under simulated\nfederated learning scenarios demonstrate the method's effectiveness. After 10\nrounds of training using a CNN model, the proposed approach achieved accuracies\nof 0.82, 0.73, and 0.66 under normal conditions, packet loss and data poisoning\nattacks, respectively, outperforming the baseline SCAFFOLD algorithm. These\nresults highlight the potential of the proposed method to improve efficiency\nand resilience in federated learning systems."
    },
    {
        "date": "2025-01",
        "title": "Enhancing Sample Utilization in Noise-Robust Deep Metric Learning With Subgroup-Based Positive-Pair Selection",
        "author": "Zhipeng Yu, Qianqian Xu, Yangbangyan Jiang, Yingfei Sun, and Qingming Huang",
        "link": "http://arxiv.org/abs/2501.11063v1",
        "abstract": "The existence of noisy labels in real-world data negatively impacts the\nperformance of deep learning models. Although much research effort has been\ndevoted to improving the robustness towards noisy labels in classification\ntasks, the problem of noisy labels in deep metric learning (DML) remains\nunder-explored. Existing noisy label learning methods designed for DML mainly\ndiscard suspicious noisy samples, resulting in a waste of the training data. To\naddress this issue, we propose a noise-robust DML framework with SubGroup-based\nPositive-pair Selection (SGPS), which constructs reliable positive pairs for\nnoisy samples to enhance the sample utilization. Specifically, SGPS first\neffectively identifies clean and noisy samples by a probability-based clean\nsample selectionstrategy. To further utilize the remaining noisy samples, we\ndiscover their potential similar samples based on the subgroup information\ngiven by a subgroup generation module and then aggregate them into informative\npositive prototypes for each noisy sample via a positive prototype generation\nmodule. Afterward, a new contrastive loss is tailored for the noisy samples\nwith their selected positive pairs. SGPS can be easily integrated into the\ntraining process of existing pair-wise DML tasks, like image retrieval and face\nrecognition. Extensive experiments on multiple synthetic and real-world\nlarge-scale label noise datasets demonstrate the effectiveness of our proposed\nmethod. Without any bells and whistles, our SGPS framework outperforms the\nstate-of-the-art noisy label DML methods. Code is available at\n\\url{https://github.com/smuelpeng/SGPS-NoiseFreeDML}."
    },
    {
        "date": "2025-01",
        "title": "Temporal Analysis of Adversarial Attacks in Federated Learning",
        "author": "Rohit Mapakshi, Sayma Akther, and Mark Stamp",
        "link": "http://arxiv.org/abs/2501.11054v1",
        "abstract": "In this paper, we experimentally analyze the robustness of selected Federated\nLearning (FL) systems in the presence of adversarial clients. We find that\ntemporal attacks significantly affect model performance in the FL models\ntested, especially when the adversaries are active throughout or during the\nlater rounds. We consider a variety of classic learning models, including\nMultinominal Logistic Regression (MLR), Random Forest, XGBoost, Support Vector\nClassifier (SVC), as well as various Neural Network models including Multilayer\nPerceptron (MLP), Convolution Neural Network (CNN), Recurrent Neural Network\n(RNN), and Long Short-Term Memory (LSTM). Our results highlight the\neffectiveness of temporal attacks and the need to develop strategies to make\nthe FL process more robust against such attacks. We also briefly consider the\neffectiveness of defense mechanisms, including outlier detection in the\naggregation algorithm."
    },
    {
        "date": "2025-01",
        "title": "Bridging the Security Gap: Lessons from 5G and What 6G Should Do Better",
        "author": "Isabella D. Lutz, and Matthew C. Valenti",
        "link": "http://arxiv.org/abs/2501.11045v1",
        "abstract": "The security requirements for future 6G mobile networks are anticipated to be\nsignificantly more complex and demanding than those of 5G. This increase stems\nfrom several factors: the proliferation of massive machine-type communications\nwill dramatically increase the density of devices competing for network access;\nsecure ultra-reliable low-latency communication will impose stringent\nrequirements on security, latency, and reliability; and the widespread\ndeployment of small cells and non-terrestrial networks, including satellite\nmega-constellations, will result in more frequent handovers. This paper\nprovides a set of security recommendations for 6G networks, with a particular\nfocus on access and handover procedures, which often lack encryption and\nintegrity protection, making them more vulnerable to exploitation. Since 6G is\nexpected to be a backward-compatible extension of 5G, and given that secure\nsystems cannot be effectively designed without a clear understanding of their\ngoals, it is imperative to first evaluate the limitations of the current\ngeneration. To this end, the paper begins by reviewing existing 5G access and\nauthentication mechanisms, highlighting several critical vulnerabilities in\nthese procedures. It then examines potential 6G challenges and concludes with\nactionable recommendations to enhance the security, resilience, and robustness\nof 6G access and handover mechanisms."
    },
    {
        "date": "2025-01",
        "title": "Beyond Any-Shot Adaptation: Predicting Optimization Outcome for Robustness Gains without Extra Pay",
        "author": "Qi Cheems Wang, Zehao Xiao, Yixiu Mao, Yun Qu, Jiayi Shen, Yiqin Lv, and Xiangyang Ji",
        "link": "http://arxiv.org/abs/2501.11039v1",
        "abstract": "The foundation model enables fast problem-solving without learning from\nscratch, and such a desirable adaptation property benefits from its adopted\ncross-task generalization paradigms, e.g., pretraining, meta-training, or\nfinetuning. Recent trends have focused on the curation of task datasets during\noptimization, which includes task selection as an indispensable consideration\nfor either adaptation robustness or sampling efficiency purposes. Despite some\nprogress, selecting crucial task batches to optimize over iteration mostly\nexhausts massive task queries and requires intensive evaluation and\ncomputations to secure robust adaptation. This work underscores the criticality\nof both robustness and learning efficiency, especially in scenarios where tasks\nare risky to collect or costly to evaluate. To this end, we present Model\nPredictive Task Sampling (MPTS), a novel active task sampling framework to\nestablish connections between the task space and adaptation risk landscape\nachieve robust adaptation. Technically, MPTS characterizes the task episodic\ninformation with a generative model and predicts optimization outcome after\nadaptation from posterior inference, i.e., forecasting task-specific adaptation\nrisk values. The resulting risk learner amortizes expensive annotation,\nevaluation, or computation operations in task robust adaptation learning\nparadigms. Extensive experimental results show that MPTS can be seamlessly\nintegrated into zero-shot, few-shot, and many-shot learning paradigms,\nincreases adaptation robustness, and retains learning efficiency without\naffording extra cost. The code will be available at the project site\nhttps://github.com/thu-rllab/MPTS."
    },
    {
        "date": "2025-01",
        "title": "Effectiveness of Adversarial Benign and Malware Examples in Evasion and Poisoning Attacks",
        "author": "Matou\u0161 Koz\u00e1k, and Martin Jure\u010dek",
        "link": "http://arxiv.org/abs/2501.10996v1",
        "abstract": "Adversarial attacks present significant challenges for malware detection\nsystems. This research investigates the effectiveness of benign and malicious\nadversarial examples (AEs) in evasion and poisoning attacks on the Portable\nExecutable file domain. A novel focus of this study is on benign AEs, which,\nalthough not directly harmful, can increase false positives and undermine trust\nin antivirus solutions. We propose modifying existing adversarial malware\ngenerators to produce benign AEs and show they are as successful as malware AEs\nin evasion attacks. Furthermore, our data show that benign AEs have a more\ndecisive influence in poisoning attacks than standard malware AEs,\ndemonstrating their superior ability to decrease the model's performance. Our\nfindings introduce new opportunities for adversaries and further increase the\nattack surface that needs to be protected by security researchers."
    },
    {
        "date": "2025-01",
        "title": "GRID: Protecting Training Graph from Link Stealing Attacks on GNN Models",
        "author": "Jiadong Lou, Xu Yuan, Rui Zhang, Xingliang Yuan, Neil Gong, and Nian-Feng Tzeng",
        "link": "http://arxiv.org/abs/2501.10985v1",
        "abstract": "Graph neural networks (GNNs) have exhibited superior performance in various\nclassification tasks on graph-structured data. However, they encounter the\npotential vulnerability from the link stealing attacks, which can infer the\npresence of a link between two nodes via measuring the similarity of its\nincident nodes' prediction vectors produced by a GNN model. Such attacks pose\nsevere security and privacy threats to the training graph used in GNN models.\nIn this work, we propose a novel solution, called Graph Link Disguise (GRID),\nto defend against link stealing attacks with the formal guarantee of GNN model\nutility for retaining prediction accuracy. The key idea of GRID is to add\ncarefully crafted noises to the nodes' prediction vectors for disguising\nadjacent nodes as n-hop indirect neighboring nodes. We take into account the\ngraph topology and select only a subset of nodes (called core nodes) covering\nall links for adding noises, which can avert the noises offset and have the\nfurther advantages of reducing both the distortion loss and the computation\ncost. Our crafted noises can ensure 1) the noisy prediction vectors of any two\nadjacent nodes have their similarity level like that of two non-adjacent nodes\nand 2) the model prediction is unchanged to ensure zero utility loss. Extensive\nexperiments on five datasets are conducted to show the effectiveness of our\nproposed GRID solution against different representative link-stealing attacks\nunder transductive settings and inductive settings respectively, as well as two\ninfluence-based attacks. Meanwhile, it achieves a much better privacy-utility\ntrade-off than existing methods when extended to GNNs."
    },
    {
        "date": "2025-01",
        "title": "CIBPU: A Conflict-Invisible Secure Branch Prediction Unit",
        "author": "Zhe Zhou, Fei Tong, Hongyu Wang, Xiaoyu Cheng, Fang Jiang, Zhikun Zhang, and Yuxing Mao",
        "link": "http://arxiv.org/abs/2501.10983v1",
        "abstract": "Previous schemes for designing secure branch prediction unit (SBPU) based on\nphysical isolation can only offer limited security and significantly affect\nBPU's prediction capability, leading to prominent performance degradation.\nMoreover, encryption-based SBPU schemes based on periodic key re-randomization\nhave the risk of being compromised by advanced attack algorithms, and the\nperformance overhead is also considerable. To this end, this paper proposes a\nconflict-invisible SBPU (CIBPU). CIBPU employs redundant storage design,\nload-aware indexing, and replacement design, as well as an encryption mechanism\nwithout requiring periodic key updates, to prevent attackers' perception of\nbranch conflicts. We provide a thorough security analysis, which shows that\nCIBPU achieves strong security throughout the BPU's lifecycle. We implement\nCIBPU in a RISC-V core model in gem5. The experimental results show that CIBPU\ncauses an average performance overhead of only 1.12%-2.20% with acceptable\nhardware storage overhead, which is the lowest among the state-of-the-art SBPU\nschemes. CIBPU has also been implemented in the open-source RISC-V core,\nSonicBOOM, which is then burned onto an FPGA board. The evaluation based on the\nboard shows an average performance degradation of 2.01%, which is approximately\nconsistent with the result obtained in gem5."
    },
    {
        "date": "2025-01",
        "title": "TSVC:Tripartite Learning with Semantic Variation Consistency for Robust Image-Text Retrieval",
        "author": "Shuai Lyu, Zijing Tian, Zhonghong Ou, Yifan Zhu, Xiao Zhang, Qiankun Ha, Haoran Luo, and Meina Song",
        "link": "http://arxiv.org/abs/2501.10935v1",
        "abstract": "Cross-modal retrieval maps data under different modality via semantic\nrelevance. Existing approaches implicitly assume that data pairs are\nwell-aligned and ignore the widely existing annotation noise, i.e., noisy\ncorrespondence (NC). Consequently, it inevitably causes performance\ndegradation. Despite attempts that employ the co-teaching paradigm with\nidentical architectures to provide distinct data perspectives, the differences\nbetween these architectures are primarily stemmed from random initialization.\nThus, the model becomes increasingly homogeneous along with the training\nprocess. Consequently, the additional information brought by this paradigm is\nseverely limited. In order to resolve this problem, we introduce a Tripartite\nlearning with Semantic Variation Consistency (TSVC) for robust image-text\nretrieval. We design a tripartite cooperative learning mechanism comprising a\nCoordinator, a Master, and an Assistant model. The Coordinator distributes\ndata, and the Assistant model supports the Master model's noisy label\nprediction with diverse data. Moreover, we introduce a soft label estimation\nmethod based on mutual information variation, which quantifies the noise in new\nsamples and assigns corresponding soft labels. We also present a new loss\nfunction to enhance robustness and optimize training effectiveness. Extensive\nexperiments on three widely used datasets demonstrate that, even at increasing\nnoise ratios, TSVC exhibits significant advantages in retrieval accuracy and\nmaintains stable training performance."
    },
    {
        "date": "2025-01",
        "title": "LegalGuardian: A Privacy-Preserving Framework for Secure Integration of Large Language Models in Legal Practice",
        "author": "M. Mikail Demir, Hakan T. Otal, and M. Abdullah Canbaz",
        "link": "http://arxiv.org/abs/2501.10915v1",
        "abstract": "Large Language Models (LLMs) hold promise for advancing legal practice by\nautomating complex tasks and improving access to justice. However, their\nadoption is limited by concerns over client confidentiality, especially when\nlawyers include sensitive Personally Identifiable Information (PII) in prompts,\nrisking unauthorized data exposure. To mitigate this, we introduce\nLegalGuardian, a lightweight, privacy-preserving framework tailored for lawyers\nusing LLM-based tools. LegalGuardian employs Named Entity Recognition (NER)\ntechniques and local LLMs to mask and unmask confidential PII within prompts,\nsafeguarding sensitive data before any external interaction. We detail its\ndevelopment and assess its effectiveness using a synthetic prompt library in\nimmigration law scenarios. Comparing traditional NER models with one-shot\nprompted local LLM, we find that LegalGuardian achieves a F1-score of 93% with\nGLiNER and 97% with Qwen2.5-14B in PII detection. Semantic similarity analysis\nconfirms that the framework maintains high fidelity in outputs, ensuring robust\nutility of LLM-based tools. Our findings indicate that legal professionals can\nharness advanced AI technologies without compromising client confidentiality or\nthe quality of legal documents."
    },
    {
        "date": "2025-01",
        "title": "Explainable Adversarial Attacks on Coarse-to-Fine Classifiers",
        "author": "Akram Heidarizadeh, Connor Hatfield, Lorenzo Lazzarotto, HanQin Cai, and George Atia",
        "link": "http://arxiv.org/abs/2501.10906v1",
        "abstract": "Traditional adversarial attacks typically aim to alter the predicted labels\nof input images by generating perturbations that are imperceptible to the human\neye. However, these approaches often lack explainability. Moreover, most\nexisting work on adversarial attacks focuses on single-stage classifiers, but\nmulti-stage classifiers are largely unexplored. In this paper, we introduce\ninstance-based adversarial attacks for multi-stage classifiers, leveraging\nLayer-wise Relevance Propagation (LRP), which assigns relevance scores to\npixels based on their influence on classification outcomes. Our approach\ngenerates explainable adversarial perturbations by utilizing LRP to identify\nand target key features critical for both coarse and fine-grained\nclassifications. Unlike conventional attacks, our method not only induces\nmisclassification but also enhances the interpretability of the model's\nbehavior across classification stages, as demonstrated by experimental results."
    },
    {
        "date": "2025-01",
        "title": "A Generative Security Application Engineering Curriculum",
        "author": "Wu-chang Feng, and David Baker-Robinson",
        "link": "http://arxiv.org/abs/2501.10900v1",
        "abstract": "Generative AI and large language models (LLMs) are transforming security by\nautomating many tasks being performed manually. With such automation changing\nthe practice of security as we know it, it is imperative that we prepare future\nstudents for the technology landscape they will ultimately face. Towards this\nend, we describe an initial curriculum and course that attempts to show\nstudents how to apply generative AI in order to solve problems in security. By\nrefocusing security education and training on aspects uniquely suited for\nhumans and showing students how to leverage automation for the rest, we believe\nwe can better align security education practices with generative AI as it\nevolves."
    },
    {
        "date": "2025-01",
        "title": "Certifying Robustness via Topological Representations",
        "author": "Jens Agerberg, Andrea Guidolin, Andrea Martinelli, Pepijn Roos Hoefgeest, David Eklund, and Martina Scolamiero",
        "link": "http://arxiv.org/abs/2501.10876v1",
        "abstract": "We propose a neural network architecture that can learn discriminative\ngeometric representations of data from persistence diagrams, common descriptors\nof Topological Data Analysis. The learned representations enjoy Lipschitz\nstability with a controllable Lipschitz constant. In adversarial learning, this\nstability can be used to certify $\\epsilon$-robustness for samples in a\ndataset, which we demonstrate on the ORBIT5K dataset representing the orbits of\na discrete dynamical system."
    },
    {
        "date": "2025-01",
        "title": "Model-Robust and Adaptive-Optimal Transfer Learning for Tackling Concept Shifts in Nonparametric Regression",
        "author": "Haotian Lin, and Matthew Reimherr",
        "link": "http://arxiv.org/abs/2501.10870v1",
        "abstract": "When concept shifts and sample scarcity are present in the target domain of\ninterest, nonparametric regression learners often struggle to generalize\neffectively. The technique of transfer learning remedies these issues by\nleveraging data or pre-trained models from similar source domains. While\nexisting generalization analyses of kernel-based transfer learning typically\nrely on correctly specified models, we present a transfer learning procedure\nthat is robust against model misspecification while adaptively attaining\noptimality. To facilitate our analysis and avoid the risk of saturation found\nin classical misspecified results, we establish a novel result in the\nmisspecified single-task learning setting, showing that spectral algorithms\nwith fixed bandwidth Gaussian kernels can attain minimax convergence rates\ngiven the true function is in a Sobolev space, which may be of independent\ninterest. Building on this, we derive the adaptive convergence rates of the\nexcess risk for specifying Gaussian kernels in a prevalent class of hypothesis\ntransfer learning algorithms. Our results are minimax optimal up to logarithmic\nfactors and elucidate the key determinants of transfer efficiency."
    },
    {
        "date": "2025-01",
        "title": "A comprehensive survey on RPL routing-based attacks, defences and future directions in Internet of Things",
        "author": "Anil K Prajapati, Emmanuel S Pilli, Ramesh B Battula, Vijay Varadharajan, Abhishek Verma, and R C Joshi",
        "link": "http://arxiv.org/abs/2501.10817v1",
        "abstract": "The Internet of Things (IoT) is a network of digital devices like sensors,\nprocessors, embedded and communication devices that can connect to and exchange\ndata with other devices and systems over the internet. IoT devices have\nlimitations on power, memory, and computational resources. Researchers have\ndeveloped the IPv6 Over Low-power Wireless Personal Area Network (6LoWPAN)\nprotocols to provide wireless connectivity among these devices while overcoming\nthe constraints on resources. 6LoWPAN has been approved subsequently by the\nInternet Engineering Task Force (IETF). The IETF Routing Over Low-power and\nLossy Networks (ROLL) standardized the Routing Protocol for LLNs known as RPL\n(IETF RFC 6550), which is part of the 6LoWPAN stack. However, IoT devices are\nvulnerable to various attacks on RPL-based routing. This survey provides an in\ndepth study of existing RPL-based attacks and defense published from year 2011\nto 2024 from highly reputed journals and conferences. By thematic analysis of\nexisting routing attacks on RPL, we developed a novel attack taxonomy which\nfocuses on the nature of routing attacks and classifies them into 12 major\ncategories. Subsequently, the impact of each attack on the network is analyzed\nand discussed real life scenarios of these attacks. Another contribution of\nthis survey proposed a novel taxonomy for classification of defense mechanisms\ninto 8 major categories against routing attacks based on type of defense\nstrategy. The detailed analysis of each defense mechanism with real life\napplicability is explained. Furthermore, evaluation tools such as testbeds and\nsimulators for RPL-based attack and defense are discussed and critically\nanalyzed in terms of real world applicability. Finally, open research\nchallenges are presented on the basis of research gaps of existing literature\nalong with research directions for practitioners and researchers."
    },
    {
        "date": "2025-01",
        "title": "Robust Local Polynomial Regression with Similarity Kernels",
        "author": "Yaniv Shulman",
        "link": "http://arxiv.org/abs/2501.10729v1",
        "abstract": "Local Polynomial Regression (LPR) is a widely used nonparametric method for\nmodeling complex relationships due to its flexibility and simplicity. It\nestimates a regression function by fitting low-degree polynomials to localized\nsubsets of the data, weighted by proximity. However, traditional LPR is\nsensitive to outliers and high-leverage points, which can significantly affect\nestimation accuracy. This paper revisits the kernel function used to compute\nregression weights and proposes a novel framework that incorporates both\npredictor and response variables in the weighting mechanism. By introducing two\npositive definite kernels, the proposed method robustly estimates weights,\nmitigating the influence of outliers through localized density estimation. The\nmethod is implemented in Python and is publicly available at\nhttps://github.com/yaniv-shulman/rsklpr, demonstrating competitive performance\nin synthetic benchmark experiments. Compared to standard LPR, the proposed\napproach consistently improves robustness and accuracy, especially in\nheteroscedastic and noisy environments, without requiring multiple iterations.\nThis advancement provides a promising extension to traditional LPR, opening new\npossibilities for robust regression applications."
    },
    {
        "date": "2025-01",
        "title": "Distributionally Robust Policy Evaluation and Learning for Continuous Treatment with Observational Data",
        "author": "Cheuk Hang Leung, Yiyan Huang, Yijun Li, and Qi Wu",
        "link": "http://arxiv.org/abs/2501.10693v1",
        "abstract": "Using offline observational data for policy evaluation and learning allows\ndecision-makers to evaluate and learn a policy that connects characteristics\nand interventions. Most existing literature has focused on either discrete\ntreatment spaces or assumed no difference in the distributions between the\npolicy-learning and policy-deployed environments. These restrict applications\nin many real-world scenarios where distribution shifts are present with\ncontinuous treatment. To overcome these challenges, this paper focuses on\ndeveloping a distributionally robust policy under a continuous treatment\nsetting. The proposed distributionally robust estimators are established using\nthe Inverse Probability Weighting (IPW) method extended from the discrete one\nfor policy evaluation and learning under continuous treatments. Specifically,\nwe introduce a kernel function into the proposed IPW estimator to mitigate the\nexclusion of observations that can occur in the standard IPW method to\ncontinuous treatments. We then provide finite-sample analysis that guarantees\nthe convergence of the proposed distributionally robust policy evaluation and\nlearning estimators. The comprehensive experiments further verify the\neffectiveness of our approach when distribution shifts are present."
    },
    {
        "date": "2025-01",
        "title": "Latent-space adversarial training with post-aware calibration for defending large language models against jailbreak attacks",
        "author": "Xin Yi, Yue Li, Linlin Wang, Xiaoling Wang, and Liang He",
        "link": "http://arxiv.org/abs/2501.10639v1",
        "abstract": "Ensuring safety alignment has become a critical requirement for large\nlanguage models (LLMs), particularly given their widespread deployment in\nreal-world applications. However, LLMs remain susceptible to jailbreak attacks,\nwhich exploit system vulnerabilities to bypass safety measures and generate\nharmful outputs. Although numerous defense mechanisms based on adversarial\ntraining have been proposed, a persistent challenge lies in the exacerbation of\nover-refusal behaviors, which compromise the overall utility of the model. To\naddress these challenges, we propose a Latent-space Adversarial Training with\nPost-aware Calibration (LATPC) framework. During the adversarial training\nphase, LATPC compares harmful and harmless instructions in the latent space and\nextracts safety-critical dimensions to construct refusal features attack,\nprecisely simulating agnostic jailbreak attack types requiring adversarial\nmitigation. At the inference stage, an embedding-level calibration mechanism is\nemployed to alleviate over-refusal behaviors with minimal computational\noverhead. Experimental results demonstrate that, compared to various defense\nmethods across five types of jailbreak attacks, LATPC framework achieves a\nsuperior balance between safety and utility. Moreover, our analysis underscores\nthe effectiveness of extracting safety-critical dimensions from the latent\nspace for constructing robust refusal feature attacks."
    },
    {
        "date": "2025-01",
        "title": "Differentiable Adversarial Attacks for Marked Temporal Point Processes",
        "author": "Pritish Chakraborty, Vinayak Gupta, Rahul R, Srikanta J. Bedathur, and Abir De",
        "link": "http://arxiv.org/abs/2501.10606v1",
        "abstract": "Marked temporal point processes (MTPPs) have been shown to be extremely\neffective in modeling continuous time event sequences (CTESs). In this work, we\npresent adversarial attacks designed specifically for MTPP models. A key\ncriterion for a good adversarial attack is its imperceptibility. For objects\nsuch as images or text, this is often achieved by bounding perturbation in some\nfixed $L_p$ norm-ball. However, similarly minimizing distance norms between two\nCTESs in the context of MTPPs is challenging due to their sequential nature and\nvarying time-scales and lengths. We address this challenge by first permuting\nthe events and then incorporating the additive noise to the arrival timestamps.\nHowever, the worst case optimization of such adversarial attacks is a hard\ncombinatorial problem, requiring exploration across a permutation space that is\nfactorially large in the length of the input sequence. As a result, we propose\na novel differentiable scheme PERMTPP using which we can perform adversarial\nattacks by learning to minimize the likelihood, while minimizing the distance\nbetween two CTESs. Our experiments on four real-world datasets demonstrate the\noffensive and defensive capabilities, and lower inference times of PERMTPP."
    },
    {
        "date": "2025-01",
        "title": "Picachv: Formally Verified Data Use Policy Enforcement for Secure Data Analytics",
        "author": "Haobin Hiroki Chen, Hongbo Chen, Mingshen Sun, Chenghong Wang, and XiaoFeng Wang",
        "link": "http://arxiv.org/abs/2501.10560v1",
        "abstract": "Ensuring the proper use of sensitive data in analytics under complex privacy\npolicies is an increasingly critical challenge. Many existing approaches lack\nportability, verifiability, and scalability across diverse data processing\nframeworks. We introduce Picachv, a novel security monitor that automatically\nenforces data use policies. It works on relational algebra as an abstraction\nfor program semantics, enabling policy enforcement on query plans generated by\nprograms during execution. This approach simplifies analysis across diverse\nanalytical operations and supports various front-end query languages. By\nformalizing both data use policies and relational algebra semantics in Coq, we\nprove that Picachv correctly enforces policies. Picachv also leverages Trusted\nExecution Environments (TEEs) to enhance trust in runtime, providing provable\npolicy compliance to stakeholders that the analytical tasks comply with their\ndata use policies. We integrated Picachv into Polars, a state-of-the-art data\nanalytics framework, and evaluate its performance using the TPC-H benchmark. We\nalso apply our approach to real-world use cases. Our work demonstrates the\npractical application of formal methods in securing data analytics, addressing\nkey challenges."
    },
    {
        "date": "2025-01",
        "title": "Credit Risk Identification in Supply Chains Using Generative Adversarial Networks",
        "author": "Zizhou Zhang, Xinshi Li, Yu Cheng, Zhenrui Chen, and Qianying Liu",
        "link": "http://arxiv.org/abs/2501.10348v3",
        "abstract": "Credit risk management within supply chains has emerged as a critical\nresearch area due to its significant implications for operational stability and\nfinancial sustainability. The intricate interdependencies among supply chain\nparticipants mean that credit risks can propagate across networks, with impacts\nvarying by industry. This study explores the application of Generative\nAdversarial Networks (GANs) to enhance credit risk identification in supply\nchains. GANs enable the generation of synthetic credit risk scenarios,\naddressing challenges related to data scarcity and imbalanced datasets. By\nleveraging GAN-generated data, the model improves predictive accuracy while\neffectively capturing dynamic and temporal dependencies in supply chain data.\nThe research focuses on three representative industries-manufacturing (steel),\ndistribution (pharmaceuticals), and services (e-commerce) to assess\nindustry-specific credit risk contagion. Experimental results demonstrate that\nthe GAN-based model outperforms traditional methods, including logistic\nregression, decision trees, and neural networks, achieving superior accuracy,\nrecall, and F1 scores. The findings underscore the potential of GANs in\nproactive risk management, offering robust tools for mitigating financial\ndisruptions in supply chains. Future research could expand the model by\nincorporating external market factors and supplier relationships to further\nenhance predictive capabilities. Keywords- Generative Adversarial Networks\n(GANs); Supply Chain Risk; Credit Risk Identification; Machine Learning; Data\nAugmentation"
    },
    {
        "date": "2025-01",
        "title": "Hierarchical Autoregressive Transformers: Combining Byte- and Word-Level Processing for Robust, Adaptable Language Models",
        "author": "Pit Neitemeier, Bj\u00f6rn Deiseroth, Constantin Eichenberg, and Lukas Balles",
        "link": "http://arxiv.org/abs/2501.10322v2",
        "abstract": "Tokenization is a fundamental step in natural language processing, breaking\ntext into units that computational models can process. While learned subword\ntokenizers have become the de-facto standard, they present challenges such as\nlarge vocabularies, limited adaptability to new domains or languages, and\nsensitivity to spelling errors and variations. To overcome these limitations,\nwe investigate a hierarchical architecture for autoregressive language\nmodelling that combines character-level and word-level processing. It employs a\nlightweight character-level encoder to convert character sequences into word\nembeddings, which are then processed by a word-level backbone model and decoded\nback into characters via a compact character-level decoder. This method retains\nthe sequence compression benefits of word-level tokenization without relying on\na rigid, predefined vocabulary. We demonstrate, at scales up to 7 billion\nparameters, that hierarchical transformers match the downstream task\nperformance of subword-tokenizer-based models while exhibiting significantly\ngreater robustness to input perturbations. Additionally, during continued\npretraining on an out-of-domain language, our model trains almost twice as\nfast, achieves superior performance on the target language, and retains more of\nits previously learned knowledge. Hierarchical transformers pave the way for\nNLP systems that are more robust, flexible, and generalizable across languages\nand domains."
    },
    {
        "date": "2025-01",
        "title": "Robust Egoistic Rigid Body Localization",
        "author": "Niclas F\u00fchrling, Giuseppe Thadeu Freitas de Abreu, David Gonz\u00e1lez G., and Osvaldo Gonsa",
        "link": "http://arxiv.org/abs/2501.10219v1",
        "abstract": "We consider a robust and self-reliant (or \"egoistic\") variation of the rigid\nbody localization (RBL) problem, in which a primary rigid body seeks to\nestimate the pose (i.e., location and orientation) of another rigid body (or\n\"target\"), relative to its own, without the assistance of external\ninfrastructure, without prior knowledge of the shape of the target, and taking\ninto account the possibility that the available observations are incomplete.\nThree complementary contributions are then offered for such a scenario. The\nfirst is a method to estimate the translation vector between the center point\nof both rigid bodies, which unlike existing techniques does not require that\nboth objects have the same shape or even the same number of landmark points.\nThis technique is shown to significantly outperform the state-of-the-art (SotA)\nunder complete information, but to be sensitive to data erasures, even when\nenhanced by matrix completion methods. The second contribution, designed to\noffer improved performance in the presence of incomplete information, offers a\nrobust alternative to the latter, at the expense of a slight relative loss\nunder complete information. Finally, the third contribution is a scheme for the\nestimation of the rotation matrix describing the relative orientation of the\ntarget rigid body with respect to the primary. Comparisons of the proposed\nschemes and SotA techniques demonstrate the advantage of the contributed\nmethods in terms of root mean square error (RMSE) performance under fully\ncomplete information and incomplete conditions."
    },
    {
        "date": "2025-01",
        "title": "Provably Safeguarding a Classifier from OOD and Adversarial Samples: an Extreme Value Theory Approach",
        "author": "Nicolas Atienza, Christophe Labreuche, Johanne Cohen, and Michele Sebag",
        "link": "http://arxiv.org/abs/2501.10202v1",
        "abstract": "This paper introduces a novel method, Sample-efficient Probabilistic\nDetection using Extreme Value Theory (SPADE), which transforms a classifier\ninto an abstaining classifier, offering provable protection against\nout-of-distribution and adversarial samples. The approach is based on a\nGeneralized Extreme Value (GEV) model of the training distribution in the\nclassifier's latent space, enabling the formal characterization of OOD samples.\nInterestingly, under mild assumptions, the GEV model also allows for formally\ncharacterizing adversarial samples. The abstaining classifier, which rejects\nsamples based on their assessment by the GEV model, provably avoids OOD and\nadversarial samples. The empirical validation of the approach, conducted on\nvarious neural architectures (ResNet, VGG, and Vision Transformer) and medium\nand large-sized datasets (CIFAR-10, CIFAR-100, and ImageNet), demonstrates its\nfrugality, stability, and efficiency compared to the state of the art."
    },
    {
        "date": "2025-01",
        "title": "Contributions to the Decision Theoretic Foundations of Machine Learning and Robust Statistics under Weakly Structured Information",
        "author": "Christoph Jansen",
        "link": "http://arxiv.org/abs/2501.10195v1",
        "abstract": "This habilitation thesis is cumulative and, therefore, is collecting and\nconnecting research that I (together with several co-authors) have conducted\nover the last few years. Thus, the absolute core of the work is formed by the\nten publications listed on page 5 under the name Contributions 1 to 10. The\nreferences to the complete versions of these articles are also found in this\nlist, making them as easily accessible as possible for readers wishing to dive\ndeep into the different research projects. The chapters following this thesis,\nnamely Parts A to C and the concluding remarks, serve to place the articles in\na larger scientific context, to (briefly) explain their respective content on a\nless formal level, and to highlight some interesting perspectives for future\nresearch in their respective contexts. Naturally, therefore, the following\npresentation has neither the level of detail nor the formal rigor that can\n(hopefully) be found in the papers. The purpose of the following text is to\nprovide the reader an easy and high-level access to this interesting and\nimportant research field as a whole, thereby, advertising it to a broader\naudience."
    },
    {
        "date": "2025-01",
        "title": "Secure Semantic Communication With Homomorphic Encryption",
        "author": "Rui Meng, Dayu Fan, Haixiao Gao, Yifan Yuan, Bizhu Wang, Xiaodong Xu, Mengying Sun, Chen Dong, Xiaofeng Tao, Ping Zhang, and Dusit Niyato",
        "link": "http://arxiv.org/abs/2501.10182v1",
        "abstract": "In recent years, Semantic Communication (SemCom), which aims to achieve\nefficient and reliable transmission of meaning between agents, has garnered\nsignificant attention from both academia and industry. To ensure the security\nof communication systems, encryption techniques are employed to safeguard\nconfidentiality and integrity. However, traditional cryptography-based\nencryption algorithms encounter obstacles when applied to SemCom. Motivated by\nthis, this paper explores the feasibility of applying homomorphic encryption to\nSemCom. Initially, we review the encryption algorithms utilized in mobile\ncommunication systems and analyze the challenges associated with their\napplication to SemCom. Subsequently, we employ scale-invariant feature\ntransform to demonstrate that semantic features can be preserved in homomorphic\nencrypted ciphertext. Based on this finding, we propose a task-oriented SemCom\nscheme secured through homomorphic encryption. We design the privacy preserved\ndeep joint source-channel coding (JSCC) encoder and decoder, and the frequency\nof key updates can be adjusted according to service requirements without\ncompromising transmission performance. Simulation results validate that, when\ncompared to plaintext images, the proposed scheme can achieve almost the same\nclassification accuracy performance when dealing with homomorphic ciphertext\nimages. Furthermore, we provide potential future research directions for\nhomomorphic encrypted SemCom."
    },
    {
        "date": "2025-01",
        "title": "Robotic World Model: A Neural Network Simulator for Robust Policy Optimization in Robotics",
        "author": "Chenhao Li, Andreas Krause, and Marco Hutter",
        "link": "http://arxiv.org/abs/2501.10100v1",
        "abstract": "Learning robust and generalizable world models is crucial for enabling\nefficient and scalable robotic control in real-world environments. In this\nwork, we introduce a novel framework for learning world models that accurately\ncapture complex, partially observable, and stochastic dynamics. The proposed\nmethod employs a dual-autoregressive mechanism and self-supervised training to\nachieve reliable long-horizon predictions without relying on domain-specific\ninductive biases, ensuring adaptability across diverse robotic tasks. We\nfurther propose a policy optimization framework that leverages world models for\nefficient training in imagined environments and seamless deployment in\nreal-world systems. Through extensive experiments, our approach consistently\noutperforms state-of-the-art methods, demonstrating superior autoregressive\nprediction accuracy, robustness to noise, and generalization across\nmanipulation and locomotion tasks. Notably, policies trained with our method\nare successfully deployed on ANYmal D hardware in a zero-shot transfer,\nachieving robust performance with minimal sim-to-real performance loss. This\nwork advances model-based reinforcement learning by addressing the challenges\nof long-horizon prediction, error accumulation, and sim-to-real transfer. By\nproviding a scalable and robust framework, the introduced methods pave the way\nfor adaptive and efficient robotic systems in real-world applications."
    },
    {
        "date": "2025-01",
        "title": "Efficient Simulation of Quantum Secure Multiparty Computation",
        "author": "Kartick Sutradhar",
        "link": "http://arxiv.org/abs/2501.10083v1",
        "abstract": "One of the key characteristics of secure quantum communication is quantum\nsecure multiparty computation. In this paper, we propose a quantum secure\nmultiparty summation (QSMS) protocol that can be applied to many complex\nquantum operations. It is based on the $(t, n)$ threshold approach. We combine\nthe classical and quantum phenomena to make this protocol realistic and secure.\nBecause the current protocols employ the $(n, n)$ threshold approach, which\nrequires all honest players to execute the quantum multiparty summation\nprotocol, they have certain security and efficiency problems. However, we\nemploy a $(t, n)$ threshold approach, which requires the quantum summation\nprotocol to be computed only by $t$ honest players. Our suggested protocol is\nmore economical, practical, and secure than alternative protocols."
    },
    {
        "date": "2025-01",
        "title": "Robust Change Captioning in Remote Sensing: SECOND-CC Dataset and MModalCC Framework",
        "author": "Ali Can Karaca, M. Enes Ozelbas, Saadettin Berber, Orkhan Karimli, Turabi Yildirim, and M. Fatih Amasyali",
        "link": "http://arxiv.org/abs/2501.10075v1",
        "abstract": "Remote sensing change captioning (RSICC) aims to describe changes between\nbitemporal images in natural language. Existing methods often fail under\nchallenges like illumination differences, viewpoint changes, blur effects,\nleading to inaccuracies, especially in no-change regions. Moreover, the images\nacquired at different spatial resolutions and have registration errors tend to\naffect the captions. To address these issues, we introduce SECOND-CC, a novel\nRSICC dataset featuring high-resolution RGB image pairs, semantic segmentation\nmaps, and diverse real-world scenarios. SECOND-CC which contains 6,041 pairs of\nbitemporal RS images and 30,205 sentences describing the differences between\nimages. Additionally, we propose MModalCC, a multimodal framework that\nintegrates semantic and visual data using advanced attention mechanisms,\nincluding Cross-Modal Cross Attention (CMCA) and Multimodal Gated Cross\nAttention (MGCA). Detailed ablation studies and attention visualizations\nfurther demonstrate its effectiveness and ability to address RSICC challenges.\nComprehensive experiments show that MModalCC outperforms state-of-the-art RSICC\nmethods, including RSICCformer, Chg2Cap, and PSNet with +4.6% improvement on\nBLEU4 score and +9.6% improvement on CIDEr score. We will make our dataset and\ncodebase publicly available to facilitate future research at\nhttps://github.com/ChangeCapsInRS/SecondCC"
    },
    {
        "date": "2025-01",
        "title": "Spatiotemporal Prediction of Secondary Crashes by Rebalancing Dynamic and Static Data with Generative Adversarial Networks",
        "author": "Junlan Chen, Yiqun Li, Chenyu Ling, Ziyuan Pu, and Xiucheng Guo",
        "link": "http://arxiv.org/abs/2501.10041v1",
        "abstract": "Data imbalance is a common issue in analyzing and predicting sudden traffic\nevents. Secondary crashes constitute only a small proportion of all crashes.\nThese secondary crashes, triggered by primary crashes, significantly exacerbate\ntraffic congestion and increase the severity of incidents. However, the severe\nimbalance of secondary crash data poses significant challenges for prediction\nmodels, affecting their generalization ability and prediction accuracy.\nExisting methods fail to fully address the complexity of traffic crash data,\nparticularly the coexistence of dynamic and static features, and often struggle\nto effectively handle data samples of varying lengths. Furthermore, most\ncurrent studies predict the occurrence probability and spatiotemporal\ndistribution of secondary crashes separately, lacking an integrated solution.\nTo address these challenges, this study proposes a hybrid model named\nVarFusiGAN-Transformer, aimed at improving the fidelity of secondary crash data\ngeneration and jointly predicting the occurrence and spatiotemporal\ndistribution of secondary crashes. The VarFusiGAN-Transformer model employs\nLong Short-Term Memory (LSTM) networks to enhance the generation of\nmultivariate long-time series data, incorporating a static data generator and\nan auxiliary discriminator to model the joint distribution of dynamic and\nstatic features. In addition, the model's prediction module achieves\nsimultaneous prediction of both the occurrence and spatiotemporal distribution\nof secondary crashes. Compared to existing methods, the proposed model\ndemonstrates superior performance in generating high-fidelity data and\nimproving prediction accuracy."
    },
    {
        "date": "2025-01",
        "title": "CaFA: Cost-aware, Feasible Attacks With Database Constraints Against Neural Tabular Classifiers",
        "author": "Matan Ben-Tov, Daniel Deutch, Nave Frost, and Mahmood Sharif",
        "link": "http://arxiv.org/abs/2501.10013v1",
        "abstract": "This work presents CaFA, a system for Cost-aware Feasible Attacks for\nassessing the robustness of neural tabular classifiers against adversarial\nexamples realizable in the problem space, while minimizing adversaries' effort.\nTo this end, CaFA leverages TabPGD$-$an algorithm we set forth to generate\nadversarial perturbations suitable for tabular data$-$ and incorporates\nintegrity constraints automatically mined by state-of-the-art database methods.\nAfter producing adversarial examples in the feature space via TabPGD, CaFA\nprojects them on the mined constraints, leading, in turn, to better attack\nrealizability. We tested CaFA with three datasets and two architectures and\nfound, among others, that the constraints we use are of higher quality\n(measured via soundness and completeness) than ones employed in prior work.\nMoreover, CaFA achieves higher feasible success rates$-$i.e., it generates\nadversarial examples that are often misclassified while satisfying\nconstraints$-$than prior attacks while simultaneously perturbing few features\nwith lower magnitudes, thus saving effort and improving inconspicuousness. We\nopen-source CaFA, hoping it will serve as a generic system enabling\nmachine-learning engineers to assess their models' robustness against\nrealizable attacks, thus advancing deployed models' trustworthiness."
    },
    {
        "date": "2025-01",
        "title": "Advancing Image Security with Quantum Key Distribution and Multi-Layer Chaotic Encryption for Quantum Resilient Transmission",
        "author": "Tasmin Karim, Md. Shazzad Hossain Shaon, Md. Fahim Sultan, and Mst Shapna Akter",
        "link": "http://arxiv.org/abs/2501.09895v2",
        "abstract": "Quantum security improves cryptographic protocols by applying quantum\nmechanics principles, assuring resistance to both quantum and conventional\ncomputer attacks. This work addresses these issues by integrating Quantum Key\nDistribution (QKD) utilizing the E91 method with Multi-Layer Chaotic\nEncryption, which employs a variety of patterns to detect eavesdropping,\nresulting in a highly secure image-transmission architecture. The method\nleverages entropy calculations to determine the unpredictability and integrity\nof encrypted and decrypted pictures, guaranteeing strong security. Extensive\nstatistical scenarios illustrate the framework's effectiveness in image\nencryption while preserving high entropy and sensitivity to the original\nvisuals. The findings indicate significant improvement in encryption and\ndecryption performance, demonstrating the framework's potential as a robust\nresponse to weaknesses introduced by advances in quantum computing. Several\nmetrics, such as Peak Signal-to-Noise Ratio (PSNR), Structural Similarity Index\n(SSIM), Normalized Cross-Correlation (NCC), Bit Error Rate (BER), entropy\nvalues for original, encrypted, and decrypted images, and the correlation\nbetween original and decrypted images, validate the framework's effectiveness.\nThe combination of QKD with Multi-Layer Chaotic Encryption provides a scalable\nand resilient technique to secure image communication. As quantum computing\nadvances, this framework offers a future-proof approach for defining secure\ncommunication protocols in crucial sectors such as medical treatment, forensic\ncomputing, and national security, where information confidentiality is\nvaluable."
    },
    {
        "date": "2025-01",
        "title": "FLORA: Formal Language Model Enables Robust Training-free Zero-shot Object Referring Analysis",
        "author": "Zhe Chen, and Zijing Chen",
        "link": "http://arxiv.org/abs/2501.09887v1",
        "abstract": "Object Referring Analysis (ORA), commonly known as referring expression\ncomprehension, requires the identification and localization of specific objects\nin an image based on natural descriptions. Unlike generic object detection, ORA\nrequires both accurate language understanding and precise visual localization,\nmaking it inherently more complex. Although recent pre-trained large visual\ngrounding detectors have achieved significant progress, they heavily rely on\nextensively labeled data and time-consuming learning. To address these, we\nintroduce a novel, training-free framework for zero-shot ORA, termed FLORA\n(Formal Language for Object Referring and Analysis). FLORA harnesses the\ninherent reasoning capabilities of large language models (LLMs) and integrates\na formal language model - a logical framework that regulates language within\nstructured, rule-based descriptions - to provide effective zero-shot ORA. More\nspecifically, our formal language model (FLM) enables an effective,\nlogic-driven interpretation of object descriptions without necessitating any\ntraining processes. Built upon FLM-regulated LLM outputs, we further devise a\nBayesian inference framework and employ appropriate off-the-shelf interpretive\nmodels to finalize the reasoning, delivering favorable robustness against LLM\nhallucinations and compelling ORA performance in a training-free manner. In\npractice, our FLORA boosts the zero-shot performance of existing pretrained\ngrounding detectors by up to around 45%. Our comprehensive evaluation across\ndifferent challenging datasets also confirms that FLORA consistently surpasses\ncurrent state-of-the-art zero-shot methods in both detection and segmentation\ntasks associated with zero-shot ORA. We believe our probabilistic parsing and\nreasoning of the LLM outputs elevate the reliability and interpretability of\nzero-shot ORA. We shall release codes upon publication."
    },
    {
        "date": "2025-01",
        "title": "Generalized Single-Image-Based Morphing Attack Detection Using Deep Representations from Vision Transformer",
        "author": "Haoyu Zhang, Raghavendra Ramachandra, Kiran Raja, and Christoph Busch",
        "link": "http://arxiv.org/abs/2501.09817v1",
        "abstract": "Face morphing attacks have posed severe threats to Face Recognition Systems\n(FRS), which are operated in border control and passport issuance use cases.\nCorrespondingly, morphing attack detection algorithms (MAD) are needed to\ndefend against such attacks. MAD approaches must be robust enough to handle\nunknown attacks in an open-set scenario where attacks can originate from\nvarious morphing generation algorithms, post-processing and the diversity of\nprinters/scanners. The problem of generalization is further pronounced when the\ndetection has to be made on a single suspected image. In this paper, we propose\na generalized single-image-based MAD (S-MAD) algorithm by learning the encoding\nfrom Vision Transformer (ViT) architecture. Compared to CNN-based\narchitectures, ViT model has the advantage on integrating local and global\ninformation and hence can be suitable to detect the morphing traces widely\ndistributed among the face region. Extensive experiments are carried out on\nface morphing datasets generated using publicly available FRGC face datasets.\nSeveral state-of-the-art (SOTA) MAD algorithms, including representative ones\nthat have been publicly evaluated, have been selected and benchmarked with our\nViT-based approach. Obtained results demonstrate the improved detection\nperformance of the proposed S-MAD method on inter-dataset testing (when\ndifferent data is used for training and testing) and comparable performance on\nintra-dataset testing (when the same data is used for training and testing)\nexperimental protocol."
    },
    {
        "date": "2025-01",
        "title": "Ruling the Unruly: Designing Effective, Low-Noise Network Intrusion Detection Rules for Security Operations Centers",
        "author": "Koen T. W. Teuwen, Tom Mulders, Emmanuele Zambon, and Luca Allodi",
        "link": "http://arxiv.org/abs/2501.09808v1",
        "abstract": "Many Security Operations Centers (SOCs) today still heavily rely on\nsignature-based Network Intrusion Detection Systems (NIDS) such as Suricata.\nThe specificity of intrusion detection rules and the coverage provided by\nrulesets are common concerns within the professional community surrounding\nSOCs, which impact the effectiveness of automated alert post-processing\napproaches. We postulate a better understanding of factors influencing the\nquality of rules can help address current SOC issues. In this paper, we\ncharacterize the rules in use at a collaborating commercial (managed) SOC\nserving customers in sectors including education and IT management. During this\nprocess, we discover six relevant design principles, which we consolidate\nthrough interviews with experienced rule designers at the SOC.We then validate\nour design principles by quantitatively assessing their effect on rule\nspecificity. We find that several of these design considerations significantly\nimpact unnecessary workload caused by rules. For instance, rules that leverage\nproxies for detection, and rules that do not employ alert throttling or do not\ndistinguish (un)successful malicious actions, cause significantly more workload\nfor SOC analysts. Moreover, rules that match a generalized characteristic to\ndetect malicious behavior, which is believed to increase coverage, also\nsignificantly increase workload, suggesting a tradeoff must be struck between\nrule specificity and coverage. We show that these design principles can be\napplied successfully at a SOC to reduce workload whilst maintaining coverage\ndespite the prevalence of violations of the principles."
    },
    {
        "date": "2025-01",
        "title": "W3ID: A Quantum Computing-Secure Digital Identity System Redefining Standards for Web3 and Digital Twins",
        "author": "Joseph Yun, Eli Lifton, Eunseo Lee, Yohan Yun, Abigail Song, Joshua Lee, Cristian Jimenez-Bert, Benedict Song, Yejun Lee, Alex Seo, and Sijung Yun",
        "link": "http://arxiv.org/abs/2501.09802v1",
        "abstract": "The rapid advancements in quantum computing present significant threats to\nexisting encryption standards and internet security. Simultaneously, the advent\nof Web 3.0 marks a transformative era in internet history, emphasizing enhanced\ndata security, decentralization, and user ownership. This white paper\nintroduces the W3ID, an abbreviation of Web3 standard meeting universal digital\nID, which is a Universal Digital Identity (UDI) model designed to meet Web3\nstandards while addressing vulnerabilities posed by quantum computing. W3ID\ninnovatively generates secure Digital Object Identifiers (DOIs) tailored for\nthe decentralized Web 3.0 ecosystem. Additionally, W3ID employs a dual-key\nsystem for secure authentication, enhancing both public and private\nverification mechanisms. To further enhance encryption strength and\nauthentication integrity in the quantum computing era, W3ID incorporates an\nadvanced security mechanism. By requiring quadruple application of SHA-256,\nwith consecutive matches for validation, the system expands the number of\npossibilities to 256^4, which is approximately 4.3 billion times the current\nSHA-256 capacity. This dramatic increase in computational complexity ensures\nthat even advanced quantum computing systems would face significant challenges\nin executing brute-force attacks. W3ID redefines digital identity standards for\nWeb 3.0 and the quantum computing era, setting a new benchmark for security,\nscalability, and decentralization in the global digital twin ecosystem."
    },
    {
        "date": "2025-01",
        "title": "Unified Face Matching and Physical-Digital Spoofing Attack Detection",
        "author": "Arun Kunwar, and Ajita Rattani",
        "link": "http://arxiv.org/abs/2501.09635v1",
        "abstract": "Face recognition technology has dramatically transformed the landscape of\nsecurity, surveillance, and authentication systems, offering a user-friendly\nand non-invasive biometric solution. However, despite its significant\nadvantages, face recognition systems face increasing threats from physical and\ndigital spoofing attacks. Current research typically treats face recognition\nand attack detection as distinct classification challenges. This approach\nnecessitates the implementation of separate models for each task, leading to\nconsiderable computational complexity, particularly on devices with limited\nresources. Such inefficiencies can stifle scalability and hinder performance.\nIn response to these challenges, this paper introduces an innovative unified\nmodel designed for face recognition and detection of physical and digital\nattacks. By leveraging the advanced Swin Transformer backbone and incorporating\nHiLo attention in a convolutional neural network framework, we address unified\nface recognition and spoof attack detection more effectively. Moreover, we\nintroduce augmentation techniques that replicate the traits of physical and\ndigital spoofing cues, significantly enhancing our model robustness. Through\ncomprehensive experimental evaluation across various datasets, we showcase the\neffectiveness of our model in unified face recognition and spoof detection.\nAdditionally, we confirm its resilience against unseen physical and digital\nspoofing attacks, underscoring its potential for real-world applications."
    },
    {
        "date": "2025-01",
        "title": "Weight for Robustness: A Comprehensive Approach towards Optimal Fault-Tolerant Asynchronous ML",
        "author": "Tehila Dahan, and Kfir Y. Levy",
        "link": "http://arxiv.org/abs/2501.09621v1",
        "abstract": "We address the challenges of Byzantine-robust training in asynchronous\ndistributed machine learning systems, aiming to enhance efficiency amid massive\nparallelization and heterogeneous computing resources. Asynchronous systems,\nmarked by independently operating workers and intermittent updates, uniquely\nstruggle with maintaining integrity against Byzantine failures, which encompass\nmalicious or erroneous actions that disrupt learning. The inherent delays in\nsuch settings not only introduce additional bias to the system but also obscure\nthe disruptions caused by Byzantine faults. To tackle these issues, we adapt\nthe Byzantine framework to asynchronous dynamics by introducing a novel\nweighted robust aggregation framework. This allows for the extension of robust\naggregators and a recent meta-aggregator to their weighted versions, mitigating\nthe effects of delayed updates. By further incorporating a recent\nvariance-reduction technique, we achieve an optimal convergence rate for the\nfirst time in an asynchronous Byzantine environment. Our methodology is\nrigorously validated through empirical and theoretical analysis, demonstrating\nits effectiveness in enhancing fault tolerance and optimizing performance in\nasynchronous ML systems."
    },
    {
        "date": "2025-01",
        "title": "Adversarial-Ensemble Kolmogorov Arnold Networks for Enhancing Indoor Wi-Fi Positioning: A Defensive Approach Against Spoofing and Signal Manipulation Attacks",
        "author": "Mitul Goswami, Romit Chatterjee, Somnath Mahato, and Prasant Kumar Pattnaik",
        "link": "http://arxiv.org/abs/2501.09609v1",
        "abstract": "The research presents a study on enhancing the robustness of Wi-Fi-based\nindoor positioning systems against adversarial attacks. The goal is to improve\nthe positioning accuracy and resilience of these systems under two attack\nscenarios: Wi-Fi Spoofing and Signal Strength Manipulation. Three models are\ndeveloped and evaluated: a baseline model (M_Base), an adversarially trained\nrobust model (M_Rob), and an ensemble model (M_Ens). All models utilize a\nKolmogorov-Arnold Network (KAN) architecture. The robust model is trained with\nadversarially perturbed data, while the ensemble model combines predictions\nfrom both the base and robust models. Experimental results show that the robust\nmodel reduces positioning error by approximately 10% compared to the baseline,\nachieving 2.03 meters error under Wi-Fi spoofing and 2.00 meters under signal\nstrength manipulation. The ensemble model further outperforms with errors of\n2.01 meters and 1.975 meters for the respective attack types. This analysis\nhighlights the effectiveness of adversarial training techniques in mitigating\nattack impacts. The findings underscore the importance of considering\nadversarial scenarios in developing indoor positioning systems, as improved\nresilience can significantly enhance the accuracy and reliability of such\nsystems in mission-critical environments."
    },
    {
        "date": "2025-01",
        "title": "Normal-NeRF: Ambiguity-Robust Normal Estimation for Highly Reflective Scenes",
        "author": "Ji Shi, Xianghua Ying, Ruohao Guo, Bowei Xing, and Wenzhen Yue",
        "link": "http://arxiv.org/abs/2501.09460v1",
        "abstract": "Neural Radiance Fields (NeRF) often struggle with reconstructing and\nrendering highly reflective scenes. Recent advancements have developed various\nreflection-aware appearance models to enhance NeRF's capability to render\nspecular reflections. However, the robust reconstruction of highly reflective\nscenes is still hindered by the inherent shape ambiguity on specular surfaces.\nExisting methods typically rely on additional geometry priors to regularize the\nshape prediction, but this can lead to oversmoothed geometry in complex scenes.\nObserving the critical role of surface normals in parameterizing reflections,\nwe introduce a transmittance-gradient-based normal estimation technique that\nremains robust even under ambiguous shape conditions. Furthermore, we propose a\ndual activated densities module that effectively bridges the gap between smooth\nsurface normals and sharp object boundaries. Combined with a reflection-aware\nappearance model, our proposed method achieves robust reconstruction and\nhigh-fidelity rendering of scenes featuring both highly specular reflections\nand intricate geometric structures. Extensive experiments demonstrate that our\nmethod outperforms existing state-of-the-art methods on various datasets."
    },
    {
        "date": "2025-01",
        "title": "Double Visual Defense: Adversarial Pre-training and Instruction Tuning for Improving Vision-Language Model Robustness",
        "author": "Zeyu Wang, Cihang Xie, Brian Bartoldson, and Bhavya Kailkhura",
        "link": "http://arxiv.org/abs/2501.09446v1",
        "abstract": "This paper investigates the robustness of vision-language models against\nadversarial visual perturbations and introduces a novel ``double visual\ndefense\" to enhance this robustness. Unlike previous approaches that resort to\nlightweight adversarial fine-tuning of a pre-trained CLIP model, we perform\nlarge-scale adversarial vision-language pre-training from scratch using\nweb-scale data. We then strengthen the defense by incorporating adversarial\nvisual instruction tuning. The resulting models from each stage, $\\Delta$CLIP\nand $\\Delta^2$LLaVA, show substantially enhanced zero-shot robustness and set a\nnew state-of-the-art in adversarial defense for vision-language models. For\nexample, the adversarial robustness of $\\Delta$CLIP surpasses that of the\nprevious best models on ImageNet-1k by ~20%. %For example, $\\Delta$CLIP\nsurpasses the previous best models on ImageNet-1k by ~20% in terms of\nadversarial robustness. Similarly, compared to prior art, $\\Delta^2$LLaVA\nbrings a ~30% robustness improvement to image captioning task and a ~20%\nrobustness improvement to visual question answering task. Furthermore, our\nmodels exhibit stronger zero-shot recognition capability, fewer hallucinations,\nand superior reasoning performance compared to baselines. Our project page is\nhttps://doublevisualdefense.github.io/."
    },
    {
        "date": "2025-01",
        "title": "Towards Robust and Realistic Human Pose Estimation via WiFi Signals",
        "author": "Yang Chen, Jingcai Guo, Song Guo, Jingren Zhou, and Dacheng Tao",
        "link": "http://arxiv.org/abs/2501.09411v2",
        "abstract": "Robust WiFi-based human pose estimation is a challenging task that bridges\ndiscrete and subtle WiFi signals to human skeletons. This paper revisits this\nproblem and reveals two critical yet overlooked issues: 1) cross-domain gap,\ni.e., due to significant variations between source-target domain pose\ndistributions; and 2) structural fidelity gap, i.e., predicted skeletal poses\nmanifest distorted topology, usually with misplaced joints and disproportionate\nbone lengths. This paper fills these gaps by reformulating the task into a\nnovel two-phase framework dubbed DT-Pose: Domain-consistent representation\nlearning and Topology-constrained Pose decoding. Concretely, we first propose a\ntemporal-consistent contrastive learning strategy with uniformity\nregularization, coupled with self-supervised masking-reconstruction operations,\nto enable robust learning of domain-consistent and motion-discriminative\nWiFi-specific representations. Beyond this, we introduce a simple yet effective\npose decoder with task prompts, which integrates Graph Convolution Network\n(GCN) and Transformer layers to constrain the topology structure of the\ngenerated skeleton by exploring the adjacent-overarching relationships among\nhuman joints. Extensive experiments conducted on various benchmark datasets\nhighlight the superior performance of our method in tackling these fundamental\nchallenges in both 2D/3D human pose estimation tasks."
    },
    {
        "date": "2025-01",
        "title": "Quantum-Enhanced Transformers for Robust Acoustic Scene Classification in IoT Environments",
        "author": "Minh K. Quan, Mayuri Wijayasundara, Sujeeva Setunge, and Pubudu N. Pathirana",
        "link": "http://arxiv.org/abs/2501.09394v1",
        "abstract": "The proliferation of Internet of Things (IoT) devices equipped with acoustic\nsensors necessitates robust acoustic scene classification (ASC) capabilities,\neven in noisy and data-limited environments. Traditional machine learning\nmethods often struggle to generalize effectively under such conditions. To\naddress this, we introduce Q-ASC, a novel Quantum-Inspired Acoustic Scene\nClassifier that leverages the power of quantum-inspired transformers. By\nintegrating quantum concepts like superposition and entanglement, Q-ASC\nachieves superior feature learning and enhanced noise resilience compared to\nclassical models. Furthermore, we introduce a Quantum Variational Autoencoder\n(QVAE) based data augmentation technique to mitigate the challenge of limited\nlabeled data in IoT deployments. Extensive evaluations on the Tampere\nUniversity of Technology (TUT) Acoustic Scenes 2016 benchmark dataset\ndemonstrate that Q-ASC achieves remarkable accuracy between 68.3% and 88.5%\nunder challenging conditions, outperforming state-of-the-art methods by over 5%\nin the best case. This research paves the way for deploying intelligent\nacoustic sensing in IoT networks, with potential applications in smart homes,\nindustrial monitoring, and environmental surveillance, even in adverse acoustic\nenvironments."
    },
    {
        "date": "2025-01",
        "title": "Neural Honeytrace: A Robust Plug-and-Play Watermarking Framework against Model Extraction Attacks",
        "author": "Yixiao Xu, Binxing Fang, Rui Wang, Yinghai Zhou, Shouling Ji, Yuan Liu, Mohan Li, and Zhihong Tian",
        "link": "http://arxiv.org/abs/2501.09328v2",
        "abstract": "Developing high-performance deep learning models is resource-intensive,\nleading model owners to utilize Machine Learning as a Service (MLaaS) platforms\ninstead of publicly releasing their models. However, malicious users may\nexploit query interfaces to execute model extraction attacks, reconstructing\nthe target model's functionality locally. While prior research has investigated\ntriggerable watermarking techniques for asserting ownership, existing methods\nface significant challenges: (1) most approaches require additional training,\nresulting in high overhead and limited flexibility, and (2) they often fail to\naccount for advanced attackers, leaving them vulnerable to adaptive attacks.\n  In this paper, we propose Neural Honeytrace, a robust plug-and-play\nwatermarking framework against model extraction attacks. We first formulate a\nwatermark transmission model from an information-theoretic perspective,\nproviding an interpretable account of the principles and limitations of\nexisting triggerable watermarking. Guided by the model, we further introduce:\n(1) a similarity-based training-free watermarking method for plug-and-play and\nflexible watermarking, and (2) a distribution-based multi-step watermark\ninformation transmission strategy for robust watermarking. Comprehensive\nexperiments on four datasets demonstrate that Neural Honeytrace outperforms\nprevious methods in efficiency and resisting adaptive attacks. Neural\nHoneytrace reduces the average number of samples required for a worst-case\nt-Test-based copyright claim from $12,000$ to $200$ with zero training cost."
    },
    {
        "date": "2025-01",
        "title": "Cooperative Decentralized Backdoor Attacks on Vertical Federated Learning",
        "author": "Seohyun Lee, Wenzhi Fang, Anindya Bijoy Das, Seyyedali Hosseinalipour, David J. Love, and Christopher G. Brinton",
        "link": "http://arxiv.org/abs/2501.09320v1",
        "abstract": "Federated learning (FL) is vulnerable to backdoor attacks, where adversaries\nalter model behavior on target classification labels by embedding triggers into\ndata samples. While these attacks have received considerable attention in\nhorizontal FL, they are less understood for vertical FL (VFL), where devices\nhold different features of the samples, and only the server holds the labels.\nIn this work, we propose a novel backdoor attack on VFL which (i) does not rely\non gradient information from the server and (ii) considers potential collusion\namong multiple adversaries for sample selection and trigger embedding. Our\nlabel inference model augments variational autoencoders with metric learning,\nwhich adversaries can train locally. A consensus process over the adversary\ngraph topology determines which datapoints to poison. We further propose\nmethods for trigger splitting across the adversaries, with an intensity-based\nimplantation scheme skewing the server towards the trigger. Our convergence\nanalysis reveals the impact of backdoor perturbations on VFL indicated by a\nstationarity gap for the trained model, which we verify empirically as well. We\nconduct experiments comparing our attack with recent backdoor VFL approaches,\nfinding that ours obtains significantly higher success rates for the same main\ntask performance despite not using server information. Additionally, our\nresults verify the impact of collusion on attack performance."
    },
    {
        "date": "2025-01",
        "title": "Clone-Robust AI Alignment",
        "author": "Ariel D. Procaccia, Benjamin Schiffer, and Shirley Zhang",
        "link": "http://arxiv.org/abs/2501.09254v1",
        "abstract": "A key challenge in training Large Language Models (LLMs) is properly aligning\nthem with human preferences. Reinforcement Learning with Human Feedback (RLHF)\nuses pairwise comparisons from human annotators to train reward functions and\nhas emerged as a popular alignment method. However, input datasets in RLHF are\nnot necessarily balanced in the types of questions and answers that are\nincluded. Therefore, we want RLHF algorithms to perform well even when the set\nof alternatives is not uniformly distributed. Drawing on insights from social\nchoice theory, we introduce robustness to approximate clones, a desirable\nproperty of RLHF algorithms which requires that adding near-duplicate\nalternatives does not significantly change the learned reward function. We\nfirst demonstrate that the standard RLHF algorithm based on regularized maximum\nlikelihood estimation (MLE) fails to satisfy this property. We then propose the\nweighted MLE, a new RLHF algorithm that modifies the standard regularized MLE\nby weighting alternatives based on their similarity to other alternatives. This\nnew algorithm guarantees robustness to approximate clones while preserving\ndesirable theoretical properties."
    },
    {
        "date": "2025-01",
        "title": "Practical Spoofing Attacks on Galileo Open Service Navigation Message Authentication",
        "author": "Haiyang Wang, Yuanyu Zhang, Xinghui Zhu, Ji He, Shuangtrui Zhao, Yulong Shen, and Xiaohong Jiang",
        "link": "http://arxiv.org/abs/2501.09246v1",
        "abstract": "This paper examines the Galileo Open Service Navigation Message\nAuthentication (OSNMA) and, for the first time, discovers two critical\nvulnerabilities, namely artificially-manipulated time synchronization (ATS) and\ninterruptible message authentication (IMA). ATS allows attackers falsify a\nreceiver's signals and/or local reference time (LRT) while still fulfilling the\ntime synchronization (TS) requirement. IMA allows temporary interruption of the\nnavigation data authentication process due to the reception of a broken message\n(probably caused by spoofing attacks) and restores the authentication later. By\nexploiting the ATS vulnerability, we propose a TS-comply replay (TSR) attack\nwith two variants (real-time and non-real-time), where attackers replay signals\nto a victim receiver while strictly complying with the TS rule. We further\npropose a TS-comply forgery (TSF) attack, where attackers first use a\npreviously-disclosed key to forge a message based on the OSNMA protocol, then\ntamper with the vitcim receiver's LRT correspondingly to comply with the TS\nrule and finally transmit the forged message to the receiver. Finally, we\npropose a concatenating replay (CR) attack based on the IMA vulnerability,\nwhere attackers concatenate replayed signals to the victim receiver's signals\nin a way that still enables correct verification of the navigation data in the\nreplayed signals. To validate the effectiveness of the proposed attacks, we\nconduct real-world experiments with a commercial Galileo receiver manufactured\nby Septentrio, two software-defined radio (SDR) devices, open-source\nGalileo-SDR-SIM and OSNMAlib software. The results showed that all the attacks\ncan successfully pass the OSNMA scheme and the TSF attack can spoof receivers\nto arbitrary locations."
    },
    {
        "date": "2025-01",
        "title": "Benchmarking Robustness of Contrastive Learning Models for Medical Image-Report Retrieval",
        "author": "Demetrio Deanda, Yuktha Priya Masupalli, Jeong Yang, Young Lee, Zechun Cao, and Gongbo Liang",
        "link": "http://arxiv.org/abs/2501.09134v1",
        "abstract": "Medical images and reports offer invaluable insights into patient health. The\nheterogeneity and complexity of these data hinder effective analysis. To bridge\nthis gap, we investigate contrastive learning models for cross-domain\nretrieval, which associates medical images with their corresponding clinical\nreports. This study benchmarks the robustness of four state-of-the-art\ncontrastive learning models: CLIP, CXR-RePaiR, MedCLIP, and CXR-CLIP. We\nintroduce an occlusion retrieval task to evaluate model performance under\nvarying levels of image corruption. Our findings reveal that all evaluated\nmodels are highly sensitive to out-of-distribution data, as evidenced by the\nproportional decrease in performance with increasing occlusion levels. While\nMedCLIP exhibits slightly more robustness, its overall performance remains\nsignificantly behind CXR-CLIP and CXR-RePaiR. CLIP, trained on a\ngeneral-purpose dataset, struggles with medical image-report retrieval,\nhighlighting the importance of domain-specific training data. The evaluation of\nthis work suggests that more effort needs to be spent on improving the\nrobustness of these models. By addressing these limitations, we can develop\nmore reliable cross-domain retrieval models for medical applications."
    },
    {
        "date": "2025-01",
        "title": "Salient Information Preserving Adversarial Training Improves Clean and Robust Accuracy",
        "author": "Timothy Redgrave, and Adam Czajka",
        "link": "http://arxiv.org/abs/2501.09086v1",
        "abstract": "In this work we introduce Salient Information Preserving Adversarial Training\n(SIP-AT), an intuitive method for relieving the robustness-accuracy trade-off\nincurred by traditional adversarial training. SIP-AT uses salient image regions\nto guide the adversarial training process in such a way that fragile features\ndeemed meaningful by an annotator remain unperturbed during training, allowing\nmodels to learn highly predictive non-robust features without sacrificing\noverall robustness. This technique is compatible with both human-based and\nautomatically generated salience estimates, allowing SIP-AT to be used as a\npart of human-driven model development without forcing SIP-AT to be reliant\nupon additional human data. We perform experiments across multiple datasets and\narchitectures and demonstrate that SIP-AT is able to boost the clean accuracy\nof models while maintaining a high degree of robustness against attacks at\nmultiple epsilon levels. We complement our central experiments with an\nobservational study measuring the rate at which human subjects successfully\nidentify perturbed images. This study helps build a more intuitive\nunderstanding of adversarial attack strength and demonstrates the heightened\nimportance of low-epsilon robustness. Our results demonstrate the efficacy of\nSIP-AT and provide valuable insight into the risks posed by adversarial samples\nof various strengths."
    },
    {
        "date": "2025-01",
        "title": "Improving Stability Estimates in Adversarial Explainable AI through Alternate Search Methods",
        "author": "Christopher Burger, and Charles Walter",
        "link": "http://arxiv.org/abs/2501.09006v1",
        "abstract": "Advances in the effectiveness of machine learning models have come at the\ncost of enormous complexity resulting in a poor understanding of how they\nfunction. Local surrogate methods have been used to approximate the workings of\nthese complex models, but recent work has revealed their vulnerability to\nadversarial attacks where the explanation produced is appreciably different\nwhile the meaning and structure of the complex model's output remains similar.\nThis prior work has focused on the existence of these weaknesses but not on\ntheir magnitude. Here we explore using an alternate search method with the goal\nof finding minimum viable perturbations, the fewest perturbations necessary to\nachieve a fixed similarity value between the original and altered text's\nexplanation. Intuitively, a method that requires fewer perturbations to expose\na given level of instability is inferior to one which requires more. This\nnuance allows for superior comparisons of the stability of explainability\nmethods."
    },
    {
        "date": "2025-01",
        "title": "Securing the AI Frontier: Urgent Ethical and Regulatory Imperatives for AI-Driven Cybersecurity",
        "author": "Vikram Kulothungan",
        "link": "http://arxiv.org/abs/2501.10467v1",
        "abstract": "This paper critically examines the evolving ethical and regulatory challenges\nposed by the integration of artificial intelligence (AI) in cybersecurity. We\ntrace the historical development of AI regulation, highlighting major\nmilestones from theoretical discussions in the 1940s to the implementation of\nrecent global frameworks such as the European Union AI Act. The current\nregulatory landscape is analyzed, emphasizing risk-based approaches,\nsector-specific regulations, and the tension between fostering innovation and\nmitigating risks. Ethical concerns such as bias, transparency, accountability,\nprivacy, and human oversight are explored in depth, along with their\nimplications for AI-driven cybersecurity systems. Furthermore, we propose\nstrategies for promoting AI literacy and public engagement, essential for\nshaping a future regulatory framework. Our findings underscore the need for a\nunified, globally harmonized regulatory approach that addresses the unique\nrisks of AI in cybersecurity. We conclude by identifying future research\nopportunities and recommending pathways for collaboration between policymakers,\nindustry leaders, and researchers to ensure the responsible deployment of AI\ntechnologies in cybersecurity."
    },
    {
        "date": "2025-01",
        "title": "Improving the Efficiency of Self-Supervised Adversarial Training through Latent Clustering-Based Selection",
        "author": "Somrita Ghosh, Yuelin Xu, and Xiao Zhang",
        "link": "http://arxiv.org/abs/2501.10466v1",
        "abstract": "Compared with standard learning, adversarially robust learning is widely\nrecognized to demand significantly more training examples. Recent works propose\nthe use of self-supervised adversarial training (SSAT) with external or\nsynthetically generated unlabeled data to enhance model robustness. However,\nSSAT requires a substantial amount of extra unlabeled data, significantly\nincreasing memory usage and model training times. To address these challenges,\nwe propose novel methods to strategically select a small subset of unlabeled\ndata essential for SSAT and robustness improvement. Our selection prioritizes\ndata points near the model's decision boundary based on latent clustering-based\ntechniques, efficiently identifying a critical subset of unlabeled data with a\nhigher concentration of boundary-adjacent points. While focusing on\nnear-boundary data, our methods are designed to maintain a balanced ratio\nbetween boundary and non-boundary data points to avoid overfitting. Our\nexperiments on image benchmarks show that integrating our selection strategies\ninto self-supervised adversarial training can largely reduce memory and\ncomputational requirements while achieving high model robustness. In\nparticular, our latent clustering-based selection method with k-means is the\nmost effective, achieving nearly identical test-time robust accuracies with 5\nto 10 times less external or generated unlabeled data when applied to image\nbenchmarks. Additionally, we validate the generalizability of our approach\nacross various application scenarios, including a real-world medical dataset\nfor COVID-19 chest X-ray classification."
    },
    {
        "date": "2025-01",
        "title": "Exploring ChatGPT for Face Presentation Attack Detection in Zero and Few-Shot in-Context Learning",
        "author": "Alain Komaty, Hatef Otroshi Shahreza, Anjith George, and Sebastien Marcel",
        "link": "http://arxiv.org/abs/2501.08799v1",
        "abstract": "This study highlights the potential of ChatGPT (specifically GPT-4o) as a\ncompetitive alternative for Face Presentation Attack Detection (PAD),\noutperforming several PAD models, including commercial solutions, in specific\nscenarios. Our results show that GPT-4o demonstrates high consistency,\nparticularly in few-shot in-context learning, where its performance improves as\nmore examples are provided (reference data). We also observe that detailed\nprompts enable the model to provide scores reliably, a behavior not observed\nwith concise prompts. Additionally, explanation-seeking prompts slightly\nenhance the model's performance by improving its interpretability. Remarkably,\nthe model exhibits emergent reasoning capabilities, correctly predicting the\nattack type (print or replay) with high accuracy in few-shot scenarios, despite\nnot being explicitly instructed to classify attack types. Despite these\nstrengths, GPT-4o faces challenges in zero-shot tasks, where its performance is\nlimited compared to specialized PAD systems. Experiments were conducted on a\nsubset of the SOTERIA dataset, ensuring compliance with data privacy\nregulations by using only data from consenting individuals. These findings\nunderscore GPT-4o's promise in PAD applications, laying the groundwork for\nfuture research to address broader data privacy concerns and improve\ncross-dataset generalization. Code available here:\nhttps://gitlab.idiap.ch/bob/bob.paper.wacv2025_chatgpt_face_pad"
    },
    {
        "date": "2025-01",
        "title": "Multilingual Email Phishing Attacks Detection using OSINT and Machine Learning",
        "author": "Panharith An, Rana Shafi, Tionge Mughogho, and Onyango Allan Onyango",
        "link": "http://arxiv.org/abs/2501.08723v1",
        "abstract": "Email phishing remains a prevalent cyber threat, targeting victims to extract\nsensitive information or deploy malicious software. This paper explores the\nintegration of open-source intelligence (OSINT) tools and machine learning (ML)\nmodels to enhance phishing detection across multilingual datasets. Using Nmap\nand theHarvester, this study extracted 17 features, including domain names, IP\naddresses, and open ports, to improve detection accuracy. Multilingual email\ndatasets, including English and Arabic, were analyzed to address the\nlimitations of ML models trained predominantly on English data. Experiments\nwith five classification algorithms: Decision Tree, Random Forest, Support\nVector Machine, XGBoost, and Multinomial Na\\\"ive Bayes. It revealed that Random\nForest achieved the highest performance, with an accuracy of 97.37% for both\nEnglish and Arabic datasets. For OSINT-enhanced datasets, the model\ndemonstrated an improvement in accuracy compared to baseline models without\nOSINT features. These findings highlight the potential of combining OSINT tools\nwith advanced ML models to detect phishing emails more effectively across\ndiverse languages and contexts. This study contributes an approach to phishing\ndetection by incorporating OSINT features and evaluating their impact on\nmultilingual datasets, addressing a critical gap in cybersecurity research."
    },
    {
        "date": "2025-01",
        "title": "Tag&Tab: Pretraining Data Detection in Large Language Models Using Keyword-Based Membership Inference Attack",
        "author": "Sagiv Antebi, Edan Habler, Asaf Shabtai, and Yuval Elovici",
        "link": "http://arxiv.org/abs/2501.08454v1",
        "abstract": "Large language models (LLMs) have become essential digital task assistance\ntools. Their training relies heavily on the collection of vast amounts of data,\nwhich may include copyright-protected or sensitive information. Recent studies\non the detection of pretraining data in LLMs have primarily focused on\nsentence-level or paragraph-level membership inference attacks (MIAs), usually\ninvolving probability analysis of the target model prediction tokens. However,\nthe proposed methods often demonstrate poor performance, specifically in terms\nof accuracy, failing to account for the semantic importance of textual content\nand word significance. To address these shortcomings, we propose Tag&Tab, a\nnovel approach for detecting data that has been used as part of the LLM\npretraining. Our method leverages advanced natural language processing (NLP)\ntechniques to tag keywords in the input text - a process we term Tagging. Then,\nthe LLM is used to obtain the probabilities of these keywords and calculate\ntheir average log-likelihood to determine input text membership, a process we\nrefer to as Tabbing. Our experiments on three benchmark datasets (BookMIA,\nMIMIR, and the Pile) and several open-source LLMs of varying sizes demonstrate\nan average increase in the AUC scores ranging from 4.1% to 12.1% over\nstate-of-the-art methods. Tag&Tab not only sets a new standard for data leakage\ndetection in LLMs, but its outstanding performance is a testament to the\nimportance of words in MIAs on LLMs."
    },
    {
        "date": "2025-01",
        "title": "Secure Composition of Quantum Key Distribution and Symmetric Key Encryption",
        "author": "Kunal Dey, and Reihaneh Safavi-Naini",
        "link": "http://arxiv.org/abs/2501.08435v1",
        "abstract": "Quantum key distribution (QKD) allows Alice and Bob to share a secret key\nover an insecure channel with proven information-theoretic security against an\nadversary whose strategy is bounded only by the laws of physics.\nComposability-based security proofs of QKD ensure that using the established\nkey with a one-time-pad encryption scheme provides information theoretic\nsecrecy for the message. In this paper, we consider the problem of using the\nQKD established key with a secure symmetric key-based encryption algorithm and\nuse an approach based on hybrid encryption to provide a proof of security for\nthe composition.\n  Hybrid encryption was first proposed as a public key cryptographic algorithm\nwith proven security for messages of unrestricted length. We use an extension\nof this framework to correlated randomness setting (Sharifian et al. in ISIT\n2021) to propose a quantum-enabled Key Encapsulation Mechanism (qKEM) and\nquantum-enabled hybrid encryption (qHE), and prove a composition theorem for\nthe security of the qHE. We construct a qKEM with proven security using an\nexisting QKD (Portmann et al. in Rev. of Mod. Physics 2022). Using this qKEM\nwith a secure Data Encapsulation Mechanism (DEM), that can be constructed using\na one-time symmetric key encryption scheme, results in an efficient encryption\nsystem for unrestricted length messages with proved security against an\nadversary with access to efficient computations on a quantum computer (i.e.\npost-quantum secure encryption without using any computational assumptions.)"
    },
    {
        "date": "2025-01",
        "title": "Cross-Modal Transferable Image-to-Video Attack on Video Quality Metrics",
        "author": "Georgii Gotin, Ekaterina Shumitskaya, Anastasia Antsiferova, and Dmitriy Vatolin",
        "link": "http://arxiv.org/abs/2501.08415v1",
        "abstract": "Recent studies have revealed that modern image and video quality assessment\n(IQA/VQA) metrics are vulnerable to adversarial attacks. An attacker can\nmanipulate a video through preprocessing to artificially increase its quality\nscore according to a certain metric, despite no actual improvement in visual\nquality. Most of the attacks studied in the literature are white-box attacks,\nwhile black-box attacks in the context of VQA have received less attention.\nMoreover, some research indicates a lack of transferability of adversarial\nexamples generated for one model to another when applied to VQA. In this paper,\nwe propose a cross-modal attack method, IC2VQA, aimed at exploring the\nvulnerabilities of modern VQA models. This approach is motivated by the\nobservation that the low-level feature spaces of images and videos are similar.\nWe investigate the transferability of adversarial perturbations across\ndifferent modalities; specifically, we analyze how adversarial perturbations\ngenerated on a white-box IQA model with an additional CLIP module can\neffectively target a VQA model. The addition of the CLIP module serves as a\nvaluable aid in increasing transferability, as the CLIP model is known for its\neffective capture of low-level semantics. Extensive experiments demonstrate\nthat IC2VQA achieves a high success rate in attacking three black-box VQA\nmodels. We compare our method with existing black-box attack strategies,\nhighlighting its superiority in terms of attack success within the same number\nof iterations and levels of attack strength. We believe that the proposed\nmethod will contribute to the deeper analysis of robust VQA metrics."
    },
    {
        "date": "2025-01",
        "title": "Diffusion Adversarial Post-Training for One-Step Video Generation",
        "author": "Shanchuan Lin, Xin Xia, Yuxi Ren, Ceyuan Yang, Xuefeng Xiao, and Lu Jiang",
        "link": "http://arxiv.org/abs/2501.08316v1",
        "abstract": "The diffusion models are widely used for image and video generation, but\ntheir iterative generation process is slow and expansive. While existing\ndistillation approaches have demonstrated the potential for one-step generation\nin the image domain, they still suffer from significant quality degradation. In\nthis work, we propose Adversarial Post-Training (APT) against real data\nfollowing diffusion pre-training for one-step video generation. To improve the\ntraining stability and quality, we introduce several improvements to the model\narchitecture and training procedures, along with an approximated R1\nregularization objective. Empirically, our experiments show that our\nadversarial post-trained model, Seaweed-APT, can generate 2-second, 1280x720,\n24fps videos in real time using a single forward evaluation step. Additionally,\nour model is capable of generating 1024px images in a single step, achieving\nquality comparable to state-of-the-art methods."
    },
    {
        "date": "2025-01",
        "title": "Towards an End-to-End (E2E) Adversarial Learning and Application in the Physical World",
        "author": "Dudi Biton, Jacob Shams, Satoru Koda, Asaf Shabtai, Yuval Elovici, and Ben Nassi",
        "link": "http://arxiv.org/abs/2501.08258v2",
        "abstract": "The traditional learning process of patch-based adversarial attacks,\nconducted in the digital domain and then applied in the physical domain (e.g.,\nvia printed stickers), may suffer from reduced performance due to adversarial\npatches' limited transferability from the digital domain to the physical\ndomain. Given that previous studies have considered using projectors to apply\nadversarial attacks, we raise the following question: can adversarial learning\n(i.e., patch generation) be performed entirely in the physical domain with a\nprojector? In this work, we propose the Physical-domain Adversarial Patch\nLearning Augmentation (PAPLA) framework, a novel end-to-end (E2E) framework\nthat converts adversarial learning from the digital domain to the physical\ndomain using a projector. We evaluate PAPLA across multiple scenarios,\nincluding controlled laboratory settings and realistic outdoor environments,\ndemonstrating its ability to ensure attack success compared to conventional\ndigital learning-physical application (DL-PA) methods. We also analyze the\nimpact of environmental factors, such as projection surface color, projector\nstrength, ambient light, distance, and angle of the target object relative to\nthe camera, on the effectiveness of projected patches. Finally, we demonstrate\nthe feasibility of the attack against a parked car and a stop sign in a\nreal-world outdoor environment. Our results show that under specific\nconditions, E2E adversarial learning in the physical domain eliminates the\ntransferability issue and ensures evasion by object detectors. Finally, we\nprovide insights into the challenges and opportunities of applying adversarial\nlearning in the physical domain and explain where such an approach is more\neffective than using a sticker."
    },
    {
        "date": "2025-01",
        "title": "CWEval: Outcome-driven Evaluation on Functionality and Security of LLM Code Generation",
        "author": "Jinjun Peng, Leyi Cui, Kele Huang, Junfeng Yang, and Baishakhi Ray",
        "link": "http://arxiv.org/abs/2501.08200v1",
        "abstract": "Large Language Models (LLMs) have significantly aided developers by\ngenerating or assisting in code writing, enhancing productivity across various\ntasks. While identifying incorrect code is often straightforward, detecting\nvulnerabilities in functionally correct code is more challenging, especially\nfor developers with limited security knowledge, which poses considerable\nsecurity risks of using LLM-generated code and underscores the need for robust\nevaluation benchmarks that assess both functional correctness and security.\nCurrent benchmarks like CyberSecEval and SecurityEval attempt to solve it but\nare hindered by unclear and impractical specifications, failing to assess both\nfunctionality and security accurately. To tackle these deficiencies, we\nintroduce CWEval, a novel outcome-driven evaluation framework designed to\nenhance the evaluation of secure code generation by LLMs. This framework not\nonly assesses code functionality but also its security simultaneously with\nhigh-quality task specifications and outcome-driven test oracles which provides\nhigh accuracy. Coupled with CWEval-bench, a multilingual, security-critical\ncoding benchmark, CWEval provides a rigorous empirical security evaluation on\nLLM-generated code, overcoming previous benchmarks' shortcomings. Through our\nevaluations, CWEval reveals a notable portion of functional but insecure code\nproduced by LLMs, and shows a serious inaccuracy of previous evaluations,\nultimately contributing significantly to the field of secure code generation.\nWe open-source our artifact at: https://github.com/Co1lin/CWEval ."
    },
    {
        "date": "2025-01",
        "title": "Energy Backdoor Attack to Deep Neural Networks",
        "author": "Hanene F. Z. Brachemi Meftah, Wassim Hamidouche, Sid Ahmed Fezza, Olivier D\u00e9forges, and Kassem Kallas",
        "link": "http://arxiv.org/abs/2501.08152v1",
        "abstract": "The rise of deep learning (DL) has increased computing complexity and energy\nuse, prompting the adoption of application specific integrated circuits (ASICs)\nfor energy-efficient edge and mobile deployment. However, recent studies have\ndemonstrated the vulnerability of these accelerators to energy attacks. Despite\nthe development of various inference time energy attacks in prior research,\nbackdoor energy attacks remain unexplored. In this paper, we design an\ninnovative energy backdoor attack against deep neural networks (DNNs) operating\non sparsity-based accelerators. Our attack is carried out in two distinct\nphases: backdoor injection and backdoor stealthiness. Experimental results\nusing ResNet-18 and MobileNet-V2 models trained on CIFAR-10 and Tiny ImageNet\ndatasets show the effectiveness of our proposed attack in increasing energy\nconsumption on trigger samples while preserving the model's performance for\nclean/regular inputs. This demonstrates the vulnerability of DNNs to energy\nbackdoor attacks. The source code of our attack is available at:\nhttps://github.com/hbrachemi/energy_backdoor."
    },
    {
        "date": "2025-01",
        "title": "RoHan: Robust Hand Detection in Operation Room",
        "author": "Roi Papo, Sapir Gershov, Tom Friedman, Itay Or, Gil Bolotin, and Shlomi Laufer",
        "link": "http://arxiv.org/abs/2501.08115v2",
        "abstract": "Hand-specific localization has garnered significant interest within the\ncomputer vision community. Although there are numerous datasets with hand\nannotations from various angles and settings, domain transfer techniques\nfrequently struggle in surgical environments. This is mainly due to the limited\navailability of gloved hand instances and the unique challenges of operating\nrooms (ORs). Thus, hand-detection models tailored to OR settings require\nextensive training and expensive annotation processes. To overcome these\nchallenges, we present \"RoHan\" - a novel approach for robust hand detection in\nthe OR, leveraging advanced semi-supervised domain adaptation techniques to\ntackle the challenges of varying recording conditions, diverse glove colors,\nand occlusions common in surgical settings. Our methodology encompasses two\nmain stages: (1) data augmentation strategy that utilizes \"Artificial Gloves,\"\na method for augmenting publicly available hand datasets with synthetic images\nof hands-wearing gloves; (2) semi-supervised domain adaptation pipeline that\nimproves detection performance in real-world OR settings through iterative\nprediction refinement and efficient frame filtering. We evaluate our method\nusing two datasets: simulated enterotomy repair and saphenous vein graft\nharvesting. \"RoHan\" substantially reduces the need for extensive labeling and\nmodel training, paving the way for the practical implementation of hand\ndetection technologies in medical settings."
    },
    {
        "date": "2025-01",
        "title": "CellOMaps: A Compact Representation for Robust Classification of Lung Adenocarcinoma Growth Patterns",
        "author": "Arwa Al-Rubaian, Gozde N. Gunesli, Wajd A. Althakfi, Ayesha Azam, David Snead, Nasir M. Rajpoot, and Shan E Ahmed Raza",
        "link": "http://arxiv.org/abs/2501.08094v1",
        "abstract": "Lung adenocarcinoma (LUAD) is a morphologically heterogeneous disease,\ncharacterized by five primary histological growth patterns. The classification\nof such patterns is crucial due to their direct relation to prognosis but the\nhigh subjectivity and observer variability pose a major challenge. Although\nseveral studies have developed machine learning methods for growth pattern\nclassification, they either only report the predominant pattern per slide or\nlack proper evaluation. We propose a generalizable machine learning pipeline\ncapable of classifying lung tissue into one of the five patterns or as\nnon-tumor. The proposed pipeline's strength lies in a novel compact Cell\nOrganization Maps (cellOMaps) representation that captures the cellular spatial\npatterns from Hematoxylin and Eosin whole slide images (WSIs). The proposed\npipeline provides state-of-the-art performance on LUAD growth pattern\nclassification when evaluated on both internal unseen slides and external\ndatasets, significantly outperforming the current approaches. In addition, our\npreliminary results show that the model's outputs can be used to predict\npatients Tumor Mutational Burden (TMB) levels."
    },
    {
        "date": "2025-01",
        "title": "Robust Low-Light Human Pose Estimation through Illumination-Texture Modulation",
        "author": "Feng Zhang, Ze Li, Xiatian Zhu, and Lei Chen",
        "link": "http://arxiv.org/abs/2501.08038v1",
        "abstract": "As critical visual details become obscured, the low visibility and high ISO\nnoise in extremely low-light images pose a significant challenge to human pose\nestimation. Current methods fail to provide high-quality representations due to\nreliance on pixel-level enhancements that compromise semantics and the\ninability to effectively handle extreme low-light conditions for robust feature\nlearning. In this work, we propose a frequency-based framework for low-light\nhuman pose estimation, rooted in the \"divide-and-conquer\" principle. Instead of\nuniformly enhancing the entire image, our method focuses on task-relevant\ninformation. By applying dynamic illumination correction to the low-frequency\ncomponents and low-rank denoising to the high-frequency components, we\neffectively enhance both the semantic and texture information essential for\naccurate pose estimation. As a result, this targeted enhancement method results\nin robust, high-quality representations, significantly improving pose\nestimation performance. Extensive experiments demonstrating its superiority\nover state-of-the-art methods in various challenging low-light scenarios."
    },
    {
        "date": "2025-01",
        "title": "READ: Reinforcement-based Adversarial Learning for Text Classification with Limited Labeled Data",
        "author": "Rohit Sharma, Shanu Kumar, and Avinash Kumar",
        "link": "http://arxiv.org/abs/2501.08035v1",
        "abstract": "Pre-trained transformer models such as BERT have shown massive gains across\nmany text classification tasks. However, these models usually need enormous\nlabeled data to achieve impressive performances. Obtaining labeled data is\noften expensive and time-consuming, whereas collecting unlabeled data using\nsome heuristics is relatively much cheaper for any task. Therefore, this paper\nproposes a method that encapsulates reinforcement learning-based text\ngeneration and semi-supervised adversarial learning approaches in a novel way\nto improve the model's performance. Our method READ, Reinforcement-based\nAdversarial learning, utilizes an unlabeled dataset to generate diverse\nsynthetic text through reinforcement learning, improving the model's\ngeneralization capability using adversarial learning. Our experimental results\nshow that READ outperforms the existing state-of-art methods on multiple\ndatasets."
    },
    {
        "date": "2025-01",
        "title": "Self-Instruct Few-Shot Jailbreaking: Decompose the Attack into Pattern and Behavior Learning",
        "author": "Jiaqi Hua, and Wanxu Wei",
        "link": "http://arxiv.org/abs/2501.07959v1",
        "abstract": "Recently, several works have been conducted on jailbreaking Large Language\nModels (LLMs) with few-shot malicious demos. In particular, Zheng et al. (2024)\nfocuses on improving the efficiency of Few-Shot Jailbreaking (FSJ) by injecting\nspecial tokens into the demos and employing demo-level random search.\nNevertheless, this method lacks generality since it specifies the\ninstruction-response structure. Moreover, the reason why inserting special\ntokens takes effect in inducing harmful behaviors is only empirically\ndiscussed. In this paper, we take a deeper insight into the mechanism of\nspecial token injection and propose Self-Instruct Few-Shot Jailbreaking\n(Self-Instruct-FSJ) facilitated with the demo-level greedy search. This\nframework decomposes the FSJ attack into pattern and behavior learning to\nexploit the model's vulnerabilities in a more generalized and efficient way. We\nconduct elaborate experiments to evaluate our method on common open-source\nmodels and compare it with baseline algorithms. Our code is available at\nhttps://github.com/iphosi/Self-Instruct-FSJ."
    },
    {
        "date": "2025-01",
        "title": "Robust Hyperspectral Image Panshapring via Sparse Spatial-Spectral Representation",
        "author": "Chia-Ming Lee, Yu-Fan Lin, Li-Wei Kang, and Chih-Chung Hsu",
        "link": "http://arxiv.org/abs/2501.07953v1",
        "abstract": "High-resolution hyperspectral imaging plays a crucial role in various remote\nsensing applications, yet its acquisition often faces fundamental limitations\ndue to hardware constraints. This paper introduces S$^{3}$RNet, a novel\nframework for hyperspectral image pansharpening that effectively combines\nlow-resolution hyperspectral images (LRHSI) with high-resolution multispectral\nimages (HRMSI) through sparse spatial-spectral representation. The core of\nS$^{3}$RNet is the Multi-Branch Fusion Network (MBFN), which employs parallel\nbranches to capture complementary features at different spatial and spectral\nscales. Unlike traditional approaches that treat all features equally, our\nSpatial-Spectral Attention Weight Block (SSAWB) dynamically adjusts feature\nweights to maintain sparse representation while suppressing noise and\nredundancy. To enhance feature propagation, we incorporate the Dense Feature\nAggregation Block (DFAB), which efficiently aggregates inputted features\nthrough dense connectivity patterns. This integrated design enables S$^{3}$RNet\nto selectively emphasize the most informative features from differnt scale\nwhile maintaining computational efficiency. Comprehensive experiments\ndemonstrate that S$^{3}$RNet achieves state-of-the-art performance across\nmultiple evaluation metrics, showing particular strength in maintaining high\nreconstruction quality even under challenging noise conditions. The code will\nbe made publicly available."
    },
    {
        "date": "2025-01",
        "title": "Gandalf the Red: Adaptive Security for LLMs",
        "author": "Niklas Pfister, V\u00e1clav Volhejn, Manuel Knott, Santiago Arias, Julia Bazi\u0144ska, Mykhailo Bichurin, Alan Commike, Janet Darling, Peter Dienes, Matthew Fiedler, David Haber, Matthias Kraft, Marco Lancini, Max Mathys, Dami\u00e1n Pascual-Ortiz, Jakub Podolak, Adri\u00e0 Romero-L\u00f3pez, Kyriacos Shiarlis, Andreas Signer, Zsolt Terek, Athanasios Theocharis, Daniel Timbrell, Samuel Trautwein, Samuel Watts, Natalie Wu, and Mateo Rojas-Carulla",
        "link": "http://arxiv.org/abs/2501.07927v1",
        "abstract": "Current evaluations of defenses against prompt attacks in large language\nmodel (LLM) applications often overlook two critical factors: the dynamic\nnature of adversarial behavior and the usability penalties imposed on\nlegitimate users by restrictive defenses. We propose D-SEC (Dynamic Security\nUtility Threat Model), which explicitly separates attackers from legitimate\nusers, models multi-step interactions, and rigorously expresses the\nsecurity-utility in an optimizable form. We further address the shortcomings in\nexisting evaluations by introducing Gandalf, a crowd-sourced, gamified\nred-teaming platform designed to generate realistic, adaptive attack datasets.\nUsing Gandalf, we collect and release a dataset of 279k prompt attacks.\nComplemented by benign user data, our analysis reveals the interplay between\nsecurity and utility, showing that defenses integrated in the LLM (e.g., system\nprompts) can degrade usability even without blocking requests. We demonstrate\nthat restricted application domains, defense-in-depth, and adaptive defenses\nare effective strategies for building secure and useful LLM applications. Code\nis available at\n\\href{https://github.com/lakeraai/dsec-gandalf}{\\texttt{https://github.com/lakeraai/dsec-gandalf}}."
    },
    {
        "date": "2025-01",
        "title": "VENOM: Text-driven Unrestricted Adversarial Example Generation with Diffusion Models",
        "author": "Hui Kuurila-Zhang, Haoyu Chen, and Guoying Zhao",
        "link": "http://arxiv.org/abs/2501.07922v1",
        "abstract": "Adversarial attacks have proven effective in deceiving machine learning\nmodels by subtly altering input images, motivating extensive research in recent\nyears. Traditional methods constrain perturbations within $l_p$-norm bounds,\nbut advancements in Unrestricted Adversarial Examples (UAEs) allow for more\ncomplex, generative-model-based manipulations. Diffusion models now lead UAE\ngeneration due to superior stability and image quality over GANs. However,\nexisting diffusion-based UAE methods are limited to using reference images and\nface challenges in generating Natural Adversarial Examples (NAEs) directly from\nrandom noise, often producing uncontrolled or distorted outputs. In this work,\nwe introduce VENOM, the first text-driven framework for high-quality\nunrestricted adversarial examples generation through diffusion models. VENOM\nunifies image content generation and adversarial synthesis into a single\nreverse diffusion process, enabling high-fidelity adversarial examples without\nsacrificing attack success rate (ASR). To stabilize this process, we\nincorporate an adaptive adversarial guidance strategy with momentum, ensuring\nthat the generated adversarial examples $x^*$ align with the distribution\n$p(x)$ of natural images. Extensive experiments demonstrate that VENOM achieves\nsuperior ASR and image quality compared to prior methods, marking a significant\nadvancement in adversarial example generation and providing insights into model\nvulnerabilities for improved defense development."
    },
    {
        "date": "2025-01",
        "title": "A Comparative Analysis of DNN-based White-Box Explainable AI Methods in Network Security",
        "author": "Osvaldo Arreche, and Mustafa Abdallah",
        "link": "http://arxiv.org/abs/2501.07801v1",
        "abstract": "New research focuses on creating artificial intelligence (AI) solutions for\nnetwork intrusion detection systems (NIDS), drawing its inspiration from the\never-growing number of intrusions on networked systems, increasing its\ncomplexity and intelligibility. Hence, the use of explainable AI (XAI)\ntechniques in real-world intrusion detection systems comes from the requirement\nto comprehend and elucidate black-box AI models to security analysts. In an\neffort to meet such requirements, this paper focuses on applying and evaluating\nWhite-Box XAI techniques (particularly LRP, IG, and DeepLift) for NIDS via an\nend-to-end framework for neural network models, using three widely used network\nintrusion datasets (NSL-KDD, CICIDS-2017, and RoEduNet-SIMARGL2021), assessing\nits global and local scopes, and examining six distinct assessment measures\n(descriptive accuracy, sparsity, stability, robustness, efficiency, and\ncompleteness). We also compare the performance of white-box XAI methods with\nblack-box XAI methods. The results show that using White-box XAI techniques\nscores high in robustness and completeness, which are crucial metrics for IDS.\nMoreover, the source codes for the programs developed for our XAI evaluation\nframework are available to be improved and used by the research community."
    },
    {
        "date": "2025-01",
        "title": "Deep Learning for Disease Outbreak Prediction: A Robust Early Warning Signal for Transcritical Bifurcations",
        "author": "Reza Miry, Amit K. Chakraborty, Russell Greiner, Mark A. Lewis, Hao Wang, Tianyu Guan, and Pouria Ramazi",
        "link": "http://arxiv.org/abs/2501.07764v1",
        "abstract": "Early Warning Signals (EWSs) are vital for implementing preventive measures\nbefore a disease turns into a pandemic. While new diseases exhibit unique\nbehaviors, they often share fundamental characteristics from a dynamical\nsystems perspective. Moreover, measurements during disease outbreaks are often\ncorrupted by different noise sources, posing challenges for Time Series\nClassification (TSC) tasks. In this study, we address the problem of having a\nrobust EWS for disease outbreak prediction using a best-performing deep\nlearning model in the domain of TSC. We employed two simulated datasets to\ntrain the model: one representing generated dynamical systems with randomly\nselected polynomial terms to model new disease behaviors, and another\nsimulating noise-induced disease dynamics to account for noisy measurements.\nThe model's performance was analyzed using both simulated data from different\ndisease models and real-world data, including influenza and COVID-19. Results\ndemonstrate that the proposed model outperforms previous models, effectively\nproviding EWSs of impending outbreaks across various scenarios. This study\nbridges advancements in deep learning with the ability to provide robust early\nwarning signals in noisy environments, making it highly applicable to\nreal-world crises involving emerging disease outbreaks."
    },
    {
        "date": "2025-01",
        "title": "Distributed Identity for Zero Trust and Segmented Access Control: A Novel Approach to Securing Network Infrastructure",
        "author": "Sina Ahmadi",
        "link": "http://arxiv.org/abs/2501.09032v1",
        "abstract": "\"Distributed Identity\" refers to the transition from centralized identity\nsystems using Decentralized Identifiers (DID) and Verifiable Credentials (VC)\nfor secure and privacy-preserving authentications. With distributed identity,\ncontrol of identity data is returned to the user, making credential-based\nattacks impossible due to the lack of a single point of failure. This study\nassesses the security improvements achieved when distributed identity is\nemployed with the ZTA principle, particularly concerning lateral movements\nwithin segmented networks. It also considers areas such as the implementation\nspecifications of the framework, the advantages and disadvantages of the method\nto organizations, and the issues of compatibility and generalizability.\nFurthermore, the study highlights privacy and regulatory compliance, including\nthe General Data Protection Regulation (GDPR) and California Consumer Data\nPrivacy Act (CCPA), analyzing potential solutions to these problems. The study\nimplies that adopting distributed identities can enhance overall security\npostures by an order of magnitude, providing contextual and least-privilege\nauthorization and user privacy. The research recommends refining technical\nstandards, expanding the use of distributed identity in practice, and\ndiscussing its applications for the contemporary digital security landscape."
    },
    {
        "date": "2025-01",
        "title": "A Review of Detection, Evolution, and Data Reconstruction Strategies for False Data Injection Attacks in Power Cyber-Physical Systems",
        "author": "Xiaoyong Bo",
        "link": "http://arxiv.org/abs/2501.10441v1",
        "abstract": "The integration of information and physical systems in modern power grids has\nheightened vulnerabilities to False Data Injection Attacks (FDIAs), threatening\nthe secure operation of power cyber-physical systems (CPS). This paper reviews\nFDIA detection, evolution, and data reconstruction strategies, highlighting\ncross-domain coordination, multi-temporal evolution, and stealth\ncharacteristics. Challenges in existing detection methods, including poor\ninterpretability and data imbalance, are discussed, alongside advanced\nstate-aware and action-control data reconstruction techniques. Key issues, such\nas modeling FDIA evolution and distinguishing malicious data from regular\nfaults, are identified. Future directions to enhance system resilience and\ndetection accuracy are proposed, contributing to the secure operation of power\nCPS."
    },
    {
        "date": "2025-01",
        "title": "A Review on the Security Vulnerabilities of the IoMT against Malware Attacks and DDoS",
        "author": "Lily Dzamesi, and Nelly Elsayed",
        "link": "http://arxiv.org/abs/2501.07703v1",
        "abstract": "The Internet of Medical Things (IoMT) has transformed the healthcare industry\nby connecting medical devices in monitoring treatment outcomes of patients.\nThis increased connectivity has resulted to significant security\nvulnerabilities in the case of malware and Distributed Denial of Service (DDoS)\nattacks. This literature review examines the vulnerabilities of IoMT devices,\nfocusing on critical threats and exploring mitigation strategies. We conducted\na comprehensive search across leading databases such as ACM Digital Library,\nIEEE Xplore, and Elsevier to analyze peer-reviewed studies published within the\nlast five years (from 2019 to 2024). The review shows that inadequate\nencryption protocols, weak authentication methods, and irregular firmware\nupdates are the main causes of risks associated with IoMT devices. We have\nidentified emerging solutions like machine learning algorithms, blockchain\ntechnology, and edge computing as promising approaches to enhance IoMT\nsecurity. This review emphasizes the pressing need to develop lightweight\nsecurity measures and standardized protocols to protect patient data and ensure\nthe integrity of healthcare services."
    },
    {
        "date": "2025-01",
        "title": "Masking Countermeasures Against Side-Channel Attacks on Quantum Computers",
        "author": "Jason T. LeGrow, Travis Morrison, Jamie Sikora, and Nicolas Swanson",
        "link": "http://arxiv.org/abs/2501.07695v1",
        "abstract": "We propose a modification to the transpiler of a quantum computer to\nsafeguard against side-channel attacks aimed at learning information about a\nquantum circuit. We demonstrate that if it is feasible to shield a specific\nsubset of gates from side-channel attacks, then it is possible to conceal all\ninformation in a quantum circuit by transpiling it into a new circuit whose\ndepth grows linearly, depending on the quantum computer's architecture. We\nprovide concrete examples of implementing this protection on IBM's quantum\ncomputers, utilizing their virtual gates and editing their transpiler."
    },
    {
        "date": "2025-01",
        "title": "Exploring and Mitigating Adversarial Manipulation of Voting-Based Leaderboards",
        "author": "Yangsibo Huang, Milad Nasr, Anastasios Angelopoulos, Nicholas Carlini, Wei-Lin Chiang, Christopher A. Choquette-Choo, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Ken Ziyu Liu, Ion Stoica, Florian Tramer, and Chiyuan Zhang",
        "link": "http://arxiv.org/abs/2501.07493v1",
        "abstract": "It is now common to evaluate Large Language Models (LLMs) by having humans\nmanually vote to evaluate model outputs, in contrast to typical benchmarks that\nevaluate knowledge or skill at some particular task. Chatbot Arena, the most\npopular benchmark of this type, ranks models by asking users to select the\nbetter response between two randomly selected models (without revealing which\nmodel was responsible for the generations). These platforms are widely trusted\nas a fair and accurate measure of LLM capabilities. In this paper, we show that\nif bot protection and other defenses are not implemented, these voting-based\nbenchmarks are potentially vulnerable to adversarial manipulation.\nSpecifically, we show that an attacker can alter the leaderboard (to promote\ntheir favorite model or demote competitors) at the cost of roughly a thousand\nvotes (verified in a simulated, offline version of Chatbot Arena). Our attack\nconsists of two steps: first, we show how an attacker can determine which model\nwas used to generate a given reply with more than $95\\%$ accuracy; and then,\nthe attacker can use this information to consistently vote for (or against) a\ntarget model. Working with the Chatbot Arena developers, we identify, propose,\nand implement mitigations to improve the robustness of Chatbot Arena against\nadversarial manipulation, which, based on our analysis, substantially increases\nthe cost of such attacks. Some of these defenses were present before our\ncollaboration, such as bot protection with Cloudflare, malicious user\ndetection, and rate limiting. Others, including reCAPTCHA and login are being\nintegrated to strengthen the security in Chatbot Arena."
    },
    {
        "date": "2025-01",
        "title": "Encrypted Computation of Collision Probability for Secure Satellite Conjunction Analysis",
        "author": "Jihoon Suh, Michael Hibbard, Kaoru Teranishi, Takashi Tanaka, Moriba Jah, and Maruthi Akella",
        "link": "http://arxiv.org/abs/2501.07476v1",
        "abstract": "The computation of collision probability ($\\mathcal{P}_c$) is crucial for\nspace environmentalism and sustainability by providing decision-making\nknowledge that can prevent collisions between anthropogenic space objects.\nHowever, the accuracy and precision of $\\mathcal{P}_c$ computations is often\ncompromised by limitations in computational resources and data availability.\nWhile significant improvements have been made in the computational aspects, the\nrising concerns regarding the privacy of collaborative data sharing can be a\nmajor limiting factor in the future conjunction analysis and risk assessment,\nespecially as the space environment grows increasingly privatized, competitive,\nand fraught with conflicting strategic interests. This paper argues that the\nimportance of privacy measures in space situational awareness (SSA) is\nunderappreciated, and regulatory and compliance measures currently in place are\nnot sufficient by themselves, presenting a significant gap.\n  To address this gap, we introduce a novel encrypted architecture that\nleverages advanced cryptographic techniques, including homomorphic encryption\n(HE) and multi-party computation (MPC), to safeguard the privacy of entities\ncomputing space sustainability metrics, inter alia, $\\mathcal{P}_c$. Our\nproposed protocol, Encrypted $\\mathcal{P}_c$, integrates the Monte Carlo\nestimation algorithm with cryptographic solutions, enabling secure collision\nprobability computation without exposing sensitive or proprietary information.\nThis research advances secure conjunction analysis by developing a secure MPC\nprotocol for $\\mathcal{P}_c$ computation and highlights the need for innovative\nprotocols to ensure a more secure and cooperative SSA landscape."
    },
    {
        "date": "2025-01",
        "title": "Am I Infected? Lessons from Operating a Large-Scale IoT Security Diagnostic Service",
        "author": "Takayuki Sasaki, Tomoya Inazawa, Youhei Yamaguchi, Simon Parkin, Michel van Eeten, Katsunari Yoshioka, and Tsutomu Matsumoto",
        "link": "http://arxiv.org/abs/2501.07326v1",
        "abstract": "There is an expectation that users of home IoT devices will be able to secure\nthose devices, but they may lack information about what they need to do. In\nFebruary 2022, we launched a web service that scans users' IoT devices to\ndetermine how secure they are. The service aims to diagnose and remediate\nvulnerabilities and malware infections of IoT devices of Japanese users. This\npaper reports on findings from operating this service drawn from three studies:\n(1) the engagement of 114,747 users between February, 2022 - May, 2024; (2) a\nlarge-scale evaluation survey among service users (n=4,103), and; (3) an\ninvestigation and targeted survey (n=90) around the remediation actions of\nusers of non-secure devices. During the operation, we notified 417 (0.36%)\nusers that one or more of their devices were detected as vulnerable, and 171\n(0.15%) users that one of their devices was infected with malware. The service\nfound no issues for 99% of users. Still, 96% of all users evaluated the service\npositively, most often for it providing reassurance, being free of charge, and\nshort diagnosis time. Of the 171 users with malware infections, 67 returned to\nthe service later for a new check, with 59 showing improvement. Of the 417\nusers with vulnerable devices, 151 users revisited and re-diagnosed, where 75\nshowed improvement. We report on lessons learned, including a consideration of\nthe capabilities that non-expert users will assume of a security scan."
    },
    {
        "date": "2025-01",
        "title": "Generating Poisoning Attacks against Ridge Regression Models with Categorical Features",
        "author": "Monse Guedes-Ayala, Lars Schewe, Zeynep Suvak, and Miguel Anjos",
        "link": "http://arxiv.org/abs/2501.07275v1",
        "abstract": "Machine Learning (ML) models have become a very powerful tool to extract\ninformation from large datasets and use it to make accurate predictions and\nautomated decisions. However, ML models can be vulnerable to external attacks,\ncausing them to underperform or deviate from their expected tasks. One way to\nattack ML models is by injecting malicious data to mislead the algorithm during\nthe training phase, which is referred to as a poisoning attack. We can prepare\nfor such situations by designing anticipated attacks, which are later used for\ncreating and testing defence strategies. In this paper, we propose an algorithm\nto generate strong poisoning attacks for a ridge regression model containing\nboth numerical and categorical features that explicitly models and poisons\ncategorical features. We model categorical features as SOS-1 sets and formulate\nthe problem of designing poisoning attacks as a bilevel optimization problem\nthat is nonconvex mixed-integer in the upper-level and unconstrained convex\nquadratic in the lower-level. We present the mathematical formulation of the\nproblem, introduce a single-level reformulation based on the Karush-Kuhn-Tucker\n(KKT) conditions of the lower level, find bounds for the lower-level variables\nto accelerate solver performance, and propose a new algorithm to poison\ncategorical features. Numerical experiments show that our method improves the\nmean squared error of all datasets compared to the previous benchmark in the\nliterature."
    },
    {
        "date": "2025-01",
        "title": "MOS-Attack: A Scalable Multi-objective Adversarial Attack Framework",
        "author": "Ping Guo, Cheng Gong, Xi Lin, Fei Liu, Zhichao Lu, Qingfu Zhang, and Zhenkun Wang",
        "link": "http://arxiv.org/abs/2501.07251v2",
        "abstract": "Crafting adversarial examples is crucial for evaluating and enhancing the\nrobustness of Deep Neural Networks (DNNs), presenting a challenge equivalent to\nmaximizing a non-differentiable 0-1 loss function.\n  However, existing single objective methods, namely adversarial attacks focus\non a surrogate loss function, do not fully harness the benefits of engaging\nmultiple loss functions, as a result of insufficient understanding of their\nsynergistic and conflicting nature.\n  To overcome these limitations, we propose the Multi-Objective Set-based\nAttack (MOS Attack), a novel adversarial attack framework leveraging multiple\nloss functions and automatically uncovering their interrelations.\n  The MOS Attack adopts a set-based multi-objective optimization strategy,\nenabling the incorporation of numerous loss functions without additional\nparameters.\n  It also automatically mines synergistic patterns among various losses,\nfacilitating the generation of potent adversarial attacks with fewer\nobjectives.\n  Extensive experiments have shown that our MOS Attack outperforms\nsingle-objective attacks. Furthermore, by harnessing the identified synergistic\npatterns, MOS Attack continues to show superior results with a reduced number\nof loss functions."
    },
    {
        "date": "2025-01",
        "title": "A Secure Remote Password Protocol From The Learning With Errors Problem",
        "author": "Huapeng Li, and Baocheng Wang",
        "link": "http://arxiv.org/abs/2501.07208v1",
        "abstract": "Secure Remote Password (SRP) protocol is an essential password-authenticated\nkey exchange (PAKE) protocol based on the discrete logarithm problem (DLP). The\nprotocol is specifically designed to obtain a session key and it has been\nwidely used in various scenarios due to its attractive security features. In\nthe SRP protocol, the server is not required to save any data directly\nassociated with passwords. And this makes attackers who manage to corrupt the\nserver fail to impersonate the client unless performing a brute-force search\nfor the password. However, the development of quantum computing has potentially\nmade classic DLP-based public-key cryptography schemes not secure, including\nthe SRP protocol. So it is significant to design a quantum-resistant SRP\nprotocol. In this paper, based on the original scheme, we propose a\npost-quantum SRP protocol from the learning with errors (LWE) problem. And we\ngive rigorous proof and analyses on the correctness and security of the scheme.\nBesides being resistant to known quantum attacks, it maintains the various\nsecure qualities of the original protocol."
    },
    {
        "date": "2025-01",
        "title": "Beyond Security-by-design: Securing a compromised system",
        "author": "Awais Rashid, Sana Belguith, Matthew Bradbury, Sadie Creese, Ivan Flechais, and Neeraj Suri",
        "link": "http://arxiv.org/abs/2501.07207v1",
        "abstract": "Digital infrastructures are seeing convergence and connectivity at\nunprecedented scale. This is true for both current critical national\ninfrastructures and emerging future systems that are highly cyber-physical in\nnature with complex intersections between humans and technologies, e.g., smart\ncities, intelligent transportation, high-value manufacturing and Industry 4.0.\nDiverse legacy and non-legacy software systems underpinned by heterogeneous\nhardware compose on-the-fly to deliver services to millions of users with\nvarying requirements and unpredictable actions. This complexity is compounded\nby intricate and complicated supply-chains with many digital assets and\nservices outsourced to third parties. The reality is that, at any particular\npoint in time, there will be untrusted, partially-trusted or compromised\nelements across the infrastructure. Given this reality, and the societal scale\nof digital infrastructures, delivering secure and resilient operations is a\nmajor challenge. We argue that this requires us to move beyond the paradigm of\nsecurity-by-design and embrace the challenge of securing-a-compromised-system."
    },
    {
        "date": "2025-01",
        "title": "Generalizable Graph Neural Networks for Robust Power Grid Topology Control",
        "author": "Matthijs de Jong, Jan Viebahn, and Yuliya Shapovalova",
        "link": "http://arxiv.org/abs/2501.07186v1",
        "abstract": "The energy transition necessitates new congestion management methods. One\nsuch method is controlling the grid topology with machine learning (ML). This\napproach has gained popularity following the Learning to Run a Power Network\n(L2RPN) competitions. Graph neural networks (GNNs) are a class of ML models\nthat reflect graph structure in their computation, which makes them suitable\nfor power grid modeling. Various GNN approaches for topology control have thus\nbeen proposed. We propose the first GNN model for grid topology control that\nuses only GNN layers. Additionally, we identify the busbar information\nasymmetry problem that the popular homogeneous graph representation suffers\nfrom, and propose a heterogeneous graph representation to resolve it. We train\nboth homogeneous and heterogeneous GNNs and fully connected neural networks\n(FCNN) baselines on an imitation learning task. We evaluate the models\naccording to their classification accuracy and grid operation ability. We find\nthat the heterogeneous GNNs perform best on in-distribution networks, followed\nby the FCNNs, and lastly, the homogeneous GNNs. We also find that both GNN\ntypes generalize better to out-of-distribution networks than FCNNs."
    },
    {
        "date": "2025-01",
        "title": "Robust Single Object Tracking in LiDAR Point Clouds under Adverse Weather Conditions",
        "author": "Xiantong Zhao, Xiuping Liu, Shengjing Tian, and Yinan Han",
        "link": "http://arxiv.org/abs/2501.07133v1",
        "abstract": "3D single object tracking (3DSOT) in LiDAR point clouds is a critical task\nfor outdoor perception, enabling real-time perception of object location,\norientation, and motion. Despite the impressive performance of current 3DSOT\nmethods, evaluating them on clean datasets inadequately reflects their\ncomprehensive performance, as the adverse weather conditions in real-world\nsurroundings has not been considered. One of the main obstacles is the lack of\nadverse weather benchmarks for the evaluation of 3DSOT. To this end, this work\nproposes a challenging benchmark for LiDAR-based 3DSOT in adverse weather,\nwhich comprises two synthetic datasets (KITTI-A and nuScenes-A) and one\nreal-world dataset (CADC-SOT) spanning three weather types: rain, fog, and\nsnow. Based on this benchmark, five representative 3D trackers from different\ntracking frameworks conducted robustness evaluation, resulting in significant\nperformance degradations. This prompts the question: What are the factors that\ncause current advanced methods to fail on such adverse weather samples?\nConsequently, we explore the impacts of adverse weather and answer the above\nquestion from three perspectives: 1) target distance; 2) template shape\ncorruption; and 3) target shape corruption. Finally, based on domain\nrandomization and contrastive learning, we designed a dual-branch tracking\nframework for adverse weather, named DRCT, achieving excellent performance in\nbenchmarks."
    },
    {
        "date": "2025-01",
        "title": "Beyond the Surface: An NLP-based Methodology to Automatically Estimate CVE Relevance for CAPEC Attack Patterns",
        "author": "Silvia Bonomi, Andrea Ciavotta, Simone Lenti, and Alessandro Palma",
        "link": "http://arxiv.org/abs/2501.07131v1",
        "abstract": "Threat analysis is continuously growing in importance due to the\nalways-increasing complexity and frequency of cyber attacks. Analyzing threats\ndemands significant effort from security experts, leading to delays in the\nsecurity analysis process. Different cybersecurity knowledge bases are\ncurrently available to support this task but manual efforts are often required\nto correlate such heterogenous sources into a unified view that would enable a\nmore comprehensive assessment. To address this gap, we propose a methodology\nleveraging Natural Language Processing (NLP) to effectively and efficiently\nassociate Common Vulnerabilities and Exposure (CVE) vulnerabilities with Common\nAttack Pattern Enumeration and Classification (CAPEC) attack patterns. The\nproposed technique combines semantic similarity with keyword analysis to\nimprove the accuracy of association estimations. Experimental evaluations\ndemonstrate superior performance compared to state-of-the-art models, reducing\nmanual effort and analysis time, and enabling cybersecurity professionals to\nprioritize critical tasks."
    },
    {
        "date": "2025-01",
        "title": "SFC-GAN: A Generative Adversarial Network for Brain Functional and Structural Connectome Translation",
        "author": "Yee-Fan Tan, Jun Lin Liow, Pei-Sze Tan, Fuad Noman, Raphael C. -W. Phan, Hernando Ombao, and Chee-Ming Ting",
        "link": "http://arxiv.org/abs/2501.07055v1",
        "abstract": "Modern brain imaging technologies have enabled the detailed reconstruction of\nhuman brain connectomes, capturing structural connectivity (SC) from diffusion\nMRI and functional connectivity (FC) from functional MRI. Understanding the\nintricate relationships between SC and FC is vital for gaining deeper insights\ninto the brain's functional and organizational mechanisms. However, obtaining\nboth SC and FC modalities simultaneously remains challenging, hindering\ncomprehensive analyses. Existing deep generative models typically focus on\nsynthesizing a single modality or unidirectional translation between FC and SC,\nthereby missing the potential benefits of bi-directional translation,\nespecially in scenarios where only one connectome is available. Therefore, we\npropose Structural-Functional Connectivity GAN (SFC-GAN), a novel framework for\nbidirectional translation between SC and FC. This approach leverages the\nCycleGAN architecture, incorporating convolutional layers to effectively\ncapture the spatial structures of brain connectomes. To preserve the\ntopological integrity of these connectomes, we employ a structure-preserving\nloss that guides the model in capturing both global and local connectome\npatterns while maintaining symmetry. Our framework demonstrates superior\nperformance in translating between SC and FC, outperforming baseline models in\nsimilarity and graph property evaluations compared to ground truth data, each\ntranslated modality can be effectively utilized for downstream classification."
    },
    {
        "date": "2025-01",
        "title": "Protego: Detecting Adversarial Examples for Vision Transformers via Intrinsic Capabilities",
        "author": "Jialin Wu, Kaikai Pan, Yanjiao Chen, Jiangyi Deng, Shengyuan Pang, and Wenyuan Xu",
        "link": "http://arxiv.org/abs/2501.07044v1",
        "abstract": "Transformer models have excelled in natural language tasks, prompting the\nvision community to explore their implementation in computer vision problems.\nHowever, these models are still influenced by adversarial examples. In this\npaper, we investigate the attack capabilities of six common adversarial attacks\non three pretrained ViT models to reveal the vulnerability of ViT models. To\nunderstand and analyse the bias in neural network decisions when the input is\nadversarial, we use two visualisation techniques that are attention rollout and\ngrad attention rollout. To prevent ViT models from adversarial attack, we\npropose Protego, a detection framework that leverages the transformer intrinsic\ncapabilities to detection adversarial examples of ViT models. Nonetheless, this\nis challenging due to a diversity of attack strategies that may be adopted by\nadversaries. Inspired by the attention mechanism, we know that the token of\nprediction contains all the information from the input sample. Additionally,\nthe attention region for adversarial examples differs from that of normal\nexamples. Given these points, we can train a detector that achieves superior\nperformance than existing detection methods to identify adversarial examples.\nOur experiments have demonstrated the high effectiveness of our detection\nmethod. For these six adversarial attack methods, our detector's AUC scores all\nexceed 0.95. Protego may advance investigations in metaverse security."
    }
]