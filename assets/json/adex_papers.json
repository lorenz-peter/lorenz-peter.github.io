[
    {
        "date": "2025-01",
        "title": "A Note on Implementation Errors in Recent Adaptive Attacks Against Multi-Resolution Self-Ensembles",
        "author": "Stanislav Fort",
        "link": "http://arxiv.org/abs/2501.14496v1",
        "abstract": "This note documents an implementation issue in recent adaptive attacks (Zhang\net al. [2024]) against the multi-resolution self-ensemble defense (Fort and\nLakshminarayanan [2024]). The implementation allowed adversarial perturbations\nto exceed the standard $L_\\infty = 8/255$ bound by up to a factor of\n20$\\times$, reaching magnitudes of up to $L_\\infty = 160/255$. When attacks are\nproperly constrained within the intended bounds, the defense maintains\nnon-trivial robustness. Beyond highlighting the importance of careful\nvalidation in adversarial machine learning research, our analysis reveals an\nintriguing finding: properly bounded adaptive attacks against strong\nmulti-resolution self-ensembles often align with human perception, suggesting\nthe need to reconsider how we measure adversarial robustness."
    },
    {
        "date": "2025-01",
        "title": "Learning more with the same effort: how randomization improves the robustness of a robotic deep reinforcement learning agent",
        "author": "Luc\u00eda G\u00fcitta-L\u00f3pez, Jaime Boal, and \u00c1lvaro J. L\u00f3pez-L\u00f3pez",
        "link": "http://arxiv.org/abs/2501.14443v1",
        "abstract": "The industrial application of Deep Reinforcement Learning (DRL) is frequently\nslowed down because of the inability to generate the experience required to\ntrain the models. Collecting data often involves considerable time and economic\neffort that is unaffordable in most cases. Fortunately, devices like robots can\nbe trained with synthetic experience thanks to virtual environments. With this\napproach, the sample efficiency problems of artificial agents are mitigated,\nbut another issue arises: the need for efficiently transferring the synthetic\nexperience into the real world (sim-to-real).\n  This paper analyzes the robustness of a state-of-the-art sim-to-real\ntechnique known as progressive neural networks (PNNs) and studies how adding\ndiversity to the synthetic experience can complement it. To better understand\nthe drivers that lead to a lack of robustness, the robotic agent is still\ntested in a virtual environment to ensure total control on the divergence\nbetween the simulated and real models.\n  The results show that a PNN-like agent exhibits a substantial decrease in its\nrobustness at the beginning of the real training phase. Randomizing certain\nvariables during simulation-based training significantly mitigates this issue.\nOn average, the increase in the model's accuracy is around 25% when diversity\nis introduced in the training process. This improvement can be translated into\na decrease in the required real experience for the same final robustness\nperformance. Notwithstanding, adding real experience to agents should still be\nbeneficial regardless of the quality of the virtual experience fed into the\nagent."
    },
    {
        "date": "2025-01",
        "title": "Timelock-Free Rationally-Secure Virtual Channels",
        "author": "Zeta Avarikioti, Yuheng Wang, and Yuyi Wang",
        "link": "http://arxiv.org/abs/2501.14418v1",
        "abstract": "Payment channel networks (PCNs) offer a promising solution to address the\nlimited transaction throughput of deployed blockchains. However, several\nattacks have recently been proposed that stress the vulnerability of PCNs to\ntimelock and censoring attacks. To address such attacks, we introduce\nThunderdome, the first timelock-free PCN. Instead, Thunderdome leverages the\ndesign rationale of virtual channels to extend a timelock-free payment channel\nprimitive, thereby enabling multi-hop transactions without timelocks. Previous\nworks either utilize timelocks or do not accommodate transactions between\nparties that do not share a channel.\n  At its core, Thunderdome relies on a committee of non-trusted watchtowers,\nknown as wardens, who ensure that no honest party loses funds, even when\noffline, during the channel closure process. We introduce tailored incentive\nmechanisms to ensure that all participants follow the protocol's correct\nexecution. Besides a traditional security proof that assumes an honest majority\nof the committee, we conduct a formal game-theoretic analysis to demonstrate\nthe security of Thunderdome when all participants, including wardens, act\nrationally. We implement a proof of concept of Thunderdome on Ethereum to\nvalidate its feasibility and evaluate its costs. Our evaluation shows that\ndeploying Thunderdome, including opening the underlying payment channel, costs\napproximately \\$15 (0.0089 ETH), while the worst-case cost for closing a\nchannel is about \\$7 (0.004 ETH)."
    },
    {
        "date": "2025-01",
        "title": "Online Inverse Linear Optimization: Improved Regret Bound, Robustness to Suboptimality, and Toward Tight Regret Analysis",
        "author": "Shinsaku Sakaue, Taira Tsuchiya, Han Bao, and Taihei Oki",
        "link": "http://arxiv.org/abs/2501.14349v1",
        "abstract": "We study an online learning problem where, over $T$ rounds, a learner\nobserves both time-varying sets of feasible actions and an agent's optimal\nactions, selected by solving linear optimization over the feasible actions. The\nlearner sequentially makes predictions of the agent's underlying linear\nobjective function, and their quality is measured by the regret, the cumulative\ngap between optimal objective values and those achieved by following the\nlearner's predictions. A seminal work by B\\\"armann et al. (ICML 2017) showed\nthat online learning methods can be applied to this problem to achieve regret\nbounds of $O(\\sqrt{T})$. Recently, Besbes et al. (COLT 2021, Oper. Res. 2023)\nsignificantly improved the result by achieving an $O(n^4\\ln T)$ regret bound,\nwhere $n$ is the dimension of the ambient space of objective vectors. Their\nmethod, based on the ellipsoid method, runs in polynomial time but is\ninefficient for large $n$ and $T$. In this paper, we obtain an $O(n\\ln T)$\nregret bound, improving upon the previous bound of $O(n^4\\ln T)$ by a factor of\n$n^3$. Our method is simple and efficient: we apply the online Newton step\n(ONS) to appropriate exp-concave loss functions. Moreover, for the case where\nthe agent's actions are possibly suboptimal, we establish an $O(n\\ln\nT+\\sqrt{\\Delta_Tn\\ln T})$ regret bound, where $\\Delta_T$ is the cumulative\nsuboptimality of the agent's actions. This bound is achieved by using MetaGrad,\nwhich runs ONS with $\\Theta(\\ln T)$ different learning rates in parallel. We\nalso provide a simple instance that implies an $\\Omega(n)$ lower bound, showing\nthat our $O(n\\ln T)$ bound is tight up to an $O(\\ln T)$ factor. This gives rise\nto a natural question: can the $O(\\ln T)$ factor in the upper bound be removed?\nFor the special case of $n=2$, we show that an $O(1)$ regret bound is possible,\nwhile we delineate challenges in extending this result to higher dimensions."
    },
    {
        "date": "2025-01",
        "title": "Securing DRAM at Scale: ARFM-Driven Row Hammer Defense with Unveiling the Threat of Short tRC Patterns",
        "author": "Nogeun Joo, Donghyuk Kim, Hyunjun Cho, Junseok Noh, Dongha Jung, and Joo-Young Kim",
        "link": "http://arxiv.org/abs/2501.14328v1",
        "abstract": "To address the issue of powerful row hammer (RH) attacks, our study involved\nan extensive analysis of the prevalent attack patterns in the field. We\ndiscovered a strong correlation between the timing and density of the\nactive-to-active command period, ${tRC}$, and the likelihood of RH attacks. In\nthis paper, we introduce MARC, an innovative ARFM-driven RH mitigation IP that\nsignificantly reinforces existing RH mitigation IPs. MARC dynamically adjusts\nthe frequency of RFM in response to the severity of the RH attack environment,\noffering a tailored security solution that not only detects the threats but\nalso adapts to varying threat levels. MARC's detection mechanism has\ndemonstrated remarkable efficiency, identifying over 99\\% of attack patterns.\nMoreover, MARC is designed as a compact hardware module, facilitating tight\nintegration either on the memory controller-side or DRAM-side within the memory\nsystem. It only occupies a negligible hardware area of 3363~\\textit{$\\mu m^2$}.\nBy activating ARFM based on MARC's detection, the additional energy overhead is\nalso negligible in normal workloads. We conduct experiments to compare the\nhighest row count throughout the patterns, defined as max exposure, between the\nvanilla RH mitigation IPs and the MARC-enhanced versions of the same IPs,\nfocusing on both DRAM-side and memory controller-side. On the DRAM-side, MARC +\nprobabilistic scheme and MARC + counter-based tracking scheme achieve\n8.1$\\times$ and 1.5$\\times$ improvement in max exposure ratio compared to the\nvanilla IPs, respectively. On the memory controller-side, the MARC + PARA and\nMARC + Graphene achieve 50$\\times$ and 5.7$\\times$ improvement in max exposure\nratio compared to the vanilla IPs, respectively. MARC ensures optimal security\nwithout sacrificing system performance, making MARC a pioneering solution in\nthe realm of RH attack mitigation."
    },
    {
        "date": "2025-01",
        "title": "Relative Layer-Wise Relevance Propagation: a more Robust Neural Networks eXplaination",
        "author": "Eric Nyiri, and Olivier Gibaru",
        "link": "http://arxiv.org/abs/2501.14322v1",
        "abstract": "Machine learning methods are solving very successfully a plethora of tasks,\nbut they have the disadvantage of not providing any information about their\ndecision. Consequently, estimating the reasoning of the system provides\nadditional information. For this, Layer-Wise Relevance Propagation (LRP) is one\nof the methods in eXplainable Machine Learning (XML). Its purpose is to provide\ncontributions of any neural network output in the domain of its input. The main\ndrawback of current methods is mainly due to division by small values. To\novercome this problem, we provide a new definition called Relative LRP where\nthe classical conservation law is satisfied up to a multiplicative factor but\nwithout divisions by small values except for Resnet skip connection. In this\narticle, we will focus on image classification. This allows us to visualize the\ncontributions of a pixel to the predictions of a multi-layer neural network.\nPixel contributions provide a focus to further analysis on regions of potential\ninterest. R-LRP can be applied for any dense, CNN or residual neural networks.\nMoreover, R-LRP doesn't need any hyperparameters to tune contrary to other LRP\nmethods. We then compare the R-LRP method on different datasets with simple\nCNN, VGG16, VGG19 and Resnet50 networks."
    },
    {
        "date": "2025-01",
        "title": "Scalable Benchmarking and Robust Learning for Noise-Free Ego-Motion and 3D Reconstruction from Noisy Video",
        "author": "Xiaohao Xu, Tianyi Zhang, Shibo Zhao, Xiang Li, Sibo Wang, Yongqi Chen, Ye Li, Bhiksha Raj, Matthew Johnson-Roberson, Sebastian Scherer, and Xiaonan Huang",
        "link": "http://arxiv.org/abs/2501.14319v1",
        "abstract": "We aim to redefine robust ego-motion estimation and photorealistic 3D\nreconstruction by addressing a critical limitation: the reliance on noise-free\ndata in existing models. While such sanitized conditions simplify evaluation,\nthey fail to capture the unpredictable, noisy complexities of real-world\nenvironments. Dynamic motion, sensor imperfections, and synchronization\nperturbations lead to sharp performance declines when these models are deployed\nin practice, revealing an urgent need for frameworks that embrace and excel\nunder real-world noise. To bridge this gap, we tackle three core challenges:\nscalable data generation, comprehensive benchmarking, and model robustness\nenhancement. First, we introduce a scalable noisy data synthesis pipeline that\ngenerates diverse datasets simulating complex motion, sensor imperfections, and\nsynchronization errors. Second, we leverage this pipeline to create\nRobust-Ego3D, a benchmark rigorously designed to expose noise-induced\nperformance degradation, highlighting the limitations of current learning-based\nmethods in ego-motion accuracy and 3D reconstruction quality. Third, we propose\nCorrespondence-guided Gaussian Splatting (CorrGS), a novel test-time adaptation\nmethod that progressively refines an internal clean 3D representation by\naligning noisy observations with rendered RGB-D frames from clean 3D map,\nenhancing geometric alignment and appearance restoration through visual\ncorrespondence. Extensive experiments on synthetic and real-world data\ndemonstrate that CorrGS consistently outperforms prior state-of-the-art\nmethods, particularly in scenarios involving rapid motion and dynamic\nillumination."
    },
    {
        "date": "2025-01",
        "title": "Distributionally Robust Coreset Selection under Covariate Shift",
        "author": "Tomonari Tanaka, Hiroyuki Hanada, Hanting Yang, Tatsuya Aoyama, Yu Inatsu, Satoshi Akahane, Yoshito Okura, Noriaki Hashimoto, Taro Murayama, Hanju Lee, Shinya Kojima, and Ichiro Takeuchi",
        "link": "http://arxiv.org/abs/2501.14253v1",
        "abstract": "Coreset selection, which involves selecting a small subset from an existing\ntraining dataset, is an approach to reducing training data, and various\napproaches have been proposed for this method. In practical situations where\nthese methods are employed, it is often the case that the data distributions\ndiffer between the development phase and the deployment phase, with the latter\nbeing unknown. Thus, it is challenging to select an effective subset of\ntraining data that performs well across all deployment scenarios. We therefore\npropose Distributionally Robust Coreset Selection (DRCS). DRCS theoretically\nderives an estimate of the upper bound for the worst-case test error, assuming\nthat the future covariate distribution may deviate within a defined range from\nthe training distribution. Furthermore, by selecting instances in a way that\nsuppresses the estimate of the upper bound for the worst-case test error, DRCS\nachieves distributionally robust training instance selection. This study is\nprimarily applicable to convex training computation, but we demonstrate that it\ncan also be applied to deep learning under appropriate approximations. In this\npaper, we focus on covariate shift, a type of data distribution shift, and\ndemonstrate the effectiveness of DRCS through experiments."
    },
    {
        "date": "2025-01",
        "title": "Siren: A Learning-Based Multi-Turn Attack Framework for Simulating Real-World Human Jailbreak Behaviors",
        "author": "Yi Zhao, and Youzhi Zhang",
        "link": "http://arxiv.org/abs/2501.14250v1",
        "abstract": "Large language models (LLMs) are widely used in real-world applications,\nraising concerns about their safety and trustworthiness. While red-teaming with\njailbreak prompts exposes the vulnerabilities of LLMs, current efforts focus\nprimarily on single-turn attacks, overlooking the multi-turn strategies used by\nreal-world adversaries. Existing multi-turn methods rely on static patterns or\npredefined logical chains, failing to account for the dynamic strategies during\nattacks. We propose Siren, a learning-based multi-turn attack framework\ndesigned to simulate real-world human jailbreak behaviors. Siren consists of\nthree stages: (1) training set construction utilizing Turn-Level LLM feedback\n(Turn-MF), (2) post-training attackers with supervised fine-tuning (SFT) and\ndirect preference optimization (DPO), and (3) interactions between the\nattacking and target LLMs. Experiments demonstrate that Siren achieves an\nattack success rate (ASR) of 90% with LLaMA-3-8B as the attacker against\nGemini-1.5-Pro as the target model, and 70% with Mistral-7B against GPT-4o,\nsignificantly outperforming single-turn baselines. Moreover, Siren with a\n7B-scale model achieves performance comparable to a multi-turn baseline that\nleverages GPT-4o as the attacker, while requiring fewer turns and employing\ndecomposition strategies that are better semantically aligned with attack\ngoals. We hope Siren inspires the development of stronger defenses against\nadvanced multi-turn jailbreak attacks under realistic scenarios. Code is\navailable at https://github.com/YiyiyiZhao/siren. Warning: This paper contains\npotentially harmful text."
    },
    {
        "date": "2025-01",
        "title": "GreedyPixel: Fine-Grained Black-Box Adversarial Attack Via Greedy Algorithm",
        "author": "Hanrui Wang, Ching-Chun Chang, Chun-Shien Lu, Christopher Leckie, and Isao Echizen",
        "link": "http://arxiv.org/abs/2501.14230v1",
        "abstract": "A critical requirement for deep learning models is ensuring their robustness\nagainst adversarial attacks. These attacks commonly introduce noticeable\nperturbations, compromising the visual fidelity of adversarial examples.\nAnother key challenge is that while white-box algorithms can generate effective\nadversarial perturbations, they require access to the model gradients, limiting\ntheir practicality in many real-world scenarios. Existing attack mechanisms\nstruggle to achieve similar efficacy without access to these gradients. In this\npaper, we introduce GreedyPixel, a novel pixel-wise greedy algorithm designed\nto generate high-quality adversarial examples using only query-based feedback\nfrom the target model. GreedyPixel improves computational efficiency in what is\ntypically a brute-force process by perturbing individual pixels in sequence,\nguided by a pixel-wise priority map. This priority map is constructed by\nranking gradients obtained from a surrogate model, providing a structured path\nfor perturbation. Our results demonstrate that GreedyPixel achieves attack\nsuccess rates comparable to white-box methods without the need for gradient\ninformation, and surpasses existing algorithms in black-box settings, offering\nhigher success rates, reduced computational time, and imperceptible\nperturbations. These findings underscore the advantages of GreedyPixel in terms\nof attack efficacy, time efficiency, and visual quality."
    },
    {
        "date": "2025-01",
        "title": "SelfPrompt: Confidence-Aware Semi-Supervised Tuning for Robust Vision-Language Model Adaptation",
        "author": "Shuvendu Roy, and Ali Etemad",
        "link": "http://arxiv.org/abs/2501.14148v1",
        "abstract": "We present SelfPrompt, a novel prompt-tuning approach for vision-language\nmodels (VLMs) in a semi-supervised learning setup. Existing methods for tuning\nVLMs in semi-supervised setups struggle with the negative impact of the\nmiscalibrated VLMs on pseudo-labelling, and the accumulation of noisy\npseudo-labels. SelfPrompt addresses these challenges by introducing a\ncluster-guided pseudo-labelling method that improves pseudo-label accuracy, and\na confidence-aware semi-supervised learning module that maximizes the\nutilization of unlabelled data by combining supervised learning and\nweakly-supervised learning. Additionally, we investigate our method in an\nactive semi-supervised learning setup, where the labelled set is strategically\nselected to ensure the best utilization of a limited labelling budget. To this\nend, we propose a weakly-supervised sampling technique that selects a diverse\nand representative labelled set, which can be seamlessly integrated into\nexisting methods to enhance their performance. We conduct extensive evaluations\nacross 13 datasets, significantly surpassing state-of-the-art performances with\naverage improvements of 6.23% in standard semi-supervised learning, 6.25% in\nactive semi-supervised learning, and 4.9% in base-to-novel generalization,\nusing a 2-shot setup. Furthermore, SelfPrompt shows excellent generalization in\nsingle-shot settings, achieving an average improvement of 11.78%."
    },
    {
        "date": "2025-01",
        "title": "Reinforcement Learning Platform for Adversarial Black-box Attacks with Custom Distortion Filters",
        "author": "Soumyendu Sarkar, Ashwin Ramesh Babu, Sajad Mousavi, Vineet Gundecha, Sahand Ghorbanpour, Avisek Naug, Ricardo Luna Gutierrez, and Antonio Guillen",
        "link": "http://arxiv.org/abs/2501.14122v1",
        "abstract": "We present a Reinforcement Learning Platform for Adversarial Black-box\nuntargeted and targeted attacks, RLAB, that allows users to select from various\ndistortion filters to create adversarial examples. The platform uses a\nReinforcement Learning agent to add minimum distortion to input images while\nstill causing misclassification by the target model. The agent uses a novel\ndual-action method to explore the input image at each step to identify\nsensitive regions for adding distortions while removing noises that have less\nimpact on the target model. This dual action leads to faster and more efficient\nconvergence of the attack. The platform can also be used to measure the\nrobustness of image classification models against specific distortion types.\nAlso, retraining the model with adversarial samples significantly improved\nrobustness when evaluated on benchmark datasets. The proposed platform\noutperforms state-of-the-art methods in terms of the average number of queries\nrequired to cause misclassification. This advances trustworthiness with a\npositive social impact."
    },
    {
        "date": "2025-01",
        "title": "MedSlice: Fine-Tuned Large Language Models for Secure Clinical Note Sectioning",
        "author": "Joshua Davis, Thomas Sounack, Kate Sciacca, Jessie M Brain, Brigitte N Durieux, Nicole D Agaronnik, and Charlotta Lindvall",
        "link": "http://arxiv.org/abs/2501.14105v1",
        "abstract": "Extracting sections from clinical notes is crucial for downstream analysis\nbut is challenging due to variability in formatting and labor-intensive nature\nof manual sectioning. While proprietary large language models (LLMs) have shown\npromise, privacy concerns limit their accessibility. This study develops a\npipeline for automated note sectioning using open-source LLMs, focusing on\nthree sections: History of Present Illness, Interval History, and Assessment\nand Plan. We fine-tuned three open-source LLMs to extract sections using a\ncurated dataset of 487 progress notes, comparing results relative to\nproprietary models (GPT-4o, GPT-4o mini). Internal and external validity were\nassessed via precision, recall and F1 score. Fine-tuned Llama 3.1 8B\noutperformed GPT-4o (F1=0.92). On the external validity test set, performance\nremained high (F1= 0.85). Fine-tuned open-source LLMs can surpass proprietary\nmodels in clinical note sectioning, offering advantages in cost, performance,\nand accessibility."
    },
    {
        "date": "2025-01",
        "title": "Towards Robust Multimodal Open-set Test-time Adaptation via Adaptive Entropy-aware Optimization",
        "author": "Hao Dong, Eleni Chatzi, and Olga Fink",
        "link": "http://arxiv.org/abs/2501.13924v1",
        "abstract": "Test-time adaptation (TTA) has demonstrated significant potential in\naddressing distribution shifts between training and testing data. Open-set\ntest-time adaptation (OSTTA) aims to adapt a source pre-trained model online to\nan unlabeled target domain that contains unknown classes. This task becomes\nmore challenging when multiple modalities are involved. Existing methods have\nprimarily focused on unimodal OSTTA, often filtering out low-confidence samples\nwithout addressing the complexities of multimodal data. In this work, we\npresent Adaptive Entropy-aware Optimization (AEO), a novel framework\nspecifically designed to tackle Multimodal Open-set Test-time Adaptation\n(MM-OSTTA) for the first time. Our analysis shows that the entropy difference\nbetween known and unknown samples in the target domain strongly correlates with\nMM-OSTTA performance. To leverage this, we propose two key components:\nUnknown-aware Adaptive Entropy Optimization (UAE) and Adaptive Modality\nPrediction Discrepancy Optimization (AMP). These components enhance the ability\nof model to distinguish unknown class samples during online adaptation by\namplifying the entropy difference between known and unknown samples. To\nthoroughly evaluate our proposed methods in the MM-OSTTA setting, we establish\na new benchmark derived from existing datasets. This benchmark includes two\ndownstream tasks and incorporates five modalities. Extensive experiments across\nvarious domain shift situations demonstrate the efficacy and versatility of the\nAEO framework. Additionally, we highlight the strong performance of AEO in\nlong-term and continual MM-OSTTA settings, both of which are challenging and\nhighly relevant to real-world applications. Our source code is available at\nhttps://github.com/donghao51/AEO."
    },
    {
        "date": "2025-01",
        "title": "Logical Maneuvers: Detecting and Mitigating Adversarial Hardware Faults in Space",
        "author": "Fatemeh Khojasteh Dana, Saleh Khalaj Monfared, and Shahin Tajik",
        "link": "http://arxiv.org/abs/2501.13894v1",
        "abstract": "Satellites are highly vulnerable to adversarial glitches or high-energy\nradiation in space, which could cause faults on the onboard computer. Various\nradiation- and fault-tolerant methods, such as error correction codes (ECC) and\nredundancy-based approaches, have been explored over the last decades to\nmitigate temporary soft errors on software and hardware. However, conventional\nECC methods fail to deal with hard errors or permanent faults in the hardware\ncomponents. This work introduces a detection- and response-based countermeasure\nto deal with partially damaged processor chips. It recovers the processor chip\nfrom permanent faults and enables continuous operation with available undamaged\nresources on the chip. We incorporate digitally-compatible delay-based sensors\non the target processor's chip to reliably detect the incoming radiation or\nglitching attempts on the physical fabric of the chip, even before a fault\noccurs. Upon detecting a fault in one or more components of the processor's\narithmetic logic unit (ALU), our countermeasure employs adaptive software\nrecompilations to resynthesize and substitute the affected instructions with\ninstructions of still functioning components to accomplish the task.\nFurthermore, if the fault is more widespread and prevents the correct operation\nof the entire processor, our approach deploys adaptive hardware partial\nreconfigurations to replace and reroute the failed components to undamaged\nlocations of the chip. To validate our claims, we deploy a high-energy\nnear-infrared (NIR) laser beam on a RISC-V processor implemented on a 28~nm\nFPGA to emulate radiation and even hard errors by partially damaging the FPGA\nfabric. We demonstrate that our sensor can confidently detect the radiation and\ntrigger the processor testing and fault recovery mechanisms. Finally, we\ndiscuss the overhead imposed by our countermeasure."
    },
    {
        "date": "2025-01",
        "title": "PhotoGAN: Generative Adversarial Neural Network Acceleration with Silicon Photonics",
        "author": "Tharini Suresh, Salma Afifi, and Sudeep Pasricha",
        "link": "http://arxiv.org/abs/2501.13828v1",
        "abstract": "Generative Adversarial Networks (GANs) are at the forefront of AI innovation,\ndriving advancements in areas such as image synthesis, medical imaging, and\ndata augmentation. However, the unique computational operations within GANs,\nsuch as transposed convolutions and instance normalization, introduce\nsignificant inefficiencies when executed on traditional electronic\naccelerators, resulting in high energy consumption and suboptimal performance.\nTo address these challenges, we introduce PhotoGAN, the first silicon-photonic\naccelerator designed to handle the specialized operations of GAN models. By\nleveraging the inherent high throughput and energy efficiency of silicon\nphotonics, PhotoGAN offers an innovative, reconfigurable architecture capable\nof accelerating transposed convolutions and other GAN-specific layers. The\naccelerator also incorporates a sparse computation optimization technique to\nreduce redundant operations, improving computational efficiency. Our\nexperimental results demonstrate that PhotoGAN achieves at least 4.4x higher\nGOPS and 2.18x lower energy-per-bit (EPB) compared to state-of-the-art\naccelerators, including GPUs and TPUs. These findings showcase PhotoGAN as a\npromising solution for the next generation of GAN acceleration, providing\nsubstantial gains in both performance and energy efficiency."
    },
    {
        "date": "2025-01",
        "title": "WAFBOOSTER: Automatic Boosting of WAF Security Against Mutated Malicious Payloads",
        "author": "Cong Wu, Jing Chen, Simeng Zhu, Wenqi Feng, Ruiying Du, and Yang Xiang",
        "link": "http://arxiv.org/abs/2501.14008v1",
        "abstract": "Web application firewall (WAF) examines malicious traffic to and from a web\napplication via a set of security rules. It plays a significant role in\nsecuring Web applications against web attacks. However, as web attacks grow in\nsophistication, it is becoming increasingly difficult for WAFs to block the\nmutated malicious payloads designed to bypass their defenses. In response to\nthis critical security issue, we have developed a novel learning-based\nframework called WAFBOOSTER, designed to unveil potential bypasses in WAF\ndetections and suggest rules to fortify their security. Using a combination of\nshadow models and payload generation techniques, we can identify malicious\npayloads and remove or modify them as needed. WAFBOOSTER generates signatures\nfor these malicious payloads using advanced clustering and regular expression\nmatching techniques to repair any security gaps we uncover. In our\ncomprehensive evaluation of eight real-world WAFs, WAFBOOSTER improved the true\nrejection rate of mutated malicious payloads from 21% to 96%, with no false\nrejections. WAFBOOSTER achieves a false acceptance rate 3X lower than\nstate-of-the-art methods for generating malicious payloads. With WAFBOOSTER, we\nhave taken a step forward in securing web applications against the\never-evolving threats."
    },
    {
        "date": "2025-01",
        "title": "Defending against Adversarial Malware Attacks on ML-based Android Malware Detection Systems",
        "author": "Ping He, Lorenzo Cavallaro, and Shouling Ji",
        "link": "http://arxiv.org/abs/2501.13782v1",
        "abstract": "Android malware presents a persistent threat to users' privacy and data\nintegrity. To combat this, researchers have proposed machine learning-based\n(ML-based) Android malware detection (AMD) systems. However, adversarial\nAndroid malware attacks compromise the detection integrity of the ML-based AMD\nsystems, raising significant concerns. Existing defenses against adversarial\nAndroid malware provide protections against feature space attacks which\ngenerate adversarial feature vectors only, leaving protection against realistic\nthreats from problem space attacks which generate real adversarial malware an\nopen problem. In this paper, we address this gap by proposing ADD, a practical\nadversarial Android malware defense framework designed as a plug-in to enhance\nthe adversarial robustness of the ML-based AMD systems against problem space\nattacks. Our extensive evaluation across various ML-based AMD systems\ndemonstrates that ADD is effective against state-of-the-art problem space\nadversarial Android malware attacks. Additionally, ADD shows the defense\neffectiveness in enhancing the adversarial robustness of real-world antivirus\nsolutions."
    },
    {
        "date": "2025-01",
        "title": "Crossfire: An Elastic Defense Framework for Graph Neural Networks Under Bit Flip Attacks",
        "author": "Lorenz Kummer, Samir Moustafa, Wilfried Gansterer, and Nils Kriege",
        "link": "http://arxiv.org/abs/2501.13776v1",
        "abstract": "Bit Flip Attacks (BFAs) are a well-established class of adversarial attacks,\noriginally developed for Convolutional Neural Networks within the computer\nvision domain. Most recently, these attacks have been extended to target Graph\nNeural Networks (GNNs), revealing significant vulnerabilities. This new\ndevelopment naturally raises questions about the best strategies to defend GNNs\nagainst BFAs, a challenge for which no solutions currently exist. Given the\napplications of GNNs in critical fields, any defense mechanism must not only\nmaintain network performance, but also verifiably restore the network to its\npre-attack state. Verifiably restoring the network to its pre-attack state also\neliminates the need for costly evaluations on test data to ensure network\nquality. We offer first insights into the effectiveness of existing honeypot-\nand hashing-based defenses against BFAs adapted from the computer vision domain\nto GNNs, and characterize the shortcomings of these approaches. To overcome\ntheir limitations, we propose Crossfire, a hybrid approach that exploits weight\nsparsity and combines hashing and honeypots with bit-level correction of\nout-of-distribution weight elements to restore network integrity. Crossfire is\nretraining-free and does not require labeled data. Averaged over 2,160\nexperiments on six benchmark datasets, Crossfire offers a 21.8% higher\nprobability than its competitors of reconstructing a GNN attacked by a BFA to\nits pre-attack state. These experiments cover up to 55 bit flips from various\nattacks. Moreover, it improves post-repair prediction quality by 10.85%.\nComputational and storage overheads are negligible compared to the inherent\ncomplexity of even the simplest GNNs."
    },
    {
        "date": "2025-01",
        "title": "2-Tier SimCSE: Elevating BERT for Robust Sentence Embeddings",
        "author": "Yumeng Wang, Ziran Zhou, and Junjin Wang",
        "link": "http://arxiv.org/abs/2501.13758v1",
        "abstract": "Effective sentence embeddings that capture semantic nuances and generalize\nwell across diverse contexts are crucial for natural language processing tasks.\nWe address this challenge by applying SimCSE (Simple Contrastive Learning of\nSentence Embeddings) using contrastive learning to fine-tune the minBERT model\nfor sentiment analysis, semantic textual similarity (STS), and paraphrase\ndetection. Our contributions include experimenting with three different dropout\ntechniques, namely standard dropout, curriculum dropout, and adaptive dropout,\nto tackle overfitting, proposing a novel 2-Tier SimCSE Fine-tuning Model that\ncombines both unsupervised and supervised SimCSE on STS task, and exploring\ntransfer learning potential for Paraphrase and SST tasks. Our findings\ndemonstrate the effectiveness of SimCSE, with the 2-Tier model achieving\nsuperior performance on the STS task, with an average test score of 0.742\nacross all three downstream tasks. The results of error analysis reveals\nchallenges in handling complex sentiments and reliance on lexical overlap for\nparaphrase detection, highlighting areas for future research. The ablation\nstudy revealed that removing Adaptive Dropout in the Single-Task Unsupervised\nSimCSE Model led to improved performance on the STS task, indicating\noverfitting due to added parameters. Transfer learning from SimCSE models on\nParaphrase and SST tasks did not enhance performance, suggesting limited\ntransferability of knowledge from the STS task."
    },
    {
        "date": "2025-01",
        "title": "Exact Soft Analytical Side-Channel Attacks using Tractable Circuits",
        "author": "Thomas Wedenig, Rishub Nagpal, Ga\u00ebtan Cassiers, Stefan Mangard, and Robert Peharz",
        "link": "http://arxiv.org/abs/2501.13748v1",
        "abstract": "Detecting weaknesses in cryptographic algorithms is of utmost importance for\ndesigning secure information systems. The state-of-the-art soft analytical\nside-channel attack (SASCA) uses physical leakage information to make\nprobabilistic predictions about intermediate computations and combines these\n\"guesses\" with the known algorithmic logic to compute the posterior\ndistribution over the key. This attack is commonly performed via loopy belief\npropagation, which, however, lacks guarantees in terms of convergence and\ninference quality. In this paper, we develop a fast and exact inference method\nfor SASCA, denoted as ExSASCA, by leveraging knowledge compilation and\ntractable probabilistic circuits. When attacking the Advanced Encryption\nStandard (AES), the most widely used encryption algorithm to date, ExSASCA\noutperforms SASCA by more than 31% top-1 success rate absolute. By leveraging\nsparse belief messages, this performance is achieved with little more\ncomputational cost than SASCA, and about 3 orders of magnitude less than exact\ninference via exhaustive enumeration. Even with dense belief messages, ExSASCA\nstill uses 6 times less computations than exhaustive inference."
    },
    {
        "date": "2025-01",
        "title": "A Comprehensive Framework for Building Highly Secure, Network-Connected Devices: Chip to App",
        "author": "Khan Reaz, and Gerhard Wunder",
        "link": "http://arxiv.org/abs/2501.13716v1",
        "abstract": "The rapid expansion of connected devices has amplified the need for robust\nand scalable security frameworks. This paper proposes a holistic approach to\nsecuring network-connected devices, covering essential layers: hardware,\nfirmware, communication, and application. At the hardware level, we focus on\nsecure key management, reliable random number generation, and protecting\ncritical assets. Firmware security is addressed through mechanisms like\ncryptographic integrity validation and secure boot processes. For secure\ncommunication, we emphasize TLS 1.3 and optimized cipher suites tailored for\nboth standard and resource-constrained devices. To overcome the challenges of\nIoT, compact digital certificates, such as CBOR, are recommended to reduce\noverhead and enhance performance. Additionally, the paper explores\nforward-looking solutions, including post-quantum cryptography, to future-proof\nsystems against emerging threats. This framework provides actionable guidelines\nfor manufacturers and system administrators to build secure devices that\nmaintain confidentiality, integrity, and availability throughout their\nlifecycle."
    },
    {
        "date": "2025-01",
        "title": "Certified Robustness Under Bounded Levenshtein Distance",
        "author": "Elias Abad Rocamora, Grigorios G. Chrysos, and Volkan Cevher",
        "link": "http://arxiv.org/abs/2501.13676v1",
        "abstract": "Text classifiers suffer from small perturbations, that if chosen\nadversarially, can dramatically change the output of the model. Verification\nmethods can provide robustness certificates against such adversarial\nperturbations, by computing a sound lower bound on the robust accuracy.\nNevertheless, existing verification methods incur in prohibitive costs and\ncannot practically handle Levenshtein distance constraints. We propose the\nfirst method for computing the Lipschitz constant of convolutional classifiers\nwith respect to the Levenshtein distance. We use these Lipschitz constant\nestimates for training 1-Lipschitz classifiers. This enables computing the\ncertified radius of a classifier in a single forward pass. Our method, LipsLev,\nis able to obtain $38.80$% and $13.93$% verified accuracy at distance $1$ and\n$2$ respectively in the AG-News dataset, while being $4$ orders of magnitude\nfaster than existing approaches. We believe our work can open the door to more\nefficient verification in the text domain."
    },
    {
        "date": "2025-01",
        "title": "Device-aware Optical Adversarial Attack for a Portable Projector-camera System",
        "author": "Ning Jiang, Yanhong Liu, Dingheng Zeng, Yue Feng, Weihong Deng, and Ying Li",
        "link": "http://arxiv.org/abs/2501.14005v1",
        "abstract": "Deep-learning-based face recognition (FR) systems are susceptible to\nadversarial examples in both digital and physical domains. Physical attacks\npresent a greater threat to deployed systems as adversaries can easily access\nthe input channel, allowing them to provide malicious inputs to impersonate a\nvictim. This paper addresses the limitations of existing projector-camera-based\nadversarial light attacks in practical FR setups. By incorporating device-aware\nadaptations into the digital attack algorithm, such as resolution-aware and\ncolor-aware adjustments, we mitigate the degradation from digital to physical\ndomains. Experimental validation showcases the efficacy of our proposed\nalgorithm against real and spoof adversaries, achieving high physical\nsimilarity scores in FR models and state-of-the-art commercial systems. On\naverage, there is only a 14% reduction in scores from digital to physical\nattacks, with high attack success rate in both white- and black-box scenarios."
    },
    {
        "date": "2025-01",
        "title": "Towards Robust Incremental Learning under Ambiguous Supervision",
        "author": "Rui Wang, Mingxuan Xia, Chang Yao, Lei Feng, Junbo Zhao, Gang Chen, and Haobo Wang",
        "link": "http://arxiv.org/abs/2501.13584v2",
        "abstract": "Traditional Incremental Learning (IL) targets to handle sequential\nfully-supervised learning problems where novel classes emerge from time to\ntime. However, due to inherent annotation uncertainty and ambiguity, collecting\nhigh-quality annotated data in a dynamic learning system can be extremely\nexpensive. To mitigate this problem, we propose a novel weakly-supervised\nlearning paradigm called Incremental Partial Label Learning (IPLL), where the\nsequentially arrived data relate to a set of candidate labels rather than the\nground truth. Technically, we develop the Prototype-Guided Disambiguation and\nReplay Algorithm (PGDR) which leverages the class prototypes as a proxy to\nmitigate two intertwined challenges in IPLL, i.e., label ambiguity and\ncatastrophic forgetting. To handle the former, PGDR encapsulates a\nmomentum-based pseudo-labeling algorithm along with prototype-guided\ninitialization, resulting in a balanced perception of classes. To alleviate\nforgetting, we develop a memory replay technique that collects\nwell-disambiguated samples while maintaining representativeness and diversity.\nBy jointly distilling knowledge from curated memory data, our framework\nexhibits a great disambiguation ability for samples of new tasks and achieves\nless forgetting of knowledge. Extensive experiments demonstrate that PGDR\nachieves superior"
    },
    {
        "date": "2025-01",
        "title": "Black-Box Adversarial Attack on Vision Language Models for Autonomous Driving",
        "author": "Lu Wang, Tianyuan Zhang, Yang Qu, Siyuan Liang, Yuwei Chen, Aishan Liu, Xianglong Liu, and Dacheng Tao",
        "link": "http://arxiv.org/abs/2501.13563v1",
        "abstract": "Vision-language models (VLMs) have significantly advanced autonomous driving\n(AD) by enhancing reasoning capabilities; however, these models remain highly\nsusceptible to adversarial attacks. While existing research has explored\nwhite-box attacks to some extent, the more practical and challenging black-box\nscenarios remain largely underexplored due to their inherent difficulty. In\nthis paper, we take the first step toward designing black-box adversarial\nattacks specifically targeting VLMs in AD. We identify two key challenges for\nachieving effective black-box attacks in this context: the effectiveness across\ndriving reasoning chains in AD systems and the dynamic nature of driving\nscenarios. To address this, we propose Cascading Adversarial Disruption (CAD).\nIt first introduces Decision Chain Disruption, which targets low-level\nreasoning breakdown by generating and injecting deceptive semantics, ensuring\nthe perturbations remain effective across the entire decision-making chain.\nBuilding on this, we present Risky Scene Induction, which addresses dynamic\nadaptation by leveraging a surrogate VLM to understand and construct high-level\nrisky scenarios that are likely to result in critical errors in the current\ndriving contexts. Extensive experiments conducted on multiple AD VLMs and\nbenchmarks demonstrate that CAD achieves state-of-the-art attack effectiveness,\nsignificantly outperforming existing methods (+13.43% on average). Moreover, we\nvalidate its practical applicability through real-world attacks on AD vehicles\npowered by VLMs, where the route completion rate drops by 61.11% and the\nvehicle crashes directly into the obstacle vehicle with adversarial patches.\nFinally, we release CADA dataset, comprising 18,808 adversarial\nvisual-question-answer pairs, to facilitate further evaluation and research in\nthis critical domain. Our codes and dataset will be available after paper's\nacceptance."
    },
    {
        "date": "2025-01",
        "title": "POPS: From History to Mitigation of DNS Cache Poisoning Attacks",
        "author": "Yehuda Afek, Harel Berger, and Anat Bremler-Barr",
        "link": "http://arxiv.org/abs/2501.13540v1",
        "abstract": "We present a novel yet simple and comprehensive DNS cache POisoning\nPrevention System (POPS), designed to integrate as a module in Intrusion\nPrevention Systems (IPS). POPS addresses statistical DNS poisoning attacks,\nincluding those documented from 2002 to the present, and offers robust\nprotection against similar future threats. It consists of two main components:\na detection module that employs three simple rules, and a mitigation module\nthat leverages the TC flag in the DNS header to enhance security. Once\nactivated, the mitigation module has zero false positives or negatives,\ncorrecting any such errors on the side of the detection module.\n  We first analyze POPS against historical DNS services and attacks, showing\nthat it would have mitigated all network-based statistical poisoning attacks,\nyielding a success rate of only 0.0076% for the adversary. We then simulate\nPOPS on traffic benchmarks (PCAPs) incorporating current potential\nnetwork-based statistical poisoning attacks, and benign PCAPs; the simulated\nattacks still succeed with a probability of 0.0076%. This occurs because five\nmalicious packets go through before POPS detects the attack and activates the\nmitigation module. In addition, POPS completes its task using only 20%-50% of\nthe time required by other tools (e.g., Suricata or Snort), and after examining\njust 5%-10% as many packets. Furthermore, it successfully identifies DNS cache\npoisoning attacks-such as fragmentation attacks-that both Suricata and Snort\nfail to detect, underscoring its superiority in providing comprehensive DNS\nprotection."
    },
    {
        "date": "2025-01",
        "title": "Overcoming Support Dilution for Robust Few-shot Semantic Segmentation",
        "author": "Wailing Tang, Biqi Yang, Pheng-Ann Heng, Yun-Hui Liu, and Chi-Wing Fu",
        "link": "http://arxiv.org/abs/2501.13529v1",
        "abstract": "Few-shot Semantic Segmentation (FSS) is a challenging task that utilizes\nlimited support images to segment associated unseen objects in query images.\nHowever, recent FSS methods are observed to perform worse, when enlarging the\nnumber of shots. As the support set enlarges, existing FSS networks struggle to\nconcentrate on the high-contributed supports and could easily be overwhelmed by\nthe low-contributed supports that could severely impair the mask predictions.\nIn this work, we study this challenging issue, called support dilution, our\ngoal is to recognize, select, preserve, and enhance those high-contributed\nsupports in the raw support pool. Technically, our method contains three novel\nparts. First, we propose a contribution index, to quantitatively estimate if a\nhigh-contributed support dilutes. Second, we develop the Symmetric Correlation\n(SC) module to preserve and enhance the high-contributed support features,\nminimizing the distraction by the low-contributed features. Third, we design\nthe Support Image Pruning operation, to retrieve a compact and high quality\nsubset by discarding low-contributed supports. We conduct extensive experiments\non two FSS benchmarks, COCO-20i and PASCAL-5i, the segmentation results\ndemonstrate the compelling performance of our solution over state-of-the-art\nFSS approaches. Besides, we apply our solution for online segmentation and\nreal-world segmentation, convincing segmentation results showing the practical\nability of our work for real-world demonstrations."
    },
    {
        "date": "2025-01",
        "title": "Robust Amortized Bayesian Inference with Self-Consistency Losses on Unlabeled Data",
        "author": "Aayush Mishra, Daniel Habermann, Marvin Schmitt, Stefan T. Radev, and Paul-Christian B\u00fcrkner",
        "link": "http://arxiv.org/abs/2501.13483v1",
        "abstract": "Neural amortized Bayesian inference (ABI) can solve probabilistic inverse\nproblems orders of magnitude faster than classical methods. However, neural ABI\nis not yet sufficiently robust for widespread and safe applicability. In\nparticular, when performing inference on observations outside of the scope of\nthe simulated data seen during training, for example, because of model\nmisspecification, the posterior approximations are likely to become highly\nbiased. Due to the bad pre-asymptotic behavior of current neural posterior\nestimators in the out-of-simulation regime, the resulting estimation biases\ncannot be fixed in acceptable time by just simulating more training data. In\nthis proof-of-concept paper, we propose a semi-supervised approach that enables\ntraining not only on (labeled) simulated data generated from the model, but\nalso on unlabeled data originating from any source, including real-world data.\nTo achieve the latter, we exploit Bayesian self-consistency properties that can\nbe transformed into strictly proper losses without requiring knowledge of true\nparameter values, that is, without requiring data labels. The results of our\ninitial experiments show remarkable improvements in the robustness of ABI on\nout-of-simulation data. Even if the observed data is far away from both labeled\nand unlabeled training data, inference remains highly accurate. If our findings\nalso generalize to other scenarios and model classes, we believe that our new\nmethod represents a major breakthrough in neural ABI."
    },
    {
        "date": "2025-01",
        "title": "Adaptive Few-Shot Learning (AFSL): Tackling Data Scarcity with Stability, Robustness, and Versatility",
        "author": "Rishabh Agrawal",
        "link": "http://arxiv.org/abs/2501.13479v1",
        "abstract": "Few-shot learning (FSL) enables machine learning models to generalize\neffectively with minimal labeled data, making it crucial for data-scarce\ndomains such as healthcare, robotics, and natural language processing. Despite\nits potential, FSL faces challenges including sensitivity to initialization,\ndifficulty in adapting to diverse domains, and vulnerability to noisy datasets.\nTo address these issues, this paper introduces Adaptive Few-Shot Learning\n(AFSL), a framework that integrates advancements in meta-learning, domain\nalignment, noise resilience, and multi-modal integration. AFSL consists of four\nkey modules: a Dynamic Stability Module for performance consistency, a\nContextual Domain Alignment Module for domain adaptation, a Noise-Adaptive\nResilience Module for handling noisy data, and a Multi-Modal Fusion Module for\nintegrating diverse modalities. This work also explores strategies such as\ntask-aware data augmentation, semi-supervised learning, and explainable AI\ntechniques to enhance the applicability and robustness of FSL. AFSL provides\nscalable, reliable, and impactful solutions for real-world, high-stakes\ndomains."
    },
    {
        "date": "2025-01",
        "title": "GC-ConsFlow: Leveraging Optical Flow Residuals and Global Context for Robust Deepfake Detection",
        "author": "Jiaxin Chen, Miao Hu, Dengyong Zhang, and Jingyang Meng",
        "link": "http://arxiv.org/abs/2501.13435v1",
        "abstract": "The rapid development of Deepfake technology has enabled the generation of\nhighly realistic manipulated videos, posing severe social and ethical\nchallenges. Existing Deepfake detection methods primarily focused on either\nspatial or temporal inconsistencies, often neglecting the interplay between the\ntwo or suffering from interference caused by natural facial motions. To address\nthese challenges, we propose the global context consistency flow (GC-ConsFlow),\na novel dual-stream framework that effectively integrates spatial and temporal\nfeatures for robust Deepfake detection. The global grouped context aggregation\nmodule (GGCA), integrated into the global context-aware frame flow stream\n(GCAF), enhances spatial feature extraction by aggregating grouped global\ncontext information, enabling the detection of subtle, spatial artifacts within\nframes. The flow-gradient temporal consistency stream (FGTC), rather than\ndirectly modeling the residuals, it is used to improve the robustness of\ntemporal feature extraction against the inconsistency introduced by unnatural\nfacial motion using optical flow residuals and gradient-based features. By\ncombining these two streams, GC-ConsFlow demonstrates the effectiveness and\nrobustness in capturing complementary spatiotemporal forgery traces. Extensive\nexperiments show that GC-ConsFlow outperforms existing state-of-the-art methods\nin detecting Deepfake videos under various compression scenarios."
    },
    {
        "date": "2025-01",
        "title": "AEON: Adaptive Estimation of Instance-Dependent In-Distribution and Out-of-Distribution Label Noise for Robust Learning",
        "author": "Arpit Garg, Cuong Nguyen, Rafael Felix, Yuyuan Liu, Thanh-Toan Do, and Gustavo Carneiro",
        "link": "http://arxiv.org/abs/2501.13389v1",
        "abstract": "Robust training with noisy labels is a critical challenge in image\nclassification, offering the potential to reduce reliance on costly clean-label\ndatasets. Real-world datasets often contain a mix of in-distribution (ID) and\nout-of-distribution (OOD) instance-dependent label noise, a challenge that is\nrarely addressed simultaneously by existing methods and is further compounded\nby the lack of comprehensive benchmarking datasets. Furthermore, even though\ncurrent noisy-label learning approaches attempt to find noisy-label samples\nduring training, these methods do not aim to estimate ID and OOD noise rates to\npromote their effectiveness in the selection of such noisy-label samples, and\nthey are often represented by inefficient multi-stage learning algorithms. We\npropose the Adaptive Estimation of Instance-Dependent In-Distribution and\nOut-of-Distribution Label Noise (AEON) approach to address these research gaps.\nAEON is an efficient one-stage noisy-label learning methodology that\ndynamically estimates instance-dependent ID and OOD label noise rates to\nenhance robustness to complex noise settings. Additionally, we introduce a new\nbenchmark reflecting real-world ID and OOD noise scenarios. Experiments\ndemonstrate that AEON achieves state-of-the-art performance on both synthetic\nand real-world datasets"
    },
    {
        "date": "2025-01",
        "title": "False Sense of Security on Protected Wi-Fi Networks",
        "author": "Yong Zhi Lim, Hazmei Bin Abdul Rahman, and Biplab Sikdar",
        "link": "http://arxiv.org/abs/2501.13363v1",
        "abstract": "The Wi-Fi technology (IEEE 802.11) was introduced in 1997. With the\nincreasing use and deployment of such networks, their security has also\nattracted considerable attention. Current Wi-Fi networks use WPA2 (Wi-Fi\nProtected Access 2) for security (authentication and encryption) between access\npoints and clients. According to the IEEE 802.11i-2004 standard, wireless\nnetworks secured with WPA2-PSK (Pre-Shared Key) are required to be protected\nwith a passphrase between 8 to 63 ASCII characters. However, a poorly chosen\npassphrase significantly reduces the effectiveness of both WPA2 and\nWPA3-Personal Transition Mode. The objective of this paper is to empirically\nevaluate password choices in the wild and evaluate weakness in current common\npractices. We collected a total of 3,352 password hashes from Wi-Fi access\npoints and determine the passphrases that were protecting them. We then analyze\nthese passwords to investigate the impact of user's behavior and preference for\nconvenience on passphrase strength in secured private Wi-Fi networks in\nSingapore. We characterized the predictability of passphrases that use the\nminimum required length of 8 numeric or alphanumeric characters, and/or symbols\nstipulated in wireless security standards, and the usage of default passwords,\nand found that 16 percent of the passwords show such behavior. Our results also\nindicate the prevalence of the use of default passwords by hardware\nmanufacturers. We correlate our results with our findings and recommend methods\nthat will improve the overall security and future of our Wi-Fi networks."
    },
    {
        "date": "2025-01",
        "title": "50 Shades of Deceptive Patterns: A Unified Taxonomy, Multimodal Detection, and Security Implications",
        "author": "Zewei Shi, Ruoxi Sun, Jieshan Chen, Jiamou Sun, Minhui Xue, Yansong Gao, Feng Liu, and Xingliang Yuan",
        "link": "http://arxiv.org/abs/2501.13351v1",
        "abstract": "Deceptive patterns (DPs) are user interface designs deliberately crafted to\nmanipulate users into unintended decisions, often by exploiting cognitive\nbiases for the benefit of companies or services. While numerous studies have\nexplored ways to identify these deceptive patterns, many existing solutions\nrequire significant human intervention and struggle to keep pace with the\nevolving nature of deceptive designs. To address these challenges, we expanded\nthe deceptive pattern taxonomy from security and privacy perspectives, refining\nits categories and scope. We created a comprehensive dataset of deceptive\npatterns by integrating existing small-scale datasets with new samples,\nresulting in 6,725 images and 10,421 DP instances from mobile apps and\nwebsites. We then developed DPGuard, a novel automatic tool leveraging\ncommercial multimodal large language models (MLLMs) for deceptive pattern\ndetection. Experimental results show that DPGuard outperforms state-of-the-art\nmethods. Finally, we conducted an extensive empirical evaluation on 2,000\npopular mobile apps and websites, revealing that 23.61% of mobile screenshots\nand 47.27% of website screenshots feature at least one deceptive pattern\ninstance. Through four unexplored case studies that inform security\nimplications, we highlight the critical importance of the unified taxonomy in\naddressing the growing challenges of Internet deception."
    },
    {
        "date": "2025-01",
        "title": "Retrievals Can Be Detrimental: A Contrastive Backdoor Attack Paradigm on Retrieval-Augmented Diffusion Models",
        "author": "Hao Fang, Xiaohang Sui, Hongyao Yu, Jiawei Kong, Sijin Yu, Bin Chen, Hao Wu, and Shu-Tao Xia",
        "link": "http://arxiv.org/abs/2501.13340v1",
        "abstract": "Diffusion models (DMs) have recently demonstrated remarkable generation\ncapability. However, their training generally requires huge computational\nresources and large-scale datasets. To solve these, recent studies empower DMs\nwith the advanced Retrieval-Augmented Generation (RAG) technique and propose\nretrieval-augmented diffusion models (RDMs). By incorporating rich knowledge\nfrom an auxiliary database, RAG enhances diffusion models' generation and\ngeneralization ability while significantly reducing model parameters. Despite\nthe great success, RAG may introduce novel security issues that warrant further\ninvestigation. In this paper, we reveal that the RDM is susceptible to backdoor\nattacks by proposing a multimodal contrastive attack approach named BadRDM. Our\nframework fully considers RAG's characteristics and is devised to manipulate\nthe retrieved items for given text triggers, thereby further controlling the\ngenerated contents. Specifically, we first insert a tiny portion of images into\nthe retrieval database as target toxicity surrogates. Subsequently, a malicious\nvariant of contrastive learning is adopted to inject backdoors into the\nretriever, which builds shortcuts from triggers to the toxicity surrogates.\nFurthermore, we enhance the attacks through novel entropy-based selection and\ngenerative augmentation strategies that can derive better toxicity surrogates.\nExtensive experiments on two mainstream tasks demonstrate the proposed BadRDM\nachieves outstanding attack effects while preserving the model's benign\nutility."
    },
    {
        "date": "2025-01",
        "title": "Gradient-Free Adversarial Purification with Diffusion Models",
        "author": "Xuelong Dai, Dong Wang, Duan Mingxing, and Bin Xiao",
        "link": "http://arxiv.org/abs/2501.13336v1",
        "abstract": "Adversarial training and adversarial purification are two effective and\npractical defense methods to enhance a model's robustness against adversarial\nattacks. However, adversarial training necessitates additional training, while\nadversarial purification suffers from low time efficiency. More critically,\ncurrent defenses are designed under the perturbation-based adversarial threat\nmodel, which is ineffective against the recently proposed unrestricted\nadversarial attacks. In this paper, we propose an effective and efficient\nadversarial defense method that counters both perturbation-based and\nunrestricted adversarial attacks. Our defense is inspired by the observation\nthat adversarial attacks are typically located near the decision boundary and\nare sensitive to pixel changes. To address this, we introduce adversarial\nanti-aliasing to mitigate adversarial modifications. Additionally, we propose\nadversarial super-resolution, which leverages prior knowledge from clean\ndatasets to benignly recover images. These approaches do not require additional\ntraining and are computationally efficient without calculating gradients.\nExtensive experiments against both perturbation-based and unrestricted\nadversarial attacks demonstrate that our defense method outperforms\nstate-of-the-art adversarial purification methods."
    },
    {
        "date": "2025-01",
        "title": "Watching the AI Watchdogs: A Fairness and Robustness Analysis of AI Safety Moderation Classifiers",
        "author": "Akshit Achara, and Anshuman Chhabra",
        "link": "http://arxiv.org/abs/2501.13302v1",
        "abstract": "AI Safety Moderation (ASM) classifiers are designed to moderate content on\nsocial media platforms and to serve as guardrails that prevent Large Language\nModels (LLMs) from being fine-tuned on unsafe inputs. Owing to their potential\nfor disparate impact, it is crucial to ensure that these classifiers: (1) do\nnot unfairly classify content belonging to users from minority groups as unsafe\ncompared to those from majority groups and (2) that their behavior remains\nrobust and consistent across similar inputs. In this work, we thus examine the\nfairness and robustness of four widely-used, closed-source ASM classifiers:\nOpenAI Moderation API, Perspective API, Google Cloud Natural Language (GCNL)\nAPI, and Clarifai API. We assess fairness using metrics such as demographic\nparity and conditional statistical parity, comparing their performance against\nASM models and a fair-only baseline. Additionally, we analyze robustness by\ntesting the classifiers' sensitivity to small and natural input perturbations.\nOur findings reveal potential fairness and robustness gaps, highlighting the\nneed to mitigate these issues in future versions of these models."
    },
    {
        "date": "2025-01",
        "title": "Enhancing Robust Fairness via Confusional Spectral Regularization",
        "author": "Gaojie Jin, Sihao Wu, Jiaxu Liu, Tianjin Huang, and Ronghui Mu",
        "link": "http://arxiv.org/abs/2501.13273v1",
        "abstract": "Recent research has highlighted a critical issue known as ``robust fairness\",\nwhere robust accuracy varies significantly across different classes,\nundermining the reliability of deep neural networks (DNNs). A common approach\nto address this has been to dynamically reweight classes during training,\ngiving more weight to those with lower empirical robust performance. However,\nwe find there is a divergence of class-wise robust performance between training\nset and testing set, which limits the effectiveness of these explicit\nreweighting methods, indicating the need for a principled alternative. In this\nwork, we derive a robust generalization bound for the worst-class robust error\nwithin the PAC-Bayesian framework, accounting for unknown data distributions.\nOur analysis shows that the worst-class robust error is influenced by two main\nfactors: the spectral norm of the empirical robust confusion matrix and the\ninformation embedded in the model and training set. While the latter has been\nextensively studied, we propose a novel regularization technique targeting the\nspectral norm of the robust confusion matrix to improve worst-class robust\naccuracy and enhance robust fairness. We validate our approach through\ncomprehensive experiments on various datasets and models, demonstrating its\neffectiveness in enhancing robust fairness."
    },
    {
        "date": "2025-01",
        "title": "Threat-based Security Controls to Protect Industrial Control Systems",
        "author": "Maryam Karimi, and Haritha Srinivasan",
        "link": "http://arxiv.org/abs/2501.13268v1",
        "abstract": "This paper analyzes the reported threats to Industrial Control Systems\n(ICS)/Operational Technology (OT) and identifies common tactics, techniques,\nand procedures (TTP) used by threat actors. The paper then uses the MITRE\nATT&CK framework to map the common TTPs and provide an understanding of the\nsecurity controls needed to defend against the reported ICS threats. The paper\nalso includes a review of ICS testbeds and ideas for future research using the\nidentified controls."
    },
    {
        "date": "2025-01",
        "title": "Active RIS-Assisted URLLC NOMA-Based 5G Network with FBL under Jamming Attacks",
        "author": "Ghazal Asemian, Mohammadreza Amini, and Burak Kantarci",
        "link": "http://arxiv.org/abs/2501.13231v1",
        "abstract": "In this paper, we tackle the challenge of jamming attacks in Ultra-Reliable\nLow Latency Communication (URLLC) within Non-Orthogonal Multiple Access\n(NOMA)-based 5G networks under Finite Blocklength (FBL) conditions. We\nintroduce an innovative approach that employs Reconfigurable Intelligent\nSurfaces (RIS) with active elements to enhance energy efficiency while ensuring\nreliability and meeting latency requirements. Our approach incorporates the\ntraffic model, making it practical for real-world scenarios with dynamic\ntraffic loads. We thoroughly analyze the impact of blocklength and packet\narrival rate on network performance metrics and investigate the optimal\namplitude value and number of RIS elements. Our results indicate that\nincreasing the number of RIS elements from 4 to 400 can improve\nsignal-to-jamming-plus-noise ratio (SJNR) by 13.64\\%. Additionally, optimizing\nblocklength and packet arrival rate can achieve a 31.68% improvement in energy\nefficiency and reduced latency. These findings underscore the importance of\noptimized settings for effective jamming mitigation."
    },
    {
        "date": "2025-01",
        "title": "Joint Task Offloading and User Scheduling in 5G MEC under Jamming Attacks",
        "author": "Mohammadreza Amini, Burak Kantarci, Claude D'Amours, and Melike Erol-Kantarci",
        "link": "http://arxiv.org/abs/2501.13227v1",
        "abstract": "In this paper, we propose a novel joint task offloading and user scheduling\n(JTO-US) framework for 5G mobile edge computing (MEC) systems under security\nthreats from jamming attacks. The goal is to minimize the delay and the ratio\nof dropped tasks, taking into account both communication and computation\ndelays. The system model includes a 5G network equipped with MEC servers and an\nadversarial on-off jammer that disrupts communication. The proposed framework\noptimally schedules tasks and users to minimize the impact of jamming while\nensuring that high-priority tasks are processed efficiently. Genetic algorithm\n(GA) is used to solve the optimization problem, and the results are compared\nwith benchmark methods such as GA without considering jamming effect, Shortest\nJob First (SJF), and Shortest Deadline First (SDF). The simulation results\ndemonstrate that the proposed JTO-US framework achieves the lowest drop ratio\nand effectively manages priority tasks, outperforming existing methods.\nParticularly, when the jamming probability is 0.8, the proposed framework\nmitigates the jammer's impact by reducing the drop ratio to 63%, compared to\n89% achieved by the next best method."
    },
    {
        "date": "2025-01",
        "title": "Robust Representation Consistency Model via Contrastive Denoising",
        "author": "Jiachen Lei, Julius Berner, Jiongxiao Wang, Zhongzhu Chen, Zhongjia Ba, Kui Ren, Jun Zhu, and Anima Anandkumar",
        "link": "http://arxiv.org/abs/2501.13094v1",
        "abstract": "Robustness is essential for deep neural networks, especially in\nsecurity-sensitive applications. To this end, randomized smoothing provides\ntheoretical guarantees for certifying robustness against adversarial\nperturbations. Recently, diffusion models have been successfully employed for\nrandomized smoothing to purify noise-perturbed samples before making\npredictions with a standard classifier. While these methods excel at small\nperturbation radii, they struggle with larger perturbations and incur a\nsignificant computational overhead during inference compared to classical\nmethods. To address this, we reformulate the generative modeling task along the\ndiffusion trajectories in pixel space as a discriminative task in the latent\nspace. Specifically, we use instance discrimination to achieve consistent\nrepresentations along the trajectories by aligning temporally adjacent points.\nAfter fine-tuning based on the learned representations, our model enables\nimplicit denoising-then-classification via a single prediction, substantially\nreducing inference costs. We conduct extensive experiments on various datasets\nand achieve state-of-the-art performance with minimal computation budget during\ninference. For example, our method outperforms the certified accuracy of\ndiffusion-based methods on ImageNet across all perturbation radii by 5.3% on\naverage, with up to 11.6% at larger radii, while reducing inference costs by\n85$\\times$ on average. Codes are available at:\nhttps://github.com/jiachenlei/rRCM."
    },
    {
        "date": "2025-01",
        "title": "CHaRNet: Conditioned Heatmap Regression for Robust Dental Landmark Localization",
        "author": "Jos\u00e9 Rodr\u00edguez-Ortega, and Siham Tabik",
        "link": "http://arxiv.org/abs/2501.13073v2",
        "abstract": "Identifying anatomical landmarks in 3D dental models is crucial for\northodontic treatment. Manually placing these key points is complex,\ntime-consuming, and requires expert knowledge. While some machine learning\nmethods have been proposed for automatic tooth landmark detection in 3D\nIntraoral Scans (IOS), research remains limited, with no fully end-to-end\napproaches that avoid teeth segmentation. We propose CHaRNet (Conditioned\nHeatmap Regression Network), the first end-to-end deep learning method for\ntooth landmark detection in 3D IOS. Unlike traditional two-stage methods that\nsegment teeth before detecting landmarks, CHaRNet directly detects landmarks on\nthe input point cloud. It consists of four key modules: (1) a point cloud\nencoder, (2) a point cloud decoder with a heatmap regression head, (3) a teeth\npresence classification head, and (4) the innovative Conditioned Heatmap\nRegression (CHaR) module. The CHaR module refines landmark regression by\nleveraging teeth presence classification, enabling dynamic adaptation to cases\nwith missing teeth and improving accuracy in complex dental models. We evaluate\nCHaRNet using five point cloud learning algorithms to validate the\neffectiveness of the CHaR module and test it on a clinical dataset of 1,214\nannotated 3D dental models. Both the dataset and code will be publicly released\nto address the lack of open datasets in orthodontics, promote benchmarking, and\ninspire new research. CHaRNet achieves a Mean Euclidean Distance Error (MEDE)\nof 1.28 mm and a Mean Success Ratio (MSR) of 82.40%, demonstrating robust\nperformance. Notably, it excels in handling irregular dental geometries, such\nas models with missing teeth. This end-to-end approach streamlines orthodontic\nworkflows, improves 3D IOS analysis precision, and facilitates efficient\ncomputer-assisted treatment planning."
    },
    {
        "date": "2025-01",
        "title": "Robust Body Composition Analysis by Generating 3D CT Volumes from Limited 2D Slices",
        "author": "Lianrui Zuo, Xin Yu, Dingjie Su, Kaiwen Xu, Aravind R. Krishnan, Yihao Liu, Shunxing Bao, Fabien Maldonado, Luigi Ferrucci, and Bennett A. Landman",
        "link": "http://arxiv.org/abs/2501.13071v1",
        "abstract": "Body composition analysis provides valuable insights into aging, disease\nprogression, and overall health conditions. Due to concerns of radiation\nexposure, two-dimensional (2D) single-slice computed tomography (CT) imaging\nhas been used repeatedly for body composition analysis. However, this approach\nintroduces significant spatial variability that can impact the accuracy and\nrobustness of the analysis. To mitigate this issue and facilitate body\ncomposition analysis, this paper presents a novel method to generate 3D CT\nvolumes from limited number of 2D slices using a latent diffusion model (LDM).\nOur approach first maps 2D slices into a latent representation space using a\nvariational autoencoder. An LDM is then trained to capture the 3D context of a\nstack of these latent representations. To accurately interpolate\nintermediateslices and construct a full 3D volume, we utilize body part\nregression to determine the spatial location and distance between the acquired\nslices. Experiments on both in-house and public 3D abdominal CT datasets\ndemonstrate that the proposed method significantly enhances body composition\nanalysis compared to traditional 2D-based analysis, with a reduced error rate\nfrom 23.3% to 15.2%."
    },
    {
        "date": "2025-01",
        "title": "Generative AI Misuse Potential in Cyber Security Education: A Case Study of a UK Degree Program",
        "author": "Carlton Shepherd",
        "link": "http://arxiv.org/abs/2501.12883v3",
        "abstract": "Recent advances in generative artificial intelligence (AI), such as ChatGPT,\nGoogle Gemini, and other large language models (LLMs), pose significant\nchallenges to upholding academic integrity in higher education. This paper\ninvestigates the susceptibility of a Master's-level cyber security degree\nprogram at a UK Russell Group university, accredited by a leading national\nbody, to LLM misuse. Through the application and extension of a quantitative\nassessment framework, we identify a high exposure to misuse, particularly in\nindependent project- and report-based assessments. Contributing factors,\nincluding block teaching and a predominantly international cohort, are\nhighlighted as potential amplifiers of these vulnerabilities. To address these\nchallenges, we discuss the adoption of LLM-resistant assessments, detection\ntools, and the importance of fostering an ethical learning environment. These\napproaches aim to uphold academic standards while preparing students for the\ncomplexities of real-world cyber security."
    },
    {
        "date": "2025-01",
        "title": "Intelligent Attacks on Cyber-Physical Systems and Critical Infrastructures",
        "author": "Alan Oliveira de S\u00e1, Charles Bezerra Prado, Mariana Luiza Flavio, and Luiz F. Rust da C. Carmo",
        "link": "http://arxiv.org/abs/2501.12762v1",
        "abstract": "This chapter provides an overview of the evolving landscape of attacks in\ncyber-physical systems (CPS) and critical infrastructures, highlighting the\npossible use of Artificial Intelligence (AI) algorithms to develop intelligent\ncyberattacks. It describes various existing methods used to carry out\nintelligent attacks in Operational Technology (OT) environments and discusses\nAI-driven tools that automate penetration tests in Information Technology (IT)\nsystems, which could potentially be used as attack tools. The chapter also\ndiscusses mitigation strategies to counter these emerging intelligent attacks\nby hindering the learning process of AI-based attacks and points to future\nresearch directions on the matter."
    },
    {
        "date": "2025-01",
        "title": "Modality Unified Attack for Omni-Modality Person Re-Identification",
        "author": "Yuan Bian, Min Liu, Yunqi Yi, Xueping Wang, Yunfeng Ma, and Yaonan Wang",
        "link": "http://arxiv.org/abs/2501.12761v1",
        "abstract": "Deep learning based person re-identification (re-id) models have been widely\nemployed in surveillance systems. Recent studies have demonstrated that\nblack-box single-modality and cross-modality re-id models are vulnerable to\nadversarial examples (AEs), leaving the robustness of multi-modality re-id\nmodels unexplored. Due to the lack of knowledge about the specific type of\nmodel deployed in the target black-box surveillance system, we aim to generate\nmodality unified AEs for omni-modality (single-, cross- and multi-modality)\nre-id models. Specifically, we propose a novel Modality Unified Attack method\nto train modality-specific adversarial generators to generate AEs that\neffectively attack different omni-modality models. A multi-modality model is\nadopted as the surrogate model, wherein the features of each modality are\nperturbed by metric disruption loss before fusion. To collapse the common\nfeatures of omni-modality models, Cross Modality Simulated Disruption approach\nis introduced to mimic the cross-modality feature embeddings by intentionally\nfeeding images to non-corresponding modality-specific subnetworks of the\nsurrogate model. Moreover, Multi Modality Collaborative Disruption strategy is\ndevised to facilitate the attacker to comprehensively corrupt the informative\ncontent of person images by leveraging a multi modality feature collaborative\nmetric disruption loss. Extensive experiments show that our MUA method can\neffectively attack the omni-modality re-id models, achieving 55.9%, 24.4%,\n49.0% and 62.7% mean mAP Drop Rate, respectively."
    },
    {
        "date": "2025-01",
        "title": "Bad-PFL: Exploring Backdoor Attacks against Personalized Federated Learning",
        "author": "Mingyuan Fan, Zhanyi Hu, Fuyi Wang, and Cen Chen",
        "link": "http://arxiv.org/abs/2501.12736v1",
        "abstract": "Data heterogeneity and backdoor attacks rank among the most significant\nchallenges facing federated learning (FL). For data heterogeneity, personalized\nfederated learning (PFL) enables each client to maintain a private personalized\nmodel to cater to client-specific knowledge. Meanwhile, vanilla FL has proven\nvulnerable to backdoor attacks. However, recent advancements in PFL community\nhave demonstrated a potential immunity against such attacks. This paper\nexplores this intersection further, revealing that existing federated backdoor\nattacks fail in PFL because backdoors about manually designed triggers struggle\nto survive in personalized models. To tackle this, we design Bad-PFL, which\nemploys features from natural data as our trigger. As long as the model is\ntrained on natural data, it inevitably embeds the backdoor associated with our\ntrigger, ensuring its longevity in personalized models. Moreover, our trigger\nundergoes mutual reinforcement training with the model, further solidifying the\nbackdoor's durability and enhancing attack effectiveness. The large-scale\nexperiments across three benchmark datasets demonstrate the superior\nperformance of our attack against various PFL methods, even when equipped with\nstate-of-the-art defense mechanisms."
    },
    {
        "date": "2025-01",
        "title": "FedDAG: Federated Domain Adversarial Generation Towards Generalizable Medical Image Analysis",
        "author": "Haoxuan Che, Yifei Wu, Haibo Jin, Yong Xia, and Hao Chen",
        "link": "http://arxiv.org/abs/2501.13967v1",
        "abstract": "Federated domain generalization aims to train a global model from multiple\nsource domains and ensure its generalization ability to unseen target domains.\n{Due to the target domain being with unknown domain shifts, attempting to\napproximate these gaps by source domains may be the key to improving model\ngeneralization capability.} Existing works mainly focus on sharing and\nrecombining local domain-specific attributes to increase data diversity and\nsimulate potential domain shifts. {However, these methods may be insufficient\nsince only the local attribute recombination can be hard to touch the\nout-of-distribution of global data.} In this paper, we propose a\nsimple-yet-efficient framework named Federated Domain Adversarial Generation\n(FedDAG). {It aims to simulate the domain shift and improve the model\ngeneralization by adversarially generating novel domains different from local\nand global source domains.} Specifically, it generates novel-style images by\nmaximizing the instance-level feature discrepancy between original and\ngenerated images and trains a generalizable task model by minimizing their\nfeature discrepancy. {Further, we observed that FedDAG could cause different\nperformance improvements for local models. It may be due to inherent data\nisolation and heterogeneity among clients, exacerbating the imbalance in their\ngeneralization contributions to the global model.} {Ignoring this imbalance can\nlead the global model's generalization ability to be sub-optimal, further\nlimiting the novel domain generation procedure. } Thus, to mitigate this\nimbalance, FedDAG hierarchically aggregates local models at the within-client\nand across-client levels by using the sharpness concept to evaluate client\nmodel generalization contributions. {Extensive experiments across four medical\nbenchmarks demonstrate FedDAG's ability to enhance generalization in federated\nmedical scenarios.}"
    },
    {
        "date": "2025-01",
        "title": "Towards Robust Multi-tab Website Fingerprinting",
        "author": "Xinhao Deng, Xiyuan Zhao, Qilei Yin, Zhuotao Liu, Qi Li, Mingwei Xu, Ke Xu, and Jianping Wu",
        "link": "http://arxiv.org/abs/2501.12622v1",
        "abstract": "Website fingerprinting enables an eavesdropper to determine which websites a\nuser is visiting over an encrypted connection. State-of-the-art website\nfingerprinting (WF) attacks have demonstrated effectiveness even against\nTor-protected network traffic. However, existing WF attacks have critical\nlimitations on accurately identifying websites in multi-tab browsing sessions,\nwhere the holistic pattern of individual websites is no longer preserved, and\nthe number of tabs opened by a client is unknown a priori. In this paper, we\npropose ARES, a novel WF framework natively designed for multi-tab WF attacks.\nARES formulates the multi-tab attack as a multi-label classification problem\nand solves it using the novel Transformer-based models. Specifically, ARES\nextracts local patterns based on multi-level traffic aggregation features and\nutilizes the improved self-attention mechanism to analyze the correlations\nbetween these local patterns, effectively identifying websites. We implement a\nprototype of ARES and extensively evaluate its effectiveness using our\nlarge-scale datasets collected over multiple months. The experimental results\nillustrate that ARES achieves optimal performance in several realistic\nscenarios. Further, ARES remains robust even against various WF defenses."
    },
    {
        "date": "2025-01",
        "title": "Robustness of Selected Learning Models under Label-Flipping Attack",
        "author": "Sarvagya Bhargava, and Mark Stamp",
        "link": "http://arxiv.org/abs/2501.12516v1",
        "abstract": "In this paper we compare traditional machine learning and deep learning\nmodels trained on a malware dataset when subjected to adversarial attack based\non label-flipping. Specifically, we investigate the robustness of Support\nVector Machines (SVM), Random Forest, Gaussian Naive Bayes (GNB), Gradient\nBoosting Machine (GBM), LightGBM, XGBoost, Multilayer Perceptron (MLP),\nConvolutional Neural Network (CNN), MobileNet, and DenseNet models when facing\nvarying percentages of misleading labels. We empirically assess the the\naccuracy of each of these models under such an adversarial attack on the\ntraining data. This research aims to provide insights into which models are\ninherently more robust, in the sense of being better able to resist intentional\ndisruptions to the training data. We find wide variation in the robustness of\nthe models tested to adversarial attack, with our MLP model achieving the best\ncombination of initial accuracy and robustness."
    },
    {
        "date": "2025-01",
        "title": "Adaptive Cyber-Attack Detection in IIoT Using Attention-Based LSTM-CNN Models",
        "author": "Afrah Gueriani, Hamza Kheddar, and Ahmed Cherif Mazari",
        "link": "http://arxiv.org/abs/2501.13962v1",
        "abstract": "The rapid expansion of the industrial Internet of things (IIoT) has\nintroduced new challenges in securing critical infrastructures against\nsophisticated cyberthreats. This study presents the development and evaluation\nof an advanced Intrusion detection (IDS) based on a hybrid LSTM-convolution\nneural network (CNN)-Attention architecture, specifically designed to detect\nand classify cyberattacks in IIoT environments. The research focuses on two key\nclassification tasks: binary and multi-class classification. The proposed\nmodels was rigorously tested using the Edge-IIoTset dataset. To mitigate the\nclass imbalance in the dataset, the synthetic minority over-sampling technique\n(SMOTE) was employed to generate synthetic samples for the underrepresented\nclasses. This ensured that the model could learn effectively from all classes,\nthereby improving the overall classification performance. Through systematic\nexperimentation, various deep learning (DL) models were compared, ultimately\ndemonstrating that the LSTM-CNN-Attention model consistently outperformed\nothers across key performance metrics. In binary classification, the model\nachieved near-perfect accuracy, while in multi-class classification, it\nmaintained a high accuracy level (99.04%), effectively categorizing different\nattack types with a loss value of 0.0220%."
    },
    {
        "date": "2025-01",
        "title": "A Fast, Scalable, and Robust Deep Learning-based Iterative Reconstruction Framework for Accelerated Industrial Cone-beam X-ray Computed Tomography",
        "author": "Aniket Pramanik, Obaidullah Rahman, Singanallur V. Venkatakrishnan, and Amirkoushyar Ziabari",
        "link": "http://arxiv.org/abs/2501.13961v1",
        "abstract": "Cone-beam X-ray Computed Tomography (XCT) with large detectors and\ncorresponding large-scale 3D reconstruction plays a pivotal role in\nmicron-scale characterization of materials and parts across various industries.\nIn this work, we present a novel deep neural network-based iterative algorithm\nthat integrates an artifact reduction-trained CNN as a prior model with\nautomated regularization parameter selection, tailored for large-scale\nindustrial cone-beam XCT data. Our method achieves high-quality 3D\nreconstructions even for extremely dense thick metal parts - which\ntraditionally pose challenges to industrial CT images - in just a few\niterations. Furthermore, we show the generalizability of our approach to\nout-of-distribution scans obtained under diverse scanning conditions. Our\nmethod effectively handles significant noise and streak artifacts, surpassing\nstate-of-the-art supervised learning methods trained on the same data."
    },
    {
        "date": "2025-01",
        "title": "Cinepro: Robust Training of Foundation Models for Cancer Detection in Prostate Ultrasound Cineloops",
        "author": "Mohamed Harmanani, Amoon Jamzad, Minh Nguyen Nhat To, Paul F. R. Wilson, Zhuoxin Guo, Fahimeh Fooladgar, Samira Sojoudi, Mahdi Gilany, Silvia Chang, Peter Black, Michael Leveridge, Robert Siemens, Purang Abolmaesumi, and Parvin Mousavi",
        "link": "http://arxiv.org/abs/2501.12331v1",
        "abstract": "Prostate cancer (PCa) detection using deep learning (DL) models has shown\npotential for enhancing real-time guidance during biopsies. However, prostate\nultrasound images lack pixel-level cancer annotations, introducing label noise.\nCurrent approaches often focus on limited regions of interest (ROIs),\ndisregarding anatomical context necessary for accurate diagnosis. Foundation\nmodels can overcome this limitation by analyzing entire images to capture\nglobal spatial relationships; however, they still encounter challenges stemming\nfrom the weak labels associated with coarse pathology annotations in ultrasound\ndata. We introduce Cinepro, a novel framework that strengthens foundation\nmodels' ability to localize PCa in ultrasound cineloops. Cinepro adapts robust\ntraining by integrating the proportion of cancer tissue reported by pathology\nin a biopsy core into its loss function to address label noise, providing a\nmore nuanced supervision. Additionally, it leverages temporal data across\nmultiple frames to apply robust augmentations, enhancing the model's ability to\nlearn stable cancer-related features. Cinepro demonstrates superior performance\non a multi-center prostate ultrasound dataset, achieving an AUROC of 77.1% and\na balanced accuracy of 83.8%, surpassing current benchmarks. These findings\nunderscore Cinepro's promise in advancing foundation models for weakly labeled\nultrasound data."
    },
    {
        "date": "2025-01",
        "title": "With Great Backbones Comes Great Adversarial Transferability",
        "author": "Erik Arakelyan, Karen Hambardzumyan, Davit Papikyan, Pasquale Minervini, Albert Gordo, Isabelle Augenstein, and Aram H. Markosyan",
        "link": "http://arxiv.org/abs/2501.12275v1",
        "abstract": "Advances in self-supervised learning (SSL) for machine vision have improved\nrepresentation robustness and model performance, giving rise to pre-trained\nbackbones like \\emph{ResNet} and \\emph{ViT} models tuned with SSL methods such\nas \\emph{SimCLR}. Due to the computational and data demands of pre-training,\nthe utilization of such backbones becomes a strenuous necessity. However,\nemploying these backbones may inherit vulnerabilities to adversarial attacks.\nWhile adversarial robustness has been studied under \\emph{white-box} and\n\\emph{black-box} settings, the robustness of models tuned on pre-trained\nbackbones remains largely unexplored. Additionally, the role of tuning\nmeta-information in mitigating exploitation risks is unclear. This work\nsystematically evaluates the adversarial robustness of such models across\n$20,000$ combinations of tuning meta-information, including fine-tuning\ntechniques, backbone families, datasets, and attack types. We propose using\nproxy models to transfer attacks, simulating varying levels of target knowledge\nby fine-tuning these proxies with diverse configurations. Our findings reveal\nthat proxy-based attacks approach the effectiveness of \\emph{white-box}\nmethods, even with minimal tuning knowledge. We also introduce a naive\n\"backbone attack,\" leveraging only the backbone to generate adversarial\nsamples, which outperforms \\emph{black-box} attacks and rivals \\emph{white-box}\nmethods, highlighting critical risks in model-sharing practices. Finally, our\nablations reveal how increasing tuning meta-information impacts attack\ntransferability, measuring each meta-information combination."
    },
    {
        "date": "2025-01",
        "title": "mmCooper: A Multi-agent Multi-stage Communication-efficient and Collaboration-robust Cooperative Perception Framework",
        "author": "Bingyi Liu, Jian Teng, Hongfei Xue, Enshu Wang, Chuanhui Zhu, Pu Wang, and Libing Wu",
        "link": "http://arxiv.org/abs/2501.12263v1",
        "abstract": "Collaborative perception significantly enhances individual vehicle perception\nperformance through the exchange of sensory information among agents. However,\nreal-world deployment faces challenges due to bandwidth constraints and\ninevitable calibration errors during information exchange. To address these\nissues, we propose mmCooper, a novel multi-agent, multi-stage,\ncommunication-efficient, and collaboration-robust cooperative perception\nframework. Our framework leverages a multi-stage collaboration strategy that\ndynamically and adaptively balances intermediate- and late-stage information to\nshare among agents, enhancing perceptual performance while maintaining\ncommunication efficiency. To support robust collaboration despite potential\nmisalignments and calibration errors, our framework captures multi-scale\ncontextual information for robust fusion in the intermediate stage and\ncalibrates the received detection results to improve accuracy in the late\nstage. We validate the effectiveness of mmCooper through extensive experiments\non real-world and simulated datasets. The results demonstrate the superiority\nof our proposed framework and the effectiveness of each component."
    },
    {
        "date": "2025-01",
        "title": "Empower Healthcare through a Self-Sovereign Identity Infrastructure for Secure Electronic Health Data Access",
        "author": "Antonio L\u00f3pez Mart\u00ednez, Montassar Naghmouchi, Maryline Laurent, Joaquin Garcia-Alfaro, Manuel Gil P\u00e9rez, Antonio Ruiz Mart\u00ednez, and Pantaleone Nespoli",
        "link": "http://arxiv.org/abs/2501.12229v1",
        "abstract": "Health data is one of the most sensitive data for people, which attracts the\nattention of malicious activities. We propose an open-source health data\nmanagement framework, that follows a patient-centric approach. The proposed\nframework implements the Self-Sovereign Identity paradigm with innovative\ntechnologies such as Decentralized Identifiers and Verifiable Credentials. The\nframework uses Blockchain technology to provide immutability, verifiable data\nregistry, and auditability, as well as an agent-based model to provide\nprotection and privacy for the patient data. We also define different use cases\nregarding the daily patient-practitioner-laboratory interactions and specific\nfunctions to cover patient data loss, data access revocation, and emergency\ncases where patients are unable to give consent and access to their data. To\naddress this design, a proof of concept is created with an interaction between\npatient and doctor. The most feasible technologies are selected and the created\ndesign is validated. We discuss the differences and novelties of this\nframework, which includes the patient-centric approach also for data storage,\nthe designed recovery and emergency plan, the defined backup procedure, and the\nselected blockchain platform."
    },
    {
        "date": "2025-01",
        "title": "You Can't Eat Your Cake and Have It Too: The Performance Degradation of LLMs with Jailbreak Defense",
        "author": "Wuyuao Mai, Geng Hong, Pei Chen, Xudong Pan, Baojun Liu, Yuan Zhang, Haixin Duan, and Min Yang",
        "link": "http://arxiv.org/abs/2501.12210v1",
        "abstract": "With the rise of generative large language models (LLMs) like LLaMA and\nChatGPT, these models have significantly transformed daily life and work by\nproviding advanced insights. However, as jailbreak attacks continue to\ncircumvent built-in safety mechanisms, exploiting carefully crafted scenarios\nor tokens, the safety risks of LLMs have come into focus. While numerous\ndefense strategies--such as prompt detection, modification, and model\nfine-tuning--have been proposed to counter these attacks, a critical question\narises: do these defenses compromise the utility and usability of LLMs for\nlegitimate users? Existing research predominantly focuses on the effectiveness\nof defense strategies without thoroughly examining their impact on performance,\nleaving a gap in understanding the trade-offs between LLM safety and\nperformance. Our research addresses this gap by conducting a comprehensive\nstudy on the utility degradation, safety elevation, and exaggerated-safety\nescalation of LLMs with jailbreak defense strategies. We propose USEBench, a\nnovel benchmark designed to evaluate these aspects, along with USEIndex, a\ncomprehensive metric for assessing overall model performance. Through\nexperiments on seven state-of-the-art LLMs, we found that mainstream jailbreak\ndefenses fail to ensure both safety and performance simultaneously. Although\nmodel-finetuning performs the best overall, their effectiveness varies across\nLLMs. Furthermore, vertical comparisons reveal that developers commonly\nprioritize performance over safety when iterating or fine-tuning their LLMs."
    },
    {
        "date": "2025-01",
        "title": "FedCLEAN: byzantine defense by CLustering Errors of Activation maps in Non-IID federated learning environments",
        "author": "Mehdi Ben Ghali, Reda Bellafqira, and Gouenou Coatrieux",
        "link": "http://arxiv.org/abs/2501.12123v1",
        "abstract": "Federated Learning (FL) enables clients to collaboratively train a global\nmodel using their local datasets while reinforcing data privacy. However, FL is\nsusceptible to poisoning attacks. Existing defense mechanisms assume that\nclients' data are independent and identically distributed (IID), making them\nineffective in real-world applications where data are non-IID. This paper\npresents FedCLEAN, the first defense capable of filtering attackers' model\nupdates in a non-IID FL environment. The originality of FedCLEAN is twofold.\nFirst, it relies on a client confidence score derived from the reconstruction\nerrors of each client's model activation maps for a given trigger set, with\nreconstruction errors obtained by means of a Conditional Variational\nAutoencoder trained according to a novel server-side strategy. Second, we\npropose an ad-hoc trust propagation algorithm based on client scores, which\nallows building a cluster of benign clients while flagging potential attackers.\nExperimental results on the datasets MNIST and FashionMNIST demonstrate the\nrobustness of FedCLEAN against Byzantine attackers in non-IID scenarios and a\nclose-to-zero benign client misclassification rate, even in the absence of an\nattack."
    },
    {
        "date": "2025-01",
        "title": "Application of Machine Learning Techniques for Secure Traffic in NoC-based Manycores",
        "author": "Geaninne Lopes, C\u00e9sar Marcon, and Fernando Moraes",
        "link": "http://arxiv.org/abs/2501.12034v1",
        "abstract": "Like most computer systems, a manycore can also be the target of security\nattacks. It is essential to ensure the security of the NoC since all\ninformation travels through its channels, and any interference in the traffic\nof messages can reflect on the entire chip, causing communication problems.\nAmong the possible attacks on NoC, Denial of Service (DoS) attacks are the most\ncited in the literature. The state of the art shows a lack of work that can\ndetect such attacks through learning techniques. On the other hand, these\ntechniques are widely explored in computer network security via an Intrusion\nDetection System (IDS). In this context, the main goal of this document is to\npresent the progress of a work that explores an IDS technique using machine\nlearning and temporal series for detecting DoS attacks in NoC-based manycore\nsystems. To fulfill this goal, it is necessary to extract traffic data from a\nmanycore NoC and execute the learning techniques in the extracted data.\nHowever, while low-level platforms offer precision and slow execution,\nhigh-level platforms offer higher speed and data incompatible with reality.\nTherefore, a platform is being developed using the OVP tool, which has a higher\nlevel of abstraction. To solve the low precision problem, the developed\nplatform will have its data validated with a low-level platform."
    },
    {
        "date": "2025-01",
        "title": "Ratio Attack on G+G Convoluted Gaussian Signature",
        "author": "Chik How Tan, Theo Fanuela Prabowo, and Wei Guo Foo",
        "link": "http://arxiv.org/abs/2501.12009v1",
        "abstract": "A lattice-based signature, called G+G convoluted Gaussian signature was\nproposed in ASIACRYPT 2023 and was proved secure in the quantum random oracle\nmodel. In this paper, we propose a ratio attack on the G+G convoluted Gaussian\nsignature to recover the secret key. The attack exploits the fact, proved in\nthis paper, that the secret key can be obtained from the expected value of the\nratio of signatures which follows a truncated Cauchy distribution. Moreover, we\nalso compute the number of signatures required to successfully recover the\nsecret key. Furthermore, we simulate the ratio attack in Sagemath with a few\ndifferent parameters as a proof-of-concept of the ratio attack."
    },
    {
        "date": "2025-01",
        "title": "BRC20 Snipping Attack",
        "author": "Minfeng Qi, Qin Wang, Ningran Li, Shiping Chen, and Tianqing Zhu",
        "link": "http://arxiv.org/abs/2501.11942v1",
        "abstract": "In this paper, we introduce and implement BRC20 sniping attack. Our attack\nmanipulates the BRC20 token transfers in open markets and disrupts the fairness\namong bidding participants. The long-standing principle of ``highest bidder\nwins'' is rendered ineffective.\n  Typically, open BRC20 token markets rely on Partially Signed Bitcoin\nTransactions (PSBT) to broadcast selling intents and wait for buying auctions.\nOur attack targets the BRC20 buying process (i.e., transfer) by injecting a\nfront-running transaction to complete the full signature of the PSBT. At its\ncore, the attack exploits the mempool's fee-based transaction selection\nmechanism to snipe the victim transaction, replicate metadata, and front-run\nthe legesmate transaction. This attack applies to platforms using PSBT for\nBRC20 token transfers, including popular Bitcoin exchanges and marketplaces\n(e.g., Magic Eden, Unisat, Gate.io, OKX).\n  We implemented and tested the attack on a Bitcoin testnet (regtest),\nvalidating its effectiveness through multiple experimental rounds. Results show\nthat the attacker consistently replaces legitimate transactions by submitting\nhigher-fee PSBTs. We have also made responsible disclosures to the mentioned\nexchanges."
    },
    {
        "date": "2025-01",
        "title": "LuxVeri at GenAI Detection Task 1: Inverse Perplexity Weighted Ensemble for Robust Detection of AI-Generated Text across English and Multilingual Contexts",
        "author": "Md Kamrujjaman Mobin, and Md Saiful Islam",
        "link": "http://arxiv.org/abs/2501.11914v1",
        "abstract": "This paper presents a system developed for Task 1 of the COLING 2025 Workshop\non Detecting AI-Generated Content, focusing on the binary classification of\nmachine-generated versus human-written text. Our approach utilizes an ensemble\nof models, with weights assigned according to each model's inverse perplexity,\nto enhance classification accuracy. For the English text detection task, we\ncombined RoBERTa-base, RoBERTa-base with the OpenAI detector, and\nBERT-base-cased, achieving a Macro F1-score of 0.7458, which ranked us 12th out\nof 35 teams. We ensembled RemBERT, XLM-RoBERTa-base, and\nBERT-base-multilingual-case for the multilingual text detection task, employing\nthe same inverse perplexity weighting technique. This resulted in a Macro\nF1-score of 0.7513, positioning us 4th out of 25 teams. Our results demonstrate\nthe effectiveness of inverse perplexity weighting in improving the robustness\nof machine-generated text detection across both monolingual and multilingual\nsettings, highlighting the potential of ensemble methods for this challenging\ntask."
    },
    {
        "date": "2025-01",
        "title": "Enhancing Adversarial Transferability via Component-Wise Augmentation Method",
        "author": "Hangyu Liu, Bo Peng, Pengxiang Ding, and Donglin Wang",
        "link": "http://arxiv.org/abs/2501.11901v1",
        "abstract": "Deep Neural Networks (DNNs) are highly vulnerable to adversarial examples,\nwhich pose significant challenges in security-sensitive applications. Among\nvarious adversarial attack strategies, input transformation-based attacks have\ndemonstrated remarkable effectiveness in enhancing adversarial transferability.\nHowever, existing methods fail to diversify attention regions across models\nadequately and introduce excessive information loss during transformations. In\nthis paper, we introduce a novel input transformation-based method, termed\nComponent-Wise Augmentation (CWA), designed to enhance transferability by\nlocally applying block-wise transformations. CWA strategically integrates\ninterpolation and selective rotation on individual image blocks to diversify\nmodel attention regions while preserving semantic integrity. Extensive\nexperiments on the standard ImageNet dataset show that CWA consistently\noutperforms state-of-the-art methods in both attack success rates and stability\nacross CNN- and Transformer-based models, while also demonstrating superior\nperformance against multiple defense methods."
    },
    {
        "date": "2025-01",
        "title": "LASER: Lip Landmark Assisted Speaker Detection for Robustness",
        "author": "Le Thien Phuc Nguyen, Zhuoran Yu, and Yong Jae Lee",
        "link": "http://arxiv.org/abs/2501.11899v1",
        "abstract": "Active Speaker Detection (ASD) aims to identify speaking individuals in\ncomplex visual scenes. While humans can easily detect speech by matching lip\nmovements to audio, current ASD models struggle to establish this\ncorrespondence, often misclassifying non-speaking instances when audio and lip\nmovements are unsynchronized. To address this limitation, we propose Lip\nlandmark Assisted Speaker dEtection for Robustness (LASER). Unlike models that\nrely solely on facial frames, LASER explicitly focuses on lip movements by\nintegrating lip landmarks in training. Specifically, given a face track, LASER\nextracts frame-level visual features and the 2D coordinates of lip landmarks\nusing a lightweight detector. These coordinates are encoded into dense feature\nmaps, providing spatial and structural information on lip positions.\nRecognizing that landmark detectors may sometimes fail under challenging\nconditions (e.g., low resolution, occlusions, extreme angles), we incorporate\nan auxiliary consistency loss to align predictions from both lip-aware and\nface-only features, ensuring reliable performance even when lip data is absent.\nExtensive experiments across multiple datasets show that LASER outperforms\nstate-of-the-art models, especially in scenarios with desynchronized audio and\nvisuals, demonstrating robust performance in real-world video contexts. Code is\navailable at \\url{https://github.com/plnguyen2908/LASER_ASD}."
    },
    {
        "date": "2025-01",
        "title": "Cross-Entropy Attacks to Language Models via Rare Event Simulation",
        "author": "Mingze Ni, Yongshun Gong, and Wei Liu",
        "link": "http://arxiv.org/abs/2501.11852v1",
        "abstract": "Black-box textual adversarial attacks are challenging due to the lack of\nmodel information and the discrete, non-differentiable nature of text. Existing\nmethods often lack versatility for attacking different models, suffer from\nlimited attacking performance due to the inefficient optimization with word\nsaliency ranking, and frequently sacrifice semantic integrity to achieve better\nattack outcomes. This paper introduces a novel approach to textual adversarial\nattacks, which we call Cross-Entropy Attacks (CEA), that uses Cross-Entropy\noptimization to address the above issues. Our CEA approach defines adversarial\nobjectives for both soft-label and hard-label settings and employs CE\noptimization to identify optimal replacements. Through extensive experiments on\ndocument classification and language translation problems, we demonstrate that\nour attack method excels in terms of attacking performance, imperceptibility,\nand sentence quality."
    },
    {
        "date": "2025-01",
        "title": "FedMUA: Exploring the Vulnerabilities of Federated Learning to Malicious Unlearning Attacks",
        "author": "Jian Chen, Zehui Lin, Wanyu Lin, Wenlong Shi, Xiaoyan Yin, and Di Wang",
        "link": "http://arxiv.org/abs/2501.11848v1",
        "abstract": "Recently, the practical needs of ``the right to be forgotten'' in federated\nlearning gave birth to a paradigm known as federated unlearning, which enables\nthe server to forget personal data upon the client's removal request. Existing\nstudies on federated unlearning have primarily focused on efficiently\neliminating the influence of requested data from the client's model without\nretraining from scratch, however, they have rarely doubted the reliability of\nthe global model posed by the discrepancy between its prediction performance\nbefore and after unlearning. To bridge this gap, we take the first step by\nintroducing a novel malicious unlearning attack dubbed FedMUA, aiming to unveil\npotential vulnerabilities emerging from federated learning during the\nunlearning process. The crux of FedMUA is to mislead the global model into\nunlearning more information associated with the influential samples for the\ntarget sample than anticipated, thus inducing adverse effects on target samples\nfrom other clients. To achieve this, we design a novel two-step method, known\nas Influential Sample Identification and Malicious Unlearning Generation, to\nidentify and subsequently generate malicious feature unlearning requests within\nthe influential samples. By doing so, we can significantly alter the\npredictions pertaining to the target sample by initiating the malicious feature\nunlearning requests, leading to the deliberate manipulation for the user\nadversely. Additionally, we design a new defense mechanism that is highly\nresilient against malicious unlearning attacks. Extensive experiments on three\nrealistic datasets reveal that FedMUA effectively induces misclassification on\ntarget samples and can achieve an 80% attack success rate by triggering only\n0.3% malicious unlearning requests."
    },
    {
        "date": "2025-01",
        "title": "CogMorph: Cognitive Morphing Attacks for Text-to-Image Models",
        "author": "Zonglei Jing, Zonghao Ying, Le Wang, Siyuan Liang, Aishan Liu, Xianglong Liu, and Dacheng Tao",
        "link": "http://arxiv.org/abs/2501.11815v2",
        "abstract": "The development of text-to-image (T2I) generative models, that enable the\ncreation of high-quality synthetic images from textual prompts, has opened new\nfrontiers in creative design and content generation. However, this paper\nreveals a significant and previously unrecognized ethical risk inherent in this\ntechnology and introduces a novel method, termed the Cognitive Morphing Attack\n(CogMorph), which manipulates T2I models to generate images that retain the\noriginal core subjects but embeds toxic or harmful contextual elements. This\nnuanced manipulation exploits the cognitive principle that human perception of\nconcepts is shaped by the entire visual scene and its context, producing images\nthat amplify emotional harm far beyond attacks that merely preserve the\noriginal semantics. To address this, we first construct an imagery toxicity\ntaxonomy spanning 10 major and 48 sub-categories, aligned with human\ncognitive-perceptual dimensions, and further build a toxicity risk matrix\nresulting in 1,176 high-quality T2I toxic prompts. Based on this, our CogMorph\nfirst introduces Cognitive Toxicity Augmentation, which develops a cognitive\ntoxicity knowledge base with rich external toxic representations for humans\n(e.g., fine-grained visual features) that can be utilized to further guide the\noptimization of adversarial prompts. In addition, we present Contextual\nHierarchical Morphing, which hierarchically extracts critical parts of the\noriginal prompt (e.g., scenes, subjects, and body parts), and then iteratively\nretrieves and fuses toxic features to inject harmful contexts. Extensive\nexperiments on multiple open-sourced T2I models and black-box commercial APIs\n(e.g., DALLE-3) demonstrate the efficacy of CogMorph which significantly\noutperforms other baselines by large margins (+20.62% on average)."
    },
    {
        "date": "2025-01",
        "title": "Blockchain Security Risk Assessment in Quantum Era, Migration Strategies and Proactive Defense",
        "author": "Yaser Baseri, Abdelhakim Hafid, Yahya Shahsavari, Dimitrios Makrakis, and Hassan Khodaiemehr",
        "link": "http://arxiv.org/abs/2501.11798v1",
        "abstract": "The emergence of quantum computing presents a formidable challenge to the\nsecurity of blockchain systems. Traditional cryptographic algorithms,\nfoundational to digital signatures, message encryption, and hashing functions,\nbecome vulnerable to the immense computational power of quantum computers. This\npaper conducts a thorough risk assessment of transitioning to quantum-resistant\nblockchains, comprehensively analyzing potential threats targeting vital\nblockchain components: the network, mining pools, transaction verification\nmechanisms, smart contracts, and user wallets. By elucidating the intricate\nchallenges and strategic considerations inherent in transitioning to\nquantum-resistant algorithms, the paper evaluates risks and highlights\nobstacles in securing blockchain components with quantum-resistant\ncryptography. It offers a hybrid migration strategy to facilitate a smooth\ntransition from classical to quantum-resistant cryptography. The analysis\nextends to prominent blockchains such as Bitcoin, Ethereum, Ripple, Litecoin,\nand Zcash, assessing vulnerable components, potential impacts, and associated\nSTRIDE threats, thereby identifying areas susceptible to quantum attacks.\nBeyond analysis, the paper provides actionable guidance for designing secure\nand resilient blockchain ecosystems in the quantum computing era. Recognizing\nthe looming threat of quantum computers, this research advocates for a\nproactive transition to quantum-resistant blockchain networks. It proposes a\ntailored security blueprint that strategically fortifies each component against\nthe evolving landscape of quantum-induced cyber threats. Emphasizing the\ncritical need for blockchain stakeholders to adopt proactive measures and\nimplement quantum-resistant solutions, the paper underscores the importance of\nembracing these insights to navigate the complexities of the quantum era with\nresilience and confidence."
    },
    {
        "date": "2025-01",
        "title": "Provably effective detection of effective data poisoning attacks",
        "author": "Jonathan Gallagher, Yasaman Esfandiari, Callen MacPhee, and Michael Warren",
        "link": "http://arxiv.org/abs/2501.11795v1",
        "abstract": "This paper establishes a mathematically precise definition of dataset\npoisoning attack and proves that the very act of effectively poisoning a\ndataset ensures that the attack can be effectively detected. On top of a\nmathematical guarantee that dataset poisoning is identifiable by a new\nstatistical test that we call the Conformal Separability Test, we provide\nexperimental evidence that we can adequately detect poisoning attempts in the\nreal world."
    },
    {
        "date": "2025-01",
        "title": "Disentangling stellar atmospheric parameters in astronomical spectra using Generative Adversarial Neural Networks",
        "author": "Minia Manteiga, Ra\u00fal Santove\u00f1a, Marco A. \u00c1lvarez, Carlos Dafonte, Manuel G. Penedo, Silvana Navarro, and Luis Corral",
        "link": "http://arxiv.org/abs/2501.11762v1",
        "abstract": "A method based on Generative Adversaria! Networks (GANs) is developed for\ndisentangling the physical (effective temperature and gravity) and chemical\n(metallicity, overabundance of a-elements with respect to iron) atmospheric\nproperties in astronomical spectra. Using a projection of the stellar spectra,\ncommonly called latent space, in which the contribution dueto one or several\nmain stellar physicochemical properties is minimised while others are enhanced,\nit was possible to maximise the information related to certain properties,\nwhich can then be extracted using artificial neural networks (ANN) as\nregressors with higher accuracy than a reference method based on the use of ANN\ntrained with the original spectra. Methods. Our model utilises autoencoders,\ncomprising two artificial neural networks: an encoder anda decoder which\ntransform input data into a low-dimensional representation known as latent\nspace. It also uses discriminators, which are additional neural networks aimed\nat transforming the traditional autoencoder training into an adversaria!\napproach, to disentangle or reinforce the astrophysical parameters from the\nlatent space. The GANDALF tool is described. It was developed to define, train,\nand test our GAN model with a web framework to show how the disentangling\nalgorithm works visually. It is open to the community in Github. Results. The\nperformance of our approach for retrieving atmospheric stellar properties from\nspectra is demonstrated using Gaia Radial Velocity Spectrograph (RVS) data from\nDR3. We use a data-driven perspective and obtain very competitive values, ali\nwithin the literature errors, and with the advantage of an important\ndimensionality reduction of the data to be processed."
    },
    {
        "date": "2025-01",
        "title": "Enhancing IoT Network Security through Adaptive Curriculum Learning and XAI",
        "author": "Sathwik Narkedimilli, Sujith Makam, Amballa Venkata Sriram, Sai Prashanth Mallellu, MSVPJ Sathvik, and Ranga Rao Venkatesha Prasad",
        "link": "http://arxiv.org/abs/2501.11618v1",
        "abstract": "To address the critical need for secure IoT networks, this study presents a\nscalable and lightweight curriculum learning framework enhanced with\nExplainable AI (XAI) techniques, including LIME, to ensure transparency and\nadaptability. The proposed model employs novel neural network architecture\nutilized at every stage of Curriculum Learning to efficiently capture and focus\non both short- and long-term temporal dependencies, improve learning stability,\nand enhance accuracy while remaining lightweight and robust against noise in\nsequential IoT data. Robustness is achieved through staged learning, where the\nmodel iteratively refines itself by removing low-relevance features and\noptimizing performance. The workflow includes edge-optimized quantization and\npruning to ensure portability that could easily be deployed in the edge-IoT\ndevices. An ensemble model incorporating Random Forest, XGBoost, and the staged\nlearning base further enhances generalization. Experimental results demonstrate\n98% accuracy on CIC-IoV-2024 and CIC-APT-IIoT-2024 datasets and 97% on\nEDGE-IIoT, establishing this framework as a robust, transparent, and\nhigh-performance solution for IoT network security."
    },
    {
        "date": "2025-01",
        "title": "Rethinking Membership Inference Attacks Against Transfer Learning",
        "author": "Cong Wu, Jing Chen, Qianru Fang, Kun He, Ziming Zhao, Hao Ren, Guowen Xu, Yang Liu, and Yang Xiang",
        "link": "http://arxiv.org/abs/2501.11577v1",
        "abstract": "Transfer learning, successful in knowledge translation across related tasks,\nfaces a substantial privacy threat from membership inference attacks (MIAs).\nThese attacks, despite posing significant risk to ML model's training data,\nremain limited-explored in transfer learning. The interaction between teacher\nand student models in transfer learning has not been thoroughly explored in\nMIAs, potentially resulting in an under-examined aspect of privacy\nvulnerabilities within transfer learning. In this paper, we propose a new MIA\nvector against transfer learning, to determine whether a specific data point\nwas used to train the teacher model while only accessing the student model in a\nwhite-box setting. Our method delves into the intricate relationship between\nteacher and student models, analyzing the discrepancies in hidden layer\nrepresentations between the student model and its shadow counterpart. These\nidentified differences are then adeptly utilized to refine the shadow model's\ntraining process and to inform membership inference decisions effectively. Our\nmethod, evaluated across four datasets in diverse transfer learning tasks,\nreveals that even when an attacker only has access to the student model, the\nteacher model's training data remains susceptible to MIAs. We believe our work\nunveils the unexplored risk of membership inference in transfer learning."
    },
    {
        "date": "2025-01",
        "title": "Graph Defense Diffusion Model",
        "author": "Xin He, Wenqi Fan, Yili Wang, Chengyi Liu, Rui Miao, Xin Juan, and Xin Wang",
        "link": "http://arxiv.org/abs/2501.11568v1",
        "abstract": "Graph Neural Networks (GNNs) demonstrate significant potential in various\napplications but remain highly vulnerable to adversarial attacks, which can\ngreatly degrade their performance. Existing graph purification methods attempt\nto address this issue by filtering attacked graphs; however, they struggle to\neffectively defend against multiple types of adversarial attacks simultaneously\ndue to their limited flexibility, and they lack comprehensive modeling of graph\ndata due to their heavy reliance on heuristic prior knowledge. To overcome\nthese challenges, we propose a more versatile approach for defending against\nadversarial attacks on graphs. In this work, we introduce the Graph Defense\nDiffusion Model (GDDM), a flexible purification method that leverages the\ndenoising and modeling capabilities of diffusion models. The iterative nature\nof diffusion models aligns well with the stepwise process of adversarial\nattacks, making them particularly suitable for defense. By iteratively adding\nand removing noise, GDDM effectively purifies attacked graphs, restoring their\noriginal structure and features. Our GDDM consists of two key components: (1)\nGraph Structure-Driven Refiner, which preserves the basic fidelity of the graph\nduring the denoising process, and ensures that the generated graph remains\nconsistent with the original scope; and (2) Node Feature-Constrained\nRegularizer, which removes residual impurities from the denoised graph, further\nenhances the purification effect. Additionally, we design tailored denoising\nstrategies to handle different types of adversarial attacks, improving the\nmodel's adaptability to various attack scenarios. Extensive experiments\nconducted on three real-world datasets demonstrate that GDDM outperforms\nstate-of-the-art methods in defending against a wide range of adversarial\nattacks, showcasing its robustness and effectiveness."
    },
    {
        "date": "2025-01",
        "title": "Secure Resource Allocation via Constrained Deep Reinforcement Learning",
        "author": "Jianfei Sun, Qiang Gao, Cong Wu, Yuxian Li, Jiacheng Wang, and Dusit Niyato",
        "link": "http://arxiv.org/abs/2501.11557v1",
        "abstract": "The proliferation of Internet of Things (IoT) devices and the advent of 6G\ntechnologies have introduced computationally intensive tasks that often surpass\nthe processing capabilities of user devices. Efficient and secure resource\nallocation in serverless multi-cloud edge computing environments is essential\nfor supporting these demands and advancing distributed computing. However,\nexisting solutions frequently struggle with the complexity of multi-cloud\ninfrastructures, robust security integration, and effective application of\ntraditional deep reinforcement learning (DRL) techniques under system\nconstraints. To address these challenges, we present SARMTO, a novel framework\nthat integrates an action-constrained DRL model. SARMTO dynamically balances\nresource allocation, task offloading, security, and performance by utilizing a\nMarkov decision process formulation, an adaptive security mechanism, and\nsophisticated optimization techniques. Extensive simulations across varying\nscenarios, including different task loads, data sizes, and MEC capacities, show\nthat SARMTO consistently outperforms five baseline approaches, achieving up to\na 40% reduction in system costs and a 41.5% improvement in energy efficiency\nover state-of-the-art methods. These enhancements highlight SARMTO's potential\nto revolutionize resource management in intricate distributed computing\nenvironments, opening the door to more efficient and secure IoT and edge\ncomputing applications."
    },
    {
        "date": "2025-01",
        "title": "An Exploratory Study on the Engineering of Security Features",
        "author": "Kevin Hermann, Sven Peldszus, Jan-Philipp Stegh\u00f6fer, and Thorsten Berger",
        "link": "http://arxiv.org/abs/2501.11546v1",
        "abstract": "Software security is of utmost importance for most software systems.\nDevelopers must systematically select, plan, design, implement, and especially\nmaintain and evolve security features -- functionalities to mitigate attacks or\nprotect personal data such as cryptography or access control, to ensure the\nsecurity of their software. While security features are usually available in\nlibraries, additional code needs to be written and maintained to integrate\nsecurity features and not all desired features can be reused this way. While\nthere have been studies on the use of such libraries, surprisingly little is\nknown about how developers engineer security features, how they select what\nsecurity features to implement, and the implications on maintenance. We\ntherefore currently rely on assumptions that are largely based on common sense\nor individual examples. However, researchers require hard empirical data to\nunderstand what practitioners need and how they view security, which we\ncurrently lack to provide them with effective solutions. We contribute an\nexploratory study with 26 knowledgeable industrial participants. We study how\nsecurity features of software systems are selected and engineered in practice,\nwhat their code-level characteristics are, and the challenges practitioners\nface. Based on the empirical data gathered, we validate four common assumptions\nand gain insights into engineering practices."
    },
    {
        "date": "2025-01",
        "title": "On the Adversarial Vulnerabilities of Transfer Learning in Remote Sensing",
        "author": "Tao Bai, Xingjian Tian, Yonghao Xu, and Bihan Wen",
        "link": "http://arxiv.org/abs/2501.11462v1",
        "abstract": "The use of pretrained models from general computer vision tasks is widespread\nin remote sensing, significantly reducing training costs and improving\nperformance. However, this practice also introduces vulnerabilities to\ndownstream tasks, where publicly available pretrained models can be used as a\nproxy to compromise downstream models. This paper presents a novel Adversarial\nNeuron Manipulation method, which generates transferable perturbations by\nselectively manipulating single or multiple neurons in pretrained models.\nUnlike existing attacks, this method eliminates the need for domain-specific\ninformation, making it more broadly applicable and efficient. By targeting\nmultiple fragile neurons, the perturbations achieve superior attack\nperformance, revealing critical vulnerabilities in deep learning models.\nExperiments on diverse models and remote sensing datasets validate the\neffectiveness of the proposed method. This low-access adversarial neuron\nmanipulation technique highlights a significant security risk in transfer\nlearning models, emphasizing the urgent need for more robust defenses in their\ndesign when addressing the safety-critical remote sensing tasks."
    },
    {
        "date": "2025-01",
        "title": "Nested Annealed Training Scheme for Generative Adversarial Networks",
        "author": "Chang Wan, Ming-Hsuan Yang, Minglu Li, Yunliang Jiang, and Zhonglong Zheng",
        "link": "http://arxiv.org/abs/2501.11318v1",
        "abstract": "Recently, researchers have proposed many deep generative models, including\ngenerative adversarial networks(GANs) and denoising diffusion models. Although\nsignificant breakthroughs have been made and empirical success has been\nachieved with the GAN, its mathematical underpinnings remain relatively\nunknown. This paper focuses on a rigorous mathematical theoretical framework:\nthe composite-functional-gradient GAN (CFG)[1]. Specifically, we reveal the\ntheoretical connection between the CFG model and score-based models. We find\nthat the training objective of the CFG discriminator is equivalent to finding\nan optimal D(x). The optimal gradient of D(x) differentiates the integral of\nthe differences between the score functions of real and synthesized samples.\nConversely, training the CFG generator involves finding an optimal G(x) that\nminimizes this difference. In this paper, we aim to derive an annealed weight\npreceding the weight of the CFG discriminator. This new explicit theoretical\nexplanation model is called the annealed CFG method. To overcome the limitation\nof the annealed CFG method, as the method is not readily applicable to the SOTA\nGAN model, we propose a nested annealed training scheme (NATS). This scheme\nkeeps the annealed weight from the CFG method and can be seamlessly adapted to\nvarious GAN models, no matter their structural, loss, or regularization\ndifferences. We conduct thorough experimental evaluations on various benchmark\ndatasets for image generation. The results show that our annealed CFG and NATS\nmethods significantly improve the quality and diversity of the synthesized\nsamples. This improvement is clear when comparing the CFG method and the SOTA\nGAN models."
    },
    {
        "date": "2025-01",
        "title": "PD-SORT: Occlusion-Robust Multi-Object Tracking Using Pseudo-Depth Cues",
        "author": "Yanchao Wang, Dawei Zhang, Run Li, Zhonglong Zheng, and Minglu Li",
        "link": "http://arxiv.org/abs/2501.11288v1",
        "abstract": "Multi-object tracking (MOT) is a rising topic in video processing\ntechnologies and has important application value in consumer electronics.\nCurrently, tracking-by-detection (TBD) is the dominant paradigm for MOT, which\nperforms target detection and association frame by frame. However, the\nassociation performance of TBD methods degrades in complex scenes with heavy\nocclusions, which hinders the application of such methods in real-world\nscenarios.To this end, we incorporate pseudo-depth cues to enhance the\nassociation performance and propose Pseudo-Depth SORT (PD-SORT). First, we\nextend the Kalman filter state vector with pseudo-depth states. Second, we\nintroduce a novel depth volume IoU (DVIoU) by combining the conventional 2D IoU\nwith pseudo-depth. Furthermore, we develop a quantized pseudo-depth measurement\n(QPDM) strategy for more robust data association. Besides, we also integrate\ncamera motion compensation (CMC) to handle dynamic camera situations. With the\nabove designs, PD-SORT significantly alleviates the occlusion-induced ambiguous\nassociations and achieves leading performances on DanceTrack, MOT17, and MOT20.\nNote that the improvement is especially obvious on DanceTrack, where objects\nshow complex motions, similar appearances, and frequent occlusions. The code is\navailable at https://github.com/Wangyc2000/PD_SORT."
    },
    {
        "date": "2025-01",
        "title": "Cybersecurity and Frequent Cyber Attacks on IoT Devices in Healthcare: Issues and Solutions",
        "author": "Zag ElSayed, Ahmed Abdelgawad, and Nelly Elsayed",
        "link": "http://arxiv.org/abs/2501.11250v1",
        "abstract": "Integrating Internet of Things (IoT) devices in healthcare has revolutionized\npatient care, offering improved monitoring, diagnostics, and treatment.\nHowever, the proliferation of these devices has also introduced significant\ncybersecurity challenges. This paper reviews the current landscape of\ncybersecurity threats targeting IoT devices in healthcare, discusses the\nunderlying issues contributing to these vulnerabilities, and explores potential\nsolutions. Additionally, this study offers solutions and suggestions for\nresearchers, agencies, and security specialists to overcome these IoT in\nhealthcare cybersecurity vulnerabilities. A comprehensive literature survey\nhighlights the nature and frequency of cyber attacks, their impact on\nhealthcare systems, and emerging strategies to mitigate these risks."
    },
    {
        "date": "2025-01",
        "title": "Conditional Feature Importance with Generative Modeling Using Adversarial Random Forests",
        "author": "Kristin Blesch, Niklas Koenen, Jan Kapar, Pegah Golchian, Lukas Burk, Markus Loecher, and Marvin N. Wright",
        "link": "http://arxiv.org/abs/2501.11178v1",
        "abstract": "This paper proposes a method for measuring conditional feature importance via\ngenerative modeling. In explainable artificial intelligence (XAI), conditional\nfeature importance assesses the impact of a feature on a prediction model's\nperformance given the information of other features. Model-agnostic post hoc\nmethods to do so typically evaluate changes in the predictive performance under\non-manifold feature value manipulations. Such procedures require creating\nfeature values that respect conditional feature distributions, which can be\nchallenging in practice. Recent advancements in generative modeling can\nfacilitate this. For tabular data, which may consist of both categorical and\ncontinuous features, the adversarial random forest (ARF) stands out as a\ngenerative model that can generate on-manifold data points without requiring\nintensive tuning efforts or computational resources, making it a promising\ncandidate model for subroutines in XAI methods. This paper proposes cARFi\n(conditional ARF feature importance), a method for measuring conditional\nfeature importance through feature values sampled from ARF-estimated\nconditional distributions. cARFi requires only little tuning to yield robust\nimportance scores that can flexibly adapt for conditional or marginal notions\nof feature importance, including straightforward extensions to condition on\nfeature subsets and allows for inferring the significance of feature\nimportances through statistical tests."
    },
    {
        "date": "2025-01",
        "title": "Counteracting temporal attacks in Video Copy Detection",
        "author": "Katarzyna Fojcik, and Piotr Syga",
        "link": "http://arxiv.org/abs/2501.11171v1",
        "abstract": "Video Copy Detection (VCD) plays a crucial role in copyright protection and\ncontent verification by identifying duplicates and near-duplicates in\nlarge-scale video databases. The META AI Challenge on video copy detection\nprovided a benchmark for evaluating state-of-the-art methods, with the\nDual-level detection approach emerging as a winning solution. This method\nintegrates Video Editing Detection and Frame Scene Detection to handle\nadversarial transformations and large datasets efficiently. However, our\nanalysis reveals significant limitations in the VED component, particularly in\nits ability to handle exact copies. Moreover, Dual-level detection shows\nvulnerability to temporal attacks. To address it, we propose an improved frame\nselection strategy based on local maxima of interframe differences, which\nenhances robustness against adversarial temporal modifications while\nsignificantly reducing computational overhead. Our method achieves an increase\nof 1.4 to 5.8 times in efficiency over the standard 1 FPS approach. Compared to\nDual-level detection method, our approach maintains comparable micro-average\nprecision ($\\mu$AP) while also demonstrating improved robustness against\ntemporal attacks. Given 56\\% reduced representation size and the inference time\nof more than 2 times faster, our approach is more suitable to real-world\nresource restriction."
    },
    {
        "date": "2025-01",
        "title": "Federated Testing (FedTest): A New Scheme to Enhance Convergence and Mitigate Adversarial Attacks in Federating Learning",
        "author": "Mustafa Ghaleb, Mohanad Obeed, Muhamad Felemban, Anas Chaaban, and Halim Yanikomeroglu",
        "link": "http://arxiv.org/abs/2501.11167v1",
        "abstract": "Federated Learning (FL) has emerged as a significant paradigm for training\nmachine learning models. This is due to its data-privacy-preserving property\nand its efficient exploitation of distributed computational resources. This is\nachieved by conducting the training process in parallel at distributed users.\nHowever, traditional FL strategies grapple with difficulties in evaluating the\nquality of received models, handling unbalanced models, and reducing the impact\nof detrimental models. To resolve these problems, we introduce a novel\nfederated learning framework, which we call federated testing for federated\nlearning (FedTest). In the FedTest method, the local data of a specific user is\nused to train the model of that user and test the models of the other users.\nThis approach enables users to test each other's models and determine an\naccurate score for each. This score can then be used to aggregate the models\nefficiently and identify any malicious ones. Our numerical results reveal that\nthe proposed method not only accelerates convergence rates but also diminishes\nthe potential influence of malicious users. This significantly enhances the\noverall efficiency and robustness of FL systems."
    },
    {
        "date": "2025-01",
        "title": "A Novel Pearson Correlation-Based Merging Algorithm for Robust Distributed Machine Learning with Heterogeneous Data",
        "author": "Mohammad Ghabel Rahmat, and Majid Khalilian",
        "link": "http://arxiv.org/abs/2501.11112v1",
        "abstract": "Federated learning faces significant challenges in scenarios with\nheterogeneous data distributions and adverse network conditions, such as\ndelays, packet loss, and data poisoning attacks. This paper proposes a novel\nmethod based on the SCAFFOLD algorithm to improve the quality of local updates\nand enhance the robustness of the global model. The key idea is to form\nintermediary nodes by merging local models with high similarity, using the\nPearson correlation coefficient as a similarity measure. The proposed merging\nalgorithm reduces the number of local nodes while maintaining the accuracy of\nthe global model, effectively addressing communication overhead and bandwidth\nconsumption. Experimental results on the MNIST dataset under simulated\nfederated learning scenarios demonstrate the method's effectiveness. After 10\nrounds of training using a CNN model, the proposed approach achieved accuracies\nof 0.82, 0.73, and 0.66 under normal conditions, packet loss and data poisoning\nattacks, respectively, outperforming the baseline SCAFFOLD algorithm. These\nresults highlight the potential of the proposed method to improve efficiency\nand resilience in federated learning systems."
    },
    {
        "date": "2025-01",
        "title": "Enhancing Sample Utilization in Noise-Robust Deep Metric Learning With Subgroup-Based Positive-Pair Selection",
        "author": "Zhipeng Yu, Qianqian Xu, Yangbangyan Jiang, Yingfei Sun, and Qingming Huang",
        "link": "http://arxiv.org/abs/2501.11063v1",
        "abstract": "The existence of noisy labels in real-world data negatively impacts the\nperformance of deep learning models. Although much research effort has been\ndevoted to improving the robustness towards noisy labels in classification\ntasks, the problem of noisy labels in deep metric learning (DML) remains\nunder-explored. Existing noisy label learning methods designed for DML mainly\ndiscard suspicious noisy samples, resulting in a waste of the training data. To\naddress this issue, we propose a noise-robust DML framework with SubGroup-based\nPositive-pair Selection (SGPS), which constructs reliable positive pairs for\nnoisy samples to enhance the sample utilization. Specifically, SGPS first\neffectively identifies clean and noisy samples by a probability-based clean\nsample selectionstrategy. To further utilize the remaining noisy samples, we\ndiscover their potential similar samples based on the subgroup information\ngiven by a subgroup generation module and then aggregate them into informative\npositive prototypes for each noisy sample via a positive prototype generation\nmodule. Afterward, a new contrastive loss is tailored for the noisy samples\nwith their selected positive pairs. SGPS can be easily integrated into the\ntraining process of existing pair-wise DML tasks, like image retrieval and face\nrecognition. Extensive experiments on multiple synthetic and real-world\nlarge-scale label noise datasets demonstrate the effectiveness of our proposed\nmethod. Without any bells and whistles, our SGPS framework outperforms the\nstate-of-the-art noisy label DML methods. Code is available at\n\\url{https://github.com/smuelpeng/SGPS-NoiseFreeDML}."
    },
    {
        "date": "2025-01",
        "title": "Temporal Analysis of Adversarial Attacks in Federated Learning",
        "author": "Rohit Mapakshi, Sayma Akther, and Mark Stamp",
        "link": "http://arxiv.org/abs/2501.11054v1",
        "abstract": "In this paper, we experimentally analyze the robustness of selected Federated\nLearning (FL) systems in the presence of adversarial clients. We find that\ntemporal attacks significantly affect model performance in the FL models\ntested, especially when the adversaries are active throughout or during the\nlater rounds. We consider a variety of classic learning models, including\nMultinominal Logistic Regression (MLR), Random Forest, XGBoost, Support Vector\nClassifier (SVC), as well as various Neural Network models including Multilayer\nPerceptron (MLP), Convolution Neural Network (CNN), Recurrent Neural Network\n(RNN), and Long Short-Term Memory (LSTM). Our results highlight the\neffectiveness of temporal attacks and the need to develop strategies to make\nthe FL process more robust against such attacks. We also briefly consider the\neffectiveness of defense mechanisms, including outlier detection in the\naggregation algorithm."
    },
    {
        "date": "2025-01",
        "title": "Bridging the Security Gap: Lessons from 5G and What 6G Should Do Better",
        "author": "Isabella D. Lutz, and Matthew C. Valenti",
        "link": "http://arxiv.org/abs/2501.11045v1",
        "abstract": "The security requirements for future 6G mobile networks are anticipated to be\nsignificantly more complex and demanding than those of 5G. This increase stems\nfrom several factors: the proliferation of massive machine-type communications\nwill dramatically increase the density of devices competing for network access;\nsecure ultra-reliable low-latency communication will impose stringent\nrequirements on security, latency, and reliability; and the widespread\ndeployment of small cells and non-terrestrial networks, including satellite\nmega-constellations, will result in more frequent handovers. This paper\nprovides a set of security recommendations for 6G networks, with a particular\nfocus on access and handover procedures, which often lack encryption and\nintegrity protection, making them more vulnerable to exploitation. Since 6G is\nexpected to be a backward-compatible extension of 5G, and given that secure\nsystems cannot be effectively designed without a clear understanding of their\ngoals, it is imperative to first evaluate the limitations of the current\ngeneration. To this end, the paper begins by reviewing existing 5G access and\nauthentication mechanisms, highlighting several critical vulnerabilities in\nthese procedures. It then examines potential 6G challenges and concludes with\nactionable recommendations to enhance the security, resilience, and robustness\nof 6G access and handover mechanisms."
    },
    {
        "date": "2025-01",
        "title": "Beyond Any-Shot Adaptation: Predicting Optimization Outcome for Robustness Gains without Extra Pay",
        "author": "Qi Cheems Wang, Zehao Xiao, Yixiu Mao, Yun Qu, Jiayi Shen, Yiqin Lv, and Xiangyang Ji",
        "link": "http://arxiv.org/abs/2501.11039v1",
        "abstract": "The foundation model enables fast problem-solving without learning from\nscratch, and such a desirable adaptation property benefits from its adopted\ncross-task generalization paradigms, e.g., pretraining, meta-training, or\nfinetuning. Recent trends have focused on the curation of task datasets during\noptimization, which includes task selection as an indispensable consideration\nfor either adaptation robustness or sampling efficiency purposes. Despite some\nprogress, selecting crucial task batches to optimize over iteration mostly\nexhausts massive task queries and requires intensive evaluation and\ncomputations to secure robust adaptation. This work underscores the criticality\nof both robustness and learning efficiency, especially in scenarios where tasks\nare risky to collect or costly to evaluate. To this end, we present Model\nPredictive Task Sampling (MPTS), a novel active task sampling framework to\nestablish connections between the task space and adaptation risk landscape\nachieve robust adaptation. Technically, MPTS characterizes the task episodic\ninformation with a generative model and predicts optimization outcome after\nadaptation from posterior inference, i.e., forecasting task-specific adaptation\nrisk values. The resulting risk learner amortizes expensive annotation,\nevaluation, or computation operations in task robust adaptation learning\nparadigms. Extensive experimental results show that MPTS can be seamlessly\nintegrated into zero-shot, few-shot, and many-shot learning paradigms,\nincreases adaptation robustness, and retains learning efficiency without\naffording extra cost. The code will be available at the project site\nhttps://github.com/thu-rllab/MPTS."
    },
    {
        "date": "2025-01",
        "title": "Effectiveness of Adversarial Benign and Malware Examples in Evasion and Poisoning Attacks",
        "author": "Matou\u0161 Koz\u00e1k, and Martin Jure\u010dek",
        "link": "http://arxiv.org/abs/2501.10996v1",
        "abstract": "Adversarial attacks present significant challenges for malware detection\nsystems. This research investigates the effectiveness of benign and malicious\nadversarial examples (AEs) in evasion and poisoning attacks on the Portable\nExecutable file domain. A novel focus of this study is on benign AEs, which,\nalthough not directly harmful, can increase false positives and undermine trust\nin antivirus solutions. We propose modifying existing adversarial malware\ngenerators to produce benign AEs and show they are as successful as malware AEs\nin evasion attacks. Furthermore, our data show that benign AEs have a more\ndecisive influence in poisoning attacks than standard malware AEs,\ndemonstrating their superior ability to decrease the model's performance. Our\nfindings introduce new opportunities for adversaries and further increase the\nattack surface that needs to be protected by security researchers."
    },
    {
        "date": "2025-01",
        "title": "GRID: Protecting Training Graph from Link Stealing Attacks on GNN Models",
        "author": "Jiadong Lou, Xu Yuan, Rui Zhang, Xingliang Yuan, Neil Gong, and Nian-Feng Tzeng",
        "link": "http://arxiv.org/abs/2501.10985v1",
        "abstract": "Graph neural networks (GNNs) have exhibited superior performance in various\nclassification tasks on graph-structured data. However, they encounter the\npotential vulnerability from the link stealing attacks, which can infer the\npresence of a link between two nodes via measuring the similarity of its\nincident nodes' prediction vectors produced by a GNN model. Such attacks pose\nsevere security and privacy threats to the training graph used in GNN models.\nIn this work, we propose a novel solution, called Graph Link Disguise (GRID),\nto defend against link stealing attacks with the formal guarantee of GNN model\nutility for retaining prediction accuracy. The key idea of GRID is to add\ncarefully crafted noises to the nodes' prediction vectors for disguising\nadjacent nodes as n-hop indirect neighboring nodes. We take into account the\ngraph topology and select only a subset of nodes (called core nodes) covering\nall links for adding noises, which can avert the noises offset and have the\nfurther advantages of reducing both the distortion loss and the computation\ncost. Our crafted noises can ensure 1) the noisy prediction vectors of any two\nadjacent nodes have their similarity level like that of two non-adjacent nodes\nand 2) the model prediction is unchanged to ensure zero utility loss. Extensive\nexperiments on five datasets are conducted to show the effectiveness of our\nproposed GRID solution against different representative link-stealing attacks\nunder transductive settings and inductive settings respectively, as well as two\ninfluence-based attacks. Meanwhile, it achieves a much better privacy-utility\ntrade-off than existing methods when extended to GNNs."
    },
    {
        "date": "2025-01",
        "title": "CIBPU: A Conflict-Invisible Secure Branch Prediction Unit",
        "author": "Zhe Zhou, Fei Tong, Hongyu Wang, Xiaoyu Cheng, Fang Jiang, Zhikun Zhang, and Yuxing Mao",
        "link": "http://arxiv.org/abs/2501.10983v1",
        "abstract": "Previous schemes for designing secure branch prediction unit (SBPU) based on\nphysical isolation can only offer limited security and significantly affect\nBPU's prediction capability, leading to prominent performance degradation.\nMoreover, encryption-based SBPU schemes based on periodic key re-randomization\nhave the risk of being compromised by advanced attack algorithms, and the\nperformance overhead is also considerable. To this end, this paper proposes a\nconflict-invisible SBPU (CIBPU). CIBPU employs redundant storage design,\nload-aware indexing, and replacement design, as well as an encryption mechanism\nwithout requiring periodic key updates, to prevent attackers' perception of\nbranch conflicts. We provide a thorough security analysis, which shows that\nCIBPU achieves strong security throughout the BPU's lifecycle. We implement\nCIBPU in a RISC-V core model in gem5. The experimental results show that CIBPU\ncauses an average performance overhead of only 1.12%-2.20% with acceptable\nhardware storage overhead, which is the lowest among the state-of-the-art SBPU\nschemes. CIBPU has also been implemented in the open-source RISC-V core,\nSonicBOOM, which is then burned onto an FPGA board. The evaluation based on the\nboard shows an average performance degradation of 2.01%, which is approximately\nconsistent with the result obtained in gem5."
    },
    {
        "date": "2025-01",
        "title": "TSVC:Tripartite Learning with Semantic Variation Consistency for Robust Image-Text Retrieval",
        "author": "Shuai Lyu, Zijing Tian, Zhonghong Ou, Yifan Zhu, Xiao Zhang, Qiankun Ha, Haoran Luo, and Meina Song",
        "link": "http://arxiv.org/abs/2501.10935v1",
        "abstract": "Cross-modal retrieval maps data under different modality via semantic\nrelevance. Existing approaches implicitly assume that data pairs are\nwell-aligned and ignore the widely existing annotation noise, i.e., noisy\ncorrespondence (NC). Consequently, it inevitably causes performance\ndegradation. Despite attempts that employ the co-teaching paradigm with\nidentical architectures to provide distinct data perspectives, the differences\nbetween these architectures are primarily stemmed from random initialization.\nThus, the model becomes increasingly homogeneous along with the training\nprocess. Consequently, the additional information brought by this paradigm is\nseverely limited. In order to resolve this problem, we introduce a Tripartite\nlearning with Semantic Variation Consistency (TSVC) for robust image-text\nretrieval. We design a tripartite cooperative learning mechanism comprising a\nCoordinator, a Master, and an Assistant model. The Coordinator distributes\ndata, and the Assistant model supports the Master model's noisy label\nprediction with diverse data. Moreover, we introduce a soft label estimation\nmethod based on mutual information variation, which quantifies the noise in new\nsamples and assigns corresponding soft labels. We also present a new loss\nfunction to enhance robustness and optimize training effectiveness. Extensive\nexperiments on three widely used datasets demonstrate that, even at increasing\nnoise ratios, TSVC exhibits significant advantages in retrieval accuracy and\nmaintains stable training performance."
    },
    {
        "date": "2025-01",
        "title": "LegalGuardian: A Privacy-Preserving Framework for Secure Integration of Large Language Models in Legal Practice",
        "author": "M. Mikail Demir, Hakan T. Otal, and M. Abdullah Canbaz",
        "link": "http://arxiv.org/abs/2501.10915v1",
        "abstract": "Large Language Models (LLMs) hold promise for advancing legal practice by\nautomating complex tasks and improving access to justice. However, their\nadoption is limited by concerns over client confidentiality, especially when\nlawyers include sensitive Personally Identifiable Information (PII) in prompts,\nrisking unauthorized data exposure. To mitigate this, we introduce\nLegalGuardian, a lightweight, privacy-preserving framework tailored for lawyers\nusing LLM-based tools. LegalGuardian employs Named Entity Recognition (NER)\ntechniques and local LLMs to mask and unmask confidential PII within prompts,\nsafeguarding sensitive data before any external interaction. We detail its\ndevelopment and assess its effectiveness using a synthetic prompt library in\nimmigration law scenarios. Comparing traditional NER models with one-shot\nprompted local LLM, we find that LegalGuardian achieves a F1-score of 93% with\nGLiNER and 97% with Qwen2.5-14B in PII detection. Semantic similarity analysis\nconfirms that the framework maintains high fidelity in outputs, ensuring robust\nutility of LLM-based tools. Our findings indicate that legal professionals can\nharness advanced AI technologies without compromising client confidentiality or\nthe quality of legal documents."
    },
    {
        "date": "2025-01",
        "title": "Explainable Adversarial Attacks on Coarse-to-Fine Classifiers",
        "author": "Akram Heidarizadeh, Connor Hatfield, Lorenzo Lazzarotto, HanQin Cai, and George Atia",
        "link": "http://arxiv.org/abs/2501.10906v1",
        "abstract": "Traditional adversarial attacks typically aim to alter the predicted labels\nof input images by generating perturbations that are imperceptible to the human\neye. However, these approaches often lack explainability. Moreover, most\nexisting work on adversarial attacks focuses on single-stage classifiers, but\nmulti-stage classifiers are largely unexplored. In this paper, we introduce\ninstance-based adversarial attacks for multi-stage classifiers, leveraging\nLayer-wise Relevance Propagation (LRP), which assigns relevance scores to\npixels based on their influence on classification outcomes. Our approach\ngenerates explainable adversarial perturbations by utilizing LRP to identify\nand target key features critical for both coarse and fine-grained\nclassifications. Unlike conventional attacks, our method not only induces\nmisclassification but also enhances the interpretability of the model's\nbehavior across classification stages, as demonstrated by experimental results."
    },
    {
        "date": "2025-01",
        "title": "A Generative Security Application Engineering Curriculum",
        "author": "Wu-chang Feng, and David Baker-Robinson",
        "link": "http://arxiv.org/abs/2501.10900v1",
        "abstract": "Generative AI and large language models (LLMs) are transforming security by\nautomating many tasks being performed manually. With such automation changing\nthe practice of security as we know it, it is imperative that we prepare future\nstudents for the technology landscape they will ultimately face. Towards this\nend, we describe an initial curriculum and course that attempts to show\nstudents how to apply generative AI in order to solve problems in security. By\nrefocusing security education and training on aspects uniquely suited for\nhumans and showing students how to leverage automation for the rest, we believe\nwe can better align security education practices with generative AI as it\nevolves."
    },
    {
        "date": "2025-01",
        "title": "Certifying Robustness via Topological Representations",
        "author": "Jens Agerberg, Andrea Guidolin, Andrea Martinelli, Pepijn Roos Hoefgeest, David Eklund, and Martina Scolamiero",
        "link": "http://arxiv.org/abs/2501.10876v1",
        "abstract": "We propose a neural network architecture that can learn discriminative\ngeometric representations of data from persistence diagrams, common descriptors\nof Topological Data Analysis. The learned representations enjoy Lipschitz\nstability with a controllable Lipschitz constant. In adversarial learning, this\nstability can be used to certify $\\epsilon$-robustness for samples in a\ndataset, which we demonstrate on the ORBIT5K dataset representing the orbits of\na discrete dynamical system."
    },
    {
        "date": "2025-01",
        "title": "Model-Robust and Adaptive-Optimal Transfer Learning for Tackling Concept Shifts in Nonparametric Regression",
        "author": "Haotian Lin, and Matthew Reimherr",
        "link": "http://arxiv.org/abs/2501.10870v1",
        "abstract": "When concept shifts and sample scarcity are present in the target domain of\ninterest, nonparametric regression learners often struggle to generalize\neffectively. The technique of transfer learning remedies these issues by\nleveraging data or pre-trained models from similar source domains. While\nexisting generalization analyses of kernel-based transfer learning typically\nrely on correctly specified models, we present a transfer learning procedure\nthat is robust against model misspecification while adaptively attaining\noptimality. To facilitate our analysis and avoid the risk of saturation found\nin classical misspecified results, we establish a novel result in the\nmisspecified single-task learning setting, showing that spectral algorithms\nwith fixed bandwidth Gaussian kernels can attain minimax convergence rates\ngiven the true function is in a Sobolev space, which may be of independent\ninterest. Building on this, we derive the adaptive convergence rates of the\nexcess risk for specifying Gaussian kernels in a prevalent class of hypothesis\ntransfer learning algorithms. Our results are minimax optimal up to logarithmic\nfactors and elucidate the key determinants of transfer efficiency."
    },
    {
        "date": "2025-01",
        "title": "A comprehensive survey on RPL routing-based attacks, defences and future directions in Internet of Things",
        "author": "Anil K Prajapati, Emmanuel S Pilli, Ramesh B Battula, Vijay Varadharajan, Abhishek Verma, and R C Joshi",
        "link": "http://arxiv.org/abs/2501.10817v1",
        "abstract": "The Internet of Things (IoT) is a network of digital devices like sensors,\nprocessors, embedded and communication devices that can connect to and exchange\ndata with other devices and systems over the internet. IoT devices have\nlimitations on power, memory, and computational resources. Researchers have\ndeveloped the IPv6 Over Low-power Wireless Personal Area Network (6LoWPAN)\nprotocols to provide wireless connectivity among these devices while overcoming\nthe constraints on resources. 6LoWPAN has been approved subsequently by the\nInternet Engineering Task Force (IETF). The IETF Routing Over Low-power and\nLossy Networks (ROLL) standardized the Routing Protocol for LLNs known as RPL\n(IETF RFC 6550), which is part of the 6LoWPAN stack. However, IoT devices are\nvulnerable to various attacks on RPL-based routing. This survey provides an in\ndepth study of existing RPL-based attacks and defense published from year 2011\nto 2024 from highly reputed journals and conferences. By thematic analysis of\nexisting routing attacks on RPL, we developed a novel attack taxonomy which\nfocuses on the nature of routing attacks and classifies them into 12 major\ncategories. Subsequently, the impact of each attack on the network is analyzed\nand discussed real life scenarios of these attacks. Another contribution of\nthis survey proposed a novel taxonomy for classification of defense mechanisms\ninto 8 major categories against routing attacks based on type of defense\nstrategy. The detailed analysis of each defense mechanism with real life\napplicability is explained. Furthermore, evaluation tools such as testbeds and\nsimulators for RPL-based attack and defense are discussed and critically\nanalyzed in terms of real world applicability. Finally, open research\nchallenges are presented on the basis of research gaps of existing literature\nalong with research directions for practitioners and researchers."
    },
    {
        "date": "2025-01",
        "title": "Robust Local Polynomial Regression with Similarity Kernels",
        "author": "Yaniv Shulman",
        "link": "http://arxiv.org/abs/2501.10729v1",
        "abstract": "Local Polynomial Regression (LPR) is a widely used nonparametric method for\nmodeling complex relationships due to its flexibility and simplicity. It\nestimates a regression function by fitting low-degree polynomials to localized\nsubsets of the data, weighted by proximity. However, traditional LPR is\nsensitive to outliers and high-leverage points, which can significantly affect\nestimation accuracy. This paper revisits the kernel function used to compute\nregression weights and proposes a novel framework that incorporates both\npredictor and response variables in the weighting mechanism. By introducing two\npositive definite kernels, the proposed method robustly estimates weights,\nmitigating the influence of outliers through localized density estimation. The\nmethod is implemented in Python and is publicly available at\nhttps://github.com/yaniv-shulman/rsklpr, demonstrating competitive performance\nin synthetic benchmark experiments. Compared to standard LPR, the proposed\napproach consistently improves robustness and accuracy, especially in\nheteroscedastic and noisy environments, without requiring multiple iterations.\nThis advancement provides a promising extension to traditional LPR, opening new\npossibilities for robust regression applications."
    },
    {
        "date": "2025-01",
        "title": "Distributionally Robust Policy Evaluation and Learning for Continuous Treatment with Observational Data",
        "author": "Cheuk Hang Leung, Yiyan Huang, Yijun Li, and Qi Wu",
        "link": "http://arxiv.org/abs/2501.10693v1",
        "abstract": "Using offline observational data for policy evaluation and learning allows\ndecision-makers to evaluate and learn a policy that connects characteristics\nand interventions. Most existing literature has focused on either discrete\ntreatment spaces or assumed no difference in the distributions between the\npolicy-learning and policy-deployed environments. These restrict applications\nin many real-world scenarios where distribution shifts are present with\ncontinuous treatment. To overcome these challenges, this paper focuses on\ndeveloping a distributionally robust policy under a continuous treatment\nsetting. The proposed distributionally robust estimators are established using\nthe Inverse Probability Weighting (IPW) method extended from the discrete one\nfor policy evaluation and learning under continuous treatments. Specifically,\nwe introduce a kernel function into the proposed IPW estimator to mitigate the\nexclusion of observations that can occur in the standard IPW method to\ncontinuous treatments. We then provide finite-sample analysis that guarantees\nthe convergence of the proposed distributionally robust policy evaluation and\nlearning estimators. The comprehensive experiments further verify the\neffectiveness of our approach when distribution shifts are present."
    },
    {
        "date": "2025-01",
        "title": "Latent-space adversarial training with post-aware calibration for defending large language models against jailbreak attacks",
        "author": "Xin Yi, Yue Li, Linlin Wang, Xiaoling Wang, and Liang He",
        "link": "http://arxiv.org/abs/2501.10639v1",
        "abstract": "Ensuring safety alignment has become a critical requirement for large\nlanguage models (LLMs), particularly given their widespread deployment in\nreal-world applications. However, LLMs remain susceptible to jailbreak attacks,\nwhich exploit system vulnerabilities to bypass safety measures and generate\nharmful outputs. Although numerous defense mechanisms based on adversarial\ntraining have been proposed, a persistent challenge lies in the exacerbation of\nover-refusal behaviors, which compromise the overall utility of the model. To\naddress these challenges, we propose a Latent-space Adversarial Training with\nPost-aware Calibration (LATPC) framework. During the adversarial training\nphase, LATPC compares harmful and harmless instructions in the latent space and\nextracts safety-critical dimensions to construct refusal features attack,\nprecisely simulating agnostic jailbreak attack types requiring adversarial\nmitigation. At the inference stage, an embedding-level calibration mechanism is\nemployed to alleviate over-refusal behaviors with minimal computational\noverhead. Experimental results demonstrate that, compared to various defense\nmethods across five types of jailbreak attacks, LATPC framework achieves a\nsuperior balance between safety and utility. Moreover, our analysis underscores\nthe effectiveness of extracting safety-critical dimensions from the latent\nspace for constructing robust refusal feature attacks."
    },
    {
        "date": "2025-01",
        "title": "Differentiable Adversarial Attacks for Marked Temporal Point Processes",
        "author": "Pritish Chakraborty, Vinayak Gupta, Rahul R, Srikanta J. Bedathur, and Abir De",
        "link": "http://arxiv.org/abs/2501.10606v1",
        "abstract": "Marked temporal point processes (MTPPs) have been shown to be extremely\neffective in modeling continuous time event sequences (CTESs). In this work, we\npresent adversarial attacks designed specifically for MTPP models. A key\ncriterion for a good adversarial attack is its imperceptibility. For objects\nsuch as images or text, this is often achieved by bounding perturbation in some\nfixed $L_p$ norm-ball. However, similarly minimizing distance norms between two\nCTESs in the context of MTPPs is challenging due to their sequential nature and\nvarying time-scales and lengths. We address this challenge by first permuting\nthe events and then incorporating the additive noise to the arrival timestamps.\nHowever, the worst case optimization of such adversarial attacks is a hard\ncombinatorial problem, requiring exploration across a permutation space that is\nfactorially large in the length of the input sequence. As a result, we propose\na novel differentiable scheme PERMTPP using which we can perform adversarial\nattacks by learning to minimize the likelihood, while minimizing the distance\nbetween two CTESs. Our experiments on four real-world datasets demonstrate the\noffensive and defensive capabilities, and lower inference times of PERMTPP."
    },
    {
        "date": "2025-01",
        "title": "Picachv: Formally Verified Data Use Policy Enforcement for Secure Data Analytics",
        "author": "Haobin Hiroki Chen, Hongbo Chen, Mingshen Sun, Chenghong Wang, and XiaoFeng Wang",
        "link": "http://arxiv.org/abs/2501.10560v1",
        "abstract": "Ensuring the proper use of sensitive data in analytics under complex privacy\npolicies is an increasingly critical challenge. Many existing approaches lack\nportability, verifiability, and scalability across diverse data processing\nframeworks. We introduce Picachv, a novel security monitor that automatically\nenforces data use policies. It works on relational algebra as an abstraction\nfor program semantics, enabling policy enforcement on query plans generated by\nprograms during execution. This approach simplifies analysis across diverse\nanalytical operations and supports various front-end query languages. By\nformalizing both data use policies and relational algebra semantics in Coq, we\nprove that Picachv correctly enforces policies. Picachv also leverages Trusted\nExecution Environments (TEEs) to enhance trust in runtime, providing provable\npolicy compliance to stakeholders that the analytical tasks comply with their\ndata use policies. We integrated Picachv into Polars, a state-of-the-art data\nanalytics framework, and evaluate its performance using the TPC-H benchmark. We\nalso apply our approach to real-world use cases. Our work demonstrates the\npractical application of formal methods in securing data analytics, addressing\nkey challenges."
    },
    {
        "date": "2025-01",
        "title": "Credit Risk Identification in Supply Chains Using Generative Adversarial Networks",
        "author": "Zizhou Zhang, Xinshi Li, Yu Cheng, Zhenrui Chen, and Qianying Liu",
        "link": "http://arxiv.org/abs/2501.10348v3",
        "abstract": "Credit risk management within supply chains has emerged as a critical\nresearch area due to its significant implications for operational stability and\nfinancial sustainability. The intricate interdependencies among supply chain\nparticipants mean that credit risks can propagate across networks, with impacts\nvarying by industry. This study explores the application of Generative\nAdversarial Networks (GANs) to enhance credit risk identification in supply\nchains. GANs enable the generation of synthetic credit risk scenarios,\naddressing challenges related to data scarcity and imbalanced datasets. By\nleveraging GAN-generated data, the model improves predictive accuracy while\neffectively capturing dynamic and temporal dependencies in supply chain data.\nThe research focuses on three representative industries-manufacturing (steel),\ndistribution (pharmaceuticals), and services (e-commerce) to assess\nindustry-specific credit risk contagion. Experimental results demonstrate that\nthe GAN-based model outperforms traditional methods, including logistic\nregression, decision trees, and neural networks, achieving superior accuracy,\nrecall, and F1 scores. The findings underscore the potential of GANs in\nproactive risk management, offering robust tools for mitigating financial\ndisruptions in supply chains. Future research could expand the model by\nincorporating external market factors and supplier relationships to further\nenhance predictive capabilities. Keywords- Generative Adversarial Networks\n(GANs); Supply Chain Risk; Credit Risk Identification; Machine Learning; Data\nAugmentation"
    },
    {
        "date": "2025-01",
        "title": "Hierarchical Autoregressive Transformers: Combining Byte- and Word-Level Processing for Robust, Adaptable Language Models",
        "author": "Pit Neitemeier, Bj\u00f6rn Deiseroth, Constantin Eichenberg, and Lukas Balles",
        "link": "http://arxiv.org/abs/2501.10322v2",
        "abstract": "Tokenization is a fundamental step in natural language processing, breaking\ntext into units that computational models can process. While learned subword\ntokenizers have become the de-facto standard, they present challenges such as\nlarge vocabularies, limited adaptability to new domains or languages, and\nsensitivity to spelling errors and variations. To overcome these limitations,\nwe investigate a hierarchical architecture for autoregressive language\nmodelling that combines character-level and word-level processing. It employs a\nlightweight character-level encoder to convert character sequences into word\nembeddings, which are then processed by a word-level backbone model and decoded\nback into characters via a compact character-level decoder. This method retains\nthe sequence compression benefits of word-level tokenization without relying on\na rigid, predefined vocabulary. We demonstrate, at scales up to 7 billion\nparameters, that hierarchical transformers match the downstream task\nperformance of subword-tokenizer-based models while exhibiting significantly\ngreater robustness to input perturbations. Additionally, during continued\npretraining on an out-of-domain language, our model trains almost twice as\nfast, achieves superior performance on the target language, and retains more of\nits previously learned knowledge. Hierarchical transformers pave the way for\nNLP systems that are more robust, flexible, and generalizable across languages\nand domains."
    },
    {
        "date": "2025-01",
        "title": "Robust Egoistic Rigid Body Localization",
        "author": "Niclas F\u00fchrling, Giuseppe Thadeu Freitas de Abreu, David Gonz\u00e1lez G., and Osvaldo Gonsa",
        "link": "http://arxiv.org/abs/2501.10219v1",
        "abstract": "We consider a robust and self-reliant (or \"egoistic\") variation of the rigid\nbody localization (RBL) problem, in which a primary rigid body seeks to\nestimate the pose (i.e., location and orientation) of another rigid body (or\n\"target\"), relative to its own, without the assistance of external\ninfrastructure, without prior knowledge of the shape of the target, and taking\ninto account the possibility that the available observations are incomplete.\nThree complementary contributions are then offered for such a scenario. The\nfirst is a method to estimate the translation vector between the center point\nof both rigid bodies, which unlike existing techniques does not require that\nboth objects have the same shape or even the same number of landmark points.\nThis technique is shown to significantly outperform the state-of-the-art (SotA)\nunder complete information, but to be sensitive to data erasures, even when\nenhanced by matrix completion methods. The second contribution, designed to\noffer improved performance in the presence of incomplete information, offers a\nrobust alternative to the latter, at the expense of a slight relative loss\nunder complete information. Finally, the third contribution is a scheme for the\nestimation of the rotation matrix describing the relative orientation of the\ntarget rigid body with respect to the primary. Comparisons of the proposed\nschemes and SotA techniques demonstrate the advantage of the contributed\nmethods in terms of root mean square error (RMSE) performance under fully\ncomplete information and incomplete conditions."
    },
    {
        "date": "2025-01",
        "title": "Provably Safeguarding a Classifier from OOD and Adversarial Samples: an Extreme Value Theory Approach",
        "author": "Nicolas Atienza, Christophe Labreuche, Johanne Cohen, and Michele Sebag",
        "link": "http://arxiv.org/abs/2501.10202v1",
        "abstract": "This paper introduces a novel method, Sample-efficient Probabilistic\nDetection using Extreme Value Theory (SPADE), which transforms a classifier\ninto an abstaining classifier, offering provable protection against\nout-of-distribution and adversarial samples. The approach is based on a\nGeneralized Extreme Value (GEV) model of the training distribution in the\nclassifier's latent space, enabling the formal characterization of OOD samples.\nInterestingly, under mild assumptions, the GEV model also allows for formally\ncharacterizing adversarial samples. The abstaining classifier, which rejects\nsamples based on their assessment by the GEV model, provably avoids OOD and\nadversarial samples. The empirical validation of the approach, conducted on\nvarious neural architectures (ResNet, VGG, and Vision Transformer) and medium\nand large-sized datasets (CIFAR-10, CIFAR-100, and ImageNet), demonstrates its\nfrugality, stability, and efficiency compared to the state of the art."
    },
    {
        "date": "2025-01",
        "title": "Contributions to the Decision Theoretic Foundations of Machine Learning and Robust Statistics under Weakly Structured Information",
        "author": "Christoph Jansen",
        "link": "http://arxiv.org/abs/2501.10195v1",
        "abstract": "This habilitation thesis is cumulative and, therefore, is collecting and\nconnecting research that I (together with several co-authors) have conducted\nover the last few years. Thus, the absolute core of the work is formed by the\nten publications listed on page 5 under the name Contributions 1 to 10. The\nreferences to the complete versions of these articles are also found in this\nlist, making them as easily accessible as possible for readers wishing to dive\ndeep into the different research projects. The chapters following this thesis,\nnamely Parts A to C and the concluding remarks, serve to place the articles in\na larger scientific context, to (briefly) explain their respective content on a\nless formal level, and to highlight some interesting perspectives for future\nresearch in their respective contexts. Naturally, therefore, the following\npresentation has neither the level of detail nor the formal rigor that can\n(hopefully) be found in the papers. The purpose of the following text is to\nprovide the reader an easy and high-level access to this interesting and\nimportant research field as a whole, thereby, advertising it to a broader\naudience."
    },
    {
        "date": "2025-01",
        "title": "Secure Semantic Communication With Homomorphic Encryption",
        "author": "Rui Meng, Dayu Fan, Haixiao Gao, Yifan Yuan, Bizhu Wang, Xiaodong Xu, Mengying Sun, Chen Dong, Xiaofeng Tao, Ping Zhang, and Dusit Niyato",
        "link": "http://arxiv.org/abs/2501.10182v1",
        "abstract": "In recent years, Semantic Communication (SemCom), which aims to achieve\nefficient and reliable transmission of meaning between agents, has garnered\nsignificant attention from both academia and industry. To ensure the security\nof communication systems, encryption techniques are employed to safeguard\nconfidentiality and integrity. However, traditional cryptography-based\nencryption algorithms encounter obstacles when applied to SemCom. Motivated by\nthis, this paper explores the feasibility of applying homomorphic encryption to\nSemCom. Initially, we review the encryption algorithms utilized in mobile\ncommunication systems and analyze the challenges associated with their\napplication to SemCom. Subsequently, we employ scale-invariant feature\ntransform to demonstrate that semantic features can be preserved in homomorphic\nencrypted ciphertext. Based on this finding, we propose a task-oriented SemCom\nscheme secured through homomorphic encryption. We design the privacy preserved\ndeep joint source-channel coding (JSCC) encoder and decoder, and the frequency\nof key updates can be adjusted according to service requirements without\ncompromising transmission performance. Simulation results validate that, when\ncompared to plaintext images, the proposed scheme can achieve almost the same\nclassification accuracy performance when dealing with homomorphic ciphertext\nimages. Furthermore, we provide potential future research directions for\nhomomorphic encrypted SemCom."
    },
    {
        "date": "2025-01",
        "title": "Robotic World Model: A Neural Network Simulator for Robust Policy Optimization in Robotics",
        "author": "Chenhao Li, Andreas Krause, and Marco Hutter",
        "link": "http://arxiv.org/abs/2501.10100v1",
        "abstract": "Learning robust and generalizable world models is crucial for enabling\nefficient and scalable robotic control in real-world environments. In this\nwork, we introduce a novel framework for learning world models that accurately\ncapture complex, partially observable, and stochastic dynamics. The proposed\nmethod employs a dual-autoregressive mechanism and self-supervised training to\nachieve reliable long-horizon predictions without relying on domain-specific\ninductive biases, ensuring adaptability across diverse robotic tasks. We\nfurther propose a policy optimization framework that leverages world models for\nefficient training in imagined environments and seamless deployment in\nreal-world systems. Through extensive experiments, our approach consistently\noutperforms state-of-the-art methods, demonstrating superior autoregressive\nprediction accuracy, robustness to noise, and generalization across\nmanipulation and locomotion tasks. Notably, policies trained with our method\nare successfully deployed on ANYmal D hardware in a zero-shot transfer,\nachieving robust performance with minimal sim-to-real performance loss. This\nwork advances model-based reinforcement learning by addressing the challenges\nof long-horizon prediction, error accumulation, and sim-to-real transfer. By\nproviding a scalable and robust framework, the introduced methods pave the way\nfor adaptive and efficient robotic systems in real-world applications."
    },
    {
        "date": "2025-01",
        "title": "Efficient Simulation of Quantum Secure Multiparty Computation",
        "author": "Kartick Sutradhar",
        "link": "http://arxiv.org/abs/2501.10083v1",
        "abstract": "One of the key characteristics of secure quantum communication is quantum\nsecure multiparty computation. In this paper, we propose a quantum secure\nmultiparty summation (QSMS) protocol that can be applied to many complex\nquantum operations. It is based on the $(t, n)$ threshold approach. We combine\nthe classical and quantum phenomena to make this protocol realistic and secure.\nBecause the current protocols employ the $(n, n)$ threshold approach, which\nrequires all honest players to execute the quantum multiparty summation\nprotocol, they have certain security and efficiency problems. However, we\nemploy a $(t, n)$ threshold approach, which requires the quantum summation\nprotocol to be computed only by $t$ honest players. Our suggested protocol is\nmore economical, practical, and secure than alternative protocols."
    },
    {
        "date": "2025-01",
        "title": "Robust Change Captioning in Remote Sensing: SECOND-CC Dataset and MModalCC Framework",
        "author": "Ali Can Karaca, M. Enes Ozelbas, Saadettin Berber, Orkhan Karimli, Turabi Yildirim, and M. Fatih Amasyali",
        "link": "http://arxiv.org/abs/2501.10075v1",
        "abstract": "Remote sensing change captioning (RSICC) aims to describe changes between\nbitemporal images in natural language. Existing methods often fail under\nchallenges like illumination differences, viewpoint changes, blur effects,\nleading to inaccuracies, especially in no-change regions. Moreover, the images\nacquired at different spatial resolutions and have registration errors tend to\naffect the captions. To address these issues, we introduce SECOND-CC, a novel\nRSICC dataset featuring high-resolution RGB image pairs, semantic segmentation\nmaps, and diverse real-world scenarios. SECOND-CC which contains 6,041 pairs of\nbitemporal RS images and 30,205 sentences describing the differences between\nimages. Additionally, we propose MModalCC, a multimodal framework that\nintegrates semantic and visual data using advanced attention mechanisms,\nincluding Cross-Modal Cross Attention (CMCA) and Multimodal Gated Cross\nAttention (MGCA). Detailed ablation studies and attention visualizations\nfurther demonstrate its effectiveness and ability to address RSICC challenges.\nComprehensive experiments show that MModalCC outperforms state-of-the-art RSICC\nmethods, including RSICCformer, Chg2Cap, and PSNet with +4.6% improvement on\nBLEU4 score and +9.6% improvement on CIDEr score. We will make our dataset and\ncodebase publicly available to facilitate future research at\nhttps://github.com/ChangeCapsInRS/SecondCC"
    },
    {
        "date": "2025-01",
        "title": "Spatiotemporal Prediction of Secondary Crashes by Rebalancing Dynamic and Static Data with Generative Adversarial Networks",
        "author": "Junlan Chen, Yiqun Li, Chenyu Ling, Ziyuan Pu, and Xiucheng Guo",
        "link": "http://arxiv.org/abs/2501.10041v1",
        "abstract": "Data imbalance is a common issue in analyzing and predicting sudden traffic\nevents. Secondary crashes constitute only a small proportion of all crashes.\nThese secondary crashes, triggered by primary crashes, significantly exacerbate\ntraffic congestion and increase the severity of incidents. However, the severe\nimbalance of secondary crash data poses significant challenges for prediction\nmodels, affecting their generalization ability and prediction accuracy.\nExisting methods fail to fully address the complexity of traffic crash data,\nparticularly the coexistence of dynamic and static features, and often struggle\nto effectively handle data samples of varying lengths. Furthermore, most\ncurrent studies predict the occurrence probability and spatiotemporal\ndistribution of secondary crashes separately, lacking an integrated solution.\nTo address these challenges, this study proposes a hybrid model named\nVarFusiGAN-Transformer, aimed at improving the fidelity of secondary crash data\ngeneration and jointly predicting the occurrence and spatiotemporal\ndistribution of secondary crashes. The VarFusiGAN-Transformer model employs\nLong Short-Term Memory (LSTM) networks to enhance the generation of\nmultivariate long-time series data, incorporating a static data generator and\nan auxiliary discriminator to model the joint distribution of dynamic and\nstatic features. In addition, the model's prediction module achieves\nsimultaneous prediction of both the occurrence and spatiotemporal distribution\nof secondary crashes. Compared to existing methods, the proposed model\ndemonstrates superior performance in generating high-fidelity data and\nimproving prediction accuracy."
    },
    {
        "date": "2025-01",
        "title": "CaFA: Cost-aware, Feasible Attacks With Database Constraints Against Neural Tabular Classifiers",
        "author": "Matan Ben-Tov, Daniel Deutch, Nave Frost, and Mahmood Sharif",
        "link": "http://arxiv.org/abs/2501.10013v1",
        "abstract": "This work presents CaFA, a system for Cost-aware Feasible Attacks for\nassessing the robustness of neural tabular classifiers against adversarial\nexamples realizable in the problem space, while minimizing adversaries' effort.\nTo this end, CaFA leverages TabPGD$-$an algorithm we set forth to generate\nadversarial perturbations suitable for tabular data$-$ and incorporates\nintegrity constraints automatically mined by state-of-the-art database methods.\nAfter producing adversarial examples in the feature space via TabPGD, CaFA\nprojects them on the mined constraints, leading, in turn, to better attack\nrealizability. We tested CaFA with three datasets and two architectures and\nfound, among others, that the constraints we use are of higher quality\n(measured via soundness and completeness) than ones employed in prior work.\nMoreover, CaFA achieves higher feasible success rates$-$i.e., it generates\nadversarial examples that are often misclassified while satisfying\nconstraints$-$than prior attacks while simultaneously perturbing few features\nwith lower magnitudes, thus saving effort and improving inconspicuousness. We\nopen-source CaFA, hoping it will serve as a generic system enabling\nmachine-learning engineers to assess their models' robustness against\nrealizable attacks, thus advancing deployed models' trustworthiness."
    },
    {
        "date": "2025-01",
        "title": "Advancing Image Security with Quantum Key Distribution and Multi-Layer Chaotic Encryption for Quantum Resilient Transmission",
        "author": "Tasmin Karim, Md. Shazzad Hossain Shaon, Md. Fahim Sultan, and Mst Shapna Akter",
        "link": "http://arxiv.org/abs/2501.09895v1",
        "abstract": "Quantum security improves cryptographic protocols by applying quantum\nmechanics principles, assuring resistance to both quantum and conventional\ncomputer attacks. This work addresses these issues by integrating Quantum Key\nDistribution (QKD) utilizing the E91 method with Multi-Layer Chaotic\nEncryption, which employs a variety of patterns to detect eavesdropping,\nresulting in a highly secure image-transmission architecture. The method\nleverages entropy calculations to determine the unpredictability and integrity\nof encrypted and decrypted pictures, guaranteeing strong security. Extensive\nstatistical scenarios illustrate the framework's effectiveness in image\nencryption while preserving high entropy and sensitivity to the original\nvisuals. The findings indicate significant improvement in encryption and\ndecryption performance, demonstrating the framework's potential as a robust\nresponse to weaknesses introduced by advances in quantum computing. Several\nmetrics, such as Peak Signal-to-Noise Ratio (PSNR), Structural Similarity Index\n(SSIM), Normalized Cross-Correlation (NCC), Bit Error Rate (BER), entropy\nvalues for original, encrypted, and decrypted images, and the correlation\nbetween original and decrypted images, validate the framework's effectiveness.\nThe combination of QKD with Multi-Layer Chaotic Encryption provides a scalable\nand resilient technique to secure image communication. As quantum computing\nadvances, this framework offers a future-proof approach for defining secure\ncommunication protocols in crucial sectors such as medical treatment, forensic\ncomputing, and national security, where information confidentiality is\nvaluable."
    },
    {
        "date": "2025-01",
        "title": "FLORA: Formal Language Model Enables Robust Training-free Zero-shot Object Referring Analysis",
        "author": "Zhe Chen, and Zijing Chen",
        "link": "http://arxiv.org/abs/2501.09887v1",
        "abstract": "Object Referring Analysis (ORA), commonly known as referring expression\ncomprehension, requires the identification and localization of specific objects\nin an image based on natural descriptions. Unlike generic object detection, ORA\nrequires both accurate language understanding and precise visual localization,\nmaking it inherently more complex. Although recent pre-trained large visual\ngrounding detectors have achieved significant progress, they heavily rely on\nextensively labeled data and time-consuming learning. To address these, we\nintroduce a novel, training-free framework for zero-shot ORA, termed FLORA\n(Formal Language for Object Referring and Analysis). FLORA harnesses the\ninherent reasoning capabilities of large language models (LLMs) and integrates\na formal language model - a logical framework that regulates language within\nstructured, rule-based descriptions - to provide effective zero-shot ORA. More\nspecifically, our formal language model (FLM) enables an effective,\nlogic-driven interpretation of object descriptions without necessitating any\ntraining processes. Built upon FLM-regulated LLM outputs, we further devise a\nBayesian inference framework and employ appropriate off-the-shelf interpretive\nmodels to finalize the reasoning, delivering favorable robustness against LLM\nhallucinations and compelling ORA performance in a training-free manner. In\npractice, our FLORA boosts the zero-shot performance of existing pretrained\ngrounding detectors by up to around 45%. Our comprehensive evaluation across\ndifferent challenging datasets also confirms that FLORA consistently surpasses\ncurrent state-of-the-art zero-shot methods in both detection and segmentation\ntasks associated with zero-shot ORA. We believe our probabilistic parsing and\nreasoning of the LLM outputs elevate the reliability and interpretability of\nzero-shot ORA. We shall release codes upon publication."
    },
    {
        "date": "2025-01",
        "title": "Generalized Single-Image-Based Morphing Attack Detection Using Deep Representations from Vision Transformer",
        "author": "Haoyu Zhang, Raghavendra Ramachandra, Kiran Raja, and Christoph Busch",
        "link": "http://arxiv.org/abs/2501.09817v1",
        "abstract": "Face morphing attacks have posed severe threats to Face Recognition Systems\n(FRS), which are operated in border control and passport issuance use cases.\nCorrespondingly, morphing attack detection algorithms (MAD) are needed to\ndefend against such attacks. MAD approaches must be robust enough to handle\nunknown attacks in an open-set scenario where attacks can originate from\nvarious morphing generation algorithms, post-processing and the diversity of\nprinters/scanners. The problem of generalization is further pronounced when the\ndetection has to be made on a single suspected image. In this paper, we propose\na generalized single-image-based MAD (S-MAD) algorithm by learning the encoding\nfrom Vision Transformer (ViT) architecture. Compared to CNN-based\narchitectures, ViT model has the advantage on integrating local and global\ninformation and hence can be suitable to detect the morphing traces widely\ndistributed among the face region. Extensive experiments are carried out on\nface morphing datasets generated using publicly available FRGC face datasets.\nSeveral state-of-the-art (SOTA) MAD algorithms, including representative ones\nthat have been publicly evaluated, have been selected and benchmarked with our\nViT-based approach. Obtained results demonstrate the improved detection\nperformance of the proposed S-MAD method on inter-dataset testing (when\ndifferent data is used for training and testing) and comparable performance on\nintra-dataset testing (when the same data is used for training and testing)\nexperimental protocol."
    },
    {
        "date": "2025-01",
        "title": "Ruling the Unruly: Designing Effective, Low-Noise Network Intrusion Detection Rules for Security Operations Centers",
        "author": "Koen T. W. Teuwen, Tom Mulders, Emmanuele Zambon, and Luca Allodi",
        "link": "http://arxiv.org/abs/2501.09808v1",
        "abstract": "Many Security Operations Centers (SOCs) today still heavily rely on\nsignature-based Network Intrusion Detection Systems (NIDS) such as Suricata.\nThe specificity of intrusion detection rules and the coverage provided by\nrulesets are common concerns within the professional community surrounding\nSOCs, which impact the effectiveness of automated alert post-processing\napproaches. We postulate a better understanding of factors influencing the\nquality of rules can help address current SOC issues. In this paper, we\ncharacterize the rules in use at a collaborating commercial (managed) SOC\nserving customers in sectors including education and IT management. During this\nprocess, we discover six relevant design principles, which we consolidate\nthrough interviews with experienced rule designers at the SOC.We then validate\nour design principles by quantitatively assessing their effect on rule\nspecificity. We find that several of these design considerations significantly\nimpact unnecessary workload caused by rules. For instance, rules that leverage\nproxies for detection, and rules that do not employ alert throttling or do not\ndistinguish (un)successful malicious actions, cause significantly more workload\nfor SOC analysts. Moreover, rules that match a generalized characteristic to\ndetect malicious behavior, which is believed to increase coverage, also\nsignificantly increase workload, suggesting a tradeoff must be struck between\nrule specificity and coverage. We show that these design principles can be\napplied successfully at a SOC to reduce workload whilst maintaining coverage\ndespite the prevalence of violations of the principles."
    },
    {
        "date": "2025-01",
        "title": "W3ID: A Quantum Computing-Secure Digital Identity System Redefining Standards for Web3 and Digital Twins",
        "author": "Joseph Yun, Eli Lifton, Eunseo Lee, Yohan Yun, Abigail Song, Joshua Lee, Cristian Jimenez-Bert, Benedict Song, Yejun Lee, Alex Seo, and Sijung Yun",
        "link": "http://arxiv.org/abs/2501.09802v1",
        "abstract": "The rapid advancements in quantum computing present significant threats to\nexisting encryption standards and internet security. Simultaneously, the advent\nof Web 3.0 marks a transformative era in internet history, emphasizing enhanced\ndata security, decentralization, and user ownership. This white paper\nintroduces the W3ID, an abbreviation of Web3 standard meeting universal digital\nID, which is a Universal Digital Identity (UDI) model designed to meet Web3\nstandards while addressing vulnerabilities posed by quantum computing. W3ID\ninnovatively generates secure Digital Object Identifiers (DOIs) tailored for\nthe decentralized Web 3.0 ecosystem. Additionally, W3ID employs a dual-key\nsystem for secure authentication, enhancing both public and private\nverification mechanisms. To further enhance encryption strength and\nauthentication integrity in the quantum computing era, W3ID incorporates an\nadvanced security mechanism. By requiring quadruple application of SHA-256,\nwith consecutive matches for validation, the system expands the number of\npossibilities to 256^4, which is approximately 4.3 billion times the current\nSHA-256 capacity. This dramatic increase in computational complexity ensures\nthat even advanced quantum computing systems would face significant challenges\nin executing brute-force attacks. W3ID redefines digital identity standards for\nWeb 3.0 and the quantum computing era, setting a new benchmark for security,\nscalability, and decentralization in the global digital twin ecosystem."
    },
    {
        "date": "2025-01",
        "title": "Unified Face Matching and Physical-Digital Spoofing Attack Detection",
        "author": "Arun Kunwar, and Ajita Rattani",
        "link": "http://arxiv.org/abs/2501.09635v1",
        "abstract": "Face recognition technology has dramatically transformed the landscape of\nsecurity, surveillance, and authentication systems, offering a user-friendly\nand non-invasive biometric solution. However, despite its significant\nadvantages, face recognition systems face increasing threats from physical and\ndigital spoofing attacks. Current research typically treats face recognition\nand attack detection as distinct classification challenges. This approach\nnecessitates the implementation of separate models for each task, leading to\nconsiderable computational complexity, particularly on devices with limited\nresources. Such inefficiencies can stifle scalability and hinder performance.\nIn response to these challenges, this paper introduces an innovative unified\nmodel designed for face recognition and detection of physical and digital\nattacks. By leveraging the advanced Swin Transformer backbone and incorporating\nHiLo attention in a convolutional neural network framework, we address unified\nface recognition and spoof attack detection more effectively. Moreover, we\nintroduce augmentation techniques that replicate the traits of physical and\ndigital spoofing cues, significantly enhancing our model robustness. Through\ncomprehensive experimental evaluation across various datasets, we showcase the\neffectiveness of our model in unified face recognition and spoof detection.\nAdditionally, we confirm its resilience against unseen physical and digital\nspoofing attacks, underscoring its potential for real-world applications."
    },
    {
        "date": "2025-01",
        "title": "Weight for Robustness: A Comprehensive Approach towards Optimal Fault-Tolerant Asynchronous ML",
        "author": "Tehila Dahan, and Kfir Y. Levy",
        "link": "http://arxiv.org/abs/2501.09621v1",
        "abstract": "We address the challenges of Byzantine-robust training in asynchronous\ndistributed machine learning systems, aiming to enhance efficiency amid massive\nparallelization and heterogeneous computing resources. Asynchronous systems,\nmarked by independently operating workers and intermittent updates, uniquely\nstruggle with maintaining integrity against Byzantine failures, which encompass\nmalicious or erroneous actions that disrupt learning. The inherent delays in\nsuch settings not only introduce additional bias to the system but also obscure\nthe disruptions caused by Byzantine faults. To tackle these issues, we adapt\nthe Byzantine framework to asynchronous dynamics by introducing a novel\nweighted robust aggregation framework. This allows for the extension of robust\naggregators and a recent meta-aggregator to their weighted versions, mitigating\nthe effects of delayed updates. By further incorporating a recent\nvariance-reduction technique, we achieve an optimal convergence rate for the\nfirst time in an asynchronous Byzantine environment. Our methodology is\nrigorously validated through empirical and theoretical analysis, demonstrating\nits effectiveness in enhancing fault tolerance and optimizing performance in\nasynchronous ML systems."
    },
    {
        "date": "2025-01",
        "title": "Adversarial-Ensemble Kolmogorov Arnold Networks for Enhancing Indoor Wi-Fi Positioning: A Defensive Approach Against Spoofing and Signal Manipulation Attacks",
        "author": "Mitul Goswami, Romit Chatterjee, Somnath Mahato, and Prasant Kumar Pattnaik",
        "link": "http://arxiv.org/abs/2501.09609v1",
        "abstract": "The research presents a study on enhancing the robustness of Wi-Fi-based\nindoor positioning systems against adversarial attacks. The goal is to improve\nthe positioning accuracy and resilience of these systems under two attack\nscenarios: Wi-Fi Spoofing and Signal Strength Manipulation. Three models are\ndeveloped and evaluated: a baseline model (M_Base), an adversarially trained\nrobust model (M_Rob), and an ensemble model (M_Ens). All models utilize a\nKolmogorov-Arnold Network (KAN) architecture. The robust model is trained with\nadversarially perturbed data, while the ensemble model combines predictions\nfrom both the base and robust models. Experimental results show that the robust\nmodel reduces positioning error by approximately 10% compared to the baseline,\nachieving 2.03 meters error under Wi-Fi spoofing and 2.00 meters under signal\nstrength manipulation. The ensemble model further outperforms with errors of\n2.01 meters and 1.975 meters for the respective attack types. This analysis\nhighlights the effectiveness of adversarial training techniques in mitigating\nattack impacts. The findings underscore the importance of considering\nadversarial scenarios in developing indoor positioning systems, as improved\nresilience can significantly enhance the accuracy and reliability of such\nsystems in mission-critical environments."
    },
    {
        "date": "2025-01",
        "title": "Normal-NeRF: Ambiguity-Robust Normal Estimation for Highly Reflective Scenes",
        "author": "Ji Shi, Xianghua Ying, Ruohao Guo, Bowei Xing, and Wenzhen Yue",
        "link": "http://arxiv.org/abs/2501.09460v1",
        "abstract": "Neural Radiance Fields (NeRF) often struggle with reconstructing and\nrendering highly reflective scenes. Recent advancements have developed various\nreflection-aware appearance models to enhance NeRF's capability to render\nspecular reflections. However, the robust reconstruction of highly reflective\nscenes is still hindered by the inherent shape ambiguity on specular surfaces.\nExisting methods typically rely on additional geometry priors to regularize the\nshape prediction, but this can lead to oversmoothed geometry in complex scenes.\nObserving the critical role of surface normals in parameterizing reflections,\nwe introduce a transmittance-gradient-based normal estimation technique that\nremains robust even under ambiguous shape conditions. Furthermore, we propose a\ndual activated densities module that effectively bridges the gap between smooth\nsurface normals and sharp object boundaries. Combined with a reflection-aware\nappearance model, our proposed method achieves robust reconstruction and\nhigh-fidelity rendering of scenes featuring both highly specular reflections\nand intricate geometric structures. Extensive experiments demonstrate that our\nmethod outperforms existing state-of-the-art methods on various datasets."
    },
    {
        "date": "2025-01",
        "title": "Double Visual Defense: Adversarial Pre-training and Instruction Tuning for Improving Vision-Language Model Robustness",
        "author": "Zeyu Wang, Cihang Xie, Brian Bartoldson, and Bhavya Kailkhura",
        "link": "http://arxiv.org/abs/2501.09446v1",
        "abstract": "This paper investigates the robustness of vision-language models against\nadversarial visual perturbations and introduces a novel ``double visual\ndefense\" to enhance this robustness. Unlike previous approaches that resort to\nlightweight adversarial fine-tuning of a pre-trained CLIP model, we perform\nlarge-scale adversarial vision-language pre-training from scratch using\nweb-scale data. We then strengthen the defense by incorporating adversarial\nvisual instruction tuning. The resulting models from each stage, $\\Delta$CLIP\nand $\\Delta^2$LLaVA, show substantially enhanced zero-shot robustness and set a\nnew state-of-the-art in adversarial defense for vision-language models. For\nexample, the adversarial robustness of $\\Delta$CLIP surpasses that of the\nprevious best models on ImageNet-1k by ~20%. %For example, $\\Delta$CLIP\nsurpasses the previous best models on ImageNet-1k by ~20% in terms of\nadversarial robustness. Similarly, compared to prior art, $\\Delta^2$LLaVA\nbrings a ~30% robustness improvement to image captioning task and a ~20%\nrobustness improvement to visual question answering task. Furthermore, our\nmodels exhibit stronger zero-shot recognition capability, fewer hallucinations,\nand superior reasoning performance compared to baselines. Our project page is\nhttps://doublevisualdefense.github.io/."
    },
    {
        "date": "2025-01",
        "title": "Towards Robust and Realistic Human Pose Estimation via WiFi Signals",
        "author": "Yang Chen, Jingcai Guo, Song Guo, Jingren Zhou, and Dacheng Tao",
        "link": "http://arxiv.org/abs/2501.09411v2",
        "abstract": "Robust WiFi-based human pose estimation is a challenging task that bridges\ndiscrete and subtle WiFi signals to human skeletons. This paper revisits this\nproblem and reveals two critical yet overlooked issues: 1) cross-domain gap,\ni.e., due to significant variations between source-target domain pose\ndistributions; and 2) structural fidelity gap, i.e., predicted skeletal poses\nmanifest distorted topology, usually with misplaced joints and disproportionate\nbone lengths. This paper fills these gaps by reformulating the task into a\nnovel two-phase framework dubbed DT-Pose: Domain-consistent representation\nlearning and Topology-constrained Pose decoding. Concretely, we first propose a\ntemporal-consistent contrastive learning strategy with uniformity\nregularization, coupled with self-supervised masking-reconstruction operations,\nto enable robust learning of domain-consistent and motion-discriminative\nWiFi-specific representations. Beyond this, we introduce a simple yet effective\npose decoder with task prompts, which integrates Graph Convolution Network\n(GCN) and Transformer layers to constrain the topology structure of the\ngenerated skeleton by exploring the adjacent-overarching relationships among\nhuman joints. Extensive experiments conducted on various benchmark datasets\nhighlight the superior performance of our method in tackling these fundamental\nchallenges in both 2D/3D human pose estimation tasks."
    },
    {
        "date": "2025-01",
        "title": "Quantum-Enhanced Transformers for Robust Acoustic Scene Classification in IoT Environments",
        "author": "Minh K. Quan, Mayuri Wijayasundara, Sujeeva Setunge, and Pubudu N. Pathirana",
        "link": "http://arxiv.org/abs/2501.09394v1",
        "abstract": "The proliferation of Internet of Things (IoT) devices equipped with acoustic\nsensors necessitates robust acoustic scene classification (ASC) capabilities,\neven in noisy and data-limited environments. Traditional machine learning\nmethods often struggle to generalize effectively under such conditions. To\naddress this, we introduce Q-ASC, a novel Quantum-Inspired Acoustic Scene\nClassifier that leverages the power of quantum-inspired transformers. By\nintegrating quantum concepts like superposition and entanglement, Q-ASC\nachieves superior feature learning and enhanced noise resilience compared to\nclassical models. Furthermore, we introduce a Quantum Variational Autoencoder\n(QVAE) based data augmentation technique to mitigate the challenge of limited\nlabeled data in IoT deployments. Extensive evaluations on the Tampere\nUniversity of Technology (TUT) Acoustic Scenes 2016 benchmark dataset\ndemonstrate that Q-ASC achieves remarkable accuracy between 68.3% and 88.5%\nunder challenging conditions, outperforming state-of-the-art methods by over 5%\nin the best case. This research paves the way for deploying intelligent\nacoustic sensing in IoT networks, with potential applications in smart homes,\nindustrial monitoring, and environmental surveillance, even in adverse acoustic\nenvironments."
    },
    {
        "date": "2025-01",
        "title": "Neural Honeytrace: A Robust Plug-and-Play Watermarking Framework against Model Extraction Attacks",
        "author": "Yixiao Xu, Binxing Fang, Rui Wang, Yinghai Zhou, Shouling Ji, Yuan Liu, Mohan Li, and Zhihong Tian",
        "link": "http://arxiv.org/abs/2501.09328v2",
        "abstract": "Developing high-performance deep learning models is resource-intensive,\nleading model owners to utilize Machine Learning as a Service (MLaaS) platforms\ninstead of publicly releasing their models. However, malicious users may\nexploit query interfaces to execute model extraction attacks, reconstructing\nthe target model's functionality locally. While prior research has investigated\ntriggerable watermarking techniques for asserting ownership, existing methods\nface significant challenges: (1) most approaches require additional training,\nresulting in high overhead and limited flexibility, and (2) they often fail to\naccount for advanced attackers, leaving them vulnerable to adaptive attacks.\n  In this paper, we propose Neural Honeytrace, a robust plug-and-play\nwatermarking framework against model extraction attacks. We first formulate a\nwatermark transmission model from an information-theoretic perspective,\nproviding an interpretable account of the principles and limitations of\nexisting triggerable watermarking. Guided by the model, we further introduce:\n(1) a similarity-based training-free watermarking method for plug-and-play and\nflexible watermarking, and (2) a distribution-based multi-step watermark\ninformation transmission strategy for robust watermarking. Comprehensive\nexperiments on four datasets demonstrate that Neural Honeytrace outperforms\nprevious methods in efficiency and resisting adaptive attacks. Neural\nHoneytrace reduces the average number of samples required for a worst-case\nt-Test-based copyright claim from $12,000$ to $200$ with zero training cost."
    },
    {
        "date": "2025-01",
        "title": "Cooperative Decentralized Backdoor Attacks on Vertical Federated Learning",
        "author": "Seohyun Lee, Wenzhi Fang, Anindya Bijoy Das, Seyyedali Hosseinalipour, David J. Love, and Christopher G. Brinton",
        "link": "http://arxiv.org/abs/2501.09320v1",
        "abstract": "Federated learning (FL) is vulnerable to backdoor attacks, where adversaries\nalter model behavior on target classification labels by embedding triggers into\ndata samples. While these attacks have received considerable attention in\nhorizontal FL, they are less understood for vertical FL (VFL), where devices\nhold different features of the samples, and only the server holds the labels.\nIn this work, we propose a novel backdoor attack on VFL which (i) does not rely\non gradient information from the server and (ii) considers potential collusion\namong multiple adversaries for sample selection and trigger embedding. Our\nlabel inference model augments variational autoencoders with metric learning,\nwhich adversaries can train locally. A consensus process over the adversary\ngraph topology determines which datapoints to poison. We further propose\nmethods for trigger splitting across the adversaries, with an intensity-based\nimplantation scheme skewing the server towards the trigger. Our convergence\nanalysis reveals the impact of backdoor perturbations on VFL indicated by a\nstationarity gap for the trained model, which we verify empirically as well. We\nconduct experiments comparing our attack with recent backdoor VFL approaches,\nfinding that ours obtains significantly higher success rates for the same main\ntask performance despite not using server information. Additionally, our\nresults verify the impact of collusion on attack performance."
    },
    {
        "date": "2025-01",
        "title": "Clone-Robust AI Alignment",
        "author": "Ariel D. Procaccia, Benjamin Schiffer, and Shirley Zhang",
        "link": "http://arxiv.org/abs/2501.09254v1",
        "abstract": "A key challenge in training Large Language Models (LLMs) is properly aligning\nthem with human preferences. Reinforcement Learning with Human Feedback (RLHF)\nuses pairwise comparisons from human annotators to train reward functions and\nhas emerged as a popular alignment method. However, input datasets in RLHF are\nnot necessarily balanced in the types of questions and answers that are\nincluded. Therefore, we want RLHF algorithms to perform well even when the set\nof alternatives is not uniformly distributed. Drawing on insights from social\nchoice theory, we introduce robustness to approximate clones, a desirable\nproperty of RLHF algorithms which requires that adding near-duplicate\nalternatives does not significantly change the learned reward function. We\nfirst demonstrate that the standard RLHF algorithm based on regularized maximum\nlikelihood estimation (MLE) fails to satisfy this property. We then propose the\nweighted MLE, a new RLHF algorithm that modifies the standard regularized MLE\nby weighting alternatives based on their similarity to other alternatives. This\nnew algorithm guarantees robustness to approximate clones while preserving\ndesirable theoretical properties."
    },
    {
        "date": "2025-01",
        "title": "Practical Spoofing Attacks on Galileo Open Service Navigation Message Authentication",
        "author": "Haiyang Wang, Yuanyu Zhang, Xinghui Zhu, Ji He, Shuangtrui Zhao, Yulong Shen, and Xiaohong Jiang",
        "link": "http://arxiv.org/abs/2501.09246v1",
        "abstract": "This paper examines the Galileo Open Service Navigation Message\nAuthentication (OSNMA) and, for the first time, discovers two critical\nvulnerabilities, namely artificially-manipulated time synchronization (ATS) and\ninterruptible message authentication (IMA). ATS allows attackers falsify a\nreceiver's signals and/or local reference time (LRT) while still fulfilling the\ntime synchronization (TS) requirement. IMA allows temporary interruption of the\nnavigation data authentication process due to the reception of a broken message\n(probably caused by spoofing attacks) and restores the authentication later. By\nexploiting the ATS vulnerability, we propose a TS-comply replay (TSR) attack\nwith two variants (real-time and non-real-time), where attackers replay signals\nto a victim receiver while strictly complying with the TS rule. We further\npropose a TS-comply forgery (TSF) attack, where attackers first use a\npreviously-disclosed key to forge a message based on the OSNMA protocol, then\ntamper with the vitcim receiver's LRT correspondingly to comply with the TS\nrule and finally transmit the forged message to the receiver. Finally, we\npropose a concatenating replay (CR) attack based on the IMA vulnerability,\nwhere attackers concatenate replayed signals to the victim receiver's signals\nin a way that still enables correct verification of the navigation data in the\nreplayed signals. To validate the effectiveness of the proposed attacks, we\nconduct real-world experiments with a commercial Galileo receiver manufactured\nby Septentrio, two software-defined radio (SDR) devices, open-source\nGalileo-SDR-SIM and OSNMAlib software. The results showed that all the attacks\ncan successfully pass the OSNMA scheme and the TSF attack can spoof receivers\nto arbitrary locations."
    },
    {
        "date": "2025-01",
        "title": "Benchmarking Robustness of Contrastive Learning Models for Medical Image-Report Retrieval",
        "author": "Demetrio Deanda, Yuktha Priya Masupalli, Jeong Yang, Young Lee, Zechun Cao, and Gongbo Liang",
        "link": "http://arxiv.org/abs/2501.09134v1",
        "abstract": "Medical images and reports offer invaluable insights into patient health. The\nheterogeneity and complexity of these data hinder effective analysis. To bridge\nthis gap, we investigate contrastive learning models for cross-domain\nretrieval, which associates medical images with their corresponding clinical\nreports. This study benchmarks the robustness of four state-of-the-art\ncontrastive learning models: CLIP, CXR-RePaiR, MedCLIP, and CXR-CLIP. We\nintroduce an occlusion retrieval task to evaluate model performance under\nvarying levels of image corruption. Our findings reveal that all evaluated\nmodels are highly sensitive to out-of-distribution data, as evidenced by the\nproportional decrease in performance with increasing occlusion levels. While\nMedCLIP exhibits slightly more robustness, its overall performance remains\nsignificantly behind CXR-CLIP and CXR-RePaiR. CLIP, trained on a\ngeneral-purpose dataset, struggles with medical image-report retrieval,\nhighlighting the importance of domain-specific training data. The evaluation of\nthis work suggests that more effort needs to be spent on improving the\nrobustness of these models. By addressing these limitations, we can develop\nmore reliable cross-domain retrieval models for medical applications."
    },
    {
        "date": "2025-01",
        "title": "Salient Information Preserving Adversarial Training Improves Clean and Robust Accuracy",
        "author": "Timothy Redgrave, and Adam Czajka",
        "link": "http://arxiv.org/abs/2501.09086v1",
        "abstract": "In this work we introduce Salient Information Preserving Adversarial Training\n(SIP-AT), an intuitive method for relieving the robustness-accuracy trade-off\nincurred by traditional adversarial training. SIP-AT uses salient image regions\nto guide the adversarial training process in such a way that fragile features\ndeemed meaningful by an annotator remain unperturbed during training, allowing\nmodels to learn highly predictive non-robust features without sacrificing\noverall robustness. This technique is compatible with both human-based and\nautomatically generated salience estimates, allowing SIP-AT to be used as a\npart of human-driven model development without forcing SIP-AT to be reliant\nupon additional human data. We perform experiments across multiple datasets and\narchitectures and demonstrate that SIP-AT is able to boost the clean accuracy\nof models while maintaining a high degree of robustness against attacks at\nmultiple epsilon levels. We complement our central experiments with an\nobservational study measuring the rate at which human subjects successfully\nidentify perturbed images. This study helps build a more intuitive\nunderstanding of adversarial attack strength and demonstrates the heightened\nimportance of low-epsilon robustness. Our results demonstrate the efficacy of\nSIP-AT and provide valuable insight into the risks posed by adversarial samples\nof various strengths."
    },
    {
        "date": "2025-01",
        "title": "Improving Stability Estimates in Adversarial Explainable AI through Alternate Search Methods",
        "author": "Christopher Burger, and Charles Walter",
        "link": "http://arxiv.org/abs/2501.09006v1",
        "abstract": "Advances in the effectiveness of machine learning models have come at the\ncost of enormous complexity resulting in a poor understanding of how they\nfunction. Local surrogate methods have been used to approximate the workings of\nthese complex models, but recent work has revealed their vulnerability to\nadversarial attacks where the explanation produced is appreciably different\nwhile the meaning and structure of the complex model's output remains similar.\nThis prior work has focused on the existence of these weaknesses but not on\ntheir magnitude. Here we explore using an alternate search method with the goal\nof finding minimum viable perturbations, the fewest perturbations necessary to\nachieve a fixed similarity value between the original and altered text's\nexplanation. Intuitively, a method that requires fewer perturbations to expose\na given level of instability is inferior to one which requires more. This\nnuance allows for superior comparisons of the stability of explainability\nmethods."
    },
    {
        "date": "2025-01",
        "title": "Securing the AI Frontier: Urgent Ethical and Regulatory Imperatives for AI-Driven Cybersecurity",
        "author": "Vikram Kulothungan",
        "link": "http://arxiv.org/abs/2501.10467v1",
        "abstract": "This paper critically examines the evolving ethical and regulatory challenges\nposed by the integration of artificial intelligence (AI) in cybersecurity. We\ntrace the historical development of AI regulation, highlighting major\nmilestones from theoretical discussions in the 1940s to the implementation of\nrecent global frameworks such as the European Union AI Act. The current\nregulatory landscape is analyzed, emphasizing risk-based approaches,\nsector-specific regulations, and the tension between fostering innovation and\nmitigating risks. Ethical concerns such as bias, transparency, accountability,\nprivacy, and human oversight are explored in depth, along with their\nimplications for AI-driven cybersecurity systems. Furthermore, we propose\nstrategies for promoting AI literacy and public engagement, essential for\nshaping a future regulatory framework. Our findings underscore the need for a\nunified, globally harmonized regulatory approach that addresses the unique\nrisks of AI in cybersecurity. We conclude by identifying future research\nopportunities and recommending pathways for collaboration between policymakers,\nindustry leaders, and researchers to ensure the responsible deployment of AI\ntechnologies in cybersecurity."
    },
    {
        "date": "2025-01",
        "title": "Improving the Efficiency of Self-Supervised Adversarial Training through Latent Clustering-Based Selection",
        "author": "Somrita Ghosh, Yuelin Xu, and Xiao Zhang",
        "link": "http://arxiv.org/abs/2501.10466v1",
        "abstract": "Compared with standard learning, adversarially robust learning is widely\nrecognized to demand significantly more training examples. Recent works propose\nthe use of self-supervised adversarial training (SSAT) with external or\nsynthetically generated unlabeled data to enhance model robustness. However,\nSSAT requires a substantial amount of extra unlabeled data, significantly\nincreasing memory usage and model training times. To address these challenges,\nwe propose novel methods to strategically select a small subset of unlabeled\ndata essential for SSAT and robustness improvement. Our selection prioritizes\ndata points near the model's decision boundary based on latent clustering-based\ntechniques, efficiently identifying a critical subset of unlabeled data with a\nhigher concentration of boundary-adjacent points. While focusing on\nnear-boundary data, our methods are designed to maintain a balanced ratio\nbetween boundary and non-boundary data points to avoid overfitting. Our\nexperiments on image benchmarks show that integrating our selection strategies\ninto self-supervised adversarial training can largely reduce memory and\ncomputational requirements while achieving high model robustness. In\nparticular, our latent clustering-based selection method with k-means is the\nmost effective, achieving nearly identical test-time robust accuracies with 5\nto 10 times less external or generated unlabeled data when applied to image\nbenchmarks. Additionally, we validate the generalizability of our approach\nacross various application scenarios, including a real-world medical dataset\nfor COVID-19 chest X-ray classification."
    },
    {
        "date": "2025-01",
        "title": "Exploring ChatGPT for Face Presentation Attack Detection in Zero and Few-Shot in-Context Learning",
        "author": "Alain Komaty, Hatef Otroshi Shahreza, Anjith George, and Sebastien Marcel",
        "link": "http://arxiv.org/abs/2501.08799v1",
        "abstract": "This study highlights the potential of ChatGPT (specifically GPT-4o) as a\ncompetitive alternative for Face Presentation Attack Detection (PAD),\noutperforming several PAD models, including commercial solutions, in specific\nscenarios. Our results show that GPT-4o demonstrates high consistency,\nparticularly in few-shot in-context learning, where its performance improves as\nmore examples are provided (reference data). We also observe that detailed\nprompts enable the model to provide scores reliably, a behavior not observed\nwith concise prompts. Additionally, explanation-seeking prompts slightly\nenhance the model's performance by improving its interpretability. Remarkably,\nthe model exhibits emergent reasoning capabilities, correctly predicting the\nattack type (print or replay) with high accuracy in few-shot scenarios, despite\nnot being explicitly instructed to classify attack types. Despite these\nstrengths, GPT-4o faces challenges in zero-shot tasks, where its performance is\nlimited compared to specialized PAD systems. Experiments were conducted on a\nsubset of the SOTERIA dataset, ensuring compliance with data privacy\nregulations by using only data from consenting individuals. These findings\nunderscore GPT-4o's promise in PAD applications, laying the groundwork for\nfuture research to address broader data privacy concerns and improve\ncross-dataset generalization. Code available here:\nhttps://gitlab.idiap.ch/bob/bob.paper.wacv2025_chatgpt_face_pad"
    },
    {
        "date": "2025-01",
        "title": "Multilingual Email Phishing Attacks Detection using OSINT and Machine Learning",
        "author": "Panharith An, Rana Shafi, Tionge Mughogho, and Onyango Allan Onyango",
        "link": "http://arxiv.org/abs/2501.08723v1",
        "abstract": "Email phishing remains a prevalent cyber threat, targeting victims to extract\nsensitive information or deploy malicious software. This paper explores the\nintegration of open-source intelligence (OSINT) tools and machine learning (ML)\nmodels to enhance phishing detection across multilingual datasets. Using Nmap\nand theHarvester, this study extracted 17 features, including domain names, IP\naddresses, and open ports, to improve detection accuracy. Multilingual email\ndatasets, including English and Arabic, were analyzed to address the\nlimitations of ML models trained predominantly on English data. Experiments\nwith five classification algorithms: Decision Tree, Random Forest, Support\nVector Machine, XGBoost, and Multinomial Na\\\"ive Bayes. It revealed that Random\nForest achieved the highest performance, with an accuracy of 97.37% for both\nEnglish and Arabic datasets. For OSINT-enhanced datasets, the model\ndemonstrated an improvement in accuracy compared to baseline models without\nOSINT features. These findings highlight the potential of combining OSINT tools\nwith advanced ML models to detect phishing emails more effectively across\ndiverse languages and contexts. This study contributes an approach to phishing\ndetection by incorporating OSINT features and evaluating their impact on\nmultilingual datasets, addressing a critical gap in cybersecurity research."
    },
    {
        "date": "2025-01",
        "title": "Tag&Tab: Pretraining Data Detection in Large Language Models Using Keyword-Based Membership Inference Attack",
        "author": "Sagiv Antebi, Edan Habler, Asaf Shabtai, and Yuval Elovici",
        "link": "http://arxiv.org/abs/2501.08454v1",
        "abstract": "Large language models (LLMs) have become essential digital task assistance\ntools. Their training relies heavily on the collection of vast amounts of data,\nwhich may include copyright-protected or sensitive information. Recent studies\non the detection of pretraining data in LLMs have primarily focused on\nsentence-level or paragraph-level membership inference attacks (MIAs), usually\ninvolving probability analysis of the target model prediction tokens. However,\nthe proposed methods often demonstrate poor performance, specifically in terms\nof accuracy, failing to account for the semantic importance of textual content\nand word significance. To address these shortcomings, we propose Tag&Tab, a\nnovel approach for detecting data that has been used as part of the LLM\npretraining. Our method leverages advanced natural language processing (NLP)\ntechniques to tag keywords in the input text - a process we term Tagging. Then,\nthe LLM is used to obtain the probabilities of these keywords and calculate\ntheir average log-likelihood to determine input text membership, a process we\nrefer to as Tabbing. Our experiments on three benchmark datasets (BookMIA,\nMIMIR, and the Pile) and several open-source LLMs of varying sizes demonstrate\nan average increase in the AUC scores ranging from 4.1% to 12.1% over\nstate-of-the-art methods. Tag&Tab not only sets a new standard for data leakage\ndetection in LLMs, but its outstanding performance is a testament to the\nimportance of words in MIAs on LLMs."
    },
    {
        "date": "2025-01",
        "title": "Secure Composition of Quantum Key Distribution and Symmetric Key Encryption",
        "author": "Kunal Dey, and Reihaneh Safavi-Naini",
        "link": "http://arxiv.org/abs/2501.08435v1",
        "abstract": "Quantum key distribution (QKD) allows Alice and Bob to share a secret key\nover an insecure channel with proven information-theoretic security against an\nadversary whose strategy is bounded only by the laws of physics.\nComposability-based security proofs of QKD ensure that using the established\nkey with a one-time-pad encryption scheme provides information theoretic\nsecrecy for the message. In this paper, we consider the problem of using the\nQKD established key with a secure symmetric key-based encryption algorithm and\nuse an approach based on hybrid encryption to provide a proof of security for\nthe composition.\n  Hybrid encryption was first proposed as a public key cryptographic algorithm\nwith proven security for messages of unrestricted length. We use an extension\nof this framework to correlated randomness setting (Sharifian et al. in ISIT\n2021) to propose a quantum-enabled Key Encapsulation Mechanism (qKEM) and\nquantum-enabled hybrid encryption (qHE), and prove a composition theorem for\nthe security of the qHE. We construct a qKEM with proven security using an\nexisting QKD (Portmann et al. in Rev. of Mod. Physics 2022). Using this qKEM\nwith a secure Data Encapsulation Mechanism (DEM), that can be constructed using\na one-time symmetric key encryption scheme, results in an efficient encryption\nsystem for unrestricted length messages with proved security against an\nadversary with access to efficient computations on a quantum computer (i.e.\npost-quantum secure encryption without using any computational assumptions.)"
    },
    {
        "date": "2025-01",
        "title": "Cross-Modal Transferable Image-to-Video Attack on Video Quality Metrics",
        "author": "Georgii Gotin, Ekaterina Shumitskaya, Anastasia Antsiferova, and Dmitriy Vatolin",
        "link": "http://arxiv.org/abs/2501.08415v1",
        "abstract": "Recent studies have revealed that modern image and video quality assessment\n(IQA/VQA) metrics are vulnerable to adversarial attacks. An attacker can\nmanipulate a video through preprocessing to artificially increase its quality\nscore according to a certain metric, despite no actual improvement in visual\nquality. Most of the attacks studied in the literature are white-box attacks,\nwhile black-box attacks in the context of VQA have received less attention.\nMoreover, some research indicates a lack of transferability of adversarial\nexamples generated for one model to another when applied to VQA. In this paper,\nwe propose a cross-modal attack method, IC2VQA, aimed at exploring the\nvulnerabilities of modern VQA models. This approach is motivated by the\nobservation that the low-level feature spaces of images and videos are similar.\nWe investigate the transferability of adversarial perturbations across\ndifferent modalities; specifically, we analyze how adversarial perturbations\ngenerated on a white-box IQA model with an additional CLIP module can\neffectively target a VQA model. The addition of the CLIP module serves as a\nvaluable aid in increasing transferability, as the CLIP model is known for its\neffective capture of low-level semantics. Extensive experiments demonstrate\nthat IC2VQA achieves a high success rate in attacking three black-box VQA\nmodels. We compare our method with existing black-box attack strategies,\nhighlighting its superiority in terms of attack success within the same number\nof iterations and levels of attack strength. We believe that the proposed\nmethod will contribute to the deeper analysis of robust VQA metrics."
    },
    {
        "date": "2025-01",
        "title": "Diffusion Adversarial Post-Training for One-Step Video Generation",
        "author": "Shanchuan Lin, Xin Xia, Yuxi Ren, Ceyuan Yang, Xuefeng Xiao, and Lu Jiang",
        "link": "http://arxiv.org/abs/2501.08316v1",
        "abstract": "The diffusion models are widely used for image and video generation, but\ntheir iterative generation process is slow and expansive. While existing\ndistillation approaches have demonstrated the potential for one-step generation\nin the image domain, they still suffer from significant quality degradation. In\nthis work, we propose Adversarial Post-Training (APT) against real data\nfollowing diffusion pre-training for one-step video generation. To improve the\ntraining stability and quality, we introduce several improvements to the model\narchitecture and training procedures, along with an approximated R1\nregularization objective. Empirically, our experiments show that our\nadversarial post-trained model, Seaweed-APT, can generate 2-second, 1280x720,\n24fps videos in real time using a single forward evaluation step. Additionally,\nour model is capable of generating 1024px images in a single step, achieving\nquality comparable to state-of-the-art methods."
    },
    {
        "date": "2025-01",
        "title": "Towards an End-to-End (E2E) Adversarial Learning and Application in the Physical World",
        "author": "Dudi Biton, Jacob Shams, Satoru Koda, Asaf Shabtai, Yuval Elovici, and Ben Nassi",
        "link": "http://arxiv.org/abs/2501.08258v2",
        "abstract": "The traditional learning process of patch-based adversarial attacks,\nconducted in the digital domain and then applied in the physical domain (e.g.,\nvia printed stickers), may suffer from reduced performance due to adversarial\npatches' limited transferability from the digital domain to the physical\ndomain. Given that previous studies have considered using projectors to apply\nadversarial attacks, we raise the following question: can adversarial learning\n(i.e., patch generation) be performed entirely in the physical domain with a\nprojector? In this work, we propose the Physical-domain Adversarial Patch\nLearning Augmentation (PAPLA) framework, a novel end-to-end (E2E) framework\nthat converts adversarial learning from the digital domain to the physical\ndomain using a projector. We evaluate PAPLA across multiple scenarios,\nincluding controlled laboratory settings and realistic outdoor environments,\ndemonstrating its ability to ensure attack success compared to conventional\ndigital learning-physical application (DL-PA) methods. We also analyze the\nimpact of environmental factors, such as projection surface color, projector\nstrength, ambient light, distance, and angle of the target object relative to\nthe camera, on the effectiveness of projected patches. Finally, we demonstrate\nthe feasibility of the attack against a parked car and a stop sign in a\nreal-world outdoor environment. Our results show that under specific\nconditions, E2E adversarial learning in the physical domain eliminates the\ntransferability issue and ensures evasion by object detectors. Finally, we\nprovide insights into the challenges and opportunities of applying adversarial\nlearning in the physical domain and explain where such an approach is more\neffective than using a sticker."
    },
    {
        "date": "2025-01",
        "title": "CWEval: Outcome-driven Evaluation on Functionality and Security of LLM Code Generation",
        "author": "Jinjun Peng, Leyi Cui, Kele Huang, Junfeng Yang, and Baishakhi Ray",
        "link": "http://arxiv.org/abs/2501.08200v1",
        "abstract": "Large Language Models (LLMs) have significantly aided developers by\ngenerating or assisting in code writing, enhancing productivity across various\ntasks. While identifying incorrect code is often straightforward, detecting\nvulnerabilities in functionally correct code is more challenging, especially\nfor developers with limited security knowledge, which poses considerable\nsecurity risks of using LLM-generated code and underscores the need for robust\nevaluation benchmarks that assess both functional correctness and security.\nCurrent benchmarks like CyberSecEval and SecurityEval attempt to solve it but\nare hindered by unclear and impractical specifications, failing to assess both\nfunctionality and security accurately. To tackle these deficiencies, we\nintroduce CWEval, a novel outcome-driven evaluation framework designed to\nenhance the evaluation of secure code generation by LLMs. This framework not\nonly assesses code functionality but also its security simultaneously with\nhigh-quality task specifications and outcome-driven test oracles which provides\nhigh accuracy. Coupled with CWEval-bench, a multilingual, security-critical\ncoding benchmark, CWEval provides a rigorous empirical security evaluation on\nLLM-generated code, overcoming previous benchmarks' shortcomings. Through our\nevaluations, CWEval reveals a notable portion of functional but insecure code\nproduced by LLMs, and shows a serious inaccuracy of previous evaluations,\nultimately contributing significantly to the field of secure code generation.\nWe open-source our artifact at: https://github.com/Co1lin/CWEval ."
    },
    {
        "date": "2025-01",
        "title": "Energy Backdoor Attack to Deep Neural Networks",
        "author": "Hanene F. Z. Brachemi Meftah, Wassim Hamidouche, Sid Ahmed Fezza, Olivier D\u00e9forges, and Kassem Kallas",
        "link": "http://arxiv.org/abs/2501.08152v1",
        "abstract": "The rise of deep learning (DL) has increased computing complexity and energy\nuse, prompting the adoption of application specific integrated circuits (ASICs)\nfor energy-efficient edge and mobile deployment. However, recent studies have\ndemonstrated the vulnerability of these accelerators to energy attacks. Despite\nthe development of various inference time energy attacks in prior research,\nbackdoor energy attacks remain unexplored. In this paper, we design an\ninnovative energy backdoor attack against deep neural networks (DNNs) operating\non sparsity-based accelerators. Our attack is carried out in two distinct\nphases: backdoor injection and backdoor stealthiness. Experimental results\nusing ResNet-18 and MobileNet-V2 models trained on CIFAR-10 and Tiny ImageNet\ndatasets show the effectiveness of our proposed attack in increasing energy\nconsumption on trigger samples while preserving the model's performance for\nclean/regular inputs. This demonstrates the vulnerability of DNNs to energy\nbackdoor attacks. The source code of our attack is available at:\nhttps://github.com/hbrachemi/energy_backdoor."
    },
    {
        "date": "2025-01",
        "title": "RoHan: Robust Hand Detection in Operation Room",
        "author": "Roi Papo, Sapir Gershov, Tom Friedman, Itay Or, Gil Bolotin, and Shlomi Laufer",
        "link": "http://arxiv.org/abs/2501.08115v2",
        "abstract": "Hand-specific localization has garnered significant interest within the\ncomputer vision community. Although there are numerous datasets with hand\nannotations from various angles and settings, domain transfer techniques\nfrequently struggle in surgical environments. This is mainly due to the limited\navailability of gloved hand instances and the unique challenges of operating\nrooms (ORs). Thus, hand-detection models tailored to OR settings require\nextensive training and expensive annotation processes. To overcome these\nchallenges, we present \"RoHan\" - a novel approach for robust hand detection in\nthe OR, leveraging advanced semi-supervised domain adaptation techniques to\ntackle the challenges of varying recording conditions, diverse glove colors,\nand occlusions common in surgical settings. Our methodology encompasses two\nmain stages: (1) data augmentation strategy that utilizes \"Artificial Gloves,\"\na method for augmenting publicly available hand datasets with synthetic images\nof hands-wearing gloves; (2) semi-supervised domain adaptation pipeline that\nimproves detection performance in real-world OR settings through iterative\nprediction refinement and efficient frame filtering. We evaluate our method\nusing two datasets: simulated enterotomy repair and saphenous vein graft\nharvesting. \"RoHan\" substantially reduces the need for extensive labeling and\nmodel training, paving the way for the practical implementation of hand\ndetection technologies in medical settings."
    },
    {
        "date": "2025-01",
        "title": "CellOMaps: A Compact Representation for Robust Classification of Lung Adenocarcinoma Growth Patterns",
        "author": "Arwa Al-Rubaian, Gozde N. Gunesli, Wajd A. Althakfi, Ayesha Azam, David Snead, Nasir M. Rajpoot, and Shan E Ahmed Raza",
        "link": "http://arxiv.org/abs/2501.08094v1",
        "abstract": "Lung adenocarcinoma (LUAD) is a morphologically heterogeneous disease,\ncharacterized by five primary histological growth patterns. The classification\nof such patterns is crucial due to their direct relation to prognosis but the\nhigh subjectivity and observer variability pose a major challenge. Although\nseveral studies have developed machine learning methods for growth pattern\nclassification, they either only report the predominant pattern per slide or\nlack proper evaluation. We propose a generalizable machine learning pipeline\ncapable of classifying lung tissue into one of the five patterns or as\nnon-tumor. The proposed pipeline's strength lies in a novel compact Cell\nOrganization Maps (cellOMaps) representation that captures the cellular spatial\npatterns from Hematoxylin and Eosin whole slide images (WSIs). The proposed\npipeline provides state-of-the-art performance on LUAD growth pattern\nclassification when evaluated on both internal unseen slides and external\ndatasets, significantly outperforming the current approaches. In addition, our\npreliminary results show that the model's outputs can be used to predict\npatients Tumor Mutational Burden (TMB) levels."
    },
    {
        "date": "2025-01",
        "title": "Robust Low-Light Human Pose Estimation through Illumination-Texture Modulation",
        "author": "Feng Zhang, Ze Li, Xiatian Zhu, and Lei Chen",
        "link": "http://arxiv.org/abs/2501.08038v1",
        "abstract": "As critical visual details become obscured, the low visibility and high ISO\nnoise in extremely low-light images pose a significant challenge to human pose\nestimation. Current methods fail to provide high-quality representations due to\nreliance on pixel-level enhancements that compromise semantics and the\ninability to effectively handle extreme low-light conditions for robust feature\nlearning. In this work, we propose a frequency-based framework for low-light\nhuman pose estimation, rooted in the \"divide-and-conquer\" principle. Instead of\nuniformly enhancing the entire image, our method focuses on task-relevant\ninformation. By applying dynamic illumination correction to the low-frequency\ncomponents and low-rank denoising to the high-frequency components, we\neffectively enhance both the semantic and texture information essential for\naccurate pose estimation. As a result, this targeted enhancement method results\nin robust, high-quality representations, significantly improving pose\nestimation performance. Extensive experiments demonstrating its superiority\nover state-of-the-art methods in various challenging low-light scenarios."
    },
    {
        "date": "2025-01",
        "title": "READ: Reinforcement-based Adversarial Learning for Text Classification with Limited Labeled Data",
        "author": "Rohit Sharma, Shanu Kumar, and Avinash Kumar",
        "link": "http://arxiv.org/abs/2501.08035v1",
        "abstract": "Pre-trained transformer models such as BERT have shown massive gains across\nmany text classification tasks. However, these models usually need enormous\nlabeled data to achieve impressive performances. Obtaining labeled data is\noften expensive and time-consuming, whereas collecting unlabeled data using\nsome heuristics is relatively much cheaper for any task. Therefore, this paper\nproposes a method that encapsulates reinforcement learning-based text\ngeneration and semi-supervised adversarial learning approaches in a novel way\nto improve the model's performance. Our method READ, Reinforcement-based\nAdversarial learning, utilizes an unlabeled dataset to generate diverse\nsynthetic text through reinforcement learning, improving the model's\ngeneralization capability using adversarial learning. Our experimental results\nshow that READ outperforms the existing state-of-art methods on multiple\ndatasets."
    },
    {
        "date": "2025-01",
        "title": "Self-Instruct Few-Shot Jailbreaking: Decompose the Attack into Pattern and Behavior Learning",
        "author": "Jiaqi Hua, and Wanxu Wei",
        "link": "http://arxiv.org/abs/2501.07959v1",
        "abstract": "Recently, several works have been conducted on jailbreaking Large Language\nModels (LLMs) with few-shot malicious demos. In particular, Zheng et al. (2024)\nfocuses on improving the efficiency of Few-Shot Jailbreaking (FSJ) by injecting\nspecial tokens into the demos and employing demo-level random search.\nNevertheless, this method lacks generality since it specifies the\ninstruction-response structure. Moreover, the reason why inserting special\ntokens takes effect in inducing harmful behaviors is only empirically\ndiscussed. In this paper, we take a deeper insight into the mechanism of\nspecial token injection and propose Self-Instruct Few-Shot Jailbreaking\n(Self-Instruct-FSJ) facilitated with the demo-level greedy search. This\nframework decomposes the FSJ attack into pattern and behavior learning to\nexploit the model's vulnerabilities in a more generalized and efficient way. We\nconduct elaborate experiments to evaluate our method on common open-source\nmodels and compare it with baseline algorithms. Our code is available at\nhttps://github.com/iphosi/Self-Instruct-FSJ."
    },
    {
        "date": "2025-01",
        "title": "Robust Hyperspectral Image Panshapring via Sparse Spatial-Spectral Representation",
        "author": "Chia-Ming Lee, Yu-Fan Lin, Li-Wei Kang, and Chih-Chung Hsu",
        "link": "http://arxiv.org/abs/2501.07953v1",
        "abstract": "High-resolution hyperspectral imaging plays a crucial role in various remote\nsensing applications, yet its acquisition often faces fundamental limitations\ndue to hardware constraints. This paper introduces S$^{3}$RNet, a novel\nframework for hyperspectral image pansharpening that effectively combines\nlow-resolution hyperspectral images (LRHSI) with high-resolution multispectral\nimages (HRMSI) through sparse spatial-spectral representation. The core of\nS$^{3}$RNet is the Multi-Branch Fusion Network (MBFN), which employs parallel\nbranches to capture complementary features at different spatial and spectral\nscales. Unlike traditional approaches that treat all features equally, our\nSpatial-Spectral Attention Weight Block (SSAWB) dynamically adjusts feature\nweights to maintain sparse representation while suppressing noise and\nredundancy. To enhance feature propagation, we incorporate the Dense Feature\nAggregation Block (DFAB), which efficiently aggregates inputted features\nthrough dense connectivity patterns. This integrated design enables S$^{3}$RNet\nto selectively emphasize the most informative features from differnt scale\nwhile maintaining computational efficiency. Comprehensive experiments\ndemonstrate that S$^{3}$RNet achieves state-of-the-art performance across\nmultiple evaluation metrics, showing particular strength in maintaining high\nreconstruction quality even under challenging noise conditions. The code will\nbe made publicly available."
    },
    {
        "date": "2025-01",
        "title": "Gandalf the Red: Adaptive Security for LLMs",
        "author": "Niklas Pfister, V\u00e1clav Volhejn, Manuel Knott, Santiago Arias, Julia Bazi\u0144ska, Mykhailo Bichurin, Alan Commike, Janet Darling, Peter Dienes, Matthew Fiedler, David Haber, Matthias Kraft, Marco Lancini, Max Mathys, Dami\u00e1n Pascual-Ortiz, Jakub Podolak, Adri\u00e0 Romero-L\u00f3pez, Kyriacos Shiarlis, Andreas Signer, Zsolt Terek, Athanasios Theocharis, Daniel Timbrell, Samuel Trautwein, Samuel Watts, Natalie Wu, and Mateo Rojas-Carulla",
        "link": "http://arxiv.org/abs/2501.07927v1",
        "abstract": "Current evaluations of defenses against prompt attacks in large language\nmodel (LLM) applications often overlook two critical factors: the dynamic\nnature of adversarial behavior and the usability penalties imposed on\nlegitimate users by restrictive defenses. We propose D-SEC (Dynamic Security\nUtility Threat Model), which explicitly separates attackers from legitimate\nusers, models multi-step interactions, and rigorously expresses the\nsecurity-utility in an optimizable form. We further address the shortcomings in\nexisting evaluations by introducing Gandalf, a crowd-sourced, gamified\nred-teaming platform designed to generate realistic, adaptive attack datasets.\nUsing Gandalf, we collect and release a dataset of 279k prompt attacks.\nComplemented by benign user data, our analysis reveals the interplay between\nsecurity and utility, showing that defenses integrated in the LLM (e.g., system\nprompts) can degrade usability even without blocking requests. We demonstrate\nthat restricted application domains, defense-in-depth, and adaptive defenses\nare effective strategies for building secure and useful LLM applications. Code\nis available at\n\\href{https://github.com/lakeraai/dsec-gandalf}{\\texttt{https://github.com/lakeraai/dsec-gandalf}}."
    },
    {
        "date": "2025-01",
        "title": "VENOM: Text-driven Unrestricted Adversarial Example Generation with Diffusion Models",
        "author": "Hui Kuurila-Zhang, Haoyu Chen, and Guoying Zhao",
        "link": "http://arxiv.org/abs/2501.07922v1",
        "abstract": "Adversarial attacks have proven effective in deceiving machine learning\nmodels by subtly altering input images, motivating extensive research in recent\nyears. Traditional methods constrain perturbations within $l_p$-norm bounds,\nbut advancements in Unrestricted Adversarial Examples (UAEs) allow for more\ncomplex, generative-model-based manipulations. Diffusion models now lead UAE\ngeneration due to superior stability and image quality over GANs. However,\nexisting diffusion-based UAE methods are limited to using reference images and\nface challenges in generating Natural Adversarial Examples (NAEs) directly from\nrandom noise, often producing uncontrolled or distorted outputs. In this work,\nwe introduce VENOM, the first text-driven framework for high-quality\nunrestricted adversarial examples generation through diffusion models. VENOM\nunifies image content generation and adversarial synthesis into a single\nreverse diffusion process, enabling high-fidelity adversarial examples without\nsacrificing attack success rate (ASR). To stabilize this process, we\nincorporate an adaptive adversarial guidance strategy with momentum, ensuring\nthat the generated adversarial examples $x^*$ align with the distribution\n$p(x)$ of natural images. Extensive experiments demonstrate that VENOM achieves\nsuperior ASR and image quality compared to prior methods, marking a significant\nadvancement in adversarial example generation and providing insights into model\nvulnerabilities for improved defense development."
    },
    {
        "date": "2025-01",
        "title": "A Comparative Analysis of DNN-based White-Box Explainable AI Methods in Network Security",
        "author": "Osvaldo Arreche, and Mustafa Abdallah",
        "link": "http://arxiv.org/abs/2501.07801v1",
        "abstract": "New research focuses on creating artificial intelligence (AI) solutions for\nnetwork intrusion detection systems (NIDS), drawing its inspiration from the\never-growing number of intrusions on networked systems, increasing its\ncomplexity and intelligibility. Hence, the use of explainable AI (XAI)\ntechniques in real-world intrusion detection systems comes from the requirement\nto comprehend and elucidate black-box AI models to security analysts. In an\neffort to meet such requirements, this paper focuses on applying and evaluating\nWhite-Box XAI techniques (particularly LRP, IG, and DeepLift) for NIDS via an\nend-to-end framework for neural network models, using three widely used network\nintrusion datasets (NSL-KDD, CICIDS-2017, and RoEduNet-SIMARGL2021), assessing\nits global and local scopes, and examining six distinct assessment measures\n(descriptive accuracy, sparsity, stability, robustness, efficiency, and\ncompleteness). We also compare the performance of white-box XAI methods with\nblack-box XAI methods. The results show that using White-box XAI techniques\nscores high in robustness and completeness, which are crucial metrics for IDS.\nMoreover, the source codes for the programs developed for our XAI evaluation\nframework are available to be improved and used by the research community."
    },
    {
        "date": "2025-01",
        "title": "Deep Learning for Disease Outbreak Prediction: A Robust Early Warning Signal for Transcritical Bifurcations",
        "author": "Reza Miry, Amit K. Chakraborty, Russell Greiner, Mark A. Lewis, Hao Wang, Tianyu Guan, and Pouria Ramazi",
        "link": "http://arxiv.org/abs/2501.07764v1",
        "abstract": "Early Warning Signals (EWSs) are vital for implementing preventive measures\nbefore a disease turns into a pandemic. While new diseases exhibit unique\nbehaviors, they often share fundamental characteristics from a dynamical\nsystems perspective. Moreover, measurements during disease outbreaks are often\ncorrupted by different noise sources, posing challenges for Time Series\nClassification (TSC) tasks. In this study, we address the problem of having a\nrobust EWS for disease outbreak prediction using a best-performing deep\nlearning model in the domain of TSC. We employed two simulated datasets to\ntrain the model: one representing generated dynamical systems with randomly\nselected polynomial terms to model new disease behaviors, and another\nsimulating noise-induced disease dynamics to account for noisy measurements.\nThe model's performance was analyzed using both simulated data from different\ndisease models and real-world data, including influenza and COVID-19. Results\ndemonstrate that the proposed model outperforms previous models, effectively\nproviding EWSs of impending outbreaks across various scenarios. This study\nbridges advancements in deep learning with the ability to provide robust early\nwarning signals in noisy environments, making it highly applicable to\nreal-world crises involving emerging disease outbreaks."
    },
    {
        "date": "2025-01",
        "title": "Distributed Identity for Zero Trust and Segmented Access Control: A Novel Approach to Securing Network Infrastructure",
        "author": "Sina Ahmadi",
        "link": "http://arxiv.org/abs/2501.09032v1",
        "abstract": "\"Distributed Identity\" refers to the transition from centralized identity\nsystems using Decentralized Identifiers (DID) and Verifiable Credentials (VC)\nfor secure and privacy-preserving authentications. With distributed identity,\ncontrol of identity data is returned to the user, making credential-based\nattacks impossible due to the lack of a single point of failure. This study\nassesses the security improvements achieved when distributed identity is\nemployed with the ZTA principle, particularly concerning lateral movements\nwithin segmented networks. It also considers areas such as the implementation\nspecifications of the framework, the advantages and disadvantages of the method\nto organizations, and the issues of compatibility and generalizability.\nFurthermore, the study highlights privacy and regulatory compliance, including\nthe General Data Protection Regulation (GDPR) and California Consumer Data\nPrivacy Act (CCPA), analyzing potential solutions to these problems. The study\nimplies that adopting distributed identities can enhance overall security\npostures by an order of magnitude, providing contextual and least-privilege\nauthorization and user privacy. The research recommends refining technical\nstandards, expanding the use of distributed identity in practice, and\ndiscussing its applications for the contemporary digital security landscape."
    },
    {
        "date": "2025-01",
        "title": "A Review of Detection, Evolution, and Data Reconstruction Strategies for False Data Injection Attacks in Power Cyber-Physical Systems",
        "author": "Xiaoyong Bo",
        "link": "http://arxiv.org/abs/2501.10441v1",
        "abstract": "The integration of information and physical systems in modern power grids has\nheightened vulnerabilities to False Data Injection Attacks (FDIAs), threatening\nthe secure operation of power cyber-physical systems (CPS). This paper reviews\nFDIA detection, evolution, and data reconstruction strategies, highlighting\ncross-domain coordination, multi-temporal evolution, and stealth\ncharacteristics. Challenges in existing detection methods, including poor\ninterpretability and data imbalance, are discussed, alongside advanced\nstate-aware and action-control data reconstruction techniques. Key issues, such\nas modeling FDIA evolution and distinguishing malicious data from regular\nfaults, are identified. Future directions to enhance system resilience and\ndetection accuracy are proposed, contributing to the secure operation of power\nCPS."
    },
    {
        "date": "2025-01",
        "title": "A Review on the Security Vulnerabilities of the IoMT against Malware Attacks and DDoS",
        "author": "Lily Dzamesi, and Nelly Elsayed",
        "link": "http://arxiv.org/abs/2501.07703v1",
        "abstract": "The Internet of Medical Things (IoMT) has transformed the healthcare industry\nby connecting medical devices in monitoring treatment outcomes of patients.\nThis increased connectivity has resulted to significant security\nvulnerabilities in the case of malware and Distributed Denial of Service (DDoS)\nattacks. This literature review examines the vulnerabilities of IoMT devices,\nfocusing on critical threats and exploring mitigation strategies. We conducted\na comprehensive search across leading databases such as ACM Digital Library,\nIEEE Xplore, and Elsevier to analyze peer-reviewed studies published within the\nlast five years (from 2019 to 2024). The review shows that inadequate\nencryption protocols, weak authentication methods, and irregular firmware\nupdates are the main causes of risks associated with IoMT devices. We have\nidentified emerging solutions like machine learning algorithms, blockchain\ntechnology, and edge computing as promising approaches to enhance IoMT\nsecurity. This review emphasizes the pressing need to develop lightweight\nsecurity measures and standardized protocols to protect patient data and ensure\nthe integrity of healthcare services."
    },
    {
        "date": "2025-01",
        "title": "Masking Countermeasures Against Side-Channel Attacks on Quantum Computers",
        "author": "Jason T. LeGrow, Travis Morrison, Jamie Sikora, and Nicolas Swanson",
        "link": "http://arxiv.org/abs/2501.07695v1",
        "abstract": "We propose a modification to the transpiler of a quantum computer to\nsafeguard against side-channel attacks aimed at learning information about a\nquantum circuit. We demonstrate that if it is feasible to shield a specific\nsubset of gates from side-channel attacks, then it is possible to conceal all\ninformation in a quantum circuit by transpiling it into a new circuit whose\ndepth grows linearly, depending on the quantum computer's architecture. We\nprovide concrete examples of implementing this protection on IBM's quantum\ncomputers, utilizing their virtual gates and editing their transpiler."
    },
    {
        "date": "2025-01",
        "title": "Exploring and Mitigating Adversarial Manipulation of Voting-Based Leaderboards",
        "author": "Yangsibo Huang, Milad Nasr, Anastasios Angelopoulos, Nicholas Carlini, Wei-Lin Chiang, Christopher A. Choquette-Choo, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Ken Ziyu Liu, Ion Stoica, Florian Tramer, and Chiyuan Zhang",
        "link": "http://arxiv.org/abs/2501.07493v1",
        "abstract": "It is now common to evaluate Large Language Models (LLMs) by having humans\nmanually vote to evaluate model outputs, in contrast to typical benchmarks that\nevaluate knowledge or skill at some particular task. Chatbot Arena, the most\npopular benchmark of this type, ranks models by asking users to select the\nbetter response between two randomly selected models (without revealing which\nmodel was responsible for the generations). These platforms are widely trusted\nas a fair and accurate measure of LLM capabilities. In this paper, we show that\nif bot protection and other defenses are not implemented, these voting-based\nbenchmarks are potentially vulnerable to adversarial manipulation.\nSpecifically, we show that an attacker can alter the leaderboard (to promote\ntheir favorite model or demote competitors) at the cost of roughly a thousand\nvotes (verified in a simulated, offline version of Chatbot Arena). Our attack\nconsists of two steps: first, we show how an attacker can determine which model\nwas used to generate a given reply with more than $95\\%$ accuracy; and then,\nthe attacker can use this information to consistently vote for (or against) a\ntarget model. Working with the Chatbot Arena developers, we identify, propose,\nand implement mitigations to improve the robustness of Chatbot Arena against\nadversarial manipulation, which, based on our analysis, substantially increases\nthe cost of such attacks. Some of these defenses were present before our\ncollaboration, such as bot protection with Cloudflare, malicious user\ndetection, and rate limiting. Others, including reCAPTCHA and login are being\nintegrated to strengthen the security in Chatbot Arena."
    },
    {
        "date": "2025-01",
        "title": "Encrypted Computation of Collision Probability for Secure Satellite Conjunction Analysis",
        "author": "Jihoon Suh, Michael Hibbard, Kaoru Teranishi, Takashi Tanaka, Moriba Jah, and Maruthi Akella",
        "link": "http://arxiv.org/abs/2501.07476v1",
        "abstract": "The computation of collision probability ($\\mathcal{P}_c$) is crucial for\nspace environmentalism and sustainability by providing decision-making\nknowledge that can prevent collisions between anthropogenic space objects.\nHowever, the accuracy and precision of $\\mathcal{P}_c$ computations is often\ncompromised by limitations in computational resources and data availability.\nWhile significant improvements have been made in the computational aspects, the\nrising concerns regarding the privacy of collaborative data sharing can be a\nmajor limiting factor in the future conjunction analysis and risk assessment,\nespecially as the space environment grows increasingly privatized, competitive,\nand fraught with conflicting strategic interests. This paper argues that the\nimportance of privacy measures in space situational awareness (SSA) is\nunderappreciated, and regulatory and compliance measures currently in place are\nnot sufficient by themselves, presenting a significant gap.\n  To address this gap, we introduce a novel encrypted architecture that\nleverages advanced cryptographic techniques, including homomorphic encryption\n(HE) and multi-party computation (MPC), to safeguard the privacy of entities\ncomputing space sustainability metrics, inter alia, $\\mathcal{P}_c$. Our\nproposed protocol, Encrypted $\\mathcal{P}_c$, integrates the Monte Carlo\nestimation algorithm with cryptographic solutions, enabling secure collision\nprobability computation without exposing sensitive or proprietary information.\nThis research advances secure conjunction analysis by developing a secure MPC\nprotocol for $\\mathcal{P}_c$ computation and highlights the need for innovative\nprotocols to ensure a more secure and cooperative SSA landscape."
    },
    {
        "date": "2025-01",
        "title": "Am I Infected? Lessons from Operating a Large-Scale IoT Security Diagnostic Service",
        "author": "Takayuki Sasaki, Tomoya Inazawa, Youhei Yamaguchi, Simon Parkin, Michel van Eeten, Katsunari Yoshioka, and Tsutomu Matsumoto",
        "link": "http://arxiv.org/abs/2501.07326v1",
        "abstract": "There is an expectation that users of home IoT devices will be able to secure\nthose devices, but they may lack information about what they need to do. In\nFebruary 2022, we launched a web service that scans users' IoT devices to\ndetermine how secure they are. The service aims to diagnose and remediate\nvulnerabilities and malware infections of IoT devices of Japanese users. This\npaper reports on findings from operating this service drawn from three studies:\n(1) the engagement of 114,747 users between February, 2022 - May, 2024; (2) a\nlarge-scale evaluation survey among service users (n=4,103), and; (3) an\ninvestigation and targeted survey (n=90) around the remediation actions of\nusers of non-secure devices. During the operation, we notified 417 (0.36%)\nusers that one or more of their devices were detected as vulnerable, and 171\n(0.15%) users that one of their devices was infected with malware. The service\nfound no issues for 99% of users. Still, 96% of all users evaluated the service\npositively, most often for it providing reassurance, being free of charge, and\nshort diagnosis time. Of the 171 users with malware infections, 67 returned to\nthe service later for a new check, with 59 showing improvement. Of the 417\nusers with vulnerable devices, 151 users revisited and re-diagnosed, where 75\nshowed improvement. We report on lessons learned, including a consideration of\nthe capabilities that non-expert users will assume of a security scan."
    },
    {
        "date": "2025-01",
        "title": "Generating Poisoning Attacks against Ridge Regression Models with Categorical Features",
        "author": "Monse Guedes-Ayala, Lars Schewe, Zeynep Suvak, and Miguel Anjos",
        "link": "http://arxiv.org/abs/2501.07275v1",
        "abstract": "Machine Learning (ML) models have become a very powerful tool to extract\ninformation from large datasets and use it to make accurate predictions and\nautomated decisions. However, ML models can be vulnerable to external attacks,\ncausing them to underperform or deviate from their expected tasks. One way to\nattack ML models is by injecting malicious data to mislead the algorithm during\nthe training phase, which is referred to as a poisoning attack. We can prepare\nfor such situations by designing anticipated attacks, which are later used for\ncreating and testing defence strategies. In this paper, we propose an algorithm\nto generate strong poisoning attacks for a ridge regression model containing\nboth numerical and categorical features that explicitly models and poisons\ncategorical features. We model categorical features as SOS-1 sets and formulate\nthe problem of designing poisoning attacks as a bilevel optimization problem\nthat is nonconvex mixed-integer in the upper-level and unconstrained convex\nquadratic in the lower-level. We present the mathematical formulation of the\nproblem, introduce a single-level reformulation based on the Karush-Kuhn-Tucker\n(KKT) conditions of the lower level, find bounds for the lower-level variables\nto accelerate solver performance, and propose a new algorithm to poison\ncategorical features. Numerical experiments show that our method improves the\nmean squared error of all datasets compared to the previous benchmark in the\nliterature."
    },
    {
        "date": "2025-01",
        "title": "MOS-Attack: A Scalable Multi-objective Adversarial Attack Framework",
        "author": "Ping Guo, Cheng Gong, Xi Lin, Fei Liu, Zhichao Lu, Qingfu Zhang, and Zhenkun Wang",
        "link": "http://arxiv.org/abs/2501.07251v2",
        "abstract": "Crafting adversarial examples is crucial for evaluating and enhancing the\nrobustness of Deep Neural Networks (DNNs), presenting a challenge equivalent to\nmaximizing a non-differentiable 0-1 loss function.\n  However, existing single objective methods, namely adversarial attacks focus\non a surrogate loss function, do not fully harness the benefits of engaging\nmultiple loss functions, as a result of insufficient understanding of their\nsynergistic and conflicting nature.\n  To overcome these limitations, we propose the Multi-Objective Set-based\nAttack (MOS Attack), a novel adversarial attack framework leveraging multiple\nloss functions and automatically uncovering their interrelations.\n  The MOS Attack adopts a set-based multi-objective optimization strategy,\nenabling the incorporation of numerous loss functions without additional\nparameters.\n  It also automatically mines synergistic patterns among various losses,\nfacilitating the generation of potent adversarial attacks with fewer\nobjectives.\n  Extensive experiments have shown that our MOS Attack outperforms\nsingle-objective attacks. Furthermore, by harnessing the identified synergistic\npatterns, MOS Attack continues to show superior results with a reduced number\nof loss functions."
    },
    {
        "date": "2025-01",
        "title": "A Secure Remote Password Protocol From The Learning With Errors Problem",
        "author": "Huapeng Li, and Baocheng Wang",
        "link": "http://arxiv.org/abs/2501.07208v1",
        "abstract": "Secure Remote Password (SRP) protocol is an essential password-authenticated\nkey exchange (PAKE) protocol based on the discrete logarithm problem (DLP). The\nprotocol is specifically designed to obtain a session key and it has been\nwidely used in various scenarios due to its attractive security features. In\nthe SRP protocol, the server is not required to save any data directly\nassociated with passwords. And this makes attackers who manage to corrupt the\nserver fail to impersonate the client unless performing a brute-force search\nfor the password. However, the development of quantum computing has potentially\nmade classic DLP-based public-key cryptography schemes not secure, including\nthe SRP protocol. So it is significant to design a quantum-resistant SRP\nprotocol. In this paper, based on the original scheme, we propose a\npost-quantum SRP protocol from the learning with errors (LWE) problem. And we\ngive rigorous proof and analyses on the correctness and security of the scheme.\nBesides being resistant to known quantum attacks, it maintains the various\nsecure qualities of the original protocol."
    },
    {
        "date": "2025-01",
        "title": "Beyond Security-by-design: Securing a compromised system",
        "author": "Awais Rashid, Sana Belguith, Matthew Bradbury, Sadie Creese, Ivan Flechais, and Neeraj Suri",
        "link": "http://arxiv.org/abs/2501.07207v1",
        "abstract": "Digital infrastructures are seeing convergence and connectivity at\nunprecedented scale. This is true for both current critical national\ninfrastructures and emerging future systems that are highly cyber-physical in\nnature with complex intersections between humans and technologies, e.g., smart\ncities, intelligent transportation, high-value manufacturing and Industry 4.0.\nDiverse legacy and non-legacy software systems underpinned by heterogeneous\nhardware compose on-the-fly to deliver services to millions of users with\nvarying requirements and unpredictable actions. This complexity is compounded\nby intricate and complicated supply-chains with many digital assets and\nservices outsourced to third parties. The reality is that, at any particular\npoint in time, there will be untrusted, partially-trusted or compromised\nelements across the infrastructure. Given this reality, and the societal scale\nof digital infrastructures, delivering secure and resilient operations is a\nmajor challenge. We argue that this requires us to move beyond the paradigm of\nsecurity-by-design and embrace the challenge of securing-a-compromised-system."
    },
    {
        "date": "2025-01",
        "title": "Generalizable Graph Neural Networks for Robust Power Grid Topology Control",
        "author": "Matthijs de Jong, Jan Viebahn, and Yuliya Shapovalova",
        "link": "http://arxiv.org/abs/2501.07186v1",
        "abstract": "The energy transition necessitates new congestion management methods. One\nsuch method is controlling the grid topology with machine learning (ML). This\napproach has gained popularity following the Learning to Run a Power Network\n(L2RPN) competitions. Graph neural networks (GNNs) are a class of ML models\nthat reflect graph structure in their computation, which makes them suitable\nfor power grid modeling. Various GNN approaches for topology control have thus\nbeen proposed. We propose the first GNN model for grid topology control that\nuses only GNN layers. Additionally, we identify the busbar information\nasymmetry problem that the popular homogeneous graph representation suffers\nfrom, and propose a heterogeneous graph representation to resolve it. We train\nboth homogeneous and heterogeneous GNNs and fully connected neural networks\n(FCNN) baselines on an imitation learning task. We evaluate the models\naccording to their classification accuracy and grid operation ability. We find\nthat the heterogeneous GNNs perform best on in-distribution networks, followed\nby the FCNNs, and lastly, the homogeneous GNNs. We also find that both GNN\ntypes generalize better to out-of-distribution networks than FCNNs."
    },
    {
        "date": "2025-01",
        "title": "Robust Single Object Tracking in LiDAR Point Clouds under Adverse Weather Conditions",
        "author": "Xiantong Zhao, Xiuping Liu, Shengjing Tian, and Yinan Han",
        "link": "http://arxiv.org/abs/2501.07133v1",
        "abstract": "3D single object tracking (3DSOT) in LiDAR point clouds is a critical task\nfor outdoor perception, enabling real-time perception of object location,\norientation, and motion. Despite the impressive performance of current 3DSOT\nmethods, evaluating them on clean datasets inadequately reflects their\ncomprehensive performance, as the adverse weather conditions in real-world\nsurroundings has not been considered. One of the main obstacles is the lack of\nadverse weather benchmarks for the evaluation of 3DSOT. To this end, this work\nproposes a challenging benchmark for LiDAR-based 3DSOT in adverse weather,\nwhich comprises two synthetic datasets (KITTI-A and nuScenes-A) and one\nreal-world dataset (CADC-SOT) spanning three weather types: rain, fog, and\nsnow. Based on this benchmark, five representative 3D trackers from different\ntracking frameworks conducted robustness evaluation, resulting in significant\nperformance degradations. This prompts the question: What are the factors that\ncause current advanced methods to fail on such adverse weather samples?\nConsequently, we explore the impacts of adverse weather and answer the above\nquestion from three perspectives: 1) target distance; 2) template shape\ncorruption; and 3) target shape corruption. Finally, based on domain\nrandomization and contrastive learning, we designed a dual-branch tracking\nframework for adverse weather, named DRCT, achieving excellent performance in\nbenchmarks."
    },
    {
        "date": "2025-01",
        "title": "Beyond the Surface: An NLP-based Methodology to Automatically Estimate CVE Relevance for CAPEC Attack Patterns",
        "author": "Silvia Bonomi, Andrea Ciavotta, Simone Lenti, and Alessandro Palma",
        "link": "http://arxiv.org/abs/2501.07131v1",
        "abstract": "Threat analysis is continuously growing in importance due to the\nalways-increasing complexity and frequency of cyber attacks. Analyzing threats\ndemands significant effort from security experts, leading to delays in the\nsecurity analysis process. Different cybersecurity knowledge bases are\ncurrently available to support this task but manual efforts are often required\nto correlate such heterogenous sources into a unified view that would enable a\nmore comprehensive assessment. To address this gap, we propose a methodology\nleveraging Natural Language Processing (NLP) to effectively and efficiently\nassociate Common Vulnerabilities and Exposure (CVE) vulnerabilities with Common\nAttack Pattern Enumeration and Classification (CAPEC) attack patterns. The\nproposed technique combines semantic similarity with keyword analysis to\nimprove the accuracy of association estimations. Experimental evaluations\ndemonstrate superior performance compared to state-of-the-art models, reducing\nmanual effort and analysis time, and enabling cybersecurity professionals to\nprioritize critical tasks."
    },
    {
        "date": "2025-01",
        "title": "SFC-GAN: A Generative Adversarial Network for Brain Functional and Structural Connectome Translation",
        "author": "Yee-Fan Tan, Jun Lin Liow, Pei-Sze Tan, Fuad Noman, Raphael C. -W. Phan, Hernando Ombao, and Chee-Ming Ting",
        "link": "http://arxiv.org/abs/2501.07055v1",
        "abstract": "Modern brain imaging technologies have enabled the detailed reconstruction of\nhuman brain connectomes, capturing structural connectivity (SC) from diffusion\nMRI and functional connectivity (FC) from functional MRI. Understanding the\nintricate relationships between SC and FC is vital for gaining deeper insights\ninto the brain's functional and organizational mechanisms. However, obtaining\nboth SC and FC modalities simultaneously remains challenging, hindering\ncomprehensive analyses. Existing deep generative models typically focus on\nsynthesizing a single modality or unidirectional translation between FC and SC,\nthereby missing the potential benefits of bi-directional translation,\nespecially in scenarios where only one connectome is available. Therefore, we\npropose Structural-Functional Connectivity GAN (SFC-GAN), a novel framework for\nbidirectional translation between SC and FC. This approach leverages the\nCycleGAN architecture, incorporating convolutional layers to effectively\ncapture the spatial structures of brain connectomes. To preserve the\ntopological integrity of these connectomes, we employ a structure-preserving\nloss that guides the model in capturing both global and local connectome\npatterns while maintaining symmetry. Our framework demonstrates superior\nperformance in translating between SC and FC, outperforming baseline models in\nsimilarity and graph property evaluations compared to ground truth data, each\ntranslated modality can be effectively utilized for downstream classification."
    },
    {
        "date": "2025-01",
        "title": "Protego: Detecting Adversarial Examples for Vision Transformers via Intrinsic Capabilities",
        "author": "Jialin Wu, Kaikai Pan, Yanjiao Chen, Jiangyi Deng, Shengyuan Pang, and Wenyuan Xu",
        "link": "http://arxiv.org/abs/2501.07044v1",
        "abstract": "Transformer models have excelled in natural language tasks, prompting the\nvision community to explore their implementation in computer vision problems.\nHowever, these models are still influenced by adversarial examples. In this\npaper, we investigate the attack capabilities of six common adversarial attacks\non three pretrained ViT models to reveal the vulnerability of ViT models. To\nunderstand and analyse the bias in neural network decisions when the input is\nadversarial, we use two visualisation techniques that are attention rollout and\ngrad attention rollout. To prevent ViT models from adversarial attack, we\npropose Protego, a detection framework that leverages the transformer intrinsic\ncapabilities to detection adversarial examples of ViT models. Nonetheless, this\nis challenging due to a diversity of attack strategies that may be adopted by\nadversaries. Inspired by the attention mechanism, we know that the token of\nprediction contains all the information from the input sample. Additionally,\nthe attention region for adversarial examples differs from that of normal\nexamples. Given these points, we can train a detector that achieves superior\nperformance than existing detection methods to identify adversarial examples.\nOur experiments have demonstrated the high effectiveness of our detection\nmethod. For these six adversarial attack methods, our detector's AUC scores all\nexceed 0.95. Protego may advance investigations in metaverse security."
    },
    {
        "date": "2025-01",
        "title": "Hybrid Scheme of Post-Quantum Cryptography and Elliptic-Curve Cryptography for Certificates -- A Case Study of Security Credential Management System in Vehicle-to-Everything Communications",
        "author": "Abel C. H. Chen, and Bon-Yeh Lin",
        "link": "http://arxiv.org/abs/2501.07028v1",
        "abstract": "Due to the current standard of Security Credential Management System (SCMS)\nfor Vehicle-to-Everything (V2X) communications using asymmetric cryptography,\nspecifically Elliptic-Curve Cryptography (ECC), which may be vulnerable to\nquantum computing attacks. Therefore, the V2X SCMS is threatened by quantum\ncomputing attacks. However, although the National Institute of Standards and\nTechnology (NIST) has already selected Post-Quantum Cryptography (PQC)\nalgorithms as the standard, the current PQC algorithms may have issues such as\nlonger public key lengths, longer signature lengths, or lower signature\ngeneration and verification efficiency, which may not fully meet the\nrequirements of V2X communication applications. In view of the challenges in\nV2X communication, such as packet length, signature generation and verification\nefficiency, security level, and vehicle privacy, this study proposes a hybrid\ncertificate scheme of PQC and ECC. By leveraging the strengths of both PQC and\nECC, this scheme aims to overcome the challenges in V2X communication. PQC is\nused to establish a security level resistant to quantum computing attacks,\nwhile ECC is utilized to establish anonymous certificates and reduce packet\nlength to meet the requirements of V2X communication. In the practical\nexperiments, the study implemented the SCMS end entity based on the Chunghwa\nTelecom SCMS and the Clientron On-Board Unit (OBU) to conduct field tests in\nDanhai New Town in New Taipei City. The performance of various existing hybrid\ncertificate schemes combining PQC (e.g., Dilithium, Falcon, and SPHINCS+) and\nECC is compared, and a practical solution is provided for V2X industries."
    },
    {
        "date": "2025-01",
        "title": "Layer-Wise Security Framework and Analysis for the Quantum Internet",
        "author": "Zebo Yang, Ali Ghubaish, Raj Jain, Ala Al-Fuqaha, Aiman Erbad, Ramana Kompella, Hassan Shapourian, and Reza Nejabati",
        "link": "http://arxiv.org/abs/2501.06989v1",
        "abstract": "With its significant security potential, the quantum internet is poised to\nrevolutionize technologies like cryptography and communications. Although it\nboasts enhanced security over traditional networks, the quantum internet still\nencounters unique security challenges essential for safeguarding its\nConfidentiality, Integrity, and Availability (CIA). This study explores these\nchallenges by analyzing the vulnerabilities and the corresponding mitigation\nstrategies across different layers of the quantum internet, including physical,\nlink, network, and application layers. We assess the severity of potential\nattacks, evaluate the expected effectiveness of mitigation strategies, and\nidentify vulnerabilities within diverse network configurations, integrating\nboth classical and quantum approaches. Our research highlights the dynamic\nnature of these security issues and emphasizes the necessity for adaptive\nsecurity measures. The findings underline the need for ongoing research into\nthe security dimension of the quantum internet to ensure its robustness,\nencourage its adoption, and maximize its impact on society."
    },
    {
        "date": "2025-01",
        "title": "Robust Hybrid Classical-Quantum Transfer Learning Model for Text Classification Using GPT-Neo 125M with LoRA & SMOTE Enhancement",
        "author": "Santanam Wishal",
        "link": "http://arxiv.org/abs/2501.10435v2",
        "abstract": "This research introduces a hybrid classical-quantum framework for text\nclassification, integrating GPT-Neo 125M with Low-Rank Adaptation (LoRA) and\nSynthetic Minority Over-sampling Technique (SMOTE) using quantum computing\nbackends. While the GPT-Neo 125M baseline remains the best-performing model,\nthe implementation of LoRA and SMOTE enhances the hybrid model, resulting in\nimproved accuracy, faster convergence, and better generalization. Experiments\non IBM's 127-qubit quantum backend and Pennylane's 32-qubit simulation\ndemonstrate the viability of combining classical neural networks with quantum\ncircuits. This framework underscores the potential of hybrid architectures for\nadvancing natural language processing applications."
    },
    {
        "date": "2025-01",
        "title": "ByzSFL: Achieving Byzantine-Robust Secure Federated Learning with Zero-Knowledge Proofs",
        "author": "Yongming Fan, Rui Zhu, Zihao Wang, Chenghong Wang, Haixu Tang, Ye Dong, Hyunghoon Cho, and Lucila Ohno-Machado",
        "link": "http://arxiv.org/abs/2501.06953v1",
        "abstract": "The advancement of AI models, especially those powered by deep learning,\nfaces significant challenges in data-sensitive industries like healthcare and\nfinance due to the distributed and private nature of data. Federated Learning\n(FL) and Secure Federated Learning (SFL) enable collaborative model training\nwithout data sharing, enhancing privacy by encrypting shared intermediate\nresults. However, SFL currently lacks effective Byzantine robustness, a\ncritical property that ensures model performance remains intact even when some\nparticipants act maliciously. Existing Byzantine-robust methods in FL are\nincompatible with SFL due to the inefficiency and limitations of encryption\noperations in handling complex aggregation calculations. This creates a\nsignificant gap in secure and robust model training.\n  To address this gap, we propose ByzSFL, a novel SFL system that achieves\nByzantine-robust secure aggregation with high efficiency. Our approach offloads\naggregation weight calculations to individual parties and introduces a\npractical zero-knowledge proof (ZKP) protocol toolkit. This toolkit supports\nwidely used operators for calculating aggregation weights, ensuring correct\ncomputations without compromising data privacy. Not only does this method\nmaintain aggregation integrity, but it also significantly boosts computational\nefficiency, making ByzSFL approximately 100 times faster than existing\nsolutions. Furthermore, our method aligns with open-source AI trends, enabling\nplaintext publication of the final model without additional information\nleakage, thereby enhancing the practicality and robustness of SFL in real-world\napplications."
    },
    {
        "date": "2025-01",
        "title": "Super-Resolution of 3D Micro-CT Images Using Generative Adversarial Networks: Enhancing Resolution and Segmentation Accuracy",
        "author": "Evgeny Ugolkov, Xupeng He, Hyung Kwak, and Hussein Hoteit",
        "link": "http://arxiv.org/abs/2501.06939v1",
        "abstract": "We develop a procedure for substantially improving the quality of segmented\n3D micro-Computed Tomography (micro-CT) images of rocks with a Machine Learning\n(ML) Generative Model. The proposed model enhances the resolution eightfold\n(8x) and addresses segmentation inaccuracies due to the overlapping X-ray\nattenuation in micro-CT measurement for different rock minerals and phases. The\nproposed generative model is a 3D Deep Convolutional Wasserstein Generative\nAdversarial Network with Gradient Penalty (3D DC WGAN-GP). The algorithm is\ntrained on segmented 3D low-resolution micro-CT images and segmented unpaired\ncomplementary 2D high-resolution Laser Scanning Microscope (LSM) images. The\nalgorithm was demonstrated on multiple samples of Berea sandstones. We achieved\nhigh-quality super-resolved 3D images with a resolution of 0.4375 micro-m/voxel\nand accurate segmentation for constituting minerals and pore space. The\ndescribed procedure can significantly expand the modern capabilities of digital\nrock physics."
    },
    {
        "date": "2025-01",
        "title": "OFDM-based JCAS under Attack: The Dual Threat of Spoofing and Jamming in WLAN Sensing",
        "author": "Hasan Can Yildirim, Musa Furkan Keskin, Henk Wymeersch, and Francois Horlin",
        "link": "http://arxiv.org/abs/2501.06798v1",
        "abstract": "This study reveals the vulnerabilities of Wireless Local Area Networks (WLAN)\nsensing, under the scope of joint communication and sensing (JCAS), focusing on\ntarget spoofing and deceptive jamming techniques. We use orthogonal\nfrequency-division multiplexing (OFDM) to explore how adversaries can exploit\nWLAN's sensing capabilities to inject false targets and disrupt normal\noperations. Unlike traditional methods that require sophisticated digital\nradio-frequency memory hardware, we demonstrate that much simpler\nsoftware-defined radios can effectively serve as deceptive jammers in WLAN\nsettings. Through comprehensive modeling and practical experiments, we show how\ndeceptive jammers can manipulate the range-Doppler map (RDM) by altering signal\nintegrity, thereby posing significant security threats to OFDM-based JCAS\nsystems. Our findings comprehensively evaluate jammer impact on RDMs and\npropose several jamming strategies that vary in complexity and detectability."
    },
    {
        "date": "2025-01",
        "title": "KeTS: Kernel-based Trust Segmentation against Model Poisoning Attacks",
        "author": "Ankit Gangwal, Mauro Conti, and Tommaso Pauselli",
        "link": "http://arxiv.org/abs/2501.06729v1",
        "abstract": "Federated Learning (FL) enables multiple users to collaboratively train a\nglobal model in a distributed manner without revealing their personal data.\nHowever, FL remains vulnerable to model poisoning attacks, where malicious\nactors inject crafted updates to compromise the global model's accuracy. These\nvulnerabilities are particularly severe in non-homogeneous environments, where\nclients exhibit varying proportions of class labels, resulting in heterogeneous\nupdates. In such settings, benign outliers are often misclassified as false\npositives, while maliciously crafted uploads evade detection and are aggregated\nat the server. Existing defense mechanisms struggle in such real-world\nsettings, resulting in significant declines in the global FL model's\nperformance.\n  We propose a novel defense mechanism, Kernel-based Trust Segmentation (KeTS),\nto counter model poisoning attacks. Unlike existing approaches, KeTS analyzes\nthe evolution of each client's updates and effectively segments malicious\nclients using Kernel Density Estimation (KDE), even in the presence of benign\noutliers. We thoroughly evaluate KeTS's performance against the six most\neffective model poisoning attacks (i.e., Trim-Attack, Krum-Attack, Min-Max\nattack, Min-Sum attack, and their variants) on two different datasets (i.e.,\nMNIST and Fashion-MNIST) and compare its performance with three classical\nrobust schemes (i.e., Krum, Trim-Mean, and Median) and a state-of-the-art\ndefense (i.e., FLTrust). Our results show that KeTS outperforms the existing\ndefenses in every attack setting; beating the best-performing defense by an\noverall average of >24% (on MNIST) and >14% (on Fashion-MNIST). A series of\nfurther experiments (varying poisoning approaches, attacker population, etc.)\nreveal the consistent and superior performance of KeTS under diverse\nconditions."
    },
    {
        "date": "2025-01",
        "title": "SafeSplit: A Novel Defense Against Client-Side Backdoor Attacks in Split Learning",
        "author": "Phillip Rieger, Alessandro Pegoraro, Kavita Kumari, Tigist Abera, Jonathan Knauer, and Ahmad-Reza Sadeghi",
        "link": "http://arxiv.org/abs/2501.06650v1",
        "abstract": "Split Learning (SL) is a distributed deep learning approach enabling multiple\nclients and a server to collaboratively train and infer on a shared deep neural\nnetwork (DNN) without requiring clients to share their private local data. The\nDNN is partitioned in SL, with most layers residing on the server and a few\ninitial layers and inputs on the client side. This configuration allows\nresource-constrained clients to participate in training and inference. However,\nthe distributed architecture exposes SL to backdoor attacks, where malicious\nclients can manipulate local datasets to alter the DNN's behavior. Existing\ndefenses from other distributed frameworks like Federated Learning are not\napplicable, and there is a lack of effective backdoor defenses specifically\ndesigned for SL.\n  We present SafeSplit, the first defense against client-side backdoor attacks\nin Split Learning (SL). SafeSplit enables the server to detect and filter out\nmalicious client behavior by employing circular backward analysis after a\nclient's training is completed, iteratively reverting to a trained checkpoint\nwhere the model under examination is found to be benign. It uses a two-fold\nanalysis to identify client-induced changes and detect poisoned models. First,\na static analysis in the frequency domain measures the differences in the\nlayer's parameters at the server. Second, a dynamic analysis introduces a novel\nrotational distance metric that assesses the orientation shifts of the server's\nlayer parameters during training. Our comprehensive evaluation across various\ndata distributions, client counts, and attack scenarios demonstrates the high\nefficacy of this dual analysis in mitigating backdoor attacks while preserving\nmodel utility."
    },
    {
        "date": "2025-01",
        "title": "Quantum Annealing for Robust Principal Component Analysis",
        "author": "Ian Tomeo, Panos P. Markopoulos, and Andreas Savakis",
        "link": "http://arxiv.org/abs/2501.10431v1",
        "abstract": "Principal component analysis is commonly used for dimensionality reduction,\nfeature extraction, denoising, and visualization. The most commonly used\nprincipal component analysis method is based upon optimization of the L2-norm,\nhowever, the L2-norm is known to exaggerate the contribution of errors and\noutliers. When optimizing over the L1-norm, the components generated are known\nto exhibit robustness or resistance to outliers in the data. The L1-norm\ncomponents can be solved for with a binary optimization problem. Previously,\nL1-BF has been used to solve the binary optimization for multiple components\nsimultaneously. In this paper we propose QAPCA, a new method for finding\nprincipal components using quantum annealing hardware which will optimize over\nthe robust L1-norm. The conditions required for convergence of the annealing\nproblem are discussed. The potential speedup when using quantum annealing is\ndemonstrated through complexity analysis and experimental results. To showcase\nperformance against classical principal component analysis techniques\nexperiments upon synthetic Gaussian data, a fault detection scenario and breast\ncancer diagnostic data are studied. We find that the reconstruction error when\nusing QAPCA is comparable to that when using L1-BF."
    },
    {
        "date": "2025-01",
        "title": "RogueRFM: Attacking Refresh Management for Covert-Channel and Denial-of-Service",
        "author": "Hritvik Taneja, and Moinuddin Qureshi",
        "link": "http://arxiv.org/abs/2501.06646v1",
        "abstract": "With lowering thresholds, transparently defending against Rowhammer within\nDRAM is challenging due to the lack of time to perform mitigation. Commercially\ndeployed in-DRAM defenses like TRR that steal time from normal refreshes~(REF)\nto perform mitigation have been proven ineffective against Rowhammer. In\nresponse, a new Refresh Management (RFM) interface has been added to the DDR5\nspecifications. RFM provides dedicated time to an in-DRAM defense to perform\nmitigation. Several recent works have used RFM for the intended purpose -\nbuilding better Rowhammer defenses. However, to the best of our knowledge, no\nprior study has looked at the potential security implications of this new\nfeature if an attacker subjects it to intentional misuse.\n  Our paper shows that RFM introduces new side effects in the system - the\nactivity of one bank causes interference with the operation of the other banks.\nThus, the latency of a bank becomes dependent on the activity of other banks.\nWe use these side effects to build two new attacks. First, a novel memory-based\ncovert channel, which has a bandwidth of up to 31.3 KB/s, and is also effective\neven in a bank-partitioned system. Second, a new Denial-of-Service (DOS) attack\npattern that exploits the activity within a single bank to reduce the\nperformance of the other banks. Our experiments on SPEC2017, PARSEC, and LIGRA\nworkloads show a slowdown of up to 67\\% when running alongside our DOS pattern.\nWe also discuss potential countermeasures for our attacks."
    },
    {
        "date": "2025-01",
        "title": "Exploring Pose-Based Anomaly Detection for Retail Security: A Real-World Shoplifting Dataset and Benchmark",
        "author": "Narges Rashvand, Ghazal Alinezhad Noghre, Armin Danesh Pazho, Shanle Yao, and Hamed Tabkhi",
        "link": "http://arxiv.org/abs/2501.06591v1",
        "abstract": "Shoplifting poses a significant challenge for retailers, resulting in\nbillions of dollars in annual losses. Traditional security measures often fall\nshort, highlighting the need for intelligent solutions capable of detecting\nshoplifting behaviors in real time. This paper frames shoplifting detection as\nan anomaly detection problem, focusing on the identification of deviations from\ntypical shopping patterns. We introduce PoseLift, a privacy-preserving dataset\nspecifically designed for shoplifting detection, addressing challenges such as\ndata scarcity, privacy concerns, and model biases. PoseLift is built in\ncollaboration with a retail store and contains anonymized human pose data from\nreal-world scenarios. By preserving essential behavioral information while\nanonymizing identities, PoseLift balances privacy and utility. We benchmark\nstate-of-the-art pose-based anomaly detection models on this dataset,\nevaluating performance using a comprehensive set of metrics. Our results\ndemonstrate that pose-based approaches achieve high detection accuracy while\neffectively addressing privacy and bias concerns inherent in traditional\nmethods. As one of the first datasets capturing real-world shoplifting\nbehaviors, PoseLift offers researchers a valuable tool to advance computer\nvision ethically and will be publicly available to foster innovation and\ncollaboration. The dataset is available at\nhttps://github.com/TeCSAR-UNCC/PoseLift."
    },
    {
        "date": "2025-01",
        "title": "Determination of galaxy photometric redshifts using Conditional Generative Adversarial Networks (CGANs)",
        "author": "M. Garcia-Fernandez",
        "link": "http://arxiv.org/abs/2501.06532v1",
        "abstract": "Accurate and reliable photometric redshifts determination is one of the key\naspects for wide-field photometric surveys. Determination of photometric\nredshift for galaxies, has been traditionally solved by use of machine-learning\nand artificial intelligence techniques trained on a calibration sample of\ngalaxies, where both photometry and spectrometry are determined. On this paper,\nwe present a new algorithmic approach for determining photometric redshifts of\ngalaxies using Conditional Generative Adversarial Networks (CGANs). Proposed\nCGAN implementation, approaches photometric redshift determination as a\nprobabilistic regression, where instead of determining a single value for the\nestimated redshift of the galaxy, a full probability density is computed. The\nmethodology proposed, is tested with data from Dark Energy Survey (DES) Y1 data\nand compared with other existing algorithm such as a Random Forest regressor."
    },
    {
        "date": "2025-01",
        "title": "FocusDD: Real-World Scene Infusion for Robust Dataset Distillation",
        "author": "Youbing Hu, Yun Cheng, Olga Saukh, Firat Ozdemir, Anqi Lu, Zhiqiang Cao, and Zhijun Li",
        "link": "http://arxiv.org/abs/2501.06405v1",
        "abstract": "Dataset distillation has emerged as a strategy to compress real-world\ndatasets for efficient training. However, it struggles with large-scale and\nhigh-resolution datasets, limiting its practicality. This paper introduces a\nnovel resolution-independent dataset distillation method Focus ed Dataset\nDistillation (FocusDD), which achieves diversity and realism in distilled data\nby identifying key information patches, thereby ensuring the generalization\ncapability of the distilled dataset across different network architectures.\nSpecifically, FocusDD leverages a pre-trained Vision Transformer (ViT) to\nextract key image patches, which are then synthesized into a single distilled\nimage. These distilled images, which capture multiple targets, are suitable not\nonly for classification tasks but also for dense tasks such as object\ndetection. To further improve the generalization of the distilled dataset, each\nsynthesized image is augmented with a downsampled view of the original image.\nExperimental results on the ImageNet-1K dataset demonstrate that, with 100\nimages per class (IPC), ResNet50 and MobileNet-v2 achieve validation accuracies\nof 71.0% and 62.6%, respectively, outperforming state-of-the-art methods by\n2.8% and 4.7%. Notably, FocusDD is the first method to use distilled datasets\nfor object detection tasks. On the COCO2017 dataset, with an IPC of 50,\nYOLOv11n and YOLOv11s achieve 24.4% and 32.1% mAP, respectively, further\nvalidating the effectiveness of our approach."
    },
    {
        "date": "2025-01",
        "title": "Towards Robust Nonlinear Subspace Clustering: A Kernel Learning Approach",
        "author": "Kunpeng Xu, Lifei Chen, and Shengrui Wang",
        "link": "http://arxiv.org/abs/2501.06368v2",
        "abstract": "Kernel-based subspace clustering, which addresses the nonlinear structures in\ndata, is an evolving area of research. Despite noteworthy progressions,\nprevailing methodologies predominantly grapple with limitations relating to (i)\nthe influence of predefined kernels on model performance; (ii) the difficulty\nof preserving the original manifold structures in the nonlinear space; (iii)\nthe dependency of spectral-type strategies on the ideal block diagonal\nstructure of the affinity matrix. This paper presents DKLM, a novel paradigm\nfor kernel-induced nonlinear subspace clustering. DKLM provides a data-driven\napproach that directly learns the kernel from the data's self-representation,\nensuring adaptive weighting and satisfying the multiplicative triangle\ninequality constraint, which enhances the robustness of the learned kernel. By\nleveraging this learned kernel, DKLM preserves the local manifold structure of\ndata in a nonlinear space while promoting the formation of an optimal\nblock-diagonal affinity matrix. A thorough theoretical examination of DKLM\nreveals its relationship with existing clustering paradigms. Comprehensive\nexperiments on synthetic and real-world datasets demonstrate the effectiveness\nof the proposed method."
    },
    {
        "date": "2025-01",
        "title": "Resilient Endurance-Aware NVM-based PUF against Learning-based Attacks",
        "author": "Hassan Nassar, Ming-Liang Wei, Chia-Lin Yang, J\u00f6rg Henkel, and Kuan-Hsun Chen",
        "link": "http://arxiv.org/abs/2501.06367v1",
        "abstract": "Physical Unclonable Functions (PUFs) based on Non-Volatile Memory (NVM)\ntechnology have emerged as a promising solution for secure authentication and\ncryptographic applications. By leveraging the multi-level cell (MLC)\ncharacteristic of NVMs, these PUFs can generate a wide range of unique\nresponses, enhancing their resilience to machine learning (ML) modeling\nattacks. However, a significant issue with NVM-based PUFs is their endurance\nproblem; frequent write operations lead to wear and degradation over time,\nreducing the reliability and lifespan of the PUF.\n  This paper addresses these issues by offering a comprehensive model to\npredict and analyze the effects of endurance changes on NVM PUFs. This model\nprovides insights into how wear impacts the PUF's quality and helps in\ndesigning more robust PUFs. Building on this model, we present a novel design\nfor NVM PUFs that significantly improves endurance. Our design approach\nincorporates advanced techniques to distribute write operations more evenly and\nreduce stress on individual cells. The result is an NVM PUF that demonstrates a\n$62\\times$ improvement in endurance compared to current state-of-the-art\nsolutions while maintaining protection against learning-based attacks."
    },
    {
        "date": "2025-01",
        "title": "Towards Iris Presentation Attack Detection with Foundation Models",
        "author": "Juan E. Tapia, L\u00e1zaro Janier Gonz\u00e1lez-Soler, and Christoph Busch",
        "link": "http://arxiv.org/abs/2501.06312v1",
        "abstract": "Foundation models are becoming increasingly popular due to their strong\ngeneralization capabilities resulting from being trained on huge datasets.\nThese generalization capabilities are attractive in areas such as NIR Iris\nPresentation Attack Detection (PAD), in which databases are limited in the\nnumber of subjects and diversity of attack instruments, and there is no\ncorrespondence between the bona fide and attack images because, most of the\ntime, they do not belong to the same subjects. This work explores an iris PAD\napproach based on two foundation models, DinoV2 and VisualOpenClip. The results\nshow that fine-tuning prediction with a small neural network as head overpasses\nthe state-of-the-art performance based on deep learning approaches. However,\nsystems trained from scratch have still reached better results if bona fide and\nattack images are available."
    },
    {
        "date": "2025-01",
        "title": "Reinforcement Learning-Driven Adaptation Chains: A Robust Framework for Multi-Cloud Workflow Security",
        "author": "Nafiseh Soveizi, and Dimka Karastoyanova",
        "link": "http://arxiv.org/abs/2501.06305v1",
        "abstract": "Cloud computing has emerged as a crucial solution for managing data- and\ncompute-intensive workflows, offering scalability to address dynamic demands.\nHowever, security concerns persist, especially for workflows involving\nsensitive data and tasks. One of the main gaps in the literature is the lack of\nrobust and flexible measures for reacting to these security violations. To\naddress this, we propose an innovative approach leveraging Reinforcement\nLearning (RL) to formulate adaptation chains, responding effectively to\nsecurity violations within cloud-based workflows. These chains consist of\nsequences of adaptation actions tailored to attack characteristics, workflow\ndependencies, and user-defined requirements. Unlike conventional single-task\nadaptations, adaptation chains provide a comprehensive mitigation strategy by\ntaking into account both control and data dependencies between tasks, thereby\naccommodating conflicting objectives effectively. Moreover, our RL-based\napproach uses insights from past responses to mitigate uncertainties associated\nwith adaptation costs. We evaluate the method using our jBPM and Cloudsim Plus\nbased implementation and compare the impact of selected adaptation chains on\nworkflows with the single adaptation approach. Results demonstrate that the\nadaptation chain approach outperforms in terms of total adaptation cost,\noffering resilience and adaptability against security threats."
    },
    {
        "date": "2025-01",
        "title": "Multi-layered Authentication and Key Management Scheme for Secure IoV",
        "author": "Morteza Azmoudeh Afshar, Nesrine Benchoubane, Busra Cayoren, Gunes Karabulut Kurt, and Enver Ozdemir",
        "link": "http://arxiv.org/abs/2501.06087v1",
        "abstract": "The rapid development of Vehicular Ad-hoc Networks (VANETs) within the\nInternet of Vehicles (IoV) necessitates efficient and secure authentication\nmethods to support high-speed, high-density environments. Current group\nauthentication schemes provide user identity protection and unlinkability but\nface limitations, such as reliance on a central group manager and vulnerability\nto collaborative attacks. This paper presents a privacy-preserving\nauthentication scheme that incorporates batch authentication, mutual\nauthentication, and secure key establishment, enabling users to authenticate\none another without a central authority. Our proposed scheme facilitates\nsimultaneous multi-user authentication, significantly enhancing scalability and\nsecurity in dynamic IoV networks. Results from realistic implementations show\nthat our method achieves average authentication and verification times of 10.61\nms and 1.78 ms, respectively, for a fleet of 100 vehicles, outperforming\nexisting methods. Scalability tests demonstrate efficient processing for larger\ngroups of up to 500 vehicles, where average authentication times remain low,\nestablishing our scheme as a robust solution for secure communication in IoV\nsystems."
    },
    {
        "date": "2025-01",
        "title": "Enhancing, Refining, and Fusing: Towards Robust Multi-Scale and Dense Ship Detection",
        "author": "Congxia Zhao, Xiongjun Fu, Jian Dong, Shen Cao, and Chunyan Zhang",
        "link": "http://arxiv.org/abs/2501.06053v1",
        "abstract": "Synthetic aperture radar (SAR) imaging, celebrated for its high resolution,\nall-weather capability, and day-night operability, is indispensable for\nmaritime applications. However, ship detection in SAR imagery faces significant\nchallenges, including complex backgrounds, densely arranged targets, and large\nscale variations. To address these issues, we propose a novel framework,\nCenter-Aware SAR Ship Detector (CASS-Det), designed for robust multi-scale and\ndensely packed ship detection. CASS-Det integrates three key innovations: (1) a\ncenter enhancement module (CEM) that employs rotational convolution to\nemphasize ship centers, improving localization while suppressing background\ninterference; (2) a neighbor attention module (NAM) that leverages cross-layer\ndependencies to refine ship boundaries in densely populated scenes; and (3) a\ncross-connected feature pyramid network (CC-FPN) that enhances multi-scale\nfeature fusion by integrating shallow and deep features. Extensive experiments\non the SSDD, HRSID, and LS-SSDD-v1.0 datasets demonstrate the state-of-the-art\nperformance of CASS-Det, excelling at detecting multi-scale and densely\narranged ships."
    },
    {
        "date": "2025-01",
        "title": "Effective faking of verbal deception detection with target-aligned adversarial attacks",
        "author": "Bennett Kleinberg, Riccardo Loconte, and Bruno Verschuere",
        "link": "http://arxiv.org/abs/2501.05962v1",
        "abstract": "Background: Deception detection through analysing language is a promising\navenue using both human judgments and automated machine learning judgments. For\nboth forms of credibility assessment, automated adversarial attacks that\nrewrite deceptive statements to appear truthful pose a serious threat. Methods:\nWe used a dataset of 243 truthful and 262 fabricated autobiographical stories\nin a deception detection task for humans and machine learning models. A large\nlanguage model was tasked to rewrite deceptive statements so that they appear\ntruthful. In Study 1, humans who made a deception judgment or used the\ndetailedness heuristic and two machine learning models (a fine-tuned language\nmodel and a simple n-gram model) judged original or adversarial modifications\nof deceptive statements. In Study 2, we manipulated the target alignment of the\nmodifications, i.e. tailoring the attack to whether the statements would be\nassessed by humans or computer models. Results: When adversarial modifications\nwere aligned with their target, human (d=-0.07 and d=-0.04) and machine\njudgments (51% accuracy) dropped to the chance level. When the attack was not\naligned with the target, both human heuristics judgments (d=0.30 and d=0.36)\nand machine learning predictions (63-78%) were significantly better than\nchance. Conclusions: Easily accessible language models can effectively help\nanyone fake deception detection efforts both by humans and machine learning\nmodels. Robustness against adversarial modifications for humans and machines\ndepends on that target alignment. We close with suggestions on advancing\ndeception research with adversarial attack designs."
    },
    {
        "date": "2025-01",
        "title": "Security Testing Framework for Web Applications: Benchmarking ZAP V2.12.0 and V2.13.0 by OWASP as an example",
        "author": "Usha-Sri Potti, Hong-Sheng Huang, Hsuan-Tung Chen, and Hung-Min Sun",
        "link": "http://arxiv.org/abs/2501.05907v1",
        "abstract": "The Huge growth in the usage of web applications has raised concerns\nregarding their security vulnerabilities, which in turn pushes toward robust\nsecurity testing tools. This study compares OWASP ZAP, the leading open-source\nweb application vulnerability scanner, across its two most recent iterations.\nWhile comparing their performance to the OWASP Benchmark, the study evaluates\ntheir efficiency in spotting vulnerabilities in the purposefully vulnerable\napplication, OWASP Benchmark project. The research methodology involves\nconducting systematic scans of OWASP Benchmark using both v2.12.0 and v2.13.0\nof OWASP ZAP. The OWASP Benchmark provides a standardized framework to evaluate\nthe scanner's abilities in identifying security flaws, Insecure Cookies, Path\ntraversal, SQL injection, and more. Results obtained from this benchmark\ncomparison offer valuable insights into the strengths and weaknesses of each\nversion of the tool. This study aids in web application security testing by\nshedding light on how well-known scanners work at spotting vulnerabilities. The\nknowledge gained from this study can assist security professionals and\ndevelopers in making informed decisions to support their web application\nsecurity status. In conclusion, this study comprehensively analyzes ZAP's\ncapabilities in detecting security flaws using OWASP Benchmark v1.2. The\nfindings add to the continuing debates about online application security tools\nand establish the framework for future studies and developments in the research\nfield of web application security testing."
    },
    {
        "date": "2025-01",
        "title": "Fine-tuning is Not Fine: Mitigating Backdoor Attacks in GNNs with Limited Clean Data",
        "author": "Jiale Zhang, Bosen Rao, Chengcheng Zhu, Xiaobing Sun, Qingming Li, Haibo Hu, Xiapu Luo, Qingqing Ye, and Shouling Ji",
        "link": "http://arxiv.org/abs/2501.05835v1",
        "abstract": "Graph Neural Networks (GNNs) have achieved remarkable performance through\ntheir message-passing mechanism. However, recent studies have highlighted the\nvulnerability of GNNs to backdoor attacks, which can lead the model to\nmisclassify graphs with attached triggers as the target class. The\neffectiveness of recent promising defense techniques, such as fine-tuning or\ndistillation, is heavily contingent on having comprehensive knowledge of the\nsufficient training dataset. Empirical studies have shown that fine-tuning\nmethods require a clean dataset of 20% to reduce attack accuracy to below 25%,\nwhile distillation methods require a clean dataset of 15%. However, obtaining\nsuch a large amount of clean data is commonly impractical.\n  In this paper, we propose a practical backdoor mitigation framework, denoted\nas GRAPHNAD, which can capture high-quality intermediate-layer representations\nin GNNs to enhance the distillation process with limited clean data. To achieve\nthis, we address the following key questions: How to identify the appropriate\nattention representations in graphs for distillation? How to enhance\ndistillation with limited data? By adopting the graph attention transfer\nmethod, GRAPHNAD can effectively align the intermediate-layer attention\nrepresentations of the backdoored model with that of the teacher model, forcing\nthe backdoor neurons to transform into benign ones. Besides, we extract the\nrelation maps from intermediate-layer transformation and enforce the relation\nmaps of the backdoored model to be consistent with that of the teacher model,\nthereby ensuring model accuracy while further reducing the influence of\nbackdoors. Extensive experimental results show that by fine-tuning a teacher\nmodel with only 3% of the clean data, GRAPHNAD can reduce the attack success\nrate to below 5%."
    },
    {
        "date": "2025-01",
        "title": "Robust Counterfactual Explanations under Model Multiplicity Using Multi-Objective Optimization",
        "author": "Keita Kinjo",
        "link": "http://arxiv.org/abs/2501.05795v2",
        "abstract": "In recent years, explainability in machine learning has gained importance. In\nthis context, counterfactual explanation (CE), which is an explanation method\nthat uses examples, has attracted attention. However, it has been pointed out\nthat CE is not robust when there are multiple machine-learning models. These\nproblems are important when using machine learning to make safe decisions. In\nthis paper, we propose robust CEs that introduce a new viewpoint - Pareto\nimprovement - and a method that uses multi-objective optimization to generate\nit. To evaluate the proposed method, we conducted experiments using both\nsimulated and actual data. The results demonstrate that the proposed method is\nrobust and useful. We believe that this research will contribute to a wide\nrange of research areas, such as explainability in machine learning,\ndecision-making, and action planning based on machine learning."
    },
    {
        "date": "2025-01",
        "title": "UV-Attack: Physical-World Adversarial Attacks for Person Detection via Dynamic-NeRF-based UV Mapping",
        "author": "Yanjie Li, Wenxuan Zhang, Kaisheng Liang, and Bin Xiao",
        "link": "http://arxiv.org/abs/2501.05783v1",
        "abstract": "In recent research, adversarial attacks on person detectors using patches or\nstatic 3D model-based texture modifications have struggled with low success\nrates due to the flexible nature of human movement. Modeling the 3D\ndeformations caused by various actions has been a major challenge. Fortunately,\nadvancements in Neural Radiance Fields (NeRF) for dynamic human modeling offer\nnew possibilities. In this paper, we introduce UV-Attack, a groundbreaking\napproach that achieves high success rates even with extensive and unseen human\nactions. We address the challenge above by leveraging dynamic-NeRF-based UV\nmapping. UV-Attack can generate human images across diverse actions and\nviewpoints, and even create novel actions by sampling from the SMPL parameter\nspace. While dynamic NeRF models are capable of modeling human bodies,\nmodifying clothing textures is challenging because they are embedded in neural\nnetwork parameters. To tackle this, UV-Attack generates UV maps instead of RGB\nimages and modifies the texture stacks. This approach enables real-time texture\nedits and makes the attack more practical. We also propose a novel Expectation\nover Pose Transformation loss (EoPT) to improve the evasion success rate on\nunseen poses and views. Our experiments show that UV-Attack achieves a 92.75%\nattack success rate against the FastRCNN model across varied poses in dynamic\nvideo settings, significantly outperforming the state-of-the-art AdvCamou\nattack, which only had a 28.50% ASR. Moreover, we achieve 49.5% ASR on the\nlatest YOLOv8 detector in black-box settings. This work highlights the\npotential of dynamic NeRF-based UV mapping for creating more effective\nadversarial attacks on person detectors, addressing key challenges in modeling\nhuman movement and texture modification."
    },
    {
        "date": "2025-01",
        "title": "Underwater Image Enhancement using Generative Adversarial Networks: A Survey",
        "author": "Kancharagunta Kishan Babu, Ashreen Tabassum, Bommakanti Navaneeth, Tenneti Jahnavi, and Yenka Akshaya",
        "link": "http://arxiv.org/abs/2501.06273v1",
        "abstract": "In recent years, there has been a surge of research focused on underwater\nimage enhancement using Generative Adversarial Networks (GANs), driven by the\nneed to overcome the challenges posed by underwater environments. Issues such\nas light attenuation, scattering, and color distortion severely degrade the\nquality of underwater images, limiting their use in critical applications.\nGenerative Adversarial Networks (GANs) have emerged as a powerful tool for\nenhancing underwater photos due to their ability to learn complex\ntransformations and generate realistic outputs. These advancements have been\napplied to real-world applications, including marine biology and ecosystem\nmonitoring, coral reef health assessment, underwater archaeology, and\nautonomous underwater vehicle (AUV) navigation. This paper explores all major\napproaches to underwater image enhancement, from physical and physics-free\nmodels to Convolutional Neural Network (CNN)-based models and state-of-the-art\nGAN-based methods. It provides a comprehensive analysis of these methods,\nevaluation metrics, datasets, and loss functions, offering a holistic view of\nthe field. Furthermore, the paper delves into the limitations and challenges\nfaced by current methods, such as generalization issues, high computational\ndemands, and dataset biases, while suggesting potential directions for future\nresearch."
    },
    {
        "date": "2025-01",
        "title": "Learning-based Detection of GPS Spoofing Attack for Quadrotors",
        "author": "Pengyu Wang, Zhaohua Yang, Jialu Li, and Ling Shi",
        "link": "http://arxiv.org/abs/2501.07597v1",
        "abstract": "Safety-critical cyber-physical systems (CPS), such as quadrotor UAVs, are\nparticularly prone to cyber attacks, which can result in significant\nconsequences if not detected promptly and accurately. During outdoor\noperations, the nonlinear dynamics of UAV systems, combined with non-Gaussian\nnoise, pose challenges to the effectiveness of conventional statistical and\nmachine learning methods. To overcome these limitations, we present QUADFormer,\nan advanced attack detection framework for quadrotor UAVs leveraging a\ntransformer-based architecture. This framework features a residue generator\nthat produces sequences sensitive to anomalies, which are then analyzed by the\ntransformer to capture statistical patterns for detection and classification.\nFurthermore, an alert mechanism ensures UAVs can operate safely even when under\nattack. Extensive simulations and experimental evaluations highlight that\nQUADFormer outperforms existing state-of-the-art techniques in detection\naccuracy."
    },
    {
        "date": "2025-01",
        "title": "An Efficient Key Expansion Method Applied to Security Credential Management System",
        "author": "Abel C. H. Chen",
        "link": "http://arxiv.org/abs/2501.05627v1",
        "abstract": "In recent years, U.S. Department of Transportation has adopts Institute of\nElectrical and Electronics Engineers (IEEE) 1609 series to build the security\ncredential management system (SCMS) for being the standard of connected cars in\nU.S. Furthermore, a butterfly key expansion (BKE) method in SCMS has been\ndesigned to provide pseudonym certificates for improving the privacy of\nconnected cars. However, the BKE method is designed based on elliptic curve\ncryptography (ECC) in the standard of IEEE 1609.2.1, but more execution time is\nrequired for key expansion. Therefore, this study proposes an original\nefficient key expansion method, and the mathematical principles have been\nproposed to prove the encryption/decryption feasibility, car privacy, and\nmethod efficiency. In a practical environment, the proposed method improves the\nefficiency of key expansion method in IEEE 1609.2.1-2022 with the same security\nstrength thousands of times."
    },
    {
        "date": "2025-01",
        "title": "Session-Level Dynamic Ad Load Optimization using Offline Robust Reinforcement Learning",
        "author": "Tao Liu, Qi Xu, Wei Shi, Zhigang Hua, and Shuang Yang",
        "link": "http://arxiv.org/abs/2501.05591v1",
        "abstract": "Session-level dynamic ad load optimization aims to personalize the density\nand types of delivered advertisements in real time during a user's online\nsession by dynamically balancing user experience quality and ad monetization.\nTraditional causal learning-based approaches struggle with key technical\nchallenges, especially in handling confounding bias and distribution shifts. In\nthis paper, we develop an offline deep Q-network (DQN)-based framework that\neffectively mitigates confounding bias in dynamic systems and demonstrates more\nthan 80% offline gains compared to the best causal learning-based production\nbaseline. Moreover, to improve the framework's robustness against unanticipated\ndistribution shifts, we further enhance our framework with a novel offline\nrobust dueling DQN approach. This approach achieves more stable rewards on\nmultiple OpenAI-Gym datasets as perturbations increase, and provides an\nadditional 5% offline gains on real-world ad delivery data.\n  Deployed across multiple production systems, our approach has achieved\noutsized topline gains. Post-launch online A/B tests have shown double-digit\nimprovements in the engagement-ad score trade-off efficiency, significantly\nenhancing our platform's capability to serve both consumers and advertisers."
    },
    {
        "date": "2025-01",
        "title": "Enforcing Fundamental Relations via Adversarial Attacks on Input Parameter Correlations",
        "author": "Timo Saala, Lucie Flek, Alexander Jung, Akbar Karimi, Alexander Schmidt, Matthias Schott, Philipp Soldin, and Christopher Wiebusch",
        "link": "http://arxiv.org/abs/2501.05588v1",
        "abstract": "Correlations between input parameters play a crucial role in many scientific\nclassification tasks, since these are often related to fundamental laws of\nnature. For example, in high energy physics, one of the common deep learning\nuse-cases is the classification of signal and background processes in particle\ncollisions. In many such cases, the fundamental principles of the correlations\nbetween observables are often better understood than the actual distributions\nof the observables themselves. In this work, we present a new adversarial\nattack algorithm called Random Distribution Shuffle Attack (RDSA), emphasizing\nthe correlations between observables in the network rather than individual\nfeature characteristics. Correct application of the proposed novel attack can\nresult in a significant improvement in classification performance -\nparticularly in the context of data augmentation - when using the generated\nadversaries within adversarial training. Given that correlations between input\nfeatures are also crucial in many other disciplines. We demonstrate the RDSA\neffectiveness on six classification tasks, including two particle collision\nchallenges (using CERN Open Data), hand-written digit recognition (MNIST784),\nhuman activity recognition (HAR), weather forecasting (Rain in Australia), and\nICU patient mortality (MIMIC-IV), demonstrating a general use case beyond\nfundamental physics for this new type of adversarial attack algorithms."
    },
    {
        "date": "2025-01",
        "title": "Is Your Autonomous Vehicle Safe? Understanding the Threat of Electromagnetic Signal Injection Attacks on Traffic Scene Perception",
        "author": "Wenhao Liao, Sineng Yan, Youqian Zhang, Xinwei Zhai, Yuanyuan Wang, and Eugene Yujun Fu",
        "link": "http://arxiv.org/abs/2501.05239v1",
        "abstract": "Autonomous vehicles rely on camera-based perception systems to comprehend\ntheir driving environment and make crucial decisions, thereby ensuring vehicles\nto steer safely. However, a significant threat known as Electromagnetic Signal\nInjection Attacks (ESIA) can distort the images captured by these cameras,\nleading to incorrect AI decisions and potentially compromising the safety of\nautonomous vehicles. Despite the serious implications of ESIA, there is limited\nunderstanding of its impacts on the robustness of AI models across various and\ncomplex driving scenarios. To address this gap, our research analyzes the\nperformance of different models under ESIA, revealing their vulnerabilities to\nthe attacks. Moreover, due to the challenges in obtaining real-world attack\ndata, we develop a novel ESIA simulation method and generate a simulated attack\ndataset for different driving scenarios. Our research provides a comprehensive\nsimulation and evaluation framework, aiming to enhance the development of more\nrobust AI models and secure intelligent systems, ultimately contributing to the\nadvancement of safer and more reliable technology across various fields."
    }
]