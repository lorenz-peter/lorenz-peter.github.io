[
    {
        "date": "2025-02",
        "title": "PRVQL: Progressive Knowledge-guided Refinement for Robust Egocentric Visual Query Localization",
        "author": "Bing Fan, Yunhe Feng, Yapeng Tian, Yuewei Lin, Yan Huang, and Heng Fan",
        "link": "http://arxiv.org/abs/2502.07707v1",
        "abstract": "Egocentric visual query localization (EgoVQL) focuses on localizing the\ntarget of interest in space and time from first-person videos, given a visual\nquery. Despite recent progressive, existing methods often struggle to handle\nsevere object appearance changes and cluttering background in the video due to\nlacking sufficient target cues, leading to degradation. Addressing this, we\nintroduce PRVQL, a novel Progressive knowledge-guided Refinement framework for\nEgoVQL. The core is to continuously exploit target-relevant knowledge directly\nfrom videos and utilize it as guidance to refine both query and video features\nfor improving target localization. Our PRVQL contains multiple processing\nstages. The target knowledge from one stage, comprising appearance and spatial\nknowledge extracted via two specially designed knowledge learning modules, are\nutilized as guidance to refine the query and videos features for the next\nstage, which are used to generate more accurate knowledge for further feature\nrefinement. With such a progressive process, target knowledge in PRVQL can be\ngradually improved, which, in turn, leads to better refined query and video\nfeatures for localization in the final stage. Compared to previous methods, our\nPRVQL, besides the given object cues, enjoys additional crucial target\ninformation from a video as guidance to refine features, and hence enhances\nEgoVQL in complicated scenes. In our experiments on challenging Ego4D, PRVQL\nachieves state-of-the-art result and largely surpasses other methods, showing\nits efficacy. Our code, model and results will be released at\nhttps://github.com/fb-reps/PRVQL."
    },
    {
        "date": "2025-02",
        "title": "JBShield: Defending Large Language Models from Jailbreak Attacks through Activated Concept Analysis and Manipulation",
        "author": "Shenyi Zhang, Yuchen Zhai, Keyan Guo, Hongxin Hu, Shengnan Guo, Zheng Fang, Lingchen Zhao, Chao Shen, Cong Wang, and Qian Wang",
        "link": "http://arxiv.org/abs/2502.07557v1",
        "abstract": "Despite the implementation of safety alignment strategies, large language\nmodels (LLMs) remain vulnerable to jailbreak attacks, which undermine these\nsafety guardrails and pose significant security threats. Some defenses have\nbeen proposed to detect or mitigate jailbreaks, but they are unable to\nwithstand the test of time due to an insufficient understanding of jailbreak\nmechanisms. In this work, we investigate the mechanisms behind jailbreaks based\non the Linear Representation Hypothesis (LRH), which states that neural\nnetworks encode high-level concepts as subspaces in their hidden\nrepresentations. We define the toxic semantics in harmful and jailbreak prompts\nas toxic concepts and describe the semantics in jailbreak prompts that\nmanipulate LLMs to comply with unsafe requests as jailbreak concepts. Through\nconcept extraction and analysis, we reveal that LLMs can recognize the toxic\nconcepts in both harmful and jailbreak prompts. However, unlike harmful\nprompts, jailbreak prompts activate the jailbreak concepts and alter the LLM\noutput from rejection to compliance. Building on our analysis, we propose a\ncomprehensive jailbreak defense framework, JBShield, consisting of two key\ncomponents: jailbreak detection JBShield-D and mitigation JBShield-M.\nJBShield-D identifies jailbreak prompts by determining whether the input\nactivates both toxic and jailbreak concepts. When a jailbreak prompt is\ndetected, JBShield-M adjusts the hidden representations of the target LLM by\nenhancing the toxic concept and weakening the jailbreak concept, ensuring LLMs\nproduce safe content. Extensive experiments demonstrate the superior\nperformance of JBShield, achieving an average detection accuracy of 0.95 and\nreducing the average attack success rate of various jailbreak attacks to 2%\nfrom 61% across distinct LLMs."
    },
    {
        "date": "2025-02",
        "title": "CodePhys: Robust Video-based Remote Physiological Measurement through Latent Codebook Querying",
        "author": "Shuyang Chu, Menghan Xia, Mengyao Yuan, Xin Liu, Tapio Seppanen, Guoying Zhao, and Jingang Shi",
        "link": "http://arxiv.org/abs/2502.07526v1",
        "abstract": "Remote photoplethysmography (rPPG) aims to measure non-contact physiological\nsignals from facial videos, which has shown great potential in many\napplications. Most existing methods directly extract video-based rPPG features\nby designing neural networks for heart rate estimation. Although they can\nachieve acceptable results, the recovery of rPPG signal faces intractable\nchallenges when interference from real-world scenarios takes place on facial\nvideo. Specifically, facial videos are inevitably affected by non-physiological\nfactors (e.g., camera device noise, defocus, and motion blur), leading to the\ndistortion of extracted rPPG signals. Recent rPPG extraction methods are easily\naffected by interference and degradation, resulting in noisy rPPG signals. In\nthis paper, we propose a novel method named CodePhys, which innovatively treats\nrPPG measurement as a code query task in a noise-free proxy space (i.e.,\ncodebook) constructed by ground-truth PPG signals. We consider noisy rPPG\nfeatures as queries and generate high-fidelity rPPG features by matching them\nwith noise-free PPG features from the codebook. Our approach also incorporates\na spatial-aware encoder network with a spatial attention mechanism to highlight\nphysiologically active areas and uses a distillation loss to reduce the\ninfluence of non-periodic visual interference. Experimental results on four\nbenchmark datasets demonstrate that CodePhys outperforms state-of-the-art\nmethods in both intra-dataset and cross-dataset settings."
    },
    {
        "date": "2025-02",
        "title": "RoMA: Robust Malware Attribution via Byte-level Adversarial Training with Global Perturbations and Adversarial Consistency Regularization",
        "author": "Yuxia Sun, Huihong Chen, Jingcai Guo, Aoxiang Sun, Zhetao Li, and Haolin Liu",
        "link": "http://arxiv.org/abs/2502.07492v1",
        "abstract": "Attributing APT (Advanced Persistent Threat) malware to their respective\ngroups is crucial for threat intelligence and cybersecurity. However, APT\nadversaries often conceal their identities, rendering attribution inherently\nadversarial. Existing machine learning-based attribution models, while\neffective, remain highly vulnerable to adversarial attacks. For example, the\nstate-of-the-art byte-level model MalConv sees its accuracy drop from over 90%\nto below 2% under PGD (projected gradient descent) attacks. Existing\ngradient-based adversarial training techniques for malware detection or image\nprocessing were applied to malware attribution in this study, revealing that\nboth robustness and training efficiency require significant improvement. To\naddress this, we propose RoMA, a novel single-step adversarial training\napproach that integrates global perturbations to generate enhanced adversarial\nsamples and employs adversarial consistency regularization to improve\nrepresentation quality and resilience. A novel APT malware dataset named AMG18,\nwith diverse samples and realistic class imbalances, is introduced for\nevaluation. Extensive experiments show that RoMA significantly outperforms\nseven competing methods in both adversarial robustness (e.g., achieving over\n80% robust accuracy-more than twice that of the next-best method under PGD\nattacks) and training efficiency (e.g., more than twice as fast as the\nsecond-best method in terms of accuracy), while maintaining superior standard\naccuracy in non-adversarial scenarios."
    },
    {
        "date": "2025-02",
        "title": "MoENAS: Mixture-of-Expert based Neural Architecture Search for jointly Accurate, Fair, and Robust Edge Deep Neural Networks",
        "author": "Lotfi Abdelkrim Mecharbat, Alberto Marchisio, Muhammad Shafique, Mohammad M. Ghassemi, and Tuka Alhanai",
        "link": "http://arxiv.org/abs/2502.07422v1",
        "abstract": "There has been a surge in optimizing edge Deep Neural Networks (DNNs) for\naccuracy and efficiency using traditional optimization techniques such as\npruning, and more recently, employing automatic design methodologies. However,\nthe focus of these design techniques has often overlooked critical metrics such\nas fairness, robustness, and generalization. As a result, when evaluating SOTA\nedge DNNs' performance in image classification using the FACET dataset, we\nfound that they exhibit significant accuracy disparities (14.09%) across 10\ndifferent skin tones, alongside issues of non-robustness and poor\ngeneralizability. In response to these observations, we introduce\nMixture-of-Experts-based Neural Architecture Search (MoENAS), an automatic\ndesign technique that navigates through a space of mixture of experts to\ndiscover accurate, fair, robust, and general edge DNNs. MoENAS improves the\naccuracy by 4.02% compared to SOTA edge DNNs and reduces the skin tone accuracy\ndisparities from 14.09% to 5.60%, while enhancing robustness by 3.80% and\nminimizing overfitting to 0.21%, all while keeping model size close to\nstate-of-the-art models average size (+0.4M). With these improvements, MoENAS\nestablishes a new benchmark for edge DNN design, paving the way for the\ndevelopment of more inclusive and robust edge DNNs."
    },
    {
        "date": "2025-02",
        "title": "Mining Power Destruction Attacks in the Presence of Petty-Compliant Mining Pools",
        "author": "Roozbeh Sarenche, Svetla Nikova, and Bart Preneel",
        "link": "http://arxiv.org/abs/2502.07410v1",
        "abstract": "Bitcoin's security relies on its Proof-of-Work consensus, where miners solve\npuzzles to propose blocks. The puzzle's difficulty is set by the difficulty\nadjustment mechanism (DAM), based on the network's available mining power.\nAttacks that destroy some portion of mining power can exploit the DAM to lower\ndifficulty, making such attacks profitable. In this paper, we analyze three\ntypes of mining power destruction attacks in the presence of petty-compliant\nmining pools: selfish mining, bribery, and mining power distraction attacks. We\nanalyze selfish mining while accounting for the distribution of mining power\namong pools, a factor often overlooked in the literature. Our findings indicate\nthat selfish mining can be more destructive when the non-adversarial mining\nshare is well distributed among pools. We also introduce a novel bribery\nattack, where the adversarial pool bribes petty-compliant pools to orphan\nothers' blocks. For small pools, we demonstrate that the bribery attack can\ndominate strategies like selfish mining or undercutting. Lastly, we present the\nmining distraction attack, where the adversarial pool incentivizes\npetty-compliant pools to abandon Bitcoin's puzzle and mine for a simpler\npuzzle, thus wasting some part of their mining power. Similar to the previous\nattacks, this attack can lower the mining difficulty, but with the difference\nthat it does not generate any evidence of mining power destruction, such as\norphan blocks."
    },
    {
        "date": "2025-02",
        "title": "Robust Indoor Localization in Dynamic Environments: A Multi-source Unsupervised Domain Adaptation Framework",
        "author": "Jiyu Jiao, Xiaojun Wang, and Chengpei Han",
        "link": "http://arxiv.org/abs/2502.07246v1",
        "abstract": "Fingerprint localization has gained significant attention due to its\ncost-effective deployment, low complexity, and high efficacy. However,\ntraditional methods, while effective for static data, often struggle in dynamic\nenvironments where data distributions and feature spaces evolve-a common\noccurrence in real-world scenarios. To address the challenges of robustness and\nadaptability in fingerprint localization for dynamic indoor environments, this\npaper proposes DF-Loc, an end-to-end dynamic fingerprint localization system\nbased on multi-source unsupervised domain adaptation (MUDA). DF-Loc leverages\nhistorical data from multiple time scales to facilitate knowledge transfer in\nspecific feature spaces, thereby enhancing generalization capabilities in the\ntarget domain and reducing reliance on labeled data. Specifically, the system\nincorporates a Quality Control (QC) module for CSI data preprocessing and\nemploys image processing techniques for CSI fingerprint feature reconstruction.\nAdditionally, a multi-scale attention-based feature fusion backbone network is\ndesigned to extract multi-level transferable fingerprint features. Finally, a\ndual-stage alignment model aligns the distributions of multiple source-target\ndomain pairs, improving regression characteristics in the target domain.\nExtensive experiments conducted in office and classroom environments\ndemonstrate that DF-Loc outperforms comparative methods in terms of both\nlocalization accuracy and robustness. With 60% of reference points used for\ntraining, DF-Loc achieves average localization errors of 0.79m and 3.72m in\n\"same-test\" scenarios, and 0.94m and 4.39m in \"different-test\" scenarios,\nrespectively. This work pioneers an end-to-end multi-source transfer learning\napproach for fingerprint localization, providing valuable insights for future\nresearch in dynamic environments."
    },
    {
        "date": "2025-02",
        "title": "Simplifying Adversarially Robust PAC Learning with Tolerance",
        "author": "Hassan Ashtiani, Vinayak Pathak, and Ruth Urner",
        "link": "http://arxiv.org/abs/2502.07232v1",
        "abstract": "Adversarially robust PAC learning has proved to be challenging, with the\ncurrently best known learners [Montasser et al., 2021a] relying on improper\nmethods based on intricate compression schemes, resulting in sample complexity\nexponential in the VC-dimension. A series of follow up work considered a\nslightly relaxed version of the problem called adversarially robust learning\nwith tolerance [Ashtiani et al., 2023, Bhattacharjee et al., 2023, Raman et\nal., 2024] and achieved better sample complexity in terms of the VC-dimension.\nHowever, those algorithms were either improper and complex, or required\nadditional assumptions on the hypothesis class H. We prove, for the first time,\nthe existence of a simpler learner that achieves a sample complexity linear in\nthe VC-dimension without requiring additional assumptions on H. Even though our\nlearner is improper, it is \"almost proper\" in the sense that it outputs a\nhypothesis that is \"similar\" to a hypothesis in H.\n  We also use the ideas from our algorithm to construct a semi-supervised\nlearner in the tolerant setting. This simple algorithm achieves comparable\nbounds to the previous (non-tolerant) semi-supervised algorithm of Attias et\nal. [2022a], but avoids the use of intricate subroutines from previous works,\nand is \"almost proper.\""
    },
    {
        "date": "2025-02",
        "title": "CAT: Contrastive Adversarial Training for Evaluating the Robustness of Protective Perturbations in Latent Diffusion Models",
        "author": "Sen Peng, Mingyue Wang, Jianfei He, Jijia Yang, and Xiaohua Jia",
        "link": "http://arxiv.org/abs/2502.07225v1",
        "abstract": "Latent diffusion models have recently demonstrated superior capabilities in\nmany downstream image synthesis tasks. However, customization of latent\ndiffusion models using unauthorized data can severely compromise the privacy\nand intellectual property rights of data owners. Adversarial examples as\nprotective perturbations have been developed to defend against unauthorized\ndata usage by introducing imperceptible noise to customization samples,\npreventing diffusion models from effectively learning them. In this paper, we\nfirst reveal that the primary reason adversarial examples are effective as\nprotective perturbations in latent diffusion models is the distortion of their\nlatent representations, as demonstrated through qualitative and quantitative\nexperiments. We then propose the Contrastive Adversarial Training (CAT)\nutilizing adapters as an adaptive attack against these protection methods,\nhighlighting their lack of robustness. Extensive experiments demonstrate that\nour CAT method significantly reduces the effectiveness of protective\nperturbations in customization configurations, urging the community to\nreconsider and enhance the robustness of existing protective perturbation\nmethods. Code is available at \\hyperlink{here}{https://github.com/senp98/CAT}."
    },
    {
        "date": "2025-02",
        "title": "Color-Quality Invariance for Robust Medical Image Segmentation",
        "author": "Ravi Shah, Atsushi Fukuda, and Quan Huu Cap",
        "link": "http://arxiv.org/abs/2502.07200v1",
        "abstract": "Single-source domain generalization (SDG) in medical image segmentation\nremains a significant challenge, particularly for images with varying color\ndistributions and qualities. Previous approaches often struggle when models\ntrained on high-quality images fail to generalize to low-quality test images\ndue to these color and quality shifts. In this work, we propose two novel\ntechniques to enhance generalization: dynamic color image normalization (DCIN)\nmodule and color-quality generalization (CQG) loss. The DCIN dynamically\nnormalizes the color of test images using two reference image selection\nstrategies. Specifically, the DCIN utilizes a global reference image selection\n(GRIS), which finds a universal reference image, and a local reference image\nselection (LRIS), which selects a semantically similar reference image per test\nsample. Additionally, CQG loss enforces invariance to color and quality\nvariations by ensuring consistent segmentation predictions across transformed\nimage pairs. Experimental results show that our proposals significantly improve\nsegmentation performance over the baseline on two target domain datasets,\ndespite being trained solely on a single source domain. Notably, our model\nachieved up to a 32.3-point increase in Dice score compared to the baseline,\nconsistently producing robust and usable results even under substantial domain\nshifts. Our work contributes to the development of more robust medical image\nsegmentation models that generalize across unseen domains. The implementation\ncode is available at https://github.com/RaviShah1/DCIN-CQG."
    },
    {
        "date": "2025-02",
        "title": "Enhancing Robustness Of Digital Shadow For CO2 Storage Monitoring With Augmented Rock Physics Modeling",
        "author": "Abhinav Prakash Gahlot, and Felix J. Herrmann",
        "link": "http://arxiv.org/abs/2502.07171v1",
        "abstract": "To meet climate targets, the IPCC underscores the necessity of technologies\ncapable of removing gigatonnes of CO2 annually, with Geological Carbon Storage\n(GCS) playing a central role. GCS involves capturing CO2 and injecting it into\ndeep geological formations for long-term storage, requiring precise monitoring\nto ensure containment and prevent leakage. Time-lapse seismic imaging is\nessential for tracking CO2 migration but often struggles to capture the\ncomplexities of multi-phase subsurface flow. Digital Shadows (DS), leveraging\nmachine learning-driven data assimilation techniques such as nonlinear Bayesian\nfiltering and generative AI, provide a more detailed, uncertainty-aware\nmonitoring approach. By incorporating uncertainties in reservoir properties, DS\nframeworks improve CO2 migration forecasts, reducing risks in GCS operations.\nHowever, data assimilation depends on assumptions regarding reservoir\nproperties, rock physics models, and initial conditions, which, if inaccurate,\ncan compromise prediction reliability. This study demonstrates that augmenting\nforecast ensembles with diverse rock physics models mitigates the impact of\nincorrect assumptions and improves predictive accuracy, particularly in\ndifferentiating uniform versus patchy saturation models."
    },
    {
        "date": "2025-02",
        "title": "Does Training on Synthetic Data Make Models Less Robust?",
        "author": "Lingze Zhang, and Ellie Pavlick",
        "link": "http://arxiv.org/abs/2502.07164v1",
        "abstract": "An increasingly common practice is to train large language models (LLMs)\nusing synthetic data. Often this synthetic data is produced by the same or\nsimilar LLMs as those it is being used to train. This raises the question of\nwhether the synthetic data might in fact exacerbate certain \"blindspots\" by\nreinforcing heuristics that the LLM already encodes. In this paper, we conduct\nsimulated experiments on the natural language inference (NLI) task with\nLlama-2-7B-hf models. We use MultiNLI as the general task and HANS, a targeted\nevaluation set designed to measure the presence of specific heuristic\nstrategies for NLI, as our \"blindspot\" task. Our goal is to determine whether\nperformance disparities between the general and blind spot tasks emerge. Our\nresults indicate that synthetic data does not reinforce blindspots in the way\nwe expected. Specifically, we see that, while fine-tuning with synthetic data\ndoesn't necessarily reduce the use of the heuristic, it also does not make it\nworse as we hypothesized."
    },
    {
        "date": "2025-02",
        "title": "Towards a Robust Framework for Multimodal Hate Detection: A Study on Video vs. Image-based Content",
        "author": "Girish A. Koushik, Diptesh Kanojia, and Helen Treharne",
        "link": "http://arxiv.org/abs/2502.07138v1",
        "abstract": "Social media platforms enable the propagation of hateful content across\ndifferent modalities such as textual, auditory, and visual, necessitating\neffective detection methods. While recent approaches have shown promise in\nhandling individual modalities, their effectiveness across different modality\ncombinations remains unexplored. This paper presents a systematic analysis of\nfusion-based approaches for multimodal hate detection, focusing on their\nperformance across video and image-based content. Our comprehensive evaluation\nreveals significant modality-specific limitations: while simple embedding\nfusion achieves state-of-the-art performance on video content (HateMM dataset)\nwith a 9.9% points F1-score improvement, it struggles with complex image-text\nrelationships in memes (Hateful Memes dataset). Through detailed ablation\nstudies and error analysis, we demonstrate how current fusion approaches fail\nto capture nuanced cross-modal interactions, particularly in cases involving\nbenign confounders. Our findings provide crucial insights for developing more\nrobust hate detection systems and highlight the need for modality-specific\narchitectural considerations. The code is available at\nhttps://github.com/gak97/Video-vs-Meme-Hate."
    },
    {
        "date": "2025-02",
        "title": "Game of Coding With an Unknown Adversary",
        "author": "Hanzaleh Akbarinodehi, Parsa Moradi, and Mohammad Ali Maddah-Ali",
        "link": "http://arxiv.org/abs/2502.07109v1",
        "abstract": "Motivated by emerging decentralized applications, the \\emph{game of coding}\nframework has been recently introduced to address scenarios where the\nadversary's control over coded symbols surpasses the fundamental limits of\ntraditional coding theory. Still, the reward mechanism available in\ndecentralized systems, motivates the adversary to act rationally. While the\ndecoder, as the data collector (DC), has an acceptance and rejection mechanism,\nfollowed by an estimation module, the adversary aims to maximize its utility,\nas an increasing function of (1) the chance of acceptance (to increase the\nreward), and (2) estimation error. On the other hand, the decoder also adjusts\nits acceptance rule to maximize its own utility, as (1) an increasing function\nof the chance of acceptance (to keep the system functional), (2) decreasing\nfunction of the estimation error. Prior works within this framework rely on the\nassumption that the game is complete, that is, both the DC and the adversary\nare fully aware of each other's utility functions. However, in practice, the\ndecoder is often unaware of the utility of the adversary. To address this\nlimitation, we develop an algorithm enabling the DC to commit to a strategy\nthat achieves within the vicinity of the equilibrium, without knowledge of the\nadversary's utility function. Our approach builds on an observation that at the\nequilibrium, the relationship between the probability of acceptance and the\nmean squared error (MSE) follows a predetermined curve independent of the\nspecific utility functions of the players. By exploiting this invariant\nrelationship, the DC can iteratively refine its strategy based on observable\nparameters, converging to a near-optimal solution. We provide theoretical\nguarantees on sample complexity and accuracy of the proposed scheme."
    },
    {
        "date": "2025-02",
        "title": "Large Language Models in Software Security: A Survey of Vulnerability Detection Techniques and Insights",
        "author": "Ze Sheng, Zhicheng Chen, Shuning Gu, Heqing Huang, Guofei Gu, and Jeff Huang",
        "link": "http://arxiv.org/abs/2502.07049v1",
        "abstract": "Large Language Models (LLMs) are emerging as transformative tools for\nsoftware vulnerability detection, addressing critical challenges in the\nsecurity domain. Traditional methods, such as static and dynamic analysis,\noften falter due to inefficiencies, high false positive rates, and the growing\ncomplexity of modern software systems. By leveraging their ability to analyze\ncode structures, identify patterns, and generate repair sugges- tions, LLMs,\nexemplified by models like GPT, BERT, and CodeBERT, present a novel and\nscalable approach to mitigating vulnerabilities. This paper provides a detailed\nsurvey of LLMs in vulnerability detection. It examines key aspects, including\nmodel architectures, application methods, target languages, fine-tuning\nstrategies, datasets, and evaluation metrics. We also analyze the scope of\ncurrent research problems, highlighting the strengths and weaknesses of\nexisting approaches. Further, we address challenges such as cross-language\nvulnerability detection, multimodal data integration, and repository-level\nanalysis. Based on these findings, we propose solutions for issues like dataset\nscalability, model interpretability, and applications in low-resource\nscenarios. Our contributions are threefold: (1) a systematic review of how LLMs\nare applied in vulnerability detection; (2) an analysis of shared patterns and\ndifferences across studies, with a unified framework for understanding the\nfield; and (3) a summary of key challenges and future research directions. This\nwork provides valuable insights for advancing LLM-based vulnerability\ndetection. We also maintain and regularly update latest selected paper on\nhttps://github.com/OwenSanzas/LLM-For-Vulnerability-Detection"
    },
    {
        "date": "2025-02",
        "title": "AstroLoc: Robust Space to Ground Image Localizer",
        "author": "Gabriele Berton, Alex Stoken, and Carlo Masone",
        "link": "http://arxiv.org/abs/2502.07003v1",
        "abstract": "Astronauts take thousands of photos of Earth per day from the International\nSpace Station, which, once localized on Earth's surface, are used for a\nmultitude of tasks, ranging from climate change research to disaster\nmanagement. The localization process, which has been performed manually for\ndecades, has recently been approached through image retrieval solutions: given\nan astronaut photo, find its most similar match among a large database of\ngeo-tagged satellite images, in a task called Astronaut Photography\nLocalization (APL). Yet, existing APL approaches are trained only using\nsatellite images, without taking advantage of the millions open-source\nastronaut photos. In this work we present the first APL pipeline capable of\nleveraging astronaut photos for training. We first produce full localization\ninformation for 300,000 manually weakly labeled astronaut photos through an\nautomated pipeline, and then use these images to train a model, called\nAstroLoc. AstroLoc learns a robust representation of Earth's surface features\nthrough two losses: astronaut photos paired with their matching satellite\ncounterparts in a pairwise loss, and a second loss on clusters of satellite\nimagery weighted by their relevance to astronaut photography via unsupervised\nmining. We find that AstroLoc achieves a staggering 35% average improvement in\nrecall@1 over previous SOTA, pushing the limits of existing datasets with a\nrecall@100 consistently over 99%. Finally, we note that AstroLoc, without any\nfine-tuning, provides excellent results for related tasks like the\nlost-in-space satellite problem and historical space imagery localization."
    },
    {
        "date": "2025-02",
        "title": "Resurrecting saturated LLM benchmarks with adversarial encoding",
        "author": "Igor Ivanov, and Dmitrii Volkov",
        "link": "http://arxiv.org/abs/2502.06738v1",
        "abstract": "Recent work showed that small changes in benchmark questions can reduce LLMs'\nreasoning and recall. We explore two such changes: pairing questions and adding\nmore answer options, on three benchmarks: WMDP-bio, GPQA, and MMLU variants. We\nfind that for more capable models, these predictably reduce performance,\nessentially heightening the performance ceiling of a benchmark and unsaturating\nit again. We suggest this approach can resurrect old benchmarks."
    },
    {
        "date": "2025-02",
        "title": "Pinning Is Futile: You Need More Than Local Dependency Versioning to Defend against Supply Chain Attacks",
        "author": "Hao He, Bogdan Vasilescu, and Christian K\u00e4stner",
        "link": "http://arxiv.org/abs/2502.06662v1",
        "abstract": "Recent high-profile incidents in open-source software have greatly raised\npractitioner attention on software supply chain attacks. To guard against\npotential malicious package updates, security practitioners advocate pinning\ndependency to specific versions rather than floating in version ranges.\nHowever, it remains controversial whether pinning carries a meaningful security\nbenefit that outweighs the cost of maintaining outdated and possibly vulnerable\ndependencies. In this paper, we quantify, through counterfactual analysis and\nsimulations, the security and maintenance impact of version constraints in the\nnpm ecosystem. By simulating dependency resolutions over historical time\npoints, we find that pinning direct dependencies not only (as expected)\nincreases the cost of maintaining vulnerable and outdated dependencies, but\nalso (surprisingly) even increases the risk of exposure to malicious package\nupdates in larger dependency graphs due to the specifics of npm's dependency\nresolution mechanism. Finally, we explore collective pinning strategies to\nsecure the ecosystem against supply chain attacks, suggesting specific changes\nto npm to enable such interventions. Our study provides guidance for\npractitioners and tool designers to manage their supply chains more securely."
    },
    {
        "date": "2025-02",
        "title": "Automatic ISA analysis for Secure Context Switching",
        "author": "Neelu S. Kalani, Thomas Bourgeat, Guerney D. H. Hunt, and Wojciech Ozga",
        "link": "http://arxiv.org/abs/2502.06609v1",
        "abstract": "Instruction set architectures are complex, with hundreds of registers and\ninstructions that can modify dozens of them during execution, variably on each\ninstance. Prose-style ISA specifications struggle to capture these intricacies\nof the ISAs, where often the important details about a single register are\nspread out across hundreds of pages of documentation. Ensuring that all\nISA-state is swapped in context switch implementations of privileged software\nrequires meticulous examination of these pages. This manual process is tedious\nand error-prone.\n  We propose a tool called Sailor that leverages machine-readable ISA\nspecifications written in Sail to automate this task. Sailor determines the\nISA-state necessary to swap during the context switch using the data collected\nfrom Sail and a novel algorithm to classify ISA-state as security-sensitive.\nUsing Sailor's output, we identify three different classes of mishandled\nISA-state across four open-source confidential computing systems. We further\nreveal five distinct security vulnerabilities that can be exploited using the\nmishandled ISA-state. This research exposes an often overlooked attack surface\nthat stems from mishandled ISA-state, enabling unprivileged adversaries to\nexploit system vulnerabilities."
    },
    {
        "date": "2025-02",
        "title": "Robust Scatter Matrix Estimation for Elliptical Distributions in Polynomial Time",
        "author": "Gleb Novikov",
        "link": "http://arxiv.org/abs/2502.06564v1",
        "abstract": "We study the problem of computationally efficient robust estimation of\nscatter matrices of elliptical distributions under the strong contamination\nmodel. We design polynomial time algorithms that achieve dimension-independent\nerror in Frobenius norm.\n  Our first result is a sequence of efficient algorithms that approaches nearly\noptimal error. Specifically, under a mild assumption on the eigenvalues of the\nscatter matrix $\\Sigma$, for every $t \\in \\mathbb{N}$, we design an estimator\nthat, given $n = d^{O(t)}$ samples, in time $n^{O(t)}$ finds $\\hat{\\Sigma}$\nsuch that $ \\Vert{\\Sigma^{-1/2}\\, ({\\hat{\\Sigma} - \\Sigma})\\,\n\\Sigma^{-1/2}}\\Vert_{\\text{F}} \\le O(t \\cdot \\varepsilon^{1-\\frac{1}{t}})$,\nwhere $\\varepsilon$ is the fraction of corruption. We do not require any\nassumptions on the moments of the distribution, while all previously known\ncomputationally efficient algorithms for robust covariance/scatter estimation\nwith dimension-independent error rely on strong assumptions on the moments,\nsuch as sub-Gaussianity or (certifiable) hypercontractivity.\n  Furthermore, under a stronger assumption on the eigenvalues of $\\Sigma$\n(that, in particular, is satisfied by all matrices with constant condition\nnumber),\n  we provide a fast (sub-quadratic in the input size) algorithm that, given\nnearly optimal number of samples $n = \\tilde{O}(d^2/\\varepsilon)$, in time\n$\\tilde{O}({nd^2 poly(1/\\varepsilon)})$ finds $\\hat{\\Sigma}$ such that\n$\\Vert\\hat{\\Sigma} - \\Sigma\\Vert_{\\text{F}} \\le O(\\Vert{\\Sigma}\\Vert \\cdot\n\\sqrt{\\varepsilon})$.\n  Our approach is based on robust covariance estimation of the spatial sign\n(the projection onto the sphere of radius $\\sqrt{d}$) of elliptical\ndistributions."
    },
    {
        "date": "2025-02",
        "title": "Krum Federated Chain (KFC): Using blockchain to defend against adversarial attacks in Federated Learning",
        "author": "Mario Garc\u00eda-M\u00e1rquez, Nuria Rodr\u00edguez-Barroso, M. Victoria Luz\u00f3n, and Francisco Herrera",
        "link": "http://arxiv.org/abs/2502.06917v1",
        "abstract": "Federated Learning presents a nascent approach to machine learning, enabling\ncollaborative model training across decentralized devices while safeguarding\ndata privacy. However, its distributed nature renders it susceptible to\nadversarial attacks. Integrating blockchain technology with Federated Learning\noffers a promising avenue to enhance security and integrity. In this paper, we\ntackle the potential of blockchain in defending Federated Learning against\nadversarial attacks. First, we test Proof of Federated Learning, a well known\nconsensus mechanism designed ad-hoc to federated contexts, as a defense\nmechanism demonstrating its efficacy against Byzantine and backdoor attacks\nwhen at least one miner remains uncompromised. Second, we propose Krum\nFederated Chain, a novel defense strategy combining Krum and Proof of Federated\nLearning, valid to defend against any configuration of Byzantine or backdoor\nattacks, even when all miners are compromised. Our experiments conducted on\nimage classification datasets validate the effectiveness of our proposed\napproaches."
    },
    {
        "date": "2025-02",
        "title": "An Efficient Security Model for Industrial Internet of Things (IIoT) System Based on Machine Learning Principles",
        "author": "Sahar L. Qaddoori, and Qutaiba I. Ali",
        "link": "http://arxiv.org/abs/2502.06502v1",
        "abstract": "This paper presents a security paradigm for edge devices to defend against\nvarious internal and external threats. The first section of the manuscript\nproposes employing machine learning models to identify MQTT-based (Message\nQueue Telemetry Transport) attacks using the Intrusion Detection and Prevention\nSystem (IDPS) for edge nodes. Because the Machine Learning (ML) model cannot be\ntrained directly on low-performance platforms (such as edge devices),a new\nmethodology for updating ML models is proposed to provide a tradeoff between\nthe model performance and the computational complexity. The proposed\nmethodology involves training the model on a high-performance computing\nplatform and then installing the trained model as a detection engine on\nlow-performance platforms (such as the edge node of the edge layer) to identify\nnew attacks. Multiple security techniques have been employed in the second half\nof the manuscript to verify that the exchanged trained model and the exchanged\ndata files are valid and undiscoverable (information authenticity and privacy)\nand that the source (such as a fog node or edge device) is indeed what it it\nclaimed to be (source authentication and message integrity). Finally, the\nproposed security paradigm is found to be effective against various internal\nand external threats and can be applied to a low-cost single-board computer\n(SBC)."
    },
    {
        "date": "2025-02",
        "title": "Robust Watermarks Leak: Channel-Aware Feature Extraction Enables Adversarial Watermark Manipulation",
        "author": "Zhongjie Ba, Yitao Zhang, Peng Cheng, Bin Gong, Xinyu Zhang, Qinglong Wang, and Kui Ren",
        "link": "http://arxiv.org/abs/2502.06418v1",
        "abstract": "Watermarking plays a key role in the provenance and detection of AI-generated\ncontent. While existing methods prioritize robustness against real-world\ndistortions (e.g., JPEG compression and noise addition), we reveal a\nfundamental tradeoff: such robust watermarks inherently improve the redundancy\nof detectable patterns encoded into images, creating exploitable information\nleakage. To leverage this, we propose an attack framework that extracts leakage\nof watermark patterns through multi-channel feature learning using a\npre-trained vision model. Unlike prior works requiring massive data or detector\naccess, our method achieves both forgery and detection evasion with a single\nwatermarked image. Extensive experiments demonstrate that our method achieves a\n60\\% success rate gain in detection evasion and 51\\% improvement in forgery\naccuracy compared to state-of-the-art methods while maintaining visual\nfidelity. Our work exposes the robustness-stealthiness paradox: current\n\"robust\" watermarks sacrifice security for distortion resistance, providing\ninsights for future watermark design."
    },
    {
        "date": "2025-02",
        "title": "When Data Manipulation Meets Attack Goals: An In-depth Survey of Attacks for VLMs",
        "author": "Aobotao Dai, Xinyu Ma, Lei Chen, Songze Li, and Lin Wang",
        "link": "http://arxiv.org/abs/2502.06390v2",
        "abstract": "Vision-Language Models (VLMs) have gained considerable prominence in recent\nyears due to their remarkable capability to effectively integrate and process\nboth textual and visual information. This integration has significantly\nenhanced performance across a diverse spectrum of applications, such as scene\nperception and robotics. However, the deployment of VLMs has also given rise to\ncritical safety and security concerns, necessitating extensive research to\nassess the potential vulnerabilities these VLM systems may harbor. In this\nwork, we present an in-depth survey of the attack strategies tailored for VLMs.\nWe categorize these attacks based on their underlying objectives - namely\njailbreak, camouflage, and exploitation - while also detailing the various\nmethodologies employed for data manipulation of VLMs. Meanwhile, we outline\ncorresponding defense mechanisms that have been proposed to mitigate these\nvulnerabilities. By discerning key connections and distinctions among the\ndiverse types of attacks, we propose a compelling taxonomy for VLM attacks.\nMoreover, we summarize the evaluation metrics that comprehensively describe the\ncharacteristics and impact of different attacks on VLMs. Finally, we conclude\nwith a discussion of promising future research directions that could further\nenhance the robustness and safety of VLMs, emphasizing the importance of\nongoing exploration in this critical area of study. To facilitate community\nengagement, we maintain an up-to-date project page, accessible at:\nhttps://github.com/AobtDai/VLM_Attack_Paper_List."
    },
    {
        "date": "2025-02",
        "title": "Hyperparameters in Score-Based Membership Inference Attacks",
        "author": "Gauri Pradhan, Joonas J\u00e4lk\u00f6, Marlon Tobaben, and Antti Honkela",
        "link": "http://arxiv.org/abs/2502.06374v1",
        "abstract": "Membership Inference Attacks (MIAs) have emerged as a valuable framework for\nevaluating privacy leakage by machine learning models. Score-based MIAs are\ndistinguished, in particular, by their ability to exploit the confidence scores\nthat the model generates for particular inputs. Existing score-based MIAs\nimplicitly assume that the adversary has access to the target model's\nhyperparameters, which can be used to train the shadow models for the attack.\nIn this work, we demonstrate that the knowledge of target hyperparameters is\nnot a prerequisite for MIA in the transfer learning setting. Based on this, we\npropose a novel approach to select the hyperparameters for training the shadow\nmodels for MIA when the attacker has no prior knowledge about them by matching\nthe output distributions of target and shadow models. We demonstrate that using\nthe new approach yields hyperparameters that lead to an attack near\nindistinguishable in performance from an attack that uses target\nhyperparameters to train the shadow models. Furthermore, we study the empirical\nprivacy risk of unaccounted use of training data for hyperparameter\noptimization (HPO) in differentially private (DP) transfer learning. We find no\nstatistically significant evidence that performing HPO using training data\nwould increase vulnerability to MIA."
    },
    {
        "date": "2025-02",
        "title": "Accelerating Outlier-robust Rotation Estimation by Stereographic Projection",
        "author": "Taosi Xu, Yinlong Liu, Xianbo Wang, and Zhi-Xin Yang",
        "link": "http://arxiv.org/abs/2502.06337v1",
        "abstract": "Rotation estimation plays a fundamental role in many computer vision and\nrobot tasks. However, efficiently estimating rotation in large inputs\ncontaining numerous outliers (i.e., mismatches) and noise is a recognized\nchallenge. Many robust rotation estimation methods have been designed to\naddress this challenge. Unfortunately, existing methods are often inapplicable\ndue to their long computation time and the risk of local optima. In this paper,\nwe propose an efficient and robust rotation estimation method. Specifically,\nour method first investigates geometric constraints involving only the rotation\naxis. Then, it uses stereographic projection and spatial voting techniques to\nidentify the rotation axis and angle. Furthermore, our method efficiently\nobtains the optimal rotation estimation and can estimate multiple rotations\nsimultaneously. To verify the feasibility of our method, we conduct comparative\nexperiments using both synthetic and real-world data. The results show that,\nwith GPU assistance, our method can solve large-scale ($10^6$ points) and\nseverely corrupted (90\\% outlier rate) rotation estimation problems within 0.07\nseconds, with an angular error of only 0.01 degrees, which is superior to\nexisting methods in terms of accuracy and efficiency."
    },
    {
        "date": "2025-02",
        "title": "Dynamic Pricing with Adversarially-Censored Demands",
        "author": "Jianyu Xu, Yining Wang, Xi Chen, and Yu-Xiang Wang",
        "link": "http://arxiv.org/abs/2502.06168v1",
        "abstract": "We study an online dynamic pricing problem where the potential demand at each\ntime period $t=1,2,\\ldots, T$ is stochastic and dependent on the price.\nHowever, a perishable inventory is imposed at the beginning of each time $t$,\ncensoring the potential demand if it exceeds the inventory level. To address\nthis problem, we introduce a pricing algorithm based on the optimistic\nestimates of derivatives. We show that our algorithm achieves\n$\\tilde{O}(\\sqrt{T})$ optimal regret even with adversarial inventory series.\nOur findings advance the state-of-the-art in online decision-making problems\nwith censored feedback, offering a theoretically optimal solution against\nadversarial observations."
    },
    {
        "date": "2025-02",
        "title": "Enhanced Hybrid Deep Learning Approach for Botnet Attacks Detection in IoT Environment",
        "author": "A. Karthick kumar, S. Rathnamala, T. Vijayashanthi, M. Prabhananthakumar, Alavikunhu Panthakkan, Shadi Atalla, and Wathiq Mansoor",
        "link": "http://arxiv.org/abs/2502.06138v1",
        "abstract": "Cyberattacks in an Internet of Things (IoT) environment can have significant\nimpacts because of the interconnected nature of devices and systems. An\nattacker uses a network of compromised IoT devices in a botnet attack to carry\nout various harmful activities. Detecting botnet attacks poses several\nchallenges because of the intricate and evolving nature of these threats.\nBotnet attacks erode trust in IoT devices and systems, undermining confidence\nin their security, reliability, and integrity. Deep learning techniques have\nsignificantly enhanced the detection of botnet attacks due to their ability to\nanalyze and learn from complex patterns in data. This research proposed the\nstacking of Deep convolutional neural networks, Bi-Directional Long Short-Term\nMemory (Bi-LSTM), Bi-Directional Gated Recurrent Unit (Bi-GRU), and Recurrent\nNeural Networks (RNN) for botnet attacks detection. The UNSW-NB15 dataset is\nutilized for botnet attacks detection. According to experimental results, the\nproposed model accurately provides for the intricate patterns and features of\nbotnet attacks, with a testing accuracy of 99.76%. The proposed model also\nidentifies botnets with a high ROC-AUC curve value of 99.18%. A performance\ncomparison of the proposed method with existing state-of-the-art models\nconfirms its higher performance. The outcomes of this research could strengthen\ncyber security procedures and safeguard against new attacks."
    },
    {
        "date": "2025-02",
        "title": "Benchmarking Prompt Engineering Techniques for Secure Code Generation with GPT Models",
        "author": "Marc Bruni, Fabio Gabrielli, Mohammad Ghafari, and Martin Kropp",
        "link": "http://arxiv.org/abs/2502.06039v1",
        "abstract": "Prompt engineering reduces reasoning mistakes in Large Language Models\n(LLMs). However, its effectiveness in mitigating vulnerabilities in\nLLM-generated code remains underexplored. To address this gap, we implemented a\nbenchmark to automatically assess the impact of various prompt engineering\nstrategies on code security. Our benchmark leverages two peer-reviewed prompt\ndatasets and employs static scanners to evaluate code security at scale. We\ntested multiple prompt engineering techniques on GPT-3.5-turbo, GPT-4o, and\nGPT-4o-mini. Our results show that for GPT-4o and GPT-4o-mini, a\nsecurity-focused prompt prefix can reduce the occurrence of security\nvulnerabilities by up to 56%. Additionally, all tested models demonstrated the\nability to detect and repair between 41.9% and 68.7% of vulnerabilities in\npreviously generated code when using iterative prompting techniques. Finally,\nwe introduce a \"prompt agent\" that demonstrates how the most effective\ntechniques can be applied in real-world development workflows."
    },
    {
        "date": "2025-02",
        "title": "A Conditional Tabular GAN-Enhanced Intrusion Detection System for Rare Attacks in IoT Networks",
        "author": "Safaa Menssouri, and El Mehdi Amhoud",
        "link": "http://arxiv.org/abs/2502.06031v1",
        "abstract": "Internet of things (IoT) networks, boosted by 6G technology, are transforming\nvarious industries. However, their widespread adoption introduces significant\nsecurity risks, particularly in detecting rare but potentially damaging\ncyber-attacks. This makes the development of robust IDS crucial for monitoring\nnetwork traffic and ensuring their safety. Traditional IDS often struggle with\ndetecting rare attacks due to severe class imbalances in IoT data. In this\npaper, we propose a novel two-stage system called conditional tabular\ngenerative synthetic minority data generation with deep neural network\n(CTGSM-DNN). In the first stage, a conditional tabular generative adversarial\nnetwork (CTGAN) is employed to generate synthetic data for rare attack classes.\nIn the second stage, the SMOTEENN method is applied to improve dataset quality.\nThe full study was conducted using the CSE-CIC-IDS2018 dataset, and we assessed\nthe performance of the proposed IDS using different evaluation metrics. The\nexperimental results demonstrated the effectiveness of the proposed multiclass\nclassifier, achieving an overall accuracy of 99.90% and 80% accuracy in\ndetecting rare attacks."
    },
    {
        "date": "2025-02",
        "title": "The AI Security Zugzwang",
        "author": "Lampis Alevizos",
        "link": "http://arxiv.org/abs/2502.06000v1",
        "abstract": "In chess, zugzwang describes a scenario where any move worsens the player's\nposition. Organizations face a similar dilemma right now at the intersection of\nartificial intelligence (AI) and cybersecurity. AI adoption creates an\ninevitable paradox: delaying it poses strategic risks, rushing it introduces\npoorly understood vulnerabilities, and even incremental adoption leads to\ncascading complexities. In this work we formalize this challenge as the AI\nSecurity Zugzwang, a phenomenon where security leaders must make decisions\nunder conditions of inevitable risk. Grounded in game theory, security\neconomics, and organizational decision theory, we characterize AI security\nzugzwang through three key properties, the forced movement, predictable\nvulnerability creation, and temporal pressure. Additionally, we develop a\ntaxonomy to categorize forced-move scenarios across AI adoption,\nimplementation, operational and governance contexts and provide corresponding\nstrategic mitigations. Our framework is supported by a practical decision\nflowchart, demonstrated through a real-world example of Copilot adoption, thus,\nshowing how security lead"
    },
    {
        "date": "2025-02",
        "title": "Detection of Physiological Data Tampering Attacks with Quantum Machine Learning",
        "author": "Md. Saif Hassan Onim, and Himanshu Thapliyal",
        "link": "http://arxiv.org/abs/2502.05966v1",
        "abstract": "The widespread use of cloud-based medical devices and wearable sensors has\nmade physiological data susceptible to tampering. These attacks can compromise\nthe reliability of healthcare systems which can be critical and\nlife-threatening. Detection of such data tampering is of immediate need.\nMachine learning has been used to detect anomalies in datasets but the\nperformance of Quantum Machine Learning (QML) is still yet to be evaluated for\nphysiological sensor data. Thus, our study compares the effectiveness of QML\nfor detecting physiological data tampering, focusing on two types of white-box\nattacks: data poisoning and adversarial perturbation. The results show that QML\nmodels are better at identifying label-flipping attacks, achieving accuracy\nrates of 75%-95% depending on the data and attack severity. This superior\nperformance is due to the ability of quantum algorithms to handle complex and\nhigh-dimensional data. However, both QML and classical models struggle to\ndetect more sophisticated adversarial perturbation attacks, which subtly alter\ndata without changing its statistical properties. Although QML performed poorly\nagainst this attack with around 45%-65% accuracy, it still outperformed\nclassical algorithms in some cases."
    },
    {
        "date": "2025-02",
        "title": "Cyri: A Conversational AI-based Assistant for Supporting the Human User in Detecting and Responding to Phishing Attacks",
        "author": "Antonio La Torre, and Marco Angelini",
        "link": "http://arxiv.org/abs/2502.05951v1",
        "abstract": "This work introduces Cyri, an AI-powered conversational assistant designed to\nsupport a human user in detecting and analyzing phishing emails by leveraging\nLarge Language Models. Cyri has been designed to scrutinize emails for semantic\nfeatures used in phishing attacks, such as urgency, and undesirable\nconsequences, using an approach that unifies features already established in\nthe literature with others by Cyri features extraction methodology. Cyri can be\ndirectly plugged into a client mail or webmail, ensuring seamless integration\nwith the user's email workflow while maintaining data privacy through local\nprocessing. By performing analyses on the user's machine, Cyri eliminates the\nneed to transmit sensitive email data over the internet, reducing associated\nsecurity risks. The Cyri user interface has been designed to reduce habituation\neffects and enhance user engagement. It employs dynamic visual cues and\ncontext-specific explanations to keep users alert and informed while using\nemails. Additionally, it allows users to explore identified malicious semantic\nfeatures both through conversation with the agent and visual exploration,\nobtaining the advantages of both modalities for expert or non-expert users. It\nalso allows users to keep track of the conversation, supports the user in\nsolving additional questions on both computed features or new parts of the\nmail, and applies its detection on demand. To evaluate Cyri, we crafted a\ncomprehensive dataset of 420 phishing emails and 420 legitimate emails. Results\ndemonstrate high effectiveness in identifying critical phishing semantic\nfeatures fundamental to phishing detection. A user study involving 10\nparticipants, both experts and non-experts, evaluated Cyri's effectiveness and\nusability. Results indicated that Cyri significantly aided users in identifying\nphishing emails and enhanced their understanding of phishing tactics."
    },
    {
        "date": "2025-02",
        "title": "Sign-Symmetry Learning Rules are Robust Fine-Tuners",
        "author": "Aymene Berriche, Mehdi Zakaria Adjal, and Riyadh Baghdadi",
        "link": "http://arxiv.org/abs/2502.05925v1",
        "abstract": "Backpropagation (BP) has long been the predominant method for training neural\nnetworks due to its effectiveness. However, numerous alternative approaches,\nbroadly categorized under feedback alignment, have been proposed, many of which\nare motivated by the search for biologically plausible learning mechanisms.\nDespite their theoretical appeal, these methods have consistently\nunderperformed compared to BP, leading to a decline in research interest. In\nthis work, we revisit the role of such methods and explore how they can be\nintegrated into standard neural network training pipelines. Specifically, we\npropose fine-tuning BP-pre-trained models using Sign-Symmetry learning rules\nand demonstrate that this approach not only maintains performance parity with\nBP but also enhances robustness. Through extensive experiments across multiple\ntasks and benchmarks, we establish the validity of our approach. Our findings\nintroduce a novel perspective on neural network training and open new research\ndirections for leveraging biologically inspired learning rules in deep\nlearning."
    },
    {
        "date": "2025-02",
        "title": "Certifying Language Model Robustness with Fuzzed Randomized Smoothing: An Efficient Defense Against Backdoor Attacks",
        "author": "Bowei He, Lihao Yin, Hui-Ling Zhen, Jianping Zhang, Lanqing Hong, Mingxuan Yuan, and Chen Ma",
        "link": "http://arxiv.org/abs/2502.06892v1",
        "abstract": "The widespread deployment of pre-trained language models (PLMs) has exposed\nthem to textual backdoor attacks, particularly those planted during the\npre-training stage. These attacks pose significant risks to high-reliability\napplications, as they can stealthily affect multiple downstream tasks. While\ncertifying robustness against such threats is crucial, existing defenses\nstruggle with the high-dimensional, interdependent nature of textual data and\nthe lack of access to original poisoned pre-training data. To address these\nchallenges, we introduce \\textbf{F}uzzed \\textbf{R}andomized \\textbf{S}moothing\n(\\textbf{FRS}), a novel approach for efficiently certifying language model\nrobustness against backdoor attacks. FRS integrates software robustness\ncertification techniques with biphased model parameter smoothing, employing\nMonte Carlo tree search for proactive fuzzing to identify vulnerable textual\nsegments within the Damerau-Levenshtein space. This allows for targeted and\nefficient text randomization, while eliminating the need for access to poisoned\ntraining data during model smoothing. Our theoretical analysis demonstrates\nthat FRS achieves a broader certified robustness radius compared to existing\nmethods. Extensive experiments across various datasets, model configurations,\nand attack strategies validate FRS's superiority in terms of defense\nefficiency, accuracy, and robustness."
    },
    {
        "date": "2025-02",
        "title": "Secure Visual Data Processing via Federated Learning",
        "author": "Pedro Santos, T\u00e2nia Carvalho, Filipe Magalh\u00e3es, and Lu\u00eds Antunes",
        "link": "http://arxiv.org/abs/2502.06889v1",
        "abstract": "As the demand for privacy in visual data management grows, safeguarding\nsensitive information has become a critical challenge. This paper addresses the\nneed for privacy-preserving solutions in large-scale visual data processing by\nleveraging federated learning. Although there have been developments in this\nfield, previous research has mainly focused on integrating object detection\nwith either anonymization or federated learning. However, these pairs often\nfail to address complex privacy concerns. On the one hand, object detection\nwith anonymization alone can be vulnerable to reverse techniques. On the other\nhand, federated learning may not provide sufficient privacy guarantees.\nTherefore, we propose a new approach that combines object detection, federated\nlearning and anonymization. Combining these three components aims to offer a\nrobust privacy protection strategy by addressing different vulnerabilities in\nvisual data. Our solution is evaluated against traditional centralized models,\nshowing that while there is a slight trade-off in accuracy, the privacy\nbenefits are substantial, making it well-suited for privacy sensitive\napplications."
    },
    {
        "date": "2025-02",
        "title": "GOLD: Graph Out-of-Distribution Detection via Implicit Adversarial Latent Generation",
        "author": "Danny Wang, Ruihong Qiu, Guangdong Bai, and Zi Huang",
        "link": "http://arxiv.org/abs/2502.05780v1",
        "abstract": "Despite graph neural networks' (GNNs) great success in modelling\ngraph-structured data, out-of-distribution (OOD) test instances still pose a\ngreat challenge for current GNNs. One of the most effective techniques to\ndetect OOD nodes is to expose the detector model with an additional OOD\nnode-set, yet the extra OOD instances are often difficult to obtain in\npractice. Recent methods for image data address this problem using OOD data\nsynthesis, typically relying on pre-trained generative models like Stable\nDiffusion. However, these approaches require vast amounts of additional data,\nas well as one-for-all pre-trained generative models, which are not available\nfor graph data. Therefore, we propose the GOLD framework for graph OOD\ndetection, an implicit adversarial learning pipeline with synthetic OOD\nexposure without pre-trained models. The implicit adversarial training process\nemploys a novel alternating optimisation framework by training: (1) a latent\ngenerative model to regularly imitate the in-distribution (ID) embeddings from\nan evolving GNN, and (2) a GNN encoder and an OOD detector to accurately\nclassify ID data while increasing the energy divergence between the ID\nembeddings and the generative model's synthetic embeddings. This novel approach\nimplicitly transforms the synthetic embeddings into pseudo-OOD instances\nrelative to the ID data, effectively simulating exposure to OOD scenarios\nwithout auxiliary data. Extensive OOD detection experiments are conducted on\nfive benchmark graph datasets, verifying the superior performance of GOLD\nwithout using real OOD data compared with the state-of-the-art OOD exposure and\nnon-exposure baselines."
    },
    {
        "date": "2025-02",
        "title": "Effective Black-Box Multi-Faceted Attacks Breach Vision Large Language Model Guardrails",
        "author": "Yijun Yang, Lichao Wang, Xiao Yang, Lanqing Hong, and Jun Zhu",
        "link": "http://arxiv.org/abs/2502.05772v1",
        "abstract": "Vision Large Language Models (VLLMs) integrate visual data processing,\nexpanding their real-world applications, but also increasing the risk of\ngenerating unsafe responses. In response, leading companies have implemented\nMulti-Layered safety defenses, including alignment training, safety system\nprompts, and content moderation. However, their effectiveness against\nsophisticated adversarial attacks remains largely unexplored. In this paper, we\npropose MultiFaceted Attack, a novel attack framework designed to\nsystematically bypass Multi-Layered Defenses in VLLMs. It comprises three\ncomplementary attack facets: Visual Attack that exploits the multimodal nature\nof VLLMs to inject toxic system prompts through images; Alignment Breaking\nAttack that manipulates the model's alignment mechanism to prioritize the\ngeneration of contrasting responses; and Adversarial Signature that deceives\ncontent moderators by strategically placing misleading information at the end\nof the response. Extensive evaluations on eight commercial VLLMs in a black-box\nsetting demonstrate that MultiFaceted Attack achieves a 61.56% attack success\nrate, surpassing state-of-the-art methods by at least 42.18%."
    },
    {
        "date": "2025-02",
        "title": "Filter, Obstruct and Dilute: Defending Against Backdoor Attacks on Semi-Supervised Learning",
        "author": "Xinrui Wang, Chuanxing Geng, Wenhai Wan, Shao-yuan Li, and Songcan Chen",
        "link": "http://arxiv.org/abs/2502.05755v1",
        "abstract": "Recent studies have verified that semi-supervised learning (SSL) is\nvulnerable to data poisoning backdoor attacks. Even a tiny fraction of\ncontaminated training data is sufficient for adversaries to manipulate up to\n90\\% of the test outputs in existing SSL methods. Given the emerging threat of\nbackdoor attacks designed for SSL, this work aims to protect SSL against such\nrisks, marking it as one of the few known efforts in this area. Specifically,\nwe begin by identifying that the spurious correlations between the backdoor\ntriggers and the target class implanted by adversaries are the primary cause of\nmanipulated model predictions during the test phase. To disrupt these\ncorrelations, we utilize three key techniques: Gaussian Filter, complementary\nlearning and trigger mix-up, which collectively filter, obstruct and dilute the\ninfluence of backdoor attacks in both data pre-processing and feature learning.\nExperimental results demonstrate that our proposed method, Backdoor Invalidator\n(BI), significantly reduces the average attack success rate from 84.7\\% to\n1.8\\% across different state-of-the-art backdoor attacks. It is also worth\nmentioning that BI does not sacrifice accuracy on clean data and is supported\nby a theoretical guarantee of its generalization capability."
    },
    {
        "date": "2025-02",
        "title": "Impact of Data Poisoning Attacks on Feasibility and Optimality of Neural Power System Optimizers",
        "author": "Nora Agah, Meiyi Li, and Javad Mohammadi",
        "link": "http://arxiv.org/abs/2502.05727v1",
        "abstract": "The increased integration of clean yet stochastic energy resources and the\ngrowing number of extreme weather events are narrowing the decision-making\nwindow of power grid operators. This time constraint is fueling a plethora of\nresearch on Machine Learning-, or ML-, based optimization proxies. While\nfinding a fast solution is appealing, the inherent vulnerabilities of the\nlearning-based methods are hindering their adoption. One of these\nvulnerabilities is data poisoning attacks, which adds perturbations to ML\ntraining data, leading to incorrect decisions. The impact of poisoning attacks\non learning-based power system optimizers have not been thoroughly studied,\nwhich creates a critical vulnerability. In this paper, we examine the impact of\ndata poisoning attacks on ML-based optimization proxies that are used to solve\nthe DC Optimal Power Flow problem. Specifically, we compare the resilience of\nthree different methods-a penalty-based method, a post-repair approach, and a\ndirect mapping approach-against the adverse effects of poisoning attacks. We\nwill use the optimality and feasibility of these proxies as performance\nmetrics. The insights of this work will establish a foundation for enhancing\nthe resilience of neural power system optimizers."
    },
    {
        "date": "2025-02",
        "title": "Mobile Application Threats and Security",
        "author": "Timur Mirzoev, Mark Miller, Shamimara Lasker, and Michael Brannon",
        "link": "http://arxiv.org/abs/2502.05685v1",
        "abstract": "The movement to mobile computing solutions provides flexibility to different\nusers whether it is a business user, a student, or even providing entertainment\nto children and adults of all ages. Due to these emerging technologies mobile\nusers are unable to safeguard private information in a very effective way and\ncybercrimes are increasing day by day. This manuscript will focus on security\nvulnerabilities in the mobile computing industry, especially focusing on\ntablets and smart phones. This study will dive into current security threats\nfor the Android & Apple iOS market, exposing security risks and threats that\nthe novice or average user may not be aware of. The purpose of this study is to\nanalyze current security risks and threats, and provide solutions that may be\ndeployed to protect against such threats."
    },
    {
        "date": "2025-02",
        "title": "Rigid Body Adversarial Attacks",
        "author": "Aravind Ramakrishnan, David I. W. Levin, and Alec Jacobson",
        "link": "http://arxiv.org/abs/2502.05669v1",
        "abstract": "Due to their performance and simplicity, rigid body simulators are often used\nin applications where the objects of interest can considered very stiff.\nHowever, no material has infinite stiffness, which means there are potentially\ncases where the non-zero compliance of the seemingly rigid object can cause a\nsignificant difference between its trajectories when simulated in a rigid body\nor deformable simulator.\n  Similarly to how adversarial attacks are developed against image classifiers,\nwe propose an adversarial attack against rigid body simulators. In this\nadversarial attack, we solve an optimization problem to construct perceptually\nrigid adversarial objects that have the same collision geometry and moments of\nmass to a reference object, so that they behave identically in rigid body\nsimulations but maximally different in more accurate deformable simulations. We\ndemonstrate the validity of our method by comparing simulations of several\nexamples in commercially available simulators."
    },
    {
        "date": "2025-02",
        "title": "Adversarial Machine Learning: Attacks, Defenses, and Open Challenges",
        "author": "Pranav K Jha",
        "link": "http://arxiv.org/abs/2502.05637v1",
        "abstract": "Adversarial Machine Learning (AML) addresses vulnerabilities in AI systems\nwhere adversaries manipulate inputs or training data to degrade performance.\nThis article provides a comprehensive analysis of evasion and poisoning\nattacks, formalizes defense mechanisms with mathematical rigor, and discusses\nthe challenges of implementing robust solutions in adaptive threat models.\nAdditionally, it highlights open challenges in certified robustness,\nscalability, and real-world deployment."
    },
    {
        "date": "2025-02",
        "title": "Dual Defense: Enhancing Privacy and Mitigating Poisoning Attacks in Federated Learning",
        "author": "Runhua Xu, Shiqi Gao, Chao Li, James Joshi, and Jianxin Li",
        "link": "http://arxiv.org/abs/2502.05547v1",
        "abstract": "Federated learning (FL) is inherently susceptible to privacy breaches and\npoisoning attacks. To tackle these challenges, researchers have separately\ndevised secure aggregation mechanisms to protect data privacy and robust\naggregation methods that withstand poisoning attacks. However, simultaneously\naddressing both concerns is challenging; secure aggregation facilitates\npoisoning attacks as most anomaly detection techniques require access to\nunencrypted local model updates, which are obscured by secure aggregation. Few\nrecent efforts to simultaneously tackle both challenges offen depend on\nimpractical assumption of non-colluding two-server setups that disrupt FL's\ntopology, or three-party computation which introduces scalability issues,\ncomplicating deployment and application. To overcome this dilemma, this paper\nintroduce a Dual Defense Federated learning (DDFed) framework. DDFed\nsimultaneously boosts privacy protection and mitigates poisoning attacks,\nwithout introducing new participant roles or disrupting the existing FL\ntopology. DDFed initially leverages cutting-edge fully homomorphic encryption\n(FHE) to securely aggregate model updates, without the impractical requirement\nfor non-colluding two-server setups and ensures strong privacy protection.\nAdditionally, we proposes a unique two-phase anomaly detection mechanism for\nencrypted model updates, featuring secure similarity computation and\nfeedback-driven collaborative selection, with additional measures to prevent\npotential privacy breaches from Byzantine clients incorporated into the\ndetection process. We conducted extensive experiments on various model\npoisoning attacks and FL scenarios, including both cross-device and cross-silo\nFL. Experiments on publicly available datasets demonstrate that DDFed\nsuccessfully protects model privacy and effectively defends against model\npoisoning threats."
    },
    {
        "date": "2025-02",
        "title": "Democratic Training Against Universal Adversarial Perturbations",
        "author": "Bing Sun, Jun Sun, and Wei Zhao",
        "link": "http://arxiv.org/abs/2502.05542v1",
        "abstract": "Despite their advances and success, real-world deep neural networks are known\nto be vulnerable to adversarial attacks. Universal adversarial perturbation, an\ninput-agnostic attack, poses a serious threat for them to be deployed in\nsecurity-sensitive systems. In this case, a single universal adversarial\nperturbation deceives the model on a range of clean inputs without requiring\ninput-specific optimization, which makes it particularly threatening. In this\nwork, we observe that universal adversarial perturbations usually lead to\nabnormal entropy spectrum in hidden layers, which suggests that the prediction\nis dominated by a small number of ``feature'' in such cases (rather than\ndemocratically by many features). Inspired by this, we propose an efficient yet\neffective defense method for mitigating UAPs called \\emph{Democratic Training}\nby performing entropy-based model enhancement to suppress the effect of the\nuniversal adversarial perturbations in a given model. \\emph{Democratic\nTraining} is evaluated with 7 neural networks trained on 5 benchmark datasets\nand 5 types of state-of-the-art universal adversarial attack methods. The\nresults show that it effectively reduces the attack success rate, improves\nmodel robustness and preserves the model accuracy on clean samples."
    },
    {
        "date": "2025-02",
        "title": "Do Spikes Protect Privacy? Investigating Black-Box Model Inversion Attacks in Spiking Neural Networks",
        "author": "Hamed Poursiami, Ayana Moshruba, and Maryam Parsa",
        "link": "http://arxiv.org/abs/2502.05509v1",
        "abstract": "As machine learning models become integral to security-sensitive\napplications, concerns over data leakage from adversarial attacks continue to\nrise. Model Inversion (MI) attacks pose a significant privacy threat by\nenabling adversaries to reconstruct training data from model outputs. While MI\nattacks on Artificial Neural Networks (ANNs) have been widely studied, Spiking\nNeural Networks (SNNs) remain largely unexplored in this context. Due to their\nevent-driven and discrete computations, SNNs introduce fundamental differences\nin information processing that may offer inherent resistance to such attacks. A\ncritical yet underexplored aspect of this threat lies in black-box settings,\nwhere attackers operate through queries without direct access to model\nparameters or gradients-representing a more realistic adversarial scenario in\ndeployed systems. This work presents the first study of black-box MI attacks on\nSNNs. We adapt a generative adversarial MI framework to the spiking domain by\nincorporating rate-based encoding for input transformation and decoding\nmechanisms for output interpretation. Our results show that SNNs exhibit\nsignificantly greater resistance to MI attacks than ANNs, as demonstrated by\ndegraded reconstructions, increased instability in attack convergence, and\noverall reduced attack effectiveness across multiple evaluation metrics.\nFurther analysis suggests that the discrete and temporally distributed nature\nof SNN decision boundaries disrupts surrogate modeling, limiting the attacker's\nability to approximate the target model."
    },
    {
        "date": "2025-02",
        "title": "Gen-DFL: Decision-Focused Generative Learning for Robust Decision Making",
        "author": "Prince Zizhuang Wang, Jinhao Liang, Shuyi Chen, Ferdinando Fioretto, and Shixiang Zhu",
        "link": "http://arxiv.org/abs/2502.05468v1",
        "abstract": "Decision-focused learning (DFL) integrates predictive models with downstream\noptimization, directly training machine learning models to minimize decision\nerrors. While DFL has been shown to provide substantial advantages when\ncompared to a counterpart that treats the predictive and prescriptive models\nseparately, it has also been shown to struggle in high-dimensional and\nrisk-sensitive settings, limiting its applicability in real-world settings. To\naddress this limitation, this paper introduces decision-focused generative\nlearning (Gen-DFL), a novel framework that leverages generative models to\nadaptively model uncertainty and improve decision quality. Instead of relying\non fixed uncertainty sets, Gen-DFL learns a structured representation of the\noptimization parameters and samples from the tail regions of the learned\ndistribution to enhance robustness against worst-case scenarios. This approach\nmitigates over-conservatism while capturing complex dependencies in the\nparameter space. The paper shows, theoretically, that Gen-DFL achieves improved\nworst-case performance bounds compared to traditional DFL. Empirically, it\nevaluates Gen-DFL on various scheduling and logistics problems, demonstrating\nits strong performance against existing DFL methods."
    },
    {
        "date": "2025-02",
        "title": "SMaCk: Efficient Instruction Cache Attacks via Self-Modifying Code Conflicts",
        "author": "Seonghun Son, Daniel Moghimi, and Berk Gulmezoglu",
        "link": "http://arxiv.org/abs/2502.05429v1",
        "abstract": "Self-modifying code (SMC) allows programs to alter their own instructions,\noptimizing performance and functionality on x86 processors. Despite its\nbenefits, SMC introduces unique microarchitectural behaviors that can be\nexploited for malicious purposes. In this paper, we explore the security\nimplications of SMC by examining how specific x86 instructions affecting\ninstruction cache lines lead to measurable timing discrepancies between cache\nhits and misses. These discrepancies facilitate refined cache attacks, making\nthem less noisy and more effective. We introduce novel attack techniques that\nleverage these timing variations to enhance existing methods such as\nPrime+Probe and Flush+Reload. Our advanced techniques allow adversaries to more\nprecisely attack cryptographic keys and create covert channels akin to Spectre\nacross various x86 platforms. Finally, we propose a dynamic detection\nmethodology utilizing hardware performance counters to mitigate these enhanced\nthreats."
    },
    {
        "date": "2025-02",
        "title": "BF-GAN: Development of an AI-driven Bubbly Flow Image Generation Model Using Generative Adversarial Networks",
        "author": "Wen Zhou, Shuichiro Miwa, Yang Liu, and Koji Okamoto",
        "link": "http://arxiv.org/abs/2502.06863v1",
        "abstract": "A generative AI architecture called bubbly flow generative adversarial\nnetworks (BF-GAN) is developed, designed to generate realistic and high-quality\nbubbly flow images through physically conditioned inputs, jg and jf. Initially,\n52 sets of bubbly flow experiments under varying conditions are conducted to\ncollect 140,000 bubbly flow images with physical labels of jg and jf for\ntraining data. A multi-scale loss function is then developed, incorporating\nmismatch loss and pixel loss to enhance the generative performance of BF-GAN\nfurther. Regarding evaluative metrics of generative AI, the BF-GAN has\nsurpassed conventional GAN. Physically, key parameters of bubbly flow generated\nby BF-GAN are extracted and compared with measurement values and empirical\ncorrelations, validating BF-GAN's generative performance. The comparative\nanalysis demonstrate that the BF-GAN can generate realistic and high-quality\nbubbly flow images with any given jg and jf within the research scope.\n  BF-GAN offers a generative AI solution for two-phase flow research,\nsubstantially lowering the time and cost required to obtain high-quality data.\nIn addition, it can function as a benchmark dataset generator for bubbly flow\ndetection and segmentation algorithms, enhancing overall productivity in this\nresearch domain. The BF-GAN model is available online\n(https://github.com/zhouzhouwen/BF-GAN)."
    },
    {
        "date": "2025-02",
        "title": "Towards LLM Unlearning Resilient to Relearning Attacks: A Sharpness-Aware Minimization Perspective and Beyond",
        "author": "Chongyu Fan, Jinghan Jia, Yihua Zhang, Anil Ramakrishna, Mingyi Hong, and Sijia Liu",
        "link": "http://arxiv.org/abs/2502.05374v1",
        "abstract": "The LLM unlearning technique has recently been introduced to comply with data\nregulations and address the safety and ethical concerns of LLMs by removing the\nundesired data-model influence. However, state-of-the-art unlearning methods\nface a critical vulnerability: they are susceptible to ``relearning'' the\nremoved information from a small number of forget data points, known as\nrelearning attacks. In this paper, we systematically investigate how to make\nunlearned models robust against such attacks. For the first time, we establish\na connection between robust unlearning and sharpness-aware minimization (SAM)\nthrough a unified robust optimization framework, in an analogy to adversarial\ntraining designed to defend against adversarial attacks. Our analysis for SAM\nreveals that smoothness optimization plays a pivotal role in mitigating\nrelearning attacks. Thus, we further explore diverse smoothing strategies to\nenhance unlearning robustness. Extensive experiments on benchmark datasets,\nincluding WMDP and MUSE, demonstrate that SAM and other smoothness optimization\napproaches consistently improve the resistance of LLM unlearning to relearning\nattacks. Notably, smoothness-enhanced unlearning also helps defend against\n(input-level) jailbreaking attacks, broadening our proposal's impact in\nrobustifying LLM unlearning. Codes are available at\nhttps://github.com/OPTML-Group/Unlearn-Smooth."
    },
    {
        "date": "2025-02",
        "title": "Removing Neural Signal Artifacts with Autoencoder-Targeted Adversarial Transformers (AT-AT)",
        "author": "Benjamin J. Choi",
        "link": "http://arxiv.org/abs/2502.05332v1",
        "abstract": "Electromyogenic (EMG) noise is a major contamination source in EEG data that\ncan impede accurate analysis of brain-specific neural activity. Recent\nliterature on EMG artifact removal has moved beyond traditional linear\nalgorithms in favor of machine learning-based systems. However, existing deep\nlearning-based filtration methods often have large compute footprints and\nprohibitively long training times. In this study, we present a new machine\nlearning-based system for filtering EMG interference from EEG data using an\nautoencoder-targeted adversarial transformer (AT-AT). By leveraging the\nlightweight expressivity of an autoencoder to determine optimal time-series\ntransformer application sites, our AT-AT architecture achieves a >90% model\nsize reduction compared to published artifact removal models. The addition of\nadversarial training ensures that filtered signals adhere to the fundamental\ncharacteristics of EEG data. We trained AT-AT using published neural data from\n67 subjects and found that the system was able to achieve comparable test\nperformance to larger models; AT-AT posted a mean reconstructive correlation\ncoefficient above 0.95 at an initial signal-to-noise ratio (SNR) of 2 dB and\n0.70 at -7 dB SNR. Further research generalizing these results to broader\nsample sizes beyond these isolated test cases will be crucial; while outside\nthe scope of this study, we also include results from a real-world deployment\nof AT-AT in the Appendix."
    },
    {
        "date": "2025-02",
        "title": "From Counterfactuals to Trees: Competitive Analysis of Model Extraction Attacks",
        "author": "Awa Khouna, Julien Ferry, and Thibaut Vidal",
        "link": "http://arxiv.org/abs/2502.05325v1",
        "abstract": "The advent of Machine Learning as a Service (MLaaS) has heightened the\ntrade-off between model explainability and security. In particular,\nexplainability techniques, such as counterfactual explanations, inadvertently\nincrease the risk of model extraction attacks, enabling unauthorized\nreplication of proprietary models. In this paper, we formalize and characterize\nthe risks and inherent complexity of model reconstruction, focusing on the\n\"oracle'' queries required for faithfully inferring the underlying prediction\nfunction. We present the first formal analysis of model extraction attacks\nthrough the lens of competitive analysis, establishing a foundational framework\nto evaluate their efficiency. Focusing on models based on additive decision\ntrees (e.g., decision trees, gradient boosting, and random forests), we\nintroduce novel reconstruction algorithms that achieve provably perfect\nfidelity while demonstrating strong anytime performance. Our framework provides\ntheoretical bounds on the query complexity for extracting tree-based model,\noffering new insights into the security vulnerabilities of their deployment."
    },
    {
        "date": "2025-02",
        "title": "MELON: Indirect Prompt Injection Defense via Masked Re-execution and Tool Comparison",
        "author": "Kaijie Zhu, Xianjun Yang, Jindong Wang, Wenbo Guo, and William Yang Wang",
        "link": "http://arxiv.org/abs/2502.05174v1",
        "abstract": "Recent research has explored that LLM agents are vulnerable to indirect\nprompt injection (IPI) attacks, where malicious tasks embedded in\ntool-retrieved information can redirect the agent to take unauthorized actions.\nExisting defenses against IPI have significant limitations: either require\nessential model training resources, lack effectiveness against sophisticated\nattacks, or harm the normal utilities. We present MELON (Masked re-Execution\nand TooL comparisON), a novel IPI defense. Our approach builds on the\nobservation that under a successful attack, the agent's next action becomes\nless dependent on user tasks and more on malicious tasks. Following this, we\ndesign MELON to detect attacks by re-executing the agent's trajectory with a\nmasked user prompt modified through a masking function. We identify an attack\nif the actions generated in the original and masked executions are similar. We\nalso include three key designs to reduce the potential false positives and\nfalse negatives. Extensive evaluation on the IPI benchmark AgentDojo\ndemonstrates that MELON outperforms SOTA defenses in both attack prevention and\nutility preservation. Moreover, we show that combining MELON with a SOTA prompt\naugmentation defense (denoted as MELON-Aug) further improves its performance.\nWe also conduct a detailed ablation study to validate our key designs."
    },
    {
        "date": "2025-02",
        "title": "ChallengeMe: An Adversarial Learning-enabled Text Summarization Framework",
        "author": "Xiaoyu Deng, Ye Zhang, Tianmin Guo, Yongzhe Zhang, Zhengjian Kang, and Hang Yang",
        "link": "http://arxiv.org/abs/2502.05084v1",
        "abstract": "The astonishing performance of large language models (LLMs) and their\nremarkable achievements in production and daily life have led to their\nwidespread application in collaborative tasks. However, current large models\nface challenges such as hallucination and lack of specificity in content\ngeneration in vertical domain tasks. Inspired by the contrast and\nclassification mechanisms in human cognitive processes, this paper constructs\nan adversarial learning-based prompt framework named ChallengeMe, which\nincludes three cascaded solutions: generation prompts, evaluation prompts, and\nfeedback optimization. In this process, we designed seven core optimization\ndimensions and set the threshold for adversarial learning. The results of mixed\ncase studies on the text summarization task show that the proposed framework\ncan generate more accurate and fluent text summaries compared to the current\nadvanced mainstream LLMs."
    },
    {
        "date": "2025-02",
        "title": "New Security Challenges Towards In-Sensor Computing Systems",
        "author": "Mashrafi Kajol, and Qiaoyan Yu",
        "link": "http://arxiv.org/abs/2502.05046v1",
        "abstract": "Data collection and processing in advanced health monitoring systems are\nexperiencing revolutionary change. In-Sensor Computing (ISC) systems emerge as\na promising alternative to save energy on massive data transmission,\nanalog-to-digital conversion, and ineffective processing. While the new\nparadigm shift of ISC systems gains increasing attention, the highly compacted\nsystems could incur new challenges from a hardware security perspective. This\nwork first conducts a literature review to highlight the research trend of this\ntopic and then performs comprehensive analyses on the root of security\nchallenges. This is the first work that compares the security challenges of\ntraditional sensor-involved computing systems and emerging ISC systems.\nFurthermore, new attack scenarios are predicted for board-, chip-, and\ndevice-level ISC systems. Two proof-of-concept demos are provided to inspire\nnew countermeasure designs against unique hardware security threats in ISC\nsystems."
    },
    {
        "date": "2025-02",
        "title": "Federated Learning for Anomaly Detection in Energy Consumption Data: Assessing the Vulnerability to Adversarial Attacks",
        "author": "Yohannis Kifle Telila, Damitha Senevirathne, Dumindu Tissera, Apurva Narayan, Miriam A. M. Capretz, and Katarina Grolinger",
        "link": "http://arxiv.org/abs/2502.05041v1",
        "abstract": "Anomaly detection is crucial in the energy sector to identify irregular\npatterns indicating equipment failures, energy theft, or other issues. Machine\nlearning techniques for anomaly detection have achieved great success, but are\ntypically centralized, involving sharing local data with a central server which\nraises privacy and security concerns. Federated Learning (FL) has been gaining\npopularity as it enables distributed learning without sharing local data.\nHowever, FL depends on neural networks, which are vulnerable to adversarial\nattacks that manipulate data, leading models to make erroneous predictions.\nWhile adversarial attacks have been explored in the image domain, they remain\nlargely unexplored in time series problems, especially in the energy domain.\nMoreover, the effect of adversarial attacks in the FL setting is also mostly\nunknown. This paper assesses the vulnerability of FL-based anomaly detection in\nenergy data to adversarial attacks. Specifically, two state-of-the-art models,\nLong Short Term Memory (LSTM) and Transformers, are used to detect anomalies in\nan FL setting, and two white-box attack methods, Fast Gradient Sign Method\n(FGSM) and Projected Gradient Descent (PGD), are employed to perturb the data.\nThe results show that FL is more sensitive to PGD attacks than to FGSM attacks,\nattributed to PGD's iterative nature, resulting in an accuracy drop of over 10%\neven with naive, weaker attacks. Moreover, FL is more affected by these attacks\nthan centralized learning, highlighting the need for defense mechanisms in FL."
    },
    {
        "date": "2025-02",
        "title": "Robust Graph Learning Against Adversarial Evasion Attacks via Prior-Free Diffusion-Based Structure Purification",
        "author": "Jiayi Luo, Qingyun Sun, Haonan Yuan, Xingcheng Fu, and Jianxin Li",
        "link": "http://arxiv.org/abs/2502.05000v1",
        "abstract": "Adversarial evasion attacks pose significant threats to graph learning, with\nlines of studies that have improved the robustness of Graph Neural Networks\n(GNNs). However, existing works rely on priors about clean graphs or attacking\nstrategies, which are often heuristic and inconsistent. To achieve robust graph\nlearning over different types of evasion attacks and diverse datasets, we\ninvestigate this problem from a prior-free structure purification perspective.\nSpecifically, we propose a novel Diffusion-based Structure Purification\nframework named DiffSP, which creatively incorporates the graph diffusion model\nto learn intrinsic distributions of clean graphs and purify the perturbed\nstructures by removing adversaries under the direction of the captured\npredictive patterns without relying on priors. DiffSP is divided into the\nforward diffusion process and the reverse denoising process, during which\nstructure purification is achieved. To avoid valuable information loss during\nthe forward process, we propose an LID-driven nonisotropic diffusion mechanism\nto selectively inject noise anisotropically. To promote semantic alignment\nbetween the clean graph and the purified graph generated during the reverse\nprocess, we reduce the generation uncertainty by the proposed graph transfer\nentropy guided denoising mechanism. Extensive experiments demonstrate the\nsuperior robustness of DiffSP against evasion attacks."
    },
    {
        "date": "2025-02",
        "title": "A Systematic Literature Review on Automated Exploit and Security Test Generation",
        "author": "Quang-Cuong Bui, Emanuele Iannone, Maria Camporese, Torge Hinrichs, Catherine Tony, L\u00e1szl\u00f3 T\u00f3th, Fabio Palomba, P\u00e9ter Heged\u0171s, Fabio Massacci, and Riccardo Scandariato",
        "link": "http://arxiv.org/abs/2502.04953v1",
        "abstract": "The exploit or the Proof of Concept of the vulnerability plays an important\nrole in developing superior vulnerability repair techniques, as it can be used\nas an oracle to verify the correctness of the patches generated by the tools.\nHowever, the vulnerability exploits are often unavailable and require time and\nexpert knowledge to craft. Obtaining them from the exploit generation\ntechniques is another potential solution. The goal of this survey is to aid the\nresearchers and practitioners in understanding the existing techniques for\nexploit generation through the analysis of their characteristics and their\nusability in practice. We identify a list of exploit generation techniques from\nliterature and group them into four categories: automated exploit generation,\nsecurity testing, fuzzing, and other techniques. Most of the techniques focus\non the memory-based vulnerabilities in C/C++ programs and web-based injection\nvulnerabilities in PHP and Java applications. We found only a few studies that\npublicly provided usable tools associated with their techniques."
    },
    {
        "date": "2025-02",
        "title": "Does Unsupervised Domain Adaptation Improve the Robustness of Amortized Bayesian Inference? A Systematic Evaluation",
        "author": "Lasse Elsem\u00fcller, Valentin Pratz, Mischa von Krause, Andreas Voss, Paul-Christian B\u00fcrkner, and Stefan T. Radev",
        "link": "http://arxiv.org/abs/2502.04949v1",
        "abstract": "Neural networks are fragile when confronted with data that significantly\ndeviates from their training distribution. This is true in particular for\nsimulation-based inference methods, such as neural amortized Bayesian inference\n(ABI), where models trained on simulated data are deployed on noisy real-world\nobservations. Recent robust approaches employ unsupervised domain adaptation\n(UDA) to match the embedding spaces of simulated and observed data. However,\nthe lack of comprehensive evaluations across different domain mismatches raises\nconcerns about the reliability in high-stakes applications. We address this gap\nby systematically testing UDA approaches across a wide range of\nmisspecification scenarios in both a controlled and a high-dimensional\nbenchmark. We demonstrate that aligning summary spaces between domains\neffectively mitigates the impact of unmodeled phenomena or noise. However, the\nsame alignment mechanism can lead to failures under prior misspecifications - a\ncritical finding with practical consequences. Our results underscore the need\nfor careful consideration of misspecification types when using UDA techniques\nto increase the robustness of ABI in practice."
    },
    {
        "date": "2025-02",
        "title": "Securing 5G Bootstrapping: A Two-Layer IBS Authentication Protocol",
        "author": "Yilu Dong, Rouzbeh Behnia, Attila A. Yavuz, and Syed Rafiul Hussain",
        "link": "http://arxiv.org/abs/2502.04915v1",
        "abstract": "The lack of authentication during the initial bootstrapping phase between\ncellular devices and base stations allows attackers to deploy fake base\nstations and send malicious messages to the devices. These attacks have been a\nlong-existing problem in cellular networks, enabling adversaries to launch\ndenial-of-service (DoS), information leakage, and location-tracking attacks.\nWhile some defense mechanisms are introduced in 5G, (e.g., encrypting user\nidentifiers to mitigate IMSI catchers), the initial communication between\ndevices and base stations remains unauthenticated, leaving a critical security\ngap. To address this, we propose E2IBS, a novel and efficient two-layer\nidentity-based signature scheme designed for seamless integration with existing\ncellular protocols. We implement E2IBS on an open-source 5G stack and conduct a\ncomprehensive performance evaluation against alternative solutions. Compared to\nthe state-of-the-art Schnorr-HIBS, E2IBS reduces attack surfaces, enables\nfine-grained lawful interception, and achieves 2x speed in verification, making\nit a practical solution for securing 5G base station authentication."
    },
    {
        "date": "2025-02",
        "title": "On the Difficulty of Constructing a Robust and Publicly-Detectable Watermark",
        "author": "Jaiden Fairoze, Guillermo Ortiz-Jim\u00e9nez, Mel Vecerik, Somesh Jha, and Sven Gowal",
        "link": "http://arxiv.org/abs/2502.04901v1",
        "abstract": "This work investigates the theoretical boundaries of creating\npublicly-detectable schemes to enable the provenance of watermarked imagery.\nMetadata-based approaches like C2PA provide unforgeability and\npublic-detectability. ML techniques offer robust retrieval and watermarking.\nHowever, no existing scheme combines robustness, unforgeability, and\npublic-detectability. In this work, we formally define such a scheme and\nestablish its existence. Although theoretically possible, we find that at\npresent, it is intractable to build certain components of our scheme without a\nleap in deep learning capabilities. We analyze these limitations and propose\nresearch directions that need to be addressed before we can practically realize\nrobust and publicly-verifiable provenance."
    },
    {
        "date": "2025-02",
        "title": "Exploit Gradient Skewness to Circumvent Byzantine Defenses for Federated Learning",
        "author": "Yuchen Liu, Chen Chen, Lingjuan Lyu, Yaochu Jin, and Gang Chen",
        "link": "http://arxiv.org/abs/2502.04890v1",
        "abstract": "Federated Learning (FL) is notorious for its vulnerability to Byzantine\nattacks. Most current Byzantine defenses share a common inductive bias: among\nall the gradients, the densely distributed ones are more likely to be honest.\nHowever, such a bias is a poison to Byzantine robustness due to a newly\ndiscovered phenomenon in this paper - gradient skew. We discover that a group\nof densely distributed honest gradients skew away from the optimal gradient\n(the average of honest gradients) due to heterogeneous data. This gradient skew\nphenomenon allows Byzantine gradients to hide within the densely distributed\nskewed gradients. As a result, Byzantine defenses are confused into believing\nthat Byzantine gradients are honest. Motivated by this observation, we propose\na novel skew-aware attack called STRIKE: first, we search for the skewed\ngradients; then, we construct Byzantine gradients within the skewed gradients.\nExperiments on three benchmark datasets validate the effectiveness of our\nattack"
    },
    {
        "date": "2025-02",
        "title": "Robust Conformal Outlier Detection under Contaminated Reference Data",
        "author": "Meshi Bashari, Matteo Sesia, and Yaniv Romano",
        "link": "http://arxiv.org/abs/2502.04807v1",
        "abstract": "Conformal prediction is a flexible framework for calibrating machine learning\npredictions, providing distribution-free statistical guarantees. In outlier\ndetection, this calibration relies on a reference set of labeled inlier data to\ncontrol the type-I error rate. However, obtaining a perfectly labeled inlier\nreference set is often unrealistic, and a more practical scenario involves\naccess to a contaminated reference set containing a small fraction of outliers.\nThis paper analyzes the impact of such contamination on the validity of\nconformal methods. We prove that under realistic, non-adversarial settings,\ncalibration on contaminated data yields conservative type-I error control,\nshedding light on the inherent robustness of conformal methods. This\nconservativeness, however, typically results in a loss of power. To alleviate\nthis limitation, we propose a novel, active data-cleaning framework that\nleverages a limited labeling budget and an outlier detection model to\nselectively annotate data points in the contaminated reference set that are\nsuspected as outliers. By removing only the annotated outliers in this\n``suspicious'' subset, we can effectively enhance power while mitigating the\nrisk of inflating the type-I error rate, as supported by our theoretical\nanalysis. Experiments on real datasets validate the conservative behavior of\nconformal methods under contamination and show that the proposed data-cleaning\nstrategy improves power without sacrificing validity."
    },
    {
        "date": "2025-02",
        "title": "DMPA: Model Poisoning Attacks on Decentralized Federated Learning for Model Differences",
        "author": "Chao Feng, Yunlong Li, Yuanzhe Gao, Alberto Huertas Celdr\u00e1n, Jan von der Assen, G\u00e9r\u00f4me Bovet, and Burkhard Stiller",
        "link": "http://arxiv.org/abs/2502.04771v1",
        "abstract": "Federated learning (FL) has garnered significant attention as a prominent\nprivacy-preserving Machine Learning (ML) paradigm. Decentralized FL (DFL)\neschews traditional FL's centralized server architecture, enhancing the\nsystem's robustness and scalability. However, these advantages of DFL also\ncreate new vulnerabilities for malicious participants to execute adversarial\nattacks, especially model poisoning attacks. In model poisoning attacks,\nmalicious participants aim to diminish the performance of benign models by\ncreating and disseminating the compromised model. Existing research on model\npoisoning attacks has predominantly concentrated on undermining global models\nwithin the Centralized FL (CFL) paradigm, while there needs to be more research\nin DFL. To fill the research gap, this paper proposes an innovative model\npoisoning attack called DMPA. This attack calculates the differential\ncharacteristics of multiple malicious client models and obtains the most\neffective poisoning strategy, thereby orchestrating a collusive attack by\nmultiple participants. The effectiveness of this attack is validated across\nmultiple datasets, with results indicating that the DMPA approach consistently\nsurpasses existing state-of-the-art FL model poisoning attack strategies."
    },
    {
        "date": "2025-02",
        "title": "Mechanistic Understandings of Representation Vulnerabilities and Engineering Robust Vision Transformers",
        "author": "Chashi Mahiul Islam, Samuel Jacob Chacko, Mao Nishino, and Xiuwen Liu",
        "link": "http://arxiv.org/abs/2502.04679v1",
        "abstract": "While transformer-based models dominate NLP and vision applications, their\nunderlying mechanisms to map the input space to the label space semantically\nare not well understood. In this paper, we study the sources of known\nrepresentation vulnerabilities of vision transformers (ViT), where perceptually\nidentical images can have very different representations and semantically\nunrelated images can have the same representation. Our analysis indicates that\nimperceptible changes to the input can result in significant representation\nchanges, particularly in later layers, suggesting potential instabilities in\nthe performance of ViTs. Our comprehensive study reveals that adversarial\neffects, while subtle in early layers, propagate and amplify through the\nnetwork, becoming most pronounced in middle to late layers. This insight\nmotivates the development of NeuroShield-ViT, a novel defense mechanism that\nstrategically neutralizes vulnerable neurons in earlier layers to prevent the\ncascade of adversarial effects. We demonstrate NeuroShield-ViT's effectiveness\nacross various attacks, particularly excelling against strong iterative\nattacks, and showcase its remarkable zero-shot generalization capabilities.\nWithout fine-tuning, our method achieves a competitive accuracy of 77.8% on\nadversarial examples, surpassing conventional robustness methods. Our results\nshed new light on how adversarial effects propagate through ViT layers, while\nproviding a promising approach to enhance the robustness of vision transformers\nagainst adversarial attacks. Additionally, they provide a promising approach to\nenhance the robustness of vision transformers against adversarial attacks."
    },
    {
        "date": "2025-02",
        "title": "Adversarially-Robust TD Learning with Markovian Data: Finite-Time Rates and Fundamental Limits",
        "author": "Sreejeet Maity, and Aritra Mitra",
        "link": "http://arxiv.org/abs/2502.04662v1",
        "abstract": "One of the most basic problems in reinforcement learning (RL) is policy\nevaluation: estimating the long-term return, i.e., value function,\ncorresponding to a given fixed policy. The celebrated Temporal Difference (TD)\nlearning algorithm addresses this problem, and recent work has investigated\nfinite-time convergence guarantees for this algorithm and variants thereof.\nHowever, these guarantees hinge on the reward observations being always\ngenerated from a well-behaved (e.g., sub-Gaussian) true reward distribution.\nMotivated by harsh, real-world environments where such an idealistic assumption\nmay no longer hold, we revisit the policy evaluation problem from the\nperspective of adversarial robustness. In particular, we consider a\nHuber-contaminated reward model where an adversary can arbitrarily corrupt each\nreward sample with a small probability $\\epsilon$. Under this observation\nmodel, we first show that the adversary can cause the vanilla TD algorithm to\nconverge to any arbitrary value function. We then develop a novel algorithm\ncalled Robust-TD and prove that its finite-time guarantees match that of\nvanilla TD with linear function approximation up to a small $O(\\epsilon)$ term\nthat captures the effect of corruption. We complement this result with a\nminimax lower bound, revealing that such an additive corruption-induced term is\nunavoidable. To our knowledge, these results are the first of their kind in the\ncontext of adversarial robustness of stochastic approximation schemes driven by\nMarkov noise. The key new technical tool that enables our results is an\nanalysis of the Median-of-Means estimator with corrupted, time-correlated data\nthat might be of independent interest to the literature on robust statistics."
    },
    {
        "date": "2025-02",
        "title": "Confidence Elicitation: A New Attack Vector for Large Language Models",
        "author": "Brian Formento, Chuan Sheng Foo, and See-Kiong Ng",
        "link": "http://arxiv.org/abs/2502.04643v2",
        "abstract": "A fundamental issue in deep learning has been adversarial robustness. As\nthese systems have scaled, such issues have persisted. Currently, large\nlanguage models (LLMs) with billions of parameters suffer from adversarial\nattacks just like their earlier, smaller counterparts. However, the threat\nmodels have changed. Previously, having gray-box access, where input embeddings\nor output logits/probabilities were visible to the user, might have been\nreasonable. However, with the introduction of closed-source models, no\ninformation about the model is available apart from the generated output. This\nmeans that current black-box attacks can only utilize the final prediction to\ndetect if an attack is successful. In this work, we investigate and demonstrate\nthe potential of attack guidance, akin to using output probabilities, while\nhaving only black-box access in a classification setting. This is achieved\nthrough the ability to elicit confidence from the model. We empirically show\nthat the elicited confidence is calibrated and not hallucinated for current\nLLMs. By minimizing the elicited confidence, we can therefore increase the\nlikelihood of misclassification. Our new proposed paradigm demonstrates\npromising state-of-the-art results on three datasets across two models\n(LLaMA-3-8B-Instruct and Mistral-7B-Instruct-V0.3) when comparing our technique\nto existing hard-label black-box attack methods that introduce word-level\nsubstitutions."
    },
    {
        "date": "2025-02",
        "title": "The $\u03b1$-Alternator: Dynamic Adaptation To Varying Noise Levels In Sequences Using The Vendi Score For Improved Robustness and Performance",
        "author": "Mohammad Reza Rezaei, and Adji Bousso Dieng",
        "link": "http://arxiv.org/abs/2502.04593v1",
        "abstract": "Current state-of-the-art dynamical models, such as Mamba, assume the same\nlevel of noisiness for all elements of a given sequence, which limits their\nperformance on noisy temporal data. In this paper, we introduce the\n$\\alpha$-Alternator, a novel generative model for time-dependent data that\ndynamically adapts to the complexity introduced by varying noise levels in\nsequences. The $\\alpha$-Alternator leverages the Vendi Score (VS), a flexible\nsimilarity-based diversity metric, to adjust, at each time step $t$, the\ninfluence of the sequence element at time $t$ and the latent representation of\nthe dynamics up to that time step on the predicted future dynamics. This\ninfluence is captured by a parameter that is learned and shared across all\nsequences in a given dataset. The sign of this parameter determines the\ndirection of influence. A negative value indicates a noisy dataset, where a\nsequence element that increases the VS is considered noisy, and the model\nrelies more on the latent history when processing that element. Conversely,\nwhen the parameter is positive, a sequence element that increases the VS is\nconsidered informative, and the $\\alpha$-Alternator relies more on this new\ninput than on the latent history when updating its predicted latent dynamics.\nThe $\\alpha$-Alternator is trained using a combination of observation masking\nand Alternator loss minimization. Masking simulates varying noise levels in\nsequences, enabling the model to be more robust to these fluctuations and\nimproving its performance in trajectory prediction, imputation, and\nforecasting. Our experimental results demonstrate that the $\\alpha$-Alternator\noutperforms both Alternators and state-of-the-art state-space models across\nneural decoding and time-series forecasting benchmarks."
    },
    {
        "date": "2025-02",
        "title": "Zero-shot Meta-learning for Tabular Prediction Tasks with Adversarially Pre-trained Transformer",
        "author": "Yulun Wu, and Doron L. Bergman",
        "link": "http://arxiv.org/abs/2502.04573v1",
        "abstract": "We present an Adversarially Pre-trained Transformer (APT) that is able to\nperform zero-shot meta-learning on tabular prediction tasks without\npre-training on any real-world dataset, extending on the recent development of\nPrior-Data Fitted Networks (PFNs) and TabPFN. Specifically, APT is pre-trained\nwith adversarial synthetic data agents, who continue to shift their underlying\ndata generating distribution and deliberately challenge the model with\ndifferent synthetic datasets. In addition, we propose a mixture block\narchitecture that is able to handle classification tasks with arbitrary number\nof classes, addressing the class size limitation -- a crucial weakness of prior\ndeep tabular zero-shot learners. In experiments, we show that our framework\nmatches state-of-the-art performance on small classification tasks without\nfiltering on dataset characteristics such as number of classes and number of\nmissing values, while maintaining an average runtime under one second. On\ncommon benchmark dataset suites in both classification and regression, we show\nthat adversarial pre-training was able to enhance TabPFN's performance. In our\nanalysis, we demonstrate that the adversarial synthetic data agents were able\nto generate a more diverse collection of data compared to the ordinary random\ngenerator in TabPFN. In addition, we demonstrate that our mixture block neural\ndesign has improved generalizability and greatly accelerated pre-training."
    },
    {
        "date": "2025-02",
        "title": "SoK: \"Interoperability vs Security\" Arguments: A Technical Framework",
        "author": "Daji Landis, Elettra Bietti, and Sunoo Park",
        "link": "http://arxiv.org/abs/2502.04538v1",
        "abstract": "Concerns about big tech's monopoly power have featured prominently in recent\nmedia and policy discourse, and regulators across the US, the EU, and beyond\nhave ramped up efforts to promote healthier competition in the market. One of\nthe favored approaches is to require certain kinds of interoperation between\nplatforms, to mitigate the current concentration of power in the biggest\ncompanies. Unsurprisingly, interoperability initiatives have generally been met\nwith vocal resistance by big tech companies. Perhaps more surprisingly, a\nsignificant part of that pushback has been in the name of security -- that is,\narguing against interoperation on the basis that it will undermine security.\n  We conduct a detailed examination of \"security vs. interoperability\"\narguments in the context of recent antitrust proceedings in the US and the EU.\nFirst, we propose a taxonomy of such arguments. Second, we provide several\ndetailed case studies, which illustrate our taxonomy's utility in disentangling\nwhere security and interoperability are and are not in tension, where securing\ninteroperable systems presents novel engineering challenges, and where\n\"security arguments\" against interoperability are really more about\nanti-competitive behavior than security. Third, we undertake a comparative\nanalysis that highlights key considerations around the interplay of economic\nincentives, market power, and security across diverse contexts where security\nand interoperability may appear to be in tension. We believe systematically\ndistinguishing cases and patterns within our taxonomy and analytical framework\ncan be a valuable analytical tool for experts and non-experts alike in today's\nfast-paced regulatory landscape."
    },
    {
        "date": "2025-02",
        "title": "Robust Probabilistic Model Checking with Continuous Reward Domains",
        "author": "Xiaotong Ji, Hanchun Wang, Antonio Filieri, and Ilenia Epifani",
        "link": "http://arxiv.org/abs/2502.04530v1",
        "abstract": "Probabilistic model checking traditionally verifies properties on the\nexpected value of a measure of interest. This restriction may fail to capture\nthe quality of service of a significant proportion of a system's runs,\nespecially when the probability distribution of the measure of interest is\npoorly represented by its expected value due to heavy-tail behaviors or\nmultiple modalities. Recent works inspired by distributional reinforcement\nlearning use discrete histograms to approximate integer reward distribution,\nbut they struggle with continuous reward space and present challenges in\nbalancing accuracy and scalability. We propose a novel method for handling both\ncontinuous and discrete reward distributions in Discrete Time Markov Chains\nusing moment matching with Erlang mixtures. By analytically deriving\nhigher-order moments through Moment Generating Functions, our method\napproximates the reward distribution with theoretically bounded error while\npreserving the statistical properties of the true distribution. This detailed\ndistributional insight enables the formulation and robust model checking of\nquality properties based on the entire reward distribution function, rather\nthan restricting to its expected value. We include a theoretical foundation\nensuring bounded approximation errors, along with an experimental evaluation\ndemonstrating our method's accuracy and scalability in practical model-checking\nproblems."
    },
    {
        "date": "2025-02",
        "title": "Group-Adaptive Threshold Optimization for Robust AI-Generated Text Detection",
        "author": "Minseok Jung, Cynthia Fuertes Panizo, Liam Dugan, Yi R., Fung, Pin-Yu Chen, and Paul Pu Liang",
        "link": "http://arxiv.org/abs/2502.04528v2",
        "abstract": "The advancement of large language models (LLMs) has made it difficult to\ndifferentiate human-written text from AI-generated text. Several AI-text\ndetectors have been developed in response, which typically utilize a fixed\nglobal threshold (e.g., {\\theta} = 0.5) to classify machine-generated text.\nHowever, we find that one universal threshold can fail to account for\nsubgroup-specific distributional variations. For example, when using a fixed\nthreshold, detectors make more false positive errors on shorter human-written\ntext than longer, and more positive classifications on neurotic writing styles\nthan open among long text. These discrepancies can lead to misclassification\nthat disproportionately affects certain groups. We address this critical\nlimitation by introducing FairOPT, an algorithm for group-specific threshold\noptimization in AI-generated content classifiers. Our approach partitions data\ninto subgroups based on attributes (e.g., text length and writing style) and\nlearns decision thresholds for each group, which enables careful balancing of\nperformance and fairness metrics within each subgroup. In experiments with four\nAI text classifiers on three datasets, FairOPT enhances overall F1 score and\ndecreases balanced error rate (BER) discrepancy across subgroups. Our framework\npaves the way for more robust and fair classification criteria in AI-generated\noutput detection."
    },
    {
        "date": "2025-02",
        "title": "The 23andMe Data Breach: Analyzing Credential Stuffing Attacks, Security Vulnerabilities, and Mitigation Strategies",
        "author": "Ryan Holthouse, Serena Owens, and Suman Bhunia",
        "link": "http://arxiv.org/abs/2502.04303v1",
        "abstract": "In October 2023, 23andMe, a prominent provider of personal genetic testing,\nancestry, and health information services, suffered a significant data breach\norchestrated by a cybercriminal known as ``Golem.'' Initially, approximately\n14,000 user accounts were compromised by a credential smear attack, exploiting\nreused usernames and passwords from previous data leaks. However, due to the\ninterconnected nature of 23andMe's DNA Relatives and Family Tree features, the\nbreach expanded exponentially, exposing sensitive personal and genetic data of\napproximately 5.5 million users and 1.4 million additional profiles. The attack\nhighlights the increasing threat of credential stuffing, exacerbated by poor\npassword hygiene and the absence of robust security measures such as\nmulti-factor authentication (MFA) and rate limiting. In response, 23andMe\nmandated password resets, implemented email-based two-step verification, and\nadvised users to update passwords across other services. This paper critically\nanalyzes the attack methodology, its impact on users and the company, and\nexplores potential mitigation strategies, including enhanced authentication\nprotocols, proactive breach detection, and improved cybersecurity practices.\nThe findings underscore the necessity of stronger user authentication measures\nand corporate responsibility in safeguarding sensitive genetic and personal\ndata."
    },
    {
        "date": "2025-02",
        "title": "Adapting to Evolving Adversaries with Regularized Continual Robust Training",
        "author": "Sihui Dai, Christian Cianfarani, Arjun Bhagoji, Vikash Sehwag, and Prateek Mittal",
        "link": "http://arxiv.org/abs/2502.04248v1",
        "abstract": "Robust training methods typically defend against specific attack types, such\nas Lp attacks with fixed budgets, and rarely account for the fact that\ndefenders may encounter new attacks over time. A natural solution is to adapt\nthe defended model to new adversaries as they arise via fine-tuning, a method\nwhich we call continual robust training (CRT). However, when implemented\nnaively, fine-tuning on new attacks degrades robustness on previous attacks.\nThis raises the question: how can we improve the initial training and\nfine-tuning of the model to simultaneously achieve robustness against previous\nand new attacks? We present theoretical results which show that the gap in a\nmodel's robustness against different attacks is bounded by how far each attack\nperturbs a sample in the model's logit space, suggesting that regularizing with\nrespect to this logit space distance can help maintain robustness against\nprevious attacks. Extensive experiments on 3 datasets (CIFAR-10, CIFAR-100, and\nImageNette) and over 100 attack combinations demonstrate that the proposed\nregularization improves robust accuracy with little overhead in training time.\nOur findings and open-source code lay the groundwork for the deployment of\nmodels robust to evolving attacks."
    },
    {
        "date": "2025-02",
        "title": "Saflo: eBPF-Based MPTCP Scheduler for Mitigating Traffic Analysis Attacks in Cellular Networks",
        "author": "Sangwoo Lee, Liuyi Jin, and Radu Stoleru",
        "link": "http://arxiv.org/abs/2502.04236v1",
        "abstract": "This paper presents the $\\underline{\\textbf{saf}}$e\nsub$\\underline{\\textbf{flo}}$w (Saflo) eBPF-based multipath TCP (MPTCP)\nscheduler, designed to mitigate traffic analysis attacks in cellular networks.\nTraffic analysis attacks, which exploit vulnerabilities in Downlink Control\nInformation (DCI) messages, remain a significant security threat in LTE/5G\nnetworks. To counter such threats, the Saflo scheduler employs multipath\ncommunication combined with additional security-related tasks. Specifically, it\nutilizes eBPF tools to operate in both kernel and user spaces. In the kernel\nspace, the eBPF scheduler performs multipath scheduling while excluding paths\ndisabled by the user-space programs. The user-space programs conduct\nsecurity-related computations and machine learning-based attack detection,\ndetermining whether each path should be enabled or disabled. This approach\noffloads computationally intensive tasks to user-space programs, enabling\ntimely multipath scheduling in kernel space. The Saflo scheduler was evaluated\nin a private LTE/5G testbed. The results demonstrated that it significantly\nreduces the accuracy of video identification and user identification attacks in\ncellular networks while maintaining reasonable network performance for users."
    },
    {
        "date": "2025-02",
        "title": "XAttnMark: Learning Robust Audio Watermarking with Cross-Attention",
        "author": "Yixin Liu, Lie Lu, Jihui Jin, Lichao Sun, and Andrea Fanelli",
        "link": "http://arxiv.org/abs/2502.04230v2",
        "abstract": "The rapid proliferation of generative audio synthesis and editing\ntechnologies has raised significant concerns about copyright infringement, data\nprovenance, and the spread of misinformation through deepfake audio.\nWatermarking offers a proactive solution by embedding imperceptible,\nidentifiable, and traceable marks into audio content. While recent neural\nnetwork-based watermarking methods like WavMark and AudioSeal have improved\nrobustness and quality, they struggle to achieve both robust detection and\naccurate attribution simultaneously. This paper introduces Cross-Attention\nRobust Audio Watermark (XAttnMark), which bridges this gap by leveraging\npartial parameter sharing between the generator and the detector, a\ncross-attention mechanism for efficient message retrieval, and a temporal\nconditioning module for improved message distribution. Additionally, we propose\na psychoacoustic-aligned temporal-frequency masking loss that captures\nfine-grained auditory masking effects, enhancing watermark imperceptibility.\nOur approach achieves state-of-the-art performance in both detection and\nattribution, demonstrating superior robustness against a wide range of audio\ntransformations, including challenging generative editing with strong editing\nstrength. The project webpage is available at\nhttps://liuyixin-louis.github.io/xattnmark/."
    },
    {
        "date": "2025-02",
        "title": "Provably Robust Explainable Graph Neural Networks against Graph Perturbation Attacks",
        "author": "Jiate Li, Meng Pang, Yun Dong, Jinyuan Jia, and Binghui Wang",
        "link": "http://arxiv.org/abs/2502.04224v1",
        "abstract": "Explaining Graph Neural Network (XGNN) has gained growing attention to\nfacilitate the trust of using GNNs, which is the mainstream method to learn\ngraph data. Despite their growing attention, Existing XGNNs focus on improving\nthe explanation performance, and its robustness under attacks is largely\nunexplored. We noticed that an adversary can slightly perturb the graph\nstructure such that the explanation result of XGNNs is largely changed. Such\nvulnerability of XGNNs could cause serious issues particularly in\nsafety/security-critical applications. In this paper, we take the first step to\nstudy the robustness of XGNN against graph perturbation attacks, and propose\nXGNNCert, the first provably robust XGNN. Particularly, our XGNNCert can\nprovably ensure the explanation result for a graph under the worst-case graph\nperturbation attack is close to that without the attack, while not affecting\nthe GNN prediction, when the number of perturbed edges is bounded. Evaluation\nresults on multiple graph datasets and GNN explainers show the effectiveness of\nXGNNCert."
    },
    {
        "date": "2025-02",
        "title": "\"Short-length\" Adversarial Training Helps LLMs Defend \"Long-length\" Jailbreak Attacks: Theoretical and Empirical Evidence",
        "author": "Shaopeng Fu, Liang Ding, and Di Wang",
        "link": "http://arxiv.org/abs/2502.04204v1",
        "abstract": "Jailbreak attacks against large language models (LLMs) aim to induce harmful\nbehaviors in LLMs through carefully crafted adversarial prompts. To mitigate\nattacks, one way is to perform adversarial training (AT)-based alignment, i.e.,\ntraining LLMs on some of the most adversarial prompts to help them learn how to\nbehave safely under attacks. During AT, the length of adversarial prompts plays\na critical role in the robustness of aligned LLMs. This paper focuses on\nadversarial suffix jailbreak attacks and unveils that to defend against a\njailbreak attack with an adversarial suffix of length $\\Theta(M)$, it is enough\nto align LLMs on prompts with adversarial suffixes of length\n$\\Theta(\\sqrt{M})$. Theoretically, we analyze the adversarial in-context\nlearning of linear transformers on linear regression tasks and prove a robust\ngeneralization bound for trained transformers. The bound depends on the term\n$\\Theta(\\sqrt{M_{\\text{test}}}/M_{\\text{train}})$, where $M_{\\text{train}}$ and\n$M_{\\text{test}}$ are the number of adversarially perturbed in-context samples\nduring training and testing. Empirically, we conduct AT on popular open-source\nLLMs and evaluate their robustness against jailbreak attacks of different\nadversarial suffix lengths. Results confirm a positive correlation between the\nattack success rate and the ratio of the square root of the adversarial suffix\nduring jailbreaking to the length during AT. Our findings show that it is\npractical to defend \"long-length\" jailbreak attacks via efficient\n\"short-length\" AT. The code is available at https://github.com/fshp971/adv-icl."
    },
    {
        "date": "2025-02",
        "title": "Safeguarding connected autonomous vehicle communication: Protocols, intra- and inter-vehicular attacks and defenses",
        "author": "Mohammed Aledhari, Rehma Razzak, Mohamed Rahouti, Abbas Yazdinejad, Reza M. Parizi, Basheer Qolomany, Mohsen Guizani, Junaid Qadir, and Ala Al-Fuqaha",
        "link": "http://arxiv.org/abs/2502.04201v1",
        "abstract": "The advancements in autonomous driving technology, coupled with the growing\ninterest from automotive manufacturers and tech companies, suggest a rising\nadoption of Connected Autonomous Vehicles (CAVs) in the near future. Despite\nsome evidence of higher accident rates in AVs, these incidents tend to result\nin less severe injuries compared to traditional vehicles due to cooperative\nsafety measures. However, the increased complexity of CAV systems exposes them\nto significant security vulnerabilities, potentially compromising their\nperformance and communication integrity. This paper contributes by presenting a\ndetailed analysis of existing security frameworks and protocols, focusing on\nintra- and inter-vehicle communications. We systematically evaluate the\neffectiveness of these frameworks in addressing known vulnerabilities and\npropose a set of best practices for enhancing CAV communication security. The\npaper also provides a comprehensive taxonomy of attack vectors in CAV\necosystems and suggests future research directions for designing more robust\nsecurity mechanisms. Our key contributions include the development of a new\nclassification system for CAV security threats, the proposal of practical\nsecurity protocols, and the introduction of use cases that demonstrate how\nthese protocols can be integrated into real-world CAV applications. These\ninsights are crucial for advancing secure CAV adoption and ensuring the safe\nintegration of autonomous vehicles into intelligent transportation systems."
    },
    {
        "date": "2025-02",
        "title": "Generative Adversarial Networks Bridging Art and Machine Intelligence",
        "author": "Junhao Song, Yichao Zhang, Ziqian Bi, Tianyang Wang, Keyu Chen, Ming Li, Qian Niu, Junyu Liu, Benji Peng, Sen Zhang, Ming Liu, Jiawei Xu, Xuanhe Pan, Jinlang Wang, Pohsun Feng, Yizhu Wen, Lawrence K. Q. Yan, Hong-Ming Tseng, Xinyuan Song, Jintao Ren, Silin Chen, Yunze Wang, Weiche Hsieh, Bowen Jing, Junjie Yang, Jun Zhou, Zheyu Yao, and Chia Xin Liang",
        "link": "http://arxiv.org/abs/2502.04116v2",
        "abstract": "Generative Adversarial Networks (GAN) have greatly influenced the development\nof computer vision and artificial intelligence in the past decade and also\nconnected art and machine intelligence together. This book begins with a\ndetailed introduction to the fundamental principles and historical development\nof GANs, contrasting them with traditional generative models and elucidating\nthe core adversarial mechanisms through illustrative Python examples. The text\nsystematically addresses the mathematical and theoretical underpinnings\nincluding probability theory, statistics, and game theory providing a solid\nframework for understanding the objectives, loss functions, and optimisation\nchallenges inherent to GAN training. Subsequent chapters review classic\nvariants such as Conditional GANs, DCGANs, InfoGAN, and LAPGAN before\nprogressing to advanced training methodologies like Wasserstein GANs, GANs with\ngradient penalty, least squares GANs, and spectral normalisation techniques.\nThe book further examines architectural enhancements and task-specific\nadaptations in generators and discriminators, showcasing practical\nimplementations in high resolution image generation, artistic style transfer,\nvideo synthesis, text to image generation and other multimedia applications.\nThe concluding sections offer insights into emerging research trends, including\nself-attention mechanisms, transformer-based generative models, and a\ncomparative analysis with diffusion models, thus charting promising directions\nfor future developments in both academic and applied settings."
    },
    {
        "date": "2025-02",
        "title": "The Gradient Puppeteer: Adversarial Domination in Gradient Leakage Attacks through Model Poisoning",
        "author": "Kunlan Xiang, Haomiao Yang, Meng Hao, Haoxin Wang, Shaofeng Li, Zikang Ding, and Tianwei Zhang",
        "link": "http://arxiv.org/abs/2502.04106v1",
        "abstract": "In Federated Learning (FL), clients share gradients with a central server\nwhile keeping their data local. However, malicious servers could deliberately\nmanipulate the models to reconstruct clients' data from shared gradients,\nposing significant privacy risks. Although such active gradient leakage attacks\n(AGLAs) have been widely studied, they suffer from several limitations\nincluding incomplete attack coverage and poor stealthiness. In this paper, we\naddress these limitations with two core contributions. First, we introduce a\nnew theoretical analysis approach, which uniformly models AGLAs as backdoor\npoisoning. This analysis approach reveals that the core principle of AGLAs is\nto bias the gradient space to prioritize the reconstruction of a small subset\nof samples while sacrificing the majority, which theoretically explains the\nabove limitations of existing AGLAs. Second, we propose Enhanced Gradient\nGlobal Vulnerability (EGGV), the first AGLA that achieves complete attack\ncoverage while evading client-side detection. In particular, EGGV employs a\ngradient projector and a jointly optimized discriminator to assess gradient\nvulnerability, steering the gradient space toward the point most prone to data\nleakage. Extensive experiments show that EGGV achieves complete attack coverage\nand surpasses SOTA with at least a 43% increase in reconstruction quality\n(PSNR) and a 45% improvement in stealthiness (D-SNR)."
    },
    {
        "date": "2025-02",
        "title": "Smart IoT Security: Lightweight Machine Learning Techniques for Multi-Class Attack Detection in IoT Networks",
        "author": "Shahran Rahman Alve, Muhammad Zawad Mahmud, Samiha Islam, Md. Asaduzzaman Chowdhury, and Jahirul Islam",
        "link": "http://arxiv.org/abs/2502.04057v1",
        "abstract": "In the growing terrain of the Internet of Things (IoT), it is vital that\nnetworks are secure to protect against a range of cyber threats. Based on the\nstrong machine learning framework, this study proposes novel lightweight\nensemble approaches for improving multi-class attack detection of IoT devices.\nUsing the large CICIoT 2023 dataset with 34 attack types distributed amongst 10\nattack categories, we systematically evaluated the performance of a wide\nvariety of modern machine learning methods with the aim of establishing the\nbest-performing algorithmic choice to secure IoT applications. In particular,\nwe explore approaches based on ML classifiers to tackle the biocharges\ncharacterized by the challenging and heterogeneous nature of attack vectors in\nIoT environments. The method that performed best was the Decision Tree, with an\naccuracy of 99.56% and an F1 score of 99.62%, showing that this model is\ncapable of accurately and reliably detecting threats.The Random Forest model\nwas the next best-performing model with 98.22% and an F1 score of 98.24%,\nsuggesting that ML methods are quite effective in a situation of\nhigh-dimensional data. Our results highlight the potential for using ML\nclassifiers in bolstering security for IoT devices and also serve as\nmotivations for future investigations targeting scalable, keystroke-based\nattack detection systems. We believe that our method provides a new path to\ndevelop complex machine learning algorithms for low-resource IoT devices,\nbalancing both accuracy and time efficiency needs. In summary, these\ncontributions enrich the state of the art of the IoT security literature,\nlaying down solid ground and guidelines for the deployment of smart, adaptive\nsecurity in IoT settings."
    },
    {
        "date": "2025-02",
        "title": "Comparing privacy notions for protection against reconstruction attacks in machine learning",
        "author": "Sayan Biswas, Mark Dras, Pedro Faustini, Natasha Fernandes, Annabelle McIver, Catuscia Palamidessi, and Parastoo Sadeghi",
        "link": "http://arxiv.org/abs/2502.04045v1",
        "abstract": "Within the machine learning community, reconstruction attacks are a principal\nconcern and have been identified even in federated learning (FL), which was\ndesigned with privacy preservation in mind. In response to these threats, the\nprivacy community recommends the use of differential privacy (DP) in the\nstochastic gradient descent algorithm, termed DP-SGD. However, the\nproliferation of variants of DP in recent years\\textemdash such as metric\nprivacy\\textemdash has made it challenging to conduct a fair comparison between\ndifferent mechanisms due to the different meanings of the privacy parameters\n$\\epsilon$ and $\\delta$ across different variants. Thus, interpreting the\npractical implications of $\\epsilon$ and $\\delta$ in the FL context and amongst\nvariants of DP remains ambiguous. In this paper, we lay a foundational\nframework for comparing mechanisms with differing notions of privacy\nguarantees, namely $(\\epsilon,\\delta)$-DP and metric privacy. We provide two\nfoundational means of comparison: firstly, via the well-established\n$(\\epsilon,\\delta)$-DP guarantees, made possible through the R\\'enyi\ndifferential privacy framework; and secondly, via Bayes' capacity, which we\nidentify as an appropriate measure for reconstruction threats."
    },
    {
        "date": "2025-02",
        "title": "Improving the Perturbation-Based Explanation of Deepfake Detectors Through the Use of Adversarially-Generated Samples",
        "author": "Konstantinos Tsigos, Evlampios Apostolidis, and Vasileios Mezaris",
        "link": "http://arxiv.org/abs/2502.03957v1",
        "abstract": "In this paper, we introduce the idea of using adversarially-generated samples\nof the input images that were classified as deepfakes by a detector, to form\nperturbation masks for inferring the importance of different input features and\nproduce visual explanations. We generate these samples based on Natural\nEvolution Strategies, aiming to flip the original deepfake detector's decision\nand classify these samples as real. We apply this idea to four\nperturbation-based explanation methods (LIME, SHAP, SOBOL and RISE) and\nevaluate the performance of the resulting modified methods using a SOTA\ndeepfake detection model, a benchmarking dataset (FaceForensics++) and a\ncorresponding explanation evaluation framework. Our quantitative assessments\ndocument the mostly positive contribution of the proposed perturbation approach\nin the performance of explanation methods. Our qualitative analysis shows the\ncapacity of the modified explanation methods to demarcate the manipulated image\nregions more accurately, and thus to provide more useful explanations."
    },
    {
        "date": "2025-02",
        "title": "Time-based GNSS attack detection",
        "author": "Marco Spanghero, and Panos Papadimitratos",
        "link": "http://arxiv.org/abs/2502.03868v1",
        "abstract": "To safeguard Civilian Global Navigation Satellite Systems (GNSS) external\ninformation available to the platform encompassing the GNSS receiver can be\nused to detect attacks. Cross-checking the GNSS-provided time against\nalternative multiple trusted time sources can lead to attack detection aiming\nat controlling the GNSS receiver time. Leveraging external, network-connected\nsecure time providers and onboard clock references, we achieve detection even\nunder fine-grained time attacks. We provide an extensive evaluation of our\nmulti-layered defense against adversaries mounting attacks against the GNSS\nreceiver along with controlling the network link. We implement adversaries\nspanning from simplistic spoofers to advanced ones synchronized with the GNSS\nconstellation. We demonstrate attack detection is possible in all tested cases\n(sharp discontinuity, smooth take-over, and coordinated network manipulation)\nwithout changes to the structure of the GNSS receiver. Leveraging the diversity\nof the reference time sources, detection of take-over time push as low as 150us\nis possible. Smooth take-overs forcing variations as low as 30ns are also\ndetected based on on-board precision oscillators. The method (and thus the\nevaluation) is largely agnostic to the satellite constellation and the attacker\ntype, making time-based data validation of GNSS information compatible with\nexisting receivers and readily deployable."
    },
    {
        "date": "2025-02",
        "title": "Synthetic Poisoning Attacks: The Impact of Poisoned MRI Image on U-Net Brain Tumor Segmentation",
        "author": "Tianhao Li, Tianyu Zeng, Yujia Zheng, Chulong Zhang, Jingyu Lu, Haotian Huang, Chuangxin Chu, Fang-Fang Yin, and Zhenyu Yang",
        "link": "http://arxiv.org/abs/2502.03825v1",
        "abstract": "Deep learning-based medical image segmentation models, such as U-Net, rely on\nhigh-quality annotated datasets to achieve accurate predictions. However, the\nincreasing use of generative models for synthetic data augmentation introduces\npotential risks, particularly in the absence of rigorous quality control. In\nthis paper, we investigate the impact of synthetic MRI data on the robustness\nand segmentation accuracy of U-Net models for brain tumor segmentation.\nSpecifically, we generate synthetic T1-contrast-enhanced (T1-Ce) MRI scans\nusing a GAN-based model with a shared encoding-decoding framework and\nshortest-path regularization. To quantify the effect of synthetic data\ncontamination, we train U-Net models on progressively \"poisoned\" datasets,\nwhere synthetic data proportions range from 16.67% to 83.33%. Experimental\nresults on a real MRI validation set reveal a significant performance\ndegradation as synthetic data increases, with Dice coefficients dropping from\n0.8937 (33.33% synthetic) to 0.7474 (83.33% synthetic). Accuracy and\nsensitivity exhibit similar downward trends, demonstrating the detrimental\neffect of synthetic data on segmentation robustness. These findings underscore\nthe importance of quality control in synthetic data integration and highlight\nthe risks of unregulated synthetic augmentation in medical image analysis. Our\nstudy provides critical insights for the development of more reliable and\ntrustworthy AI-driven medical imaging systems."
    },
    {
        "date": "2025-02",
        "title": "SoK: Benchmarking Poisoning Attacks and Defenses in Federated Learning",
        "author": "Heyi Zhang, Yule Liu, Xinlei He, Jun Wu, Tianshuo Cong, and Xinyi Huang",
        "link": "http://arxiv.org/abs/2502.03801v1",
        "abstract": "Federated learning (FL) enables collaborative model training while preserving\ndata privacy, but its decentralized nature exposes it to client-side data\npoisoning attacks (DPAs) and model poisoning attacks (MPAs) that degrade global\nmodel performance. While numerous proposed defenses claim substantial\neffectiveness, their evaluation is typically done in isolation with limited\nattack strategies, raising concerns about their validity. Additionally,\nexisting studies overlook the mutual effectiveness of defenses against both\nDPAs and MPAs, causing fragmentation in this field. This paper aims to provide\na unified benchmark and analysis of defenses against DPAs and MPAs, clarifying\nthe distinction between these two similar but slightly distinct domains. We\npresent a systematic taxonomy of poisoning attacks and defense strategies,\noutlining their design, strengths, and limitations. Then, a unified comparative\nevaluation across FL algorithms and data heterogeneity is conducted to validate\ntheir individual and mutual effectiveness and derive key insights for design\nprinciples and future research. Along with the analysis, we frame our work to a\nunified benchmark, FLPoison, with high modularity and scalability to evaluate\n15 representative poisoning attacks and 17 defense strategies, facilitating\nfuture research in this domain. Code is available at\nhttps://github.com/vio1etus/FLPoison."
    },
    {
        "date": "2025-02",
        "title": "BitAbuse: A Dataset of Visually Perturbed Texts for Defending Phishing Attacks",
        "author": "Hanyong Lee, Chaelyn Lee, Yongjae Lee, and Jaesung Lee",
        "link": "http://arxiv.org/abs/2502.05225v1",
        "abstract": "Phishing often targets victims through visually perturbed texts to bypass\nsecurity systems. The noise contained in these texts functions as an\nadversarial attack, designed to deceive language models and hinder their\nability to accurately interpret the content. However, since it is difficult to\nobtain sufficient phishing cases, previous studies have used synthetic datasets\nthat do not contain real-world cases. In this study, we propose the BitAbuse\ndataset, which includes real-world phishing cases, to address the limitations\nof previous research. Our dataset comprises a total of 325,580 visually\nperturbed texts. The dataset inputs are drawn from the raw corpus, consisting\nof visually perturbed sentences and sentences generated through an artificial\nperturbation process. Each input sentence is labeled with its corresponding\nground truth, representing the restored, non-perturbed version. Language models\ntrained on our proposed dataset demonstrated significantly better performance\ncompared to previous methods, achieving an accuracy of approximately 96%. Our\nanalysis revealed a significant gap between real-world and synthetic examples,\nunderscoring the value of our dataset for building reliable pre-trained models\nfor restoration tasks. We release the BitAbuse dataset, which includes\nreal-world phishing cases annotated with visual perturbations, to support\nfuture research in adversarial attack defense."
    },
    {
        "date": "2025-02",
        "title": "A Survey on Backdoor Threats in Large Language Models (LLMs): Attacks, Defenses, and Evaluations",
        "author": "Yihe Zhou, Tao Ni, Wei-Bin Lee, and Qingchuan Zhao",
        "link": "http://arxiv.org/abs/2502.05224v1",
        "abstract": "Large Language Models (LLMs) have achieved significantly advanced\ncapabilities in understanding and generating human language text, which have\ngained increasing popularity over recent years. Apart from their\nstate-of-the-art natural language processing (NLP) performance, considering\ntheir widespread usage in many industries, including medicine, finance,\neducation, etc., security concerns over their usage grow simultaneously. In\nrecent years, the evolution of backdoor attacks has progressed with the\nadvancement of defense mechanisms against them and more well-developed features\nin the LLMs. In this paper, we adapt the general taxonomy for classifying\nmachine learning attacks on one of the subdivisions - training-time white-box\nbackdoor attacks. Besides systematically classifying attack methods, we also\nconsider the corresponding defense methods against backdoor attacks. By\nproviding an extensive summary of existing works, we hope this survey can serve\nas a guideline for inspiring future research that further extends the attack\nscenarios and creates a stronger defense against them for more robust LLMs."
    },
    {
        "date": "2025-02",
        "title": "Improving Adversarial Robustness via Phase and Amplitude-aware Prompting",
        "author": "Yibo Xu, Dawei Zhou, Decheng Liu, and Nannan Wang",
        "link": "http://arxiv.org/abs/2502.03758v1",
        "abstract": "Deep neural networks are found to be vulnerable to adversarial noises. The\nprompt-based defense has been increasingly studied due to its high efficiency.\nHowever, existing prompt-based defenses mainly exploited mixed prompt patterns,\nwhere critical patterns closely related to object semantics lack sufficient\nfocus. The phase and amplitude spectra have been proven to be highly related to\nspecific semantic patterns and crucial for robustness. To this end, in this\npaper, we propose a Phase and Amplitude-aware Prompting (PAP) defense.\nSpecifically, we construct phase-level and amplitude-level prompts for each\nclass, and adjust weights for prompting according to the model's robust\nperformance under these prompts during training. During testing, we select\nprompts for each image using its predicted label to obtain the prompted image,\nwhich is inputted to the model to get the final prediction. Experimental\nresults demonstrate the effectiveness of our method."
    },
    {
        "date": "2025-02",
        "title": "PRISM: A Robust Framework for Skill-based Meta-Reinforcement Learning with Noisy Demonstrations",
        "author": "Sanghyeon Lee, Sangjun Bae, Yisak Park, and Seungyul Han",
        "link": "http://arxiv.org/abs/2502.03752v1",
        "abstract": "Meta-reinforcement learning (Meta-RL) facilitates rapid adaptation to unseen\ntasks but faces challenges in long-horizon environments. Skill-based approaches\ntackle this by decomposing state-action sequences into reusable skills and\nemploying hierarchical decision-making. However, these methods are highly\nsusceptible to noisy offline demonstrations, resulting in unstable skill\nlearning and degraded performance. To overcome this, we propose Prioritized\nRefinement for Skill-Based Meta-RL (PRISM), a robust framework that integrates\nexploration near noisy data to generate online trajectories and combines them\nwith offline data. Through prioritization, PRISM extracts high-quality data to\nlearn task-relevant skills effectively. By addressing the impact of noise, our\nmethod ensures stable skill learning and achieves superior performance in\nlong-horizon tasks, even with noisy and sub-optimal data."
    },
    {
        "date": "2025-02",
        "title": "Detecting Backdoor Attacks via Similarity in Semantic Communication Systems",
        "author": "Ziyang Wei, Yili Jiang, Jiaqi Huang, Fangtian Zhong, and Sohan Gyawali",
        "link": "http://arxiv.org/abs/2502.03721v1",
        "abstract": "Semantic communication systems, which leverage Generative AI (GAI) to\ntransmit semantic meaning rather than raw data, are poised to revolutionize\nmodern communications. However, they are vulnerable to backdoor attacks, a type\nof poisoning manipulation that embeds malicious triggers into training\ndatasets. As a result, Backdoor attacks mislead the inference for poisoned\nsamples while clean samples remain unaffected. The existing defenses may alter\nthe model structure (such as neuron pruning that potentially degrades inference\nperformance on clean inputs, or impose strict requirements on data formats\n(such as ``Semantic Shield\" that requires image-text pairs). To address these\nlimitations, this work proposes a defense mechanism that leverages semantic\nsimilarity to detect backdoor attacks without modifying the model structure or\nimposing data format constraints. By analyzing deviations in semantic feature\nspace and establishing a threshold-based detection framework, the proposed\napproach effectively identifies poisoned samples. The experimental results\ndemonstrate high detection accuracy and recall across varying poisoning ratios,\nunderlining the significant effectiveness of our proposed solution."
    },
    {
        "date": "2025-02",
        "title": "Following Devils' Footprint: Towards Real-time Detection of Price Manipulation Attacks",
        "author": "Bosi Zhang, Ningyu He, Xiaohui Hu, Kai Ma, and Haoyu Wang",
        "link": "http://arxiv.org/abs/2502.03718v1",
        "abstract": "Price manipulation attack is one of the notorious threats in decentralized\nfinance (DeFi) applications, which allows attackers to exchange tokens at an\nextensively deviated price from the market. Existing efforts usually rely on\nreactive methods to identify such kind of attacks after they have happened,\ne.g., detecting attack transactions in the post-attack stage, which cannot\nmitigate or prevent price manipulation attacks timely. From the perspective of\nattackers, they usually need to deploy attack contracts in the pre-attack\nstage. Thus, if we can identify these attack contracts in a proactive manner,\nwe can raise alarms and mitigate the threats. With the core idea in mind, in\nthis work, we shift our attention from the victims to the attackers.\nSpecifically, we propose SMARTCAT, a novel approach for identifying price\nmanipulation attacks in the pre-attack stage proactively. For generality, it\nconducts analysis on bytecode and does not require any source code and\ntransaction data. For accuracy, it depicts the control- and data-flow\ndependency relationships among function calls into a token flow graph. For\nscalability, it filters out those suspicious paths, in which it conducts\ninter-contract analysis as necessary. To this end, SMARTCAT can pinpoint\nattacks in real time once they have been deployed on a chain. The evaluation\nresults illustrate that SMARTCAT significantly outperforms existing baselines\nwith 91.6% recall and ~100% precision. Moreover, SMARTCAT also uncovers 616\nattack contracts in-the-wild, accounting for \\$9.25M financial losses, with\nonly 19 cases publicly reported. By applying SMARTCAT as a real-time detector\nin Ethereum and Binance Smart Chain, it has raised 14 alarms 99 seconds after\nthe corresponding deployment on average. These attacks have already led to\n$641K financial losses, and seven of them are still waiting for their ripe\ntime."
    },
    {
        "date": "2025-02",
        "title": "MultiQ&A: An Analysis in Measuring Robustness via Automated Crowdsourcing of Question Perturbations and Answers",
        "author": "Nicole Cho, and William Watson",
        "link": "http://arxiv.org/abs/2502.03711v1",
        "abstract": "One critical challenge in the institutional adoption journey of Large\nLanguage Models (LLMs) stems from their propensity to hallucinate in generated\nresponses. To address this, we propose MultiQ&A, a systematic approach for\nevaluating the robustness and consistency of LLM-generated answers. We\ndemonstrate MultiQ&A's ability to crowdsource question perturbations and their\nrespective answers through independent LLM agents at scale. Our experiments\nculminated in the examination of 1.9 million question perturbations and 2.3\nmillion answers. Furthermore, MultiQ&A shows that ensembled LLMs, such as\ngpt-3.5-turbo, remain relatively robust and consistent under perturbations.\nMultiQ&A provides clarity in the response generation space, offering an\neffective method for inspecting disagreements and variability. Therefore, our\nsystem offers a potential framework for institutional LLM adoption with the\nability to measure confidence, consistency, and the quantification of\nhallucinations."
    },
    {
        "date": "2025-02",
        "title": "How vulnerable is my policy? Adversarial attacks on modern behavior cloning policies",
        "author": "Basavasagar Patil, Akansha Kalra, Guanhong Tao, and Daniel S. Brown",
        "link": "http://arxiv.org/abs/2502.03698v1",
        "abstract": "Learning from Demonstration (LfD) algorithms have shown promising results in\nrobotic manipulation tasks, but their vulnerability to adversarial attacks\nremains underexplored. This paper presents a comprehensive study of adversarial\nattacks on both classic and recently proposed algorithms, including Behavior\nCloning (BC), LSTM-GMM, Implicit Behavior Cloning (IBC), Diffusion Policy (DP),\nand VQ-Behavior Transformer (VQ-BET). We study the vulnerability of these\nmethods to untargeted, targeted and universal adversarial perturbations. While\nexplicit policies, such as BC, LSTM-GMM and VQ-BET can be attacked in the same\nmanner as standard computer vision models, we find that attacks for implicit\nand denoising policy models are nuanced and require developing novel attack\nmethods. Our experiments on several simulated robotic manipulation tasks reveal\nthat most of the current methods are highly vulnerable to adversarial\nperturbations. We also show that these attacks are transferable across\nalgorithms, architectures, and tasks, raising concerning security\nvulnerabilities with potentially a white-box threat model. In addition, we test\nthe efficacy of a randomized smoothing, a widely used adversarial defense\ntechnique, and highlight its limitation in defending against attacks on complex\nand multi-modal action distribution common in complex control tasks. In\nsummary, our findings highlight the vulnerabilities of modern BC algorithms,\npaving way for future work in addressing such limitations."
    },
    {
        "date": "2025-02",
        "title": "DocMIA: Document-Level Membership Inference Attacks against DocVQA Models",
        "author": "Khanh Nguyen, Raouf Kerkouche, Mario Fritz, and Dimosthenis Karatzas",
        "link": "http://arxiv.org/abs/2502.03692v1",
        "abstract": "Document Visual Question Answering (DocVQA) has introduced a new paradigm for\nend-to-end document understanding, and quickly became one of the standard\nbenchmarks for multimodal LLMs. Automating document processing workflows,\ndriven by DocVQA models, presents significant potential for many business\nsectors. However, documents tend to contain highly sensitive information,\nraising concerns about privacy risks associated with training such DocVQA\nmodels. One significant privacy vulnerability, exploited by the membership\ninference attack, is the possibility for an adversary to determine if a\nparticular record was part of the model's training data. In this paper, we\nintroduce two novel membership inference attacks tailored specifically to\nDocVQA models. These attacks are designed for two different adversarial\nscenarios: a white-box setting, where the attacker has full access to the model\narchitecture and parameters, and a black-box setting, where only the model's\noutputs are available. Notably, our attacks assume the adversary lacks access\nto auxiliary datasets, which is more realistic in practice but also more\nchallenging. Our unsupervised methods outperform existing state-of-the-art\nmembership inference attacks across a variety of DocVQA models and datasets,\ndemonstrating their effectiveness and highlighting the privacy risks in this\ndomain."
    },
    {
        "date": "2025-02",
        "title": "Towards Fair and Robust Face Parsing for Generative AI: A Multi-Objective Approach",
        "author": "Sophia J. Abraham, Jonathan D. Hauenstein, and Walter J. Scheirer",
        "link": "http://arxiv.org/abs/2502.04391v1",
        "abstract": "Face parsing is a fundamental task in computer vision, enabling applications\nsuch as identity verification, facial editing, and controllable image\nsynthesis. However, existing face parsing models often lack fairness and\nrobustness, leading to biased segmentation across demographic groups and errors\nunder occlusions, noise, and domain shifts. These limitations affect downstream\nface synthesis, where segmentation biases can degrade generative model outputs.\nWe propose a multi-objective learning framework that optimizes accuracy,\nfairness, and robustness in face parsing. Our approach introduces a\nhomotopy-based loss function that dynamically adjusts the importance of these\nobjectives during training. To evaluate its impact, we compare multi-objective\nand single-objective U-Net models in a GAN-based face synthesis pipeline\n(Pix2PixHD). Our results show that fairness-aware and robust segmentation\nimproves photorealism and consistency in face generation. Additionally, we\nconduct preliminary experiments using ControlNet, a structured conditioning\nmodel for diffusion-based synthesis, to explore how segmentation quality\ninfluences guided image generation. Our findings demonstrate that\nmulti-objective face parsing improves demographic consistency and robustness,\nleading to higher-quality GAN-based synthesis."
    },
    {
        "date": "2025-02",
        "title": "Towards Scalable Defenses against Intimate Partner Infiltrations",
        "author": "Weisi Yang, Shinan Liu, Feng Xiao, Nick Feamster, and Stephen Xia",
        "link": "http://arxiv.org/abs/2502.03682v1",
        "abstract": "Intimate Partner Infiltration (IPI)--a type of Intimate Partner Violence\n(IPV) that typically requires physical access to a victim's device--is a\npervasive concern in the United States, often manifesting through digital\nsurveillance, control, and monitoring. Unlike conventional cyberattacks, IPI\nperpetrators leverage close proximity and personal knowledge to circumvent\nstandard protections, underscoring the need for targeted interventions. While\nsecurity clinics and other human-centered approaches effectively tailor\nsolutions for survivors, their scalability remains constrained by resource\nlimitations and the need for specialized counseling. In this paper, we present\nAID, an Automated IPI Detection system that continuously monitors for\nunauthorized access and suspicious behaviors on smartphones. AID employs a\ntwo-stage architecture to process multimodal signals stealthily and preserve\nuser privacy. A brief calibration phase upon installation enables AID to adapt\nto each user's behavioral patterns, achieving high accuracy with minimal false\nalarms. Our 27-participant user study demonstrates that AID achieves highly\naccurate detection of non-owner access and fine-grained IPI-related activities,\nattaining an end-to-end top-3 F1 score of 0.981 with a false positive rate of\n4%. These findings suggest that AID can serve as a forensic tool within\nsecurity clinics, scaling their ability to identify IPI tactics and deliver\npersonalized, far-reaching support to survivors."
    },
    {
        "date": "2025-02",
        "title": "KDA: A Knowledge-Distilled Attacker for Generating Diverse Prompts to Jailbreak LLMs",
        "author": "Buyun Liang, Kwan Ho Ryan Chan, Darshan Thaker, Jinqi Luo, and Ren\u00e9 Vidal",
        "link": "http://arxiv.org/abs/2502.05223v1",
        "abstract": "Jailbreak attacks exploit specific prompts to bypass LLM safeguards, causing\nthe LLM to generate harmful, inappropriate, and misaligned content. Current\njailbreaking methods rely heavily on carefully designed system prompts and\nnumerous queries to achieve a single successful attack, which is costly and\nimpractical for large-scale red-teaming. To address this challenge, we propose\nto distill the knowledge of an ensemble of SOTA attackers into a single\nopen-source model, called Knowledge-Distilled Attacker (KDA), which is\nfinetuned to automatically generate coherent and diverse attack prompts without\nthe need for meticulous system prompt engineering. Compared to existing\nattackers, KDA achieves higher attack success rates and greater cost-time\nefficiency when targeting multiple SOTA open-source and commercial black-box\nLLMs. Furthermore, we conducted a quantitative diversity analysis of prompts\ngenerated by baseline methods and KDA, identifying diverse and ensemble attacks\nas key factors behind KDA's effectiveness and efficiency."
    },
    {
        "date": "2025-02",
        "title": "AdaPhish: AI-Powered Adaptive Defense and Education Resource Against Deceptive Emails",
        "author": "Rei Meguro, and Ng S. T. Chong",
        "link": "http://arxiv.org/abs/2502.03622v2",
        "abstract": "Phishing attacks remain a significant threat in the digital age, yet\norganizations lack effective methods to tackle phishing attacks without leaking\nsensitive information. Phish bowl initiatives are a vital part of cybersecurity\nefforts against these attacks. However, traditional phish bowls require manual\nanonymization and are often limited to internal use. To overcome these\nlimitations, we introduce AdaPhish, an AI-powered phish bowl platform that\nautomatically anonymizes and analyzes phishing emails using large language\nmodels (LLMs) and vector databases. AdaPhish achieves real-time detection and\nadaptation to new phishing tactics while enabling long-term tracking of\nphishing trends. Through automated reporting, adaptive analysis, and real-time\nalerts, AdaPhish presents a scalable, collaborative solution for phishing\ndetection and cybersecurity education."
    },
    {
        "date": "2025-02",
        "title": "Swarm Characteristic Classification using Robust Neural Networks with Optimized Controllable Inputs",
        "author": "Donald W. Peltier III, Isaac Kaminer, Abram Clark, and Marko Orescanin",
        "link": "http://arxiv.org/abs/2502.03619v1",
        "abstract": "Having the ability to infer characteristics of autonomous agents would\nprofoundly revolutionize defense, security, and civil applications. Our\nprevious work was the first to demonstrate that supervised neural network time\nseries classification (NN TSC) could rapidly predict the tactics of swarming\nautonomous agents in military contexts, providing intelligence to inform\ncounter-maneuvers. However, most autonomous interactions, especially military\nengagements, are fraught with uncertainty, raising questions about the\npracticality of using a pretrained classifier. This article addresses that\nchallenge by leveraging expected operational variations to construct a richer\ndataset, resulting in a more robust NN with improved inference performance in\nscenarios characterized by significant uncertainties. Specifically, diverse\ndatasets are created by simulating variations in defender numbers, defender\nmotions, and measurement noise levels. Key findings indicate that robust NNs\ntrained on an enriched dataset exhibit enhanced classification accuracy and\noffer operational flexibility, such as reducing resources required and offering\nadherence to trajectory constraints. Furthermore, we present a new framework\nfor optimally deploying a trained NN by the defenders. The framework involves\noptimizing defender trajectories that elicit adversary responses that maximize\nthe probability of correct NN tactic classification while also satisfying\noperational constraints imposed on the defenders."
    },
    {
        "date": "2025-02",
        "title": "A Novel Zero-Touch, Zero-Trust, AI/ML Enablement Framework for IoT Network Security",
        "author": "Sushil Shakya, Robert Abbas, and Sasa Maric",
        "link": "http://arxiv.org/abs/2502.03614v1",
        "abstract": "The IoT facilitates a connected, intelligent, and sustainable society;\ntherefore, it is imperative to protect the IoT ecosystem. The IoT-based 5G and\n6G will leverage the use of machine learning and artificial intelligence\n(ML/AI) more to pave the way for autonomous and collaborative secure IoT\nnetworks. Zero-touch, zero-trust IoT security with AI and machine learning (ML)\nenablement frameworks offers a powerful approach to securing the expanding\nlandscape of Internet of Things (IoT) devices. This paper presents a novel\nframework based on the integration of Zero Trust, Zero Touch, and AI/ML powered\nfor the detection, mitigation, and prevention of DDoS attacks in modern IoT\necosystems. The focus will be on the new integrated framework by establishing\nzero trust for all IoT traffic, fixed and mobile 5G/6G IoT network traffic, and\ndata security (quarantine-zero touch and dynamic policy enforcement). We\nperform a comparative analysis of five machine learning models, namely,\nXGBoost, Random Forest, K-Nearest Neighbors, Stochastic Gradient Descent, and\nNative Bayes, by comparing these models based on accuracy, precision, recall,\nF1-score, and ROC-AUC. Results show that the best performance in detecting and\nmitigating different DDoS vectors comes from the ensemble-based approaches."
    },
    {
        "date": "2025-02",
        "title": "Optimizing Robustness and Accuracy in Mixture of Experts: A Dual-Model Approach",
        "author": "Xu Zhang, Kaidi Xu, Ziqing Hu, and Ren Wang",
        "link": "http://arxiv.org/abs/2502.06832v1",
        "abstract": "Mixture of Experts (MoE) have shown remarkable success in leveraging\nspecialized expert networks for complex machine learning tasks. However, their\nsusceptibility to adversarial attacks presents a critical challenge for\ndeployment in robust applications. This paper addresses the critical question\nof how to incorporate robustness into MoEs while maintaining high natural\naccuracy. We begin by analyzing the vulnerability of MoE components, finding\nthat expert networks are notably more susceptible to adversarial attacks than\nthe router. Based on this insight, we propose a targeted robust training\ntechnique that integrates a novel loss function to enhance the adversarial\nrobustness of MoE, requiring only the robustification of one additional expert\nwithout compromising training or inference efficiency. Building on this, we\nintroduce a dual-model strategy that linearly combines a standard MoE model\nwith our robustified MoE model using a smoothing parameter. This approach\nallows for flexible control over the robustness-accuracy trade-off. We further\nprovide theoretical foundations by deriving certified robustness bounds for\nboth the single MoE and the dual-model. To push the boundaries of robustness\nand accuracy, we propose a novel joint training strategy JTDMoE for the\ndual-model. This joint training enhances both robustness and accuracy beyond\nwhat is achievable with separate models. Experimental results on CIFAR-10 and\nTinyImageNet datasets using ResNet18 and Vision Transformer (ViT) architectures\ndemonstrate the effectiveness of our proposed methods."
    },
    {
        "date": "2025-02",
        "title": "Towards Fair Medical AI: Adversarial Debiasing of 3D CT Foundation Embeddings",
        "author": "Guangyao Zheng, Michael A. Jacobs, Vladimir Braverman, and Vishwa S. Parekh",
        "link": "http://arxiv.org/abs/2502.04386v1",
        "abstract": "Self-supervised learning has revolutionized medical imaging by enabling\nefficient and generalizable feature extraction from large-scale unlabeled\ndatasets. Recently, self-supervised foundation models have been extended to\nthree-dimensional (3D) computed tomography (CT) data, generating compact,\ninformation-rich embeddings with 1408 features that achieve state-of-the-art\nperformance on downstream tasks such as intracranial hemorrhage detection and\nlung cancer risk forecasting. However, these embeddings have been shown to\nencode demographic information, such as age, sex, and race, which poses a\nsignificant risk to the fairness of clinical applications.\n  In this work, we propose a Variation Autoencoder (VAE) based adversarial\ndebiasing framework to transform these embeddings into a new latent space where\ndemographic information is no longer encoded, while maintaining the performance\nof critical downstream tasks. We validated our approach on the NLST lung cancer\nscreening dataset, demonstrating that the debiased embeddings effectively\neliminate multiple encoded demographic information and improve fairness without\ncompromising predictive accuracy for lung cancer risk at 1-year and 2-year\nintervals. Additionally, our approach ensures the embeddings are robust against\nadversarial bias attacks. These results highlight the potential of adversarial\ndebiasing techniques to ensure fairness and equity in clinical applications of\nself-supervised 3D CT embeddings, paving the way for their broader adoption in\nunbiased medical decision-making."
    },
    {
        "date": "2025-02",
        "title": "A Hybrid Blockchain-IPFS Solution for Secure and Scalable Data Collection and Storage for Smart Water Meters",
        "author": "Thandile Nododile, and Clement Nyirenda",
        "link": "http://arxiv.org/abs/2502.03427v1",
        "abstract": "Scalable and secure data management is important in Internet of Things (IoT)\napplications such as smart water meters, where traditional blockchain storage\ncan be restrictive due to high data volumes. This paper investigates a hybrid\nblockchain and InterPlanetary File System (IPFS) approach designed to optimise\nstorage efficiency, enhance throughput, and reduce block time by offloading\nlarge data off-chain to IPFS while preserving on-chain integrity. A\nsubstrate-based private blockchain was developed to store smart water meter\n(SWM) data, and controlled experiments were conducted to evaluate blockchain\nperformance with and without IPFS. Key metrics, including block size, block\ntime, and transaction throughput, were analysed across varying data volumes and\nnode counts. Results show that integrating IPFS significantly reduces on-chain\nstorage demands, leading to smaller block sizes, increased throughput, and\nimproved block times compared to blockchain-only storage. These findings\nhighlight the potential of hybrid blockchain-IPFS models for efficiently and\nsecurely managing high-volume IoT data."
    },
    {
        "date": "2025-02",
        "title": "The Adoption of Artificial Intelligence in Different Network Security Concepts",
        "author": "Mamoon A. Al Jbaar, Adel Jalal Yousif, and Qutaiba I. Ali",
        "link": "http://arxiv.org/abs/2502.03398v1",
        "abstract": "The obstacles of each security system combined with the increase of\ncyber-attacks, negatively affect the effectiveness of network security\nmanagement and rise the activities to be taken by the security staff and\nnetwork administrators. So, there is a growing need for the automated auditing\nand intelligent reporting strategies for reliable network security with as less\nmodel complexity as possible. Newly, artificial intelligence has been\neffectively applied to various network security issues, and numerous studies\nhave been conducted that utilize various artificial intelligence techniques for\nthe purposes of encryption and secure communication, in addition to using\nartificial intelligence to perform a large number of data encryption operations\nin record time. The aim of the study is to present and discuss the most\nprominent methods of artificial intelligence recently used in the field of\nnetwork security including user authentication, Key exchanging,\nencryption/decryption, data integrity and intrusion detection system."
    },
    {
        "date": "2025-02",
        "title": "Robust Autonomy Emerges from Self-Play",
        "author": "Marco Cusumano-Towner, David Hafner, Alex Hertzberg, Brody Huval, Aleksei Petrenko, Eugene Vinitsky, Erik Wijmans, Taylor Killian, Stuart Bowers, Ozan Sener, Philipp Kr\u00e4henb\u00fchl, and Vladlen Koltun",
        "link": "http://arxiv.org/abs/2502.03349v1",
        "abstract": "Self-play has powered breakthroughs in two-player and multi-player games.\nHere we show that self-play is a surprisingly effective strategy in another\ndomain. We show that robust and naturalistic driving emerges entirely from\nself-play in simulation at unprecedented scale -- 1.6~billion~km of driving.\nThis is enabled by Gigaflow, a batched simulator that can synthesize and train\non 42 years of subjective driving experience per hour on a single 8-GPU node.\nThe resulting policy achieves state-of-the-art performance on three independent\nautonomous driving benchmarks. The policy outperforms the prior state of the\nart when tested on recorded real-world scenarios, amidst human drivers, without\never seeing human data during training. The policy is realistic when assessed\nagainst human references and achieves unprecedented robustness, averaging 17.5\nyears of continuous driving between incidents in simulation."
    },
    {
        "date": "2025-02",
        "title": "Aero-LLM: A Distributed Framework for Secure UAV Communication and Intelligent Decision-Making",
        "author": "Balakrishnan Dharmalingam, Rajdeep Mukherjee, Brett Piggott, Guohuan Feng, and Anyi Liu",
        "link": "http://arxiv.org/abs/2502.05220v1",
        "abstract": "Increased utilization of unmanned aerial vehicles (UAVs) in critical\noperations necessitates secure and reliable communication with Ground Control\nStations (GCS). This paper introduces Aero-LLM, a framework integrating\nmultiple Large Language Models (LLMs) to enhance UAV mission security and\noperational efficiency. Unlike conventional singular LLMs, Aero-LLM leverages\nmultiple specialized LLMs for various tasks, such as inferencing, anomaly\ndetection, and forecasting, deployed across onboard systems, edge, and cloud\nservers. This dynamic, distributed architecture reduces performance bottleneck\nand increases security capabilities. Aero-LLM's evaluation demonstrates\noutstanding task-specific metrics and robust defense against cyber threats,\nsignificantly enhancing UAV decision-making and operational capabilities and\nsecurity resilience against cyber attacks, setting a new standard for secure,\nintelligent UAV operations."
    },
    {
        "date": "2025-02",
        "title": "Exploring the Security Threats of Knowledge Base Poisoning in Retrieval-Augmented Code Generation",
        "author": "Bo Lin, Shangwen Wang, Liqian Chen, and Xiaoguang Mao",
        "link": "http://arxiv.org/abs/2502.03233v1",
        "abstract": "The integration of Large Language Models (LLMs) into software development has\nrevolutionized the field, particularly through the use of Retrieval-Augmented\nCode Generation (RACG) systems that enhance code generation with information\nfrom external knowledge bases. However, the security implications of RACG\nsystems, particularly the risks posed by vulnerable code examples in the\nknowledge base, remain largely unexplored. This risk is particularly concerning\ngiven that public code repositories, which often serve as the sources for\nknowledge base collection in RACG systems, are usually accessible to anyone in\nthe community. Malicious attackers can exploit this accessibility to inject\nvulnerable code into the knowledge base, making it toxic. Once these poisoned\nsamples are retrieved and incorporated into the generated code, they can\npropagate security vulnerabilities into the final product. This paper presents\nthe first comprehensive study on the security risks associated with RACG\nsystems, focusing on how vulnerable code in the knowledge base compromises the\nsecurity of generated code. We investigate the LLM-generated code security\nacross different settings through extensive experiments using four major LLMs,\ntwo retrievers, and two poisoning scenarios. Our findings highlight the\nsignificant threat of knowledge base poisoning, where even a single poisoned\ncode example can compromise up to 48% of generated code. Our findings provide\ncrucial insights into vulnerability introduction in RACG systems and offer\npractical mitigation recommendations, thereby helping improve the security of\nLLM-generated code in future works."
    },
    {
        "date": "2025-02",
        "title": "Adversarial Dependence Minimization",
        "author": "Pierre-Fran\u00e7ois De Plaen, Tinne Tuytelaars, Marc Proesmans, and Luc Van Gool",
        "link": "http://arxiv.org/abs/2502.03227v1",
        "abstract": "Many machine learning techniques rely on minimizing the covariance between\noutput feature dimensions to extract minimally redundant representations from\ndata. However, these methods do not eliminate all dependencies/redundancies, as\nlinearly uncorrelated variables can still exhibit nonlinear relationships. This\nwork provides a differentiable and scalable algorithm for dependence\nminimization that goes beyond linear pairwise decorrelation. Our method employs\nan adversarial game where small networks identify dependencies among feature\ndimensions, while the encoder exploits this information to reduce dependencies.\nWe provide empirical evidence of the algorithm's convergence and demonstrate\nits utility in three applications: extending PCA to nonlinear decorrelation,\nimproving the generalization of image classification methods, and preventing\ndimensional collapse in self-supervised representation learning."
    },
    {
        "date": "2025-02",
        "title": "Secure Resource Management in Cloud Computing: Challenges, Strategies and Meta-Analysis",
        "author": "Deepika Saxena, Smruti Rekha Swain, Jatinder Kumar, Sakshi Patni, Kishu Gupta, Ashutosh Kumar Singh, and Volker Lindenstruth",
        "link": "http://arxiv.org/abs/2502.03149v1",
        "abstract": "Secure resource management (SRM) within a cloud computing environment is a\ncritical yet infrequently studied research topic. This paper provides a\ncomprehensive survey and comparative performance evaluation of potential cyber\nthreat countermeasure strategies that address security challenges during cloud\nworkload execution and resource management. Cybersecurity is explored\nspecifically in the context of cloud resource management, with an emphasis on\nidentifying the associated challenges. The cyber threat countermeasure methods\nare categorized into three classes: defensive strategies, mitigating\nstrategies, and hybrid strategies. The existing countermeasure strategies\nbelonging to each class are thoroughly discussed and compared. In addition to\nconceptual and theoretical analysis, the leading countermeasure strategies\nwithin these categories are implemented on a common platform and examined using\ntwo real-world virtual machine (VM) data traces. Based on this comprehensive\nstudy and performance evaluation, the paper discusses the trade-offs among\nthese countermeasure strategies and their utility, providing imperative\nconcluding remarks on the holistic study of cloud cyber threat countermeasures\nand secure resource management. Furthermore, the study suggests future\nmethodologies that could effectively address the emerging challenges of secure\ncloud resource management."
    },
    {
        "date": "2025-02",
        "title": "Gotham Dataset 2025: A Reproducible Large-Scale IoT Network Dataset for Intrusion Detection and Security Research",
        "author": "Othmane Belarbi, Theodoros Spyridopoulos, Eirini Anthi, Omer Rana, Pietro Carnelli, and Aftab Khan",
        "link": "http://arxiv.org/abs/2502.03134v1",
        "abstract": "In this paper, a dataset of IoT network traffic is presented. Our dataset was\ngenerated by utilising the Gotham testbed, an emulated large-scale Internet of\nThings (IoT) network designed to provide a realistic and heterogeneous\nenvironment for network security research. The testbed includes 78 emulated IoT\ndevices operating on various protocols, including MQTT, CoAP, and RTSP. Network\ntraffic was captured in Packet Capture (PCAP) format using tcpdump, and both\nbenign and malicious traffic were recorded. Malicious traffic was generated\nthrough scripted attacks, covering a variety of attack types, such as Denial of\nService (DoS), Telnet Brute Force, Network Scanning, CoAP Amplification, and\nvarious stages of Command and Control (C&C) communication. The data were\nsubsequently processed in Python for feature extraction using the Tshark tool,\nand the resulting data was converted to Comma Separated Values (CSV) format and\nlabelled. The data repository includes the raw network traffic in PCAP format\nand the processed labelled data in CSV format. Our dataset was collected in a\ndistributed manner, where network traffic was captured separately for each IoT\ndevice at the interface between the IoT gateway and the device. Our dataset was\ncollected in a distributed manner, where network traffic was separately\ncaptured for each IoT device at the interface between the IoT gateway and the\ndevice. With its diverse traffic patterns and attack scenarios, this dataset\nprovides a valuable resource for developing Intrusion Detection Systems and\nsecurity mechanisms tailored to complex, large-scale IoT environments. The\ndataset is publicly available at Zenodo."
    },
    {
        "date": "2025-02",
        "title": "RLOMM: An Efficient and Robust Online Map Matching Framework with Reinforcement Learning",
        "author": "Minxiao Chen, Haitao Yuan, Nan Jiang, Zhihan Zheng, Sai Wu, Ao Zhou, and Shangguang Wang",
        "link": "http://arxiv.org/abs/2502.06825v1",
        "abstract": "Online map matching is a fundamental problem in location-based services,\naiming to incrementally match trajectory data step-by-step onto a road network.\nHowever, existing methods fail to meet the needs for efficiency, robustness,\nand accuracy required by large-scale online applications, making this task\nstill a challenging problem. This paper introduces a novel framework that\nachieves high accuracy and efficient matching while ensuring robustness in\nhandling diverse scenarios. To improve efficiency, we begin by modeling the\nonline map matching problem as an Online Markov Decision Process (OMDP) based\non its inherent characteristics. This approach helps efficiently merge\nhistorical and real-time data, reducing unnecessary calculations. Next, to\nenhance the model's robustness, we design a reinforcement learning method,\nenabling robust handling of real-time data from dynamically changing\nenvironments. In particular, we propose a novel model learning process and a\ncomprehensive reward function, allowing the model to make reasonable current\nmatches from a future-oriented perspective, and to continuously update and\noptimize during the decision-making process based on feedback. Lastly, to\naddress the heterogeneity between trajectories and roads, we design distinct\ngraph structures, facilitating efficient representation learning through graph\nand recurrent neural networks. To further align trajectory and road data, we\nintroduce contrastive learning to decrease their distance in the latent space,\nthereby promoting effective integration of the two. Extensive evaluations on\nthree real-world datasets confirm that our method significantly outperforms\nexisting state-of-the-art solutions in terms of accuracy, efficiency and\nrobustness."
    },
    {
        "date": "2025-02",
        "title": "RoboGrasp: A Universal Grasping Policy for Robust Robotic Control",
        "author": "Yiqi Huang, Travis Davies, Jiahuan Yan, Xiang Chen, Yu Tian, and Luhui Hu",
        "link": "http://arxiv.org/abs/2502.03072v1",
        "abstract": "Imitation learning and world models have shown significant promise in\nadvancing generalizable robotic learning, with robotic grasping remaining a\ncritical challenge for achieving precise manipulation. Existing methods often\nrely heavily on robot arm state data and RGB images, leading to overfitting to\nspecific object shapes or positions. To address these limitations, we propose\nRoboGrasp, a universal grasping policy framework that integrates pretrained\ngrasp detection models with robotic learning. By leveraging robust visual\nguidance from object detection and segmentation tasks, RoboGrasp significantly\nenhances grasp precision, stability, and generalizability, achieving up to 34%\nhigher success rates in few-shot learning and grasping box prompt tasks. Built\non diffusion-based methods, RoboGrasp is adaptable to various robotic learning\nparadigms, enabling precise and reliable manipulation across diverse and\ncomplex scenarios. This framework represents a scalable and versatile solution\nfor tackling real-world challenges in robotic grasping."
    },
    {
        "date": "2025-02",
        "title": "Understanding and Enhancing the Transferability of Jailbreaking Attacks",
        "author": "Runqi Lin, Bo Han, Fengwang Li, and Tongling Liu",
        "link": "http://arxiv.org/abs/2502.03052v1",
        "abstract": "Jailbreaking attacks can effectively manipulate open-source large language\nmodels (LLMs) to produce harmful responses. However, these attacks exhibit\nlimited transferability, failing to disrupt proprietary LLMs consistently. To\nreliably identify vulnerabilities in proprietary LLMs, this work investigates\nthe transferability of jailbreaking attacks by analysing their impact on the\nmodel's intent perception. By incorporating adversarial sequences, these\nattacks can redirect the source LLM's focus away from malicious-intent tokens\nin the original input, thereby obstructing the model's intent recognition and\neliciting harmful responses. Nevertheless, these adversarial sequences fail to\nmislead the target LLM's intent perception, allowing the target LLM to refocus\non malicious-intent tokens and abstain from responding. Our analysis further\nreveals the inherent distributional dependency within the generated adversarial\nsequences, whose effectiveness stems from overfitting the source LLM's\nparameters, resulting in limited transferability to target LLMs. To this end,\nwe propose the Perceived-importance Flatten (PiF) method, which uniformly\ndisperses the model's focus across neutral-intent tokens in the original input,\nthus obscuring malicious-intent tokens without relying on overfitted\nadversarial sequences. Extensive experiments demonstrate that PiF provides an\neffective and efficient red-teaming evaluation for proprietary LLMs."
    },
    {
        "date": "2025-02",
        "title": "Membership Inference Attack Should Move On to Distributional Statistics for Distilled Generative Models",
        "author": "Muxing Li, Zesheng Ye, Yixuan Li, Andy Song, Guangquan Zhang, and Feng Liu",
        "link": "http://arxiv.org/abs/2502.02970v1",
        "abstract": "Membership inference attacks (MIAs) determine whether certain data instances\nwere used to train a model by exploiting the differences in how the model\nresponds to seen versus unseen instances. This capability makes MIAs important\nin assessing privacy leakage within modern generative AI systems. However, this\npaper reveals an oversight in existing MIAs against \\emph{distilled generative\nmodels}: attackers can no longer detect a teacher model's training instances\nindividually when targeting the distilled student model, as the student learns\nfrom the teacher-generated data rather than its original member data,\npreventing direct instance-level memorization. Nevertheless, we find that\nstudent-generated samples exhibit a significantly stronger distributional\nalignment with teacher's member data than non-member data. This leads us to\nposit that MIAs \\emph{on distilled generative models should shift from\ninstance-level to distribution-level statistics}. We thereby introduce a\n\\emph{set-based} MIA framework that measures \\emph{relative} distributional\ndiscrepancies between student-generated data\\emph{sets} and potential\nmember/non-member data\\emph{sets}, Empirically, distributional statistics\nreliably distinguish a teacher's member data from non-member data through the\ndistilled model. Finally, we discuss scenarios in which our setup faces\nlimitations."
    },
    {
        "date": "2025-02",
        "title": "Large Language Model Adversarial Landscape Through the Lens of Attack Objectives",
        "author": "Nan Wang, Kane Walter, Yansong Gao, and Alsharif Abuadbba",
        "link": "http://arxiv.org/abs/2502.02960v1",
        "abstract": "Large Language Models (LLMs) represent a transformative leap in artificial\nintelligence, enabling the comprehension, generation, and nuanced interaction\nwith human language on an unparalleled scale. However, LLMs are increasingly\nvulnerable to a range of adversarial attacks that threaten their privacy,\nreliability, security, and trustworthiness. These attacks can distort outputs,\ninject biases, leak sensitive information, or disrupt the normal functioning of\nLLMs, posing significant challenges across various applications.\n  In this paper, we provide a novel comprehensive analysis of the adversarial\nlandscape of LLMs, framed through the lens of attack objectives. By\nconcentrating on the core goals of adversarial actors, we offer a fresh\nperspective that examines threats from the angles of privacy, integrity,\navailability, and misuse, moving beyond conventional taxonomies that focus\nsolely on attack techniques. This objective-driven adversarial landscape not\nonly highlights the strategic intent behind different adversarial approaches\nbut also sheds light on the evolving nature of these threats and the\neffectiveness of current defenses. Our analysis aims to guide researchers and\npractitioners in better understanding, anticipating, and mitigating these\nattacks, ultimately contributing to the development of more resilient and\nrobust LLM systems."
    },
    {
        "date": "2025-02",
        "title": "Robust Reward Alignment via Hypothesis Space Batch Cutting",
        "author": "Zhixian Xie, Haode Zhang, Yizhe Feng, and Wanxin Jin",
        "link": "http://arxiv.org/abs/2502.02921v2",
        "abstract": "Reward design for reinforcement learning and optimal control agents is\nchallenging. Preference-based alignment addresses this by enabling agents to\nlearn rewards from ranked trajectory pairs provided by humans. However,\nexisting methods often struggle from poor robustness to unknown false human\npreferences. In this work, we propose a robust and efficient reward alignment\nmethod based on a novel and geometrically interpretable perspective: hypothesis\nspace batched cutting. Our method iteratively refines the reward hypothesis\nspace through \"cuts\" based on batches of human preferences. Within each batch,\nhuman preferences, queried based on disagreement, are grouped using a voting\nfunction to determine the appropriate cut, ensuring a bounded human query\ncomplexity. To handle unknown erroneous preferences, we introduce a\nconservative cutting method within each batch, preventing erroneous human\npreferences from making overly aggressive cuts to the hypothesis space. This\nguarantees provable robustness against false preferences. We evaluate our\nmethod in a model predictive control setting across diverse tasks, including\nDM-Control, dexterous in-hand manipulation, and locomotion. The results\ndemonstrate that our framework achieves comparable or superior performance to\nstate-of-the-art methods in error-free settings while significantly\noutperforming existing method when handling high percentage of erroneous human\npreferences."
    },
    {
        "date": "2025-02",
        "title": "SPARC: Subspace-Aware Prompt Adaptation for Robust Continual Learning in LLMs",
        "author": "Dinithi Jayasuriya, Sina Tayebati, Davide Ettori, Ranganath Krishnan, and Amit Ranjan Trivedi",
        "link": "http://arxiv.org/abs/2502.02909v1",
        "abstract": "We propose SPARC, a lightweight continual learning framework for large\nlanguage models (LLMs) that enables efficient task adaptation through prompt\ntuning in a lower-dimensional space. By leveraging principal component analysis\n(PCA), we identify a compact subspace of the training data. Optimizing prompts\nin this lower-dimensional space enhances training efficiency, as it focuses\nupdates on the most relevant features while reducing computational overhead.\nFurthermore, since the model's internal structure remains unaltered, the\nextensive knowledge gained from pretraining is fully preserved, ensuring that\npreviously learned information is not compromised during adaptation. Our method\nachieves high knowledge retention in both task-incremental and\ndomain-incremental continual learning setups while fine-tuning only 0.04% of\nthe model's parameters. Additionally, by integrating LoRA, we enhance\nadaptability to computational constraints, allowing for a tradeoff between\naccuracy and training cost. Experiments on the SuperGLUE benchmark demonstrate\nthat our PCA-based prompt tuning combined with LoRA maintains full knowledge\nretention while improving accuracy, utilizing only 1% of the model's\nparameters. These results establish our approach as a scalable and\nresource-efficient solution for continual learning in LLMs."
    },
    {
        "date": "2025-02",
        "title": "PoleStack: Robust Pole Estimation of Irregular Objects from Silhouette Stacking",
        "author": "Jacopo Villa, Jay W. McMahon, and Issa A. D. Nesnas",
        "link": "http://arxiv.org/abs/2502.02907v1",
        "abstract": "We present an algorithm to estimate the rotation pole of a principal-axis\nrotator using silhouette images collected from multiple camera poses. First, a\nset of images is stacked to form a single silhouette-stack image, where the\nobject's rotation introduces reflective symmetry about the imaged pole\ndirection. We estimate this projected-pole direction by identifying maximum\nsymmetry in the silhouette stack. To handle unknown center-of-mass image\nlocation, we apply the Discrete Fourier Transform to produce the\nsilhouette-stack amplitude spectrum, achieving translation invariance and\nincreased robustness to noise. Second, the 3D pole orientation is estimated by\ncombining two or more projected-pole measurements collected from different\ncamera orientations. We demonstrate degree-level pole estimation accuracy using\nlow-resolution imagery, showing robustness to severe surface shadowing and\ncentroid-based image-registration errors. The proposed approach could be\nsuitable for pole estimation during both the approach phase toward a target\nobject and while hovering."
    },
    {
        "date": "2025-02",
        "title": "Wolfpack Adversarial Attack for Robust Multi-Agent Reinforcement Learning",
        "author": "Sunwoo Lee, Jaebak Hwang, Yonghyeon Jo, and Seungyul Han",
        "link": "http://arxiv.org/abs/2502.02844v1",
        "abstract": "Traditional robust methods in multi-agent reinforcement learning (MARL) often\nstruggle against coordinated adversarial attacks in cooperative scenarios. To\naddress this limitation, we propose the Wolfpack Adversarial Attack framework,\ninspired by wolf hunting strategies, which targets an initial agent and its\nassisting agents to disrupt cooperation. Additionally, we introduce the\nWolfpack-Adversarial Learning for MARL (WALL) framework, which trains robust\nMARL policies to defend against the proposed Wolfpack attack by fostering\nsystem-wide collaboration. Experimental results underscore the devastating\nimpact of the Wolfpack attack and the significant robustness improvements\nachieved by WALL."
    },
    {
        "date": "2025-02",
        "title": "SimMark: A Robust Sentence-Level Similarity-Based Watermarking Algorithm for Large Language Models",
        "author": "Amirhossein Dabiriaghdam, and Lele Wang",
        "link": "http://arxiv.org/abs/2502.02787v1",
        "abstract": "The rapid proliferation of large language models (LLMs) has created an urgent\nneed for reliable methods to detect whether a text is generated by such models.\nIn this paper, we propose SimMark, a posthoc watermarking algorithm that makes\nLLMs' outputs traceable without requiring access to the model's internal\nlogits, enabling compatibility with a wide range of LLMs, including API-only\nmodels. By leveraging the similarity of semantic sentence embeddings and\nrejection sampling to impose detectable statistical patterns imperceptible to\nhumans, and employing a soft counting mechanism, SimMark achieves robustness\nagainst paraphrasing attacks. Experimental results demonstrate that SimMark\nsets a new benchmark for robust watermarking of LLM-generated content,\nsurpassing prior sentence-level watermarking techniques in robustness, sampling\nefficiency, and applicability across diverse domains, all while preserving the\ntext quality."
    },
    {
        "date": "2025-02",
        "title": "MARAGE: Transferable Multi-Model Adversarial Attack for Retrieval-Augmented Generation Data Extraction",
        "author": "Xiao Hu, Eric Liu, Weizhou Wang, Xiangyu Guo, and David Lie",
        "link": "http://arxiv.org/abs/2502.04360v1",
        "abstract": "Retrieval-Augmented Generation (RAG) offers a solution to mitigate\nhallucinations in Large Language Models (LLMs) by grounding their outputs to\nknowledge retrieved from external sources. The use of private resources and\ndata in constructing these external data stores can expose them to risks of\nextraction attacks, in which attackers attempt to steal data from these private\ndatabases. Existing RAG extraction attacks often rely on manually crafted\nprompts, which limit their effectiveness. In this paper, we introduce a\nframework called MARAGE for optimizing an adversarial string that, when\nappended to user queries submitted to a target RAG system, causes outputs\ncontaining the retrieved RAG data verbatim. MARAGE leverages a continuous\noptimization scheme that integrates gradients from multiple models with\ndifferent architectures simultaneously to enhance the transferability of the\noptimized string to unseen models. Additionally, we propose a strategy that\nemphasizes the initial tokens in the target RAG data, further improving the\nattack's generalizability. Evaluations show that MARAGE consistently\noutperforms both manual and optimization-based baselines across multiple LLMs\nand RAG datasets, while maintaining robust transferability to previously unseen\nmodels. Moreover, we conduct probing tasks to shed light on the reasons why\nMARAGE is more effective compared to the baselines and to analyze the impact of\nour approach on the model's internal state."
    },
    {
        "date": "2025-02",
        "title": "Unveiling Privacy and Security Gaps in Female Health Apps",
        "author": "Muhammad Hassan, Mahnoor Jameel, Tian Wang, and Masooda Bashir",
        "link": "http://arxiv.org/abs/2502.02749v1",
        "abstract": "Female Health Applications (FHA), a growing segment of FemTech, aim to\nprovide affordable and accessible healthcare solutions for women globally.\nThese applications gather and monitor health and reproductive data from\nmillions of users. With ongoing debates on women's reproductive rights and\nprivacy, it's crucial to assess how these apps protect users' privacy. In this\npaper, we undertake a security and data protection assessment of 45 popular\nFHAs. Our investigation uncovers harmful permissions, extensive collection of\nsensitive personal and medical data, and the presence of numerous third-party\ntracking libraries. Furthermore, our examination of their privacy policies\nreveals deviations from fundamental data privacy principles. These findings\nhighlight a significant lack of privacy and security measures for FemTech apps,\nespecially as women's reproductive rights face growing political challenges.\nThe results and recommendations provide valuable insights for users, app\ndevelopers, and policymakers, paving the way for better privacy and security in\nFemale Health Applications."
    },
    {
        "date": "2025-02",
        "title": "Achievable distributional robustness when the robust risk is only partially identified",
        "author": "Julia Kostin, Nicola Gnecco, and Fanny Yang",
        "link": "http://arxiv.org/abs/2502.02710v1",
        "abstract": "In safety-critical applications, machine learning models should generalize\nwell under worst-case distribution shifts, that is, have a small robust risk.\nInvariance-based algorithms can provably take advantage of structural\nassumptions on the shifts when the training distributions are heterogeneous\nenough to identify the robust risk. However, in practice, such identifiability\nconditions are rarely satisfied -- a scenario so far underexplored in the\ntheoretical literature. In this paper, we aim to fill the gap and propose to\nstudy the more general setting when the robust risk is only partially\nidentifiable. In particular, we introduce the worst-case robust risk as a new\nmeasure of robustness that is always well-defined regardless of\nidentifiability. Its minimum corresponds to an algorithm-independent\n(population) minimax quantity that measures the best achievable robustness\nunder partial identifiability. While these concepts can be defined more\nbroadly, in this paper we introduce and derive them explicitly for a linear\nmodel for concreteness of the presentation. First, we show that existing\nrobustness methods are provably suboptimal in the partially identifiable case.\nWe then evaluate these methods and the minimizer of the (empirical) worst-case\nrobust risk on real-world gene expression data and find a similar trend: the\ntest error of existing robustness methods grows increasingly suboptimal as the\nfraction of data from unseen environments increases, whereas accounting for\npartial identifiability allows for better generalization."
    },
    {
        "date": "2025-02",
        "title": "Intelligent Sensing-to-Action for Robust Autonomy at the Edge: Opportunities and Challenges",
        "author": "Amit Ranjan Trivedi, Sina Tayebati, Hemant Kumawat, Nastaran Darabi, Divake Kumar, Adarsh Kumar Kosta, Yeshwanth Venkatesha, Dinithi Jayasuriya, Nethmi Jayasinghe, Priyadarshini Panda, Saibal Mukhopadhyay, and Kaushik Roy",
        "link": "http://arxiv.org/abs/2502.02692v1",
        "abstract": "Autonomous edge computing in robotics, smart cities, and autonomous vehicles\nrelies on the seamless integration of sensing, processing, and actuation for\nreal-time decision-making in dynamic environments. At its core is the\nsensing-to-action loop, which iteratively aligns sensor inputs with\ncomputational models to drive adaptive control strategies. These loops can\nadapt to hyper-local conditions, enhancing resource efficiency and\nresponsiveness, but also face challenges such as resource constraints,\nsynchronization delays in multi-modal data fusion, and the risk of cascading\nerrors in feedback loops. This article explores how proactive, context-aware\nsensing-to-action and action-to-sensing adaptations can enhance efficiency by\ndynamically adjusting sensing and computation based on task demands, such as\nsensing a very limited part of the environment and predicting the rest. By\nguiding sensing through control actions, action-to-sensing pathways can improve\ntask relevance and resource use, but they also require robust monitoring to\nprevent cascading errors and maintain reliability. Multi-agent sensing-action\nloops further extend these capabilities through coordinated sensing and actions\nacross distributed agents, optimizing resource use via collaboration.\nAdditionally, neuromorphic computing, inspired by biological systems, provides\nan efficient framework for spike-based, event-driven processing that conserves\nenergy, reduces latency, and supports hierarchical control--making it ideal for\nmulti-agent optimization. This article highlights the importance of end-to-end\nco-design strategies that align algorithmic models with hardware and\nenvironmental dynamics and improve cross-layer interdependencies to improve\nthroughput, precision, and adaptability for energy-efficient edge autonomy in\ncomplex environments."
    },
    {
        "date": "2025-02",
        "title": "OverThink: Slowdown Attacks on Reasoning LLMs",
        "author": "Abhinav Kumar, Jaechul Roh, Ali Naseh, Marzena Karpinska, Mohit Iyyer, Amir Houmansadr, and Eugene Bagdasarian",
        "link": "http://arxiv.org/abs/2502.02542v2",
        "abstract": "We increase overhead for applications that rely on reasoning LLMs-we force\nmodels to spend an amplified number of reasoning tokens, i.e., \"overthink\", to\nrespond to the user query while providing contextually correct answers. The\nadversary performs an OVERTHINK attack by injecting decoy reasoning problems\ninto the public content that is used by the reasoning LLM (e.g., for RAG\napplications) during inference time. Due to the nature of our decoy problems\n(e.g., a Markov Decision Process), modified texts do not violate safety\nguardrails. We evaluated our attack across closed-(OpenAI o1, o1-mini, o3-mini)\nand open-(DeepSeek R1) weights reasoning models on the FreshQA and SQuAD\ndatasets. Our results show up to 18x slowdown on FreshQA dataset and 46x\nslowdown on SQuAD dataset. The attack also shows high transferability across\nmodels. To protect applications, we discuss and implement defenses leveraging\nLLM-based and system design approaches. Finally, we discuss societal,\nfinancial, and energy impacts of OVERTHINK attack which could amplify the costs\nfor third-party applications operating reasoning models."
    },
    {
        "date": "2025-02",
        "title": "Optimal Security Response to Network Intrusions in IT Systems",
        "author": "Kim Hammar",
        "link": "http://arxiv.org/abs/2502.02541v1",
        "abstract": "Cybersecurity is one of the most pressing technological challenges of our\ntime and requires measures from all sectors of society. A key measure is\nautomated security response, which enables automated mitigation and recovery\nfrom cyber attacks. Significant strides toward such automation have been made\ndue to the development of rule-based response systems. However, these systems\nhave a critical drawback: they depend on domain experts to configure the rules,\na process that is both error-prone and inefficient. Framing security response\nas an optimal control problem shows promise in addressing this limitation but\nintroduces new challenges. Chief among them is bridging the gap between\ntheoretical optimality and operational performance. Current response systems\nwith theoretical optimality guarantees have only been validated analytically or\nin simulation, leaving their practical utility unproven.\n  This thesis tackles the aforementioned challenges by developing a practical\nmethodology for optimal security response in IT infrastructures. It encompasses\ntwo systems. First, it includes an emulation system that replicates key\ncomponents of the target infrastructure. We use this system to gather\nmeasurements and logs, based on which we identify a game-theoretic model.\nSecond, it includes a simulation system where game-theoretic response\nstrategies are optimized through stochastic approximation to meet a given\nobjective, such as mitigating potential attacks while maintaining operational\nservices. These strategies are then evaluated and refined in the emulation\nsystem to close the gap between theoretical and operational performance. We\nprove structural properties of optimal response strategies and derive efficient\nalgorithms for computing them. This enables us to solve a previously unsolved\nproblem: demonstrating optimal security response against network intrusions on\nan IT infrastructure."
    },
    {
        "date": "2025-02",
        "title": "Uncertainty Quantification for Collaborative Object Detection Under Adversarial Attacks",
        "author": "Huiqun Huang, Cong Chen, Jean-Philippe Monteuuis, Jonathan Petit, and Fei Miao",
        "link": "http://arxiv.org/abs/2502.02537v1",
        "abstract": "Collaborative Object Detection (COD) and collaborative perception can\nintegrate data or features from various entities, and improve object detection\naccuracy compared with individual perception. However, adversarial attacks pose\na potential threat to the deep learning COD models, and introduce high output\nuncertainty. With unknown attack models, it becomes even more challenging to\nimprove COD resiliency and quantify the output uncertainty for highly dynamic\nperception scenes such as autonomous vehicles. In this study, we propose the\nTrusted Uncertainty Quantification in Collaborative Perception framework\n(TUQCP). TUQCP leverages both adversarial training and uncertainty\nquantification techniques to enhance the adversarial robustness of existing COD\nmodels. More specifically, TUQCP first adds perturbations to the shared\ninformation of randomly selected agents during object detection collaboration\nby adversarial training. TUQCP then alleviates the impacts of adversarial\nattacks by providing output uncertainty estimation through learning-based\nmodule and uncertainty calibration through conformal prediction. Our framework\nworks for early and intermediate collaboration COD models and single-agent\nobject detection models. We evaluate TUQCP on V2X-Sim, a comprehensive\ncollaborative perception dataset for autonomous driving, and demonstrate a\n80.41% improvement in object detection accuracy compared to the baselines under\nthe same adversarial attacks. TUQCP demonstrates the importance of uncertainty\nquantification to COD under adversarial attacks."
    },
    {
        "date": "2025-02",
        "title": "Privacy Attacks on Image AutoRegressive Models",
        "author": "Antoni Kowalczuk, Jan Dubi\u0144ski, Franziska Boenisch, and Adam Dziedzic",
        "link": "http://arxiv.org/abs/2502.02514v1",
        "abstract": "Image autoregressive (IAR) models have surpassed diffusion models (DMs) in\nboth image quality (FID: 1.48 vs. 1.58) and generation speed. However, their\nprivacy risks remain largely unexplored. To address this, we conduct a\ncomprehensive privacy analysis comparing IARs to DMs. We develop a novel\nmembership inference attack (MIA) that achieves a significantly higher success\nrate in detecting training images (TPR@FPR=1%: 86.38% for IARs vs. 4.91% for\nDMs). Using this MIA, we perform dataset inference (DI) and find that IARs\nrequire as few as six samples to detect dataset membership, compared to 200 for\nDMs, indicating higher information leakage. Additionally, we extract hundreds\nof training images from an IAR (e.g., 698 from VAR-d30). Our findings highlight\na fundamental privacy-utility trade-off: while IARs excel in generation quality\nand speed, they are significantly more vulnerable to privacy attacks. This\nsuggests that incorporating techniques from DMs, such as per-token probability\nmodeling using diffusion, could help mitigate IARs' privacy risks. Our code is\navailable at https://github.com/sprintml/privacy_attacks_against_iars."
    },
    {
        "date": "2025-02",
        "title": "Investigating the Robustness of Deductive Reasoning with Large Language Models",
        "author": "Fabian Hoppe, Filip Ilievski, and Jan-Christoph Kalo",
        "link": "http://arxiv.org/abs/2502.04352v1",
        "abstract": "Large Language Models (LLMs) have been shown to achieve impressive results\nfor many reasoning-based Natural Language Processing (NLP) tasks, suggesting a\ndegree of deductive reasoning capability. However, it remains unclear to which\nextent LLMs, in both informal and autoformalisation methods, are robust on\nlogical deduction tasks. Moreover, while many LLM-based deduction methods have\nbeen proposed, there is a lack of a systematic study that analyses the impact\nof their design components. Addressing these two challenges, we propose the\nfirst study of the robustness of LLM-based deductive reasoning methods. We\ndevise a framework with two families of perturbations: adversarial noise and\ncounterfactual statements, which jointly generate seven perturbed datasets. We\norganize the landscape of LLM reasoners according to their reasoning format,\nformalisation syntax, and feedback for error recovery. The results show that\nadversarial noise affects autoformalisation, while counterfactual statements\ninfluence all approaches. Detailed feedback does not improve overall accuracy\ndespite reducing syntax errors, pointing to the challenge of LLM-based methods\nto self-correct effectively."
    },
    {
        "date": "2025-02",
        "title": "CoRPA: Adversarial Image Generation for Chest X-rays Using Concept Vector Perturbations and Generative Models",
        "author": "Amy Rafferty, Rishi Ramaesh, and Ajitha Rajan",
        "link": "http://arxiv.org/abs/2502.05214v1",
        "abstract": "Deep learning models for medical image classification tasks are becoming\nwidely implemented in AI-assisted diagnostic tools, aiming to enhance\ndiagnostic accuracy, reduce clinician workloads, and improve patient outcomes.\nHowever, their vulnerability to adversarial attacks poses significant risks to\npatient safety. Current attack methodologies use general techniques such as\nmodel querying or pixel value perturbations to generate adversarial examples\ndesigned to fool a model. These approaches may not adequately address the\nunique characteristics of clinical errors stemming from missed or incorrectly\nidentified clinical features. We propose the Concept-based Report Perturbation\nAttack (CoRPA), a clinically-focused black-box adversarial attack framework\ntailored to the medical imaging domain. CoRPA leverages clinical concepts to\ngenerate adversarial radiological reports and images that closely mirror\nrealistic clinical misdiagnosis scenarios. We demonstrate the utility of CoRPA\nusing the MIMIC-CXR-JPG dataset of chest X-rays and radiological reports. Our\nevaluation reveals that deep learning models exhibiting strong resilience to\nconventional adversarial attacks are significantly less robust when subjected\nto CoRPA's clinically-focused perturbations. This underscores the importance of\naddressing domain-specific vulnerabilities in medical AI systems. By\nintroducing a specialized adversarial attack framework, this study provides a\nfoundation for developing robust, real-world-ready AI models in healthcare,\nensuring their safe and reliable deployment in high-stakes clinical\nenvironments."
    },
    {
        "date": "2025-02",
        "title": "Catoni Contextual Bandits are Robust to Heavy-tailed Rewards",
        "author": "Chenlu Ye, Yujia Jin, Alekh Agarwal, and Tong Zhang",
        "link": "http://arxiv.org/abs/2502.02486v1",
        "abstract": "Typical contextual bandit algorithms assume that the rewards at each round\nlie in some fixed range $[0, R]$, and their regret scales polynomially with\nthis reward range $R$. However, many practical scenarios naturally involve\nheavy-tailed rewards or rewards where the worst-case range can be substantially\nlarger than the variance. In this paper, we develop an algorithmic approach\nbuilding on Catoni's estimator from robust statistics, and apply it to\ncontextual bandits with general function approximation. When the variance of\nthe reward at each round is known, we use a variance-weighted regression\napproach and establish a regret bound that depends only on the cumulative\nreward variance and logarithmically on the reward range $R$ as well as the\nnumber of rounds $T$. For the unknown-variance case, we further propose a\ncareful peeling-based algorithm and remove the need for cumbersome variance\nestimation. With additional dependence on the fourth moment, our algorithm also\nenjoys a variance-based bound with logarithmic reward-range dependence.\nMoreover, we demonstrate the optimality of the leading-order term in our regret\nbound through a matching lower bound."
    },
    {
        "date": "2025-02",
        "title": "Medical Multimodal Model Stealing Attacks via Adversarial Domain Alignment",
        "author": "Yaling Shen, Zhixiong Zhuang, Kun Yuan, Maria-Irina Nicolae, Nassir Navab, Nicolas Padoy, and Mario Fritz",
        "link": "http://arxiv.org/abs/2502.02438v1",
        "abstract": "Medical multimodal large language models (MLLMs) are becoming an instrumental\npart of healthcare systems, assisting medical personnel with decision making\nand results analysis. Models for radiology report generation are able to\ninterpret medical imagery, thus reducing the workload of radiologists. As\nmedical data is scarce and protected by privacy regulations, medical MLLMs\nrepresent valuable intellectual property. However, these assets are potentially\nvulnerable to model stealing, where attackers aim to replicate their\nfunctionality via black-box access. So far, model stealing for the medical\ndomain has focused on classification; however, existing attacks are not\neffective against MLLMs. In this paper, we introduce Adversarial Domain\nAlignment (ADA-STEAL), the first stealing attack against medical MLLMs.\nADA-STEAL relies on natural images, which are public and widely available, as\nopposed to their medical counterparts. We show that data augmentation with\nadversarial noise is sufficient to overcome the data distribution gap between\nnatural images and the domain-specific distribution of the victim MLLM.\nExperiments on the IU X-RAY and MIMIC-CXR radiology datasets demonstrate that\nAdversarial Domain Alignment enables attackers to steal the medical MLLM\nwithout any access to medical data."
    },
    {
        "date": "2025-02",
        "title": "TransformDAS: Mapping \u03a6-OTDR Signals to Riemannian Manifold for Robust Classification",
        "author": "Jiaju Kang, Puyu Han, Yang Chun, Xu Wang, and Luqi Gong",
        "link": "http://arxiv.org/abs/2502.02428v1",
        "abstract": "Phase-sensitive optical time-domain reflectometry ({\\Phi}-OTDR) is a widely\nused distributed fiber optic sensing system in engineering. Machine learning\nalgorithms for {\\Phi}-OTDR event classification require high volumes and\nquality of datasets; however, high-quality datasets are currently extremely\nscarce in the field, leading to a lack of robustness in models, which is\nmanifested by higher false alarm rates in real-world scenarios. One promising\napproach to address this issue is to augment existing data using generative\nmodels combined with a small amount of real-world data. We explored mapping\nboth {\\Phi}-OTDR features in a GAN-based generative pipeline and signal\nfeatures in a Transformer classifier to hyperbolic space to seek more effective\nmodel generalization. The results indicate that state-of-the-art models exhibit\nstronger generalization performance and lower false alarm rates in real-world\nscenarios when trained on augmented datasets. TransformDAS, in particular,\ndemonstrates the best classification performance, highlighting the benefits of\nRiemannian manifold mapping in {\\Phi}-OTDR data generation and model\nclassification."
    },
    {
        "date": "2025-02",
        "title": "Target Attack Backdoor Malware Analysis and Attribution",
        "author": "Anthony Cheuk Tung Lai, Vitaly Kamluk, Alan Ho, Ping Fan Ke, and Byron Wai",
        "link": "http://arxiv.org/abs/2502.02335v2",
        "abstract": "Backdoor Malware are installed by an attacker on the victim's server(s) for\nauthorized access. A customized backdoor is weaponized to execute unauthorized\nsystem, database and application commands to access the user credentials and\nconfidential digital assets. Recently, we discovered and analyzed a targeted\npersistent module backdoor in Web Server in an online business company that was\nundetectable by their deployed Anti-Virus software for a year. This led us to\ncarry out research to detect this specific type of persistent module backdoor\ninstalled in Web servers. Other than typical Malware static analysis, we carry\nout analysis with binary similarity, strings, and command obfuscation over the\nbackdoor, resulting in the Target Attack Backdoor Malware Analysis Matrix\n(TABMAX) for organizations to detect this sophisticated target attack backdoor\ninstead of a general one which can be detected by Anti-Virus detectors. Our\nfindings show that backdoor malware can be designed with different APIs,\ncommands, strings, and query language on top of preferred libraries used by\ntypical Malware."
    },
    {
        "date": "2025-02",
        "title": "FRAUD-RLA: A new reinforcement learning adversarial attack against credit card fraud detection",
        "author": "Daniele Lunghi, Yannick Molinghen, Alkis Simitsis, Tom Lenaerts, and Gianluca Bontempi",
        "link": "http://arxiv.org/abs/2502.02290v1",
        "abstract": "Adversarial attacks pose a significant threat to data-driven systems, and\nresearchers have spent considerable resources studying them. Despite its\neconomic relevance, this trend largely overlooked the issue of credit card\nfraud detection. To address this gap, we propose a new threat model that\ndemonstrates the limitations of existing attacks and highlights the necessity\nto investigate new approaches. We then design a new adversarial attack for\ncredit card fraud detection, employing reinforcement learning to bypass\nclassifiers. This attack, called FRAUD-RLA, is designed to maximize the\nattacker's reward by optimizing the exploration-exploitation tradeoff and\nworking with significantly less required knowledge than competitors. Our\nexperiments, conducted on three different heterogeneous datasets and against\ntwo fraud detection systems, indicate that FRAUD-RLA is effective, even\nconsidering the severe limitations imposed by our threat model."
    },
    {
        "date": "2025-02",
        "title": "Adversarial ML Problems Are Getting Harder to Solve and to Evaluate",
        "author": "Javier Rando, Jie Zhang, Nicholas Carlini, and Florian Tram\u00e8r",
        "link": "http://arxiv.org/abs/2502.02260v1",
        "abstract": "In the past decade, considerable research effort has been devoted to securing\nmachine learning (ML) models that operate in adversarial settings. Yet,\nprogress has been slow even for simple \"toy\" problems (e.g., robustness to\nsmall adversarial perturbations) and is often hindered by non-rigorous\nevaluations. Today, adversarial ML research has shifted towards studying\nlarger, general-purpose language models. In this position paper, we argue that\nthe situation is now even worse: in the era of LLMs, the field of adversarial\nML studies problems that are (1) less clearly defined, (2) harder to solve, and\n(3) even more challenging to evaluate. As a result, we caution that yet another\ndecade of work on adversarial ML may fail to produce meaningful progress."
    },
    {
        "date": "2025-02",
        "title": "DERMARK: A Dynamic, Efficient and Robust Multi-bit Watermark for Large Language Models",
        "author": "Qihao Lin, Chen Tang, Lan zhang, Junyang zhang, and Xiangyang Li",
        "link": "http://arxiv.org/abs/2502.05213v1",
        "abstract": "Well-trained large language models (LLMs) present significant risks,\nincluding potential malicious use and copyright infringement. Current studies\naim to trace the distribution of LLM-generated texts by implicitly embedding\nwatermarks. Among these, the single-bit watermarking method can only determine\nwhether a given text was generated by an LLM. In contrast, the multi-bit\nwatermarking method embeds richer information into the generated text, which\ncan identify which LLM generated and distributed a given text to which user.\nHowever, existing efforts embed the multi-bit watermark directly into the\ngenerated text without accounting for its watermarking capacity. This approach\ncan result in embedding failures when the text's watermarking capacity is\ninsufficient. In this paper, we derive the watermark embedding distribution\nbased on the logits of LLMs and propose a formal inequality to segment the text\noptimally for watermark embedding. Building on this foundation, we propose\nDERMARK, a dynamic, efficient, and robust multi-bit watermarking method.\nDERMARK divides the text into segments of varying lengths for each bit\nembedding, adaptively matching the text's capacity. It achieves this with\nnegligible overhead and robust performance against text editing by minimizing\nwatermark extraction loss. Comprehensive experiments demonstrate that, compared\nto the SOTA method, our method reduces the number of tokens required for\nembedding each bit by 20\\%, reduces watermark embedding time by 50\\%, and is\nrobust to text editing and watermark erasure attacks."
    },
    {
        "date": "2025-02",
        "title": "An Attack-Driven Incident Response and Defense System (ADIRDS)",
        "author": "Anthony Cheuk Tung Lai, Siu Ming Yiu, Ping Fan Ke, and Alan Ho",
        "link": "http://arxiv.org/abs/2502.02230v1",
        "abstract": "One of the major goals of incident response is to help an organization or a\nsystem owner to quickly identify and halt the attacks to minimize the damages\n(and financial loss) to the system being attacked. Typical incident responses\nrely very much on the log information captured by the system during the attacks\nand if needed, may need to isolate the victim from the network to avoid further\ndestructive attacks. However, there are real cases that there are insufficient\nlog records/information for the incident response team to identify the attacks\nand their origins while the attacked system cannot be stopped due to service\nrequirements (zero downtime online systems) such as online gaming sites.\nTypical incident response procedures and industrial standards do not provide an\nadequate solution to address this scenario. In this paper, being motivated by a\nreal case, we propose a solution, called \"Attack-Driven Incident Response and\nDefense System (ADIRDS)\" to tackle this problem. ADIRDS is an online monitoring\nsystem to run with the real system. By modeling the real system as a graph,\ncritical nodes/assets of the system are closely monitored. Instead of relying\non the original logging system, evidence will be collected from the attack\ntechnique perspectives. To migrate the risks, realistic honeypots with very\nsimilar business context as the real system are deployed to trap the attackers.\nWe successfully apply this system to a real case. Based on our experiments, we\nverify that our new approach of designing the realistic honeypots is effective,\n38 unique attacker's IP addresses were captured. We also compare the\nperformance of our realistic honey with both low and high interactive honeypots\nproposed in the literature, the results found that our proposed honeypot can\nsuccessfully cheat the attackers to attack our honeypot, which verifies that\nour honeypot is more effective."
    },
    {
        "date": "2025-02",
        "title": "A Robust Remote Photoplethysmography Method",
        "author": "Alexey Protopopov",
        "link": "http://arxiv.org/abs/2502.02229v1",
        "abstract": "Remote photoplethysmography (rPPG) is a method for measuring a subjects heart\nrate remotely using a camera. Factors such as subject movement, ambient light\nlevel, makeup etc. complicate such measurements by distorting the observed\npulse. Recent works on this topic have proposed a variety of approaches for\naccurately measuring heart rate in humans, however these methods were tested in\nideal conditions, where the subject does not make significant movements and all\nmeasurements are taken at the same level of illumination. In more realistic\nconditions these methods suffer from decreased accuracy. The study proposes a\nmore robust method that is less susceptible to distortions and has minimal\nhardware requirements. The proposed method uses a combination of mathematical\ntransforms to calculate the subjects heart rate. It performs best when used\nwith a camera that has been modified by removing its infrared filter, although\nusing an unmodified camera is also possible. The method was tested on 26 videos\ntaken from 19 volunteers of varying gender and age. The obtained results were\ncompared to reference data and the average mean absolute error was found to be\nat 1.95 beats per minute, which is noticeably better than the results from\nprevious works. The remote photoplethysmography method proposed in the present\narticle is more resistant to distortions than methods from previous\npublications and thus allows one to remotely and accurately measure the\nsubjects heart rate without imposing any significant limitations on the\nsubjects behavior."
    },
    {
        "date": "2025-02",
        "title": "Progressive Correspondence Regenerator for Robust 3D Registration",
        "author": "Guiyu Zhao, Sheng Ao, Ye Zhang, Kai Xu, and Yulan Guo",
        "link": "http://arxiv.org/abs/2502.02163v2",
        "abstract": "Obtaining enough high-quality correspondences is crucial for robust\nregistration. Existing correspondence refinement methods mostly follow the\nparadigm of outlier removal, which either fails to correctly identify the\naccurate correspondences under extreme outlier ratios, or select too few\ncorrect correspondences to support robust registration. To address this\nchallenge, we propose a novel approach named Regor, which is a progressive\ncorrespondence regenerator that generates higher-quality matches whist\nsufficiently robust for numerous outliers. In each iteration, we first apply\nprior-guided local grouping and generalized mutual matching to generate the\nlocal region correspondences. A powerful center-aware three-point consistency\nis then presented to achieve local correspondence correction, instead of\nremoval. Further, we employ global correspondence refinement to obtain accurate\ncorrespondences from a global perspective. Through progressive iterations, this\nprocess yields a large number of high-quality correspondences. Extensive\nexperiments on both indoor and outdoor datasets demonstrate that the proposed\nRegor significantly outperforms existing outlier removal techniques. More\ncritically, our approach obtain 10 times more correct correspondences than\noutlier removal methods. As a result, our method is able to achieve robust\nregistration even with weak features. The code will be released."
    },
    {
        "date": "2025-02",
        "title": "Dual-Flow: Transferable Multi-Target, Instance-Agnostic Attacks via In-the-wild Cascading Flow Optimization",
        "author": "Yixiao Chen, Shikun Sun, Jianshu Li, Ruoyu Li, Zhe Li, and Junliang Xing",
        "link": "http://arxiv.org/abs/2502.02096v2",
        "abstract": "Adversarial attacks are widely used to evaluate model robustness, and in\nblack-box scenarios, the transferability of these attacks becomes crucial.\nExisting generator-based attacks have excellent generalization and\ntransferability due to their instance-agnostic nature. However, when training\ngenerators for multi-target tasks, the success rate of transfer attacks is\nrelatively low due to the limitations of the model's capacity. To address these\nchallenges, we propose a novel Dual-Flow framework for multi-target\ninstance-agnostic adversarial attacks, utilizing Cascading Distribution Shift\nTraining to develop an adversarial velocity function. Extensive experiments\ndemonstrate that Dual-Flow significantly improves transferability over previous\nmulti-target generative attacks. For example, it increases the success rate\nfrom Inception-v3 to ResNet-152 by 34.58%. Furthermore, our attack method shows\nsubstantially stronger robustness against defense mechanisms, such as\nadversarially trained models."
    },
    {
        "date": "2025-02",
        "title": "Robust and Secure Code Watermarking for Large Language Models via ML/Crypto Codesign",
        "author": "Ruisi Zhang, Neusha Javidnia, Nojan Sheybani, and Farinaz Koushanfar",
        "link": "http://arxiv.org/abs/2502.02068v2",
        "abstract": "This paper introduces RoSeMary, the first-of-its-kind ML/Crypto codesign\nwatermarking framework that regulates LLM-generated code to avoid intellectual\nproperty rights violations and inappropriate misuse in software development.\nHigh-quality watermarks adhering to the detectability-fidelity-robustness\ntri-objective are limited due to codes' low-entropy nature. Watermark\nverification, however, often needs to reveal the signature and requires\nre-encoding new ones for code reuse, which potentially compromising the\nsystem's usability. To overcome these challenges, RoSeMary obtains high-quality\nwatermarks by training the watermark insertion and extraction modules\nend-to-end to ensure (i) unaltered watermarked code functionality and (ii)\nenhanced detectability and robustness leveraging pre-trained CodeT5 as the\ninsertion backbone to enlarge the code syntactic and variable rename\ntransformation search space. In the deployment, RoSeMary uses zero-knowledge\nproofs for secure verification without revealing the underlying signatures.\nExtensive evaluations demonstrated RoSeMary achieves high detection accuracy\nwhile preserving the code functionality. RoSeMary is also robust against\nattacks and provides efficient secure watermark verification."
    },
    {
        "date": "2025-02",
        "title": "RAPID: Robust and Agile Planner Using Inverse Reinforcement Learning for Vision-Based Drone Navigation",
        "author": "Minwoo Kim, Geunsik Bae, Jinwoo Lee, Woojae Shin, Changseung Kim, Myong-Yol Choi, Heejung Shin, and Hyondong Oh",
        "link": "http://arxiv.org/abs/2502.02054v1",
        "abstract": "This paper introduces a learning-based visual planner for agile drone flight\nin cluttered environments. The proposed planner generates collision-free\nwaypoints in milliseconds, enabling drones to perform agile maneuvers in\ncomplex environments without building separate perception, mapping, and\nplanning modules. Learning-based methods, such as behavior cloning (BC) and\nreinforcement learning (RL), demonstrate promising performance in visual\nnavigation but still face inherent limitations. BC is susceptible to\ncompounding errors due to limited expert imitation, while RL struggles with\nreward function design and sample inefficiency. To address these limitations,\nthis paper proposes an inverse reinforcement learning (IRL)-based framework for\nhigh-speed visual navigation. By leveraging IRL, it is possible to reduce the\nnumber of interactions with simulation environments and improve capability to\ndeal with high-dimensional spaces while preserving the robustness of RL\npolicies. A motion primitive-based path planning algorithm collects an expert\ndataset with privileged map data from diverse environments, ensuring\ncomprehensive scenario coverage. By leveraging both the acquired expert and\nlearner dataset gathered from the agent's interactions with the simulation\nenvironments, a robust reward function and policy are learned across diverse\nstates. While the proposed method is trained in a simulation environment only,\nit can be directly applied to real-world scenarios without additional training\nor tuning. The performance of the proposed method is validated in both\nsimulation and real-world environments, including forests and various\nstructures. The trained policy achieves an average speed of 7 m/s and a maximum\nspeed of 8.8 m/s in real flight experiments. To the best of our knowledge, this\nis the first work to successfully apply an IRL framework for high-speed visual\nnavigation of drones."
    },
    {
        "date": "2025-02",
        "title": "SMTFL: Secure Model Training to Untrusted Participants in Federated Learning",
        "author": "Zhihui Zhao, Xiaorong Dong, Yimo Ren, Jianhua Wang, Dan Yu, Hongsong Zhu, and Yongle Chen",
        "link": "http://arxiv.org/abs/2502.02038v1",
        "abstract": "Federated learning is an essential distributed model training technique.\nHowever, threats such as gradient inversion attacks and poisoning attacks pose\nsignificant risks to the privacy of training data and the model correctness. We\npropose a novel approach called SMTFL to achieve secure model training in\nfederated learning without relying on trusted participants. To safeguard\ngradients privacy against gradient inversion attacks, clients are dynamically\ngrouped, allowing one client's gradient to be divided to obfuscate the\ngradients of other clients within the group. This method incorporates checks\nand balances to reduce the collusion for inferring specific client data. To\ndetect poisoning attacks from malicious clients, we assess the impact of\naggregated gradients on the global model's performance, enabling effective\nidentification and exclusion of malicious clients. Each client's gradients are\nencrypted and stored, with decryption collectively managed by all clients. The\ndetected poisoning gradients are invalidated from the global model through a\nunlearning method. To our best knowledge, we present the first practical secure\naggregation scheme, which does not require trusted participants, avoids the\nperformance degradation associated with traditional noise-injection, and aviods\ncomplex cryptographic operations during gradient aggregation. Evaluation\nresults are encouraging based on four datasets and two models: SMTFL is\neffective against poisoning attacks and gradient inversion attacks, achieving\nan accuracy rate of over 95% in locating malicious clients, while keeping the\nfalse positive rate for honest clients within 5%. The model accuracy is also\nnearly restored to its pre-attack state when SMTFL is deployed."
    },
    {
        "date": "2025-02",
        "title": "Multi-Domain Graph Foundation Models: Robust Knowledge Transfer via Topology Alignment",
        "author": "Shuo Wang, Bokui Wang, Zhixiang Shen, Boyan Deng, and Zhao Kang",
        "link": "http://arxiv.org/abs/2502.02017v1",
        "abstract": "Recent advances in CV and NLP have inspired researchers to develop\ngeneral-purpose graph foundation models through pre-training across diverse\ndomains. However, a fundamental challenge arises from the substantial\ndifferences in graph topologies across domains. Additionally, real-world graphs\nare often sparse and prone to noisy connections and adversarial attacks. To\naddress these issues, we propose the Multi-Domain Graph Foundation Model\n(MDGFM), a unified framework that aligns and leverages cross-domain topological\ninformation to facilitate robust knowledge transfer. MDGFM bridges different\ndomains by adaptively balancing features and topology while refining original\ngraphs to eliminate noise and align topological structures. To further enhance\nknowledge transfer, we introduce an efficient prompt-tuning approach. By\naligning topologies, MDGFM not only improves multi-domain pre-training but also\nenables robust knowledge transfer to unseen domains. Theoretical analyses\nprovide guarantees of MDGFM's effectiveness and domain generalization\ncapabilities. Extensive experiments on both homophilic and heterophilic graph\ndatasets validate the robustness and efficacy of our method."
    },
    {
        "date": "2025-02",
        "title": "Optimizing Spot Instance Reliability and Security Using Cloud-Native Data and Tools",
        "author": "Shubham Malhotra",
        "link": "http://arxiv.org/abs/2502.01966v1",
        "abstract": "This paper represents \"Cloudlab\", a comprehensive, cloud - native laboratory\ndesigned to support network security research and training. Built on Google\nCloud and adhering to GitOps methodologies, Cloudlab facilitates the the\ncreation, testing, and deployment of secure, containerized workloads using\nKubernetes and serverless architectures. The lab integrates tools like Palo\nAlto Networks firewalls, Bridgecrew for \"Security as Code,\" and automated\nGitHub workflows to establish a robust Continuous Integration/Continuous\nMachine Learning pipeline. By providing an adaptive and scalable environment,\nCloudlab supports advanced security concepts such as role-based access control,\nPolicy as Code, and container security. This initiative enables data scientists\nand engineers to explore cutting-edge practices in a dynamic cloud-native\necosystem, fostering innovation and improving operational resilience in modern\nIT infrastructures."
    },
    {
        "date": "2025-02",
        "title": "Query-Based and Unnoticeable Graph Injection Attack from Neighborhood Perspective",
        "author": "Chang Liu, Hai Huang, Yujie Xing, and Xingquan Zuo",
        "link": "http://arxiv.org/abs/2502.01936v1",
        "abstract": "The robustness of Graph Neural Networks (GNNs) has become an increasingly\nimportant topic due to their expanding range of applications. Various attack\nmethods have been proposed to explore the vulnerabilities of GNNs, ranging from\nGraph Modification Attacks (GMA) to the more practical and flexible Graph\nInjection Attacks (GIA). However, existing methods face two key challenges: (i)\ntheir reliance on surrogate models, which often leads to reduced attack\neffectiveness due to structural differences and prior biases, and (ii) existing\nGIA methods often sacrifice attack success rates in undefended settings to\nbypass certain defense models, thereby limiting their overall effectiveness. To\novercome these limitations, we propose QUGIA, a Query-based and Unnoticeable\nGraph Injection Attack. QUGIA injects nodes by first selecting edges based on\nvictim node connections and then generating node features using a Bayesian\nframework. This ensures that the injected nodes are similar to the original\ngraph nodes, implicitly preserving homophily and making the attack more\nunnoticeable. Unlike previous methods, QUGIA does not rely on surrogate models,\nthereby avoiding performance degradation and achieving better generalization.\nExtensive experiments on six real-world datasets with diverse characteristics\ndemonstrate that QUGIA achieves unnoticeable attacks and outperforms\nstate-of-the-art attackers. The code will be released upon acceptance."
    },
    {
        "date": "2025-02",
        "title": "Distributionally Robust Direct Preference Optimization",
        "author": "Zaiyan Xu, Sushil Vemuri, Kishan Panaganti, Dileep Kalathil, Rahul Jain, and Deepak Ramachandran",
        "link": "http://arxiv.org/abs/2502.01930v1",
        "abstract": "A major challenge in aligning large language models (LLMs) with human\npreferences is the issue of distribution shift. LLM alignment algorithms rely\non static preference datasets, assuming that they accurately represent\nreal-world user preferences. However, user preferences vary significantly\nacross geographical regions, demographics, linguistic patterns, and evolving\ncultural trends. This preference distribution shift leads to catastrophic\nalignment failures in many real-world applications. We address this problem\nusing the principled framework of distributionally robust optimization, and\ndevelop two novel distributionally robust direct preference optimization (DPO)\nalgorithms, namely, Wasserstein DPO (WDPO) and Kullback-Leibler DPO (KLDPO). We\ncharacterize the sample complexity of learning the optimal policy parameters\nfor WDPO and KLDPO. Moreover, we propose scalable gradient descent-style\nlearning algorithms by developing suitable approximations for the challenging\nminimax loss functions of WDPO and KLDPO. Our empirical experiments demonstrate\nthe superior performance of WDPO and KLDPO in substantially improving the\nalignment when there is a preference distribution shift."
    },
    {
        "date": "2025-02",
        "title": "INTACT: Inducing Noise Tolerance through Adversarial Curriculum Training for LiDAR-based Safety-Critical Perception and Autonomy",
        "author": "Nastaran Darabi, Divake Kumar, Sina Tayebati, and Amit Ranjan Trivedi",
        "link": "http://arxiv.org/abs/2502.01896v1",
        "abstract": "In this work, we present INTACT, a novel two-phase framework designed to\nenhance the robustness of deep neural networks (DNNs) against noisy LiDAR data\nin safety-critical perception tasks. INTACT combines meta-learning with\nadversarial curriculum training (ACT) to systematically address challenges\nposed by data corruption and sparsity in 3D point clouds. The meta-learning\nphase equips a teacher network with task-agnostic priors, enabling it to\ngenerate robust saliency maps that identify critical data regions. The ACT\nphase leverages these saliency maps to progressively expose a student network\nto increasingly complex noise patterns, ensuring targeted perturbation and\nimproved noise resilience. INTACT's effectiveness is demonstrated through\ncomprehensive evaluations on object detection, tracking, and classification\nbenchmarks using diverse datasets, including KITTI, Argoverse, and ModelNet40.\nResults indicate that INTACT improves model robustness by up to 20% across all\ntasks, outperforming standard adversarial and curriculum training methods. This\nframework not only addresses the limitations of conventional training\nstrategies but also offers a scalable and efficient solution for real-world\ndeployment in resource-constrained safety-critical systems. INTACT's principled\nintegration of meta-learning and adversarial training establishes a new\nparadigm for noise-tolerant 3D perception in safety-critical applications.\nINTACT improved KITTI Multiple Object Tracking Accuracy (MOTA) by 9.6% (64.1%\n-> 75.1%) and by 12.4% under Gaussian noise (52.5% -> 73.7%). Similarly, KITTI\nmean Average Precision (mAP) rose from 59.8% to 69.8% (50% point drop) and\n49.3% to 70.9% (Gaussian noise), highlighting the framework's ability to\nenhance deep learning model resilience in safety-critical object tracking\nscenarios."
    },
    {
        "date": "2025-02",
        "title": "A Privacy-Preserving Domain Adversarial Federated learning for multi-site brain functional connectivity analysis",
        "author": "Yipu Zhang, Likai Wang, Kuan-Jui Su, Aiying Zhang, Hao Zhu, Xiaowen Liu, Hui Shen, Vince D. Calhoun, Yuping Wang, and Hongwen Deng",
        "link": "http://arxiv.org/abs/2502.01885v1",
        "abstract": "Resting-state functional magnetic resonance imaging (rs-fMRI) and its derived\nfunctional connectivity networks (FCNs) have become critical for understanding\nneurological disorders. However, collaborative analyses and the\ngeneralizability of models still face significant challenges due to privacy\nregulations and the non-IID (non-independent and identically distributed)\nproperty of multiple data sources. To mitigate these difficulties, we propose\nDomain Adversarial Federated Learning (DAFed), a novel federated deep learning\nframework specifically designed for non-IID fMRI data analysis in multi-site\nsettings. DAFed addresses these challenges through feature disentanglement,\ndecomposing the latent feature space into domain-invariant and domain-specific\ncomponents, to ensure robust global learning while preserving local data\nspecificity. Furthermore, adversarial training facilitates effective knowledge\ntransfer between labeled and unlabeled datasets, while a contrastive learning\nmodule enhances the global representation of domain-invariant features. We\nevaluated DAFed on the diagnosis of ASD and further validated its\ngeneralizability in the classification of AD, demonstrating its superior\nclassification accuracy compared to state-of-the-art methods. Additionally, an\nenhanced Score-CAM module identifies key brain regions and functional\nconnectivity significantly associated with ASD and MCI, respectively,\nuncovering shared neurobiological patterns across sites. These findings\nhighlight the potential of DAFed to advance multi-site collaborative research\nin neuroimaging while protecting data confidentiality."
    },
    {
        "date": "2025-02",
        "title": "Decoding FL Defenses: Systemization, Pitfalls, and Remedies",
        "author": "Momin Ahmad Khan, Virat Shejwalkar, Yasra Chandio, Amir Houmansadr, and Fatima Muhammad Anwar",
        "link": "http://arxiv.org/abs/2502.05211v1",
        "abstract": "While the community has designed various defenses to counter the threat of\npoisoning attacks in Federated Learning (FL), there are no guidelines for\nevaluating these defenses. These defenses are prone to subtle pitfalls in their\nexperimental setups that lead to a false sense of security, rendering them\nunsuitable for practical deployment. In this paper, we systematically\nunderstand, identify, and provide a better approach to address these\nchallenges. First, we design a comprehensive systemization of FL defenses along\nthree dimensions: i) how client updates are processed, ii) what the server\nknows, and iii) at what stage the defense is applied. Next, we thoroughly\nsurvey 50 top-tier defense papers and identify the commonly used components in\ntheir evaluation setups. Based on this survey, we uncover six distinct pitfalls\nand study their prevalence. For example, we discover that around 30% of these\nworks solely use the intrinsically robust MNIST dataset, and 40% employ\nsimplistic attacks, which may inadvertently portray their defense as robust.\nUsing three representative defenses as case studies, we perform a critical\nreevaluation to study the impact of the identified pitfalls and show how they\nlead to incorrect conclusions about robustness. We provide actionable\nrecommendations to help researchers overcome each pitfall."
    },
    {
        "date": "2025-02",
        "title": "Reliability-Driven LiDAR-Camera Fusion for Robust 3D Object Detection",
        "author": "Reza Sadeghian, Niloofar Hooshyaripour, Chris Joslin, and WonSook Lee",
        "link": "http://arxiv.org/abs/2502.01856v1",
        "abstract": "Accurate and robust 3D object detection is essential for autonomous driving,\nwhere fusing data from sensors like LiDAR and camera enhances detection\naccuracy. However, sensor malfunctions such as corruption or disconnection can\ndegrade performance, and existing fusion models often struggle to maintain\nreliability when one modality fails. To address this, we propose ReliFusion, a\nnovel LiDAR-camera fusion framework operating in the bird's-eye view (BEV)\nspace. ReliFusion integrates three key components: the Spatio-Temporal Feature\nAggregation (STFA) module, which captures dependencies across frames to\nstabilize predictions over time; the Reliability module, which assigns\nconfidence scores to quantify the dependability of each modality under\nchallenging conditions; and the Confidence-Weighted Mutual Cross-Attention\n(CW-MCA) module, which dynamically balances information from LiDAR and camera\nmodalities based on these confidence scores. Experiments on the nuScenes\ndataset show that ReliFusion significantly outperforms state-of-the-art\nmethods, achieving superior robustness and accuracy in scenarios with limited\nLiDAR fields of view and severe sensor malfunctions."
    },
    {
        "date": "2025-02",
        "title": "Security and Quality in LLM-Generated Code: A Multi-Language, Multi-Model Analysis",
        "author": "Mohammed Kharma, Soohyeon Choi, Mohammed AlKhanafseh, and David Mohaisen",
        "link": "http://arxiv.org/abs/2502.01853v1",
        "abstract": "Artificial Intelligence (AI)-driven code generation tools are increasingly\nused throughout the software development lifecycle to accelerate coding tasks.\nHowever, the security of AI-generated code using Large Language Models (LLMs)\nremains underexplored, with studies revealing various risks and weaknesses.\nThis paper analyzes the security of code generated by LLMs across different\nprogramming languages. We introduce a dataset of 200 tasks grouped into six\ncategories to evaluate the performance of LLMs in generating secure and\nmaintainable code. Our research shows that while LLMs can automate code\ncreation, their security effectiveness varies by language. Many models fail to\nutilize modern security features in recent compiler and toolkit updates, such\nas Java 17. Moreover, outdated methods are still commonly used, particularly in\nC++. This highlights the need for advancing LLMs to enhance security and\nquality while incorporating emerging best practices in programming languages."
    },
    {
        "date": "2025-02",
        "title": "Preparing for Kyber in Securing Intelligent Transportation Systems Communications: A Case Study on Fault-Enabled Chosen-Ciphertext Attack",
        "author": "Kaiyuan Zhang, M Sabbir Salek, Antian Wang, Mizanur Rahman, Mashrur Chowdhury, and Yingjie Lao",
        "link": "http://arxiv.org/abs/2502.01848v1",
        "abstract": "Intelligent transportation systems (ITS) are characterized by wired or\nwireless communication among different entities, such as vehicles, roadside\ninfrastructure, and traffic management infrastructure. These communications\ndemand different levels of security, depending on how sensitive the data is.\nThe national ITS reference architecture (ARC-IT) defines three security levels,\ni.e., high, moderate, and low-security levels, based on the different security\nrequirements of ITS applications. In this study, we present a generalized\napproach to secure ITS communications using a standardized key encapsulation\nmechanism, known as Kyber, designed for post-quantum cryptography (PQC). We\nmodified the encryption and decryption systems for ITS communications while\nmapping the security levels of ITS applications to the three versions of Kyber,\ni.e., Kyber-512, Kyber-768, and Kyber-1024. Then, we conducted a case study\nusing a benchmark fault-enabled chosen-ciphertext attack to evaluate the\nsecurity provided by the different Kyber versions. The encryption and\ndecryption times observed for different Kyber security levels and the total\nnumber of iterations required to recover the secret key using the\nchosen-ciphertext attack are presented. Our analyses show that higher security\nlevels increase the time required for a successful attack, with Kyber-512 being\nbreached in 183 seconds, Kyber-768 in 337 seconds, and Kyber-1024 in 615\nseconds. In addition, attack time instabilities are observed for Kyber-512,\n768, and 1024 under 5,000, 6,000, and 8,000 inequalities, respectively. The\nrelationships among the different Kyber versions, and the respective attack\nrequirements and performances underscore the ITS communication security Kyber\ncould provide in the PQC era."
    },
    {
        "date": "2025-02",
        "title": "Efficient Denial of Service Attack Detection in IoT using Kolmogorov-Arnold Networks",
        "author": "Oleksandr Kuznetsov",
        "link": "http://arxiv.org/abs/2502.01835v1",
        "abstract": "The proliferation of Internet of Things (IoT) devices has created a pressing\nneed for efficient security solutions, particularly against Denial of Service\n(DoS) attacks. While existing detection approaches demonstrate high accuracy,\nthey often require substantial computational resources, making them impractical\nfor IoT deployment. This paper introduces a novel lightweight approach to DoS\nattack detection based on Kolmogorov-Arnold Networks (KANs). By leveraging\nspline-based transformations instead of traditional weight matrices, our\nsolution achieves state-of-the-art detection performance while maintaining\nminimal resource requirements. Experimental evaluation on the CICIDS2017\ndataset demonstrates 99.0% detection accuracy with only 0.19 MB memory\nfootprint and 2.00 ms inference time per sample. Compared to existing\nsolutions, KAN reduces memory requirements by up to 98% while maintaining\ncompetitive detection rates. The model's linear computational complexity\nensures efficient scaling with input size, making it particularly suitable for\nlarge-scale IoT deployments. We provide comprehensive performance comparisons\nwith recent approaches and demonstrate effectiveness across various DoS attack\npatterns. Our solution addresses the critical challenge of implementing\nsophisticated attack detection on resource-constrained devices, offering a\npractical approach to enhancing IoT security without compromising computational\nefficiency."
    },
    {
        "date": "2025-02",
        "title": "Firewalls to Secure Dynamic LLM Agentic Networks",
        "author": "Sahar Abdelnabi, Amr Gomaa, Eugene Bagdasarian, Per Ola Kristensson, and Reza Shokri",
        "link": "http://arxiv.org/abs/2502.01822v1",
        "abstract": "Future LLM agents are likely to communicate on behalf of users with other\nentity-representing agents on tasks that entail long-horizon plans with\ninterdependent goals. Current work does not focus on such agentic networks, nor\ndoes it address their challenges. Thus, we first identify the required\nproperties of agents' communication, which should be proactive and adaptable.\nIt needs to satisfy 1) privacy: agents should not share more than what is\nneeded for the task, and 2) security: the communication must preserve integrity\nand maintain utility against selfish entities. We design a use case (travel\nplanning) as a testbed that exemplifies these requirements, and we show\nexamples of how this can go wrong. Next, we propose a practical design,\ninspired by established network security principles, for constrained LLM\nagentic networks that balance adaptability, security, and privacy. Our\nframework automatically constructs and updates task-specific rules from prior\nsimulations to build firewalls. We offer layers of defense to 1) convert\nfree-form input to a task-specific protocol, 2) dynamically abstract users'\ndata to a task-specific degree of permissiveness, and 3) self-correct the\nagents' trajectory."
    },
    {
        "date": "2025-02",
        "title": "CTC-DRO: Robust Optimization for Reducing Language Disparities in Speech Recognition",
        "author": "Martijn Bartelds, Ananjan Nandi, Moussa Koulako Bala Doumbouya, Dan Jurafsky, Tatsunori Hashimoto, and Karen Livescu",
        "link": "http://arxiv.org/abs/2502.01777v1",
        "abstract": "Modern deep learning models often achieve high overall performance, but\nconsistently fail on specific subgroups. Group distributionally robust\noptimization (group DRO) addresses this problem by minimizing the worst-group\nloss, but it fails when group losses misrepresent performance differences\nbetween groups. This is common in domains like speech, where the widely used\nconnectionist temporal classification (CTC) loss scales with input length and\nvaries with linguistic and acoustic properties, leading to spurious differences\nbetween group losses. We present CTC-DRO, which addresses the shortcomings of\nthe group DRO objective by smoothing the group weight update to prevent\noveremphasis on consistently high-loss groups, while using input length-matched\nbatching to mitigate CTC's scaling issues. We evaluate CTC-DRO on the task of\nmultilingual automatic speech recognition (ASR) across five language sets from\nthe ML-SUPERB 2.0 benchmark. CTC-DRO consistently outperforms group DRO and\nCTC-based baseline models, reducing the worst-language error by up to 65.9% and\nthe average error by up to 47.7%. CTC-DRO can be applied to ASR with minimal\ncomputational costs, and offers the potential for reducing group disparities in\nother domains with similar challenges."
    },
    {
        "date": "2025-02",
        "title": "Robust Federated Finetuning of LLMs via Alternating Optimization of LoRA",
        "author": "Shuangyi Chen, Yuanxin Guo, Yue Ju, Harik Dalal, and Ashish Khisti",
        "link": "http://arxiv.org/abs/2502.01755v1",
        "abstract": "Parameter-Efficient Fine-Tuning (PEFT) methods like Low-Rank Adaptation\n(LoRA) optimize federated training by reducing computational and communication\ncosts. We propose RoLoRA, a federated framework using alternating optimization\nto fine-tune LoRA adapters. Our approach emphasizes the importance of learning\nup and down projection matrices to enhance expressiveness and robustness. We\nuse both theoretical analysis and extensive experiments to demonstrate the\nadvantages of RoLoRA over prior approaches that either generate imperfect model\nupdates or limit expressiveness of the model. We present theoretical analysis\non a simplified linear model to demonstrate the importance of learning both\ndown-projection and up-projection matrices in LoRA. We provide extensive\nexperimental evaluations on a toy neural network on MNIST as well as large\nlanguage models including RoBERTa-Large, Llama-2-7B on diverse tasks to\ndemonstrate the advantages of RoLoRA over other methods."
    },
    {
        "date": "2025-02",
        "title": "Model Tampering Attacks Enable More Rigorous Evaluations of LLM Capabilities",
        "author": "Zora Che, Stephen Casper, Robert Kirk, Anirudh Satheesh, Stewart Slocum, Lev E McKinney, Rohit Gandikota, Aidan Ewart, Domenic Rosati, Zichu Wu, Zikui Cai, Bilal Chughtai, Yarin Gal, Furong Huang, and Dylan Hadfield-Menell",
        "link": "http://arxiv.org/abs/2502.05209v1",
        "abstract": "Evaluations of large language model (LLM) risks and capabilities are\nincreasingly being incorporated into AI risk management and governance\nframeworks. Currently, most risk evaluations are conducted by designing inputs\nthat elicit harmful behaviors from the system. However, a fundamental\nlimitation of this approach is that the harmfulness of the behaviors identified\nduring any particular evaluation can only lower bound the model's\nworst-possible-case behavior. As a complementary method for eliciting harmful\nbehaviors, we propose evaluating LLMs with model tampering attacks which allow\nfor modifications to latent activations or weights. We pit state-of-the-art\ntechniques for removing harmful LLM capabilities against a suite of 5\ninput-space and 6 model tampering attacks. In addition to benchmarking these\nmethods against each other, we show that (1) model resilience to capability\nelicitation attacks lies on a low-dimensional robustness subspace; (2) the\nattack success rate of model tampering attacks can empirically predict and\noffer conservative estimates for the success of held-out input-space attacks;\nand (3) state-of-the-art unlearning methods can easily be undone within 16\nsteps of fine-tuning. Together these results highlight the difficulty of\nremoving harmful LLM capabilities and show that model tampering attacks enable\nsubstantially more rigorous evaluations than input-space attacks alone. We\nrelease models at https://huggingface.co/LLM-GAT"
    },
    {
        "date": "2025-02",
        "title": "Adversarial Reasoning at Jailbreaking Time",
        "author": "Mahdi Sabbaghi, Paul Kassianik, George Pappas, Yaron Singer, Amin Karbasi, and Hamed Hassani",
        "link": "http://arxiv.org/abs/2502.01633v1",
        "abstract": "As large language models (LLMs) are becoming more capable and widespread, the\nstudy of their failure cases is becoming increasingly important. Recent\nadvances in standardizing, measuring, and scaling test-time compute suggest new\nmethodologies for optimizing models to achieve high performance on hard tasks.\nIn this paper, we apply these advances to the task of model jailbreaking:\neliciting harmful responses from aligned LLMs. We develop an adversarial\nreasoning approach to automatic jailbreaking via test-time computation that\nachieves SOTA attack success rates (ASR) against many aligned LLMs, even the\nones that aim to trade inference-time compute for adversarial robustness. Our\napproach introduces a new paradigm in understanding LLM vulnerabilities, laying\nthe foundation for the development of more robust and trustworthy AI systems."
    },
    {
        "date": "2025-02",
        "title": "Robust-LLaVA: On the Effectiveness of Large-Scale Robust Image Encoders for Multi-modal Large Language Models",
        "author": "Hashmat Shadab Malik, Fahad Shamshad, Muzammal Naseer, Karthik Nandakumar, Fahad Khan, and Salman Khan",
        "link": "http://arxiv.org/abs/2502.01576v1",
        "abstract": "Multi-modal Large Language Models (MLLMs) excel in vision-language tasks but\nremain vulnerable to visual adversarial perturbations that can induce\nhallucinations, manipulate responses, or bypass safety mechanisms. Existing\nmethods seek to mitigate these risks by applying constrained adversarial\nfine-tuning to CLIP vision encoders on ImageNet-scale data, ensuring their\ngeneralization ability is preserved. However, this limited adversarial training\nrestricts robustness and broader generalization. In this work, we explore an\nalternative approach of leveraging existing vision classification models that\nhave been adversarially pre-trained on large-scale data. Our analysis reveals\ntwo principal contributions: (1) the extensive scale and diversity of\nadversarial pre-training enables these models to demonstrate superior\nrobustness against diverse adversarial threats, ranging from imperceptible\nperturbations to advanced jailbreaking attempts, without requiring additional\nadversarial training, and (2) end-to-end MLLM integration with these robust\nmodels facilitates enhanced adaptation of language components to robust visual\nfeatures, outperforming existing plug-and-play methodologies on complex\nreasoning tasks. Through systematic evaluation across visual\nquestion-answering, image captioning, and jail-break attacks, we demonstrate\nthat MLLMs trained with these robust models achieve superior adversarial\nrobustness while maintaining favorable clean performance. Our framework\nachieves 2x and 1.5x average robustness gains in captioning and VQA tasks,\nrespectively, and delivers over 10% improvement against jailbreak attacks. Code\nand pretrained models will be available at\nhttps://github.com/HashmatShadab/Robust-LLaVA."
    },
    {
        "date": "2025-02",
        "title": "Search-Based Adversarial Estimates for Improving Sample Efficiency in Off-Policy Reinforcement Learning",
        "author": "Federico Malato, and Ville Hautamaki",
        "link": "http://arxiv.org/abs/2502.01558v1",
        "abstract": "Sample inefficiency is a long-lasting challenge in deep reinforcement\nlearning (DRL). Despite dramatic improvements have been made, the problem is\nfar from being solved and is especially challenging in environments with sparse\nor delayed rewards. In our work, we propose to use Adversarial Estimates as a\nnew, simple and efficient approach to mitigate this problem for a class of\nfeedback-based DRL algorithms. Our approach leverages latent similarity search\nfrom a small set of human-collected trajectories to boost learning, using only\nfive minutes of human-recorded experience. The results of our study show\nalgorithms trained with Adversarial Estimates converge faster than their\noriginal version. Moreover, we discuss how our approach could enable learning\nin feedback-based algorithms in extreme scenarios with very sparse rewards."
    },
    {
        "date": "2025-02",
        "title": "Mitigation of Camouflaged Adversarial Attacks in Autonomous Vehicles--A Case Study Using CARLA Simulator",
        "author": "Yago Romano Martinez, Brady Carter, Abhijeet Solanki, Wesam Al Amiri, Syed Rafay Hasan, and Terry N. Guo",
        "link": "http://arxiv.org/abs/2502.05208v1",
        "abstract": "Autonomous vehicles (AVs) rely heavily on cameras and artificial intelligence\n(AI) to make safe and accurate driving decisions. However, since AI is the core\nenabling technology, this raises serious cyber threats that hinder the\nlarge-scale adoption of AVs. Therefore, it becomes crucial to analyze the\nresilience of AV security systems against sophisticated attacks that manipulate\ncamera inputs, deceiving AI models. In this paper, we develop\ncamera-camouflaged adversarial attacks targeting traffic sign recognition (TSR)\nin AVs. Specifically, if the attack is initiated by modifying the texture of a\nstop sign to fool the AV's object detection system, thereby affecting the AV\nactuators. The attack's effectiveness is tested using the CARLA AV simulator\nand the results show that such an attack can delay the auto-braking response to\nthe stop sign, resulting in potential safety issues. We conduct extensive\nexperiments under various conditions, confirming that our new attack is\neffective and robust. Additionally, we address the attack by presenting\nmitigation strategies. The proposed attack and defense methods are applicable\nto other end-to-end trained autonomous cyber-physical systems."
    },
    {
        "date": "2025-02",
        "title": "mWhisper-Flamingo for Multilingual Audio-Visual Noise-Robust Speech Recognition",
        "author": "Andrew Rouditchenko, Samuel Thomas, Hilde Kuehne, Rogerio Feris, and James Glass",
        "link": "http://arxiv.org/abs/2502.01547v2",
        "abstract": "Audio-Visual Speech Recognition (AVSR) combines lip-based video with audio\nand can improve performance in noise, but most methods are trained only on\nEnglish data. One limitation is the lack of large-scale multilingual video\ndata, which makes it hard hard to train models from scratch. In this work, we\npropose mWhisper-Flamingo for multilingual AVSR which combines the strengths of\na pre-trained audio model (Whisper) and video model (AV-HuBERT). To enable\nbetter multi-modal integration and improve the noisy multilingual performance,\nwe introduce decoder modality dropout where the model is trained both on paired\naudio-visual inputs and separate audio/visual inputs. mWhisper-Flamingo\nachieves state-of-the-art WER on MuAViC, an AVSR dataset of 9 languages.\nAudio-visual mWhisper-Flamingo consistently outperforms audio-only Whisper on\nall languages in noisy conditions."
    },
    {
        "date": "2025-02",
        "title": "A Multi-Scale Feature Fusion Framework Integrating Frequency Domain and Cross-View Attention for Dual-View X-ray Security Inspections",
        "author": "Shilong Hong, Yanzhou Zhou, and Weichao Xu",
        "link": "http://arxiv.org/abs/2502.01710v2",
        "abstract": "With the rapid development of modern transportation systems and the\nexponential growth of logistics volumes, intelligent X-ray-based security\ninspection systems play a crucial role in public safety. Although single-view\nX-ray equipment is widely deployed, it struggles to accurately identify\ncontraband in complex stacking scenarios due to strong viewpoint dependency and\ninadequate feature representation. To address this, we propose an innovative\nmulti-scale interactive feature fusion framework tailored for dual-view X-ray\nsecurity inspection image classification. The framework comprises three core\nmodules: the Frequency Domain Interaction Module (FDIM) enhances\nfrequency-domain features through Fourier transform; the Multi-Scale Cross-View\nFeature Enhancement (MSCFE) leverages cross-view attention mechanisms to\nstrengthen feature interactions; and the Convolutional Attention Fusion Module\n(CAFM) efficiently fuses features by integrating channel attention with\ndepthwise-separable convolutions. Experimental results demonstrate that our\nmethod outperforms existing state-of-the-art approaches across multiple\nbackbone architectures, particularly excelling in complex scenarios with\nocclusions and object stacking."
    },
    {
        "date": "2025-02",
        "title": "Topic-FlipRAG: Topic-Orientated Adversarial Opinion Manipulation Attacks to Retrieval-Augmented Generation Models",
        "author": "Yuyang Gong, Zhuo Chen, Miaokun Chen, Fengchang Yu, Wei Lu, Xiaofeng Wang, Xiaozhong Liu, and Jiawei Liu",
        "link": "http://arxiv.org/abs/2502.01386v1",
        "abstract": "Retrieval-Augmented Generation (RAG) systems based on Large Language Models\n(LLMs) have become essential for tasks such as question answering and content\ngeneration. However, their increasing impact on public opinion and information\ndissemination has made them a critical focus for security research due to\ninherent vulnerabilities. Previous studies have predominantly addressed attacks\ntargeting factual or single-query manipulations. In this paper, we address a\nmore practical scenario: topic-oriented adversarial opinion manipulation\nattacks on RAG models, where LLMs are required to reason and synthesize\nmultiple perspectives, rendering them particularly susceptible to systematic\nknowledge poisoning. Specifically, we propose Topic-FlipRAG, a two-stage\nmanipulation attack pipeline that strategically crafts adversarial\nperturbations to influence opinions across related queries. This approach\ncombines traditional adversarial ranking attack techniques and leverages the\nextensive internal relevant knowledge and reasoning capabilities of LLMs to\nexecute semantic-level perturbations. Experiments show that the proposed\nattacks effectively shift the opinion of the model's outputs on specific\ntopics, significantly impacting user information perception. Current mitigation\nmethods cannot effectively defend against such attacks, highlighting the\nnecessity for enhanced safeguards for RAG systems, and offering crucial\ninsights for LLM security research."
    },
    {
        "date": "2025-02",
        "title": "Metric Privacy in Federated Learning for Medical Imaging: Improving Convergence and Preventing Client Inference Attacks",
        "author": "Judith S\u00e1inz-Pardo D\u00edaz, Andreas Athanasiou, Kangsoo Jung, Catuscia Palamidessi, and \u00c1lvaro L\u00f3pez Garc\u00eda",
        "link": "http://arxiv.org/abs/2502.01352v1",
        "abstract": "Federated learning is a distributed learning technique that allows training a\nglobal model with the participation of different data owners without the need\nto share raw data. This architecture is orchestrated by a central server that\naggregates the local models from the clients. This server may be trusted, but\nnot all nodes in the network. Then, differential privacy (DP) can be used to\nprivatize the global model by adding noise. However, this may affect\nconvergence across the rounds of the federated architecture, depending also on\nthe aggregation strategy employed. In this work, we aim to introduce the notion\nof metric-privacy to mitigate the impact of classical server side global-DP on\nthe convergence of the aggregated model. Metric-privacy is a relaxation of DP,\nsuitable for domains provided with a notion of distance. We apply it from the\nserver side by computing a distance for the difference between the local\nmodels. We compare our approach with standard DP by analyzing the impact on six\nclassical aggregation strategies. The proposed methodology is applied to an\nexample of medical imaging and different scenarios are simulated across\nhomogeneous and non-i.i.d clients. Finally, we introduce a novel client\ninference attack, where a semi-honest client tries to find whether another\nclient participated in the training and study how it can be mitigated using DP\nand metric-privacy. Our evaluation shows that metric-privacy can increase the\nperformance of the model compared to standard DP, while offering similar\nprotection against client inference attacks."
    },
    {
        "date": "2025-02",
        "title": "A Statistical Learning Perspective on Semi-dual Adversarial Neural Optimal Transport Solvers",
        "author": "Roman Tarasov, Petr Mokrov, Milena Gazdieva, Evgeny Burnaev, and Alexander Korotin",
        "link": "http://arxiv.org/abs/2502.01310v1",
        "abstract": "Neural network based Optimal Transport (OT) is a recent and fruitful\ndirection in the generative modeling community. It finds its applications in\nvarious fields such as domain translation, image super-resolution,\ncomputational biology and others. Among the existing approaches to OT, of\nconsiderable interest are adversarial minimax solvers based on semi-dual\nformulations of OT problems. While promising, these methods lack theoretical\ninvestigation from a statistical learning perspective. Our work fills this gap\nby establishing upper bounds on the generalization error of an approximate OT\nmap recovered by the minimax quadratic OT solver. Importantly, the bounds we\nderive depend solely on some standard statistical and mathematical properties\nof the considered functional classes (neural networks). While our analysis\nfocuses on the quadratic OT, we believe that similar bounds could be derived\nfor more general OT formulations, paving the promising direction for future\nresearch."
    },
    {
        "date": "2025-02",
        "title": "Boosting Graph Robustness Against Backdoor Attacks: An Over-Similarity Perspective",
        "author": "Chang Liu, Hai Huang, Yujie Xing, and Xingquan Zuo",
        "link": "http://arxiv.org/abs/2502.01272v1",
        "abstract": "Graph Neural Networks (GNNs) have achieved notable success in tasks such as\nsocial and transportation networks. However, recent studies have highlighted\nthe vulnerability of GNNs to backdoor attacks, raising significant concerns\nabout their reliability in real-world applications. Despite initial efforts to\ndefend against specific graph backdoor attacks, existing defense methods face\ntwo main challenges: either the inability to establish a clear distinction\nbetween triggers and clean nodes, resulting in the removal of many clean nodes,\nor the failure to eliminate the impact of triggers, making it challenging to\nrestore the target nodes to their pre-attack state. Through empirical analysis\nof various existing graph backdoor attacks, we observe that the triggers\ngenerated by these methods exhibit over-similarity in both features and\nstructure. Based on this observation, we propose a novel graph backdoor defense\nmethod SimGuard. We first utilizes a similarity-based metric to detect triggers\nand then employs contrastive learning to train a backdoor detector that\ngenerates embeddings capable of separating triggers from clean nodes, thereby\nimproving detection efficiency. Extensive experiments conducted on real-world\ndatasets demonstrate that our proposed method effectively defends against\nvarious graph backdoor attacks while preserving performance on clean nodes. The\ncode will be released upon acceptance."
    },
    {
        "date": "2025-02",
        "title": "FSPGD: Rethinking Black-box Attacks on Semantic Segmentation",
        "author": "Eun-Sol Park, MiSo Park, Seung Park, and Yong-Goo Shin",
        "link": "http://arxiv.org/abs/2502.01262v1",
        "abstract": "Transferability, the ability of adversarial examples crafted for one model to\ndeceive other models, is crucial for black-box attacks. Despite advancements in\nattack methods for semantic segmentation, transferability remains limited,\nreducing their effectiveness in real-world applications. To address this, we\nintroduce the Feature Similarity Projected Gradient Descent (FSPGD) attack, a\nnovel black-box approach that enhances both attack performance and\ntransferability. Unlike conventional segmentation attacks that rely on output\npredictions for gradient calculation, FSPGD computes gradients from\nintermediate layer features. Specifically, our method introduces a loss\nfunction that targets local information by comparing features between clean\nimages and adversarial examples, while also disrupting contextual information\nby accounting for spatial relationships between objects. Experiments on Pascal\nVOC 2012 and Cityscapes datasets demonstrate that FSPGD achieves superior\ntransferability and attack performance, establishing a new state-of-the-art\nbenchmark. Code is available at https://github.com/KU-AIVS/FSPGD."
    },
    {
        "date": "2025-02",
        "title": "The dark deep side of DeepSeek: Fine-tuning attacks against the safety alignment of CoT-enabled models",
        "author": "Zhiyuan Xu, Joseph Gardiner, and Sana Belguith",
        "link": "http://arxiv.org/abs/2502.01225v1",
        "abstract": "Large language models are typically trained on vast amounts of data during\nthe pre-training phase, which may include some potentially harmful information.\nFine-tuning attacks can exploit this by prompting the model to reveal such\nbehaviours, leading to the generation of harmful content. In this paper, we\nfocus on investigating the performance of the Chain of Thought based reasoning\nmodel, DeepSeek, when subjected to fine-tuning attacks. Specifically, we\nexplore how fine-tuning manipulates the model's output, exacerbating the\nharmfulness of its responses while examining the interaction between the Chain\nof Thought reasoning and adversarial inputs. Through this study, we aim to shed\nlight on the vulnerability of Chain of Thought enabled models to fine-tuning\nattacks and the implications for their safety and ethical deployment."
    },
    {
        "date": "2025-02",
        "title": "On the Robustness of Temporal Factual Knowledge in Language Models",
        "author": "Hichem Ammar Khodja, Fr\u00e9d\u00e9ric B\u00e9chet, Quentin Brabant, Alexis Nasr, and Gw\u00e9nol\u00e9 Lecorv\u00e9",
        "link": "http://arxiv.org/abs/2502.01220v1",
        "abstract": "This paper explores the temporal robustness of language models (LMs) in\nhandling factual knowledge. While LMs can often complete simple factual\nstatements, their ability to manage temporal facts (those valid only within\nspecific timeframes) remains uncertain. We design a controlled experiment to\ntest the robustness of temporal factual knowledge inside LMs, which we use to\nevaluate several pretrained and instruction-tuned models using prompts on\npopular Wikidata facts, assessing their performance across different temporal\ngranularities (Day, Month, and Year). Our findings indicate that even very\nlarge state-of-the-art models, such as Llama-3.1-70B, vastly lack robust\nknowledge of temporal facts. In addition, they are incapable of generalizing\ntheir knowledge from one granularity to another. These results highlight the\ninherent limitations of using LMs as temporal knowledge bases. The source code\nand data to reproduce our experiments will be released."
    },
    {
        "date": "2025-02",
        "title": "Towards Robust and Reliable Concept Representations: Reliability-Enhanced Concept Embedding Model",
        "author": "Yuxuan Cai, Xiyu Wang, Satoshi Tsutsui, Winnie Pang, and Bihan Wen",
        "link": "http://arxiv.org/abs/2502.01191v1",
        "abstract": "Concept Bottleneck Models (CBMs) aim to enhance interpretability by\npredicting human-understandable concepts as intermediates for decision-making.\nHowever, these models often face challenges in ensuring reliable concept\nrepresentations, which can propagate to downstream tasks and undermine\nrobustness, especially under distribution shifts. Two inherent issues\ncontribute to concept unreliability: sensitivity to concept-irrelevant features\n(e.g., background variations) and lack of semantic consistency for the same\nconcept across different samples. To address these limitations, we propose the\nReliability-Enhanced Concept Embedding Model (RECEM), which introduces a\ntwo-fold strategy: Concept-Level Disentanglement to separate irrelevant\nfeatures from concept-relevant information and a Concept Mixup mechanism to\nensure semantic alignment across samples. These mechanisms work together to\nimprove concept reliability, enabling the model to focus on meaningful object\nattributes and generate faithful concept representations. Experimental results\ndemonstrate that RECEM consistently outperforms existing baselines across\nmultiple datasets, showing superior performance under background and domain\nshifts. These findings highlight the effectiveness of disentanglement and\nalignment strategies in enhancing both reliability and robustness in CBMs."
    },
    {
        "date": "2025-02",
        "title": "Enhancing Environmental Robustness in Few-shot Learning via Conditional Representation Learning",
        "author": "Qianyu Guo, Jingrong Wu, Tianxing Wu, Haofen Wang, Weifeng Ge, and Wenqiang Zhang",
        "link": "http://arxiv.org/abs/2502.01183v1",
        "abstract": "Few-shot learning (FSL) has recently been extensively utilized to overcome\nthe scarcity of training data in domain-specific visual recognition. In\nreal-world scenarios, environmental factors such as complex backgrounds,\nvarying lighting conditions, long-distance shooting, and moving targets often\ncause test images to exhibit numerous incomplete targets or noise disruptions.\nHowever, current research on evaluation datasets and methodologies has largely\nignored the concept of \"environmental robustness\", which refers to maintaining\nconsistent performance in complex and diverse physical environments. This\nneglect has led to a notable decline in the performance of FSL models during\npractical testing compared to their training performance. To bridge this gap,\nwe introduce a new real-world multi-domain few-shot learning (RD-FSL)\nbenchmark, which includes four domains and six evaluation datasets. The test\nimages in this benchmark feature various challenging elements, such as\ncamouflaged objects, small targets, and blurriness. Our evaluation experiments\nreveal that existing methods struggle to utilize training images effectively to\ngenerate accurate feature representations for challenging test images. To\naddress this problem, we propose a novel conditional representation learning\nnetwork (CRLNet) that integrates the interactions between training and testing\nimages as conditional information in their respective representation processes.\nThe main goal is to reduce intra-class variance or enhance inter-class variance\nat the feature representation level. Finally, comparative experiments reveal\nthat CRLNet surpasses the current state-of-the-art methods, achieving\nperformance improvements ranging from 6.83% to 16.98% across diverse settings\nand backbones. The source code and dataset are available at\nhttps://github.com/guoqianyu-alberta/Conditional-Representation-Learning."
    },
    {
        "date": "2025-02",
        "title": "Gradient Norm-based Fine-Tuning for Backdoor Defense in Automatic Speech Recognition",
        "author": "Nanjun Zhou, Weilin Lin, and Li Liu",
        "link": "http://arxiv.org/abs/2502.01152v1",
        "abstract": "Backdoor attacks have posed a significant threat to the security of deep\nneural networks (DNNs). Despite considerable strides in developing defenses\nagainst backdoor attacks in the visual domain, the specialized defenses for the\naudio domain remain empty. Furthermore, the defenses adapted from the visual to\naudio domain demonstrate limited effectiveness. To fill this gap, we propose\nGradient Norm-based FineTuning (GN-FT), a novel defense strategy against the\nattacks in the audio domain, based on the observation from the corresponding\nbackdoored models. Specifically, we first empirically find that the backdoored\nneurons exhibit greater gradient values compared to other neurons, while clean\nneurons stay the lowest. On this basis, we fine-tune the backdoored model by\nincorporating the gradient norm regularization, aiming to weaken and reduce the\nbackdoored neurons. We further approximate the loss computation for lower\nimplementation costs. Extensive experiments on two speech recognition datasets\nacross five models demonstrate the superior performance of our proposed method.\nTo the best of our knowledge, this work is the first specialized and effective\ndefense against backdoor attacks in the audio domain."
    },
    {
        "date": "2025-02",
        "title": "Towards Robust and Generalizable Lensless Imaging with Modular Learned Reconstruction",
        "author": "Eric Bezzam, Yohann Perron, and Martin Vetterli",
        "link": "http://arxiv.org/abs/2502.01102v1",
        "abstract": "Lensless cameras disregard the conventional design that imaging should mimic\nthe human eye. This is done by replacing the lens with a thin mask, and moving\nimage formation to the digital post-processing. State-of-the-art lensless\nimaging techniques use learned approaches that combine physical modeling and\nneural networks. However, these approaches make simplifying modeling\nassumptions for ease of calibration and computation. Moreover, the\ngeneralizability of learned approaches to lensless measurements of new masks\nhas not been studied. To this end, we utilize a modular learned reconstruction\nin which a key component is a pre-processor prior to image recovery. We\ntheoretically demonstrate the pre-processor's necessity for standard image\nrecovery techniques (Wiener filtering and iterative algorithms), and through\nextensive experiments show its effectiveness for multiple lensless imaging\napproaches and across datasets of different mask types (amplitude and phase).\nWe also perform the first generalization benchmark across mask types to\nevaluate how well reconstructions trained with one system generalize to others.\nOur modular reconstruction enables us to use pre-trained components and\ntransfer learning on new systems to cut down weeks of tedious measurements and\ntraining. As part of our work, we open-source four datasets, and software for\nmeasuring datasets and for training our modular reconstruction."
    },
    {
        "date": "2025-02",
        "title": "BC-GAN: A Generative Adversarial Network for Synthesizing a Batch of Collocated Clothing",
        "author": "Dongliang Zhou, Haijun Zhang, Jianghong Ma, and Jianyang Shi",
        "link": "http://arxiv.org/abs/2502.01080v1",
        "abstract": "Collocated clothing synthesis using generative networks has become an\nemerging topic in the field of fashion intelligence, as it has significant\npotential economic value to increase revenue in the fashion industry. In\nprevious studies, several works have attempted to synthesize\nvisually-collocated clothing based on a given clothing item using generative\nadversarial networks (GANs) with promising results. These works, however, can\nonly accomplish the synthesis of one collocated clothing item each time.\nNevertheless, users may require different clothing items to meet their multiple\nchoices due to their personal tastes and different dressing scenarios. To\naddress this limitation, we introduce a novel batch clothing generation\nframework, named BC-GAN, which is able to synthesize multiple\nvisually-collocated clothing images simultaneously. In particular, to further\nimprove the fashion compatibility of synthetic results, BC-GAN proposes a new\nfashion compatibility discriminator in a contrastive learning perspective by\nfully exploiting the collocation relationship among all clothing items. Our\nmodel was examined in a large-scale dataset with compatible outfits constructed\nby ourselves. Extensive experiment results confirmed the effectiveness of our\nproposed BC-GAN in comparison to state-of-the-art methods in terms of\ndiversity, visual authenticity, and fashion compatibility."
    },
    {
        "date": "2025-02",
        "title": "Adversarial Robustness in Two-Stage Learning-to-Defer: Algorithms and Guarantees",
        "author": "Yannis Montreuil, Axel Carlier, Lai Xing Ng, and Wei Tsang Ooi",
        "link": "http://arxiv.org/abs/2502.01027v1",
        "abstract": "Learning-to-Defer (L2D) facilitates optimal task allocation between AI\nsystems and decision-makers. Despite its potential, we show that current\ntwo-stage L2D frameworks are highly vulnerable to adversarial attacks, which\ncan misdirect queries or overwhelm decision agents, significantly degrading\nsystem performance. This paper conducts the first comprehensive analysis of\nadversarial robustness in two-stage L2D frameworks. We introduce two novel\nattack strategies -- untargeted and targeted -- that exploit inherent\nstructural vulnerabilities in these systems. To mitigate these threats, we\npropose SARD, a robust, convex, deferral algorithm rooted in Bayes and\n$(\\mathcal{R},\\mathcal{G})$-consistency. Our approach guarantees optimal task\nallocation under adversarial perturbations for all surrogates in the\ncross-entropy family. Extensive experiments on classification, regression, and\nmulti-task benchmarks validate the robustness of SARD."
    },
    {
        "date": "2025-02",
        "title": "Secure & Personalized Music-to-Video Generation via CHARCHA",
        "author": "Mehul Agarwal, Gauri Agarwal, Santiago Benoit, Andrew Lippman, and Jean Oh",
        "link": "http://arxiv.org/abs/2502.02610v1",
        "abstract": "Music is a deeply personal experience and our aim is to enhance this with a\nfully-automated pipeline for personalized music video generation. Our work\nallows listeners to not just be consumers but co-creators in the music video\ngeneration process by creating personalized, consistent and context-driven\nvisuals based on lyrics, rhythm and emotion in the music. The pipeline combines\nmultimodal translation and generation techniques and utilizes low-rank\nadaptation on listeners' images to create immersive music videos that reflect\nboth the music and the individual. To ensure the ethical use of users'\nidentity, we also introduce CHARCHA (patent pending), a facial identity\nverification protocol that protects people against unauthorized use of their\nface while at the same time collecting authorized images from users for\npersonalizing their videos. This paper thus provides a secure and innovative\nframework for creating deeply personalized music videos."
    },
    {
        "date": "2025-02",
        "title": "Detection of Distributed Denial of Service Attacks based on Machine Learning Algorithms",
        "author": "Md. Abdur Rahman",
        "link": "http://arxiv.org/abs/2502.00975v1",
        "abstract": "Distributed Denial of Service (DDoS) attacks make the challenges to provide\nthe services of the data resources to the web clients. In this paper, we\nconcern to study and apply different Machine Learning (ML) techniques to\nseparate the DDoS attack instances from benign instances. Our experimental\nresults show that forward and backward data bytes of our dataset are observed\nmore similar for DDoS attacks compared to the data bytes for benign attempts.\nThis paper uses different machine learning techniques for the detection of the\nattacks efficiently in order to make sure the offered services from web servers\navailable. This results from the proposed approach suggest that 97.1% of DDoS\nattacks are successfully detected by the Support Vector Machine (SVM). These\naccuracies are better while comparing to the several existing machine learning\napproaches."
    },
    {
        "date": "2025-02",
        "title": "AI-Powered Spearphishing Cyber Attacks: Fact or Fiction?",
        "author": "Matthew Kemp, Harsha Kalutarage, and M. Omar Al-Kadri",
        "link": "http://arxiv.org/abs/2502.00961v1",
        "abstract": "Due to society's continuing technological advance, the capabilities of\nmachine learning-based artificial intelligence systems continue to expand and\ninfluence a wider degree of topics. Alongside this expansion of technology,\nthere is a growing number of individuals willing to misuse these systems to\ndefraud and mislead others. Deepfake technology, a set of deep learning\nalgorithms that are capable of replacing the likeness or voice of one\nindividual with another with alarming accuracy, is one of these technologies.\nThis paper investigates the threat posed by malicious use of this technology,\nparticularly in the form of spearphishing attacks. It uses deepfake technology\nto create spearphishing-like attack scenarios and validate them against average\nindividuals. Experimental results show that 66% of participants failed to\nidentify AI created audio as fake while 43% failed to identify such videos as\nfake, confirming the growing fear of threats posed by the use of these\ntechnologies by cybercriminals."
    },
    {
        "date": "2025-02",
        "title": "SecPE: Secure Prompt Ensembling for Private and Robust Large Language Models",
        "author": "Jiawen Zhang, Kejia Chen, Zunlei Feng, Jian Lou, Mingli Song, Jian Liu, and Xiaohu Yang",
        "link": "http://arxiv.org/abs/2502.00847v1",
        "abstract": "With the growing popularity of LLMs among the general public users,\nprivacy-preserving and adversarial robustness have become two pressing demands\nfor LLM-based services, which have largely been pursued separately but rarely\njointly. In this paper, to the best of our knowledge, we are among the first\nattempts towards robust and private LLM inference by tightly integrating two\ndisconnected fields: private inference and prompt ensembling. The former\nprotects users' privacy by encrypting inference data transmitted and processed\nby LLMs, while the latter enhances adversarial robustness by yielding an\naggregated output from multiple prompted LLM responses. Although widely\nrecognized as effective individually, private inference for prompt ensembling\ntogether entails new challenges that render the naive combination of existing\ntechniques inefficient. To overcome the hurdles, we propose SecPE, which\ndesigns efficient fully homomorphic encryption (FHE) counterparts for the core\nalgorithmic building blocks of prompt ensembling. We conduct extensive\nexperiments on 8 tasks to evaluate the accuracy, robustness, and efficiency of\nSecPE. The results show that SecPE maintains high clean accuracy and offers\nbetter robustness at the expense of merely $2.5\\%$ efficiency overhead compared\nto baseline private inference methods, indicating a satisfactory\n``accuracy-robustness-efficiency'' tradeoff. For the efficiency of the\nencrypted Argmax operation that incurs major slowdown for prompt ensembling,\nSecPE is 35.4x faster than the state-of-the-art peers, which can be of\nindependent interest beyond this work."
    },
    {
        "date": "2025-02",
        "title": "Federated Generalised Variational Inference: A Robust Probabilistic Federated Learning Framework",
        "author": "Terje Mildner, Oliver Hamelijnck, Paris Giampouras, and Theodoros Damoulas",
        "link": "http://arxiv.org/abs/2502.00846v1",
        "abstract": "We introduce FedGVI, a probabilistic Federated Learning (FL) framework that\nis provably robust to both prior and likelihood misspecification. FedGVI\naddresses limitations in both frequentist and Bayesian FL by providing unbiased\npredictions under model misspecification, with calibrated uncertainty\nquantification. Our approach generalises previous FL approaches, specifically\nPartitioned Variational Inference (Ashman et al., 2022), by allowing robust and\nconjugate updates, decreasing computational complexity at the clients. We offer\ntheoretical analysis in terms of fixed-point convergence, optimality of the\ncavity distribution, and provable robustness. Additionally, we empirically\ndemonstrate the effectiveness of FedGVI in terms of improved robustness and\npredictive performance on multiple synthetic and real world classification data\nsets."
    },
    {
        "date": "2025-02",
        "title": "Activation Approximations Can Incur Safety Vulnerabilities Even in Aligned LLMs: Comprehensive Analysis and Defense",
        "author": "Jiawen Zhang, Kejia Chen, Lipeng He, Jian Lou, Dan Li, Zunlei Feng, Mingli Song, Jian Liu, Kui Ren, and Xiaohu Yang",
        "link": "http://arxiv.org/abs/2502.00840v1",
        "abstract": "Large Language Models (LLMs) have showcased remarkable capabilities across\nvarious domains. Accompanying the evolving capabilities and expanding\ndeployment scenarios of LLMs, their deployment challenges escalate due to their\nsheer scale and the advanced yet complex activation designs prevalent in\nnotable model series, such as Llama, Gemma, and Mistral. These challenges have\nbecome particularly pronounced in resource-constrained deployment scenarios,\nwhere mitigating inference efficiency bottlenecks is imperative. Among various\nrecent efforts, activation approximation has emerged as a promising avenue for\npursuing inference efficiency, sometimes considered indispensable in\napplications such as private inference. Despite achieving substantial speedups\nwith minimal impact on utility, even appearing sound and practical for\nreal-world deployment, the safety implications of activation approximations\nremain unclear. In this work, we fill this critical gap in LLM safety by\nconducting the first systematic safety evaluation of activation approximations.\nOur safety vetting spans seven sota techniques across three popular categories,\nrevealing consistent safety degradation across ten safety-aligned LLMs."
    },
    {
        "date": "2025-02",
        "title": "Boosting Adversarial Robustness and Generalization with Structural Prior",
        "author": "Zhichao Hou, Weizhi Gao, Hamid Krim, and Xiaorui Liu",
        "link": "http://arxiv.org/abs/2502.00834v1",
        "abstract": "This work investigates a novel approach to boost adversarial robustness and\ngeneralization by incorporating structural prior into the design of deep\nlearning models. Specifically, our study surprisingly reveals that existing\ndictionary learning-inspired convolutional neural networks (CNNs) provide a\nfalse sense of security against adversarial attacks. To address this, we\npropose Elastic Dictionary Learning Networks (EDLNets), a novel ResNet\narchitecture that significantly enhances adversarial robustness and\ngeneralization. This novel and effective approach is supported by a theoretical\nrobustness analysis using influence functions. Moreover, extensive and reliable\nexperiments demonstrate consistent and significant performance improvement on\nopen robustness leaderboards such as RobustBench, surpassing state-of-the-art\nbaselines. To the best of our knowledge, this is the first work to discover and\nvalidate that structural prior can reliably enhance deep learning robustness\nunder strong adaptive attacks, unveiling a promising direction for future\nresearch."
    },
    {
        "date": "2025-02",
        "title": "Adversarial Semantic Augmentation for Training Generative Adversarial Networks under Limited Data",
        "author": "Mengping Yang, Zhe Wang, Ziqiu Chi, Dongdong Li, and Wenli Du",
        "link": "http://arxiv.org/abs/2502.00800v1",
        "abstract": "Generative adversarial networks (GANs) have made remarkable achievements in\nsynthesizing images in recent years. Typically, training GANs requires massive\ndata, and the performance of GANs deteriorates significantly when training data\nis limited. To improve the synthesis performance of GANs in low-data regimes,\nexisting approaches use various data augmentation techniques to enlarge the\ntraining sets. However, it is identified that these augmentation techniques may\nleak or even alter the data distribution. To remedy this, we propose an\nadversarial semantic augmentation (ASA) technique to enlarge the training data\nat the semantic level instead of the image level. Concretely, considering\nsemantic features usually encode informative information of images, we estimate\nthe covariance matrices of semantic features for both real and generated images\nto find meaningful transformation directions. Such directions translate\noriginal features to another semantic representation, e.g., changing the\nbackgrounds or expressions of the human face dataset. Moreover, we derive an\nupper bound of the expected adversarial loss. By optimizing the upper bound,\nour semantic augmentation is implicitly achieved. Such design avoids redundant\nsampling of the augmented features and introduces negligible computation\noverhead, making our approach computation efficient. Extensive experiments on\nboth few-shot and large-scale datasets demonstrate that our method consistently\nimprove the synthesis quality under various data regimes, and further\nvisualized and analytic results suggesting satisfactory versatility of our\nproposed method."
    },
    {
        "date": "2025-02",
        "title": "From Compliance to Exploitation: Jailbreak Prompt Attacks on Multimodal LLMs",
        "author": "Chun Wai Chiu, Linghan Huang, Bo Li, and Huaming Chen",
        "link": "http://arxiv.org/abs/2502.00735v1",
        "abstract": "Large Language Models (LLMs) have seen widespread applications across various\ndomains due to their growing ability to process diverse types of input data,\nincluding text, audio, image and video. While LLMs have demonstrated\noutstanding performance in understanding and generating contexts for different\nscenarios, they are vulnerable to prompt-based attacks, which are mostly via\ntext input. In this paper, we introduce the first voice-based jailbreak attack\nagainst multimodal LLMs, termed as Flanking Attack, which can process different\ntypes of input simultaneously towards the multimodal LLMs. Our work is\nmotivated by recent advancements in monolingual voice-driven large language\nmodels, which have introduced new attack surfaces beyond traditional text-based\nvulnerabilities for LLMs. To investigate these risks, we examine the frontier\nmultimodal LLMs, which can be accessed via different types of inputs such as\naudio input, focusing on how adversarial prompts can bypass its defense\nmechanisms. We propose a novel strategy, in which the disallowed prompt is\nflanked by benign, narrative-driven prompts. It is integrated in the Flanking\nAttack which attempts to humanizes the interaction context and execute the\nattack through a fictional setting. To better evaluate the attack performance,\nwe present a semi-automated self-assessment framework for policy violation\ndetection. We demonstrate that Flank Attack is capable of manipulating\nstate-of-the-art LLMs into generating misaligned and forbidden outputs, which\nachieves an average attack success rate ranging from 0.67 to 0.93 across seven\nforbidden scenarios. These findings highlight both the potency of prompt-based\nobfuscation in voice-enabled contexts and the limitations of current LLMs'\nmoderation safeguards and the urgent need for advanced defense strategies to\naddress the challenges posed by evolving, context-rich attacks."
    },
    {
        "date": "2025-02",
        "title": "\"I am bad\": Interpreting Stealthy, Universal and Robust Audio Jailbreaks in Audio-Language Models",
        "author": "Isha Gupta, David Khachaturov, and Robert Mullins",
        "link": "http://arxiv.org/abs/2502.00718v1",
        "abstract": "The rise of multimodal large language models has introduced innovative\nhuman-machine interaction paradigms but also significant challenges in machine\nlearning safety. Audio-Language Models (ALMs) are especially relevant due to\nthe intuitive nature of spoken communication, yet little is known about their\nfailure modes. This paper explores audio jailbreaks targeting ALMs, focusing on\ntheir ability to bypass alignment mechanisms. We construct adversarial\nperturbations that generalize across prompts, tasks, and even base audio\nsamples, demonstrating the first universal jailbreaks in the audio modality,\nand show that these remain effective in simulated real-world conditions. Beyond\ndemonstrating attack feasibility, we analyze how ALMs interpret these audio\nadversarial examples and reveal them to encode imperceptible first-person toxic\nspeech - suggesting that the most effective perturbations for eliciting toxic\noutputs specifically embed linguistic features within the audio signal. These\nresults have important implications for understanding the interactions between\ndifferent modalities in multimodal models, and offer actionable insights for\nenhancing defenses against adversarial audio attacks."
    },
    {
        "date": "2025-02",
        "title": "DPBloomfilter: Securing Bloom Filters with Differential Privacy",
        "author": "Yekun Ke, Yingyu Liang, Zhizhou Sha, Zhenmei Shi, and Zhao Song",
        "link": "http://arxiv.org/abs/2502.00693v1",
        "abstract": "The Bloom filter is a simple yet space-efficient probabilistic data structure\nthat supports membership queries for dramatically large datasets. It is widely\nutilized and implemented across various industrial scenarios, often handling\nmassive datasets that include sensitive user information necessitating privacy\npreservation. To address the challenge of maintaining privacy within the Bloom\nfilter, we have developed the DPBloomfilter. This innovation integrates the\nclassical differential privacy mechanism, specifically the Random Response\ntechnique, into the Bloom filter, offering robust privacy guarantees under the\nsame running complexity as the standard Bloom filter. Through rigorous\nsimulation experiments, we have demonstrated that our DPBloomfilter algorithm\nmaintains high utility while ensuring privacy protections. To the best of our\nknowledge, this is the first work to provide differential privacy guarantees\nfor the Bloom filter for membership query problems."
    },
    {
        "date": "2025-02",
        "title": "Towards Robust Multimodal Large Language Models Against Jailbreak Attacks",
        "author": "Ziyi Yin, Yuanpu Cao, Han Liu, Ting Wang, Jinghui Chen, and Fenhlong Ma",
        "link": "http://arxiv.org/abs/2502.00653v1",
        "abstract": "While multimodal large language models (MLLMs) have achieved remarkable\nsuccess in recent advancements, their susceptibility to jailbreak attacks has\ncome to light. In such attacks, adversaries exploit carefully crafted prompts\nto coerce models into generating harmful or undesirable content. Existing\ndefense mechanisms often rely on external inference steps or safety alignment\ntraining, both of which are less effective and impractical when facing\nsophisticated adversarial perturbations in white-box scenarios. To address\nthese challenges and bolster MLLM robustness, we introduce SafeMLLM by adopting\nan adversarial training framework that alternates between an attack step for\ngenerating adversarial noise and a model updating step. At the attack step,\nSafeMLLM generates adversarial perturbations through a newly proposed\ncontrastive embedding attack (CoE-Attack), which optimizes token embeddings\nunder a contrastive objective. SafeMLLM then updates model parameters to\nneutralize the perturbation effects while preserving model utility on benign\ninputs. We evaluate SafeMLLM across six MLLMs and six jailbreak methods\nspanning multiple modalities. Experimental results show that SafeMLLM\neffectively defends against diverse attacks, maintaining robust performance and\nutilities."
    },
    {
        "date": "2025-02",
        "title": "Integrating Cybersecurity Frameworks into IT Security: A Comprehensive Analysis of Threat Mitigation Strategies and Adaptive Technologies",
        "author": "Amit Lokare, Shripad Bankar, and Padmajeet Mhaske",
        "link": "http://arxiv.org/abs/2502.00651v1",
        "abstract": "The cybersecurity threat landscape is constantly actively making it\nimperative to develop sound frameworks to protect the IT structures. Based on\nthis introduction, this paper aims to discuss the application of cybersecurity\nframeworks into the IT security with focus placed on the role of such\nframeworks in addressing the changing nature of cybersecurity threats. It\nexplores widely used models, including the NIST Cybersecurity Framework, Zero\nTrust Architecture, and the ISO/IEC 27001, and how they apply to industries\nincluding finance, healthcare and government. The discussion also singles out\nsuch technologies as Artificial Intelligence (AI) and Machine Learning (ML) as\nthe core for real-time threat detection and response mechanisms. As these\nintegration challenges demonstrate, the study provides tangible and proven\napproaches to tackle framework implementation issues such as legitimate\nsecurity issues, limited availability of funds and resources, and compliance\nwith legal requirements. By capturing current trends and exposures, the\nfindings promote strong, portfolio-based and risk-appropriate security\napproaches adjusted for organizational goals and capable to prevent advanced\ncyber threats."
    },
    {
        "date": "2025-02",
        "title": "TrojanTime: Backdoor Attacks on Time Series Classification",
        "author": "Chang Dong, Zechao Sun, Guangdong Bai, Shuying Piao, Weitong Chen, and Wei Emma Zhang",
        "link": "http://arxiv.org/abs/2502.00646v1",
        "abstract": "Time Series Classification (TSC) is highly vulnerable to backdoor attacks,\nposing significant security threats. Existing methods primarily focus on data\npoisoning during the training phase, designing sophisticated triggers to\nimprove stealthiness and attack success rate (ASR). However, in practical\nscenarios, attackers often face restrictions in accessing training data.\nMoreover, it is a challenge for the model to maintain generalization ability on\nclean test data while remaining vulnerable to poisoned inputs when data is\ninaccessible. To address these challenges, we propose TrojanTime, a novel\ntwo-step training algorithm. In the first stage, we generate a pseudo-dataset\nusing an external arbitrary dataset through target adversarial attacks. The\nclean model is then continually trained on this pseudo-dataset and its poisoned\nversion. To ensure generalization ability, the second stage employs a carefully\ndesigned training strategy, combining logits alignment and batch norm freezing.\nWe evaluate TrojanTime using five types of triggers across four TSC\narchitectures in UCR benchmark datasets from diverse domains. The results\ndemonstrate the effectiveness of TrojanTime in executing backdoor attacks while\nmaintaining clean accuracy. Finally, to mitigate this threat, we propose a\ndefensive unlearning strategy that effectively reduces the ASR while preserving\nclean accuracy."
    },
    {
        "date": "2025-02",
        "title": "DesCLIP: Robust Continual Adaptation via General Attribute Descriptions for Pretrained Vision-Language Models",
        "author": "Chiyuan He, Zihuan Qiu, Fanman Meng, Linfeng Xu, Qingbo Wu, and Hongliang Li",
        "link": "http://arxiv.org/abs/2502.00618v1",
        "abstract": "Continual adaptation of vision-language models (VLMs) focuses on leveraging\ncross-modal pretrained knowledge to incrementally adapt for expanding\ndownstream tasks and datasets, while tackling the challenge of knowledge\nforgetting. Existing research often focuses on connecting visual features with\nspecific class text in downstream tasks, overlooking the latent relationships\nbetween general and specialized knowledge. Our findings reveal that forcing\nmodels to optimize inappropriate visual-text matches exacerbates forgetting of\nVLMs. To tackle this issue, we propose DesCLIP, which leverages general\nattribute (GA) descriptions to guide the understanding of specific class\nobjects, enabling VLMs to establish robust \\textit{vision-GA-class} trilateral\nassociations rather than relying solely on \\textit{vision-class} connections.\nSpecifically, we introduce a language assistant to generate concrete GA\ndescription candidates via proper request prompts. Then, an anchor-based\nembedding filter is designed to obtain highly relevant GA description\nembeddings, which are leveraged as the paired text embeddings for\nvisual-textual instance matching, thereby tuning the visual encoder.\nCorrespondingly, the class text embeddings are gradually calibrated to align\nwith these shared GA description embeddings. Extensive experiments demonstrate\nthe advancements and efficacy of our proposed method, with comprehensive\nempirical evaluations highlighting its superior performance compared to\nexisting pretrained and VLM-based continual learning methods."
    },
    {
        "date": "2025-02",
        "title": "Robust Knowledge Distillation in Federated Learning: Counteracting Backdoor Attacks",
        "author": "Ebtisaam Alharbi, Leandro Soriano Marcolino, Qiang Ni, and Antonios Gouglidis",
        "link": "http://arxiv.org/abs/2502.00587v1",
        "abstract": "Federated Learning (FL) enables collaborative model training across multiple\ndevices while preserving data privacy. However, it remains susceptible to\nbackdoor attacks, where malicious participants can compromise the global model.\nExisting defence methods are limited by strict assumptions on data\nheterogeneity (Non-Independent and Identically Distributed data) and the\nproportion of malicious clients, reducing their practicality and effectiveness.\nTo overcome these limitations, we propose Robust Knowledge Distillation (RKD),\na novel defence mechanism that enhances model integrity without relying on\nrestrictive assumptions. RKD integrates clustering and model selection\ntechniques to identify and filter out malicious updates, forming a reliable\nensemble of models. It then employs knowledge distillation to transfer the\ncollective insights from this ensemble to a global model. Extensive evaluations\ndemonstrate that RKD effectively mitigates backdoor threats while maintaining\nhigh model performance, outperforming current state-of-the-art defence methods\nacross various scenarios."
    },
    {
        "date": "2025-02",
        "title": "Defense Against the Dark Prompts: Mitigating Best-of-N Jailbreaking with Prompt Evaluation",
        "author": "Stuart Armstrong, Matija Franklin, Connor Stevens, and Rebecca Gorman",
        "link": "http://arxiv.org/abs/2502.00580v1",
        "abstract": "Recent work showed Best-of-N (BoN) jailbreaking using repeated use of random\naugmentations (such as capitalization, punctuation, etc) is effective against\nall major large language models (LLMs). We have found that $100\\%$ of the BoN\npaper's successful jailbreaks (confidence interval $[99.65\\%, 100.00\\%]$) and\n$99.8\\%$ of successful jailbreaks in our replication (confidence interval\n$[99.28\\%, 99.98\\%]$) were blocked with our Defense Against The Dark Prompts\n(DATDP) method. The DATDP algorithm works by repeatedly utilizing an evaluation\nLLM to evaluate a prompt for dangerous or manipulative behaviors--unlike some\nother approaches, DATDP also explicitly looks for jailbreaking attempts--until\na robust safety rating is generated. This success persisted even when utilizing\nsmaller LLMs to power the evaluation (Claude and LLaMa-3-8B-instruct proved\nalmost equally capable). These results show that, though language models are\nsensitive to seemingly innocuous changes to inputs, they seem also capable of\nsuccessfully evaluating the dangers of these inputs. Versions of DATDP can\ntherefore be added cheaply to generative AI systems to produce an immediate\nsignificant increase in safety."
    },
    {
        "date": "2025-02",
        "title": "Doubly Robust Monte Carlo Tree Search",
        "author": "Manqing Liu, and Andrew L. Beam",
        "link": "http://arxiv.org/abs/2502.01672v1",
        "abstract": "We present Doubly Robust Monte Carlo Tree Search (DR-MCTS), a novel algorithm\nthat integrates Doubly Robust (DR) off-policy estimation into Monte Carlo Tree\nSearch (MCTS) to enhance sample efficiency and decision quality in complex\nenvironments. Our approach introduces a hybrid estimator that combines MCTS\nrollouts with DR estimation, offering theoretical guarantees of unbiasedness\nand variance reduction under specified conditions. Empirical evaluations in\nTic-Tac-Toe and the partially observable VirtualHome environment demonstrate\nDR-MCTS's superior performance over standard MCTS. In Tic-Tac-Toe, DR-MCTS\nachieves an 88% win rate compared to a 10% win rate for standard MCTS. In\ncompound VirtualHome tasks, DR-MCTS attains a 20.7% success rate versus 10.3%\nfor standard MCTS. Our scaling analysis reveals that DR-MCTS exhibits better\nsample efficiency, notably outperforming standard MCTS with larger language\nmodels while using a smaller model. These results underscore DR-MCTS's\npotential for efficient decision-making in complex, real-world scenarios where\nsample efficiency is paramount."
    },
    {
        "date": "2025-02",
        "title": "Data Overvaluation Attack and Truthful Data Valuation",
        "author": "Shuyuan Zheng, Sudong Cai, Chuan Xiao, Yang Cao, Jianbin Qin, Masatoshi Yoshikawa, and Makoto Onizuka",
        "link": "http://arxiv.org/abs/2502.00494v2",
        "abstract": "In collaborative machine learning, data valuation, i.e., evaluating the\ncontribution of each client' data to the machine learning model, has become a\ncritical task for incentivizing and selecting positive data contributions.\nHowever, existing studies often assume that clients engage in data valuation\ntruthfully, overlooking the practical motivation for clients to exaggerate\ntheir contributions. To unlock this threat, this paper introduces the first\ndata overvaluation attack, enabling strategic clients to have their data\nsignificantly overvalued. Furthermore, we propose a truthful data valuation\nmetric, named Truth-Shapley. Truth-Shapley is the unique metric that guarantees\nsome promising axioms for data valuation while ensuring that clients' optimal\nstrategy is to perform truthful data valuation. Our experiments demonstrate the\nvulnerability of existing data valuation metrics to the data overvaluation\nattack and validate the robustness and effectiveness of Truth-Shapley."
    },
    {
        "date": "2025-02",
        "title": "Oscillations Make Neural Networks Robust to Quantization",
        "author": "Jonathan Wensh\u00f8j, Bob Pepin, and Raghavendra Selvan",
        "link": "http://arxiv.org/abs/2502.00490v1",
        "abstract": "We challenge the prevailing view that oscillations in Quantization Aware\nTraining (QAT) are merely undesirable artifacts caused by the Straight-Through\nEstimator (STE). Through theoretical analysis of QAT in linear models, we\ndemonstrate that the gradient of the loss function can be decomposed into two\nterms: the original full-precision loss and a term that causes quantization\noscillations. Based on these insights, we propose a novel regularization method\nthat induces oscillations to improve quantization robustness. Contrary to\ntraditional methods that focuses on minimizing the effects of oscillations, our\napproach leverages the beneficial aspects of weight oscillations to preserve\nmodel performance under quantization. Our empirical results on ResNet-18 and\nTiny ViT demonstrate that this counter-intuitive strategy matches QAT accuracy\nat >= 3-bit weight quantization, while maintaining close to full precision\naccuracy at bits greater than the target bit. Our work therefore provides a new\nperspective on model preparation for quantization, particularly for finding\nweights that are robust to changes in the bit of the quantizer -- an area where\ncurrent methods struggle to match the accuracy of QAT at specific bits."
    }
]