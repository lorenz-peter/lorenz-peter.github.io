[
    {
        "date": "2025-06",
        "title": "Proportional Sensitivity in Generative Adversarial Network (GAN)-Augmented Brain Tumor Classification Using Convolutional Neural Network",
        "author": "Mahin Montasir Afif, Abdullah Al Noman, K. M. Tahsin Kabir, Md. Mortuza Ahmmed, Md. Mostafizur Rahman, Mufti Mahmud, and Md. Ashraful Babu",
        "link": "http://arxiv.org/abs/2506.17165v1",
        "abstract": "Generative Adversarial Networks (GAN) have shown potential in expanding\nlimited medical imaging datasets. This study explores how different ratios of\nGAN-generated and real brain tumor MRI images impact the performance of a CNN\nin classifying healthy vs. tumorous scans. A DCGAN was used to create synthetic\nimages which were mixed with real ones at various ratios to train a custom CNN.\nThe CNN was then evaluated on a separate real-world test set. Our results\nindicate that the model maintains high sensitivity and precision in tumor\nclassification, even when trained predominantly on synthetic data. When only a\nsmall portion of GAN data was added, such as 900 real images and 100 GAN\nimages, the model achieved excellent performance, with test accuracy reaching\n95.2%, and precision, recall, and F1-score all exceeding 95%. However, as the\nproportion of GAN images increased further, performance gradually declined.\nThis study suggests that while GANs are useful for augmenting limited datasets\nespecially when real data is scarce, too much synthetic data can introduce\nartifacts that affect the model's ability to generalize to real world cases."
    },
    {
        "date": "2025-06",
        "title": "Analyzing PDFs like Binaries: Adversarially Robust PDF Malware Analysis via Intermediate Representation and Language Model",
        "author": "Side Liu, Jiang Ming, Guodong Zhou, Xinyi Liu, Jianming Fu, and Guojun Peng",
        "link": "http://arxiv.org/abs/2506.17162v1",
        "abstract": "Malicious PDF files have emerged as a persistent threat and become a popular\nattack vector in web-based attacks. While machine learning-based PDF malware\nclassifiers have shown promise, these classifiers are often susceptible to\nadversarial attacks, undermining their reliability. To address this issue,\nrecent studies have aimed to enhance the robustness of PDF classifiers. Despite\nthese efforts, the feature engineering underlying these studies remains\noutdated. Consequently, even with the application of cutting-edge machine\nlearning techniques, these approaches fail to fundamentally resolve the issue\nof feature instability.\n  To tackle this, we propose a novel approach for PDF feature extraction and\nPDF malware detection. We introduce the PDFObj IR (PDF Object Intermediate\nRepresentation), an assembly-like language framework for PDF objects, from\nwhich we extract semantic features using a pretrained language model.\nAdditionally, we construct an Object Reference Graph to capture structural\nfeatures, drawing inspiration from program analysis. This dual approach enables\nus to analyze and detect PDF malware based on both semantic and structural\nfeatures. Experimental results demonstrate that our proposed classifier\nachieves strong adversarial robustness while maintaining an exceptionally low\nfalse positive rate of only 0.07% on baseline dataset compared to\nstate-of-the-art PDF malware classifiers."
    },
    {
        "date": "2025-06",
        "title": "Robust Training with Data Augmentation for Medical Imaging Classification",
        "author": "Josu\u00e9 Mart\u00ednez-Mart\u00ednez, Olivia Brown, Mostafa Karami, and Sheida Nabavi",
        "link": "http://arxiv.org/abs/2506.17133v1",
        "abstract": "Deep neural networks are increasingly being used to detect and diagnose\nmedical conditions using medical imaging. Despite their utility, these models\nare highly vulnerable to adversarial attacks and distribution shifts, which can\naffect diagnostic reliability and undermine trust among healthcare\nprofessionals. In this study, we propose a robust training algorithm with data\naugmentation (RTDA) to mitigate these vulnerabilities in medical image\nclassification. We benchmark classifier robustness against adversarial\nperturbations and natural variations of RTDA and six competing baseline\ntechniques, including adversarial training and data augmentation approaches in\nisolation and combination, using experimental data sets with three different\nimaging technologies (mammograms, X-rays, and ultrasound). We demonstrate that\nRTDA achieves superior robustness against adversarial attacks and improved\ngeneralization performance in the presence of distribution shift in each image\nclassification task while maintaining high clean accuracy."
    },
    {
        "date": "2025-06",
        "title": "RGBTrack: Fast, Robust Depth-Free 6D Pose Estimation and Tracking",
        "author": "Teng Guo, and Jingjin Yu",
        "link": "http://arxiv.org/abs/2506.17119v1",
        "abstract": "We introduce a robust framework, RGBTrack, for real-time 6D pose estimation\nand tracking that operates solely on RGB data, thereby eliminating the need for\ndepth input for such dynamic and precise object pose tracking tasks. Building\non the FoundationPose architecture, we devise a novel binary search strategy\ncombined with a render-and-compare mechanism to efficiently infer depth and\ngenerate robust pose hypotheses from true-scale CAD models. To maintain stable\ntracking in dynamic scenarios, including rapid movements and occlusions,\nRGBTrack integrates state-of-the-art 2D object tracking (XMem) with a Kalman\nfilter and a state machine for proactive object pose recovery. In addition,\nRGBTrack's scale recovery module dynamically adapts CAD models of unknown scale\nusing an initial depth estimate, enabling seamless integration with modern\ngenerative reconstruction techniques. Extensive evaluations on benchmark\ndatasets demonstrate that RGBTrack's novel depth-free approach achieves\ncompetitive accuracy and real-time performance, making it a promising practical\nsolution candidate for application areas including robotics, augmented reality,\nand computer vision.\n  The source code for our implementation will be made publicly available at\nhttps://github.com/GreatenAnoymous/RGBTrack.git."
    },
    {
        "date": "2025-06",
        "title": "Robust Reinforcement Learning for Discrete Compositional Generation via General Soft Operators",
        "author": "Marco Jiralerspong, Esther Derman, Danilo Vucetic, Nikolay Malkin, Bilun Sun, Tianyu Zhang, Pierre-Luc Bacon, and Gauthier Gidel",
        "link": "http://arxiv.org/abs/2506.17007v1",
        "abstract": "A major bottleneck in scientific discovery involves narrowing a large\ncombinatorial set of objects, such as proteins or molecules, to a small set of\npromising candidates. While this process largely relies on expert knowledge,\nrecent methods leverage reinforcement learning (RL) to enhance this filtering.\nThey achieve this by estimating proxy reward functions from available datasets\nand using regularization to generate more diverse candidates. These reward\nfunctions are inherently uncertain, raising a particularly salient challenge\nfor scientific discovery. In this work, we show that existing methods, often\nframed as sampling proportional to a reward function, are inadequate and yield\nsuboptimal candidates, especially in large search spaces. To remedy this issue,\nwe take a robust RL approach and introduce a unified operator that seeks\nrobustness to the uncertainty of the proxy reward function. This general\noperator targets peakier sampling distributions while encompassing known soft\nRL operators. It also leads us to a novel algorithm that identifies\nhigher-quality, diverse candidates in both synthetic and real-world tasks.\nUltimately, our work offers a new, flexible perspective on discrete\ncompositional generation tasks. Code: https://github.com/marcojira/tgm."
    },
    {
        "date": "2025-06",
        "title": "SmartGuard: Leveraging Large Language Models for Network Attack Detection through Audit Log Analysis and Summarization",
        "author": "Hao Zhang, Shuo Shao, Song Li, Zhenyu Zhong, Yan Liu, Zhan Qin, and Kui Ren",
        "link": "http://arxiv.org/abs/2506.16981v1",
        "abstract": "End-point monitoring solutions are widely deployed in today's enterprise\nenvironments to support advanced attack detection and investigation. These\nmonitors continuously record system-level activities as audit logs and provide\ndeep visibility into security events. Unfortunately, existing methods of\nsemantic analysis based on audit logs have low granularity, only reaching the\nsystem call level, making it difficult to effectively classify highly covert\nbehaviors. Additionally, existing works mainly match audit log streams with\nrule knowledge bases describing behaviors, which heavily rely on expertise and\nlack the ability to detect unknown attacks and provide interpretive\ndescriptions. In this paper, we propose SmartGuard, an automated method that\ncombines abstracted behaviors from audit event semantics with large language\nmodels. SmartGuard extracts specific behaviors (function level) from incoming\nsystem logs and constructs a knowledge graph, divides events by threads, and\ncombines event summaries with graph embeddings to achieve information diagnosis\nand provide explanatory narratives through large language models. Our\nevaluation shows that SmartGuard achieves an average F1 score of 96\\% in\nassessing malicious behaviors and demonstrates good scalability across multiple\nmodels and unknown attacks. It also possesses excellent fine-tuning\ncapabilities, allowing experts to assist in timely system updates."
    },
    {
        "date": "2025-06",
        "title": "MM-AttacKG: A Multimodal Approach to Attack Graph Construction with Large Language Models",
        "author": "Yongheng Zhang, Xinyun Zhao, Yunshan Ma, Haokai Ma, Yingxiao Guan, Guozheng Yang, Yuliang Lu, and Xiang Wang",
        "link": "http://arxiv.org/abs/2506.16968v1",
        "abstract": "Cyber Threat Intelligence (CTI) parsing aims to extract key threat\ninformation from massive data, transform it into actionable intelligence,\nenhance threat detection and defense efficiency, including attack graph\nconstruction, intelligence fusion and indicator extraction. Among these\nresearch topics, Attack Graph Construction (AGC) is essential for visualizing\nand understanding the potential attack paths of threat events from CTI reports.\nExisting approaches primarily construct the attack graphs purely from the\ntextual data to reveal the logical threat relationships between entities within\nthe attack behavioral sequence. However, they typically overlook the specific\nthreat information inherent in visual modalities, which preserves the key\nthreat details from inherently-multimodal CTI report. Therefore, we enhance the\neffectiveness of attack graph construction by analyzing visual information\nthrough Multimodal Large Language Models (MLLMs). Specifically, we propose a\nnovel framework, MM-AttacKG, which can effectively extract key information from\nthreat images and integrate it into attack graph construction, thereby\nenhancing the comprehensiveness and accuracy of attack graphs. It first employs\na threat image parsing module to extract critical threat information from\nimages and generate descriptions using MLLMs. Subsequently, it builds an\niterative question-answering pipeline tailored for image parsing to refine the\nunderstanding of threat images. Finally, it achieves content-level integration\nbetween attack graphs and image-based answers through MLLMs, completing threat\ninformation enhancement. The experimental results demonstrate that MM-AttacKG\ncan accurately identify key information in threat images and significantly\nimprove the quality of multimodal attack graph construction, effectively\naddressing the shortcomings of existing methods in utilizing image-based threat\ninformation."
    },
    {
        "date": "2025-06",
        "title": "Towards Effective Complementary Security Analysis using Large Language Models",
        "author": "Jonas Wagner, Simon M\u00fcller, Christian N\u00e4ther, Jan-Philipp Stegh\u00f6fer, and Andreas Both",
        "link": "http://arxiv.org/abs/2506.16899v1",
        "abstract": "A key challenge in security analysis is the manual evaluation of potential\nsecurity weaknesses generated by static application security testing (SAST)\ntools. Numerous false positives (FPs) in these reports reduce the effectiveness\nof security analysis. We propose using Large Language Models (LLMs) to improve\nthe assessment of SAST findings. We investigate the ability of LLMs to reduce\nFPs while trying to maintain a perfect true positive rate, using datasets\nextracted from the OWASP Benchmark (v1.2) and a real-world software project.\nOur results indicate that advanced prompting techniques, such as\nChain-of-Thought and Self-Consistency, substantially improve FP detection.\nNotably, some LLMs identified approximately 62.5% of FPs in the OWASP Benchmark\ndataset without missing genuine weaknesses. Combining detections from different\nLLMs would increase this FP detection to approximately 78.9%. Additionally, we\ndemonstrate our approach's generalizability using a real-world dataset covering\nfive SAST tools, three programming languages, and infrastructure files. The\nbest LLM detected 33.85% of all FPs without missing genuine weaknesses, while\ncombining detections from different LLMs would increase this detection to\n38.46%. Our findings highlight the potential of LLMs to complement traditional\nSAST tools, enhancing automation and reducing resources spent addressing false\nalarms."
    },
    {
        "date": "2025-06",
        "title": "Robust Group Anomaly Detection for Quasi-Periodic Network Time Series",
        "author": "Kai Yang, Shaoyu Dou, Pan Luo, Xin Wang, and H. Vincent Poor",
        "link": "http://arxiv.org/abs/2506.16815v1",
        "abstract": "Many real-world multivariate time series are collected from a network of\nphysical objects embedded with software, electronics, and sensors. The\nquasi-periodic signals generated by these objects often follow a similar\nrepetitive and periodic pattern, but have variations in the period, and come in\ndifferent lengths caused by timing (synchronization) errors. Given a multitude\nof such quasi-periodic time series, can we build machine learning models to\nidentify those time series that behave differently from the majority of the\nobservations? In addition, can the models help human experts to understand how\nthe decision was made? We propose a sequence to Gaussian Mixture Model\n(seq2GMM) framework. The overarching goal of this framework is to identify\nunusual and interesting time series within a network time series database. We\nfurther develop a surrogate-based optimization algorithm that can efficiently\ntrain the seq2GMM model. Seq2GMM exhibits strong empirical performance on a\nplurality of public benchmark datasets, outperforming state-of-the-art anomaly\ndetection techniques by a significant margin. We also theoretically analyze the\nconvergence property of the proposed training algorithm and provide numerical\nresults to substantiate our theoretical claims."
    },
    {
        "date": "2025-06",
        "title": "Robust Dynamic Material Handling via Adaptive Constrained Evolutionary Reinforcement Learning",
        "author": "Chengpeng Hu, Ziming Wang, Bo Yuan, Jialin Liu, Chengqi Zhang, and Xin Yao",
        "link": "http://arxiv.org/abs/2506.16795v1",
        "abstract": "Dynamic material handling (DMH) involves the assignment of dynamically\narriving material transporting tasks to suitable vehicles in real time for\nminimising makespan and tardiness. In real-world scenarios, historical task\nrecords are usually available, which enables the training of a decision policy\non multiple instances consisting of historical records. Recently, reinforcement\nlearning has been applied to solve DMH. Due to the occurrence of dynamic events\nsuch as new tasks, adaptability is highly required. Solving DMH is challenging\nsince constraints including task delay should be satisfied. A feedback is\nreceived only when all tasks are served, which leads to sparse reward. Besides,\nmaking the best use of limited computational resources and historical records\nfor training a robust policy is crucial. The time allocated to different\nproblem instances would highly impact the learning process. To tackle those\nchallenges, this paper proposes a novel adaptive constrained evolutionary\nreinforcement learning (ACERL) approach, which maintains a population of actors\nfor diverse exploration. ACERL accesses each actor for tackling sparse rewards\nand constraint violation to restrict the behaviour of the policy. Moreover,\nACERL adaptively selects the most beneficial training instances for improving\nthe policy. Extensive experiments on eight training and eight unseen test\ninstances demonstrate the outstanding performance of ACERL compared with\nseveral state-of-the-art algorithms. Policies trained by ACERL can schedule the\nvehicles while fully satisfying the constraints. Additional experiments on 40\nunseen noised instances show the robust performance of ACERL. Cross-validation\nfurther presents the overall effectiveness of ACREL. Besides, a rigorous\nablation study highlights the coordination and benefits of each ingredient of\nACERL."
    },
    {
        "date": "2025-06",
        "title": "Cross-Modal Obfuscation for Jailbreak Attacks on Large Vision-Language Models",
        "author": "Lei Jiang, Zixun Zhang, Zizhou Wang, Xiaobing Sun, Zhen Li, Liangli Zhen, and Xiaohua Xu",
        "link": "http://arxiv.org/abs/2506.16760v1",
        "abstract": "Large Vision-Language Models (LVLMs) demonstrate exceptional performance\nacross multimodal tasks, yet remain vulnerable to jailbreak attacks that bypass\nbuilt-in safety mechanisms to elicit restricted content generation. Existing\nblack-box jailbreak methods primarily rely on adversarial textual prompts or\nimage perturbations, yet these approaches are highly detectable by standard\ncontent filtering systems and exhibit low query and computational efficiency.\nIn this work, we present Cross-modal Adversarial Multimodal Obfuscation (CAMO),\na novel black-box jailbreak attack framework that decomposes malicious prompts\ninto semantically benign visual and textual fragments. By leveraging LVLMs'\ncross-modal reasoning abilities, CAMO covertly reconstructs harmful\ninstructions through multi-step reasoning, evading conventional detection\nmechanisms. Our approach supports adjustable reasoning complexity and requires\nsignificantly fewer queries than prior attacks, enabling both stealth and\nefficiency. Comprehensive evaluations conducted on leading LVLMs validate\nCAMO's effectiveness, showcasing robust performance and strong cross-model\ntransferability. These results underscore significant vulnerabilities in\ncurrent built-in safety mechanisms, emphasizing an urgent need for advanced,\nalignment-aware security and safety solutions in vision-language systems."
    },
    {
        "date": "2025-06",
        "title": "Off-Policy Actor-Critic for Adversarial Observation Robustness: Virtual Alternative Training via Symmetric Policy Evaluation",
        "author": "Kosuke Nakanishi, Akihiro Kubo, Yuji Yasui, and Shin Ishii",
        "link": "http://arxiv.org/abs/2506.16753v1",
        "abstract": "Recently, robust reinforcement learning (RL) methods designed to handle\nadversarial input observations have received significant attention, motivated\nby RL's inherent vulnerabilities. While existing approaches have demonstrated\nreasonable success, addressing worst-case scenarios over long time horizons\nrequires both minimizing the agent's cumulative rewards for adversaries and\ntraining agents to counteract them through alternating learning. However, this\nprocess introduces mutual dependencies between the agent and the adversary,\nmaking interactions with the environment inefficient and hindering the\ndevelopment of off-policy methods. In this work, we propose a novel off-policy\nmethod that eliminates the need for additional environmental interactions by\nreformulating adversarial learning as a soft-constrained optimization problem.\nOur approach is theoretically supported by the symmetric property of policy\nevaluation between the agent and the adversary. The implementation is available\nat https://github.com/nakanakakosuke/VALT_SAC."
    },
    {
        "date": "2025-06",
        "title": "DepthVanish: Optimizing Adversarial Interval Structures for Stereo-Depth-Invisible Patches",
        "author": "Yun Xing, Yue Cao, Nhat Chung, Jie Zhang, Ivor Tsang, Ming-Ming Cheng, Yang Liu, Lei Ma, and Qing Guo",
        "link": "http://arxiv.org/abs/2506.16690v1",
        "abstract": "Stereo Depth estimation is a critical task in autonomous driving and\nrobotics, where inaccuracies (such as misidentifying nearby objects as distant)\ncan lead to dangerous situations. Adversarial attacks against stereo depth\nestimation can help reveal vulnerabilities before deployment. Previous work has\nshown that repeating optimized textures can effectively mislead stereo depth\nestimation in digital settings. However, our research reveals that these\nnaively repeated texture structures perform poorly in physical-world\nimplementations, i.e., when deployed as patches, limiting their practical\nutility for testing stereo depth estimation systems. In this work, for the\nfirst time, we discover that introducing regular intervals between repeated\ntextures, creating a striped structure, significantly enhances the patch attack\neffectiveness. Through extensive experimentation, we analyze how variations of\nthis novel structure influence the performance. Based on these insights, we\ndevelop a novel stereo depth attack that jointly optimizes both the striped\nstructure and texture elements. Our generated adversarial patches can be\ninserted into any scenes and successfully attack state-of-the-art stereo depth\nestimation methods, i.e., RAFT-Stereo and STTR. Most critically, our patch can\nalso attack commercial RGB-D cameras (Intel RealSense) in real-world\nconditions, demonstrating their practical relevance for security assessment of\nstereo systems."
    },
    {
        "date": "2025-06",
        "title": "Reimagination with Test-time Observation Interventions: Distractor-Robust World Model Predictions for Visual Model Predictive Control",
        "author": "Yuxin Chen, Jianglan Wei, Chenfeng Xu, Boyi Li, Masayoshi Tomizuka, Andrea Bajcsy, and Ran Tian",
        "link": "http://arxiv.org/abs/2506.16565v1",
        "abstract": "World models enable robots to \"imagine\" future observations given current\nobservations and planned actions, and have been increasingly adopted as\ngeneralized dynamics models to facilitate robot learning. Despite their\npromise, these models remain brittle when encountering novel visual distractors\nsuch as objects and background elements rarely seen during training.\nSpecifically, novel distractors can corrupt action outcome predictions, causing\ndownstream failures when robots rely on the world model imaginations for\nplanning or action verification. In this work, we propose Reimagination with\nObservation Intervention (ReOI), a simple yet effective test-time strategy that\nenables world models to predict more reliable action outcomes in open-world\nscenarios where novel and unanticipated visual distractors are inevitable.\nGiven the current robot observation, ReOI first detects visual distractors by\nidentifying which elements of the scene degrade in physically implausible ways\nduring world model prediction. Then, it modifies the current observation to\nremove these distractors and bring the observation closer to the training\ndistribution. Finally, ReOI \"reimagines\" future outcomes with the modified\nobservation and reintroduces the distractors post-hoc to preserve visual\nconsistency for downstream planning and verification. We validate our approach\non a suite of robotic manipulation tasks in the context of action verification,\nwhere the verifier needs to select desired action plans based on predictions\nfrom a world model. Our results show that ReOI is robust to both\nin-distribution and out-of-distribution visual distractors. Notably, it\nimproves task success rates by up to 3x in the presence of novel distractors,\nsignificantly outperforming action verification that relies on world model\npredictions without imagination interventions."
    },
    {
        "date": "2025-06",
        "title": "One Sample is Enough to Make Conformal Prediction Robust",
        "author": "Soroush H. Zargarbashi, Mohammad Sadegh Akhondzadeh, and Aleksandar Bojchevski",
        "link": "http://arxiv.org/abs/2506.16553v1",
        "abstract": "Given any model, conformal prediction (CP) returns prediction sets guaranteed\nto include the true label with high adjustable probability. Robust CP (RCP)\nextends this to inputs with worst-case noise. A well-established approach is to\nuse randomized smoothing for RCP since it is applicable to any black-box model\nand provides smaller sets compared to deterministic methods. However, current\nsmoothing-based RCP requires many model forward passes per each input which is\ncomputationally expensive. We show that conformal prediction attains some\nrobustness even with a forward pass on a single randomly perturbed input. Using\nany binary certificate we propose a single sample robust CP (RCP1). Our\napproach returns robust sets with smaller average set size compared to SOTA\nmethods which use many (e.g. around 100) passes per input. Our key insight is\nto certify the conformal prediction procedure itself rather than individual\nscores. Our approach is agnostic to the setup (classification and regression).\nWe further extend our approach to smoothing-based robust conformal risk\ncontrol."
    },
    {
        "date": "2025-06",
        "title": "SAFER-D: A Self-Adaptive Security Framework for Distributed Computing Architectures",
        "author": "Marco Stadler, Michael Vierhauser, Michael Riegler, Daniel Waghubinger, and Johannes Sametinger",
        "link": "http://arxiv.org/abs/2506.16545v1",
        "abstract": "The rise of the Internet of Things and Cyber-Physical Systems has introduced\nnew challenges on ensuring secure and robust communication. The growing number\nof connected devices increases network complexity, leading to higher latency\nand traffic. Distributed computing architectures (DCAs) have gained prominence\nto address these issues. This shift has significantly expanded the attack\nsurface, requiring additional security measures to protect all components --\nfrom sensors and actuators to edge nodes and central servers. Recent incidents\nhighlight the difficulty of this task: Cyberattacks, like distributed denial of\nservice attacks, continue to pose severe threats and cause substantial damage.\nImplementing a holistic defense mechanism remains an open challenge,\nparticularly against attacks that demand both enhanced resilience and rapid\nresponse. Addressing this gap requires innovative solutions to enhance the\nsecurity of DCAs. In this work, we present our holistic self-adaptive security\nframework which combines different adaptation strategies to create\ncomprehensive and efficient defense mechanisms. We describe how to incorporate\nthe framework into a real-world use case scenario and further evaluate its\napplicability and efficiency. Our evaluation yields promising results,\nindicating great potential to further extend the research on our framework."
    },
    {
        "date": "2025-06",
        "title": "Robust Reward Modeling via Causal Rubrics",
        "author": "Pragya Srivastava, Harman Singh, Rahul Madhavan, Gandharv Patil, Sravanti Addepalli, Arun Suggala, Rengarajan Aravamudhan, Soumya Sharma, Anirban Laha, Aravindan Raghuveer, Karthikeyan Shanmugam, and Doina Precup",
        "link": "http://arxiv.org/abs/2506.16507v1",
        "abstract": "Reward models (RMs) are fundamental to aligning Large Language Models (LLMs)\nvia human feedback, yet they often suffer from reward hacking. They tend to\nlatch on to superficial or spurious attributes, such as response length or\nformatting, mistaking these cues learned from correlations in training data for\nthe true causal drivers of quality (e.g., factuality, relevance). This occurs\nbecause standard training objectives struggle to disentangle these factors,\nleading to brittle RMs and misaligned policies. We introduce Crome (Causally\nRobust Reward Modeling), a novel framework grounded in an explicit causal model\ndesigned to mitigate reward hacking. Crome employs the following synthetic\ntargeted augmentations during training: (1) Causal Augmentations, which are\npairs that differ along specific causal attributes, to enforce sensitivity\nalong each causal attribute individually, and (2) Neutral Augmentations, which\nare tie-label pairs varying primarily in spurious attributes, to enforce\ninvariance along spurious attributes. Notably, our augmentations are produced\nwithout any knowledge of spurious factors, via answer interventions only along\ncausal rubrics, that are identified by querying an oracle LLM. Empirically,\nCrome significantly outperforms standard baselines on RewardBench, improving\naverage accuracy by up to 5.4% and achieving gains of up to 13.2% and 7.2% in\nspecific categories. The robustness of Crome is further testified by the\nconsistent gains obtained in a Best-of-N inference setting across increasing N,\nacross various benchmarks, including the popular RewardBench (covering chat,\nchat-hard, safety, and reasoning tasks), the safety-focused WildGuardTest, and\nthe reasoning-specific GSM8k."
    },
    {
        "date": "2025-06",
        "title": "Black-Box Privacy Attacks on Shared Representations in Multitask Learning",
        "author": "John Abascal, Nicol\u00e1s Berrios, Alina Oprea, Jonathan Ullman, Adam Smith, and Matthew Jagielski",
        "link": "http://arxiv.org/abs/2506.16460v1",
        "abstract": "Multitask learning (MTL) has emerged as a powerful paradigm that leverages\nsimilarities among multiple learning tasks, each with insufficient samples to\ntrain a standalone model, to solve them simultaneously while minimizing data\nsharing across users and organizations. MTL typically accomplishes this goal by\nlearning a shared representation that captures common structure among the tasks\nby embedding data from all tasks into a common feature space. Despite being\ndesigned to be the smallest unit of shared information necessary to effectively\nlearn patterns across multiple tasks, these shared representations can\ninadvertently leak sensitive information about the particular tasks they were\ntrained on.\n  In this work, we investigate what information is revealed by the shared\nrepresentations through the lens of inference attacks. Towards this, we propose\na novel, black-box task-inference threat model where the adversary, given the\nembedding vectors produced by querying the shared representation on samples\nfrom a particular task, aims to determine whether that task was present when\ntraining the shared representation. We develop efficient, purely black-box\nattacks on machine learning models that exploit the dependencies between\nembeddings from the same task without requiring shadow models or labeled\nreference data. We evaluate our attacks across vision and language domains for\nmultiple use cases of MTL and demonstrate that even with access only to fresh\ntask samples rather than training data, a black-box adversary can successfully\ninfer a task's inclusion in training. To complement our experiments, we provide\ntheoretical analysis of a simplified learning setting and show a strict\nseparation between adversaries with training samples and fresh samples from the\ntarget task's distribution."
    },
    {
        "date": "2025-06",
        "title": "Probe before You Talk: Towards Black-box Defense against Backdoor Unalignment for Large Language Models",
        "author": "Biao Yi, Tiansheng Huang, Sishuo Chen, Tong Li, Zheli Liu, Zhixuan Chu, and Yiming Li",
        "link": "http://arxiv.org/abs/2506.16447v1",
        "abstract": "Backdoor unalignment attacks against Large Language Models (LLMs) enable the\nstealthy compromise of safety alignment using a hidden trigger while evading\nnormal safety auditing. These attacks pose significant threats to the\napplications of LLMs in the real-world Large Language Model as a Service\n(LLMaaS) setting, where the deployed model is a fully black-box system that can\nonly interact through text. Furthermore, the sample-dependent nature of the\nattack target exacerbates the threat. Instead of outputting a fixed label, the\nbackdoored LLM follows the semantics of any malicious command with the hidden\ntrigger, significantly expanding the target space. In this paper, we introduce\nBEAT, a black-box defense that detects triggered samples during inference to\ndeactivate the backdoor. It is motivated by an intriguing observation (dubbed\nthe probe concatenate effect), where concatenated triggered samples\nsignificantly reduce the refusal rate of the backdoored LLM towards a malicious\nprobe, while non-triggered samples have little effect. Specifically, BEAT\nidentifies whether an input is triggered by measuring the degree of distortion\nin the output distribution of the probe before and after concatenation with the\ninput. Our method addresses the challenges of sample-dependent targets from an\nopposite perspective. It captures the impact of the trigger on the refusal\nsignal (which is sample-independent) instead of sample-specific successful\nattack behaviors. It overcomes black-box access limitations by using multiple\nsampling to approximate the output distribution. Extensive experiments are\nconducted on various backdoor attacks and LLMs (including the closed-source\nGPT-3.5-turbo), verifying the effectiveness and efficiency of our defense.\nBesides, we also preliminarily verify that BEAT can effectively defend against\npopular jailbreak attacks, as they can be regarded as 'natural backdoors'."
    },
    {
        "date": "2025-06",
        "title": "Robustness Evaluation of OCR-based Visual Document Understanding under Multi-Modal Adversarial Attacks",
        "author": "Dong Nguyen Tien, and Dung D. Le",
        "link": "http://arxiv.org/abs/2506.16407v1",
        "abstract": "Visual Document Understanding (VDU) systems have achieved strong performance\nin information extraction by integrating textual, layout, and visual signals.\nHowever, their robustness under realistic adversarial perturbations remains\ninsufficiently explored. We introduce the first unified framework for\ngenerating and evaluating multi-modal adversarial attacks on OCR-based VDU\nmodels. Our method covers six gradient-based layout attack scenarios,\nincorporating manipulations of OCR bounding boxes, pixels, and texts across\nboth word and line granularities, with constraints on layout perturbation\nbudget (e.g., IoU >= 0.6) to preserve plausibility.\n  Experimental results across four datasets (FUNSD, CORD, SROIE, DocVQA) and\nsix model families demonstrate that line-level attacks and compound\nperturbations (BBox + Pixel + Text) yield the most severe performance\ndegradation. Projected Gradient Descent (PGD)-based BBox perturbations\noutperform random-shift baselines in all investigated models. Ablation studies\nfurther validate the impact of layout budget, text modification, and\nadversarial transferability."
    },
    {
        "date": "2025-06",
        "title": "Physical-Layer Signal Injection Attacks on EV Charging Ports: Bypassing Authentication via Electrical-Level Exploits",
        "author": "Hetian Shi, Yi He, Shangru Song, Jianwei Zhuge, and Jian Mao",
        "link": "http://arxiv.org/abs/2506.16400v1",
        "abstract": "The proliferation of electric vehicles in recent years has significantly\nexpanded the charging infrastructure while introducing new security risks to\nboth vehicles and chargers. In this paper, we investigate the security of major\ncharging protocols such as SAE J1772, CCS, IEC 61851, GB/T 20234, and NACS,\nuncovering new physical signal spoofing attacks in their authentication\nmechanisms. By inserting a compact malicious device into the charger connector,\nattackers can inject fraudulent signals to sabotage the charging process,\nleading to denial of service, vehicle-induced charger lockout, and damage to\nthe chargers or the vehicle's charge management system. To demonstrate the\nfeasibility of our attacks, we propose PORTulator, a proof-of-concept (PoC)\nattack hardware, including a charger gun plugin device for injecting physical\nsignals and a wireless controller for remote manipulation. By evaluating\nPORTulator on multiple real-world chargers, we identify 7 charging standards\nused by 20 charger piles that are vulnerable to our attacks. The root cause is\nthat chargers use simple physical signals for authentication and control,\nmaking them easily spoofed by attackers. To address this issue, we propose\nenhancing authentication circuits by integrating non-resistive memory\ncomponents and utilizing dynamic high-frequency Pulse Width Modulation (PWM)\nsignals to counter such physical signal spoofing attacks."
    },
    {
        "date": "2025-06",
        "title": "SycnMapV2: Robust and Adaptive Unsupervised Segmentation",
        "author": "Heng Zhang, Zikang Wan, and Danilo Vasconcellos Vargas",
        "link": "http://arxiv.org/abs/2506.16297v1",
        "abstract": "Human vision excels at segmenting visual cues without the need for explicit\ntraining, and it remains remarkably robust even as noise severity increases. In\ncontrast, existing AI algorithms struggle to maintain accuracy under similar\nconditions. Here, we present SyncMapV2, the first to solve unsupervised\nsegmentation with state-of-the-art robustness. SyncMapV2 exhibits a minimal\ndrop in mIoU, only 0.01%, under digital corruption, compared to a 23.8% drop\nobserved in SOTA methods.This superior performance extends across various types\nof corruption: noise (7.3% vs. 37.7%), weather (7.5% vs. 33.8%), and blur (7.0%\nvs. 29.5%). Notably, SyncMapV2 accomplishes this without any robust training,\nsupervision, or loss functions. It is based on a learning paradigm that uses\nself-organizing dynamical equations combined with concepts from random\nnetworks. Moreover,unlike conventional methods that require re-initialization\nfor each new input, SyncMapV2 adapts online, mimicking the continuous\nadaptability of human vision. Thus, we go beyond the accurate and robust\nresults, and present the first algorithm that can do all the above online,\nadapting to input rather than re-initializing. In adaptability tests, SyncMapV2\ndemonstrates near-zero performance degradation, which motivates and fosters a\nnew generation of robust and adaptive intelligence in the near future."
    },
    {
        "date": "2025-06",
        "title": "R3eVision: A Survey on Robust Rendering, Restoration, and Enhancement for 3D Low-Level Vision",
        "author": "Weeyoung Kwon, Jeahun Sung, Minkyu Jeon, Chanho Eom, and Jihyong Oh",
        "link": "http://arxiv.org/abs/2506.16262v1",
        "abstract": "Neural rendering methods such as Neural Radiance Fields (NeRF) and 3D\nGaussian Splatting (3DGS) have achieved significant progress in photorealistic\n3D scene reconstruction and novel view synthesis. However, most existing models\nassume clean and high-resolution (HR) multi-view inputs, which limits their\nrobustness under real-world degradations such as noise, blur, low-resolution\n(LR), and weather-induced artifacts. To address these limitations, the emerging\nfield of 3D Low-Level Vision (3D LLV) extends classical 2D Low-Level Vision\ntasks including super-resolution (SR), deblurring, weather degradation removal,\nrestoration, and enhancement into the 3D spatial domain. This survey, referred\nto as R\\textsuperscript{3}eVision, provides a comprehensive overview of robust\nrendering, restoration, and enhancement for 3D LLV by formalizing the\ndegradation-aware rendering problem and identifying key challenges related to\nspatio-temporal consistency and ill-posed optimization. Recent methods that\nintegrate LLV into neural rendering frameworks are categorized to illustrate\nhow they enable high-fidelity 3D reconstruction under adverse conditions.\nApplication domains such as autonomous driving, AR/VR, and robotics are also\ndiscussed, where reliable 3D perception from degraded inputs is critical. By\nreviewing representative methods, datasets, and evaluation protocols, this work\npositions 3D LLV as a fundamental direction for robust 3D content generation\nand scene-level reconstruction in real-world environments."
    },
    {
        "date": "2025-06",
        "title": "FOCoOp: Enhancing Out-of-Distribution Robustness in Federated Prompt Learning for Vision-Language Models",
        "author": "Xinting Liao, Weiming Liu, Jiaming Qian, Pengyang Zhou, Jiahe Xu, Wenjie Wang, Chaochao Chen, Xiaolin Zheng, and Tat-Seng Chua",
        "link": "http://arxiv.org/abs/2506.16218v1",
        "abstract": "Federated prompt learning (FPL) for vision-language models is a powerful\napproach to collaboratively adapt models across distributed clients while\npreserving data privacy. However, existing FPL approaches suffer from a\ntrade-off between performance and robustness, particularly in\nout-of-distribution (OOD) shifts, limiting their reliability in real-world\nscenarios. The inherent in-distribution (ID) data heterogeneity among different\nclients makes it more challenging to maintain this trade-off. To fill this gap,\nwe introduce a Federated OOD-aware Context Optimization (FOCoOp) framework,\nwhich captures diverse distributions among clients using ID global prompts,\nlocal prompts, and OOD prompts. Specifically, FOCoOp leverages three sets of\nprompts to create both class-level and distribution-level separations, which\nadapt to OOD shifts through bi-level distributionally robust optimization.\nAdditionally, FOCoOp improves the discrimination consistency among clients,\ni.e., calibrating global prompts, seemingly OOD prompts, and OOD prompts by\nsemi-unbalanced optimal transport. The extensive experiments on real-world\ndatasets demonstrate that FOCoOp effectively captures decentralized\nheterogeneous distributions and enhances robustness of different OOD shifts.\nThe project is available at GitHub."
    },
    {
        "date": "2025-06",
        "title": "From Coarse to Continuous: Progressive Refinement Implicit Neural Representation for Motion-Robust Anisotropic MRI Reconstruction",
        "author": "Zhenxuan Zhang, Lipei Zhang, Yanqi Cheng, Zi Wang, Fanwen Wang, Haosen Zhang, Yue Yang, Yinzhe Wu, Jiahao Huang, Angelica I Aviles-Rivero, Zhifan Gao, Guang Yang, and Peter J. Lally",
        "link": "http://arxiv.org/abs/2506.16210v1",
        "abstract": "In motion-robust magnetic resonance imaging (MRI), slice-to-volume\nreconstruction is critical for recovering anatomically consistent 3D brain\nvolumes from 2D slices, especially under accelerated acquisitions or patient\nmotion. However, this task remains challenging due to hierarchical structural\ndisruptions. It includes local detail loss from k-space undersampling, global\nstructural aliasing caused by motion, and volumetric anisotropy. Therefore, we\npropose a progressive refinement implicit neural representation (PR-INR)\nframework. Our PR-INR unifies motion correction, structural refinement, and\nvolumetric synthesis within a geometry-aware coordinate space. Specifically, a\nmotion-aware diffusion module is first employed to generate coarse volumetric\nreconstructions that suppress motion artifacts and preserve global anatomical\nstructures. Then, we introduce an implicit detail restoration module that\nperforms residual refinement by aligning spatial coordinates with visual\nfeatures. It corrects local structures and enhances boundary precision.\nFurther, a voxel continuous-aware representation module represents the image as\na continuous function over 3D coordinates. It enables accurate inter-slice\ncompletion and high-frequency detail recovery. We evaluate PR-INR on five\npublic MRI datasets under various motion conditions (3% and 5% displacement),\nundersampling rates (4x and 8x) and slice resolutions (scale = 5). Experimental\nresults demonstrate that PR-INR outperforms state-of-the-art methods in both\nquantitative reconstruction metrics and visual quality. It further shows\ngeneralization and robustness across diverse unseen domains."
    },
    {
        "date": "2025-06",
        "title": "Integrating Generative Adversarial Networks and Convolutional Neural Networks for Enhanced Traffic Accidents Detection and Analysis",
        "author": "Zhenghao Xi, Xiang Liu, Yaqi Liu, Yitong Cai, and Yangyu Zheng",
        "link": "http://arxiv.org/abs/2506.16186v1",
        "abstract": "Accident detection using Closed Circuit Television (CCTV) footage is one of\nthe most imperative features for enhancing transport safety and efficient\ntraffic control. To this end, this research addresses the issues of supervised\nmonitoring and data deficiency in accident detection systems by adapting\nexcellent deep learning technologies. The motivation arises from rising\nstatistics in the number of car accidents worldwide; this calls for innovation\nand the establishment of a smart, efficient and automated way of identifying\naccidents and calling for help to save lives. Addressing the problem of the\nscarcity of data, the presented framework joins Generative Adversarial Networks\n(GANs) for synthesizing data and Convolutional Neural Networks (CNN) for model\ntraining. Video frames for accidents and non-accidents are collected from\nYouTube videos, and we perform resizing, image enhancement and image\nnormalisation pixel range adjustments. Three models are used: CNN, Fine-tuned\nConvolutional Neural Network (FTCNN) and Vision Transformer (VIT) worked best\nfor detecting accidents from CCTV, obtaining an accuracy rate of 94% and 95%,\nwhile the CNN model obtained 88%. Such results show that the proposed framework\nsuits traffic safety applications due to its high real-time accident detection\ncapabilities and broad-scale applicability. This work lays the foundation for\nintelligent surveillance systems in the future for real-time traffic\nmonitoring, smart city framework, and integration of intelligent surveillance\nsystems into emergency management systems."
    },
    {
        "date": "2025-06",
        "title": "MBA: Multimodal Bidirectional Attack for Referring Expression Segmentation Models",
        "author": "Xingbai Chen, Tingchao Fu, Renyang Liu, Wei Zhou, and Chao Yi",
        "link": "http://arxiv.org/abs/2506.16157v1",
        "abstract": "Referring Expression Segmentation (RES) enables precise object segmentation\nin images based on natural language descriptions, offering high flexibility and\nbroad applicability in real-world vision tasks. Despite its impressive\nperformance, the robustness of RES models against adversarial examples remains\nlargely unexplored. While prior adversarial attack methods have explored\nadversarial robustness on conventional segmentation models, they perform poorly\nwhen directly applied to RES, failing to expose vulnerabilities in its\nmultimodal structure. Moreover, in practical open-world scenarios, users\ntypically issue multiple, diverse referring expressions to interact with the\nsame image, highlighting the need for adversarial examples that generalize\nacross varied textual inputs. To address these multimodal challenges, we\npropose a novel adversarial attack strategy termed \\textbf{Multimodal\nBidirectional Attack}, tailored for RES models. Our method introduces learnable\nproxy textual embedding perturbation and jointly performs visual-aligned\noptimization on the image modality and textual-adversarial optimization on the\ntextual modality during attack generation. This dual optimization framework\nencourages adversarial images to actively adapt to more challenging text\nembedding during optimization, thereby enhancing their cross-text\ntransferability, which refers to the ability of adversarial examples to remain\neffective under a variety of unseen or semantically diverse textual inputs.\nExtensive experiments conducted on multiple RES models and benchmark datasets\ndemonstrate the superior effectiveness of our method compared to existing\nmethods."
    },
    {
        "date": "2025-06",
        "title": "Probing the Robustness of Large Language Models Safety to Latent Perturbations",
        "author": "Tianle Gu, Kexin Huang, Zongqi Wang, Yixu Wang, Jie Li, Yuanqi Yao, Yang Yao, Yujiu Yang, Yan Teng, and Yingchun Wang",
        "link": "http://arxiv.org/abs/2506.16078v1",
        "abstract": "Safety alignment is a key requirement for building reliable Artificial\nGeneral Intelligence. Despite significant advances in safety alignment, we\nobserve that minor latent shifts can still trigger unsafe responses in aligned\nmodels. We argue that this stems from the shallow nature of existing alignment\nmethods, which focus on surface-level refusal behaviors without sufficiently\naltering internal representations. Consequently, small shifts in hidden\nactivations can re-trigger harmful behaviors embedded in the latent space. To\nexplore the robustness of safety alignment to latent perturbations, we\nintroduce a probing method that measures the Negative Log-Likelihood of the\noriginal response generated by the model. This probe quantifies local\nsensitivity in the latent space, serving as a diagnostic tool for identifying\nvulnerable directions. Based on this signal, we construct effective jailbreak\ntrajectories, giving rise to the Activation Steering Attack (ASA). More\nimportantly, these insights offer a principled foundation for improving\nalignment robustness. To this end, we introduce Layer-wise Adversarial Patch\nTraining~(LAPT), a fine-tuning strategy that inject controlled perturbations\ninto hidden representations during training. Experimental results highlight\nthat LAPT strengthen alignment robustness without compromising general\ncapabilities. Our findings reveal fundamental flaws in current alignment\nparadigms and call for representation-level training strategies that move\nbeyond surface-level behavior supervision. Codes and results are available at\nhttps://github.com/Carol-gutianle/LatentSafety."
    },
    {
        "date": "2025-06",
        "title": "A Lightweight RL-Driven Deep Unfolding Network for Robust WMMSE Precoding in Massive MU-MIMO-OFDM Systems",
        "author": "Kexuan Wang, and An Liu",
        "link": "http://arxiv.org/abs/2506.16072v1",
        "abstract": "Weighted Minimum Mean Square Error (WMMSE) precoding is widely recognized for\nits near-optimal weighted sum rate performance. However, its practical\ndeployment in massive multi-user (MU) multiple-input multiple-output (MIMO)\northogonal frequency-division multiplexing (OFDM) systems is hindered by the\nassumption of perfect channel state information (CSI) and high computational\ncomplexity. To address these issues, we first develop a wideband stochastic\nWMMSE (SWMMSE) algorithm that iteratively maximizes the ergodic weighted\nsum-rate (EWSR) under imperfect CSI. Building on this, we propose a lightweight\nreinforcement learning (RL)-driven deep unfolding (DU) network (RLDDU-Net),\nwhere each SWMMSE iteration is mapped to a network layer. Specifically, its DU\nmodule integrates approximation techniques and leverages beam-domain sparsity\nas well as frequency-domain subcarrier correlation, significantly accelerating\nconvergence and reducing computational overhead. Furthermore, the RL module\nadaptively adjusts the network depth and generates compensation matrices to\nmitigate approximation errors. Simulation results under imperfect CSI\ndemonstrate that RLDDU-Net outperforms existing baselines in EWSR performance\nwhile offering superior computational and convergence efficiency."
    },
    {
        "date": "2025-06",
        "title": "Floating-Point Neural Networks Are Provably Robust Universal Approximators",
        "author": "Geonho Hwang, Wonyeol Lee, Yeachan Park, Sejun Park, and Feras Saad",
        "link": "http://arxiv.org/abs/2506.16065v1",
        "abstract": "The classical universal approximation (UA) theorem for neural networks\nestablishes mild conditions under which a feedforward neural network can\napproximate a continuous function $f$ with arbitrary accuracy. A recent result\nshows that neural networks also enjoy a more general interval universal\napproximation (IUA) theorem, in the sense that the abstract interpretation\nsemantics of the network using the interval domain can approximate the direct\nimage map of $f$ (i.e., the result of applying $f$ to a set of inputs) with\narbitrary accuracy. These theorems, however, rest on the unrealistic assumption\nthat the neural network computes over infinitely precise real numbers, whereas\ntheir software implementations in practice compute over finite-precision\nfloating-point numbers. An open question is whether the IUA theorem still holds\nin the floating-point setting.\n  This paper introduces the first IUA theorem for floating-point neural\nnetworks that proves their remarkable ability to perfectly capture the direct\nimage map of any rounded target function $f$, showing no limits exist on their\nexpressiveness. Our IUA theorem in the floating-point setting exhibits material\ndifferences from the real-valued setting, which reflects the fundamental\ndistinctions between these two computational models. This theorem also implies\nsurprising corollaries, which include (i) the existence of provably robust\nfloating-point neural networks; and (ii) the computational completeness of the\nclass of straight-line programs that use only floating-point additions and\nmultiplications for the class of all floating-point programs that halt."
    },
    {
        "date": "2025-06",
        "title": "Efficient Blockchain-based Steganography via Backcalculating Generative Adversarial Network",
        "author": "Zhuo Chen, Jialing He, Jiacheng Wang, Zehui Xiong, Tao Xiang, Liehuang Zhu, and Dusit Niyato",
        "link": "http://arxiv.org/abs/2506.16023v1",
        "abstract": "Blockchain-based steganography enables data hiding via encoding the covert\ndata into a specific blockchain transaction field. However, previous works\nfocus on the specific field-embedding methods while lacking a consideration on\nrequired field-generation embedding. In this paper, we propose a generic\nblockchain-based steganography framework (GBSF). The sender generates the\nrequired fields such as amount and fees, where the additional covert data is\nembedded to enhance the channel capacity. Based on GBSF, we design a reversible\ngenerative adversarial network (R-GAN) that utilizes the generative adversarial\nnetwork with a reversible generator to generate the required fields and encode\nadditional covert data into the input noise of the reversible generator. We\nthen explore the performance flaw of R-GAN. To further improve the performance,\nwe propose R-GAN with Counter-intuitive data preprocessing and Custom\nactivation functions, namely CCR-GAN. The counter-intuitive data preprocessing\n(CIDP) mechanism is used to reduce decoding errors in covert data, while it\nincurs gradient explosion for model convergence. The custom activation function\nnamed ClipSigmoid is devised to overcome the problem. Theoretical justification\nfor CIDP and ClipSigmoid is also provided. We also develop a mechanism named\nT2C, which balances capacity and concealment. We conduct experiments using the\ntransaction amount of the Bitcoin mainnet as the required field to verify the\nfeasibility. We then apply the proposed schemes to other transaction fields and\nblockchains to demonstrate the scalability. Finally, we evaluate capacity and\nconcealment for various blockchains and transaction fields and explore the\ntrade-off between capacity and concealment. The results demonstrate that R-GAN\nand CCR-GAN are able to enhance the channel capacity effectively and outperform\nstate-of-the-art works."
    },
    {
        "date": "2025-06",
        "title": "Quantum Artificial Intelligence for Secure Autonomous Vehicle Navigation: An Architectural Proposal",
        "author": "Hemanth Kannamarlapudi, and Sowmya Chintalapudi",
        "link": "http://arxiv.org/abs/2506.16000v1",
        "abstract": "Navigation is a very crucial aspect of autonomous vehicle ecosystem which\nheavily relies on collecting and processing large amounts of data in various\nstates and taking a confident and safe decision to define the next vehicle\nmaneuver. In this paper, we propose a novel architecture based on Quantum\nArtificial Intelligence by enabling quantum and AI at various levels of\nnavigation decision making and communication process in Autonomous vehicles :\nQuantum Neural Networks for multimodal sensor fusion, Nav-Q for Quantum\nreinforcement learning for navigation policy optimization and finally\npost-quantum cryptographic protocols for secure communication. Quantum neural\nnetworks uses quantum amplitude encoding to fuse data from various sensors like\nLiDAR, radar, camera, GPS and weather etc., This approach gives a unified\nquantum state representation between heterogeneous sensor modalities. Nav-Q\nmodule processes the fused quantum states through variational quantum circuits\nto learn optimal navigation policies under swift dynamic and complex\nconditions. Finally, post quantum cryptographic protocols are used to secure\ncommunication channels for both within vehicle communication and V2X (Vehicle\nto Everything) communications and thus secures the autonomous vehicle\ncommunication from both classical and quantum security threats. Thus, the\nproposed framework addresses fundamental challenges in autonomous vehicles\nnavigation by providing quantum performance and future proof security. Index\nTerms Quantum Computing, Autonomous Vehicles, Sensor Fusion"
    },
    {
        "date": "2025-06",
        "title": "Adversarial Attacks and Detection in Visual Place Recognition for Safer Robot Navigation",
        "author": "Connor Malone, Owen Claxton, Iman Shames, and Michael Milford",
        "link": "http://arxiv.org/abs/2506.15988v1",
        "abstract": "Stand-alone Visual Place Recognition (VPR) systems have little defence\nagainst a well-designed adversarial attack, which can lead to disastrous\nconsequences when deployed for robot navigation. This paper extensively\nanalyzes the effect of four adversarial attacks common in other perception\ntasks and four novel VPR-specific attacks on VPR localization performance. We\nthen propose how to close the loop between VPR, an Adversarial Attack Detector\n(AAD), and active navigation decisions by demonstrating the performance benefit\nof simulated AADs in a novel experiment paradigm -- which we detail for the\nrobotics community to use as a system framework. In the proposed experiment\nparadigm, we see the addition of AADs across a range of detection accuracies\ncan improve performance over baseline; demonstrating a significant improvement\n-- such as a ~50% reduction in the mean along-track localization error -- can\nbe achieved with True Positive and False Positive detection rates of only 75%\nand up to 25% respectively. We examine a variety of metrics including:\nAlong-Track Error, Percentage of Time Attacked, Percentage of Time in an\n`Unsafe' State, and Longest Continuous Time Under Attack. Expanding further on\nthese results, we provide the first investigation into the efficacy of the Fast\nGradient Sign Method (FGSM) adversarial attack for VPR. The analysis in this\nwork highlights the need for AADs in real-world systems for trustworthy\nnavigation, and informs quantitative requirements for system design."
    },
    {
        "date": "2025-06",
        "title": "Double Entendre: Robust Audio-Based AI-Generated Lyrics Detection via Multi-View Fusion",
        "author": "Markus Frohmann, Gabriel Meseguer-Brocal, Markus Schedl, and Elena V. Epure",
        "link": "http://arxiv.org/abs/2506.15981v1",
        "abstract": "The rapid advancement of AI-based music generation tools is revolutionizing\nthe music industry but also posing challenges to artists, copyright holders,\nand providers alike. This necessitates reliable methods for detecting such\nAI-generated content. However, existing detectors, relying on either audio or\nlyrics, face key practical limitations: audio-based detectors fail to\ngeneralize to new or unseen generators and are vulnerable to audio\nperturbations; lyrics-based methods require cleanly formatted and accurate\nlyrics, unavailable in practice. To overcome these limitations, we propose a\nnovel, practically grounded approach: a multimodal, modular late-fusion\npipeline that combines automatically transcribed sung lyrics and speech\nfeatures capturing lyrics-related information within the audio. By relying on\nlyrical aspects directly from audio, our method enhances robustness, mitigates\nsusceptibility to low-level artifacts, and enables practical applicability.\nExperiments show that our method, DE-detect, outperforms existing lyrics-based\ndetectors while also being more robust to audio perturbations. Thus, it offers\nan effective, robust solution for detecting AI-generated music in real-world\nscenarios. Our code is available at\nhttps://github.com/deezer/robust-AI-lyrics-detection."
    },
    {
        "date": "2025-06",
        "title": "TRUST: Transparent, Robust and Ultra-Sparse Trees",
        "author": "Albert Dorador",
        "link": "http://arxiv.org/abs/2506.15791v1",
        "abstract": "Piecewise-constant regression trees remain popular for their\ninterpretability, yet often lag behind black-box models like Random Forest in\npredictive accuracy. In this work, we introduce TRUST (Transparent, Robust, and\nUltra-Sparse Trees), a novel regression tree model that combines the accuracy\nof Random Forests with the interpretability of shallow decision trees and\nsparse linear models. TRUST further enhances transparency by leveraging Large\nLanguage Models to generate tailored, user-friendly explanations. Extensive\nvalidation on synthetic and real-world benchmark datasets demonstrates that\nTRUST consistently outperforms other interpretable models -- including CART,\nLasso, and Node Harvest -- in predictive accuracy, while matching the accuracy\nof Random Forest and offering substantial gains in both accuracy and\ninterpretability over M5', a well-established model that is conceptually\nrelated."
    },
    {
        "date": "2025-06",
        "title": "CAWR: Corruption-Averse Advantage-Weighted Regression for Robust Policy Optimization",
        "author": "Ranting Hu",
        "link": "http://arxiv.org/abs/2506.15654v1",
        "abstract": "Offline reinforcement learning (offline RL) algorithms often require\nadditional constraints or penalty terms to address distribution shift issues,\nsuch as adding implicit or explicit policy constraints during policy\noptimization to reduce the estimation bias of functions. This paper focuses on\na limitation of the Advantage-Weighted Regression family (AWRs), i.e., the\npotential for learning over-conservative policies due to data corruption,\nspecifically the poor explorations in suboptimal offline data. We study it from\ntwo perspectives: (1) how poor explorations impact the theoretically optimal\npolicy based on KL divergence, and (2) how such poor explorations affect the\napproximation of the theoretically optimal policy. We prove that such\nover-conservatism is mainly caused by the sensitivity of the loss function for\npolicy optimization to poor explorations, and the proportion of poor\nexplorations in offline datasets. To address this concern, we propose\nCorruption-Averse Advantage-Weighted Regression (CAWR), which incorporates a\nset of robust loss functions during policy optimization and an advantage-based\nprioritized experience replay method to filter out poor explorations. Numerical\nexperiments on the D4RL benchmark show that our method can learn superior\npolicies from suboptimal offline data, significantly enhancing the performance\nof policy optimization."
    },
    {
        "date": "2025-06",
        "title": "Insights on Adversarial Attacks for Tabular Machine Learning via a Systematic Literature Review",
        "author": "Salijona Dyrmishi, Mohamed Djilani, Thibault Simonetto, Salah Ghamizi, and Maxime Cordy",
        "link": "http://arxiv.org/abs/2506.15506v1",
        "abstract": "Adversarial attacks in machine learning have been extensively reviewed in\nareas like computer vision and NLP, but research on tabular data remains\nscattered. This paper provides the first systematic literature review focused\non adversarial attacks targeting tabular machine learning models. We highlight\nkey trends, categorize attack strategies and analyze how they address practical\nconsiderations for real-world applicability. Additionally, we outline current\nchallenges and open research questions. By offering a clear and structured\noverview, this review aims to guide future efforts in understanding and\naddressing adversarial vulnerabilities in tabular machine learning."
    },
    {
        "date": "2025-06",
        "title": "Semi-supervised Graph Anomaly Detection via Robust Homophily Learning",
        "author": "Guoguo Ai, Hezhe Qiao, Hui Yan, and Guansong Pang",
        "link": "http://arxiv.org/abs/2506.15448v1",
        "abstract": "Semi-supervised graph anomaly detection (GAD) utilizes a small set of labeled\nnormal nodes to identify abnormal nodes from a large set of unlabeled nodes in\na graph. Current methods in this line posit that 1) normal nodes share a\nsimilar level of homophily and 2) the labeled normal nodes can well represent\nthe homophily patterns in the normal class. However, this assumption often does\nnot hold well since normal nodes in a graph can exhibit diverse homophily in\nreal-world GAD datasets. In this paper, we propose RHO, namely Robust Homophily\nLearning, to adaptively learn such homophily patterns. RHO consists of two\nnovel modules, adaptive frequency response filters (AdaFreq) and graph\nnormality alignment (GNA). AdaFreq learns a set of adaptive spectral filters\nthat capture different frequency components of the labeled normal nodes with\nvarying homophily in the channel-wise and cross-channel views of node\nattributes. GNA is introduced to enforce consistency between the channel-wise\nand cross-channel homophily representations to robustify the normality learned\nby the filters in the two views. Experiments on eight real-world GAD datasets\nshow that RHO can effectively learn varying, often under-represented, homophily\nin the small normal node set and substantially outperforms state-of-the-art\ncompeting methods. Code is available at https://github.com/mala-lab/RHO."
    },
    {
        "date": "2025-06",
        "title": "VLMInferSlow: Evaluating the Efficiency Robustness of Large Vision-Language Models as a Service",
        "author": "Xiasi Wang, Tianliang Yao, Simin Chen, Runqi Wang, Lei YE, Kuofeng Gao, Yi Huang, and Yuan Yao",
        "link": "http://arxiv.org/abs/2506.15755v1",
        "abstract": "Vision-Language Models (VLMs) have demonstrated great potential in real-world\napplications. While existing research primarily focuses on improving their\naccuracy, the efficiency remains underexplored. Given the real-time demands of\nmany applications and the high inference overhead of VLMs, efficiency\nrobustness is a critical issue. However, previous studies evaluate efficiency\nrobustness under unrealistic assumptions, requiring access to the model\narchitecture and parameters -- an impractical scenario in ML-as-a-service\nsettings, where VLMs are deployed via inference APIs. To address this gap, we\npropose VLMInferSlow, a novel approach for evaluating VLM efficiency robustness\nin a realistic black-box setting. VLMInferSlow incorporates fine-grained\nefficiency modeling tailored to VLM inference and leverages zero-order\noptimization to search for adversarial examples. Experimental results show that\nVLMInferSlow generates adversarial images with imperceptible perturbations,\nincreasing the computational cost by up to 128.47%. We hope this research\nraises the community's awareness about the efficiency robustness of VLMs."
    },
    {
        "date": "2025-06",
        "title": "RAS-Eval: A Comprehensive Benchmark for Security Evaluation of LLM Agents in Real-World Environments",
        "author": "Yuchuan Fu, Xiaohan Yuan, and Dongxia Wang",
        "link": "http://arxiv.org/abs/2506.15253v1",
        "abstract": "The rapid deployment of Large language model (LLM) agents in critical domains\nlike healthcare and finance necessitates robust security frameworks. To address\nthe absence of standardized evaluation benchmarks for these agents in dynamic\nenvironments, we introduce RAS-Eval, a comprehensive security benchmark\nsupporting both simulated and real-world tool execution. RAS-Eval comprises 80\ntest cases and 3,802 attack tasks mapped to 11 Common Weakness Enumeration\n(CWE) categories, with tools implemented in JSON, LangGraph, and Model Context\nProtocol (MCP) formats. We evaluate 6 state-of-the-art LLMs across diverse\nscenarios, revealing significant vulnerabilities: attacks reduced agent task\ncompletion rates (TCR) by 36.78% on average and achieved an 85.65% success rate\nin academic settings. Notably, scaling laws held for security capabilities,\nwith larger models outperforming smaller counterparts. Our findings expose\ncritical risks in real-world agent deployments and provide a foundational\nframework for future security research. Code and data are available at\nhttps://github.com/lanzer-tree/RAS-Eval."
    },
    {
        "date": "2025-06",
        "title": "RA-NeRF: Robust Neural Radiance Field Reconstruction with Accurate Camera Pose Estimation under Complex Trajectories",
        "author": "Qingsong Yan, Qiang Wang, Kaiyong Zhao, Jie Chen, Bo Li, Xiaowen Chu, and Fei Deng",
        "link": "http://arxiv.org/abs/2506.15242v1",
        "abstract": "Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have emerged\nas powerful tools for 3D reconstruction and SLAM tasks. However, their\nperformance depends heavily on accurate camera pose priors. Existing approaches\nattempt to address this issue by introducing external constraints but fall\nshort of achieving satisfactory accuracy, particularly when camera trajectories\nare complex. In this paper, we propose a novel method, RA-NeRF, capable of\npredicting highly accurate camera poses even with complex camera trajectories.\nFollowing the incremental pipeline, RA-NeRF reconstructs the scene using NeRF\nwith photometric consistency and incorporates flow-driven pose regulation to\nenhance robustness during initialization and localization. Additionally,\nRA-NeRF employs an implicit pose filter to capture the camera movement pattern\nand eliminate the noise for pose estimation. To validate our method, we conduct\nextensive experiments on the Tanks\\&Temple dataset for standard evaluation, as\nwell as the NeRFBuster dataset, which presents challenging camera pose\ntrajectories. On both datasets, RA-NeRF achieves state-of-the-art results in\nboth camera pose estimation and visual quality, demonstrating its effectiveness\nand robustness in scene reconstruction under complex pose trajectories."
    },
    {
        "date": "2025-06",
        "title": "From LLMs to MLLMs to Agents: A Survey of Emerging Paradigms in Jailbreak Attacks and Defenses within LLM Ecosystem",
        "author": "Yanxu Mao, Tiehan Cui, Peipei Liu, Datao You, and Hongsong Zhu",
        "link": "http://arxiv.org/abs/2506.15170v1",
        "abstract": "Large language models (LLMs) are rapidly evolving from single-modal systems\nto multimodal LLMs and intelligent agents, significantly expanding their\ncapabilities while introducing increasingly severe security risks. This paper\npresents a systematic survey of the growing complexity of jailbreak attacks and\ncorresponding defense mechanisms within the expanding LLM ecosystem. We first\ntrace the developmental trajectory from LLMs to MLLMs and Agents, highlighting\nthe core security challenges emerging at each stage. Next, we categorize\nmainstream jailbreak techniques from both the attack impact and visibility\nperspectives, and provide a comprehensive analysis of representative attack\nmethods, related datasets, and evaluation metrics. On the defense side, we\norganize existing strategies based on response timing and technical approach,\noffering a structured understanding of their applicability and implementation.\nFurthermore, we identify key limitations in existing surveys, such as\ninsufficient attention to agent-specific security issues, the absence of a\nclear taxonomy for hybrid jailbreak methods, a lack of detailed analysis of\nexperimental setups, and outdated coverage of recent advancements. To address\nthese limitations, we provide an updated synthesis of recent work and outline\nfuture research directions in areas such as dataset construction, evaluation\nframework optimization, and strategy generalization. Our study seeks to enhance\nthe understanding of jailbreak mechanisms and facilitate the advancement of\nmore resilient and adaptive defense strategies in the context of ever more\ncapable LLMs."
    },
    {
        "date": "2025-06",
        "title": "Echo-DND: A dual noise diffusion model for robust and precise left ventricle segmentation in echocardiography",
        "author": "Abdur Rahman, Keerthiveena Balraj, Manojkumar Ramteke, and Anurag Singh Rathore",
        "link": "http://arxiv.org/abs/2506.15166v1",
        "abstract": "Recent advancements in diffusion probabilistic models (DPMs) have\nrevolutionized image processing, demonstrating significant potential in medical\napplications. Accurate segmentation of the left ventricle (LV) in\nechocardiograms is crucial for diagnostic procedures and necessary treatments.\nHowever, ultrasound images are notoriously noisy with low contrast and\nambiguous LV boundaries, thereby complicating the segmentation process. To\naddress these challenges, this paper introduces Echo-DND, a novel dual-noise\ndiffusion model specifically designed for this task. Echo-DND leverages a\nunique combination of Gaussian and Bernoulli noises. It also incorporates a\nmulti-scale fusion conditioning module to improve segmentation precision.\nFurthermore, it utilizes spatial coherence calibration to maintain spatial\nintegrity in segmentation masks. The model's performance was rigorously\nvalidated on the CAMUS and EchoNet-Dynamic datasets. Extensive evaluations\ndemonstrate that the proposed framework outperforms existing SOTA models. It\nachieves high Dice scores of 0.962 and 0.939 on these datasets, respectively.\nThe proposed Echo-DND model establishes a new standard in echocardiogram\nsegmentation, and its architecture holds promise for broader applicability in\nother medical imaging tasks, potentially improving diagnostic accuracy across\nvarious medical domains. Project page: https://abdur75648.github.io/Echo-DND"
    },
    {
        "date": "2025-06",
        "title": "Robust Instant Policy: Leveraging Student's t-Regression Model for Robust In-context Imitation Learning of Robot Manipulation",
        "author": "Hanbit Oh, Andrea M. Salcedo-V\u00e1zquez, Ixchel G. Ramirez-Alpizar, and Yukiyasu Domae",
        "link": "http://arxiv.org/abs/2506.15157v1",
        "abstract": "Imitation learning (IL) aims to enable robots to perform tasks autonomously\nby observing a few human demonstrations. Recently, a variant of IL, called\nIn-Context IL, utilized off-the-shelf large language models (LLMs) as instant\npolicies that understand the context from a few given demonstrations to perform\na new task, rather than explicitly updating network models with large-scale\ndemonstrations. However, its reliability in the robotics domain is undermined\nby hallucination issues such as LLM-based instant policy, which occasionally\ngenerates poor trajectories that deviate from the given demonstrations. To\nalleviate this problem, we propose a new robust in-context imitation learning\nalgorithm called the robust instant policy (RIP), which utilizes a Student's\nt-regression model to be robust against the hallucinated trajectories of\ninstant policies to allow reliable trajectory generation. Specifically, RIP\ngenerates several candidate robot trajectories to complete a given task from an\nLLM and aggregates them using the Student's t-distribution, which is beneficial\nfor ignoring outliers (i.e., hallucinations); thereby, a robust trajectory\nagainst hallucinations is generated. Our experiments, conducted in both\nsimulated and real-world environments, show that RIP significantly outperforms\nstate-of-the-art IL methods, with at least $26\\%$ improvement in task success\nrates, particularly in low-data scenarios for everyday tasks. Video results\navailable at https://sites.google.com/view/robustinstantpolicy."
    },
    {
        "date": "2025-06",
        "title": "Diffusion-based Counterfactual Augmentation: Towards Robust and Interpretable Knee Osteoarthritis Grading",
        "author": "Zhe Wang, Yuhua Ru, Aladine Chetouani, Tina Shiang, Fang Chen, Fabian Bauer, Liping Zhang, Didier Hans, Rachid Jennane, William Ewing Palmer, Mohamed Jarraya, and Yung Hsin Chen",
        "link": "http://arxiv.org/abs/2506.15748v1",
        "abstract": "Automated grading of Knee Osteoarthritis (KOA) from radiographs is challenged\nby significant inter-observer variability and the limited robustness of deep\nlearning models, particularly near critical decision boundaries. To address\nthese limitations, this paper proposes a novel framework, Diffusion-based\nCounterfactual Augmentation (DCA), which enhances model robustness and\ninterpretability by generating targeted counterfactual examples. The method\nnavigates the latent space of a diffusion model using a Stochastic Differential\nEquation (SDE), governed by balancing a classifier-informed boundary drive with\na manifold constraint. The resulting counterfactuals are then used within a\nself-corrective learning strategy to improve the classifier by focusing on its\nspecific areas of uncertainty. Extensive experiments on the public\nOsteoarthritis Initiative (OAI) and Multicenter Osteoarthritis Study (MOST)\ndatasets demonstrate that this approach significantly improves classification\naccuracy across multiple model architectures. Furthermore, the method provides\ninterpretability by visualizing minimal pathological changes and revealing that\nthe learned latent space topology aligns with clinical knowledge of KOA\nprogression. The DCA framework effectively converts model uncertainty into a\nrobust training signal, offering a promising pathway to developing more\naccurate and trustworthy automated diagnostic systems. Our code is available at\nhttps://github.com/ZWang78/DCA."
    },
    {
        "date": "2025-06",
        "title": "EVA-S2PMLP: Secure and Scalable Two-Party MLP via Spatial Transformation",
        "author": "Shizhao Peng, Shoumo Li, and Tianle Tao",
        "link": "http://arxiv.org/abs/2506.15102v1",
        "abstract": "Privacy-preserving neural network training in vertically partitioned\nscenarios is vital for secure collaborative modeling across institutions. This\npaper presents \\textbf{EVA-S2PMLP}, an Efficient, Verifiable, and Accurate\nSecure Two-Party Multi-Layer Perceptron framework that introduces spatial-scale\noptimization for enhanced privacy and performance. To enable reliable\ncomputation under real-number domain, EVA-S2PMLP proposes a secure\ntransformation pipeline that maps scalar inputs to vector and matrix spaces\nwhile preserving correctness. The framework includes a suite of atomic\nprotocols for linear and non-linear secure computations, with modular support\nfor secure activation, matrix-vector operations, and loss evaluation.\nTheoretical analysis confirms the reliability, security, and asymptotic\ncomplexity of each protocol. Extensive experiments show that EVA-S2PMLP\nachieves high inference accuracy and significantly reduced communication\noverhead, with up to $12.3\\times$ improvement over baselines. Evaluation on\nbenchmark datasets demonstrates that the framework maintains model utility\nwhile ensuring strict data confidentiality, making it a practical solution for\nprivacy-preserving neural network training in finance, healthcare, and\ncross-organizational AI applications."
    },
    {
        "date": "2025-06",
        "title": "International Security Applications of Flexible Hardware-Enabled Guarantees",
        "author": "Onni Aarne, and James Petrie",
        "link": "http://arxiv.org/abs/2506.15100v1",
        "abstract": "As AI capabilities advance rapidly, flexible hardware-enabled guarantees\n(flexHEGs) offer opportunities to address international security challenges\nthrough comprehensive governance frameworks. This report examines how flexHEGs\ncould enable internationally trustworthy AI governance by establishing\nstandardized designs, robust ecosystem defenses, and clear operational\nparameters for AI-relevant chips. We analyze four critical international\nsecurity applications: limiting proliferation to address malicious use,\nimplementing safety norms to prevent loss of control, managing risks from\nmilitary AI systems, and supporting strategic stability through\nbalance-of-power mechanisms while respecting national sovereignty. The report\nexplores both targeted deployments for specific high-risk facilities and\ncomprehensive deployments covering all AI-relevant compute. We examine two\nprimary governance models: verification-based agreements that enable\ntransparent compliance monitoring, and ruleset-based agreements that\nautomatically enforce international rules through cryptographically-signed\nupdates. Through game-theoretic analysis, we demonstrate that comprehensive\nflexHEG agreements could remain stable under reasonable assumptions about state\npreferences and catastrophic risks. The report addresses critical\nimplementation challenges including technical thresholds for AI-relevant chips,\nmanagement of existing non-flexHEG hardware, and safeguards against abuse of\ngovernance power. While requiring significant international coordination,\nflexHEGs could provide a technical foundation for managing AI risks at the\nscale and speed necessary to address emerging threats to international security\nand stability."
    },
    {
        "date": "2025-06",
        "title": "Toward a Lightweight, Scalable, and Parallel Secure Encryption Engine",
        "author": "Rasha Karakchi, Rye Stahle-Smith, Nishant Chinnasami, and Tiffany Yu",
        "link": "http://arxiv.org/abs/2506.15070v1",
        "abstract": "The exponential growth of Internet of Things (IoT) applications has\nintensified the demand for efficient, high-throughput, and energy-efficient\ndata processing at the edge. Conventional CPU-centric encryption methods suffer\nfrom performance bottlenecks and excessive data movement, especially in\nlatency-sensitive and resource-constrained environments. In this paper, we\npresent SPiME, a lightweight, scalable, and FPGA-compatible Secure\nProcessor-in-Memory Encryption architecture that integrates the Advanced\nEncryption Standard (AES-128) directly into a Processing-in-Memory (PiM)\nframework. SPiME is designed as a modular array of parallel PiM units, each\ncombining an AES core with a minimal control unit to enable distributed\nin-place encryption with minimal overhead. The architecture is fully\nimplemented in Verilog and tested on multiple AMD UltraScale and UltraScale+\nFPGAs. Evaluation results show that SPiME can scale beyond 4,000 parallel units\nwhile maintaining less than 5\\% utilization of key FPGA resources on high-end\ndevices. It delivers over 25~Gbps in sustained encryption throughput with\npredictable, low-latency performance. The design's portability,\nconfigurability, and resource efficiency make it a compelling solution for\nsecure edge computing, embedded cryptographic systems, and customizable\nhardware accelerators."
    },
    {
        "date": "2025-06",
        "title": "Systems-Theoretic and Data-Driven Security Analysis in ML-enabled Medical Devices",
        "author": "Gargi Mitra, Mohammadreza Hallajiyan, Inji Kim, Athish Pranav Dharmalingam, Mohammed Elnawawy, Shahrear Iqbal, Karthik Pattabiraman, and Homa Alemzadeh",
        "link": "http://arxiv.org/abs/2506.15028v1",
        "abstract": "The integration of AI/ML into medical devices is rapidly transforming\nhealthcare by enhancing diagnostic and treatment facilities. However, this\nadvancement also introduces serious cybersecurity risks due to the use of\ncomplex and often opaque models, extensive interconnectivity, interoperability\nwith third-party peripheral devices, Internet connectivity, and vulnerabilities\nin the underlying technologies. These factors contribute to a broad attack\nsurface and make threat prevention, detection, and mitigation challenging.\nGiven the highly safety-critical nature of these devices, a cyberattack on\nthese devices can cause the ML models to mispredict, thereby posing significant\nsafety risks to patients. Therefore, ensuring the security of these devices\nfrom the time of design is essential. This paper underscores the urgency of\naddressing the cybersecurity challenges in ML-enabled medical devices at the\npre-market phase. We begin by analyzing publicly available data on device\nrecalls and adverse events, and known vulnerabilities, to understand the threat\nlandscape of AI/ML-enabled medical devices and their repercussions on patient\nsafety. Building on this analysis, we introduce a suite of tools and techniques\ndesigned by us to assist security analysts in conducting comprehensive\npremarket risk assessments. Our work aims to empower manufacturers to embed\ncybersecurity as a core design principle in AI/ML-enabled medical devices,\nthereby making them safe for patients."
    },
    {
        "date": "2025-06",
        "title": "FORTRESS: Frontier Risk Evaluation for National Security and Public Safety",
        "author": "Christina Q. Knight, Kaustubh Deshpande, Ved Sirdeshmukh, Meher Mankikar, Scale Red Team, SEAL Research Team, and Julian Michael",
        "link": "http://arxiv.org/abs/2506.14922v1",
        "abstract": "The rapid advancement of large language models (LLMs) introduces dual-use\ncapabilities that could both threaten and bolster national security and public\nsafety (NSPS). Models implement safeguards to protect against potential misuse\nrelevant to NSPS and allow for benign users to receive helpful information.\nHowever, current benchmarks often fail to test safeguard robustness to\npotential NSPS risks in an objective, robust way. We introduce FORTRESS: 500\nexpert-crafted adversarial prompts with instance-based rubrics of 4-7 binary\nquestions for automated evaluation across 3 domains (unclassified information\nonly): Chemical, Biological, Radiological, Nuclear and Explosive (CBRNE),\nPolitical Violence & Terrorism, and Criminal & Financial Illicit Activities,\nwith 10 total subcategories across these domains. Each prompt-rubric pair has a\ncorresponding benign version to test for model over-refusals. This evaluation\nof frontier LLMs' safeguard robustness reveals varying trade-offs between\npotential risks and model usefulness: Claude-3.5-Sonnet demonstrates a low\naverage risk score (ARS) (14.09 out of 100) but the highest over-refusal score\n(ORS) (21.8 out of 100), while Gemini 2.5 Pro shows low over-refusal (1.4) but\na high average potential risk (66.29). Deepseek-R1 has the highest ARS at\n78.05, but the lowest ORS at only 0.06. Models such as o1 display a more even\ntrade-off between potential risks and over-refusals (with an ARS of 21.69 and\nORS of 5.2). To provide policymakers and researchers with a clear understanding\nof models' potential risks, we publicly release FORTRESS at\nhttps://huggingface.co/datasets/ScaleAI/fortress_public. We also maintain a\nprivate set for evaluation."
    },
    {
        "date": "2025-06",
        "title": "Frequency-Calibrated Membership Inference Attacks on Medical Image Diffusion Models",
        "author": "Xinkai Zhao, Yuta Tokuoka, Junichiro Iwasawa, and Keita Oda",
        "link": "http://arxiv.org/abs/2506.14919v1",
        "abstract": "The increasing use of diffusion models for image generation, especially in\nsensitive areas like medical imaging, has raised significant privacy concerns.\nMembership Inference Attack (MIA) has emerged as a potential approach to\ndetermine if a specific image was used to train a diffusion model, thus\nquantifying privacy risks. Existing MIA methods often rely on diffusion\nreconstruction errors, where member images are expected to have lower\nreconstruction errors than non-member images. However, applying these methods\ndirectly to medical images faces challenges. Reconstruction error is influenced\nby inherent image difficulty, and diffusion models struggle with high-frequency\ndetail reconstruction. To address these issues, we propose a\nFrequency-Calibrated Reconstruction Error (FCRE) method for MIAs on medical\nimage diffusion models. By focusing on reconstruction errors within a specific\nmid-frequency range and excluding both high-frequency (difficult to\nreconstruct) and low-frequency (less informative) regions, our\nfrequency-selective approach mitigates the confounding factor of inherent image\ndifficulty. Specifically, we analyze the reverse diffusion process, obtain the\nmid-frequency reconstruction error, and compute the structural similarity index\nscore between the reconstructed and original images. Membership is determined\nby comparing this score to a threshold. Experiments on several medical image\ndatasets demonstrate that our FCRE method outperforms existing MIA methods."
    },
    {
        "date": "2025-06",
        "title": "CDP: Towards Robust Autoregressive Visuomotor Policy Learning via Causal Diffusion",
        "author": "Jiahua Ma, Yiran Qin, Yixiong Li, Xuanqi Liao, Yulan Guo, and Ruimao Zhang",
        "link": "http://arxiv.org/abs/2506.14769v1",
        "abstract": "Diffusion Policy (DP) enables robots to learn complex behaviors by imitating\nexpert demonstrations through action diffusion. However, in practical\napplications, hardware limitations often degrade data quality, while real-time\nconstraints restrict model inference to instantaneous state and scene\nobservations. These limitations seriously reduce the efficacy of learning from\nexpert demonstrations, resulting in failures in object localization, grasp\nplanning, and long-horizon task execution. To address these challenges, we\npropose Causal Diffusion Policy (CDP), a novel transformer-based diffusion\nmodel that enhances action prediction by conditioning on historical action\nsequences, thereby enabling more coherent and context-aware visuomotor policy\nlearning. To further mitigate the computational cost associated with\nautoregressive inference, a caching mechanism is also introduced to store\nattention key-value pairs from previous timesteps, substantially reducing\nredundant computations during execution. Extensive experiments in both\nsimulated and real-world environments, spanning diverse 2D and 3D manipulation\ntasks, demonstrate that CDP uniquely leverages historical action sequences to\nachieve significantly higher accuracy than existing methods. Moreover, even\nwhen faced with degraded input observation quality, CDP maintains remarkable\nprecision by reasoning through temporal continuity, which highlights its\npractical robustness for robotic control under realistic, imperfect conditions."
    },
    {
        "date": "2025-06",
        "title": "SCISSOR: Mitigating Semantic Bias through Cluster-Aware Siamese Networks for Robust Classification",
        "author": "Shuo Yang, Bardh Prenkaj, and Gjergji Kasneci",
        "link": "http://arxiv.org/abs/2506.14587v2",
        "abstract": "Shortcut learning undermines model generalization to out-of-distribution\ndata. While the literature attributes shortcuts to biases in superficial\nfeatures, we show that imbalances in the semantic distribution of sample\nembeddings induce spurious semantic correlations, compromising model\nrobustness. To address this issue, we propose SCISSOR (Semantic Cluster\nIntervention for Suppressing ShORtcut), a Siamese network-based debiasing\napproach that remaps the semantic space by discouraging latent clusters\nexploited as shortcuts. Unlike prior data-debiasing approaches, SCISSOR\neliminates the need for data augmentation and rewriting. We evaluate SCISSOR on\n6 models across 4 benchmarks: Chest-XRay and Not-MNIST in computer vision, and\nGYAFC and Yelp in NLP tasks. Compared to several baselines, SCISSOR reports\n+5.3 absolute points in F1 score on GYAFC, +7.3 on Yelp, +7.7 on Chest-XRay,\nand +1 on Not-MNIST. SCISSOR is also highly advantageous for lightweight models\nwith ~9.5% improvement on F1 for ViT on computer vision datasets and ~11.9% for\nBERT on NLP. Our study redefines the landscape of model generalization by\naddressing overlooked semantic biases, establishing SCISSOR as a foundational\nframework for mitigating shortcut learning and fostering more robust,\nbias-resistant AI systems."
    },
    {
        "date": "2025-06",
        "title": "Busting the Paper Ballot: Voting Meets Adversarial Machine Learning",
        "author": "Kaleel Mahmood, Caleb Manicke, Ethan Rathbun, Aayushi Verma, Sohaib Ahmad, Nicholas Stamatakis, Laurent Michel, and Benjamin Fuller",
        "link": "http://arxiv.org/abs/2506.14582v1",
        "abstract": "We show the security risk associated with using machine learning classifiers\nin United States election tabulators. The central classification task in\nelection tabulation is deciding whether a mark does or does not appear on a\nbubble associated to an alternative in a contest on the ballot. Barretto et al.\n(E-Vote-ID 2021) reported that convolutional neural networks are a viable\noption in this field, as they outperform simple feature-based classifiers.\n  Our contributions to election security can be divided into four parts. To\ndemonstrate and analyze the hypothetical vulnerability of machine learning\nmodels on election tabulators, we first introduce four new ballot datasets.\nSecond, we train and test a variety of different models on our new datasets.\nThese models include support vector machines, convolutional neural networks (a\nbasic CNN, VGG and ResNet), and vision transformers (Twins and CaiT). Third,\nusing our new datasets and trained models, we demonstrate that traditional\nwhite box attacks are ineffective in the voting domain due to gradient masking.\nOur analyses further reveal that gradient masking is a product of numerical\ninstability. We use a modified difference of logits ratio loss to overcome this\nissue (Croce and Hein, ICML 2020). Fourth, in the physical world, we conduct\nattacks with the adversarial examples generated using our new methods. In\ntraditional adversarial machine learning, a high (50% or greater) attack\nsuccess rate is ideal. However, for certain elections, even a 5% attack success\nrate can flip the outcome of a race. We show such an impact is possible in the\nphysical domain. We thoroughly discuss attack realism, and the challenges and\npracticality associated with printing and scanning ballot adversarial examples."
    },
    {
        "date": "2025-06",
        "title": "Doppelg\u00e4nger Method: Breaking Role Consistency in LLM Agent via Prompt-based Transferable Adversarial Attack",
        "author": "Daewon Kang, YeongHwan Shin, Doyeon Kim, Kyu-Hwan Jung, and Meong Hi Son",
        "link": "http://arxiv.org/abs/2506.14539v1",
        "abstract": "Since the advent of large language models, prompt engineering now enables the\nrapid, low-effort creation of diverse autonomous agents that are already in\nwidespread use. Yet this convenience raises urgent concerns about the safety,\nrobustness, and behavioral consistency of the underlying prompts, along with\nthe pressing challenge of preventing those prompts from being exposed to user's\nattempts. In this paper, we propose the ''Doppelg\\\"anger method'' to\ndemonstrate the risk of an agent being hijacked, thereby exposing system\ninstructions and internal information. Next, we define the ''Prompt Alignment\nCollapse under Adversarial Transfer (PACAT)'' level to evaluate the\nvulnerability to this adversarial transfer attack. We also propose a ''Caution\nfor Adversarial Transfer (CAT)'' prompt to counter the Doppelg\\\"anger method.\nThe experimental results demonstrate that the Doppelg\\\"anger method can\ncompromise the agent's consistency and expose its internal information. In\ncontrast, CAT prompts enable effective defense against this adversarial attack."
    },
    {
        "date": "2025-06",
        "title": "I Speak and You Find: Robust 3D Visual Grounding with Noisy and Ambiguous Speech Inputs",
        "author": "Yu Qi, Lipeng Gu, Honghua Chen, Liangliang Nan, and Mingqiang Wei",
        "link": "http://arxiv.org/abs/2506.14495v1",
        "abstract": "Existing 3D visual grounding methods rely on precise text prompts to locate\nobjects within 3D scenes. Speech, as a natural and intuitive modality, offers a\npromising alternative. Real-world speech inputs, however, often suffer from\ntranscription errors due to accents, background noise, and varying speech\nrates, limiting the applicability of existing 3DVG methods. To address these\nchallenges, we propose \\textbf{SpeechRefer}, a novel 3DVG framework designed to\nenhance performance in the presence of noisy and ambiguous speech-to-text\ntranscriptions. SpeechRefer integrates seamlessly with xisting 3DVG models and\nintroduces two key innovations. First, the Speech Complementary Module captures\nacoustic similarities between phonetically related words and highlights subtle\ndistinctions, generating complementary proposal scores from the speech signal.\nThis reduces dependence on potentially erroneous transcriptions. Second, the\nContrastive Complementary Module employs contrastive learning to align\nerroneous text features with corresponding speech features, ensuring robust\nperformance even when transcription errors dominate. Extensive experiments on\nthe SpeechRefer and peechNr3D datasets demonstrate that SpeechRefer improves\nthe performance of existing 3DVG methods by a large margin, which highlights\nSpeechRefer's potential to bridge the gap between noisy speech inputs and\nreliable 3DVG, enabling more intuitive and practical multimodal systems."
    },
    {
        "date": "2025-06",
        "title": "LingoLoop Attack: Trapping MLLMs via Linguistic Context and State Entrapment into Endless Loops",
        "author": "Jiyuan Fu, Kaixun Jiang, Lingyi Hong, Jinglun Li, Haijing Guo, Dingkang Yang, Zhaoyu Chen, and Wenqiang Zhang",
        "link": "http://arxiv.org/abs/2506.14493v1",
        "abstract": "Multimodal Large Language Models (MLLMs) have shown great promise but require\nsubstantial computational resources during inference. Attackers can exploit\nthis by inducing excessive output, leading to resource exhaustion and service\ndegradation. Prior energy-latency attacks aim to increase generation time by\nbroadly shifting the output token distribution away from the EOS token, but\nthey neglect the influence of token-level Part-of-Speech (POS) characteristics\non EOS and sentence-level structural patterns on output counts, limiting their\nefficacy. To address this, we propose LingoLoop, an attack designed to induce\nMLLMs to generate excessively verbose and repetitive sequences. First, we find\nthat the POS tag of a token strongly affects the likelihood of generating an\nEOS token. Based on this insight, we propose a POS-Aware Delay Mechanism to\npostpone EOS token generation by adjusting attention weights guided by POS\ninformation. Second, we identify that constraining output diversity to induce\nrepetitive loops is effective for sustained generation. We introduce a\nGenerative Path Pruning Mechanism that limits the magnitude of hidden states,\nencouraging the model to produce persistent loops. Extensive experiments\ndemonstrate LingoLoop can increase generated tokens by up to 30 times and\nenergy consumption by a comparable factor on models like Qwen2.5-VL-3B,\nconsistently driving MLLMs towards their maximum generation limits. These\nfindings expose significant MLLMs' vulnerabilities, posing challenges for their\nreliable deployment. The code will be released publicly following the paper's\nacceptance."
    },
    {
        "date": "2025-06",
        "title": "ReDASH: Fast and efficient Scaling in Arithmetic Garbled Circuits for Secure Outsourced Inference",
        "author": "Felix Maurer, Jonas Sander, and Thomas Eisenbarth",
        "link": "http://arxiv.org/abs/2506.14489v1",
        "abstract": "ReDash extends Dash's arithmetic garbled circuits to provide a more flexible\nand efficient framework for secure outsourced inference. By introducing a novel\ngarbled scaling gadget based on a generalized base extension for the residue\nnumber system, ReDash removes Dash's limitation of scaling exclusively by\npowers of two. This enables arbitrary scaling factors drawn from the residue\nnumber system's modular base, allowing for tailored quantization schemes and\nmore efficient model evaluation.\n  Through the new $\\text{ScaleQuant}^+$ quantization mechanism, ReDash supports\noptimized modular bases that can significantly reduce the overhead of\narithmetic operations during convolutional neural network inference. ReDash\nachieves up to a 33-fold speedup in overall inference time compared to Dash\nDespite these enhancements, ReDash preserves the robust security guarantees of\narithmetic garbling. By delivering both performance gains and quantization\nflexibility, ReDash expands the practicality of garbled convolutional neural\nnetwork inference."
    },
    {
        "date": "2025-06",
        "title": "GUI-Robust: A Comprehensive Dataset for Testing GUI Agent Robustness in Real-World Anomalies",
        "author": "Jingqi Yang, Zhilong Song, Jiawei Chen, Mingli Song, Sheng Zhou, linjun sun, Xiaogang Ouyang, Chun Chen, and Can Wang",
        "link": "http://arxiv.org/abs/2506.14477v1",
        "abstract": "The development of high-quality datasets is crucial for benchmarking and\nadvancing research in Graphical User Interface (GUI) agents. Despite their\nimportance, existing datasets are often constructed under idealized conditions,\noverlooking the diverse anomalies frequently encountered in real-world\ndeployments. To address this limitation, we introduce GUI-Robust, a novel\ndataset designed for comprehensive GUI agent evaluation, explicitly\nincorporating seven common types of anomalies observed in everyday GUI\ninteractions. Furthermore, we propose a semi-automated dataset construction\nparadigm that collects user action sequences from natural interactions via RPA\ntools and then generate corresponding step and task descriptions for these\nactions with the assistance of MLLMs. This paradigm significantly reduces\nannotation time cost by a factor of over 19 times. Finally, we assess\nstate-of-the-art GUI agents using the GUI-Robust dataset, revealing their\nsubstantial performance degradation in abnormal scenarios. We anticipate that\nour work will highlight the importance of robustness in GUI agents and inspires\nmore future research in this direction. The dataset and code are available at\nhttps://github.com/chessbean1/GUI-Robust.."
    },
    {
        "date": "2025-06",
        "title": "LexiMark: Robust Watermarking via Lexical Substitutions to Enhance Membership Verification of an LLM's Textual Training Data",
        "author": "Eyal German, Sagiv Antebi, Edan Habler, Asaf Shabtai, and Yuval Elovici",
        "link": "http://arxiv.org/abs/2506.14474v1",
        "abstract": "Large language models (LLMs) can be trained or fine-tuned on data obtained\nwithout the owner's consent. Verifying whether a specific LLM was trained on\nparticular data instances or an entire dataset is extremely challenging.\nDataset watermarking addresses this by embedding identifiable modifications in\ntraining data to detect unauthorized use. However, existing methods often lack\nstealth, making them relatively easy to detect and remove. In light of these\nlimitations, we propose LexiMark, a novel watermarking technique designed for\ntext and documents, which embeds synonym substitutions for carefully selected\nhigh-entropy words. Our method aims to enhance an LLM's memorization\ncapabilities on the watermarked text without altering the semantic integrity of\nthe text. As a result, the watermark is difficult to detect, blending\nseamlessly into the text with no visible markers, and is resistant to removal\ndue to its subtle, contextually appropriate substitutions that evade automated\nand manual detection. We evaluated our method using baseline datasets from\nrecent studies and seven open-source models: LLaMA-1 7B, LLaMA-3 8B, Mistral\n7B, Pythia 6.9B, as well as three smaller variants from the Pythia family\n(160M, 410M, and 1B). Our evaluation spans multiple training settings,\nincluding continued pretraining and fine-tuning scenarios. The results\ndemonstrate significant improvements in AUROC scores compared to existing\nmethods, underscoring our method's effectiveness in reliably verifying whether\nunauthorized watermarked data was used in LLM training."
    },
    {
        "date": "2025-06",
        "title": "HiLight: A Hierarchical Reinforcement Learning Framework with Global Adversarial Guidance for Large-Scale Traffic Signal Control",
        "author": "Yaqiao Zhu, Hongkai Wen, Geyong Min, and Man Luo",
        "link": "http://arxiv.org/abs/2506.14391v1",
        "abstract": "Efficient traffic signal control (TSC) is essential for mitigating urban\ncongestion, yet existing reinforcement learning (RL) methods face challenges in\nscaling to large networks while maintaining global coordination. Centralized RL\nsuffers from scalability issues, while decentralized approaches often lack\nunified objectives, resulting in limited network-level efficiency. In this\npaper, we propose HiLight, a hierarchical reinforcement learning framework with\nglobal adversarial guidance for large-scale TSC. HiLight consists of a\nhigh-level Meta-Policy, which partitions the traffic network into subregions\nand generates sub-goals using a Transformer-LSTM architecture, and a low-level\nSub-Policy, which controls individual intersections with global awareness. To\nimprove the alignment between global planning and local execution, we introduce\nan adversarial training mechanism, where the Meta-Policy generates challenging\nyet informative sub-goals, and the Sub-Policy learns to surpass these targets,\nleading to more effective coordination. We evaluate HiLight across both\nsynthetic and real-world benchmarks, and additionally construct a large-scale\nManhattan network with diverse traffic conditions, including peak transitions,\nadverse weather, and holiday surges. Experimental results show that HiLight\nexhibits significant advantages in large-scale scenarios and remains\ncompetitive across standard benchmarks of varying sizes."
    },
    {
        "date": "2025-06",
        "title": "Excessive Reasoning Attack on Reasoning LLMs",
        "author": "Wai Man Si, Mingjie Li, Michael Backes, and Yang Zhang",
        "link": "http://arxiv.org/abs/2506.14374v1",
        "abstract": "Recent reasoning large language models (LLMs), such as OpenAI o1 and\nDeepSeek-R1, exhibit strong performance on complex tasks through test-time\ninference scaling. However, prior studies have shown that these models often\nincur significant computational costs due to excessive reasoning, such as\nfrequent switching between reasoning trajectories (e.g., underthinking) or\nredundant reasoning on simple questions (e.g., overthinking). In this work, we\nexpose a novel threat: adversarial inputs can be crafted to exploit excessive\nreasoning behaviors and substantially increase computational overhead without\ncompromising model utility. Therefore, we propose a novel loss framework\nconsisting of three components: (1) Priority Cross-Entropy Loss, a modification\nof the standard cross-entropy objective that emphasizes key tokens by\nleveraging the autoregressive nature of LMs; (2) Excessive Reasoning Loss,\nwhich encourages the model to initiate additional reasoning paths during\ninference; and (3) Delayed Termination Loss, which is designed to extend the\nreasoning process and defer the generation of final outputs. We optimize and\nevaluate our attack for the GSM8K and ORCA datasets on\nDeepSeek-R1-Distill-LLaMA and DeepSeek-R1-Distill-Qwen. Empirical results\ndemonstrate a 3x to 9x increase in reasoning length with comparable utility\nperformance. Furthermore, our crafted adversarial inputs exhibit\ntransferability, inducing computational overhead in o3-mini, o1-mini,\nDeepSeek-R1, and QWQ models."
    },
    {
        "date": "2025-06",
        "title": "Towards Robust Learning to Optimize with Theoretical Guarantees",
        "author": "Qingyu Song, Wei Lin, Juncheng Wang, and Hong Xu",
        "link": "http://arxiv.org/abs/2506.14263v1",
        "abstract": "Learning to optimize (L2O) is an emerging technique to solve mathematical\noptimization problems with learning-based methods. Although with great success\nin many real-world scenarios such as wireless communications, computer\nnetworks, and electronic design, existing L2O works lack theoretical\ndemonstration of their performance and robustness in out-of-distribution (OOD)\nscenarios. We address this gap by providing comprehensive proofs. First, we\nprove a sufficient condition for a robust L2O model with homogeneous\nconvergence rates over all In-Distribution (InD) instances. We assume an L2O\nmodel achieves robustness for an InD scenario. Based on our proposed\nmethodology of aligning OOD problems to InD problems, we also demonstrate that\nthe L2O model's convergence rate in OOD scenarios will deteriorate by an\nequation of the L2O model's input features. Moreover, we propose an L2O model\nwith a concise gradient-only feature construction and a novel gradient-based\nhistory modeling method. Numerical simulation demonstrates that our proposed\nmodel outperforms the state-of-the-art baseline in both InD and OOD scenarios\nand achieves up to 10 $\\times$ convergence speedup. The code of our method can\nbe found from https://github.com/NetX-lab/GoMathL2O-Official."
    },
    {
        "date": "2025-06",
        "title": "synth-dacl: Does Synthetic Defect Data Enhance Segmentation Accuracy and Robustness for Real-World Bridge Inspections?",
        "author": "Johannes Flotzinger, Fabian Deuser, Achref Jaziri, Heiko Neumann, Norbert Oswald, Visvanathan Ramesh, and Thomas Braml",
        "link": "http://arxiv.org/abs/2506.14255v1",
        "abstract": "Adequate bridge inspection is increasingly challenging in many countries due\nto growing ailing stocks, compounded with a lack of staff and financial\nresources. Automating the key task of visual bridge inspection, classification\nof defects and building components on pixel level, improves efficiency,\nincreases accuracy and enhances safety in the inspection process and resulting\nbuilding assessment. Models overtaking this task must cope with an assortment\nof real-world conditions. They must be robust to variations in image quality,\nas well as background texture, as defects often appear on surfaces of diverse\ntexture and degree of weathering. dacl10k is the largest and most diverse\ndataset for real-world concrete bridge inspections. However, the dataset\nexhibits class imbalance, which leads to notably poor model performance\nparticularly when segmenting fine-grained classes such as cracks and cavities.\nThis work introduces \"synth-dacl\", a compilation of three novel dataset\nextensions based on synthetic concrete textures. These extensions are designed\nto balance class distribution in dacl10k and enhance model performance,\nespecially for crack and cavity segmentation. When incorporating the synth-dacl\nextensions, we observe substantial improvements in model robustness across 15\nperturbed test sets. Notably, on the perturbed test set, a model trained on\ndacl10k combined with all synthetic extensions achieves a 2% increase in mean\nIoU, F1 score, Recall, and Precision compared to the same model trained solely\non dacl10k."
    },
    {
        "date": "2025-06",
        "title": "Efficient Retail Video Annotation: A Robust Key Frame Generation Approach for Product and Customer Interaction Analysis",
        "author": "Varun Mannam, and Zhenyu Shi",
        "link": "http://arxiv.org/abs/2506.14854v2",
        "abstract": "Accurate video annotation plays a vital role in modern retail applications,\nincluding customer behavior analysis, product interaction detection, and\nin-store activity recognition. However, conventional annotation methods heavily\nrely on time-consuming manual labeling by human annotators, introducing\nnon-robust frame selection and increasing operational costs. To address these\nchallenges in the retail domain, we propose a deep learning-based approach that\nautomates key-frame identification in retail videos and provides automatic\nannotations of products and customers. Our method leverages deep neural\nnetworks to learn discriminative features by embedding video frames and\nincorporating object detection-based techniques tailored for retail\nenvironments. Experimental results showcase the superiority of our approach\nover traditional methods, achieving accuracy comparable to human annotator\nlabeling while enhancing the overall efficiency of retail video annotation.\nRemarkably, our approach leads to an average of 2 times cost savings in video\nannotation. By allowing human annotators to verify/adjust less than 5% of\ndetected frames in the video dataset, while automating the annotation process\nfor the remaining frames without reducing annotation quality, retailers can\nsignificantly reduce operational costs. The automation of key-frame detection\nenables substantial time and effort savings in retail video labeling tasks,\nproving highly valuable for diverse retail applications such as shopper journey\nanalysis, product interaction detection, and in-store security monitoring."
    },
    {
        "date": "2025-06",
        "title": "Robust Physics-Informed Neural Network Approach for Estimating Heterogeneous Elastic Properties from Noisy Displacement Data",
        "author": "Tatthapong Srikitrungruang, Matthew Lemon, Sina Aghaee Dabaghan Fard, Jaesung Lee, and Yuxiao Zhou",
        "link": "http://arxiv.org/abs/2506.14036v2",
        "abstract": "Accurately estimating spatially heterogeneous elasticity parameters,\nparticularly Young's modulus and Poisson's ratio, from noisy displacement\nmeasurements remains significantly challenging in inverse elasticity problems.\nExisting inverse estimation techniques are often limited by instability,\npronounced sensitivity to measurement noise, and difficulty in recovering\nabsolute-scale Young's modulus. This work presents a novel Inverse Elasticity\nPhysics-Informed Neural Network (IE-PINN) specifically designed to robustly\nreconstruct heterogeneous distributions of elasticity parameters from noisy\ndisplacement data based on linear elasticity physics. IE-PINN integrates three\ndistinct neural network architectures dedicated to separately modeling\ndisplacement fields, strain fields, and elasticity distributions, thereby\nsignificantly enhancing stability and accuracy against measurement noise.\nAdditionally, a two-phase estimation strategy is introduced: the first phase\nrecovers relative spatial distributions of Young's modulus and Poisson's ratio,\nand the second phase calibrates the absolute scale of Young's modulus using\nimposed loading boundary conditions. Additional methodological innovations,\nincluding positional encoding, sine activation functions, and a sequential\npretraining protocol, further enhance the model's performance and robustness.\nExtensive numerical experiments demonstrate that IE-PINN effectively overcomes\ncritical limitations encountered by existing methods, delivering accurate\nabsolute-scale elasticity estimations even under severe noise conditions. This\nadvancement holds substantial potential for clinical imaging diagnostics and\nmechanical characterization, where measurements typically encounter substantial\nnoise."
    },
    {
        "date": "2025-06",
        "title": "Membership Inference Attacks as Privacy Tools: Reliability, Disparity and Ensemble",
        "author": "Zhiqi Wang, Chengyu Zhang, Yuetian Chen, Nathalie Baracaldo, Swanand Kadhe, and Lei Yu",
        "link": "http://arxiv.org/abs/2506.13972v1",
        "abstract": "Membership inference attacks (MIAs) pose a significant threat to the privacy\nof machine learning models and are widely used as tools for privacy assessment,\nauditing, and machine unlearning. While prior MIA research has primarily\nfocused on performance metrics such as AUC, accuracy, and TPR@low FPR - either\nby developing new methods to enhance these metrics or using them to evaluate\nprivacy solutions - we found that it overlooks the disparities among different\nattacks. These disparities, both between distinct attack methods and between\nmultiple instantiations of the same method, have crucial implications for the\nreliability and completeness of MIAs as privacy evaluation tools. In this\npaper, we systematically investigate these disparities through a novel\nframework based on coverage and stability analysis. Extensive experiments\nreveal significant disparities in MIAs, their potential causes, and their\nbroader implications for privacy evaluation. To address these challenges, we\npropose an ensemble framework with three distinct strategies to harness the\nstrengths of state-of-the-art MIAs while accounting for their disparities. This\nframework not only enables the construction of more powerful attacks but also\nprovides a more robust and comprehensive methodology for privacy evaluation."
    },
    {
        "date": "2025-06",
        "title": "ExtendAttack: Attacking Servers of LRMs via Extending Reasoning",
        "author": "Zhenhao Zhu, Yue Liu, Yingwei Ma, Hongcheng Gao, Nuo Chen, Yanpei Guo, Wenjie Qu, Huiying Xu, Xinzhong Zhu, and Jiaheng Zhang",
        "link": "http://arxiv.org/abs/2506.13737v1",
        "abstract": "Large Reasoning Models (LRMs) have demonstrated promising performance in\ncomplex tasks. However, the resource-consuming reasoning processes may be\nexploited by attackers to maliciously occupy the resources of the servers,\nleading to a crash, like the DDoS attack in cyber. To this end, we propose a\nnovel attack method on LRMs termed ExtendAttack to maliciously occupy the\nresources of servers by stealthily extending the reasoning processes of LRMs.\nConcretely, we systematically obfuscate characters within a benign prompt,\ntransforming them into a complex, poly-base ASCII representation. This compels\nthe model to perform a series of computationally intensive decoding sub-tasks\nthat are deeply embedded within the semantic structure of the query itself.\nExtensive experiments demonstrate the effectiveness of our proposed\nExtendAttack. Remarkably, it increases the length of the model's response by\nover 2.5 times for the o3 model on the HumanEval benchmark. Besides, it\npreserves the original meaning of the query and achieves comparable answer\naccuracy, showing the stealthiness."
    },
    {
        "date": "2025-06",
        "title": "Weakest Link in the Chain: Security Vulnerabilities in Advanced Reasoning Models",
        "author": "Arjun Krishna, Aaditya Rastogi, and Erick Galinkin",
        "link": "http://arxiv.org/abs/2506.13726v1",
        "abstract": "The introduction of advanced reasoning capabilities have improved the\nproblem-solving performance of large language models, particularly on math and\ncoding benchmarks. However, it remains unclear whether these reasoning models\nare more or less vulnerable to adversarial prompt attacks than their\nnon-reasoning counterparts. In this work, we present a systematic evaluation of\nweaknesses in advanced reasoning models compared to similar non-reasoning\nmodels across a diverse set of prompt-based attack categories. Using\nexperimental data, we find that on average the reasoning-augmented models are\n\\emph{slightly more robust} than non-reasoning models (42.51\\% vs 45.53\\%\nattack success rate, lower is better). However, this overall trend masks\nsignificant category-specific differences: for certain attack types the\nreasoning models are substantially \\emph{more vulnerable} (e.g., up to 32\npercentage points worse on a tree-of-attacks prompt), while for others they are\nmarkedly \\emph{more robust} (e.g., 29.8 points better on cross-site scripting\ninjection). Our findings highlight the nuanced security implications of\nadvanced reasoning in language models and emphasize the importance of\nstress-testing safety across diverse adversarial techniques."
    },
    {
        "date": "2025-06",
        "title": "Adversarial Disentanglement by Backpropagation with Physics-Informed Variational Autoencoder",
        "author": "Ioannis Christoforos Koune, and Alice Cicirello",
        "link": "http://arxiv.org/abs/2506.13658v1",
        "abstract": "Inference and prediction under partial knowledge of a physical system is\nchallenging, particularly when multiple confounding sources influence the\nmeasured response. Explicitly accounting for these influences in physics-based\nmodels is often infeasible due to epistemic uncertainty, cost, or time\nconstraints, resulting in models that fail to accurately describe the behavior\nof the system. On the other hand, data-driven machine learning models such as\nvariational autoencoders are not guaranteed to identify a parsimonious\nrepresentation. As a result, they can suffer from poor generalization\nperformance and reconstruction accuracy in the regime of limited and noisy\ndata. We propose a physics-informed variational autoencoder architecture that\ncombines the interpretability of physics-based models with the flexibility of\ndata-driven models. To promote disentanglement of the known physics and\nconfounding influences, the latent space is partitioned into physically\nmeaningful variables that parametrize a physics-based model, and data-driven\nvariables that capture variability in the domain and class of the physical\nsystem. The encoder is coupled with a decoder that integrates physics-based and\ndata-driven components, and constrained by an adversarial training objective\nthat prevents the data-driven components from overriding the known physics,\nensuring that the physics-grounded latent variables remain interpretable. We\ndemonstrate that the model is able to disentangle features of the input signal\nand separate the known physics from confounding influences using supervision in\nthe form of class and domain observables. The model is evaluated on a series of\nsynthetic case studies relevant to engineering structures, demonstrating the\nfeasibility of the proposed approach."
    },
    {
        "date": "2025-06",
        "title": "EBS-CFL: Efficient and Byzantine-robust Secure Clustered Federated Learning",
        "author": "Zhiqiang Li, Haiyong Bao, Menghong Guan, Hao Pan, Cheng Huang, and Hong-Ning Dai",
        "link": "http://arxiv.org/abs/2506.13612v1",
        "abstract": "Despite federated learning (FL)'s potential in collaborative learning, its\nperformance has deteriorated due to the data heterogeneity of distributed\nusers. Recently, clustered federated learning (CFL) has emerged to address this\nchallenge by partitioning users into clusters according to their similarity.\nHowever, CFL faces difficulties in training when users are unwilling to share\ntheir cluster identities due to privacy concerns. To address these issues, we\npresent an innovative Efficient and Robust Secure Aggregation scheme for CFL,\ndubbed EBS-CFL. The proposed EBS-CFL supports effectively training CFL while\nmaintaining users' cluster identity confidentially. Moreover, it detects\npotential poisonous attacks without compromising individual client gradients by\ndiscarding negatively correlated gradients and aggregating positively\ncorrelated ones using a weighted approach. The server also authenticates\ncorrect gradient encoding by clients. EBS-CFL has high efficiency with\nclient-side overhead O(ml + m^2) for communication and O(m^2l) for computation,\nwhere m is the number of cluster identities, and l is the gradient size. When m\n= 1, EBS-CFL's computational efficiency of client is at least O(log n) times\nbetter than comparison schemes, where n is the number of clients.In addition,\nwe validate the scheme through extensive experiments. Finally, we theoretically\nprove the scheme's security."
    },
    {
        "date": "2025-06",
        "title": "Unlearning-Enhanced Website Fingerprinting Attack: Against Backdoor Poisoning in Anonymous Networks",
        "author": "Yali Yuan, Kai Xu, Ruolin Ma, and Yuchen Zhang",
        "link": "http://arxiv.org/abs/2506.13563v1",
        "abstract": "Website Fingerprinting (WF) is an effective tool for regulating and governing\nthe dark web. However, its performance can be significantly degraded by\nbackdoor poisoning attacks in practical deployments. This paper aims to address\nthe problem of hidden backdoor poisoning attacks faced by Website\nFingerprinting attack, and designs a feasible mothed that integrates unlearning\ntechnology to realize detection of automatic poisoned points and complete\nremoval of its destructive effects, requiring only a small number of known\npoisoned test points. Taking Tor onion routing as an example, our method\nevaluates the influence value of each training sample on these known poisoned\ntest points as the basis for judgment. We optimize the use of influence scores\nto identify poisoned samples within the training dataset. Furthermore, by\nquantifying the difference between the contribution of model parameters on the\ntaining data and the clean data, the target parameters are dynamically adjusted\nto eliminate the impact of the backdoor attacks. Experiments on public datasets\nunder the assumptions of closed-world (CW) and open-world (OW) verify the\neffectiveness of the proposed method. In complex scenes containing both clean\nwebsite fingerprinting features and backdoor triggers, the accuracy of the\nmodel on the poisoned dataset and the test dataset is stable at about 80%,\nsignificantly outperforming the traditional WF attack models. In addition, the\nproposed method achieves a 2-3 times speedup in runtime efficiency compared to\nbaseline methods. By incorporating machine unlearning, we realize a WF attack\nmodel that exhibits enhanced resistance to backdoor poisoning and faster\nexecution speeds in adversarial settings."
    },
    {
        "date": "2025-06",
        "title": "LapDDPM: A Conditional Graph Diffusion Model for scRNA-seq Generation with Spectral Adversarial Perturbations",
        "author": "Lorenzo Bini, and Stephane Marchand-Maillet",
        "link": "http://arxiv.org/abs/2506.13344v1",
        "abstract": "Generating high-fidelity and biologically plausible synthetic single-cell RNA\nsequencing (scRNA-seq) data, especially with conditional control, is\nchallenging due to its high dimensionality, sparsity, and complex biological\nvariations. Existing generative models often struggle to capture these unique\ncharacteristics and ensure robustness to structural noise in cellular networks.\nWe introduce LapDDPM, a novel conditional Graph Diffusion Probabilistic Model\nfor robust and high-fidelity scRNA-seq generation. LapDDPM uniquely integrates\ngraph-based representations with a score-based diffusion model, enhanced by a\nnovel spectral adversarial perturbation mechanism on graph edge weights. Our\ncontributions are threefold: we leverage Laplacian Positional Encodings (LPEs)\nto enrich the latent space with crucial cellular relationship information; we\ndevelop a conditional score-based diffusion model for effective learning and\ngeneration from complex scRNA-seq distributions; and we employ a unique\nspectral adversarial training scheme on graph edge weights, boosting robustness\nagainst structural variations. Extensive experiments on diverse scRNA-seq\ndatasets demonstrate LapDDPM's superior performance, achieving high fidelity\nand generating biologically-plausible, cell-type-specific samples. LapDDPM sets\na new benchmark for conditional scRNA-seq data generation, offering a robust\ntool for various downstream biological applications."
    },
    {
        "date": "2025-06",
        "title": "Navigating the Black Box: Leveraging LLMs for Effective Text-Level Graph Injection Attacks",
        "author": "Yuefei Lyu, Chaozhuo Li, Xi Zhang, and Tianle Zhang",
        "link": "http://arxiv.org/abs/2506.13276v1",
        "abstract": "Text-attributed graphs (TAGs) integrate textual data with graph structures,\nproviding valuable insights in applications such as social network analysis and\nrecommendation systems. Graph Neural Networks (GNNs) effectively capture both\ntopological structure and textual information in TAGs but are vulnerable to\nadversarial attacks. Existing graph injection attack (GIA) methods assume that\nattackers can directly manipulate the embedding layer, producing\nnon-explainable node embeddings. Furthermore, the effectiveness of these\nattacks often relies on surrogate models with high training costs. Thus, this\npaper introduces ATAG-LLM, a novel black-box GIA framework tailored for TAGs.\nOur approach leverages large language models (LLMs) to generate interpretable\ntext-level node attributes directly, ensuring attacks remain feasible in\nreal-world scenarios. We design strategies for LLM prompting that balance\nexploration and reliability to guide text generation, and propose a similarity\nassessment method to evaluate attack text effectiveness in disrupting graph\nhomophily. This method efficiently perturbs the target node with minimal\ntraining costs in a strict black-box setting, ensuring a text-level graph\ninjection attack for TAGs. Experiments on real-world TAG datasets validate the\nsuperior performance of ATAG-LLM compared to state-of-the-art embedding-level\nand text-level attack methods."
    },
    {
        "date": "2025-06",
        "title": "Building Automotive Security on Internet Standards: An Integration of DNSSEC, DANE, and DANCE to Authenticate and Authorize In-Car Services",
        "author": "Timo Salomon, Mehmet Mueller, Philipp Meyer, and Thomas C. Schmidt",
        "link": "http://arxiv.org/abs/2506.13261v1",
        "abstract": "The automotive industry is undergoing a software-as-a-service transformation\nthat enables software-defined functions and post-sale updates via cloud and\nvehicle-to-everything communication. Connectivity in cars introduces\nsignificant security challenges, as remote attacks on vehicles have become\nincreasingly prevalent. Current automotive designs call for security solutions\nthat address the entire lifetime of a vehicle. In this paper, we propose to\nauthenticate and authorize in-vehicle services by integrating DNSSEC, DANE, and\nDANCE with automotive middleware. Our approach decouples the cryptographic\nauthentication of the service from that of the service deployment with the help\nof DNSSEC and thereby largely simplifies key management. We propose to\nauthenticate in-vehicle services by certificates that are solely generated by\nthe service suppliers but published on deployment via DNSSEC TLSA records\nsolely signed by the OEM. Building on well-established Internet standards\nensures interoperability with various current and future protocols, scalable\nmanagement of credentials for millions of connected vehicles at\nwell-established security levels. We back our design proposal by a security\nanalysis using the STRIDE threat model and by evaluations in a realistic\nin-vehicle setup that demonstrate its effectiveness."
    },
    {
        "date": "2025-06",
        "title": "No-Regret Learning Under Adversarial Resource Constraints: A Spending Plan Is All You Need!",
        "author": "Francesco Emanuele Stradi, Matteo Castiglioni, Alberto Marchesi, Nicola Gatti, and Christian Kroer",
        "link": "http://arxiv.org/abs/2506.13244v3",
        "abstract": "We study online decision making problems under resource constraints, where\nboth reward and cost functions are drawn from distributions that may change\nadversarially over time. We focus on two canonical settings: $(i)$ online\nresource allocation where rewards and costs are observed before action\nselection, and $(ii)$ online learning with resource constraints where they are\nobserved after action selection, under full feedback or bandit feedback. It is\nwell known that achieving sublinear regret in these settings is impossible when\nreward and cost distributions may change arbitrarily over time. To address this\nchallenge, we analyze a framework in which the learner is guided by a spending\nplan--a sequence prescribing expected resource usage across rounds. We design\ngeneral (primal-)dual methods that achieve sublinear regret with respect to\nbaselines that follow the spending plan. Crucially, the performance of our\nalgorithms improves when the spending plan ensures a well-balanced distribution\nof the budget across rounds. We additionally provide a robust variant of our\nmethods to handle worst-case scenarios where the spending plan is highly\nimbalanced. To conclude, we study the regret of our algorithms when competing\nagainst benchmarks that deviate from the prescribed spending plan."
    },
    {
        "date": "2025-06",
        "title": "Robustness of Reinforcement Learning-Based Traffic Signal Control under Incidents: A Comparative Study",
        "author": "Dang Viet Anh Nguyen, Carlos Lima Azevedo, Tomer Toledo, and Filipe Rodrigues",
        "link": "http://arxiv.org/abs/2506.13836v1",
        "abstract": "Reinforcement learning-based traffic signal control (RL-TSC) has emerged as a\npromising approach for improving urban mobility. However, its robustness under\nreal-world disruptions such as traffic incidents remains largely underexplored.\nIn this study, we introduce T-REX, an open-source, SUMO-based simulation\nframework for training and evaluating RL-TSC methods under dynamic, incident\nscenarios. T-REX models realistic network-level performance considering\ndrivers' probabilistic rerouting, speed adaptation, and contextual\nlane-changing, enabling the simulation of congestion propagation under\nincidents. To assess robustness, we propose a suite of metrics that extend\nbeyond conventional traffic efficiency measures. Through extensive experiments\nacross synthetic and real-world networks, we showcase T-REX for the evaluation\nof several state-of-the-art RL-TSC methods under multiple real-world deployment\nparadigms. Our findings show that while independent value-based and\ndecentralized pressure-based methods offer fast convergence and generalization\nin stable traffic conditions and homogeneous networks, their performance\ndegrades sharply under incident-driven distribution shifts. In contrast,\nhierarchical coordination methods tend to offer more stable and adaptable\nperformance in large-scale, irregular networks, benefiting from their\nstructured decision-making architecture. However, this comes with the trade-off\nof slower convergence and higher training complexity. These findings highlight\nthe need for robustness-aware design and evaluation in RL-TSC research. T-REX\ncontributes to this effort by providing an open, standardized and reproducible\nplatform for benchmarking RL methods under dynamic and disruptive traffic\nscenarios."
    },
    {
        "date": "2025-06",
        "title": "Using LLMs for Security Advisory Investigations: How Far Are We?",
        "author": "Bayu Fedra Abdullah, Yusuf Sulistyo Nugroho, Brittany Reid, Raula Gaikovina Kula, Kazumasa Shimari, and Kenichi Matsumoto",
        "link": "http://arxiv.org/abs/2506.13161v1",
        "abstract": "Large Language Models (LLMs) are increasingly used in software security, but\ntheir trustworthiness in generating accurate vulnerability advisories remains\nuncertain. This study investigates the ability of ChatGPT to (1) generate\nplausible security advisories from CVE-IDs, (2) differentiate real from fake\nCVE-IDs, and (3) extract CVE-IDs from advisory descriptions. Using a curated\ndataset of 100 real and 100 fake CVE-IDs, we manually analyzed the credibility\nand consistency of the model's outputs. The results show that ChatGPT generated\nplausible security advisories for 96% of given input real CVE-IDs and 97% of\ngiven input fake CVE-IDs, demonstrating a limitation in differentiating between\nreal and fake IDs. Furthermore, when these generated advisories were\nreintroduced to ChatGPT to identify their original CVE-ID, the model produced a\nfake CVE-ID in 6% of cases from real advisories. These findings highlight both\nthe strengths and limitations of ChatGPT in cybersecurity applications. While\nthe model demonstrates potential for automating advisory generation, its\ninability to reliably authenticate CVE-IDs or maintain consistency upon\nre-evaluation underscores the risks associated with its deployment in critical\nsecurity tasks. Our study emphasizes the importance of using LLMs with caution\nin cybersecurity workflows and suggests the need for further improvements in\ntheir design to improve reliability and applicability in security advisory\ngeneration."
    },
    {
        "date": "2025-06",
        "title": "Buy it Now, Track Me Later: Attacking User Privacy via Wi-Fi AP Online Auctions",
        "author": "Steven Su, Erik Rye, Dave Levin, and Robert Beverly",
        "link": "http://arxiv.org/abs/2506.13052v2",
        "abstract": "Static and hard-coded layer-two network identifiers are well known to present\nsecurity vulnerabilities and endanger user privacy. In this work, we introduce\na new privacy attack against Wi-Fi access points listed on secondhand\nmarketplaces. Specifically, we demonstrate the ability to remotely gather a\nlarge quantity of layer-two Wi-Fi identifiers by programmatically querying the\neBay marketplace and applying state-of-the-art computer vision techniques to\nextract IEEE 802.11 BSSIDs from the seller's posted images of the hardware. By\nleveraging data from a global Wi-Fi Positioning System (WPS) that geolocates\nBSSIDs, we obtain the physical locations of these devices both pre- and\npost-sale. In addition to validating the degree to which a seller's location\nmatches the location of the device, we examine cases of device movement -- once\nthe device is sold and then subsequently re-used in a new environment. Our work\nhighlights a previously unrecognized privacy vulnerability and suggests, yet\nagain, the strong need to protect layer-two network identifiers."
    },
    {
        "date": "2025-06",
        "title": "HKD4VLM: A Progressive Hybrid Knowledge Distillation Framework for Robust Multimodal Hallucination and Factuality Detection in VLMs",
        "author": "Zijian Zhang, Xuecheng Wu, Danlei Huang, Siyu Yan, Chong Peng, and Xuezhi Cao",
        "link": "http://arxiv.org/abs/2506.13038v2",
        "abstract": "Driven by the rapid progress in vision-language models (VLMs), the\nresponsible behavior of large-scale multimodal models has become a prominent\nresearch area, particularly focusing on hallucination detection and factuality\nchecking. In this paper, we present the solution for the two tracks of\nResponsible AI challenge. Inspirations from the general domain demonstrate that\na smaller distilled VLM can often outperform a larger VLM that is directly\ntuned on downstream tasks, while achieving higher efficiency. We thus jointly\ntackle two tasks from the perspective of knowledge distillation and propose a\nprogressive hybrid knowledge distillation framework termed HKD4VLM.\nSpecifically, the overall framework can be decomposed into Pyramid-like\nProgressive Online Distillation and Ternary-Coupled Refinement Distillation,\nhierarchically moving from coarse-grained knowledge alignment to fine-grained\nrefinement. Besides, we further introduce the mapping shift-enhanced inference\nand diverse augmentation strategies to enhance model performance and\nrobustness. Extensive experimental results demonstrate the effectiveness of our\nHKD4VLM. Ablation studies provide insights into the critical design choices\ndriving performance gains."
    },
    {
        "date": "2025-06",
        "title": "Position: Certified Robustness Does Not (Yet) Imply Model Security",
        "author": "Andrew C. Cullen, Paul Montague, Sarah M. Erfani, and Benjamin I. P. Rubinstein",
        "link": "http://arxiv.org/abs/2506.13024v1",
        "abstract": "While certified robustness is widely promoted as a solution to adversarial\nexamples in Artificial Intelligence systems, significant challenges remain\nbefore these techniques can be meaningfully deployed in real-world\napplications. We identify critical gaps in current research, including the\nparadox of detection without distinction, the lack of clear criteria for\npractitioners to evaluate certification schemes, and the potential security\nrisks arising from users' expectations surrounding ``guaranteed\" robustness\nclaims. This position paper is a call to arms for the certification research\ncommunity, proposing concrete steps to address these fundamental challenges and\nadvance the field toward practical applicability."
    },
    {
        "date": "2025-06",
        "title": "Rectifying Privacy and Efficacy Measurements in Machine Unlearning: A New Inference Attack Perspective",
        "author": "Nima Naderloui, Shenao Yan, Binghui Wang, Jie Fu, Wendy Hui Wang, Weiran Liu, and Yuan Hong",
        "link": "http://arxiv.org/abs/2506.13009v1",
        "abstract": "Machine unlearning focuses on efficiently removing specific data from trained\nmodels, addressing privacy and compliance concerns with reasonable costs.\nAlthough exact unlearning ensures complete data removal equivalent to\nretraining, it is impractical for large-scale models, leading to growing\ninterest in inexact unlearning methods. However, the lack of formal guarantees\nin these methods necessitates the need for robust evaluation frameworks to\nassess their privacy and effectiveness. In this work, we first identify several\nkey pitfalls of the existing unlearning evaluation frameworks, e.g., focusing\non average-case evaluation or targeting random samples for evaluation,\nincomplete comparisons with the retraining baseline. Then, we propose RULI\n(Rectified Unlearning Evaluation Framework via Likelihood Inference), a novel\nframework to address critical gaps in the evaluation of inexact unlearning\nmethods. RULI introduces a dual-objective attack to measure both unlearning\nefficacy and privacy risks at a per-sample granularity. Our findings reveal\nsignificant vulnerabilities in state-of-the-art unlearning methods, where RULI\nachieves higher attack success rates, exposing privacy risks underestimated by\nexisting methods. Built on a game-based foundation and validated through\nempirical evaluations on both image and text data (spanning tasks from\nclassification to generation), RULI provides a rigorous, scalable, and\nfine-grained methodology for evaluating unlearning techniques."
    },
    {
        "date": "2025-06",
        "title": "Open Source, Open Threats? Investigating Security Challenges in Open-Source Software",
        "author": "Seyed Ali Akhavani, Behzad Ousat, and Amin Kharraz",
        "link": "http://arxiv.org/abs/2506.12995v1",
        "abstract": "Open-source software (OSS) has become increasingly more popular across\ndifferent domains. However, this rapid development and widespread adoption come\nwith a security cost. The growing complexity and openness of OSS ecosystems\nhave led to increased exposure to vulnerabilities and attack surfaces. This\npaper investigates the trends and patterns of reported vulnerabilities within\nOSS platforms, focusing on the implications of these findings for security\npractices. To understand the dynamics of OSS vulnerabilities, we analyze a\ncomprehensive dataset comprising 31,267 unique vulnerability reports from\nGitHub's advisory database and Snyk.io, belonging to 14,675 packages across 10\nprogramming languages. Our analysis reveals a significant surge in reported\nvulnerabilities, increasing at an annual rate of 98%, far outpacing the 25%\naverage annual growth in the number of open-source software (OSS) packages.\nAdditionally, we observe an 85% increase in the average lifespan of\nvulnerabilities across ecosystems during the studied period, indicating a\npotential decline in security. We identify the most prevalent Common Weakness\nEnumerations (CWEs) across programming languages and find that, on average,\njust seven CWEs are responsible for over 50% of all reported vulnerabilities.\nWe further examine these commonly observed CWEs and highlight\necosystem-specific trends. Notably, we find that vulnerabilities associated\nwith intentionally malicious packages comprise 49% of reports in the NPM\necosystem and 14% in PyPI, an alarming indication of targeted attacks within\npackage repositories. We conclude with an in-depth discussion of the\ncharacteristics and attack vectors associated with these malicious packages."
    },
    {
        "date": "2025-06",
        "title": "Identifying and Investigating Global News Coverage of Critical Events Such as Disasters and Terrorist Attacks",
        "author": "Erica Cai, Xi Chen, Reagan Grey Keeney, Ethan Zuckerman, Brendan O'Connor, and Przemyslaw A. Grabowicz",
        "link": "http://arxiv.org/abs/2506.12925v1",
        "abstract": "Comparative studies of news coverage are challenging to conduct because\nmethods to identify news articles about the same event in different languages\nrequire expertise that is difficult to scale. We introduce an AI-powered method\nfor identifying news articles based on an event FINGERPRINT, which is a minimal\nset of metadata required to identify critical events. Our event coverage\nidentification method, FINGERPRINT TO ARTICLE MATCHING FOR EVENTS (FAME),\nefficiently identifies news articles about critical world events, specifically\nterrorist attacks and several types of natural disasters. FAME does not require\ntraining data and is able to automatically and efficiently identify news\narticles that discuss an event given its fingerprint: time, location, and class\n(such as storm or flood). The method achieves state-of-the-art performance and\nscales to massive databases of tens of millions of news articles and hundreds\nof events happening globally. We use FAME to identify 27,441 articles that\ncover 470 natural disaster and terrorist attack events that happened in 2020.\nTo this end, we use a massive database of news articles in three languages from\nMediaCloud, and three widely used, expert-curated databases of critical events:\nEM-DAT, USGS, and GTD. Our case study reveals patterns consistent with prior\nliterature: coverage of disasters and terrorist attacks correlates to death\ncounts, to the GDP of a country where the event occurs, and to trade volume\nbetween the reporting country and the country where the event occurred. We\nshare our NLP annotations and cross-country media attention data to support the\nefforts of researchers and media monitoring organizations."
    },
    {
        "date": "2025-06",
        "title": "Intriguing Frequency Interpretation of Adversarial Robustness for CNNs and ViTs",
        "author": "Lu Chen, Han Yang, Hu Wang, Yuxin Cao, Shaofeng Li, and Yuan Luo",
        "link": "http://arxiv.org/abs/2506.12875v1",
        "abstract": "Adversarial examples have attracted significant attention over the years, yet\nunderstanding their frequency-based characteristics remains insufficient. In\nthis paper, we investigate the intriguing properties of adversarial examples in\nthe frequency domain for the image classification task, with the following key\nfindings. (1) As the high-frequency components increase, the performance gap\nbetween adversarial and natural examples becomes increasingly pronounced. (2)\nThe model performance against filtered adversarial examples initially increases\nto a peak and declines to its inherent robustness. (3) In Convolutional Neural\nNetworks, mid- and high-frequency components of adversarial examples exhibit\ntheir attack capabilities, while in Transformers, low- and mid-frequency\ncomponents of adversarial examples are particularly effective. These results\nsuggest that different network architectures have different frequency\npreferences and that differences in frequency components between adversarial\nand natural examples may directly influence model robustness. Based on our\nfindings, we further conclude with three useful proposals that serve as a\nvaluable reference to the AI model security community."
    },
    {
        "date": "2025-06",
        "title": "Active Adversarial Noise Suppression for Image Forgery Localization",
        "author": "Rongxuan Peng, Shunquan Tan, Xianbo Mo, Alex C. Kot, and Jiwu Huang",
        "link": "http://arxiv.org/abs/2506.12871v1",
        "abstract": "Recent advances in deep learning have significantly propelled the development\nof image forgery localization. However, existing models remain highly\nvulnerable to adversarial attacks: imperceptible noise added to forged images\ncan severely mislead these models. In this paper, we address this challenge\nwith an Adversarial Noise Suppression Module (ANSM) that generate a defensive\nperturbation to suppress the attack effect of adversarial noise. We observe\nthat forgery-relevant features extracted from adversarial and original forged\nimages exhibit distinct distributions. To bridge this gap, we introduce\nForgery-relevant Features Alignment (FFA) as a first-stage training strategy,\nwhich reduces distributional discrepancies by minimizing the channel-wise\nKullback-Leibler divergence between these features. To further refine the\ndefensive perturbation, we design a second-stage training strategy, termed\nMask-guided Refinement (MgR), which incorporates a dual-mask constraint. MgR\nensures that the perturbation remains effective for both adversarial and\noriginal forged images, recovering forgery localization accuracy to their\noriginal level. Extensive experiments across various attack algorithms\ndemonstrate that our method significantly restores the forgery localization\nmodel's performance on adversarial images. Notably, when ANSM is applied to\noriginal forged images, the performance remains nearly unaffected. To our best\nknowledge, this is the first report of adversarial defense in image forgery\nlocalization tasks. We have released the source code and anti-forensics\ndataset."
    },
    {
        "date": "2025-06",
        "title": "TrojanTO: Action-Level Backdoor Attacks against Trajectory Optimization Models",
        "author": "Yang Dai, Oubo Ma, Longfei Zhang, Xingxing Liang, Xiaochun Cao, Shouling Ji, Jiaheng Zhang, Jincai Huang, and Li Shen",
        "link": "http://arxiv.org/abs/2506.12815v1",
        "abstract": "Recent advances in Trajectory Optimization (TO) models have achieved\nremarkable success in offline reinforcement learning. However, their\nvulnerabilities against backdoor attacks are poorly understood. We find that\nexisting backdoor attacks in reinforcement learning are based on reward\nmanipulation, which are largely ineffective against the TO model due to its\ninherent sequence modeling nature. Moreover, the complexities introduced by\nhigh-dimensional action spaces further compound the challenge of action\nmanipulation. To address these gaps, we propose TrojanTO, the first\naction-level backdoor attack against TO models. TrojanTO employs alternating\ntraining to enhance the connection between triggers and target actions for\nattack effectiveness. To improve attack stealth, it utilizes precise poisoning\nvia trajectory filtering for normal performance and batch poisoning for trigger\nconsistency. Extensive evaluations demonstrate that TrojanTO effectively\nimplants backdoor attacks across diverse tasks and attack objectives with a low\nattack budget (0.3\\% of trajectories). Furthermore, TrojanTO exhibits broad\napplicability to DT, GDT, and DC, underscoring its scalability across diverse\nTO model architectures."
    },
    {
        "date": "2025-06",
        "title": "Federated Neuroevolution O-RAN: Enhancing the Robustness of Deep Reinforcement Learning xApps",
        "author": "Mohammadreza Kouchaki, Aly Sabri Abdalla, and Vuk Marojevic",
        "link": "http://arxiv.org/abs/2506.12812v1",
        "abstract": "The open radio access network (O-RAN) architecture introduces RAN intelligent\ncontrollers (RICs) to facilitate the management and optimization of the\ndisaggregated RAN. Reinforcement learning (RL) and its advanced form, deep RL\n(DRL), are increasingly employed for designing intelligent controllers, or\nxApps, to be deployed in the near-real time (near-RT) RIC. These models often\nencounter local optima, which raise concerns about their reliability for RAN\nintelligent control. We therefore introduce Federated O-RAN enabled\nNeuroevolution (NE)-enhanced DRL (F-ONRL) that deploys an NE-based optimizer\nxApp in parallel to the RAN controller xApps. This NE-DRL xApp framework\nenables effective exploration and exploitation in the near-RT RIC without\ndisrupting RAN operations. We implement the NE xApp along with a DRL xApp and\ndeploy them on Open AI Cellular (OAIC) platform and present numerical results\nthat demonstrate the improved robustness of xApps while effectively balancing\nthe additional computational load."
    },
    {
        "date": "2025-06",
        "title": "Predicting Genetic Mutations from Single-Cell Bone Marrow Images in Acute Myeloid Leukemia Using Noise-Robust Deep Learning Models",
        "author": "Garima Jain, Ravi Kant Gupta, Priyansh Jain, Abhijeet Patil, Ardhendu Sekhar, Gajendra Smeeta, Sanghamitra Pati, and Amit Sethi",
        "link": "http://arxiv.org/abs/2506.12798v1",
        "abstract": "In this study, we propose a robust methodology for identification of myeloid\nblasts followed by prediction of genetic mutation in single-cell images of\nblasts, tackling challenges associated with label accuracy and data noise. We\ntrained an initial binary classifier to distinguish between leukemic (blasts)\nand non-leukemic cells images, achieving 90 percent accuracy. To evaluate the\nmodels generalization, we applied this model to a separate large unlabeled\ndataset and validated the predictions with two haemato-pathologists, finding an\napproximate error rate of 20 percent in the leukemic and non-leukemic labels.\nAssuming this level of label noise, we further trained a four-class model on\nimages predicted as blasts to classify specific mutations. The mutation labels\nwere known for only a bag of cell images extracted from a single slide. Despite\nthe tumor label noise, our mutation classification model achieved 85 percent\naccuracy across four mutation classes, demonstrating resilience to label\ninconsistencies. This study highlights the capability of machine learning\nmodels to work with noisy labels effectively while providing accurate,\nclinically relevant mutation predictions, which is promising for diagnostic\napplications in areas such as haemato-pathology."
    },
    {
        "date": "2025-06",
        "title": "Unconstrained Robust Online Convex Optimization",
        "author": "Jiujia Zhang, and Ashok Cutkosky",
        "link": "http://arxiv.org/abs/2506.12781v1",
        "abstract": "This paper addresses online learning with ``corrupted'' feedback. Our learner\nis provided with potentially corrupted gradients $\\tilde g_t$ instead of the\n``true'' gradients $g_t$. We make no assumptions about how the corruptions\narise: they could be the result of outliers, mislabeled data, or even malicious\ninterference. We focus on the difficult ``unconstrained'' setting in which our\nalgorithm must maintain low regret with respect to any comparison point $u \\in\n\\mathbb{R}^d$. The unconstrained setting is significantly more challenging as\nexisting algorithms suffer extremely high regret even with very tiny amounts of\ncorruption (which is not true in the case of a bounded domain). Our algorithms\nguarantee regret $ \\|u\\|G (\\sqrt{T} + k) $ when $G \\ge \\max_t \\|g_t\\|$ is\nknown, where $k$ is a measure of the total amount of corruption. When $G$ is\nunknown we incur an extra additive penalty of $(\\|u\\|^2+G^2) k$."
    },
    {
        "date": "2025-06",
        "title": "Base3: a simple interpolation-based ensemble method for robust dynamic link prediction",
        "author": "Kondrup Emma",
        "link": "http://arxiv.org/abs/2506.12764v1",
        "abstract": "Dynamic link prediction remains a central challenge in temporal graph\nlearning, particularly in designing models that are both effective and\npractical for real-world deployment. Existing approaches often rely on complex\nneural architectures, which are computationally intensive and difficult to\ninterpret.\n  In this work, we build on the strong recurrence-based foundation of the\nEdgeBank baseline, by supplementing it with inductive capabilities. We do so by\nleveraging the predictive power of non-learnable signals from two complementary\nperspectives: historical edge recurrence, as captured by EdgeBank, and global\nnode popularity, as introduced in the PopTrack model. We propose t-CoMem, a\nlightweight memory module that tracks temporal co-occurrence patterns and\nneighborhood activity. Building on this, we introduce Base3, an\ninterpolation-based model that fuses EdgeBank, PopTrack, and t-CoMem into a\nunified scoring framework. This combination effectively bridges local and\nglobal temporal dynamics -- repetition, popularity, and context -- without\nrelying on training. Evaluated on the Temporal Graph Benchmark, Base3 achieves\nperformance competitive with state-of-the-art deep models, even outperforming\nthem on some datasets. Importantly, it considerably improves on existing\nbaselines' performance under more realistic and challenging negative sampling\nstrategies -- offering a simple yet robust alternative for temporal graph\nlearning."
    },
    {
        "date": "2025-06",
        "title": "Learning to Fuse: Modality-Aware Adaptive Scheduling for Robust Multimodal Foundation Models",
        "author": "Liam Bennett, Mason Clark, Lucas Anderson, Hana Satou, and Olivia Martinez",
        "link": "http://arxiv.org/abs/2506.12733v1",
        "abstract": "Multimodal foundation models have achieved impressive progress across a wide\nrange of vision-language tasks. However, existing approaches often adopt fixed\nor task-specific fusion strategies, neglecting the intrinsic variability of\nmodality reliability and sample complexity. In this paper, we propose\nModality-Aware Adaptive Fusion Scheduling (MA-AFS), a general framework that\nlearns to dynamically modulate the contribution of each modality on a\nper-instance basis. MA-AFS introduces a lightweight neural scheduler that\npredicts modality fusion weights by integrating visual and textual entropy\nsignals along with cross-modal agreement cues. This enables the model to\nadaptively emphasize more reliable modalities, especially under noisy, missing,\nor misaligned inputs. We formulate the fusion process as a differentiable\nscheduling mechanism, analyze its theoretical consistency and regularization\neffect, and demonstrate that it improves robustness without increasing model\ncapacity significantly. Extensive experiments on image-text retrieval,\ncaptioning, and visual question answering show that MA-AFS achieves consistent\nperformance gains over strong baselines such as CLIP, ALBEF, and BLIP.\nMoreover, MA-AFS exhibits improved robustness under modality corruption and\nenhanced generalization under domain shifts. Our work highlights the importance\nof adaptive fusion and opens a promising direction toward reliable and\nuncertainty-aware multimodal learning."
    },
    {
        "date": "2025-06",
        "title": "SecurityLingua: Efficient Defense of LLM Jailbreak Attacks via Security-Aware Prompt Compression",
        "author": "Yucheng Li, Surin Ahn, Huiqiang Jiang, Amir H. Abdi, Yuqing Yang, and Lili Qiu",
        "link": "http://arxiv.org/abs/2506.12707v1",
        "abstract": "Large language models (LLMs) have achieved widespread adoption across\nnumerous applications. However, many LLMs are vulnerable to malicious attacks\neven after safety alignment. These attacks typically bypass LLMs' safety\nguardrails by wrapping the original malicious instructions inside adversarial\njailbreaks prompts. Previous research has proposed methods such as adversarial\ntraining and prompt rephrasing to mitigate these safety vulnerabilities, but\nthese methods often reduce the utility of LLMs or lead to significant\ncomputational overhead and online latency. In this paper, we propose\nSecurityLingua, an effective and efficient approach to defend LLMs against\njailbreak attacks via security-oriented prompt compression. Specifically, we\ntrain a prompt compressor designed to discern the \"true intention\" of the input\nprompt, with a particular focus on detecting the malicious intentions of\nadversarial prompts. Then, in addition to the original prompt, the intention is\npassed via the system prompt to the target LLM to help it identify the true\nintention of the request. SecurityLingua ensures a consistent user experience\nby leaving the original input prompt intact while revealing the user's\npotentially malicious intention and stimulating the built-in safety guardrails\nof the LLM. Moreover, thanks to prompt compression, SecurityLingua incurs only\na negligible overhead and extra token cost compared to all existing defense\nmethods, making it an especially practical solution for LLM defense.\nExperimental results demonstrate that SecurityLingua can effectively defend\nagainst malicious attacks and maintain utility of the LLM with negligible\ncompute and latency overhead. Our code is available at\nhttps://aka.ms/SecurityLingua."
    },
    {
        "date": "2025-06",
        "title": "NAP-Tuning: Neural Augmented Prompt Tuning for Adversarially Robust Vision-Language Models",
        "author": "Jiaming Zhang, Xin Wang, Xingjun Ma, Lingyu Qiu, Yu-Gang Jiang, and Jitao Sang",
        "link": "http://arxiv.org/abs/2506.12706v1",
        "abstract": "Vision-Language Models (VLMs) such as CLIP have demonstrated remarkable\ncapabilities in understanding relationships between visual and textual data\nthrough joint embedding spaces. Despite their effectiveness, these models\nremain vulnerable to adversarial attacks, particularly in the image modality,\nposing significant security concerns. Building upon our previous work on\nAdversarial Prompt Tuning (AdvPT), which introduced learnable text prompts to\nenhance adversarial robustness in VLMs without extensive parameter training, we\npresent a significant extension by introducing the Neural Augmentor framework\nfor Multi-modal Adversarial Prompt Tuning (NAP-Tuning).Our key innovations\ninclude: (1) extending AdvPT from text-only to multi-modal prompting across\nboth text and visual modalities, (2) expanding from single-layer to multi-layer\nprompt architectures, and (3) proposing a novel architecture-level redesign\nthrough our Neural Augmentor approach, which implements feature purification to\ndirectly address the distortions introduced by adversarial attacks in feature\nspace. Our NAP-Tuning approach incorporates token refiners that learn to\nreconstruct purified features through residual connections, allowing for\nmodality-specific and layer-specific feature correction.Comprehensive\nexperiments demonstrate that NAP-Tuning significantly outperforms existing\nmethods across various datasets and attack types. Notably, our approach shows\nsignificant improvements over the strongest baselines under the challenging\nAutoAttack benchmark, outperforming them by 33.5% on ViT-B16 and 33.0% on\nViT-B32 architectures while maintaining competitive clean accuracy."
    },
    {
        "date": "2025-06",
        "title": "DR-SAC: Distributionally Robust Soft Actor-Critic for Reinforcement Learning under Uncertainty",
        "author": "Mingxuan Cui, Duo Zhou, Yuxuan Han, Grani A. Hanasusanto, Qiong Wang, Huan Zhang, and Zhengyuan Zhou",
        "link": "http://arxiv.org/abs/2506.12622v1",
        "abstract": "Deep reinforcement learning (RL) has achieved significant success, yet its\napplication in real-world scenarios is often hindered by a lack of robustness\nto environmental uncertainties. To solve this challenge, some robust RL\nalgorithms have been proposed, but most are limited to tabular settings. In\nthis work, we propose Distributionally Robust Soft Actor-Critic (DR-SAC), a\nnovel algorithm designed to enhance the robustness of the state-of-the-art Soft\nActor-Critic (SAC) algorithm. DR-SAC aims to maximize the expected value with\nentropy against the worst possible transition model lying in an uncertainty\nset. A distributionally robust version of the soft policy iteration is derived\nwith a convergence guarantee. For settings where nominal distributions are\nunknown, such as offline RL, a generative modeling approach is proposed to\nestimate the required nominal distributions from data. Furthermore,\nexperimental results on a range of continuous control benchmark tasks\ndemonstrate our algorithm achieves up to $9.8$ times the average reward of the\nSAC baseline under common perturbations. Additionally, compared with existing\nrobust reinforcement learning algorithms, DR-SAC significantly improves\ncomputing efficiency and applicability to large-scale problems."
    },
    {
        "date": "2025-06",
        "title": "Existence of Adversarial Examples for Random Convolutional Networks via Isoperimetric Inequalities on $\\mathbb{so}(d)$",
        "author": "Amit Daniely",
        "link": "http://arxiv.org/abs/2506.12613v1",
        "abstract": "We show that adversarial examples exist for various random convolutional\nnetworks, and furthermore, that this is a relatively simple consequence of the\nisoperimetric inequality on the special orthogonal group $\\mathbb{so}(d)$. This\nextends and simplifies a recent line of work which shows similar results for\nrandom fully connected networks."
    },
    {
        "date": "2025-06",
        "title": "Deep Fusion of Ultra-Low-Resolution Thermal Camera and Gyroscope Data for Lighting-Robust and Compute-Efficient Rotational Odometry",
        "author": "Farida Mohsen, and Ali Safa",
        "link": "http://arxiv.org/abs/2506.12536v1",
        "abstract": "Accurate rotational odometry is crucial for autonomous robotic systems,\nparticularly for small, power-constrained platforms such as drones and mobile\nrobots. This study introduces thermal-gyro fusion, a novel sensor fusion\napproach that integrates ultra-low-resolution thermal imaging with gyroscope\nreadings for rotational odometry. Unlike RGB cameras, thermal imaging is\ninvariant to lighting conditions and, when fused with gyroscopic data,\nmitigates drift which is a common limitation of inertial sensors. We first\ndevelop a multimodal data acquisition system to collect synchronized thermal\nand gyroscope data, along with rotational speed labels, across diverse\nenvironments. Subsequently, we design and train a lightweight Convolutional\nNeural Network (CNN) that fuses both modalities for rotational speed\nestimation. Our analysis demonstrates that thermal-gyro fusion enables a\nsignificant reduction in thermal camera resolution without significantly\ncompromising accuracy, thereby improving computational efficiency and memory\nutilization. These advantages make our approach well-suited for real-time\ndeployment in resource-constrained robotic systems. Finally, to facilitate\nfurther research, we publicly release our dataset as supplementary material."
    },
    {
        "date": "2025-06",
        "title": "Similarity as Reward Alignment: Robust and Versatile Preference-based Reinforcement Learning",
        "author": "Sara Rajaram, R. James Cotton, and Fabian H. Sinz",
        "link": "http://arxiv.org/abs/2506.12529v1",
        "abstract": "Preference-based Reinforcement Learning (PbRL) entails a variety of\napproaches for aligning models with human intent to alleviate the burden of\nreward engineering. However, most previous PbRL work has not investigated the\nrobustness to labeler errors, inevitable with labelers who are non-experts or\noperate under time constraints. Additionally, PbRL algorithms often target very\nspecific settings (e.g. pairwise ranked preferences or purely offline\nlearning). We introduce Similarity as Reward Alignment (SARA), a simple\ncontrastive framework that is both resilient to noisy labels and adaptable to\ndiverse feedback formats and training paradigms. SARA learns a latent\nrepresentation of preferred samples and computes rewards as similarities to the\nlearned latent. We demonstrate strong performance compared to baselines on\ncontinuous control offline RL benchmarks. We further demonstrate SARA's\nversatility in applications such as trajectory filtering for downstream tasks,\ncross-task preference transfer, and reward shaping in online learning."
    },
    {
        "date": "2025-06",
        "title": "When Forgetting Triggers Backdoors: A Clean Unlearning Attack",
        "author": "Marco Arazzi, Antonino Nocera, and Vinod P",
        "link": "http://arxiv.org/abs/2506.12522v1",
        "abstract": "Machine unlearning has emerged as a key component in ensuring ``Right to be\nForgotten'', enabling the removal of specific data points from trained models.\nHowever, even when the unlearning is performed without poisoning the forget-set\n(clean unlearning), it can be exploited for stealthy attacks that existing\ndefenses struggle to detect. In this paper, we propose a novel {\\em clean}\nbackdoor attack that exploits both the model learning phase and the subsequent\nunlearning requests. Unlike traditional backdoor methods, during the first\nphase, our approach injects a weak, distributed malicious signal across\nmultiple classes. The real attack is then activated and amplified by\nselectively unlearning {\\em non-poisoned} samples. This strategy results in a\npowerful and stealthy novel attack that is hard to detect or mitigate,\nhighlighting critical vulnerabilities in current unlearning mechanisms and\nhighlighting the need for more robust defenses."
    },
    {
        "date": "2025-06",
        "title": "Exploiting AI for Attacks: On the Interplay between Adversarial AI and Offensive AI",
        "author": "Saskia Laura Schr\u00f6er, Luca Pajola, Alberto Castagnaro, Giovanni Apruzzese, and Mauro Conti",
        "link": "http://arxiv.org/abs/2506.12519v1",
        "abstract": "As Artificial Intelligence (AI) continues to evolve, it has transitioned from\na research-focused discipline to a widely adopted technology, enabling\nintelligent solutions across various sectors. In security, AI's role in\nstrengthening organizational resilience has been studied for over two decades.\nWhile much attention has focused on AI's constructive applications, the\nincreasing maturity and integration of AI have also exposed its darker\npotentials. This article explores two emerging AI-related threats and the\ninterplay between them: AI as a target of attacks (`Adversarial AI') and AI as\na means to launch attacks on any target (`Offensive AI') -- potentially even on\nanother AI. By cutting through the confusion and explaining these threats in\nplain terms, we introduce the complex and often misunderstood interplay between\nAdversarial AI and Offensive AI, offering a clear and accessible introduction\nto the challenges posed by these threats."
    },
    {
        "date": "2025-06",
        "title": "Robust LLM Unlearning with MUDMAN: Meta-Unlearning with Disruption Masking And Normalization",
        "author": "Filip Sondej, Yushi Yang, Miko\u0142aj Kniejski, and Marcel Windys",
        "link": "http://arxiv.org/abs/2506.12484v2",
        "abstract": "Language models can retain dangerous knowledge and skills even after\nextensive safety fine-tuning, posing both misuse and misalignment risks. Recent\nstudies show that even specialized unlearning methods can be easily reversed.\nTo address this, we systematically evaluate many existing and novel components\nof unlearning methods and identify ones crucial for irreversible unlearning.\n  We introduce Disruption Masking, a technique in which we only allow updating\nweights, where the signs of the unlearning gradient and the retaining gradient\nare the same. This ensures all updates are non-disruptive.\n  Additionally, we identify the need for normalizing the unlearning gradients,\nand also confirm the usefulness of meta-learning. We combine these insights\ninto MUDMAN (Meta-Unlearning with Disruption Masking and Normalization) and\nvalidate its effectiveness at preventing the recovery of dangerous\ncapabilities. MUDMAN outperforms the prior TAR method by 40\\%, setting a new\nstate-of-the-art for robust unlearning."
    },
    {
        "date": "2025-06",
        "title": "Towards Safety and Security Testing of Cyberphysical Power Systems by Shape Validation",
        "author": "Alexander Geiger, Immanuel Hacker, \u00d6mer Sen, and Andreas Ulbig",
        "link": "http://arxiv.org/abs/2506.12466v1",
        "abstract": "The increasing complexity of cyberphysical power systems leads to larger\nattack surfaces to be exploited by malicious actors and a higher risk of faults\nthrough misconfiguration. We propose to meet those risks with a declarative\napproach to describe cyberphysical power systems and to automatically evaluate\nsecurity and safety controls. We leverage Semantic Web technologies as a\nwell-standardized framework, providing languages to specify ontologies, rules\nand shape constraints. We model infrastructure through an ontology which\ncombines external ontologies, architecture and data models for sufficient\nexpressivity and interoperability with external systems. The ontology can\nenrich itself through rules defined in SPARQL, allowing for the inference of\nknowledge that is not explicitly stated. Through the evaluation of SHACL shape\nconstraints we can then validate the data and verify safety and security\nconstraints. We demonstrate this concept with two use cases and illustrate how\nthis solution can be developed further in a community-driven fashion."
    },
    {
        "date": "2025-06",
        "title": "Merlin: Multi-View Representation Learning for Robust Multivariate Time Series Forecasting with Unfixed Missing Rates",
        "author": "Chengqing Yu, Fei Wang, Chuanguang Yang, Zezhi Shao, Tao Sun, Tangwen Qian, Wei Wei, Zhulin An, and Yongjun Xu",
        "link": "http://arxiv.org/abs/2506.12459v1",
        "abstract": "Multivariate Time Series Forecasting (MTSF) involves predicting future values\nof multiple interrelated time series. Recently, deep learning-based MTSF models\nhave gained significant attention for their promising ability to mine semantics\n(global and local information) within MTS data. However, these models are\npervasively susceptible to missing values caused by malfunctioning data\ncollectors. These missing values not only disrupt the semantics of MTS, but\ntheir distribution also changes over time. Nevertheless, existing models lack\nrobustness to such issues, leading to suboptimal forecasting performance. To\nthis end, in this paper, we propose Multi-View Representation Learning\n(Merlin), which can help existing models achieve semantic alignment between\nincomplete observations with different missing rates and complete observations\nin MTS. Specifically, Merlin consists of two key modules: offline knowledge\ndistillation and multi-view contrastive learning. The former utilizes a teacher\nmodel to guide a student model in mining semantics from incomplete\nobservations, similar to those obtainable from complete observations. The\nlatter improves the student model's robustness by learning from\npositive/negative data pairs constructed from incomplete observations with\ndifferent missing rates, ensuring semantic alignment across different missing\nrates. Therefore, Merlin is capable of effectively enhancing the robustness of\nexisting models against unfixed missing rates while preserving forecasting\naccuracy. Experiments on four real-world datasets demonstrate the superiority\nof Merlin."
    },
    {
        "date": "2025-06",
        "title": "On the existence of consistent adversarial attacks in high-dimensional linear classification",
        "author": "Matteo Vilucchio, Lenka Zdeborov\u00e1, and Bruno Loureiro",
        "link": "http://arxiv.org/abs/2506.12454v1",
        "abstract": "What fundamentally distinguishes an adversarial attack from a\nmisclassification due to limited model expressivity or finite data? In this\nwork, we investigate this question in the setting of high-dimensional binary\nclassification, where statistical effects due to limited data availability play\na central role. We introduce a new error metric that precisely capture this\ndistinction, quantifying model vulnerability to consistent adversarial attacks\n-- perturbations that preserve the ground-truth labels. Our main technical\ncontribution is an exact and rigorous asymptotic characterization of these\nmetrics in both well-specified models and latent space models, revealing\ndifferent vulnerability patterns compared to standard robust error measures.\nThe theoretical results demonstrate that as models become more\noverparameterized, their vulnerability to label-preserving perturbations grows,\noffering theoretical insight into the mechanisms underlying model sensitivity\nto adversarial attacks."
    },
    {
        "date": "2025-06",
        "title": "LARGO: Low-Rank Regulated Gradient Projection for Robust Parameter Efficient Fine-Tuning",
        "author": "Haotian Zhang, Liu Liu, Baosheng Yu, Jiayan Qiu, Yanwei Ren, and Xianglong Liu",
        "link": "http://arxiv.org/abs/2506.12394v1",
        "abstract": "The advent of parameter-efficient fine-tuning methods has significantly\nreduced the computational burden of adapting large-scale pretrained models to\ndiverse downstream tasks. However, existing approaches often struggle to\nachieve robust performance under domain shifts while maintaining computational\nefficiency. To address this challenge, we propose Low-rAnk Regulated Gradient\nProjection (LARGO) algorithm that integrates dynamic constraints into low-rank\nadaptation methods. Specifically, LARGO incorporates parallel trainable\ngradient projections to dynamically regulate layer-wise updates, retaining the\nOut-Of-Distribution robustness of pretrained model while preserving inter-layer\nindependence. Additionally, it ensures computational efficiency by mitigating\nthe influence of gradient dependencies across layers during weight updates.\nBesides, through leveraging singular value decomposition of pretrained weights\nfor structured initialization, we incorporate an SVD-based initialization\nstrategy that minimizing deviation from pretrained knowledge. Through extensive\nexperiments on diverse benchmarks, LARGO achieves state-of-the-art performance\nacross in-domain and out-of-distribution scenarios, demonstrating improved\nrobustness under domain shifts with significantly lower computational overhead\ncompared to existing PEFT methods. The source code will be released soon."
    },
    {
        "date": "2025-06",
        "title": "Restoring Gaussian Blurred Face Images for Deanonymization Attacks",
        "author": "Haoyu Zhai, Shuo Wang, Pirouz Naghavi, Qingying Hao, and Gang Wang",
        "link": "http://arxiv.org/abs/2506.12344v1",
        "abstract": "Gaussian blur is widely used to blur human faces in sensitive photos before\nthe photos are posted on the Internet. However, it is unclear to what extent\nthe blurred faces can be restored and used to re-identify the person,\nespecially under a high-blurring setting. In this paper, we explore this\nquestion by developing a deblurring method called Revelio. The key intuition is\nto leverage a generative model's memorization effect and approximate the\ninverse function of Gaussian blur for face restoration. Compared with existing\nmethods, we design the deblurring process to be identity-preserving. It uses a\nconditional Diffusion model for preliminary face restoration and then uses an\nidentity retrieval model to retrieve related images to further enhance\nfidelity. We evaluate Revelio with large public face image datasets and show\nthat it can effectively restore blurred faces, especially under a high-blurring\nsetting. It has a re-identification accuracy of 95.9%, outperforming existing\nsolutions. The result suggests that Gaussian blur should not be used for face\nanonymization purposes. We also demonstrate the robustness of this method\nagainst mismatched Gaussian kernel sizes and functions, and test preliminary\ncountermeasures and adaptive attacks to inspire future work."
    },
    {
        "date": "2025-06",
        "title": "Image Corruption-Inspired Membership Inference Attacks against Large Vision-Language Models",
        "author": "Zongyu Wu, Minhua Lin, Zhiwei Zhang, Fali Wang, Xianren Zhang, Xiang Zhang, and Suhang Wang",
        "link": "http://arxiv.org/abs/2506.12340v2",
        "abstract": "Large vision-language models (LVLMs) have demonstrated outstanding\nperformance in many downstream tasks. However, LVLMs are trained on large-scale\ndatasets, which can pose privacy risks if training images contain sensitive\ninformation. Therefore, it is important to detect whether an image is used to\ntrain the LVLM. Recent studies have investigated membership inference attacks\n(MIAs) against LVLMs, including detecting image-text pairs and single-modality\ncontent. In this work, we focus on detecting whether a target image is used to\ntrain the target LVLM. We design simple yet effective Image Corruption-Inspired\nMembership Inference Attacks (ICIMIA) against LLVLMs, which are inspired by\nLVLM's different sensitivity to image corruption for member and non-member\nimages. We first perform an MIA method under the white-box setting, where we\ncan obtain the embeddings of the image through the vision part of the target\nLVLM. The attacks are based on the embedding similarity between the image and\nits corrupted version. We further explore a more practical scenario where we\nhave no knowledge about target LVLMs and we can only query the target LVLMs\nwith an image and a question. We then conduct the attack by utilizing the\noutput text embeddings' similarity. Experiments on existing datasets validate\nthe effectiveness of our proposed attack methods under those two different\nsettings."
    },
    {
        "date": "2025-06",
        "title": "GroupNL: Low-Resource and Robust CNN Design over Cloud and Device",
        "author": "Chuntao Ding, Jianhang Xie, Junna Zhang, Salman Raza, Shangguang Wang, and Jiannong Cao",
        "link": "http://arxiv.org/abs/2506.12335v1",
        "abstract": "It has become mainstream to deploy Convolutional Neural Network (CNN) models\non ubiquitous Internet of Things (IoT) devices with the help of the cloud to\nprovide users with a variety of high-quality services. Most existing methods\nhave two limitations: (i) low robustness in handling corrupted image data\ncollected by IoT devices; and (ii) high consumption of computational and\ntransmission resources. To this end, we propose the Grouped NonLinear\ntransformation generation method (GroupNL), which generates diversified feature\nmaps by utilizing data-agnostic Nonlinear Transformation Functions (NLFs) to\nimprove the robustness of the CNN model. Specifically, partial convolution\nfilters are designated as seed filters in a convolutional layer, and a small\nset of feature maps, i.e., seed feature maps, are first generated based on\nvanilla convolution operation. Then, we split seed feature maps into several\ngroups, each with a set of different NLFs, to generate corresponding diverse\nfeature maps with in-place nonlinear processing. Moreover, GroupNL effectively\nreduces the parameter transmission between multiple nodes during model training\nby setting the hyperparameters of NLFs to random initialization and not\nupdating them during model training, and reduces the computing resources by\nusing NLFs to generate feature maps instead of most feature maps generated\nbased on sliding windows. Experimental results on CIFAR-10, GTSRB, CIFAR-10-C,\nIcons50, and ImageNet-1K datasets in NVIDIA RTX GPU platforms show that the\nproposed GroupNL outperforms other state-of-the-art methods in model robust and\ntraining acceleration. Specifically, on the Icons-50 dataset, the accuracy of\nGroupNL-ResNet-18 achieves approximately 2.86% higher than the vanilla\nResNet-18. GroupNL improves training speed by about 53% compared to vanilla CNN\nwhen trained on a cluster of 8 NVIDIA RTX 4090 GPUs on the ImageNet-1K dataset."
    },
    {
        "date": "2025-06",
        "title": "A Fast, Reliable, and Secure Programming Language for LLM Agents with Code Actions",
        "author": "Stephen Mell, Botong Zhang, David Mell, Shuo Li, Ramya Ramalingam, Nathan Yu, Steve Zdancewic, and Osbert Bastani",
        "link": "http://arxiv.org/abs/2506.12202v1",
        "abstract": "Modern large language models (LLMs) are often deployed as agents, calling\nexternal tools adaptively to solve tasks. Rather than directly calling tools,\nit can be more effective for LLMs to write code to perform the tool calls,\nenabling them to automatically generate complex control flow such as\nconditionals and loops. Such code actions are typically provided as Python\ncode, since LLMs are quite proficient at it; however, Python may not be the\nideal language due to limited built-in support for performance, security, and\nreliability. We propose a novel programming language for code actions, called\nQuasar, which has several benefits: (1) automated parallelization to improve\nperformance, (2) uncertainty quantification to improve reliability and mitigate\nhallucinations, and (3) security features enabling the user to validate\nactions. LLMs can write code in a subset of Python, which is automatically\ntranspiled to Quasar. We evaluate our approach on the ViperGPT visual question\nanswering agent, applied to the GQA dataset, demonstrating that LLMs with\nQuasar actions instead of Python actions retain strong performance, while\nreducing execution time when possible by 42%, improving security by reducing\nuser approval interactions when possible by 52%, and improving reliability by\napplying conformal prediction to achieve a desired target coverage level."
    },
    {
        "date": "2025-06",
        "title": "A Neural Rejection System Against Universal Adversarial Perturbations in Radio Signal Classification",
        "author": "Lu Zhang, Sangarapillai Lambotharan, Gan Zheng, and Fabio Roli",
        "link": "http://arxiv.org/abs/2506.11901v1",
        "abstract": "Advantages of deep learning over traditional methods have been demonstrated\nfor radio signal classification in the recent years. However, various\nresearchers have discovered that even a small but intentional feature\nperturbation known as adversarial examples can significantly deteriorate the\nperformance of the deep learning based radio signal classification. Among\nvarious kinds of adversarial examples, universal adversarial perturbation has\ngained considerable attention due to its feature of being data independent,\nhence as a practical strategy to fool the radio signal classification with a\nhigh success rate. Therefore, in this paper, we investigate a defense system\ncalled neural rejection system to propose against universal adversarial\nperturbations, and evaluate its performance by generating white-box universal\nadversarial perturbations. We show that the proposed neural rejection system is\nable to defend universal adversarial perturbations with significantly higher\naccuracy than the undefended deep neural network."
    },
    {
        "date": "2025-06",
        "title": "Attention-based Adversarial Robust Distillation in Radio Signal Classifications for Low-Power IoT Devices",
        "author": "Lu Zhang, Sangarapillai Lambotharan, Gan Zheng, Guisheng Liao, Basil AsSadhan, and Fabio Roli",
        "link": "http://arxiv.org/abs/2506.11892v1",
        "abstract": "Due to great success of transformers in many applications such as natural\nlanguage processing and computer vision, transformers have been successfully\napplied in automatic modulation classification. We have shown that\ntransformer-based radio signal classification is vulnerable to imperceptible\nand carefully crafted attacks called adversarial examples. Therefore, we\npropose a defense system against adversarial examples in transformer-based\nmodulation classifications. Considering the need for computationally efficient\narchitecture particularly for Internet of Things (IoT)-based applications or\noperation of devices in environment where power supply is limited, we propose a\ncompact transformer for modulation classification. The advantages of robust\ntraining such as adversarial training in transformers may not be attainable in\ncompact transformers. By demonstrating this, we propose a novel compact\ntransformer that can enhance robustness in the presence of adversarial attacks.\nThe new method is aimed at transferring the adversarial attention map from the\nrobustly trained large transformer to a compact transformer. The proposed\nmethod outperforms the state-of-the-art techniques for the considered white-box\nscenarios including fast gradient method and projected gradient descent\nattacks. We have provided reasoning of the underlying working mechanisms and\ninvestigated the transferability of the adversarial examples between different\narchitectures. The proposed method has the potential to protect the transformer\nfrom the transferability of adversarial examples."
    },
    {
        "date": "2025-06",
        "title": "Robust Molecular Property Prediction via Densifying Scarce Labeled Data",
        "author": "Jina Kim, Jeffrey Willette, Bruno Andreis, and Sung Ju Hwang",
        "link": "http://arxiv.org/abs/2506.11877v1",
        "abstract": "A widely recognized limitation of molecular prediction models is their\nreliance on structures observed in the training data, resulting in poor\ngeneralization to out-of-distribution compounds. Yet in drug discovery, the\ncompounds most critical for advancing research often lie beyond the training\nset, making the bias toward the training data particularly problematic. This\nmismatch introduces substantial covariate shift, under which standard deep\nlearning models produce unstable and inaccurate predictions. Furthermore, the\nscarcity of labeled data, stemming from the onerous and costly nature of\nexperimental validation, further exacerbates the difficulty of achieving\nreliable generalization. To address these limitations, we propose a novel\nmeta-learning-based approach that leverages unlabeled data to interpolate\nbetween in-distribution (ID) and out-of-distribution (OOD) data, enabling the\nmodel to meta-learn how to generalize beyond the training distribution. We\ndemonstrate significant performance gains over state-of-the-art methods on\nchallenging real-world datasets that exhibit substantial covariate shift."
    },
    {
        "date": "2025-06",
        "title": "In Defense of Defensive Forecasting",
        "author": "Juan Carlos Perdomo, and Benjamin Recht",
        "link": "http://arxiv.org/abs/2506.11848v1",
        "abstract": "This tutorial provides a survey of algorithms for Defensive Forecasting,\nwhere predictions are derived not by prognostication but by correcting past\nmistakes. Pioneered by Vovk, Defensive Forecasting frames the goal of\nprediction as a sequential game, and derives predictions to minimize metrics no\nmatter what outcomes occur. We present an elementary introduction to this\ngeneral theory and derive simple, near-optimal algorithms for online learning,\ncalibration, prediction with expert advice, and online conformal prediction."
    },
    {
        "date": "2025-06",
        "title": "TrustGLM: Evaluating the Robustness of GraphLLMs Against Prompt, Text, and Structure Attacks",
        "author": "Qihai Zhang, Xinyue Sheng, Yuanfu Sun, and Qiaoyu Tan",
        "link": "http://arxiv.org/abs/2506.11844v1",
        "abstract": "Inspired by the success of large language models (LLMs), there is a\nsignificant research shift from traditional graph learning methods to LLM-based\ngraph frameworks, formally known as GraphLLMs. GraphLLMs leverage the reasoning\npower of LLMs by integrating three key components: the textual attributes of\ninput nodes, the structural information of node neighborhoods, and\ntask-specific prompts that guide decision-making. Despite their promise, the\nrobustness of GraphLLMs against adversarial perturbations remains largely\nunexplored-a critical concern for deploying these models in high-stakes\nscenarios. To bridge the gap, we introduce TrustGLM, a comprehensive study\nevaluating the vulnerability of GraphLLMs to adversarial attacks across three\ndimensions: text, graph structure, and prompt manipulations. We implement\nstate-of-the-art attack algorithms from each perspective to rigorously assess\nmodel resilience. Through extensive experiments on six benchmark datasets from\ndiverse domains, our findings reveal that GraphLLMs are highly susceptible to\ntext attacks that merely replace a few semantically similar words in a node's\ntextual attribute. We also find that standard graph structure attack methods\ncan significantly degrade model performance, while random shuffling of the\ncandidate label set in prompt templates leads to substantial performance drops.\nBeyond characterizing these vulnerabilities, we investigate defense techniques\ntailored to each attack vector through data-augmented training and adversarial\ntraining, which show promising potential to enhance the robustness of\nGraphLLMs. We hope that our open-sourced library will facilitate rapid,\nequitable evaluation and inspire further innovative research in this field."
    },
    {
        "date": "2025-06",
        "title": "SEC-bench: Automated Benchmarking of LLM Agents on Real-World Software Security Tasks",
        "author": "Hwiwon Lee, Ziqi Zhang, Hanxiao Lu, and Lingming Zhang",
        "link": "http://arxiv.org/abs/2506.11791v1",
        "abstract": "Rigorous security-focused evaluation of large language model (LLM) agents is\nimperative for establishing trust in their safe deployment throughout the\nsoftware development lifecycle. However, existing benchmarks largely rely on\nsynthetic challenges or simplified vulnerability datasets that fail to capture\nthe complexity and ambiguity encountered by security engineers in practice. We\nintroduce SEC-bench, the first fully automated benchmarking framework for\nevaluating LLM agents on authentic security engineering tasks. SEC-bench\nemploys a novel multi-agent scaffold that automatically constructs code\nrepositories with harnesses, reproduces vulnerabilities in isolated\nenvironments, and generates gold patches for reliable evaluation. Our framework\nautomatically creates high-quality software vulnerability datasets with\nreproducible artifacts at a cost of only $0.87 per instance. Using SEC-bench,\nwe implement two critical software security tasks to rigorously evaluate LLM\nagents' capabilities: proof-of-concept (PoC) generation and vulnerability\npatching. A comprehensive evaluation of state-of-the-art LLM code agents\nreveals significant performance gaps, achieving at most 18.0% success in PoC\ngeneration and 34.0% in vulnerability patching on our complete dataset. These\nresults highlight the crucial steps needed toward developing LLM agents that\nare more practical, intelligent, and autonomous for security engineering."
    },
    {
        "date": "2025-06",
        "title": "LLMs on support of privacy and security of mobile apps: state of the art and research directions",
        "author": "Tran Thanh Lam Nguyen, Barbara Carminati, and Elena Ferrari",
        "link": "http://arxiv.org/abs/2506.11679v1",
        "abstract": "Modern life has witnessed the explosion of mobile devices. However, besides\nthe valuable features that bring convenience to end users, security and privacy\nrisks still threaten users of mobile apps. The increasing sophistication of\nthese threats in recent years has underscored the need for more advanced and\nefficient detection approaches. In this chapter, we explore the application of\nLarge Language Models (LLMs) to identify security risks and privacy violations\nand mitigate them for the mobile application ecosystem. By introducing\nstate-of-the-art research that applied LLMs to mitigate the top 10 common\nsecurity risks of smartphone platforms, we highlight the feasibility and\npotential of LLMs to replace traditional analysis methods, such as dynamic and\nhybrid analysis of mobile apps. As a representative example of LLM-based\nsolutions, we present an approach to detect sensitive data leakage when users\nshare images online, a common behavior of smartphone users nowadays. Finally,\nwe discuss open research challenges."
    },
    {
        "date": "2025-06",
        "title": "Machine Unlearning for Robust DNNs: Attribution-Guided Partitioning and Neuron Pruning in Noisy Environments",
        "author": "Deliang Jin, Gang Chen, Shuo Feng, Yufeng Ling, and Haoran Zhu",
        "link": "http://arxiv.org/abs/2506.11615v1",
        "abstract": "Deep neural networks (DNNs) have achieved remarkable success across diverse\ndomains, but their performance can be severely degraded by noisy or corrupted\ntraining data. Conventional noise mitigation methods often rely on explicit\nassumptions about noise distributions or require extensive retraining, which\ncan be impractical for large-scale models. Inspired by the principles of\nmachine unlearning, we propose a novel framework that integrates\nattribution-guided data partitioning, discriminative neuron pruning, and\ntargeted fine-tuning to mitigate the impact of noisy samples. Our approach\nemploys gradient-based attribution to probabilistically distinguish\nhigh-quality examples from potentially corrupted ones without imposing\nrestrictive assumptions on the noise. It then applies regression-based\nsensitivity analysis to identify and prune neurons that are most vulnerable to\nnoise. Finally, the resulting network is fine-tuned on the high-quality data\nsubset to efficiently recover and enhance its generalization performance. This\nintegrated unlearning-inspired framework provides several advantages over\nconventional noise-robust learning approaches. Notably, it combines data-level\nunlearning with model-level adaptation, thereby avoiding the need for full\nmodel retraining or explicit noise modeling. We evaluate our method on\nrepresentative tasks (e.g., CIFAR-10 image classification and speech\nrecognition) under various noise levels and observe substantial gains in both\naccuracy and efficiency. For example, our framework achieves approximately a\n10% absolute accuracy improvement over standard retraining on CIFAR-10 with\ninjected label noise, while reducing retraining time by up to 47% in some\nsettings. These results demonstrate the effectiveness and scalability of the\nproposed approach for achieving robust generalization in noisy environments."
    },
    {
        "date": "2025-06",
        "title": "KCES: Training-Free Defense for Robust Graph Neural Networks via Kernel Complexity",
        "author": "Yaning Jia, Shenyang Deng, Chiyu Ma, Yaoqing Yang, and Soroush Vosoughi",
        "link": "http://arxiv.org/abs/2506.11611v2",
        "abstract": "Graph Neural Networks (GNNs) have achieved impressive success across a wide\nrange of graph-based tasks, yet they remain highly vulnerable to small,\nimperceptible perturbations and adversarial attacks. Although numerous defense\nmethods have been proposed to address these vulnerabilities, many rely on\nheuristic metrics, overfit to specific attack patterns, and suffer from high\ncomputational complexity. In this paper, we propose Kernel Complexity-Based\nEdge Sanitization (KCES), a training-free, model-agnostic defense framework.\nKCES leverages Graph Kernel Complexity (GKC), a novel metric derived from the\ngraph's Gram matrix that characterizes GNN generalization via its test error\nbound. Building on GKC, we define a KC score for each edge, measuring the\nchange in GKC when the edge is removed. Edges with high KC scores, typically\nintroduced by adversarial perturbations, are pruned to mitigate their harmful\neffects, thereby enhancing GNNs' robustness. KCES can also be seamlessly\nintegrated with existing defense strategies as a plug-and-play module without\nrequiring training. Theoretical analysis and extensive experiments demonstrate\nthat KCES consistently enhances GNN robustness, outperforms state-of-the-art\nbaselines, and amplifies the effectiveness of existing defenses, offering a\nprincipled and efficient solution for securing GNNs."
    },
    {
        "date": "2025-06",
        "title": "SecONNds: Secure Outsourced Neural Network Inference on ImageNet",
        "author": "Shashank Balla",
        "link": "http://arxiv.org/abs/2506.11586v1",
        "abstract": "The widespread adoption of outsourced neural network inference presents\nsignificant privacy challenges, as sensitive user data is processed on\nuntrusted remote servers. Secure inference offers a privacy-preserving\nsolution, but existing frameworks suffer from high computational overhead and\ncommunication costs, rendering them impractical for real-world deployment. We\nintroduce SecONNds, a non-intrusive secure inference framework optimized for\nlarge ImageNet-scale Convolutional Neural Networks. SecONNds integrates a novel\nfully Boolean Goldreich-Micali-Wigderson (GMW) protocol for secure comparison\n-- addressing Yao's millionaires' problem -- using preprocessed Beaver's bit\ntriples generated from Silent Random Oblivious Transfer. Our novel protocol\nachieves an online speedup of 17$\\times$ in nonlinear operations compared to\nstate-of-the-art solutions while reducing communication overhead. To further\nenhance performance, SecONNds employs Number Theoretic Transform (NTT)\npreprocessing and leverages GPU acceleration for homomorphic encryption\noperations, resulting in speedups of 1.6$\\times$ on CPU and 2.2$\\times$ on GPU\nfor linear operations. We also present SecONNds-P, a bit-exact variant that\nensures verifiable full-precision results in secure computation, matching the\nresults of plaintext computations. Evaluated on a 37-bit quantized SqueezeNet\nmodel, SecONNds achieves an end-to-end inference time of 2.8 s on GPU and 3.6 s\non CPU, with a total communication of just 420 MiB. SecONNds' efficiency and\nreduced computational load make it well-suited for deploying privacy-sensitive\napplications in resource-constrained environments. SecONNds is open source and\ncan be accessed from: https://github.com/shashankballa/SecONNds."
    },
    {
        "date": "2025-06",
        "title": "Linearly Solving Robust Rotation Estimation",
        "author": "Yinlong Liu, Tianyu Huang, and Zhi-Xin Yang",
        "link": "http://arxiv.org/abs/2506.11547v1",
        "abstract": "Rotation estimation plays a fundamental role in computer vision and robot\ntasks, and extremely robust rotation estimation is significantly useful for\nsafety-critical applications. Typically, estimating a rotation is considered a\nnon-linear and non-convex optimization problem that requires careful design.\nHowever, in this paper, we provide some new perspectives that solving a\nrotation estimation problem can be reformulated as solving a linear model\nfitting problem without dropping any constraints and without introducing any\nsingularities. In addition, we explore the dual structure of a rotation motion,\nrevealing that it can be represented as a great circle on a quaternion sphere\nsurface. Accordingly, we propose an easily understandable voting-based method\nto solve rotation estimation. The proposed method exhibits exceptional\nrobustness to noise and outliers and can be computed in parallel with graphics\nprocessing units (GPUs) effortlessly. Particularly, leveraging the power of\nGPUs, the proposed method can obtain a satisfactory rotation solution for\nlarge-scale($10^6$) and severely corrupted (99$\\%$ outlier ratio) rotation\nestimation problems under 0.5 seconds. Furthermore, to validate our theoretical\nframework and demonstrate the superiority of our proposed method, we conduct\ncontrolled experiments and real-world dataset experiments. These experiments\nprovide compelling evidence supporting the effectiveness and robustness of our\napproach in solving rotation estimation problems."
    },
    {
        "date": "2025-06",
        "title": "Robust Filtering -- Novel Statistical Learning and Inference Algorithms with Applications",
        "author": "Aamir Hussain Chughtai",
        "link": "http://arxiv.org/abs/2506.11530v1",
        "abstract": "State estimation or filtering serves as a fundamental task to enable\nintelligent decision-making in applications such as autonomous vehicles,\nrobotics, healthcare monitoring, smart grids, intelligent transportation, and\npredictive maintenance. Standard filtering assumes prior knowledge of noise\nstatistics to extract latent system states from noisy sensor data. However,\nreal-world scenarios involve abnormalities like outliers, biases, drifts, and\nmissing observations with unknown or partially known statistics, limiting\nconventional approaches. This thesis presents novel robust nonlinear filtering\nmethods to mitigate these challenges. Based on insights from our filtering\nproposals, we extend the formulations to offline estimation/learning setups and\npropose smoothing extensions. Our methods leverage Bayesian inference\nframeworks, employing both deterministic and stochastic approximation\ntechniques including Variational Inference (VI) and Particle Filters/Sequential\nMonte Carlo (SMC). We also study theoretical estimation limits using Bayesian\nCram\\'er-Rao bounds (BCRBs) in the context of measurement abnormalities. To\nvalidate the performance gains of the proposed methods, we perform simulations\nand experiments in scenarios including target tracking, indoor localization, 3D\npoint cloud registration, mesh registration, and pose graph optimization. The\nfundamental nature of the work makes it useful in diverse applications, with\npossible future extensions toward developing outlier-robust machine learning\npipelines, learning system dynamics from anomalous data, and addressing\nchallenges in generative AI where standard diffusion models struggle with\noutliers, imbalanced datasets, and mode collapse."
    },
    {
        "date": "2025-06",
        "title": "Investigating Vulnerabilities and Defenses Against Audio-Visual Attacks: A Comprehensive Survey Emphasizing Multimodal Models",
        "author": "Jinming Wen, Xinyi Wu, Shuai Zhao, Yanhao Jia, and Yuwen Li",
        "link": "http://arxiv.org/abs/2506.11521v1",
        "abstract": "Multimodal large language models (MLLMs), which bridge the gap between\naudio-visual and natural language processing, achieve state-of-the-art\nperformance on several audio-visual tasks. Despite the superior performance of\nMLLMs, the scarcity of high-quality audio-visual training data and\ncomputational resources necessitates the utilization of third-party data and\nopen-source MLLMs, a trend that is increasingly observed in contemporary\nresearch. This prosperity masks significant security risks. Empirical studies\ndemonstrate that the latest MLLMs can be manipulated to produce malicious or\nharmful content. This manipulation is facilitated exclusively through\ninstructions or inputs, including adversarial perturbations and malevolent\nqueries, effectively bypassing the internal security mechanisms embedded within\nthe models. To gain a deeper comprehension of the inherent security\nvulnerabilities associated with audio-visual-based multimodal models, a series\nof surveys investigates various types of attacks, including adversarial and\nbackdoor attacks. While existing surveys on audio-visual attacks provide a\ncomprehensive overview, they are limited to specific types of attacks, which\nlack a unified review of various types of attacks. To address this issue and\ngain insights into the latest trends in the field, this paper presents a\ncomprehensive and systematic review of audio-visual attacks, which include\nadversarial attacks, backdoor attacks, and jailbreak attacks. Furthermore, this\npaper also reviews various types of attacks in the latest audio-visual-based\nMLLMs, a dimension notably absent in existing surveys. Drawing upon\ncomprehensive insights from a substantial review, this paper delineates both\nchallenges and emergent trends for future research on audio-visual attacks and\ndefense."
    },
    {
        "date": "2025-06",
        "title": "On the Natural Robustness of Vision-Language Models Against Visual Perception Attacks in Autonomous Driving",
        "author": "Pedram MohajerAnsari, Amir Salarpour, Michael K\u00fchr, Siyu Huang, Mohammad Hamad, Sebastian Steinhorst, Habeeb Olufowobi, and Mert D. Pes\u00e9",
        "link": "http://arxiv.org/abs/2506.11472v1",
        "abstract": "Autonomous vehicles (AVs) rely on deep neural networks (DNNs) for critical\ntasks such as traffic sign recognition (TSR), automated lane centering (ALC),\nand vehicle detection (VD). However, these models are vulnerable to attacks\nthat can cause misclassifications and compromise safety. Traditional defense\nmechanisms, including adversarial training, often degrade benign accuracy and\nfail to generalize against unseen attacks. In this work, we introduce Vehicle\nVision Language Models (V2LMs), fine-tuned vision-language models specialized\nfor AV perception. Our findings demonstrate that V2LMs inherently exhibit\nsuperior robustness against unseen attacks without requiring adversarial\ntraining, maintaining significantly higher accuracy than conventional DNNs\nunder adversarial conditions. We evaluate two deployment strategies: Solo Mode,\nwhere individual V2LMs handle specific perception tasks, and Tandem Mode, where\na single unified V2LM is fine-tuned for multiple tasks simultaneously.\nExperimental results reveal that DNNs suffer performance drops of 33% to 46%\nunder attacks, whereas V2LMs maintain adversarial accuracy with reductions of\nless than 8% on average. The Tandem Mode further offers a memory-efficient\nalternative while achieving comparable robustness to Solo Mode. We also explore\nintegrating V2LMs as parallel components to AV perception to enhance resilience\nagainst adversarial threats. Our results suggest that V2LMs offer a promising\npath toward more secure and resilient AV perception systems."
    },
    {
        "date": "2025-06",
        "title": "DRIFT: Dynamic Rule-Based Defense with Injection Isolation for Securing LLM Agents",
        "author": "Hao Li, Xiaogeng Liu, Hung-Chun Chiu, Dianqi Li, Ning Zhang, and Chaowei Xiao",
        "link": "http://arxiv.org/abs/2506.12104v1",
        "abstract": "Large Language Models (LLMs) are increasingly central to agentic systems due\nto their strong reasoning and planning capabilities. By interacting with\nexternal environments through predefined tools, these agents can carry out\ncomplex user tasks. Nonetheless, this interaction also introduces the risk of\nprompt injection attacks, where malicious inputs from external sources can\nmislead the agent's behavior, potentially resulting in economic loss, privacy\nleakage, or system compromise. System-level defenses have recently shown\npromise by enforcing static or predefined policies, but they still face two key\nchallenges: the ability to dynamically update security rules and the need for\nmemory stream isolation. To address these challenges, we propose DRIFT, a\nDynamic Rule-based Isolation Framework for Trustworthy agentic systems, which\nenforces both control- and data-level constraints. A Secure Planner first\nconstructs a minimal function trajectory and a JSON-schema-style parameter\nchecklist for each function node based on the user query. A Dynamic Validator\nthen monitors deviations from the original plan, assessing whether changes\ncomply with privilege limitations and the user's intent. Finally, an Injection\nIsolator detects and masks any instructions that may conflict with the user\nquery from the memory stream to mitigate long-term risks. We empirically\nvalidate the effectiveness of DRIFT on the AgentDojo benchmark, demonstrating\nits strong security performance while maintaining high utility across diverse\nmodels -- showcasing both its robustness and adaptability."
    },
    {
        "date": "2025-06",
        "title": "GaussMarker: Robust Dual-Domain Watermark for Diffusion Models",
        "author": "Kecen Li, Zhicong Huang, Xinwen Hou, and Cheng Hong",
        "link": "http://arxiv.org/abs/2506.11444v1",
        "abstract": "As Diffusion Models (DM) generate increasingly realistic images, related\nissues such as copyright and misuse have become a growing concern. Watermarking\nis one of the promising solutions. Existing methods inject the watermark into\nthe single-domain of initial Gaussian noise for generation, which suffers from\nunsatisfactory robustness. This paper presents the first dual-domain DM\nwatermarking approach using a pipelined injector to consistently embed\nwatermarks in both the spatial and frequency domains. To further boost\nrobustness against certain image manipulations and advanced attacks, we\nintroduce a model-independent learnable Gaussian Noise Restorer (GNR) to refine\nGaussian noise extracted from manipulated images and enhance detection\nrobustness by integrating the detection scores of both watermarks. GaussMarker\nefficiently achieves state-of-the-art performance under eight image distortions\nand four advanced attacks across three versions of Stable Diffusion with better\nrecall and lower false positive rates, as preferred in real applications."
    },
    {
        "date": "2025-06",
        "title": "Improving Group Robustness on Spurious Correlation via Evidential Alignment",
        "author": "Wenqian Ye, Guangtao Zheng, and Aidong Zhang",
        "link": "http://arxiv.org/abs/2506.11347v2",
        "abstract": "Deep neural networks often learn and rely on spurious correlations, i.e.,\nsuperficial associations between non-causal features and the targets. For\ninstance, an image classifier may identify camels based on the desert\nbackgrounds. While it can yield high overall accuracy during training, it\ndegrades generalization on more diverse scenarios where such correlations do\nnot hold. This problem poses significant challenges for out-of-distribution\nrobustness and trustworthiness. Existing methods typically mitigate this issue\nby using external group annotations or auxiliary deterministic models to learn\nunbiased representations. However, such information is costly to obtain, and\ndeterministic models may fail to capture the full spectrum of biases learned by\nthe models. To address these limitations, we propose Evidential Alignment, a\nnovel framework that leverages uncertainty quantification to understand the\nbehavior of the biased models without requiring group annotations. By\nquantifying the evidence of model prediction with second-order risk\nminimization and calibrating the biased models with the proposed evidential\ncalibration technique, Evidential Alignment identifies and suppresses spurious\ncorrelations while preserving core features. We theoretically justify the\neffectiveness of our method as capable of learning the patterns of biased\nmodels and debiasing the model without requiring any spurious correlation\nannotations. Empirical results demonstrate that our method significantly\nimproves group robustness across diverse architectures and data modalities,\nproviding a scalable and principled solution to spurious correlations."
    },
    {
        "date": "2025-06",
        "title": "Poutine: Vision-Language-Trajectory Pre-Training and Reinforcement Learning Post-Training Enable Robust End-to-End Autonomous Driving",
        "author": "Luke Rowe, Rodrigue de Schaetzen, Roger Girgis, Christopher Pal, and Liam Paull",
        "link": "http://arxiv.org/abs/2506.11234v1",
        "abstract": "We present Poutine, a 3B-parameter vision-language model (VLM) tailored for\nend-to-end autonomous driving in long-tail driving scenarios. Poutine is\ntrained in two stages. To obtain strong base driving capabilities, we train\nPoutine-Base in a self-supervised vision-language-trajectory (VLT) next-token\nprediction fashion on 83 hours of CoVLA nominal driving and 11 hours of Waymo\nlong-tail driving. Accompanying language annotations are auto-generated with a\n72B-parameter VLM. Poutine is obtained by fine-tuning Poutine-Base with Group\nRelative Policy Optimization (GRPO) using less than 500 preference-labeled\nframes from the Waymo validation set. We show that both VLT pretraining and RL\nfine-tuning are critical to attain strong driving performance in the long-tail.\nPoutine-Base achieves a rater-feedback score (RFS) of 8.12 on the validation\nset, nearly matching Waymo's expert ground-truth RFS. The final Poutine model\nachieves an RFS of 7.99 on the official Waymo test set, placing 1st in the 2025\nWaymo Vision-Based End-to-End Driving Challenge by a significant margin. These\nresults highlight the promise of scalable VLT pre-training and lightweight RL\nfine-tuning to enable robust and generalizable autonomy."
    },
    {
        "date": "2025-06",
        "title": "Monitoring Decomposition Attacks in LLMs with Lightweight Sequential Monitors",
        "author": "Chen Yueh-Han, Nitish Joshi, Yulin Chen, Maksym Andriushchenko, Rico Angell, and He He",
        "link": "http://arxiv.org/abs/2506.10949v2",
        "abstract": "Current LLM safety defenses fail under decomposition attacks, where a\nmalicious goal is decomposed into benign subtasks that circumvent refusals. The\nchallenge lies in the existing shallow safety alignment techniques: they only\ndetect harm in the immediate prompt and do not reason about long-range intent,\nleaving them blind to malicious intent that emerges over a sequence of\nseemingly benign instructions. We therefore propose adding an external monitor\nthat observes the conversation at a higher granularity. To facilitate our study\nof monitoring decomposition attacks, we curate the largest and most diverse\ndataset to date, including question-answering, text-to-image, and agentic\ntasks. We verify our datasets by testing them on frontier LLMs and show an 87%\nattack success rate on average on GPT-4o. This confirms that decomposition\nattack is broadly effective. Additionally, we find that random tasks can be\ninjected into the decomposed subtasks to further obfuscate malicious intents.\nTo defend in real time, we propose a lightweight sequential monitoring\nframework that cumulatively evaluates each subtask. We show that a carefully\nprompt engineered lightweight monitor achieves a 93% defense success rate,\nbeating reasoning models like o3 mini as a monitor. Moreover, it remains robust\nagainst random task injection and cuts cost by 90% and latency by 50%. Our\nfindings suggest that lightweight sequential monitors are highly effective in\nmitigating decomposition attacks and are viable in deployment."
    },
    {
        "date": "2025-06",
        "title": "Lattice Climber Attack: Adversarial attacks for randomized mixtures of classifiers",
        "author": "Lucas Gnecco-Heredia, Benjamin Negrevergne, and Yann Chevaleyre",
        "link": "http://arxiv.org/abs/2506.10888v1",
        "abstract": "Finite mixtures of classifiers (a.k.a. randomized ensembles) have been\nproposed as a way to improve robustness against adversarial attacks. However,\nexisting attacks have been shown to not suit this kind of classifier. In this\npaper, we discuss the problem of attacking a mixture in a principled way and\nintroduce two desirable properties of attacks based on a geometrical analysis\nof the problem (effectiveness and maximality). We then show that existing\nattacks do not meet both of these properties. Finally, we introduce a new\nattack called {\\em lattice climber attack} with theoretical guarantees in the\nbinary linear setting, and demonstrate its performance by conducting\nexperiments on synthetic and real datasets."
    },
    {
        "date": "2025-06",
        "title": "Viability of Future Actions: Robust Safety in Reinforcement Learning via Entropy Regularization",
        "author": "Pierre-Fran\u00e7ois Massiani, Alexander von Rohr, Lukas Haverbeck, and Sebastian Trimpe",
        "link": "http://arxiv.org/abs/2506.10871v1",
        "abstract": "Despite the many recent advances in reinforcement learning (RL), the question\nof learning policies that robustly satisfy state constraints under unknown\ndisturbances remains open. In this paper, we offer a new perspective on\nachieving robust safety by analyzing the interplay between two well-established\ntechniques in model-free RL: entropy regularization, and constraints\npenalization. We reveal empirically that entropy regularization in constrained\nRL inherently biases learning toward maximizing the number of future viable\nactions, thereby promoting constraints satisfaction robust to action noise.\nFurthermore, we show that by relaxing strict safety constraints through\npenalties, the constrained RL problem can be approximated arbitrarily closely\nby an unconstrained one and thus solved using standard model-free RL. This\nreformulation preserves both safety and optimality while empirically improving\nresilience to disturbances. Our results indicate that the connection between\nentropy regularization and robustness is a promising avenue for further\nempirical and theoretical investigation, as it enables robust safety in RL\nthrough simple reward shaping."
    },
    {
        "date": "2025-06",
        "title": "Advanced fraud detection using machine learning models: enhancing financial transaction security",
        "author": "Nudrat Fariha, Md Nazmuddin Moin Khan, Md Iqbal Hossain, Syed Ali Reza, Joy Chakra Bortty, Kazi Sharmin Sultana, Md Shadidur Islam Jawad, Saniah Safat, Md Abdul Ahad, and Maksuda Begum",
        "link": "http://arxiv.org/abs/2506.10842v1",
        "abstract": "The rise of digital payments has accelerated the need for intelligent and\nscalable systems to detect fraud. This research presents an end-to-end,\nfeature-rich machine learning framework for detecting credit card transaction\nanomalies and fraud using real-world data. The study begins by merging\ntransactional, cardholder, merchant, and merchant category datasets from a\nrelational database to create a unified analytical view. Through the feature\nengineering process, we extract behavioural signals such as average spending,\ndeviation from historical patterns, transaction timing irregularities, and\ncategory frequency metrics. These features are enriched with temporal markers\nsuch as hour, day of week, and weekend indicators to expose all latent patterns\nthat indicate fraudulent behaviours. Exploratory data analysis reveals\ncontextual transaction trends across all the dataset features. Using the\ntransactional data, we train and evaluate a range of unsupervised models:\nIsolation Forest, One Class SVM, and a deep autoencoder trained to reconstruct\nnormal behavior. These models flag the top 1% of reconstruction errors as\noutliers. PCA visualizations illustrate each models ability to separate\nanomalies into a two-dimensional latent space. We further segment the\ntransaction landscape using K-Means clustering and DBSCAN to identify dense\nclusters of normal activity and isolate sparse, suspicious regions."
    },
    {
        "date": "2025-06",
        "title": "Efficiency Robustness of Dynamic Deep Learning Systems",
        "author": "Ravishka Rathnasuriya, Tingxi Li, Zexin Xu, Zihe Song, Mirazul Haque, Simin Chen, and Wei Yang",
        "link": "http://arxiv.org/abs/2506.10831v1",
        "abstract": "Deep Learning Systems (DLSs) are increasingly deployed in real-time\napplications, including those in resourceconstrained environments such as\nmobile and IoT devices. To address efficiency challenges, Dynamic Deep Learning\nSystems (DDLSs) adapt inference computation based on input complexity, reducing\noverhead. While this dynamic behavior improves efficiency, such behavior\nintroduces new attack surfaces. In particular, efficiency adversarial attacks\nexploit these dynamic mechanisms to degrade system performance. This paper\nsystematically explores efficiency robustness of DDLSs, presenting the first\ncomprehensive taxonomy of efficiency attacks. We categorize these attacks based\non three dynamic behaviors: (i) attacks on dynamic computations per inference,\n(ii) attacks on dynamic inference iterations, and (iii) attacks on dynamic\noutput production for downstream tasks. Through an in-depth evaluation, we\nanalyze adversarial strategies that target DDLSs efficiency and identify key\nchallenges in securing these systems. In addition, we investigate existing\ndefense mechanisms, demonstrating their limitations against increasingly\npopular efficiency attacks and the necessity for novel mitigation strategies to\nsecure future adaptive DDLSs."
    },
    {
        "date": "2025-06",
        "title": "Quantum Computing and Cybersecurity in Accounting and Finance: Current and the Future Challenges and Opportunities for Securing Accounting and Finance Systems",
        "author": "Huma Habib Shadan, and Sardar Islam",
        "link": "http://arxiv.org/abs/2506.12096v1",
        "abstract": "Quantum computing is revolutionising information systems and will have a\nsignificant impact on accounting and finance, especially in the area of\ncybersecurity. It presents both opportunities and risks in ensuring\nconfidentiality and protecting financial data. The purpose of this thesis is to\nshow the application of quantum technologies in accounting cybersecurity,\nutilising quantum algorithms and QKD to overcome the limitations of classical\ncomputing.\n  The literature review reveals the vulnerabilities of the current accounting\ncybersecurity to quantum attacks and the need for quantum-resistant\ncryptographic mechanisms. It elaborates on the risks associated with\nconventional encryption in the context of quantum capabilities. This study\ncontributes to the understanding of how quantum computing can revolutionise\naccounting cybersecurity by enhancing quantum-resistant algorithms and\nutilising quantum key distribution (QKD) in accounting.\n  The study employs PSALSAR systematic review methodology to ensure rigour and\ndepth. The analysis shows that quantum computing enhances encryption techniques\nto superior possibilities than classical ones. Using quantum technologies in\naccounting minimises data breaches and unauthorised access. The study concludes\nthat quantum-resistant algorithms and quantum key distribution (QKD) are\nnecessary for securing the accounting and finance systems of the future.\n  Keywords Quantum Computing, Cybersecurity, Accounting, Machine Learning,\nArtificial Intelligence, Quantum Key Distribution, Operations Management"
    },
    {
        "date": "2025-06",
        "title": "ME: Trigger Element Combination Backdoor Attack on Copyright Infringement",
        "author": "Feiyu Yang, Siyuan Liang, Aishan Liu, and Dacheng Tao",
        "link": "http://arxiv.org/abs/2506.10776v1",
        "abstract": "The capability of generative diffusion models (DMs) like Stable Diffusion\n(SD) in replicating training data could be taken advantage of by attackers to\nlaunch the Copyright Infringement Attack, with duplicated poisoned image-text\npairs. SilentBadDiffusion (SBD) is a method proposed recently, which shew\noutstanding performance in attacking SD in text-to-image tasks. However, the\nfeasible data resources in this area are still limited, some of them are even\nconstrained or prohibited due to the issues like copyright ownership or\ninappropriate contents; And not all of the images in current datasets are\nsuitable for the proposed attacking methods; Besides, the state-of-the-art\n(SoTA) performance of SBD is far from ideal when few generated poisoning\nsamples could be adopted for attacks. In this paper, we raised new datasets\naccessible for researching in attacks like SBD, and proposed Multi-Element (ME)\nattack method based on SBD by increasing the number of poisonous visual-text\nelements per poisoned sample to enhance the ability of attacking, while\nimporting Discrete Cosine Transform (DCT) for the poisoned samples to maintain\nthe stealthiness. The Copyright Infringement Rate (CIR) / First Attack Epoch\n(FAE) we got on the two new datasets were 16.78% / 39.50 and 51.20% / 23.60,\nrespectively close to or even outperformed benchmark Pokemon and Mijourney\ndatasets. In condition of low subsampling ratio (5%, 6 poisoned samples), MESI\nand DCT earned CIR / FAE of 0.23% / 84.00 and 12.73% / 65.50, both better than\noriginal SBD, which failed to attack at all."
    },
    {
        "date": "2025-06",
        "title": "ObfusBFA: A Holistic Approach to Safeguarding DNNs from Different Types of Bit-Flip Attacks",
        "author": "Xiaobei Yan, Han Qiu, and Tianwei Zhang",
        "link": "http://arxiv.org/abs/2506.10744v1",
        "abstract": "Bit-flip attacks (BFAs) represent a serious threat to Deep Neural Networks\n(DNNs), where flipping a small number of bits in the model parameters or binary\ncode can significantly degrade the model accuracy or mislead the model\nprediction in a desired way. Existing defenses exclusively focus on protecting\nmodels for specific attacks and platforms, while lacking effectiveness for\nother scenarios. We propose ObfusBFA, an efficient and holistic methodology to\nmitigate BFAs targeting both the high-level model weights and low-level\ncodebase (executables or shared libraries). The key idea of ObfusBFA is to\nintroduce random dummy operations during the model inference, which effectively\ntransforms the delicate attacks into random bit flips, making it much harder\nfor attackers to pinpoint and exploit vulnerable bits. We design novel\nalgorithms to identify critical bits and insert obfuscation operations. We\nevaluate ObfusBFA against different types of attacks, including the adaptive\nscenarios where the attacker increases the flip bit budget to attempt to\ncircumvent our defense. The results show that ObfusBFA can consistently\npreserve the model accuracy across various datasets and DNN architectures while\nsignificantly reducing the attack success rates. Additionally, it introduces\nminimal latency and storage overhead, making it a practical solution for\nreal-world applications."
    },
    {
        "date": "2025-06",
        "title": "TED-LaST: Towards Robust Backdoor Defense Against Adaptive Attacks",
        "author": "Xiaoxing Mo, Yuxuan Cheng, Nan Sun, Leo Yu Zhang, Wei Luo, and Shang Gao",
        "link": "http://arxiv.org/abs/2506.10722v1",
        "abstract": "Deep Neural Networks (DNNs) are vulnerable to backdoor attacks, where\nattackers implant hidden triggers during training to maliciously control model\nbehavior. Topological Evolution Dynamics (TED) has recently emerged as a\npowerful tool for detecting backdoor attacks in DNNs. However, TED can be\nvulnerable to backdoor attacks that adaptively distort topological\nrepresentation distributions across network layers. To address this limitation,\nwe propose TED-LaST (Topological Evolution Dynamics against Laundry, Slow\nrelease, and Target mapping attack strategies), a novel defense strategy that\nenhances TED's robustness against adaptive attacks. TED-LaST introduces two key\ninnovations: label-supervised dynamics tracking and adaptive layer emphasis.\nThese enhancements enable the identification of stealthy threats that evade\ntraditional TED-based defenses, even in cases of inseparability in topological\nspace and subtle topological perturbations. We review and classify data\npoisoning tricks in state-of-the-art adaptive attacks and propose enhanced\nadaptive attack with target mapping, which can dynamically shift malicious\ntasks and fully leverage the stealthiness that adaptive attacks possess. Our\ncomprehensive experiments on multiple datasets (CIFAR-10, GTSRB, and\nImageNet100) and model architectures (ResNet20, ResNet101) show that TED-LaST\neffectively counteracts sophisticated backdoors like Adap-Blend, Adapt-Patch,\nand the proposed enhanced adaptive attack. TED-LaST sets a new benchmark for\nrobust backdoor detection, substantially enhancing DNN security against\nevolving threats."
    },
    {
        "date": "2025-06",
        "title": "Unsourced Adversarial CAPTCHA: A Bi-Phase Adversarial CAPTCHA Framework",
        "author": "Xia Du, Xiaoyuan Liu, Jizhe Zhou, Zheng Lin, Chi-man Pun, Cong Wu, Tao Li, Zhe Chen, Wei Ni, and Jun Luo",
        "link": "http://arxiv.org/abs/2506.10685v2",
        "abstract": "With the rapid advancements in deep learning, traditional CAPTCHA schemes are\nincreasingly vulnerable to automated attacks powered by deep neural networks\n(DNNs). Existing adversarial attack methods often rely on original image\ncharacteristics, resulting in distortions that hinder human interpretation and\nlimit applicability in scenarios lacking initial input images. To address these\nchallenges, we propose the Unsourced Adversarial CAPTCHA (UAC), a novel\nframework generating high-fidelity adversarial examples guided by\nattacker-specified text prompts. Leveraging a Large Language Model (LLM), UAC\nenhances CAPTCHA diversity and supports both targeted and untargeted attacks.\nFor targeted attacks, the EDICT method optimizes dual latent variables in a\ndiffusion model for superior image quality. In untargeted attacks, especially\nfor black-box scenarios, we introduce bi-path unsourced adversarial CAPTCHA\n(BP-UAC), a two-step optimization strategy employing multimodal gradients and\nbi-path optimization for efficient misclassification. Experiments show BP-UAC\nachieves high attack success rates across diverse systems, generating natural\nCAPTCHAs indistinguishable to humans and DNNs."
    },
    {
        "date": "2025-06",
        "title": "Contrastive Matrix Completion with Denoising and Augmented Graph Views for Robust Recommendation",
        "author": "Narges Nemati, and Mostafa Haghir Chehreghani",
        "link": "http://arxiv.org/abs/2506.10658v1",
        "abstract": "Matrix completion is a widely adopted framework in recommender systems, as\npredicting the missing entries in the user-item rating matrix enables a\ncomprehensive understanding of user preferences. However, current graph neural\nnetwork (GNN)-based approaches are highly sensitive to noisy or irrelevant\nedges--due to their inherent message-passing mechanisms--and are prone to\noverfitting, which limits their generalizability. To overcome these challenges,\nwe propose a novel method called Matrix Completion using Contrastive Learning\n(MCCL). Our approach begins by extracting local neighborhood subgraphs for each\ninteraction and subsequently generates two distinct graph representations. The\nfirst representation emphasizes denoising by integrating GNN layers with an\nattention mechanism, while the second is obtained via a graph variational\nautoencoder that aligns the feature distribution with a standard prior. A\nmutual learning loss function is employed during training to gradually\nharmonize these representations, enabling the model to capture common patterns\nand significantly enhance its generalizability. Extensive experiments on\nseveral real-world datasets demonstrate that our approach not only improves the\nnumerical accuracy of the predicted scores--achieving up to a 0.8% improvement\nin RMSE--but also produces superior rankings with improvements of up to 36% in\nranking metrics."
    },
    {
        "date": "2025-06",
        "title": "Robust Unsupervised Adaptation of a Speech Recogniser Using Entropy Minimisation and Speaker Codes",
        "author": "Rogier C. van Dalen, Shucong Zhang, Titouan Parcollet, and Sourav Bhattacharya",
        "link": "http://arxiv.org/abs/2506.10653v1",
        "abstract": "Speech recognisers usually perform optimally only in a specific environment\nand need to be adapted to work well in another. For adaptation to a new\nspeaker, there is often too little data for fine-tuning to be robust, and that\ndata is usually unlabelled. This paper proposes a combination of approaches to\nmake adaptation to a single minute of data robust. First, instead of estimating\nthe adaptation parameters with cross-entropy on a single error-prone hypothesis\nor \"pseudo-label\", this paper proposes a novel loss function, the conditional\nentropy over complete hypotheses. Using multiple hypotheses makes adaptation\nmore robust to errors in the initial recognition. Second, a \"speaker code\"\ncharacterises a speaker in a vector short enough that it requires little data\nto estimate. On a far-field noise-augmented version of Common Voice, the\nproposed scheme yields a 20% relative improvement in word error rate on one\nminute of adaptation data, increasing on 10 minutes to 29%."
    },
    {
        "date": "2025-06",
        "title": "CyFence: Securing Cyber-Physical Controllers via Trusted Execution Environment",
        "author": "Stefano Longari, Alessandro Pozone, Jessica Leoni, Mario Polino, Michele Carminati, Mara Tanelli, and Stefano Zanero",
        "link": "http://arxiv.org/abs/2506.10638v1",
        "abstract": "In the last decades, Cyber-physical Systems (CPSs) have experienced a\nsignificant technological evolution and increased connectivity, at the cost of\ngreater exposure to cyber-attacks. Since many CPS are used in safety-critical\nsystems, such attacks entail high risks and potential safety harms. Although\nseveral defense strategies have been proposed, they rarely exploit the\ncyber-physical nature of the system. In this work, we exploit the nature of CPS\nby proposing CyFence, a novel architecture that improves the resilience of\nclosed-loop control systems against cyber-attacks by adding a semantic check,\nused to confirm that the system is behaving as expected. To ensure the security\nof the semantic check code, we use the Trusted Execution Environment\nimplemented by modern processors. We evaluate CyFence considering a real-world\napplication, consisting of an active braking digital controller, demonstrating\nthat it can mitigate different types of attacks with a negligible computation\noverhead."
    },
    {
        "date": "2025-06",
        "title": "Assessing the Resilience of Automotive Intrusion Detection Systems to Adversarial Manipulation",
        "author": "Stefano Longari, Paolo Cerracchio, Michele Carminati, and Stefano Zanero",
        "link": "http://arxiv.org/abs/2506.10620v1",
        "abstract": "The security of modern vehicles has become increasingly important, with the\ncontroller area network (CAN) bus serving as a critical communication backbone\nfor various Electronic Control Units (ECUs). The absence of robust security\nmeasures in CAN, coupled with the increasing connectivity of vehicles, makes\nthem susceptible to cyberattacks. While intrusion detection systems (IDSs) have\nbeen developed to counter such threats, they are not foolproof. Adversarial\nattacks, particularly evasion attacks, can manipulate inputs to bypass\ndetection by IDSs. This paper extends our previous work by investigating the\nfeasibility and impact of gradient-based adversarial attacks performed with\ndifferent degrees of knowledge against automotive IDSs. We consider three\nscenarios: white-box (attacker with full system knowledge), grey-box (partial\nsystem knowledge), and the more realistic black-box (no knowledge of the IDS'\ninternal workings or data). We evaluate the effectiveness of the proposed\nattacks against state-of-the-art IDSs on two publicly available datasets.\nAdditionally, we study effect of the adversarial perturbation on the attack\nimpact and evaluate real-time feasibility by precomputing evasive payloads for\ntimed injection based on bus traffic. Our results demonstrate that, besides\nattacks being challenging due to the automotive domain constraints, their\neffectiveness is strongly dependent on the dataset quality, the target IDS, and\nthe attacker's degree of knowledge."
    },
    {
        "date": "2025-06",
        "title": "Boosting Adversarial Transferability for Hyperspectral Image Classification Using 3D Structure-invariant Transformation and Intermediate Feature Distance",
        "author": "Chun Liu, Bingqian Zhu, Tao Xu, Zheng Zheng, Zheng Li, Wei Yang, Zhigang Han, and Jiayao Wang",
        "link": "http://arxiv.org/abs/2506.10459v1",
        "abstract": "Deep Neural Networks (DNNs) are vulnerable to adversarial attacks, which pose\nsecurity challenges to hyperspectral image (HSI) classification technologies\nbased on DNNs. In the domain of natural images, numerous transfer-based\nadversarial attack methods have been studied. However, HSIs differ from natural\nimages due to their high-dimensional and rich spectral information. Current\nresearch on HSI adversarial examples remains limited and faces challenges in\nfully utilizing the structural and feature information of images. To address\nthese issues, this paper proposes a novel method to enhance the transferability\nof the adversarial examples for HSI classification models. First, while keeping\nthe image structure unchanged, the proposed method randomly divides the image\ninto blocks in both spatial and spectral dimensions. Then, various\ntransformations are applied on a block by block basis to increase input\ndiversity and mitigate overfitting. Second, a feature distancing loss targeting\nintermediate layers is designed, which measures the distance between the\namplified features of the original examples and the features of the adversarial\nexamples as the primary loss, while the output layer prediction serves as the\nauxiliary loss. This guides the perturbation to disrupt the features of the\ntrue class in adversarial examples, effectively enhancing transferability.\nExtensive experiments demonstrate that the adversarial examples generated by\nthe proposed method achieve effective transferability to black-box models on\ntwo public HSI datasets. Furthermore, the method maintains robust attack\nperformance even under defense strategies."
    },
    {
        "date": "2025-06",
        "title": "Towards Robust Multimodal Emotion Recognition under Missing Modalities and Distribution Shifts",
        "author": "Guowei Zhong, Ruohong Huan, Mingzhen Wu, Ronghua Liang, and Peng Chen",
        "link": "http://arxiv.org/abs/2506.10452v1",
        "abstract": "Recent advancements in Multimodal Emotion Recognition (MER) face challenges\nin addressing both modality missing and Out-Of-Distribution (OOD) data\nsimultaneously. Existing methods often rely on specific models or introduce\nexcessive parameters, which limits their practicality. To address these issues,\nwe propose a novel robust MER framework, Causal Inference Distiller (CIDer),\nand introduce a new task, Random Modality Feature Missing (RMFM), to generalize\nthe definition of modality missing. CIDer integrates two key components: a\nModel-Specific Self-Distillation (MSSD) module and a Model-Agnostic Causal\nInference (MACI) module. MSSD enhances robustness under the RMFM task through a\nweight-sharing self-distillation approach applied across low-level features,\nattention maps, and high-level representations. Additionally, a Word-level\nSelf-aligned Attention Module (WSAM) reduces computational complexity, while a\nMultimodal Composite Transformer (MCT) facilitates efficient multimodal fusion.\nTo tackle OOD challenges, MACI employs a tailored causal graph to mitigate\nlabel and language biases using a Multimodal Causal Module (MCM) and\nfine-grained counterfactual texts. Notably, MACI can independently enhance OOD\ngeneralization with minimal additional parameters. Furthermore, we also\nintroduce the new repartitioned MER OOD datasets. Experimental results\ndemonstrate that CIDer achieves robust performance in both RMFM and OOD\nscenarios, with fewer parameters and faster training compared to\nstate-of-the-art methods. The implementation of this work is publicly\naccessible at https://github.com/gw-zhong/CIDer."
    },
    {
        "date": "2025-06",
        "title": "SOFT: Selective Data Obfuscation for Protecting LLM Fine-tuning against Membership Inference Attacks",
        "author": "Kaiyuan Zhang, Siyuan Cheng, Hanxi Guo, Yuetian Chen, Zian Su, Shengwei An, Yuntao Du, Charles Fleming, Ashish Kundu, Xiangyu Zhang, and Ninghui Li",
        "link": "http://arxiv.org/abs/2506.10424v1",
        "abstract": "Large language models (LLMs) have achieved remarkable success and are widely\nadopted for diverse applications. However, fine-tuning these models often\ninvolves private or sensitive information, raising critical privacy concerns.\nIn this work, we conduct the first comprehensive study evaluating the\nvulnerability of fine-tuned LLMs to membership inference attacks (MIAs). Our\nempirical analysis demonstrates that MIAs exploit the loss reduction during\nfine-tuning, making them highly effective in revealing membership information.\nThese findings motivate the development of our defense. We propose SOFT\n(\\textbf{S}elective data \\textbf{O}bfuscation in LLM\n\\textbf{F}ine-\\textbf{T}uning), a novel defense technique that mitigates\nprivacy leakage by leveraging influential data selection with an adjustable\nparameter to balance utility preservation and privacy protection. Our extensive\nexperiments span six diverse domains and multiple LLM architectures and scales.\nResults show that SOFT effectively reduces privacy risks while maintaining\ncompetitive model performance, offering a practical and scalable solution to\nsafeguard sensitive information in fine-tuned LLMs."
    },
    {
        "date": "2025-06",
        "title": "Collapsing Sequence-Level Data-Policy Coverage via Poisoning Attack in Offline Reinforcement Learning",
        "author": "Xue Zhou, Dapeng Man, Chen Xu, Fanyi Zeng, Tao Liu, Huan Wang, Shucheng He, Chaoyang Gao, and Wu Yang",
        "link": "http://arxiv.org/abs/2506.11172v1",
        "abstract": "Offline reinforcement learning (RL) heavily relies on the coverage of\npre-collected data over the target policy's distribution. Existing studies aim\nto improve data-policy coverage to mitigate distributional shifts, but overlook\nsecurity risks from insufficient coverage, and the single-step analysis is not\nconsistent with the multi-step decision-making nature of offline RL. To address\nthis, we introduce the sequence-level concentrability coefficient to quantify\ncoverage, and reveal its exponential amplification on the upper bound of\nestimation errors through theoretical analysis. Building on this, we propose\nthe Collapsing Sequence-Level Data-Policy Coverage (CSDPC) poisoning attack.\nConsidering the continuous nature of offline RL data, we convert state-action\npairs into decision units, and extract representative decision patterns that\ncapture multi-step behavior. We identify rare patterns likely to cause\ninsufficient coverage, and poison them to reduce coverage and exacerbate\ndistributional shifts. Experiments show that poisoning just 1% of the dataset\ncan degrade agent performance by 90%. This finding provides new perspectives\nfor analyzing and safeguarding the security of offline RL."
    },
    {
        "date": "2025-06",
        "title": "History-Aware Neural Operator: Robust Data-Driven Constitutive Modeling of Path-Dependent Materials",
        "author": "Binyao Guo, Zihan Lin, and QiZhi He",
        "link": "http://arxiv.org/abs/2506.10352v1",
        "abstract": "This study presents an end-to-end learning framework for data-driven modeling\nof path-dependent inelastic materials using neural operators. The framework is\nbuilt on the premise that irreversible evolution of material responses,\ngoverned by hidden dynamics, can be inferred from observable data.\n  We develop the History-Aware Neural Operator (HANO), an autoregressive model\nthat predicts path-dependent material responses from short segments of recent\nstrain-stress history without relying on hidden state variables, thereby\novercoming self-consistency issues commonly encountered in recurrent neural\nnetwork (RNN)-based models. Built on a Fourier-based neural operator backbone,\nHANO enables discretization-invariant learning. To enhance its ability to\ncapture both global loading patterns and critical local path dependencies, we\nembed a hierarchical self-attention mechanism that facilitates multiscale\nfeature extraction.\n  Beyond ensuring self-consistency, HANO mitigates sensitivity to initial\nhidden states, a commonly overlooked issue that can lead to instability in\nrecurrent models when applied to generalized loading paths. By modeling\nstress-strain evolution as a continuous operator rather than relying on fixed\ninput-output mappings, HANO naturally accommodates varying path discretizations\nand exhibits robust performance under complex conditions, including irregular\nsampling, multi-cycle loading, noisy data, and pre-stressed states. We evaluate\nHANO on two benchmark problems: elastoplasticity with hardening and progressive\nanisotropic damage in brittle solids. Results show that HANO consistently\noutperforms baseline models in predictive accuracy, generalization, and\nrobustness. With its demonstrated capabilities, HANO provides an effective\ndata-driven surrogate for simulating inelastic materials and is well-suited for\nintegration with classical numerical solvers."
    },
    {
        "date": "2025-06",
        "title": "Adaptive Chosen-Ciphertext Security of Distributed Broadcast Encryption",
        "author": "Kwangsu Lee",
        "link": "http://arxiv.org/abs/2506.10338v1",
        "abstract": "Distributed broadcast encryption (DBE) is a specific kind of broadcast\nencryption (BE) where users independently generate their own public and private\nkeys, and a sender can efficiently create a ciphertext for a subset of users by\nusing the public keys of the subset users. Previously proposed DBE schemes have\nbeen proven in the adaptive chosen-plaintext attack (CPA) security model and\nhave the disadvantage of requiring linear number of pairing operations when\nverifying the public key of a user. In this paper, we propose an efficient DBE\nscheme in bilinear groups and prove adaptive chosen-ciphertext attack (CCA)\nsecurity for the first time. To do this, we first propose a semi-static CCA\nsecure DBE scheme and prove the security under the $q$-Type assumption. Then,\nby modifying the generic transformation of Gentry and Waters that converts a\nsemi-static CPA secure DBE scheme into an adaptive CPA secure DBE scheme to be\napplied to CCA secure DBE schemes, we propose an adaptive CCA secure DBE scheme\nand prove its adaptive CCA security. Our proposed DBE scheme is efficient\nbecause it requires constant size ciphertexts, constant size private keys, and\nlinear size public keys, and the public key verification requires only a\nconstant number of pairing operations and efficient group membership checks."
    },
    {
        "date": "2025-06",
        "title": "Distributionally-Constrained Adversaries in Online Learning",
        "author": "Mo\u00efse Blanchard, and Samory Kpotufe",
        "link": "http://arxiv.org/abs/2506.10293v2",
        "abstract": "There has been much recent interest in understanding the continuum from\nadversarial to stochastic settings in online learning, with various frameworks\nincluding smoothed settings proposed to bridge this gap. We consider the more\ngeneral and flexible framework of distributionally constrained adversaries in\nwhich instances are drawn from distributions chosen by an adversary within some\nconstrained distribution class [RST11]. Compared to smoothed analysis, we\nconsider general distributional classes which allows for a fine-grained\nunderstanding of learning settings between fully stochastic and fully\nadversarial for which a learner can achieve non-trivial regret. We give a\ncharacterization for which distribution classes are learnable in this context\nagainst both oblivious and adaptive adversaries, providing insights into the\ntypes of interplay between the function class and distributional constraints on\nadversaries that enable learnability. In particular, our results recover and\ngeneralize learnability for known smoothed settings. Further, we show that for\nseveral natural function classes including linear classifiers, learning can be\nachieved without any prior knowledge of the distribution class -- in other\nwords, a learner can simultaneously compete against any constrained adversary\nwithin learnable distribution classes."
    },
    {
        "date": "2025-06",
        "title": "VQC-MLPNet: An Unconventional Hybrid Quantum-Classical Architecture for Scalable and Robust Quantum Machine Learning",
        "author": "Jun Qi, Chao-Han Yang, Pin-Yu Chen, and Min-Hsiu Hsieh",
        "link": "http://arxiv.org/abs/2506.10275v1",
        "abstract": "Variational Quantum Circuits (VQCs) offer a novel pathway for quantum machine\nlearning, yet their practical application is hindered by inherent limitations\nsuch as constrained linear expressivity, optimization challenges, and acute\nsensitivity to quantum hardware noise. This work introduces VQC-MLPNet, a\nscalable and robust hybrid quantum-classical architecture designed to overcome\nthese obstacles. By innovatively employing quantum circuits to dynamically\ngenerate parameters for classical Multi-Layer Perceptrons (MLPs) via amplitude\nencoding and parameterized quantum operations, VQC-MLPNet substantially expands\nrepresentation capabilities and augments training stability. We provide\nrigorous theoretical guarantees via statistical learning techniques and Neural\nTangent Kernel analysis, explicitly deriving upper bounds on approximation,\nuniform deviation, and optimization errors. These theoretical insights\ndemonstrate exponential improvements in representation capacity relative to\nquantum circuit depth and the number of qubits, providing clear computational\nadvantages over standalone quantum circuits and existing hybrid quantum\narchitectures. Our theoretical claims are empirically corroborated through\nextensive experiments, including classifying semiconductor quantum-dot charge\nstates and predicting genomic transcription factor binding sites, demonstrating\nresilient performance even under realistic IBM quantum noise simulations. This\nresearch establishes a theoretically sound and practically robust framework,\nadvancing the frontiers of quantum-enhanced learning for unconventional\ncomputing paradigms in the Noisy Intermediate-Scale Quantum era and beyond."
    },
    {
        "date": "2025-06",
        "title": "Prompt Attacks Reveal Superficial Knowledge Removal in Unlearning Methods",
        "author": "Yeonwoo Jang, Shariqah Hossain, Ashwin Sreevatsa, and Diogo Cruz",
        "link": "http://arxiv.org/abs/2506.10236v1",
        "abstract": "In this work, we show that some machine unlearning methods may fail when\nsubjected to straightforward prompt attacks. We systematically evaluate eight\nunlearning techniques across three model families, and employ output-based,\nlogit-based, and probe analysis to determine to what extent supposedly\nunlearned knowledge can be retrieved. While methods like RMU and TAR\ndemonstrate robust unlearning, ELM remains vulnerable to specific prompt\nattacks (e.g., Hindi filler text in original prompt recovering 57.3% accuracy).\nOur logit analysis also confirms that unlearned models are generally not hiding\nknowledge by modifying the way the answer is formatted, as the correlation\nbetween output and logit accuracy is strong. These results challenge prevailing\nassumptions about unlearning effectiveness and highlight the need for\nevaluation frameworks that can reliably distinguish between true knowledge\nremoval and superficial output suppression. We also publicly make available our\nevaluation framework to easily evaluate prompting techniques to retrieve\nunlearning knowledge."
    },
    {
        "date": "2025-06",
        "title": "DynaSubVAE: Adaptive Subgrouping for Scalable and Robust OOD Detection",
        "author": "Tina Behrouzi, Sana Tonekaboni, Rahul G. Krishnan, and Anna Goldenberg",
        "link": "http://arxiv.org/abs/2506.10200v1",
        "abstract": "Real-world observational data often contain existing or emerging\nheterogeneous subpopulations that deviate from global patterns. The majority of\nmodels tend to overlook these underrepresented groups, leading to inaccurate or\neven harmful predictions. Existing solutions often rely on detecting these\nsamples as Out-of-domain (OOD) rather than adapting the model to new emerging\npatterns. We introduce DynaSubVAE, a Dynamic Subgrouping Variational\nAutoencoder framework that jointly performs representation learning and\nadaptive OOD detection. Unlike conventional approaches, DynaSubVAE evolves with\nthe data by dynamically updating its latent structure to capture new trends. It\nleverages a novel non-parametric clustering mechanism, inspired by Gaussian\nMixture Models, to discover and model latent subgroups based on embedding\nsimilarity. Extensive experiments show that DynaSubVAE achieves competitive\nperformance in both near-OOD and far-OOD detection, and excels in class-OOD\nscenarios where an entire class is missing during training. We further\nillustrate that our dynamic subgrouping mechanism outperforms standalone\nclustering methods such as GMM and KMeans++ in terms of both OOD accuracy and\nregret precision."
    },
    {
        "date": "2025-06",
        "title": "Unconditionally Secure Wireless-Wired Ground-Satellite-Ground Communication Networks Utilizing Classical and Quantum Noise",
        "author": "Lucas Truax, Sandip Roy, and Laszlo B. Kish",
        "link": "http://arxiv.org/abs/2506.10147v1",
        "abstract": "In this paper, we introduce the Kirchhoff-Law-Johnson-Noise (KLJN) as an\napproach to securing satellite communications. KLJN has the potential to\nrevolutionize satellite communication security through its combination of\nsimplicity, cost-effectiveness, and resilience with unconditional security.\nUnlike quantum key distribution (QKD), which requires complex, fragile, and\nexpensive infrastructure like photon detectors and dedicated optical links,\nKLJN operates using standard electronic components and wires, significantly\nreducing implementation costs and logistical hurdles. KLJN's security, grounded\nin the fundamental laws of classical physics, is impervious to environmental\nand radiation-induced noise, making it highly reliable in the harsh conditions\nof satellite communications. This robustness, coupled with its ability to\nintegrate seamlessly with existing infrastructure, positions KLJN as a\nrevolutionary alternative to quantum solutions for ensuring secure, resilient\nsatellite communications. The authors explore the value of achieving\nunconditionally secure communications in strategic ground-to-satellite networks\nwhich address vulnerabilities posed by advanced computational threats,\nincluding quantum computing. Our team has examined two leading approaches to\nunconditional security - the KLJN scheme and QKD - and analyzed the potential\nuse of each for space systems. While QKD leverages quantum mechanics for\nsecurity, it faces challenges related to cost, complexity, and environmental\nsensitivity. In contrast, the KLJN scheme utilizes classical physics principles\nto provide a simpler, more cost-effective, and resilient alternative,\nparticularly for ground-based systems. The study concludes that KLJN offers\nsignificant advantages in simplicity, cost-efficiency, and robustness, making\nit a practical choice for many secure communication applications."
    },
    {
        "date": "2025-06",
        "title": "RoCA: Robust Cross-Domain End-to-End Autonomous Driving",
        "author": "Rajeev Yasarla, Shizhong Han, Hsin-Pai Cheng, Litian Liu, Shweta Mahajan, Apratim Bhattacharyya, Yunxiao Shi, Risheek Garrepalli, Hong Cai, and Fatih Porikli",
        "link": "http://arxiv.org/abs/2506.10145v2",
        "abstract": "End-to-end (E2E) autonomous driving has recently emerged as a new paradigm,\noffering significant potential. However, few studies have looked into the\npractical challenge of deployment across domains (e.g., cities). Although\nseveral works have incorporated Large Language Models (LLMs) to leverage their\nopen-world knowledge, LLMs do not guarantee cross-domain driving performance\nand may incur prohibitive retraining costs during domain adaptation. In this\npaper, we propose RoCA, a novel framework for robust cross-domain E2E\nautonomous driving. RoCA formulates the joint probabilistic distribution over\nthe tokens that encode ego and surrounding vehicle information in the E2E\npipeline. Instantiating with a Gaussian process (GP), RoCA learns a set of\nbasis tokens with corresponding trajectories, which span diverse driving\nscenarios. Then, given any driving scene, it is able to probabilistically infer\nthe future trajectory. By using RoCA together with a base E2E model in\nsource-domain training, we improve the generalizability of the base model,\nwithout requiring extra inference computation. In addition, RoCA enables robust\nadaptation on new target domains, significantly outperforming direct\nfinetuning. We extensively evaluate RoCA on various cross-domain scenarios and\nshow that it achieves strong domain generalization and adaptation performance."
    },
    {
        "date": "2025-06",
        "title": "Kvasir-VQA-x1: A Multimodal Dataset for Medical Reasoning and Robust MedVQA in Gastrointestinal Endoscopy",
        "author": "Sushant Gautam, Michael A. Riegler, and P\u00e5l Halvorsen",
        "link": "http://arxiv.org/abs/2506.09958v1",
        "abstract": "Medical Visual Question Answering (MedVQA) is a promising field for\ndeveloping clinical decision support systems, yet progress is often limited by\nthe available datasets, which can lack clinical complexity and visual\ndiversity. To address these gaps, we introduce Kvasir-VQA-x1, a new,\nlarge-scale dataset for gastrointestinal (GI) endoscopy. Our work significantly\nexpands upon the original Kvasir-VQA by incorporating 159,549 new\nquestion-answer pairs that are designed to test deeper clinical reasoning. We\ndeveloped a systematic method using large language models to generate these\nquestions, which are stratified by complexity to better assess a model's\ninference capabilities. To ensure our dataset prepares models for real-world\nclinical scenarios, we have also introduced a variety of visual augmentations\nthat mimic common imaging artifacts. The dataset is structured to support two\nmain evaluation tracks: one for standard VQA performance and another to test\nmodel robustness against these visual perturbations. By providing a more\nchallenging and clinically relevant benchmark, Kvasir-VQA-x1 aims to accelerate\nthe development of more reliable and effective multimodal AI systems for use in\nclinical settings. The dataset is fully accessible and adheres to FAIR data\nprinciples, making it a valuable resource for the wider research community.\nCode and data: https://github.com/Simula/Kvasir-VQA-x1 and\nhttps://huggingface.co/datasets/SimulaMet/Kvasir-VQA-x1"
    },
    {
        "date": "2025-06",
        "title": "Apollo: A Posteriori Label-Only Membership Inference Attack Towards Machine Unlearning",
        "author": "Liou Tang, James Joshi, and Ashish Kundu",
        "link": "http://arxiv.org/abs/2506.09923v1",
        "abstract": "Machine Unlearning (MU) aims to update Machine Learning (ML) models following\nrequests to remove training samples and their influences on a trained model\nefficiently without retraining the original ML model from scratch. While MU\nitself has been employed to provide privacy protection and regulatory\ncompliance, it can also increase the attack surface of the model. Existing\nprivacy inference attacks towards MU that aim to infer properties of the\nunlearned set rely on the weaker threat model that assumes the attacker has\naccess to both the unlearned model and the original model, limiting their\nfeasibility toward real-life scenarios. We propose a novel privacy attack, A\nPosteriori Label-Only Membership Inference Attack towards MU, Apollo, that\ninfers whether a data sample has been unlearned, following a strict threat\nmodel where an adversary has access to the label-output of the unlearned model\nonly. We demonstrate that our proposed attack, while requiring less access to\nthe target model compared to previous attacks, can achieve relatively high\nprecision on the membership status of the unlearned samples."
    },
    {
        "date": "2025-06",
        "title": "A look at adversarial attacks on radio waveforms from discrete latent space",
        "author": "Attanasia Garuso, Silvija Kokalj-Filipovic, and Yagna Kaasaragadda",
        "link": "http://arxiv.org/abs/2506.09896v1",
        "abstract": "Having designed a VQVAE that maps digital radio waveforms into discrete\nlatent space, and yields a perfectly classifiable reconstruction of the\noriginal data, we here analyze the attack suppressing properties of VQVAE when\nan adversarial attack is performed on high-SNR radio-frequency (RF)\ndata-points. To target amplitude modulations from a subset of digitally\nmodulated waveform classes, we first create adversarial attacks that preserve\nthe phase between the in-phase and quadrature component whose values are\nadversarially changed. We compare them with adversarial attacks of the same\nintensity where phase is not preserved. We test the classification accuracy of\nsuch adversarial examples on a classifier trained to deliver 100% accuracy on\nthe original data. To assess the ability of VQVAE to suppress the strength of\nthe attack, we evaluate the classifier accuracy on the reconstructions by VQVAE\nof the adversarial datapoints and show that VQVAE substantially decreases the\neffectiveness of the attack. We also compare the I/Q plane diagram of the\nattacked data, their reconstructions and the original data. Finally, using\nmultiple methods and metrics, we compare the probability distribution of the\nVQVAE latent space with and without attack. Varying the attack strength, we\nobserve interesting properties of the discrete space, which may help detect the\nattacks."
    },
    {
        "date": "2025-06",
        "title": "A Weighted Loss Approach to Robust Federated Learning under Data Heterogeneity",
        "author": "Johan Erbani, Sonia Ben Mokhtar, Pierre-Edouard Portier, Elod Egyed-Zsigmond, and Diana Nurbakova",
        "link": "http://arxiv.org/abs/2506.09824v2",
        "abstract": "Federated learning (FL) is a machine learning paradigm that enables multiple\ndata holders to collaboratively train a machine learning model without sharing\ntheir training data with external parties. In this paradigm, workers locally\nupdate a model and share with a central server their updated gradients (or\nmodel parameters). While FL seems appealing from a privacy perspective, it\nopens a number of threats from a security perspective as (Byzantine)\nparticipants can contribute poisonous gradients (or model parameters) harming\nmodel convergence. Byzantine-resilient FL addresses this issue by ensuring that\nthe training proceeds as if Byzantine participants were absent. Towards this\npurpose, common strategies ignore outlier gradients during model aggregation,\nassuming that Byzantine gradients deviate more from honest gradients than\nhonest gradients do from each other. However, in heterogeneous settings, honest\ngradients may differ significantly, making it difficult to distinguish honest\noutliers from Byzantine ones. In this paper, we introduce the Worker Label\nAlignement Loss (WoLA), a weighted loss that aligns honest worker gradients\ndespite data heterogeneity, which facilitates the identification of Byzantines'\ngradients. This approach significantly outperforms state-of-the-art methods in\nheterogeneous settings. In this paper, we provide both theoretical insights and\nempirical evidence of its effectiveness."
    },
    {
        "date": "2025-06",
        "title": "Physical Layer-Based Device Fingerprinting for Wireless Security: From Theory to Practice",
        "author": "Junqing Zhang, Francesco Ardizzon, Mattia Piana, Guanxiong Shen, and Stefano Tomasin",
        "link": "http://arxiv.org/abs/2506.09807v1",
        "abstract": "The identification of the devices from which a message is received is part of\nsecurity mechanisms to ensure authentication in wireless communications.\nConventional authentication approaches are cryptography-based, which, however,\nare usually computationally expensive and not adequate in the Internet of\nThings (IoT), where devices tend to be low-cost and with limited resources.\nThis paper provides a comprehensive survey of physical layer-based device\nfingerprinting, which is an emerging device authentication for wireless\nsecurity. In particular, this article focuses on hardware impairment-based\nidentity authentication and channel features-based authentication. They are\npassive techniques that are readily applicable to legacy IoT devices. Their\nintrinsic hardware and channel features, algorithm design methodologies,\napplication scenarios, and key research questions are extensively reviewed\nhere. The remaining research challenges are discussed, and future work is\nsuggested that can further enhance the physical layer-based device\nfingerprinting."
    },
    {
        "date": "2025-06",
        "title": "Devil's Hand: Data Poisoning Attacks to Locally Private Graph Learning Protocols",
        "author": "Longzhu He, Chaozhuo Li, Peng Tang, Litian Zhang, and Sen Su",
        "link": "http://arxiv.org/abs/2506.09803v1",
        "abstract": "Graph neural networks (GNNs) have achieved significant success in graph\nrepresentation learning and have been applied to various domains. However, many\nreal-world graphs contain sensitive personal information, such as user profiles\nin social networks, raising serious privacy concerns when graph learning is\nperformed using GNNs. To address this issue, locally private graph learning\nprotocols have gained considerable attention. These protocols leverage the\nprivacy advantages of local differential privacy (LDP) and the effectiveness of\nGNN's message-passing in calibrating noisy data, offering strict privacy\nguarantees for users' local data while maintaining high utility (e.g., node\nclassification accuracy) for graph learning. Despite these advantages, such\nprotocols may be vulnerable to data poisoning attacks, a threat that has not\nbeen considered in previous research. Identifying and addressing these threats\nis crucial for ensuring the robustness and security of privacy-preserving graph\nlearning frameworks. This work introduces the first data poisoning attack\ntargeting locally private graph learning protocols. The attacker injects fake\nusers into the protocol, manipulates these fake users to establish links with\ngenuine users, and sends carefully crafted data to the server, ultimately\ncompromising the utility of private graph learning. The effectiveness of the\nattack is demonstrated both theoretically and empirically. In addition, several\ndefense strategies have also been explored, but their limited effectiveness\nhighlights the need for more robust defenses."
    },
    {
        "date": "2025-06",
        "title": "Empirical and computer-aided robustness analysis of long-step and accelerated methods in smooth convex optimization",
        "author": "Pierre Vernimmen, and Fran\u00e7ois Glineur",
        "link": "http://arxiv.org/abs/2506.09730v2",
        "abstract": "This work assesses both empirically and theoretically, using the performance\nestimation methodology, how robust different first-order optimization methods\nare when subject to relative inexactness in their gradient computations.\nRelative inexactness occurs, for example, when compressing the gradient using\nfewer bits of information, which happens when dealing with large-scale problems\non GPUs. Three major families of methods are analyzed: constant step gradient\ndescent, long-step methods, and accelerated methods. The latter two are first\nshown to be theoretically not robust to inexactness. Then, a semi-heuristic\nshortening factor is introduced to improve their theoretical guarantees. All\nmethods are subsequently tested on a concrete inexact problem, with two\ndifferent types of relative inexactness, and it is observed that both\naccelerated methods are much more robust than expected, and that the shortening\nfactor significantly helps the long-step methods. In the end, all shortened\nmethods appear to be promising, even in this inexact setting."
    },
    {
        "date": "2025-06",
        "title": "On the Virtues of Information Security in the UK Climate Movement",
        "author": "Mikaela Brough, Rikke Bjerg Jensen, and Martin R. Albrecht",
        "link": "http://arxiv.org/abs/2506.09719v1",
        "abstract": "We report on an ethnographic study with members of the climate movement in\nthe United Kingdom (UK). We conducted participant observation and interviews at\nprotests and in various activist settings. Reporting on the findings as they\nrelate to information security, we show that members of the UK climate movement\nwrestled with (i) a fundamental tension between openness and secrecy; (ii)\ntensions between autonomy and collective interdependence in\ninformation-security decision-making; (iii) conflicting activist ideals that\nshape security discourses; and (iv) pressures from different social gazes --\nfrom each other, from people outside the movement and from their adversaries.\nOverall, our findings shed light on the social complexities of\ninformation-security research in activist settings and provoke methodological\nquestions about programmes that aim to design for activists."
    },
    {
        "date": "2025-06",
        "title": "Evasion Attacks Against Bayesian Predictive Models",
        "author": "Pablo G. Arce, Roi Naveiro, and David R\u00edos Insua",
        "link": "http://arxiv.org/abs/2506.09640v1",
        "abstract": "There is an increasing interest in analyzing the behavior of machine learning\nsystems against adversarial attacks. However, most of the research in\nadversarial machine learning has focused on studying weaknesses against evasion\nor poisoning attacks to predictive models in classical setups, with the\nsusceptibility of Bayesian predictive models to attacks remaining\nunderexplored. This paper introduces a general methodology for designing\noptimal evasion attacks against such models. We investigate two adversarial\nobjectives: perturbing specific point predictions and altering the entire\nposterior predictive distribution. For both scenarios, we propose novel\ngradient-based attacks and study their implementation and properties in various\ncomputational setups."
    },
    {
        "date": "2025-06",
        "title": "The Everyday Security of Living with Conflict",
        "author": "Jessica McClearn, Reem Talhouk, and Rikke Bjerg Jensen",
        "link": "http://arxiv.org/abs/2506.09580v1",
        "abstract": "When `cyber' is used as a prefix, attention is typically drawn to the\ntechnological and spectacular aspects of war and conflict -- and, by extension,\nsecurity. We offer a different approach to engaging with and understanding\nsecurity in such contexts, by foregrounding the everyday -- mundane --\nexperiences of security within communities living with and fleeing from war. We\ndo so through three vignettes from our field research in Colombia, Lebanon and\nSweden, respectively, and by highlighting the significance of ethnography for\nsecurity research with communities living in regions afflicted by war. We\nconclude by setting out a call to action for security researchers and\npractitioners to consider such lived experiences in the design of security\ntechnology that aims to cater to the needs of communities in `global conflict\nand disaster regions'."
    },
    {
        "date": "2025-06",
        "title": "TooBadRL: Trigger Optimization to Boost Effectiveness of Backdoor Attacks on Deep Reinforcement Learning",
        "author": "Songze Li, Mingxuan Zhang, Kang Wei, and Shouling Ji",
        "link": "http://arxiv.org/abs/2506.09562v2",
        "abstract": "Deep reinforcement learning (DRL) has achieved remarkable success in a wide\nrange of sequential decision-making domains, including robotics, healthcare,\nsmart grids, and finance. Recent research demonstrates that attackers can\nefficiently exploit system vulnerabilities during the training phase to execute\nbackdoor attacks, producing malicious actions when specific trigger patterns\nare present in the state observations. However, most existing backdoor attacks\nrely primarily on simplistic and heuristic trigger configurations, overlooking\nthe potential efficacy of trigger optimization. To address this gap, we\nintroduce TooBadRL (Trigger Optimization to Boost Effectiveness of Backdoor\nAttacks on DRL), the first framework to systematically optimize DRL backdoor\ntriggers along three critical axes, i.e., temporal, spatial, and magnitude.\nSpecifically, we first introduce a performance-aware adaptive freezing\nmechanism for injection timing. Then, we formulate dimension selection as a\ncooperative game, utilizing Shapley value analysis to identify the most\ninfluential state variable for the injection dimension. Furthermore, we propose\na gradient-based adversarial procedure to optimize the injection magnitude\nunder environment constraints. Evaluations on three mainstream DRL algorithms\nand nine benchmark tasks show that TooBadRL significantly improves attack\nsuccess rates, while ensuring minimal degradation of normal task performance.\nThese results highlight the previously underappreciated importance of\nprincipled trigger optimization in DRL backdoor attacks. The source code of\nTooBadRL can be found at https://github.com/S3IC-Lab/TooBadRL."
    },
    {
        "date": "2025-06",
        "title": "AngleRoCL: Angle-Robust Concept Learning for Physically View-Invariant T2I Adversarial Patches",
        "author": "Wenjun Ji, Yuxiang Fu, Luyang Ying, Deng-Ping Fan, Yuyi Wang, Ming-Ming Cheng, Ivor Tsang, and Qing Guo",
        "link": "http://arxiv.org/abs/2506.09538v1",
        "abstract": "Cutting-edge works have demonstrated that text-to-image (T2I) diffusion\nmodels can generate adversarial patches that mislead state-of-the-art object\ndetectors in the physical world, revealing detectors' vulnerabilities and\nrisks. However, these methods neglect the T2I patches' attack effectiveness\nwhen observed from different views in the physical world (i.e., angle\nrobustness of the T2I adversarial patches). In this paper, we study the angle\nrobustness of T2I adversarial patches comprehensively, revealing their\nangle-robust issues, demonstrating that texts affect the angle robustness of\ngenerated patches significantly, and task-specific linguistic instructions fail\nto enhance the angle robustness. Motivated by the studies, we introduce\nAngle-Robust Concept Learning (AngleRoCL), a simple and flexible approach that\nlearns a generalizable concept (i.e., text embeddings in implementation)\nrepresenting the capability of generating angle-robust patches. The learned\nconcept can be incorporated into textual prompts and guides T2I models to\ngenerate patches with their attack effectiveness inherently resistant to\nviewpoint variations. Through extensive simulation and physical-world\nexperiments on five SOTA detectors across multiple views, we demonstrate that\nAngleRoCL significantly enhances the angle robustness of T2I adversarial\npatches compared to baseline methods. Our patches maintain high attack success\nrates even under challenging viewing conditions, with over 50% average relative\nimprovement in attack effectiveness across multiple angles. This research\nadvances the understanding of physically angle-robust patches and provides\ninsights into the relationship between textual concepts and physical properties\nin T2I-generated contents."
    },
    {
        "date": "2025-06",
        "title": "The Security Overview and Analysis of 3GPP 5G MAC CE",
        "author": "Jin Cao, Yuanyuan Yang, Ruhui Ma, Sheng Li, and Hui Li",
        "link": "http://arxiv.org/abs/2506.09502v2",
        "abstract": "To more effectively control and allocate network resources, MAC CE has been\nintroduced into the network protocol, which is a type of control signaling\nlocated in the MAC layer. Since MAC CE lacks encryption and integrity\nprotection mechanisms provided by PDCP, the control signaling carried by MAC CE\nis vulnerable to interception or tampering by attackers during resource\nscheduling and allocation. Currently, the 3GPP has analyzed the security risks\nof Layer 1/Layer 2 Triggered Mobility (LTM), where handover signaling sent to\nthe UE via MAC CE by the network can lead to privacy leaks and network attacks.\nHowever, in addition to LTM, there may be other potential security\nvulnerabilities in other protocol procedures. Therefore, this paper explores\nthe security threats to MAC CE and the corresponding protection mechanisms. The\nresearch is expected to support the 3GPP's study of MAC CE and be integrated\nwith the security research of lower-layer protocols, thereby enhancing the\nsecurity and reliability of the entire communication system."
    },
    {
        "date": "2025-06",
        "title": "LLMs Cannot Reliably Judge (Yet?): A Comprehensive Assessment on the Robustness of LLM-as-a-Judge",
        "author": "Songze Li, Chuokun Xu, Jiaying Wang, Xueluan Gong, Chen Chen, Jirui Zhang, Jun Wang, Kwok-Yan Lam, and Shouling Ji",
        "link": "http://arxiv.org/abs/2506.09443v1",
        "abstract": "Large Language Models (LLMs) have demonstrated remarkable intelligence across\nvarious tasks, which has inspired the development and widespread adoption of\nLLM-as-a-Judge systems for automated model testing, such as red teaming and\nbenchmarking. However, these systems are susceptible to adversarial attacks\nthat can manipulate evaluation outcomes, raising concerns about their\nrobustness and, consequently, their trustworthiness. Existing evaluation\nmethods adopted by LLM-based judges are often piecemeal and lack a unified\nframework for comprehensive assessment. Furthermore, prompt template and model\nselections for improving judge robustness have been rarely explored, and their\nperformance in real-world settings remains largely unverified. To address these\ngaps, we introduce RobustJudge, a fully automated and scalable framework\ndesigned to systematically evaluate the robustness of LLM-as-a-Judge systems.\nRobustJudge investigates the impact of attack methods and defense strategies\n(RQ1), explores the influence of prompt template and model selection (RQ2), and\nassesses the robustness of real-world LLM-as-a-Judge applications (RQ3).Our\nmain findings are: (1) LLM-as-a-Judge systems are still vulnerable to a range\nof adversarial attacks, including Combined Attack and PAIR, while defense\nmechanisms such as Re-tokenization and LLM-based Detectors offer improved\nprotection; (2) Robustness is highly sensitive to the choice of prompt template\nand judge models. Our proposed prompt template optimization method can improve\nrobustness, and JudgeLM-13B demonstrates strong performance as a robust\nopen-source judge; (3) Applying RobustJudge to Alibaba's PAI platform reveals\npreviously unreported vulnerabilities. The source code of RobustJudge is\nprovided at https://github.com/S3IC-Lab/RobustJudge."
    },
    {
        "date": "2025-06",
        "title": "Generalization Error Analysis for Attack-Free and Byzantine-Resilient Decentralized Learning with Data Heterogeneity",
        "author": "Haoxiang Ye, Tao Sun, and Qing Ling",
        "link": "http://arxiv.org/abs/2506.09438v1",
        "abstract": "Decentralized learning, which facilitates joint model training across\ngeographically scattered agents, has gained significant attention in the field\nof signal and information processing in recent years. While the optimization\nerrors of decentralized learning algorithms have been extensively studied,\ntheir generalization errors remain relatively under-explored. As the\ngeneralization errors reflect the scalability of trained models on unseen data\nand are crucial in determining the performance of trained models in real-world\napplications, understanding the generalization errors of decentralized learning\nis of paramount importance. In this paper, we present fine-grained\ngeneralization error analysis for both attack-free and Byzantine-resilient\ndecentralized learning with heterogeneous data as well as under mild\nassumptions, in contrast to prior studies that consider homogeneous data and/or\nrely on a stringent bounded stochastic gradient assumption. Our results shed\nlight on the impact of data heterogeneity, model initialization and stochastic\ngradient noise -- factors that have not been closely investigated before -- on\nthe generalization error of decentralized learning. We also reveal that\nByzantine attacks performed by malicious agents largely affect the\ngeneralization error, and their negative impact is inherently linked to the\ndata heterogeneity while remaining independent on the sample size. Numerical\nexperiments on both convex and non-convex tasks are conducted to validate our\ntheoretical findings."
    },
    {
        "date": "2025-06",
        "title": "Securing Open RAN: A Survey of Cryptographic Challenges and Emerging Solutions for 5G",
        "author": "Ryan Barker, and Fatemeh Afghah",
        "link": "http://arxiv.org/abs/2506.09418v1",
        "abstract": "The advent of Open Radio Access Networks (O-RAN) introduces modularity and\nflexibility into 5G deployments but also surfaces novel security challenges\nacross disaggregated interfaces. This literature review synthesizes recent\nresearch across thirteen academic and industry sources, examining\nvulnerabilities such as cipher bidding-down attacks, partial encryption\nexposure on control/user planes, and performance trade-offs in securing O-RAN\ninterfaces like E2 and O1. The paper surveys key cryptographic tools -- SNOW-V,\nAES-256, and ZUC-256 -- evaluating their throughput, side-channel resilience,\nand adaptability to heterogeneous slices (eMBB, URLLC, mMTC). Emphasis is\nplaced on emerging testbeds and AI-driven controllers that facilitate dynamic\norchestration, anomaly detection, and secure configuration. We conclude by\noutlining future research directions, including hardware offloading,\ncross-layer cipher adaptation, and alignment with 3GPP TS 33.501 and O-RAN\nAlliance security mandates, all of which point toward the need for integrated,\nzero-trust architectures in 6G."
    },
    {
        "date": "2025-06",
        "title": "Token Constraint Decoding Improves Robustness on Question Answering for Large Language Models",
        "author": "Jui-Ming Yao, Hao-Yuan Chen, Zi-Xian Tang, Bing-Jia Tan, Sheng-Wei Peng, Bing-Cheng Xie, and Shun-Feng Su",
        "link": "http://arxiv.org/abs/2506.09408v1",
        "abstract": "Large Language Models (LLMs) have demonstrated impressive performance on\nmultiple-choice question answering (MCQA) benchmarks, yet they remain highly\nvulnerable to minor input perturbations. In this paper, we introduce and\nevaluate Token Constraint Decoding (TCD). This simple yet effective\ninference-time algorithm enforces alignment between token-level predictions to\nenhance robustness in noisy settings. Through extensive experiments on\nCommonsenseQA, MMLU, and MMLU-Pro, we show that TCD, especially when paired\nwith prompt engineering (PE) fixes, significantly restores performance degraded\nby input noise, yielding up to +39\\% absolute gains for weaker models like\nGemma3 1B. Penalty sweep analyses further reveal that TCD implicitly\nregularizes overconfident outputs, with different models requiring distinct\npenalty schedules to maximize resilience. Our findings establish TCD as a\npractical, model-agnostic approach for improving reasoning stability under\nreal-world imperfections and pave the way for more reliable deployment of LLMs\nin safety-critical or user-facing applications."
    },
    {
        "date": "2025-06",
        "title": "ContextBuddy: AI-Enhanced Contextual Insights for Security Alert Investigation (Applied to Intrusion Detection)",
        "author": "Ronal Singh, Mohan Baruwal Chhetri, Surya Nepal, and Cecile Paris",
        "link": "http://arxiv.org/abs/2506.09365v1",
        "abstract": "Modern Security Operations Centres (SOCs) integrate diverse tools, such as\nSIEM, IDS, and XDR systems, offering rich contextual data, including alert\nenrichments, flow features, and similar case histories. Yet, analysts must\nstill manually determine which of these contextual cues are most relevant when\nvalidating specific alerts. We introduce ContextBuddy, an AI assistant that\nlearns from analysts' prior investigations to help them identify the most\nrelevant context for new alerts. Rather than providing enrichments,\nContextBuddy models how analysts have previously selected context and suggests\ntailored cues based on the characteristics of each alert. We formulate context\nselection as a sequential decision-making problem and apply imitation learning\n(IL) to capture analysts' strategies, evaluating multiple IL approaches.\nThrough staged evaluation, we validate ContextBuddy using two intrusion\ndetection datasets (HIKARI-2021, UNSW-NB15). In simulation-based experiments,\nContextBuddy helped simulated reinforcement learning analysts improve\nclassification accuracy (p < 0.001) (increasing F1 by 2.5% for HIKARI and 9%\nfor UNSW), reducing false negatives (1.5% for HIKARI and 10% for UNSW), and\nkeeping false positives below 1%. Decision confidence among agents also\nimproved by 2-3% (p < 0.001). In a within-subject user study (N=13; power =\n0.8), non-experts using ContextBuddy improved classification accuracy by 21.1%\n(p = 0.008) and reduced alert validation time by 24% (p = 0.01). These results\ndemonstrate that by learning context-selection patterns from analysts,\nContextBuddy can yield notable improvements in investigation effectiveness and\nefficiency."
    },
    {
        "date": "2025-06",
        "title": "Autoregressive Adversarial Post-Training for Real-Time Interactive Video Generation",
        "author": "Shanchuan Lin, Ceyuan Yang, Hao He, Jianwen Jiang, Yuxi Ren, Xin Xia, Yang Zhao, Xuefeng Xiao, and Lu Jiang",
        "link": "http://arxiv.org/abs/2506.09350v1",
        "abstract": "Existing large-scale video generation models are computationally intensive,\npreventing adoption in real-time and interactive applications. In this work, we\npropose autoregressive adversarial post-training (AAPT) to transform a\npre-trained latent video diffusion model into a real-time, interactive video\ngenerator. Our model autoregressively generates a latent frame at a time using\na single neural function evaluation (1NFE). The model can stream the result to\nthe user in real time and receive interactive responses as controls to generate\nthe next latent frame. Unlike existing approaches, our method explores\nadversarial training as an effective paradigm for autoregressive generation.\nThis not only allows us to design an architecture that is more efficient for\none-step generation while fully utilizing the KV cache, but also enables\ntraining the model in a student-forcing manner that proves to be effective in\nreducing error accumulation during long video generation. Our experiments\ndemonstrate that our 8B model achieves real-time, 24fps, streaming video\ngeneration at 736x416 resolution on a single H100, or 1280x720 on 8xH100 up to\na minute long (1440 frames). Visit our research website at\nhttps://seaweed-apt.com/2"
    },
    {
        "date": "2025-06",
        "title": "Adversarial Surrogate Risk Bounds for Binary Classification",
        "author": "Natalie S. Frank",
        "link": "http://arxiv.org/abs/2506.09348v1",
        "abstract": "A central concern in classification is the vulnerability of machine learning\nmodels to adversarial attacks. Adversarial training is one of the most popular\ntechniques for training robust classifiers, which involves minimizing an\nadversarial surrogate risk. Recent work characterized when a minimizing\nsequence of an adversarial surrogate risk is also a minimizing sequence of the\nadversarial classification risk for binary classification -- a property known\nas adversarial consistency. However, these results do not address the rate at\nwhich the adversarial classification risk converges to its optimal value for\nsuch a sequence of functions that minimize the adversarial surrogate. This\npaper provides surrogate risk bounds that quantify that convergence rate.\nAdditionally, we derive distribution-dependent surrogate risk bounds in the\nstandard (non-adversarial) learning setting, that may be of independent\ninterest."
    },
    {
        "date": "2025-06",
        "title": "PatchGuard: Adversarially Robust Anomaly Detection and Localization through Vision Transformers and Pseudo Anomalies",
        "author": "Mojtaba Nafez, Amirhossein Koochakian, Arad Maleki, Jafar Habibi, and Mohammad Hossein Rohban",
        "link": "http://arxiv.org/abs/2506.09237v1",
        "abstract": "Anomaly Detection (AD) and Anomaly Localization (AL) are crucial in fields\nthat demand high reliability, such as medical imaging and industrial\nmonitoring. However, current AD and AL approaches are often susceptible to\nadversarial attacks due to limitations in training data, which typically\ninclude only normal, unlabeled samples. This study introduces PatchGuard, an\nadversarially robust AD and AL method that incorporates pseudo anomalies with\nlocalization masks within a Vision Transformer (ViT)-based architecture to\naddress these vulnerabilities. We begin by examining the essential properties\nof pseudo anomalies, and follow it by providing theoretical insights into the\nattention mechanisms required to enhance the adversarial robustness of AD and\nAL systems. We then present our approach, which leverages Foreground-Aware\nPseudo-Anomalies to overcome the deficiencies of previous anomaly-aware\nmethods. Our method incorporates these crafted pseudo-anomaly samples into a\nViT-based framework, with adversarial training guided by a novel loss function\ndesigned to improve model robustness, as supported by our theoretical analysis.\nExperimental results on well-established industrial and medical datasets\ndemonstrate that PatchGuard significantly outperforms previous methods in\nadversarial settings, achieving performance gains of $53.2\\%$ in AD and\n$68.5\\%$ in AL, while also maintaining competitive accuracy in non-adversarial\nsettings. The code repository is available at\nhttps://github.com/rohban-lab/PatchGuard ."
    },
    {
        "date": "2025-06",
        "title": "Perception Characteristics Distance: Measuring Stability and Robustness of Perception System in Dynamic Conditions under a Certain Decision Rule",
        "author": "Boyu Jiang, Liang Shi, Zhengzhi Lin, Loren Stowe, and Feng Guo",
        "link": "http://arxiv.org/abs/2506.09217v1",
        "abstract": "The performance of perception systems in autonomous driving systems (ADS) is\nstrongly influenced by object distance, scene dynamics, and environmental\nconditions such as weather. AI-based perception outputs are inherently\nstochastic, with variability driven by these external factors, while\ntraditional evaluation metrics remain static and event-independent, failing to\ncapture fluctuations in confidence over time. In this work, we introduce the\nPerception Characteristics Distance (PCD) -- a novel evaluation metric that\nquantifies the farthest distance at which an object can be reliably detected,\nincorporating uncertainty in model outputs. To support this, we present the\nSensorRainFall dataset, collected on the Virginia Smart Road using a\nsensor-equipped vehicle (cameras, radar, LiDAR) under controlled daylight-clear\nand daylight-rain scenarios, with precise ground-truth distances to the target\nobjects. Statistical analysis reveals the presence of change points in the\nvariance of detection confidence score with distance. By averaging the PCD\nvalues across a range of detection quality thresholds and probabilistic\nthresholds, we compute the mean PCD (mPCD), which captures the overall\nperception characteristics of a system with respect to detection distance.\nApplying state-of-the-art perception models shows that mPCD captures meaningful\nreliability differences under varying weather conditions -- differences that\nstatic metrics overlook. PCD provides a principled, distribution-aware measure\nof perception performance, supporting safer and more robust ADS operation,\nwhile the SensorRainFall dataset offers a valuable benchmark for evaluation.\nThe SensorRainFall dataset is publicly available at\nhttps://www.kaggle.com/datasets/datadrivenwheels/sensorrainfall, and the\nevaluation code is open-sourced at\nhttps://github.com/datadrivenwheels/PCD_Python."
    },
    {
        "date": "2025-06",
        "title": "Robust Noise Attenuation via Adaptive Pooling of Transformer Outputs",
        "author": "Greyson Brothers",
        "link": "http://arxiv.org/abs/2506.09215v1",
        "abstract": "We investigate the design of pooling methods used to summarize the outputs of\ntransformer embedding models, primarily motivated by reinforcement learning and\nvision applications. This work considers problems where a subset of the input\nvectors contains requisite information for a downstream task (signal) while the\nrest are distractors (noise). By framing pooling as vector quantization with\nthe goal of minimizing signal loss, we demonstrate that the standard methods\nused to aggregate transformer outputs, AvgPool, MaxPool, and ClsToken, are\nvulnerable to performance collapse as the signal-to-noise ratio (SNR) of inputs\nfluctuates. We then show that an attention-based adaptive pooling method can\napproximate the signal-optimal vector quantizer within derived error bounds for\nany SNR. Our theoretical results are first validated by supervised experiments\non a synthetic dataset designed to isolate the SNR problem, then generalized to\nstandard relational reasoning, multi-agent reinforcement learning, and vision\nbenchmarks with noisy observations, where transformers with adaptive pooling\ndisplay superior robustness across tasks."
    },
    {
        "date": "2025-06",
        "title": "Adversarial Text Generation with Dynamic Contextual Perturbation",
        "author": "Hetvi Waghela, Jaydip Sen, Sneha Rakshit, and Subhasis Dasgupta",
        "link": "http://arxiv.org/abs/2506.09148v1",
        "abstract": "Adversarial attacks on Natural Language Processing (NLP) models expose\nvulnerabilities by introducing subtle perturbations to input text, often\nleading to misclassification while maintaining human readability. Existing\nmethods typically focus on word-level or local text segment alterations,\noverlooking the broader context, which results in detectable or semantically\ninconsistent perturbations. We propose a novel adversarial text attack scheme\nnamed Dynamic Contextual Perturbation (DCP). DCP dynamically generates\ncontext-aware perturbations across sentences, paragraphs, and documents,\nensuring semantic fidelity and fluency. Leveraging the capabilities of\npre-trained language models, DCP iteratively refines perturbations through an\nadversarial objective function that balances the dual objectives of inducing\nmodel misclassification and preserving the naturalness of the text. This\ncomprehensive approach allows DCP to produce more sophisticated and effective\nadversarial examples that better mimic natural language patterns. Our\nexperimental results, conducted on various NLP models and datasets, demonstrate\nthe efficacy of DCP in challenging the robustness of state-of-the-art NLP\nsystems. By integrating dynamic contextual analysis, DCP significantly enhances\nthe subtlety and impact of adversarial attacks. This study highlights the\ncritical role of context in adversarial attacks and lays the groundwork for\ncreating more robust NLP systems capable of withstanding sophisticated\nadversarial strategies."
    },
    {
        "date": "2025-06",
        "title": "Towards Robust Deep Reinforcement Learning against Environmental State Perturbation",
        "author": "Chenxu Wang, and Huaping Liu",
        "link": "http://arxiv.org/abs/2506.08961v1",
        "abstract": "Adversarial attacks and robustness in Deep Reinforcement Learning (DRL) have\nbeen widely studied in various threat models; however, few consider\nenvironmental state perturbations, which are natural in embodied scenarios. To\nimprove the robustness of DRL agents, we formulate the problem of environmental\nstate perturbation, introducing a preliminary non-targeted attack method as a\ncalibration adversary, and then propose a defense framework, named Boosted\nAdversarial Training (BAT), which first tunes the agents via supervised\nlearning to avoid catastrophic failure and subsequently adversarially trains\nthe agent with reinforcement learning. Extensive experimental results\nsubstantiate the vulnerability of mainstream agents under environmental state\nperturbations and the effectiveness of our proposed attack. The defense results\ndemonstrate that while existing robust reinforcement learning algorithms may\nnot be suitable, our BAT framework can significantly enhance the robustness of\nagents against environmental state perturbations across various situations."
    },
    {
        "date": "2025-06",
        "title": "AdversariaL attacK sAfety aLIgnment(ALKALI): Safeguarding LLMs through GRACE: Geometric Representation-Aware Contrastive Enhancement- Introducing Adversarial Vulnerability Quality Index (AVQI)",
        "author": "Danush Khanna, Krishna Kumar, Basab Ghosh, Vinija Jain, Vasu Sharma, Aman Chadha, and Amitava Das",
        "link": "http://arxiv.org/abs/2506.08885v2",
        "abstract": "Adversarial threats against LLMs are escalating faster than current defenses\ncan adapt. We expose a critical geometric blind spot in alignment: adversarial\nprompts exploit latent camouflage, embedding perilously close to the safe\nrepresentation manifold while encoding unsafe intent thereby evading surface\nlevel defenses like Direct Preference Optimization (DPO), which remain blind to\nthe latent geometry. We introduce ALKALI, the first rigorously curated\nadversarial benchmark and the most comprehensive to date spanning 9,000 prompts\nacross three macro categories, six subtypes, and fifteen attack families.\nEvaluation of 21 leading LLMs reveals alarmingly high Attack Success Rates\n(ASRs) across both open and closed source models, exposing an underlying\nvulnerability we term latent camouflage, a structural blind spot where\nadversarial completions mimic the latent geometry of safe ones. To mitigate\nthis vulnerability, we introduce GRACE - Geometric Representation Aware\nContrastive Enhancement, an alignment framework coupling preference learning\nwith latent space regularization. GRACE enforces two constraints: latent\nseparation between safe and adversarial completions, and adversarial cohesion\namong unsafe and jailbreak behaviors. These operate over layerwise pooled\nembeddings guided by a learned attention profile, reshaping internal geometry\nwithout modifying the base model, and achieve up to 39% ASR reduction.\nMoreover, we introduce AVQI, a geometry aware metric that quantifies latent\nalignment failure via cluster separation and compactness. AVQI reveals when\nunsafe completions mimic the geometry of safe ones, offering a principled lens\ninto how models internally encode safety. We make the code publicly available\nat https://anonymous.4open.science/r/alkali-B416/README.md."
    },
    {
        "date": "2025-06",
        "title": "SmartAttack: Air-Gap Attack via Smartwatches",
        "author": "Mordechai Guri",
        "link": "http://arxiv.org/abs/2506.08866v1",
        "abstract": "Air-gapped systems are considered highly secure against data leaks due to\ntheir physical isolation from external networks. Despite this protection,\nultrasonic communication has been demonstrated as an effective method for\nexfiltrating data from such systems. While smartphones have been extensively\nstudied in the context of ultrasonic covert channels, smartwatches remain an\nunderexplored yet effective attack vector.\n  In this paper, we propose and evaluate SmartAttack, a novel method that\nleverages smartwatches as receivers for ultrasonic covert communication in\nair-gapped environments. Our approach utilizes the built-in microphones of\nsmartwatches to capture covert signals in real time within the ultrasonic\nfrequency range of 18-22 kHz. Through experimental validation, we assess the\nfeasibility of this attack under varying environmental conditions, distances,\norientations, and noise levels. Furthermore, we analyze smartwatch-specific\nfactors that influence ultrasonic covert channels, including their continuous\npresence on the user's wrist, the impact of the human body on signal\npropagation, and the directional constraints of built-in microphones. Our\nfindings highlight the security risks posed by smartwatches in high-security\nenvironments and outline mitigation strategies to counteract this emerging\nthreat."
    },
    {
        "date": "2025-06",
        "title": "Design Patterns for Securing LLM Agents against Prompt Injections",
        "author": "Luca Beurer-Kellner, Beat Buesser Ana-Maria Cre\u0163u, Edoardo Debenedetti, Daniel Dobos, Daniel Fabian, Marc Fischer, David Froelicher, Kathrin Grosse, Daniel Naeff, Ezinwanne Ozoani, Andrew Paverd, Florian Tram\u00e8r, and V\u00e1clav Volhejn",
        "link": "http://arxiv.org/abs/2506.08837v2",
        "abstract": "As AI agents powered by Large Language Models (LLMs) become increasingly\nversatile and capable of addressing a broad spectrum of tasks, ensuring their\nsecurity has become a critical challenge. Among the most pressing threats are\nprompt injection attacks, which exploit the agent's resilience on natural\nlanguage inputs -- an especially dangerous threat when agents are granted tool\naccess or handle sensitive information. In this work, we propose a set of\nprincipled design patterns for building AI agents with provable resistance to\nprompt injection. We systematically analyze these patterns, discuss their\ntrade-offs in terms of utility and security, and illustrate their real-world\napplicability through a series of case studies."
    },
    {
        "date": "2025-06",
        "title": "Lightweight Electronic Signatures and Reliable Access Control Included in Sensor Networks to Prevent Cyber Attacks from Modifying Patient Data",
        "author": "Mishall Al-Zubaidie",
        "link": "http://arxiv.org/abs/2506.08828v1",
        "abstract": "Digital terrorism is a major cause of securing patient/healthcare providers\ndata and information. Sensitive topics that may have an impact on a patient's\nhealth or even national security include patient health records and information\non healthcare providers. Health databases and data sets have been continually\nbreached by many, regular assaults, as well as local and remote servers\nequipped with wireless sensor networks (WSNs) in diverse locations. The problem\nwas addressed by some contemporary strategies that were created to stop these\nassaults and guarantee the privacy of patient data and information transferred\nand gathered by sensors. Nevertheless, the literature analysis outlines many\nindications of weakness that persist in these methods. This study suggests a\nnovel, reliable method that bolsters the information security and data gathered\nby sensors and kept on base station datasets. The proposed approach combines a\nnumber of security mechanisms, including symmetric cryptography for encryption,\nasymmetric cryptography for access control and signatures, and the Lesamnta-LW\nmethod in the signature process. Users' information is shielded from prying\neyes by the careful application of these measures and a sound approach.\nInvestigational comparisons, security studies, and thorough results show that\nthe suggested method is better than earlier methods."
    },
    {
        "date": "2025-06",
        "title": "Lightweight and High-Throughput Secure Logging for Internet of Things and Cold Cloud Continuum",
        "author": "Saif E. Nouma, and Attila A. Yavuz",
        "link": "http://arxiv.org/abs/2506.08781v1",
        "abstract": "The growing deployment of resource-limited Internet of Things (IoT) devices\nand their expanding attack surfaces demand efficient and scalable security\nmechanisms. System logs are vital for the trust and auditability of IoT, and\noffloading their maintenance to a Cold Storage-as-a-Service (Cold-STaaS)\nenhances cost-effectiveness and reliability. However, existing cryptographic\nlogging solutions either burden low-end IoT devices with heavy computation or\ncreate verification delays and storage inefficiencies at Cold-STaaS. There is a\npressing need for cryptographic primitives that balance security, performance,\nand scalability across IoT-Cold-STaaS continuum.\n  In this work, we present Parallel Optimal Signatures for Secure Logging\n(POSLO), a novel digital signature framework that, to our knowledge, is the\nfirst to offer constant-size signatures and public keys, near-optimal signing\nefficiency, and tunable fine-to-coarse-grained verification for log auditing.\nPOSLO achieves these properties through efficient randomness management,\nflexible aggregation, and multiple algorithmic instantiations. It also\nintroduces a GPU-accelerated batch verification framework that exploits\nhomomorphic signature aggregation to deliver ultra-fast performance. For\nexample, POSLO can verify 231 log entries per second on a mid-range consumer\nGPU (NVIDIA GTX 3060) while being significantly more compact than\nstate-of-the-art. POSLO also preserves signer-side efficiency, offering\nsubstantial battery savings for IoT devices, and is well-suited for the\nIoT-Cold-STaaS ecosystem."
    },
    {
        "date": "2025-06",
        "title": "Normalized Radon Cumulative Distribution Transforms for Invariance and Robustness in Optimal Transport Based Image Classification",
        "author": "Matthias Beckmann, Robert Beinert, and Jonas Bresch",
        "link": "http://arxiv.org/abs/2506.08761v1",
        "abstract": "The Radon cumulative distribution transform (R-CDT), is an easy-to-compute\nfeature extractor that facilitates image classification tasks especially in the\nsmall data regime. It is closely related to the sliced Wasserstein distance and\nprovably guaranties the linear separability of image classes that emerge from\ntranslations or scalings. In many real-world applications, like the recognition\nof watermarks in filigranology, however, the data is subject to general affine\ntransformations originating from the measurement process. To overcome this\nissue, we recently introduced the so-called max-normalized R-CDT that only\nrequires elementary operations and guaranties the separability under arbitrary\naffine transformations. The aim of this paper is to continue our study of the\nmax-normalized R-CDT especially with respect to its robustness against\nnon-affine image deformations. Our sensitivity analysis shows that its\nseparability properties are stable provided the Wasserstein-infinity distance\nbetween the samples can be controlled. Since the Wasserstein-infinity distance\nonly allows small local image deformations, we moreover introduce a\nmean-normalized version of the R-CDT. In this case, robustness relates to the\nWasserstein-2 distance and also covers image deformations caused by impulsive\nnoise for instance. Our theoretical results are supported by numerical\nexperiments showing the effectiveness of our novel feature extractors as well\nas their robustness against local non-affine deformations and impulsive noise."
    },
    {
        "date": "2025-06",
        "title": "Towards Secure and Private Language Models for Nuclear Power Plants",
        "author": "Muhammad Anwar, Mishca de Costa, Issam Hammad, and Daniel Lau",
        "link": "http://arxiv.org/abs/2506.08746v1",
        "abstract": "This paper introduces a domain-specific Large Language Model for nuclear\napplications, built from the publicly accessible Essential CANDU textbook.\nDrawing on a compact Transformer-based architecture, the model is trained on a\nsingle GPU to protect the sensitive data inherent in nuclear operations.\nDespite relying on a relatively small dataset, it shows encouraging signs of\ncapturing specialized nuclear vocabulary, though the generated text sometimes\nlacks syntactic coherence. By focusing exclusively on nuclear content, this\napproach demonstrates the feasibility of in-house LLM solutions that align with\nrigorous cybersecurity and data confidentiality standards. Early successes in\ntext generation underscore the model's utility for specialized tasks, while\nalso revealing the need for richer corpora, more sophisticated preprocessing,\nand instruction fine-tuning to enhance domain accuracy. Future directions\ninclude extending the dataset to cover diverse nuclear subtopics, refining\ntokenization to reduce noise, and systematically evaluating the model's\nreadiness for real-world applications in nuclear domain."
    },
    {
        "date": "2025-06",
        "title": "On the Ethics of Using LLMs for Offensive Security",
        "author": "Andreas Happe, and J\u00fcrgen Cito",
        "link": "http://arxiv.org/abs/2506.08693v1",
        "abstract": "Large Language Models (LLMs) have rapidly evolved over the past few years and\nare currently evaluated for their efficacy within the domain of offensive\ncyber-security. While initial forays showcase the potential of LLMs to enhance\nsecurity research, they also raise critical ethical concerns regarding the\ndual-use of offensive security tooling.\n  This paper analyzes a set of papers that leverage LLMs for offensive\nsecurity, focusing on how ethical considerations are expressed and justified in\ntheir work. The goal is to assess the culture of AI in offensive security\nresearch regarding ethics communication, highlighting trends, best practices,\nand gaps in current discourse.\n  We provide insights into how the academic community navigates the fine line\nbetween innovation and ethical responsibility. Particularly, our results show\nthat 13 of 15 reviewed prototypes (86.6\\%) mentioned ethical considerations and\nare thus aware of the potential dual-use of their research. Main motivation\ngiven for the research was allowing broader access to penetration-testing as\nwell as preparing defenders for AI-guided attackers."
    },
    {
        "date": "2025-06",
        "title": "Towards Robust Real-World Multivariate Time Series Forecasting: A Unified Framework for Dependency, Asynchrony, and Missingness",
        "author": "Jinkwan Jang, Hyungjin Park, Jinmyeong Choi, and Taesup Kim",
        "link": "http://arxiv.org/abs/2506.08660v1",
        "abstract": "Real-world time series data are inherently multivariate, often exhibiting\ncomplex inter-channel dependencies. Each channel is typically sampled at its\nown period and is prone to missing values due to various practical and\noperational constraints. These characteristics pose fundamental challenges\nrelated to channel dependency, sampling asynchrony, and missingness, all of\nwhich must be addressed to enable robust and reliable forecasting in practical\nsettings. However, most existing architectures are built on oversimplified\nassumptions, such as identical sampling periods across channels and fully\nobserved inputs at test time, which often do not hold in real-world scenarios.\nTo bridge this gap, we propose ChannelTokenFormer, a Transformer-based\nforecasting model with a flexible architecture designed to explicitly capture\ncross-channel interactions, accommodate channel-wise asynchronous sampling, and\neffectively handle missing values. Extensive experiments on three benchmark\ndatasets modified to reflect practical settings, along with one real-world\nindustrial dataset, demonstrate the superior robustness and accuracy of\nChannelTokenFormer under challenging real-world conditions."
    },
    {
        "date": "2025-06",
        "title": "Towards Class-wise Fair Adversarial Training via Anti-Bias Soft Label Distillation",
        "author": "Shiji Zhao, Chi Chen, Ranjie Duan, Xizhe Wang, and Xingxing Wei",
        "link": "http://arxiv.org/abs/2506.08611v1",
        "abstract": "Adversarial Training (AT) is widely recognized as an effective approach to\nenhance the adversarial robustness of Deep Neural Networks. As a variant of AT,\nAdversarial Robustness Distillation (ARD) has shown outstanding performance in\nenhancing the robustness of small models. However, both AT and ARD face robust\nfairness issue: these models tend to display strong adversarial robustness\nagainst some classes (easy classes) while demonstrating weak adversarial\nrobustness against others (hard classes). This paper explores the underlying\nfactors of this problem and points out the smoothness degree of soft labels for\ndifferent classes significantly impacts the robust fairness from both empirical\nobservation and theoretical analysis. Based on the above exploration, we\npropose Anti-Bias Soft Label Distillation (ABSLD) within the Knowledge\nDistillation framework to enhance the adversarial robust fairness.\nSpecifically, ABSLD adaptively reduces the student's error risk gap between\ndifferent classes, which is accomplished by adjusting the class-wise smoothness\ndegree of teacher's soft labels during the training process, and the adjustment\nis managed by assigning varying temperatures to different classes.\nAdditionally, as a label-based approach, ABSLD is highly adaptable and can be\nintegrated with the sample-based methods. Extensive experiments demonstrate\nABSLD outperforms state-of-the-art methods on the comprehensive performance of\nrobustness and fairness."
    },
    {
        "date": "2025-06",
        "title": "Towards Cross-Subject EMG Pattern Recognition via Dual-Branch Adversarial Feature Disentanglement",
        "author": "Xinyue Niu, and Akira Furui",
        "link": "http://arxiv.org/abs/2506.08555v2",
        "abstract": "Cross-subject electromyography (EMG) pattern recognition faces significant\nchallenges due to inter-subject variability in muscle anatomy, electrode\nplacement, and signal characteristics. Traditional methods rely on\nsubject-specific calibration data to adapt models to new users, an approach\nthat is both time-consuming and impractical for large-scale, real-world\ndeployment. This paper presents an approach to eliminate calibration\nrequirements through feature disentanglement, enabling effective cross-subject\ngeneralization. We propose an end-to-end dual-branch adversarial neural network\nthat simultaneously performs pattern recognition and individual identification\nby disentangling EMG features into pattern-specific and subject-specific\ncomponents. The pattern-specific components facilitate robust pattern\nrecognition for new users without model calibration, while the subject-specific\ncomponents enable downstream applications such as task-invariant biometric\nidentification. Experimental results demonstrate that the proposed model\nachieves robust performance on data from unseen users, outperforming various\nbaseline methods in cross-subject scenarios. Overall, this study offers a new\nperspective for cross-subject EMG pattern recognition without model calibration\nand highlights the proposed model's potential for broader applications, such as\ntask-independent biometric systems."
    },
    {
        "date": "2025-06",
        "title": "Robust Evolutionary Multi-Objective Network Architecture Search for Reinforcement Learning (EMNAS-RL)",
        "author": "Nihal Acharya Adde, Alexandra Gianzina, Hanno Gottschalk, and Andreas Ebert",
        "link": "http://arxiv.org/abs/2506.08533v1",
        "abstract": "This paper introduces Evolutionary Multi-Objective Network Architecture\nSearch (EMNAS) for the first time to optimize neural network architectures in\nlarge-scale Reinforcement Learning (RL) for Autonomous Driving (AD). EMNAS uses\ngenetic algorithms to automate network design, tailored to enhance rewards and\nreduce model size without compromising performance. Additionally,\nparallelization techniques are employed to accelerate the search, and\nteacher-student methodologies are implemented to ensure scalable optimization.\nThis research underscores the potential of transfer learning as a robust\nframework for optimizing performance across iterative learning processes by\neffectively leveraging knowledge from earlier generations to enhance learning\nefficiency and stability in subsequent generations. Experimental results\ndemonstrate that tailored EMNAS outperforms manually designed models, achieving\nhigher rewards with fewer parameters. The findings of these strategies\ncontribute positively to EMNAS for RL in autonomous driving, advancing the\nfield toward better-performing networks suitable for real-world scenarios."
    },
    {
        "date": "2025-06",
        "title": "Robust Visual Localization via Semantic-Guided Multi-Scale Transformer",
        "author": "Zhongtao Tian, Wenhao Huang, Zhidong Chen, and Xiao Wei Sun",
        "link": "http://arxiv.org/abs/2506.08526v1",
        "abstract": "Visual localization remains challenging in dynamic environments where\nfluctuating lighting, adverse weather, and moving objects disrupt appearance\ncues. Despite advances in feature representation, current absolute pose\nregression methods struggle to maintain consistency under varying conditions.\nTo address this challenge, we propose a framework that synergistically combines\nmulti-scale feature learning with semantic scene understanding. Our approach\nemploys a hierarchical Transformer with cross-scale attention to fuse geometric\ndetails and contextual cues, preserving spatial precision while adapting to\nenvironmental changes. We improve the performance of this architecture with\nsemantic supervision via neural scene representation during training, guiding\nthe network to learn view-invariant features that encode persistent structural\ninformation while suppressing complex environmental interference. Experiments\non TartanAir demonstrate that our approach outperforms existing pose regression\nmethods in challenging scenarios with dynamic objects, illumination changes,\nand occlusions. Our findings show that integrating multi-scale processing with\nsemantic guidance offers a promising strategy for robust visual localization in\nreal-world dynamic environments."
    },
    {
        "date": "2025-06",
        "title": "DiffGradCAM: A Universal Class Activation Map Resistant to Adversarial Training",
        "author": "Jacob Piland, Chris Sweet, and Adam Czakja",
        "link": "http://arxiv.org/abs/2506.08514v1",
        "abstract": "Class Activation Mapping (CAM) and its gradient-based variants (e.g.,\nGradCAM) have become standard tools for explaining Convolutional Neural Network\n(CNN) predictions. However, these approaches typically focus on individual\nlogits, while for neural networks using softmax, the class membership\nprobability estimates depend \\textit{only} on the \\textit{differences} between\nlogits, not on their absolute values. This disconnect leaves standard CAMs\nvulnerable to adversarial manipulation, such as passive fooling, where a model\nis trained to produce misleading CAMs without affecting decision performance.\nWe introduce \\textbf{Salience-Hoax Activation Maps (SHAMs)}, an\n\\emph{entropy-aware form of passive fooling} that serves as a benchmark for CAM\nrobustness under adversarial conditions. To address the passive fooling\nvulnerability, we then propose \\textbf{DiffGradCAM}, a novel, lightweight, and\ncontrastive approach to class activation mapping that is both non-suceptible to\npassive fooling, but also matches the output of standard CAM methods such as\nGradCAM in the non-adversarial case. Together, SHAM and DiffGradCAM establish a\nnew framework for probing and improving the robustness of saliency-based\nexplanations. We validate both contributions across multi-class tasks with few\nand many classes."
    },
    {
        "date": "2025-06",
        "title": "Secure Data Access in Cloud Environments Using Quantum Cryptography",
        "author": "S. Vasavi Venkata Lakshmi, and Ziaul Haque Choudhury",
        "link": "http://arxiv.org/abs/2506.10028v1",
        "abstract": "Cloud computing has made storing and accessing data easier but keeping it\nsecure is a big challenge nowadays. Traditional methods of ensuring data may\nnot be strong enough in the future when powerful quantum computers become\navailable. To solve this problem, this study uses quantum cryptography to\nprotect data in the cloud environment. Quantum Key Distribution (QKD) creates\nsecure keys by sending information using quantum particles like photons.\nSpecifically, we use the BB84 protocol, a simple and reliable way to make\nsecure keys that cannot be stolen without detection. To protect the data, we\nuse the Quantum One Time pad (QOTP) for encryption and decryption, ensuring the\ndata stays completely private. This study shows how these Quantum methods can\nbe applied in cloud systems to provide a strong defense against hackers, even\nif they have access to quantum computers. The combination of QKD, BB84, and\nQOTP creates a safe and reliable way to keep data secure when it is stored or\nshared in the cloud. Using quantum cryptography, this paper provides a way to\nensure data security now and in the future, making cloud computing safer for\neveryone to store their data securely and safely."
    },
    {
        "date": "2025-06",
        "title": "One Patch to Rule Them All: Transforming Static Patches into Dynamic Attacks in the Physical World",
        "author": "Xingshuo Han, Chen Ling, Shiyi Yao, Haozhao Wang, Hangcheng Liu, Yutong Wu, Shengmin Xu, Changhai Ou, Xinyi Huang, and Tianwei Zhang",
        "link": "http://arxiv.org/abs/2506.08482v1",
        "abstract": "Numerous methods have been proposed to generate physical adversarial patches\n(PAPs) against real-world machine learning systems. However, each existing PAP\ntypically supports only a single, fixed attack goal, and switching to a\ndifferent objective requires re-generating and re-deploying a new PAP. This\nrigidity limits their practicality in dynamic environments like autonomous\ndriving, where traffic conditions and attack goals can change rapidly. For\nexample, if no obstacles are present around the target vehicle, the attack may\nfail to cause meaningful consequences.\n  To overcome this limitation, we propose SwitchPatch, a novel PAP that is\nstatic yet enables dynamic and controllable attack outcomes based on real-time\nscenarios. Attackers can alter pre-defined conditions, e.g., by projecting\ndifferent natural-color lights onto SwitchPatch to seamlessly switch between\nattack goals. Unlike prior work, SwitchPatch does not require re-generation or\nre-deployment for different objectives, significantly reducing cost and\ncomplexity. Furthermore, SwitchPatch remains benign when the enabling\nconditions are absent, enhancing its stealth.\n  We evaluate SwitchPatch on two key tasks: traffic sign recognition\n(classification and detection) and depth estimation. First, we conduct\ntheoretical analysis and empirical studies to demonstrate the feasibility of\nSwitchPatch and explore how many goals it can support using techniques like\ncolor light projection and occlusion. Second, we perform simulation-based\nexperiments and ablation studies to verify its effectiveness and\ntransferability. Third, we conduct outdoor tests using a Unmanned Ground\nVehicle (UGV) to confirm its robustness in the physical world. Overall,\nSwitchPatch introduces a flexible and practical adversarial strategy that can\nbe adapted to diverse tasks and real-world conditions."
    },
    {
        "date": "2025-06",
        "title": "The interplay of robustness and generalization in quantum machine learning",
        "author": "Julian Berberich, Tobias Fellner, and Christian Holm",
        "link": "http://arxiv.org/abs/2506.08455v1",
        "abstract": "While adversarial robustness and generalization have individually received\nsubstantial attention in the recent literature on quantum machine learning,\ntheir interplay is much less explored. In this chapter, we address this\ninterplay for variational quantum models, which were recently proposed as\nfunction approximators in supervised learning. We discuss recent results\nquantifying both robustness and generalization via Lipschitz bounds, which\nexplicitly depend on model parameters. Thus, they give rise to a\nregularization-based training approach for robust and generalizable quantum\nmodels, highlighting the importance of trainable data encoding strategies. The\npractical implications of the theoretical results are demonstrated with an\napplication to time series analysis."
    },
    {
        "date": "2025-06",
        "title": "GPS Spoofing Attacks on AI-based Navigation Systems with Obstacle Avoidance in UAV",
        "author": "Ji Hyuk Jung, Mi Yeon Hong, and Ji Won Yoon",
        "link": "http://arxiv.org/abs/2506.08445v1",
        "abstract": "Recently, approaches using Deep Reinforcement Learning (DRL) have been\nproposed to solve UAV navigation systems in complex and unknown environments.\nHowever, despite extensive research and attention, systematic studies on\nvarious security aspects have not yet been conducted. Therefore, in this paper,\nwe conduct research on security vulnerabilities in DRL-based navigation\nsystems, particularly focusing on GPS spoofing attacks against the system. Many\nrecent basic DRL-based navigation systems fundamentally share an efficient\nstructure. This paper presents an attack model that operates through GPS\nspoofing attacks briefly modeling the range of spoofing attack against EKF\nsensor fusion of PX4 autopilot, and combine this with the DRL-based system to\ndesign attack scenarios that are closer to reality. Finally, this paper\nexperimentally demonstrated that attacks are possible both in the basic DRL\nsystem and in attack models combining the DRL system with PX4 autopilot system."
    },
    {
        "date": "2025-06",
        "title": "Boosting Gradient Leakage Attacks: Data Reconstruction in Realistic FL Settings",
        "author": "Mingyuan Fan, Fuyi Wang, Cen Chen, and Jianying Zhou",
        "link": "http://arxiv.org/abs/2506.08435v1",
        "abstract": "Federated learning (FL) enables collaborative model training among multiple\nclients without the need to expose raw data. Its ability to safeguard privacy,\nat the heart of FL, has recently been a hot-button debate topic. To elaborate,\nseveral studies have introduced a type of attacks known as gradient leakage\nattacks (GLAs), which exploit the gradients shared during training to\nreconstruct clients' raw data. On the flip side, some literature, however,\ncontends no substantial privacy risk in practical FL environments due to the\neffectiveness of such GLAs being limited to overly relaxed conditions, such as\nsmall batch sizes and knowledge of clients' data distributions.\n  This paper bridges this critical gap by empirically demonstrating that\nclients' data can still be effectively reconstructed, even within realistic FL\nenvironments. Upon revisiting GLAs, we recognize that their performance\nfailures stem from their inability to handle the gradient matching problem. To\nalleviate the performance bottlenecks identified above, we develop FedLeak,\nwhich introduces two novel techniques, partial gradient matching and gradient\nregularization. Moreover, to evaluate the performance of FedLeak in real-world\nFL environments, we formulate a practical evaluation protocol grounded in a\nthorough review of extensive FL literature and industry practices. Under this\nprotocol, FedLeak can still achieve high-fidelity data reconstruction, thereby\nunderscoring the significant vulnerability in FL systems and the urgent need\nfor more effective defense methods."
    },
    {
        "date": "2025-06",
        "title": "Single-Node Trigger Backdoor Attacks in Graph-Based Recommendation Systems",
        "author": "Runze Li, Di Jin, Xiaobao Wang, Dongxiao He, Bingdao Feng, and Zhen Wang",
        "link": "http://arxiv.org/abs/2506.08401v1",
        "abstract": "Graph recommendation systems have been widely studied due to their ability to\neffectively capture the complex interactions between users and items. However,\nthese systems also exhibit certain vulnerabilities when faced with attacks. The\nprevailing shilling attack methods typically manipulate recommendation results\nby injecting a large number of fake nodes and edges. However, such attack\nstrategies face two primary challenges: low stealth and high destructiveness.\nTo address these challenges, this paper proposes a novel graph backdoor attack\nmethod that aims to enhance the exposure of target items to the target user in\na covert manner, without affecting other unrelated nodes. Specifically, we\ndesign a single-node trigger generator, which can effectively expose multiple\ntarget items to the target user by inserting only one fake user node.\nAdditionally, we introduce constraint conditions between the target nodes and\nirrelevant nodes to mitigate the impact of fake nodes on the recommendation\nsystem's performance. Experimental results show that the exposure of the target\nitems reaches no less than 50% in 99% of the target users, while the impact on\nthe recommendation system's performance is controlled within approximately 5%."
    },
    {
        "date": "2025-06",
        "title": "SUTA-LM: Bridging Test-Time Adaptation and Language Model Rescoring for Robust ASR",
        "author": "Wei-Ping Huang, Guan-Ting Lin, and Hung-yi Lee",
        "link": "http://arxiv.org/abs/2506.11121v1",
        "abstract": "Despite progress in end-to-end ASR, real-world domain mismatches still cause\nperformance drops, which Test-Time Adaptation (TTA) aims to mitigate by\nadjusting models during inference. Recent work explores combining TTA with\nexternal language models, using techniques like beam search rescoring or\ngenerative error correction. In this work, we identify a previously overlooked\nchallenge: TTA can interfere with language model rescoring, revealing the\nnontrivial nature of effectively combining the two methods. Based on this\ninsight, we propose SUTA-LM, a simple yet effective extension of SUTA, an\nentropy-minimization-based TTA approach, with language model rescoring. SUTA-LM\nfirst applies a controlled adaptation process guided by an auto-step selection\nmechanism leveraging both acoustic and linguistic information, followed by\nlanguage model rescoring to refine the outputs. Experiments on 18 diverse ASR\ndatasets show that SUTA-LM achieves robust results across a wide range of\ndomains."
    },
    {
        "date": "2025-06",
        "title": "AlphaFold Database Debiasing for Robust Inverse Folding",
        "author": "Cheng Tan, Zhenxiao Cao, Zhangyang Gao, Siyuan Li, Yufei Huang, and Stan Z. Li",
        "link": "http://arxiv.org/abs/2506.08365v1",
        "abstract": "The AlphaFold Protein Structure Database (AFDB) offers unparalleled\nstructural coverage at near-experimental accuracy, positioning it as a valuable\nresource for data-driven protein design. However, its direct use in training\ndeep models that are sensitive to fine-grained atomic geometry, such as inverse\nfolding, exposes a critical limitation. Comparative analysis of structural\nfeature distributions reveals that AFDB structures exhibit distinct statistical\nregularities, reflecting a systematic geometric bias that deviates from the\nconformational diversity found in experimentally determined structures from the\nProtein Data Bank (PDB). While AFDB structures are cleaner and more idealized,\nPDB structures capture the intrinsic variability and physical realism essential\nfor generalization in downstream tasks. To address this discrepancy, we\nintroduce a Debiasing Structure AutoEncoder (DeSAE) that learns to reconstruct\nnative-like conformations from intentionally corrupted backbone geometries. By\ntraining the model to recover plausible structural states, DeSAE implicitly\ncaptures a more robust and natural structural manifold. At inference, applying\nDeSAE to AFDB structures produces debiased structures that significantly\nimprove inverse folding performance across multiple benchmarks. This work\nhighlights the critical impact of subtle systematic biases in predicted\nstructures and presents a principled framework for debiasing, significantly\nboosting the performance of structure-based learning tasks like inverse\nfolding."
    }
]