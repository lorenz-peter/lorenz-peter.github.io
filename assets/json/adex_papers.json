[{"date":"2025-06","title":"SeedVR2: One-Step Video Restoration via Diffusion Adversarial Post-Training","author":"Jianyi Wang, Shanchuan Lin, Zhijie Lin, Yuxi Ren, Meng Wei, Zongsheng Yue, Shangchen Zhou, Hao Chen, Yang Zhao, Ceyuan Yang, Xuefeng Xiao, Chen Change Loy, and Lu Jiang","link":"http://arxiv.org/abs/2506.05301v1","abstract":"Recent advances in diffusion-based video restoration (VR) demonstrate\nsignificant improvement in visual quality, yet yield a prohibitive\ncomputational cost during inference. While several distillation-based\napproaches have exhibited the potential of one-step image restoration,\nextending existing approaches to VR remains challenging and underexplored,\nparticularly when dealing with high-resolution video in real-world settings. In\nthis work, we propose a one-step diffusion-based VR model, termed as SeedVR2,\nwhich performs adversarial VR training against real data. To handle the\nchallenging high-resolution VR within a single step, we introduce several\nenhancements to both model architecture and training procedures. Specifically,\nan adaptive window attention mechanism is proposed, where the window size is\ndynamically adjusted to fit the output resolutions, avoiding window\ninconsistency observed under high-resolution VR using window attention with a\npredefined window size. To stabilize and improve the adversarial post-training\ntowards VR, we further verify the effectiveness of a series of losses,\nincluding a proposed feature matching loss without significantly sacrificing\ntraining efficiency. Extensive experiments show that SeedVR2 can achieve\ncomparable or even better performance compared with existing VR approaches in a\nsingle step."},{"date":"2025-06","title":"A Smooth Sea Never Made a Skilled $\\texttt{SAILOR}$: Robust Imitation via Learning to Search","author":"Arnav Kumar Jain, Vibhakar Mohta, Subin Kim, Atiksh Bhardwaj, Juntao Ren, Yunhai Feng, Sanjiban Choudhury, and Gokul Swamy","link":"http://arxiv.org/abs/2506.05294v1","abstract":"The fundamental limitation of the behavioral cloning (BC) approach to\nimitation learning is that it only teaches an agent what the expert did at\nstates the expert visited. This means that when a BC agent makes a mistake\nwhich takes them out of the support of the demonstrations, they often don't\nknow how to recover from it. In this sense, BC is akin to giving the agent the\nfish -- giving them dense supervision across a narrow set of states -- rather\nthan teaching them to fish: to be able to reason independently about achieving\nthe expert's outcome even when faced with unseen situations at test-time. In\nresponse, we explore learning to search (L2S) from expert demonstrations, i.e.\nlearning the components required to, at test time, plan to match expert\noutcomes, even after making a mistake. These include (1) a world model and (2)\na reward model. We carefully ablate the set of algorithmic and design decisions\nrequired to combine these and other components for stable and\nsample/interaction-efficient learning of recovery behavior without additional\nhuman corrections. Across a dozen visual manipulation tasks from three\nbenchmarks, our approach $\\texttt{SAILOR}$ consistently out-performs\nstate-of-the-art Diffusion Policies trained via BC on the same data.\nFurthermore, scaling up the amount of demonstrations used for BC by\n5-10$\\times$ still leaves a performance gap. We find that $\\texttt{SAILOR}$ can\nidentify nuanced failures and is robust to reward hacking. Our code is\navailable at https://github.com/arnavkj1995/SAILOR ."},{"date":"2025-06","title":"Can Foundation Models Generalise the Presentation Attack Detection Capabilities on ID Cards?","author":"Juan E. Tapia, and Christoph Busch","link":"http://arxiv.org/abs/2506.05263v1","abstract":"Nowadays, one of the main challenges in presentation attack detection (PAD)\non ID cards is obtaining generalisation capabilities for a diversity of\ncountries that are issuing ID cards. Most PAD systems are trained on one, two,\nor three ID documents because of privacy protection concerns. As a result, they\ndo not obtain competitive results for commercial purposes when tested in an\nunknown new ID card country. In this scenario, Foundation Models (FM) trained\non huge datasets can help to improve generalisation capabilities. This work\nintends to improve and benchmark the capabilities of FM and how to use them to\nadapt the generalisation on PAD of ID Documents. Different test protocols were\nused, considering zero-shot and fine-tuning and two different ID card datasets.\nOne private dataset based on Chilean IDs and one open-set based on three ID\ncountries: Finland, Spain, and Slovakia. Our findings indicate that bona fide\nimages are the key to generalisation."},{"date":"2025-06","title":"Robust Moment Identification for Nonlinear PDEs via a Neural ODE Approach","author":"Shaoxuan Chen, Su Yang, Panayotis G. Kevrekidis, and Wei Zhu","link":"http://arxiv.org/abs/2506.05245v1","abstract":"We propose a data-driven framework for learning reduced-order moment dynamics\nfrom PDE-governed systems using Neural ODEs. In contrast to derivative-based\nmethods like SINDy, which necessitate densely sampled data and are sensitive to\nnoise, our approach based on Neural ODEs directly models moment trajectories,\nenabling robust learning from sparse and potentially irregular time series.\nUsing as an application platform the nonlinear Schr\\\"{o}dinger equation, the\nframework accurately recovers governing moment dynamics when closure is\navailable, even with limited and irregular observations. For systems without\nanalytical closure, we introduce a data-driven coordinate transformation\nstrategy based on Stiefel manifold optimization, enabling the discovery of\nlow-dimensional representations in which the moment dynamics become closed,\nfacilitating interpretable and reliable modeling. We also explore cases where a\nclosure model is not known, such as a Fisher-KPP reaction-diffusion system.\nHere we demonstrate that Neural ODEs can still effectively approximate the\nunclosed moment dynamics and achieve superior extrapolation accuracy compared\nto physical-expert-derived ODE models. This advantage remains robust even under\nsparse and irregular sampling, highlighting the method's robustness in\ndata-limited settings. Our results highlight the Neural ODE framework as a\npowerful and flexible tool for learning interpretable, low-dimensional moment\ndynamics in complex PDE-governed systems."},{"date":"2025-06","title":"Learning Theory of Decentralized Robust Kernel-Based Learning Algorithm","author":"Zhan Yu","link":"http://arxiv.org/abs/2506.05215v1","abstract":"We propose a new decentralized robust kernel-based learning algorithm within\nthe framework of reproducing kernel Hilbert space (RKHS) by utilizing a\nnetworked system that can be represented as a connected graph. The robust loss\nfunction $\\mathcal{L}_\\sigma$ induced by a windowing function $W$ and a\nrobustness scaling parameter $\\sigma>0$, can encompass a broad spectrum of\nrobust losses. Consequently, the proposed algorithm effectively provides a\nunified decentralized learning framework for robust regression, which\nfundamentally differs from the existing distributed robust kernel learning\nschemes, all of which are divide-and-conquer based. We rigorously establish the\nlearning theory and offer a comprehensive convergence analysis for the\nalgorithm. We show each local robust estimator generated from the decentralized\nalgorithm can be utilized to approximate the regression function. Based on\nkernel-based integral operator techniques, we derive general high confidence\nconvergence bounds for each local approximating sequence in terms of the mean\nsquare distance, RKHS norm, and generalization error, respectively. Moreover,\nwe provide rigorous selection rules for local sample size and show that, under\nproperly selected step size and scaling parameter $\\sigma$, the decentralized\nrobust algorithm can achieve optimal learning rates (up to logarithmic factors)\nin both norms. The parameter $\\sigma$ is shown to be essential for enhancing\nrobustness while also ensuring favorable convergence behavior. The intrinsic\nconnection among decentralization, sample selection, robustness of the\nalgorithm, and its convergence is clearly reflected."},{"date":"2025-06","title":"Membership Inference Attacks on Sequence Models","author":"Lorenzo Rossi, Michael Aerni, Jie Zhang, and Florian Tram\u00e8r","link":"http://arxiv.org/abs/2506.05126v1","abstract":"Sequence models, such as Large Language Models (LLMs) and autoregressive\nimage generators, have a tendency to memorize and inadvertently leak sensitive\ninformation. While this tendency has critical legal implications, existing\ntools are insufficient to audit the resulting risks. We hypothesize that those\ntools' shortcomings are due to mismatched assumptions. Thus, we argue that\neffectively measuring privacy leakage in sequence models requires leveraging\nthe correlations inherent in sequential generation. To illustrate this, we\nadapt a state-of-the-art membership inference attack to explicitly model\nwithin-sequence correlations, thereby demonstrating how a strong existing\nattack can be naturally extended to suit the structure of sequence models.\nThrough a case study, we show that our adaptations consistently improve the\neffectiveness of memorization audits without introducing additional\ncomputational costs. Our work hence serves as an important stepping stone\ntoward reliable memorization audits for large sequence models."},{"date":"2025-06","title":"Practical Manipulation Model for Robust Deepfake Detection","author":"Benedikt Hopf, and Radu Timofte","link":"http://arxiv.org/abs/2506.05119v1","abstract":"Modern deepfake detection models have achieved strong performance even on the\nchallenging cross-dataset task. However, detection performance under non-ideal\nconditions remains very unstable, limiting success on some benchmark datasets\nand making it easy to circumvent detection. Inspired by the move to a more\nreal-world degradation model in the area of image super-resolution, we have\ndeveloped a Practical Manipulation Model (PMM) that covers a larger set of\npossible forgeries. We extend the space of pseudo-fakes by using Poisson\nblending, more diverse masks, generator artifacts, and distractors.\nAdditionally, we improve the detectors' generality and robustness by adding\nstrong degradations to the training images. We demonstrate that these changes\nnot only significantly enhance the model's robustness to common image\ndegradations but also improve performance on standard benchmark datasets.\nSpecifically, we show clear increases of $3.51\\%$ and $6.21\\%$ AUC on the DFDC\nand DFDCP datasets, respectively, over the s-o-t-a LAA backbone. Furthermore,\nwe highlight the lack of robustness in previous detectors and our improvements\nin this regard. Code can be found at https://github.com/BenediktHopf/PMM"},{"date":"2025-06","title":"Identifying and Understanding Cross-Class Features in Adversarial Training","author":"Zeming Wei, Yiwen Guo, and Yisen Wang","link":"http://arxiv.org/abs/2506.05032v1","abstract":"Adversarial training (AT) has been considered one of the most effective\nmethods for making deep neural networks robust against adversarial attacks,\nwhile the training mechanisms and dynamics of AT remain open research problems.\nIn this paper, we present a novel perspective on studying AT through the lens\nof class-wise feature attribution. Specifically, we identify the impact of a\nkey family of features on AT that are shared by multiple classes, which we call\ncross-class features. These features are typically useful for robust\nclassification, which we offer theoretical evidence to illustrate through a\nsynthetic data model. Through systematic studies across multiple model\narchitectures and settings, we find that during the initial stage of AT, the\nmodel tends to learn more cross-class features until the best robustness\ncheckpoint. As AT further squeezes the training robust loss and causes robust\noverfitting, the model tends to make decisions based on more class-specific\nfeatures. Based on these discoveries, we further provide a unified view of two\nexisting properties of AT, including the advantage of soft-label training and\nrobust overfitting. Overall, these insights refine the current understanding of\nAT mechanisms and provide new perspectives on studying them. Our code is\navailable at https://github.com/PKU-ML/Cross-Class-Features-AT."},{"date":"2025-06","title":"Attack Effect Model based Malicious Behavior Detection","author":"Limin Wang, Lei Bu, Muzimiao Zhang, Shihong Cang, and Kai Ye","link":"http://arxiv.org/abs/2506.05001v1","abstract":"Traditional security detection methods face three key challenges: inadequate\ndata collection that misses critical security events, resource-intensive\nmonitoring systems, and poor detection algorithms with high false positive\nrates. We present FEAD (Focus-Enhanced Attack Detection), a framework that\naddresses these issues through three innovations: (1) an attack model-driven\napproach that extracts security-critical monitoring items from online attack\nreports for comprehensive coverage; (2) efficient task decomposition that\noptimally distributes monitoring across existing collectors to minimize\noverhead; and (3) locality-aware anomaly analysis that leverages the clustering\nbehavior of malicious activities in provenance graphs to improve detection\naccuracy. Evaluations demonstrate FEAD achieves 8.23% higher F1-score than\nexisting solutions with only 5.4% overhead, confirming that focus-based designs\nsignificantly enhance detection performance."},{"date":"2025-06","title":"Robustness as Architecture: Designing IQA Models to Withstand Adversarial Perturbations","author":"Igor Meleshin, Anna Chistyakova, Anastasia Antsiferova, and Dmitriy Vatolin","link":"http://arxiv.org/abs/2506.04951v1","abstract":"Image Quality Assessment (IQA) models are increasingly relied upon to\nevaluate image quality in real-world systems -- from compression and\nenhancement to generation and streaming. Yet their adoption brings a\nfundamental risk: these models are inherently unstable. Adversarial\nmanipulations can easily fool them, inflating scores and undermining trust.\nTraditionally, such vulnerabilities are addressed through data-driven defenses\n-- adversarial retraining, regularization, or input purification. But what if\nthis is the wrong lens? What if robustness in perceptual models is not\nsomething to learn but something to design? In this work, we propose a\nprovocative idea: robustness as an architectural prior. Rather than training\nmodels to resist perturbations, we reshape their internal structure to suppress\nsensitivity from the ground up. We achieve this by enforcing orthogonal\ninformation flow, constraining the network to norm-preserving operations -- and\nfurther stabilizing the system through pruning and fine-tuning. The result is a\nrobust IQA architecture that withstands adversarial attacks without requiring\nadversarial training or significant changes to the original model. This\napproach suggests a shift in perspective: from optimizing robustness through\ndata to engineering it through design."},{"date":"2025-06","title":"Towards a Multi-Agent Simulation of Cyber-attackers and Cyber-defenders Battles","author":"Julien Soul\u00e9, Jean-Paul Jamont, Michel Occello, Paul Th\u00e9ron, and Louis-Marie Traonouez","link":"http://arxiv.org/abs/2506.04849v1","abstract":"As cyber-attacks show to be more and more complex and coordinated,\ncyber-defenders strategy through multi-agent approaches could be key to tackle\nagainst cyber-attacks as close as entry points in a networked system. This\npaper presents a Markovian modeling and implementation through a simulator of\nfighting cyber-attacker agents and cyber-defender agents deployed on host\nnetwork nodes. It aims to provide an experimental framework to implement\nrealistically based coordinated cyber-attack scenarios while assessing\ncyber-defenders dynamic organizations. We abstracted network nodes by sets of\nproperties including agents' ones. Actions applied by agents model how the\nnetwork reacts depending in a given state and what properties are to change.\nCollective choice of the actions brings the whole environment closer or farther\nfrom respective cyber-attackers and cyber-defenders goals. Using the simulator,\nwe implemented a realistically inspired scenario with several behavior\nimplementation approaches for cyber-defenders and cyber-attackers."},{"date":"2025-06","title":"On Automating Security Policies with Contemporary LLMs","author":"Pablo Fern\u00e1ndez Saura, K. R. Jayaram, Vatche Isahagian, Jorge Bernal Bernab\u00e9, and Antonio Skarmeta","link":"http://arxiv.org/abs/2506.04838v1","abstract":"The complexity of modern computing environments and the growing\nsophistication of cyber threats necessitate a more robust, adaptive, and\nautomated approach to security enforcement. In this paper, we present a\nframework leveraging large language models (LLMs) for automating attack\nmitigation policy compliance through an innovative combination of in-context\nlearning and retrieval-augmented generation (RAG). We begin by describing how\nour system collects and manages both tool and API specifications, storing them\nin a vector database to enable efficient retrieval of relevant information. We\nthen detail the architectural pipeline that first decomposes high-level\nmitigation policies into discrete tasks and subsequently translates each task\ninto a set of actionable API calls. Our empirical evaluation, conducted using\npublicly available CTI policies in STIXv2 format and Windows API documentation,\ndemonstrates significant improvements in precision, recall, and F1-score when\nemploying RAG compared to a non-RAG baseline."},{"date":"2025-06","title":"Fool the Stoplight: Realistic Adversarial Patch Attacks on Traffic Light Detectors","author":"Svetlana Pavlitska, Jamie Robb, Nikolai Polley, Melih Yazgan, and J. Marius Z\u00f6llner","link":"http://arxiv.org/abs/2506.04823v1","abstract":"Realistic adversarial attacks on various camera-based perception tasks of\nautonomous vehicles have been successfully demonstrated so far. However, only a\nfew works considered attacks on traffic light detectors. This work shows how\nCNNs for traffic light detection can be attacked with printed patches. We\npropose a threat model, where each instance of a traffic light is attacked with\na patch placed under it, and describe a training strategy. We demonstrate\nsuccessful adversarial patch attacks in universal settings. Our experiments\nshow realistic targeted red-to-green label-flipping attacks and attacks on\npictogram classification. Finally, we perform a real-world evaluation with\nprinted patches and demonstrate attacks in the lab settings with a mobile\ntraffic light for construction sites and in a test area with stationary traffic\nlights. Our code is available at\nhttps://github.com/KASTEL-MobilityLab/attacks-on-traffic-light-detection."},{"date":"2025-06","title":"LogicPuzzleRL: Cultivating Robust Mathematical Reasoning in LLMs via Reinforcement Learning","author":"Zhen Hao Wong, Jingwen Deng, Runming He, Zirong Chen, Qijie You, Hejun Dong, Hao Liang, Chengyu Shen, Bin Cui, and Wentao Zhang","link":"http://arxiv.org/abs/2506.04821v1","abstract":"Large language models (LLMs) excel at many supervised tasks but often\nstruggle with structured reasoning in unfamiliar settings. This discrepancy\nsuggests that standard fine-tuning pipelines may instill narrow,\ndomain-specific heuristics rather than fostering general-purpose thinking\nstrategies. In this work, we propose a \"play to learn\" framework that\nfine-tunes LLMs through reinforcement learning on a suite of seven custom logic\npuzzles, each designed to cultivate distinct reasoning skills such as\nconstraint propagation, spatial consistency, and symbolic deduction. Using a\nreinforcement learning setup with verifiable rewards, models receive binary\nfeedback based on puzzle correctness, encouraging iterative, hypothesis-driven\nproblem solving. We demonstrate that this training approach significantly\nimproves out-of-distribution performance on a range of mathematical benchmarks,\nespecially for mid-difficulty problems that require multi-step reasoning.\nAnalyses across problem categories and difficulty levels reveal that puzzle\ntraining promotes transferable reasoning routines, strengthening algebraic\nmanipulation, geometric inference, and combinatorial logic, while offering\nlimited gains on rote or highly specialized tasks. These findings show that\nreinforcement learning over logic puzzles reshapes the internal reasoning of\nLLMs, enabling more robust and compositional generalization without relying on\ntask-specific symbolic tools."},{"date":"2025-06","title":"SRD: Reinforcement-Learned Semantic Perturbation for Backdoor Defense in VLMs","author":"Shuhan Xu, Siyuan Liang, Hongling Zheng, Yong Luo, Aishan Liu, and Dacheng Tao","link":"http://arxiv.org/abs/2506.04743v1","abstract":"Vision-Language Models (VLMs) have achieved remarkable performance in image\ncaptioning, but recent studies show they are vulnerable to backdoor attacks.\nAttackers can inject imperceptible perturbations-such as local pixel triggers\nor global semantic phrases-into the training data, causing the model to\ngenerate malicious, attacker-controlled captions for specific inputs. These\nattacks are hard to detect and defend due to their stealthiness and cross-modal\nnature. By analyzing attack samples, we identify two key vulnerabilities: (1)\nabnormal attention concentration on specific image regions, and (2) semantic\ndrift and incoherence in generated captions. To counter this, we propose\nSemantic Reward Defense (SRD), a reinforcement learning framework that\nmitigates backdoor behavior without prior knowledge of triggers. SRD uses a\nDeep Q-Network to learn policies for applying discrete perturbations (e.g.,\nocclusion, color masking) to sensitive image regions, aiming to disrupt the\nactivation of malicious pathways. We design a semantic fidelity score as the\nreward signal, which jointly evaluates semantic consistency and linguistic\nfluency of the output, guiding the agent toward generating robust yet faithful\ncaptions. Experiments across mainstream VLMs and datasets show SRD reduces\nattack success rates to 5.6%, while preserving caption quality on clean inputs\nwith less than 10% performance drop. SRD offers a trigger-agnostic,\ninterpretable defense paradigm against stealthy backdoor threats in multimodal\ngenerative models."},{"date":"2025-06","title":"Robust Few-Shot Vision-Language Model Adaptation","author":"Hanxin Wang, Tian Liu, and Shu Kong","link":"http://arxiv.org/abs/2506.04713v1","abstract":"Pretrained VLMs achieve strong performance on downstream tasks when adapted\nwith just a few labeled examples. As the adapted models inevitably encounter\nout-of-distribution (OOD) test data that deviates from the in-distribution (ID)\ntask-specific training data, enhancing OOD generalization in few-shot\nadaptation is critically important. We study robust few-shot VLM adaptation,\naiming to increase both ID and OOD accuracy. By comparing different adaptation\nmethods (e.g., prompt tuning, linear probing, contrastive finetuning, and full\nfinetuning), we uncover three key findings: (1) finetuning with proper\nhyperparameters significantly outperforms the popular VLM adaptation methods\nprompt tuning and linear probing; (2) visual encoder-only finetuning achieves\nbetter efficiency and accuracy than contrastively finetuning both visual and\ntextual encoders; (3) finetuning the top layers of the visual encoder provides\nthe best balance between ID and OOD accuracy. Building on these findings, we\npropose partial finetuning of the visual encoder empowered with two simple\naugmentation techniques: (1) retrieval augmentation which retrieves\ntask-relevant data from the VLM's pretraining dataset to enhance adaptation,\nand (2) adversarial perturbation which promotes robustness during finetuning.\nResults show that the former/latter boosts OOD/ID accuracy while slightly\nsacrificing the ID/OOD accuracy. Yet, perhaps understandably, naively combining\nthe two does not maintain their best OOD/ID accuracy. We address this dilemma\nwith the developed SRAPF, Stage-wise Retrieval Augmentation-based Adversarial\nPartial Finetuning. SRAPF consists of two stages: (1) partial finetuning the\nvisual encoder using both ID and retrieved data, and (2) adversarial partial\nfinetuning with few-shot ID data. Extensive experiments demonstrate that SRAPF\nachieves the state-of-the-art ID and OOD accuracy on the ImageNet OOD\nbenchmarks."},{"date":"2025-06","title":"MMRefine: Unveiling the Obstacles to Robust Refinement in Multimodal Large Language Models","author":"Gio Paik, Geewook Kim, and Jinbae Im","link":"http://arxiv.org/abs/2506.04688v1","abstract":"This paper introduces MMRefine, a MultiModal Refinement benchmark designed to\nevaluate the error refinement capabilities of Multimodal Large Language Models\n(MLLMs). As the emphasis shifts toward enhancing reasoning during inference,\nMMRefine provides a framework that evaluates MLLMs' abilities to detect and\ncorrect errors across six distinct scenarios beyond just comparing final\naccuracy before and after refinement. Furthermore, the benchmark analyzes the\nrefinement performance by categorizing errors into six error types. Experiments\nwith various open and closed MLLMs reveal bottlenecks and factors impeding\nrefinement performance, highlighting areas for improvement in effective\nreasoning enhancement. Our code and dataset are publicly available at\nhttps://github.com/naver-ai/MMRefine."},{"date":"2025-06","title":"Scaling Laws for Robust Comparison of Open Foundation Language-Vision Models and Datasets","author":"Marianna Nezhurina, Tomer Porian, Giovanni Pucceti, Tommie Kerssies, Romain Beaumont, Mehdi Cherti, and Jenia Jitsev","link":"http://arxiv.org/abs/2506.04598v1","abstract":"In studies of transferable learning, scaling laws are obtained for various\nimportant foundation models to predict their properties and performance at\nlarger scales. We show here how scaling law derivation can also be used for\nmodel and dataset comparison, allowing to decide which procedure is to be\npreferred for pre-training. For the first time, full scaling laws based on\ndense measurements across a wide span of model and samples seen scales are\nderived for two important language-vision learning procedures, CLIP and MaMMUT,\nthat use either contrastive only or contrastive and captioning text generative\nloss. Ensuring sufficient prediction accuracy for held out points, we use\nderived scaling laws to compare both models, obtaining evidence for MaMMUT's\nstronger improvement with scale and better sample efficiency than standard\nCLIP. To strengthen validity of the comparison, we show scaling laws for\nvarious downstream tasks, classification, retrieval, and segmentation, and for\ndifferent open datasets, DataComp, DFN and Re-LAION, observing consistently the\nsame trends. We show that comparison can also be performed when deriving\nscaling laws with a constant learning rate schedule, reducing compute cost.\nAccurate derivation of scaling laws provides thus means to perform model and\ndataset comparison across scale spans, avoiding misleading conclusions based on\nmeasurements from single reference scales only, paving the road for systematic\ncomparison and improvement of open foundation models and datasets for their\ncreation. We release all the pre-trained models with their intermediate\ncheckpoints, including openMaMMUT-L/14, which achieves $80.3\\%$ zero-shot\nImageNet-1k accuracy, trained on 12.8B samples from DataComp-1.4B. Code for\nreproducing experiments in the paper and raw experiments data can be found at\nhttps://github.com/LAION-AI/scaling-laws-for-comparison."},{"date":"2025-06","title":"SUCEA: Reasoning-Intensive Retrieval for Adversarial Fact-checking through Claim Decomposition and Editing","author":"Hongjun Liu, Yilun Zhao, Arman Cohan, and Chen Zhao","link":"http://arxiv.org/abs/2506.04583v1","abstract":"Automatic fact-checking has recently received more attention as a means of\ncombating misinformation. Despite significant advancements, fact-checking\nsystems based on retrieval-augmented language models still struggle to tackle\nadversarial claims, which are intentionally designed by humans to challenge\nfact-checking systems. To address these challenges, we propose a training-free\nmethod designed to rephrase the original claim, making it easier to locate\nsupporting evidence. Our modular framework, SUCEA, decomposes the task into\nthree steps: 1) Claim Segmentation and Decontextualization that segments\nadversarial claims into independent sub-claims; 2) Iterative Evidence Retrieval\nand Claim Editing that iteratively retrieves evidence and edits the subclaim\nbased on the retrieved evidence; 3) Evidence Aggregation and Label Prediction\nthat aggregates all retrieved evidence and predicts the entailment label.\nExperiments on two challenging fact-checking datasets demonstrate that our\nframework significantly improves on both retrieval and entailment label\naccuracy, outperforming four strong claim-decomposition-based baselines."},{"date":"2025-06","title":"BESA: Boosting Encoder Stealing Attack with Perturbation Recovery","author":"Xuhao Ren, Haotian Liang, Yajie Wang, Chuan Zhang, Zehui Xiong, and Liehuang Zhu","link":"http://arxiv.org/abs/2506.04556v1","abstract":"To boost the encoder stealing attack under the perturbation-based defense\nthat hinders the attack performance, we propose a boosting encoder stealing\nattack with perturbation recovery named BESA. It aims to overcome\nperturbation-based defenses. The core of BESA consists of two modules:\nperturbation detection and perturbation recovery, which can be combined with\ncanonical encoder stealing attacks. The perturbation detection module utilizes\nthe feature vectors obtained from the target encoder to infer the defense\nmechanism employed by the service provider. Once the defense mechanism is\ndetected, the perturbation recovery module leverages the well-designed\ngenerative model to restore a clean feature vector from the perturbed one.\nThrough extensive evaluations based on various datasets, we demonstrate that\nBESA significantly enhances the surrogate encoder accuracy of existing encoder\nstealing attacks by up to 24.63\\% when facing state-of-the-art defenses and\ncombinations of multiple defenses."},{"date":"2025-06","title":"Neurosymbolic Artificial Intelligence for Robust Network Intrusion Detection: From Scratch to Transfer Learning","author":"Huynh T. T. Tran, Jacob Sander, Achraf Cohen, Brian Jalaian, and Nathaniel D. Bastian","link":"http://arxiv.org/abs/2506.04454v1","abstract":"Network Intrusion Detection Systems (NIDS) play a vital role in protecting\ndigital infrastructures against increasingly sophisticated cyber threats. In\nthis paper, we extend ODXU, a Neurosymbolic AI (NSAI) framework that integrates\ndeep embedded clustering for feature extraction, symbolic reasoning using\nXGBoost, and comprehensive uncertainty quantification (UQ) to enhance\nrobustness, interpretability, and generalization in NIDS. The extended ODXU\nincorporates score-based methods (e.g., Confidence Scoring, Shannon Entropy)\nand metamodel-based techniques, including SHAP values and Information Gain, to\nassess the reliability of predictions. Experimental results on the CIC-IDS-2017\ndataset show that ODXU outperforms traditional neural models across six\nevaluation metrics, including classification accuracy and false omission rate.\nWhile transfer learning has seen widespread adoption in fields such as computer\nvision and natural language processing, its potential in cybersecurity has not\nbeen thoroughly explored. To bridge this gap, we develop a transfer learning\nstrategy that enables the reuse of a pre-trained ODXU model on a different\ndataset. Our ablation study on ACI-IoT-2023 demonstrates that the optimal\ntransfer configuration involves reusing the pre-trained autoencoder, retraining\nthe clustering module, and fine-tuning the XGBoost classifier, and outperforms\ntraditional neural models when trained with as few as 16,000 samples\n(approximately 50% of the training data). Additionally, results show that\nmetamodel-based UQ methods consistently outperform score-based approaches on\nboth datasets."},{"date":"2025-06","title":"Gradient Inversion Attacks on Parameter-Efficient Fine-Tuning","author":"Hasin Us Sami, Swapneel Sen, Amit K. Roy-Chowdhury, Srikanth V. Krishnamurthy, and Basak Guler","link":"http://arxiv.org/abs/2506.04453v1","abstract":"Federated learning (FL) allows multiple data-owners to collaboratively train\nmachine learning models by exchanging local gradients, while keeping their\nprivate data on-device. To simultaneously enhance privacy and training\nefficiency, recently parameter-efficient fine-tuning (PEFT) of large-scale\npretrained models has gained substantial attention in FL. While keeping a\npretrained (backbone) model frozen, each user fine-tunes only a few lightweight\nmodules to be used in conjunction, to fit specific downstream applications.\nAccordingly, only the gradients with respect to these lightweight modules are\nshared with the server. In this work, we investigate how the privacy of the\nfine-tuning data of the users can be compromised via a malicious design of the\npretrained model and trainable adapter modules. We demonstrate gradient\ninversion attacks on a popular PEFT mechanism, the adapter, which allow an\nattacker to reconstruct local data samples of a target user, using only the\naccessible adapter gradients. Via extensive experiments, we demonstrate that a\nlarge batch of fine-tuning images can be retrieved with high fidelity. Our\nattack highlights the need for privacy-preserving mechanisms for PEFT, while\nopening up several future directions. Our code is available at\nhttps://github.com/info-ucr/PEFTLeak."},{"date":"2025-06","title":"Through the Stealth Lens: Rethinking Attacks and Defenses in RAG","author":"Sarthak Choudhary, Nils Palumbo, Ashish Hooda, Krishnamurthy Dj Dvijotham, and Somesh Jha","link":"http://arxiv.org/abs/2506.04390v1","abstract":"Retrieval-augmented generation (RAG) systems are vulnerable to attacks that\ninject poisoned passages into the retrieved set, even at low corruption rates.\nWe show that existing attacks are not designed to be stealthy, allowing\nreliable detection and mitigation. We formalize stealth using a\ndistinguishability-based security game. If a few poisoned passages are designed\nto control the response, they must differentiate themselves from benign ones,\ninherently compromising stealth. This motivates the need for attackers to\nrigorously analyze intermediate signals involved in\ngeneration$\\unicode{x2014}$such as attention patterns or next-token probability\ndistributions$\\unicode{x2014}$to avoid easily detectable traces of\nmanipulation. Leveraging attention patterns, we propose a passage-level\nscore$\\unicode{x2014}$the Normalized Passage Attention\nScore$\\unicode{x2014}$used by our Attention-Variance Filter algorithm to\nidentify and filter potentially poisoned passages. This method mitigates\nexisting attacks, improving accuracy by up to $\\sim 20 \\%$ over baseline\ndefenses. To probe the limits of attention-based defenses, we craft stealthier\nadaptive attacks that obscure such traces, achieving up to $35 \\%$ attack\nsuccess rate, and highlight the challenges in improving stealth."},{"date":"2025-06","title":"TRiSM for Agentic AI: A Review of Trust, Risk, and Security Management in LLM-based Agentic Multi-Agent Systems","author":"Shaina Raza, Ranjan Sapkota, Manoj Karkee, and Christos Emmanouilidis","link":"http://arxiv.org/abs/2506.04133v1","abstract":"Agentic AI systems, built on large language models (LLMs) and deployed in\nmulti-agent configurations, are redefining intelligent autonomy, collaboration\nand decision-making across enterprise and societal domains. This review\npresents a structured analysis of Trust, Risk, and Security Management (TRiSM)\nin the context of LLM-based agentic multi-agent systems (AMAS). We begin by\nexamining the conceptual foundations of agentic AI, its architectural\ndifferences from traditional AI agents, and the emerging system designs that\nenable scalable, tool-using autonomy. The TRiSM in the agentic AI framework is\nthen detailed through four pillars governance, explainability, ModelOps, and\nprivacy/security each contextualized for agentic LLMs. We identify unique\nthreat vectors and introduce a comprehensive risk taxonomy for the agentic AI\napplications, supported by case studies illustrating real-world\nvulnerabilities. Furthermore, the paper also surveys trust-building mechanisms,\ntransparency and oversight techniques, and state-of-the-art explainability\nstrategies in distributed LLM agent systems. Additionally, metrics for\nevaluating trust, interpretability, and human-centered performance are reviewed\nalongside open benchmarking challenges. Security and privacy are addressed\nthrough encryption, adversarial defense, and compliance with evolving AI\nregulations. The paper concludes with a roadmap for responsible agentic AI,\nproposing research directions to align emerging multi-agent systems with robust\nTRiSM principles for safe, accountable, and transparent deployment."},{"date":"2025-06","title":"Privacy and Security Threat for OpenAI GPTs","author":"Wei Wenying, Zhao Kaifa, Xue Lei, and Fan Ming","link":"http://arxiv.org/abs/2506.04036v1","abstract":"Large language models (LLMs) demonstrate powerful information handling\ncapabilities and are widely integrated into chatbot applications. OpenAI\nprovides a platform for developers to construct custom GPTs, extending\nChatGPT's functions and integrating external services. Since its release in\nNovember 2023, over 3 million custom GPTs have been created. However, such a\nvast ecosystem also conceals security and privacy threats. For developers,\ninstruction leaking attacks threaten the intellectual property of instructions\nin custom GPTs through carefully crafted adversarial prompts. For users,\nunwanted data access behavior by custom GPTs or integrated third-party services\nraises significant privacy concerns. To systematically evaluate the scope of\nthreats in real-world LLM applications, we develop three phases instruction\nleaking attacks target GPTs with different defense level. Our widespread\nexperiments on 10,000 real-world custom GPTs reveal that over 98.8% of GPTs are\nvulnerable to instruction leaking attacks via one or more adversarial prompts,\nand half of the remaining GPTs can also be attacked through multiround\nconversations. We also developed a framework to assess the effectiveness of\ndefensive strategies and identify unwanted behaviors in custom GPTs. Our\nfindings show that 77.5% of custom GPTs with defense strategies are vulnerable\nto basic instruction leaking attacks. Additionally, we reveal that 738 custom\nGPTs collect user conversational information, and identified 8 GPTs exhibiting\ndata access behaviors that are unnecessary for their intended functionalities.\nOur findings raise awareness among GPT developers about the importance of\nintegrating specific defensive strategies in their instructions and highlight\nusers' concerns about data privacy when using LLM-based applications."},{"date":"2025-06","title":"RAID: A Dataset for Testing the Adversarial Robustness of AI-Generated Image Detectors","author":"Hicham Eddoubi, Jonas Ricker, Federico Cocchi, Lorenzo Baraldi, Angelo Sotgiu, Maura Pintor, Marcella Cornia, Lorenzo Baraldi, Asja Fischer, Rita Cucchiara, and Battista Biggio","link":"http://arxiv.org/abs/2506.03988v2","abstract":"AI-generated images have reached a quality level at which humans are\nincapable of reliably distinguishing them from real images. To counteract the\ninherent risk of fraud and disinformation, the detection of AI-generated images\nis a pressing challenge and an active research topic. While many of the\npresented methods claim to achieve high detection accuracy, they are usually\nevaluated under idealized conditions. In particular, the adversarial robustness\nis often neglected, potentially due to a lack of awareness or the substantial\neffort required to conduct a comprehensive robustness analysis. In this work,\nwe tackle this problem by providing a simpler means to assess the robustness of\nAI-generated image detectors. We present RAID (Robust evaluation of\nAI-generated image Detectors), a dataset of 72k diverse and highly transferable\nadversarial examples. The dataset is created by running attacks against an\nensemble of seven state-of-the-art detectors and images generated by four\ndifferent text-to-image models. Extensive experiments show that our methodology\ngenerates adversarial images that transfer with a high success rate to unseen\ndetectors, which can be used to quickly provide an approximate yet still\nreliable estimate of a detector's adversarial robustness. Our findings indicate\nthat current state-of-the-art AI-generated image detectors can be easily\ndeceived by adversarial examples, highlighting the critical need for the\ndevelopment of more robust methods. We release our dataset at\nhttps://huggingface.co/datasets/aimagelab/RAID and evaluation code at\nhttps://github.com/pralab/RAID."},{"date":"2025-06","title":"Causality-Aware Contrastive Learning for Robust Multivariate Time-Series Anomaly Detection","author":"HyunGi Kim, Jisoo Mok, Dongjun Lee, Jaihyun Lew, Sungjae Kim, and Sungroh Yoon","link":"http://arxiv.org/abs/2506.03964v1","abstract":"Utilizing the complex inter-variable causal relationships within multivariate\ntime-series provides a promising avenue toward more robust and reliable\nmultivariate time-series anomaly detection (MTSAD) but remains an underexplored\narea of research. This paper proposes Causality-Aware contrastive learning for\nRObust multivariate Time-Series (CAROTS), a novel MTSAD pipeline that\nincorporates the notion of causality into contrastive learning. CAROTS employs\ntwo data augmentors to obtain causality-preserving and -disturbing samples that\nserve as a wide range of normal variations and synthetic anomalies,\nrespectively. With causality-preserving and -disturbing samples as positives\nand negatives, CAROTS performs contrastive learning to train an encoder whose\nlatent space separates normal and abnormal samples based on causality.\nMoreover, CAROTS introduces a similarity-filtered one-class contrastive loss\nthat encourages the contrastive learning process to gradually incorporate more\nsemantically diverse samples with common causal relationships. Extensive\nexperiments on five real-world and two synthetic datasets validate that the\nintegration of causal relationships endows CAROTS with improved MTSAD\ncapabilities. The code is available at https://github.com/kimanki/CAROTS."},{"date":"2025-06","title":"DiffCAP: Diffusion-based Cumulative Adversarial Purification for Vision Language Models","author":"Jia Fu, Yongtao Wu, Yihang Chen, Kunyu Peng, Xiao Zhang, Volkan Cevher, Sepideh Pashami, and Anders Holst","link":"http://arxiv.org/abs/2506.03933v1","abstract":"Vision Language Models (VLMs) have shown remarkable capabilities in\nmultimodal understanding, yet their susceptibility to perturbations poses a\nsignificant threat to their reliability in real-world applications. Despite\noften being imperceptible to humans, these perturbations can drastically alter\nmodel outputs, leading to erroneous interpretations and decisions. This paper\nintroduces DiffCAP, a novel diffusion-based purification strategy that can\neffectively neutralize adversarial corruptions in VLMs. We observe that adding\nminimal noise to an adversarially corrupted image significantly alters its\nlatent embedding with respect to VLMs. Building on this insight, DiffCAP\ncumulatively injects random Gaussian noise into adversarially perturbed input\ndata. This process continues until the embeddings of two consecutive noisy\nimages reach a predefined similarity threshold, indicating a potential approach\nto neutralize the adversarial effect. Subsequently, a pretrained diffusion\nmodel is employed to denoise the stabilized image, recovering a clean\nrepresentation suitable for the VLMs to produce an output. Through extensive\nexperiments across six datasets with three VLMs under varying attack strengths\nin three task scenarios, we show that DiffCAP consistently outperforms existing\ndefense techniques by a substantial margin. Notably, DiffCAP significantly\nreduces both hyperparameter tuning complexity and the required diffusion time,\nthereby accelerating the denoising process. Equipped with strong theoretical\nand empirical support, DiffCAP provides a robust and practical solution for\nsecurely deploying VLMs in adversarial environments."},{"date":"2025-06","title":"RadialRouter: Structured Representation for Efficient and Robust Large Language Models Routing","author":"Ruihan Jin, Pengpeng Shao, Zhengqi Wen, Jinyang Wu, Mingkuan Feng, Shuai Zhang, and Jianhua Tao","link":"http://arxiv.org/abs/2506.03880v1","abstract":"The rapid advancements in large language models (LLMs) have led to the\nemergence of routing techniques, which aim to efficiently select the optimal\nLLM from diverse candidates to tackle specific tasks, optimizing performance\nwhile reducing costs. Current LLM routing methods are limited in effectiveness\ndue to insufficient exploration of the intrinsic connection between user\nqueries and the characteristics of LLMs. To address this issue, in this paper,\nwe present RadialRouter, a novel framework for LLM routing which employs a\nlightweight Transformer-based backbone with a radial structure named\nRadialFormer to articulate the query-LLMs relationship. The optimal LLM\nselection is performed based on the final states of RadialFormer. The pipeline\nis further refined by an objective function that combines Kullback-Leibler\ndivergence with the query-query contrastive loss to enhance robustness.\nExperimental results on RouterBench show that RadialRouter significantly\noutperforms existing routing methods by 9.2\\% and 5.8\\% in the Balance and Cost\nFirst scenarios, respectively. Additionally, its adaptability toward different\nperformance-cost trade-offs and the dynamic LLM pool demonstrates practical\napplication potential."},{"date":"2025-06","title":"Evaluating Apple Intelligence's Writing Tools for Privacy Against Large Language Model-Based Inference Attacks: Insights from Early Datasets","author":"Mohd. Farhan Israk Soumik, Syed Mhamudul Hasan, and Abdur R. Shahid","link":"http://arxiv.org/abs/2506.03870v1","abstract":"The misuse of Large Language Models (LLMs) to infer emotions from text for\nmalicious purposes, known as emotion inference attacks, poses a significant\nthreat to user privacy. In this paper, we investigate the potential of Apple\nIntelligence's writing tools, integrated across iPhone, iPad, and MacBook, to\nmitigate these risks through text modifications such as rewriting and tone\nadjustment. By developing early novel datasets specifically for this purpose,\nwe empirically assess how different text modifications influence LLM-based\ndetection. This capability suggests strong potential for Apple Intelligence's\nwriting tools as privacy-preserving mechanisms. Our findings lay the groundwork\nfor future adaptive rewriting systems capable of dynamically neutralizing\nsensitive emotional content to enhance user privacy. To the best of our\nknowledge, this research provides the first empirical analysis of Apple\nIntelligence's text-modification tools within a privacy-preservation context\nwith the broader goal of developing on-device, user-centric privacy-preserving\nmechanisms to protect against LLMs-based advanced inference attacks on deployed\nsystems."},{"date":"2025-06","title":"Prediction Inconsistency Helps Achieve Generalizable Detection of Adversarial Examples","author":"Sicong Han, Chenhao Lin, Zhengyu Zhao, Xiyuan Wang, Xinlei He, Qian Li, Cong Wang, Qian Wang, and Chao Shen","link":"http://arxiv.org/abs/2506.03765v1","abstract":"Adversarial detection protects models from adversarial attacks by refusing\nsuspicious test samples. However, current detection methods often suffer from\nweak generalization: their effectiveness tends to degrade significantly when\napplied to adversarially trained models rather than naturally trained ones, and\nthey generally struggle to achieve consistent effectiveness across both\nwhite-box and black-box attack settings. In this work, we observe that an\nauxiliary model, differing from the primary model in training strategy or model\narchitecture, tends to assign low confidence to the primary model's predictions\non adversarial examples (AEs), while preserving high confidence on normal\nexamples (NEs). Based on this discovery, we propose Prediction Inconsistency\nDetector (PID), a lightweight and generalizable detection framework to\ndistinguish AEs from NEs by capturing the prediction inconsistency between the\nprimal and auxiliary models. PID is compatible with both naturally and\nadversarially trained primal models and outperforms four detection methods\nacross 3 white-box, 3 black-box, and 1 mixed adversarial attacks. Specifically,\nPID achieves average AUC scores of 99.29\\% and 99.30\\% on CIFAR-10 when the\nprimal model is naturally and adversarially trained, respectively, and 98.31%\nand 96.81% on ImageNet under the same conditions, outperforming existing SOTAs\nby 4.70%$\\sim$25.46%."},{"date":"2025-06","title":"Dropout-Robust Mechanisms for Differentially Private and Fully Decentralized Mean Estimation","author":"C\u00e9sar Sabater, Sonia Ben Mokhtar, and Jan Ramon","link":"http://arxiv.org/abs/2506.03746v1","abstract":"Achieving differentially private computations in decentralized settings poses\nsignificant challenges, particularly regarding accuracy, communication cost,\nand robustness against information leakage. While cryptographic solutions offer\npromise, they often suffer from high communication overhead or require\ncentralization in the presence of network failures. Conversely, existing fully\ndecentralized approaches typically rely on relaxed adversarial models or\npairwise noise cancellation, the latter suffering from substantial accuracy\ndegradation if parties unexpectedly disconnect. In this work, we propose IncA,\na new protocol for fully decentralized mean estimation, a widely used primitive\nin data-intensive processing. Our protocol, which enforces differential\nprivacy, requires no central orchestration and employs low-variance correlated\nnoise, achieved by incrementally injecting sensitive information into the\ncomputation. First, we theoretically demonstrate that, when no parties\npermanently disconnect, our protocol achieves accuracy comparable to that of a\ncentralized setting-already an improvement over most existing decentralized\ndifferentially private techniques. Second, we empirically show that our use of\nlow-variance correlated noise significantly mitigates the accuracy loss\nexperienced by existing techniques in the presence of dropouts."},{"date":"2025-06","title":"ComRoPE: Scalable and Robust Rotary Position Embedding Parameterized by Trainable Commuting Angle Matrices","author":"Hao Yu, Tangyu Jiang, Shuning Jia, Shannan Yan, Shunning Liu, Haolong Qian, Guanghao Li, Shuting Dong, Huaisong Zhang, and Chun Yuan","link":"http://arxiv.org/abs/2506.03737v1","abstract":"The Transformer architecture has revolutionized various regions since it was\nproposed, and its effectiveness largely depends on the ability to encode\npositional information. Traditional position encoding methods exhibit\nsignificant limitations due to lack of robustness and flexibility of position.\nTherefore, Rotary Positional Encoding (RoPE) was proposed to alleviate these\nissues, which integrates positional information by rotating the embeddings in\nthe attention mechanism. However, RoPE requires manually defined rotation\nmatrices with limited transformation space, constraining the model's capacity.\nIn this work, we propose ComRoPE, which generalizes RoPE by defining it in\nterms of trainable commuting angle matrices. Specifically, we demonstrate that\npairwise commutativity of these matrices is essential for RoPE to achieve\nscalability and positional robustness. We formally define the RoPE Equation,\nwhich is an essential condition that ensures consistent performance with\nposition offsets. Based on the theoretical analysis, we present two types of\ntrainable commuting angle matrices as sufficient solutions to the RoPE\nequation, which significantly improve performance, surpassing the current\nstate-of-the-art method by 1.6% at training resolution and 2.9% at higher\nresolution on the ImageNet-1K dataset. Furthermore, our framework shows\nversatility in generalizing to existing RoPE formulations and offering new\ninsights for future positional encoding research. To ensure reproducibility,\nthe source code and instructions are available at\nhttps://github.com/Longin-Yu/ComRoPE"},{"date":"2025-06","title":"BiXFormer: A Robust Framework for Maximizing Modality Effectiveness in Multi-Modal Semantic Segmentation","author":"Jialei Chen, Xu Zheng, Danda Pani Paudel, Luc Van Gool, Hiroshi Murase, and Daisuke Deguchi","link":"http://arxiv.org/abs/2506.03675v1","abstract":"Utilizing multi-modal data enhances scene understanding by providing\ncomplementary semantic and geometric information. Existing methods fuse\nfeatures or distill knowledge from multiple modalities into a unified\nrepresentation, improving robustness but restricting each modality's ability to\nfully leverage its strengths in different situations. We reformulate\nmulti-modal semantic segmentation as a mask-level classification task and\npropose BiXFormer, which integrates Unified Modality Matching (UMM) and Cross\nModality Alignment (CMA) to maximize modality effectiveness and handle missing\nmodalities. Specifically, BiXFormer first categorizes multi-modal inputs into\nRGB and X, where X represents any non-RGB modalities, e.g., depth, allowing\nseparate processing for each. This design leverages the well-established\npretraining for RGB, while addressing the relative lack of attention to X\nmodalities. Then, we propose UMM, which includes Modality Agnostic Matching\n(MAM) and Complementary Matching (CM). MAM assigns labels to features from all\nmodalities without considering modality differences, leveraging each modality's\nstrengths. CM then reassigns unmatched labels to remaining unassigned features\nwithin their respective modalities, ensuring that each available modality\ncontributes to the final prediction and mitigating the impact of missing\nmodalities. Moreover, to further facilitate UMM, we introduce CMA, which\nenhances the weaker queries assigned in CM by aligning them with optimally\nmatched queries from MAM. Experiments on both synthetic and real-world\nmulti-modal benchmarks demonstrate the effectiveness of our method, achieving\nsignificant improvements in mIoU of +2.75% and +22.74% over the prior arts."},{"date":"2025-06","title":"SubSearch: Robust Estimation and Outlier Detection for Stochastic Block Models via Subgraph Search","author":"Leonardo Martins Bianco, Christine Keribin, and Zacharie Naulet","link":"http://arxiv.org/abs/2506.03657v1","abstract":"Community detection is a fundamental task in graph analysis, with methods\noften relying on fitting models like the Stochastic Block Model (SBM) to\nobserved networks. While many algorithms can accurately estimate SBM parameters\nwhen the input graph is a perfect sample from the model, real-world graphs\nrarely conform to such idealized assumptions. Therefore, robust algorithms are\ncrucial-ones that can recover model parameters even when the data deviates from\nthe assumed distribution. In this work, we propose SubSearch, an algorithm for\nrobustly estimating SBM parameters by exploring the space of subgraphs in\nsearch of one that closely aligns with the model's assumptions. Our approach\nalso functions as an outlier detection method, properly identifying nodes\nresponsible for the graph's deviation from the model and going beyond simple\ntechniques like pruning high-degree nodes. Extensive experiments on both\nsynthetic and real-world datasets demonstrate the effectiveness of our method."},{"date":"2025-06","title":"Robustness of Prompting: Enhancing Robustness of Large Language Models Against Prompting Attacks","author":"Lin Mu, Guowei Chu, Li Ni, Lei Sang, Zhize Wu, Peiquan Jin, and Yiwen Zhang","link":"http://arxiv.org/abs/2506.03627v1","abstract":"Large Language Models (LLMs) have demonstrated remarkable performance across\nvarious tasks by effectively utilizing a prompting strategy. However, they are\nhighly sensitive to input perturbations, such as typographical errors or slight\ncharacter order errors, which can substantially degrade their performance.\nDespite advances in prompting techniques, developing a prompting strategy that\nexplicitly mitigates the negative impact of such perturbations remains an open\nchallenge. To bridge this gap, we propose Robustness of Prompting (RoP), a\nnovel prompting strategy specifically designed to enhance the robustness of\nLLMs. RoP consists of two stages: Error Correction and Guidance. In the Error\nCorrection stage, RoP applies diverse perturbation methods to generate\nadversarial examples, which are then used to construct prompts that\nautomatically correct input errors. In the Guidance stage, RoP generates an\noptimal guidance prompting based on the corrected input, steering the model\ntoward more robust and accurate inferences. Through comprehensive experiments\nspanning arithmetic, commonsense, and logical reasoning tasks, we demonstrate\nthat RoP significantly improves LLMs' robustness against adversarial\nperturbations. Notably, it maintains model accuracy with only minimal\ndegradation compared to clean input scenarios, thereby establishing RoP as a\npractical and effective approach for enhancing LLM robustness in real-world\napplications."},{"date":"2025-06","title":"ViTSGMM: A Robust Semi-Supervised Image Recognition Network Using Sparse Labels","author":"Rui Yann, and Xianglei Xing","link":"http://arxiv.org/abs/2506.03582v1","abstract":"We present ViTSGMM, an image recognition network that leverages\nsemi-supervised learning in a highly efficient manner. Existing works often\nrely on complex training techniques and architectures, while their\ngeneralization ability when dealing with extremely limited labeled data remains\nto be improved. To address these limitations, we construct a hierarchical\nmixture density classification decision mechanism by optimizing mutual\ninformation between feature representations and target classes, compressing\nredundant information while retaining crucial discriminative components.\nExperimental results demonstrate that our method achieves state-of-the-art\nperformance on STL-10 and CIFAR-10/100 datasets when using negligible labeled\nsamples. Notably, this paper also reveals a long-overlooked data leakage issue\nin the STL-10 dataset for semi-supervised learning tasks and removes duplicates\nto ensure the reliability of experimental results. Code available at\nhttps://github.com/Shu1L0n9/ViTSGMM."},{"date":"2025-06","title":"Quantum Secure Key Exchange with Position-based Credentials","author":"Wen Yu Kon, Ignatius William Primaatmaja, Kaushik Chakraborty, and Charles Lim","link":"http://arxiv.org/abs/2506.03549v1","abstract":"Quantum key distribution (QKD) provides an information-theoretic way of\nsecurely exchanging secret keys, and typically relies on pre-shared keys or\npublic keys for message authentication. To lift the requirement of pre-shared\nor public keys, Buhrman et. al. [SIAM J. Comput. 43, 150 (2014)] proposed\nutilizing the location of a party as a credential. Here, we extend upon the\nproposal, develop a QKD protocol with location credentials using quantum\nposition verification (QPV) based message and identity authentication. By using\nQKD with delayed authentication as a base, and later simplifying QPV-based\nmessage authentication, we significantly reduce the number of QPV runs, which\ncurrently acts as a bottleneck. Besides demonstrating security for the proposed\nprotocol, we also provide improvements to QPV security analysis, including\ngeneralization of the QPV adversary model, tightening a trace distance bound\nusing semidefinite programming, and propose a multi-basis QPV requiring only\nBB84 state preparation but with multiple measurement basis."},{"date":"2025-06","title":"Robust Neural Rendering in the Wild with Asymmetric Dual 3D Gaussian Splatting","author":"Chengqi Li, Zhihao Shi, Yangdi Lu, Wenbo He, and Xiangyu Xu","link":"http://arxiv.org/abs/2506.03538v1","abstract":"3D reconstruction from in-the-wild images remains a challenging task due to\ninconsistent lighting conditions and transient distractors. Existing methods\ntypically rely on heuristic strategies to handle the low-quality training data,\nwhich often struggle to produce stable and consistent reconstructions,\nfrequently resulting in visual artifacts. In this work, we propose Asymmetric\nDual 3DGS, a novel framework that leverages the stochastic nature of these\nartifacts: they tend to vary across different training runs due to minor\nrandomness. Specifically, our method trains two 3D Gaussian Splatting (3DGS)\nmodels in parallel, enforcing a consistency constraint that encourages\nconvergence on reliable scene geometry while suppressing inconsistent\nartifacts. To prevent the two models from collapsing into similar failure modes\ndue to confirmation bias, we introduce a divergent masking strategy that\napplies two complementary masks: a multi-cue adaptive mask and a\nself-supervised soft mask, which leads to an asymmetric training process of the\ntwo models, reducing shared error modes. In addition, to improve the efficiency\nof model training, we introduce a lightweight variant called Dynamic EMA Proxy,\nwhich replaces one of the two models with a dynamically updated Exponential\nMoving Average (EMA) proxy, and employs an alternating masking strategy to\npreserve divergence. Extensive experiments on challenging real-world datasets\ndemonstrate that our method consistently outperforms existing approaches while\nachieving high efficiency. Codes and trained models will be released."},{"date":"2025-06","title":"Target Semantics Clustering via Text Representations for Robust Universal Domain Adaptation","author":"Weinan He, Zilei Wang, and Yixin Zhang","link":"http://arxiv.org/abs/2506.03521v1","abstract":"Universal Domain Adaptation (UniDA) focuses on transferring source domain\nknowledge to the target domain under both domain shift and unknown category\nshift. Its main challenge lies in identifying common class samples and aligning\nthem. Current methods typically obtain target domain semantics centers from an\nunconstrained continuous image representation space. Due to domain shift and\nthe unknown number of clusters, these centers often result in complex and less\nrobust alignment algorithm. In this paper, based on vision-language models, we\nsearch for semantic centers in a semantically meaningful and discrete text\nrepresentation space. The constrained space ensures almost no domain bias and\nappropriate semantic granularity for these centers, enabling a simple and\nrobust adaptation algorithm. Specifically, we propose TArget Semantics\nClustering (TASC) via Text Representations, which leverages information\nmaximization as a unified objective and involves two stages. First, with the\nfrozen encoders, a greedy search-based framework is used to search for an\noptimal set of text embeddings to represent target semantics. Second, with the\nsearch results fixed, encoders are refined based on gradient descent,\nsimultaneously achieving robust domain alignment and private class clustering.\nAdditionally, we propose Universal Maximum Similarity (UniMS), a scoring\nfunction tailored for detecting open-set samples in UniDA. Experimentally, we\nevaluate the universality of UniDA algorithms under four category shift\nscenarios. Extensive experiments on four benchmarks demonstrate the\neffectiveness and robustness of our method, which has achieved state-of-the-art\nperformance."},{"date":"2025-06","title":"Software Bill of Materials in Software Supply Chain Security A Systematic Literature Review","author":"Eric O'Donoghue, Yvette Hastings, Ernesto Ortiz, and A. Redempta Manzi Muneza","link":"http://arxiv.org/abs/2506.03507v2","abstract":"Software Bill of Materials (SBOMs) are increasingly regarded as essential\ntools for securing software supply chains (SSCs), yet their real-world use and\nadoption barriers remain poorly understood. This systematic literature review\nsynthesizes evidence from 40 peer-reviewed studies to evaluate how SBOMs are\ncurrently used to bolster SSC security. We identify five primary application\nareas: vulnerability management, transparency, component assessment, risk\nassessment, and SSC integrity. Despite clear promise, adoption is hindered by\nsignificant barriers: generation tooling, data privacy, format/standardization,\nsharing/distribution, cost/overhead, vulnerability exploitability, maintenance,\nanalysis tooling, false positives, hidden packages, and tampering. To structure\nour analysis, we map these barriers to the ISO/IEC 25019:2023 Quality-in-Use\nmodel, revealing critical deficiencies in SBOM trustworthiness, usability, and\nsuitability for security tasks. We also highlight key gaps in the literature.\nThese include the absence of applying machine learning techniques to assess\nSBOMs and limited evaluation of SBOMs and SSCs using software quality assurance\ntechniques. Our findings provide actionable insights for researchers, tool\ndevelopers, and practitioners seeking to advance SBOM-driven SSC security and\nlay a foundation for future work at the intersection of SSC assurance,\nautomation, and empirical software engineering."},{"date":"2025-06","title":"RoNFA: Robust Neural Field-based Approach for Few-Shot Image Classification with Noisy Labels","author":"Nan Xiang, Lifeng Xing, and Dequan Jin","link":"http://arxiv.org/abs/2506.03461v1","abstract":"In few-shot learning (FSL), the labeled samples are scarce. Thus, label\nerrors can significantly reduce classification accuracy. Since label errors are\ninevitable in realistic learning tasks, improving the robustness of the model\nin the presence of label errors is critical. This paper proposes a new robust\nneural field-based image approach (RoNFA) for few-shot image classification\nwith noisy labels. RoNFA consists of two neural fields for feature and category\nrepresentation. They correspond to the feature space and category set. Each\nneuron in the field for category representation (FCR) has a receptive field\n(RF) on the field for feature representation (FFR) centered at the\nrepresentative neuron for its category generated by soft clustering. In the\nprediction stage, the range of these receptive fields adapts according to the\nneuronal activation in FCR to ensure prediction accuracy. These learning\nstrategies provide the proposed model with excellent few-shot learning\ncapability and strong robustness against label noises. The experimental results\non real-world FSL datasets with three different types of label noise\ndemonstrate that the proposed method significantly outperforms state-of-the-art\nFSL methods. Its accuracy obtained in the presence of noisy labels even\nsurpasses the results obtained by state-of-the-art FSL methods trained on clean\nsupport sets, indicating its strong robustness against noisy labels."},{"date":"2025-06","title":"Robustness in Both Domains: CLIP Needs a Robust Text Encoder","author":"Elias Abad Rocamora, Christian Schlarmann, Naman Deep Singh, Yongtao Wu, Matthias Hein, and Volkan Cevher","link":"http://arxiv.org/abs/2506.03355v1","abstract":"Adversarial input attacks can cause a significant shift of CLIP embeddings.\nThis can affect the downstream robustness of models incorporating CLIP in the\npipeline, such as text-to-image generative models or large vision language\nmodels. While some efforts have been done towards making the CLIP image\nencoders robust, the robustness of text encoders remains unexplored. In this\nwork, we cover this gap in the literature. We propose LEAF: an efficient\nadversarial finetuning method for the text domain, with the ability to scale to\nlarge CLIP models. Our models significantly improve the zero-shot adversarial\naccuracy in the text domain, while maintaining the vision performance provided\nby robust image encoders. When combined with text-to-image diffusion models, we\ncan improve the generation quality under adversarial noise. When employing our\nrobust CLIP encoders in multimodal retrieval tasks, we improve the recall under\nadversarial noise over standard CLIP models. Finally, we show that robust text\nencoders facilitate better reconstruction of input text from its embedding via\ndirect optimization."},{"date":"2025-06","title":"Adversarial Attacks on Robotic Vision Language Action Models","author":"Eliot Krzysztof Jones, Alexander Robey, Andy Zou, Zachary Ravichandran, George J. Pappas, Hamed Hassani, Matt Fredrikson, and J. Zico Kolter","link":"http://arxiv.org/abs/2506.03350v1","abstract":"The emergence of vision-language-action models (VLAs) for end-to-end control\nis reshaping the field of robotics by enabling the fusion of multimodal sensory\ninputs at the billion-parameter scale. The capabilities of VLAs stem primarily\nfrom their architectures, which are often based on frontier large language\nmodels (LLMs). However, LLMs are known to be susceptible to adversarial misuse,\nand given the significant physical risks inherent to robotics, questions remain\nregarding the extent to which VLAs inherit these vulnerabilities. Motivated by\nthese concerns, in this work we initiate the study of adversarial attacks on\nVLA-controlled robots. Our main algorithmic contribution is the adaptation and\napplication of LLM jailbreaking attacks to obtain complete control authority\nover VLAs. We find that textual attacks, which are applied once at the\nbeginning of a rollout, facilitate full reachability of the action space of\ncommonly used VLAs and often persist over longer horizons. This differs\nsignificantly from LLM jailbreaking literature, as attacks in the real world do\nnot have to be semantically linked to notions of harm. We make all code\navailable at https://github.com/eliotjones1/robogcg ."},{"date":"2025-06","title":"Explicitly Modeling Subcortical Vision with a Neuro-Inspired Front-End Improves CNN Robustness","author":"Lucas Piper, Arlindo L. Oliveira, and Tiago Marques","link":"http://arxiv.org/abs/2506.03089v1","abstract":"Convolutional neural networks (CNNs) trained on object recognition achieve\nhigh task performance but continue to exhibit vulnerability under a range of\nvisual perturbations and out-of-domain images, when compared with biological\nvision. Prior work has demonstrated that coupling a standard CNN with a\nfront-end block (VOneBlock) that mimics the primate primary visual cortex (V1)\ncan improve overall model robustness. Expanding on this, we introduce Early\nVision Networks (EVNets), a new class of hybrid CNNs that combine the VOneBlock\nwith a novel SubcorticalBlock, whose architecture draws from computational\nmodels in neuroscience and is parameterized to maximize alignment with\nsubcortical responses reported across multiple experimental studies. Without\nbeing optimized to do so, the assembly of the SubcorticalBlock with the\nVOneBlock improved V1 alignment across most standard V1 benchmarks, and better\nmodeled extra-classical receptive field phenomena. In addition, EVNets exhibit\nstronger emergent shape bias and overperform the base CNN architecture by 8.5%\non an aggregate benchmark of robustness evaluations, including adversarial\nperturbations, common corruptions, and domain shifts. Finally, we show that\nEVNets can be further improved when paired with a state-of-the-art data\naugmentation technique, surpassing the performance of the isolated data\naugmentation approach by 7.3% on our robustness benchmark. This result reveals\ncomplementary benefits between changes in architecture to better mimic biology\nand training-based machine learning approaches."},{"date":"2025-06","title":"On the Benefits of Accelerated Optimization in Robust and Private Estimation","author":"Laurentiu Andrei Marchis, and Po-Ling Loh","link":"http://arxiv.org/abs/2506.03044v1","abstract":"We study the advantages of accelerated gradient methods, specifically based\non the Frank-Wolfe method and projected gradient descent, for privacy and\nheavy-tailed robustness. Our approaches are as follows: For the Frank-Wolfe\nmethod, our technique is based on a tailored learning rate and a uniform lower\nbound on the gradient of the $\\ell_2$-norm over the constraint set. For\naccelerating projected gradient descent, we use the popular variant based on\nNesterov's momentum, and we optimize our objective over $\\mathbb{R}^p$. These\naccelerations reduce iteration complexity, translating into stronger\nstatistical guarantees for empirical and population risk minimization. Our\nanalysis covers three settings: non-random data, random model-free data, and\nparametric models (linear regression and generalized linear models).\nMethodologically, we approach both privacy and robustness based on noisy\ngradients. We ensure differential privacy via the Gaussian mechanism and\nadvanced composition, and we achieve heavy-tailed robustness using a geometric\nmedian-of-means estimator, which also sharpens the dependency on the dimension\nof the covariates. Finally, we compare our rates to existing bounds and\nidentify scenarios where our methods attain optimal convergence."},{"date":"2025-06","title":"On the Robustness of Tabular Foundation Models: Test-Time Attacks and In-Context Defenses","author":"Mohamed Djilani, Thibault Simonetto, Karim Tit, Florian Tambon, Paul R\u00e9camier, Salah Ghamizi, Maxime Cordy, and Mike Papadakis","link":"http://arxiv.org/abs/2506.02978v1","abstract":"Recent tabular Foundational Models (FM) such as TabPFN and TabICL, leverage\nin-context learning to achieve strong performance without gradient updates or\nfine-tuning. However, their robustness to adversarial manipulation remains\nlargely unexplored. In this work, we present a comprehensive study of the\nadversarial vulnerabilities of tabular FM, focusing on both their fragility to\ntargeted test-time attacks and their potential misuse as adversarial tools. We\nshow on three benchmarks in finance, cybersecurity and healthcare, that small,\nstructured perturbations to test inputs can significantly degrade prediction\naccuracy, even when training context remain fixed. Additionally, we demonstrate\nthat tabular FM can be repurposed to generate transferable evasion to\nconventional models such as random forests and XGBoost, and on a lesser extent\nto deep tabular models. To improve tabular FM, we formulate the robustification\nproblem as an optimization of the weights (adversarial fine-tuning), or the\ncontext (adversarial in-context learning). We introduce an in-context\nadversarial training strategy that incrementally replaces the context with\nadversarial perturbed instances, without updating model weights. Our approach\nimproves robustness across multiple tabular benchmarks. Together, these\nfindings position tabular FM as both a target and a source of adversarial\nthreats, highlighting the urgent need for robust training and evaluation\npractices in this emerging paradigm."},{"date":"2025-06","title":"ATAG: AI-Agent Application Threat Assessment with Attack Graphs","author":"Parth Atulbhai Gandhi, Akansha Shukla, David Tayouri, Beni Ifland, Yuval Elovici, Rami Puzis, and Asaf Shabtai","link":"http://arxiv.org/abs/2506.02859v1","abstract":"Evaluating the security of multi-agent systems (MASs) powered by large\nlanguage models (LLMs) is challenging, primarily because of the systems'\ncomplex internal dynamics and the evolving nature of LLM vulnerabilities.\nTraditional attack graph (AG) methods often lack the specific capabilities to\nmodel attacks on LLMs. This paper introduces AI-agent application Threat\nassessment with Attack Graphs (ATAG), a novel framework designed to\nsystematically analyze the security risks associated with AI-agent\napplications. ATAG extends the MulVAL logic-based AG generation tool with\ncustom facts and interaction rules to accurately represent AI-agent topologies,\nvulnerabilities, and attack scenarios. As part of this research, we also\ncreated the LLM vulnerability database (LVD) to initiate the process of\nstandardizing LLM vulnerabilities documentation. To demonstrate ATAG's\nefficacy, we applied it to two multi-agent applications. Our case studies\ndemonstrated the framework's ability to model and generate AGs for\nsophisticated, multi-step attack scenarios exploiting vulnerabilities such as\nprompt injection, excessive agency, sensitive information disclosure, and\ninsecure output handling across interconnected agents. ATAG is an important\nstep toward a robust methodology and toolset to help understand, visualize, and\nprioritize complex attack paths in multi-agent AI systems (MAASs). It\nfacilitates proactive identification and mitigation of AI-agent threats in\nmulti-agent applications."},{"date":"2025-06","title":"Enhancing Abnormality Identification: Robust Out-of-Distribution Strategies for Deepfake Detection","author":"Luca Maiano, Fabrizio Casadei, and Irene Amerini","link":"http://arxiv.org/abs/2506.02857v1","abstract":"Detecting deepfakes has become a critical challenge in Computer Vision and\nArtificial Intelligence. Despite significant progress in detection techniques,\ngeneralizing them to open-set scenarios continues to be a persistent\ndifficulty. Neural networks are often trained on the closed-world assumption,\nbut with new generative models constantly evolving, it is inevitable to\nencounter data generated by models that are not part of the training\ndistribution. To address these challenges, in this paper, we propose two novel\nOut-Of-Distribution (OOD) detection approaches. The first approach is trained\nto reconstruct the input image, while the second incorporates an attention\nmechanism for detecting OODs. Our experiments validate the effectiveness of the\nproposed approaches compared to existing state-of-the-art techniques. Our\nmethod achieves promising results in deepfake detection and ranks among the\ntop-performing configurations on the benchmark, demonstrating their potential\nfor robust, adaptable solutions in dynamic, real-world applications."},{"date":"2025-06","title":"Doubly-Robust Estimation of Counterfactual Policy Mean Embeddings","author":"Houssam Zenati, Bariscan Bozkurt, and Arthur Gretton","link":"http://arxiv.org/abs/2506.02793v1","abstract":"Estimating the distribution of outcomes under counterfactual policies is\ncritical for decision-making in domains such as recommendation, advertising,\nand healthcare. We analyze a novel framework-Counterfactual Policy Mean\nEmbedding (CPME)-that represents the entire counterfactual outcome distribution\nin a reproducing kernel Hilbert space (RKHS), enabling flexible and\nnonparametric distributional off-policy evaluation. We introduce both a plug-in\nestimator and a doubly robust estimator; the latter enjoys improved uniform\nconvergence rates by correcting for bias in both the outcome embedding and\npropensity models. Building on this, we develop a doubly robust kernel test\nstatistic for hypothesis testing, which achieves asymptotic normality and thus\nenables computationally efficient testing and straightforward construction of\nconfidence intervals. Our framework also supports sampling from the\ncounterfactual distribution. Numerical simulations illustrate the practical\nbenefits of CPME over existing methods."},{"date":"2025-06","title":"Privacy Leaks by Adversaries: Adversarial Iterations for Membership Inference Attack","author":"Jing Xue, Zhishen Sun, Haishan Ye, Luo Luo, Xiangyu Chang, Ivor Tsang, and Guang Dai","link":"http://arxiv.org/abs/2506.02711v1","abstract":"Membership inference attack (MIA) has become one of the most widely used and\neffective methods for evaluating the privacy risks of machine learning models.\nThese attacks aim to determine whether a specific sample is part of the model's\ntraining set by analyzing the model's output. While traditional membership\ninference attacks focus on leveraging the model's posterior output, such as\nconfidence on the target sample, we propose IMIA, a novel attack strategy that\nutilizes the process of generating adversarial samples to infer membership. We\npropose to infer the member properties of the target sample using the number of\niterations required to generate its adversarial sample. We conduct experiments\nacross multiple models and datasets, and our results demonstrate that the\nnumber of iterations for generating an adversarial sample is a reliable feature\nfor membership inference, achieving strong performance both in black-box and\nwhite-box attack scenarios. This work provides a new perspective for evaluating\nmodel privacy and highlights the potential of adversarial example-based\nfeatures for privacy leakage assessment."},{"date":"2025-06","title":"Poster: FedBlockParadox -- A Framework for Simulating and Securing Decentralized Federated Learning","author":"Gabriele Digregorio, Francesco Bleggi, Federico Caroli, Michele Carminati, Stefano Zanero, and Stefano Longari","link":"http://arxiv.org/abs/2506.02679v1","abstract":"A significant body of research in decentralized federated learning focuses on\ncombining the privacy-preserving properties of federated learning with the\nresilience and transparency offered by blockchain-based systems. While these\napproaches are promising, they often lack flexible tools to evaluate system\nrobustness under adversarial conditions. To fill this gap, we present\nFedBlockParadox, a modular framework for modeling and evaluating decentralized\nfederated learning systems built on blockchain technologies, with a focus on\nresilience against a broad spectrum of adversarial attack scenarios. It\nsupports multiple consensus protocols, validation methods, aggregation\nstrategies, and configurable attack models. By enabling controlled experiments,\nFedBlockParadox provides a valuable resource for researchers developing secure,\ndecentralized learning solutions. The framework is open-source and built to be\nextensible by the community."},{"date":"2025-06","title":"Beyond Invisibility: Learning Robust Visible Watermarks for Stronger Copyright Protection","author":"Tianci Liu, Tong Yang, Quan Zhang, and Qi Lei","link":"http://arxiv.org/abs/2506.02665v1","abstract":"As AI advances, copyrighted content faces growing risk of unauthorized use,\nwhether through model training or direct misuse. Building upon invisible\nadversarial perturbation, recent works developed copyright protections against\nspecific AI techniques such as unauthorized personalization through DreamBooth\nthat are misused. However, these methods offer only short-term security, as\nthey require retraining whenever the underlying model architectures change. To\nestablish long-term protection aiming at better robustness, we go beyond\ninvisible perturbation, and propose a universal approach that embeds\n\\textit{visible} watermarks that are \\textit{hard-to-remove} into images.\nGrounded in a new probabilistic and inverse problem-based formulation, our\nframework maximizes the discrepancy between the \\textit{optimal} reconstruction\nand the original content. We develop an effective and efficient approximation\nalgorithm to circumvent a intractable bi-level optimization. Experimental\nresults demonstrate superiority of our approach across diverse scenarios."},{"date":"2025-06","title":"EALG: Evolutionary Adversarial Generation of Language Model-Guided Generators for Combinatorial Optimization","author":"Ruibo Duan, Yuxin Liu, Xinyao Dong, and Chenglin Fan","link":"http://arxiv.org/abs/2506.02594v1","abstract":"Generating challenging instances is crucial for the evaluation and\nadvancement of combinatorial optimization solvers. In this work, we introduce\nEALG (Evolutionary Adversarial Generation of Language Model-Guided Generators),\na novel framework that automates the co-evolution of optimization problem\ninstances and their corresponding heuristic solvers using large language models\n(LLMs). EALG leverages a mutation-based adversarial approach that dynamically\nevolves instance generation procedures to create increasingly difficult\nproblems, while simultaneously synthesizing adaptive heuristic algorithms\nthrough interactions with LLMs guided by algorithmic structure. Unlike existing\napproaches that focus solely on static benchmark creation or manual solver\ndesign, EALG provides a seamless pipeline from instance generation to solver\nsynthesis. Experimental results demonstrate that EALG generates significantly\nharder instances than current benchmarks, and its synthesized solvers\ngeneralize effectively across a broad spectrum of combinatorial tasks. This\nwork explores a new paradigm for combinatorial optimization that integrates\ninstance generation with solver design, resulting in state-of-the-art\nperformance."},{"date":"2025-06","title":"VerificAgent: Integrating Expert Knowledge and Fact-Checked Memory for Robust Domain-Specific Task Planning","author":"Thong Q. Nguyen, Shubhang Desai, Yash Jain, Tanvir Aumi, and Vishal Chowdhary","link":"http://arxiv.org/abs/2506.02539v1","abstract":"Continual memory augmentation allows computer-use agents (CUAs) to learn from\npast interactions and refine their task-solving strategies over time. However,\nunchecked memory accumulation can introduce spurious or hallucinated\n\"learnings\" that degrade agent performance, particularly in domain-specific\nworkflows such as productivity software. We present a novel framework,\nVerificAgent, that effectively manages memory for CUAs through (1) an\nexpert-curated seed of domain knowledge, (2) iterative, trajectory-based memory\nrefinement during training, and (3) a post-hoc fact-checking pass by human\nexperts to sanitize accumulated memory before deployment. On OSWorld\nproductivity tasks, VerificAgent achieves a 111.1% relative improvement in\nsuccess rate over baseline CUA without any additional fine-tuning."},{"date":"2025-06","title":"VPI-Bench: Visual Prompt Injection Attacks for Computer-Use Agents","author":"Tri Cao, Bennett Lim, Yue Liu, Yuan Sui, Yuexin Li, Shumin Deng, Lin Lu, Nay Oo, Shuicheng Yan, and Bryan Hooi","link":"http://arxiv.org/abs/2506.02456v1","abstract":"Computer-Use Agents (CUAs) with full system access enable powerful task\nautomation but pose significant security and privacy risks due to their ability\nto manipulate files, access user data, and execute arbitrary commands. While\nprior work has focused on browser-based agents and HTML-level attacks, the\nvulnerabilities of CUAs remain underexplored. In this paper, we investigate\nVisual Prompt Injection (VPI) attacks, where malicious instructions are\nvisually embedded within rendered user interfaces, and examine their impact on\nboth CUAs and Browser-Use Agents (BUAs). We propose VPI-Bench, a benchmark of\n306 test cases across five widely used platforms, to evaluate agent robustness\nunder VPI threats. Each test case is a variant of a web platform, designed to\nbe interactive, deployed in a realistic environment, and containing a visually\nembedded malicious prompt. Our empirical study shows that current CUAs and BUAs\ncan be deceived at rates of up to 51% and 100%, respectively, on certain\nplatforms. The experimental results also indicate that system prompt defenses\noffer only limited improvements. These findings highlight the need for robust,\ncontext-aware defenses to ensure the safe deployment of multimodal AI agents in\nreal-world environments. The code and dataset are available at:\nhttps://github.com/cua-framework/agents"},{"date":"2025-06","title":"Dynamic Epsilon Scheduling: A Multi-Factor Adaptive Perturbation Budget for Adversarial Training","author":"Alan Mitkiy, James Smith, Hana Satou, Hiroshi Tanaka, Emily Johnson, and F Monkey","link":"http://arxiv.org/abs/2506.04263v1","abstract":"Adversarial training is among the most effective strategies for defending\ndeep neural networks against adversarial examples. A key limitation of existing\nadversarial training approaches lies in their reliance on a fixed perturbation\nbudget, which fails to account for instance-specific robustness\ncharacteristics. While prior works such as IAAT and MMA introduce\ninstance-level adaptations, they often rely on heuristic or static\napproximations of data robustness. In this paper, we propose Dynamic Epsilon\nScheduling (DES), a novel framework that adaptively adjusts the adversarial\nperturbation budget per instance and per training iteration. DES integrates\nthree key factors: (1) the distance to the decision boundary approximated via\ngradient-based proxies, (2) prediction confidence derived from softmax entropy,\nand (3) model uncertainty estimated via Monte Carlo dropout. By combining these\ncues into a unified scheduling strategy, DES tailors the perturbation budget\ndynamically to guide more effective adversarial learning. Experimental results\non CIFAR-10 and CIFAR-100 show that our method consistently improves both\nadversarial robustness and standard accuracy compared to fixed-epsilon\nbaselines and prior adaptive methods. Moreover, we provide theoretical insights\ninto the stability and convergence of our scheduling policy. This work opens a\nnew avenue for instance-aware, data-driven adversarial training methods."},{"date":"2025-06","title":"AERO: A Redirection-Based Optimization Framework Inspired by Judo for Robust Probabilistic Forecasting","author":"Karthikeyan Vaiapury","link":"http://arxiv.org/abs/2506.02415v1","abstract":"Optimization remains a fundamental pillar of machine learning, yet existing\nmethods often struggle to maintain stability and adaptability in dynamic, non\nlinear systems, especially under uncertainty. We introduce AERO (Adversarial\nEnergy-based Redirection Optimization), a novel framework inspired by the\nredirection principle in Judo, where external disturbances are leveraged rather\nthan resisted. AERO reimagines optimization as a redirection process guided by\n15 interrelated axioms encompassing adversarial correction, energy\nconservation, and disturbance-aware learning. By projecting gradients,\nintegrating uncertainty driven dynamics, and managing learning energy, AERO\noffers a principled approach to stable and robust model updates. Applied to\nprobabilistic solar energy forecasting, AERO demonstrates substantial gains in\npredictive accuracy, reliability, and adaptability, especially in noisy and\nuncertain environments. Our findings highlight AERO as a compelling new\ndirection in the theoretical and practical landscape of optimization."},{"date":"2025-06","title":"GAdaBoost: An Efficient and Robust AdaBoost Algorithm Based on Granular-Ball Structure","author":"Qin Xie, Qinghua Zhang, Shuyin Xia, Xinran Zhou, and Guoyin Wang","link":"http://arxiv.org/abs/2506.02390v1","abstract":"Adaptive Boosting (AdaBoost) faces significant challenges posed by label\nnoise, especially in multiclass classification tasks. Existing methods either\nlack mechanisms to handle label noise effectively or suffer from high\ncomputational costs due to redundant data usage. Inspired by granular\ncomputing, this paper proposes granular adaptive boosting (GAdaBoost), a novel\ntwo-stage framework comprising a data granulation stage and an adaptive\nboosting stage, to enhance efficiency and robustness under noisy conditions. To\nvalidate its feasibility, an extension of SAMME, termed GAdaBoost.SA, is\nproposed. Specifically, first, a granular-ball generation method is designed to\ncompress data while preserving diversity and mitigating label noise. Second,\nthe granular ball-based SAMME algorithm focuses on granular balls rather than\nindividual samples, improving efficiency and reducing sensitivity to noise.\nExperimental results on some noisy datasets show that the proposed approach\nachieves superior robustness and efficiency compared with existing methods,\ndemonstrating that this work effectively extends AdaBoost and SAMME."},{"date":"2025-06","title":"Exploring Explanations Improves the Robustness of In-Context Learning","author":"Ukyo Honda, and Tatsushi Oka","link":"http://arxiv.org/abs/2506.02378v1","abstract":"In-context learning (ICL) has emerged as a successful paradigm for leveraging\nlarge language models (LLMs). However, it often struggles to generalize beyond\nthe distribution of the provided demonstrations. A recent advancement in\nenhancing robustness is ICL with explanations (X-ICL), which improves\nprediction reliability by guiding LLMs to understand and articulate the\nreasoning behind correct labels. Building on this approach, we introduce an\nadvanced framework that extends X-ICL by systematically exploring explanations\nfor all possible labels (X$^2$-ICL), thereby enabling more comprehensive and\nrobust decision-making. Experimental results on multiple natural language\nunderstanding datasets validate the effectiveness of X$^2$-ICL, demonstrating\nsignificantly improved robustness to out-of-distribution data compared to the\nexisting ICL approaches."},{"date":"2025-06","title":"Fire360: A Benchmark for Robust Perception and Episodic Memory in Degraded 360-Degree Firefighting Videos","author":"Aditi Tiwari, Farzaneh Masoud, Dac Trong Nguyen, Jill Kraft, Heng Ji, and Klara Nahrstedt","link":"http://arxiv.org/abs/2506.02167v1","abstract":"Modern AI systems struggle most in environments where reliability is critical\n- scenes with smoke, poor visibility, and structural deformation. Each year,\ntens of thousands of firefighters are injured on duty, often due to breakdowns\nin situational perception. We introduce Fire360, a benchmark for evaluating\nperception and reasoning in safety-critical firefighting scenarios. The dataset\nincludes 228 360-degree videos from professional training sessions under\ndiverse conditions (e.g., low light, thermal distortion), annotated with action\nsegments, object locations, and degradation metadata. Fire360 supports five\ntasks: Visual Question Answering, Temporal Action Captioning, Object\nLocalization, Safety-Critical Reasoning, and Transformed Object Retrieval\n(TOR). TOR tests whether models can match pristine exemplars to fire-damaged\ncounterparts in unpaired scenes, evaluating transformation-invariant\nrecognition. While human experts achieve 83.5% on TOR, models like GPT-4o lag\nsignificantly, exposing failures in reasoning under degradation. By releasing\nFire360 and its evaluation suite, we aim to advance models that not only see,\nbut also remember, reason, and act under uncertainty. The dataset is available\nat: https://uofi.box.com/v/fire360dataset."},{"date":"2025-06","title":"Mitigating Data Poisoning Attacks to Local Differential Privacy","author":"Xiaolin Li, Ninghui Li, Boyang Wang, and Wenhai Sun","link":"http://arxiv.org/abs/2506.02156v1","abstract":"The distributed nature of local differential privacy (LDP) invites data\npoisoning attacks and poses unforeseen threats to the underlying LDP-supported\napplications. In this paper, we propose a comprehensive mitigation framework\nfor popular frequency estimation, which contains a suite of novel defenses,\nincluding malicious user detection, attack pattern recognition, and damaged\nutility recovery. In addition to existing attacks, we explore new adaptive\nadversarial activities for our mitigation design. For detection, we present a\nnew method to precisely identify bogus reports and thus LDP aggregation can be\nperformed over the ``clean'' data. When the attack behavior becomes stealthy\nand direct filtering out malicious users is difficult, we further propose a\ndetection that can effectively recognize hidden adversarial patterns, thus\nfacilitating the decision-making of service providers. These detection methods\nrequire no additional data and attack information and incur minimal\ncomputational cost. Our experiment demonstrates their excellent performance and\nsubstantial improvement over previous work in various settings. In addition, we\nconduct an empirical analysis of LDP post-processing for corrupted data\nrecovery and propose a new post-processing method, through which we reveal new\ninsights into protocol recommendations in practice and key design principles\nfor future research."},{"date":"2025-06","title":"ReconXF: Graph Reconstruction Attack via Public Feature Explanations on Privatized Node Features and Labels","author":"Rishi Raj Sahoo, Rucha Bhalchandra Joshi, and Subhankar Mishra","link":"http://arxiv.org/abs/2506.02134v1","abstract":"Graph Neural Networks (GNNs) achieve high performance across many\napplications but function as black-box models, limiting their use in critical\ndomains like healthcare and criminal justice. Explainability methods address\nthis by providing feature-level explanations that identify important node\nattributes for predictions. These explanations create privacy risks. Combined\nwith auxiliary information, feature explanations can enable adversaries to\nreconstruct graph structure, exposing sensitive relationships. Existing graph\nreconstruction attacks assume access to original auxiliary data, but practical\nsystems use differential privacy to protect node features and labels while\nproviding explanations for transparency. We study a threat model where\nadversaries access public feature explanations along with privatized node\nfeatures and labels. We show that existing explanation-based attacks like GSEF\nperform poorly with privatized data due to noise from differential privacy\nmechanisms. We propose ReconXF, a graph reconstruction attack for scenarios\nwith public explanations and privatized auxiliary data. Our method adapts\nexplanation-based frameworks by incorporating denoising mechanisms that handle\ndifferential privacy noise while exploiting structural signals in explanations.\nExperiments across multiple datasets show ReconXF outperforms SoTA methods in\nprivatized settings, with improvements in AUC and average precision. Results\nindicate that public explanations combined with denoising enable graph\nstructure recovery even under the privacy protection of auxiliary data. Code is\navailable at (link to be made public after acceptance)."},{"date":"2025-06","title":"Fast and Robust Rotation Averaging with Anisotropic Coordinate Descent","author":"Yaroslava Lochman, Carl Olsson, and Christopher Zach","link":"http://arxiv.org/abs/2506.01940v1","abstract":"Anisotropic rotation averaging has recently been explored as a natural\nextension of respective isotropic methods. In the anisotropic formulation,\nuncertainties of the estimated relative rotations -- obtained via standard\ntwo-view optimization -- are propagated to the optimization of absolute\nrotations. The resulting semidefinite relaxations are able to recover global\nminima but scale poorly with the problem size. Local methods are fast and also\nadmit robust estimation but are sensitive to initialization. They usually\nemploy minimum spanning trees and therefore suffer from drift accumulation and\ncan get trapped in poor local minima. In this paper, we attempt to bridge the\ngap between optimality, robustness and efficiency of anisotropic rotation\naveraging. We analyze a family of block coordinate descent methods initially\nproposed to optimize the standard chordal distances, and derive a much simpler\nformulation and an anisotropic extension obtaining a fast general solver. We\nintegrate this solver into the extended anisotropic large-scale robust rotation\naveraging pipeline. The resulting algorithm achieves state-of-the-art\nperformance on public structure-from-motion datasets. Project page:\nhttps://ylochman.github.io/acd"},{"date":"2025-06","title":"COALESCE: Economic and Security Dynamics of Skill-Based Task Outsourcing Among Team of Autonomous LLM Agents","author":"Manish Bhatt, Ronald F. Del Rosario, Vineeth Sai Narajala, and Idan Habler","link":"http://arxiv.org/abs/2506.01900v1","abstract":"The meteoric rise and proliferation of autonomous Large Language Model (LLM)\nagents promise significant capabilities across various domains. However, their\ndeployment is increasingly constrained by substantial computational demands,\nspecifically for Graphics Processing Unit (GPU) resources. This paper addresses\nthe critical problem of optimizing resource utilization in LLM agent systems.\nWe introduce COALESCE (Cost-Optimized and Secure Agent Labour Exchange via\nSkill-based Competence Estimation), a novel framework designed to enable\nautonomous LLM agents to dynamically outsource specific subtasks to\nspecialized, cost-effective third-party LLM agents. The framework integrates\nmechanisms for hybrid skill representation, dynamic skill discovery, automated\ntask decomposition, a unified cost model comparing internal execution costs\nagainst external outsourcing prices, simplified market-based decision-making\nalgorithms, and a standardized communication protocol between LLM agents.\nComprehensive validation through 239 theoretical simulations demonstrates\n41.8\\% cost reduction potential, while large-scale empirical validation across\n240 real LLM tasks confirms 20.3\\% cost reduction with proper epsilon-greedy\nexploration, establishing both theoretical viability and practical\neffectiveness. The emergence of proposed open standards like Google's\nAgent2Agent (A2A) protocol further underscores the need for frameworks like\nCOALESCE that can leverage such standards for efficient agent interaction. By\nfacilitating a dynamic market for agent capabilities, potentially utilizing\nprotocols like A2A for communication, COALESCE aims to significantly reduce\noperational costs, enhance system scalability, and foster the emergence of\nspecialized agent economies, making complex LLM agent functionalities more\naccessible and economically viable."},{"date":"2025-06","title":"Which Factors Make Code LLMs More Vulnerable to Backdoor Attacks? A Systematic Study","author":"Chenyu Wang, Zhou Yang, Yaniv Harel, and David Lo","link":"http://arxiv.org/abs/2506.01825v1","abstract":"Code LLMs are increasingly employed in software development. However, studies\nhave shown that they are vulnerable to backdoor attacks: when a trigger (a\nspecific input pattern) appears in the input, the backdoor will be activated\nand cause the model to generate malicious outputs. Researchers have designed\nvarious triggers and demonstrated the feasibility of implanting backdoors by\npoisoning a fraction of the training data. Some basic conclusions have been\nmade, such as backdoors becoming easier to implant when more training data are\nmodified. However, existing research has not explored other factors influencing\nbackdoor attacks on Code LLMs, such as training batch size, epoch number, and\nthe broader design space for triggers, e.g., trigger length.\n  To bridge this gap, we use code summarization as an example to perform an\nempirical study that systematically investigates the factors affecting backdoor\neffectiveness and understands the extent of the threat posed. Three categories\nof factors are considered: data, model, and inference, revealing previously\noverlooked findings. We find that the prevailing consensus -- that attacks are\nineffective at extremely low poisoning rates -- is incorrect. The absolute\nnumber of poisoned samples matters as well. Specifically, poisoning just 20 out\nof 454K samples (0.004\\% poisoning rate -- far below the minimum setting of\n0.1\\% in prior studies) successfully implants backdoors! Moreover, the common\ndefense is incapable of removing even a single poisoned sample from it.\nAdditionally, small batch sizes increase the risk of backdoor attacks. We also\nuncover other critical factors such as trigger types, trigger length, and the\nrarity of tokens in the triggers, leading to valuable insights for assessing\nCode LLMs' vulnerability to backdoor attacks. Our study highlights the urgent\nneed for defense mechanisms against extremely low poisoning rate settings."},{"date":"2025-06","title":"DRAUN: An Algorithm-Agnostic Data Reconstruction Attack on Federated Unlearning Systems","author":"Hithem Lamri, Manaar Alam, Haiyan Jiang, and Michail Maniatakos","link":"http://arxiv.org/abs/2506.01777v1","abstract":"Federated Unlearning (FU) enables clients to remove the influence of specific\ndata from a collaboratively trained shared global model, addressing regulatory\nrequirements such as GDPR and CCPA. However, this unlearning process introduces\na new privacy risk: A malicious server may exploit unlearning updates to\nreconstruct the data requested for removal, a form of Data Reconstruction\nAttack (DRA). While DRAs for machine unlearning have been studied extensively\nin centralized Machine Learning-as-a-Service (MLaaS) settings, their\napplicability to FU remains unclear due to the decentralized, client-driven\nnature of FU. This work presents DRAUN, the first attack framework to\nreconstruct unlearned data in FU systems. DRAUN targets optimization-based\nunlearning methods, which are widely adopted for their efficiency. We\ntheoretically demonstrate why existing DRAs targeting machine unlearning in\nMLaaS fail in FU and show how DRAUN overcomes these limitations. We validate\nour approach through extensive experiments on four datasets and four model\narchitectures, evaluating its performance against five popular unlearning\nmethods, effectively demonstrating that state-of-the-art FU methods remain\nvulnerable to DRAs."},{"date":"2025-06","title":"Predictive-CSM: Lightweight Fragment Security for 6LoWPAN IoT Networks","author":"Somayeh Sobati-M","link":"http://arxiv.org/abs/2506.01767v1","abstract":"Fragmentation is a routine part of communication in 6LoWPAN-based IoT\nnetworks,\n  designed to accommodate small frame sizes on constrained wireless links.\nHowever, this process\n  introduces a critical vulnerability fragments are typically stored and\nprocessed before their\n  legitimacy is confirmed, allowing attackers to exploit this gap with minimal\neffort.\n  In this work, we explore a defense strategy that takes a more adaptive,\nbehavior-aware approach to this problem. Our system, called Predictive-CSM,\nintroduces a combination of two\n  lightweight mechanisms. The first tracks how each node behaves over time,\nrewarding consistent\n  and successful interactions while quickly penalizing suspicious or failing\npatterns. The second\n  checks the integrity of packet fragments using a chained hash, allowing\nincomplete or manipulated sequences to be caught early, before they can occupy\nmemory or waste processing time.\n  We put this system to the test using a set of targeted attack simulations,\nincluding early fragment injection, replayed headers, and flooding with fake\ndata. Across all scenarios, Predictive CSM preserved network delivery and\nmaintained energy efficiency, even under pressure. Rather\n  than relying on heavyweight cryptography or rigid filters, this approach\nallows constrained de vices to adapt their defenses in real time based on what\nthey observe, not just what they're\n  told. In that way, it offers a step forward for securing fragmented\ncommunication in real world\n  IoT systems"},{"date":"2025-06","title":"Robust Satisficing Gaussian Process Bandits Under Adversarial Attacks","author":"Artun Saday, Ya\u015far Cahit Y\u0131ld\u0131r\u0131m, and Cem Tekin","link":"http://arxiv.org/abs/2506.01625v1","abstract":"We address the problem of Gaussian Process (GP) optimization in the presence\nof unknown and potentially varying adversarial perturbations. Unlike\ntraditional robust optimization approaches that focus on maximizing performance\nunder worst-case scenarios, we consider a robust satisficing objective, where\nthe goal is to consistently achieve a predefined performance threshold $\\tau$,\neven under adversarial conditions. We propose two novel algorithms based on\ndistinct formulations of robust satisficing, and show that they are instances\nof a general robust satisficing framework. Further, each algorithm offers\ndifferent guarantees depending on the nature of the adversary. Specifically, we\nderive two regret bounds: one that is sublinear over time, assuming certain\nconditions on the adversary and the satisficing threshold $\\tau$, and another\nthat scales with the perturbation magnitude but requires no assumptions on the\nadversary. Through extensive experiments, we demonstrate that our approach\noutperforms the established robust optimization methods in achieving the\nsatisficing objective, particularly when the ambiguity set of the robust\noptimization framework is inaccurately specified."},{"date":"2025-06","title":"Silence is Golden: Leveraging Adversarial Examples to Nullify Audio Control in LDM-based Talking-Head Generation","author":"Yuan Gan, Jiaxu Miao, Yunze Wang, and Yi Yang","link":"http://arxiv.org/abs/2506.01591v1","abstract":"Advances in talking-head animation based on Latent Diffusion Models (LDM)\nenable the creation of highly realistic, synchronized videos. These fabricated\nvideos are indistinguishable from real ones, increasing the risk of potential\nmisuse for scams, political manipulation, and misinformation. Hence, addressing\nthese ethical concerns has become a pressing issue in AI security. Recent\nproactive defense studies focused on countering LDM-based models by adding\nperturbations to portraits. However, these methods are ineffective at\nprotecting reference portraits from advanced image-to-video animation. The\nlimitations are twofold: 1) they fail to prevent images from being manipulated\nby audio signals, and 2) diffusion-based purification techniques can\neffectively eliminate protective perturbations. To address these challenges, we\npropose Silencer, a two-stage method designed to proactively protect the\nprivacy of portraits. First, a nullifying loss is proposed to ignore audio\ncontrol in talking-head generation. Second, we apply anti-purification loss in\nLDM to optimize the inverted latent feature to generate robust perturbations.\nExtensive experiments demonstrate the effectiveness of Silencer in proactively\nprotecting portrait privacy. We hope this work will raise awareness among the\nAI security community regarding critical ethical issues related to talking-head\ngeneration techniques. Code: https://github.com/yuangan/Silencer."},{"date":"2025-06","title":"Enhancing Diffusion-based Unrestricted Adversarial Attacks via Adversary Preferences Alignment","author":"Kaixun Jiang, Zhaoyu Chen, Haijing Guo, Jinglun Li, Jiyuan Fu, Pinxue Guo, Hao Tang, Bo Li, and Wenqiang Zhang","link":"http://arxiv.org/abs/2506.01511v1","abstract":"Preference alignment in diffusion models has primarily focused on benign\nhuman preferences (e.g., aesthetic). In this paper, we propose a novel\nperspective: framing unrestricted adversarial example generation as a problem\nof aligning with adversary preferences. Unlike benign alignment, adversarial\nalignment involves two inherently conflicting preferences: visual consistency\nand attack effectiveness, which often lead to unstable optimization and reward\nhacking (e.g., reducing visual quality to improve attack success). To address\nthis, we propose APA (Adversary Preferences Alignment), a two-stage framework\nthat decouples conflicting preferences and optimizes each with differentiable\nrewards. In the first stage, APA fine-tunes LoRA to improve visual consistency\nusing rule-based similarity reward. In the second stage, APA updates either the\nimage latent or prompt embedding based on feedback from a substitute\nclassifier, guided by trajectory-level and step-wise rewards. To enhance\nblack-box transferability, we further incorporate a diffusion augmentation\nstrategy. Experiments demonstrate that APA achieves significantly better attack\ntransferability while maintaining high visual consistency, inspiring further\nresearch to approach adversarial attacks from an alignment perspective. Code\nwill be available at https://github.com/deep-kaixun/APA."},{"date":"2025-06","title":"Robust Federated Learning against Noisy Clients via Masked Optimization","author":"Xuefeng Jiang, Tian Wen, Zhiqin Yang, Lvhua Wu, Yufeng Chen, Sheng Sun, Yuwei Wang, and Min Liu","link":"http://arxiv.org/abs/2506.02079v1","abstract":"In recent years, federated learning (FL) has made significant advance in\nprivacy-sensitive applications. However, it can be hard to ensure that FL\nparticipants provide well-annotated data for training. The corresponding\nannotations from different clients often contain complex label noise at varying\nlevels. This label noise issue has a substantial impact on the performance of\nthe trained models, and clients with greater noise levels can be largely\nattributed for this degradation. To this end, it is necessary to develop an\neffective optimization strategy to alleviate the adverse effects of these noisy\nclients.In this study, we present a two-stage optimization framework,\nMaskedOptim, to address this intricate label noise problem. The first stage is\ndesigned to facilitate the detection of noisy clients with higher label noise\nrates. The second stage focuses on rectifying the labels of the noisy clients'\ndata through an end-to-end label correction mechanism, aiming to mitigate the\nnegative impacts caused by misinformation within datasets. This is achieved by\nlearning the potential ground-truth labels of the noisy clients' datasets via\nbackpropagation. To further enhance the training robustness, we apply the\ngeometric median based model aggregation instead of the commonly-used vanilla\naveraged model aggregation. We implement sixteen related methods and conduct\nevaluations on three image datasets and one text dataset with diverse label\nnoise patterns for a comprehensive comparison. Extensive experimental results\nindicate that our proposed framework shows its robustness in different\nscenarios. Additionally, our label correction framework effectively enhances\nthe data quality of the detected noisy clients' local datasets. % Our codes\nwill be open-sourced to facilitate related research communities. Our codes are\navailable via https://github.com/Sprinter1999/MaskedOptim ."},{"date":"2025-06","title":"Variance-Based Defense Against Blended Backdoor Attacks","author":"Sujeevan Aseervatham, Achraf Kerzazi, and Youn\u00e8s Bennani","link":"http://arxiv.org/abs/2506.01444v1","abstract":"Backdoor attacks represent a subtle yet effective class of cyberattacks\ntargeting AI models, primarily due to their stealthy nature. The model behaves\nnormally on clean data but exhibits malicious behavior only when the attacker\nembeds a specific trigger into the input. This attack is performed during the\ntraining phase, where the adversary corrupts a small subset of the training\ndata by embedding a pattern and modifying the labels to a chosen target. The\nobjective is to make the model associate the pattern with the target label\nwhile maintaining normal performance on unaltered data. Several defense\nmechanisms have been proposed to sanitize training data-sets. However, these\nmethods often rely on the availability of a clean dataset to compute\nstatistical anomalies, which may not always be feasible in real-world scenarios\nwhere datasets can be unavailable or compromised. To address this limitation,\nwe propose a novel defense method that trains a model on the given dataset,\ndetects poisoned classes, and extracts the critical part of the attack trigger\nbefore identifying the poisoned instances. This approach enhances\nexplainability by explicitly revealing the harmful part of the trigger. The\neffectiveness of our method is demonstrated through experimental evaluations on\nwell-known image datasets and comparative analysis against three\nstate-of-the-art algorithms: SCAn, ABL, and AGPD."},{"date":"2025-06","title":"Self-Refining Language Model Anonymizers via Adversarial Distillation","author":"Kyuyoung Kim, Hyunjun Jeon, and Jinwoo Shin","link":"http://arxiv.org/abs/2506.01420v1","abstract":"Large language models (LLMs) are increasingly used in sensitive domains,\nwhere their ability to infer personal data from seemingly benign text poses\nemerging privacy risks. While recent LLM-based anonymization methods help\nmitigate such risks, they often rely on proprietary models (e.g., GPT-4),\nraising concerns about cost and the potential exposure of sensitive data to\nuntrusted external systems. To address this, we introduce SElf-refining\nAnonymization with Language model (SEAL), a novel distillation framework for\ntraining small language models (SLMs) to perform effective anonymization\nwithout relying on external costly models at inference time. We leverage\nadversarial interactions between an LLM anonymizer and an inference model to\ncollect trajectories of anonymized texts and inferred attributes, which are\nused to distill anonymization, adversarial inference, and utility evaluation\ncapabilities into SLMs via supervised fine-tuning and preference learning. The\nresulting models learn to both anonymize text and critique their outputs,\nenabling iterative improvement of anonymization quality via self-refinement.\nExperiments on SynthPAI, a dataset of synthetic personal profiles and text\ncomments, demonstrate that SLMs trained with SEAL achieve substantial\nimprovements in anonymization capabilities. Notably, 8B models attain a\nprivacy-utility trade-off comparable to that of the GPT-4 anonymizer and, with\nself-refinement, even surpass it in terms of privacy. These results show the\neffectiveness of our adversarial distillation framework in training SLMs as\nefficient anonymizers. To facilitate further research, we release the full\ndataset used in our experiments."},{"date":"2025-06","title":"Formal Security Analysis of SPV Clients Versus Home-Based Full Nodes in Bitcoin-Derived Systems","author":"Craig Steven Wright","link":"http://arxiv.org/abs/2506.01384v1","abstract":"This paper presents a mathematically rigorous formal analysis of Simplified\nPayment Verification (SPV) clients, as specified in Section 8 of the original\nBitcoin white paper, versus non-mining full nodes operated by home users. It\ndefines security as resistance to divergence from global consensus and models\ntransaction acceptance, enforcement capability, and divergence probability\nunder adversarial conditions. The results demonstrate that SPV clients, despite\nomitting script verification, are cryptographically sufficient under\nhonest-majority assumptions and topologically less vulnerable to attack than\nstructurally passive, non-enforcing full nodes. The paper introduces new axioms\non behavioral divergence and communication topology, proving that home-based\nfull nodes increase systemic entropy without contributing to consensus\nintegrity. Using a series of formally defined lemmas, propositions, and Monte\nCarlo simulation results, it is shown that SPV clients represent the rational\nequilibrium strategy for non-mining participants. This challenges the\nprevailing narrative that home validators enhance network security, providing\nformal and operational justifications for the sufficiency of SPV models."},{"date":"2025-06","title":"TimeGraph: Synthetic Benchmark Datasets for Robust Time-Series Causal Discovery","author":"Muhammad Hasan Ferdous, Emam Hossain, and Md Osman Gani","link":"http://arxiv.org/abs/2506.01361v1","abstract":"Robust causal discovery in time series datasets depends on reliable benchmark\ndatasets with known ground-truth causal relationships. However, such datasets\nremain scarce, and existing synthetic alternatives often overlook critical\ntemporal properties inherent in real-world data, including nonstationarity\ndriven by trends and seasonality, irregular sampling intervals, and the\npresence of unobserved confounders. To address these challenges, we introduce\nTimeGraph, a comprehensive suite of synthetic time-series benchmark datasets\nthat systematically incorporates both linear and nonlinear dependencies while\nmodeling key temporal characteristics such as trends, seasonal effects, and\nheterogeneous noise patterns. Each dataset is accompanied by a fully specified\ncausal graph featuring varying densities and diverse noise distributions and is\nprovided in two versions: one including unobserved confounders and one without,\nthereby offering extensive coverage of real-world complexity while preserving\nmethodological neutrality. We further demonstrate the utility of TimeGraph\nthrough systematic evaluations of state-of-the-art causal discovery algorithms\nincluding PCMCI+, LPCMCI, and FGES across a diverse array of configurations and\nmetrics. Our experiments reveal significant variations in algorithmic\nperformance under realistic temporal conditions, underscoring the need for\nrobust synthetic benchmarks in the fair and transparent assessment of causal\ndiscovery methods. The complete TimeGraph suite, including dataset generation\nscripts, evaluation metrics, and recommended experimental protocols, is freely\navailable to facilitate reproducible research and foster community-driven\nadvancements in time-series causal discovery."},{"date":"2025-06","title":"Distributionally Robust Learning in Survival Analysis","author":"Yeping Jin, Lauren Wise, and Ioannis Paschalidis","link":"http://arxiv.org/abs/2506.01348v1","abstract":"We introduce an innovative approach that incorporates a Distributionally\nRobust Learning (DRL) approach into Cox regression to enhance the robustness\nand accuracy of survival predictions. By formulating a DRL framework with a\nWasserstein distance-based ambiguity set, we develop a variant Cox model that\nis less sensitive to assumptions about the underlying data distribution and\nmore resilient to model misspecification and data perturbations. By leveraging\nWasserstein duality, we reformulate the original min-max DRL problem into a\ntractable regularized empirical risk minimization problem, which can be\ncomputed by exponential conic programming. We provide guarantees on the finite\nsample behavior of our DRL-Cox model. Moreover, through extensive simulations\nand real world case studies, we demonstrate that our regression model achieves\nsuperior performance in terms of prediction accuracy and robustness compared\nwith traditional methods."},{"date":"2025-06","title":"ETDI: Mitigating Tool Squatting and Rug Pull Attacks in Model Context Protocol (MCP) by using OAuth-Enhanced Tool Definitions and Policy-Based Access Control","author":"Manish Bhatt, Vineeth Sai Narajala, and Idan Habler","link":"http://arxiv.org/abs/2506.01333v1","abstract":"The Model Context Protocol (MCP) plays a crucial role in extending the\ncapabilities of Large Language Models (LLMs) by enabling integration with\nexternal tools and data sources. However, the standard MCP specification\npresents significant security vulnerabilities, notably Tool Poisoning and Rug\nPull attacks. This paper introduces the Enhanced Tool Definition Interface\n(ETDI), a security extension designed to fortify MCP. ETDI incorporates\ncryptographic identity verification, immutable versioned tool definitions, and\nexplicit permission management, often leveraging OAuth 2.0. We further propose\nextending MCP with fine-grained, policy-based access control, where tool\ncapabilities are dynamically evaluated against explicit policies using a\ndedicated policy engine, considering runtime context beyond static OAuth\nscopes. This layered approach aims to establish a more secure, trustworthy, and\ncontrollable ecosystem for AI applications interacting with LLMs and external\ntools."},{"date":"2025-06","title":"Unlearning's Blind Spots: Over-Unlearning and Prototypical Relearning Attack","author":"SeungBum Ha, Saerom Park, and Sung Whan Yoon","link":"http://arxiv.org/abs/2506.01318v2","abstract":"Machine unlearning (MU) aims to expunge a designated forget set from a\ntrained model without costly retraining, yet the existing techniques overlook\ntwo critical blind spots: \"over-unlearning\" that deteriorates retained data\nnear the forget set, and post-hoc \"relearning\" attacks that aim to resurrect\nthe forgotten knowledge. We first derive the over-unlearning metric\nOU@{\\epsilon}, which represents the collateral damage to the nearby region of\nthe forget set, where the over-unlearning mainly appears. Next, we expose an\nunforeseen relearning threat on MU, i.e., the Prototypical Relearning Attack,\nwhich exploits the per-class prototype of the forget class with just a few\nsamples, and easily restores the pre-unlearning performance. To counter both\nblind spots, we introduce Spotter, a plug-and-play objective that combines (i)\na masked knowledge-distillation penalty on the nearby region of forget set to\nsuppress OU@{\\epsilon}, and (ii) an intra-class dispersion loss that scatters\nforget-class embeddings, neutralizing prototypical relearning attacks. On\nCIFAR-10, as one of validations, Spotter reduces OU@{\\epsilon}by below the\n0.05X of the baseline, drives forget accuracy to 0%, preserves accuracy of the\nretain set within 1% of difference with the original, and denies the\nprototype-attack by keeping the forget set accuracy within <1%, without\naccessing retained data. It confirms that Spotter is a practical remedy of the\nunlearning's blind spots."},{"date":"2025-06","title":"Align is not Enough: Multimodal Universal Jailbreak Attack against Multimodal Large Language Models","author":"Youze Wang, Wenbo Hu, Yinpeng Dong, Jing Liu, Hanwang Zhang, and Richang Hong","link":"http://arxiv.org/abs/2506.01307v1","abstract":"Large Language Models (LLMs) have evolved into Multimodal Large Language\nModels (MLLMs), significantly enhancing their capabilities by integrating\nvisual information and other types, thus aligning more closely with the nature\nof human intelligence, which processes a variety of data forms beyond just\ntext. Despite advancements, the undesirable generation of these models remains\na critical concern, particularly due to vulnerabilities exposed by text-based\njailbreak attacks, which have represented a significant threat by challenging\nexisting safety protocols. Motivated by the unique security risks posed by the\nintegration of new and old modalities for MLLMs, we propose a unified\nmultimodal universal jailbreak attack framework that leverages iterative\nimage-text interactions and transfer-based strategy to generate a universal\nadversarial suffix and image. Our work not only highlights the interaction of\nimage-text modalities can be used as a critical vulnerability but also\nvalidates that multimodal universal jailbreak attacks can bring higher-quality\nundesirable generations across different MLLMs. We evaluate the undesirable\ncontext generation of MLLMs like LLaVA, Yi-VL, MiniGPT4, MiniGPT-v2, and\nInstructBLIP, and reveal significant multimodal safety alignment issues,\nhighlighting the inadequacy of current safety mechanisms against sophisticated\nmultimodal attacks. This study underscores the urgent need for robust safety\nmeasures in MLLMs, advocating for a comprehensive review and enhancement of\nsecurity protocols to mitigate potential risks associated with multimodal\ncapabilities."},{"date":"2025-06","title":"Adversarial learning for nonparametric regression: Minimax rate and adaptive estimation","author":"Jingfu Peng, and Yuhong Yang","link":"http://arxiv.org/abs/2506.01267v1","abstract":"Despite tremendous advancements of machine learning models and algorithms in\nvarious application domains, they are known to be vulnerable to subtle, natural\nor intentionally crafted perturbations in future input data, known as\nadversarial attacks. While numerous adversarial learning methods have been\nproposed, fundamental questions about their statistical optimality in robust\nloss remain largely unanswered. In particular, the minimax rate of convergence\nand the construction of rate-optimal estimators under future $X$-attacks are\nyet to be worked out.\n  In this paper, we address this issue in the context of nonparametric\nregression, under suitable assumptions on the smoothness of the regression\nfunction and the geometric structure of the input perturbation set. We first\nestablish the minimax rate of convergence under adversarial $L_q$-risks with $1\n\\leq q \\leq \\infty$ and propose a piecewise local polynomial estimator that\nachieves the minimax optimality. The established minimax rate elucidates how\nthe smoothness level and perturbation magnitude affect the fundamental limit of\nadversarial learning under future $X$-attacks. Furthermore, we construct a\ndata-driven adaptive estimator that is shown to achieve, within a logarithmic\nfactor, the optimal rate across a broad scale of nonparametric and adversarial\nclasses."},{"date":"2025-06","title":"Stress-Testing ML Pipelines with Adversarial Data Corruption","author":"Jiongli Zhu, Geyang Xu, Felipe Lorenzi, Boris Glavic, and Babak Salimi","link":"http://arxiv.org/abs/2506.01230v1","abstract":"Structured data-quality issues, such as missing values correlated with\ndemographics, culturally biased labels, or systemic selection biases, routinely\ndegrade the reliability of machine-learning pipelines. Regulators now\nincreasingly demand evidence that high-stakes systems can withstand these\nrealistic, interdependent errors, yet current robustness evaluations typically\nuse random or overly simplistic corruptions, leaving worst-case scenarios\nunexplored. We introduce SAVAGE, a causally inspired framework that (i)\nformally models realistic data-quality issues through dependency graphs and\nflexible corruption templates, and (ii) systematically discovers corruption\npatterns that maximally degrade a target performance metric. SAVAGE employs a\nbi-level optimization approach to efficiently identify vulnerable data\nsubpopulations and fine-tune corruption severity, treating the full ML\npipeline, including preprocessing and potentially non-differentiable models, as\na black box. Extensive experiments across multiple datasets and ML tasks (data\ncleaning, fairness-aware learning, uncertainty quantification) demonstrate that\neven a small fraction (around 5 %) of structured corruptions identified by\nSAVAGE severely impacts model performance, far exceeding random or manually\ncrafted errors, and invalidating core assumptions of existing techniques. Thus,\nSAVAGE provides a practical tool for rigorous pipeline stress-testing, a\nbenchmark for evaluating robustness methods, and actionable guidance for\ndesigning more resilient data workflows."},{"date":"2025-06","title":"SPEAR: Security Posture Evaluation using AI Planner-Reasoning on Attack-Connectivity Hypergraphs","author":"Rakesh Podder, Turgay Caglar, Shadaab Kawnain Bashir, Sarath Sreedharan, Indrajit Ray, and Indrakshi Ray","link":"http://arxiv.org/abs/2506.01227v1","abstract":"Graph-based frameworks are often used in network hardening to help a cyber\ndefender understand how a network can be attacked and how the best defenses can\nbe deployed. However, incorporating network connectivity parameters in the\nattack graph, reasoning about the attack graph when we do not have access to\ncomplete information, providing system administrator suggestions in an\nunderstandable format, and allowing them to do what-if analysis on various\nscenarios and attacker motives is still missing. We fill this gap by presenting\nSPEAR, a formal framework with tool support for security posture evaluation and\nanalysis that keeps human-in-the-loop. SPEAR uses the causal formalism of AI\nplanning to model vulnerabilities and configurations in a networked system. It\nautomatically converts network configurations and vulnerability descriptions\ninto planning models expressed in the Planning Domain Definition Language\n(PDDL). SPEAR identifies a set of diverse security hardening strategies that\ncan be presented in a manner understandable to the domain expert. These allow\nthe administrator to explore the network hardening solution space in a\nsystematic fashion and help evaluate the impact and compare the different\nsolutions."},{"date":"2025-06","title":"Dirty and Clean-Label attack detection using GAN discriminators","author":"John W. Smutny","link":"http://arxiv.org/abs/2506.01224v2","abstract":"Gathering enough images to train a deep computer vision model is a constant\nchallenge. Unfortunately, collecting images from unknown sources can leave your\nmodel s behavior at risk of being manipulated by a dirty-label or clean-label\nattack unless the images are properly inspected. Manually inspecting each\nimage-label pair is impractical and common poison-detection methods that\ninvolve re-training your model can be time consuming. This research uses GAN\ndiscriminators to protect a single class against mislabeled and different\nlevels of modified images. The effect of said perturbation on a basic\nconvolutional neural network classifier is also included for reference. The\nresults suggest that after training on a single class, GAN discriminator s\nconfidence scores can provide a threshold to identify mislabeled images and\nidentify 100% of the tested poison starting at a perturbation epsilon magnitude\nof 0.20, after decision threshold calibration using in-class samples.\nDevelopers can use this report as a basis to train their own discriminators to\nprotect high valued classes in their CV models."},{"date":"2025-06","title":"FedRPCA: Enhancing Federated LoRA Aggregation Using Robust PCA","author":"Divyansh Jhunjhunwala, Arian Raje, Madan Ravi Ganesh, Chaithanya Kumar Mummadi, Chaoqun Dong, Jiawei Zhou, Wan-Yi Lin, Gauri Joshi, and Zhenzhen Li","link":"http://arxiv.org/abs/2506.01194v1","abstract":"LoRA has emerged as one of the most promising fine-tuning techniques,\nespecially for federated learning (FL), since it significantly reduces\ncommunication and computation costs at resource-constrained clients. However,\ndata heterogeneity remains a significant challenge for LoRA-based FL, and the\nconventional aggregation strategy based on FedAvg suffers from slow convergence\nand suboptimal accuracy. Motivated by recent advances in model merging,\nparticularly Task Arithmetic, we explore the idea of aggregating client LoRA\nparameters using scaled averaging. We first observe that a naive application of\nTask Arithmetic is ineffective due to the high cosine similarity between client\nupdates, indicating significant common knowledge in the updates across clients.\nTo address this issue, we propose decomposing client LoRA updates via Robust\nPrincipal Component Analysis (Robust-PCA) into a common low-rank component and\nclient-specific sparse components. Our proposed algorithm FedRPCA aggregates\nthe low-rank components through averaging, consolidating common knowledge, and\napplies scaled averaging to the sparse components to amplify client-specific\nknowledge. We evaluate our approach across a variety of vision and language\ntasks and demonstrate that it achieves higher final accuracy and faster\nconvergence compared to competing baselines."},{"date":"2025-06","title":"Doubly Robust Alignment for Large Language Models","author":"Erhan Xu, Kai Ye, Hongyi Zhou, Luhan Zhu, Francesco Quinzan, and Chengchun Shi","link":"http://arxiv.org/abs/2506.01183v1","abstract":"This paper studies reinforcement learning from human feedback (RLHF) for\naligning large language models with human preferences. While RLHF has\ndemonstrated promising results, many algorithms are highly sensitive to\nmisspecifications in the underlying preference model (e.g., the Bradley-Terry\nmodel), the reference policy, or the reward function, resulting in undesirable\nfine-tuning. To address model misspecification, we propose a doubly robust\npreference optimization algorithm that remains consistent when either the\npreference model or the reference policy is correctly specified (without\nrequiring both). Our proposal demonstrates superior and more robust performance\nthan state-of-the-art algorithms, both in theory and in practice. The code is\navailable at https://github.com/DRPO4LLM/DRPO4LLM"},{"date":"2025-06","title":"Neuro-Symbolic Generative Diffusion Models for Physically Grounded, Robust, and Safe Generation","author":"Jacob K. Christopher, Michael Cardei, Jinhao Liang, and Ferdinando Fioretto","link":"http://arxiv.org/abs/2506.01121v1","abstract":"Despite the remarkable generative capabilities of diffusion models, their\nintegration into safety-critical or scientifically rigorous applications\nremains hindered by the need to ensure compliance with stringent physical,\nstructural, and operational constraints. To address this challenge, this paper\nintroduces Neuro-Symbolic Diffusion (NSD), a novel framework that interleaves\ndiffusion steps with symbolic optimization, enabling the generation of\ncertifiably consistent samples under user-defined functional and logic\nconstraints. This key feature is provided for both standard and discrete\ndiffusion models, enabling, for the first time, the generation of both\ncontinuous (e.g., images and trajectories) and discrete (e.g., molecular\nstructures and natural language) outputs that comply with constraints. This\nability is demonstrated on tasks spanning three key challenges: (1) Safety, in\nthe context of non-toxic molecular generation and collision-free trajectory\noptimization; (2) Data scarcity, in domains such as drug discovery and\nmaterials engineering; and (3) Out-of-domain generalization, where enforcing\nsymbolic constraints allows adaptation beyond the training distribution."},{"date":"2025-06","title":"IDCloak: A Practical Secure Multi-party Dataset Join Framework for Vertical Privacy-preserving Machine Learning","author":"Shuyu Chen, Guopeng Lin, Haoyu Niu, Lushan Song, Chengxun Hong, and Weili Han","link":"http://arxiv.org/abs/2506.01072v1","abstract":"Vertical privacy-preserving machine learning (vPPML) enables multiple parties\nto train models on their vertically distributed datasets while keeping datasets\nprivate. In vPPML, it is critical to perform the secure dataset join, which\naligns features corresponding to intersection IDs across datasets and forms a\nsecret-shared and joint training dataset. However, existing methods for this\nstep could be impractical due to: (1) they are insecure when they expose\nintersection IDs; or (2) they rely on a strong trust assumption requiring a\nnon-colluding auxiliary server; or (3) they are limited to the two-party\nsetting.\n  This paper proposes IDCloak, the first practical secure multi-party dataset\njoin framework for vPPML that keeps IDs private without a non-colluding\nauxiliary server. IDCloak consists of two protocols: (1) a circuit-based\nmulti-party private set intersection protocol (cmPSI), which obtains\nsecret-shared flags indicating intersection IDs via an optimized communication\nstructure combining OKVS and OPRF; (2) a secure multi-party feature alignment\nprotocol, which obtains the secret-shared and joint dataset using secret-shared\nflags, via our proposed efficient secure shuffle protocol. Experiments show\nthat: (1) compared to the state-of-the-art secure two-party dataset join\nframework (iPrivjoin), IDCloak demonstrates higher efficiency in the two-party\nsetting and comparable performance when the party number increases; (2)\ncompared to the state-of-the-art cmPSI protocol under honest majority, our\nproposed cmPSI protocol provides a stronger security guarantee (dishonest\nmajority) while improving efficiency by up to $7.78\\times$ in time and\n$8.73\\times$ in communication sizes; (3) our proposed secure shuffle protocol\noutperforms the state-of-the-art shuffle protocol by up to $138.34\\times$ in\ntime and $132.13\\times$ in communication sizes."},{"date":"2025-06","title":"Fighting Fire with Fire (F3): A Training-free and Efficient Visual Adversarial Example Purification Method in LVLMs","author":"Yudong Zhang, Ruobing Xie, Yiqing Huang, Jiansheng Chen, Xingwu Sun, Zhanhui Kang, Di Wang, and Yu Wang","link":"http://arxiv.org/abs/2506.01064v1","abstract":"Recent advances in large vision-language models (LVLMs) have showcased their\nremarkable capabilities across a wide range of multimodal vision-language\ntasks. However, these models remain vulnerable to visual adversarial attacks,\nwhich can substantially compromise their performance. Despite their potential\nimpact, the development of effective methods for purifying such adversarial\nexamples has received relatively limited attention. In this paper, we introduce\nF3, a novel adversarial purification framework that employs a counterintuitive\n\"fighting fire with fire\" strategy: intentionally introducing simple\nperturbations to adversarial examples to mitigate their harmful effects.\nSpecifically, F3 leverages cross-modal attentions derived from randomly\nperturbed adversary examples as reference targets. By injecting noise into\nthese adversarial examples, F3 effectively refines their attention, resulting\nin cleaner and more reliable model outputs. Remarkably, this seemingly\nparadoxical approach of employing noise to counteract adversarial attacks\nyields impressive purification results. Furthermore, F3 offers several distinct\nadvantages: it is training-free and straightforward to implement, and exhibits\nsignificant computational efficiency improvements compared to existing\npurification methods. These attributes render F3 particularly suitable for\nlarge-scale industrial applications where both robust performance and\noperational efficiency are critical priorities. The code will be made publicly\navailable."},{"date":"2025-06","title":"Simple Prompt Injection Attacks Can Leak Personal Data Observed by LLM Agents During Task Execution","author":"Meysam Alizadeh, Zeynab Samei, Daria Stetsenko, and Fabrizio Gilardi","link":"http://arxiv.org/abs/2506.01055v1","abstract":"Previous benchmarks on prompt injection in large language models (LLMs) have\nprimarily focused on generic tasks and attacks, offering limited insights into\nmore complex threats like data exfiltration. This paper examines how prompt\ninjection can cause tool-calling agents to leak personal data observed during\ntask execution. Using a fictitious banking agent, we develop data flow-based\nattacks and integrate them into AgentDojo, a recent benchmark for agentic\nsecurity. To enhance its scope, we also create a richer synthetic dataset of\nhuman-AI banking conversations. In 16 user tasks from AgentDojo, LLMs show a\n15-50 percentage point drop in utility under attack, with average attack\nsuccess rates (ASR) around 20 percent; some defenses reduce ASR to zero. Most\nLLMs, even when successfully tricked by the attack, avoid leaking highly\nsensitive data like passwords, likely due to safety alignments, but they remain\nvulnerable to disclosing other personal data. The likelihood of password\nleakage increases when a password is requested along with one or two additional\npersonal details. In an extended evaluation across 48 tasks, the average ASR is\naround 15 percent, with no built-in AgentDojo defense fully preventing leakage.\nTasks involving data extraction or authorization workflows, which closely\nresemble the structure of exfiltration attacks, exhibit the highest ASRs,\nhighlighting the interaction between task type, agent performance, and defense\nefficacy."},{"date":"2025-06","title":"Autoregressive Images Watermarking through Lexical Biasing: An Approach Resistant to Regeneration Attack","author":"Siqi Hui, Yiren Song, Sanping Zhou, Ye Deng, Wenli Huang, and Jinjun Wang","link":"http://arxiv.org/abs/2506.01011v1","abstract":"Autoregressive (AR) image generation models have gained increasing attention\nfor their breakthroughs in synthesis quality, highlighting the need for robust\nwatermarking to prevent misuse. However, existing in-generation watermarking\ntechniques are primarily designed for diffusion models, where watermarks are\nembedded within diffusion latent states. This design poses significant\nchallenges for direct adaptation to AR models, which generate images\nsequentially through token prediction. Moreover, diffusion-based regeneration\nattacks can effectively erase such watermarks by perturbing diffusion latent\nstates. To address these challenges, we propose Lexical Bias Watermarking\n(LBW), a novel framework designed for AR models that resists regeneration\nattacks. LBW embeds watermarks directly into token maps by biasing token\nselection toward a predefined green list during generation. This approach\nensures seamless integration with existing AR models and extends naturally to\npost-hoc watermarking. To increase the security against white-box attacks,\ninstead of using a single green list, the green list for each image is randomly\nsampled from a pool of green lists. Watermark detection is performed via\nquantization and statistical analysis of the token distribution. Extensive\nexperiments demonstrate that LBW achieves superior watermark robustness,\nparticularly in resisting regeneration attacks."},{"date":"2025-06","title":"CAPAA: Classifier-Agnostic Projector-Based Adversarial Attack","author":"Zhan Li, Mingyu Zhao, Xin Dong, Haibin Ling, and Bingyao Huang","link":"http://arxiv.org/abs/2506.00978v1","abstract":"Projector-based adversarial attack aims to project carefully designed light\npatterns (i.e., adversarial projections) onto scenes to deceive deep image\nclassifiers. It has potential applications in privacy protection and the\ndevelopment of more robust classifiers. However, existing approaches primarily\nfocus on individual classifiers and fixed camera poses, often neglecting the\ncomplexities of multi-classifier systems and scenarios with varying camera\nposes. This limitation reduces their effectiveness when introducing new\nclassifiers or camera poses. In this paper, we introduce Classifier-Agnostic\nProjector-Based Adversarial Attack (CAPAA) to address these issues. First, we\ndevelop a novel classifier-agnostic adversarial loss and optimization framework\nthat aggregates adversarial and stealthiness loss gradients from multiple\nclassifiers. Then, we propose an attention-based gradient weighting mechanism\nthat concentrates perturbations on regions of high classification activation,\nthereby improving the robustness of adversarial projections when applied to\nscenes with varying camera poses. Our extensive experimental evaluations\ndemonstrate that CAPAA achieves both a higher attack success rate and greater\nstealthiness compared to existing baselines. Codes are available at:\nhttps://github.com/ZhanLiQxQ/CAPAA."},{"date":"2025-06","title":"Hidden Representation Clustering with Multi-Task Representation Learning towards Robust Online Budget Allocation","author":"Xiaohan Wang, Yu Zhang, Guibin Jiang, Bing Cheng, and Wei Lin","link":"http://arxiv.org/abs/2506.00959v1","abstract":"Marketing optimization, commonly formulated as an online budget allocation\nproblem, has emerged as a pivotal factor in driving user growth. Most existing\nresearch addresses this problem by following the principle of 'first predict\nthen optimize' for each individual, which presents challenges related to\nlarge-scale counterfactual prediction and solving complexity trade-offs. Note\nthat the practical data quality is uncontrollable, and the solving scale tends\nto be tens of millions. Therefore, the existing approaches make the robust\nbudget allocation non-trivial, especially in industrial scenarios with\nconsiderable data noise. To this end, this paper proposes a novel approach that\nsolves the problem from the cluster perspective. Specifically, we propose a\nmulti-task representation network to learn the inherent attributes of\nindividuals and project the original features into high-dimension hidden\nrepresentations through the first two layers of the trained network. Then, we\ndivide these hidden representations into $K$ groups through partitioning-based\nclustering, thus reformulating the problem as an integer stochastic programming\nproblem under different total budgets. Finally, we distill the representation\nmodule and clustering model into a multi-category model to facilitate online\ndeployment. Offline experiments validate the effectiveness and superiority of\nour approach compared to six state-of-the-art marketing optimization\nalgorithms. Online A/B tests on the Meituan platform indicate that the approach\noutperforms the online algorithm by 0.53% and 0.65%, considering order volume\n(OV) and gross merchandise volume (GMV), respectively."},{"date":"2025-06","title":"SafeGenes: Evaluating the Adversarial Robustness of Genomic Foundation Models","author":"Huixin Zhan, and Jason H. Moore","link":"http://arxiv.org/abs/2506.00821v1","abstract":"Genomic Foundation Models (GFMs), such as Evolutionary Scale Modeling (ESM),\nhave demonstrated significant success in variant effect prediction. However,\ntheir adversarial robustness remains largely unexplored. To address this gap,\nwe propose SafeGenes: a framework for Secure analysis of genomic foundation\nmodels, leveraging adversarial attacks to evaluate robustness against both\nengineered near-identical adversarial Genes and embedding-space manipulations.\nIn this study, we assess the adversarial vulnerabilities of GFMs using two\napproaches: the Fast Gradient Sign Method (FGSM) and a soft prompt attack. FGSM\nintroduces minimal perturbations to input sequences, while the soft prompt\nattack optimizes continuous embeddings to manipulate model predictions without\nmodifying the input tokens. By combining these techniques, SafeGenes provides a\ncomprehensive assessment of GFM susceptibility to adversarial manipulation.\nTargeted soft prompt attacks led to substantial performance degradation, even\nin large models such as ESM1b and ESM1v. These findings expose critical\nvulnerabilities in current foundation models, opening new research directions\ntoward improving their security and robustness in high-stakes genomic\napplications such as variant effect prediction."},{"date":"2025-06","title":"TIME: TabPFN-Integrated Multimodal Engine for Robust Tabular-Image Learning","author":"Jiaqi Luo, Yuan Yuan, and Shixin Xu","link":"http://arxiv.org/abs/2506.00813v1","abstract":"Tabular-image multimodal learning, which integrates structured tabular data\nwith imaging data, holds great promise for a variety of tasks, especially in\nmedical applications. Yet, two key challenges remain: (1) the lack of a\nstandardized, pretrained representation for tabular data, as is commonly\navailable in vision and language domains; and (2) the difficulty of handling\nmissing values in the tabular modality, which are common in real-world medical\ndatasets. To address these issues, we propose the TabPFN-Integrated Multimodal\nEngine (TIME), a novel multimodal framework that builds on the recently\nintroduced tabular foundation model, TabPFN. TIME leverages TabPFN as a frozen\ntabular encoder to generate robust, strong embeddings that are naturally\nresilient to missing data, and combines them with image features from\npretrained vision backbones. We explore a range of fusion strategies and\ntabular encoders, and evaluate our approach on both natural and medical\ndatasets. Extensive experiments demonstrate that TIME consistently outperforms\ncompetitive baselines across both complete and incomplete tabular inputs,\nunderscoring its practical value in real-world multimodal learning scenarios."},{"date":"2025-06","title":"Unlearning Inversion Attacks for Graph Neural Networks","author":"Jiahao Zhang, Yilong Wang, Zhiwei Zhang, Xiaorui Liu, and Suhang Wang","link":"http://arxiv.org/abs/2506.00808v1","abstract":"Graph unlearning methods aim to efficiently remove the impact of sensitive\ndata from trained GNNs without full retraining, assuming that deleted\ninformation cannot be recovered. In this work, we challenge this assumption by\nintroducing the graph unlearning inversion attack: given only black-box access\nto an unlearned GNN and partial graph knowledge, can an adversary reconstruct\nthe removed edges? We identify two key challenges: varying\nprobability-similarity thresholds for unlearned versus retained edges, and the\ndifficulty of locating unlearned edge endpoints, and address them with\nTrendAttack. First, we derive and exploit the confidence pitfall, a theoretical\nand empirical pattern showing that nodes adjacent to unlearned edges exhibit a\nlarge drop in model confidence. Second, we design an adaptive prediction\nmechanism that applies different similarity thresholds to unlearned and other\nmembership edges. Our framework flexibly integrates existing membership\ninference techniques and extends them with trend features. Experiments on four\nreal-world datasets demonstrate that TrendAttack significantly outperforms\nstate-of-the-art GNN membership inference baselines, exposing a critical\nprivacy vulnerability in current graph unlearning methods."},{"date":"2025-05","title":"Poster: Adapting Pretrained Vision Transformers with LoRA Against Attack Vectors","author":"Richard E. Neddo, Sean Willis, Zander Blasingame, and Chen Liu","link":"http://arxiv.org/abs/2506.00661v1","abstract":"Image classifiers, such as those used for autonomous vehicle navigation, are\nlargely known to be susceptible to adversarial attacks that target the input\nimage set. There is extensive discussion on adversarial attacks including\nperturbations that alter the input images to cause malicious misclassifications\nwithout perceivable modification. This work proposes a countermeasure for such\nattacks by adjusting the weights and classes of pretrained vision transformers\nwith a low-rank adaptation to become more robust against adversarial attacks\nand allow for scalable fine-tuning without retraining."},{"date":"2025-05","title":"Amatriciana: Exploiting Temporal GNNs for Robust and Efficient Money Laundering Detection","author":"Marco Di Gennaro, Francesco Panebianco, Marco Pianta, Stefano Zanero, and Michele Carminati","link":"http://arxiv.org/abs/2506.00654v1","abstract":"Money laundering is a financial crime that poses a serious threat to\nfinancial integrity and social security. The growing number of transactions\nmakes it necessary to use automatic tools that help law enforcement agencies\ndetect such criminal activity. In this work, we present Amatriciana, a novel\napproach based on Graph Neural Networks to detect money launderers inside a\ngraph of transactions by considering temporal information. Amatriciana uses the\nwhole graph of transactions without splitting it into several time-based\nsubgraphs, exploiting all relational information in the dataset. Our\nexperiments on a public dataset reveal that the model can learn from a limited\namount of data. Furthermore, when more data is available, the model outperforms\nother State-of-the-art approaches; in particular, Amatriciana decreases the\nnumber of False Positives (FPs) while detecting many launderers. In summary,\nAmatriciana achieves an F1 score of 0.76. In addition, it lowers the FPs by 55%\nwith respect to other State-of-the-art models."},{"date":"2025-05","title":"AgentAuditor: Human-Level Safety and Security Evaluation for LLM Agents","author":"Hanjun Luo, Shenyu Dai, Chiming Ni, Xinfeng Li, Guibin Zhang, Kun Wang, Tongliang Liu, and Hanan Salam","link":"http://arxiv.org/abs/2506.00641v1","abstract":"Despite the rapid advancement of LLM-based agents, the reliable evaluation of\ntheir safety and security remains a significant challenge. Existing rule-based\nor LLM-based evaluators often miss dangers in agents' step-by-step actions,\noverlook subtle meanings, fail to see how small issues compound, and get\nconfused by unclear safety or security rules. To overcome this evaluation\ncrisis, we introduce \\sys, a universal, training-free, memory-augmented\nreasoning framework that empowers LLM evaluators to emulate human expert\nevaluators. \\sys constructs an experiential memory by having an LLM adaptively\nextract structured semantic features (e.g., scenario, risk, behavior) and\ngenerate associated chain-of-thought reasoning traces for past interactions. A\nmulti-stage, context-aware retrieval-augmented generation process then\ndynamically retrieves the most relevant reasoning experiences to guide the LLM\nevaluator's assessment of new cases. Moreover, we developed \\data, the first\nbenchmark designed to check how well LLM-based evaluators can spot both safety\nrisks and security threats. \\data comprises \\textbf{2293} meticulously\nannotated interaction records, covering \\textbf{15} risk types across\n\\textbf{29} application scenarios. A key feature of \\data is its nuanced\napproach to ambiguous risk situations, employing ``Strict'' and ``Lenient''\njudgment standards. Experiments demonstrate that \\sys not only consistently\nimproves the evaluation performance of LLMs across all benchmarks but also sets\na new state-of-the-art in LLM-as-a-judge for agent safety and security,\nachieving human-level accuracy. Our work is openly openly accessible."},{"date":"2025-05","title":"A \"Wenlu\" Brain System for Multimodal Cognition and Embodied Decision-Making: A Secure New Architecture for Deep Integration of Foundation Models and Domain Knowledge","author":"Liang Geng","link":"http://arxiv.org/abs/2506.00570v1","abstract":"With the rapid penetration of artificial intelligence across industries and\nscenarios, a key challenge in building the next-generation intelligent core\nlies in effectively integrating the language understanding capabilities of\nfoundation models with domain-specific knowledge bases in complex real-world\napplications. This paper proposes a multimodal cognition and embodied\ndecision-making brain system, ``Wenlu\", designed to enable secure fusion of\nprivate knowledge and public models, unified processing of multimodal data such\nas images and speech, and closed-loop decision-making from cognition to\nautomatic generation of hardware-level code. The system introduces a\nbrain-inspired memory tagging and replay mechanism, seamlessly integrating\nuser-private data, industry-specific knowledge, and general-purpose language\nmodels. It provides precise and efficient multimodal services for enterprise\ndecision support, medical analysis, autonomous driving, robotic control, and\nmore. Compared with existing solutions, ``Wenlu\" demonstrates significant\nadvantages in multimodal processing, privacy security, end-to-end hardware\ncontrol code generation, self-learning, and sustainable updates, thus laying a\nsolid foundation for constructing the next-generation intelligent core."},{"date":"2025-05","title":"Docker under Siege: Securing Containers in the Modern Era","author":"Gogulakrishnan Thiyagarajan, and Prabhudarshi Nayak","link":"http://arxiv.org/abs/2506.02043v1","abstract":"Containerization, driven by Docker, has transformed application development\nand deployment by enhancing efficiency and scalability. However, the rapid\nadoption of container technologies introduces significant security challenges\nthat require careful management. This paper investigates key areas of container\nsecurity, including runtime protection, network safeguards, configuration best\npractices, supply chain security, and comprehensive monitoring and logging\nsolutions. We identify common vulnerabilities within these domains and provide\nactionable recommendations to address and mitigate these risks. By integrating\nsecurity throughout the Software Development Lifecycle (SDLC), organizations\ncan reinforce their security posture, creating a resilient and reliable\ncontainerized application infrastructure that withstands evolving threats."},{"date":"2025-05","title":"The Security Threat of Compressed Projectors in Large Vision-Language Models","author":"Yudong Zhang, Ruobing Xie, Xingwu Sun, Jiansheng Chen, Zhanhui Kang, Di Wang, and Yu Wang","link":"http://arxiv.org/abs/2506.00534v1","abstract":"The choice of a suitable visual language projector (VLP) is critical to the\nsuccessful training of large visual language models (LVLMs). Mainstream VLPs\ncan be broadly categorized into compressed and uncompressed projectors, and\neach offering distinct advantages in performance and computational efficiency.\nHowever, their security implications have not been thoroughly examined. Our\ncomprehensive evaluation reveals significant differences in their security\nprofiles: compressed projectors exhibit substantial vulnerabilities, allowing\nadversaries to successfully compromise LVLMs even with minimal knowledge of\nstructural information. In stark contrast, uncompressed projectors demonstrate\nrobust security properties and do not introduce additional vulnerabilities.\nThese findings provide critical guidance for researchers in selecting optimal\nVLPs that enhance the security and reliability of visual language models. The\ncode will be released."},{"date":"2025-05","title":"Robust and Verifiable MPC with Applications to Linear Machine Learning Inference","author":"Tzu-Shen Wang, Jimmy Dani, Juan Garay, Soamar Homsi, and Nitesh Saxena","link":"http://arxiv.org/abs/2506.00518v1","abstract":"In this work, we present an efficient secure multi-party computation MPC\nprotocol that provides strong security guarantees in settings with dishonest\nmajority of participants who may behave arbitrarily. Unlike the popular MPC\nimplementation known as SPDZ [Crypto '12], which only ensures security with\nabort, our protocol achieves both complete identifiability and robustness. With\ncomplete identifiability, honest parties can detect and unanimously agree on\nthe identity of any malicious party. Robustness allows the protocol to continue\nwith the computation without requiring a restart, even when malicious behavior\nis detected. Additionally, our approach addresses the performance limitations\nobserved in the protocol by Cunningham et al. [ICITS '17], which, while\nachieving complete identifiability, is hindered by the costly exponentiation\noperations required by the choice of commitment scheme.\n  Our protocol is based on the approach by Rivinius et al. [S&P '22], utilizing\nlattice-based commitment for better efficiency. We achieved robustness with the\nhelp of a semi-honest trusted third party. We benchmark our robust protocol,\nshowing the efficient recovery from parties' malicious behavior.\n  Finally, we benchmark our protocol on a ML-as-a-service scenario, wherein\nclients off-load the desired computation to the servers, and verify the\ncomputation result. We benchmark on linear ML inference, running on various\ndatasets. While our efficiency is slightly lower compared to SPDZ's, we offer\nstronger security properties that provide distinct advantages."},{"date":"2025-05","title":"Monitoring Robustness and Individual Fairness","author":"Ashutosh Gupta, Thomas A. Henzinger, Konstantin Kueffner, Kaushik Mallik, and David Pape","link":"http://arxiv.org/abs/2506.00496v1","abstract":"Input-output robustness appears in various different forms in the literature,\nsuch as robustness of AI models to adversarial or semantic perturbations and\nindividual fairness of AI models that make decisions about humans.\n  We propose runtime monitoring of input-output robustness of deployed,\nblack-box AI models, where the goal is to design monitors that would observe\none long execution sequence of the model, and would raise an alarm whenever it\nis detected that two similar inputs from the past led to dissimilar outputs.\n  This way, monitoring will complement existing offline ``robustification''\napproaches to increase the trustworthiness of AI decision-makers.\n  We show that the monitoring problem can be cast as the fixed-radius nearest\nneighbor (FRNN) search problem, which, despite being well-studied, lacks\nsuitable online solutions.\n  We present our tool Clemont, which offers a number of lightweight monitors,\nsome of which use upgraded online variants of existing FRNN algorithms, and one\nuses a novel algorithm based on binary decision diagrams -- a data-structure\ncommonly used in software and hardware verification.\n  We have also developed an efficient parallelization technique that can\nsubstantially cut down the computation time of monitors for which the distance\nbetween input-output pairs is measured using the $L_\\infty$ norm.\n  Using standard benchmarks from the literature of adversarial and semantic\nrobustness and individual fairness, we perform a comparative study of different\nmonitors in \\tool, and demonstrate their effectiveness in correctly detecting\nrobustness violations at runtime."},{"date":"2025-05","title":"Beyond the Protocol: Unveiling Attack Vectors in the Model Context Protocol Ecosystem","author":"Hao Song, Yiming Shen, Wenxuan Luo, Leixin Guo, Ting Chen, Jiashui Wang, Beibei Li, Xiaosong Zhang, and Jiachi Chen","link":"http://arxiv.org/abs/2506.02040v2","abstract":"The Model Context Protocol (MCP) is an emerging standard designed to enable\nseamless interaction between Large Language Model (LLM) applications and\nexternal tools or resources. Within a short period, thousands of MCP services\nhave already been developed and deployed. However, the client-server\nintegration architecture inherent in MCP may expand the attack surface against\nLLM Agent systems, introducing new vulnerabilities that allow attackers to\nexploit by designing malicious MCP servers. In this paper, we present the first\nsystematic study of attack vectors targeting the MCP ecosystem. Our analysis\nidentifies four categories of attacks, i.e., Tool Poisoning Attacks, Puppet\nAttacks, Rug Pull Attacks, and Exploitation via Malicious External Resources.\nTo evaluate the feasibility of these attacks, we conduct experiments following\nthe typical steps of launching an attack through malicious MCP servers:\nupload-download-attack. Specifically, we first construct malicious MCP servers\nand successfully upload them to three widely used MCP aggregation platforms.\nThe results indicate that current audit mechanisms are insufficient to identify\nand prevent the proposed attack methods. Next, through a user study and\ninterview with 20 participants, we demonstrate that users struggle to identify\nmalicious MCP servers and often unknowingly install them from aggregator\nplatforms. Finally, we demonstrate that these attacks can trigger harmful\nbehaviors within the user's local environment-such as accessing private files\nor controlling devices to transfer digital assets-by deploying a\nproof-of-concept (PoC) framework against five leading LLMs. Additionally, based\non interview results, we discuss four key challenges faced by the current\nsecurity ecosystem surrounding MCP servers. These findings underscore the\nurgent need for robust security mechanisms to defend against malicious MCP\nservers."},{"date":"2025-05","title":"Hybrid Cloud Security: Balancing Performance, Cost, and Compliance in Multi-Cloud Deployments","author":"Anjani kumar Polinati","link":"http://arxiv.org/abs/2506.00426v1","abstract":"The pervasive use of hybrid cloud computing models has changed enterprise as\nwell as Information Technology services infrastructure by giving businesses\nsimple and cost-effective options of combining on-premise IT equipment with\npublic cloud services. hybrid cloud solutions deploy multifaceted models of\nsecurity, performance optimization, and cost efficiency, conventionally\nfragmented in the cloud computing milieu. This paper examines how organizations\nmanage these parameters in hybrid cloud ecosystems while providing solutions to\nthe challenges they face in operationalizing hybrid cloud adoptions. The study\ncaptures the challenges of achieving a balance in resource distribution between\non-premise and cloud resources (herein referred to as the \"resource allocation\nchallenge\"), the complexity of pricing models from cloud providers like AWS,\nMicrosoft Azure, Google Cloud (herein called the 'pricing complexity problem'),\nand the urgency for strong security infrastructure to safeguard sensitive\ninformation (known as 'the information security problem'). This study\ndemonstrates the security and performance management solutions proposed were\nvalidated in a detailed case study of adoption of AWS and Azure based hybrid\ncloud and provides useful guidance. Also, a hybrid cloud security and cost\noptimization framework based on zero trust architecture, encryption, hybrid\ncloud policies, and others, is proposed.\n  The conclusion includes recommendations for research on automation of hybrid\ncloud service management, integration of multi-clouds, and the ever-present\nquestion of data privacy, stressing how those matters affect contemporary\nenterprises."},{"date":"2025-05","title":"Teaching an Old LLM Secure Coding: Localized Preference Optimization on Distilled Preferences","author":"Mohammad Saqib, Saikat Chakraborty, Santu Karmaker, and Niranjan Balasubramanian","link":"http://arxiv.org/abs/2506.00419v1","abstract":"LLM generated code often contains security issues. We address two key\nchallenges in improving secure code generation. First, obtaining high quality\ntraining data covering a broad set of security issues is critical. To address\nthis, we introduce a method for distilling a preference dataset of insecure and\nsecure code pairs from frontier LLMs, along with a security reasoning that\nexplains the issues and the fix. The key idea here is to make use of security\nknowledge sources to devise a systematic prompting strategy that ensures broad\ncoverage. Second, aligning models to secure code requires focusing on localized\nregions of code. Direct preference optimization methods, like SimPO, are not\ndesigned to handle these localized differences and turn out to be ineffective.\nWe address this with a new localized preference optimization algorithm that\nmasks the security related tokens in both the winning (secure) and losing\n(insecure) responses. To prevent loss in code quality, we also add a\nregularizer. Evaluations show that both training on our dataset, DiSCo, and the\nnew preference optimization algorithm, LPO, yield substantial reductions in\ncode insecurity while also improving overall code quality. Code and dataset are\navailable at https://github.com/StonyBrookNLP/disco-lpo."},{"date":"2025-05","title":"Label-shift robust federated feature screening for high-dimensional classification","author":"Qi Qin, Erbo Li, Xingxiang Li, Yifan Sun, Wu Wang, and Chen Xu","link":"http://arxiv.org/abs/2506.00379v1","abstract":"Distributed and federated learning are important tools for high-dimensional\nclassification of large datasets. To reduce computational costs and overcome\nthe curse of dimensionality, feature screening plays a pivotal role in\neliminating irrelevant features during data preprocessing. However, data\nheterogeneity, particularly label shifting across different clients, presents\nsignificant challenges for feature screening. This paper introduces a general\nframework that unifies existing screening methods and proposes a novel utility,\nlabel-shift robust federated feature screening (LR-FFS), along with its\nfederated estimation procedure. The framework facilitates a uniform analysis of\nmethods and systematically characterizes their behaviors under label shift\nconditions. Building upon this framework, LR-FFS leverages conditional\ndistribution functions and expectations to address label shift without adding\ncomputational burdens and remains robust against model misspecification and\noutliers. Additionally, the federated procedure ensures computational\nefficiency and privacy protection while maintaining screening effectiveness\ncomparable to centralized processing. We also provide a false discovery rate\n(FDR) control method for federated feature screening. Experimental results and\ntheoretical analyses demonstrate LR-FFS's superior performance across diverse\nclient environments, including those with varying class distributions, sample\nsizes, and missing categorical data."},{"date":"2025-05","title":"Adversarial Machine Learning for Robust Password Strength Estimation","author":"Pappu Jha, Hanzla Hamid, Oluseyi Olukola, Ashim Dahal, and Nick Rahimi","link":"http://arxiv.org/abs/2506.00373v1","abstract":"Passwords remain one of the most common methods for securing sensitive data\nin the digital age. However, weak password choices continue to pose significant\nrisks to data security and privacy. This study aims to solve the problem by\nfocusing on developing robust password strength estimation models using\nadversarial machine learning, a technique that trains models on intentionally\ncrafted deceptive passwords to expose and address vulnerabilities posed by such\npasswords. We apply five classification algorithms and use a dataset with more\nthan 670,000 samples of adversarial passwords to train the models. Results\ndemonstrate that adversarial training improves password strength classification\naccuracy by up to 20% compared to traditional machine learning models. It\nhighlights the importance of integrating adversarial machine learning into\nsecurity systems to enhance their robustness against modern adaptive threats.\n  Keywords: adversarial attack, password strength, classification, machine\nlearning"},{"date":"2025-05","title":"$\\texttt{AVROBUSTBENCH}$: Benchmarking the Robustness of Audio-Visual Recognition Models at Test-Time","author":"Sarthak Kumar Maharana, Saksham Singh Kushwaha, Baoming Zhang, Adrian Rodriguez, Songtao Wei, Yapeng Tian, and Yunhui Guo","link":"http://arxiv.org/abs/2506.00358v1","abstract":"While recent audio-visual models have demonstrated impressive performance,\ntheir robustness to distributional shifts at test-time remains not fully\nunderstood. Existing robustness benchmarks mainly focus on single modalities,\nmaking them insufficient for thoroughly assessing the robustness of\naudio-visual models. Motivated by real-world scenarios where shifts can occur\n$\\textit{simultaneously}$ in both audio and visual modalities, we introduce\n$\\texttt{AVROBUSTBENCH}$, a comprehensive benchmark designed to evaluate the\ntest-time robustness of audio-visual recognition models.\n$\\texttt{AVROBUSTBENCH}$ comprises four audio-visual benchmark datasets,\n$\\texttt{AUDIOSET-2C}$, $\\texttt{VGGSOUND-2C}$, $\\texttt{KINETICS-2C}$, and\n$\\texttt{EPICKITCHENS-2C}$, each incorporating 75 bimodal audio-visual\ncorruptions that are $\\textit{co-occurring}$ and $\\textit{correlated}$. Through\nextensive evaluations, we observe that state-of-the-art supervised and\nself-supervised audio-visual models exhibit declining robustness as corruption\nseverity increases. Furthermore, online test-time adaptation (TTA) methods, on\n$\\texttt{VGGSOUND-2C}$ and $\\texttt{KINETICS-2C}$, offer minimal improvements\nin performance under bimodal corruptions. We further propose $\\texttt{AV2C}$, a\nsimple TTA approach enabling on-the-fly cross-modal fusion by penalizing\nhigh-entropy samples, which achieves improvements on $\\texttt{VGGSOUND-2C}$. We\nhope that $\\texttt{AVROBUSTBENCH}$ will steer the development of more effective\nand robust audio-visual TTA approaches. Our code is available\n$\\href{https://github.com/sarthaxxxxx/AV-C-Robustness-Benchmark}{here}$."},{"date":"2025-05","title":"Enabling Secure and Ephemeral AI Workloads in Data Mesh Environments","author":"Chinkit Patel, and Kee Siong Ng","link":"http://arxiv.org/abs/2506.00352v1","abstract":"Many large enterprises that operate highly governed and complex ICT\nenvironments have no efficient and effective way to support their Data and AI\nteams in rapidly spinning up and tearing down self-service data and compute\ninfrastructure, to experiment with new data analytic tools, and deploy data\nproducts into operational use. This paper proposes a key piece of the solution\nto the overall problem, in the form of an on-demand self-service data-platform\ninfrastructure to empower de-centralised data teams to build data products on\ntop of centralised templates, policies and governance. The core innovation is\nan efficient method to leverage immutable container operating systems and\ninfrastructure-as-code methodologies for creating, from scratch, vendor-neutral\nand short-lived Kubernetes clusters on-premises and in any cloud environment.\nOur proposed approach can serve as a repeatable, portable and cost-efficient\nalternative or complement to commercial Platform-as-a-Service (PaaS) offerings,\nand this is particularly important in supporting interoperability in complex\ndata mesh environments with a mix of modern and legacy compute infrastructure."},{"date":"2025-05","title":"Towards Effective and Efficient Adversarial Defense with Diffusion Models for Robust Visual Tracking","author":"Long Xu, Peng Gao, Wen-Jia Tang, Fei Wang, and Ru-Yue Yuan","link":"http://arxiv.org/abs/2506.00325v1","abstract":"Although deep learning-based visual tracking methods have made significant\nprogress, they exhibit vulnerabilities when facing carefully designed\nadversarial attacks, which can lead to a sharp decline in tracking performance.\nTo address this issue, this paper proposes for the first time a novel\nadversarial defense method based on denoise diffusion probabilistic models,\ntermed DiffDf, aimed at effectively improving the robustness of existing visual\ntracking methods against adversarial attacks. DiffDf establishes a multi-scale\ndefense mechanism by combining pixel-level reconstruction loss, semantic\nconsistency loss, and structural similarity loss, effectively suppressing\nadversarial perturbations through a gradual denoising process. Extensive\nexperimental results on several mainstream datasets show that the DiffDf method\ndemonstrates excellent generalization performance for trackers with different\narchitectures, significantly improving various evaluation metrics while\nachieving real-time inference speeds of over 30 FPS, showcasing outstanding\ndefense performance and efficiency. Codes are available at\nhttps://github.com/pgao-lab/DiffDf."},{"date":"2025-05","title":"Adversarial Threat Vectors and Risk Mitigation for Retrieval-Augmented Generation Systems","author":"Chris M. Ward, and Josh Harguess","link":"http://arxiv.org/abs/2506.00281v1","abstract":"Retrieval-Augmented Generation (RAG) systems, which integrate Large Language\nModels (LLMs) with external knowledge sources, are vulnerable to a range of\nadversarial attack vectors. This paper examines the importance of RAG systems\nthrough recent industry adoption trends and identifies the prominent attack\nvectors for RAG: prompt injection, data poisoning, and adversarial query\nmanipulation. We analyze these threats under risk management lens, and propose\nrobust prioritized control list that includes risk-mitigating actions like\ninput validation, adversarial training, and real-time monitoring."},{"date":"2025-05","title":"DeGLIF for Label Noise Robust Node Classification using GNNs","author":"Pintu Kumar, and Nandyala Hemachandra","link":"http://arxiv.org/abs/2506.00244v1","abstract":"Noisy labelled datasets are generally inexpensive compared to clean labelled\ndatasets, and the same is true for graph data. In this paper, we propose a\ndenoising technique DeGLIF: Denoising Graph Data using Leave-One-Out Influence\nFunction. DeGLIF uses a small set of clean data and the leave-one-out influence\nfunction to make label noise robust node-level prediction on graph data.\nLeave-one-out influence function approximates the change in the model\nparameters if a training point is removed from the training dataset. Recent\nadvances propose a way to calculate the leave-one-out influence function for\nGraph Neural Networks (GNNs). We extend that recent work to estimate the change\nin validation loss, if a training node is removed from the training dataset. We\nuse this estimate and a new theoretically motivated relabelling function to\ndenoise the training dataset. We propose two DeGLIF variants to identify noisy\nnodes. Both these variants do not require any information about the noise model\nor the noise level in the dataset; DeGLIF also does not estimate these\nquantities. For one of these variants, we prove that the noisy points detected\ncan indeed increase risk. We carry out detailed computational experiments on\ndifferent datasets to show the effectiveness of DeGLIF. It achieves better\naccuracy than other baseline algorithms"},{"date":"2025-05","title":"Heterogeneous Graph Backdoor Attack","author":"Jiawei Chen, Lusi Li, Daniel Takabi, Masha Sosonkina, and Rui Ning","link":"http://arxiv.org/abs/2506.00191v1","abstract":"Heterogeneous Graph Neural Networks (HGNNs) excel in modeling complex,\nmulti-typed relationships across diverse domains, yet their vulnerability to\nbackdoor attacks remains unexplored. To address this gap, we conduct the first\ninvestigation into the susceptibility of HGNNs to existing graph backdoor\nattacks, revealing three critical issues: (1) high attack budget required for\neffective backdoor injection, (2) inefficient and unreliable backdoor\nactivation, and (3) inaccurate attack effectiveness evaluation. To tackle these\nissues, we propose the Heterogeneous Graph Backdoor Attack (HGBA), the first\nbackdoor attack specifically designed for HGNNs, introducing a novel\nrelation-based trigger mechanism that establishes specific connections between\na strategically selected trigger node and poisoned nodes via the backdoor\nmetapath. HGBA achieves efficient and stealthy backdoor injection with minimal\nstructural modifications and supports easy backdoor activation through two\nflexible strategies: Self-Node Attack and Indiscriminate Attack. Additionally,\nwe improve the ASR measurement protocol, enabling a more accurate assessment of\nattack effectiveness. Extensive experiments demonstrate that HGBA far surpasses\nmultiple state-of-the-art graph backdoor attacks in black-box settings,\nefficiently attacking HGNNs with low attack budgets. Ablation studies show that\nthe strength of HBGA benefits from our trigger node selection method and\nbackdoor metapath selection strategy. In addition, HGBA shows superior\nrobustness against node feature perturbations and multiple types of existing\ngraph backdoor defense mechanisms. Finally, extension experiments demonstrate\nthat the relation-based trigger mechanism can effectively extend to tasks in\nhomogeneous graph scenarios, thereby posing severe threats to broader\nsecurity-critical domains."},{"date":"2025-05","title":"Towards Secure MLOps: Surveying Attacks, Mitigation Strategies, and Research Challenges","author":"Raj Patel, Himanshu Tripathi, Jasper Stone, Noorbakhsh Amiri Golilarz, Sudip Mittal, Shahram Rahimi, and Vini Chaudhary","link":"http://arxiv.org/abs/2506.02032v1","abstract":"The rapid adoption of machine learning (ML) technologies has driven\norganizations across diverse sectors to seek efficient and reliable methods to\naccelerate model development-to-deployment. Machine Learning Operations (MLOps)\nhas emerged as an integrative approach addressing these requirements by\nunifying relevant roles and streamlining ML workflows. As the MLOps market\ncontinues to grow, securing these pipelines has become increasingly critical.\nHowever, the unified nature of MLOps ecosystem introduces vulnerabilities,\nmaking them susceptible to adversarial attacks where a single misconfiguration\ncan lead to compromised credentials, severe financial losses, damaged public\ntrust, and the poisoning of training data. Our paper presents a systematic\napplication of the MITRE ATLAS (Adversarial Threat Landscape for\nArtificial-Intelligence Systems) framework, a comprehensive and continuously\nupdated catalog of AI-focused attacks, to systematically assess attacks across\ndifferent phases of the MLOps ecosystem. We begin by examining the preparatory\nphases during which adversaries acquire the essential intelligence required to\ninitiate their attacks. We then present a structured taxonomy of attack\ntechniques explicitly mapped to corresponding phases of the MLOps ecosystem,\nsupported by examples drawn from red-teaming exercises and real-world\nincidents. This is followed by a taxonomy of mitigation strategies aligned with\nthese attack categories, offering actionable early-stage defenses to strengthen\nthe security of MLOps ecosystem. Given the rapid evolution and adoption of\nMLOps, we further highlight key research gaps that require immediate attention.\nOur work emphasizes the importance of implementing robust security protocols\nfrom the outset, empowering practitioners to safeguard MLOps ecosystem against\nevolving cyber attacks."},{"date":"2025-05","title":"From Invariant Representations to Invariant Data: Provable Robustness to Spurious Correlations via Noisy Counterfactual Matching","author":"Ruqi Bai, Yao Ji, Zeyu Zhou, and David I. Inouye","link":"http://arxiv.org/abs/2505.24843v1","abstract":"Spurious correlations can cause model performance to degrade in new\nenvironments. Prior causality-inspired works aim to learn invariant\nrepresentations (e.g., IRM) but typically underperform empirical risk\nminimization (ERM). Recent alternatives improve robustness by leveraging\ntest-time data, but such data may be unavailable in practice. To address these\nissues, we take a data-centric approach by leveraging invariant data pairs,\npairs of samples that would have the same prediction with the optimally robust\nclassifier. We prove that certain counterfactual pairs will naturally satisfy\nthis invariance property and introduce noisy counterfactual matching (NCM), a\nsimple constraint-based method for leveraging invariant pairs for enhanced\nrobustness, even with a small set of noisy pairs-in the ideal case, each pair\ncan eliminate one spurious feature. For linear causal models, we prove that the\ntest domain error can be upper bounded by the in-domain error and a term that\ndepends on the counterfactuals' diversity and quality. We validate on a\nsynthetic dataset and demonstrate on real-world benchmarks that linear probing\non a pretrained backbone improves robustness."},{"date":"2025-05","title":"Cascading Adversarial Bias from Injection to Distillation in Language Models","author":"Harsh Chaudhari, Jamie Hayes, Matthew Jagielski, Ilia Shumailov, Milad Nasr, and Alina Oprea","link":"http://arxiv.org/abs/2505.24842v1","abstract":"Model distillation has become essential for creating smaller, deployable\nlanguage models that retain larger system capabilities. However, widespread\ndeployment raises concerns about resilience to adversarial manipulation. This\npaper investigates vulnerability of distilled models to adversarial injection\nof biased content during training. We demonstrate that adversaries can inject\nsubtle biases into teacher models through minimal data poisoning, which\npropagates to student models and becomes significantly amplified. We propose\ntwo propagation modes: Untargeted Propagation, where bias affects multiple\ntasks, and Targeted Propagation, focusing on specific tasks while maintaining\nnormal behavior elsewhere. With only 25 poisoned samples (0.25% poisoning\nrate), student models generate biased responses 76.9% of the time in targeted\nscenarios - higher than 69.4% in teacher models. For untargeted propagation,\nadversarial bias appears 6x-29x more frequently in student models on unseen\ntasks. We validate findings across six bias types (targeted advertisements,\nphishing links, narrative manipulations, insecure coding practices), various\ndistillation methods, and different modalities spanning text and code\ngeneration. Our evaluation reveals shortcomings in current defenses -\nperplexity filtering, bias detection systems, and LLM-based autorater\nframeworks - against these attacks. Results expose significant security\nvulnerabilities in distilled models, highlighting need for specialized\nsafeguards. We propose practical design principles for building effective\nadversarial bias mitigation strategies."},{"date":"2025-05","title":"ByzFL: Research Framework for Robust Federated Learning","author":"Marc Gonz\u00e1lez, Rachid Guerraoui, Rafael Pinot, Geovani Rizk, John Stephan, and Fran\u00e7ois Ta\u00efani","link":"http://arxiv.org/abs/2505.24802v1","abstract":"We present ByzFL, an open-source Python library for developing and\nbenchmarking robust federated learning (FL) algorithms. ByzFL provides a\nunified and extensible framework that includes implementations of\nstate-of-the-art robust aggregators, a suite of configurable attacks, and tools\nfor simulating a variety of FL scenarios, including heterogeneous data\ndistributions, multiple training algorithms, and adversarial threat models. The\nlibrary enables systematic experimentation via a single JSON-based\nconfiguration file and includes built-in utilities for result visualization.\nCompatible with PyTorch tensors and NumPy arrays, ByzFL is designed to\nfacilitate reproducible research and rapid prototyping of robust FL solutions.\nByzFL is available at https://byzfl.epfl.ch/, with source code hosted on\nGitHub: https://github.com/LPD-EPFL/byzfl."},{"date":"2025-05","title":"Robust Federated Learning against Model Perturbation in Edge Networks","author":"Dongzi Jin, Yong Xiao, and Yingyu Li","link":"http://arxiv.org/abs/2505.24728v1","abstract":"Federated Learning (FL) is a promising paradigm for realizing edge\nintelligence, allowing collaborative learning among distributed edge devices by\nsharing models instead of raw data. However, the shared models are often\nassumed to be ideal, which would be inevitably violated in practice due to\nvarious perturbations, leading to significant performance degradation. To\novercome this challenge, we propose a novel method, termed Sharpness-Aware\nMinimization-based Robust Federated Learning (SMRFL), which aims to improve\nmodel robustness against perturbations by exploring the geometrical property of\nthe model landscape. Specifically, SMRFL solves a min-max optimization problem\nthat promotes model convergence towards a flat minimum by minimizing the\nmaximum loss within a neighborhood of the model parameters. In this way, model\nsensitivity to perturbations is reduced, and robustness is enhanced since\nmodels in the neighborhood of the flat minimum also enjoy low loss values. The\ntheoretical result proves that SMRFL can converge at the same rate as FL\nwithout perturbations. Extensive experimental results show that SMRFL\nsignificantly enhances robustness against perturbations compared to three\nbaseline methods on two real-world datasets under three perturbation scenarios."},{"date":"2025-05","title":"On Symmetric Losses for Robust Policy Optimization with Noisy Preferences","author":"Soichiro Nishimori, Yu-Jie Zhang, Thanawat Lodkaew, and Masashi Sugiyama","link":"http://arxiv.org/abs/2505.24709v1","abstract":"Optimizing policies based on human preferences is key to aligning language\nmodels with human intent. This work focuses on reward modeling, a core\ncomponent in reinforcement learning from human feedback (RLHF), and offline\npreference optimization, such as direct preference optimization. Conventional\napproaches typically assume accurate annotations. However, real-world\npreference data often contains noise due to human errors or biases. We propose\na principled framework for robust policy optimization under noisy preferences,\nviewing reward modeling as a classification problem. This allows us to leverage\nsymmetric losses, known for their robustness to label noise in classification,\nleading to our Symmetric Preference Optimization (SymPO) method. We prove that\nsymmetric losses enable successful policy optimization even under noisy labels,\nas the resulting reward remains rank-preserving -- a property sufficient for\npolicy improvement. Experiments on synthetic and real-world tasks demonstrate\nthe effectiveness of SymPO."},{"date":"2025-05","title":"PatchDEMUX: A Certifiably Robust Framework for Multi-label Classifiers Against Adversarial Patches","author":"Dennis Jacob, Chong Xiang, and Prateek Mittal","link":"http://arxiv.org/abs/2505.24703v1","abstract":"Deep learning techniques have enabled vast improvements in computer vision\ntechnologies. Nevertheless, these models are vulnerable to adversarial patch\nattacks which catastrophically impair performance. The physically realizable\nnature of these attacks calls for certifiable defenses, which feature provable\nguarantees on robustness. While certifiable defenses have been successfully\napplied to single-label classification, limited work has been done for\nmulti-label classification. In this work, we present PatchDEMUX, a certifiably\nrobust framework for multi-label classifiers against adversarial patches. Our\napproach is a generalizable method which can extend any existing certifiable\ndefense for single-label classification; this is done by considering the\nmulti-label classification task as a series of isolated binary classification\nproblems to provably guarantee robustness. Furthermore, in the scenario where\nan attacker is limited to a single patch we propose an additional certification\nprocedure that can provide tighter robustness bounds. Using the current\nstate-of-the-art (SOTA) single-label certifiable defense PatchCleanser as a\nbackbone, we find that PatchDEMUX can achieve non-trivial robustness on the\nMS-COCO and PASCAL VOC datasets while maintaining high clean performance"},{"date":"2025-05","title":"Black-box Adversarial Attacks on CNN-based SLAM Algorithms","author":"Maria Rafaela Gkeka, Bowen Sun, Evgenia Smirni, Christos D. Antonopoulos, Spyros Lalis, and Nikolaos Bellas","link":"http://arxiv.org/abs/2505.24654v1","abstract":"Continuous advancements in deep learning have led to significant progress in\nfeature detection, resulting in enhanced accuracy in tasks like Simultaneous\nLocalization and Mapping (SLAM). Nevertheless, the vulnerability of deep neural\nnetworks to adversarial attacks remains a challenge for their reliable\ndeployment in applications, such as navigation of autonomous agents. Even\nthough CNN-based SLAM algorithms are a growing area of research there is a\nnotable absence of a comprehensive presentation and examination of adversarial\nattacks targeting CNN-based feature detectors, as part of a SLAM system. Our\nwork introduces black-box adversarial perturbations applied to the RGB images\nfed into the GCN-SLAM algorithm. Our findings on the TUM dataset [30] reveal\nthat even attacks of moderate scale can lead to tracking failure in as many as\n76% of the frames. Moreover, our experiments highlight the catastrophic impact\nof attacking depth instead of RGB input images on the SLAM system."},{"date":"2025-05","title":"A Flat Minima Perspective on Understanding Augmentations and Model Robustness","author":"Weebum Yoo, and Sung Whan Yoon","link":"http://arxiv.org/abs/2505.24592v1","abstract":"Model robustness indicates a model's capability to generalize well on\nunforeseen distributional shifts, including data corruption, adversarial\nattacks, and domain shifts. Data augmentation is one of the prevalent and\neffective ways to enhance robustness. Despite the great success of\naugmentations in different fields, a general theoretical understanding of their\nefficacy in improving model robustness is lacking. We offer a unified\ntheoretical framework to clarify how augmentations can enhance model robustness\nthrough the lens of loss surface flatness and PAC generalization bound. Our\nwork diverges from prior studies in that our analysis i) broadly encompasses\nmuch of the existing augmentation methods, and ii) is not limited to specific\ntypes of distribution shifts like adversarial attacks. We confirm our theories\nthrough simulations on the existing common corruption and adversarial\nrobustness benchmarks based on the CIFAR and ImageNet datasets, as well as\ndomain generalization benchmarks including PACS and OfficeHome."},{"date":"2025-05","title":"CHIP: Chameleon Hash-based Irreversible Passport for Robust Deep Model Ownership Verification and Active Usage Control","author":"Chaohui Xu, Qi Cui, and Chip-Hong Chang","link":"http://arxiv.org/abs/2505.24536v1","abstract":"The pervasion of large-scale Deep Neural Networks (DNNs) and their enormous\ntraining costs make their intellectual property (IP) protection of paramount\nimportance. Recently introduced passport-based methods attempt to steer DNN\nwatermarking towards strengthening ownership verification against ambiguity\nattacks by modulating the affine parameters of normalization layers.\nUnfortunately, neither watermarking nor passport-based methods provide a\nholistic protection with robust ownership proof, high fidelity, active usage\nauthorization and user traceability for offline access distributed models and\nmulti-user Machine-Learning as a Service (MLaaS) cloud model. In this paper, we\npropose a Chameleon Hash-based Irreversible Passport (CHIP) protection\nframework that utilizes the cryptographic chameleon hash function to achieve\nall these goals. The collision-resistant property of chameleon hash allows for\nstrong model ownership claim upon IP infringement and liable user traceability,\nwhile the trapdoor-collision property enables hashing of multiple user\npassports and licensee certificates to the same immutable signature to realize\nactive usage control. Using the owner passport as an oracle, multiple\nuser-specific triplets, each contains a passport-aware user model, a user\npassport, and a licensee certificate can be created for secure offline\ndistribution. The watermarked master model can also be deployed for MLaaS with\nusage permission verifiable by the provision of any trapdoor-colliding user\npassports. CHIP is extensively evaluated on four datasets and two architectures\nto demonstrate its protection versatility and robustness. Our code is released\nat https://github.com/Dshm212/CHIP."},{"date":"2025-05","title":"AMIA: Automatic Masking and Joint Intention Analysis Makes LVLMs Robust Jailbreak Defenders","author":"Yuqi Zhang, Yuchun Miao, Zuchao Li, and Liang Ding","link":"http://arxiv.org/abs/2505.24519v1","abstract":"We introduce AMIA, a lightweight, inference-only defense for Large\nVision-Language Models (LVLMs) that (1) Automatically Masks a small set of\ntext-irrelevant image patches to disrupt adversarial perturbations, and (2)\nconducts joint Intention Analysis to uncover and mitigate hidden harmful\nintents before response generation. Without any retraining, AMIA improves\ndefense success rates across diverse LVLMs and jailbreak benchmarks from an\naverage of 52.4% to 81.7%, preserves general utility with only a 2% average\naccuracy drop, and incurs only modest inference overhead. Ablation confirms\nboth masking and intention analysis are essential for a robust safety-utility\ntrade-off."},{"date":"2025-05","title":"Diversify and Conquer: Open-set Disagreement for Robust Semi-supervised Learning with Outliers","author":"Heejo Kong, Sung-Jin Kim, Gunho Jung, and Seong-Whan Lee","link":"http://arxiv.org/abs/2505.24443v1","abstract":"Conventional semi-supervised learning (SSL) ideally assumes that labeled and\nunlabeled data share an identical class distribution, however in practice, this\nassumption is easily violated, as unlabeled data often includes unknown class\ndata, i.e., outliers. The outliers are treated as noise, considerably degrading\nthe performance of SSL models. To address this drawback, we propose a novel\nframework, Diversify and Conquer (DAC), to enhance SSL robustness in the\ncontext of open-set semi-supervised learning. In particular, we note that\nexisting open-set SSL methods rely on prediction discrepancies between inliers\nand outliers from a single model trained on labeled data. This approach can be\neasily failed when the labeled data is insufficient, leading to performance\ndegradation that is worse than naive SSL that do not account for outliers. In\ncontrast, our approach exploits prediction disagreements among multiple models\nthat are differently biased towards the unlabeled distribution. By leveraging\nthe discrepancies arising from training on unlabeled data, our method enables\nrobust outlier detection even when the labeled data is underspecified. Our key\ncontribution is constructing a collection of differently biased models through\na single training process. By encouraging divergent heads to be differently\nbiased towards outliers while making consistent predictions for inliers, we\nexploit the disagreement among these heads as a measure to identify unknown\nconcepts. Our code is available at https://github.com/heejokong/DivCon."},{"date":"2025-05","title":"pyMEAL: A Multi-Encoder Augmentation-Aware Learning for Robust and Generalizable Medical Image Translation","author":"Abdul-mojeed Olabisi Ilyas, Adeleke Maradesa, Jamal Banzi, Jianpan Huang, Henry K. F. Mak, and Kannie W. Y. Chan","link":"http://arxiv.org/abs/2505.24421v1","abstract":"Medical imaging is critical for diagnostics, but clinical adoption of\nadvanced AI-driven imaging faces challenges due to patient variability, image\nartifacts, and limited model generalization. While deep learning has\ntransformed image analysis, 3D medical imaging still suffers from data scarcity\nand inconsistencies due to acquisition protocols, scanner differences, and\npatient motion. Traditional augmentation uses a single pipeline for all\ntransformations, disregarding the unique traits of each augmentation and\nstruggling with large data volumes.\n  To address these challenges, we propose a Multi-encoder Augmentation-Aware\nLearning (MEAL) framework that leverages four distinct augmentation variants\nprocessed through dedicated encoders. Three fusion strategies such as\nconcatenation (CC), fusion layer (FL), and adaptive controller block (BD) are\nintegrated to build multi-encoder models that combine augmentation-specific\nfeatures before decoding. MEAL-BD uniquely preserves augmentation-aware\nrepresentations, enabling robust, protocol-invariant feature learning.\n  As demonstrated in a Computed Tomography (CT)-to-T1-weighted Magnetic\nResonance Imaging (MRI) translation study, MEAL-BD consistently achieved the\nbest performance on both unseen- and predefined-test data. On both geometric\ntransformations (like rotations and flips) and non-augmented inputs, MEAL-BD\noutperformed other competing methods, achieving higher mean peak\nsignal-to-noise ratio (PSNR) and structural similarity index measure (SSIM)\nscores. These results establish MEAL as a reliable framework for preserving\nstructural fidelity and generalizing across clinically relevant variability. By\nreframing augmentation as a source of diverse, generalizable features, MEAL\nsupports robust, protocol-invariant learning, advancing clinically reliable\nmedical imaging solutions."},{"date":"2025-05","title":"Adversarial Preference Learning for Robust LLM Alignment","author":"Yuanfu Wang, Pengyu Wang, Chenyang Xi, Bo Tang, Junyi Zhu, Wenqiang Wei, Chen Chen, Chao Yang, Jingfeng Zhang, Chaochao Lu, Yijun Niu, Keming Mao, Zhiyu Li, Feiyu Xiong, Jie Hu, and Mingchuan Yang","link":"http://arxiv.org/abs/2505.24369v1","abstract":"Modern language models often rely on Reinforcement Learning from Human\nFeedback (RLHF) to encourage safe behaviors. However, they remain vulnerable to\nadversarial attacks due to three key limitations: (1) the inefficiency and high\ncost of human annotation, (2) the vast diversity of potential adversarial\nattacks, and (3) the risk of feedback bias and reward hacking. To address these\nchallenges, we introduce Adversarial Preference Learning (APL), an iterative\nadversarial training method incorporating three key innovations. First, a\ndirect harmfulness metric based on the model's intrinsic preference\nprobabilities, eliminating reliance on external assessment. Second, a\nconditional generative attacker that synthesizes input-specific adversarial\nvariations. Third, an iterative framework with automated closed-loop feedback,\nenabling continuous adaptation through vulnerability discovery and mitigation.\nExperiments on Mistral-7B-Instruct-v0.3 demonstrate that APL significantly\nenhances robustness, achieving 83.33% harmlessness win rate over the base model\n(evaluated by GPT-4o), reducing harmful outputs from 5.88% to 0.43% (measured\nby LLaMA-Guard), and lowering attack success rate by up to 65% according to\nHarmBench. Notably, APL maintains competitive utility, with an MT-Bench score\nof 6.59 (comparable to the baseline 6.78) and an LC-WinRate of 46.52% against\nthe base model."},{"date":"2025-05","title":"Faithful and Robust LLM-Driven Theorem Proving for NLI Explanations","author":"Xin Quan, Marco Valentino, Louise A. Dennis, and Andr\u00e9 Freitas","link":"http://arxiv.org/abs/2505.24264v1","abstract":"Natural language explanations play a fundamental role in Natural Language\nInference (NLI) by revealing how premises logically entail hypotheses. Recent\nwork has shown that the interaction of large language models (LLMs) with\ntheorem provers (TPs) can help verify and improve the validity of NLI\nexplanations. However, TPs require translating natural language into\nmachine-verifiable formal representations, a process that introduces the risk\nof semantic information loss and unfaithful interpretation, an issue compounded\nby LLMs' challenges in capturing critical logical structures with sufficient\nprecision. Moreover, LLMs are still limited in their capacity for rigorous and\nrobust proof construction within formal verification frameworks. To mitigate\nissues related to faithfulness and robustness, this paper investigates\nstrategies to (1) alleviate semantic loss during autoformalisation, (2)\nefficiently identify and correct syntactic errors in logical representations,\n(3) explicitly use logical expressions to guide LLMs in generating structured\nproof sketches, and (4) increase LLMs' capacity of interpreting TP's feedback\nfor iterative refinement. Our empirical results on e-SNLI, QASC and WorldTree\nusing different LLMs demonstrate that the proposed strategies yield significant\nimprovements in autoformalisation (+18.46%, +34.2%, +39.77%) and explanation\nrefinement (+29.5%, +51.5%, +41.25%) over the state-of-the-art model. Moreover,\nwe show that specific interventions on the hybrid LLM-TP architecture can\nsubstantially improve efficiency, drastically reducing the number of iterations\nrequired for successful verification."},{"date":"2025-05","title":"Harnessing Foundation Models for Robust and Generalizable 6-DOF Bronchoscopy Localization","author":"Qingyao Tian, Huai Liao, Xinyan Huang, Bingyu Yang, and Hongbin Liu","link":"http://arxiv.org/abs/2505.24249v1","abstract":"Vision-based 6-DOF bronchoscopy localization offers a promising solution for\naccurate and cost-effective interventional guidance. However, existing methods\nstruggle with 1) limited generalization across patient cases due to scarce\nlabeled data, and 2) poor robustness under visual degradation, as bronchoscopy\nprocedures frequently involve artifacts such as occlusions and motion blur that\nimpair visual information. To address these challenges, we propose PANSv2, a\ngeneralizable and robust bronchoscopy localization framework. Motivated by PANS\nthat leverages multiple visual cues for pose likelihood measurement, PANSv2\nintegrates depth estimation, landmark detection, and centerline constraints\ninto a unified pose optimization framework that evaluates pose probability and\nsolves for the optimal bronchoscope pose. To further enhance generalization\ncapabilities, we leverage the endoscopic foundation model EndoOmni for depth\nestimation and the video foundation model EndoMamba for landmark detection,\nincorporating both spatial and temporal analyses. Pretrained on diverse\nendoscopic datasets, these models provide stable and transferable visual\nrepresentations, enabling reliable performance across varied bronchoscopy\nscenarios. Additionally, to improve robustness to visual degradation, we\nintroduce an automatic re-initialization module that detects tracking failures\nand re-establishes pose using landmark detections once clear views are\navailable. Experimental results on bronchoscopy dataset encompassing 10 patient\ncases show that PANSv2 achieves the highest tracking success rate, with an\n18.1% improvement in SR-5 (percentage of absolute trajectory error under 5 mm)\ncompared to existing methods, showing potential towards real clinical usage."},{"date":"2025-05","title":"An Adversary-Resistant Multi-Agent LLM System via Credibility Scoring","author":"Sana Ebrahimi, Mohsen Dehghankar, and Abolfazl Asudeh","link":"http://arxiv.org/abs/2505.24239v1","abstract":"While multi-agent LLM systems show strong capabilities in various domains,\nthey are highly vulnerable to adversarial and low-performing agents. To resolve\nthis issue, in this paper, we introduce a general and adversary-resistant\nmulti-agent LLM framework based on credibility scoring. We model the\ncollaborative query-answering process as an iterative game, where the agents\ncommunicate and contribute to a final system output. Our system associates a\ncredibility score that is used when aggregating the team outputs. The\ncredibility scores are learned gradually based on the past contributions of\neach agent in query answering. Our experiments across multiple tasks and\nsettings demonstrate our system's effectiveness in mitigating adversarial\ninfluence and enhancing the resilience of multi-agent cooperation, even in the\nadversary-majority settings."},{"date":"2025-05","title":"Bootstrapping LLM Robustness for VLM Safety via Reducing the Pretraining Modality Gap","author":"Wenhan Yang, Spencer Stice, Ali Payani, and Baharan Mirzasoleiman","link":"http://arxiv.org/abs/2505.24208v1","abstract":"Ensuring Vision-Language Models (VLMs) generate safe outputs is crucial for\ntheir reliable deployment. However, LVLMs suffer from drastic safety\ndegradation compared to their LLM backbone. Even blank or irrelevant images can\ntrigger LVLMs to generate harmful responses to prompts that would otherwise be\nrefused in text-only contexts. The modality gap between image and text\nrepresentations has been recently hypothesized to contribute to safety\ndegradation of LVLMs. However, if and how the amount of modality gap affects\nLVLMs' safety is not studied. In this work, we show that the amount of modality\ngap is highly inversely correlated with VLMs' safety. Then, we show that this\nmodality gap is introduced during pretraining LVLMs and persists through\nfine-tuning. Inspired by this observation, we propose a regularization to\nreduce the modality gap during pretraining. Our extensive experiments on LLaVA\nv1.5, ShareGPT4V, and MiniGPT-4 show that our method substantially improves\nsafety alignment of LVLMs, reducing unsafe rate by up to 16.3% without\ncompromising performance, and can further boost existing defenses by up to\n18.2%."},{"date":"2025-05","title":"Don't Just Follow MLLM Plans: Robust and Efficient Planning for Open-world Agents","author":"Seungjoon Lee, Suhwan Kim, Minhyeon Oh, Youngsik Yoon, and Jungseul Ok","link":"http://arxiv.org/abs/2505.24157v1","abstract":"Developing autonomous agents capable of mastering complex, multi-step tasks\nin unpredictable, interactive environments presents a significant challenge.\nWhile Large Language Models (LLMs) offer promise for planning, existing\napproaches often rely on problematic internal knowledge or make unrealistic\nenvironmental assumptions. Although recent work explores learning planning\nknowledge, they still retain limitations due to partial reliance on external\nknowledge or impractical setups. Indeed, prior research has largely overlooked\ndeveloping agents capable of acquiring planning knowledge from scratch,\ndirectly in realistic settings. While realizing this capability is necessary,\nit presents significant challenges, primarily achieving robustness given the\nsubstantial risk of incorporating LLMs' inaccurate knowledge. Moreover,\nefficiency is crucial for practicality as learning can demand prohibitive\nexploration. In response, we introduce Robust and Efficient Planning for\nOpen-world Agents (REPOA), a novel framework designed to tackle these issues.\nREPOA features three key components: adaptive dependency learning and\nfine-grained failure-aware operation memory to enhance robustness to knowledge\ninaccuracies, and difficulty-based exploration to improve learning efficiency.\nOur evaluation in two established open-world testbeds demonstrates REPOA's\nrobust and efficient planning, showcasing its capability to successfully obtain\nchallenging late-game items that were beyond the reach of prior approaches."},{"date":"2025-05","title":"The Butterfly Effect in Pathology: Exploring Security in Pathology Foundation Models","author":"Jiashuai Liu, Yingjia Shang, Yingkang Zhan, Di Zhang, Yi Niu, Dong Wei, Xian Wu, Zeyu Gao, Chen Li, and Yefeng Zheng","link":"http://arxiv.org/abs/2505.24141v1","abstract":"With the widespread adoption of pathology foundation models in both research\nand clinical decision support systems, exploring their security has become a\ncritical concern. However, despite their growing impact, the vulnerability of\nthese models to adversarial attacks remains largely unexplored. In this work,\nwe present the first systematic investigation into the security of pathology\nfoundation models for whole slide image~(WSI) analysis against adversarial\nattacks. Specifically, we introduce the principle of \\textit{local perturbation\nwith global impact} and propose a label-free attack framework that operates\nwithout requiring access to downstream task labels. Under this attack\nframework, we revise four classical white-box attack methods and redefine the\nperturbation budget based on the characteristics of WSI. We conduct\ncomprehensive experiments on three representative pathology foundation models\nacross five datasets and six downstream tasks. Despite modifying only 0.1\\% of\npatches per slide with imperceptible noise, our attack leads to downstream\naccuracy degradation that can reach up to 20\\% in the worst cases. Furthermore,\nwe analyze key factors that influence attack success, explore the relationship\nbetween patch-level vulnerability and semantic content, and conduct a\npreliminary investigation into potential defence strategies. These findings lay\nthe groundwork for future research on the adversarial robustness and reliable\ndeployment of pathology foundation models. Our code is publicly available at:\nhttps://github.com/Jiashuai-Liu-hmos/Attack-WSI-pathology-foundation-models."},{"date":"2025-05","title":"Practical Bayes-Optimal Membership Inference Attacks","author":"Marcus Lassila, Johan \u00d6stman, Khac-Hoang Ngo, and Alexandre Graell i Amat","link":"http://arxiv.org/abs/2505.24089v1","abstract":"We develop practical and theoretically grounded membership inference attacks\n(MIAs) against both independent and identically distributed (i.i.d.) data and\ngraph-structured data. Building on the Bayesian decision-theoretic framework of\nSablayrolles et al., we derive the Bayes-optimal membership inference rule for\nnode-level MIAs against graph neural networks, addressing key open questions\nabout optimal query strategies in the graph setting. We introduce BASE and\nG-BASE, computationally efficient approximations of the Bayes-optimal attack.\nG-BASE achieves superior performance compared to previously proposed\nclassifier-based node-level MIA attacks. BASE, which is also applicable to\nnon-graph data, matches or exceeds the performance of prior state-of-the-art\nMIAs, such as LiRA and RMIA, at a significantly lower computational cost.\nFinally, we show that BASE and RMIA are equivalent under a specific\nhyperparameter setting, providing a principled, Bayes-optimal justification for\nthe RMIA attack."},{"date":"2025-05","title":"DeepBoost-AF: A Novel Unsupervised Feature Learning and Gradient Boosting Fusion for Robust Atrial Fibrillation Detection in Raw ECG Signals","author":"Alireza Jafari, Fereshteh Yousefirizi, and Vahid Seydi","link":"http://arxiv.org/abs/2505.24085v1","abstract":"Atrial fibrillation (AF) is a prevalent cardiac arrhythmia associated with\nelevated health risks, where timely detection is pivotal for mitigating\nstroke-related morbidity. This study introduces an innovative hybrid\nmethodology integrating unsupervised deep learning and gradient boosting models\nto improve AF detection. A 19-layer deep convolutional autoencoder (DCAE) is\ncoupled with three boosting classifiers-AdaBoost, XGBoost, and LightGBM\n(LGBM)-to harness their complementary advantages while addressing individual\nlimitations. The proposed framework uniquely combines DCAE with gradient\nboosting, enabling end-to-end AF identification devoid of manual feature\nextraction. The DCAE-LGBM model attains an F1-score of 95.20%, sensitivity of\n99.99%, and inference latency of four seconds, outperforming existing methods\nand aligning with clinical deployment requirements. The DCAE integration\nsignificantly enhances boosting models, positioning this hybrid system as a\nreliable tool for automated AF detection in clinical settings."},{"date":"2025-05","title":"An Advanced Cyber-Physical System Security Testbed for Substation Automation","author":"Akila Herath, Chen-Ching Liu, Junho Hong, and Mansi Girdhar","link":"http://arxiv.org/abs/2505.24021v1","abstract":"A Cyber-Physical System (CPS) testbed serves as a powerful platform for\ntesting and validating cyber intrusion detection and mitigation strategies in\nsubstations. This study presents the design and development of a CPS testbed\nthat can effectively assess the real-time dynamics of a substation. Cyber\nattacks exploiting IEC 61850-based SV and GOOSE protocols are demonstrated\nusing the testbed, along with an analysis on attack detection. Realistic timing\nmeasurements are obtained, and the time frames for deploying detection and\nmitigation strategies are evaluated."},{"date":"2025-05","title":"LLM Agents Should Employ Security Principles","author":"Kaiyuan Zhang, Zian Su, Pin-Yu Chen, Elisa Bertino, Xiangyu Zhang, and Ninghui Li","link":"http://arxiv.org/abs/2505.24019v1","abstract":"Large Language Model (LLM) agents show considerable promise for automating\ncomplex tasks using contextual reasoning; however, interactions involving\nmultiple agents and the system's susceptibility to prompt injection and other\nforms of context manipulation introduce new vulnerabilities related to privacy\nleakage and system exploitation. This position paper argues that the\nwell-established design principles in information security, which are commonly\nreferred to as security principles, should be employed when deploying LLM\nagents at scale. Design principles such as defense-in-depth, least privilege,\ncomplete mediation, and psychological acceptability have helped guide the\ndesign of mechanisms for securing information systems over the last five\ndecades, and we argue that their explicit and conscientious adoption will help\nsecure agentic systems. To illustrate this approach, we introduce AgentSandbox,\na conceptual framework embedding these security principles to provide\nsafeguards throughout an agent's life-cycle. We evaluate with state-of-the-art\nLLMs along three dimensions: benign utility, attack utility, and attack success\nrate. AgentSandbox maintains high utility for its intended functions under both\nbenign and adversarial evaluations while substantially mitigating privacy\nrisks. By embedding secure design principles as foundational elements within\nemerging LLM agent protocols, we aim to promote trustworthy agent ecosystems\naligned with user privacy expectations and evolving regulatory requirements."},{"date":"2025-05","title":"SG-Blend: Learning an Interpolation Between Improved Swish and GELU for Robust Neural Representations","author":"Gaurav Sarkar, Jay Gala, and Subarna Tripathi","link":"http://arxiv.org/abs/2505.23942v1","abstract":"The design of activation functions remains a pivotal component in optimizing\ndeep neural networks. While prevailing choices like Swish and GELU demonstrate\nconsiderable efficacy, they often exhibit domain-specific optima. This work\nintroduces SG-Blend, a novel activation function that blends our proposed\nSSwish, a first-order symmetric variant of Swish and the established GELU\nthrough dynamic interpolation. By adaptively blending these constituent\nfunctions via learnable parameters, SG-Blend aims to harness their\ncomplementary strengths: SSwish's controlled non-monotonicity and symmetry, and\nGELU's smooth, probabilistic profile, to achieve a more universally robust\nbalance between model expressivity and gradient stability. We conduct\ncomprehensive empirical evaluations across diverse modalities and\narchitectures, showing performance improvements across all considered natural\nlanguage and computer vision tasks and models. These results, achieved with\nnegligible computational overhead, underscore SG-Blend's potential as a\nversatile, drop-in replacement that consistently outperforms strong\ncontemporary baselines. The code is available at\nhttps://anonymous.4open.science/r/SGBlend-6CBC."},{"date":"2025-05","title":"FMG-Det: Foundation Model Guided Robust Object Detection","author":"Darryl Hannan, Timothy Doster, Henry Kvinge, Adam Attarian, and Yijing Watkins","link":"http://arxiv.org/abs/2505.23726v1","abstract":"Collecting high quality data for object detection tasks is challenging due to\nthe inherent subjectivity in labeling the boundaries of an object. This makes\nit difficult to not only collect consistent annotations across a dataset but\nalso to validate them, as no two annotators are likely to label the same object\nusing the exact same coordinates. These challenges are further compounded when\nobject boundaries are partially visible or blurred, which can be the case in\nmany domains. Training on noisy annotations significantly degrades detector\nperformance, rendering them unusable, particularly in few-shot settings, where\njust a few corrupted annotations can impact model performance. In this work, we\npropose FMG-Det, a simple, efficient methodology for training models with noisy\nannotations. More specifically, we propose combining a multiple instance\nlearning (MIL) framework with a pre-processing pipeline that leverages powerful\nfoundation models to correct labels prior to training. This pre-processing\npipeline, along with slight modifications to the detector head, results in\nstate-of-the-art performance across a number of datasets, for both standard and\nfew-shot scenarios, while being much simpler and more efficient than other\napproaches."},{"date":"2025-05","title":"Distributed Federated Learning for Vehicular Network Security: Anomaly Detection Benefits and Multi-Domain Attack Threats","author":"Utku Demir, Yalin E. Sagduyu, Tugba Erpek, Hossein Jafari, Sastry Kompella, and Mengran Xue","link":"http://arxiv.org/abs/2505.23706v1","abstract":"In connected and autonomous vehicles, machine learning for safety message\nclassification has become critical for detecting malicious or anomalous\nbehavior. However, conventional approaches that rely on centralized data\ncollection or purely local training face limitations due to the large scale,\nhigh mobility, and heterogeneous data distributions inherent in inter-vehicle\nnetworks. To overcome these challenges, this paper explores Distributed\nFederated Learning (DFL), whereby vehicles collaboratively train deep learning\nmodels by exchanging model updates among one-hop neighbors and propagating\nmodels over multiple hops. Using the Vehicular Reference Misbehavior (VeReMi)\nExtension Dataset, we show that DFL can significantly improve classification\naccuracy across all vehicles compared to learning strictly with local data.\nNotably, vehicles with low individual accuracy see substantial accuracy gains\nthrough DFL, illustrating the benefit of knowledge sharing across the network.\nWe further show that local training data size and time-varying network\nconnectivity correlate strongly with the model's overall accuracy. We\ninvestigate DFL's resilience and vulnerabilities under attacks in multiple\ndomains, namely wireless jamming and training data poisoning attacks. Our\nresults reveal important insights into the vulnerabilities of DFL when\nconfronted with multi-domain attacks, underlining the need for more robust\nstrategies to secure DFL in vehicular networks."},{"date":"2025-05","title":"Synopsis: Secure and private trend inference from encrypted semantic embeddings","author":"Madelyne Xiao, Palak Jain, Micha Gorelick, and Sarah Scheffler","link":"http://arxiv.org/abs/2505.23880v1","abstract":"WhatsApp and many other commonly used communication platforms guarantee\nend-to-end encryption (E2EE), which requires that service providers lack the\ncryptographic keys to read communications on their own platforms. WhatsApp's\nprivacy-preserving design makes it difficult to study important phenomena like\nthe spread of misinformation or political messaging, as users have a clear\nexpectation and desire for privacy and little incentive to forfeit that privacy\nin the process of handing over raw data to researchers, journalists, or other\nparties.\n  We introduce Synopsis, a secure architecture for analyzing messaging trends\nin consensually-donated E2EE messages using message embeddings. Since the goal\nof this system is investigative journalism workflows, Synopsis must facilitate\nboth exploratory and targeted analyses -- a challenge for systems using\ndifferential privacy (DP), and, for different reasons, a challenge for private\ncomputation approaches based on cryptography. To meet these challenges, we\ncombine techniques from the local and central DP models and wrap the system in\nmalicious-secure multi-party computation to ensure the DP query architecture is\nthe only way to access messages, preventing any party from directly viewing\nstored message embeddings.\n  Evaluations on a dataset of Hindi-language WhatsApp messages (34,024 messages\nrepresented as 500-dimensional embeddings) demonstrate the efficiency and\naccuracy of our approach. Queries on this data run in about 30 seconds, and the\naccuracy of the fine-grained interface exceeds 94% on benchmark tasks."},{"date":"2025-05","title":"Securing AI Agents with Information-Flow Control","author":"Manuel Costa, Boris K\u00f6pf, Aashish Kolluri, Andrew Paverd, Mark Russinovich, Ahmed Salem, Shruti Tople, Lukas Wutschitz, and Santiago Zanella-B\u00e9guelin","link":"http://arxiv.org/abs/2505.23643v1","abstract":"As AI agents become increasingly autonomous and capable, ensuring their\nsecurity against vulnerabilities such as prompt injection becomes critical.\nThis paper explores the use of information-flow control (IFC) to provide\nsecurity guarantees for AI agents. We present a formal model to reason about\nthe security and expressiveness of agent planners. Using this model, we\ncharacterize the class of properties enforceable by dynamic taint-tracking and\nconstruct a taxonomy of tasks to evaluate security and utility trade-offs of\nplanner designs. Informed by this exploration, we present Fides, a planner that\ntracks confidentiality and integrity labels, deterministically enforces\nsecurity policies, and introduces novel primitives for selectively hiding\ninformation. Its evaluation in AgentDojo demonstrates that this approach\nbroadens the range of tasks that can be securely accomplished. A tutorial to\nwalk readers through the the concepts introduced in the paper can be found at\nhttps://github.com/microsoft/fides"},{"date":"2025-05","title":"DRO: A Python Library for Distributionally Robust Optimization in Machine Learning","author":"Jiashuo Liu, Tianyu Wang, Henry Lam, Hongseok Namkoong, and Jose Blanchet","link":"http://arxiv.org/abs/2505.23565v1","abstract":"We introduce dro, an open-source Python library for distributionally robust\noptimization (DRO) for regression and classification problems. The library\nimplements 14 DRO formulations and 9 backbone models, enabling 79 distinct DRO\nmethods. Furthermore, dro is compatible with both scikit-learn and PyTorch.\nThrough vectorization and optimization approximation techniques, dro reduces\nruntime by 10x to over 1000x compared to baseline implementations on\nlarge-scale datasets. Comprehensive documentation is available at\nhttps://python-dro.org."},{"date":"2025-05","title":"Merge Hijacking: Backdoor Attacks to Model Merging of Large Language Models","author":"Zenghui Yuan, Yangming Xu, Jiawen Shi, Pan Zhou, and Lichao Sun","link":"http://arxiv.org/abs/2505.23561v1","abstract":"Model merging for Large Language Models (LLMs) directly fuses the parameters\nof different models finetuned on various tasks, creating a unified model for\nmulti-domain tasks. However, due to potential vulnerabilities in models\navailable on open-source platforms, model merging is susceptible to backdoor\nattacks. In this paper, we propose Merge Hijacking, the first backdoor attack\ntargeting model merging in LLMs. The attacker constructs a malicious upload\nmodel and releases it. Once a victim user merges it with any other models, the\nresulting merged model inherits the backdoor while maintaining utility across\ntasks. Merge Hijacking defines two main objectives-effectiveness and\nutility-and achieves them through four steps. Extensive experiments demonstrate\nthe effectiveness of our attack across different models, merging algorithms,\nand tasks. Additionally, we show that the attack remains effective even when\nmerging real-world models. Moreover, our attack demonstrates robustness against\ntwo inference-time defenses (Paraphrasing and CLEANGEN) and one training-time\ndefense (Fine-pruning)."},{"date":"2025-05","title":"Vid-SME: Membership Inference Attacks against Large Video Understanding Models","author":"Qi Li, Runpeng Yu, and Xinchao Wang","link":"http://arxiv.org/abs/2506.03179v1","abstract":"Multimodal large language models (MLLMs) demonstrate remarkable capabilities\nin handling complex multimodal tasks and are increasingly adopted in video\nunderstanding applications. However, their rapid advancement raises serious\ndata privacy concerns, particularly given the potential inclusion of sensitive\nvideo content, such as personal recordings and surveillance footage, in their\ntraining datasets. Determining improperly used videos during training remains a\ncritical and unresolved challenge. Despite considerable progress on membership\ninference attacks (MIAs) for text and image data in MLLMs, existing methods\nfail to generalize effectively to the video domain. These methods suffer from\npoor scalability as more frames are sampled and generally achieve negligible\ntrue positive rates at low false positive rates (TPR@Low FPR), mainly due to\ntheir failure to capture the inherent temporal variations of video frames and\nto account for model behavior differences as the number of frames varies. To\naddress these challenges, we introduce Vid-SME, the first membership inference\nmethod tailored for video data used in video understanding LLMs (VULLMs).\nVid-SME leverages the confidence of model output and integrates adaptive\nparameterization to compute Sharma-Mittal entropy (SME) for video inputs. By\nleveraging the SME difference between natural and temporally-reversed video\nframes, Vid-SME derives robust membership scores to determine whether a given\nvideo is part of the model's training set. Experiments on various self-trained\nand open-sourced VULLMs demonstrate the strong effectiveness of Vid-SME."},{"date":"2025-05","title":"A Unified Framework for Human AI Collaboration in Security Operations Centers with Trusted Autonomy","author":"Ahmad Mohsin, Helge Janicke, Ahmed Ibrahim, Iqbal H. Sarker, and Seyit Camtepe","link":"http://arxiv.org/abs/2505.23397v2","abstract":"This article presents a structured framework for Human-AI collaboration in\nSecurity Operations Centers (SOCs), integrating AI autonomy, trust calibration,\nand Human-in-the-loop decision making. Existing frameworks in SOCs often focus\nnarrowly on automation, lacking systematic structures to manage human\noversight, trust calibration, and scalable autonomy with AI. Many assume static\nor binary autonomy settings, failing to account for the varied complexity,\ncriticality, and risk across SOC tasks considering Humans and AI collaboration.\nTo address these limitations, we propose a novel autonomy tiered framework\ngrounded in five levels of AI autonomy from manual to fully autonomous, mapped\nto Human-in-the-Loop (HITL) roles and task-specific trust thresholds. This\nenables adaptive and explainable AI integration across core SOC functions,\nincluding monitoring, protection, threat detection, alert triage, and incident\nresponse. The proposed framework differentiates itself from previous research\nby creating formal connections between autonomy, trust, and HITL across various\nSOC levels, which allows for adaptive task distribution according to\noperational complexity and associated risks. The framework is exemplified\nthrough a simulated cyber range that features the cybersecurity AI-Avatar, a\nfine-tuned LLM-based SOC assistant. The AI-Avatar case study illustrates\nhuman-AI collaboration for SOC tasks, reducing alert fatigue, enhancing\nresponse coordination, and strategically calibrating trust. This research\nsystematically presents both the theoretical and practical aspects and\nfeasibility of designing next-generation cognitive SOCs that leverage AI not to\nreplace but to enhance human decision-making."},{"date":"2025-05","title":"Robust and Annotation-Free Wound Segmentation on Noisy Real-World Pressure Ulcer Images: Towards Automated DESIGN-R\\textsuperscript{\\textregistered} Assessment","author":"Yun-Cheng Tsai","link":"http://arxiv.org/abs/2505.23392v1","abstract":"Purpose: Accurate wound segmentation is essential for automated DESIGN-R\nscoring. However, existing models such as FUSegNet, which are trained primarily\non foot ulcer datasets, often fail to generalize to wounds on other body sites.\n  Methods: We propose an annotation-efficient pipeline that combines a\nlightweight YOLOv11n-based detector with the pre-trained FUSegNet segmentation\nmodel. Instead of relying on pixel-level annotations or retraining for new\nanatomical regions, our method achieves robust performance using only 500\nmanually labeled bounding boxes. This zero fine-tuning approach effectively\nbridges the domain gap and enables direct deployment across diverse wound\ntypes. This is an advance not previously demonstrated in the wound segmentation\nliterature.\n  Results: Evaluated on three real-world test sets spanning foot, sacral, and\ntrochanter wounds, our YOLO plus FUSegNet pipeline improved mean IoU by 23\npercentage points over vanilla FUSegNet and increased end-to-end DESIGN-R size\nestimation accuracy from 71 percent to 94 percent (see Table 3 for details).\n  Conclusion: Our pipeline generalizes effectively across body sites without\ntask-specific fine-tuning, demonstrating that minimal supervision, with 500\nannotated ROIs, is sufficient for scalable, annotation-light wound\nsegmentation. This capability paves the way for real-world DESIGN-R automation,\nreducing reliance on pixel-wise labeling, streamlining documentation workflows,\nand supporting objective and consistent wound scoring in clinical practice. We\nwill publicly release the trained detector weights and configuration to promote\nreproducibility and facilitate downstream deployment."},{"date":"2025-05","title":"ADG: Ambient Diffusion-Guided Dataset Recovery for Corruption-Robust Offline Reinforcement Learning","author":"Zeyuan Liu, Zhihe Yang, Jiawei Xu, Rui Yang, Jiafei Lyu, Baoxiang Wang, Yunjian Xu, and Xiu Li","link":"http://arxiv.org/abs/2505.23871v2","abstract":"Real-world datasets collected from sensors or human inputs are prone to noise\nand errors, posing significant challenges for applying offline reinforcement\nlearning (RL). While existing methods have made progress in addressing\ncorrupted actions and rewards, they remain insufficient for handling corruption\nin high-dimensional state spaces and for cases where multiple elements in the\ndataset are corrupted simultaneously. Diffusion models, known for their strong\ndenoising capabilities, offer a promising direction for this problem-but their\ntendency to overfit noisy samples limits their direct applicability. To\novercome this, we propose Ambient Diffusion-Guided Dataset Recovery (ADG), a\nnovel approach that pioneers the use of diffusion models to tackle data\ncorruption in offline RL. First, we introduce Ambient Denoising Diffusion\nProbabilistic Models (DDPM) from approximated distributions, which enable\nlearning on partially corrupted datasets with theoretical guarantees. Second,\nwe use the noise-prediction property of Ambient DDPM to distinguish between\nclean and corrupted data, and then use the clean subset to train a standard\nDDPM. Third, we employ the trained standard DDPM to refine the previously\nidentified corrupted data, enhancing data quality for subsequent offline RL\ntraining. A notable strength of ADG is its versatility-it can be seamlessly\nintegrated with any offline RL algorithm. Experiments on a range of benchmarks,\nincluding MuJoCo, Kitchen, and Adroit, demonstrate that ADG effectively\nmitigates the impact of corrupted data and improves the robustness of offline\nRL under various noise settings, achieving state-of-the-art results."},{"date":"2025-05","title":"Noise-Robustness Through Noise: Asymmetric LoRA Adaption with Poisoning Expert","author":"Zhaokun Wang, Jinyu Guo, Jingwen Pu, Lingfeng Chen, Hongli Pu, Jie Ou, Libo Qin, and Wenhong Tian","link":"http://arxiv.org/abs/2505.23868v2","abstract":"Current parameter-efficient fine-tuning methods for adapting pre-trained\nlanguage models to downstream tasks are susceptible to interference from noisy\ndata. Conventional noise-handling approaches either rely on laborious data\npre-processing or employ model architecture modifications prone to error\naccumulation. In contrast to existing noise-process paradigms, we propose a\nnoise-robust adaptation method via asymmetric LoRA poisoning experts (LoPE), a\nnovel framework that enhances model robustness to noise only with generated\nnoisy data. Drawing inspiration from the mixture-of-experts architecture, LoPE\nstrategically integrates a dedicated poisoning expert in an asymmetric LoRA\nconfiguration. Through a two-stage paradigm, LoPE performs noise injection on\nthe poisoning expert during fine-tuning to enhance its noise discrimination and\nprocessing ability. During inference, we selectively mask the dedicated\npoisoning expert to leverage purified knowledge acquired by normal experts for\nnoise-robust output. Extensive experiments demonstrate that LoPE achieves\nstrong performance and robustness purely through the low-cost noise injection,\nwhich completely eliminates the requirement of data cleaning."},{"date":"2025-05","title":"Dimension-Reduction Attack! Video Generative Models are Experts on Controllable Image Synthesis","author":"Hengyuan Cao, Yutong Feng, Biao Gong, Yijing Tian, Yunhong Lu, Chuang Liu, and Bin Wang","link":"http://arxiv.org/abs/2505.23325v1","abstract":"Video generative models can be regarded as world simulators due to their\nability to capture dynamic, continuous changes inherent in real-world\nenvironments. These models integrate high-dimensional information across\nvisual, temporal, spatial, and causal dimensions, enabling predictions of\nsubjects in various status. A natural and valuable research direction is to\nexplore whether a fully trained video generative model in high-dimensional\nspace can effectively support lower-dimensional tasks such as controllable\nimage generation. In this work, we propose a paradigm for video-to-image\nknowledge compression and task adaptation, termed \\textit{Dimension-Reduction\nAttack} (\\texttt{DRA-Ctrl}), which utilizes the strengths of video models,\nincluding long-range context modeling and flatten full-attention, to perform\nvarious generation tasks. Specially, to address the challenging gap between\ncontinuous video frames and discrete image generation, we introduce a\nmixup-based transition strategy that ensures smooth adaptation. Moreover, we\nredesign the attention structure with a tailored masking mechanism to better\nalign text prompts with image-level control. Experiments across diverse image\ngeneration tasks, such as subject-driven and spatially conditioned generation,\nshow that repurposed video models outperform those trained directly on images.\nThese results highlight the untapped potential of large-scale video generators\nfor broader visual applications. \\texttt{DRA-Ctrl} provides new insights into\nreusing resource-intensive video models and lays foundation for future unified\ngenerative models across visual modalities. The project page is\nhttps://dra-ctrl-2025.github.io/DRA-Ctrl/."},{"date":"2025-05","title":"Infi-Med: Low-Resource Medical MLLMs with Robust Reasoning Evaluation","author":"Zeyu Liu, Zhitian Hou, Yining Di, Kejing Yang, Zhijie Sang, Congkai Xie, Jingwen Yang, Siyuan Liu, Jialu Wang, Chunming Li, Ming Li, and Hongxia Yang","link":"http://arxiv.org/abs/2505.23867v1","abstract":"Multimodal large language models (MLLMs) have demonstrated promising\nprospects in healthcare, particularly for addressing complex medical tasks,\nsupporting multidisciplinary treatment (MDT), and enabling personalized\nprecision medicine. However, their practical deployment faces critical\nchallenges in resource efficiency, diagnostic accuracy, clinical\nconsiderations, and ethical privacy. To address these limitations, we propose\nInfi-Med, a comprehensive framework for medical MLLMs that introduces three key\ninnovations: (1) a resource-efficient approach through curating and\nconstructing high-quality supervised fine-tuning (SFT) datasets with minimal\nsample requirements, with a forward-looking design that extends to both\npretraining and posttraining phases; (2) enhanced multimodal reasoning\ncapabilities for cross-modal integration and clinical task understanding; and\n(3) a systematic evaluation system that assesses model performance across\nmedical modalities and task types. Our experiments demonstrate that Infi-Med\nachieves state-of-the-art (SOTA) performance in general medical reasoning while\nmaintaining rapid adaptability to clinical scenarios. The framework establishes\na solid foundation for deploying MLLMs in real-world healthcare settings by\nbalancing model effectiveness with operational constraints."},{"date":"2025-05","title":"Adversarial Semantic and Label Perturbation Attack for Pedestrian Attribute Recognition","author":"Weizhe Kong, Xiao Wang, Ruichong Gao, Chenglong Li, Yu Zhang, Xing Yang, Yaowei Wang, and Jin Tang","link":"http://arxiv.org/abs/2505.23313v1","abstract":"Pedestrian Attribute Recognition (PAR) is an indispensable task in\nhuman-centered research and has made great progress in recent years with the\ndevelopment of deep neural networks. However, the potential vulnerability and\nanti-interference ability have still not been fully explored. To bridge this\ngap, this paper proposes the first adversarial attack and defense framework for\npedestrian attribute recognition. Specifically, we exploit both global- and\npatch-level attacks on the pedestrian images, based on the pre-trained\nCLIP-based PAR framework. It first divides the input pedestrian image into\nnon-overlapping patches and embeds them into feature embeddings using a\nprojection layer. Meanwhile, the attribute set is expanded into sentences using\nprompts and embedded into attribute features using a pre-trained CLIP text\nencoder. A multi-modal Transformer is adopted to fuse the obtained vision and\ntext tokens, and a feed-forward network is utilized for attribute recognition.\nBased on the aforementioned PAR framework, we adopt the adversarial semantic\nand label-perturbation to generate the adversarial noise, termed ASL-PAR. We\nalso design a semantic offset defense strategy to suppress the influence of\nadversarial attacks. Extensive experiments conducted on both digital domains\n(i.e., PETA, PA100K, MSP60K, RAPv2) and physical domains fully validated the\neffectiveness of our proposed adversarial attack and defense strategies for the\npedestrian attribute recognition. The source code of this paper will be\nreleased on https://github.com/Event-AHU/OpenPAR."},{"date":"2025-05","title":"Disrupting Vision-Language Model-Driven Navigation Services via Adversarial Object Fusion","author":"Chunlong Xie, Jialing He, Shangwei Guo, Jiacheng Wang, Shudong Zhang, Tianwei Zhang, and Tao Xiang","link":"http://arxiv.org/abs/2505.23266v1","abstract":"We present Adversarial Object Fusion (AdvOF), a novel attack framework\ntargeting vision-and-language navigation (VLN) agents in service-oriented\nenvironments by generating adversarial 3D objects. While foundational models\nlike Large Language Models (LLMs) and Vision Language Models (VLMs) have\nenhanced service-oriented navigation systems through improved perception and\ndecision-making, their integration introduces vulnerabilities in\nmission-critical service workflows. Existing adversarial attacks fail to\naddress service computing contexts, where reliability and quality-of-service\n(QoS) are paramount. We utilize AdvOF to investigate and explore the impact of\nadversarial environments on the VLM-based perception module of VLN agents. In\nparticular, AdvOF first precisely aggregates and aligns the victim object\npositions in both 2D and 3D space, defining and rendering adversarial objects.\nThen, we collaboratively optimize the adversarial object with regularization\nbetween the adversarial and victim object across physical properties and VLM\nperceptions. Through assigning importance weights to varying views, the\noptimization is processed stably and multi-viewedly by iterative fusions from\nlocal updates and justifications. Our extensive evaluations demonstrate AdvOF\ncan effectively degrade agent performance under adversarial conditions while\nmaintaining minimal interference with normal navigation tasks. This work\nadvances the understanding of service security in VLM-powered navigation\nsystems, providing computational foundations for robust service composition in\nphysical-world deployments."},{"date":"2025-05","title":"Towards Robust Overlapping Speech Detection: A Speaker-Aware Progressive Approach Using WavLM","author":"Zhaokai Sun, Li Zhang, Qing Wang, Pan Zhou, and Lei Xie","link":"http://arxiv.org/abs/2505.23207v1","abstract":"Overlapping Speech Detection (OSD) aims to identify regions where multiple\nspeakers overlap in a conversation, a critical challenge in multi-party speech\nprocessing. This work proposes a speaker-aware progressive OSD model that\nleverages a progressive training strategy to enhance the correlation between\nsubtasks such as voice activity detection (VAD) and overlap detection. To\nimprove acoustic representation, we explore the effectiveness of\nstate-of-the-art self-supervised learning (SSL) models, including WavLM and\nwav2vec 2.0, while incorporating a speaker attention module to enrich features\nwith frame-level speaker information. Experimental results show that the\nproposed method achieves state-of-the-art performance, with an F1 score of\n82.76\\% on the AMI test set, demonstrating its robustness and effectiveness in\nOSD."},{"date":"2025-05","title":"Fooling the Watchers: Breaking AIGC Detectors via Semantic Prompt Attacks","author":"Run Hao, and Peng Ying","link":"http://arxiv.org/abs/2505.23192v1","abstract":"The rise of text-to-image (T2I) models has enabled the synthesis of\nphotorealistic human portraits, raising serious concerns about identity misuse\nand the robustness of AIGC detectors. In this work, we propose an automated\nadversarial prompt generation framework that leverages a grammar tree structure\nand a variant of the Monte Carlo tree search algorithm to systematically\nexplore the semantic prompt space. Our method generates diverse, controllable\nprompts that consistently evade both open-source and commercial AIGC detectors.\nExtensive experiments across multiple T2I models validate its effectiveness,\nand the approach ranked first in a real-world adversarial AIGC detection\ncompetition. Beyond attack scenarios, our method can also be used to construct\nhigh-quality adversarial datasets, providing valuable resources for training\nand evaluating more robust AIGC detection and defense systems."},{"date":"2025-05","title":"Learning to Incentivize in Repeated Principal-Agent Problems with Adversarial Agent Arrivals","author":"Junyan Liu, Arnab Maiti, Artin Tajdini, Kevin Jamieson, and Lillian J. Ratliff","link":"http://arxiv.org/abs/2505.23124v1","abstract":"We initiate the study of a repeated principal-agent problem over a finite\nhorizon $T$, where a principal sequentially interacts with $K\\geq 2$ types of\nagents arriving in an adversarial order. At each round, the principal\nstrategically chooses one of the $N$ arms to incentivize for an arriving agent\nof unknown type. The agent then chooses an arm based on its own utility and the\nprovided incentive, and the principal receives a corresponding reward. The\nobjective is to minimize regret against the best incentive in hindsight.\nWithout prior knowledge of agent behavior, we show that the problem becomes\nintractable, leading to linear regret. We analyze two key settings where\nsublinear regret is achievable. In the first setting, the principal knows the\narm each agent type would select greedily for any given incentive. Under this\nsetting, we propose an algorithm that achieves a regret bound of\n$O(\\min\\{\\sqrt{KT\\log N},K\\sqrt{T}\\})$ and provide a matching lower bound up to\na $\\log K$ factor. In the second setting, an agent's response varies smoothly\nwith the incentive and is governed by a Lipschitz constant $L\\geq 1$. Under\nthis setting, we show that there is an algorithm with a regret bound of\n$\\tilde{O}((LN)^{1/3}T^{2/3})$ and establish a matching lower bound up to\nlogarithmic factors. Finally, we extend our algorithmic results for both\nsettings by allowing the principal to incentivize multiple arms simultaneously\nin each round."},{"date":"2025-05","title":"DenoiseRotator: Enhance Pruning Robustness for LLMs via Importance Concentration","author":"Tianteng Gu, Bei Liu, Bo Xiao, Ke Zeng, Jiacheng Liu, and Yanmin Qian","link":"http://arxiv.org/abs/2505.23049v1","abstract":"Pruning is a widely used technique to compress large language models (LLMs)\nby removing unimportant weights, but it often suffers from significant\nperformance degradation - especially under semi-structured sparsity\nconstraints. Existing pruning methods primarily focus on estimating the\nimportance of individual weights, which limits their ability to preserve\ncritical capabilities of the model. In this work, we propose a new perspective:\nrather than merely selecting which weights to prune, we first redistribute\nparameter importance to make the model inherently more amenable to pruning. By\nminimizing the information entropy of normalized importance scores, our\napproach concentrates importance onto a smaller subset of weights, thereby\nenhancing pruning robustness. We instantiate this idea through DenoiseRotator,\nwhich applies learnable orthogonal transformations to the model's weight\nmatrices. Our method is model-agnostic and can be seamlessly integrated with\nexisting pruning techniques such as Magnitude, SparseGPT, and Wanda. Evaluated\non LLaMA3, Qwen2.5, and Mistral models under 50% unstructured and 2:4\nsemi-structured sparsity, DenoiseRotator consistently improves perplexity and\nzero-shot accuracy. For instance, on LLaMA3-70B pruned with SparseGPT at 2:4\nsemi-structured sparsity, DenoiseRotator reduces the perplexity gap to the\ndense model by 58%, narrowing the degradation from 8.1 to 3.4 points. Codes are\navailable at https://github.com/Axel-gu/DenoiseRotator."},{"date":"2025-05","title":"Diverse Prototypical Ensembles Improve Robustness to Subpopulation Shift","author":"Minh Nguyen Nhat To, Paul F RWilson, Viet Nguyen, Mohamed Harmanani, Michael Cooper, Fahimeh Fooladgar, Purang Abolmaesumi, Parvin Mousavi, and Rahul G. Krishnan","link":"http://arxiv.org/abs/2505.23027v1","abstract":"The subpopulationtion shift, characterized by a disparity in subpopulation\ndistributibetween theween the training and target datasets, can significantly\ndegrade the performance of machine learning models. Current solutions to\nsubpopulation shift involve modifying empirical risk minimization with\nre-weighting strategies to improve generalization. This strategy relies on\nassumptions about the number and nature of subpopulations and annotations on\ngroup membership, which are unavailable for many real-world datasets. Instead,\nwe propose using an ensemble of diverse classifiers to adaptively capture risk\nassociated with subpopulations. Given a feature extractor network, we replace\nits standard linear classification layer with a mixture of prototypical\nclassifiers, where each member is trained to classify the data while focusing\non different features and samples from other members. In empirical evaluation\non nine real-world datasets, covering diverse domains and kinds of\nsubpopulation shift, our method of Diverse Prototypical Ensembles (DPEs) often\noutperforms the prior state-of-the-art in worst-group accuracy. The code is\navailable at https://github.com/minhto2802/dpe4subpop"},{"date":"2025-05","title":"Context-Robust Knowledge Editing for Language Models","author":"Haewon Park, Gyubin Choi, Minjun Kim, and Yohan Jo","link":"http://arxiv.org/abs/2505.23026v2","abstract":"Knowledge editing (KE) methods offer an efficient way to modify knowledge in\nlarge language models. Current KE evaluations typically assess editing success\nby considering only the edited knowledge without any preceding contexts. In\nreal-world applications, however, preceding contexts often trigger the\nretrieval of the original knowledge and undermine the intended edit. To address\nthis issue, we develop CHED -- a benchmark designed to evaluate the context\nrobustness of KE methods. Evaluations on CHED show that they often fail when\npreceding contexts are present. To mitigate this shortcoming, we introduce\nCoRE, a KE method designed to strengthen context robustness by minimizing\ncontext-sensitive variance in hidden states of the model for edited knowledge.\nThis method not only improves the editing success rate in situations where a\npreceding context is present but also preserves the overall capabilities of the\nmodel. We provide an in-depth analysis of the differing impacts of preceding\ncontexts when introduced as user utterances versus assistant responses, and we\ndissect attention-score patterns to assess how specific tokens influence\nediting success."},{"date":"2025-05","title":"Hybrid Cross-domain Robust Reinforcement Learning","author":"Linh Le Pham Van, Minh Hoang Nguyen, Hung Le, Hung The Tran, and Sunil Gupta","link":"http://arxiv.org/abs/2505.23003v1","abstract":"Robust reinforcement learning (RL) aims to learn policies that remain\neffective despite uncertainties in its environment, which frequently arise in\nreal-world applications due to variations in environment dynamics. The robust\nRL methods learn a robust policy by maximizing value under the worst-case\nmodels within a predefined uncertainty set. Offline robust RL algorithms are\nparticularly promising in scenarios where only a fixed dataset is available and\nnew data cannot be collected. However, these approaches often require extensive\noffline data, and gathering such datasets for specific tasks in specific\nenvironments can be both costly and time-consuming. Using an imperfect\nsimulator offers a faster, cheaper, and safer way to collect data for training,\nbut it can suffer from dynamics mismatch. In this paper, we introduce HYDRO,\nthe first Hybrid Cross-Domain Robust RL framework designed to address these\nchallenges. HYDRO utilizes an online simulator to complement the limited amount\nof offline datasets in the non-trivial context of robust RL. By measuring and\nminimizing performance gaps between the simulator and the worst-case models in\nthe uncertainty set, HYDRO employs novel uncertainty filtering and prioritized\nsampling to select the most relevant and reliable simulator samples. Our\nextensive experiments demonstrate HYDRO's superior performance over existing\nmethods across various tasks, underscoring its potential to improve sample\nefficiency in offline robust RL."},{"date":"2025-05","title":"Can LLMs Deceive CLIP? Benchmarking Adversarial Compositionality of Pre-trained Multimodal Representation via Text Updates","author":"Jaewoo Ahn, Heeseung Yun, Dayoon Ko, and Gunhee Kim","link":"http://arxiv.org/abs/2505.22943v1","abstract":"While pre-trained multimodal representations (e.g., CLIP) have shown\nimpressive capabilities, they exhibit significant compositional vulnerabilities\nleading to counterintuitive judgments. We introduce Multimodal Adversarial\nCompositionality (MAC), a benchmark that leverages large language models (LLMs)\nto generate deceptive text samples to exploit these vulnerabilities across\ndifferent modalities and evaluates them through both sample-wise attack success\nrate and group-wise entropy-based diversity. To improve zero-shot methods, we\npropose a self-training approach that leverages rejection-sampling fine-tuning\nwith diversity-promoting filtering, which enhances both attack success rate and\nsample diversity. Using smaller language models like Llama-3.1-8B, our approach\ndemonstrates superior performance in revealing compositional vulnerabilities\nacross various multimodal representations, including images, videos, and\naudios."},{"date":"2025-05","title":"Unraveling LoRA Interference: Orthogonal Subspaces for Robust Model Merging","author":"Haobo Zhang, and Jiayu Zhou","link":"http://arxiv.org/abs/2505.22934v1","abstract":"Fine-tuning large language models (LMs) for individual tasks yields strong\nperformance but is expensive for deployment and storage. Recent works explore\nmodel merging to combine multiple task-specific models into a single multi-task\nmodel without additional training. However, existing merging methods often fail\nfor models fine-tuned with low-rank adaptation (LoRA), due to significant\nperformance degradation. In this paper, we show that this issue arises from a\npreviously overlooked interplay between model parameters and data\ndistributions. We propose Orthogonal Subspaces for Robust model Merging (OSRM)\nto constrain the LoRA subspace *prior* to fine-tuning, ensuring that updates\nrelevant to one task do not adversely shift outputs for others. Our approach\ncan seamlessly integrate with most existing merging algorithms, reducing the\nunintended interference among tasks. Extensive experiments on eight datasets,\ntested with three widely used LMs and two large LMs, demonstrate that our\nmethod not only boosts merging performance but also preserves single-task\naccuracy. Furthermore, our approach exhibits greater robustness to the\nhyperparameters of merging. These results highlight the importance of\ndata-parameter interaction in model merging and offer a plug-and-play solution\nfor merging LoRA models."},{"date":"2025-05","title":"Smart Surrogate Losses for Contextual Stochastic Linear Optimization with Robust Constraints","author":"Hyungki Im, Wyame Benslimane, and Paul Grigas","link":"http://arxiv.org/abs/2505.22881v1","abstract":"We study an extension of contextual stochastic linear optimization (CSLO)\nthat, in contrast to most of the existing literature, involves inequality\nconstraints that depend on uncertain parameters predicted by a machine learning\nmodel. To handle the constraint uncertainty, we use contextual uncertainty sets\nconstructed via methods like conformal prediction. Given a contextual\nuncertainty set method, we introduce the \"Smart Predict-then-Optimize with\nRobust Constraints\" (SPO-RC) loss, a feasibility-sensitive adaptation of the\nSPO loss that measures decision error of predicted objective parameters. We\nalso introduce a convex surrogate, SPO-RC+, and prove Fisher consistency with\nSPO-RC. To enhance performance, we train on truncated datasets where true\nconstraint parameters lie within the uncertainty sets, and we correct the\ninduced sample selection bias using importance reweighting techniques. Through\nexperiments on fractional knapsack and alloy production problem instances, we\ndemonstrate that SPO-RC+ effectively handles uncertainty in constraints and\nthat combining truncation with importance reweighting can further improve\nperformance."},{"date":"2025-05","title":"Operationalizing CaMeL: Strengthening LLM Defenses for Enterprise Deployment","author":"Krti Tallam, and Emma Miller","link":"http://arxiv.org/abs/2505.22852v1","abstract":"CaMeL (Capabilities for Machine Learning) introduces a capability-based\nsandbox to mitigate prompt injection attacks in large language model (LLM)\nagents. While effective, CaMeL assumes a trusted user prompt, omits\nside-channel concerns, and incurs performance tradeoffs due to its dual-LLM\ndesign. This response identifies these issues and proposes engineering\nimprovements to expand CaMeL's threat coverage and operational usability. We\nintroduce: (1) prompt screening for initial inputs, (2) output auditing to\ndetect instruction leakage, (3) a tiered-risk access model to balance usability\nand control, and (4) a verified intermediate language for formal guarantees.\nTogether, these upgrades align CaMeL with best practices in enterprise security\nand support scalable deployment."},{"date":"2025-05","title":"Security Benefits and Side Effects of Labeling AI-Generated Images","author":"Sandra H\u00f6ltervennhoff, Jonas Ricker, Maike M. Raphael, Charlotte Schwedes, Rebecca Weil, Asja Fischer, Thorsten Holz, Lea Sch\u00f6nherr, and Sascha Fahl","link":"http://arxiv.org/abs/2505.22845v1","abstract":"Generative artificial intelligence is developing rapidly, impacting humans'\ninteraction with information and digital media. It is increasingly used to\ncreate deceptively realistic misinformation, so lawmakers have imposed\nregulations requiring the disclosure of AI-generated content. However, only\nlittle is known about whether these labels reduce the risks of AI-generated\nmisinformation.\n  Our work addresses this research gap. Focusing on AI-generated images, we\nstudy the implications of labels, including the possibility of mislabeling.\nAssuming that simplicity, transparency, and trust are likely to impact the\nsuccessful adoption of such labels, we first qualitatively explore users'\nopinions and expectations of AI labeling using five focus groups. Second, we\nconduct a pre-registered online survey with over 1300 U.S. and EU participants\nto quantitatively assess the effect of AI labels on users' ability to recognize\nmisinformation containing either human-made or AI-generated images. Our focus\ngroups illustrate that, while participants have concerns about the practical\nimplementation of labeling, they consider it helpful in identifying\nAI-generated images and avoiding deception. However, considering security\nbenefits, our survey revealed an ambiguous picture, suggesting that users might\nover-rely on labels. While inaccurate claims supported by labeled AI-generated\nimages were rated less credible than those with unlabeled AI-images, the belief\nin accurate claims also decreased when accompanied by a labeled AI-generated\nimage. Moreover, we find the undesired side effect that human-made images\nconveying inaccurate claims were perceived as more credible in the presence of\nlabels."},{"date":"2025-05","title":"How Do Diffusion Models Improve Adversarial Robustness?","author":"Liu Yuezhang, and Xue-Xin Wei","link":"http://arxiv.org/abs/2505.22839v1","abstract":"Recent findings suggest that diffusion models significantly enhance empirical\nadversarial robustness. While some intuitive explanations have been proposed,\nthe precise mechanisms underlying these improvements remain unclear. In this\nwork, we systematically investigate how and how well diffusion models improve\nadversarial robustness. First, we observe that diffusion models intriguingly\nincrease, rather than decrease, the $\\ell_p$ distance to clean\nsamples--challenging the intuition that purification denoises inputs closer to\nthe original data. Second, we find that the purified images are heavily\ninfluenced by the internal randomness of diffusion models, where a compression\neffect arises within each randomness configuration. Motivated by this\nobservation, we evaluate robustness under fixed randomness and find that the\nimprovement drops to approximately 24% on CIFAR-10--substantially lower than\nprior reports approaching 70%. Importantly, we show that this remaining\nrobustness gain strongly correlates with the model's ability to compress the\ninput space, revealing the compression rate as a reliable robustness indicator\nwithout requiring gradient-based analysis. Our findings provide novel insights\ninto the mechanisms underlying diffusion-based purification, and offer guidance\nfor developing more effective and principled adversarial purification systems."},{"date":"2025-05","title":"PALADIN : Robust Neural Fingerprinting for Text-to-Image Diffusion Models","author":"Murthy L, and Subarna Tripathi","link":"http://arxiv.org/abs/2506.03170v1","abstract":"The risk of misusing text-to-image generative models for malicious uses,\nespecially due to the open-source development of such models, has become a\nserious concern. As a risk mitigation strategy, attributing generative models\nwith neural fingerprinting is emerging as a popular technique. There has been a\nplethora of recent work that aim for addressing neural fingerprinting. A\ntrade-off between the attribution accuracy and generation quality of such\nmodels has been studied extensively. None of the existing methods yet achieved\n$100\\%$ attribution accuracy. However, any model with less than \\emph{perfect}\naccuracy is practically non-deployable. In this work, we propose an accurate\nmethod to incorporate neural fingerprinting for text-to-image diffusion models\nleveraging the concepts of cyclic error correcting codes from the literature of\ncoding theory."},{"date":"2025-05","title":"Seven Security Challenges That Must be Solved in Cross-domain Multi-agent LLM Systems","author":"Ronny Ko, Jiseong Jeong, Shuyuan Zheng, Chuan Xiao, Tae-Wan Kim, Makoto Onizuka, and Won-Yong Shin","link":"http://arxiv.org/abs/2505.23847v2","abstract":"Large language models (LLMs) are rapidly evolving into autonomous agents that\ncooperate across organizational boundaries, enabling joint disaster response,\nsupply-chain optimization, and other tasks that demand decentralized expertise\nwithout surrendering data ownership. Yet, cross-domain collaboration shatters\nthe unified trust assumptions behind current alignment and containment\ntechniques. An agent benign in isolation may, when receiving messages from an\nuntrusted peer, leak secrets or violate policy, producing risks driven by\nemergent multi-agent dynamics rather than classical software bugs. This\nposition paper maps the security agenda for cross-domain multi-agent LLM\nsystems. We introduce seven categories of novel security challenges, for each\nof which we also present plausible attacks, security evaluation metrics, and\nfuture research guidelines."},{"date":"2025-05","title":"Transformers for Secure Hardware Systems: Applications, Challenges, and Outlook","author":"Banafsheh Saber Latibari, Najmeh Nazari, Avesta Sasan, Houman Homayoun, Pratik Satam, Soheil Salehi, and Hossein Sayadi","link":"http://arxiv.org/abs/2505.22605v1","abstract":"The rise of hardware-level security threats, such as side-channel attacks,\nhardware Trojans, and firmware vulnerabilities, demands advanced detection\nmechanisms that are more intelligent and adaptive. Traditional methods often\nfall short in addressing the complexity and evasiveness of modern attacks,\ndriving increased interest in machine learning-based solutions. Among these,\nTransformer models, widely recognized for their success in natural language\nprocessing and computer vision, have gained traction in the security domain due\nto their ability to model complex dependencies, offering enhanced capabilities\nin identifying vulnerabilities, detecting anomalies, and reinforcing system\nintegrity. This survey provides a comprehensive review of recent advancements\non the use of Transformers in hardware security, examining their application\nacross key areas such as side-channel analysis, hardware Trojan detection,\nvulnerability classification, device fingerprinting, and firmware security.\nFurthermore, we discuss the practical challenges of applying Transformers to\nsecure hardware systems, and highlight opportunities and future research\ndirections that position them as a foundation for next-generation\nhardware-assisted security. These insights pave the way for deeper integration\nof AI-driven techniques into hardware security frameworks, enabling more\nresilient and intelligent defenses."},{"date":"2025-05","title":"Adversarially Robust AI-Generated Image Detection for Free: An Information Theoretic Perspective","author":"Ruixuan Zhang, He Wang, Zhengyu Zhao, Zhiqing Guo, Xun Yang, Yunfeng Diao, and Meng Wang","link":"http://arxiv.org/abs/2505.22604v2","abstract":"Rapid advances in Artificial Intelligence Generated Images (AIGI) have\nfacilitated malicious use, such as forgery and misinformation. Therefore,\nnumerous methods have been proposed to detect fake images. Although such\ndetectors have been proven to be universally vulnerable to adversarial attacks,\ndefenses in this field are scarce. In this paper, we first identify that\nadversarial training (AT), widely regarded as the most effective defense,\nsuffers from performance collapse in AIGI detection. Through an\ninformation-theoretic lens, we further attribute the cause of collapse to\nfeature entanglement, which disrupts the preservation of feature-label mutual\ninformation. Instead, standard detectors show clear feature separation.\nMotivated by this difference, we propose Training-free Robust Detection via\nInformation-theoretic Measures (TRIM), the first training-free adversarial\ndefense for AIGI detection. TRIM builds on standard detectors and quantifies\nfeature shifts using prediction entropy and KL divergence. Extensive\nexperiments across multiple datasets and attacks validate the superiority of\nour TRIM, e.g., outperforming the state-of-the-art defense by 33.88% (28.91%)\non ProGAN (GenImage), while well maintaining original accuracy."},{"date":"2025-05","title":"Training RL Agents for Multi-Objective Network Defense Tasks","author":"Andres Molina-Markham, Luis Robaina, Sean Steinle, Akash Trivedi, Derek Tsui, Nicholas Potteiger, Lauren Brandt, Ransom Winder, and Ahmed Ridley","link":"http://arxiv.org/abs/2505.22531v1","abstract":"Open-ended learning (OEL) -- which emphasizes training agents that achieve\nbroad capability over narrow competency -- is emerging as a paradigm to develop\nartificial intelligence (AI) agents to achieve robustness and generalization.\nHowever, despite promising results that demonstrate the benefits of OEL,\napplying OEL to develop autonomous agents for real-world cybersecurity\napplications remains a challenge.\n  We propose a training approach, inspired by OEL, to develop autonomous\nnetwork defenders. Our results demonstrate that like in other domains, OEL\nprinciples can translate into more robust and generalizable agents for cyber\ndefense. To apply OEL to network defense, it is necessary to address several\ntechnical challenges. Most importantly, it is critical to provide a task\nrepresentation approach over a broad universe of tasks that maintains a\nconsistent interface over goals, rewards and action spaces. This way, the\nlearning agent can train with varying network conditions, attacker behaviors,\nand defender goals while being able to build on previously gained knowledge.\n  With our tools and results, we aim to fundamentally impact research that\napplies AI to solve cybersecurity problems. Specifically, as researchers\ndevelop gyms and benchmarks for cyber defense, it is paramount that they\nconsider diverse tasks with consistent representations, such as those we\npropose in our work."},{"date":"2025-05","title":"IGNIS: A Neural Network Framework for Robust Parameter Estimation in Archimedean Copulas","author":"Agnideep Aich, Ashit Baran Aich, and Bruce Wade","link":"http://arxiv.org/abs/2505.22518v1","abstract":"Parameter estimation for Archimedean copulas remains a challenging problem,\nparticularly for the recently developed A1 and A2 families that exhibit complex\ndependency structures. Traditional methods, such as the Method of Moments\n(MoM), Maximum Likelihood Estimation (MLE), and Maximum Pseudo-Likelihood\n(MPL), often struggle due to issues of non-monotonic relationship with\ndependency measures such as Kendall's tau (as in the case of A1) and numerical\ninstability. In this paper, we present the IGNIS Network, a novel, unified\nneural framework that learns a direct mapping from observable dependency\nmeasures to copula parameters, thereby overcoming the limitations of classical\napproaches. Our approach is trained on simulated data spanning five Archimedean\ncopula families including Clayton, Gumbel, Frank, A1, and A2, ensuring its\ngeneral applicability across the entire family. Extensive simulation studies\ndemonstrate that the IGNIS Network reduces estimation errors compared to MoM,\nwhile inherently enforcing parameter constraints through theory-guided\npost-processing. We further validate the practical utility of our method on\ndiverse real-world datasets, including financial returns (AAPL-MSFT),\nhealthcare metrics (CDC Diabetes indicators), and environmental measurements\n(PM2.5 air quality). Our results underscore the transformative potential of\nneural methods for robust and accurate dependence modeling in modern\napplications."},{"date":"2025-05","title":"The Meeseeks Mesh: Spatially Consistent 3D Adversarial Objects for BEV Detector","author":"Aixuan Li, Mochu Xiang, Jing Zhang, and Yuchao Dai","link":"http://arxiv.org/abs/2505.22499v2","abstract":"3D object detection is a critical component in autonomous driving systems. It\nallows real-time recognition and detection of vehicles, pedestrians and\nobstacles under varying environmental conditions. Among existing methods, 3D\nobject detection in the Bird's Eye View (BEV) has emerged as the mainstream\nframework. To guarantee a safe, robust and trustworthy 3D object detection, 3D\nadversarial attacks are investigated, where attacks are placed in 3D\nenvironments to evaluate the model performance, e.g. putting a film on a car,\nclothing a pedestrian. The vulnerability of 3D object detection models to 3D\nadversarial attacks serves as an important indicator to evaluate the robustness\nof the model against perturbations. To investigate this vulnerability, we\ngenerate non-invasive 3D adversarial objects tailored for real-world attack\nscenarios. Our method verifies the existence of universal adversarial objects\nthat are spatially consistent across time and camera views. Specifically, we\nemploy differentiable rendering techniques to accurately model the spatial\nrelationship between adversarial objects and the target vehicle. Furthermore,\nwe introduce an occlusion-aware module to enhance visual consistency and\nrealism under different viewpoints. To maintain attack effectiveness across\nmultiple frames, we design a BEV spatial feature-guided optimization strategy.\nExperimental results demonstrate that our approach can reliably suppress\nvehicle predictions from state-of-the-art 3D object detectors, serving as an\nimportant tool to test robustness of 3D object detection models before\ndeployment. Moreover, the generated adversarial objects exhibit strong\ngeneralization capabilities, retaining its effectiveness at various positions\nand distances in the scene."},{"date":"2025-05","title":"ProSpero: Active Learning for Robust Protein Design Beyond Wild-Type Neighborhoods","author":"Michal Kmicikiewicz, Vincent Fortuin, and Ewa Szczurek","link":"http://arxiv.org/abs/2505.22494v1","abstract":"Designing protein sequences of both high fitness and novelty is a challenging\ntask in data-efficient protein engineering. Exploration beyond wild-type\nneighborhoods often leads to biologically implausible sequences or relies on\nsurrogate models that lose fidelity in novel regions. Here, we propose\nProSpero, an active learning framework in which a frozen pre-trained generative\nmodel is guided by a surrogate updated from oracle feedback. By integrating\nfitness-relevant residue selection with biologically-constrained Sequential\nMonte Carlo sampling, our approach enables exploration beyond wild-type\nneighborhoods while preserving biological plausibility. We show that our\nframework remains effective even when the surrogate is misspecified. ProSpero\nconsistently outperforms or matches existing methods across diverse protein\nengineering tasks, retrieving sequences of both high fitness and novelty."},{"date":"2025-05","title":"Understanding Adversarial Training with Energy-based Models","author":"Mujtaba Hussain Mirza, Maria Rosaria Briglia, Filippo Bartolucci, Senad Beadini, Giuseppe Lisanti, and Iacopo Masi","link":"http://arxiv.org/abs/2505.22486v1","abstract":"We aim at using Energy-based Model (EBM) framework to better understand\nadversarial training (AT) in classifiers, and additionally to analyze the\nintrinsic generative capabilities of robust classifiers. By viewing standard\nclassifiers through an energy lens, we begin by analyzing how the energies of\nadversarial examples, generated by various attacks, differ from those of the\nnatural samples. The central focus of our work is to understand the critical\nphenomena of Catastrophic Overfitting (CO) and Robust Overfitting (RO) in AT\nfrom an energy perspective. We analyze the impact of existing AT approaches on\nthe energy of samples during training and observe that the behavior of the\n``delta energy' -- change in energy between original sample and its adversarial\ncounterpart -- diverges significantly when CO or RO occurs. After a thorough\nanalysis of these energy dynamics and their relationship with overfitting, we\npropose a novel regularizer, the Delta Energy Regularizer (DER), designed to\nsmoothen the energy landscape during training. We demonstrate that DER is\neffective in mitigating both CO and RO across multiple benchmarks. We further\nshow that robust classifiers, when being used as generative models, have limits\nin handling trade-off between image quality and variability. We propose an\nimproved technique based on a local class-wise principal component analysis\n(PCA) and energy-based guidance for better class-specific initialization and\nadaptive stopping, enhancing sample diversity and generation quality.\nConsidering that we do not explicitly train for generative modeling, we achieve\na competitive Inception Score (IS) and Fr\\'echet inception distance (FID)\ncompared to hybrid discriminative-generative models."},{"date":"2025-05","title":"GeneBreaker: Jailbreak Attacks against DNA Language Models with Pathogenicity Guidance","author":"Zaixi Zhang, Zhenghong Zhou, Ruofan Jin, Le Cong, and Mengdi Wang","link":"http://arxiv.org/abs/2505.23839v1","abstract":"DNA, encoding genetic instructions for almost all living organisms, fuels\ngroundbreaking advances in genomics and synthetic biology. Recently, DNA\nFoundation Models have achieved success in designing synthetic functional DNA\nsequences, even whole genomes, but their susceptibility to jailbreaking remains\nunderexplored, leading to potential concern of generating harmful sequences\nsuch as pathogens or toxin-producing genes. In this paper, we introduce\nGeneBreaker, the first framework to systematically evaluate jailbreak\nvulnerabilities of DNA foundation models. GeneBreaker employs (1) an LLM agent\nwith customized bioinformatic tools to design high-homology, non-pathogenic\njailbreaking prompts, (2) beam search guided by PathoLM and log-probability\nheuristics to steer generation toward pathogen-like sequences, and (3) a\nBLAST-based evaluation pipeline against a curated Human Pathogen Database\n(JailbreakDNABench) to detect successful jailbreaks. Evaluated on our\nJailbreakDNABench, GeneBreaker successfully jailbreaks the latest Evo series\nmodels across 6 viral categories consistently (up to 60\\% Attack Success Rate\nfor Evo2-40B). Further case studies on SARS-CoV-2 spike protein and HIV-1\nenvelope protein demonstrate the sequence and structural fidelity of jailbreak\noutput, while evolutionary modeling of SARS-CoV-2 underscores biosecurity\nrisks. Our findings also reveal that scaling DNA foundation models amplifies\ndual-use risks, motivating enhanced safety alignment and tracing mechanisms.\nOur code is at https://github.com/zaixizhang/GeneBreaker."},{"date":"2025-05","title":"Test-Time Immunization: A Universal Defense Framework Against Jailbreaks for (Multimodal) Large Language Models","author":"Yongcan Yu, Yanbo Wang, Ran He, and Jian Liang","link":"http://arxiv.org/abs/2505.22271v1","abstract":"While (multimodal) large language models (LLMs) have attracted widespread\nattention due to their exceptional capabilities, they remain vulnerable to\njailbreak attacks. Various defense methods are proposed to defend against\njailbreak attacks, however, they are often tailored to specific types of\njailbreak attacks, limiting their effectiveness against diverse adversarial\nstrategies. For instance, rephrasing-based defenses are effective against text\nadversarial jailbreaks but fail to counteract image-based attacks. To overcome\nthese limitations, we propose a universal defense framework, termed Test-time\nIMmunization (TIM), which can adaptively defend against various jailbreak\nattacks in a self-evolving way. Specifically, TIM initially trains a gist token\nfor efficient detection, which it subsequently applies to detect jailbreak\nactivities during inference. When jailbreak attempts are identified, TIM\nimplements safety fine-tuning using the detected jailbreak instructions paired\nwith refusal answers. Furthermore, to mitigate potential performance\ndegradation in the detector caused by parameter updates during safety\nfine-tuning, we decouple the fine-tuning process from the detection module.\nExtensive experiments on both LLMs and multimodal LLMs demonstrate the efficacy\nof TIM."},{"date":"2025-05","title":"Patient-Aware Feature Alignment for Robust Lung Sound Classification:Cohesion-Separation and Global Alignment Losses","author":"Seung Gyu Jeong, and Seong Eun Kim","link":"http://arxiv.org/abs/2505.23834v1","abstract":"Lung sound classification is vital for early diagnosis of respiratory\ndiseases. However, biomedical signals often exhibit inter-patient variability\neven among patients with the same symptoms, requiring a learning approach that\nconsiders individual differences. We propose a Patient-Aware Feature Alignment\n(PAFA) framework with two novel losses, Patient Cohesion-Separation Loss (PCSL)\nand Global Patient Alignment Loss (GPAL). PCSL clusters features of the same\npatient while separating those from other patients to capture patient\nvariability, whereas GPAL draws each patient's centroid toward a global center,\npreventing feature space fragmentation. Our method achieves outstanding results\non the ICBHI dataset with a score of 64.84\\% for four-class and 72.08\\% for\ntwo-class classification. These findings highlight PAFA's ability to capture\nindividualized patterns and demonstrate performance gains in distinct patient\nclusters, offering broader applications for patient-centered healthcare."},{"date":"2025-05","title":"Accountable, Scalable and DoS-resilient Secure Vehicular Communication","author":"Hongyu Jin, and Panos Papadimitratos","link":"http://arxiv.org/abs/2505.22162v1","abstract":"Paramount to vehicle safety, broadcasted Cooperative Awareness Messages\n(CAMs) and Decentralized Environmental Notification Messages (DENMs) are\npseudonymously authenticated for security and privacy protection, with each\nnode needing to have all incoming messages validated within an expiration\ndeadline. This creates an asymmetry that can be easily exploited by external\nadversaries to launch a clogging Denial of Service (DoS) attack: each forged VC\nmessage forces all neighboring nodes to cryptographically validate it; at\nincreasing rates, easy to generate forged messages gradually exhaust processing\nresources and severely degrade or deny timely validation of benign CAMs/DENMs.\nThe result can be catastrophic when awareness of neighbor vehicle positions or\ncritical reports are missed. We address this problem making the standardized VC\npseudonymous authentication DoS-resilient. We propose efficient cryptographic\nconstructs, which we term message verification facilitators, to prioritize\nprocessing resources for verification of potentially valid messages among bogus\nmessages and verify multiple messages based on one signature verification. Any\nmessage acceptance is strictly based on public-key based message\nauthentication/verification for accountability, i.e., non-repudiation is not\nsacrificed, unlike symmetric key based approaches. This further enables drastic\nmisbehavior detection, also exploiting the newly introduced facilitators, based\non probabilistic signature verification and cross-checking over multiple\nfacilitators verifying the same message; while maintaining verification latency\nlow even when under attack, trading off modest communication overhead. Our\nfacilitators can also be used for efficient discovery and verification of DENM\nor any event-driven message, including misbehavior evidence used for our\nscheme."},{"date":"2025-05","title":"Learning A Robust RGB-Thermal Detector for Extreme Modality Imbalance","author":"Chao Tian, Chao Yang, Guoqing Zhu, Qiang Wang, and Zhenyu He","link":"http://arxiv.org/abs/2505.22154v1","abstract":"RGB-Thermal (RGB-T) object detection utilizes thermal infrared (TIR) images\nto complement RGB data, improving robustness in challenging conditions.\nTraditional RGB-T detectors assume balanced training data, where both\nmodalities contribute equally. However, in real-world scenarios, modality\ndegradation-due to environmental factors or technical issues-can lead to\nextreme modality imbalance, causing out-of-distribution (OOD) issues during\ntesting and disrupting model convergence during training. This paper addresses\nthese challenges by proposing a novel base-and-auxiliary detector architecture.\nWe introduce a modality interaction module to adaptively weigh modalities based\non their quality and handle imbalanced samples effectively. Additionally, we\nleverage modality pseudo-degradation to simulate real-world imbalances in\ntraining data. The base detector, trained on high-quality pairs, provides a\nconsistency constraint for the auxiliary detector, which receives degraded\nsamples. This framework enhances model robustness, ensuring reliable\nperformance even under severe modality degradation. Experimental results\ndemonstrate the effectiveness of our method in handling extreme modality\nimbalances~(decreasing the Missing Rate by 55%) and improving performance\nacross various baseline detectors."},{"date":"2025-05","title":"Spa-VLM: Stealthy Poisoning Attacks on RAG-based VLM","author":"Lei Yu, Yechao Zhang, Ziqi Zhou, Yang Wu, Wei Wan, Minghui Li, Shengshan Hu, Pei Xiaobing, and Jing Wang","link":"http://arxiv.org/abs/2505.23828v1","abstract":"With the rapid development of the Vision-Language Model (VLM), significant\nprogress has been made in Visual Question Answering (VQA) tasks. However,\nexisting VLM often generate inaccurate answers due to a lack of up-to-date\nknowledge. To address this issue, recent research has introduced\nRetrieval-Augmented Generation (RAG) techniques, commonly used in Large\nLanguage Models (LLM), into VLM, incorporating external multi-modal knowledge\nto enhance the accuracy and practicality of VLM systems. Nevertheless, the RAG\nin LLM may be susceptible to data poisoning attacks. RAG-based VLM may also\nface the threat of this attack. This paper first reveals the vulnerabilities of\nthe RAG-based large model under poisoning attack, showing that existing\nsingle-modal RAG poisoning attacks have a 100\\% failure rate in multi-modal RAG\nscenarios. To address this gap, we propose Spa-VLM (Stealthy Poisoning Attack\non RAG-based VLM), a new paradigm for poisoning attacks on large models. We\ncarefully craft malicious multi-modal knowledge entries, including adversarial\nimages and misleading text, which are then injected into the RAG's knowledge\nbase. When users access the VLM service, the system may generate misleading\noutputs. We evaluate Spa-VLM on two Wikipedia datasets and across two different\nRAGs. Results demonstrate that our method achieves highly stealthy poisoning,\nwith the attack success rate exceeding 0.8 after injecting just 5 malicious\nentries into knowledge bases with 100K and 2M entries, outperforming\nstate-of-the-art poisoning attacks designed for RAG-based LLMs. Additionally,\nwe evaluated several defense mechanisms, all of which ultimately proved\nineffective against Spa-VLM, underscoring the effectiveness and robustness of\nour attack."},{"date":"2025-05","title":"Are classical deep neural networks weakly adversarially robust?","author":"Nuolin Sun, Linyuan Wang, Dongyang Li, Bin Yan, and Lei Li","link":"http://arxiv.org/abs/2506.02016v1","abstract":"Adversarial attacks have received increasing attention and it has been widely\nrecognized that classical DNNs have weak adversarial robustness. The most\ncommonly used adversarial defense method, adversarial training, improves the\nadversarial accuracy of DNNs by generating adversarial examples and retraining\nthe model. However, adversarial training requires a significant computational\noverhead. In this paper, inspired by existing studies focusing on the\nclustering properties of DNN output features at each layer and the Progressive\nFeedforward Collapse phenomenon, we propose a method for adversarial example\ndetection and image recognition that uses layer-wise features to construct\nfeature paths and computes the correlation between the examples feature paths\nand the class-centered feature paths. Experimental results show that the\nrecognition method achieves 82.77% clean accuracy and 44.17% adversarial\naccuracy on the ResNet-20 with PFC. Compared to the adversarial training method\nwith 77.64% clean accuracy and 52.94% adversarial accuracy, our method exhibits\na trade-off without relying on computationally expensive defense strategies.\nFurthermore, on the standard ResNet-18, our method maintains this advantage\nwith respective metrics of 80.01% and 46.1%. This result reveals inherent\nadversarial robustness in DNNs, challenging the conventional understanding of\nthe weak adversarial robustness in DNNs."},{"date":"2025-05","title":"Securing the Software Package Supply Chain for Critical Systems","author":"Ritwik Murali, and Akash Ravi","link":"http://arxiv.org/abs/2505.22023v1","abstract":"Software systems have grown as an indispensable commodity used across various\nindustries, and almost all essential services depend on them for effective\noperation. The software is no longer an independent or stand-alone piece of\ncode written by a developer but rather a collection of packages designed by\nmultiple developers across the globe. Ensuring the reliability and resilience\nof these systems is crucial since emerging threats target software supply\nchains, as demonstrated by the widespread SolarWinds hack in late 2020. These\nsupply chains extend beyond patches and updates, involving distribution\nnetworks throughout the software lifecycle. Industries like smart grids,\nmanufacturing, healthcare, and finance rely on interconnected software systems\nand their dependencies for effective functioning. To secure software modules\nand add-ons, robust distribution architectures are essential. The proposed\nchapter enhances the existing delivery frameworks by including a permissioned\nledger with Proof of Authority consensus and multi-party signatures. The\nproposed system aims to prevent attacks while permitting every stakeholder to\nverify the same. Critical systems can interface with the secure pipeline\nwithout disrupting existing functionalities, thus preventing the cascading\neffect of an attack at any point in the supply chain."},{"date":"2025-05","title":"GL-PGENet: A Parameterized Generation Framework for Robust Document Image Enhancement","author":"Zhihong Tang, and Yang Li","link":"http://arxiv.org/abs/2505.22021v1","abstract":"Document Image Enhancement (DIE) serves as a critical component in Document\nAI systems, where its performance substantially determines the effectiveness of\ndownstream tasks. To address the limitations of existing methods confined to\nsingle-degradation restoration or grayscale image processing, we present Global\nwith Local Parametric Generation Enhancement Network (GL-PGENet), a novel\narchitecture designed for multi-degraded color document images, ensuring both\nefficiency and robustness in real-world scenarios. Our solution incorporates\nthree key innovations: First, a hierarchical enhancement framework that\nintegrates global appearance correction with local refinement, enabling\ncoarse-to-fine quality improvement. Second, a Dual-Branch Local-Refine Network\nwith parametric generation mechanisms that replaces conventional direct\nprediction, producing enhanced outputs through learned intermediate parametric\nrepresentations rather than pixel-wise mapping. This approach enhances local\nconsistency while improving model generalization. Finally, a modified NestUNet\narchitecture incorporating dense block to effectively fuse low-level pixel\nfeatures and high-level semantic features, specifically adapted for document\nimage characteristics. In addition, to enhance generalization performance, we\nadopt a two-stage training strategy: large-scale pretraining on a synthetic\ndataset of 500,000+ samples followed by task-specific fine-tuning. Extensive\nexperiments demonstrate the superiority of GL-PGENet, achieving\nstate-of-the-art SSIM scores of 0.7721 on DocUNet and 0.9480 on RealDAE. The\nmodel also exhibits remarkable cross-domain adaptability and maintains\ncomputational efficiency for high-resolution images without performance\ndegradation, confirming its practical utility in real-world scenarios."},{"date":"2025-05","title":"Distributionally Robust Wireless Semantic Communication with Large AI Models","author":"Long Tan Le, Senura Hansaja Wanasekara, Zerun Niu, Yansong Shi, Nguyen H. Tran, Phuong Vo, Walid Saad, Dusit Niyato, Zhu Han, Choong Seon Hong, and H. Vincent Poor","link":"http://arxiv.org/abs/2506.03167v1","abstract":"6G wireless systems are expected to support massive volumes of data with\nultra-low latency. However, conventional bit-level transmission strategies\ncannot support the efficiency and adaptability required by modern,\ndata-intensive applications. The concept of semantic communication (SemCom)\naddresses this limitation by focusing on transmitting task-relevant semantic\ninformation instead of raw data. While recent efforts incorporating deep\nlearning and large-scale AI models have improved SemCom's performance, existing\nsystems remain vulnerable to both semantic-level and transmission-level noise\nbecause they often rely on domain-specific architectures that hinder\ngeneralizability. In this paper, a novel and generalized semantic communication\nframework called WaSeCom is proposed to systematically address uncertainty and\nenhance robustness. In particular, Wasserstein distributionally robust\noptimization is employed to provide resilience against semantic\nmisinterpretation and channel perturbations. A rigorous theoretical analysis is\nperformed to establish the robust generalization guarantees of the proposed\nframework. Experimental results on image and text transmission demonstrate that\nWaSeCom achieves improved robustness under noise and adversarial perturbations.\nThese results highlight its effectiveness in preserving semantic fidelity\nacross varying wireless conditions."},{"date":"2025-05","title":"Practical Adversarial Attacks on Stochastic Bandits via Fake Data Injection","author":"Qirun Zeng, Eric He, Richard Hoffmann, Xuchuang Wang, and Jinhang Zuo","link":"http://arxiv.org/abs/2505.21938v2","abstract":"Adversarial attacks on stochastic bandits have traditionally relied on some\nunrealistic assumptions, such as per-round reward manipulation and unbounded\nperturbations, limiting their relevance to real-world systems. We propose a\nmore practical threat model, Fake Data Injection, which reflects realistic\nadversarial constraints: the attacker can inject only a limited number of\nbounded fake feedback samples into the learner's history, simulating legitimate\ninteractions. We design efficient attack strategies under this model,\nexplicitly addressing both magnitude constraints (on reward values) and\ntemporal constraints (on when and how often data can be injected). Our\ntheoretical analysis shows that these attacks can mislead both Upper Confidence\nBound (UCB) and Thompson Sampling algorithms into selecting a target arm in\nnearly all rounds while incurring only sublinear attack cost. Experiments on\nsynthetic and real-world datasets validate the effectiveness of our strategies,\nrevealing significant vulnerabilities in widely used stochastic bandit\nalgorithms under practical adversarial scenarios."},{"date":"2025-05","title":"SpeechVerifier: Robust Acoustic Fingerprint against Tampering Attacks via Watermarking","author":"Lingfeng Yao, Chenpei Huang, Shengyao Wang, Junpei Xue, Hanqing Guo, Jiang Liu, Xun Chen, and Miao Pan","link":"http://arxiv.org/abs/2505.23821v2","abstract":"With the surge of social media, maliciously tampered public speeches,\nespecially those from influential figures, have seriously affected social\nstability and public trust. Existing speech tampering detection methods remain\ninsufficient: they either rely on external reference data or fail to be both\nsensitive to attacks and robust to benign operations, such as compression and\nresampling. To tackle these challenges, we introduce SpeechVerifer to\nproactively verify speech integrity using only the published speech itself,\ni.e., without requiring any external references. Inspired by audio\nfingerprinting and watermarking, SpeechVerifier can (i) effectively detect\ntampering attacks, (ii) be robust to benign operations and (iii) verify the\nintegrity only based on published speeches. Briefly, SpeechVerifier utilizes\nmultiscale feature extraction to capture speech features across different\ntemporal resolutions. Then, it employs contrastive learning to generate\nfingerprints that can detect modifications at varying granularities. These\nfingerprints are designed to be robust to benign operations, but exhibit\nsignificant changes when malicious tampering occurs. To enable speech\nverification in a self-contained manner, the generated fingerprints are then\nembedded into the speech signal by segment-wise watermarking. Without external\nreferences, SpeechVerifier can retrieve the fingerprint from the published\naudio and check it with the embedded watermark to verify the integrity of the\nspeech. Extensive experimental results demonstrate that the proposed\nSpeechVerifier is effective in detecting tampering attacks and robust to benign\noperations."},{"date":"2025-05","title":"Evaluating the Retrieval Robustness of Large Language Models","author":"Shuyang Cao, Karthik Radhakrishnan, David Rosenberg, Steven Lu, Pengxiang Cheng, Lu Wang, and Shiyue Zhang","link":"http://arxiv.org/abs/2505.21870v1","abstract":"Retrieval-augmented generation (RAG) generally enhances large language\nmodels' (LLMs) ability to solve knowledge-intensive tasks. But RAG may also\nlead to performance degradation due to imperfect retrieval and the model's\nlimited ability to leverage retrieved content. In this work, we evaluate the\nrobustness of LLMs in practical RAG setups (henceforth retrieval robustness).\nWe focus on three research questions: (1) whether RAG is always better than\nnon-RAG; (2) whether more retrieved documents always lead to better\nperformance; (3) and whether document orders impact results. To facilitate this\nstudy, we establish a benchmark of 1500 open-domain questions, each with\nretrieved documents from Wikipedia. We introduce three robustness metrics, each\ncorresponds to one research question. Our comprehensive experiments, involving\n11 LLMs and 3 prompting strategies, reveal that all of these LLMs exhibit\nsurprisingly high retrieval robustness; nonetheless, different degrees of\nimperfect robustness hinders them from fully utilizing the benefits of RAG."},{"date":"2025-05","title":"Rethinking Gradient-based Adversarial Attacks on Point Cloud Classification","author":"Jun Chen, Xinke Li, Mingyue Xu, Tianrui Li, and Chongshou Li","link":"http://arxiv.org/abs/2505.21854v1","abstract":"Gradient-based adversarial attacks have become a dominant approach for\nevaluating the robustness of point cloud classification models. However,\nexisting methods often rely on uniform update rules that fail to consider the\nheterogeneous nature of point clouds, resulting in excessive and perceptible\nperturbations. In this paper, we rethink the design of gradient-based attacks\nby analyzing the limitations of conventional gradient update mechanisms and\npropose two new strategies to improve both attack effectiveness and\nimperceptibility. First, we introduce WAAttack, a novel framework that\nincorporates weighted gradients and an adaptive step-size strategy to account\nfor the non-uniform contribution of points during optimization. This approach\nenables more targeted and subtle perturbations by dynamically adjusting updates\naccording to the local structure and sensitivity of each point. Second, we\npropose SubAttack, a complementary strategy that decomposes the point cloud\ninto subsets and focuses perturbation efforts on structurally critical regions.\nTogether, these methods represent a principled rethinking of gradient-based\nadversarial attacks for 3D point cloud classification. Extensive experiments\ndemonstrate that our approach outperforms state-of-the-art baselines in\ngenerating highly imperceptible adversarial examples. Code will be released\nupon paper acceptance."},{"date":"2025-05","title":"An Optimistic Algorithm for online CMDPS with Anytime Adversarial Constraints","author":"Jiahui Zhu, Kihyun Yu, Dabeen Lee, Xin Liu, and Honghao Wei","link":"http://arxiv.org/abs/2505.21841v1","abstract":"Online safe reinforcement learning (RL) plays a key role in dynamic\nenvironments, with applications in autonomous driving, robotics, and\ncybersecurity. The objective is to learn optimal policies that maximize rewards\nwhile satisfying safety constraints modeled by constrained Markov decision\nprocesses (CMDPs). Existing methods achieve sublinear regret under stochastic\nconstraints but often fail in adversarial settings, where constraints are\nunknown, time-varying, and potentially adversarially designed. In this paper,\nwe propose the Optimistic Mirror Descent Primal-Dual (OMDPD) algorithm, the\nfirst to address online CMDPs with anytime adversarial constraints. OMDPD\nachieves optimal regret O(sqrt(K)) and strong constraint violation O(sqrt(K))\nwithout relying on Slater's condition or the existence of a strictly known safe\npolicy. We further show that access to accurate estimates of rewards and\ntransitions can further improve these bounds. Our results offer practical\nguarantees for safe decision-making in adversarial environments."},{"date":"2025-05","title":"Faster Rates for Private Adversarial Bandits","author":"Hilal Asi, Vinod Raman, and Kunal Talwar","link":"http://arxiv.org/abs/2505.21790v1","abstract":"We design new differentially private algorithms for the problems of\nadversarial bandits and bandits with expert advice. For adversarial bandits, we\ngive a simple and efficient conversion of any non-private bandit algorithm to a\nprivate bandit algorithm. Instantiating our conversion with existing\nnon-private bandit algorithms gives a regret upper bound of\n$O\\left(\\frac{\\sqrt{KT}}{\\sqrt{\\epsilon}}\\right)$, improving upon the existing\nupper bound $O\\left(\\frac{\\sqrt{KT \\log(KT)}}{\\epsilon}\\right)$ for all\n$\\epsilon \\leq 1$. In particular, our algorithms allow for sublinear expected\nregret even when $\\epsilon \\leq \\frac{1}{\\sqrt{T}}$, establishing the first\nknown separation between central and local differential privacy for this\nproblem. For bandits with expert advice, we give the first differentially\nprivate algorithms, with expected regret\n$O\\left(\\frac{\\sqrt{NT}}{\\sqrt{\\epsilon}}\\right),\nO\\left(\\frac{\\sqrt{KT\\log(N)}\\log(KT)}{\\epsilon}\\right)$, and\n$\\tilde{O}\\left(\\frac{N^{1/6}K^{1/2}T^{2/3}\\log(NT)}{\\epsilon ^{1/3}} +\n\\frac{N^{1/2}\\log(NT)}{\\epsilon}\\right)$, where $K$ and $N$ are the number of\nactions and experts respectively. These rates allow us to get sublinear regret\nfor different combinations of small and large $K, N$ and $\\epsilon.$"},{"date":"2025-05","title":"System Prompt Extraction Attacks and Defenses in Large Language Models","author":"Badhan Chandra Das, M. Hadi Amini, and Yanzhao Wu","link":"http://arxiv.org/abs/2505.23817v1","abstract":"The system prompt in Large Language Models (LLMs) plays a pivotal role in\nguiding model behavior and response generation. Often containing private\nconfiguration details, user roles, and operational instructions, the system\nprompt has become an emerging attack target. Recent studies have shown that LLM\nsystem prompts are highly susceptible to extraction attacks through\nmeticulously designed queries, raising significant privacy and security\nconcerns. Despite the growing threat, there is a lack of systematic studies of\nsystem prompt extraction attacks and defenses. In this paper, we present a\ncomprehensive framework, SPE-LLM, to systematically evaluate System Prompt\nExtraction attacks and defenses in LLMs. First, we design a set of novel\nadversarial queries that effectively extract system prompts in state-of-the-art\n(SOTA) LLMs, demonstrating the severe risks of LLM system prompt extraction\nattacks. Second, we propose three defense techniques to mitigate system prompt\nextraction attacks in LLMs, providing practical solutions for secure LLM\ndeployments. Third, we introduce a set of rigorous evaluation metrics to\naccurately quantify the severity of system prompt extraction attacks in LLMs\nand conduct comprehensive experiments across multiple benchmark datasets, which\nvalidates the efficacy of our proposed SPE-LLM framework."},{"date":"2025-05","title":"FRAMES-VQA: Benchmarking Fine-Tuning Robustness across Multi-Modal Shifts in Visual Question Answering","author":"Chengyue Huang, Brisa Maneechotesuwan, Shivang Chopra, and Zsolt Kira","link":"http://arxiv.org/abs/2505.21755v1","abstract":"Visual question answering (VQA) systems face significant challenges when\nadapting to real-world data shifts, especially in multi-modal contexts. While\nrobust fine-tuning strategies are essential for maintaining performance across\nin-distribution (ID) and out-of-distribution (OOD) scenarios, current\nevaluation settings are primarily unimodal or particular to some types of OOD,\noffering limited insight into the complexities of multi-modal contexts. In this\nwork, we propose a new benchmark FRAMES-VQA (Fine-Tuning Robustness across\nMulti-Modal Shifts in VQA) for evaluating robust fine-tuning for VQA tasks. We\nutilize ten existing VQA benchmarks, including VQAv2, IV-VQA, VQA-CP, OK-VQA\nand others, and categorize them into ID, near and far OOD datasets covering\nuni-modal, multi-modal and adversarial distribution shifts. We first conduct a\ncomprehensive comparison of existing robust fine-tuning methods. We then\nquantify the distribution shifts by calculating the Mahalanobis distance using\nuni-modal and multi-modal embeddings extracted from various models. Further, we\nperform an extensive analysis to explore the interactions between uni- and\nmulti-modal shifts as well as modality importance for ID and OOD samples. These\nanalyses offer valuable guidance on developing more robust fine-tuning methods\nto handle multi-modal distribution shifts. The code is available at\nhttps://github.com/chengyuehuang511/FRAMES-VQA ."},{"date":"2025-05","title":"What is Adversarial Training for Diffusion Models?","author":"Briglia Maria Rosaria, Mujtaba Hussain Mirza, Giuseppe Lisanti, and Iacopo Masi","link":"http://arxiv.org/abs/2505.21742v1","abstract":"We answer the question in the title, showing that adversarial training (AT)\nfor diffusion models (DMs) fundamentally differs from classifiers: while AT in\nclassifiers enforces output invariance, AT in DMs requires equivariance to keep\nthe diffusion process aligned with the data distribution. AT is a way to\nenforce smoothness in the diffusion flow, improving robustness to outliers and\ncorrupted data. Unlike prior art, our method makes no assumptions about the\nnoise model and integrates seamlessly into diffusion training by adding random\nnoise, similar to randomized smoothing, or adversarial noise, akin to AT. This\nenables intrinsic capabilities such as handling noisy data, dealing with\nextreme variability such as outliers, preventing memorization, and improving\nrobustness. We rigorously evaluate our approach with proof-of-concept datasets\nwith known distributions in low- and high-dimensional space, thereby taking a\nperfect measure of errors; we further evaluate on standard benchmarks such as\nCIFAR-10, CelebA and LSUN Bedroom, showing strong performance under severe\nnoise, data corruption, and iterative adversarial attacks."},{"date":"2025-05","title":"A Joint Reconstruction-Triplet Loss Autoencoder Approach Towards Unseen Attack Detection in IoV Networks","author":"Julia Boone, Tolunay Seyfi, and Fatemeh Afghah","link":"http://arxiv.org/abs/2505.21703v1","abstract":"Internet of Vehicles (IoV) systems, while offering significant advancements\nin transportation efficiency and safety, introduce substantial security\nvulnerabilities due to their highly interconnected nature. These dynamic\nsystems produce massive amounts of data between vehicles, infrastructure, and\ncloud services and present a highly distributed framework with a wide attack\nsurface. In considering network-centered attacks on IoV systems, attacks such\nas Denial-of-Service (DoS) can prohibit the communication of essential physical\ntraffic safety information between system elements, illustrating that the\nsecurity concerns for these systems go beyond the traditional confidentiality,\nintegrity, and availability concerns of enterprise systems. Given the\ncomplexity and volume of data generated by IoV systems, traditional security\nmechanisms are often inadequate for accurately detecting sophisticated and\nevolving cyberattacks. Here, we present an unsupervised autoencoder method\ntrained entirely on benign network data for the purpose of unseen attack\ndetection in IoV networks. We leverage a weighted combination of reconstruction\nand triplet margin loss to guide the autoencoder training and develop a diverse\nrepresentation of the benign training set. We conduct extensive experiments on\nrecent network intrusion datasets from two different application domains,\nindustrial IoT and home IoT, that represent the modern IoV task. We show that\nour method performs robustly for all unseen attack types, with roughly 99%\naccuracy on benign data and between 97% and 100% performance on anomaly data.\nWe extend these results to show that our model is adaptable through the use of\ntransfer learning, achieving similarly high results while leveraging domain\nfeatures from one domain to another."},{"date":"2025-05","title":"Expert Survey: AI Reliability & Security Research Priorities","author":"Joe O'Brien, Jeremy Dolan, Jay Kim, Jonah Dykhuizen, Jeba Sania, Sebastian Becker, Jam Kraprayoon, and Cara Labrador","link":"http://arxiv.org/abs/2505.21664v1","abstract":"Our survey of 53 specialists across 105 AI reliability and security research\nareas identifies the most promising research prospects to guide strategic AI\nR&D investment. As companies are seeking to develop AI systems with broadly\nhuman-level capabilities, research on reliability and security is urgently\nneeded to ensure AI's benefits can be safely and broadly realized and prevent\nsevere harms. This study is the first to quantify expert priorities across a\ncomprehensive taxonomy of AI safety and security research directions and to\nproduce a data-driven ranking of their potential impact. These rankings may\nsupport evidence-based decisions about how to effectively deploy resources\ntoward AI reliability and security research."},{"date":"2025-05","title":"VideoMarkBench: Benchmarking Robustness of Video Watermarking","author":"Zhengyuan Jiang, Moyang Guo, Kecen Li, Yuepeng Hu, Yupu Wang, Zhicong Huang, Cheng Hong, and Neil Zhenqiang Gong","link":"http://arxiv.org/abs/2505.21620v1","abstract":"The rapid development of video generative models has led to a surge in highly\nrealistic synthetic videos, raising ethical concerns related to disinformation\nand copyright infringement. Recently, video watermarking has been proposed as a\nmitigation strategy by embedding invisible marks into AI-generated videos to\nenable subsequent detection. However, the robustness of existing video\nwatermarking methods against both common and adversarial perturbations remains\nunderexplored. In this work, we introduce VideoMarkBench, the first systematic\nbenchmark designed to evaluate the robustness of video watermarks under\nwatermark removal and watermark forgery attacks. Our study encompasses a\nunified dataset generated by three state-of-the-art video generative models,\nacross three video styles, incorporating four watermarking methods and seven\naggregation strategies used during detection. We comprehensively evaluate 12\ntypes of perturbations under white-box, black-box, and no-box threat models.\nOur findings reveal significant vulnerabilities in current watermarking\napproaches and highlight the urgent need for more robust solutions. Our code is\navailable at https://github.com/zhengyuan-jiang/VideoMarkBench."},{"date":"2025-05","title":"AdInject: Real-World Black-Box Attacks on Web Agents via Advertising Delivery","author":"Haowei Wang, Junjie Wang, Xiaojun Jia, Rupeng Zhang, Mingyang Li, Zhe Liu, Yang Liu, and Qing Wang","link":"http://arxiv.org/abs/2505.21499v1","abstract":"Vision-Language Model (VLM) based Web Agents represent a significant step\ntowards automating complex tasks by simulating human-like interaction with\nwebsites. However, their deployment in uncontrolled web environments introduces\nsignificant security vulnerabilities. Existing research on adversarial\nenvironmental injection attacks often relies on unrealistic assumptions, such\nas direct HTML manipulation, knowledge of user intent, or access to agent model\nparameters, limiting their practical applicability. In this paper, we propose\nAdInject, a novel and real-world black-box attack method that leverages the\ninternet advertising delivery to inject malicious content into the Web Agent's\nenvironment. AdInject operates under a significantly more realistic threat\nmodel than prior work, assuming a black-box agent, static malicious content\nconstraints, and no specific knowledge of user intent. AdInject includes\nstrategies for designing malicious ad content aimed at misleading agents into\nclicking, and a VLM-based ad content optimization technique that infers\npotential user intents from the target website's context and integrates these\nintents into the ad content to make it appear more relevant or critical to the\nagent's task, thus enhancing attack effectiveness. Experimental evaluations\ndemonstrate the effectiveness of AdInject, attack success rates exceeding 60%\nin most scenarios and approaching 100% in certain cases. This strongly\ndemonstrates that prevalent advertising delivery constitutes a potent and\nreal-world vector for environment injection attacks against Web Agents. This\nwork highlights a critical vulnerability in Web Agent security arising from\nreal-world environment manipulation channels, underscoring the urgent need for\ndeveloping robust defense mechanisms against such threats. Our code is\navailable at https://github.com/NicerWang/AdInject."}]