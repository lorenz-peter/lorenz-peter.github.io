[
    {
        "date": "2025-06",
        "title": "Exposing Hidden Backdoors in NFT Smart Contracts: A Static Security Analysis of Rug Pull Patterns",
        "author": "Chetan Pathade, and Shweta Hooli",
        "link": "http://arxiv.org/abs/2506.07974v1",
        "abstract": "The explosive growth of Non-Fungible Tokens (NFTs) has revolutionized digital\nownership by enabling the creation, exchange, and monetization of unique assets\non blockchain networks. However, this surge in popularity has also given rise\nto a disturbing trend: the emergence of rug pulls - fraudulent schemes where\ndevelopers exploit trust and smart contract privileges to drain user funds or\ninvalidate asset ownership. Central to many of these scams are hidden backdoors\nembedded within NFT smart contracts. Unlike unintentional bugs, these backdoors\nare deliberately coded and often obfuscated to bypass traditional audits and\nexploit investor confidence. In this paper, we present a large-scale static\nanalysis of 49,940 verified NFT smart contracts using Slither, a static\nanalysis framework, to uncover latent vulnerabilities commonly linked to rug\npulls. We introduce a custom risk scoring model that classifies contracts into\nhigh, medium, or low risk tiers based on the presence and severity of rug pull\nindicators. Our dataset was derived from verified contracts on the Ethereum\nmainnet, and we generate multiple visualizations to highlight red flag\nclusters, issue prevalence, and co-occurrence of critical vulnerabilities.\nWhile we do not perform live exploits, our results reveal how malicious\npatterns often missed by simple reviews can be surfaced through static analysis\nat scale. We conclude by offering mitigation strategies for developers,\nmarketplaces, and auditors to enhance smart contract security. By exposing how\nhidden backdoors manifest in real-world smart contracts, this work contributes\na practical foundation for detecting and mitigating NFT rug pulls through\nscalable automated analysis."
    },
    {
        "date": "2025-06",
        "title": "Secure Distributed Learning for CAVs: Defending Against Gradient Leakage with Leveled Homomorphic Encryption",
        "author": "Muhammad Ali Najjar, Ren-Yi Huang, Dumindu Samaraweera, and Prashant Shekhar",
        "link": "http://arxiv.org/abs/2506.07894v1",
        "abstract": "Federated Learning (FL) enables collaborative model training across\ndistributed clients without sharing raw data, making it a promising approach\nfor privacy-preserving machine learning in domains like Connected and\nAutonomous Vehicles (CAVs). However, recent studies have shown that exchanged\nmodel gradients remain susceptible to inference attacks such as Deep Leakage\nfrom Gradients (DLG), which can reconstruct private training data. While\nexisting defenses like Differential Privacy (DP) and Secure Multi-Party\nComputation (SMPC) offer protection, they often compromise model accuracy. To\nthat end, Homomorphic Encryption (HE) offers a promising alternative by\nenabling lossless computation directly on encrypted data, thereby preserving\nboth privacy and model utility. However, HE introduces significant\ncomputational and communication overhead, which can hinder its practical\nadoption. To address this, we systematically evaluate various leveled HE\nschemes to identify the most suitable for FL in resource-constrained\nenvironments due to its ability to support fixed-depth computations without\nrequiring costly bootstrapping. Our contributions in this paper include a\ncomprehensive evaluation of HE schemes for real-world FL applications, a\nselective encryption strategy that targets only the most sensitive gradients to\nminimize computational overhead, and the development of a full HE-based FL\npipeline that effectively mitigates DLG attacks while preserving model\naccuracy. We open-source our implementation to encourage reproducibility and\nfacilitate adoption in safety-critical domains."
    },
    {
        "date": "2025-06",
        "title": "SoK: Data Reconstruction Attacks Against Machine Learning Models: Definition, Metrics, and Benchmark",
        "author": "Rui Wen, Yiyong Liu, Michael Backes, and Yang Zhang",
        "link": "http://arxiv.org/abs/2506.07888v1",
        "abstract": "Data reconstruction attacks, which aim to recover the training dataset of a\ntarget model with limited access, have gained increasing attention in recent\nyears. However, there is currently no consensus on a formal definition of data\nreconstruction attacks or appropriate evaluation metrics for measuring their\nquality. This lack of rigorous definitions and universal metrics has hindered\nfurther advancement in this field. In this paper, we address this issue in the\nvision domain by proposing a unified attack taxonomy and formal definitions of\ndata reconstruction attacks. We first propose a set of quantitative evaluation\nmetrics that consider important criteria such as quantifiability, consistency,\nprecision, and diversity. Additionally, we leverage large language models\n(LLMs) as a substitute for human judgment, enabling visual evaluation with an\nemphasis on high-quality reconstructions. Using our proposed taxonomy and\nmetrics, we present a unified framework for systematically evaluating the\nstrengths and limitations of existing attacks and establishing a benchmark for\nfuture research. Empirical results, primarily from a memorization perspective,\nnot only validate the effectiveness of our metrics but also offer valuable\ninsights for designing new attacks."
    },
    {
        "date": "2025-06",
        "title": "Securing Unbounded Differential Privacy Against Timing Attacks",
        "author": "Zachary Ratliff, and Salil Vadhan",
        "link": "http://arxiv.org/abs/2506.07868v1",
        "abstract": "Recent works have started to theoretically investigate how we can protect\ndifferentially private programs against timing attacks, by making the joint\ndistribution the output and the runtime differentially private (JOT-DP).\nHowever, the existing approaches to JOT-DP have some limitations, particularly\nin the setting of unbounded DP (which protects the size of the dataset and\napplies to arbitrarily large datasets). First, the known conversion of pure DP\nprograms to pure JOT-DP programs in the unbounded setting (a) incurs a constant\nadditive increase in error probability (and thus does not provide vanishing\nerror as $n\\to\\infty$) (b) produces JOT-DP programs that fail to preserve the\ncomputational efficiency of the original pure DP program and (c) is analyzed in\na toy computational model in which the runtime is defined to be the number of\ncoin flips. In this work, we overcome these limitations. Specifically, we show\nthat the error required for pure JOT-DP in the unbounded setting depends on the\nmodel of computation. In a randomized RAM model where the dataset size $n$ is\ngiven (or can be computed in constant time) and we can generate random numbers\n(not just random bits) in constant time, polynomially small error probability\nis necessary and sufficient. If $n$ is not given or we only have a random-bit\ngenerator, an (arbitrarily small) constant error probability is necessary and\nsufficient. The aforementioned positive results are proven by efficient\nprocedures to convert any pure JOT-DP program $P$ in the upper-bounded setting\nto a pure JOT-DP program $P'$ in the unbounded setting, such that the output\ndistribution of $P'$ is $\\gamma$-close in total variation distance to that of\n$P$, where $\\gamma$ is either an arbitrarily small constant or polynomially\nsmall, depending on the model of computation."
    },
    {
        "date": "2025-06",
        "title": "Are Trees Really Green? A Detection Approach of IoT Malware Attacks",
        "author": "Silvia Lucia Sanna, Diego Soi, Davide Maiorca, and Giorgio Giacinto",
        "link": "http://arxiv.org/abs/2506.07836v1",
        "abstract": "Nowadays, the Internet of Things (IoT) is widely employed, and its usage is\ngrowing exponentially because it facilitates remote monitoring, predictive\nmaintenance, and data-driven decision making, especially in the healthcare and\nindustrial sectors. However, IoT devices remain vulnerable due to their\nresource constraints and difficulty in applying security patches. Consequently,\nvarious cybersecurity attacks are reported daily, such as Denial of Service,\nparticularly in IoT-driven solutions. Most attack detection methodologies are\nbased on Machine Learning (ML) techniques, which can detect attack patterns.\nHowever, the focus is more on identification rather than considering the impact\nof ML algorithms on computational resources. This paper proposes a green\nmethodology to identify IoT malware networking attacks based on flow\nprivacy-preserving statistical features. In particular, the hyperparameters of\nthree tree-based models -- Decision Trees, Random Forest and Extra-Trees -- are\noptimized based on energy consumption and test-time performance in terms of\nMatthew's Correlation Coefficient. Our results show that models maintain high\nperformance and detection accuracy while consistently reducing power usage in\nterms of watt-hours (Wh). This suggests that on-premise ML-based Intrusion\nDetection Systems are suitable for IoT and other resource-constrained devices."
    },
    {
        "date": "2025-06",
        "title": "Enhancing Adversarial Robustness with Conformal Prediction: A Framework for Guaranteed Model Reliability",
        "author": "Jie Bao, Chuangyin Dang, Rui Luo, Hanwei Zhang, and Zhixin Zhou",
        "link": "http://arxiv.org/abs/2506.07804v1",
        "abstract": "As deep learning models are increasingly deployed in high-risk applications,\nrobust defenses against adversarial attacks and reliable performance guarantees\nbecome paramount. Moreover, accuracy alone does not provide sufficient\nassurance or reliable uncertainty estimates for these models. This study\nadvances adversarial training by leveraging principles from Conformal\nPrediction. Specifically, we develop an adversarial attack method, termed OPSA\n(OPtimal Size Attack), designed to reduce the efficiency of conformal\nprediction at any significance level by maximizing model uncertainty without\nrequiring coverage guarantees. Correspondingly, we introduce OPSA-AT\n(Adversarial Training), a defense strategy that integrates OPSA within a novel\nconformal training paradigm. Experimental evaluations demonstrate that our OPSA\nattack method induces greater uncertainty compared to baseline approaches for\nvarious defenses. Conversely, our OPSA-AT defensive model significantly\nenhances robustness not only against OPSA but also other adversarial attacks,\nand maintains reliable prediction. Our findings highlight the effectiveness of\nthis integrated approach for developing trustworthy and resilient deep learning\nmodels for safety-critical domains. Our code is available at\nhttps://github.com/bjbbbb/Enhancing-Adversarial-Robustness-with-Conformal-Prediction."
    },
    {
        "date": "2025-06",
        "title": "RSafe: Incentivizing proactive reasoning to build robust and adaptive LLM safeguards",
        "author": "Jingnan Zheng, Xiangtian Ji, Yijun Lu, Chenhang Cui, Weixiang Zhao, Gelei Deng, Zhenkai Liang, An Zhang, and Tat-Seng Chua",
        "link": "http://arxiv.org/abs/2506.07736v1",
        "abstract": "Large Language Models (LLMs) continue to exhibit vulnerabilities despite\ndeliberate safety alignment efforts, posing significant risks to users and\nsociety. To safeguard against the risk of policy-violating content,\nsystem-level moderation via external guard models-designed to monitor LLM\ninputs and outputs and block potentially harmful content-has emerged as a\nprevalent mitigation strategy. Existing approaches of training guard models\nrely heavily on extensive human curated datasets and struggle with\nout-of-distribution threats, such as emerging harmful categories or jailbreak\nattacks. To address these limitations, we propose RSafe, an adaptive\nreasoning-based safeguard that conducts guided safety reasoning to provide\nrobust protection within the scope of specified safety policies. RSafe operates\nin two stages: 1) guided reasoning, where it analyzes safety risks of input\ncontent through policy-guided step-by-step reasoning, and 2) reinforced\nalignment, where rule-based RL optimizes its reasoning paths to align with\naccurate safety prediction. This two-stage training paradigm enables RSafe to\ninternalize safety principles to generalize safety protection capability over\nunseen or adversarial safety violation scenarios. During inference, RSafe\naccepts user-specified safety policies to provide enhanced safeguards tailored\nto specific safety requirements."
    },
    {
        "date": "2025-06",
        "title": "\"I wasn't sure if this is indeed a security risk\": Data-driven Understanding of Security Issue Reporting in GitHub Repositories of Open Source npm Packages",
        "author": "Rajdeep Ghosh, Shiladitya De, and Mainack Mondal",
        "link": "http://arxiv.org/abs/2506.07728v1",
        "abstract": "The npm (Node Package Manager) ecosystem is the most important package\nmanager for JavaScript development with millions of users. Consequently, a\nplethora of earlier work investigated how vulnerability reporting, patch\npropagation, and in general detection as well as resolution of security issues\nin such ecosystems can be facilitated. However, understanding the ground\nreality of security-related issue reporting by users (and bots) in npm-along\nwith the associated challenges has been relatively less explored at scale.\n  In this work, we bridge this gap by collecting 10,907,467 issues reported\nacross GitHub repositories of 45,466 diverse npm packages. We found that the\ntags associated with these issues indicate the existence of only 0.13%\nsecurity-related issues. However, our approach of manual analysis followed by\ndeveloping high accuracy machine learning models identify 1,617,738\nsecurity-related issues which are not tagged as security-related (14.8% of all\nissues) as well as 4,461,934 comments made on these issues. We found that the\nbots which are in wide use today might not be sufficient for either detecting\nor offering assistance. Furthermore, our analysis of user-developer interaction\ndata hints that many user-reported security issues might not be addressed by\ndevelopers-they are not tagged as security-related issues and might be closed\nwithout valid justification. Consequently, a correlation analysis hints that\nthe developers quickly handle security issues with known solutions (e.g.,\ncorresponding to CVE). However, security issues without such known solutions\n(even with reproducible code) might not be resolved. Our findings offer\nactionable insights for improving security management in open-source\necosystems, highlighting the need for smarter tools and better collaboration.\nThe data and code for this work is available at\nhttps://doi.org/10.5281/zenodo.15614029"
    },
    {
        "date": "2025-06",
        "title": "Evaluating Robustness in Latent Diffusion Models via Embedding Level Augmentation",
        "author": "Boris Martirosyan, and Alexey Karmanov",
        "link": "http://arxiv.org/abs/2506.07706v1",
        "abstract": "Latent diffusion models (LDMs) achieve state-of-the-art performance across\nvarious tasks, including image generation and video synthesis. However, they\ngenerally lack robustness, a limitation that remains not fully explored in\ncurrent research. In this paper, we propose several methods to address this\ngap. First, we hypothesize that the robustness of LDMs primarily should be\nmeasured without their text encoder, because if we take and explore the whole\narchitecture, the problems of image generator and text encoders wll be fused.\nSecond, we introduce novel data augmentation techniques designed to reveal\nrobustness shortcomings in LDMs when processing diverse textual prompts. We\nthen fine-tune Stable Diffusion 3 and Stable Diffusion XL models using\nDreambooth, incorporating these proposed augmentation methods across multiple\ntasks. Finally, we propose a novel evaluation pipeline specifically tailored to\nassess the robustness of LDMs fine-tuned via Dreambooth."
    },
    {
        "date": "2025-06",
        "title": "ProARD: progressive adversarial robustness distillation: provide wide range of robust students",
        "author": "Seyedhamidreza Mousavi, Seyedali Mousavi, and Masoud Daneshtalab",
        "link": "http://arxiv.org/abs/2506.07666v1",
        "abstract": "Adversarial Robustness Distillation (ARD) has emerged as an effective method\nto enhance the robustness of lightweight deep neural networks against\nadversarial attacks. Current ARD approaches have leveraged a large robust\nteacher network to train one robust lightweight student. However, due to the\ndiverse range of edge devices and resource constraints, current approaches\nrequire training a new student network from scratch to meet specific\nconstraints, leading to substantial computational costs and increased CO2\nemissions. This paper proposes Progressive Adversarial Robustness Distillation\n(ProARD), enabling the efficient one-time training of a dynamic network that\nsupports a diverse range of accurate and robust student networks without\nrequiring retraining. We first make a dynamic deep neural network based on\ndynamic layers by encompassing variations in width, depth, and expansion in\neach design stage to support a wide range of architectures. Then, we consider\nthe student network with the largest size as the dynamic teacher network.\nProARD trains this dynamic network using a weight-sharing mechanism to jointly\noptimize the dynamic teacher network and its internal student networks.\nHowever, due to the high computational cost of calculating exact gradients for\nall the students within the dynamic network, a sampling mechanism is required\nto select a subset of students. We show that random student sampling in each\niteration fails to produce accurate and robust students."
    },
    {
        "date": "2025-06",
        "title": "TimberStrike: Dataset Reconstruction Attack Revealing Privacy Leakage in Federated Tree-Based Systems",
        "author": "Marco Di Gennaro, Giovanni De Lucia, Stefano Longari, Stefano Zanero, and Michele Carminati",
        "link": "http://arxiv.org/abs/2506.07605v1",
        "abstract": "Federated Learning has emerged as a privacy-oriented alternative to\ncentralized Machine Learning, enabling collaborative model training without\ndirect data sharing. While extensively studied for neural networks, the\nsecurity and privacy implications of tree-based models remain underexplored.\nThis work introduces TimberStrike, an optimization-based dataset reconstruction\nattack targeting horizontally federated tree-based models. Our attack, carried\nout by a single client, exploits the discrete nature of decision trees by using\nsplit values and decision paths to infer sensitive training data from other\nclients. We evaluate TimberStrike on State-of-the-Art federated gradient\nboosting implementations across multiple frameworks, including Flower, NVFlare,\nand FedTree, demonstrating their vulnerability to privacy breaches. On a\npublicly available stroke prediction dataset, TimberStrike consistently\nreconstructs between 73.05% and 95.63% of the target dataset across all\nimplementations. We further analyze Differential Privacy, showing that while it\npartially mitigates the attack, it also significantly degrades model\nperformance. Our findings highlight the need for privacy-preserving mechanisms\nspecifically designed for tree-based Federated Learning systems, and we provide\npreliminary insights into their design."
    },
    {
        "date": "2025-06",
        "title": "TwinBreak: Jailbreaking LLM Security Alignments based on Twin Prompts",
        "author": "Torsten Krau\u00df, Hamid Dashtbani, and Alexandra Dmitrienko",
        "link": "http://arxiv.org/abs/2506.07596v1",
        "abstract": "Machine learning is advancing rapidly, with applications bringing notable\nbenefits, such as improvements in translation and code generation. Models like\nChatGPT, powered by Large Language Models (LLMs), are increasingly integrated\ninto daily life. However, alongside these benefits, LLMs also introduce social\nrisks. Malicious users can exploit LLMs by submitting harmful prompts, such as\nrequesting instructions for illegal activities. To mitigate this, models often\ninclude a security mechanism that automatically rejects such harmful prompts.\nHowever, they can be bypassed through LLM jailbreaks. Current jailbreaks often\nrequire significant manual effort, high computational costs, or result in\nexcessive model modifications that may degrade regular utility.\n  We introduce TwinBreak, an innovative safety alignment removal method.\nBuilding on the idea that the safety mechanism operates like an embedded\nbackdoor, TwinBreak identifies and prunes parameters responsible for this\nfunctionality. By focusing on the most relevant model layers, TwinBreak\nperforms fine-grained analysis of parameters essential to model utility and\nsafety. TwinBreak is the first method to analyze intermediate outputs from\nprompts with high structural and content similarity to isolate safety\nparameters. We present the TwinPrompt dataset containing 100 such twin prompts.\nExperiments confirm TwinBreak's effectiveness, achieving 89% to 98% success\nrates with minimal computational requirements across 16 LLMs from five vendors."
    },
    {
        "date": "2025-06",
        "title": "HeTa: Relation-wise Heterogeneous Graph Foundation Attack Model",
        "author": "Yuling Wang, Zihui Chen, Pengfei Jiao, and Xiao Wang",
        "link": "http://arxiv.org/abs/2506.07428v1",
        "abstract": "Heterogeneous Graph Neural Networks (HGNNs) are vulnerable, highlighting the\nneed for tailored attacks to assess their robustness and ensure security.\nHowever, existing HGNN attacks often require complex retraining of parameters\nto generate specific perturbations for new scenarios. Recently, foundation\nmodels have opened new horizons for the generalization of graph neural networks\nby capturing shared semantics across various graph distributions. This leads us\nto ask:Can we design a foundation attack model for HGNNs that enables\ngeneralizable perturbations across different HGNNs, and quickly adapts to new\nheterogeneous graphs (HGs)? Empirical findings reveal that, despite significant\ndifferences in model design and parameter space, different HGNNs surprisingly\nshare common vulnerability patterns from a relation-aware perspective.\nTherefore, we explore how to design foundation HGNN attack criteria by mining\nshared attack units. In this paper, we propose a novel relation-wise\nheterogeneous graph foundation attack model, HeTa. We introduce a foundation\nsurrogate model to align heterogeneity and identify the importance of shared\nrelation-aware attack units. Building on this, we implement a serialized\nrelation-by-relation attack based on the identified relational weights. In this\nway, the perturbation can be transferred to various target HGNNs and easily\nfine-tuned for new HGs. Extensive experiments exhibit powerful attack\nperformances and generalizability of our method."
    },
    {
        "date": "2025-06",
        "title": "Pixel-Sensitive and Robust Steganography Based on Polar Codes",
        "author": "Yujun Ji, Jinsheng Li, Ling Liu, Qi Cao, and Tao Dai",
        "link": "http://arxiv.org/abs/2506.07404v1",
        "abstract": "Steganography is an information hiding technique for covert communication.\nThe core issue in steganography design is the rate-distortion coding problem.\nPolar codes, which have been proven to achieve the rate-distortion bound for\nany binary symmetric source, are utilized to design a steganographic scheme\nthat can reach the embedding capacity for the Distortion-Limited Sender problem\nin certain cases. In adaptive steganography, for attack scenarios where each\nnoise element can have different intensities, existing steganographic coding\nmethods fail to resist such attacks. In this paper, we propose a\npixel-sensitive and robust steganographic scheme based on polar codes. Our\nsteganographic scheme not only matches the adaptive distortion well but is also\nrobust against sophisticated noise attacks. Futher, it is proven that our\nscheme achieves the embedding capacity in certain cases. Experimentally, a\nsteganographic scheme can be designed and implemented with a secret message\nerror rate at the $10^{-5}$ level when the attack noise is known to both the\nsender and the receiver. This demonstrates its significant robustness."
    },
    {
        "date": "2025-06",
        "title": "Beyond Jailbreaks: Revealing Stealthier and Broader LLM Security Risks Stemming from Alignment Failures",
        "author": "Yukai Zhou, Sibei Yang, and Wenjie Wang",
        "link": "http://arxiv.org/abs/2506.07402v1",
        "abstract": "Large language models (LLMs) are increasingly deployed in real-world\napplications, raising concerns about their security. While jailbreak attacks\nhighlight failures under overtly harmful queries, they overlook a critical\nrisk: incorrectly answering harmless-looking inputs can be dangerous and cause\nreal-world harm (Implicit Harm). We systematically reformulate the LLM risk\nlandscape through a structured quadrant perspective based on output factuality\nand input harmlessness, uncovering an overlooked high-risk region. To\ninvestigate this gap, we propose JailFlipBench, a benchmark aims to capture\nimplicit harm, spanning single-modal, multimodal, and factual extension\nscenarios with diverse evaluation metrics. We further develop initial JailFlip\nattack methodologies and conduct comprehensive evaluations across multiple\nopen-source and black-box LLMs, show that implicit harm present immediate and\nurgent real-world risks, calling for broader LLM safety assessments and\nalignment beyond conventional jailbreak paradigms."
    },
    {
        "date": "2025-06",
        "title": "MrM: Black-Box Membership Inference Attacks against Multimodal RAG Systems",
        "author": "Peiru Yang, Jinhua Yin, Haoran Zheng, Xueying Bai, Huili Wang, Yufei Sun, Xintian Li, Shangguang Wang, Yongfeng Huang, and Tao Qi",
        "link": "http://arxiv.org/abs/2506.07399v1",
        "abstract": "Multimodal retrieval-augmented generation (RAG) systems enhance large\nvision-language models by integrating cross-modal knowledge, enabling their\nincreasing adoption across real-world multimodal tasks. These knowledge\ndatabases may contain sensitive information that requires privacy protection.\nHowever, multimodal RAG systems inherently grant external users indirect access\nto such data, making them potentially vulnerable to privacy attacks,\nparticularly membership inference attacks (MIAs). % Existing MIA methods\ntargeting RAG systems predominantly focus on the textual modality, while the\nvisual modality remains relatively underexplored. To bridge this gap, we\npropose MrM, the first black-box MIA framework targeted at multimodal RAG\nsystems. It utilizes a multi-object data perturbation framework constrained by\ncounterfactual attacks, which can concurrently induce the RAG systems to\nretrieve the target data and generate information that leaks the membership\ninformation. Our method first employs an object-aware data perturbation method\nto constrain the perturbation to key semantics and ensure successful retrieval.\nBuilding on this, we design a counterfact-informed mask selection strategy to\nprioritize the most informative masked regions, aiming to eliminate the\ninterference of model self-knowledge and amplify attack efficacy. Finally, we\nperform statistical membership inference by modeling query trials to extract\nfeatures that reflect the reconstruction of masked semantics from response\npatterns. Experiments on two visual datasets and eight mainstream commercial\nvisual-language models (e.g., GPT-4o, Gemini-2) demonstrate that MrM achieves\nconsistently strong performance across both sample-level and set-level\nevaluations, and remains robust under adaptive defenses."
    },
    {
        "date": "2025-06",
        "title": "From Static to Adaptive Defense: Federated Multi-Agent Deep Reinforcement Learning-Driven Moving Target Defense Against DoS Attacks in UAV Swarm Networks",
        "author": "Yuyang Zhou, Guang Cheng, Kang Du, Zihan Chen, Tian Qin, and Yuyu Zhao",
        "link": "http://arxiv.org/abs/2506.07392v1",
        "abstract": "The proliferation of unmanned aerial vehicle (UAV) swarms has enabled a wide\nrange of mission-critical applications, but also exposes UAV networks to severe\nDenial-of-Service (DoS) threats due to their open wireless environment, dynamic\ntopology, and resource constraints. Traditional static or centralized defense\nmechanisms are often inadequate for such dynamic and distributed scenarios. To\naddress these challenges, we propose a novel federated multi-agent deep\nreinforcement learning (FMADRL)-driven moving target defense (MTD) framework\nfor proactive and adaptive DoS mitigation in UAV swarm networks. Specifically,\nwe design three lightweight and coordinated MTD mechanisms, including leader\nswitching, route mutation, and frequency hopping, that leverage the inherent\nflexibility of UAV swarms to disrupt attacker efforts and enhance network\nresilience. The defense problem is formulated as a multi-agent partially\nobservable Markov decision process (POMDP), capturing the distributed,\nresource-constrained, and uncertain nature of UAV swarms under attack. Each UAV\nis equipped with a local policy agent that autonomously selects MTD actions\nbased on partial observations and local experiences. By employing a policy\ngradient-based FMADRL algorithm, UAVs collaboratively optimize their defense\npolicies via reward-weighted aggregation, enabling distributed learning without\nsharing raw data and thus reducing communication overhead. Extensive\nsimulations demonstrate that our approach significantly outperforms\nstate-of-the-art baselines, achieving up to a 34.6% improvement in attack\nmitigation rate, a reduction in average recovery time of up to 94.6%, and\ndecreases in energy consumption and defense cost by as much as 29.3% and 98.3%,\nrespectively, while maintaining robust mission continuity under various DoS\nattack strategies."
    },
    {
        "date": "2025-06",
        "title": "JavelinGuard: Low-Cost Transformer Architectures for LLM Security",
        "author": "Yash Datta, and Sharath Rajasekar",
        "link": "http://arxiv.org/abs/2506.07330v1",
        "abstract": "We present JavelinGuard, a suite of low-cost, high-performance model\narchitectures designed for detecting malicious intent in Large Language Model\n(LLM) interactions, optimized specifically for production deployment. Recent\nadvances in transformer architectures, including compact BERT(Devlin et al.\n2019) variants (e.g., ModernBERT (Warner et al. 2024)), allow us to build\nhighly accurate classifiers with as few as approximately 400M parameters that\nachieve rapid inference speeds even on standard CPU hardware. We systematically\nexplore five progressively sophisticated transformer-based architectures:\nSharanga (baseline transformer classifier), Mahendra (enhanced\nattention-weighted pooling with deeper heads), Vaishnava and Ashwina (hybrid\nneural ensemble architectures), and Raudra (an advanced multi-task framework\nwith specialized loss functions). Our models are rigorously benchmarked across\nnine diverse adversarial datasets, including popular sets like the NotInject\nseries, BIPIA, Garak, ImprovedLLM, ToxicChat, WildGuard, and our newly\nintroduced JavelinBench, specifically crafted to test generalization on\nchallenging borderline and hard-negative cases. Additionally, we compare our\narchitectures against leading open-source guardrail models as well as large\ndecoder-only LLMs such as gpt-4o, demonstrating superior cost-performance\ntrade-offs in terms of accuracy, and latency. Our findings reveal that while\nRaudra's multi-task design offers the most robust performance overall, each\narchitecture presents unique trade-offs in speed, interpretability, and\nresource requirements, guiding practitioners in selecting the optimal balance\nof complexity and efficiency for real-world LLM security applications."
    },
    {
        "date": "2025-06",
        "title": "SCGAgent: Recreating the Benefits of Reasoning Models for Secure Code Generation with Agentic Workflows",
        "author": "Rebecca Saul, Hao Wang, Koushik Sen, and David Wagner",
        "link": "http://arxiv.org/abs/2506.07313v1",
        "abstract": "Large language models (LLMs) have seen widespread success in code generation\ntasks for different scenarios, both everyday and professional. However current\nLLMs, despite producing functional code, do not prioritize security and may\ngenerate code with exploitable vulnerabilities. In this work, we propose\ntechniques for generating code that is more likely to be secure and introduce\nSCGAgent, a proactive secure coding agent that implements our techniques. We\nuse security coding guidelines that articulate safe programming practices,\ncombined with LLM-generated unit tests to preserve functional correctness. In\nour evaluation, we find that SCGAgent is able to preserve nearly 98% of the\nfunctionality of the base Sonnet-3.7 LLM while achieving an approximately 25%\nimprovement in security. Moreover, SCGAgent is able to match or best the\nperformance of sophisticated reasoning LLMs using a non-reasoning model and an\nagentic workflow."
    },
    {
        "date": "2025-06",
        "title": "Uncertainty-Aware Strategies: A Model-Agnostic Framework for Robust Financial Optimization through Subsampling",
        "author": "Hans Buehler, Blanka Horvath, Yannick Limmer, and Thorsten Schmidt",
        "link": "http://arxiv.org/abs/2506.07299v1",
        "abstract": "This paper addresses the challenge of model uncertainty in quantitative\nfinance, where decisions in portfolio allocation, derivative pricing, and risk\nmanagement rely on estimating stochastic models from limited data. In practice,\nthe unavailability of the true probability measure forces reliance on an\nempirical approximation, and even small misestimations can lead to significant\ndeviations in decision quality. Building on the framework of Klibanoff et al.\n(2005), we enhance the conventional objective - whether this is expected\nutility in an investing context or a hedging metric - by superimposing an outer\n\"uncertainty measure\", motivated by traditional monetary risk measures, on the\nspace of models. In scenarios where a natural model distribution is lacking or\nBayesian methods are impractical, we propose an ad hoc subsampling strategy,\nanalogous to bootstrapping in statistical finance and related to mini-batch\nsampling in deep learning, to approximate model uncertainty. To address the\nquadratic memory demands of naive implementations, we also present an adapted\nstochastic gradient descent algorithm that enables efficient parallelization.\nThrough analytical, simulated, and empirical studies - including multi-period,\nreal data and high-dimensional examples - we demonstrate that uncertainty\nmeasures outperform traditional mixture of measures strategies and our\nmodel-agnostic subsampling-based approach not only enhances robustness against\nmodel risk but also achieves performance comparable to more elaborate Bayesian\nmethods."
    },
    {
        "date": "2025-06",
        "title": "Exploiting Inaccurate Branch History in Side-Channel Attacks",
        "author": "Yuhui Zhu, and Alessandro Biondi",
        "link": "http://arxiv.org/abs/2506.07263v1",
        "abstract": "Modern out-of-order CPUs heavily rely on speculative execution for\nperformance optimization, with branch prediction serving as a cornerstone to\nminimize stalls and maximize efficiency. Whenever shared branch prediction\nresources lack proper isolation and sanitization methods, they may originate\nsecurity vulnerabilities that expose sensitive data across different software\ncontexts.\n  This paper examines the fundamental components of modern Branch Prediction\nUnits (BPUs) and investigates how resource sharing and contention affect two\nwidely implemented but underdocumented features: Bias-Free Branch Prediction\nand Branch History Speculation. Our analysis demonstrates that these BPU\nfeatures, while designed to enhance speculative execution efficiency through\nmore accurate branch histories, can also introduce significant security risks.\nWe show that these features can inadvertently modify the Branch History Buffer\n(BHB) update behavior and create new primitives that trigger malicious\nmis-speculations.\n  This discovery exposes previously unknown cross-privilege attack surfaces for\nBranch History Injection (BHI). Based on these findings, we present three novel\nattack primitives: two Spectre attacks, namely Spectre-BSE and Spectre-BHS, and\na cross-privilege control flow side-channel attack called BiasScope. Our\nresearch identifies corresponding patterns of vulnerable control flows and\ndemonstrates exploitation on multiple processors. Finally, Chimera is\npresented: an attack demonstrator based on eBPF for a variant of Spectre-BHS\nthat is capable of leaking kernel memory contents at 24,628 bit/s."
    },
    {
        "date": "2025-06",
        "title": "Promoting Ensemble Diversity with Interactive Bayesian Distributional Robustness for Fine-tuning Foundation Models",
        "author": "Ngoc-Quan Pham, Tuan Truong, Quyen Tran, Tan Nguyen, Dinh Phung, and Trung Le",
        "link": "http://arxiv.org/abs/2506.07247v1",
        "abstract": "We introduce Interactive Bayesian Distributional Robustness (IBDR), a novel\nBayesian inference framework that allows modeling the interactions between\nparticles, thereby enhancing ensemble quality through increased particle\ndiversity. IBDR is grounded in a generalized theoretical framework that\nconnects the distributional population loss with the approximate posterior,\nmotivating a practical dual optimization procedure that enforces distributional\nrobustness while fostering particle diversity. We evaluate IBDR's performance\nagainst various baseline methods using the VTAB-1K benchmark and the common\nreasoning language task. The results consistently show that IBDR outperforms\nthese baselines, underscoring its effectiveness in real-world applications."
    },
    {
        "date": "2025-06",
        "title": "Backdoor Attack on Vision Language Models with Stealthy Semantic Manipulation",
        "author": "Zhiyuan Zhong, Zhen Sun, Yepang Liu, Xinlei He, and Guanhong Tao",
        "link": "http://arxiv.org/abs/2506.07214v1",
        "abstract": "Vision Language Models (VLMs) have shown remarkable performance, but are also\nvulnerable to backdoor attacks whereby the adversary can manipulate the model's\noutputs through hidden triggers. Prior attacks primarily rely on\nsingle-modality triggers, leaving the crucial cross-modal fusion nature of VLMs\nlargely unexplored. Unlike prior work, we identify a novel attack surface that\nleverages cross-modal semantic mismatches as implicit triggers. Based on this\ninsight, we propose BadSem (Backdoor Attack with Semantic Manipulation), a data\npoisoning attack that injects stealthy backdoors by deliberately misaligning\nimage-text pairs during training. To perform the attack, we construct SIMBad, a\ndataset tailored for semantic manipulation involving color and object\nattributes. Extensive experiments across four widely used VLMs show that BadSem\nachieves over 98% average ASR, generalizes well to out-of-distribution\ndatasets, and can transfer across poisoning modalities. Our detailed analysis\nusing attention visualization shows that backdoored models focus on\nsemantically sensitive regions under mismatched conditions while maintaining\nnormal behavior on clean inputs. To mitigate the attack, we try two defense\nstrategies based on system prompt and supervised fine-tuning but find that both\nof them fail to mitigate the semantic backdoor. Our findings highlight the\nurgent need to address semantic vulnerabilities in VLMs for their safer\ndeployment."
    },
    {
        "date": "2025-06",
        "title": "Mind the Web: The Security of Web Use Agents",
        "author": "Avishag Shapira, Parth Atulbhai Gandhi, Edan Habler, Oleg Brodt, and Asaf Shabtai",
        "link": "http://arxiv.org/abs/2506.07153v1",
        "abstract": "Web-use agents are rapidly being deployed to automate complex web tasks,\noperating with extensive browser capabilities including multi-tab navigation,\nDOM manipulation, JavaScript execution and authenticated session access.\nHowever, these powerful capabilities create a critical and previously\nunexplored attack surface. This paper demonstrates how attackers can exploit\nweb-use agents' high-privilege capabilities by embedding malicious content in\nweb pages such as comments, reviews, or advertisements that agents encounter\nduring legitimate browsing tasks. In addition, we introduce the task-aligned\ninjection technique that frame malicious commands as helpful task guidance\nrather than obvious attacks. This technique exploiting fundamental limitations\nin LLMs' contextual reasoning: agents struggle in maintaining coherent\ncontextual awareness and fail to detect when seemingly helpful web content\ncontains steering attempts that deviate from their original task goal. Through\nsystematic evaluation of four popular agents (OpenAI Operator, Browser Use, Do\nBrowser, OpenOperator), we demonstrate nine payload types that compromise\nconfidentiality, integrity, and availability, including unauthorized camera\nactivation, user impersonation, local file exfiltration, password leakage, and\ndenial of service, with validation across multiple LLMs achieving success rates\nof 80%-100%. These payloads succeed across agents with built-in safety\nmechanisms, requiring only the ability to post content on public websites,\ncreating unprecedented risks given the ease of exploitation combined with\nagents' high-privilege access. To address this attack, we propose comprehensive\nmitigation strategies including oversight mechanisms, execution constraints,\nand task-aware reasoning techniques, providing practical directions for secure\ndevelopment and deployment."
    },
    {
        "date": "2025-06",
        "title": "Quality-Diversity Red-Teaming: Automated Generation of High-Quality and Diverse Attackers for Large Language Models",
        "author": "Ren-Jian Wang, Ke Xue, Zeyu Qin, Ziniu Li, Sheng Tang, Hao-Tian Li, Shengcai Liu, and Chao Qian",
        "link": "http://arxiv.org/abs/2506.07121v1",
        "abstract": "Ensuring safety of large language models (LLMs) is important. Red teaming--a\nsystematic approach to identifying adversarial prompts that elicit harmful\nresponses from target LLMs--has emerged as a crucial safety evaluation method.\nWithin this framework, the diversity of adversarial prompts is essential for\ncomprehensive safety assessments. We find that previous approaches to\nred-teaming may suffer from two key limitations. First, they often pursue\ndiversity through simplistic metrics like word frequency or sentence embedding\nsimilarity, which may not capture meaningful variation in attack strategies.\nSecond, the common practice of training a single attacker model restricts\ncoverage across potential attack styles and risk categories. This paper\nintroduces Quality-Diversity Red-Teaming (QDRT), a new framework designed to\naddress these limitations. QDRT achieves goal-driven diversity through\nbehavior-conditioned training and implements a behavioral replay buffer in an\nopen-ended manner. Additionally, it trains multiple specialized attackers\ncapable of generating high-quality attacks across diverse styles and risk\ncategories. Our empirical evaluation demonstrates that QDRT generates attacks\nthat are both more diverse and more effective against a wide range of target\nLLMs, including GPT-2, Llama-3, Gemma-2, and Qwen2.5. This work advances the\nfield of LLM safety by providing a systematic and effective approach to\nautomated red-teaming, ultimately supporting the responsible deployment of\nLLMs."
    },
    {
        "date": "2025-06",
        "title": "RBA-FE: A Robust Brain-Inspired Audio Feature Extractor for Depression Diagnosis",
        "author": "Yu-Xuan Wu, Ziyan Huang, Bin Hu, and Zhi-Hong Guan",
        "link": "http://arxiv.org/abs/2506.07118v1",
        "abstract": "This article proposes a robust brain-inspired audio feature extractor\n(RBA-FE) model for depression diagnosis, using an improved hierarchical network\narchitecture. Most deep learning models achieve state-of-the-art performance\nfor image-based diagnostic tasks, ignoring the counterpart audio features. In\norder to tailor the noise challenge, RBA-FE leverages six acoustic features\nextracted from the raw audio, capturing both spatial characteristics and\ntemporal dependencies. This hybrid attribute helps alleviate the precision\nlimitation in audio feature extraction within other learning models like deep\nresidual shrinkage networks. To deal with the noise issues, our model\nincorporates an improved spiking neuron model, called adaptive rate smooth\nleaky integrate-and-fire (ARSLIF). The ARSLIF model emulates the mechanism of\n``retuning of cellular signal selectivity\" in the brain attention systems,\nwhich enhances the model robustness against environmental noises in audio data.\nExperimental results demonstrate that RBA-FE achieves state-of-the-art accuracy\non the MODMA dataset, respectively with 0.8750, 0.8974, 0.8750 and 0.8750 in\nprecision, accuracy, recall and F1 score. Extensive experiments on the AVEC2014\nand DAIC-WOZ datasets both show enhancements in noise robustness. It is further\nindicated by comparison that the ARSLIF neuron model suggest the abnormal\nfiring pattern within the feature extraction on depressive audio data, offering\nbrain-inspired interpretability."
    },
    {
        "date": "2025-06",
        "title": "State Entropy Regularization for Robust Reinforcement Learning",
        "author": "Uri Koren, Yonatan Ashlag, Mirco Mutti, Esther Derman, Pierre-Luc Bacon, and Shie Mannor",
        "link": "http://arxiv.org/abs/2506.07085v1",
        "abstract": "State entropy regularization has empirically shown better exploration and\nsample complexity in reinforcement learning (RL). However, its theoretical\nguarantees have not been studied. In this paper, we show that state entropy\nregularization improves robustness to structured and spatially correlated\nperturbations. These types of variation are common in transfer learning but\noften overlooked by standard robust RL methods, which typically focus on small,\nuncorrelated changes. We provide a comprehensive characterization of these\nrobustness properties, including formal guarantees under reward and transition\nuncertainty, as well as settings where the method performs poorly. Much of our\nanalysis contrasts state entropy with the widely used policy entropy\nregularization, highlighting their different benefits. Finally, from a\npractical standpoint, we illustrate that compared with policy entropy, the\nrobustness advantages of state entropy are more sensitive to the number of\nrollouts used for policy evaluation."
    },
    {
        "date": "2025-06",
        "title": "D2R: dual regularization loss with collaborative adversarial generation for model robustness",
        "author": "Zhenyu Liu, Huizhi Liang, Rajiv Ranjan, Zhanxing Zhu, Vaclav Snasel, and Varun Ojha",
        "link": "http://arxiv.org/abs/2506.07056v1",
        "abstract": "The robustness of Deep Neural Network models is crucial for defending models\nagainst adversarial attacks. Recent defense methods have employed collaborative\nlearning frameworks to enhance model robustness. Two key limitations of\nexisting methods are (i) insufficient guidance of the target model via loss\nfunctions and (ii) non-collaborative adversarial generation. We, therefore,\npropose a dual regularization loss (D2R Loss) method and a collaborative\nadversarial generation (CAG) strategy for adversarial training. D2R loss\nincludes two optimization steps. The adversarial distribution and clean\ndistribution optimizations enhance the target model's robustness by leveraging\nthe strengths of different loss functions obtained via a suitable function\nspace exploration to focus more precisely on the target model's distribution.\nCAG generates adversarial samples using a gradient-based collaboration between\nguidance and target models. We conducted extensive experiments on three\nbenchmark databases, including CIFAR-10, CIFAR-100, Tiny ImageNet, and two\npopular target models, WideResNet34-10 and PreActResNet18. Our results show\nthat D2R loss with CAG produces highly robust models."
    },
    {
        "date": "2025-06",
        "title": "Efficient $Q$-Learning and Actor-Critic Methods for Robust Average Reward Reinforcement Learning",
        "author": "Yang Xu, Swetha Ganesh, and Vaneet Aggarwal",
        "link": "http://arxiv.org/abs/2506.07040v1",
        "abstract": "We present the first $Q$-learning and actor-critic algorithms for robust\naverage reward Markov Decision Processes (MDPs) with non-asymptotic convergence\nunder contamination, TV distance and Wasserstein distance uncertainty sets. We\nshow that the robust $Q$ Bellman operator is a strict contractive mapping with\nrespect to a carefully constructed semi-norm with constant functions being\nquotiented out. This property supports a stochastic approximation update, that\nlearns the optimal robust $Q$ function in $\\tilde{\\cO}(\\epsilon^{-2})$ samples.\nWe also show that the same idea can be used for robust $Q$ function estimation,\nwhich can be further used for critic estimation. Coupling it with theories in\nrobust policy mirror descent update, we present a natural actor-critic\nalgorithm that attains an $\\epsilon$-optimal robust policy in\n$\\tilde{\\cO}(\\epsilon^{-3})$ samples. These results advance the theory of\ndistributionally robust reinforcement learning in the average reward setting."
    },
    {
        "date": "2025-06",
        "title": "NanoZone: Scalable, Efficient, and Secure Memory Protection for Arm CCA",
        "author": "Shiqi Liu, Yongpeng Gao, Mingyang Zhang, and Jie Wang",
        "link": "http://arxiv.org/abs/2506.07034v1",
        "abstract": "Arm Confidential Computing Architecture (CCA) currently isolates at the\ngranularity of an entire Confidential Virtual Machine (CVM), leaving intra-VM\nbugs such as Heartbleed unmitigated. The state-of-the-art narrows this to the\nprocess level, yet still cannot stop attacks that pivot within the same\nprocess, and prior intra-enclave schemes are either too slow or incompatible\nwith CVM-style isolation. We extend CCA with a three-tier zone model that\nspawns an unlimited number of lightweight isolation domains inside a single\nprocess, while shielding them from kernel-space adversaries. To block\ndomain-switch abuse, we also add a fast user-level Code-Pointer Integrity (CPI)\nmechanism. We developed two prototypes: a functional version on Arm's official\nsimulator to validate resistance against intra-process and kernel-space\nadversaries, and a performance variant on Arm development boards evaluated for\nsession-key isolation within server applications, in-memory key-value\nprotection, and non-volatile-memory data isolation. NanoZone incurs roughly a\n20% performance overhead while retaining 95% throughput compared to the system\nwithout fine-grained isolation."
    },
    {
        "date": "2025-06",
        "title": "HauntAttack: When Attack Follows Reasoning as a Shadow",
        "author": "Jingyuan Ma, Rui Li, Zheng Li, Junfeng Liu, Lei Sha, and Zhifang Sui",
        "link": "http://arxiv.org/abs/2506.07031v1",
        "abstract": "Emerging Large Reasoning Models (LRMs) consistently excel in mathematical and\nreasoning tasks, showcasing exceptional capabilities. However, the enhancement\nof reasoning abilities and the exposure of their internal reasoning processes\nintroduce new safety vulnerabilities. One intriguing concern is: when reasoning\nis strongly entangled with harmfulness, what safety-reasoning trade-off do LRMs\nexhibit? To address this issue, we introduce HauntAttack, a novel and\ngeneral-purpose black-box attack framework that systematically embeds harmful\ninstructions into reasoning questions. Specifically, we treat reasoning\nquestions as carriers and substitute one of their original conditions with a\nharmful instruction. This process creates a reasoning pathway in which the\nmodel is guided step by step toward generating unsafe outputs. Based on\nHauntAttack, we conduct comprehensive experiments on multiple LRMs. Our results\nreveal that even the most advanced LRMs exhibit significant safety\nvulnerabilities. Additionally, we perform a detailed analysis of different\nmodels, various types of harmful instructions, and model output patterns,\nproviding valuable insights into the security of LRMs."
    },
    {
        "date": "2025-06",
        "title": "Half-AVAE: Adversarial-Enhanced Factorized and Structured Encoder-Free VAE for Underdetermined Independent Component Analysis",
        "author": "Yuan-Hao Wei, and Yan-Jie Sun",
        "link": "http://arxiv.org/abs/2506.07011v1",
        "abstract": "This study advances the Variational Autoencoder (VAE) framework by addressing\nchallenges in Independent Component Analysis (ICA) under both determined and\nunderdetermined conditions, focusing on enhancing the independence and\ninterpretability of latent variables. Traditional VAEs map observed data to\nlatent variables and back via an encoder-decoder architecture, but struggle\nwith underdetermined ICA where the number of latent variables exceeds observed\nsignals. The proposed Half Adversarial VAE (Half-AVAE) builds on the\nencoder-free Half-VAE framework, eliminating explicit inverse mapping to tackle\nunderdetermined scenarios. By integrating adversarial networks and External\nEnhancement (EE) terms, Half-AVAE promotes mutual independence among latent\ndimensions, achieving factorized and interpretable representations. Experiments\nwith synthetic signals demonstrate that Half-AVAE outperforms baseline models,\nincluding GP-AVAE and Half-VAE, in recovering independent components under\nunderdetermined conditions, as evidenced by lower root mean square errors. The\nstudy highlights the flexibility of VAEs in variational inference, showing that\nencoder omission, combined with adversarial training and structured priors,\nenables effective solutions for complex ICA tasks, advancing applications in\ndisentanglement, causal inference, and generative modeling."
    },
    {
        "date": "2025-06",
        "title": "ModelForge: Using GenAI to Improve the Development of Security Protocols",
        "author": "Martin Duclos, Ivan A. Fernandez, Kaneesha Moore, Sudip Mittal, and Edward Zieglar",
        "link": "http://arxiv.org/abs/2506.07010v1",
        "abstract": "Formal methods can be used for verifying security protocols, but their\nadoption can be hindered by the complexity of translating natural language\nprotocol specifications into formal representations. In this paper, we\nintroduce ModelForge, a novel tool that automates the translation of protocol\nspecifications for the Cryptographic Protocol Shapes Analyzer (CPSA). By\nleveraging advances in Natural Language Processing (NLP) and Generative AI\n(GenAI), ModelForge processes protocol specifications and generates a CPSA\nprotocol definition. This approach reduces the manual effort required, making\nformal analysis more accessible. We evaluate ModelForge by fine-tuning a large\nlanguage model (LLM) to generate protocol definitions for CPSA, comparing its\nperformance with other popular LLMs. The results from our evaluation show that\nModelForge consistently produces quality outputs, excelling in syntactic\naccuracy, though some refinement is needed to handle certain protocol details.\nThe contributions of this work include the architecture and proof of concept\nfor a translating tool designed to simplify the adoption of formal methods in\nthe development of security protocols."
    },
    {
        "date": "2025-06",
        "title": "Boosting Adversarial Transferability via Commonality-Oriented Gradient Optimization",
        "author": "Yanting Gao, Yepeng Liu, Junming Liu, Qi Zhang, Hongyun Zhang, Duoqian Miao, and Cairong Zhao",
        "link": "http://arxiv.org/abs/2506.06992v1",
        "abstract": "Exploring effective and transferable adversarial examples is vital for\nunderstanding the characteristics and mechanisms of Vision Transformers (ViTs).\nHowever, adversarial examples generated from surrogate models often exhibit\nweak transferability in black-box settings due to overfitting. Existing methods\nimprove transferability by diversifying perturbation inputs or applying uniform\ngradient regularization within surrogate models, yet they have not fully\nleveraged the shared and unique features of surrogate models trained on the\nsame task, leading to suboptimal transfer performance. Therefore, enhancing\nperturbations of common information shared by surrogate models and suppressing\nthose tied to individual characteristics offers an effective way to improve\ntransferability. Accordingly, we propose a commonality-oriented gradient\noptimization strategy (COGO) consisting of two components: Commonality\nEnhancement (CE) and Individuality Suppression (IS). CE perturbs the mid-to-low\nfrequency regions, leveraging the fact that ViTs trained on the same dataset\ntend to rely more on mid-to-low frequency information for classification. IS\nemploys adaptive thresholds to evaluate the correlation between backpropagated\ngradients and model individuality, assigning weights to gradients accordingly.\nExtensive experiments demonstrate that COGO significantly improves the transfer\nsuccess rates of adversarial attacks, outperforming current state-of-the-art\nmethods."
    },
    {
        "date": "2025-06",
        "title": "Break-The-Chain: Reasoning Failures in LLMs via Adversarial Prompting in Code Generation",
        "author": "Jaechul Roh, Varun Gandhi, Shivani Anilkumar, and Arin Garg",
        "link": "http://arxiv.org/abs/2506.06971v1",
        "abstract": "Large Language Models (LLMs) have achieved remarkable success in tasks\nrequiring complex reasoning, such as code generation, mathematical problem\nsolving, and algorithmic synthesis -- especially when aided by reasoning tokens\nand Chain-of-Thought prompting. Yet, a core question remains: do these models\ntruly reason, or do they merely exploit shallow statistical patterns? In this\npaper, we systematically investigate the robustness of reasoning LLMs by\nintroducing a suite of semantically faithful yet adversarially structured\nprompt perturbations. Our evaluation -- spanning 700 perturbed code generations\nderived from LeetCode-style problems -- applies transformations such as\nstorytelling reframing, irrelevant constraint injection, example reordering,\nand numeric perturbation. We observe that while certain modifications severely\ndegrade performance (with accuracy drops up to -42.1%), others surprisingly\nimprove model accuracy by up to 35.3%, suggesting sensitivity not only to\nsemantics but also to surface-level prompt dynamics. These findings expose the\nfragility and unpredictability of current reasoning systems, underscoring the\nneed for more principles approaches to reasoning alignments and prompting\nrobustness. We release our perturbation datasets and evaluation framework to\npromote further research in trustworthy and resilient LLM reasoning."
    },
    {
        "date": "2025-06",
        "title": "Rewriting the Budget: A General Framework for Black-Box Attacks Under Cost Asymmetry",
        "author": "Mahdi Salmani, Alireza Abdollahpoorrostam, and Seyed-Mohsen Moosavi-Dezfooli",
        "link": "http://arxiv.org/abs/2506.06933v1",
        "abstract": "Traditional decision-based black-box adversarial attacks on image classifiers\naim to generate adversarial examples by slightly modifying input images while\nkeeping the number of queries low, where each query involves sending an input\nto the model and observing its output. Most existing methods assume that all\nqueries have equal cost. However, in practice, queries may incur asymmetric\ncosts; for example, in content moderation systems, certain output classes may\ntrigger additional review, enforcement, or penalties, making them more costly\nthan others. While prior work has considered such asymmetric cost settings,\neffective algorithms for this scenario remain underdeveloped. In this paper, we\npropose a general framework for decision-based attacks under asymmetric query\ncosts, which we refer to as asymmetric black-box attacks. We modify two core\ncomponents of existing attacks: the search strategy and the gradient estimation\nprocess. Specifically, we propose Asymmetric Search (AS), a more conservative\nvariant of binary search that reduces reliance on high-cost queries, and\nAsymmetric Gradient Estimation (AGREST), which shifts the sampling distribution\nto favor low-cost queries. We design efficient algorithms that minimize total\nattack cost by balancing different query types, in contrast to earlier methods\nsuch as stealthy attacks that focus only on limiting expensive (high-cost)\nqueries. Our method can be integrated into a range of existing black-box\nattacks with minimal changes. We perform both theoretical analysis and\nempirical evaluation on standard image classification benchmarks. Across\nvarious cost regimes, our method consistently achieves lower total query cost\nand smaller perturbations than existing approaches, with improvements of up to\n40% in some settings."
    },
    {
        "date": "2025-06",
        "title": "KNN-Defense: Defense against 3D Adversarial Point Clouds using Nearest-Neighbor Search",
        "author": "Nima Jamali, Matina Mahdizadeh Sani, Hanieh Naderi, and Shohreh Kasaei",
        "link": "http://arxiv.org/abs/2506.06906v1",
        "abstract": "Deep neural networks (DNNs) have demonstrated remarkable performance in\nanalyzing 3D point cloud data. However, their vulnerability to adversarial\nattacks-such as point dropping, shifting, and adding-poses a critical challenge\nto the reliability of 3D vision systems. These attacks can compromise the\nsemantic and structural integrity of point clouds, rendering many existing\ndefense mechanisms ineffective. To address this issue, a defense strategy named\nKNN-Defense is proposed, grounded in the manifold assumption and\nnearest-neighbor search in feature space. Instead of reconstructing surface\ngeometry or enforcing uniform point distributions, the method restores\nperturbed inputs by leveraging the semantic similarity of neighboring samples\nfrom the training set. KNN-Defense is lightweight and computationally\nefficient, enabling fast inference and making it suitable for real-time and\npractical applications. Empirical results on the ModelNet40 dataset\ndemonstrated that KNN-Defense significantly improves robustness across various\nattack types. In particular, under point-dropping attacks-where many existing\nmethods underperform due to the targeted removal of critical points-the\nproposed method achieves accuracy gains of 20.1%, 3.6%, 3.44%, and 7.74% on\nPointNet, PointNet++, DGCNN, and PCT, respectively. These findings suggest that\nKNN-Defense offers a scalable and effective solution for enhancing the\nadversarial resilience of 3D point cloud classifiers. (An open-source\nimplementation of the method, including code and data, is available at\nhttps://github.com/nimajam41/3d-knn-defense)."
    },
    {
        "date": "2025-06",
        "title": "Can In-Context Reinforcement Learning Recover From Reward Poisoning Attacks?",
        "author": "Paulius Sasnauskas, Yi\u011fit Yal\u0131n, and Goran Radanovi\u0107",
        "link": "http://arxiv.org/abs/2506.06891v1",
        "abstract": "We study the corruption-robustness of in-context reinforcement learning\n(ICRL), focusing on the Decision-Pretrained Transformer (DPT, Lee et al.,\n2023). To address the challenge of reward poisoning attacks targeting the DPT,\nwe propose a novel adversarial training framework, called Adversarially Trained\nDecision-Pretrained Transformer (AT-DPT). Our method simultaneously trains an\nattacker to minimize the true reward of the DPT by poisoning environment\nrewards, and a DPT model to infer optimal actions from the poisoned data. We\nevaluate the effectiveness of our approach against standard bandit algorithms,\nincluding robust baselines designed to handle reward contamination. Our results\nshow that the proposed method significantly outperforms these baselines in\nbandit settings, under a learned attacker. We additionally evaluate AT-DPT on\nan adaptive attacker, and observe similar results. Furthermore, we extend our\nevaluation to the MDP setting, confirming that the robustness observed in\nbandit scenarios generalizes to more complex environments."
    },
    {
        "date": "2025-06",
        "title": "FREE: Fast and Robust Vision Language Models with Early Exits",
        "author": "Divya Jyoti Bajpai, and Manjesh Kumar Hanawal",
        "link": "http://arxiv.org/abs/2506.06884v1",
        "abstract": "In recent years, Vision-Language Models (VLMs) have shown remarkable\nperformance improvements in Vision-Language tasks. However, their large size\nposes challenges for real-world applications where inference latency is a\nconcern. To tackle this issue, we propose employing Early Exit (EE) strategies\nin VLMs. However, training exit classifiers in VLMs is challenging,\nparticularly with limited labeled training data. To address this, we introduce\nFREE, an adversarial training approach within a GAN-based framework. Here, each\nexit consists of a transformer layer and a classifier. The transformer layer is\nadversarially trained to produce feature representations similar to the final\nlayer, while a feature classifier serves as the discriminator. Our method\nfocuses on performing input-adaptive inference that increases inference speed\nwith minimal drop in performance. Experimental results demonstrate the\neffectiveness of our approach in enhancing accuracy and model robustness by\nmitigating overthinking and the phenomenon of mid-crisis that we highlight. We\nexperimentally validate that our method speeds up the inference process by more\nthan 1.51x while retaining comparable performance. The source code is available\nat https://github.com/Div290/FREE."
    },
    {
        "date": "2025-06",
        "title": "Exploring Visual Prompting: Robustness Inheritance and Beyond",
        "author": "Qi Li, Liangzhi Li, Zhouqiang Jiang, Bowen Wang, and Keke Tang",
        "link": "http://arxiv.org/abs/2506.06823v1",
        "abstract": "Visual Prompting (VP), an efficient method for transfer learning, has shown\nits potential in vision tasks. However, previous works focus exclusively on VP\nfrom standard source models, it is still unknown how it performs under the\nscenario of a robust source model: Can the robustness of the source model be\nsuccessfully inherited? Does VP also encounter the same trade-off between\nrobustness and generalization ability as the source model during this process?\nIf such a trade-off exists, is there a strategy specifically tailored to VP to\nmitigate this limitation? In this paper, we thoroughly explore these three\nquestions for the first time and provide affirmative answers to them. To\nmitigate the trade-off faced by VP, we propose a strategy called Prompt\nBoundary Loosening (PBL). As a lightweight, plug-and-play strategy naturally\ncompatible with VP, PBL effectively ensures the successful inheritance of\nrobustness when the source model is a robust model, while significantly\nenhancing VP's generalization ability across various downstream datasets.\nExtensive experiments across various datasets show that our findings are\nuniversal and demonstrate the significant benefits of the proposed strategy."
    },
    {
        "date": "2025-06",
        "title": "LitMAS: A Lightweight and Generalized Multi-Modal Anti-Spoofing Framework for Biometric Security",
        "author": "Nidheesh Gorthi, Kartik Thakral, Rishabh Ranjan, Richa Singh, and Mayank Vatsa",
        "link": "http://arxiv.org/abs/2506.06759v1",
        "abstract": "Biometric authentication systems are increasingly being deployed in critical\napplications, but they remain susceptible to spoofing. Since most of the\nresearch efforts focus on modality-specific anti-spoofing techniques, building\na unified, resource-efficient solution across multiple biometric modalities\nremains a challenge. To address this, we propose LitMAS, a\n$\\textbf{Li}$gh$\\textbf{t}$ weight and generalizable $\\textbf{M}$ulti-modal\n$\\textbf{A}$nti-$\\textbf{S}$poofing framework designed to detect spoofing\nattacks in speech, face, iris, and fingerprint-based biometric systems. At the\ncore of LitMAS is a Modality-Aligned Concentration Loss, which enhances\ninter-class separability while preserving cross-modal consistency and enabling\nrobust spoof detection across diverse biometric traits. With just 6M\nparameters, LitMAS surpasses state-of-the-art methods by $1.36\\%$ in average\nEER across seven datasets, demonstrating high efficiency, strong\ngeneralizability, and suitability for edge deployment. Code and trained models\nare available at https://github.com/IAB-IITJ/LitMAS."
    },
    {
        "date": "2025-06",
        "title": "Fuse and Federate: Enhancing EV Charging Station Security with Multimodal Fusion and Federated Learning",
        "author": "Rabah Rahal, Abdelaziz Amara Korba, and Yacine Ghamri-Doudane",
        "link": "http://arxiv.org/abs/2506.06730v1",
        "abstract": "The rapid global adoption of electric vehicles (EVs) has established electric\nvehicle supply equipment (EVSE) as a critical component of smart grid\ninfrastructure. While essential for ensuring reliable energy delivery and\naccessibility, EVSE systems face significant cybersecurity challenges,\nincluding network reconnaissance, backdoor intrusions, and distributed\ndenial-of-service (DDoS) attacks. These emerging threats, driven by the\ninterconnected and autonomous nature of EVSE, require innovative and adaptive\nsecurity mechanisms that go beyond traditional intrusion detection systems\n(IDS). Existing approaches, whether network-based or host-based, often fail to\ndetect sophisticated and targeted attacks specifically crafted to exploit new\nvulnerabilities in EVSE infrastructure. This paper proposes a novel intrusion\ndetection framework that leverages multimodal data sources, including network\ntraffic and kernel events, to identify complex attack patterns. The framework\nemploys a distributed learning approach, enabling collaborative intelligence\nacross EVSE stations while preserving data privacy through federated learning.\nExperimental results demonstrate that the proposed framework outperforms\nexisting solutions, achieving a detection rate above 98% and a precision rate\nexceeding 97% in decentralized environments. This solution addresses the\nevolving challenges of EVSE security, offering a scalable and privacypreserving\nresponse to advanced cyber threats"
    },
    {
        "date": "2025-06",
        "title": "Mitigating Object Hallucination via Robust Local Perception Search",
        "author": "Zixian Gao, Chao Yang, Zhanhui Zhou, Xing Xu, and Chaochao Lu",
        "link": "http://arxiv.org/abs/2506.06729v1",
        "abstract": "Recent advancements in Multimodal Large Language Models (MLLMs) have enabled\nthem to effectively integrate vision and language, addressing a variety of\ndownstream tasks. However, despite their significant success, these models\nstill exhibit hallucination phenomena, where the outputs appear plausible but\ndo not align with the content of the images. To mitigate this issue, we\nintroduce Local Perception Search (LPS), a decoding method during inference\nthat is both simple and training-free, yet effectively suppresses\nhallucinations. This method leverages local visual prior information as a value\nfunction to correct the decoding process. Additionally, we observe that the\nimpact of the local visual prior on model performance is more pronounced in\nscenarios with high levels of image noise. Notably, LPS is a plug-and-play\napproach that is compatible with various models. Extensive experiments on\nwidely used hallucination benchmarks and noisy data demonstrate that LPS\nsignificantly reduces the incidence of hallucinations compared to the baseline,\nshowing exceptional performance, particularly in noisy settings."
    },
    {
        "date": "2025-06",
        "title": "Learning Robust Heterogeneous Graph Representations via Contrastive-Reconstruction under Sparse Semantics",
        "author": "Di Lin, Wanjing Ren, Xuanbin Li, and Rui Zhang",
        "link": "http://arxiv.org/abs/2506.06682v1",
        "abstract": "In graph self-supervised learning, masked autoencoders (MAE) and contrastive\nlearning (CL) are two prominent paradigms. MAE focuses on reconstructing masked\nelements, while CL maximizes similarity between augmented graph views. Recent\nstudies highlight their complementarity: MAE excels at local feature capture,\nand CL at global information extraction. Hybrid frameworks for homogeneous\ngraphs have been proposed, but face challenges in designing shared encoders to\nmeet the semantic requirements of both tasks. In semantically sparse scenarios,\nCL struggles with view construction, and gradient imbalance between positive\nand negative samples persists. This paper introduces HetCRF, a novel\ndual-channel self-supervised learning framework for heterogeneous graphs.\nHetCRF uses a two-stage aggregation strategy to adapt embedding semantics,\nmaking it compatible with both MAE and CL. To address semantic sparsity, it\nenhances encoder output for view construction instead of relying on raw\nfeatures, improving efficiency. Two positive sample augmentation strategies are\nalso proposed to balance gradient contributions. Node classification\nexperiments on four real-world heterogeneous graph datasets demonstrate that\nHetCRF outperforms state-of-the-art baselines. On datasets with missing node\nfeatures, such as Aminer and Freebase, at a 40% label rate in node\nclassification, HetCRF improves the Macro-F1 score by 2.75% and 2.2%\nrespectively compared to the second-best baseline, validating its effectiveness\nand superiority."
    },
    {
        "date": "2025-06",
        "title": "Robust Learnability of Sample-Compressible Distributions under Noisy or Adversarial Perturbations",
        "author": "Arefe Boushehrian, and Amir Najafi",
        "link": "http://arxiv.org/abs/2506.06613v1",
        "abstract": "Learning distribution families over $\\mathbb{R}^d$ is a fundamental problem\nin unsupervised learning and statistics. A central question in this setting is\nwhether a given family of distributions possesses sufficient structure to be\n(at least) information-theoretically learnable and, if so, to characterize its\nsample complexity. In 2018, Ashtiani et al. reframed \\emph{sample\ncompressibility}, originally due to Littlestone and Warmuth (1986), as a\nstructural property of distribution classes, proving that it guarantees\nPAC-learnability. This discovery subsequently enabled a series of recent\nadvancements in deriving nearly tight sample complexity bounds for various\nhigh-dimensional open problems. It has been further conjectured that the\nconverse also holds: every learnable class admits a tight sample compression\nscheme.\n  In this work, we establish that sample compressible families remain learnable\neven from perturbed samples, subject to a set of necessary and sufficient\nconditions. We analyze two models of data perturbation: (i) an additive\nindependent noise model, and (ii) an adversarial corruption model, where an\nadversary manipulates a limited subset of the samples unknown to the learner.\nOur results are general and rely on as minimal assumptions as possible. We\ndevelop a perturbation-quantization framework that interfaces naturally with\nthe compression scheme and leads to sample complexity bounds that scale\ngracefully with the noise level and corruption budget. As concrete\napplications, we establish new sample complexity bounds for learning finite\nmixtures of high-dimensional uniform distributions under both noise and\nadversarial perturbations, as well as for learning Gaussian mixture models from\nadversarially corrupted samples, resolving two open problems in the literature."
    },
    {
        "date": "2025-06",
        "title": "Cyber Security of Sensor Systems for State Sequence Estimation: an AI Approach",
        "author": "Xubin Fang, Rick S. Blum, Ramesh Bharadwaj, and Brian M. Sadler",
        "link": "http://arxiv.org/abs/2506.06572v1",
        "abstract": "Sensor systems are extremely popular today and vulnerable to sensor data\nattacks. Due to possible devastating consequences, counteracting sensor data\nattacks is an extremely important topic, which has not seen sufficient study.\nThis paper develops the first methods that accurately identify/eliminate only\nthe problematic attacked sensor data presented to a sequence\nestimation/regression algorithm under a powerful attack model constructed based\non known/observed attacks. The approach does not assume a known form for the\nstatistical model of the sensor data, allowing data-driven and machine learning\nsequence estimation/regression algorithms to be protected. A simple protection\napproach for attackers not endowed with knowledge of the details of our\nprotection approach is first developed, followed by additional processing for\nattacks based on protection system knowledge. In the cases tested for which it\nwas designed, experimental results show that the simple approach achieves\nperformance indistinguishable, to two decimal places, from that for an approach\nwhich knows which sensors are attacked. For cases where the attacker has\nknowledge of the protection approach, experimental results indicate the\nadditional processing can be configured so that the worst-case degradation\nunder the additional processing and a large number of sensors attacked can be\nmade significantly smaller than the worst-case degradation of the simple\napproach, and close to an approach which knows which sensors are attacked, for\nthe same number of attacked sensors with just a slight degradation under no\nattacks. Mathematical descriptions of the worst-case attacks are used to\ndemonstrate the additional processing will provide similar advantages for cases\nfor which we do not have numerical results. All the data-driven processing used\nin our approaches employ only unattacked training data."
    },
    {
        "date": "2025-06",
        "title": "Adapting Under Fire: Multi-Agent Reinforcement Learning for Adversarial Drift in Network Security",
        "author": "Emilia Rivas, Sabrina Saika, Ahtesham Bakht, Aritran Piplai, Nathaniel D. Bastian, and Ankit Shah",
        "link": "http://arxiv.org/abs/2506.06565v1",
        "abstract": "Evolving attacks are a critical challenge for the long-term success of\nNetwork Intrusion Detection Systems (NIDS). The rise of these changing patterns\nhas exposed the limitations of traditional network security methods. While\nsignature-based methods are used to detect different types of attacks, they\noften fail to detect unknown attacks. Moreover, the system requires frequent\nupdates with new signatures as the attackers are constantly changing their\ntactics. In this paper, we design an environment where two agents improve their\npolicies over time. The adversarial agent, referred to as the red agent,\nperturbs packets to evade the intrusion detection mechanism, whereas the blue\nagent learns new defensive policies using drift adaptation techniques to\ncounter the attacks. Both agents adapt iteratively: the red agent responds to\nthe evolving NIDS, while the blue agent adjusts to emerging attack patterns. By\nstudying the model's learned policy, we offer concrete insights into drift\nadaptation techniques with high utility. Experiments show that the blue agent\nboosts model accuracy by 30% with just 2 to 3 adaptation steps using only 25 to\n30 samples each."
    },
    {
        "date": "2025-06",
        "title": "Securing Traffic Sign Recognition Systems in Autonomous Vehicles",
        "author": "Thushari Hapuarachchi, Long Dang, and Kaiqi Xiong",
        "link": "http://arxiv.org/abs/2506.06563v1",
        "abstract": "Deep Neural Networks (DNNs) are widely used for traffic sign recognition\nbecause they can automatically extract high-level features from images. These\nDNNs are trained on large-scale datasets obtained from unknown sources.\nTherefore, it is important to ensure that the models remain secure and are not\ncompromised or poisoned during training. In this paper, we investigate the\nrobustness of DNNs trained for traffic sign recognition. First, we perform the\nerror-minimizing attacks on DNNs used for traffic sign recognition by adding\nimperceptible perturbations on training data. Then, we propose a data\naugmentation-based training method to mitigate the error-minimizing attacks.\nThe proposed training method utilizes nonlinear transformations to disrupt the\nperturbations and improve the model robustness. We experiment with two\nwell-known traffic sign datasets to demonstrate the severity of the attack and\nthe effectiveness of our mitigation scheme. The error-minimizing attacks reduce\nthe prediction accuracy of the DNNs from 99.90% to 10.6%. However, our\nmitigation scheme successfully restores the prediction accuracy to 96.05%.\nMoreover, our approach outperforms adversarial training in mitigating the\nerror-minimizing attacks. Furthermore, we propose a detection model capable of\nidentifying poisoned data even when the perturbations are imperceptible to\nhuman inspection. Our detection model achieves a success rate of over 99% in\nidentifying the attack. This research highlights the need to employ advanced\ntraining methods for DNNs in traffic sign recognition systems to mitigate the\neffects of data poisoning attacks."
    },
    {
        "date": "2025-06",
        "title": "SDN-Based False Data Detection With Its Mitigation and Machine Learning Robustness for In-Vehicle Networks",
        "author": "Long Dang, Thushari Hapuarachchi, Kaiqi Xiong, and Yi Li",
        "link": "http://arxiv.org/abs/2506.06556v1",
        "abstract": "As the development of autonomous and connected vehicles advances, the\ncomplexity of modern vehicles increases, with numerous Electronic Control Units\n(ECUs) integrated into the system. In an in-vehicle network, these ECUs\ncommunicate with one another using an standard protocol called Controller Area\nNetwork (CAN). Securing communication among ECUs plays a vital role in\nmaintaining the safety and security of the vehicle. This paper proposes a\nrobust SDN-based False Data Detection and Mitigation System (FDDMS) for\nin-vehicle networks. Leveraging the unique capabilities of Software-Defined\nNetworking (SDN), FDDMS is designed to monitor and detect false data injection\nattacks in real-time. Specifically, we focus on brake-related ECUs within an\nSDN-enabled in-vehicle network. First, we decode raw CAN data to create an\nattack model that illustrates how false data can be injected into the system.\nThen, FDDMS, incorporating a Long Short Term Memory (LSTM)-based detection\nmodel, is used to identify false data injection attacks. We further propose an\neffective variant of DeepFool attack to evaluate the model's robustness. To\ncountermeasure the impacts of four adversarial attacks including Fast gradient\ndescent method, Basic iterative method, DeepFool, and the DeepFool variant, we\nfurther enhance a re-training technique method with a threshold based selection\nstrategy. Finally, a mitigation scheme is implemented to redirect attack\ntraffic by dynamically updating flow rules through SDN. Our experimental\nresults show that the proposed FDDMS is robust against adversarial attacks and\neffectively detects and mitigates false data injection attacks in real-time."
    },
    {
        "date": "2025-06",
        "title": "A Systematic Review of Poisoning Attacks Against Large Language Models",
        "author": "Neil Fendley, Edward W. Staley, Joshua Carney, William Redman, Marie Chau, and Nathan Drenkow",
        "link": "http://arxiv.org/abs/2506.06518v1",
        "abstract": "With the widespread availability of pretrained Large Language Models (LLMs)\nand their training datasets, concerns about the security risks associated with\ntheir usage has increased significantly. One of these security risks is the\nthreat of LLM poisoning attacks where an attacker modifies some part of the LLM\ntraining process to cause the LLM to behave in a malicious way. As an emerging\narea of research, the current frameworks and terminology for LLM poisoning\nattacks are derived from earlier classification poisoning literature and are\nnot fully equipped for generative LLM settings. We conduct a systematic review\nof published LLM poisoning attacks to clarify the security implications and\naddress inconsistencies in terminology across the literature. We propose a\ncomprehensive poisoning threat model applicable to categorize a wide range of\nLLM poisoning attacks. The poisoning threat model includes four poisoning\nattack specifications that define the logistics and manipulation strategies of\nan attack as well as six poisoning metrics used to measure key characteristics\nof an attack. Under our proposed framework, we organize our discussion of\npublished LLM poisoning literature along four critical dimensions of LLM\npoisoning attacks: concept poisons, stealthy poisons, persistent poisons, and\npoisons for unique tasks, to better understand the current landscape of\nsecurity risks."
    },
    {
        "date": "2025-06",
        "title": "Membership Inference Attacks for Unseen Classes",
        "author": "Pratiksha Thaker, Neil Kale, Zhiwei Steven Wu, and Virginia Smith",
        "link": "http://arxiv.org/abs/2506.06488v1",
        "abstract": "Shadow model attacks are the state-of-the-art approach for membership\ninference attacks on machine learning models. However, these attacks typically\nassume an adversary has access to a background (nonmember) data distribution\nthat matches the distribution the target model was trained on. We initiate a\nstudy of membership inference attacks where the adversary or auditor cannot\naccess an entire subclass from the distribution -- a more extreme but realistic\nversion of distribution shift than has been studied previously. In this\nsetting, we first show that the performance of shadow model attacks degrades\ncatastrophically, and then demonstrate the promise of another approach,\nquantile regression, that does not have the same limitations. We show that\nquantile regression attacks consistently outperform shadow model attacks in the\nclass dropout setting -- for example, quantile regression attacks achieve up to\n11$\\times$ the TPR of shadow models on the unseen class on CIFAR-100, and\nachieve nontrivial TPR on ImageNet even with 90% of training classes removed.\nWe also provide a theoretical model that illustrates the potential and\nlimitations of this approach."
    },
    {
        "date": "2025-06",
        "title": "Benchmarking Misuse Mitigation Against Covert Adversaries",
        "author": "Davis Brown, Mahdi Sabbaghi, Luze Sun, Alexander Robey, George J. Pappas, Eric Wong, and Hamed Hassani",
        "link": "http://arxiv.org/abs/2506.06414v1",
        "abstract": "Existing language model safety evaluations focus on overt attacks and\nlow-stakes tasks. Realistic attackers can subvert current safeguards by\nrequesting help on small, benign-seeming tasks across many independent queries.\nBecause individual queries do not appear harmful, the attack is hard to\n{detect}. However, when combined, these fragments uplift misuse by helping the\nattacker complete hard and dangerous tasks. Toward identifying defenses against\nsuch strategies, we develop Benchmarks for Stateful Defenses (BSD), a data\ngeneration pipeline that automates evaluations of covert attacks and\ncorresponding defenses. Using this pipeline, we curate two new datasets that\nare consistently refused by frontier models and are too difficult for weaker\nopen-weight models. Our evaluations indicate that decomposition attacks are\neffective misuse enablers, and highlight stateful defenses as a countermeasure."
    },
    {
        "date": "2025-06",
        "title": "Joint-GCG: Unified Gradient-Based Poisoning Attacks on Retrieval-Augmented Generation Systems",
        "author": "Haowei Wang, Rupeng Zhang, Junjie Wang, Mingyang Li, Yuekai Huang, Dandan Wang, and Qing Wang",
        "link": "http://arxiv.org/abs/2506.06151v1",
        "abstract": "Retrieval-Augmented Generation (RAG) systems enhance Large Language Models\n(LLMs) by retrieving relevant documents from external corpora before generating\nresponses. This approach significantly expands LLM capabilities by leveraging\nvast, up-to-date external knowledge. However, this reliance on external\nknowledge makes RAG systems vulnerable to corpus poisoning attacks that\nmanipulate generated outputs via poisoned document injection. Existing\npoisoning attack strategies typically treat the retrieval and generation stages\nas disjointed, limiting their effectiveness. We propose Joint-GCG, the first\nframework to unify gradient-based attacks across both retriever and generator\nmodels through three innovations: (1) Cross-Vocabulary Projection for aligning\nembedding spaces, (2) Gradient Tokenization Alignment for synchronizing\ntoken-level gradient signals, and (3) Adaptive Weighted Fusion for dynamically\nbalancing attacking objectives. Evaluations demonstrate that Joint-GCG achieves\nat most 25% and an average of 5% higher attack success rate than previous\nmethods across multiple retrievers and generators. While optimized under a\nwhite-box assumption, the generated poisons show unprecedented transferability\nto unseen models. Joint-GCG's innovative unification of gradient-based attacks\nacross retrieval and generation stages fundamentally reshapes our understanding\nof vulnerabilities within RAG systems. Our code is available at\nhttps://github.com/NicerWang/Joint-GCG."
    },
    {
        "date": "2025-06",
        "title": "SATversary: Adversarial Attacks on Satellite Fingerprinting",
        "author": "Joshua Smailes, Sebastian K\u00f6hler, Simon Birnbach, Martin Strohmeier, and Ivan Martinovic",
        "link": "http://arxiv.org/abs/2506.06119v1",
        "abstract": "As satellite systems become increasingly vulnerable to physical layer attacks\nvia SDRs, novel countermeasures are being developed to protect critical\nsystems, particularly those lacking cryptographic protection, or those which\ncannot be upgraded to support modern cryptography. Among these is transmitter\nfingerprinting, which provides mechanisms by which communication can be\nauthenticated by looking at characteristics of the transmitter, expressed as\nimpairments on the signal.\n  Previous works show that fingerprinting can be used to classify satellite\ntransmitters, or authenticate them against SDR-equipped attackers under simple\nreplay scenarios. In this paper we build upon this by looking at attacks\ndirectly targeting the fingerprinting system, with an attacker optimizing for\nmaximum impact in jamming, spoofing, and dataset poisoning attacks, and\ndemonstrate these attacks on the SatIQ system designed to authenticate Iridium\ntransmitters. We show that an optimized jamming signal can cause a 50% error\nrate with attacker-to-victim ratios as low as -30dB (far less power than\ntraditional jamming) and demonstrate successful identity forgery during\nspoofing attacks, with an attacker successfully removing their own\ntransmitter's fingerprint from messages. We also present a data poisoning\nattack, enabling persistent message spoofing by altering the data used to\nauthenticate incoming messages to include the fingerprint of the attacker's\ntransmitter.\n  Finally, we show that our model trained to optimize spoofing attacks can also\nbe used to detect spoofing and replay attacks, even when it has never seen the\nattacker's transmitter before. Furthermore, this technique works even when the\ntraining dataset includes only a single transmitter, enabling fingerprinting to\nbe used to protect small constellations and even individual satellites,\nproviding additional protection where it is needed the most."
    },
    {
        "date": "2025-06",
        "title": "Synthetic Tabular Data: Methods, Attacks and Defenses",
        "author": "Graham Cormode, Samuel Maddock, Enayat Ullah, and Shripad Gade",
        "link": "http://arxiv.org/abs/2506.06108v1",
        "abstract": "Synthetic data is often positioned as a solution to replace sensitive\nfixed-size datasets with a source of unlimited matching data, freed from\nprivacy concerns. There has been much progress in synthetic data generation\nover the last decade, leveraging corresponding advances in machine learning and\ndata analytics. In this survey, we cover the key developments and the main\nconcepts in tabular synthetic data generation, including paradigms based on\nprobabilistic graphical models and on deep learning. We provide background and\nmotivation, before giving a technical deep-dive into the methodologies. We also\naddress the limitations of synthetic data, by studying attacks that seek to\nretrieve information about the original sensitive data. Finally, we present\nextensions and open problems in this area."
    },
    {
        "date": "2025-06",
        "title": "Minoritised Ethnic People's Security and Privacy Concerns and Responses towards Essential Online Services",
        "author": "Aunam Quyoum, Mark Wong, Sebati Ghosh, and Siamak F. Shahandashti",
        "link": "http://arxiv.org/abs/2506.06062v2",
        "abstract": "Minoritised ethnic people are marginalised in society, and therefore at a\nhigher risk of adverse online harms, including those arising from the loss of\nsecurity and privacy of personal data. Despite this, there has been very little\nresearch focused on minoritised ethnic people's security and privacy concerns,\nattitudes, and behaviours. In this work, we provide the results of one of the\nfirst studies in this regard. We explore minoritised ethnic people's\nexperiences of using essential online services across three sectors: health,\nsocial housing, and energy, their security and privacy-related concerns, and\nresponses towards these services. We conducted a thematic analysis of 44\nsemi-structured interviews with people of various reported minoritised\nethnicities in the UK. Privacy concerns and lack of control over personal data\nemerged as a major theme, with many interviewees considering privacy as their\nmost significant concern when using online services. Several creative tactics\nto exercise some agency were reported, including selective and inconsistent\ndisclosure of personal data. A core concern about how data may be used was\ndriven by a fear of repercussions, including penalisation and discrimination,\ninfluenced by prior experiences of institutional and online racism. The\nincreased concern and potential for harm resulted in minoritised ethnic people\ngrappling with a higher-stakes dilemma of whether to disclose personal\ninformation online or not. Furthermore, trust in institutions, or lack thereof,\nwas found to be embedded throughout as a basis for adapting behaviour. We draw\non our results to provide lessons learned for the design of more inclusive,\nmarginalisation-aware, and privacy-preserving online services."
    },
    {
        "date": "2025-06",
        "title": "Sample-Specific Noise Injection For Diffusion-Based Adversarial Purification",
        "author": "Yuhao Sun, Jiacheng Zhang, Zesheng Ye, Chaowei Xiao, and Feng Liu",
        "link": "http://arxiv.org/abs/2506.06027v1",
        "abstract": "Diffusion-based purification (DBP) methods aim to remove adversarial noise\nfrom the input sample by first injecting Gaussian noise through a forward\ndiffusion process, and then recovering the clean example through a reverse\ngenerative process. In the above process, how much Gaussian noise is injected\nto the input sample is key to the success of DBP methods, which is controlled\nby a constant noise level $t^*$ for all samples in existing methods. In this\npaper, we discover that an optimal $t^*$ for each sample indeed could be\ndifferent. Intuitively, the cleaner a sample is, the less the noise it should\nbe injected, and vice versa. Motivated by this finding, we propose a new\nframework, called Sample-specific Score-aware Noise Injection (SSNI).\nSpecifically, SSNI uses a pre-trained score network to estimate how much a data\npoint deviates from the clean data distribution (i.e., score norms). Then,\nbased on the magnitude of score norms, SSNI applies a reweighting function to\nadaptively adjust $t^*$ for each sample, achieving sample-specific noise\ninjections. Empirically, incorporating our framework with existing DBP methods\nresults in a notable improvement in both accuracy and robustness on CIFAR-10\nand ImageNet-1K, highlighting the necessity to allocate distinct noise levels\nto different samples in DBP methods. Our code is available at:\nhttps://github.com/tmlr-group/SSNI."
    },
    {
        "date": "2025-06",
        "title": "MCA-Bench: A Multimodal Benchmark for Evaluating CAPTCHA Robustness Against VLM-based Attacks",
        "author": "Zonglin Wu, Yule Xue, Xin Wei, and Yiren Song",
        "link": "http://arxiv.org/abs/2506.05982v1",
        "abstract": "As automated attack techniques rapidly advance, CAPTCHAs remain a critical\ndefense mechanism against malicious bots. However, existing CAPTCHA schemes\nencompass a diverse range of modalities -- from static distorted text and\nobfuscated images to interactive clicks, sliding puzzles, and logic-based\nquestions -- yet the community still lacks a unified, large-scale, multimodal\nbenchmark to rigorously evaluate their security robustness. To address this\ngap, we introduce MCA-Bench, a comprehensive and reproducible benchmarking\nsuite that integrates heterogeneous CAPTCHA types into a single evaluation\nprotocol. Leveraging a shared vision-language model backbone, we fine-tune\nspecialized cracking agents for each CAPTCHA category, enabling consistent,\ncross-modal assessments. Extensive experiments reveal that MCA-Bench\neffectively maps the vulnerability spectrum of modern CAPTCHA designs under\nvaried attack settings, and crucially offers the first quantitative analysis of\nhow challenge complexity, interaction depth, and model solvability interrelate.\nBased on these findings, we propose three actionable design principles and\nidentify key open challenges, laying the groundwork for systematic CAPTCHA\nhardening, fair benchmarking, and broader community collaboration. Datasets and\ncode are available online."
    },
    {
        "date": "2025-06",
        "title": "Quantifying Adversarial Uncertainty in Evidential Deep Learning using Conflict Resolution",
        "author": "Charmaine Barker, Daniel Bethell, and Simos Gerasimou",
        "link": "http://arxiv.org/abs/2506.05937v1",
        "abstract": "Reliability of deep learning models is critical for deployment in high-stakes\napplications, where out-of-distribution or adversarial inputs may lead to\ndetrimental outcomes. Evidential Deep Learning, an efficient paradigm for\nuncertainty quantification, models predictions as Dirichlet distributions of a\nsingle forward pass. However, EDL is particularly vulnerable to adversarially\nperturbed inputs, making overconfident errors. Conflict-aware Evidential Deep\nLearning (C-EDL) is a lightweight post-hoc uncertainty quantification approach\nthat mitigates these issues, enhancing adversarial and OOD robustness without\nretraining. C-EDL generates diverse, task-preserving transformations per input\nand quantifies representational disagreement to calibrate uncertainty estimates\nwhen needed. C-EDL's conflict-aware prediction adjustment improves detection of\nOOD and adversarial inputs, maintaining high in-distribution accuracy and low\ncomputational overhead. Our experimental evaluation shows that C-EDL\nsignificantly outperforms state-of-the-art EDL variants and competitive\nbaselines, achieving substantial reductions in coverage for OOD data (up to\n55%) and adversarial data (up to 90%), across a range of datasets, attack\ntypes, and uncertainty metrics."
    },
    {
        "date": "2025-06",
        "title": "Rethinking Semi-supervised Segmentation Beyond Accuracy: Reliability and Robustness",
        "author": "Steven Landgraf, Markus Hillemann, and Markus Ulrich",
        "link": "http://arxiv.org/abs/2506.05917v1",
        "abstract": "Semantic segmentation is critical for scene understanding but demands costly\npixel-wise annotations, attracting increasing attention to semi-supervised\napproaches to leverage abundant unlabeled data. While semi-supervised\nsegmentation is often promoted as a path toward scalable, real-world\ndeployment, it is astonishing that current evaluation protocols exclusively\nfocus on segmentation accuracy, entirely overlooking reliability and\nrobustness. These qualities, which ensure consistent performance under diverse\nconditions (robustness) and well-calibrated model confidences as well as\nmeaningful uncertainties (reliability), are essential for safety-critical\napplications like autonomous driving, where models must handle unpredictable\nenvironments and avoid sudden failures at all costs. To address this gap, we\nintroduce the Reliable Segmentation Score (RSS), a novel metric that combines\npredictive accuracy, calibration, and uncertainty quality measures via a\nharmonic mean. RSS penalizes deficiencies in any of its components, providing\nan easy and intuitive way of holistically judging segmentation models.\nComprehensive evaluations of UniMatchV2 against its predecessor and a\nsupervised baseline show that semi-supervised methods often trade reliability\nfor accuracy. While out-of-domain evaluations demonstrate UniMatchV2's\nrobustness, they further expose persistent reliability shortcomings. We\nadvocate for a shift in evaluation protocols toward more holistic metrics like\nRSS to better align semi-supervised learning research with real-world\ndeployment needs."
    },
    {
        "date": "2025-06",
        "title": "Robust sensor fusion against on-vehicle sensor staleness",
        "author": "Meng Fan, Yifan Zuo, Patrick Blaes, Harley Montgomery, and Subhasis Das",
        "link": "http://arxiv.org/abs/2506.05780v1",
        "abstract": "Sensor fusion is crucial for a performant and robust Perception system in\nautonomous vehicles, but sensor staleness, where data from different sensors\narrives with varying delays, poses significant challenges. Temporal\nmisalignment between sensor modalities leads to inconsistent object state\nestimates, severely degrading the quality of trajectory predictions that are\ncritical for safety. We present a novel and model-agnostic approach to address\nthis problem via (1) a per-point timestamp offset feature (for LiDAR and radar\nboth relative to camera) that enables fine-grained temporal awareness in sensor\nfusion, and (2) a data augmentation strategy that simulates realistic sensor\nstaleness patterns observed in deployed vehicles. Our method is integrated into\na perspective-view detection model that consumes sensor data from multiple\nLiDARs, radars and cameras. We demonstrate that while a conventional model\nshows significant regressions when one sensor modality is stale, our approach\nreaches consistently good performance across both synchronized and stale\nconditions."
    },
    {
        "date": "2025-06",
        "title": "To Protect the LLM Agent Against the Prompt Injection Attack with Polymorphic Prompt",
        "author": "Zhilong Wang, Neha Nagaraja, Lan Zhang, Hayretdin Bahsi, Pawan Patil, and Peng Liu",
        "link": "http://arxiv.org/abs/2506.05739v1",
        "abstract": "LLM agents are widely used as agents for customer support, content\ngeneration, and code assistance. However, they are vulnerable to prompt\ninjection attacks, where adversarial inputs manipulate the model's behavior.\nTraditional defenses like input sanitization, guard models, and guardrails are\neither cumbersome or ineffective. In this paper, we propose a novel,\nlightweight defense mechanism called Polymorphic Prompt Assembling (PPA), which\nprotects against prompt injection with near-zero overhead. The approach is\nbased on the insight that prompt injection requires guessing and breaking the\nstructure of the system prompt. By dynamically varying the structure of system\nprompts, PPA prevents attackers from predicting the prompt structure, thereby\nenhancing security without compromising performance. We conducted experiments\nto evaluate the effectiveness of PPA against existing attacks and compared it\nwith other defense methods."
    },
    {
        "date": "2025-06",
        "title": "Any-Class Presence Likelihood for Robust Multi-Label Classification with Abundant Negative Data",
        "author": "Dumindu Tissera, Omar Awadallah, Muhammad Umair Danish, Ayan Sadhu, and Katarina Grolinger",
        "link": "http://arxiv.org/abs/2506.05721v1",
        "abstract": "Multi-label Classification (MLC) assigns an instance to one or more\nnon-exclusive classes. A challenge arises when the dataset contains a large\nproportion of instances with no assigned class, referred to as negative data,\nwhich can overwhelm the learning process and hinder the accurate identification\nand classification of positive instances. Nevertheless, it is common in MLC\napplications such as industrial defect detection, agricultural disease\nidentification, and healthcare diagnosis to encounter large amounts of negative\ndata. Assigning a separate negative class to these instances further\ncomplicates the learning objective and introduces unnecessary redundancies. To\naddress this challenge, we redesign standard MLC loss functions by deriving a\nlikelihood of any class being present, formulated by a normalized weighted\ngeometric mean of the predicted class probabilities. We introduce a\nregularization parameter that controls the relative contribution of the absent\nclass probabilities to the any-class presence likelihood in positive instances.\nThe any-class presence likelihood complements the multi-label learning by\nencouraging the network to become more aware of implicit positive instances and\nimprove the label classification within those positive instances. Experiments\non large-scale datasets with negative data: SewerML, modified COCO, and\nChestX-ray14, across various networks and base loss functions show that our\nloss functions consistently improve MLC performance of their standard loss\ncounterparts, achieving gains of up to 6.01 percentage points in F1, 8.06 in\nF2, and 3.11 in mean average precision, all without additional parameters or\ncomputational complexity. Code available at:\nhttps://github.com/ML-for-Sensor-Data-Western/gmean-mlc"
    },
    {
        "date": "2025-06",
        "title": "SafeGenBench: A Benchmark Framework for Security Vulnerability Detection in LLM-Generated Code",
        "author": "Xinghang Li, Jingzhe Ding, Chao Peng, Bing Zhao, Xiang Gao, Hongwan Gao, and Xinchen Gu",
        "link": "http://arxiv.org/abs/2506.05692v1",
        "abstract": "The code generation capabilities of large language models(LLMs) have emerged\nas a critical dimension in evaluating their overall performance. However, prior\nresearch has largely overlooked the security risks inherent in the generated\ncode. In this work, we introduce \\benchmark, a benchmark specifically designed\nto assess the security of LLM-generated code. The dataset encompasses a wide\nrange of common software development scenarios and vulnerability types.\nBuilding upon this benchmark, we develop an automatic evaluation framework that\nleverages both static application security testing(SAST) and LLM-based judging\nto assess the presence of security vulnerabilities in model-generated code.\nThrough the empirical evaluation of state-of-the-art LLMs on \\benchmark, we\nreveal notable deficiencies in their ability to produce vulnerability-free\ncode. Our findings highlight pressing challenges and offer actionable insights\nfor future advancements in the secure code generation performance of LLMs. The\ndata and code will be released soon."
    },
    {
        "date": "2025-06",
        "title": "Emulating compact binary population synthesis simulations with robust uncertainty quantification and model comparison: Bayesian normalizing flows",
        "author": "Anarya Ray",
        "link": "http://arxiv.org/abs/2506.05657v1",
        "abstract": "Population synthesis simulations of compact binary coalescences~(CBCs) play a\ncrucial role in extracting astrophysical insights from an ensemble of\ngravitational wave~(GW) observations. However, realistic simulations are costly\nto implement for a dense grid of initial conditions. Normalizing flows can\nemulate the distribution functions of a simulated population of binary\nparameters and thereby enable empirical constraints on the astrophysical\ninitial conditions and branching fractions of various formation channels given\ndata from a catalog of GW observations. They can also be used for data\namplification in sparse regions of the CBC parameter space to guide the\ndevelopment of phenomenological population models for rarely synthesizable\nsystems with components in theorized mass gaps, without having to simulate a\nprohibitively large number of binaries. But flow predictions are wrought with\nuncertainties, especially for sparse training sets. In this work I develop a\nmethod for quantifying and marginalizing uncertainties in the emulators by\nintroducing the Bayesian Normalizing flow, a conditional density estimator\nconstructed from Bayesian neural networks. Using the exact likelihood function\nassociated with density estimators I sample the posterior distribution of flow\nparameters with suitably chosen priors to quantify and marginalize over flow\nuncertainties. I demonstrate the accuracy, calibration, and data-amplification\nimpacts of the estimated uncertainties for simulations of binary black hole\npopulations formed through common envelope evolution. I outline applications of\nthe methodology in simulation-based inference from growing GW catalogs and\nsketch other uses for general simulation-based approaches in GW astronomy."
    },
    {
        "date": "2025-06",
        "title": "FedShield-LLM: A Secure and Scalable Federated Fine-Tuned Large Language Model",
        "author": "Md Jueal Mia, and M. Hadi Amini",
        "link": "http://arxiv.org/abs/2506.05640v1",
        "abstract": "Federated Learning (FL) offers a decentralized framework for training and\nfine-tuning Large Language Models (LLMs) by leveraging computational resources\nacross organizations while keeping sensitive data on local devices. It\naddresses privacy and security concerns while navigating challenges associated\nwith the substantial computational demands of LLMs, which can be prohibitive\nfor small and medium-sized organizations. FL supports the development of\ntask-specific LLMs for cross-silo applications through fine-tuning but remains\nvulnerable to inference attacks, such as membership inference and gradient\ninversion, which threaten data privacy. Prior studies have utilized\nDifferential Privacy (DP) in LLM fine-tuning, which, despite being effective at\npreserving privacy, can degrade model performance. To overcome these\nchallenges, we propose a novel method, FedShield-LLM, that uses pruning with\nFully Homomorphic Encryption (FHE) for Low-Rank Adaptation (LoRA) parameters,\nenabling secure computations on encrypted model updates while mitigating the\nattack surface by deactivating less important LoRA parameters. Furthermore,\noptimized federated algorithms for cross-silo environments enhance scalability\nand efficiency. Parameter-efficient fine-tuning techniques like LoRA\nsubstantially reduce computational and communication overhead, making FL\nfeasible for resource-constrained clients. Experimental results show that the\nproposed method outperforms existing methods while maintaining robust privacy\nprotection, enabling organizations to collaboratively train secure and\nefficient LLMs.\n  The code and data are available at,\nhttps://github.com/solidlabnetwork/fedshield-llm"
    },
    {
        "date": "2025-06",
        "title": "Network Hexagons Under Attack: Secure Crowdsourcing of Geo-Referenced Data",
        "author": "Okemawo Obadofin, and Joao Barros",
        "link": "http://arxiv.org/abs/2506.05601v1",
        "abstract": "A critical requirement for modern-day Intelligent Transportation Systems\n(ITS) is the ability to collect geo-referenced data from connected vehicles and\nmobile devices in a safe, secure and anonymous way. The Nexagon protocol, which\nbuilds on the IETF Locator/ID Separation Protocol (LISP) and the Hierarchical\nHexagonal Clustering (H3) geo-spatial indexing system, offers a promising\nframework for dynamic, privacy-preserving data aggregation. Seeking to address\nthe critical security and privacy vulnerabilities that persist in its current\nspecification, we apply the STRIDE and LINDDUN threat modelling frameworks and\nprove among other that the Nexagon protocol is susceptible to user\nre-identification, session linkage, and sparse-region attacks. To address these\nchallenges, we propose an enhanced security architecture that combines public\nkey infrastructure (PKI) with ephemeral pseudonym certificates. Our solution\nguarantees user and device anonymity through randomized key rotation and\nadaptive geospatial resolution, thereby effectively mitigating\nre-identification and surveillance risks in sparse environments. A prototype\nimplementation over a microservice-based overlay network validates the approach\nand underscores its readiness for real-world deployment. Our results show that\nit is possible to achieve the required level of security without increasing\nlatency by more than 25% or reducing the throughput by more than 7%."
    },
    {
        "date": "2025-06",
        "title": "SeedVR2: One-Step Video Restoration via Diffusion Adversarial Post-Training",
        "author": "Jianyi Wang, Shanchuan Lin, Zhijie Lin, Yuxi Ren, Meng Wei, Zongsheng Yue, Shangchen Zhou, Hao Chen, Yang Zhao, Ceyuan Yang, Xuefeng Xiao, Chen Change Loy, and Lu Jiang",
        "link": "http://arxiv.org/abs/2506.05301v1",
        "abstract": "Recent advances in diffusion-based video restoration (VR) demonstrate\nsignificant improvement in visual quality, yet yield a prohibitive\ncomputational cost during inference. While several distillation-based\napproaches have exhibited the potential of one-step image restoration,\nextending existing approaches to VR remains challenging and underexplored,\nparticularly when dealing with high-resolution video in real-world settings. In\nthis work, we propose a one-step diffusion-based VR model, termed as SeedVR2,\nwhich performs adversarial VR training against real data. To handle the\nchallenging high-resolution VR within a single step, we introduce several\nenhancements to both model architecture and training procedures. Specifically,\nan adaptive window attention mechanism is proposed, where the window size is\ndynamically adjusted to fit the output resolutions, avoiding window\ninconsistency observed under high-resolution VR using window attention with a\npredefined window size. To stabilize and improve the adversarial post-training\ntowards VR, we further verify the effectiveness of a series of losses,\nincluding a proposed feature matching loss without significantly sacrificing\ntraining efficiency. Extensive experiments show that SeedVR2 can achieve\ncomparable or even better performance compared with existing VR approaches in a\nsingle step."
    },
    {
        "date": "2025-06",
        "title": "A Smooth Sea Never Made a Skilled $\\texttt{SAILOR}$: Robust Imitation via Learning to Search",
        "author": "Arnav Kumar Jain, Vibhakar Mohta, Subin Kim, Atiksh Bhardwaj, Juntao Ren, Yunhai Feng, Sanjiban Choudhury, and Gokul Swamy",
        "link": "http://arxiv.org/abs/2506.05294v1",
        "abstract": "The fundamental limitation of the behavioral cloning (BC) approach to\nimitation learning is that it only teaches an agent what the expert did at\nstates the expert visited. This means that when a BC agent makes a mistake\nwhich takes them out of the support of the demonstrations, they often don't\nknow how to recover from it. In this sense, BC is akin to giving the agent the\nfish -- giving them dense supervision across a narrow set of states -- rather\nthan teaching them to fish: to be able to reason independently about achieving\nthe expert's outcome even when faced with unseen situations at test-time. In\nresponse, we explore learning to search (L2S) from expert demonstrations, i.e.\nlearning the components required to, at test time, plan to match expert\noutcomes, even after making a mistake. These include (1) a world model and (2)\na reward model. We carefully ablate the set of algorithmic and design decisions\nrequired to combine these and other components for stable and\nsample/interaction-efficient learning of recovery behavior without additional\nhuman corrections. Across a dozen visual manipulation tasks from three\nbenchmarks, our approach $\\texttt{SAILOR}$ consistently out-performs\nstate-of-the-art Diffusion Policies trained via BC on the same data.\nFurthermore, scaling up the amount of demonstrations used for BC by\n5-10$\\times$ still leaves a performance gap. We find that $\\texttt{SAILOR}$ can\nidentify nuanced failures and is robust to reward hacking. Our code is\navailable at https://github.com/arnavkj1995/SAILOR ."
    },
    {
        "date": "2025-06",
        "title": "Can Foundation Models Generalise the Presentation Attack Detection Capabilities on ID Cards?",
        "author": "Juan E. Tapia, and Christoph Busch",
        "link": "http://arxiv.org/abs/2506.05263v1",
        "abstract": "Nowadays, one of the main challenges in presentation attack detection (PAD)\non ID cards is obtaining generalisation capabilities for a diversity of\ncountries that are issuing ID cards. Most PAD systems are trained on one, two,\nor three ID documents because of privacy protection concerns. As a result, they\ndo not obtain competitive results for commercial purposes when tested in an\nunknown new ID card country. In this scenario, Foundation Models (FM) trained\non huge datasets can help to improve generalisation capabilities. This work\nintends to improve and benchmark the capabilities of FM and how to use them to\nadapt the generalisation on PAD of ID Documents. Different test protocols were\nused, considering zero-shot and fine-tuning and two different ID card datasets.\nOne private dataset based on Chilean IDs and one open-set based on three ID\ncountries: Finland, Spain, and Slovakia. Our findings indicate that bona fide\nimages are the key to generalisation."
    },
    {
        "date": "2025-06",
        "title": "Robust Moment Identification for Nonlinear PDEs via a Neural ODE Approach",
        "author": "Shaoxuan Chen, Su Yang, Panayotis G. Kevrekidis, and Wei Zhu",
        "link": "http://arxiv.org/abs/2506.05245v1",
        "abstract": "We propose a data-driven framework for learning reduced-order moment dynamics\nfrom PDE-governed systems using Neural ODEs. In contrast to derivative-based\nmethods like SINDy, which necessitate densely sampled data and are sensitive to\nnoise, our approach based on Neural ODEs directly models moment trajectories,\nenabling robust learning from sparse and potentially irregular time series.\nUsing as an application platform the nonlinear Schr\\\"{o}dinger equation, the\nframework accurately recovers governing moment dynamics when closure is\navailable, even with limited and irregular observations. For systems without\nanalytical closure, we introduce a data-driven coordinate transformation\nstrategy based on Stiefel manifold optimization, enabling the discovery of\nlow-dimensional representations in which the moment dynamics become closed,\nfacilitating interpretable and reliable modeling. We also explore cases where a\nclosure model is not known, such as a Fisher-KPP reaction-diffusion system.\nHere we demonstrate that Neural ODEs can still effectively approximate the\nunclosed moment dynamics and achieve superior extrapolation accuracy compared\nto physical-expert-derived ODE models. This advantage remains robust even under\nsparse and irregular sampling, highlighting the method's robustness in\ndata-limited settings. Our results highlight the Neural ODE framework as a\npowerful and flexible tool for learning interpretable, low-dimensional moment\ndynamics in complex PDE-governed systems."
    },
    {
        "date": "2025-06",
        "title": "Learning Theory of Decentralized Robust Kernel-Based Learning Algorithm",
        "author": "Zhan Yu",
        "link": "http://arxiv.org/abs/2506.05215v1",
        "abstract": "We propose a new decentralized robust kernel-based learning algorithm within\nthe framework of reproducing kernel Hilbert space (RKHS) by utilizing a\nnetworked system that can be represented as a connected graph. The robust loss\nfunction $\\mathcal{L}_\\sigma$ induced by a windowing function $W$ and a\nrobustness scaling parameter $\\sigma>0$, can encompass a broad spectrum of\nrobust losses. Consequently, the proposed algorithm effectively provides a\nunified decentralized learning framework for robust regression, which\nfundamentally differs from the existing distributed robust kernel learning\nschemes, all of which are divide-and-conquer based. We rigorously establish the\nlearning theory and offer a comprehensive convergence analysis for the\nalgorithm. We show each local robust estimator generated from the decentralized\nalgorithm can be utilized to approximate the regression function. Based on\nkernel-based integral operator techniques, we derive general high confidence\nconvergence bounds for each local approximating sequence in terms of the mean\nsquare distance, RKHS norm, and generalization error, respectively. Moreover,\nwe provide rigorous selection rules for local sample size and show that, under\nproperly selected step size and scaling parameter $\\sigma$, the decentralized\nrobust algorithm can achieve optimal learning rates (up to logarithmic factors)\nin both norms. The parameter $\\sigma$ is shown to be essential for enhancing\nrobustness while also ensuring favorable convergence behavior. The intrinsic\nconnection among decentralization, sample selection, robustness of the\nalgorithm, and its convergence is clearly reflected."
    },
    {
        "date": "2025-06",
        "title": "Membership Inference Attacks on Sequence Models",
        "author": "Lorenzo Rossi, Michael Aerni, Jie Zhang, and Florian Tram\u00e8r",
        "link": "http://arxiv.org/abs/2506.05126v1",
        "abstract": "Sequence models, such as Large Language Models (LLMs) and autoregressive\nimage generators, have a tendency to memorize and inadvertently leak sensitive\ninformation. While this tendency has critical legal implications, existing\ntools are insufficient to audit the resulting risks. We hypothesize that those\ntools' shortcomings are due to mismatched assumptions. Thus, we argue that\neffectively measuring privacy leakage in sequence models requires leveraging\nthe correlations inherent in sequential generation. To illustrate this, we\nadapt a state-of-the-art membership inference attack to explicitly model\nwithin-sequence correlations, thereby demonstrating how a strong existing\nattack can be naturally extended to suit the structure of sequence models.\nThrough a case study, we show that our adaptations consistently improve the\neffectiveness of memorization audits without introducing additional\ncomputational costs. Our work hence serves as an important stepping stone\ntoward reliable memorization audits for large sequence models."
    },
    {
        "date": "2025-06",
        "title": "Practical Manipulation Model for Robust Deepfake Detection",
        "author": "Benedikt Hopf, and Radu Timofte",
        "link": "http://arxiv.org/abs/2506.05119v1",
        "abstract": "Modern deepfake detection models have achieved strong performance even on the\nchallenging cross-dataset task. However, detection performance under non-ideal\nconditions remains very unstable, limiting success on some benchmark datasets\nand making it easy to circumvent detection. Inspired by the move to a more\nreal-world degradation model in the area of image super-resolution, we have\ndeveloped a Practical Manipulation Model (PMM) that covers a larger set of\npossible forgeries. We extend the space of pseudo-fakes by using Poisson\nblending, more diverse masks, generator artifacts, and distractors.\nAdditionally, we improve the detectors' generality and robustness by adding\nstrong degradations to the training images. We demonstrate that these changes\nnot only significantly enhance the model's robustness to common image\ndegradations but also improve performance on standard benchmark datasets.\nSpecifically, we show clear increases of $3.51\\%$ and $6.21\\%$ AUC on the DFDC\nand DFDCP datasets, respectively, over the s-o-t-a LAA backbone. Furthermore,\nwe highlight the lack of robustness in previous detectors and our improvements\nin this regard. Code can be found at https://github.com/BenediktHopf/PMM"
    },
    {
        "date": "2025-06",
        "title": "Exploring Adversarial Watermarking in Transformer-Based Models: Transferability and Robustness Against Defense Mechanism for Medical Images",
        "author": "Rifat Sadik, Tanvir Rahman, Arpan Bhattacharjee, Bikash Chandra Halder, and Ismail Hossain",
        "link": "http://arxiv.org/abs/2506.06389v1",
        "abstract": "Deep learning models have shown remarkable success in dermatological image\nanalysis, offering potential for automated skin disease diagnosis. Previously,\nconvolutional neural network(CNN) based architectures have achieved immense\npopularity and success in computer vision (CV) based task like skin image\nrecognition, generation and video analysis. But with the emergence of\ntransformer based models, CV tasks are now are nowadays carrying out using\nthese models. Vision Transformers (ViTs) is such a transformer-based models\nthat have shown success in computer vision. It uses self-attention mechanisms\nto achieve state-of-the-art performance across various tasks. However, their\nreliance on global attention mechanisms makes them susceptible to adversarial\nperturbations. This paper aims to investigate the susceptibility of ViTs for\nmedical images to adversarial watermarking-a method that adds so-called\nimperceptible perturbations in order to fool models. By generating adversarial\nwatermarks through Projected Gradient Descent (PGD), we examine the\ntransferability of such attacks to CNNs and analyze the performance defense\nmechanism -- adversarial training. Results indicate that while performance is\nnot compromised for clean images, ViTs certainly become much more vulnerable to\nadversarial attacks: an accuracy drop of as low as 27.6%. Nevertheless,\nadversarial training raises it up to 90.0%."
    },
    {
        "date": "2025-06",
        "title": "Identifying and Understanding Cross-Class Features in Adversarial Training",
        "author": "Zeming Wei, Yiwen Guo, and Yisen Wang",
        "link": "http://arxiv.org/abs/2506.05032v1",
        "abstract": "Adversarial training (AT) has been considered one of the most effective\nmethods for making deep neural networks robust against adversarial attacks,\nwhile the training mechanisms and dynamics of AT remain open research problems.\nIn this paper, we present a novel perspective on studying AT through the lens\nof class-wise feature attribution. Specifically, we identify the impact of a\nkey family of features on AT that are shared by multiple classes, which we call\ncross-class features. These features are typically useful for robust\nclassification, which we offer theoretical evidence to illustrate through a\nsynthetic data model. Through systematic studies across multiple model\narchitectures and settings, we find that during the initial stage of AT, the\nmodel tends to learn more cross-class features until the best robustness\ncheckpoint. As AT further squeezes the training robust loss and causes robust\noverfitting, the model tends to make decisions based on more class-specific\nfeatures. Based on these discoveries, we further provide a unified view of two\nexisting properties of AT, including the advantage of soft-label training and\nrobust overfitting. Overall, these insights refine the current understanding of\nAT mechanisms and provide new perspectives on studying them. Our code is\navailable at https://github.com/PKU-ML/Cross-Class-Features-AT."
    },
    {
        "date": "2025-06",
        "title": "Attack Effect Model based Malicious Behavior Detection",
        "author": "Limin Wang, Lei Bu, Muzimiao Zhang, Shihong Cang, and Kai Ye",
        "link": "http://arxiv.org/abs/2506.05001v1",
        "abstract": "Traditional security detection methods face three key challenges: inadequate\ndata collection that misses critical security events, resource-intensive\nmonitoring systems, and poor detection algorithms with high false positive\nrates. We present FEAD (Focus-Enhanced Attack Detection), a framework that\naddresses these issues through three innovations: (1) an attack model-driven\napproach that extracts security-critical monitoring items from online attack\nreports for comprehensive coverage; (2) efficient task decomposition that\noptimally distributes monitoring across existing collectors to minimize\noverhead; and (3) locality-aware anomaly analysis that leverages the clustering\nbehavior of malicious activities in provenance graphs to improve detection\naccuracy. Evaluations demonstrate FEAD achieves 8.23% higher F1-score than\nexisting solutions with only 5.4% overhead, confirming that focus-based designs\nsignificantly enhance detection performance."
    },
    {
        "date": "2025-06",
        "title": "Robustness as Architecture: Designing IQA Models to Withstand Adversarial Perturbations",
        "author": "Igor Meleshin, Anna Chistyakova, Anastasia Antsiferova, and Dmitriy Vatolin",
        "link": "http://arxiv.org/abs/2506.04951v1",
        "abstract": "Image Quality Assessment (IQA) models are increasingly relied upon to\nevaluate image quality in real-world systems -- from compression and\nenhancement to generation and streaming. Yet their adoption brings a\nfundamental risk: these models are inherently unstable. Adversarial\nmanipulations can easily fool them, inflating scores and undermining trust.\nTraditionally, such vulnerabilities are addressed through data-driven defenses\n-- adversarial retraining, regularization, or input purification. But what if\nthis is the wrong lens? What if robustness in perceptual models is not\nsomething to learn but something to design? In this work, we propose a\nprovocative idea: robustness as an architectural prior. Rather than training\nmodels to resist perturbations, we reshape their internal structure to suppress\nsensitivity from the ground up. We achieve this by enforcing orthogonal\ninformation flow, constraining the network to norm-preserving operations -- and\nfurther stabilizing the system through pruning and fine-tuning. The result is a\nrobust IQA architecture that withstands adversarial attacks without requiring\nadversarial training or significant changes to the original model. This\napproach suggests a shift in perspective: from optimizing robustness through\ndata to engineering it through design."
    },
    {
        "date": "2025-06",
        "title": "Towards a Multi-Agent Simulation of Cyber-attackers and Cyber-defenders Battles",
        "author": "Julien Soul\u00e9, Jean-Paul Jamont, Michel Occello, Paul Th\u00e9ron, and Louis-Marie Traonouez",
        "link": "http://arxiv.org/abs/2506.04849v1",
        "abstract": "As cyber-attacks show to be more and more complex and coordinated,\ncyber-defenders strategy through multi-agent approaches could be key to tackle\nagainst cyber-attacks as close as entry points in a networked system. This\npaper presents a Markovian modeling and implementation through a simulator of\nfighting cyber-attacker agents and cyber-defender agents deployed on host\nnetwork nodes. It aims to provide an experimental framework to implement\nrealistically based coordinated cyber-attack scenarios while assessing\ncyber-defenders dynamic organizations. We abstracted network nodes by sets of\nproperties including agents' ones. Actions applied by agents model how the\nnetwork reacts depending in a given state and what properties are to change.\nCollective choice of the actions brings the whole environment closer or farther\nfrom respective cyber-attackers and cyber-defenders goals. Using the simulator,\nwe implemented a realistically inspired scenario with several behavior\nimplementation approaches for cyber-defenders and cyber-attackers."
    },
    {
        "date": "2025-06",
        "title": "On Automating Security Policies with Contemporary LLMs",
        "author": "Pablo Fern\u00e1ndez Saura, K. R. Jayaram, Vatche Isahagian, Jorge Bernal Bernab\u00e9, and Antonio Skarmeta",
        "link": "http://arxiv.org/abs/2506.04838v1",
        "abstract": "The complexity of modern computing environments and the growing\nsophistication of cyber threats necessitate a more robust, adaptive, and\nautomated approach to security enforcement. In this paper, we present a\nframework leveraging large language models (LLMs) for automating attack\nmitigation policy compliance through an innovative combination of in-context\nlearning and retrieval-augmented generation (RAG). We begin by describing how\nour system collects and manages both tool and API specifications, storing them\nin a vector database to enable efficient retrieval of relevant information. We\nthen detail the architectural pipeline that first decomposes high-level\nmitigation policies into discrete tasks and subsequently translates each task\ninto a set of actionable API calls. Our empirical evaluation, conducted using\npublicly available CTI policies in STIXv2 format and Windows API documentation,\ndemonstrates significant improvements in precision, recall, and F1-score when\nemploying RAG compared to a non-RAG baseline."
    },
    {
        "date": "2025-06",
        "title": "Fool the Stoplight: Realistic Adversarial Patch Attacks on Traffic Light Detectors",
        "author": "Svetlana Pavlitska, Jamie Robb, Nikolai Polley, Melih Yazgan, and J. Marius Z\u00f6llner",
        "link": "http://arxiv.org/abs/2506.04823v1",
        "abstract": "Realistic adversarial attacks on various camera-based perception tasks of\nautonomous vehicles have been successfully demonstrated so far. However, only a\nfew works considered attacks on traffic light detectors. This work shows how\nCNNs for traffic light detection can be attacked with printed patches. We\npropose a threat model, where each instance of a traffic light is attacked with\na patch placed under it, and describe a training strategy. We demonstrate\nsuccessful adversarial patch attacks in universal settings. Our experiments\nshow realistic targeted red-to-green label-flipping attacks and attacks on\npictogram classification. Finally, we perform a real-world evaluation with\nprinted patches and demonstrate attacks in the lab settings with a mobile\ntraffic light for construction sites and in a test area with stationary traffic\nlights. Our code is available at\nhttps://github.com/KASTEL-MobilityLab/attacks-on-traffic-light-detection."
    },
    {
        "date": "2025-06",
        "title": "LogicPuzzleRL: Cultivating Robust Mathematical Reasoning in LLMs via Reinforcement Learning",
        "author": "Zhen Hao Wong, Jingwen Deng, Runming He, Zirong Chen, Qijie You, Hejun Dong, Hao Liang, Chengyu Shen, Bin Cui, and Wentao Zhang",
        "link": "http://arxiv.org/abs/2506.04821v1",
        "abstract": "Large language models (LLMs) excel at many supervised tasks but often\nstruggle with structured reasoning in unfamiliar settings. This discrepancy\nsuggests that standard fine-tuning pipelines may instill narrow,\ndomain-specific heuristics rather than fostering general-purpose thinking\nstrategies. In this work, we propose a \"play to learn\" framework that\nfine-tunes LLMs through reinforcement learning on a suite of seven custom logic\npuzzles, each designed to cultivate distinct reasoning skills such as\nconstraint propagation, spatial consistency, and symbolic deduction. Using a\nreinforcement learning setup with verifiable rewards, models receive binary\nfeedback based on puzzle correctness, encouraging iterative, hypothesis-driven\nproblem solving. We demonstrate that this training approach significantly\nimproves out-of-distribution performance on a range of mathematical benchmarks,\nespecially for mid-difficulty problems that require multi-step reasoning.\nAnalyses across problem categories and difficulty levels reveal that puzzle\ntraining promotes transferable reasoning routines, strengthening algebraic\nmanipulation, geometric inference, and combinatorial logic, while offering\nlimited gains on rote or highly specialized tasks. These findings show that\nreinforcement learning over logic puzzles reshapes the internal reasoning of\nLLMs, enabling more robust and compositional generalization without relying on\ntask-specific symbolic tools."
    },
    {
        "date": "2025-06",
        "title": "Efficient Robust Conformal Prediction via Lipschitz-Bounded Networks",
        "author": "Thomas Massena, L\u00e9o and\u00e9ol, Thibaut Boissin, Franck Mamalet, Corentin Friedrich, Mathieu Serrurier, and S\u00e9bastien Gerchinovitz",
        "link": "http://arxiv.org/abs/2506.05434v2",
        "abstract": "Conformal Prediction (CP) has proven to be an effective post-hoc method for\nimproving the trustworthiness of neural networks by providing prediction sets\nwith finite-sample guarantees. However, under adversarial attacks, classical\nconformal guarantees do not hold anymore: this problem is addressed in the\nfield of Robust Conformal Prediction. Several methods have been proposed to\nprovide robust CP sets with guarantees under adversarial perturbations, but,\nfor large scale problems, these sets are either too large or the methods are\ntoo computationally demanding to be deployed in real life scenarios. In this\nwork, we propose a new method that leverages Lipschitz-bounded networks to\nprecisely and efficiently estimate robust CP sets. When combined with a\n1-Lipschitz robust network, we demonstrate that our lip-rcp method outperforms\nstate-of-the-art results in both the size of the robust CP sets and\ncomputational efficiency in medium and large-scale scenarios such as ImageNet.\nTaking a different angle, we also study vanilla CP under attack, and derive new\nworst-case coverage bounds of vanilla CP sets, which are valid simultaneously\nfor all adversarial attack levels. Our lip-rcp method makes this second\napproach as efficient as vanilla CP while also allowing robustness guarantees."
    },
    {
        "date": "2025-06",
        "title": "Robustness Evaluation for Video Models with Reinforcement Learning",
        "author": "Ashwin Ramesh Babu, Sajad Mousavi, Vineet Gundecha, Sahand Ghorbanpour, Avisek Naug, Antonio Guillen, Ricardo Luna Gutierrez, and Soumyendu Sarkar",
        "link": "http://arxiv.org/abs/2506.05431v1",
        "abstract": "Evaluating the robustness of Video classification models is very challenging,\nspecifically when compared to image-based models. With their increased temporal\ndimension, there is a significant increase in complexity and computational\ncost. One of the key challenges is to keep the perturbations to a minimum to\ninduce misclassification. In this work, we propose a multi-agent reinforcement\nlearning approach (spatial and temporal) that cooperatively learns to identify\nthe given video's sensitive spatial and temporal regions. The agents consider\ntemporal coherence in generating fine perturbations, leading to a more\neffective and visually imperceptible attack. Our method outperforms the\nstate-of-the-art solutions on the Lp metric and the average queries. Our method\nenables custom distortion types, making the robustness evaluation more relevant\nto the use case. We extensively evaluate 4 popular models for video action\nrecognition on two popular datasets, HMDB-51 and UCF-101."
    },
    {
        "date": "2025-06",
        "title": "Explainer-guided Targeted Adversarial Attacks against Binary Code Similarity Detection Models",
        "author": "Mingjie Chen, Tiancheng Zhu, Mingxue Zhang, Yiling He, Minghao Lin, Penghui Li, and Kui Ren",
        "link": "http://arxiv.org/abs/2506.05430v1",
        "abstract": "Binary code similarity detection (BCSD) serves as a fundamental technique for\nvarious software engineering tasks, e.g., vulnerability detection and\nclassification. Attacks against such models have therefore drawn extensive\nattention, aiming at misleading the models to generate erroneous predictions.\nPrior works have explored various approaches to generating semantic-preserving\nvariants, i.e., adversarial samples, to evaluate the robustness of the models\nagainst adversarial attacks. However, they have mainly relied on heuristic\ncriteria or iterative greedy algorithms to locate salient code influencing the\nmodel output, failing to operate on a solid theoretical basis. Moreover, when\nprocessing programs with high complexities, such attacks tend to be\ntime-consuming.\n  In this work, we propose a novel optimization for adversarial attacks against\nBCSD models. In particular, we aim to improve the attacks in a challenging\nscenario, where the attack goal is to limit the model predictions to a specific\nrange, i.e., the targeted attacks. Our attack leverages the superior capability\nof black-box, model-agnostic explainers in interpreting the model decision\nboundaries, thereby pinpointing the critical code snippet to apply\nsemantic-preserving perturbations. The evaluation results demonstrate that\ncompared with the state-of-the-art attacks, the proposed attacks achieve higher\nattack success rate in almost all scenarios, while also improving the\nefficiency and transferability. Our real-world case studies on vulnerability\ndetection and classification further demonstrate the security implications of\nour attacks, highlighting the urgent need to further enhance the robustness of\nexisting BCSD models."
    },
    {
        "date": "2025-06",
        "title": "SRD: Reinforcement-Learned Semantic Perturbation for Backdoor Defense in VLMs",
        "author": "Shuhan Xu, Siyuan Liang, Hongling Zheng, Yong Luo, Aishan Liu, and Dacheng Tao",
        "link": "http://arxiv.org/abs/2506.04743v1",
        "abstract": "Vision-Language Models (VLMs) have achieved remarkable performance in image\ncaptioning, but recent studies show they are vulnerable to backdoor attacks.\nAttackers can inject imperceptible perturbations-such as local pixel triggers\nor global semantic phrases-into the training data, causing the model to\ngenerate malicious, attacker-controlled captions for specific inputs. These\nattacks are hard to detect and defend due to their stealthiness and cross-modal\nnature. By analyzing attack samples, we identify two key vulnerabilities: (1)\nabnormal attention concentration on specific image regions, and (2) semantic\ndrift and incoherence in generated captions. To counter this, we propose\nSemantic Reward Defense (SRD), a reinforcement learning framework that\nmitigates backdoor behavior without prior knowledge of triggers. SRD uses a\nDeep Q-Network to learn policies for applying discrete perturbations (e.g.,\nocclusion, color masking) to sensitive image regions, aiming to disrupt the\nactivation of malicious pathways. We design a semantic fidelity score as the\nreward signal, which jointly evaluates semantic consistency and linguistic\nfluency of the output, guiding the agent toward generating robust yet faithful\ncaptions. Experiments across mainstream VLMs and datasets show SRD reduces\nattack success rates to 5.6%, while preserving caption quality on clean inputs\nwith less than 10% performance drop. SRD offers a trigger-agnostic,\ninterpretable defense paradigm against stealthy backdoor threats in multimodal\ngenerative models."
    },
    {
        "date": "2025-06",
        "title": "Coordinated Robustness Evaluation Framework for Vision-Language Models",
        "author": "Ashwin Ramesh Babu, Sajad Mousavi, Vineet Gundecha, Sahand Ghorbanpour, Avisek Naug, Antonio Guillen, Ricardo Luna Gutierrez, and Soumyendu Sarkar",
        "link": "http://arxiv.org/abs/2506.05429v1",
        "abstract": "Vision-language models, which integrate computer vision and natural language\nprocessing capabilities, have demonstrated significant advancements in tasks\nsuch as image captioning and visual question and answering. However, similar to\ntraditional models, they are susceptible to small perturbations, posing a\nchallenge to their robustness, particularly in deployment scenarios. Evaluating\nthe robustness of these models requires perturbations in both the vision and\nlanguage modalities to learn their inter-modal dependencies. In this work, we\ntrain a generic surrogate model that can take both image and text as input and\ngenerate joint representation which is further used to generate adversarial\nperturbations for both the text and image modalities. This coordinated attack\nstrategy is evaluated on the visual question and answering and visual reasoning\ndatasets using various state-of-the-art vision-language models. Our results\nindicate that the proposed strategy outperforms other multi-modal attacks and\nsingle-modality attacks from the recent literature. Our results demonstrate\ntheir effectiveness in compromising the robustness of several state-of-the-art\npre-trained multi-modal models such as instruct-BLIP, ViLT and others."
    },
    {
        "date": "2025-06",
        "title": "Robust Few-Shot Vision-Language Model Adaptation",
        "author": "Hanxin Wang, Tian Liu, and Shu Kong",
        "link": "http://arxiv.org/abs/2506.04713v1",
        "abstract": "Pretrained VLMs achieve strong performance on downstream tasks when adapted\nwith just a few labeled examples. As the adapted models inevitably encounter\nout-of-distribution (OOD) test data that deviates from the in-distribution (ID)\ntask-specific training data, enhancing OOD generalization in few-shot\nadaptation is critically important. We study robust few-shot VLM adaptation,\naiming to increase both ID and OOD accuracy. By comparing different adaptation\nmethods (e.g., prompt tuning, linear probing, contrastive finetuning, and full\nfinetuning), we uncover three key findings: (1) finetuning with proper\nhyperparameters significantly outperforms the popular VLM adaptation methods\nprompt tuning and linear probing; (2) visual encoder-only finetuning achieves\nbetter efficiency and accuracy than contrastively finetuning both visual and\ntextual encoders; (3) finetuning the top layers of the visual encoder provides\nthe best balance between ID and OOD accuracy. Building on these findings, we\npropose partial finetuning of the visual encoder empowered with two simple\naugmentation techniques: (1) retrieval augmentation which retrieves\ntask-relevant data from the VLM's pretraining dataset to enhance adaptation,\nand (2) adversarial perturbation which promotes robustness during finetuning.\nResults show that the former/latter boosts OOD/ID accuracy while slightly\nsacrificing the ID/OOD accuracy. Yet, perhaps understandably, naively combining\nthe two does not maintain their best OOD/ID accuracy. We address this dilemma\nwith the developed SRAPF, Stage-wise Retrieval Augmentation-based Adversarial\nPartial Finetuning. SRAPF consists of two stages: (1) partial finetuning the\nvisual encoder using both ID and retrieved data, and (2) adversarial partial\nfinetuning with few-shot ID data. Extensive experiments demonstrate that SRAPF\nachieves the state-of-the-art ID and OOD accuracy on the ImageNet OOD\nbenchmarks."
    },
    {
        "date": "2025-06",
        "title": "MMRefine: Unveiling the Obstacles to Robust Refinement in Multimodal Large Language Models",
        "author": "Gio Paik, Geewook Kim, and Jinbae Im",
        "link": "http://arxiv.org/abs/2506.04688v1",
        "abstract": "This paper introduces MMRefine, a MultiModal Refinement benchmark designed to\nevaluate the error refinement capabilities of Multimodal Large Language Models\n(MLLMs). As the emphasis shifts toward enhancing reasoning during inference,\nMMRefine provides a framework that evaluates MLLMs' abilities to detect and\ncorrect errors across six distinct scenarios beyond just comparing final\naccuracy before and after refinement. Furthermore, the benchmark analyzes the\nrefinement performance by categorizing errors into six error types. Experiments\nwith various open and closed MLLMs reveal bottlenecks and factors impeding\nrefinement performance, highlighting areas for improvement in effective\nreasoning enhancement. Our code and dataset are publicly available at\nhttps://github.com/naver-ai/MMRefine."
    },
    {
        "date": "2025-06",
        "title": "Scaling Laws for Robust Comparison of Open Foundation Language-Vision Models and Datasets",
        "author": "Marianna Nezhurina, Tomer Porian, Giovanni Pucceti, Tommie Kerssies, Romain Beaumont, Mehdi Cherti, and Jenia Jitsev",
        "link": "http://arxiv.org/abs/2506.04598v1",
        "abstract": "In studies of transferable learning, scaling laws are obtained for various\nimportant foundation models to predict their properties and performance at\nlarger scales. We show here how scaling law derivation can also be used for\nmodel and dataset comparison, allowing to decide which procedure is to be\npreferred for pre-training. For the first time, full scaling laws based on\ndense measurements across a wide span of model and samples seen scales are\nderived for two important language-vision learning procedures, CLIP and MaMMUT,\nthat use either contrastive only or contrastive and captioning text generative\nloss. Ensuring sufficient prediction accuracy for held out points, we use\nderived scaling laws to compare both models, obtaining evidence for MaMMUT's\nstronger improvement with scale and better sample efficiency than standard\nCLIP. To strengthen validity of the comparison, we show scaling laws for\nvarious downstream tasks, classification, retrieval, and segmentation, and for\ndifferent open datasets, DataComp, DFN and Re-LAION, observing consistently the\nsame trends. We show that comparison can also be performed when deriving\nscaling laws with a constant learning rate schedule, reducing compute cost.\nAccurate derivation of scaling laws provides thus means to perform model and\ndataset comparison across scale spans, avoiding misleading conclusions based on\nmeasurements from single reference scales only, paving the road for systematic\ncomparison and improvement of open foundation models and datasets for their\ncreation. We release all the pre-trained models with their intermediate\ncheckpoints, including openMaMMUT-L/14, which achieves $80.3\\%$ zero-shot\nImageNet-1k accuracy, trained on 12.8B samples from DataComp-1.4B. Code for\nreproducing experiments in the paper and raw experiments data can be found at\nhttps://github.com/LAION-AI/scaling-laws-for-comparison."
    },
    {
        "date": "2025-06",
        "title": "SUCEA: Reasoning-Intensive Retrieval for Adversarial Fact-checking through Claim Decomposition and Editing",
        "author": "Hongjun Liu, Yilun Zhao, Arman Cohan, and Chen Zhao",
        "link": "http://arxiv.org/abs/2506.04583v1",
        "abstract": "Automatic fact-checking has recently received more attention as a means of\ncombating misinformation. Despite significant advancements, fact-checking\nsystems based on retrieval-augmented language models still struggle to tackle\nadversarial claims, which are intentionally designed by humans to challenge\nfact-checking systems. To address these challenges, we propose a training-free\nmethod designed to rephrase the original claim, making it easier to locate\nsupporting evidence. Our modular framework, SUCEA, decomposes the task into\nthree steps: 1) Claim Segmentation and Decontextualization that segments\nadversarial claims into independent sub-claims; 2) Iterative Evidence Retrieval\nand Claim Editing that iteratively retrieves evidence and edits the subclaim\nbased on the retrieved evidence; 3) Evidence Aggregation and Label Prediction\nthat aggregates all retrieved evidence and predicts the entailment label.\nExperiments on two challenging fact-checking datasets demonstrate that our\nframework significantly improves on both retrieval and entailment label\naccuracy, outperforming four strong claim-decomposition-based baselines."
    },
    {
        "date": "2025-06",
        "title": "TRIDENT -- A Three-Tier Privacy-Preserving Propaganda Detection Model in Mobile Networks using Transformers, Adversarial Learning, and Differential Privacy",
        "author": "Al Nahian Bin Emran, Dhiman Goswami, Md Hasan Ullah Sadi, and Sanchari Das",
        "link": "http://arxiv.org/abs/2506.05421v1",
        "abstract": "The proliferation of propaganda on mobile platforms raises critical concerns\naround detection accuracy and user privacy. To address this, we propose TRIDENT\n- a three-tier propaganda detection model implementing transformers,\nadversarial learning, and differential privacy which integrates syntactic\nobfuscation and label perturbation to mitigate privacy leakage while\nmaintaining propaganda detection accuracy. TRIDENT leverages multilingual\nback-translation to introduce semantic variance, character-level noise, and\nentity obfuscation for differential privacy enforcement, and combines these\ntechniques into a unified defense mechanism. Using a binary propaganda\nclassification dataset, baseline transformer models (BERT, GPT-2) we achieved\nF1 scores of 0.89 and 0.90. Applying TRIDENT's third-tier defense yields a\nreduced but effective cumulative F1 of 0.83, demonstrating strong privacy\nprotection across mobile ML deployments with minimal degradation."
    },
    {
        "date": "2025-06",
        "title": "BESA: Boosting Encoder Stealing Attack with Perturbation Recovery",
        "author": "Xuhao Ren, Haotian Liang, Yajie Wang, Chuan Zhang, Zehui Xiong, and Liehuang Zhu",
        "link": "http://arxiv.org/abs/2506.04556v1",
        "abstract": "To boost the encoder stealing attack under the perturbation-based defense\nthat hinders the attack performance, we propose a boosting encoder stealing\nattack with perturbation recovery named BESA. It aims to overcome\nperturbation-based defenses. The core of BESA consists of two modules:\nperturbation detection and perturbation recovery, which can be combined with\ncanonical encoder stealing attacks. The perturbation detection module utilizes\nthe feature vectors obtained from the target encoder to infer the defense\nmechanism employed by the service provider. Once the defense mechanism is\ndetected, the perturbation recovery module leverages the well-designed\ngenerative model to restore a clean feature vector from the perturbed one.\nThrough extensive evaluations based on various datasets, we demonstrate that\nBESA significantly enhances the surrogate encoder accuracy of existing encoder\nstealing attacks by up to 24.63\\% when facing state-of-the-art defenses and\ncombinations of multiple defenses."
    },
    {
        "date": "2025-06",
        "title": "Neurosymbolic Artificial Intelligence for Robust Network Intrusion Detection: From Scratch to Transfer Learning",
        "author": "Huynh T. T. Tran, Jacob Sander, Achraf Cohen, Brian Jalaian, and Nathaniel D. Bastian",
        "link": "http://arxiv.org/abs/2506.04454v1",
        "abstract": "Network Intrusion Detection Systems (NIDS) play a vital role in protecting\ndigital infrastructures against increasingly sophisticated cyber threats. In\nthis paper, we extend ODXU, a Neurosymbolic AI (NSAI) framework that integrates\ndeep embedded clustering for feature extraction, symbolic reasoning using\nXGBoost, and comprehensive uncertainty quantification (UQ) to enhance\nrobustness, interpretability, and generalization in NIDS. The extended ODXU\nincorporates score-based methods (e.g., Confidence Scoring, Shannon Entropy)\nand metamodel-based techniques, including SHAP values and Information Gain, to\nassess the reliability of predictions. Experimental results on the CIC-IDS-2017\ndataset show that ODXU outperforms traditional neural models across six\nevaluation metrics, including classification accuracy and false omission rate.\nWhile transfer learning has seen widespread adoption in fields such as computer\nvision and natural language processing, its potential in cybersecurity has not\nbeen thoroughly explored. To bridge this gap, we develop a transfer learning\nstrategy that enables the reuse of a pre-trained ODXU model on a different\ndataset. Our ablation study on ACI-IoT-2023 demonstrates that the optimal\ntransfer configuration involves reusing the pre-trained autoencoder, retraining\nthe clustering module, and fine-tuning the XGBoost classifier, and outperforms\ntraditional neural models when trained with as few as 16,000 samples\n(approximately 50% of the training data). Additionally, results show that\nmetamodel-based UQ methods consistently outperform score-based approaches on\nboth datasets."
    },
    {
        "date": "2025-06",
        "title": "Gradient Inversion Attacks on Parameter-Efficient Fine-Tuning",
        "author": "Hasin Us Sami, Swapneel Sen, Amit K. Roy-Chowdhury, Srikanth V. Krishnamurthy, and Basak Guler",
        "link": "http://arxiv.org/abs/2506.04453v1",
        "abstract": "Federated learning (FL) allows multiple data-owners to collaboratively train\nmachine learning models by exchanging local gradients, while keeping their\nprivate data on-device. To simultaneously enhance privacy and training\nefficiency, recently parameter-efficient fine-tuning (PEFT) of large-scale\npretrained models has gained substantial attention in FL. While keeping a\npretrained (backbone) model frozen, each user fine-tunes only a few lightweight\nmodules to be used in conjunction, to fit specific downstream applications.\nAccordingly, only the gradients with respect to these lightweight modules are\nshared with the server. In this work, we investigate how the privacy of the\nfine-tuning data of the users can be compromised via a malicious design of the\npretrained model and trainable adapter modules. We demonstrate gradient\ninversion attacks on a popular PEFT mechanism, the adapter, which allow an\nattacker to reconstruct local data samples of a target user, using only the\naccessible adapter gradients. Via extensive experiments, we demonstrate that a\nlarge batch of fine-tuning images can be retrieved with high fidelity. Our\nattack highlights the need for privacy-preserving mechanisms for PEFT, while\nopening up several future directions. Our code is available at\nhttps://github.com/info-ucr/PEFTLeak."
    },
    {
        "date": "2025-06",
        "title": "Through the Stealth Lens: Rethinking Attacks and Defenses in RAG",
        "author": "Sarthak Choudhary, Nils Palumbo, Ashish Hooda, Krishnamurthy Dj Dvijotham, and Somesh Jha",
        "link": "http://arxiv.org/abs/2506.04390v1",
        "abstract": "Retrieval-augmented generation (RAG) systems are vulnerable to attacks that\ninject poisoned passages into the retrieved set, even at low corruption rates.\nWe show that existing attacks are not designed to be stealthy, allowing\nreliable detection and mitigation. We formalize stealth using a\ndistinguishability-based security game. If a few poisoned passages are designed\nto control the response, they must differentiate themselves from benign ones,\ninherently compromising stealth. This motivates the need for attackers to\nrigorously analyze intermediate signals involved in\ngeneration$\\unicode{x2014}$such as attention patterns or next-token probability\ndistributions$\\unicode{x2014}$to avoid easily detectable traces of\nmanipulation. Leveraging attention patterns, we propose a passage-level\nscore$\\unicode{x2014}$the Normalized Passage Attention\nScore$\\unicode{x2014}$used by our Attention-Variance Filter algorithm to\nidentify and filter potentially poisoned passages. This method mitigates\nexisting attacks, improving accuracy by up to $\\sim 20 \\%$ over baseline\ndefenses. To probe the limits of attention-based defenses, we craft stealthier\nadaptive attacks that obscure such traces, achieving up to $35 \\%$ attack\nsuccess rate, and highlight the challenges in improving stealth."
    },
    {
        "date": "2025-06",
        "title": "TRiSM for Agentic AI: A Review of Trust, Risk, and Security Management in LLM-based Agentic Multi-Agent Systems",
        "author": "Shaina Raza, Ranjan Sapkota, Manoj Karkee, and Christos Emmanouilidis",
        "link": "http://arxiv.org/abs/2506.04133v1",
        "abstract": "Agentic AI systems, built on large language models (LLMs) and deployed in\nmulti-agent configurations, are redefining intelligent autonomy, collaboration\nand decision-making across enterprise and societal domains. This review\npresents a structured analysis of Trust, Risk, and Security Management (TRiSM)\nin the context of LLM-based agentic multi-agent systems (AMAS). We begin by\nexamining the conceptual foundations of agentic AI, its architectural\ndifferences from traditional AI agents, and the emerging system designs that\nenable scalable, tool-using autonomy. The TRiSM in the agentic AI framework is\nthen detailed through four pillars governance, explainability, ModelOps, and\nprivacy/security each contextualized for agentic LLMs. We identify unique\nthreat vectors and introduce a comprehensive risk taxonomy for the agentic AI\napplications, supported by case studies illustrating real-world\nvulnerabilities. Furthermore, the paper also surveys trust-building mechanisms,\ntransparency and oversight techniques, and state-of-the-art explainability\nstrategies in distributed LLM agent systems. Additionally, metrics for\nevaluating trust, interpretability, and human-centered performance are reviewed\nalongside open benchmarking challenges. Security and privacy are addressed\nthrough encryption, adversarial defense, and compliance with evolving AI\nregulations. The paper concludes with a roadmap for responsible agentic AI,\nproposing research directions to align emerging multi-agent systems with robust\nTRiSM principles for safe, accountable, and transparent deployment."
    },
    {
        "date": "2025-06",
        "title": "Privacy and Security Threat for OpenAI GPTs",
        "author": "Wei Wenying, Zhao Kaifa, Xue Lei, and Fan Ming",
        "link": "http://arxiv.org/abs/2506.04036v1",
        "abstract": "Large language models (LLMs) demonstrate powerful information handling\ncapabilities and are widely integrated into chatbot applications. OpenAI\nprovides a platform for developers to construct custom GPTs, extending\nChatGPT's functions and integrating external services. Since its release in\nNovember 2023, over 3 million custom GPTs have been created. However, such a\nvast ecosystem also conceals security and privacy threats. For developers,\ninstruction leaking attacks threaten the intellectual property of instructions\nin custom GPTs through carefully crafted adversarial prompts. For users,\nunwanted data access behavior by custom GPTs or integrated third-party services\nraises significant privacy concerns. To systematically evaluate the scope of\nthreats in real-world LLM applications, we develop three phases instruction\nleaking attacks target GPTs with different defense level. Our widespread\nexperiments on 10,000 real-world custom GPTs reveal that over 98.8% of GPTs are\nvulnerable to instruction leaking attacks via one or more adversarial prompts,\nand half of the remaining GPTs can also be attacked through multiround\nconversations. We also developed a framework to assess the effectiveness of\ndefensive strategies and identify unwanted behaviors in custom GPTs. Our\nfindings show that 77.5% of custom GPTs with defense strategies are vulnerable\nto basic instruction leaking attacks. Additionally, we reveal that 738 custom\nGPTs collect user conversational information, and identified 8 GPTs exhibiting\ndata access behaviors that are unnecessary for their intended functionalities.\nOur findings raise awareness among GPT developers about the importance of\nintegrating specific defensive strategies in their instructions and highlight\nusers' concerns about data privacy when using LLM-based applications."
    },
    {
        "date": "2025-06",
        "title": "RAID: A Dataset for Testing the Adversarial Robustness of AI-Generated Image Detectors",
        "author": "Hicham Eddoubi, Jonas Ricker, Federico Cocchi, Lorenzo Baraldi, Angelo Sotgiu, Maura Pintor, Marcella Cornia, Lorenzo Baraldi, Asja Fischer, Rita Cucchiara, and Battista Biggio",
        "link": "http://arxiv.org/abs/2506.03988v3",
        "abstract": "AI-generated images have reached a quality level at which humans are\nincapable of reliably distinguishing them from real images. To counteract the\ninherent risk of fraud and disinformation, the detection of AI-generated images\nis a pressing challenge and an active research topic. While many of the\npresented methods claim to achieve high detection accuracy, they are usually\nevaluated under idealized conditions. In particular, the adversarial robustness\nis often neglected, potentially due to a lack of awareness or the substantial\neffort required to conduct a comprehensive robustness analysis. In this work,\nwe tackle this problem by providing a simpler means to assess the robustness of\nAI-generated image detectors. We present RAID (Robust evaluation of\nAI-generated image Detectors), a dataset of 72k diverse and highly transferable\nadversarial examples. The dataset is created by running attacks against an\nensemble of seven state-of-the-art detectors and images generated by four\ndifferent text-to-image models. Extensive experiments show that our methodology\ngenerates adversarial images that transfer with a high success rate to unseen\ndetectors, which can be used to quickly provide an approximate yet still\nreliable estimate of a detector's adversarial robustness. Our findings indicate\nthat current state-of-the-art AI-generated image detectors can be easily\ndeceived by adversarial examples, highlighting the critical need for the\ndevelopment of more robust methods. We release our dataset at\nhttps://huggingface.co/datasets/aimagelab/RAID and evaluation code at\nhttps://github.com/pralab/RAID."
    },
    {
        "date": "2025-06",
        "title": "Causality-Aware Contrastive Learning for Robust Multivariate Time-Series Anomaly Detection",
        "author": "HyunGi Kim, Jisoo Mok, Dongjun Lee, Jaihyun Lew, Sungjae Kim, and Sungroh Yoon",
        "link": "http://arxiv.org/abs/2506.03964v1",
        "abstract": "Utilizing the complex inter-variable causal relationships within multivariate\ntime-series provides a promising avenue toward more robust and reliable\nmultivariate time-series anomaly detection (MTSAD) but remains an underexplored\narea of research. This paper proposes Causality-Aware contrastive learning for\nRObust multivariate Time-Series (CAROTS), a novel MTSAD pipeline that\nincorporates the notion of causality into contrastive learning. CAROTS employs\ntwo data augmentors to obtain causality-preserving and -disturbing samples that\nserve as a wide range of normal variations and synthetic anomalies,\nrespectively. With causality-preserving and -disturbing samples as positives\nand negatives, CAROTS performs contrastive learning to train an encoder whose\nlatent space separates normal and abnormal samples based on causality.\nMoreover, CAROTS introduces a similarity-filtered one-class contrastive loss\nthat encourages the contrastive learning process to gradually incorporate more\nsemantically diverse samples with common causal relationships. Extensive\nexperiments on five real-world and two synthetic datasets validate that the\nintegration of causal relationships endows CAROTS with improved MTSAD\ncapabilities. The code is available at https://github.com/kimanki/CAROTS."
    },
    {
        "date": "2025-06",
        "title": "DiffCAP: Diffusion-based Cumulative Adversarial Purification for Vision Language Models",
        "author": "Jia Fu, Yongtao Wu, Yihang Chen, Kunyu Peng, Xiao Zhang, Volkan Cevher, Sepideh Pashami, and Anders Holst",
        "link": "http://arxiv.org/abs/2506.03933v1",
        "abstract": "Vision Language Models (VLMs) have shown remarkable capabilities in\nmultimodal understanding, yet their susceptibility to perturbations poses a\nsignificant threat to their reliability in real-world applications. Despite\noften being imperceptible to humans, these perturbations can drastically alter\nmodel outputs, leading to erroneous interpretations and decisions. This paper\nintroduces DiffCAP, a novel diffusion-based purification strategy that can\neffectively neutralize adversarial corruptions in VLMs. We observe that adding\nminimal noise to an adversarially corrupted image significantly alters its\nlatent embedding with respect to VLMs. Building on this insight, DiffCAP\ncumulatively injects random Gaussian noise into adversarially perturbed input\ndata. This process continues until the embeddings of two consecutive noisy\nimages reach a predefined similarity threshold, indicating a potential approach\nto neutralize the adversarial effect. Subsequently, a pretrained diffusion\nmodel is employed to denoise the stabilized image, recovering a clean\nrepresentation suitable for the VLMs to produce an output. Through extensive\nexperiments across six datasets with three VLMs under varying attack strengths\nin three task scenarios, we show that DiffCAP consistently outperforms existing\ndefense techniques by a substantial margin. Notably, DiffCAP significantly\nreduces both hyperparameter tuning complexity and the required diffusion time,\nthereby accelerating the denoising process. Equipped with strong theoretical\nand empirical support, DiffCAP provides a robust and practical solution for\nsecurely deploying VLMs in adversarial environments."
    },
    {
        "date": "2025-06",
        "title": "RadialRouter: Structured Representation for Efficient and Robust Large Language Models Routing",
        "author": "Ruihan Jin, Pengpeng Shao, Zhengqi Wen, Jinyang Wu, Mingkuan Feng, Shuai Zhang, and Jianhua Tao",
        "link": "http://arxiv.org/abs/2506.03880v1",
        "abstract": "The rapid advancements in large language models (LLMs) have led to the\nemergence of routing techniques, which aim to efficiently select the optimal\nLLM from diverse candidates to tackle specific tasks, optimizing performance\nwhile reducing costs. Current LLM routing methods are limited in effectiveness\ndue to insufficient exploration of the intrinsic connection between user\nqueries and the characteristics of LLMs. To address this issue, in this paper,\nwe present RadialRouter, a novel framework for LLM routing which employs a\nlightweight Transformer-based backbone with a radial structure named\nRadialFormer to articulate the query-LLMs relationship. The optimal LLM\nselection is performed based on the final states of RadialFormer. The pipeline\nis further refined by an objective function that combines Kullback-Leibler\ndivergence with the query-query contrastive loss to enhance robustness.\nExperimental results on RouterBench show that RadialRouter significantly\noutperforms existing routing methods by 9.2\\% and 5.8\\% in the Balance and Cost\nFirst scenarios, respectively. Additionally, its adaptability toward different\nperformance-cost trade-offs and the dynamic LLM pool demonstrates practical\napplication potential."
    },
    {
        "date": "2025-06",
        "title": "Evaluating Apple Intelligence's Writing Tools for Privacy Against Large Language Model-Based Inference Attacks: Insights from Early Datasets",
        "author": "Mohd. Farhan Israk Soumik, Syed Mhamudul Hasan, and Abdur R. Shahid",
        "link": "http://arxiv.org/abs/2506.03870v1",
        "abstract": "The misuse of Large Language Models (LLMs) to infer emotions from text for\nmalicious purposes, known as emotion inference attacks, poses a significant\nthreat to user privacy. In this paper, we investigate the potential of Apple\nIntelligence's writing tools, integrated across iPhone, iPad, and MacBook, to\nmitigate these risks through text modifications such as rewriting and tone\nadjustment. By developing early novel datasets specifically for this purpose,\nwe empirically assess how different text modifications influence LLM-based\ndetection. This capability suggests strong potential for Apple Intelligence's\nwriting tools as privacy-preserving mechanisms. Our findings lay the groundwork\nfor future adaptive rewriting systems capable of dynamically neutralizing\nsensitive emotional content to enhance user privacy. To the best of our\nknowledge, this research provides the first empirical analysis of Apple\nIntelligence's text-modification tools within a privacy-preservation context\nwith the broader goal of developing on-device, user-centric privacy-preserving\nmechanisms to protect against LLMs-based advanced inference attacks on deployed\nsystems."
    },
    {
        "date": "2025-06",
        "title": "Prediction Inconsistency Helps Achieve Generalizable Detection of Adversarial Examples",
        "author": "Sicong Han, Chenhao Lin, Zhengyu Zhao, Xiyuan Wang, Xinlei He, Qian Li, Cong Wang, Qian Wang, and Chao Shen",
        "link": "http://arxiv.org/abs/2506.03765v1",
        "abstract": "Adversarial detection protects models from adversarial attacks by refusing\nsuspicious test samples. However, current detection methods often suffer from\nweak generalization: their effectiveness tends to degrade significantly when\napplied to adversarially trained models rather than naturally trained ones, and\nthey generally struggle to achieve consistent effectiveness across both\nwhite-box and black-box attack settings. In this work, we observe that an\nauxiliary model, differing from the primary model in training strategy or model\narchitecture, tends to assign low confidence to the primary model's predictions\non adversarial examples (AEs), while preserving high confidence on normal\nexamples (NEs). Based on this discovery, we propose Prediction Inconsistency\nDetector (PID), a lightweight and generalizable detection framework to\ndistinguish AEs from NEs by capturing the prediction inconsistency between the\nprimal and auxiliary models. PID is compatible with both naturally and\nadversarially trained primal models and outperforms four detection methods\nacross 3 white-box, 3 black-box, and 1 mixed adversarial attacks. Specifically,\nPID achieves average AUC scores of 99.29\\% and 99.30\\% on CIFAR-10 when the\nprimal model is naturally and adversarially trained, respectively, and 98.31%\nand 96.81% on ImageNet under the same conditions, outperforming existing SOTAs\nby 4.70%$\\sim$25.46%."
    },
    {
        "date": "2025-06",
        "title": "Dropout-Robust Mechanisms for Differentially Private and Fully Decentralized Mean Estimation",
        "author": "C\u00e9sar Sabater, Sonia Ben Mokhtar, and Jan Ramon",
        "link": "http://arxiv.org/abs/2506.03746v1",
        "abstract": "Achieving differentially private computations in decentralized settings poses\nsignificant challenges, particularly regarding accuracy, communication cost,\nand robustness against information leakage. While cryptographic solutions offer\npromise, they often suffer from high communication overhead or require\ncentralization in the presence of network failures. Conversely, existing fully\ndecentralized approaches typically rely on relaxed adversarial models or\npairwise noise cancellation, the latter suffering from substantial accuracy\ndegradation if parties unexpectedly disconnect. In this work, we propose IncA,\na new protocol for fully decentralized mean estimation, a widely used primitive\nin data-intensive processing. Our protocol, which enforces differential\nprivacy, requires no central orchestration and employs low-variance correlated\nnoise, achieved by incrementally injecting sensitive information into the\ncomputation. First, we theoretically demonstrate that, when no parties\npermanently disconnect, our protocol achieves accuracy comparable to that of a\ncentralized setting-already an improvement over most existing decentralized\ndifferentially private techniques. Second, we empirically show that our use of\nlow-variance correlated noise significantly mitigates the accuracy loss\nexperienced by existing techniques in the presence of dropouts."
    },
    {
        "date": "2025-06",
        "title": "ComRoPE: Scalable and Robust Rotary Position Embedding Parameterized by Trainable Commuting Angle Matrices",
        "author": "Hao Yu, Tangyu Jiang, Shuning Jia, Shannan Yan, Shunning Liu, Haolong Qian, Guanghao Li, Shuting Dong, Huaisong Zhang, and Chun Yuan",
        "link": "http://arxiv.org/abs/2506.03737v1",
        "abstract": "The Transformer architecture has revolutionized various regions since it was\nproposed, and its effectiveness largely depends on the ability to encode\npositional information. Traditional position encoding methods exhibit\nsignificant limitations due to lack of robustness and flexibility of position.\nTherefore, Rotary Positional Encoding (RoPE) was proposed to alleviate these\nissues, which integrates positional information by rotating the embeddings in\nthe attention mechanism. However, RoPE requires manually defined rotation\nmatrices with limited transformation space, constraining the model's capacity.\nIn this work, we propose ComRoPE, which generalizes RoPE by defining it in\nterms of trainable commuting angle matrices. Specifically, we demonstrate that\npairwise commutativity of these matrices is essential for RoPE to achieve\nscalability and positional robustness. We formally define the RoPE Equation,\nwhich is an essential condition that ensures consistent performance with\nposition offsets. Based on the theoretical analysis, we present two types of\ntrainable commuting angle matrices as sufficient solutions to the RoPE\nequation, which significantly improve performance, surpassing the current\nstate-of-the-art method by 1.6% at training resolution and 2.9% at higher\nresolution on the ImageNet-1K dataset. Furthermore, our framework shows\nversatility in generalizing to existing RoPE formulations and offering new\ninsights for future positional encoding research. To ensure reproducibility,\nthe source code and instructions are available at\nhttps://github.com/Longin-Yu/ComRoPE"
    },
    {
        "date": "2025-06",
        "title": "BiXFormer: A Robust Framework for Maximizing Modality Effectiveness in Multi-Modal Semantic Segmentation",
        "author": "Jialei Chen, Xu Zheng, Danda Pani Paudel, Luc Van Gool, Hiroshi Murase, and Daisuke Deguchi",
        "link": "http://arxiv.org/abs/2506.03675v1",
        "abstract": "Utilizing multi-modal data enhances scene understanding by providing\ncomplementary semantic and geometric information. Existing methods fuse\nfeatures or distill knowledge from multiple modalities into a unified\nrepresentation, improving robustness but restricting each modality's ability to\nfully leverage its strengths in different situations. We reformulate\nmulti-modal semantic segmentation as a mask-level classification task and\npropose BiXFormer, which integrates Unified Modality Matching (UMM) and Cross\nModality Alignment (CMA) to maximize modality effectiveness and handle missing\nmodalities. Specifically, BiXFormer first categorizes multi-modal inputs into\nRGB and X, where X represents any non-RGB modalities, e.g., depth, allowing\nseparate processing for each. This design leverages the well-established\npretraining for RGB, while addressing the relative lack of attention to X\nmodalities. Then, we propose UMM, which includes Modality Agnostic Matching\n(MAM) and Complementary Matching (CM). MAM assigns labels to features from all\nmodalities without considering modality differences, leveraging each modality's\nstrengths. CM then reassigns unmatched labels to remaining unassigned features\nwithin their respective modalities, ensuring that each available modality\ncontributes to the final prediction and mitigating the impact of missing\nmodalities. Moreover, to further facilitate UMM, we introduce CMA, which\nenhances the weaker queries assigned in CM by aligning them with optimally\nmatched queries from MAM. Experiments on both synthetic and real-world\nmulti-modal benchmarks demonstrate the effectiveness of our method, achieving\nsignificant improvements in mIoU of +2.75% and +22.74% over the prior arts."
    },
    {
        "date": "2025-06",
        "title": "SubSearch: Robust Estimation and Outlier Detection for Stochastic Block Models via Subgraph Search",
        "author": "Leonardo Martins Bianco, Christine Keribin, and Zacharie Naulet",
        "link": "http://arxiv.org/abs/2506.03657v1",
        "abstract": "Community detection is a fundamental task in graph analysis, with methods\noften relying on fitting models like the Stochastic Block Model (SBM) to\nobserved networks. While many algorithms can accurately estimate SBM parameters\nwhen the input graph is a perfect sample from the model, real-world graphs\nrarely conform to such idealized assumptions. Therefore, robust algorithms are\ncrucial-ones that can recover model parameters even when the data deviates from\nthe assumed distribution. In this work, we propose SubSearch, an algorithm for\nrobustly estimating SBM parameters by exploring the space of subgraphs in\nsearch of one that closely aligns with the model's assumptions. Our approach\nalso functions as an outlier detection method, properly identifying nodes\nresponsible for the graph's deviation from the model and going beyond simple\ntechniques like pruning high-degree nodes. Extensive experiments on both\nsynthetic and real-world datasets demonstrate the effectiveness of our method."
    },
    {
        "date": "2025-06",
        "title": "Robustness of Prompting: Enhancing Robustness of Large Language Models Against Prompting Attacks",
        "author": "Lin Mu, Guowei Chu, Li Ni, Lei Sang, Zhize Wu, Peiquan Jin, and Yiwen Zhang",
        "link": "http://arxiv.org/abs/2506.03627v1",
        "abstract": "Large Language Models (LLMs) have demonstrated remarkable performance across\nvarious tasks by effectively utilizing a prompting strategy. However, they are\nhighly sensitive to input perturbations, such as typographical errors or slight\ncharacter order errors, which can substantially degrade their performance.\nDespite advances in prompting techniques, developing a prompting strategy that\nexplicitly mitigates the negative impact of such perturbations remains an open\nchallenge. To bridge this gap, we propose Robustness of Prompting (RoP), a\nnovel prompting strategy specifically designed to enhance the robustness of\nLLMs. RoP consists of two stages: Error Correction and Guidance. In the Error\nCorrection stage, RoP applies diverse perturbation methods to generate\nadversarial examples, which are then used to construct prompts that\nautomatically correct input errors. In the Guidance stage, RoP generates an\noptimal guidance prompting based on the corrected input, steering the model\ntoward more robust and accurate inferences. Through comprehensive experiments\nspanning arithmetic, commonsense, and logical reasoning tasks, we demonstrate\nthat RoP significantly improves LLMs' robustness against adversarial\nperturbations. Notably, it maintains model accuracy with only minimal\ndegradation compared to clean input scenarios, thereby establishing RoP as a\npractical and effective approach for enhancing LLM robustness in real-world\napplications."
    },
    {
        "date": "2025-06",
        "title": "SemiOccam: A Robust Semi-Supervised Image Recognition Network Using Sparse Labels",
        "author": "Rui Yann, and Xianglei Xing",
        "link": "http://arxiv.org/abs/2506.03582v2",
        "abstract": "We present SemiOccam, an image recognition network that leverages\nsemi-supervised learning in a highly efficient manner. Existing works often\nrely on complex training techniques and architectures, requiring hundreds of\nGPU hours for training, while their generalization ability when dealing with\nextremely limited labeled data remains to be improved. To address these\nlimitations, we construct a hierarchical mixture density classification\ndecision mechanism by optimizing mutual information between feature\nrepresentations and target classes, compressing redundant information while\nretaining crucial discriminative components. Experimental results demonstrate\nthat our method achieves state-of-the-art performance on various datasets when\nusing negligible labeled samples, and its simple architecture keeps training\ntime to minute-level. Notably, this paper reveals a long-overlooked data\nleakage issue in the STL-10 dataset for semi-supervised learning tasks and\nremoves duplicates to ensure the reliability of experimental results. We also\nrelease the deduplicated CleanSTL-10 dataset to facilitate fair and reliable\nresearch in future semi-supervised learning. Code available at\nhttps://github.com/Shu1L0n9/SemiOccam."
    },
    {
        "date": "2025-06",
        "title": "Poisoning Behavioral-based Worker Selection in Mobile Crowdsensing using Generative Adversarial Networks",
        "author": "Ruba Nasser, Ahmed Alagha, Shakti Singh, Rabeb Mizouni, Hadi Otrok, and Jamal Bentahar",
        "link": "http://arxiv.org/abs/2506.05403v1",
        "abstract": "With the widespread adoption of Artificial intelligence (AI), AI-based tools\nand components are becoming omnipresent in today's solutions. However, these\ncomponents and tools are posing a significant threat when it comes to\nadversarial attacks. Mobile Crowdsensing (MCS) is a sensing paradigm that\nleverages the collective participation of workers and their smart devices to\ncollect data. One of the key challenges faced at the selection stage is\nensuring task completion due to workers' varying behavior. AI has been utilized\nto tackle this challenge by building unique models for each worker to predict\ntheir behavior. However, the integration of AI into the system introduces\nvulnerabilities that can be exploited by malicious insiders to reduce the\nrevenue obtained by victim workers. This work proposes an adversarial attack\ntargeting behavioral-based selection models in MCS. The proposed attack\nleverages Generative Adversarial Networks (GANs) to generate poisoning points\nthat can mislead the models during the training stage without being detected.\nThis way, the potential damage introduced by GANs on worker selection in MCS\ncan be anticipated. Simulation results using a real-life dataset show the\neffectiveness of the proposed attack in compromising the victim workers' model\nand evading detection by an outlier detector, compared to a benchmark. In\naddition, the impact of the attack on reducing the payment obtained by victim\nworkers is evaluated."
    },
    {
        "date": "2025-06",
        "title": "Quantum Secure Key Exchange with Position-based Credentials",
        "author": "Wen Yu Kon, Ignatius William Primaatmaja, Kaushik Chakraborty, and Charles Lim",
        "link": "http://arxiv.org/abs/2506.03549v1",
        "abstract": "Quantum key distribution (QKD) provides an information-theoretic way of\nsecurely exchanging secret keys, and typically relies on pre-shared keys or\npublic keys for message authentication. To lift the requirement of pre-shared\nor public keys, Buhrman et. al. [SIAM J. Comput. 43, 150 (2014)] proposed\nutilizing the location of a party as a credential. Here, we extend upon the\nproposal, develop a QKD protocol with location credentials using quantum\nposition verification (QPV) based message and identity authentication. By using\nQKD with delayed authentication as a base, and later simplifying QPV-based\nmessage authentication, we significantly reduce the number of QPV runs, which\ncurrently acts as a bottleneck. Besides demonstrating security for the proposed\nprotocol, we also provide improvements to QPV security analysis, including\ngeneralization of the QPV adversary model, tightening a trace distance bound\nusing semidefinite programming, and propose a multi-basis QPV requiring only\nBB84 state preparation but with multiple measurement basis."
    },
    {
        "date": "2025-06",
        "title": "Robust Neural Rendering in the Wild with Asymmetric Dual 3D Gaussian Splatting",
        "author": "Chengqi Li, Zhihao Shi, Yangdi Lu, Wenbo He, and Xiangyu Xu",
        "link": "http://arxiv.org/abs/2506.03538v1",
        "abstract": "3D reconstruction from in-the-wild images remains a challenging task due to\ninconsistent lighting conditions and transient distractors. Existing methods\ntypically rely on heuristic strategies to handle the low-quality training data,\nwhich often struggle to produce stable and consistent reconstructions,\nfrequently resulting in visual artifacts. In this work, we propose Asymmetric\nDual 3DGS, a novel framework that leverages the stochastic nature of these\nartifacts: they tend to vary across different training runs due to minor\nrandomness. Specifically, our method trains two 3D Gaussian Splatting (3DGS)\nmodels in parallel, enforcing a consistency constraint that encourages\nconvergence on reliable scene geometry while suppressing inconsistent\nartifacts. To prevent the two models from collapsing into similar failure modes\ndue to confirmation bias, we introduce a divergent masking strategy that\napplies two complementary masks: a multi-cue adaptive mask and a\nself-supervised soft mask, which leads to an asymmetric training process of the\ntwo models, reducing shared error modes. In addition, to improve the efficiency\nof model training, we introduce a lightweight variant called Dynamic EMA Proxy,\nwhich replaces one of the two models with a dynamically updated Exponential\nMoving Average (EMA) proxy, and employs an alternating masking strategy to\npreserve divergence. Extensive experiments on challenging real-world datasets\ndemonstrate that our method consistently outperforms existing approaches while\nachieving high efficiency. Codes and trained models will be released."
    },
    {
        "date": "2025-06",
        "title": "Sylva: Tailoring Personalized Adversarial Defense in Pre-trained Models via Collaborative Fine-tuning",
        "author": "Tianyu Qi, Lei Xue, Yufeng Zhan, and Xiaobo Ma",
        "link": "http://arxiv.org/abs/2506.05402v1",
        "abstract": "The growing adoption of large pre-trained models in edge computing has made\ndeploying model inference on mobile clients both practical and popular. These\ndevices are inherently vulnerable to direct adversarial attacks, which pose a\nsubstantial threat to the robustness and security of deployed models. Federated\nadversarial training (FAT) has emerged as an effective solution to enhance\nmodel robustness while preserving client privacy. However, FAT frequently\nproduces a generalized global model, which struggles to address the diverse and\nheterogeneous data distributions across clients, resulting in insufficiently\npersonalized performance, while also encountering substantial communication\nchallenges during the training process. In this paper, we propose\n\\textit{Sylva}, a personalized collaborative adversarial training framework\ndesigned to deliver customized defense models for each client through a\ntwo-phase process. In Phase 1, \\textit{Sylva} employs LoRA for local\nadversarial fine-tuning, enabling clients to personalize model robustness while\ndrastically reducing communication costs by uploading only LoRA parameters\nduring federated aggregation. In Phase 2, a game-based layer selection strategy\nis introduced to enhance accuracy on benign data, further refining the\npersonalized model. This approach ensures that each client receives a tailored\ndefense model that balances robustness and accuracy effectively. Extensive\nexperiments on benchmark datasets demonstrate that \\textit{Sylva} can achieve\nup to 50$\\times$ improvements in communication efficiency compared to\nstate-of-the-art algorithms, while achieving up to 29.5\\% and 50.4\\%\nenhancements in adversarial robustness and benign accuracy, respectively."
    },
    {
        "date": "2025-06",
        "title": "Target Semantics Clustering via Text Representations for Robust Universal Domain Adaptation",
        "author": "Weinan He, Zilei Wang, and Yixin Zhang",
        "link": "http://arxiv.org/abs/2506.03521v1",
        "abstract": "Universal Domain Adaptation (UniDA) focuses on transferring source domain\nknowledge to the target domain under both domain shift and unknown category\nshift. Its main challenge lies in identifying common class samples and aligning\nthem. Current methods typically obtain target domain semantics centers from an\nunconstrained continuous image representation space. Due to domain shift and\nthe unknown number of clusters, these centers often result in complex and less\nrobust alignment algorithm. In this paper, based on vision-language models, we\nsearch for semantic centers in a semantically meaningful and discrete text\nrepresentation space. The constrained space ensures almost no domain bias and\nappropriate semantic granularity for these centers, enabling a simple and\nrobust adaptation algorithm. Specifically, we propose TArget Semantics\nClustering (TASC) via Text Representations, which leverages information\nmaximization as a unified objective and involves two stages. First, with the\nfrozen encoders, a greedy search-based framework is used to search for an\noptimal set of text embeddings to represent target semantics. Second, with the\nsearch results fixed, encoders are refined based on gradient descent,\nsimultaneously achieving robust domain alignment and private class clustering.\nAdditionally, we propose Universal Maximum Similarity (UniMS), a scoring\nfunction tailored for detecting open-set samples in UniDA. Experimentally, we\nevaluate the universality of UniDA algorithms under four category shift\nscenarios. Extensive experiments on four benchmarks demonstrate the\neffectiveness and robustness of our method, which has achieved state-of-the-art\nperformance."
    },
    {
        "date": "2025-06",
        "title": "Software Bill of Materials in Software Supply Chain Security A Systematic Literature Review",
        "author": "Eric O'Donoghue, Yvette Hastings, Ernesto Ortiz, and A. Redempta Manzi Muneza",
        "link": "http://arxiv.org/abs/2506.03507v2",
        "abstract": "Software Bill of Materials (SBOMs) are increasingly regarded as essential\ntools for securing software supply chains (SSCs), yet their real-world use and\nadoption barriers remain poorly understood. This systematic literature review\nsynthesizes evidence from 40 peer-reviewed studies to evaluate how SBOMs are\ncurrently used to bolster SSC security. We identify five primary application\nareas: vulnerability management, transparency, component assessment, risk\nassessment, and SSC integrity. Despite clear promise, adoption is hindered by\nsignificant barriers: generation tooling, data privacy, format/standardization,\nsharing/distribution, cost/overhead, vulnerability exploitability, maintenance,\nanalysis tooling, false positives, hidden packages, and tampering. To structure\nour analysis, we map these barriers to the ISO/IEC 25019:2023 Quality-in-Use\nmodel, revealing critical deficiencies in SBOM trustworthiness, usability, and\nsuitability for security tasks. We also highlight key gaps in the literature.\nThese include the absence of applying machine learning techniques to assess\nSBOMs and limited evaluation of SBOMs and SSCs using software quality assurance\ntechniques. Our findings provide actionable insights for researchers, tool\ndevelopers, and practitioners seeking to advance SBOM-driven SSC security and\nlay a foundation for future work at the intersection of SSC assurance,\nautomation, and empirical software engineering."
    },
    {
        "date": "2025-06",
        "title": "Robust Anti-Backdoor Instruction Tuning in LVLMs",
        "author": "Yuan Xun, Siyuan Liang, Xiaojun Jia, Xinwei Liu, and Xiaochun Cao",
        "link": "http://arxiv.org/abs/2506.05401v1",
        "abstract": "Large visual language models (LVLMs) have demonstrated excellent\ninstruction-following capabilities, yet remain vulnerable to stealthy backdoor\nattacks when finetuned using contaminated data. Existing backdoor defense\ntechniques are usually developed for single-modal visual or language models\nunder fully parameter-adjustable settings or rely on supervisory knowledge\nduring training. However, in real-world scenarios, defenders cannot modify\nfrozen visual encoders or core LLM parameters, nor possess prior knowledge of\nunknown trigger patterns or target responses. Motivated by the empirical\nfinding that LVLMs readily overfit to fixed, unknown triggers, which can embed\nmalicious associations during adapter-level tuning, we aim to design a defense\nthat operates without access to core weights or attack priors. To this end, we\nintroduce a lightweight, certified-agnostic defense framework, Robust\nInstruction Tuning, that finetunes only adapter modules and text embedding\nlayers under instruction tuning. Our method integrates two complementary\nregularizations: (1) Input Diversity Regularization, which perturbs trigger\ncomponents across training samples to disrupt consistent spurious cues; and (2)\nAnomalous Activation Regularization, which dynamically sparses adapter weights\nexhibiting abnormally sharp activations linked to backdoor patterns. These\nmechanisms jointly guide the model toward learning semantically grounded\nrepresentations rather than memorizing superficial trigger-response mappings.\n  Extensive experiments against seven attacks on Flickr30k and MSCOCO\ndemonstrate that ours\n  reduces their attack success rate to nearly zero, with an increase in\ntraining cost of less than 15%."
    },
    {
        "date": "2025-06",
        "title": "RoNFA: Robust Neural Field-based Approach for Few-Shot Image Classification with Noisy Labels",
        "author": "Nan Xiang, Lifeng Xing, and Dequan Jin",
        "link": "http://arxiv.org/abs/2506.03461v1",
        "abstract": "In few-shot learning (FSL), the labeled samples are scarce. Thus, label\nerrors can significantly reduce classification accuracy. Since label errors are\ninevitable in realistic learning tasks, improving the robustness of the model\nin the presence of label errors is critical. This paper proposes a new robust\nneural field-based image approach (RoNFA) for few-shot image classification\nwith noisy labels. RoNFA consists of two neural fields for feature and category\nrepresentation. They correspond to the feature space and category set. Each\nneuron in the field for category representation (FCR) has a receptive field\n(RF) on the field for feature representation (FFR) centered at the\nrepresentative neuron for its category generated by soft clustering. In the\nprediction stage, the range of these receptive fields adapts according to the\nneuronal activation in FCR to ensure prediction accuracy. These learning\nstrategies provide the proposed model with excellent few-shot learning\ncapability and strong robustness against label noises. The experimental results\non real-world FSL datasets with three different types of label noise\ndemonstrate that the proposed method significantly outperforms state-of-the-art\nFSL methods. Its accuracy obtained in the presence of noisy labels even\nsurpasses the results obtained by state-of-the-art FSL methods trained on clean\nsupport sets, indicating its strong robustness against noisy labels."
    },
    {
        "date": "2025-06",
        "title": "Robustness in Both Domains: CLIP Needs a Robust Text Encoder",
        "author": "Elias Abad Rocamora, Christian Schlarmann, Naman Deep Singh, Yongtao Wu, Matthias Hein, and Volkan Cevher",
        "link": "http://arxiv.org/abs/2506.03355v1",
        "abstract": "Adversarial input attacks can cause a significant shift of CLIP embeddings.\nThis can affect the downstream robustness of models incorporating CLIP in the\npipeline, such as text-to-image generative models or large vision language\nmodels. While some efforts have been done towards making the CLIP image\nencoders robust, the robustness of text encoders remains unexplored. In this\nwork, we cover this gap in the literature. We propose LEAF: an efficient\nadversarial finetuning method for the text domain, with the ability to scale to\nlarge CLIP models. Our models significantly improve the zero-shot adversarial\naccuracy in the text domain, while maintaining the vision performance provided\nby robust image encoders. When combined with text-to-image diffusion models, we\ncan improve the generation quality under adversarial noise. When employing our\nrobust CLIP encoders in multimodal retrieval tasks, we improve the recall under\nadversarial noise over standard CLIP models. Finally, we show that robust text\nencoders facilitate better reconstruction of input text from its embedding via\ndirect optimization."
    },
    {
        "date": "2025-06",
        "title": "Adversarial Attacks on Robotic Vision Language Action Models",
        "author": "Eliot Krzysztof Jones, Alexander Robey, Andy Zou, Zachary Ravichandran, George J. Pappas, Hamed Hassani, Matt Fredrikson, and J. Zico Kolter",
        "link": "http://arxiv.org/abs/2506.03350v1",
        "abstract": "The emergence of vision-language-action models (VLAs) for end-to-end control\nis reshaping the field of robotics by enabling the fusion of multimodal sensory\ninputs at the billion-parameter scale. The capabilities of VLAs stem primarily\nfrom their architectures, which are often based on frontier large language\nmodels (LLMs). However, LLMs are known to be susceptible to adversarial misuse,\nand given the significant physical risks inherent to robotics, questions remain\nregarding the extent to which VLAs inherit these vulnerabilities. Motivated by\nthese concerns, in this work we initiate the study of adversarial attacks on\nVLA-controlled robots. Our main algorithmic contribution is the adaptation and\napplication of LLM jailbreaking attacks to obtain complete control authority\nover VLAs. We find that textual attacks, which are applied once at the\nbeginning of a rollout, facilitate full reachability of the action space of\ncommonly used VLAs and often persist over longer horizons. This differs\nsignificantly from LLM jailbreaking literature, as attacks in the real world do\nnot have to be semantically linked to notions of harm. We make all code\navailable at https://github.com/eliotjones1/robogcg ."
    },
    {
        "date": "2025-06",
        "title": "Attacking Attention of Foundation Models Disrupts Downstream Tasks",
        "author": "Hondamunige Prasanna Silva, Federico Becattini, and Lorenzo Seidenari",
        "link": "http://arxiv.org/abs/2506.05394v2",
        "abstract": "Foundation models represent the most prominent and recent paradigm shift in\nartificial intelligence. Foundation models are large models, trained on broad\ndata that deliver high accuracy in many downstream tasks, often without\nfine-tuning. For this reason, models such as CLIP , DINO or Vision Transfomers\n(ViT), are becoming the bedrock of many industrial AI-powered applications.\nHowever, the reliance on pre-trained foundation models also introduces\nsignificant security concerns, as these models are vulnerable to adversarial\nattacks. Such attacks involve deliberately crafted inputs designed to deceive\nAI systems, jeopardizing their reliability. This paper studies the\nvulnerabilities of vision foundation models, focusing specifically on CLIP and\nViTs, and explores the transferability of adversarial attacks to downstream\ntasks. We introduce a novel attack, targeting the structure of\ntransformer-based architectures in a task-agnostic fashion. We demonstrate the\neffectiveness of our attack on several downstream tasks: classification,\ncaptioning, image/text retrieval, segmentation and depth estimation. Code\navailable at:https://github.com/HondamunigePrasannaSilva/attack-attention"
    },
    {
        "date": "2025-06",
        "title": "Explicitly Modeling Subcortical Vision with a Neuro-Inspired Front-End Improves CNN Robustness",
        "author": "Lucas Piper, Arlindo L. Oliveira, and Tiago Marques",
        "link": "http://arxiv.org/abs/2506.03089v1",
        "abstract": "Convolutional neural networks (CNNs) trained on object recognition achieve\nhigh task performance but continue to exhibit vulnerability under a range of\nvisual perturbations and out-of-domain images, when compared with biological\nvision. Prior work has demonstrated that coupling a standard CNN with a\nfront-end block (VOneBlock) that mimics the primate primary visual cortex (V1)\ncan improve overall model robustness. Expanding on this, we introduce Early\nVision Networks (EVNets), a new class of hybrid CNNs that combine the VOneBlock\nwith a novel SubcorticalBlock, whose architecture draws from computational\nmodels in neuroscience and is parameterized to maximize alignment with\nsubcortical responses reported across multiple experimental studies. Without\nbeing optimized to do so, the assembly of the SubcorticalBlock with the\nVOneBlock improved V1 alignment across most standard V1 benchmarks, and better\nmodeled extra-classical receptive field phenomena. In addition, EVNets exhibit\nstronger emergent shape bias and overperform the base CNN architecture by 8.5%\non an aggregate benchmark of robustness evaluations, including adversarial\nperturbations, common corruptions, and domain shifts. Finally, we show that\nEVNets can be further improved when paired with a state-of-the-art data\naugmentation technique, surpassing the performance of the isolated data\naugmentation approach by 7.3% on our robustness benchmark. This result reveals\ncomplementary benefits between changes in architecture to better mimic biology\nand training-based machine learning approaches."
    },
    {
        "date": "2025-06",
        "title": "On the Benefits of Accelerated Optimization in Robust and Private Estimation",
        "author": "Laurentiu Andrei Marchis, and Po-Ling Loh",
        "link": "http://arxiv.org/abs/2506.03044v1",
        "abstract": "We study the advantages of accelerated gradient methods, specifically based\non the Frank-Wolfe method and projected gradient descent, for privacy and\nheavy-tailed robustness. Our approaches are as follows: For the Frank-Wolfe\nmethod, our technique is based on a tailored learning rate and a uniform lower\nbound on the gradient of the $\\ell_2$-norm over the constraint set. For\naccelerating projected gradient descent, we use the popular variant based on\nNesterov's momentum, and we optimize our objective over $\\mathbb{R}^p$. These\naccelerations reduce iteration complexity, translating into stronger\nstatistical guarantees for empirical and population risk minimization. Our\nanalysis covers three settings: non-random data, random model-free data, and\nparametric models (linear regression and generalized linear models).\nMethodologically, we approach both privacy and robustness based on noisy\ngradients. We ensure differential privacy via the Gaussian mechanism and\nadvanced composition, and we achieve heavy-tailed robustness using a geometric\nmedian-of-means estimator, which also sharpens the dependency on the dimension\nof the covariates. Finally, we compare our rates to existing bounds and\nidentify scenarios where our methods attain optimal convergence."
    },
    {
        "date": "2025-06",
        "title": "On the Robustness of Tabular Foundation Models: Test-Time Attacks and In-Context Defenses",
        "author": "Mohamed Djilani, Thibault Simonetto, Karim Tit, Florian Tambon, Paul R\u00e9camier, Salah Ghamizi, Maxime Cordy, and Mike Papadakis",
        "link": "http://arxiv.org/abs/2506.02978v1",
        "abstract": "Recent tabular Foundational Models (FM) such as TabPFN and TabICL, leverage\nin-context learning to achieve strong performance without gradient updates or\nfine-tuning. However, their robustness to adversarial manipulation remains\nlargely unexplored. In this work, we present a comprehensive study of the\nadversarial vulnerabilities of tabular FM, focusing on both their fragility to\ntargeted test-time attacks and their potential misuse as adversarial tools. We\nshow on three benchmarks in finance, cybersecurity and healthcare, that small,\nstructured perturbations to test inputs can significantly degrade prediction\naccuracy, even when training context remain fixed. Additionally, we demonstrate\nthat tabular FM can be repurposed to generate transferable evasion to\nconventional models such as random forests and XGBoost, and on a lesser extent\nto deep tabular models. To improve tabular FM, we formulate the robustification\nproblem as an optimization of the weights (adversarial fine-tuning), or the\ncontext (adversarial in-context learning). We introduce an in-context\nadversarial training strategy that incrementally replaces the context with\nadversarial perturbed instances, without updating model weights. Our approach\nimproves robustness across multiple tabular benchmarks. Together, these\nfindings position tabular FM as both a target and a source of adversarial\nthreats, highlighting the urgent need for robust training and evaluation\npractices in this emerging paradigm."
    },
    {
        "date": "2025-06",
        "title": "ATAG: AI-Agent Application Threat Assessment with Attack Graphs",
        "author": "Parth Atulbhai Gandhi, Akansha Shukla, David Tayouri, Beni Ifland, Yuval Elovici, Rami Puzis, and Asaf Shabtai",
        "link": "http://arxiv.org/abs/2506.02859v1",
        "abstract": "Evaluating the security of multi-agent systems (MASs) powered by large\nlanguage models (LLMs) is challenging, primarily because of the systems'\ncomplex internal dynamics and the evolving nature of LLM vulnerabilities.\nTraditional attack graph (AG) methods often lack the specific capabilities to\nmodel attacks on LLMs. This paper introduces AI-agent application Threat\nassessment with Attack Graphs (ATAG), a novel framework designed to\nsystematically analyze the security risks associated with AI-agent\napplications. ATAG extends the MulVAL logic-based AG generation tool with\ncustom facts and interaction rules to accurately represent AI-agent topologies,\nvulnerabilities, and attack scenarios. As part of this research, we also\ncreated the LLM vulnerability database (LVD) to initiate the process of\nstandardizing LLM vulnerabilities documentation. To demonstrate ATAG's\nefficacy, we applied it to two multi-agent applications. Our case studies\ndemonstrated the framework's ability to model and generate AGs for\nsophisticated, multi-step attack scenarios exploiting vulnerabilities such as\nprompt injection, excessive agency, sensitive information disclosure, and\ninsecure output handling across interconnected agents. ATAG is an important\nstep toward a robust methodology and toolset to help understand, visualize, and\nprioritize complex attack paths in multi-agent AI systems (MAASs). It\nfacilitates proactive identification and mitigation of AI-agent threats in\nmulti-agent applications."
    },
    {
        "date": "2025-06",
        "title": "Enhancing Abnormality Identification: Robust Out-of-Distribution Strategies for Deepfake Detection",
        "author": "Luca Maiano, Fabrizio Casadei, and Irene Amerini",
        "link": "http://arxiv.org/abs/2506.02857v1",
        "abstract": "Detecting deepfakes has become a critical challenge in Computer Vision and\nArtificial Intelligence. Despite significant progress in detection techniques,\ngeneralizing them to open-set scenarios continues to be a persistent\ndifficulty. Neural networks are often trained on the closed-world assumption,\nbut with new generative models constantly evolving, it is inevitable to\nencounter data generated by models that are not part of the training\ndistribution. To address these challenges, in this paper, we propose two novel\nOut-Of-Distribution (OOD) detection approaches. The first approach is trained\nto reconstruct the input image, while the second incorporates an attention\nmechanism for detecting OODs. Our experiments validate the effectiveness of the\nproposed approaches compared to existing state-of-the-art techniques. Our\nmethod achieves promising results in deepfake detection and ranks among the\ntop-performing configurations on the benchmark, demonstrating their potential\nfor robust, adaptable solutions in dynamic, real-world applications."
    },
    {
        "date": "2025-06",
        "title": "Doubly-Robust Estimation of Counterfactual Policy Mean Embeddings",
        "author": "Houssam Zenati, Bariscan Bozkurt, and Arthur Gretton",
        "link": "http://arxiv.org/abs/2506.02793v1",
        "abstract": "Estimating the distribution of outcomes under counterfactual policies is\ncritical for decision-making in domains such as recommendation, advertising,\nand healthcare. We analyze a novel framework-Counterfactual Policy Mean\nEmbedding (CPME)-that represents the entire counterfactual outcome distribution\nin a reproducing kernel Hilbert space (RKHS), enabling flexible and\nnonparametric distributional off-policy evaluation. We introduce both a plug-in\nestimator and a doubly robust estimator; the latter enjoys improved uniform\nconvergence rates by correcting for bias in both the outcome embedding and\npropensity models. Building on this, we develop a doubly robust kernel test\nstatistic for hypothesis testing, which achieves asymptotic normality and thus\nenables computationally efficient testing and straightforward construction of\nconfidence intervals. Our framework also supports sampling from the\ncounterfactual distribution. Numerical simulations illustrate the practical\nbenefits of CPME over existing methods."
    },
    {
        "date": "2025-06",
        "title": "Privacy Leaks by Adversaries: Adversarial Iterations for Membership Inference Attack",
        "author": "Jing Xue, Zhishen Sun, Haishan Ye, Luo Luo, Xiangyu Chang, Ivor Tsang, and Guang Dai",
        "link": "http://arxiv.org/abs/2506.02711v1",
        "abstract": "Membership inference attack (MIA) has become one of the most widely used and\neffective methods for evaluating the privacy risks of machine learning models.\nThese attacks aim to determine whether a specific sample is part of the model's\ntraining set by analyzing the model's output. While traditional membership\ninference attacks focus on leveraging the model's posterior output, such as\nconfidence on the target sample, we propose IMIA, a novel attack strategy that\nutilizes the process of generating adversarial samples to infer membership. We\npropose to infer the member properties of the target sample using the number of\niterations required to generate its adversarial sample. We conduct experiments\nacross multiple models and datasets, and our results demonstrate that the\nnumber of iterations for generating an adversarial sample is a reliable feature\nfor membership inference, achieving strong performance both in black-box and\nwhite-box attack scenarios. This work provides a new perspective for evaluating\nmodel privacy and highlights the potential of adversarial example-based\nfeatures for privacy leakage assessment."
    },
    {
        "date": "2025-06",
        "title": "Poster: FedBlockParadox -- A Framework for Simulating and Securing Decentralized Federated Learning",
        "author": "Gabriele Digregorio, Francesco Bleggi, Federico Caroli, Michele Carminati, Stefano Zanero, and Stefano Longari",
        "link": "http://arxiv.org/abs/2506.02679v1",
        "abstract": "A significant body of research in decentralized federated learning focuses on\ncombining the privacy-preserving properties of federated learning with the\nresilience and transparency offered by blockchain-based systems. While these\napproaches are promising, they often lack flexible tools to evaluate system\nrobustness under adversarial conditions. To fill this gap, we present\nFedBlockParadox, a modular framework for modeling and evaluating decentralized\nfederated learning systems built on blockchain technologies, with a focus on\nresilience against a broad spectrum of adversarial attack scenarios. It\nsupports multiple consensus protocols, validation methods, aggregation\nstrategies, and configurable attack models. By enabling controlled experiments,\nFedBlockParadox provides a valuable resource for researchers developing secure,\ndecentralized learning solutions. The framework is open-source and built to be\nextensible by the community."
    },
    {
        "date": "2025-06",
        "title": "Beyond Invisibility: Learning Robust Visible Watermarks for Stronger Copyright Protection",
        "author": "Tianci Liu, Tong Yang, Quan Zhang, and Qi Lei",
        "link": "http://arxiv.org/abs/2506.02665v1",
        "abstract": "As AI advances, copyrighted content faces growing risk of unauthorized use,\nwhether through model training or direct misuse. Building upon invisible\nadversarial perturbation, recent works developed copyright protections against\nspecific AI techniques such as unauthorized personalization through DreamBooth\nthat are misused. However, these methods offer only short-term security, as\nthey require retraining whenever the underlying model architectures change. To\nestablish long-term protection aiming at better robustness, we go beyond\ninvisible perturbation, and propose a universal approach that embeds\n\\textit{visible} watermarks that are \\textit{hard-to-remove} into images.\nGrounded in a new probabilistic and inverse problem-based formulation, our\nframework maximizes the discrepancy between the \\textit{optimal} reconstruction\nand the original content. We develop an effective and efficient approximation\nalgorithm to circumvent a intractable bi-level optimization. Experimental\nresults demonstrate superiority of our approach across diverse scenarios."
    },
    {
        "date": "2025-06",
        "title": "How stealthy is stealthy? Studying the Efficacy of Black-Box Adversarial Attacks in the Real World",
        "author": "Francesco Panebianco, Mario D'Onghia, and Stefano Zanero aand Michele Carminati",
        "link": "http://arxiv.org/abs/2506.05382v1",
        "abstract": "Deep learning systems, critical in domains like autonomous vehicles, are\nvulnerable to adversarial examples (crafted inputs designed to mislead\nclassifiers). This study investigates black-box adversarial attacks in computer\nvision. This is a realistic scenario, where attackers have query-only access to\nthe target model. Three properties are introduced to evaluate attack\nfeasibility: robustness to compression, stealthiness to automatic detection,\nand stealthiness to human inspection. State-of-the-Art methods tend to\nprioritize one criterion at the expense of others. We propose ECLIPSE, a novel\nattack method employing Gaussian blurring on sampled gradients and a local\nsurrogate model. Comprehensive experiments on a public dataset highlight\nECLIPSE's advantages, demonstrating its contribution to the trade-off between\nthe three properties."
    },
    {
        "date": "2025-06",
        "title": "EALG: Evolutionary Adversarial Generation of Language Model-Guided Generators for Combinatorial Optimization",
        "author": "Ruibo Duan, Yuxin Liu, Xinyao Dong, and Chenglin Fan",
        "link": "http://arxiv.org/abs/2506.02594v1",
        "abstract": "Generating challenging instances is crucial for the evaluation and\nadvancement of combinatorial optimization solvers. In this work, we introduce\nEALG (Evolutionary Adversarial Generation of Language Model-Guided Generators),\na novel framework that automates the co-evolution of optimization problem\ninstances and their corresponding heuristic solvers using large language models\n(LLMs). EALG leverages a mutation-based adversarial approach that dynamically\nevolves instance generation procedures to create increasingly difficult\nproblems, while simultaneously synthesizing adaptive heuristic algorithms\nthrough interactions with LLMs guided by algorithmic structure. Unlike existing\napproaches that focus solely on static benchmark creation or manual solver\ndesign, EALG provides a seamless pipeline from instance generation to solver\nsynthesis. Experimental results demonstrate that EALG generates significantly\nharder instances than current benchmarks, and its synthesized solvers\ngeneralize effectively across a broad spectrum of combinatorial tasks. This\nwork explores a new paradigm for combinatorial optimization that integrates\ninstance generation with solver design, resulting in state-of-the-art\nperformance."
    },
    {
        "date": "2025-06",
        "title": "VerificAgent: Integrating Expert Knowledge and Fact-Checked Memory for Robust Domain-Specific Task Planning",
        "author": "Thong Q. Nguyen, Shubhang Desai, Yash Jain, Tanvir Aumi, and Vishal Chowdhary",
        "link": "http://arxiv.org/abs/2506.02539v1",
        "abstract": "Continual memory augmentation allows computer-use agents (CUAs) to learn from\npast interactions and refine their task-solving strategies over time. However,\nunchecked memory accumulation can introduce spurious or hallucinated\n\"learnings\" that degrade agent performance, particularly in domain-specific\nworkflows such as productivity software. We present a novel framework,\nVerificAgent, that effectively manages memory for CUAs through (1) an\nexpert-curated seed of domain knowledge, (2) iterative, trajectory-based memory\nrefinement during training, and (3) a post-hoc fact-checking pass by human\nexperts to sanitize accumulated memory before deployment. On OSWorld\nproductivity tasks, VerificAgent achieves a 111.1% relative improvement in\nsuccess rate over baseline CUA without any additional fine-tuning."
    },
    {
        "date": "2025-06",
        "title": "VPI-Bench: Visual Prompt Injection Attacks for Computer-Use Agents",
        "author": "Tri Cao, Bennett Lim, Yue Liu, Yuan Sui, Yuexin Li, Shumin Deng, Lin Lu, Nay Oo, Shuicheng Yan, and Bryan Hooi",
        "link": "http://arxiv.org/abs/2506.02456v1",
        "abstract": "Computer-Use Agents (CUAs) with full system access enable powerful task\nautomation but pose significant security and privacy risks due to their ability\nto manipulate files, access user data, and execute arbitrary commands. While\nprior work has focused on browser-based agents and HTML-level attacks, the\nvulnerabilities of CUAs remain underexplored. In this paper, we investigate\nVisual Prompt Injection (VPI) attacks, where malicious instructions are\nvisually embedded within rendered user interfaces, and examine their impact on\nboth CUAs and Browser-Use Agents (BUAs). We propose VPI-Bench, a benchmark of\n306 test cases across five widely used platforms, to evaluate agent robustness\nunder VPI threats. Each test case is a variant of a web platform, designed to\nbe interactive, deployed in a realistic environment, and containing a visually\nembedded malicious prompt. Our empirical study shows that current CUAs and BUAs\ncan be deceived at rates of up to 51% and 100%, respectively, on certain\nplatforms. The experimental results also indicate that system prompt defenses\noffer only limited improvements. These findings highlight the need for robust,\ncontext-aware defenses to ensure the safe deployment of multimodal AI agents in\nreal-world environments. The code and dataset are available at:\nhttps://github.com/cua-framework/agents"
    },
    {
        "date": "2025-06",
        "title": "Dynamic Epsilon Scheduling: A Multi-Factor Adaptive Perturbation Budget for Adversarial Training",
        "author": "Alan Mitkiy, James Smith, Hana Satou, Hiroshi Tanaka, Emily Johnson, and F Monkey",
        "link": "http://arxiv.org/abs/2506.04263v1",
        "abstract": "Adversarial training is among the most effective strategies for defending\ndeep neural networks against adversarial examples. A key limitation of existing\nadversarial training approaches lies in their reliance on a fixed perturbation\nbudget, which fails to account for instance-specific robustness\ncharacteristics. While prior works such as IAAT and MMA introduce\ninstance-level adaptations, they often rely on heuristic or static\napproximations of data robustness. In this paper, we propose Dynamic Epsilon\nScheduling (DES), a novel framework that adaptively adjusts the adversarial\nperturbation budget per instance and per training iteration. DES integrates\nthree key factors: (1) the distance to the decision boundary approximated via\ngradient-based proxies, (2) prediction confidence derived from softmax entropy,\nand (3) model uncertainty estimated via Monte Carlo dropout. By combining these\ncues into a unified scheduling strategy, DES tailors the perturbation budget\ndynamically to guide more effective adversarial learning. Experimental results\non CIFAR-10 and CIFAR-100 show that our method consistently improves both\nadversarial robustness and standard accuracy compared to fixed-epsilon\nbaselines and prior adaptive methods. Moreover, we provide theoretical insights\ninto the stability and convergence of our scheduling policy. This work opens a\nnew avenue for instance-aware, data-driven adversarial training methods."
    },
    {
        "date": "2025-06",
        "title": "AERO: A Redirection-Based Optimization Framework Inspired by Judo for Robust Probabilistic Forecasting",
        "author": "Karthikeyan Vaiapury",
        "link": "http://arxiv.org/abs/2506.02415v1",
        "abstract": "Optimization remains a fundamental pillar of machine learning, yet existing\nmethods often struggle to maintain stability and adaptability in dynamic, non\nlinear systems, especially under uncertainty. We introduce AERO (Adversarial\nEnergy-based Redirection Optimization), a novel framework inspired by the\nredirection principle in Judo, where external disturbances are leveraged rather\nthan resisted. AERO reimagines optimization as a redirection process guided by\n15 interrelated axioms encompassing adversarial correction, energy\nconservation, and disturbance-aware learning. By projecting gradients,\nintegrating uncertainty driven dynamics, and managing learning energy, AERO\noffers a principled approach to stable and robust model updates. Applied to\nprobabilistic solar energy forecasting, AERO demonstrates substantial gains in\npredictive accuracy, reliability, and adaptability, especially in noisy and\nuncertain environments. Our findings highlight AERO as a compelling new\ndirection in the theoretical and practical landscape of optimization."
    },
    {
        "date": "2025-06",
        "title": "Heterogeneous Secure Transmissions in IRS-Assisted NOMA Communications: CO-GNN Approach",
        "author": "Linlin Liang, Zongkai Tian, Haiyan Huang, Xiaoyan Li, Zhisheng Yin, Dehua Zhang, Nina Zhang, and Wenchao Zhai",
        "link": "http://arxiv.org/abs/2506.05381v1",
        "abstract": "Intelligent Reflecting Surfaces (IRS) enhance spectral efficiency by\nadjusting reflection phase shifts, while Non-Orthogonal Multiple Access (NOMA)\nincreases system capacity. Consequently, IRS-assisted NOMA communications have\ngarnered significant research interest. However, the passive nature of the IRS,\nlacking authentication and security protocols, makes these systems vulnerable\nto external eavesdropping due to the openness of electromagnetic signal\npropagation and reflection. NOMA's inherent multi-user signal superposition\nalso introduces internal eavesdropping risks during user pairing. This paper\ninvestigates secure transmissions in IRS-assisted NOMA systems with\nheterogeneous resource configuration in wireless networks to mitigate both\nexternal and internal eavesdropping. To maximize the sum secrecy rate of\nlegitimate users, we propose a combinatorial optimization graph neural network\n(CO-GNN) approach to jointly optimize beamforming at the base station, power\nallocation of NOMA users, and phase shifts of IRS for dynamic heterogeneous\nresource allocation, thereby enabling the design of dual-link or multi-link\nsecure transmissions in the presence of eavesdroppers on the same or\nheterogeneous links. The CO-GNN algorithm simplifies the complex mathematical\nproblem-solving process, eliminates the need for channel estimation, and\nenhances scalability. Simulation results demonstrate that the proposed\nalgorithm significantly enhances the secure transmission performance of the\nsystem."
    },
    {
        "date": "2025-06",
        "title": "GAdaBoost: An Efficient and Robust AdaBoost Algorithm Based on Granular-Ball Structure",
        "author": "Qin Xie, Qinghua Zhang, Shuyin Xia, Xinran Zhou, and Guoyin Wang",
        "link": "http://arxiv.org/abs/2506.02390v1",
        "abstract": "Adaptive Boosting (AdaBoost) faces significant challenges posed by label\nnoise, especially in multiclass classification tasks. Existing methods either\nlack mechanisms to handle label noise effectively or suffer from high\ncomputational costs due to redundant data usage. Inspired by granular\ncomputing, this paper proposes granular adaptive boosting (GAdaBoost), a novel\ntwo-stage framework comprising a data granulation stage and an adaptive\nboosting stage, to enhance efficiency and robustness under noisy conditions. To\nvalidate its feasibility, an extension of SAMME, termed GAdaBoost.SA, is\nproposed. Specifically, first, a granular-ball generation method is designed to\ncompress data while preserving diversity and mitigating label noise. Second,\nthe granular ball-based SAMME algorithm focuses on granular balls rather than\nindividual samples, improving efficiency and reducing sensitivity to noise.\nExperimental results on some noisy datasets show that the proposed approach\nachieves superior robustness and efficiency compared with existing methods,\ndemonstrating that this work effectively extends AdaBoost and SAMME."
    },
    {
        "date": "2025-06",
        "title": "Exploring Explanations Improves the Robustness of In-Context Learning",
        "author": "Ukyo Honda, and Tatsushi Oka",
        "link": "http://arxiv.org/abs/2506.02378v1",
        "abstract": "In-context learning (ICL) has emerged as a successful paradigm for leveraging\nlarge language models (LLMs). However, it often struggles to generalize beyond\nthe distribution of the provided demonstrations. A recent advancement in\nenhancing robustness is ICL with explanations (X-ICL), which improves\nprediction reliability by guiding LLMs to understand and articulate the\nreasoning behind correct labels. Building on this approach, we introduce an\nadvanced framework that extends X-ICL by systematically exploring explanations\nfor all possible labels (X$^2$-ICL), thereby enabling more comprehensive and\nrobust decision-making. Experimental results on multiple natural language\nunderstanding datasets validate the effectiveness of X$^2$-ICL, demonstrating\nsignificantly improved robustness to out-of-distribution data compared to the\nexisting ICL approaches."
    },
    {
        "date": "2025-06",
        "title": "Fire360: A Benchmark for Robust Perception and Episodic Memory in Degraded 360-Degree Firefighting Videos",
        "author": "Aditi Tiwari, Farzaneh Masoud, Dac Trong Nguyen, Jill Kraft, Heng Ji, and Klara Nahrstedt",
        "link": "http://arxiv.org/abs/2506.02167v1",
        "abstract": "Modern AI systems struggle most in environments where reliability is critical\n- scenes with smoke, poor visibility, and structural deformation. Each year,\ntens of thousands of firefighters are injured on duty, often due to breakdowns\nin situational perception. We introduce Fire360, a benchmark for evaluating\nperception and reasoning in safety-critical firefighting scenarios. The dataset\nincludes 228 360-degree videos from professional training sessions under\ndiverse conditions (e.g., low light, thermal distortion), annotated with action\nsegments, object locations, and degradation metadata. Fire360 supports five\ntasks: Visual Question Answering, Temporal Action Captioning, Object\nLocalization, Safety-Critical Reasoning, and Transformed Object Retrieval\n(TOR). TOR tests whether models can match pristine exemplars to fire-damaged\ncounterparts in unpaired scenes, evaluating transformation-invariant\nrecognition. While human experts achieve 83.5% on TOR, models like GPT-4o lag\nsignificantly, exposing failures in reasoning under degradation. By releasing\nFire360 and its evaluation suite, we aim to advance models that not only see,\nbut also remember, reason, and act under uncertainty. The dataset is available\nat: https://uofi.box.com/v/fire360dataset."
    },
    {
        "date": "2025-06",
        "title": "Mitigating Data Poisoning Attacks to Local Differential Privacy",
        "author": "Xiaolin Li, Ninghui Li, Boyang Wang, and Wenhai Sun",
        "link": "http://arxiv.org/abs/2506.02156v1",
        "abstract": "The distributed nature of local differential privacy (LDP) invites data\npoisoning attacks and poses unforeseen threats to the underlying LDP-supported\napplications. In this paper, we propose a comprehensive mitigation framework\nfor popular frequency estimation, which contains a suite of novel defenses,\nincluding malicious user detection, attack pattern recognition, and damaged\nutility recovery. In addition to existing attacks, we explore new adaptive\nadversarial activities for our mitigation design. For detection, we present a\nnew method to precisely identify bogus reports and thus LDP aggregation can be\nperformed over the ``clean'' data. When the attack behavior becomes stealthy\nand direct filtering out malicious users is difficult, we further propose a\ndetection that can effectively recognize hidden adversarial patterns, thus\nfacilitating the decision-making of service providers. These detection methods\nrequire no additional data and attack information and incur minimal\ncomputational cost. Our experiment demonstrates their excellent performance and\nsubstantial improvement over previous work in various settings. In addition, we\nconduct an empirical analysis of LDP post-processing for corrupted data\nrecovery and propose a new post-processing method, through which we reveal new\ninsights into protocol recommendations in practice and key design principles\nfor future research."
    },
    {
        "date": "2025-06",
        "title": "ReconXF: Graph Reconstruction Attack via Public Feature Explanations on Privatized Node Features and Labels",
        "author": "Rishi Raj Sahoo, Rucha Bhalchandra Joshi, and Subhankar Mishra",
        "link": "http://arxiv.org/abs/2506.02134v1",
        "abstract": "Graph Neural Networks (GNNs) achieve high performance across many\napplications but function as black-box models, limiting their use in critical\ndomains like healthcare and criminal justice. Explainability methods address\nthis by providing feature-level explanations that identify important node\nattributes for predictions. These explanations create privacy risks. Combined\nwith auxiliary information, feature explanations can enable adversaries to\nreconstruct graph structure, exposing sensitive relationships. Existing graph\nreconstruction attacks assume access to original auxiliary data, but practical\nsystems use differential privacy to protect node features and labels while\nproviding explanations for transparency. We study a threat model where\nadversaries access public feature explanations along with privatized node\nfeatures and labels. We show that existing explanation-based attacks like GSEF\nperform poorly with privatized data due to noise from differential privacy\nmechanisms. We propose ReconXF, a graph reconstruction attack for scenarios\nwith public explanations and privatized auxiliary data. Our method adapts\nexplanation-based frameworks by incorporating denoising mechanisms that handle\ndifferential privacy noise while exploiting structural signals in explanations.\nExperiments across multiple datasets show ReconXF outperforms SoTA methods in\nprivatized settings, with improvements in AUC and average precision. Results\nindicate that public explanations combined with denoising enable graph\nstructure recovery even under the privacy protection of auxiliary data. Code is\navailable at (link to be made public after acceptance)."
    },
    {
        "date": "2025-06",
        "title": "Fast and Robust Rotation Averaging with Anisotropic Coordinate Descent",
        "author": "Yaroslava Lochman, Carl Olsson, and Christopher Zach",
        "link": "http://arxiv.org/abs/2506.01940v1",
        "abstract": "Anisotropic rotation averaging has recently been explored as a natural\nextension of respective isotropic methods. In the anisotropic formulation,\nuncertainties of the estimated relative rotations -- obtained via standard\ntwo-view optimization -- are propagated to the optimization of absolute\nrotations. The resulting semidefinite relaxations are able to recover global\nminima but scale poorly with the problem size. Local methods are fast and also\nadmit robust estimation but are sensitive to initialization. They usually\nemploy minimum spanning trees and therefore suffer from drift accumulation and\ncan get trapped in poor local minima. In this paper, we attempt to bridge the\ngap between optimality, robustness and efficiency of anisotropic rotation\naveraging. We analyze a family of block coordinate descent methods initially\nproposed to optimize the standard chordal distances, and derive a much simpler\nformulation and an anisotropic extension obtaining a fast general solver. We\nintegrate this solver into the extended anisotropic large-scale robust rotation\naveraging pipeline. The resulting algorithm achieves state-of-the-art\nperformance on public structure-from-motion datasets. Project page:\nhttps://ylochman.github.io/acd"
    },
    {
        "date": "2025-06",
        "title": "COALESCE: Economic and Security Dynamics of Skill-Based Task Outsourcing Among Team of Autonomous LLM Agents",
        "author": "Manish Bhatt, Ronald F. Del Rosario, Vineeth Sai Narajala, and Idan Habler",
        "link": "http://arxiv.org/abs/2506.01900v1",
        "abstract": "The meteoric rise and proliferation of autonomous Large Language Model (LLM)\nagents promise significant capabilities across various domains. However, their\ndeployment is increasingly constrained by substantial computational demands,\nspecifically for Graphics Processing Unit (GPU) resources. This paper addresses\nthe critical problem of optimizing resource utilization in LLM agent systems.\nWe introduce COALESCE (Cost-Optimized and Secure Agent Labour Exchange via\nSkill-based Competence Estimation), a novel framework designed to enable\nautonomous LLM agents to dynamically outsource specific subtasks to\nspecialized, cost-effective third-party LLM agents. The framework integrates\nmechanisms for hybrid skill representation, dynamic skill discovery, automated\ntask decomposition, a unified cost model comparing internal execution costs\nagainst external outsourcing prices, simplified market-based decision-making\nalgorithms, and a standardized communication protocol between LLM agents.\nComprehensive validation through 239 theoretical simulations demonstrates\n41.8\\% cost reduction potential, while large-scale empirical validation across\n240 real LLM tasks confirms 20.3\\% cost reduction with proper epsilon-greedy\nexploration, establishing both theoretical viability and practical\neffectiveness. The emergence of proposed open standards like Google's\nAgent2Agent (A2A) protocol further underscores the need for frameworks like\nCOALESCE that can leverage such standards for efficient agent interaction. By\nfacilitating a dynamic market for agent capabilities, potentially utilizing\nprotocols like A2A for communication, COALESCE aims to significantly reduce\noperational costs, enhance system scalability, and foster the emergence of\nspecialized agent economies, making complex LLM agent functionalities more\naccessible and economically viable."
    },
    {
        "date": "2025-06",
        "title": "Which Factors Make Code LLMs More Vulnerable to Backdoor Attacks? A Systematic Study",
        "author": "Chenyu Wang, Zhou Yang, Yaniv Harel, and David Lo",
        "link": "http://arxiv.org/abs/2506.01825v1",
        "abstract": "Code LLMs are increasingly employed in software development. However, studies\nhave shown that they are vulnerable to backdoor attacks: when a trigger (a\nspecific input pattern) appears in the input, the backdoor will be activated\nand cause the model to generate malicious outputs. Researchers have designed\nvarious triggers and demonstrated the feasibility of implanting backdoors by\npoisoning a fraction of the training data. Some basic conclusions have been\nmade, such as backdoors becoming easier to implant when more training data are\nmodified. However, existing research has not explored other factors influencing\nbackdoor attacks on Code LLMs, such as training batch size, epoch number, and\nthe broader design space for triggers, e.g., trigger length.\n  To bridge this gap, we use code summarization as an example to perform an\nempirical study that systematically investigates the factors affecting backdoor\neffectiveness and understands the extent of the threat posed. Three categories\nof factors are considered: data, model, and inference, revealing previously\noverlooked findings. We find that the prevailing consensus -- that attacks are\nineffective at extremely low poisoning rates -- is incorrect. The absolute\nnumber of poisoned samples matters as well. Specifically, poisoning just 20 out\nof 454K samples (0.004\\% poisoning rate -- far below the minimum setting of\n0.1\\% in prior studies) successfully implants backdoors! Moreover, the common\ndefense is incapable of removing even a single poisoned sample from it.\nAdditionally, small batch sizes increase the risk of backdoor attacks. We also\nuncover other critical factors such as trigger types, trigger length, and the\nrarity of tokens in the triggers, leading to valuable insights for assessing\nCode LLMs' vulnerability to backdoor attacks. Our study highlights the urgent\nneed for defense mechanisms against extremely low poisoning rate settings."
    },
    {
        "date": "2025-06",
        "title": "DRAUN: An Algorithm-Agnostic Data Reconstruction Attack on Federated Unlearning Systems",
        "author": "Hithem Lamri, Manaar Alam, Haiyan Jiang, and Michail Maniatakos",
        "link": "http://arxiv.org/abs/2506.01777v1",
        "abstract": "Federated Unlearning (FU) enables clients to remove the influence of specific\ndata from a collaboratively trained shared global model, addressing regulatory\nrequirements such as GDPR and CCPA. However, this unlearning process introduces\na new privacy risk: A malicious server may exploit unlearning updates to\nreconstruct the data requested for removal, a form of Data Reconstruction\nAttack (DRA). While DRAs for machine unlearning have been studied extensively\nin centralized Machine Learning-as-a-Service (MLaaS) settings, their\napplicability to FU remains unclear due to the decentralized, client-driven\nnature of FU. This work presents DRAUN, the first attack framework to\nreconstruct unlearned data in FU systems. DRAUN targets optimization-based\nunlearning methods, which are widely adopted for their efficiency. We\ntheoretically demonstrate why existing DRAs targeting machine unlearning in\nMLaaS fail in FU and show how DRAUN overcomes these limitations. We validate\nour approach through extensive experiments on four datasets and four model\narchitectures, evaluating its performance against five popular unlearning\nmethods, effectively demonstrating that state-of-the-art FU methods remain\nvulnerable to DRAs."
    },
    {
        "date": "2025-06",
        "title": "Predictive-CSM: Lightweight Fragment Security for 6LoWPAN IoT Networks",
        "author": "Somayeh Sobati-M",
        "link": "http://arxiv.org/abs/2506.01767v1",
        "abstract": "Fragmentation is a routine part of communication in 6LoWPAN-based IoT\nnetworks,\n  designed to accommodate small frame sizes on constrained wireless links.\nHowever, this process\n  introduces a critical vulnerability fragments are typically stored and\nprocessed before their\n  legitimacy is confirmed, allowing attackers to exploit this gap with minimal\neffort.\n  In this work, we explore a defense strategy that takes a more adaptive,\nbehavior-aware approach to this problem. Our system, called Predictive-CSM,\nintroduces a combination of two\n  lightweight mechanisms. The first tracks how each node behaves over time,\nrewarding consistent\n  and successful interactions while quickly penalizing suspicious or failing\npatterns. The second\n  checks the integrity of packet fragments using a chained hash, allowing\nincomplete or manipulated sequences to be caught early, before they can occupy\nmemory or waste processing time.\n  We put this system to the test using a set of targeted attack simulations,\nincluding early fragment injection, replayed headers, and flooding with fake\ndata. Across all scenarios, Predictive CSM preserved network delivery and\nmaintained energy efficiency, even under pressure. Rather\n  than relying on heavyweight cryptography or rigid filters, this approach\nallows constrained de vices to adapt their defenses in real time based on what\nthey observe, not just what they're\n  told. In that way, it offers a step forward for securing fragmented\ncommunication in real world\n  IoT systems"
    },
    {
        "date": "2025-06",
        "title": "Robust Satisficing Gaussian Process Bandits Under Adversarial Attacks",
        "author": "Artun Saday, Ya\u015far Cahit Y\u0131ld\u0131r\u0131m, and Cem Tekin",
        "link": "http://arxiv.org/abs/2506.01625v1",
        "abstract": "We address the problem of Gaussian Process (GP) optimization in the presence\nof unknown and potentially varying adversarial perturbations. Unlike\ntraditional robust optimization approaches that focus on maximizing performance\nunder worst-case scenarios, we consider a robust satisficing objective, where\nthe goal is to consistently achieve a predefined performance threshold $\\tau$,\neven under adversarial conditions. We propose two novel algorithms based on\ndistinct formulations of robust satisficing, and show that they are instances\nof a general robust satisficing framework. Further, each algorithm offers\ndifferent guarantees depending on the nature of the adversary. Specifically, we\nderive two regret bounds: one that is sublinear over time, assuming certain\nconditions on the adversary and the satisficing threshold $\\tau$, and another\nthat scales with the perturbation magnitude but requires no assumptions on the\nadversary. Through extensive experiments, we demonstrate that our approach\noutperforms the established robust optimization methods in achieving the\nsatisficing objective, particularly when the ambiguity set of the robust\noptimization framework is inaccurately specified."
    },
    {
        "date": "2025-06",
        "title": "Silence is Golden: Leveraging Adversarial Examples to Nullify Audio Control in LDM-based Talking-Head Generation",
        "author": "Yuan Gan, Jiaxu Miao, Yunze Wang, and Yi Yang",
        "link": "http://arxiv.org/abs/2506.01591v1",
        "abstract": "Advances in talking-head animation based on Latent Diffusion Models (LDM)\nenable the creation of highly realistic, synchronized videos. These fabricated\nvideos are indistinguishable from real ones, increasing the risk of potential\nmisuse for scams, political manipulation, and misinformation. Hence, addressing\nthese ethical concerns has become a pressing issue in AI security. Recent\nproactive defense studies focused on countering LDM-based models by adding\nperturbations to portraits. However, these methods are ineffective at\nprotecting reference portraits from advanced image-to-video animation. The\nlimitations are twofold: 1) they fail to prevent images from being manipulated\nby audio signals, and 2) diffusion-based purification techniques can\neffectively eliminate protective perturbations. To address these challenges, we\npropose Silencer, a two-stage method designed to proactively protect the\nprivacy of portraits. First, a nullifying loss is proposed to ignore audio\ncontrol in talking-head generation. Second, we apply anti-purification loss in\nLDM to optimize the inverted latent feature to generate robust perturbations.\nExtensive experiments demonstrate the effectiveness of Silencer in proactively\nprotecting portrait privacy. We hope this work will raise awareness among the\nAI security community regarding critical ethical issues related to talking-head\ngeneration techniques. Code: https://github.com/yuangan/Silencer."
    },
    {
        "date": "2025-06",
        "title": "Enhancing Diffusion-based Unrestricted Adversarial Attacks via Adversary Preferences Alignment",
        "author": "Kaixun Jiang, Zhaoyu Chen, Haijing Guo, Jinglun Li, Jiyuan Fu, Pinxue Guo, Hao Tang, Bo Li, and Wenqiang Zhang",
        "link": "http://arxiv.org/abs/2506.01511v1",
        "abstract": "Preference alignment in diffusion models has primarily focused on benign\nhuman preferences (e.g., aesthetic). In this paper, we propose a novel\nperspective: framing unrestricted adversarial example generation as a problem\nof aligning with adversary preferences. Unlike benign alignment, adversarial\nalignment involves two inherently conflicting preferences: visual consistency\nand attack effectiveness, which often lead to unstable optimization and reward\nhacking (e.g., reducing visual quality to improve attack success). To address\nthis, we propose APA (Adversary Preferences Alignment), a two-stage framework\nthat decouples conflicting preferences and optimizes each with differentiable\nrewards. In the first stage, APA fine-tunes LoRA to improve visual consistency\nusing rule-based similarity reward. In the second stage, APA updates either the\nimage latent or prompt embedding based on feedback from a substitute\nclassifier, guided by trajectory-level and step-wise rewards. To enhance\nblack-box transferability, we further incorporate a diffusion augmentation\nstrategy. Experiments demonstrate that APA achieves significantly better attack\ntransferability while maintaining high visual consistency, inspiring further\nresearch to approach adversarial attacks from an alignment perspective. Code\nwill be available at https://github.com/deep-kaixun/APA."
    },
    {
        "date": "2025-06",
        "title": "Robust Federated Learning against Noisy Clients via Masked Optimization",
        "author": "Xuefeng Jiang, Tian Wen, Zhiqin Yang, Lvhua Wu, Yufeng Chen, Sheng Sun, Yuwei Wang, and Min Liu",
        "link": "http://arxiv.org/abs/2506.02079v1",
        "abstract": "In recent years, federated learning (FL) has made significant advance in\nprivacy-sensitive applications. However, it can be hard to ensure that FL\nparticipants provide well-annotated data for training. The corresponding\nannotations from different clients often contain complex label noise at varying\nlevels. This label noise issue has a substantial impact on the performance of\nthe trained models, and clients with greater noise levels can be largely\nattributed for this degradation. To this end, it is necessary to develop an\neffective optimization strategy to alleviate the adverse effects of these noisy\nclients.In this study, we present a two-stage optimization framework,\nMaskedOptim, to address this intricate label noise problem. The first stage is\ndesigned to facilitate the detection of noisy clients with higher label noise\nrates. The second stage focuses on rectifying the labels of the noisy clients'\ndata through an end-to-end label correction mechanism, aiming to mitigate the\nnegative impacts caused by misinformation within datasets. This is achieved by\nlearning the potential ground-truth labels of the noisy clients' datasets via\nbackpropagation. To further enhance the training robustness, we apply the\ngeometric median based model aggregation instead of the commonly-used vanilla\naveraged model aggregation. We implement sixteen related methods and conduct\nevaluations on three image datasets and one text dataset with diverse label\nnoise patterns for a comprehensive comparison. Extensive experimental results\nindicate that our proposed framework shows its robustness in different\nscenarios. Additionally, our label correction framework effectively enhances\nthe data quality of the detected noisy clients' local datasets. % Our codes\nwill be open-sourced to facilitate related research communities. Our codes are\navailable via https://github.com/Sprinter1999/MaskedOptim ."
    },
    {
        "date": "2025-06",
        "title": "Variance-Based Defense Against Blended Backdoor Attacks",
        "author": "Sujeevan Aseervatham, Achraf Kerzazi, and Youn\u00e8s Bennani",
        "link": "http://arxiv.org/abs/2506.01444v1",
        "abstract": "Backdoor attacks represent a subtle yet effective class of cyberattacks\ntargeting AI models, primarily due to their stealthy nature. The model behaves\nnormally on clean data but exhibits malicious behavior only when the attacker\nembeds a specific trigger into the input. This attack is performed during the\ntraining phase, where the adversary corrupts a small subset of the training\ndata by embedding a pattern and modifying the labels to a chosen target. The\nobjective is to make the model associate the pattern with the target label\nwhile maintaining normal performance on unaltered data. Several defense\nmechanisms have been proposed to sanitize training data-sets. However, these\nmethods often rely on the availability of a clean dataset to compute\nstatistical anomalies, which may not always be feasible in real-world scenarios\nwhere datasets can be unavailable or compromised. To address this limitation,\nwe propose a novel defense method that trains a model on the given dataset,\ndetects poisoned classes, and extracts the critical part of the attack trigger\nbefore identifying the poisoned instances. This approach enhances\nexplainability by explicitly revealing the harmful part of the trigger. The\neffectiveness of our method is demonstrated through experimental evaluations on\nwell-known image datasets and comparative analysis against three\nstate-of-the-art algorithms: SCAn, ABL, and AGPD."
    },
    {
        "date": "2025-06",
        "title": "Self-Refining Language Model Anonymizers via Adversarial Distillation",
        "author": "Kyuyoung Kim, Hyunjun Jeon, and Jinwoo Shin",
        "link": "http://arxiv.org/abs/2506.01420v1",
        "abstract": "Large language models (LLMs) are increasingly used in sensitive domains,\nwhere their ability to infer personal data from seemingly benign text poses\nemerging privacy risks. While recent LLM-based anonymization methods help\nmitigate such risks, they often rely on proprietary models (e.g., GPT-4),\nraising concerns about cost and the potential exposure of sensitive data to\nuntrusted external systems. To address this, we introduce SElf-refining\nAnonymization with Language model (SEAL), a novel distillation framework for\ntraining small language models (SLMs) to perform effective anonymization\nwithout relying on external costly models at inference time. We leverage\nadversarial interactions between an LLM anonymizer and an inference model to\ncollect trajectories of anonymized texts and inferred attributes, which are\nused to distill anonymization, adversarial inference, and utility evaluation\ncapabilities into SLMs via supervised fine-tuning and preference learning. The\nresulting models learn to both anonymize text and critique their outputs,\nenabling iterative improvement of anonymization quality via self-refinement.\nExperiments on SynthPAI, a dataset of synthetic personal profiles and text\ncomments, demonstrate that SLMs trained with SEAL achieve substantial\nimprovements in anonymization capabilities. Notably, 8B models attain a\nprivacy-utility trade-off comparable to that of the GPT-4 anonymizer and, with\nself-refinement, even surpass it in terms of privacy. These results show the\neffectiveness of our adversarial distillation framework in training SLMs as\nefficient anonymizers. To facilitate further research, we release the full\ndataset used in our experiments."
    },
    {
        "date": "2025-06",
        "title": "Formal Security Analysis of SPV Clients Versus Home-Based Full Nodes in Bitcoin-Derived Systems",
        "author": "Craig Steven Wright",
        "link": "http://arxiv.org/abs/2506.01384v1",
        "abstract": "This paper presents a mathematically rigorous formal analysis of Simplified\nPayment Verification (SPV) clients, as specified in Section 8 of the original\nBitcoin white paper, versus non-mining full nodes operated by home users. It\ndefines security as resistance to divergence from global consensus and models\ntransaction acceptance, enforcement capability, and divergence probability\nunder adversarial conditions. The results demonstrate that SPV clients, despite\nomitting script verification, are cryptographically sufficient under\nhonest-majority assumptions and topologically less vulnerable to attack than\nstructurally passive, non-enforcing full nodes. The paper introduces new axioms\non behavioral divergence and communication topology, proving that home-based\nfull nodes increase systemic entropy without contributing to consensus\nintegrity. Using a series of formally defined lemmas, propositions, and Monte\nCarlo simulation results, it is shown that SPV clients represent the rational\nequilibrium strategy for non-mining participants. This challenges the\nprevailing narrative that home validators enhance network security, providing\nformal and operational justifications for the sufficiency of SPV models."
    },
    {
        "date": "2025-06",
        "title": "TimeGraph: Synthetic Benchmark Datasets for Robust Time-Series Causal Discovery",
        "author": "Muhammad Hasan Ferdous, Emam Hossain, and Md Osman Gani",
        "link": "http://arxiv.org/abs/2506.01361v1",
        "abstract": "Robust causal discovery in time series datasets depends on reliable benchmark\ndatasets with known ground-truth causal relationships. However, such datasets\nremain scarce, and existing synthetic alternatives often overlook critical\ntemporal properties inherent in real-world data, including nonstationarity\ndriven by trends and seasonality, irregular sampling intervals, and the\npresence of unobserved confounders. To address these challenges, we introduce\nTimeGraph, a comprehensive suite of synthetic time-series benchmark datasets\nthat systematically incorporates both linear and nonlinear dependencies while\nmodeling key temporal characteristics such as trends, seasonal effects, and\nheterogeneous noise patterns. Each dataset is accompanied by a fully specified\ncausal graph featuring varying densities and diverse noise distributions and is\nprovided in two versions: one including unobserved confounders and one without,\nthereby offering extensive coverage of real-world complexity while preserving\nmethodological neutrality. We further demonstrate the utility of TimeGraph\nthrough systematic evaluations of state-of-the-art causal discovery algorithms\nincluding PCMCI+, LPCMCI, and FGES across a diverse array of configurations and\nmetrics. Our experiments reveal significant variations in algorithmic\nperformance under realistic temporal conditions, underscoring the need for\nrobust synthetic benchmarks in the fair and transparent assessment of causal\ndiscovery methods. The complete TimeGraph suite, including dataset generation\nscripts, evaluation metrics, and recommended experimental protocols, is freely\navailable to facilitate reproducible research and foster community-driven\nadvancements in time-series causal discovery."
    },
    {
        "date": "2025-06",
        "title": "Distributionally Robust Learning in Survival Analysis",
        "author": "Yeping Jin, Lauren Wise, and Ioannis Ch. Paschalidis",
        "link": "http://arxiv.org/abs/2506.01348v2",
        "abstract": "We introduce an innovative approach that incorporates a Distributionally\nRobust Learning (DRL) approach into Cox regression to enhance the robustness\nand accuracy of survival predictions. By formulating a DRL framework with a\nWasserstein distance-based ambiguity set, we develop a variant Cox model that\nis less sensitive to assumptions about the underlying data distribution and\nmore resilient to model misspecification and data perturbations. By leveraging\nWasserstein duality, we reformulate the original min-max DRL problem into a\ntractable regularized empirical risk minimization problem, which can be\ncomputed by exponential conic programming. We provide guarantees on the finite\nsample behavior of our DRL-Cox model. Moreover, through extensive simulations\nand real world case studies, we demonstrate that our regression model achieves\nsuperior performance in terms of prediction accuracy and robustness compared\nwith traditional methods."
    },
    {
        "date": "2025-06",
        "title": "ETDI: Mitigating Tool Squatting and Rug Pull Attacks in Model Context Protocol (MCP) by using OAuth-Enhanced Tool Definitions and Policy-Based Access Control",
        "author": "Manish Bhatt, Vineeth Sai Narajala, and Idan Habler",
        "link": "http://arxiv.org/abs/2506.01333v1",
        "abstract": "The Model Context Protocol (MCP) plays a crucial role in extending the\ncapabilities of Large Language Models (LLMs) by enabling integration with\nexternal tools and data sources. However, the standard MCP specification\npresents significant security vulnerabilities, notably Tool Poisoning and Rug\nPull attacks. This paper introduces the Enhanced Tool Definition Interface\n(ETDI), a security extension designed to fortify MCP. ETDI incorporates\ncryptographic identity verification, immutable versioned tool definitions, and\nexplicit permission management, often leveraging OAuth 2.0. We further propose\nextending MCP with fine-grained, policy-based access control, where tool\ncapabilities are dynamically evaluated against explicit policies using a\ndedicated policy engine, considering runtime context beyond static OAuth\nscopes. This layered approach aims to establish a more secure, trustworthy, and\ncontrollable ecosystem for AI applications interacting with LLMs and external\ntools."
    },
    {
        "date": "2025-06",
        "title": "Unlearning's Blind Spots: Over-Unlearning and Prototypical Relearning Attack",
        "author": "SeungBum Ha, Saerom Park, and Sung Whan Yoon",
        "link": "http://arxiv.org/abs/2506.01318v2",
        "abstract": "Machine unlearning (MU) aims to expunge a designated forget set from a\ntrained model without costly retraining, yet the existing techniques overlook\ntwo critical blind spots: \"over-unlearning\" that deteriorates retained data\nnear the forget set, and post-hoc \"relearning\" attacks that aim to resurrect\nthe forgotten knowledge. We first derive the over-unlearning metric\nOU@{\\epsilon}, which represents the collateral damage to the nearby region of\nthe forget set, where the over-unlearning mainly appears. Next, we expose an\nunforeseen relearning threat on MU, i.e., the Prototypical Relearning Attack,\nwhich exploits the per-class prototype of the forget class with just a few\nsamples, and easily restores the pre-unlearning performance. To counter both\nblind spots, we introduce Spotter, a plug-and-play objective that combines (i)\na masked knowledge-distillation penalty on the nearby region of forget set to\nsuppress OU@{\\epsilon}, and (ii) an intra-class dispersion loss that scatters\nforget-class embeddings, neutralizing prototypical relearning attacks. On\nCIFAR-10, as one of validations, Spotter reduces OU@{\\epsilon}by below the\n0.05X of the baseline, drives forget accuracy to 0%, preserves accuracy of the\nretain set within 1% of difference with the original, and denies the\nprototype-attack by keeping the forget set accuracy within <1%, without\naccessing retained data. It confirms that Spotter is a practical remedy of the\nunlearning's blind spots."
    },
    {
        "date": "2025-06",
        "title": "Align is not Enough: Multimodal Universal Jailbreak Attack against Multimodal Large Language Models",
        "author": "Youze Wang, Wenbo Hu, Yinpeng Dong, Jing Liu, Hanwang Zhang, and Richang Hong",
        "link": "http://arxiv.org/abs/2506.01307v1",
        "abstract": "Large Language Models (LLMs) have evolved into Multimodal Large Language\nModels (MLLMs), significantly enhancing their capabilities by integrating\nvisual information and other types, thus aligning more closely with the nature\nof human intelligence, which processes a variety of data forms beyond just\ntext. Despite advancements, the undesirable generation of these models remains\na critical concern, particularly due to vulnerabilities exposed by text-based\njailbreak attacks, which have represented a significant threat by challenging\nexisting safety protocols. Motivated by the unique security risks posed by the\nintegration of new and old modalities for MLLMs, we propose a unified\nmultimodal universal jailbreak attack framework that leverages iterative\nimage-text interactions and transfer-based strategy to generate a universal\nadversarial suffix and image. Our work not only highlights the interaction of\nimage-text modalities can be used as a critical vulnerability but also\nvalidates that multimodal universal jailbreak attacks can bring higher-quality\nundesirable generations across different MLLMs. We evaluate the undesirable\ncontext generation of MLLMs like LLaVA, Yi-VL, MiniGPT4, MiniGPT-v2, and\nInstructBLIP, and reveal significant multimodal safety alignment issues,\nhighlighting the inadequacy of current safety mechanisms against sophisticated\nmultimodal attacks. This study underscores the urgent need for robust safety\nmeasures in MLLMs, advocating for a comprehensive review and enhancement of\nsecurity protocols to mitigate potential risks associated with multimodal\ncapabilities."
    },
    {
        "date": "2025-06",
        "title": "Adversarial learning for nonparametric regression: Minimax rate and adaptive estimation",
        "author": "Jingfu Peng, and Yuhong Yang",
        "link": "http://arxiv.org/abs/2506.01267v1",
        "abstract": "Despite tremendous advancements of machine learning models and algorithms in\nvarious application domains, they are known to be vulnerable to subtle, natural\nor intentionally crafted perturbations in future input data, known as\nadversarial attacks. While numerous adversarial learning methods have been\nproposed, fundamental questions about their statistical optimality in robust\nloss remain largely unanswered. In particular, the minimax rate of convergence\nand the construction of rate-optimal estimators under future $X$-attacks are\nyet to be worked out.\n  In this paper, we address this issue in the context of nonparametric\nregression, under suitable assumptions on the smoothness of the regression\nfunction and the geometric structure of the input perturbation set. We first\nestablish the minimax rate of convergence under adversarial $L_q$-risks with $1\n\\leq q \\leq \\infty$ and propose a piecewise local polynomial estimator that\nachieves the minimax optimality. The established minimax rate elucidates how\nthe smoothness level and perturbation magnitude affect the fundamental limit of\nadversarial learning under future $X$-attacks. Furthermore, we construct a\ndata-driven adaptive estimator that is shown to achieve, within a logarithmic\nfactor, the optimal rate across a broad scale of nonparametric and adversarial\nclasses."
    },
    {
        "date": "2025-06",
        "title": "Stress-Testing ML Pipelines with Adversarial Data Corruption",
        "author": "Jiongli Zhu, Geyang Xu, Felipe Lorenzi, Boris Glavic, and Babak Salimi",
        "link": "http://arxiv.org/abs/2506.01230v1",
        "abstract": "Structured data-quality issues, such as missing values correlated with\ndemographics, culturally biased labels, or systemic selection biases, routinely\ndegrade the reliability of machine-learning pipelines. Regulators now\nincreasingly demand evidence that high-stakes systems can withstand these\nrealistic, interdependent errors, yet current robustness evaluations typically\nuse random or overly simplistic corruptions, leaving worst-case scenarios\nunexplored. We introduce SAVAGE, a causally inspired framework that (i)\nformally models realistic data-quality issues through dependency graphs and\nflexible corruption templates, and (ii) systematically discovers corruption\npatterns that maximally degrade a target performance metric. SAVAGE employs a\nbi-level optimization approach to efficiently identify vulnerable data\nsubpopulations and fine-tune corruption severity, treating the full ML\npipeline, including preprocessing and potentially non-differentiable models, as\na black box. Extensive experiments across multiple datasets and ML tasks (data\ncleaning, fairness-aware learning, uncertainty quantification) demonstrate that\neven a small fraction (around 5 %) of structured corruptions identified by\nSAVAGE severely impacts model performance, far exceeding random or manually\ncrafted errors, and invalidating core assumptions of existing techniques. Thus,\nSAVAGE provides a practical tool for rigorous pipeline stress-testing, a\nbenchmark for evaluating robustness methods, and actionable guidance for\ndesigning more resilient data workflows."
    },
    {
        "date": "2025-06",
        "title": "SPEAR: Security Posture Evaluation using AI Planner-Reasoning on Attack-Connectivity Hypergraphs",
        "author": "Rakesh Podder, Turgay Caglar, Shadaab Kawnain Bashir, Sarath Sreedharan, Indrajit Ray, and Indrakshi Ray",
        "link": "http://arxiv.org/abs/2506.01227v1",
        "abstract": "Graph-based frameworks are often used in network hardening to help a cyber\ndefender understand how a network can be attacked and how the best defenses can\nbe deployed. However, incorporating network connectivity parameters in the\nattack graph, reasoning about the attack graph when we do not have access to\ncomplete information, providing system administrator suggestions in an\nunderstandable format, and allowing them to do what-if analysis on various\nscenarios and attacker motives is still missing. We fill this gap by presenting\nSPEAR, a formal framework with tool support for security posture evaluation and\nanalysis that keeps human-in-the-loop. SPEAR uses the causal formalism of AI\nplanning to model vulnerabilities and configurations in a networked system. It\nautomatically converts network configurations and vulnerability descriptions\ninto planning models expressed in the Planning Domain Definition Language\n(PDDL). SPEAR identifies a set of diverse security hardening strategies that\ncan be presented in a manner understandable to the domain expert. These allow\nthe administrator to explore the network hardening solution space in a\nsystematic fashion and help evaluate the impact and compare the different\nsolutions."
    },
    {
        "date": "2025-06",
        "title": "Dirty and Clean-Label attack detection using GAN discriminators",
        "author": "John W. Smutny",
        "link": "http://arxiv.org/abs/2506.01224v2",
        "abstract": "Gathering enough images to train a deep computer vision model is a constant\nchallenge. Unfortunately, collecting images from unknown sources can leave your\nmodel s behavior at risk of being manipulated by a dirty-label or clean-label\nattack unless the images are properly inspected. Manually inspecting each\nimage-label pair is impractical and common poison-detection methods that\ninvolve re-training your model can be time consuming. This research uses GAN\ndiscriminators to protect a single class against mislabeled and different\nlevels of modified images. The effect of said perturbation on a basic\nconvolutional neural network classifier is also included for reference. The\nresults suggest that after training on a single class, GAN discriminator s\nconfidence scores can provide a threshold to identify mislabeled images and\nidentify 100% of the tested poison starting at a perturbation epsilon magnitude\nof 0.20, after decision threshold calibration using in-class samples.\nDevelopers can use this report as a basis to train their own discriminators to\nprotect high valued classes in their CV models."
    },
    {
        "date": "2025-06",
        "title": "FedRPCA: Enhancing Federated LoRA Aggregation Using Robust PCA",
        "author": "Divyansh Jhunjhunwala, Arian Raje, Madan Ravi Ganesh, Chaithanya Kumar Mummadi, Chaoqun Dong, Jiawei Zhou, Wan-Yi Lin, Gauri Joshi, and Zhenzhen Li",
        "link": "http://arxiv.org/abs/2506.01194v1",
        "abstract": "LoRA has emerged as one of the most promising fine-tuning techniques,\nespecially for federated learning (FL), since it significantly reduces\ncommunication and computation costs at resource-constrained clients. However,\ndata heterogeneity remains a significant challenge for LoRA-based FL, and the\nconventional aggregation strategy based on FedAvg suffers from slow convergence\nand suboptimal accuracy. Motivated by recent advances in model merging,\nparticularly Task Arithmetic, we explore the idea of aggregating client LoRA\nparameters using scaled averaging. We first observe that a naive application of\nTask Arithmetic is ineffective due to the high cosine similarity between client\nupdates, indicating significant common knowledge in the updates across clients.\nTo address this issue, we propose decomposing client LoRA updates via Robust\nPrincipal Component Analysis (Robust-PCA) into a common low-rank component and\nclient-specific sparse components. Our proposed algorithm FedRPCA aggregates\nthe low-rank components through averaging, consolidating common knowledge, and\napplies scaled averaging to the sparse components to amplify client-specific\nknowledge. We evaluate our approach across a variety of vision and language\ntasks and demonstrate that it achieves higher final accuracy and faster\nconvergence compared to competing baselines."
    },
    {
        "date": "2025-06",
        "title": "Doubly Robust Alignment for Large Language Models",
        "author": "Erhan Xu, Kai Ye, Hongyi Zhou, Luhan Zhu, Francesco Quinzan, and Chengchun Shi",
        "link": "http://arxiv.org/abs/2506.01183v1",
        "abstract": "This paper studies reinforcement learning from human feedback (RLHF) for\naligning large language models with human preferences. While RLHF has\ndemonstrated promising results, many algorithms are highly sensitive to\nmisspecifications in the underlying preference model (e.g., the Bradley-Terry\nmodel), the reference policy, or the reward function, resulting in undesirable\nfine-tuning. To address model misspecification, we propose a doubly robust\npreference optimization algorithm that remains consistent when either the\npreference model or the reference policy is correctly specified (without\nrequiring both). Our proposal demonstrates superior and more robust performance\nthan state-of-the-art algorithms, both in theory and in practice. The code is\navailable at https://github.com/DRPO4LLM/DRPO4LLM"
    },
    {
        "date": "2025-06",
        "title": "Neuro-Symbolic Generative Diffusion Models for Physically Grounded, Robust, and Safe Generation",
        "author": "Jacob K. Christopher, Michael Cardei, Jinhao Liang, and Ferdinando Fioretto",
        "link": "http://arxiv.org/abs/2506.01121v1",
        "abstract": "Despite the remarkable generative capabilities of diffusion models, their\nintegration into safety-critical or scientifically rigorous applications\nremains hindered by the need to ensure compliance with stringent physical,\nstructural, and operational constraints. To address this challenge, this paper\nintroduces Neuro-Symbolic Diffusion (NSD), a novel framework that interleaves\ndiffusion steps with symbolic optimization, enabling the generation of\ncertifiably consistent samples under user-defined functional and logic\nconstraints. This key feature is provided for both standard and discrete\ndiffusion models, enabling, for the first time, the generation of both\ncontinuous (e.g., images and trajectories) and discrete (e.g., molecular\nstructures and natural language) outputs that comply with constraints. This\nability is demonstrated on tasks spanning three key challenges: (1) Safety, in\nthe context of non-toxic molecular generation and collision-free trajectory\noptimization; (2) Data scarcity, in domains such as drug discovery and\nmaterials engineering; and (3) Out-of-domain generalization, where enforcing\nsymbolic constraints allows adaptation beyond the training distribution."
    },
    {
        "date": "2025-06",
        "title": "IDCloak: A Practical Secure Multi-party Dataset Join Framework for Vertical Privacy-preserving Machine Learning",
        "author": "Shuyu Chen, Guopeng Lin, Haoyu Niu, Lushan Song, Chengxun Hong, and Weili Han",
        "link": "http://arxiv.org/abs/2506.01072v1",
        "abstract": "Vertical privacy-preserving machine learning (vPPML) enables multiple parties\nto train models on their vertically distributed datasets while keeping datasets\nprivate. In vPPML, it is critical to perform the secure dataset join, which\naligns features corresponding to intersection IDs across datasets and forms a\nsecret-shared and joint training dataset. However, existing methods for this\nstep could be impractical due to: (1) they are insecure when they expose\nintersection IDs; or (2) they rely on a strong trust assumption requiring a\nnon-colluding auxiliary server; or (3) they are limited to the two-party\nsetting.\n  This paper proposes IDCloak, the first practical secure multi-party dataset\njoin framework for vPPML that keeps IDs private without a non-colluding\nauxiliary server. IDCloak consists of two protocols: (1) a circuit-based\nmulti-party private set intersection protocol (cmPSI), which obtains\nsecret-shared flags indicating intersection IDs via an optimized communication\nstructure combining OKVS and OPRF; (2) a secure multi-party feature alignment\nprotocol, which obtains the secret-shared and joint dataset using secret-shared\nflags, via our proposed efficient secure shuffle protocol. Experiments show\nthat: (1) compared to the state-of-the-art secure two-party dataset join\nframework (iPrivjoin), IDCloak demonstrates higher efficiency in the two-party\nsetting and comparable performance when the party number increases; (2)\ncompared to the state-of-the-art cmPSI protocol under honest majority, our\nproposed cmPSI protocol provides a stronger security guarantee (dishonest\nmajority) while improving efficiency by up to $7.78\\times$ in time and\n$8.73\\times$ in communication sizes; (3) our proposed secure shuffle protocol\noutperforms the state-of-the-art shuffle protocol by up to $138.34\\times$ in\ntime and $132.13\\times$ in communication sizes."
    },
    {
        "date": "2025-06",
        "title": "Fighting Fire with Fire (F3): A Training-free and Efficient Visual Adversarial Example Purification Method in LVLMs",
        "author": "Yudong Zhang, Ruobing Xie, Yiqing Huang, Jiansheng Chen, Xingwu Sun, Zhanhui Kang, Di Wang, and Yu Wang",
        "link": "http://arxiv.org/abs/2506.01064v2",
        "abstract": "Recent advances in large vision-language models (LVLMs) have showcased their\nremarkable capabilities across a wide range of multimodal vision-language\ntasks. However, these models remain vulnerable to visual adversarial attacks,\nwhich can substantially compromise their performance. Despite their potential\nimpact, the development of effective methods for purifying such adversarial\nexamples has received relatively limited attention. In this paper, we introduce\nF3, a novel adversarial purification framework that employs a counterintuitive\n\"fighting fire with fire\" strategy: intentionally introducing simple\nperturbations to adversarial examples to mitigate their harmful effects.\nSpecifically, F3 leverages cross-modal attentions derived from randomly\nperturbed adversary examples as reference targets. By injecting noise into\nthese adversarial examples, F3 effectively refines their attention, resulting\nin cleaner and more reliable model outputs. Remarkably, this seemingly\nparadoxical approach of employing noise to counteract adversarial attacks\nyields impressive purification results. Furthermore, F3 offers several distinct\nadvantages: it is training-free and straightforward to implement, and exhibits\nsignificant computational efficiency improvements compared to existing\npurification methods. These attributes render F3 particularly suitable for\nlarge-scale industrial applications where both robust performance and\noperational efficiency are critical priorities. The code will be made publicly\navailable."
    },
    {
        "date": "2025-06",
        "title": "Simple Prompt Injection Attacks Can Leak Personal Data Observed by LLM Agents During Task Execution",
        "author": "Meysam Alizadeh, Zeynab Samei, Daria Stetsenko, and Fabrizio Gilardi",
        "link": "http://arxiv.org/abs/2506.01055v1",
        "abstract": "Previous benchmarks on prompt injection in large language models (LLMs) have\nprimarily focused on generic tasks and attacks, offering limited insights into\nmore complex threats like data exfiltration. This paper examines how prompt\ninjection can cause tool-calling agents to leak personal data observed during\ntask execution. Using a fictitious banking agent, we develop data flow-based\nattacks and integrate them into AgentDojo, a recent benchmark for agentic\nsecurity. To enhance its scope, we also create a richer synthetic dataset of\nhuman-AI banking conversations. In 16 user tasks from AgentDojo, LLMs show a\n15-50 percentage point drop in utility under attack, with average attack\nsuccess rates (ASR) around 20 percent; some defenses reduce ASR to zero. Most\nLLMs, even when successfully tricked by the attack, avoid leaking highly\nsensitive data like passwords, likely due to safety alignments, but they remain\nvulnerable to disclosing other personal data. The likelihood of password\nleakage increases when a password is requested along with one or two additional\npersonal details. In an extended evaluation across 48 tasks, the average ASR is\naround 15 percent, with no built-in AgentDojo defense fully preventing leakage.\nTasks involving data extraction or authorization workflows, which closely\nresemble the structure of exfiltration attacks, exhibit the highest ASRs,\nhighlighting the interaction between task type, agent performance, and defense\nefficacy."
    },
    {
        "date": "2025-06",
        "title": "Autoregressive Images Watermarking through Lexical Biasing: An Approach Resistant to Regeneration Attack",
        "author": "Siqi Hui, Yiren Song, Sanping Zhou, Ye Deng, Wenli Huang, and Jinjun Wang",
        "link": "http://arxiv.org/abs/2506.01011v1",
        "abstract": "Autoregressive (AR) image generation models have gained increasing attention\nfor their breakthroughs in synthesis quality, highlighting the need for robust\nwatermarking to prevent misuse. However, existing in-generation watermarking\ntechniques are primarily designed for diffusion models, where watermarks are\nembedded within diffusion latent states. This design poses significant\nchallenges for direct adaptation to AR models, which generate images\nsequentially through token prediction. Moreover, diffusion-based regeneration\nattacks can effectively erase such watermarks by perturbing diffusion latent\nstates. To address these challenges, we propose Lexical Bias Watermarking\n(LBW), a novel framework designed for AR models that resists regeneration\nattacks. LBW embeds watermarks directly into token maps by biasing token\nselection toward a predefined green list during generation. This approach\nensures seamless integration with existing AR models and extends naturally to\npost-hoc watermarking. To increase the security against white-box attacks,\ninstead of using a single green list, the green list for each image is randomly\nsampled from a pool of green lists. Watermark detection is performed via\nquantization and statistical analysis of the token distribution. Extensive\nexperiments demonstrate that LBW achieves superior watermark robustness,\nparticularly in resisting regeneration attacks."
    },
    {
        "date": "2025-06",
        "title": "CAPAA: Classifier-Agnostic Projector-Based Adversarial Attack",
        "author": "Zhan Li, Mingyu Zhao, Xin Dong, Haibin Ling, and Bingyao Huang",
        "link": "http://arxiv.org/abs/2506.00978v2",
        "abstract": "Projector-based adversarial attack aims to project carefully designed light\npatterns (i.e., adversarial projections) onto scenes to deceive deep image\nclassifiers. It has potential applications in privacy protection and the\ndevelopment of more robust classifiers. However, existing approaches primarily\nfocus on individual classifiers and fixed camera poses, often neglecting the\ncomplexities of multi-classifier systems and scenarios with varying camera\nposes. This limitation reduces their effectiveness when introducing new\nclassifiers or camera poses. In this paper, we introduce Classifier-Agnostic\nProjector-Based Adversarial Attack (CAPAA) to address these issues. First, we\ndevelop a novel classifier-agnostic adversarial loss and optimization framework\nthat aggregates adversarial and stealthiness loss gradients from multiple\nclassifiers. Then, we propose an attention-based gradient weighting mechanism\nthat concentrates perturbations on regions of high classification activation,\nthereby improving the robustness of adversarial projections when applied to\nscenes with varying camera poses. Our extensive experimental evaluations\ndemonstrate that CAPAA achieves both a higher attack success rate and greater\nstealthiness compared to existing baselines. Codes are available at:\nhttps://github.com/ZhanLiQxQ/CAPAA."
    },
    {
        "date": "2025-06",
        "title": "Hidden Representation Clustering with Multi-Task Representation Learning towards Robust Online Budget Allocation",
        "author": "Xiaohan Wang, Yu Zhang, Guibin Jiang, Bing Cheng, and Wei Lin",
        "link": "http://arxiv.org/abs/2506.00959v1",
        "abstract": "Marketing optimization, commonly formulated as an online budget allocation\nproblem, has emerged as a pivotal factor in driving user growth. Most existing\nresearch addresses this problem by following the principle of 'first predict\nthen optimize' for each individual, which presents challenges related to\nlarge-scale counterfactual prediction and solving complexity trade-offs. Note\nthat the practical data quality is uncontrollable, and the solving scale tends\nto be tens of millions. Therefore, the existing approaches make the robust\nbudget allocation non-trivial, especially in industrial scenarios with\nconsiderable data noise. To this end, this paper proposes a novel approach that\nsolves the problem from the cluster perspective. Specifically, we propose a\nmulti-task representation network to learn the inherent attributes of\nindividuals and project the original features into high-dimension hidden\nrepresentations through the first two layers of the trained network. Then, we\ndivide these hidden representations into $K$ groups through partitioning-based\nclustering, thus reformulating the problem as an integer stochastic programming\nproblem under different total budgets. Finally, we distill the representation\nmodule and clustering model into a multi-category model to facilitate online\ndeployment. Offline experiments validate the effectiveness and superiority of\nour approach compared to six state-of-the-art marketing optimization\nalgorithms. Online A/B tests on the Meituan platform indicate that the approach\noutperforms the online algorithm by 0.53% and 0.65%, considering order volume\n(OV) and gross merchandise volume (GMV), respectively."
    },
    {
        "date": "2025-06",
        "title": "SafeGenes: Evaluating the Adversarial Robustness of Genomic Foundation Models",
        "author": "Huixin Zhan, and Jason H. Moore",
        "link": "http://arxiv.org/abs/2506.00821v1",
        "abstract": "Genomic Foundation Models (GFMs), such as Evolutionary Scale Modeling (ESM),\nhave demonstrated significant success in variant effect prediction. However,\ntheir adversarial robustness remains largely unexplored. To address this gap,\nwe propose SafeGenes: a framework for Secure analysis of genomic foundation\nmodels, leveraging adversarial attacks to evaluate robustness against both\nengineered near-identical adversarial Genes and embedding-space manipulations.\nIn this study, we assess the adversarial vulnerabilities of GFMs using two\napproaches: the Fast Gradient Sign Method (FGSM) and a soft prompt attack. FGSM\nintroduces minimal perturbations to input sequences, while the soft prompt\nattack optimizes continuous embeddings to manipulate model predictions without\nmodifying the input tokens. By combining these techniques, SafeGenes provides a\ncomprehensive assessment of GFM susceptibility to adversarial manipulation.\nTargeted soft prompt attacks led to substantial performance degradation, even\nin large models such as ESM1b and ESM1v. These findings expose critical\nvulnerabilities in current foundation models, opening new research directions\ntoward improving their security and robustness in high-stakes genomic\napplications such as variant effect prediction."
    },
    {
        "date": "2025-06",
        "title": "TIME: TabPFN-Integrated Multimodal Engine for Robust Tabular-Image Learning",
        "author": "Jiaqi Luo, Yuan Yuan, and Shixin Xu",
        "link": "http://arxiv.org/abs/2506.00813v1",
        "abstract": "Tabular-image multimodal learning, which integrates structured tabular data\nwith imaging data, holds great promise for a variety of tasks, especially in\nmedical applications. Yet, two key challenges remain: (1) the lack of a\nstandardized, pretrained representation for tabular data, as is commonly\navailable in vision and language domains; and (2) the difficulty of handling\nmissing values in the tabular modality, which are common in real-world medical\ndatasets. To address these issues, we propose the TabPFN-Integrated Multimodal\nEngine (TIME), a novel multimodal framework that builds on the recently\nintroduced tabular foundation model, TabPFN. TIME leverages TabPFN as a frozen\ntabular encoder to generate robust, strong embeddings that are naturally\nresilient to missing data, and combines them with image features from\npretrained vision backbones. We explore a range of fusion strategies and\ntabular encoders, and evaluate our approach on both natural and medical\ndatasets. Extensive experiments demonstrate that TIME consistently outperforms\ncompetitive baselines across both complete and incomplete tabular inputs,\nunderscoring its practical value in real-world multimodal learning scenarios."
    },
    {
        "date": "2025-06",
        "title": "Unlearning Inversion Attacks for Graph Neural Networks",
        "author": "Jiahao Zhang, Yilong Wang, Zhiwei Zhang, Xiaorui Liu, and Suhang Wang",
        "link": "http://arxiv.org/abs/2506.00808v1",
        "abstract": "Graph unlearning methods aim to efficiently remove the impact of sensitive\ndata from trained GNNs without full retraining, assuming that deleted\ninformation cannot be recovered. In this work, we challenge this assumption by\nintroducing the graph unlearning inversion attack: given only black-box access\nto an unlearned GNN and partial graph knowledge, can an adversary reconstruct\nthe removed edges? We identify two key challenges: varying\nprobability-similarity thresholds for unlearned versus retained edges, and the\ndifficulty of locating unlearned edge endpoints, and address them with\nTrendAttack. First, we derive and exploit the confidence pitfall, a theoretical\nand empirical pattern showing that nodes adjacent to unlearned edges exhibit a\nlarge drop in model confidence. Second, we design an adaptive prediction\nmechanism that applies different similarity thresholds to unlearned and other\nmembership edges. Our framework flexibly integrates existing membership\ninference techniques and extends them with trend features. Experiments on four\nreal-world datasets demonstrate that TrendAttack significantly outperforms\nstate-of-the-art GNN membership inference baselines, exposing a critical\nprivacy vulnerability in current graph unlearning methods."
    },
    {
        "date": "2025-05",
        "title": "Poster: Adapting Pretrained Vision Transformers with LoRA Against Attack Vectors",
        "author": "Richard E. Neddo, Sean Willis, Zander Blasingame, and Chen Liu",
        "link": "http://arxiv.org/abs/2506.00661v1",
        "abstract": "Image classifiers, such as those used for autonomous vehicle navigation, are\nlargely known to be susceptible to adversarial attacks that target the input\nimage set. There is extensive discussion on adversarial attacks including\nperturbations that alter the input images to cause malicious misclassifications\nwithout perceivable modification. This work proposes a countermeasure for such\nattacks by adjusting the weights and classes of pretrained vision transformers\nwith a low-rank adaptation to become more robust against adversarial attacks\nand allow for scalable fine-tuning without retraining."
    },
    {
        "date": "2025-05",
        "title": "Amatriciana: Exploiting Temporal GNNs for Robust and Efficient Money Laundering Detection",
        "author": "Marco Di Gennaro, Francesco Panebianco, Marco Pianta, Stefano Zanero, and Michele Carminati",
        "link": "http://arxiv.org/abs/2506.00654v1",
        "abstract": "Money laundering is a financial crime that poses a serious threat to\nfinancial integrity and social security. The growing number of transactions\nmakes it necessary to use automatic tools that help law enforcement agencies\ndetect such criminal activity. In this work, we present Amatriciana, a novel\napproach based on Graph Neural Networks to detect money launderers inside a\ngraph of transactions by considering temporal information. Amatriciana uses the\nwhole graph of transactions without splitting it into several time-based\nsubgraphs, exploiting all relational information in the dataset. Our\nexperiments on a public dataset reveal that the model can learn from a limited\namount of data. Furthermore, when more data is available, the model outperforms\nother State-of-the-art approaches; in particular, Amatriciana decreases the\nnumber of False Positives (FPs) while detecting many launderers. In summary,\nAmatriciana achieves an F1 score of 0.76. In addition, it lowers the FPs by 55%\nwith respect to other State-of-the-art models."
    },
    {
        "date": "2025-05",
        "title": "AgentAuditor: Human-Level Safety and Security Evaluation for LLM Agents",
        "author": "Hanjun Luo, Shenyu Dai, Chiming Ni, Xinfeng Li, Guibin Zhang, Kun Wang, Tongliang Liu, and Hanan Salam",
        "link": "http://arxiv.org/abs/2506.00641v1",
        "abstract": "Despite the rapid advancement of LLM-based agents, the reliable evaluation of\ntheir safety and security remains a significant challenge. Existing rule-based\nor LLM-based evaluators often miss dangers in agents' step-by-step actions,\noverlook subtle meanings, fail to see how small issues compound, and get\nconfused by unclear safety or security rules. To overcome this evaluation\ncrisis, we introduce \\sys, a universal, training-free, memory-augmented\nreasoning framework that empowers LLM evaluators to emulate human expert\nevaluators. \\sys constructs an experiential memory by having an LLM adaptively\nextract structured semantic features (e.g., scenario, risk, behavior) and\ngenerate associated chain-of-thought reasoning traces for past interactions. A\nmulti-stage, context-aware retrieval-augmented generation process then\ndynamically retrieves the most relevant reasoning experiences to guide the LLM\nevaluator's assessment of new cases. Moreover, we developed \\data, the first\nbenchmark designed to check how well LLM-based evaluators can spot both safety\nrisks and security threats. \\data comprises \\textbf{2293} meticulously\nannotated interaction records, covering \\textbf{15} risk types across\n\\textbf{29} application scenarios. A key feature of \\data is its nuanced\napproach to ambiguous risk situations, employing ``Strict'' and ``Lenient''\njudgment standards. Experiments demonstrate that \\sys not only consistently\nimproves the evaluation performance of LLMs across all benchmarks but also sets\na new state-of-the-art in LLM-as-a-judge for agent safety and security,\nachieving human-level accuracy. Our work is openly openly accessible."
    },
    {
        "date": "2025-05",
        "title": "A \"Wenlu\" Brain System for Multimodal Cognition and Embodied Decision-Making: A Secure New Architecture for Deep Integration of Foundation Models and Domain Knowledge",
        "author": "Liang Geng",
        "link": "http://arxiv.org/abs/2506.00570v1",
        "abstract": "With the rapid penetration of artificial intelligence across industries and\nscenarios, a key challenge in building the next-generation intelligent core\nlies in effectively integrating the language understanding capabilities of\nfoundation models with domain-specific knowledge bases in complex real-world\napplications. This paper proposes a multimodal cognition and embodied\ndecision-making brain system, ``Wenlu\", designed to enable secure fusion of\nprivate knowledge and public models, unified processing of multimodal data such\nas images and speech, and closed-loop decision-making from cognition to\nautomatic generation of hardware-level code. The system introduces a\nbrain-inspired memory tagging and replay mechanism, seamlessly integrating\nuser-private data, industry-specific knowledge, and general-purpose language\nmodels. It provides precise and efficient multimodal services for enterprise\ndecision support, medical analysis, autonomous driving, robotic control, and\nmore. Compared with existing solutions, ``Wenlu\" demonstrates significant\nadvantages in multimodal processing, privacy security, end-to-end hardware\ncontrol code generation, self-learning, and sustainable updates, thus laying a\nsolid foundation for constructing the next-generation intelligent core."
    },
    {
        "date": "2025-05",
        "title": "Docker under Siege: Securing Containers in the Modern Era",
        "author": "Gogulakrishnan Thiyagarajan, and Prabhudarshi Nayak",
        "link": "http://arxiv.org/abs/2506.02043v1",
        "abstract": "Containerization, driven by Docker, has transformed application development\nand deployment by enhancing efficiency and scalability. However, the rapid\nadoption of container technologies introduces significant security challenges\nthat require careful management. This paper investigates key areas of container\nsecurity, including runtime protection, network safeguards, configuration best\npractices, supply chain security, and comprehensive monitoring and logging\nsolutions. We identify common vulnerabilities within these domains and provide\nactionable recommendations to address and mitigate these risks. By integrating\nsecurity throughout the Software Development Lifecycle (SDLC), organizations\ncan reinforce their security posture, creating a resilient and reliable\ncontainerized application infrastructure that withstands evolving threats."
    },
    {
        "date": "2025-05",
        "title": "The Security Threat of Compressed Projectors in Large Vision-Language Models",
        "author": "Yudong Zhang, Ruobing Xie, Xingwu Sun, Jiansheng Chen, Zhanhui Kang, Di Wang, and Yu Wang",
        "link": "http://arxiv.org/abs/2506.00534v1",
        "abstract": "The choice of a suitable visual language projector (VLP) is critical to the\nsuccessful training of large visual language models (LVLMs). Mainstream VLPs\ncan be broadly categorized into compressed and uncompressed projectors, and\neach offering distinct advantages in performance and computational efficiency.\nHowever, their security implications have not been thoroughly examined. Our\ncomprehensive evaluation reveals significant differences in their security\nprofiles: compressed projectors exhibit substantial vulnerabilities, allowing\nadversaries to successfully compromise LVLMs even with minimal knowledge of\nstructural information. In stark contrast, uncompressed projectors demonstrate\nrobust security properties and do not introduce additional vulnerabilities.\nThese findings provide critical guidance for researchers in selecting optimal\nVLPs that enhance the security and reliability of visual language models. The\ncode will be released."
    },
    {
        "date": "2025-05",
        "title": "Robust and Verifiable MPC with Applications to Linear Machine Learning Inference",
        "author": "Tzu-Shen Wang, Jimmy Dani, Juan Garay, Soamar Homsi, and Nitesh Saxena",
        "link": "http://arxiv.org/abs/2506.00518v1",
        "abstract": "In this work, we present an efficient secure multi-party computation MPC\nprotocol that provides strong security guarantees in settings with dishonest\nmajority of participants who may behave arbitrarily. Unlike the popular MPC\nimplementation known as SPDZ [Crypto '12], which only ensures security with\nabort, our protocol achieves both complete identifiability and robustness. With\ncomplete identifiability, honest parties can detect and unanimously agree on\nthe identity of any malicious party. Robustness allows the protocol to continue\nwith the computation without requiring a restart, even when malicious behavior\nis detected. Additionally, our approach addresses the performance limitations\nobserved in the protocol by Cunningham et al. [ICITS '17], which, while\nachieving complete identifiability, is hindered by the costly exponentiation\noperations required by the choice of commitment scheme.\n  Our protocol is based on the approach by Rivinius et al. [S&P '22], utilizing\nlattice-based commitment for better efficiency. We achieved robustness with the\nhelp of a semi-honest trusted third party. We benchmark our robust protocol,\nshowing the efficient recovery from parties' malicious behavior.\n  Finally, we benchmark our protocol on a ML-as-a-service scenario, wherein\nclients off-load the desired computation to the servers, and verify the\ncomputation result. We benchmark on linear ML inference, running on various\ndatasets. While our efficiency is slightly lower compared to SPDZ's, we offer\nstronger security properties that provide distinct advantages."
    },
    {
        "date": "2025-05",
        "title": "Monitoring Robustness and Individual Fairness",
        "author": "Ashutosh Gupta, Thomas A. Henzinger, Konstantin Kueffner, Kaushik Mallik, and David Pape",
        "link": "http://arxiv.org/abs/2506.00496v1",
        "abstract": "Input-output robustness appears in various different forms in the literature,\nsuch as robustness of AI models to adversarial or semantic perturbations and\nindividual fairness of AI models that make decisions about humans.\n  We propose runtime monitoring of input-output robustness of deployed,\nblack-box AI models, where the goal is to design monitors that would observe\none long execution sequence of the model, and would raise an alarm whenever it\nis detected that two similar inputs from the past led to dissimilar outputs.\n  This way, monitoring will complement existing offline ``robustification''\napproaches to increase the trustworthiness of AI decision-makers.\n  We show that the monitoring problem can be cast as the fixed-radius nearest\nneighbor (FRNN) search problem, which, despite being well-studied, lacks\nsuitable online solutions.\n  We present our tool Clemont, which offers a number of lightweight monitors,\nsome of which use upgraded online variants of existing FRNN algorithms, and one\nuses a novel algorithm based on binary decision diagrams -- a data-structure\ncommonly used in software and hardware verification.\n  We have also developed an efficient parallelization technique that can\nsubstantially cut down the computation time of monitors for which the distance\nbetween input-output pairs is measured using the $L_\\infty$ norm.\n  Using standard benchmarks from the literature of adversarial and semantic\nrobustness and individual fairness, we perform a comparative study of different\nmonitors in \\tool, and demonstrate their effectiveness in correctly detecting\nrobustness violations at runtime."
    },
    {
        "date": "2025-05",
        "title": "Beyond the Protocol: Unveiling Attack Vectors in the Model Context Protocol Ecosystem",
        "author": "Hao Song, Yiming Shen, Wenxuan Luo, Leixin Guo, Ting Chen, Jiashui Wang, Beibei Li, Xiaosong Zhang, and Jiachi Chen",
        "link": "http://arxiv.org/abs/2506.02040v2",
        "abstract": "The Model Context Protocol (MCP) is an emerging standard designed to enable\nseamless interaction between Large Language Model (LLM) applications and\nexternal tools or resources. Within a short period, thousands of MCP services\nhave already been developed and deployed. However, the client-server\nintegration architecture inherent in MCP may expand the attack surface against\nLLM Agent systems, introducing new vulnerabilities that allow attackers to\nexploit by designing malicious MCP servers. In this paper, we present the first\nsystematic study of attack vectors targeting the MCP ecosystem. Our analysis\nidentifies four categories of attacks, i.e., Tool Poisoning Attacks, Puppet\nAttacks, Rug Pull Attacks, and Exploitation via Malicious External Resources.\nTo evaluate the feasibility of these attacks, we conduct experiments following\nthe typical steps of launching an attack through malicious MCP servers:\nupload-download-attack. Specifically, we first construct malicious MCP servers\nand successfully upload them to three widely used MCP aggregation platforms.\nThe results indicate that current audit mechanisms are insufficient to identify\nand prevent the proposed attack methods. Next, through a user study and\ninterview with 20 participants, we demonstrate that users struggle to identify\nmalicious MCP servers and often unknowingly install them from aggregator\nplatforms. Finally, we demonstrate that these attacks can trigger harmful\nbehaviors within the user's local environment-such as accessing private files\nor controlling devices to transfer digital assets-by deploying a\nproof-of-concept (PoC) framework against five leading LLMs. Additionally, based\non interview results, we discuss four key challenges faced by the current\nsecurity ecosystem surrounding MCP servers. These findings underscore the\nurgent need for robust security mechanisms to defend against malicious MCP\nservers."
    },
    {
        "date": "2025-05",
        "title": "Hybrid Cloud Security: Balancing Performance, Cost, and Compliance in Multi-Cloud Deployments",
        "author": "Anjani kumar Polinati",
        "link": "http://arxiv.org/abs/2506.00426v1",
        "abstract": "The pervasive use of hybrid cloud computing models has changed enterprise as\nwell as Information Technology services infrastructure by giving businesses\nsimple and cost-effective options of combining on-premise IT equipment with\npublic cloud services. hybrid cloud solutions deploy multifaceted models of\nsecurity, performance optimization, and cost efficiency, conventionally\nfragmented in the cloud computing milieu. This paper examines how organizations\nmanage these parameters in hybrid cloud ecosystems while providing solutions to\nthe challenges they face in operationalizing hybrid cloud adoptions. The study\ncaptures the challenges of achieving a balance in resource distribution between\non-premise and cloud resources (herein referred to as the \"resource allocation\nchallenge\"), the complexity of pricing models from cloud providers like AWS,\nMicrosoft Azure, Google Cloud (herein called the 'pricing complexity problem'),\nand the urgency for strong security infrastructure to safeguard sensitive\ninformation (known as 'the information security problem'). This study\ndemonstrates the security and performance management solutions proposed were\nvalidated in a detailed case study of adoption of AWS and Azure based hybrid\ncloud and provides useful guidance. Also, a hybrid cloud security and cost\noptimization framework based on zero trust architecture, encryption, hybrid\ncloud policies, and others, is proposed.\n  The conclusion includes recommendations for research on automation of hybrid\ncloud service management, integration of multi-clouds, and the ever-present\nquestion of data privacy, stressing how those matters affect contemporary\nenterprises."
    },
    {
        "date": "2025-05",
        "title": "Teaching an Old LLM Secure Coding: Localized Preference Optimization on Distilled Preferences",
        "author": "Mohammad Saqib, Saikat Chakraborty, Santu Karmaker, and Niranjan Balasubramanian",
        "link": "http://arxiv.org/abs/2506.00419v1",
        "abstract": "LLM generated code often contains security issues. We address two key\nchallenges in improving secure code generation. First, obtaining high quality\ntraining data covering a broad set of security issues is critical. To address\nthis, we introduce a method for distilling a preference dataset of insecure and\nsecure code pairs from frontier LLMs, along with a security reasoning that\nexplains the issues and the fix. The key idea here is to make use of security\nknowledge sources to devise a systematic prompting strategy that ensures broad\ncoverage. Second, aligning models to secure code requires focusing on localized\nregions of code. Direct preference optimization methods, like SimPO, are not\ndesigned to handle these localized differences and turn out to be ineffective.\nWe address this with a new localized preference optimization algorithm that\nmasks the security related tokens in both the winning (secure) and losing\n(insecure) responses. To prevent loss in code quality, we also add a\nregularizer. Evaluations show that both training on our dataset, DiSCo, and the\nnew preference optimization algorithm, LPO, yield substantial reductions in\ncode insecurity while also improving overall code quality. Code and dataset are\navailable at https://github.com/StonyBrookNLP/disco-lpo."
    },
    {
        "date": "2025-05",
        "title": "Label-shift robust federated feature screening for high-dimensional classification",
        "author": "Qi Qin, Erbo Li, Xingxiang Li, Yifan Sun, Wu Wang, and Chen Xu",
        "link": "http://arxiv.org/abs/2506.00379v1",
        "abstract": "Distributed and federated learning are important tools for high-dimensional\nclassification of large datasets. To reduce computational costs and overcome\nthe curse of dimensionality, feature screening plays a pivotal role in\neliminating irrelevant features during data preprocessing. However, data\nheterogeneity, particularly label shifting across different clients, presents\nsignificant challenges for feature screening. This paper introduces a general\nframework that unifies existing screening methods and proposes a novel utility,\nlabel-shift robust federated feature screening (LR-FFS), along with its\nfederated estimation procedure. The framework facilitates a uniform analysis of\nmethods and systematically characterizes their behaviors under label shift\nconditions. Building upon this framework, LR-FFS leverages conditional\ndistribution functions and expectations to address label shift without adding\ncomputational burdens and remains robust against model misspecification and\noutliers. Additionally, the federated procedure ensures computational\nefficiency and privacy protection while maintaining screening effectiveness\ncomparable to centralized processing. We also provide a false discovery rate\n(FDR) control method for federated feature screening. Experimental results and\ntheoretical analyses demonstrate LR-FFS's superior performance across diverse\nclient environments, including those with varying class distributions, sample\nsizes, and missing categorical data."
    },
    {
        "date": "2025-05",
        "title": "Adversarial Machine Learning for Robust Password Strength Estimation",
        "author": "Pappu Jha, Hanzla Hamid, Oluseyi Olukola, Ashim Dahal, and Nick Rahimi",
        "link": "http://arxiv.org/abs/2506.00373v1",
        "abstract": "Passwords remain one of the most common methods for securing sensitive data\nin the digital age. However, weak password choices continue to pose significant\nrisks to data security and privacy. This study aims to solve the problem by\nfocusing on developing robust password strength estimation models using\nadversarial machine learning, a technique that trains models on intentionally\ncrafted deceptive passwords to expose and address vulnerabilities posed by such\npasswords. We apply five classification algorithms and use a dataset with more\nthan 670,000 samples of adversarial passwords to train the models. Results\ndemonstrate that adversarial training improves password strength classification\naccuracy by up to 20% compared to traditional machine learning models. It\nhighlights the importance of integrating adversarial machine learning into\nsecurity systems to enhance their robustness against modern adaptive threats.\n  Keywords: adversarial attack, password strength, classification, machine\nlearning"
    },
    {
        "date": "2025-05",
        "title": "$\\texttt{AVROBUSTBENCH}$: Benchmarking the Robustness of Audio-Visual Recognition Models at Test-Time",
        "author": "Sarthak Kumar Maharana, Saksham Singh Kushwaha, Baoming Zhang, Adrian Rodriguez, Songtao Wei, Yapeng Tian, and Yunhui Guo",
        "link": "http://arxiv.org/abs/2506.00358v1",
        "abstract": "While recent audio-visual models have demonstrated impressive performance,\ntheir robustness to distributional shifts at test-time remains not fully\nunderstood. Existing robustness benchmarks mainly focus on single modalities,\nmaking them insufficient for thoroughly assessing the robustness of\naudio-visual models. Motivated by real-world scenarios where shifts can occur\n$\\textit{simultaneously}$ in both audio and visual modalities, we introduce\n$\\texttt{AVROBUSTBENCH}$, a comprehensive benchmark designed to evaluate the\ntest-time robustness of audio-visual recognition models.\n$\\texttt{AVROBUSTBENCH}$ comprises four audio-visual benchmark datasets,\n$\\texttt{AUDIOSET-2C}$, $\\texttt{VGGSOUND-2C}$, $\\texttt{KINETICS-2C}$, and\n$\\texttt{EPICKITCHENS-2C}$, each incorporating 75 bimodal audio-visual\ncorruptions that are $\\textit{co-occurring}$ and $\\textit{correlated}$. Through\nextensive evaluations, we observe that state-of-the-art supervised and\nself-supervised audio-visual models exhibit declining robustness as corruption\nseverity increases. Furthermore, online test-time adaptation (TTA) methods, on\n$\\texttt{VGGSOUND-2C}$ and $\\texttt{KINETICS-2C}$, offer minimal improvements\nin performance under bimodal corruptions. We further propose $\\texttt{AV2C}$, a\nsimple TTA approach enabling on-the-fly cross-modal fusion by penalizing\nhigh-entropy samples, which achieves improvements on $\\texttt{VGGSOUND-2C}$. We\nhope that $\\texttt{AVROBUSTBENCH}$ will steer the development of more effective\nand robust audio-visual TTA approaches. Our code is available\n$\\href{https://github.com/sarthaxxxxx/AV-C-Robustness-Benchmark}{here}$."
    },
    {
        "date": "2025-05",
        "title": "Enabling Secure and Ephemeral AI Workloads in Data Mesh Environments",
        "author": "Chinkit Patel, and Kee Siong Ng",
        "link": "http://arxiv.org/abs/2506.00352v1",
        "abstract": "Many large enterprises that operate highly governed and complex ICT\nenvironments have no efficient and effective way to support their Data and AI\nteams in rapidly spinning up and tearing down self-service data and compute\ninfrastructure, to experiment with new data analytic tools, and deploy data\nproducts into operational use. This paper proposes a key piece of the solution\nto the overall problem, in the form of an on-demand self-service data-platform\ninfrastructure to empower de-centralised data teams to build data products on\ntop of centralised templates, policies and governance. The core innovation is\nan efficient method to leverage immutable container operating systems and\ninfrastructure-as-code methodologies for creating, from scratch, vendor-neutral\nand short-lived Kubernetes clusters on-premises and in any cloud environment.\nOur proposed approach can serve as a repeatable, portable and cost-efficient\nalternative or complement to commercial Platform-as-a-Service (PaaS) offerings,\nand this is particularly important in supporting interoperability in complex\ndata mesh environments with a mix of modern and legacy compute infrastructure."
    },
    {
        "date": "2025-05",
        "title": "Towards Effective and Efficient Adversarial Defense with Diffusion Models for Robust Visual Tracking",
        "author": "Long Xu, Peng Gao, Wen-Jia Tang, Fei Wang, and Ru-Yue Yuan",
        "link": "http://arxiv.org/abs/2506.00325v1",
        "abstract": "Although deep learning-based visual tracking methods have made significant\nprogress, they exhibit vulnerabilities when facing carefully designed\nadversarial attacks, which can lead to a sharp decline in tracking performance.\nTo address this issue, this paper proposes for the first time a novel\nadversarial defense method based on denoise diffusion probabilistic models,\ntermed DiffDf, aimed at effectively improving the robustness of existing visual\ntracking methods against adversarial attacks. DiffDf establishes a multi-scale\ndefense mechanism by combining pixel-level reconstruction loss, semantic\nconsistency loss, and structural similarity loss, effectively suppressing\nadversarial perturbations through a gradual denoising process. Extensive\nexperimental results on several mainstream datasets show that the DiffDf method\ndemonstrates excellent generalization performance for trackers with different\narchitectures, significantly improving various evaluation metrics while\nachieving real-time inference speeds of over 30 FPS, showcasing outstanding\ndefense performance and efficiency. Codes are available at\nhttps://github.com/pgao-lab/DiffDf."
    },
    {
        "date": "2025-05",
        "title": "Adversarial Threat Vectors and Risk Mitigation for Retrieval-Augmented Generation Systems",
        "author": "Chris M. Ward, and Josh Harguess",
        "link": "http://arxiv.org/abs/2506.00281v1",
        "abstract": "Retrieval-Augmented Generation (RAG) systems, which integrate Large Language\nModels (LLMs) with external knowledge sources, are vulnerable to a range of\nadversarial attack vectors. This paper examines the importance of RAG systems\nthrough recent industry adoption trends and identifies the prominent attack\nvectors for RAG: prompt injection, data poisoning, and adversarial query\nmanipulation. We analyze these threats under risk management lens, and propose\nrobust prioritized control list that includes risk-mitigating actions like\ninput validation, adversarial training, and real-time monitoring."
    },
    {
        "date": "2025-05",
        "title": "DeGLIF for Label Noise Robust Node Classification using GNNs",
        "author": "Pintu Kumar, and Nandyala Hemachandra",
        "link": "http://arxiv.org/abs/2506.00244v1",
        "abstract": "Noisy labelled datasets are generally inexpensive compared to clean labelled\ndatasets, and the same is true for graph data. In this paper, we propose a\ndenoising technique DeGLIF: Denoising Graph Data using Leave-One-Out Influence\nFunction. DeGLIF uses a small set of clean data and the leave-one-out influence\nfunction to make label noise robust node-level prediction on graph data.\nLeave-one-out influence function approximates the change in the model\nparameters if a training point is removed from the training dataset. Recent\nadvances propose a way to calculate the leave-one-out influence function for\nGraph Neural Networks (GNNs). We extend that recent work to estimate the change\nin validation loss, if a training node is removed from the training dataset. We\nuse this estimate and a new theoretically motivated relabelling function to\ndenoise the training dataset. We propose two DeGLIF variants to identify noisy\nnodes. Both these variants do not require any information about the noise model\nor the noise level in the dataset; DeGLIF also does not estimate these\nquantities. For one of these variants, we prove that the noisy points detected\ncan indeed increase risk. We carry out detailed computational experiments on\ndifferent datasets to show the effectiveness of DeGLIF. It achieves better\naccuracy than other baseline algorithms"
    },
    {
        "date": "2025-05",
        "title": "Heterogeneous Graph Backdoor Attack",
        "author": "Jiawei Chen, Lusi Li, Daniel Takabi, Masha Sosonkina, and Rui Ning",
        "link": "http://arxiv.org/abs/2506.00191v1",
        "abstract": "Heterogeneous Graph Neural Networks (HGNNs) excel in modeling complex,\nmulti-typed relationships across diverse domains, yet their vulnerability to\nbackdoor attacks remains unexplored. To address this gap, we conduct the first\ninvestigation into the susceptibility of HGNNs to existing graph backdoor\nattacks, revealing three critical issues: (1) high attack budget required for\neffective backdoor injection, (2) inefficient and unreliable backdoor\nactivation, and (3) inaccurate attack effectiveness evaluation. To tackle these\nissues, we propose the Heterogeneous Graph Backdoor Attack (HGBA), the first\nbackdoor attack specifically designed for HGNNs, introducing a novel\nrelation-based trigger mechanism that establishes specific connections between\na strategically selected trigger node and poisoned nodes via the backdoor\nmetapath. HGBA achieves efficient and stealthy backdoor injection with minimal\nstructural modifications and supports easy backdoor activation through two\nflexible strategies: Self-Node Attack and Indiscriminate Attack. Additionally,\nwe improve the ASR measurement protocol, enabling a more accurate assessment of\nattack effectiveness. Extensive experiments demonstrate that HGBA far surpasses\nmultiple state-of-the-art graph backdoor attacks in black-box settings,\nefficiently attacking HGNNs with low attack budgets. Ablation studies show that\nthe strength of HBGA benefits from our trigger node selection method and\nbackdoor metapath selection strategy. In addition, HGBA shows superior\nrobustness against node feature perturbations and multiple types of existing\ngraph backdoor defense mechanisms. Finally, extension experiments demonstrate\nthat the relation-based trigger mechanism can effectively extend to tasks in\nhomogeneous graph scenarios, thereby posing severe threats to broader\nsecurity-critical domains."
    },
    {
        "date": "2025-05",
        "title": "Towards Secure MLOps: Surveying Attacks, Mitigation Strategies, and Research Challenges",
        "author": "Raj Patel, Himanshu Tripathi, Jasper Stone, Noorbakhsh Amiri Golilarz, Sudip Mittal, Shahram Rahimi, and Vini Chaudhary",
        "link": "http://arxiv.org/abs/2506.02032v1",
        "abstract": "The rapid adoption of machine learning (ML) technologies has driven\norganizations across diverse sectors to seek efficient and reliable methods to\naccelerate model development-to-deployment. Machine Learning Operations (MLOps)\nhas emerged as an integrative approach addressing these requirements by\nunifying relevant roles and streamlining ML workflows. As the MLOps market\ncontinues to grow, securing these pipelines has become increasingly critical.\nHowever, the unified nature of MLOps ecosystem introduces vulnerabilities,\nmaking them susceptible to adversarial attacks where a single misconfiguration\ncan lead to compromised credentials, severe financial losses, damaged public\ntrust, and the poisoning of training data. Our paper presents a systematic\napplication of the MITRE ATLAS (Adversarial Threat Landscape for\nArtificial-Intelligence Systems) framework, a comprehensive and continuously\nupdated catalog of AI-focused attacks, to systematically assess attacks across\ndifferent phases of the MLOps ecosystem. We begin by examining the preparatory\nphases during which adversaries acquire the essential intelligence required to\ninitiate their attacks. We then present a structured taxonomy of attack\ntechniques explicitly mapped to corresponding phases of the MLOps ecosystem,\nsupported by examples drawn from red-teaming exercises and real-world\nincidents. This is followed by a taxonomy of mitigation strategies aligned with\nthese attack categories, offering actionable early-stage defenses to strengthen\nthe security of MLOps ecosystem. Given the rapid evolution and adoption of\nMLOps, we further highlight key research gaps that require immediate attention.\nOur work emphasizes the importance of implementing robust security protocols\nfrom the outset, empowering practitioners to safeguard MLOps ecosystem against\nevolving cyber attacks."
    },
    {
        "date": "2025-05",
        "title": "From Invariant Representations to Invariant Data: Provable Robustness to Spurious Correlations via Noisy Counterfactual Matching",
        "author": "Ruqi Bai, Yao Ji, Zeyu Zhou, and David I. Inouye",
        "link": "http://arxiv.org/abs/2505.24843v1",
        "abstract": "Spurious correlations can cause model performance to degrade in new\nenvironments. Prior causality-inspired works aim to learn invariant\nrepresentations (e.g., IRM) but typically underperform empirical risk\nminimization (ERM). Recent alternatives improve robustness by leveraging\ntest-time data, but such data may be unavailable in practice. To address these\nissues, we take a data-centric approach by leveraging invariant data pairs,\npairs of samples that would have the same prediction with the optimally robust\nclassifier. We prove that certain counterfactual pairs will naturally satisfy\nthis invariance property and introduce noisy counterfactual matching (NCM), a\nsimple constraint-based method for leveraging invariant pairs for enhanced\nrobustness, even with a small set of noisy pairs-in the ideal case, each pair\ncan eliminate one spurious feature. For linear causal models, we prove that the\ntest domain error can be upper bounded by the in-domain error and a term that\ndepends on the counterfactuals' diversity and quality. We validate on a\nsynthetic dataset and demonstrate on real-world benchmarks that linear probing\non a pretrained backbone improves robustness."
    },
    {
        "date": "2025-05",
        "title": "Cascading Adversarial Bias from Injection to Distillation in Language Models",
        "author": "Harsh Chaudhari, Jamie Hayes, Matthew Jagielski, Ilia Shumailov, Milad Nasr, and Alina Oprea",
        "link": "http://arxiv.org/abs/2505.24842v1",
        "abstract": "Model distillation has become essential for creating smaller, deployable\nlanguage models that retain larger system capabilities. However, widespread\ndeployment raises concerns about resilience to adversarial manipulation. This\npaper investigates vulnerability of distilled models to adversarial injection\nof biased content during training. We demonstrate that adversaries can inject\nsubtle biases into teacher models through minimal data poisoning, which\npropagates to student models and becomes significantly amplified. We propose\ntwo propagation modes: Untargeted Propagation, where bias affects multiple\ntasks, and Targeted Propagation, focusing on specific tasks while maintaining\nnormal behavior elsewhere. With only 25 poisoned samples (0.25% poisoning\nrate), student models generate biased responses 76.9% of the time in targeted\nscenarios - higher than 69.4% in teacher models. For untargeted propagation,\nadversarial bias appears 6x-29x more frequently in student models on unseen\ntasks. We validate findings across six bias types (targeted advertisements,\nphishing links, narrative manipulations, insecure coding practices), various\ndistillation methods, and different modalities spanning text and code\ngeneration. Our evaluation reveals shortcomings in current defenses -\nperplexity filtering, bias detection systems, and LLM-based autorater\nframeworks - against these attacks. Results expose significant security\nvulnerabilities in distilled models, highlighting need for specialized\nsafeguards. We propose practical design principles for building effective\nadversarial bias mitigation strategies."
    },
    {
        "date": "2025-05",
        "title": "ByzFL: Research Framework for Robust Federated Learning",
        "author": "Marc Gonz\u00e1lez, Rachid Guerraoui, Rafael Pinot, Geovani Rizk, John Stephan, and Fran\u00e7ois Ta\u00efani",
        "link": "http://arxiv.org/abs/2505.24802v1",
        "abstract": "We present ByzFL, an open-source Python library for developing and\nbenchmarking robust federated learning (FL) algorithms. ByzFL provides a\nunified and extensible framework that includes implementations of\nstate-of-the-art robust aggregators, a suite of configurable attacks, and tools\nfor simulating a variety of FL scenarios, including heterogeneous data\ndistributions, multiple training algorithms, and adversarial threat models. The\nlibrary enables systematic experimentation via a single JSON-based\nconfiguration file and includes built-in utilities for result visualization.\nCompatible with PyTorch tensors and NumPy arrays, ByzFL is designed to\nfacilitate reproducible research and rapid prototyping of robust FL solutions.\nByzFL is available at https://byzfl.epfl.ch/, with source code hosted on\nGitHub: https://github.com/LPD-EPFL/byzfl."
    },
    {
        "date": "2025-05",
        "title": "Robust Federated Learning against Model Perturbation in Edge Networks",
        "author": "Dongzi Jin, Yong Xiao, and Yingyu Li",
        "link": "http://arxiv.org/abs/2505.24728v1",
        "abstract": "Federated Learning (FL) is a promising paradigm for realizing edge\nintelligence, allowing collaborative learning among distributed edge devices by\nsharing models instead of raw data. However, the shared models are often\nassumed to be ideal, which would be inevitably violated in practice due to\nvarious perturbations, leading to significant performance degradation. To\novercome this challenge, we propose a novel method, termed Sharpness-Aware\nMinimization-based Robust Federated Learning (SMRFL), which aims to improve\nmodel robustness against perturbations by exploring the geometrical property of\nthe model landscape. Specifically, SMRFL solves a min-max optimization problem\nthat promotes model convergence towards a flat minimum by minimizing the\nmaximum loss within a neighborhood of the model parameters. In this way, model\nsensitivity to perturbations is reduced, and robustness is enhanced since\nmodels in the neighborhood of the flat minimum also enjoy low loss values. The\ntheoretical result proves that SMRFL can converge at the same rate as FL\nwithout perturbations. Extensive experimental results show that SMRFL\nsignificantly enhances robustness against perturbations compared to three\nbaseline methods on two real-world datasets under three perturbation scenarios."
    },
    {
        "date": "2025-05",
        "title": "On Symmetric Losses for Robust Policy Optimization with Noisy Preferences",
        "author": "Soichiro Nishimori, Yu-Jie Zhang, Thanawat Lodkaew, and Masashi Sugiyama",
        "link": "http://arxiv.org/abs/2505.24709v1",
        "abstract": "Optimizing policies based on human preferences is key to aligning language\nmodels with human intent. This work focuses on reward modeling, a core\ncomponent in reinforcement learning from human feedback (RLHF), and offline\npreference optimization, such as direct preference optimization. Conventional\napproaches typically assume accurate annotations. However, real-world\npreference data often contains noise due to human errors or biases. We propose\na principled framework for robust policy optimization under noisy preferences,\nviewing reward modeling as a classification problem. This allows us to leverage\nsymmetric losses, known for their robustness to label noise in classification,\nleading to our Symmetric Preference Optimization (SymPO) method. We prove that\nsymmetric losses enable successful policy optimization even under noisy labels,\nas the resulting reward remains rank-preserving -- a property sufficient for\npolicy improvement. Experiments on synthetic and real-world tasks demonstrate\nthe effectiveness of SymPO."
    }
]