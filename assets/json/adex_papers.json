[
    {
        "date": "2025-07",
        "title": "Development and analysis of a secured VoIP system for surveillance activities",
        "author": "M. Matsive Ali",
        "link": "http://arxiv.org/abs/2507.21038v2",
        "abstract": "Since the 1990s, the telephone has been the primary mode of communication.\nHowever, Voice over Internet Protocol (VoIP), which is a highly straightforward\nand affordable form of data transfer, is now becoming an important part of\ndaily communication. VoIP is the technology that makes it possible to send\nspeech and multimedia data packets across either a public or private IP\nnetwork. However, a cyberattack known as a man-in-the-middle attack poses a\nserious concern in transferring data through any network. Therefore, the\nauthors have designed a system that sends voice over the internet within the\nrange of a router using encrypted data transfer. An embedded system comprising\nan electret microphone, Embedded C, Particle Photon microcontroller, and\nInternet of Things (IoT) technology is developed. Due to its compact size, this\ntype of device may be incorporated into automobiles, surveillance systems, or\ncovert listening tools. The VoIP system gathers sound signals using the MAX9814\nmicrophone, while the Particle Photon microcontroller securely transmits the\ndata. Devices with access can download data from the VoIP systems Transmission\nControl Protocol (TCP) server. The accessed device stores the audio locally and\nuploads the corresponding data to Google Drive. This VoIP system provides a\nsecure method of communication while conserving the integrity of the original\nsignal."
    },
    {
        "date": "2025-07",
        "title": "Improving Adversarial Robustness Through Adaptive Learning-Driven Multi-Teacher Knowledge Distillation",
        "author": "Hayat Ullah, Syed Muhammad Talha Zaidi, and Arslan Munir",
        "link": "http://arxiv.org/abs/2507.20996v1",
        "abstract": "Convolutional neural networks (CNNs) excel in computer vision but are\nsusceptible to adversarial attacks, crafted perturbations designed to mislead\npredictions. Despite advances in adversarial training, a gap persists between\nmodel accuracy and robustness. To mitigate this issue, in this paper, we\npresent a multi-teacher adversarial robustness distillation using an adaptive\nlearning strategy. Specifically, our proposed method first trained multiple\nclones of a baseline CNN model using an adversarial training strategy on a pool\nof perturbed data acquired through different adversarial attacks. Once trained,\nthese adversarially trained models are used as teacher models to supervise the\nlearning of a student model on clean data using multi-teacher knowledge\ndistillation. To ensure an effective robustness distillation, we design an\nadaptive learning strategy that controls the knowledge contribution of each\nmodel by assigning weights as per their prediction precision. Distilling\nknowledge from adversarially pre-trained teacher models not only enhances the\nlearning capabilities of the student model but also empowers it with the\ncapacity to withstand different adversarial attacks, despite having no exposure\nto adversarial data. To verify our claims, we extensively evaluated our\nproposed method on MNIST-Digits and Fashion-MNIST datasets across diverse\nexperimental settings. The obtained results exhibit the efficacy of our\nmulti-teacher adversarial distillation and adaptive learning strategy,\nenhancing CNNs' adversarial robustness against various adversarial attacks."
    },
    {
        "date": "2025-07",
        "title": "Security Tensors as a Cross-Modal Bridge: Extending Text-Aligned Safety to Vision in LVLM",
        "author": "Shen Li, Liuyi Yao, Wujia Niu, Lan Zhang, and Yaliang Li",
        "link": "http://arxiv.org/abs/2507.20994v1",
        "abstract": "Large visual-language models (LVLMs) integrate aligned large language models\n(LLMs) with visual modules to process multimodal inputs. However, the safety\nmechanisms developed for text-based LLMs do not naturally extend to visual\nmodalities, leaving LVLMs vulnerable to harmful image inputs. To address this\ncross-modal safety gap, we introduce security tensors - trainable input vectors\napplied during inference through either the textual or visual modality. These\ntensors transfer textual safety alignment to visual processing without\nmodifying the model's parameters. They are optimized using a curated dataset\ncontaining (i) malicious image-text pairs requiring rejection, (ii) contrastive\nbenign pairs with text structurally similar to malicious queries, with the\npurpose of being contrastive examples to guide visual reliance, and (iii)\ngeneral benign samples preserving model functionality. Experimental results\ndemonstrate that both textual and visual security tensors significantly enhance\nLVLMs' ability to reject diverse harmful visual inputs while maintaining\nnear-identical performance on benign tasks. Further internal analysis towards\nhidden-layer representations reveals that security tensors successfully\nactivate the language module's textual \"safety layers\" in visual inputs,\nthereby effectively extending text-based safety to the visual modality."
    },
    {
        "date": "2025-07",
        "title": "Testbed and Software Architecture for Enhancing Security in Industrial Private 5G Networks",
        "author": "Song Son Ha, Florian Foerster, Thomas Robert Doebbert, Tim Kittel, Dominik Merli, and Gerd Scholl",
        "link": "http://arxiv.org/abs/2507.20873v1",
        "abstract": "In the era of Industry 4.0, the growing need for secure and efficient\ncommunication systems has driven the development of fifth-generation (5G)\nnetworks characterized by extremely low latency, massive device connectivity\nand high data transfer speeds. However, the deployment of 5G networks presents\nsignificant security challenges, requiring advanced and robust solutions to\ncounter increasingly sophisticated cyber threats. This paper proposes a testbed\nand software architecture to strengthen the security of Private 5G Networks,\nparticularly in industrial communication environments."
    },
    {
        "date": "2025-07",
        "title": "Not Only Grey Matter: OmniBrain for Robust Multimodal Classification of Alzheimer's Disease",
        "author": "Ahmed Sharshar, Yasser Ashraf, Tameem Bakr, Salma Hassan, Hosam Elgendy, Mohammad Yaqub, and Mohsen Guizani",
        "link": "http://arxiv.org/abs/2507.20872v1",
        "abstract": "Alzheimer's disease affects over 55 million people worldwide and is projected\nto more than double by 2050, necessitating rapid, accurate, and scalable\ndiagnostics. However, existing approaches are limited because they cannot\nachieve clinically acceptable accuracy, generalization across datasets,\nrobustness to missing modalities, and explainability all at the same time. This\ninability to satisfy all these requirements simultaneously undermines their\nreliability in clinical settings. We propose OmniBrain, a multimodal framework\nthat integrates brain MRI, radiomics, gene expression, and clinical data using\na unified model with cross-attention and modality dropout. OmniBrain achieves\n$92.2 \\pm 2.4\\%$accuracy on the ANMerge dataset and generalizes to the MRI-only\nADNI dataset with $70.4 \\pm 2.7\\%$ accuracy, outperforming unimodal and prior\nmultimodal approaches. Explainability analyses highlight neuropathologically\nrelevant brain regions and genes, enhancing clinical trust. OmniBrain offers a\nrobust, interpretable, and practical solution for real-world Alzheimer's\ndiagnosis."
    },
    {
        "date": "2025-07",
        "title": "An Open-source Implementation and Security Analysis of Triad's TEE Trusted Time Protocol",
        "author": "Matthieu Bettinger, Sonia Ben Mokhtar, and Anthony Simonet-Boulogne",
        "link": "http://arxiv.org/abs/2507.20851v1",
        "abstract": "The logic of many protocols relies on time measurements. However, in Trusted\nExecution Environments (TEEs) like Intel SGX, the time source is outside the\nTrusted Computing Base: a malicious system hosting the TEE can manipulate that\nTEE's notion of time, e.g., jumping in time or affecting the perceived time\nspeed. Previous work like Triad propose protocols for TEEs to maintain a\ntrustworthy time source. However, in this paper, based on a public\nimplementation of Triad that we contribute, we empirically showcase\nvulnerabilities to this protocol. For example, an attacker controlling the\noperating system, and consequently the scheduling algorithm, may arbitrarily\nmanipulate their local TEE's clock speed. What is worse, in case of faster\nmalicious clock speeds, an attacker on a single compromised machine may\npropagate the attack to honest machines participating in Triad's Trusted Time\nprotocol, causing them to skip to timestamps arbitrarily far in the future.\nThen, infected honest machines propagate time-skips themselves to other honest\nmachines interacting with them. We discuss protocol changes to Triad for higher\nresilience against such attacks."
    },
    {
        "date": "2025-07",
        "title": "Multi-Masked Querying Network for Robust Emotion Recognition from Incomplete Multi-Modal Physiological Signals",
        "author": "Geng-Xin Xu, Xiang Zuo, and Ye Li",
        "link": "http://arxiv.org/abs/2507.20737v1",
        "abstract": "Emotion recognition from physiological data is crucial for mental health\nassessment, yet it faces two significant challenges: incomplete multi-modal\nsignals and interference from body movements and artifacts. This paper presents\na novel Multi-Masked Querying Network (MMQ-Net) to address these issues by\nintegrating multiple querying mechanisms into a unified framework.\nSpecifically, it uses modality queries to reconstruct missing data from\nincomplete signals, category queries to focus on emotional state features, and\ninterference queries to separate relevant information from noise. Extensive\nexperiment results demonstrate the superior emotion recognition performance of\nMMQ-Net compared to existing approaches, particularly under high levels of data\nincompleteness."
    },
    {
        "date": "2025-07",
        "title": "Exposing the Illusion of Fairness: Auditing Vulnerabilities to Distributional Manipulation Attacks",
        "author": "Valentin Lafargue, Adriana Laurindo Monteiro, Emmanuelle Claeys, Laurent Risser, and Jean-Michel Loubes",
        "link": "http://arxiv.org/abs/2507.20708v1",
        "abstract": "Proving the compliance of AI algorithms has become an important challenge\nwith the growing deployment of such algorithms for real-life applications.\nInspecting possible biased behaviors is mandatory to satisfy the constraints of\nthe regulations of the EU Artificial Intelligence's Act. Regulation-driven\naudits increasingly rely on global fairness metrics, with Disparate Impact\nbeing the most widely used. Yet such global measures depend highly on the\ndistribution of the sample on which the measures are computed. We investigate\nfirst how to manipulate data samples to artificially satisfy fairness criteria,\ncreating minimally perturbed datasets that remain statistically\nindistinguishable from the original distribution while satisfying prescribed\nfairness constraints. Then we study how to detect such manipulation. Our\nanalysis (i) introduces mathematically sound methods for modifying empirical\ndistributions under fairness constraints using entropic or optimal transport\nprojections, (ii) examines how an auditee could potentially circumvent fairness\ninspections, and (iii) offers recommendations to help auditors detect such data\nmanipulations. These results are validated through experiments on classical\ntabular datasets in bias detection."
    },
    {
        "date": "2025-07",
        "title": "A Novel Post-Quantum Secure Digital Signature Scheme Based on Neural Network",
        "author": "Satish Kumar, and Md. Arzoo Jamal",
        "link": "http://arxiv.org/abs/2507.20676v1",
        "abstract": "Digital signatures are fundamental cryptographic primitives that ensure the\nauthenticity and integrity of digital documents. In the post-quantum era,\nclassical public key-based signature schemes become vulnerable to brute-force\nand key-recovery attacks due to the computational power of quantum algorithms.\nMultivariate polynomial based signature schemes are among the one of the\ncryptographic constructions that offers strong security guarantees against such\nquantum threats. With the growing capabilities of neural networks, it is\nnatural to explore their potential application in the design of cryptographic\nprimitives. Neural networks inherently captures the non-linear relationships\nwithin the data, which are encoded in their synaptic weight matrices and bias\nvectors. In this paper, we propose a novel construction of a multivariate\npolynomial based digital signature scheme that leverages neural network\narchitectures. A neural network with binary weights is employed to define the\ncentral structure of the signature scheme. The design introduces a recurrent\nrandom vector, functionally analogous to an attention mechanism, which\ncontributes dynamic randomness based on the previous state, thereby enhancing\nthe scheme's security. It is demonstrated that the proposed signature scheme\nprovide security against Existential Unforgeability under adaptive\nChosen-Message Attacks (EUF-CMA). Furthermore, it is proven that direct attacks\naimed to recover the private keys are computationally infeasible within\npolynomial time, even in the presence of quantum computing abilities. The\noperational characteristics of the proposed scheme are also evaluated, with\nresults indicating notable efficiency and practical viability in post-quantum\ncryptographic applications."
    },
    {
        "date": "2025-07",
        "title": "Reminiscence Attack on Residuals: Exploiting Approximate Machine Unlearning for Privacy",
        "author": "Yaxin Xiao, Qingqing Ye, Li Hu, Huadi Zheng, Haibo Hu, Zi Liang, Haoyang Li, and Yijie Jiao",
        "link": "http://arxiv.org/abs/2507.20573v1",
        "abstract": "Machine unlearning enables the removal of specific data from ML models to\nuphold the right to be forgotten. While approximate unlearning algorithms offer\nefficient alternatives to full retraining, this work reveals that they fail to\nadequately protect the privacy of unlearned data. In particular, these\nalgorithms introduce implicit residuals which facilitate privacy attacks\ntargeting at unlearned data. We observe that these residuals persist regardless\nof model architectures, parameters, and unlearning algorithms, exposing a new\nattack surface beyond conventional output-based leakage. Based on this insight,\nwe propose the Reminiscence Attack (ReA), which amplifies the correlation\nbetween residuals and membership privacy through targeted fine-tuning\nprocesses. ReA achieves up to 1.90x and 1.12x higher accuracy than prior\nattacks when inferring class-wise and sample-wise membership, respectively. To\nmitigate such residual-induced privacy risk, we develop a dual-phase\napproximate unlearning framework that first eliminates deep-layer unlearned\ndata traces and then enforces convergence stability to prevent models from\n\"pseudo-convergence\", where their outputs are similar to retrained models but\nstill preserve unlearned residuals. Our framework works for both classification\nand generation tasks. Experimental evaluations confirm that our approach\nmaintains high unlearning efficacy, while reducing the adaptive privacy attack\naccuracy to nearly random guess, at the computational cost of 2-12% of full\nretraining from scratch."
    },
    {
        "date": "2025-07",
        "title": "Security Challenges in AI Agent Deployment: Insights from a Large Scale Public Competition",
        "author": "Andy Zou, Maxwell Lin, Eliot Jones, Micha Nowak, Mateusz Dziemian, Nick Winter, Alexander Grattan, Valent Nathanael, Ayla Croft, Xander Davies, Jai Patel, Robert Kirk, Nate Burnikell, Yarin Gal, Dan Hendrycks, J. Zico Kolter, and Matt Fredrikson",
        "link": "http://arxiv.org/abs/2507.20526v1",
        "abstract": "Recent advances have enabled LLM-powered AI agents to autonomously execute\ncomplex tasks by combining language model reasoning with tools, memory, and web\naccess. But can these systems be trusted to follow deployment policies in\nrealistic environments, especially under attack? To investigate, we ran the\nlargest public red-teaming competition to date, targeting 22 frontier AI agents\nacross 44 realistic deployment scenarios. Participants submitted 1.8 million\nprompt-injection attacks, with over 60,000 successfully eliciting policy\nviolations such as unauthorized data access, illicit financial actions, and\nregulatory noncompliance. We use these results to build the Agent Red Teaming\n(ART) benchmark - a curated set of high-impact attacks - and evaluate it across\n19 state-of-the-art models. Nearly all agents exhibit policy violations for\nmost behaviors within 10-100 queries, with high attack transferability across\nmodels and tasks. Importantly, we find limited correlation between agent\nrobustness and model size, capability, or inference-time compute, suggesting\nthat additional defenses are needed against adversarial misuse. Our findings\nhighlight critical and persistent vulnerabilities in today's AI agents. By\nreleasing the ART benchmark and accompanying evaluation framework, we aim to\nsupport more rigorous security assessment and drive progress toward safer agent\ndeployment."
    },
    {
        "date": "2025-07",
        "title": "Your Attention Matters: to Improve Model Robustness to Noise and Spurious Correlations",
        "author": "Camilo Tamayo-Rousseau, Yunjia Zhao, Yiqun Zhang, and Randall Balestriero",
        "link": "http://arxiv.org/abs/2507.20453v1",
        "abstract": "Self-attention mechanisms are foundational to Transformer architectures,\nsupporting their impressive success in a wide range of tasks. While there are\nmany self-attention variants, their robustness to noise and spurious\ncorrelations has not been well studied. This study evaluates Softmax, Sigmoid,\nLinear, Doubly Stochastic, and Cosine attention within Vision Transformers\nunder different data corruption scenarios. Through testing across the CIFAR-10,\nCIFAR-100, and Imagenette datasets, we show that Doubly Stochastic attention is\nthe most robust. Our findings inform self-attention selection in contexts with\nimperfect data."
    },
    {
        "date": "2025-07",
        "title": "When Prompts Go Wrong: Evaluating Code Model Robustness to Ambiguous, Contradictory, and Incomplete Task Descriptions",
        "author": "Maya Larbi, Amal Akli, Mike Papadakis, Rihab Bouyousfi, Maxime Cordy, Federica Sarro, and Yves Le Traon",
        "link": "http://arxiv.org/abs/2507.20439v1",
        "abstract": "Large Language Models (LLMs) have demonstrated impressive performance in code\ngeneration tasks under idealized conditions, where task descriptions are clear\nand precise. However, in practice, task descriptions frequently exhibit\nambiguity, incompleteness, or internal contradictions. In this paper, we\npresent the first empirical study examining the robustness of state-of-the-art\ncode generation models when faced with such unclear task descriptions. We\nextend the HumanEval and MBPP benchmarks by systematically introducing\nrealistic task descriptions flaws through guided mutation strategies, producing\na dataset that mirrors the messiness of informal developer instructions. We\nevaluate multiple LLMs of varying sizes and architectures, analyzing their\nfunctional correctness and failure modes across task descriptions categories.\nOur findings reveal that even minor imperfections in task description phrasing\ncan cause significant performance degradation, with contradictory task\ndescriptions resulting in numerous logical errors. Moreover, while larger\nmodels tend to be more resilient than smaller variants, they are not immune to\nthe challenges posed by unclear requirements. We further analyze semantic error\npatterns and identify correlations between description clarity, model behavior,\nand error types. Our results underscore the critical need for developing LLMs\nthat are not only powerful but also robust to the imperfections inherent in\nnatural user tasks, highlighting important considerations for improving model\ntraining strategies, designing more realistic evaluation benchmarks, and\nensuring reliable deployment in practical software development environments."
    },
    {
        "date": "2025-07",
        "title": "Two Views, One Truth: Spectral and Self-Supervised Features Fusion for Robust Speech Deepfake Detection",
        "author": "Yassine El Kheir, Arnab Das, Enes Erdem Erdogan, Fabian Ritter-Guttierez, Tim Polzehl, and Sebastian M\u00f6ller",
        "link": "http://arxiv.org/abs/2507.20417v1",
        "abstract": "Recent advances in synthetic speech have made audio deepfakes increasingly\nrealistic, posing significant security risks. Existing detection methods that\nrely on a single modality, either raw waveform embeddings or spectral based\nfeatures, are vulnerable to non spoof disturbances and often overfit to known\nforgery algorithms, resulting in poor generalization to unseen attacks. To\naddress these shortcomings, we investigate hybrid fusion frameworks that\nintegrate self supervised learning (SSL) based representations with handcrafted\nspectral descriptors (MFCC , LFCC, CQCC). By aligning and combining\ncomplementary information across modalities, these fusion approaches capture\nsubtle artifacts that single feature approaches typically overlook. We explore\nseveral fusion strategies, including simple concatenation, cross attention,\nmutual cross attention, and a learnable gating mechanism, to optimally blend\nSSL features with fine grained spectral cues. We evaluate our approach on four\nchallenging public benchmarks and report generalization performance. All fusion\nvariants consistently outperform an SSL only baseline, with the cross attention\nstrategy achieving the best generalization with a 38% relative reduction in\nequal error rate (EER). These results confirm that joint modeling of waveform\nand spectral views produces robust, domain agnostic representations for audio\ndeepfake detection."
    },
    {
        "date": "2025-07",
        "title": "Second Competition on Presentation Attack Detection on ID Card",
        "author": "Juan E. Tapia, Mario Nieto, Juan M. Espin, Alvaro S. Rocamora, Javier Barrachina, Naser Damer, Christoph Busch, Marija Ivanovska, Leon Todorov, Renat Khizbullin, Lazar Lazarevich, Aleksei Grishin, Daniel Schulz, Sebastian Gonzalez, Amir Mohammadi, Ketan Kotwal, Sebastien Marcel, Raghavendra Mudgalgundurao, Kiran Raja, Patrick Schuch, Sushrut Patwardhan, Raghavendra Ramachandra, Pedro Couto Pereira, Joao Ribeiro Pinto, Mariana Xavier, Andr\u00e9s Valenzuela, Rodrigo Lara, Borut Batagelj, Marko Peterlin, Peter Peer, Ajnas Muhammed, Diogo Nunes, and Nuno Gon\u00e7alves",
        "link": "http://arxiv.org/abs/2507.20404v1",
        "abstract": "This work summarises and reports the results of the second Presentation\nAttack Detection competition on ID cards. This new version includes new\nelements compared to the previous one. (1) An automatic evaluation platform was\nenabled for automatic benchmarking; (2) Two tracks were proposed in order to\nevaluate algorithms and datasets, respectively; and (3) A new ID card dataset\nwas shared with Track 1 teams to serve as the baseline dataset for the training\nand optimisation. The Hochschule Darmstadt, Fraunhofer-IGD, and Facephi company\njointly organised this challenge. 20 teams were registered, and 74 submitted\nmodels were evaluated. For Track 1, the \"Dragons\" team reached first place with\nan Average Ranking and Equal Error rate (EER) of AV-Rank of 40.48% and 11.44%\nEER, respectively. For the more challenging approach in Track 2, the \"Incode\"\nteam reached the best results with an AV-Rank of 14.76% and 6.36% EER,\nimproving on the results of the first edition of 74.30% and 21.87% EER,\nrespectively. These results suggest that PAD on ID cards is improving, but it\nis still a challenging problem related to the number of images, especially of\nbona fide images."
    },
    {
        "date": "2025-07",
        "title": "Detecting Visual Information Manipulation Attacks in Augmented Reality: A Multimodal Semantic Reasoning Approach",
        "author": "Yanming Xiu, and Maria Gorlatova",
        "link": "http://arxiv.org/abs/2507.20356v1",
        "abstract": "The virtual content in augmented reality (AR) can introduce misleading or\nharmful information, leading to semantic misunderstandings or user errors. In\nthis work, we focus on visual information manipulation (VIM) attacks in AR\nwhere virtual content changes the meaning of real-world scenes in subtle but\nimpactful ways. We introduce a taxonomy that categorizes these attacks into\nthree formats: character, phrase, and pattern manipulation, and three purposes:\ninformation replacement, information obfuscation, and extra wrong information.\nBased on the taxonomy, we construct a dataset, AR-VIM. It consists of 452\nraw-AR video pairs spanning 202 different scenes, each simulating a real-world\nAR scenario. To detect such attacks, we propose a multimodal semantic reasoning\nframework, VIM-Sense. It combines the language and visual understanding\ncapabilities of vision-language models (VLMs) with optical character\nrecognition (OCR)-based textual analysis. VIM-Sense achieves an attack\ndetection accuracy of 88.94% on AR-VIM, consistently outperforming vision-only\nand text-only baselines. The system reaches an average attack detection latency\nof 7.07 seconds in a simulated video processing framework and 7.17 seconds in a\nreal-world evaluation conducted on a mobile Android AR application."
    },
    {
        "date": "2025-07",
        "title": "SoK: Root Cause of \\$1 Billion Loss in Smart Contract Real-World Attacks via a Systematic Literature Review of Vulnerabilities",
        "author": "Hadis Rezaei, Mojtaba Eshghie, Karl Anderesson, and Francesco Palmieri",
        "link": "http://arxiv.org/abs/2507.20175v1",
        "abstract": "The Ethereum ecosystem, despite its maturity, continues to witness\ncatastrophic attacks, with billions of dollars in assets lost annually. In\nresponse, a significant body of research has focused on identifying and\nmitigating smart contract vulnerabilities. However, these efforts predominantly\nfocus on implementation-level bugs, leaving a critical gap between academic\nunderstanding of vulnerabilities and the root causes of real-world high-impact\nfinancial losses. We employ a two-pronged methodology: first, a systematic\nliterature review of 71 academic papers to build a comprehensive and up-to-date\ncatalog of 24 active and 5 deprecated vulnerabilities as understood by the\nresearch community. Second, we conduct an in-depth, empirical analysis of 50 of\nthe most severe real-world exploits between 2022 and 2025, collectively\nincurring over \\$1.09B in losses, to identify their true root causes. We\nintroduce the concept of \"exploit chains\" by revealing that many incidents are\nnot caused by isolated vulnerabilities but by combinations of human,\noperational, and economic design flaws that link with implementation bugs to\nenable an attack. Our analysis yields insights on how DApps are exploited in\npractice, leading to a novel, four-tier root-cause framework that moves beyond\ncode-level vulnerabilities. We find that real-world successful attacks on\nEthereum (and related networks) trace back to one of the four tiers of (1)\nprotocol logic design, (2) lifecycle and governance, (3) external dependencies,\nand (4) traditional implementation bugs (classic smart contract\nvulnerabilities). We investigate the suitability of this multi-tier incident\nroot-cause framework via a case study."
    },
    {
        "date": "2025-07",
        "title": "Robust Taxi Fare Prediction Under Noisy Conditions: A Comparative Study of GAT, TimesNet, and XGBoost",
        "author": "Padmavathi Moorthy",
        "link": "http://arxiv.org/abs/2507.20008v1",
        "abstract": "Precise fare prediction is crucial in ride-hailing platforms and urban\nmobility systems. This study examines three machine learning models-Graph\nAttention Networks (GAT), XGBoost, and TimesNet to evaluate their predictive\ncapabilities for taxi fares using a real-world dataset comprising over 55\nmillion records. Both raw (noisy) and denoised versions of the dataset are\nanalyzed to assess the impact of data quality on model performance. The study\nevaluated the models along multiple axes, including predictive accuracy,\ncalibration, uncertainty estimation, out-of-distribution (OOD) robustness, and\nfeature sensitivity. We also explore pre-processing strategies, including KNN\nimputation, Gaussian noise injection, and autoencoder-based denoising. The\nstudy reveals critical differences between classical and deep learning models\nunder realistic conditions, offering practical guidelines for building robust\nand scalable models in urban fare prediction systems."
    },
    {
        "date": "2025-07",
        "title": "\"Blockchain-Enabled Zero Trust Framework for Securing FinTech Ecosystems Against Insider Threats and Cyber Attacks\"",
        "author": "Avinash Singh, Vikas Pareek, and Asish Sharma",
        "link": "http://arxiv.org/abs/2507.19976v1",
        "abstract": "Fintech provides technological services to increase operational efficiency in\nfinancial institutions, but traditional perimeter-based defense mechanisms are\ninsufficient against evolving cyber threats like insider attacks, malware\nintrusions, and Advanced Persistent Threats (APTs). These vulnerabilities\nexpose Fintech organizations to significant risks, including financial losses\nand data breaches. To address these challenges, this paper proposes a\nblockchain-integrated Zero Trust framework, adhering to the principle of \"Never\nTrust, Always Verify.\" The framework uses Ethereum smart contracts to enforce\nMulti Factor Authentication (MFA), Role-Based Access Control (RBAC), and\nJust-In-Time (JIT) access privileges, effectively mitigating credential theft\nand insider threats, the effect of malware and APT attacks.\n  The proposed solution transforms blockchain into a Policy Engine (PE) and\nPolicy Enforcement Point (PEP), and policy storage, ensuring immutable access\ncontrol and micro-segmentation. A decentralized application (DApp) prototype\nwas developed and tested using STRIDE threat modeling, demonstrating resilience\nagainst spoofing, tampering, and privilege escalation. Comparative analysis\nwith Perimeter-based systems revealed a trade-off: while the framework\nintroduced a marginal latency increase (74.0 ms vs. 49.33 ms) and reduced\nthroughput (30.77 vs. 50.0 requests/sec), it significantly enhanced security by\neliminating single points of failure and enabling tamper-proof audit trails.\n  Experimental validation on a 200-node simulated network confirmed the\nframework's robustness, with future optimizations targeting Layer-2 solutions\nfor scalability. This work bridges the gap between Zero Trust theory and\npractical blockchain implementation, offering Fintech organizations a\ndecentralized, cost-effective security model."
    },
    {
        "date": "2025-07",
        "title": "Who Owns This Sample: Cross-Client Membership Inference Attack in Federated Graph Neural Networks",
        "author": "Kunhao Li, Di Wu, Jun Bai, Jing Xu, Lei Yang, Ziyi Zhang, Yiliao Song, Wencheng Yang, Taotao Cai, and Yan Li",
        "link": "http://arxiv.org/abs/2507.19964v1",
        "abstract": "Graph-structured data is prevalent in many real-world applications, including\nsocial networks, financial systems, and molecular biology. Graph Neural\nNetworks (GNNs) have become the de facto standard for learning from such data\ndue to their strong representation capabilities. As GNNs are increasingly\ndeployed in federated learning (FL) settings to preserve data locality and\nprivacy, new privacy threats arise from the interaction between graph\nstructures and decentralized training. In this paper, we present the first\nsystematic study of cross-client membership inference attacks (CC-MIA) against\nnode classification tasks of federated GNNs (FedGNNs), where a malicious client\naims to infer which client owns the given data. Unlike prior\ncentralized-focused work that focuses on whether a sample was included in\ntraining, our attack targets sample-to-client attribution, a finer-grained\nprivacy risk unique to federated settings. We design a general attack framework\nthat exploits FedGNNs' aggregation behaviors, gradient updates, and embedding\nproximity to link samples to their source clients across training rounds. We\nevaluate our attack across multiple graph datasets under realistic FL setups.\nResults show that our method achieves high performance on both membership\ninference and ownership identification. Our findings highlight a new privacy\nthreat in federated graph learning-client identity leakage through structural\nand model-level cues, motivating the need for attribution-robust GNN design."
    },
    {
        "date": "2025-07",
        "title": "ConSeg: Contextual Backdoor Attack Against Semantic Segmentation",
        "author": "Bilal Hussain Abbasi, Zirui Gong, Yanjun Zhang, Shang Gao, Antonio Robles-Kelly, and Leo Zhang",
        "link": "http://arxiv.org/abs/2507.19905v1",
        "abstract": "Despite significant advancements in computer vision, semantic segmentation\nmodels may be susceptible to backdoor attacks. These attacks, involving hidden\ntriggers, aim to cause the models to misclassify instances of the victim class\nas the target class when triggers are present, posing serious threats to the\nreliability of these models. To further explore the field of backdoor attacks\nagainst semantic segmentation, in this paper, we propose a simple yet effective\nbackdoor attack called Contextual Segmentation Backdoor Attack (ConSeg). ConSeg\nleverages the contextual information inherent in semantic segmentation models\nto enhance backdoor performance. Our method is motivated by an intriguing\nobservation, i.e., when the target class is set as the `co-occurring' class of\nthe victim class, the victim class can be more easily `mis-segmented'. Building\nupon this insight, ConSeg mimics the contextual information of the target class\nand rebuilds it in the victim region to establish the contextual relationship\nbetween the target class and the victim class, making the attack easier. Our\nexperiments reveal that ConSeg achieves improvements in Attack Success Rate\n(ASR) with increases of 15.55\\%, compared to existing methods, while exhibiting\nresilience against state-of-the-art backdoor defenses."
    },
    {
        "date": "2025-07",
        "title": "ATCTrack: Aligning Target-Context Cues with Dynamic Target States for Robust Vision-Language Tracking",
        "author": "X. Feng, S. Hu, X. Li, D. Zhang, M. Wu, J. Zhang, X. Chen, and K. Huang",
        "link": "http://arxiv.org/abs/2507.19875v1",
        "abstract": "Vision-language tracking aims to locate the target object in the video\nsequence using a template patch and a language description provided in the\ninitial frame. To achieve robust tracking, especially in complex long-term\nscenarios that reflect real-world conditions as recently highlighted by MGIT,\nit is essential not only to characterize the target features but also to\nutilize the context features related to the target. However, the visual and\ntextual target-context cues derived from the initial prompts generally align\nonly with the initial target state. Due to their dynamic nature, target states\nare constantly changing, particularly in complex long-term sequences. It is\nintractable for these cues to continuously guide Vision-Language Trackers\n(VLTs). Furthermore, for the text prompts with diverse expressions, our\nexperiments reveal that existing VLTs struggle to discern which words pertain\nto the target or the context, complicating the utilization of textual cues. In\nthis work, we present a novel tracker named ATCTrack, which can obtain\nmultimodal cues Aligned with the dynamic target states through comprehensive\nTarget-Context feature modeling, thereby achieving robust tracking.\nSpecifically, (1) for the visual modality, we propose an effective temporal\nvisual target-context modeling approach that provides the tracker with timely\nvisual cues. (2) For the textual modality, we achieve precise target words\nidentification solely based on textual content, and design an innovative\ncontext words calibration method to adaptively utilize auxiliary context words.\n(3) We conduct extensive experiments on mainstream benchmarks and ATCTrack\nachieves a new SOTA performance. The code and models will be released at:\nhttps://github.com/XiaokunFeng/ATCTrack."
    },
    {
        "date": "2025-07",
        "title": "ChoreoMuse: Robust Music-to-Dance Video Generation with Style Transfer and Beat-Adherent Motion",
        "author": "Xuanchen Wang, Heng Wang, and Weidong Cai",
        "link": "http://arxiv.org/abs/2507.19836v1",
        "abstract": "Modern artistic productions increasingly demand automated choreography\ngeneration that adapts to diverse musical styles and individual dancer\ncharacteristics. Existing approaches often fail to produce high-quality dance\nvideos that harmonize with both musical rhythm and user-defined choreography\nstyles, limiting their applicability in real-world creative contexts. To\naddress this gap, we introduce ChoreoMuse, a diffusion-based framework that\nuses SMPL format parameters and their variation version as intermediaries\nbetween music and video generation, thereby overcoming the usual constraints\nimposed by video resolution. Critically, ChoreoMuse supports\nstyle-controllable, high-fidelity dance video generation across diverse musical\ngenres and individual dancer characteristics, including the flexibility to\nhandle any reference individual at any resolution. Our method employs a novel\nmusic encoder MotionTune to capture motion cues from audio, ensuring that the\ngenerated choreography closely follows the beat and expressive qualities of the\ninput music. To quantitatively evaluate how well the generated dances match\nboth musical and choreographic styles, we introduce two new metrics that\nmeasure alignment with the intended stylistic cues. Extensive experiments\nconfirm that ChoreoMuse achieves state-of-the-art performance across multiple\ndimensions, including video quality, beat alignment, dance diversity, and style\nadherence, demonstrating its potential as a robust solution for a wide range of\ncreative applications. Video results can be found on our project page:\nhttps://choreomuse.github.io."
    },
    {
        "date": "2025-07",
        "title": "Quaternion-Based Robust PCA for Efficient Moving Target Detection and Background Recovery in Color Videos",
        "author": "Liyang Wang, Shiqian Wu, Shun Fang, Qile Zhu, Jiaxin Wu, and Sos Again",
        "link": "http://arxiv.org/abs/2507.19730v1",
        "abstract": "Moving target detection is a challenging computer vision task aimed at\ngenerating accurate segmentation maps in diverse in-the-wild color videos\ncaptured by static cameras. If backgrounds and targets can be simultaneously\nextracted and recombined, such synthetic data can significantly enrich\nannotated in-the-wild datasets and enhance the generalization ability of deep\nmodels. Quaternion-based RPCA (QRPCA) is a promising unsupervised paradigm for\ncolor image processing. However, in color video processing, Quaternion Singular\nValue Decomposition (QSVD) incurs high computational costs, and rank-1\nquaternion matrix fails to yield rank-1 color channels. In this paper, we\nreduce the computational complexity of QSVD to o(1) by utilizing a quaternion\nRiemannian manifold. Furthermor, we propose the universal QRPCA (uQRPCA)\nframework, which achieves a balance in simultaneously segmenting targets and\nrecovering backgrounds from color videos. Moreover, we expand to uQRPCA+ by\nintroducing the Color Rank-1 Batch (CR1B) method to further process and obtain\nthe ideal low-rank background across color channels. Experiments demonstrate\nour uQRPCA+ achieves State Of The Art (SOTA) performance on moving target\ndetection and background recovery tasks compared to existing open-source\nmethods. Our implementation is publicly available on GitHub at\nhttps://github.com/Ruchtech/uQRPCA"
    },
    {
        "date": "2025-07",
        "title": "Exemplar Med-DETR: Toward Generalized and Robust Lesion Detection in Mammogram Images and beyond",
        "author": "Sheethal Bhat, Bogdan Georgescu, Adarsh Bhandary Panambur, Mathias Zinnen, Tri-Thien Nguyen, Awais Mansoor, Karim Khalifa Elbarbary, Siming Bayer, Florin-Cristian Ghesu, Sasa Grbic, and Andreas Maier",
        "link": "http://arxiv.org/abs/2507.19621v1",
        "abstract": "Detecting abnormalities in medical images poses unique challenges due to\ndifferences in feature representations and the intricate relationship between\nanatomical structures and abnormalities. This is especially evident in\nmammography, where dense breast tissue can obscure lesions, complicating\nradiological interpretation. Despite leveraging anatomical and semantic\ncontext, existing detection methods struggle to learn effective class-specific\nfeatures, limiting their applicability across different tasks and imaging\nmodalities. In this work, we introduce Exemplar Med-DETR, a novel multi-modal\ncontrastive detector that enables feature-based detection. It employs\ncross-attention with inherently derived, intuitive class-specific exemplar\nfeatures and is trained with an iterative strategy. We achieve state-of-the-art\nperformance across three distinct imaging modalities from four public datasets.\nOn Vietnamese dense breast mammograms, we attain an mAP of 0.7 for mass\ndetection and 0.55 for calcifications, yielding an absolute improvement of 16\npercentage points. Additionally, a radiologist-supported evaluation of 100\nmammograms from an out-of-distribution Chinese cohort demonstrates a twofold\ngain in lesion detection performance. For chest X-rays and angiography, we\nachieve an mAP of 0.25 for mass and 0.37 for stenosis detection, improving\nresults by 4 and 7 percentage points, respectively. These results highlight the\npotential of our approach to advance robust and generalizable detection systems\nfor medical imaging."
    },
    {
        "date": "2025-07",
        "title": "Securing the Internet of Medical Things (IoMT): Real-World Attack Taxonomy and Practical Security Measures",
        "author": "Suman Deb, Emil Lupu, Emm Mic Drakakis, Anil Anthony Bharath, Zhen Kit Leung, Guang Rui Ma, and Anupam Chattopadhyay",
        "link": "http://arxiv.org/abs/2507.19609v1",
        "abstract": "The Internet of Medical Things (IoMT) has the potential to radically improve\nhealthcare by enabling real-time monitoring, remote diagnostics, and AI-driven\ndecision making. However, the connectivity, embedded intelligence, and\ninclusion of a wide variety of novel sensors expose medical devices to severe\ncybersecurity threats, compromising patient safety and data privacy. In\naddition, many devices also have direct capacity - individually or in\nconjunction with other IoMT devices - to perform actions on the patient, such\nas delivering an electrical stimulus, administering a drug, or activating a\nmotor, which can potentially be life-threatening. We provide a taxonomy of\npotential attacks targeting IoMT, presenting attack surfaces, vulnerabilities,\nand mitigation strategies across all layers of the IoMT architecture. It\nanswers key questions such as: What makes IoMT security different from\ntraditional IT security? What are the cybersecurity threats to medical devices?\nHow can engineers design secure IoMT systems and protect hospital networks from\ncyberattacks? By analyzing historical cyber incidents, we highlight critical\nsecurity gaps and propose practical security guidelines for medical device\nengineers and security professionals. This work bridges the gap between\nresearch and implementation, equipping healthcare stakeholders with actionable\ninsights to build resilient and privacy-preserving IoMT ecosystems. Finally, we\npresent the latest standardization and compliance frameworks, that IoMT\nsecurity designers should be aware of."
    },
    {
        "date": "2025-07",
        "title": "MOCHA: Are Code Language Models Robust Against Multi-Turn Malicious Coding Prompts?",
        "author": "Muntasir Wahed, Xiaona Zhou, Kiet A. Nguyen, Tianjiao Yu, Nirav Diwan, Gang Wang, Dilek Hakkani-T\u00fcr, and Ismini Lourentzou",
        "link": "http://arxiv.org/abs/2507.19598v1",
        "abstract": "Recent advancements in Large Language Models (LLMs) have significantly\nenhanced their code generation capabilities. However, their robustness against\nadversarial misuse, particularly through multi-turn malicious coding prompts,\nremains underexplored. In this work, we introduce code decomposition attacks,\nwhere a malicious coding task is broken down into a series of seemingly benign\nsubtasks across multiple conversational turns to evade safety filters. To\nfacilitate systematic evaluation, we introduce \\benchmarkname{}, a large-scale\nbenchmark designed to evaluate the robustness of code LLMs against both\nsingle-turn and multi-turn malicious prompts. Empirical results across open-\nand closed-source models reveal persistent vulnerabilities, especially under\nmulti-turn scenarios. Fine-tuning on MOCHA improves rejection rates while\npreserving coding ability, and importantly, enhances robustness on external\nadversarial datasets with up to 32.4% increase in rejection rates without any\nadditional supervision."
    },
    {
        "date": "2025-07",
        "title": "Observations Meet Actions: Learning Control-Sufficient Representations for Robust Policy Generalization",
        "author": "Yuliang Gu, Hongpeng Cao, Marco Caccamo, and Naira Hovakimyan",
        "link": "http://arxiv.org/abs/2507.19437v1",
        "abstract": "Capturing latent variations (\"contexts\") is key to deploying\nreinforcement-learning (RL) agents beyond their training regime. We recast\ncontext-based RL as a dual inference-control problem and formally characterize\ntwo properties and their hierarchy: observation sufficiency (preserving all\npredictive information) and control sufficiency (retaining decision-making\nrelevant information). Exploiting this dichotomy, we derive a contextual\nevidence lower bound(ELBO)-style objective that cleanly separates\nrepresentation learning from policy learning and optimizes it with Bottlenecked\nContextual Policy Optimization (BCPO), an algorithm that places a variational\ninformation-bottleneck encoder in front of any off-policy policy learner. On\nstandard continuous-control benchmarks with shifting physical parameters, BCPO\nmatches or surpasses other baselines while using fewer samples and retaining\nperformance far outside the training regime. The framework unifies theory,\ndiagnostics, and practice for context-based RL."
    },
    {
        "date": "2025-07",
        "title": "Running in CIRCLE? A Simple Benchmark for LLM Code Interpreter Security",
        "author": "Gabriel Chua",
        "link": "http://arxiv.org/abs/2507.19399v1",
        "abstract": "As large language models (LLMs) increasingly integrate native code\ninterpreters, they enable powerful real-time execution capabilities,\nsubstantially expanding their utility. However, such integrations introduce\npotential system-level cybersecurity threats, fundamentally different from\nprompt-based vulnerabilities. To systematically evaluate these\ninterpreter-specific risks, we propose CIRCLE (Code-Interpreter Resilience\nCheck for LLM Exploits), a simple benchmark comprising 1,260 prompts targeting\nCPU, memory, and disk resource exhaustion. Each risk category includes\nexplicitly malicious (\"direct\") and plausibly benign (\"indirect\") prompt\nvariants. Our automated evaluation framework assesses not only whether LLMs\nrefuse or generates risky code, but also executes the generated code within the\ninterpreter environment to evaluate code correctness, simplifications made by\nthe LLM to make the code safe, or execution timeouts. Evaluating 7 commercially\navailable models from OpenAI and Google, we uncover significant and\ninconsistent vulnerabilities. For instance, evaluations show substantial\ndisparities even within providers - OpenAI's o4-mini correctly refuses risky\nrequests at 7.1%, notably higher rates compared to GPT-4.1 at 0.5%. Results\nparticularly underscore that indirect, socially-engineered prompts\nsubstantially weaken model defenses. This highlights an urgent need for\ninterpreter-specific cybersecurity benchmarks, dedicated mitigation tools\n(e.g., guardrails), and clear industry standards to guide safe and responsible\ndeployment of LLM interpreter integrations. The benchmark dataset and\nevaluation code are publicly released to foster further research."
    },
    {
        "date": "2025-07",
        "title": "Empowering IoT Firmware Secure Update with Customization Rights",
        "author": "Weihao Chen, Yansong Gao, Boyu Kuang, Jin B. Hong, Yuqing Zhang, and Anmin Fu",
        "link": "http://arxiv.org/abs/2507.19367v1",
        "abstract": "Firmware updates remain the primary line of defense for IoT devices; however,\nthe update channel itself has become a well-established attack vector. Existing\ndefenses mainly focus on securing monolithic firmware images, leaving\nmodule-level customization -a growing user demand-largely unprotected and\ninsufficiently explored. To address this gap, we conduct a pilot study on the\nupdate workflows of 200 Linux-based IoT devices across 23 vendors, uncovering\nfive previously undocumented vulnerabilities caused by customization practices.\nA broader analysis of update-related CVEs from 2020 to 2024 reveals that over\nhalf originate from customization-induced issues. These findings highlight a\ncritical yet underexamined reality: as customization increases, so does the\nattack surface, while current defenses fail to keep pace. We propose IMUP\n(Integrity-Centric Modular Update Platform), the first framework to address two\nkey challenges: constructing a trustworthy cross-module integrity chain and\nscaling update performance under mass customization. IMUP combines three\ntechniques: per-module chameleon hashing for integrity, server-side\nproof-of-work offloading to reduce device overhead, and server-side caching to\nreuse module combinations, minimizing rebuild costs. Security analysis shows\nthat even when 95 percent of secret keys are exposed, forging a valid image\nincurs over 300 times the cost of the legitimate server. Experiments on\nheterogeneous IoT devices demonstrate that IMUP reduces server-side generation\ntime by 2.9 times and device downtime by 5.9 times compared to a\npackage-manager baseline."
    },
    {
        "date": "2025-07",
        "title": "On the Security of a Code-Based PIR Scheme",
        "author": "Svenja Lage, and Hannes Bartz",
        "link": "http://arxiv.org/abs/2507.19295v1",
        "abstract": "Private Information Retrieval (PIR) schemes allow clients to retrieve files\nfrom a database without disclosing the requested file's identity to the server.\nIn the pursuit of post-quantum security, most recent PIR schemes rely on hard\nlattice problems. In contrast, the so called CB-cPIR scheme stands out as a\npioneering effort to base PIR schemes on hard problems in coding theory,\nthereby contributing significantly to the diversification of security\nfoundations. However, our research reveals a critical vulnerability in CB-cPIR,\nsubstantially diminishing its security levels. Moreover, a comparative analysis\nwith state-of-the-art PIR schemes shows that CB-cPIR's advantages are reduced,\nmaking it less competitive in terms of the communication cost. Nevertheless,\nour findings highlight the importance of continued research into code-based PIR\nschemes, as they have the potential to provide a valuable alternative to\nlattice-based approaches."
    },
    {
        "date": "2025-07",
        "title": "Patch Pruning Strategy Based on Robust Statistical Measures of Attention Weight Diversity in Vision Transformers",
        "author": "Yuki Igaue, and Hiroaki Aizawa",
        "link": "http://arxiv.org/abs/2507.19175v1",
        "abstract": "Multi-head self-attention is a distinctive feature extraction mechanism of\nvision transformers that computes pairwise relationships among all input\npatches, contributing significantly to their high performance. However, it is\nknown to incur a quadratic computational complexity with respect to the number\nof patches. One promising approach to address this issue is patch pruning,\nwhich improves computational efficiency by identifying and removing redundant\npatches. In this work, we propose a patch pruning strategy that evaluates the\nimportance of each patch based on the variance of attention weights across\nmultiple attention heads. This approach is inspired by the design of multi-head\nself-attention, which aims to capture diverse attention patterns across\ndifferent subspaces of feature representations. The proposed method can be\neasily applied during both training and inference, and achieves improved\nthroughput while maintaining classification accuracy in scenarios such as\nfine-tuning with pre-trained models. In addition, we also found that using\nrobust statistical measures, such as the median absolute deviation in place of\nvariance, to assess patch importance can similarly lead to strong performance.\nFurthermore, by introducing overlapping patch embeddings, our method achieves\nbetter performance with comparable throughput to conventional approaches that\nutilize all patches."
    },
    {
        "date": "2025-07",
        "title": "Game-Theoretic Gradient Control for Robust Neural Network Training",
        "author": "Maria Zaitseva, Ivan Tomilov, and Natalia Gusarova",
        "link": "http://arxiv.org/abs/2507.19143v1",
        "abstract": "Feed-forward neural networks (FFNNs) are vulnerable to input noise, reducing\nprediction performance. Existing regularization methods like dropout often\nalter network architecture or overlook neuron interactions. This study aims to\nenhance FFNN noise robustness by modifying backpropagation, interpreted as a\nmulti-agent game, and exploring controlled target variable noising. Our\n\"gradient dropout\" selectively nullifies hidden layer neuron gradients with\nprobability 1 - p during backpropagation, while keeping forward passes active.\nThis is framed within compositional game theory. Additionally, target variables\nwere perturbed with white noise or stable distributions. Experiments on ten\ndiverse tabular datasets show varying impacts: improvement or diminishing of\nrobustness and accuracy, depending on dataset and hyperparameters. Notably, on\nregression tasks, gradient dropout (p = 0.9) combined with stable distribution\ntarget noising significantly increased input noise robustness, evidenced by\nflatter MSE curves and more stable SMAPE values. These results highlight the\nmethod's potential, underscore the critical role of adaptive parameter tuning,\nand open new avenues for analyzing neural networks as complex adaptive systems\nexhibiting emergent behavior within a game-theoretic framework."
    },
    {
        "date": "2025-07",
        "title": "Virtual local area network over HTTP for launching an insider attack",
        "author": "Yuksel Arslan",
        "link": "http://arxiv.org/abs/2507.19055v1",
        "abstract": "Computers and computer networks have become integral to virtually every\naspect of modern life, with the Internet playing an indispensable role.\nOrganizations, businesses, and individuals now store vast amounts of\nproprietary, confidential, and personal data digitally. As such, ensuring the\nsecurity of this data from unauthorized access is critical. Common security\nmeasures, such as firewalls, intrusion detection systems (IDS), intrusion\nprevention systems (IPS), and antivirus software, are constantly evolving to\nsafeguard computer systems and networks. However, these tools primarily focus\non defending against external threats, leaving systems vulnerable to insider\nattacks. Security solutions designed to mitigate risks originating from within\nthe organization are relatively limited and often ineffective. This paper\ndemonstrates how a Local Area Network (LAN) can be covertly exposed to the\nInternet via an insider attack. Specifically, it illustrates how an external\nmachine can gain access to a LAN by exploiting an unused secondary IP address\nof the attacked LAN, effectively bypassing existing security mechanisms by also\nexploiting Hyper Text Transfer Protocol (HTTP). Despite the presence of robust\nexternal protections, such as firewalls and IDS, this form of insider attack\nreveals significant vulnerabilities in the way internal threats are addressed."
    },
    {
        "date": "2025-07",
        "title": "UPP: Unified Point-Level Prompting for Robust Point Cloud Analysis",
        "author": "Zixiang Ai, Zhenyu Cui, Yuxin Peng, and Jiahuan Zhou",
        "link": "http://arxiv.org/abs/2507.18997v1",
        "abstract": "Pre-trained point cloud analysis models have shown promising advancements in\nvarious downstream tasks, yet their effectiveness is typically suffering from\nlow-quality point cloud (i.e., noise and incompleteness), which is a common\nissue in real scenarios due to casual object occlusions and unsatisfactory data\ncollected by 3D sensors. To this end, existing methods focus on enhancing point\ncloud quality by developing dedicated denoising and completion models. However,\ndue to the isolation between the point cloud enhancement and downstream tasks,\nthese methods fail to work in various real-world domains. In addition, the\nconflicting objectives between denoising and completing tasks further limit the\nensemble paradigm to preserve critical geometric features. To tackle the above\nchallenges, we propose a unified point-level prompting method that reformulates\npoint cloud denoising and completion as a prompting mechanism, enabling robust\nanalysis in a parameter-efficient manner. We start by introducing a\nRectification Prompter to adapt to noisy points through the predicted\nrectification vector prompts, effectively filtering noise while preserving\nintricate geometric features essential for accurate analysis. Sequentially, we\nfurther incorporate a Completion Prompter to generate auxiliary point prompts\nbased on the rectified point clouds, facilitating their robustness and\nadaptability. Finally, a Shape-Aware Unit module is exploited to efficiently\nunify and capture the filtered geometric features for the downstream point\ncloud analysis.Extensive experiments on four datasets demonstrate the\nsuperiority and robustness of our method when handling noisy and incomplete\npoint cloud data against existing state-of-the-art methods. Our code is\nreleased at https://github.com/zhoujiahuan1991/ICCV2025-UPP."
    },
    {
        "date": "2025-07",
        "title": "Secure Best Arm Identification in the Presence of a Copycat",
        "author": "Asaf Cohen, and Onur G\u00fcnl\u00fc",
        "link": "http://arxiv.org/abs/2507.18975v2",
        "abstract": "Consider the problem of best arm identification with a security constraint.\nSpecifically, assume a setup of stochastic linear bandits with $K$ arms of\ndimension $d$. In each arm pull, the player receives a reward that is the sum\nof the dot product of the arm with an unknown parameter vector and independent\nnoise. The player's goal is to identify the best arm after $T$ arm pulls.\nMoreover, assume a copycat Chloe is observing the arm pulls. The player wishes\nto keep Chloe ignorant of the best arm.\n  While a minimax--optimal algorithm identifies the best arm with an\n$\\Omega\\left(\\frac{T}{\\log(d)}\\right)$ error exponent, it easily reveals its\nbest-arm estimate to an outside observer, as the best arms are played more\nfrequently. A naive secure algorithm that plays all arms equally results in an\n$\\Omega\\left(\\frac{T}{d}\\right)$ exponent. In this paper, we propose a secure\nalgorithm that plays with \\emph{coded arms}. The algorithm does not require any\nkey or cryptographic primitives, yet achieves an\n$\\Omega\\left(\\frac{T}{\\log^2(d)}\\right)$ exponent while revealing almost no\ninformation on the best arm."
    },
    {
        "date": "2025-07",
        "title": "WiSE-OD: Benchmarking Robustness in Infrared Object Detection",
        "author": "Heitor R. Medeiros, Atif Belal, Masih Aminbeidokhti, Eric Granger, and Marco Pedersoli",
        "link": "http://arxiv.org/abs/2507.18925v1",
        "abstract": "Object detection (OD) in infrared (IR) imagery is critical for low-light and\nnighttime applications. However, the scarcity of large-scale IR datasets forces\nmodels to rely on weights pre-trained on RGB images. While fine-tuning on IR\nimproves accuracy, it often compromises robustness under distribution shifts\ndue to the inherent modality gap between RGB and IR. To address this, we\nintroduce LLVIP-C and FLIR-C, two cross-modality out-of-distribution (OOD)\nbenchmarks built by applying corruption to standard IR datasets. Additionally,\nto fully leverage the complementary knowledge from RGB and infrared trained\nmodels, we propose WiSE-OD, a weight-space ensembling method with two variants:\nWiSE-OD$_{ZS}$, which combines RGB zero-shot and IR fine-tuned weights, and\nWiSE-OD$_{LP}$, which blends zero-shot and linear probing. Evaluated across\nthree RGB-pretrained detectors and two robust baselines, WiSE-OD improves both\ncross-modality and corruption robustness without any additional training or\ninference cost."
    },
    {
        "date": "2025-07",
        "title": "Transferable and Undefendable Point Cloud Attacks via Medial Axis Transform",
        "author": "Keke Tang, Yuze Gao, Weilong Peng, Xiaofei Wang, Meie Fang, and Peican Zhu",
        "link": "http://arxiv.org/abs/2507.18870v1",
        "abstract": "Studying adversarial attacks on point clouds is essential for evaluating and\nimproving the robustness of 3D deep learning models. However, most existing\nattack methods are developed under ideal white-box settings and often suffer\nfrom limited transferability to unseen models and insufficient robustness\nagainst common defense mechanisms. In this paper, we propose MAT-Adv, a novel\nadversarial attack framework that enhances both transferability and\nundefendability by explicitly perturbing the medial axis transform (MAT)\nrepresentations, in order to induce inherent adversarialness in the resulting\npoint clouds. Specifically, we employ an autoencoder to project input point\nclouds into compact MAT representations that capture the intrinsic geometric\nstructure of point clouds. By perturbing these intrinsic representations,\nMAT-Adv introduces structural-level adversarial characteristics that remain\neffective across diverse models and defense strategies. To mitigate overfitting\nand prevent perturbation collapse, we incorporate a dropout strategy into the\noptimization of MAT perturbations, further improving transferability and\nundefendability. Extensive experiments demonstrate that MAT-Adv significantly\noutperforms existing state-of-the-art methods in both transferability and\nundefendability. Codes will be made public upon paper acceptance."
    },
    {
        "date": "2025-07",
        "title": "Ralts: Robust Aggregation for Enhancing Graph Neural Network Resilience on Bit-flip Errors",
        "author": "Wencheng Zou, and Nan Wu",
        "link": "http://arxiv.org/abs/2507.18804v1",
        "abstract": "Graph neural networks (GNNs) have been widely applied in safety-critical\napplications, such as financial and medical networks, in which compromised\npredictions may cause catastrophic consequences. While existing research on GNN\nrobustness has primarily focused on software-level threats, hardware-induced\nfaults and errors remain largely underexplored. As hardware systems progress\ntoward advanced technology nodes to meet high-performance and energy efficiency\ndemands, they become increasingly susceptible to transient faults, which can\ncause bit flips and silent data corruption, a prominent issue observed by major\ntechnology companies (e.g., Meta and Google). In response, we first present a\ncomprehensive analysis of GNN robustness against bit-flip errors, aiming to\nreveal system-level optimization opportunities for future reliable and\nefficient GNN systems. Second, we propose Ralts, a generalizable and\nlightweight solution to bolster GNN resilience to bit-flip errors.\nSpecifically, Ralts exploits various graph similarity metrics to filter out\noutliers and recover compromised graph topology, and incorporates these\nprotective techniques directly into aggregation functions to support any\nmessage-passing GNNs. Evaluation results demonstrate that Ralts effectively\nenhances GNN robustness across a range of GNN models, graph datasets, error\npatterns, and both dense and sparse architectures. On average, under a BER of\n$3\\times10^{-5}$, these robust aggregation functions improve prediction\naccuracy by at least 20\\% when errors present in model weights or node\nembeddings, and by at least 10\\% when errors occur in adjacency matrices. Ralts\nis also optimized to deliver execution efficiency comparable to built-in\naggregation functions in PyTorch Geometric."
    },
    {
        "date": "2025-07",
        "title": "Adversarial Distribution Matching for Diffusion Distillation Towards Efficient Image and Video Synthesis",
        "author": "Yanzuo Lu, Yuxi Ren, Xin Xia, Shanchuan Lin, Xing Wang, Xuefeng Xiao, Andy J. Ma, Xiaohua Xie, and Jian-Huang Lai",
        "link": "http://arxiv.org/abs/2507.18569v1",
        "abstract": "Distribution Matching Distillation (DMD) is a promising score distillation\ntechnique that compresses pre-trained teacher diffusion models into efficient\none-step or multi-step student generators. Nevertheless, its reliance on the\nreverse Kullback-Leibler (KL) divergence minimization potentially induces mode\ncollapse (or mode-seeking) in certain applications. To circumvent this inherent\ndrawback, we propose Adversarial Distribution Matching (ADM), a novel framework\nthat leverages diffusion-based discriminators to align the latent predictions\nbetween real and fake score estimators for score distillation in an adversarial\nmanner. In the context of extremely challenging one-step distillation, we\nfurther improve the pre-trained generator by adversarial distillation with\nhybrid discriminators in both latent and pixel spaces. Different from the mean\nsquared error used in DMD2 pre-training, our method incorporates the\ndistributional loss on ODE pairs collected from the teacher model, and thus\nproviding a better initialization for score distillation fine-tuning in the\nnext stage. By combining the adversarial distillation pre-training with ADM\nfine-tuning into a unified pipeline termed DMDX, our proposed method achieves\nsuperior one-step performance on SDXL compared to DMD2 while consuming less GPU\ntime. Additional experiments that apply multi-step ADM distillation on\nSD3-Medium, SD3.5-Large, and CogVideoX set a new benchmark towards efficient\nimage and video synthesis."
    },
    {
        "date": "2025-07",
        "title": "Revisiting Bisimulation Metric for Robust Representations in Reinforcement Learning",
        "author": "Leiji Zhang, Zeyu Wang, Xin Li, and Yao-Hui Li",
        "link": "http://arxiv.org/abs/2507.18519v1",
        "abstract": "Bisimulation metric has long been regarded as an effective control-related\nrepresentation learning technique in various reinforcement learning tasks.\nHowever, in this paper, we identify two main issues with the conventional\nbisimulation metric: 1) an inability to represent certain distinctive\nscenarios, and 2) a reliance on predefined weights for differences in rewards\nand subsequent states during recursive updates. We find that the first issue\narises from an imprecise definition of the reward gap, whereas the second issue\nstems from overlooking the varying importance of reward difference and\nnext-state distinctions across different training stages and task settings. To\naddress these issues, by introducing a measure for state-action pairs, we\npropose a revised bisimulation metric that features a more precise definition\nof reward gap and novel update operators with adaptive coefficient. We also\noffer theoretical guarantees of convergence for our proposed metric and its\nimproved representation distinctiveness. In addition to our rigorous\ntheoretical analysis, we conduct extensive experiments on two representative\nbenchmarks, DeepMind Control and Meta-World, demonstrating the effectiveness of\nour approach."
    },
    {
        "date": "2025-07",
        "title": "Reinforced Embodied Active Defense: Exploiting Adaptive Interaction for Robust Visual Perception in Adversarial 3D Environments",
        "author": "Xiao Yang, Lingxuan Wu, Lizhong Wang, Chengyang Ying, Hang Su, and Jun Zhu",
        "link": "http://arxiv.org/abs/2507.18484v1",
        "abstract": "Adversarial attacks in 3D environments have emerged as a critical threat to\nthe reliability of visual perception systems, particularly in safety-sensitive\napplications such as identity verification and autonomous driving. These\nattacks employ adversarial patches and 3D objects to manipulate deep neural\nnetwork (DNN) predictions by exploiting vulnerabilities within complex scenes.\nExisting defense mechanisms, such as adversarial training and purification,\nprimarily employ passive strategies to enhance robustness. However, these\napproaches often rely on pre-defined assumptions about adversarial tactics,\nlimiting their adaptability in dynamic 3D settings. To address these\nchallenges, we introduce Reinforced Embodied Active Defense (Rein-EAD), a\nproactive defense framework that leverages adaptive exploration and interaction\nwith the environment to improve perception robustness in 3D adversarial\ncontexts. By implementing a multi-step objective that balances immediate\nprediction accuracy with predictive entropy minimization, Rein-EAD optimizes\ndefense strategies over a multi-step horizon. Additionally, Rein-EAD involves\nan uncertainty-oriented reward-shaping mechanism that facilitates efficient\npolicy updates, thereby reducing computational overhead and supporting\nreal-world applicability without the need for differentiable environments.\nComprehensive experiments validate the effectiveness of Rein-EAD, demonstrating\na substantial reduction in attack success rates while preserving standard\naccuracy across diverse tasks. Notably, Rein-EAD exhibits robust generalization\nto unseen and adaptive attacks, making it suitable for real-world complex\ntasks, including 3D object classification, face recognition and autonomous\ndriving."
    },
    {
        "date": "2025-07",
        "title": "Revisiting Physically Realizable Adversarial Object Attack against LiDAR-based Detection: Clarifying Problem Formulation and Experimental Protocols",
        "author": "Luo Cheng, Hanwei Zhang, Lijun Zhang, and Holger Hermanns",
        "link": "http://arxiv.org/abs/2507.18457v1",
        "abstract": "Adversarial robustness in LiDAR-based 3D object detection is a critical\nresearch area due to its widespread application in real-world scenarios. While\nmany digital attacks manipulate point clouds or meshes, they often lack\nphysical realizability, limiting their practical impact. Physical adversarial\nobject attacks remain underexplored and suffer from poor reproducibility due to\ninconsistent setups and hardware differences. To address this, we propose a\ndevice-agnostic, standardized framework that abstracts key elements of physical\nadversarial object attacks, supports diverse methods, and provides open-source\ncode with benchmarking protocols in simulation and real-world settings. Our\nframework enables fair comparison, accelerates research, and is validated by\nsuccessfully transferring simulated attacks to a physical LiDAR system. Beyond\nthe framework, we offer insights into factors influencing attack success and\nadvance understanding of adversarial robustness in real-world LiDAR perception."
    },
    {
        "date": "2025-07",
        "title": "LoRA-Leak: Membership Inference Attacks Against LoRA Fine-tuned Language Models",
        "author": "Delong Ran, Xinlei He, Tianshuo Cong, Anyu Wang, Qi Li, and Xiaoyun Wang",
        "link": "http://arxiv.org/abs/2507.18302v1",
        "abstract": "Language Models (LMs) typically adhere to a \"pre-training and fine-tuning\"\nparadigm, where a universal pre-trained model can be fine-tuned to cater to\nvarious specialized domains. Low-Rank Adaptation (LoRA) has gained the most\nwidespread use in LM fine-tuning due to its lightweight computational cost and\nremarkable performance. Because the proportion of parameters tuned by LoRA is\nrelatively small, there might be a misleading impression that the LoRA\nfine-tuning data is invulnerable to Membership Inference Attacks (MIAs).\nHowever, we identify that utilizing the pre-trained model can induce more\ninformation leakage, which is neglected by existing MIAs. Therefore, we\nintroduce LoRA-Leak, a holistic evaluation framework for MIAs against the\nfine-tuning datasets of LMs. LoRA-Leak incorporates fifteen membership\ninference attacks, including ten existing MIAs, and five improved MIAs that\nleverage the pre-trained model as a reference. In experiments, we apply\nLoRA-Leak to three advanced LMs across three popular natural language\nprocessing tasks, demonstrating that LoRA-based fine-tuned LMs are still\nvulnerable to MIAs (e.g., 0.775 AUC under conservative fine-tuning settings).\nWe also applied LoRA-Leak to different fine-tuning settings to understand the\nresulting privacy risks. We further explore four defenses and find that only\ndropout and excluding specific LM layers during fine-tuning effectively\nmitigate MIA risks while maintaining utility. We highlight that under the\n\"pre-training and fine-tuning\" paradigm, the existence of the pre-trained model\nmakes MIA a more severe risk for LoRA-based LMs. We hope that our findings can\nprovide guidance on data privacy protection for specialized LM providers."
    },
    {
        "date": "2025-07",
        "title": "DepthDark: Robust Monocular Depth Estimation for Low-Light Environments",
        "author": "Longjian Zeng, Zunjie Zhu, Rongfeng Lu, Ming Lu, Bolun Zheng, Chenggang Yan, and Anke Xue",
        "link": "http://arxiv.org/abs/2507.18243v1",
        "abstract": "In recent years, foundation models for monocular depth estimation have\nreceived increasing attention. Current methods mainly address typical daylight\nconditions, but their effectiveness notably decreases in low-light\nenvironments. There is a lack of robust foundational models for monocular depth\nestimation specifically designed for low-light scenarios. This largely stems\nfrom the absence of large-scale, high-quality paired depth datasets for\nlow-light conditions and the effective parameter-efficient fine-tuning (PEFT)\nstrategy. To address these challenges, we propose DepthDark, a robust\nfoundation model for low-light monocular depth estimation. We first introduce a\nflare-simulation module and a noise-simulation module to accurately simulate\nthe imaging process under nighttime conditions, producing high-quality paired\ndepth datasets for low-light conditions. Additionally, we present an effective\nlow-light PEFT strategy that utilizes illumination guidance and multiscale\nfeature fusion to enhance the model's capability in low-light environments. Our\nmethod achieves state-of-the-art depth estimation performance on the\nchallenging nuScenes-Night and RobotCar-Night datasets, validating its\neffectiveness using limited training data and computing resources."
    },
    {
        "date": "2025-07",
        "title": "Information Security Based on LLM Approaches: A Review",
        "author": "Chang Gong, Zhongwen Li, and Xiaoqi Li",
        "link": "http://arxiv.org/abs/2507.18215v1",
        "abstract": "Information security is facing increasingly severe challenges, and\ntraditional protection means are difficult to cope with complex and changing\nthreats. In recent years, as an emerging intelligent technology, large language\nmodels (LLMs) have shown a broad application prospect in the field of\ninformation security. In this paper, we focus on the key role of LLM in\ninformation security, systematically review its application progress in\nmalicious behavior prediction, network threat analysis, system vulnerability\ndetection, malicious code identification, and cryptographic algorithm\noptimization, and explore its potential in enhancing security protection\nperformance. Based on neural networks and Transformer architecture, this paper\nanalyzes the technical basis of large language models and their advantages in\nnatural language processing tasks. It is shown that the introduction of large\nlanguage modeling helps to improve the detection accuracy and reduce the false\nalarm rate of security systems. Finally, this paper summarizes the current\napplication results and points out that it still faces challenges in model\ntransparency, interpretability, and scene adaptability, among other issues. It\nis necessary to explore further the optimization of the model structure and the\nimprovement of the generalization ability to realize a more intelligent and\naccurate information security protection system."
    },
    {
        "date": "2025-07",
        "title": "MatSSL: Robust Self-Supervised Representation Learning for Metallographic Image Segmentation",
        "author": "Hoang Hai Nam Nguyen, Phan Nguyen Duc Hieu, and Ho Won Lee",
        "link": "http://arxiv.org/abs/2507.18184v1",
        "abstract": "MatSSL is a streamlined self-supervised learning (SSL) architecture that\nemploys Gated Feature Fusion at each stage of the backbone to integrate\nmulti-level representations effectively. Current micrograph analysis of\nmetallic materials relies on supervised methods, which require retraining for\neach new dataset and often perform inconsistently with only a few labeled\nsamples. While SSL offers a promising alternative by leveraging unlabeled data,\nmost existing methods still depend on large-scale datasets to be effective.\nMatSSL is designed to overcome this limitation. We first perform\nself-supervised pretraining on a small-scale, unlabeled dataset and then\nfine-tune the model on multiple benchmark datasets. The resulting segmentation\nmodels achieve 69.13% mIoU on MetalDAM, outperforming the 66.73% achieved by an\nImageNet-pretrained encoder, and delivers consistently up to nearly 40%\nimprovement in average mIoU on the Environmental Barrier Coating benchmark\ndataset (EBC) compared to models pretrained with MicroNet. This suggests that\nMatSSL enables effective adaptation to the metallographic domain using only a\nsmall amount of unlabeled data, while preserving the rich and transferable\nfeatures learned from large-scale pretraining on natural images."
    },
    {
        "date": "2025-07",
        "title": "ChronoSelect: Robust Learning with Noisy Labels via Dynamics Temporal Memory",
        "author": "Jianchao Wang, Qingfeng Li, Pengcheng Zheng, Xiaorong Pu, and Yazhou Ren",
        "link": "http://arxiv.org/abs/2507.18183v1",
        "abstract": "Training deep neural networks on real-world datasets is often hampered by the\npresence of noisy labels, which can be memorized by over-parameterized models,\nleading to significant degradation in generalization performance. While\nexisting methods for learning with noisy labels (LNL) have made considerable\nprogress, they fundamentally suffer from static snapshot evaluations and fail\nto leverage the rich temporal dynamics of learning evolution. In this paper, we\npropose ChronoSelect (chrono denoting its temporal nature), a novel framework\nfeaturing an innovative four-stage memory architecture that compresses\nprediction history into compact temporal distributions. Our unique sliding\nupdate mechanism with controlled decay maintains only four dynamic memory units\nper sample, progressively emphasizing recent patterns while retaining essential\nhistorical knowledge. This enables precise three-way sample partitioning into\nclean, boundary, and noisy subsets through temporal trajectory analysis and\ndual-branch consistency. Theoretical guarantees prove the mechanism's\nconvergence and stability under noisy conditions. Extensive experiments\ndemonstrate ChronoSelect's state-of-the-art performance across synthetic and\nreal-world benchmarks."
    },
    {
        "date": "2025-07",
        "title": "Policy Disruption in Reinforcement Learning:Adversarial Attack with Large Language Models and Critical State Identification",
        "author": "Junyong Jiang, Buwei Tian, Chenxing Xu, Songze Li, and Lu Dong",
        "link": "http://arxiv.org/abs/2507.18113v1",
        "abstract": "Reinforcement learning (RL) has achieved remarkable success in fields like\nrobotics and autonomous driving, but adversarial attacks designed to mislead RL\nsystems remain challenging. Existing approaches often rely on modifying the\nenvironment or policy, limiting their practicality. This paper proposes an\nadversarial attack method in which existing agents in the environment guide the\ntarget policy to output suboptimal actions without altering the environment. We\npropose a reward iteration optimization framework that leverages large language\nmodels (LLMs) to generate adversarial rewards explicitly tailored to the\nvulnerabilities of the target agent, thereby enhancing the effectiveness of\ninducing the target agent toward suboptimal decision-making. Additionally, a\ncritical state identification algorithm is designed to pinpoint the target\nagent's most vulnerable states, where suboptimal behavior from the victim leads\nto significant degradation in overall performance. Experimental results in\ndiverse environments demonstrate the superiority of our method over existing\napproaches."
    },
    {
        "date": "2025-07",
        "title": "RECALLED: An Unbounded Resource Consumption Attack on Large Vision-Language Models",
        "author": "Haoran Gao, Yuanhe Zhang, Zhenhong Zhou, Lei Jiang, Fanyu Meng, Yujia Xiao, Kun Wang, Yang Liu, and Junlan Feng",
        "link": "http://arxiv.org/abs/2507.18053v1",
        "abstract": "Resource Consumption Attacks (RCAs) have emerged as a significant threat to\nthe deployment of Large Language Models (LLMs). With the integration of vision\nmodalities, additional attack vectors exacerbate the risk of RCAs in large\nvision-language models (LVLMs). However, existing red-teaming studies have\nlargely overlooked visual inputs as a potential attack surface, resulting in\ninsufficient mitigation strategies against RCAs in LVLMs. To address this gap,\nwe propose RECALLED (\\textbf{RE}source \\textbf{C}onsumption \\textbf{A}ttack on\n\\textbf{L}arge Vision-\\textbf{L}anguag\\textbf{E} Mo\\textbf{D}els), the first\napproach for exploiting visual modalities to trigger unbounded RCAs\nred-teaming. First, we present \\textit{Vision Guided Optimization}, a\nfine-grained pixel-level optimization, to obtain \\textit{Output Recall}\nadversarial perturbations, which can induce repeating output. Then, we inject\nthe perturbations into visual inputs, triggering unbounded generations to\nachieve the goal of RCAs. Additionally, we introduce \\textit{Multi-Objective\nParallel Losses} to generate universal attack templates and resolve\noptimization conflicts when intending to implement parallel attacks. Empirical\nresults demonstrate that RECALLED increases service response latency by over 26\n$\\uparrow$, resulting in an additional 20\\% increase in GPU utilization and\nmemory consumption. Our study exposes security vulnerabilities in LVLMs and\nestablishes a red-teaming framework that can facilitate future defense\ndevelopment against RCAs."
    },
    {
        "date": "2025-07",
        "title": "Your ATs to Ts: MITRE ATT&CK Attack Technique to P-SSCRM Task Mapping",
        "author": "Sivana Hamer, Jacob Bowen, Md Nazmul Haque, Chris Madden, and Laurie Williams",
        "link": "http://arxiv.org/abs/2507.18037v1",
        "abstract": "The MITRE Adversarial Tactics, Techniques and Common Knowledge (MITRE ATT&CK)\nAttack Technique to Proactive Software Supply Chain Risk Management Framework\n(P-SSCRM) Task mapping described in this document helps software organizations\nto determine how different tasks mitigate the attack techniques of software\nsupply chain attacks. The mapping was created through four independent\nstrategies to find agreed-upon mappings. Because each P-SSCRM task is mapped to\none or more tasks from the 10 frameworks, the mapping we provide is also a\nmapping between MITRE ATT&CK and other prominent government and industry\nframeworks."
    },
    {
        "date": "2025-07",
        "title": "Minimax Data Sanitization with Distortion Constraint and Adversarial Inference",
        "author": "Amirarsalan Moatazedian, Yauhen Yakimenka, R\u00e9mi A. Chou, and J\u00f6rg Kliewer",
        "link": "http://arxiv.org/abs/2507.17942v1",
        "abstract": "We study a privacy-preserving data-sharing setting where a privatizer\ntransforms private data into a sanitized version observed by an authorized\nreconstructor and two unauthorized adversaries, each with access to side\ninformation correlated with the private data.\n  The reconstructor is evaluated under a distortion function, while each\nadversary is evaluated using a separate loss function. The privatizer ensures\nthe reconstructor distortion remains below a fixed threshold while maximizing\nthe minimum loss across the two adversaries. This two-adversary setting models\ncases where individual users cannot reconstruct the data accurately, but their\ncombined side information enables estimation within the distortion threshold.\nThe privatizer maximizes individual loss while permitting accurate\nreconstruction only through collaboration. This echoes secret-sharing\nprinciples, but with lossy rather than perfect recovery. We frame this as a\nconstrained data-driven minimax optimization problem and propose a data-driven\ntraining procedure that alternately updates the privatizer, reconstructor, and\nadversaries. We also analyze the Gaussian and binary cases as special scenarios\nwhere optimal solutions can be obtained. These theoretical optimal results are\nbenchmarks for evaluating the proposed minimax training approach."
    },
    {
        "date": "2025-07",
        "title": "Bob's Confetti: Phonetic Memorization Attacks in Music and Video Generation",
        "author": "Jaechul Roh, Zachary Novack, Yuefeng Peng, Niloofar Mireshghallah, Taylor Berg-Kirkpatrick, and Amir Houmansadr",
        "link": "http://arxiv.org/abs/2507.17937v1",
        "abstract": "Lyrics-to-Song (LS2) generation models promise end-to-end music synthesis\nfrom text, yet their vulnerability to training data memorization remains\nunderexplored. We introduce Adversarial PhoneTic Prompting (APT), a novel\nattack where lyrics are semantically altered while preserving their acoustic\nstructure through homophonic substitutions (e.g., Eminem's famous \"mom's\nspaghetti\" $\\rightarrow$ \"Bob's confetti\"). Despite these distortions, we\nuncover a powerful form of sub-lexical memorization: models like SUNO and YuE\nregenerate outputs strikingly similar to known training content, achieving high\nsimilarity across audio-domain metrics, including CLAP, AudioJudge, and\nCoverID. This vulnerability persists across multiple languages and genres. More\nsurprisingly, we discover that phoneme-altered lyrics alone can trigger visual\nmemorization in text-to-video models. When prompted with phonetically modified\nlyrics from Lose Yourself, Veo 3 reconstructs visual elements from the original\nmusic video -- including character appearance and scene composition -- despite\nno visual cues in the prompt. We term this phenomenon phonetic-to-visual\nregurgitation. Together, these findings expose a critical vulnerability in\ntranscript-conditioned multimodal generation: phonetic prompting alone can\nunlock memorized audiovisual content, raising urgent questions about copyright,\nsafety, and content provenance in modern generative systems. Example\ngenerations are available on our demo page (jrohsc.github.io/music_attack/)."
    },
    {
        "date": "2025-07",
        "title": "FishDet-M: A Unified Large-Scale Benchmark for Robust Fish Detection and CLIP-Guided Model Selection in Diverse Aquatic Visual Domains",
        "author": "Muayad Abujabal, Lyes Saad Saoud, and Irfan Hussain",
        "link": "http://arxiv.org/abs/2507.17859v1",
        "abstract": "Accurate fish detection in underwater imagery is essential for ecological\nmonitoring, aquaculture automation, and robotic perception. However, practical\ndeployment remains limited by fragmented datasets, heterogeneous imaging\nconditions, and inconsistent evaluation protocols. To address these gaps, we\npresent \\textit{FishDet-M}, the largest unified benchmark for fish detection,\ncomprising 13 publicly available datasets spanning diverse aquatic environments\nincluding marine, brackish, occluded, and aquarium scenes. All data are\nharmonized using COCO-style annotations with both bounding boxes and\nsegmentation masks, enabling consistent and scalable cross-domain evaluation.\nWe systematically benchmark 28 contemporary object detection models, covering\nthe YOLOv8 to YOLOv12 series, R-CNN based detectors, and DETR based models.\nEvaluations are conducted using standard metrics including mAP, mAP@50, and\nmAP@75, along with scale-specific analyses (AP$_S$, AP$_M$, AP$_L$) and\ninference profiling in terms of latency and parameter count. The results\nhighlight the varying detection performance across models trained on FishDet-M,\nas well as the trade-off between accuracy and efficiency across models of\ndifferent architectures. To support adaptive deployment, we introduce a\nCLIP-based model selection framework that leverages vision-language alignment\nto dynamically identify the most semantically appropriate detector for each\ninput image. This zero-shot selection strategy achieves high performance\nwithout requiring ensemble computation, offering a scalable solution for\nreal-time applications. FishDet-M establishes a standardized and reproducible\nplatform for evaluating object detection in complex aquatic scenes. All\ndatasets, pretrained models, and evaluation tools are publicly available to\nfacilitate future research in underwater computer vision and intelligent marine\nsystems."
    },
    {
        "date": "2025-07",
        "title": "Large Learning Rates Simultaneously Achieve Robustness to Spurious Correlations and Compressibility",
        "author": "Melih Barsbey, Lucas Prieto, Stefanos Zafeiriou, and Tolga Birdal",
        "link": "http://arxiv.org/abs/2507.17748v1",
        "abstract": "Robustness and resource-efficiency are two highly desirable properties for\nmodern machine learning models. However, achieving them jointly remains a\nchallenge. In this paper, we position high learning rates as a facilitator for\nsimultaneously achieving robustness to spurious correlations and network\ncompressibility. We demonstrate that large learning rates also produce\ndesirable representation properties such as invariant feature utilization,\nclass separation, and activation sparsity. Importantly, our findings indicate\nthat large learning rates compare favorably to other hyperparameters and\nregularization methods, in consistently satisfying these properties in tandem.\nIn addition to demonstrating the positive effect of large learning rates across\ndiverse spurious correlation datasets, models, and optimizers, we also present\nstrong evidence that the previously documented success of large learning rates\nin standard classification tasks is likely due to its effect on addressing\nhidden/rare spurious correlations in the training dataset."
    },
    {
        "date": "2025-07",
        "title": "CA-Cut: Crop-Aligned Cutout for Data Augmentation to Learn More Robust Under-Canopy Navigation",
        "author": "Robel Mamo, and Taeyeong Choi",
        "link": "http://arxiv.org/abs/2507.17727v2",
        "abstract": "State-of-the-art visual under-canopy navigation methods are designed with\ndeep learning-based perception models to distinguish traversable space from\ncrop rows. While these models have demonstrated successful performance, they\nrequire large amounts of training data to ensure reliability in real-world\nfield deployment. However, data collection is costly, demanding significant\nhuman resources for in-field sampling and annotation. To address this\nchallenge, various data augmentation techniques are commonly employed during\nmodel training, such as color jittering, Gaussian blur, and horizontal flip, to\ndiversify training data and enhance model robustness. In this paper, we\nhypothesize that utilizing only these augmentation techniques may lead to\nsuboptimal performance, particularly in complex under-canopy environments with\nfrequent occlusions, debris, and non-uniform spacing of crops. Instead, we\npropose a novel augmentation method, so-called Crop-Aligned Cutout (CA-Cut)\nwhich masks random regions out in input images that are spatially distributed\naround crop rows on the sides to encourage trained models to capture high-level\ncontextual features even when fine-grained information is obstructed. Our\nextensive experiments with a public cornfield dataset demonstrate that\nmasking-based augmentations are effective for simulating occlusions and\nsignificantly improving robustness in semantic keypoint predictions for visual\nnavigation. In particular, we show that biasing the mask distribution toward\ncrop rows in CA-Cut is critical for enhancing both prediction accuracy and\ngeneralizability across diverse environments achieving up to a 36.9% reduction\nin prediction error. In addition, we conduct ablation studies to determine the\nnumber of masks, the size of each mask, and the spatial distribution of masks\nto maximize overall performance."
    },
    {
        "date": "2025-07",
        "title": "On the Interaction of Compressibility and Adversarial Robustness",
        "author": "Melih Barsbey, Ant\u00f4nio H. Ribeiro, Umut \u015eim\u015fekli, and Tolga Birdal",
        "link": "http://arxiv.org/abs/2507.17725v1",
        "abstract": "Modern neural networks are expected to simultaneously satisfy a host of\ndesirable properties: accurate fitting to training data, generalization to\nunseen inputs, parameter and computational efficiency, and robustness to\nadversarial perturbations. While compressibility and robustness have each been\nstudied extensively, a unified understanding of their interaction still remains\nelusive. In this work, we develop a principled framework to analyze how\ndifferent forms of compressibility - such as neuron-level sparsity and spectral\ncompressibility - affect adversarial robustness. We show that these forms of\ncompression can induce a small number of highly sensitive directions in the\nrepresentation space, which adversaries can exploit to construct effective\nperturbations. Our analysis yields a simple yet instructive robustness bound,\nrevealing how neuron and spectral compressibility impact $L_\\infty$ and $L_2$\nrobustness via their effects on the learned representations. Crucially, the\nvulnerabilities we identify arise irrespective of how compression is achieved -\nwhether via regularization, architectural bias, or implicit learning dynamics.\nThrough empirical evaluations across synthetic and realistic tasks, we confirm\nour theoretical predictions, and further demonstrate that these vulnerabilities\npersist under adversarial training and transfer learning, and contribute to the\nemergence of universal adversarial perturbations. Our findings show a\nfundamental tension between structured compressibility and robustness, and\nsuggest new pathways for designing models that are both efficient and secure."
    },
    {
        "date": "2025-07",
        "title": "Quantum Software Security Challenges within Shared Quantum Computing Environments",
        "author": "Samuel Ovaskainen, Majid Haghparast, and Tommi Mikkonen",
        "link": "http://arxiv.org/abs/2507.17712v1",
        "abstract": "The number of qubits in quantum computers keeps growing, but most quantum\nprograms remain relatively small because of the noisy nature of the underlying\nquantum hardware. This might lead quantum cloud providers to explore increased\nhardware utilization, and thus profitability through means such as\nmulti-programming, which would allow the execution of multiple programs in\nparallel. The adoption of such technology would bring entirely new challenges\nto the field of quantum software security. This article explores and reports\nthe key challenges identified in quantum software security within shared\nquantum computing environments."
    },
    {
        "date": "2025-07",
        "title": "Rethinking HSM and TPM Security in the Cloud: Real-World Attacks and Next-Gen Defenses",
        "author": "Shams Shaikh, and Trima P. Fernandes e Fizardo",
        "link": "http://arxiv.org/abs/2507.17655v1",
        "abstract": "As organizations rapidly migrate to the cloud, the security of cryptographic\nkey management has become a growing concern. Hardware Security Modules (HSMs)\nand Trusted Platform Modules (TPMs), traditionally seen as the gold standard\nfor securing encryption keys and digital trust, are increasingly challenged by\ncloud-native threats. Real-world breaches have exposed weaknesses in cloud\ndeployments, including misconfigurations, API abuse, and privilege escalations,\nallowing attackers to access sensitive key material and bypass protections.\nThese incidents reveal that while the hardware remains secure, the surrounding\ncloud ecosystem introduces systemic vulnerabilities. This paper analyzes\nnotable security failures involving HSMs and TPMs, identifies common attack\nvectors, and questions longstanding assumptions about their effectiveness in\ndistributed environments. We explore alternative approaches such as\nconfidential computing, post-quantum cryptography, and decentralized key\nmanagement. Our findings highlight that while HSMs and TPMs still play a role,\nmodern cloud security requires more adaptive, layered architectures. By\nevaluating both current weaknesses and emerging models, this research equips\ncloud architects and security engineers with strategies to reinforce\ncryptographic trust in the evolving threat landscape."
    },
    {
        "date": "2025-07",
        "title": "CNS-Bench: Benchmarking Image Classifier Robustness Under Continuous Nuisance Shifts",
        "author": "Olaf D\u00fcnkel, Artur Jesslen, Jiahao Xie, Christian Theobalt, Christian Rupprecht, and Adam Kortylewski",
        "link": "http://arxiv.org/abs/2507.17651v1",
        "abstract": "An important challenge when using computer vision models in the real world is\nto evaluate their performance in potential out-of-distribution (OOD) scenarios.\nWhile simple synthetic corruptions are commonly applied to test OOD robustness,\nthey often fail to capture nuisance shifts that occur in the real world.\nRecently, diffusion models have been applied to generate realistic images for\nbenchmarking, but they are restricted to binary nuisance shifts. In this work,\nwe introduce CNS-Bench, a Continuous Nuisance Shift Benchmark to quantify OOD\nrobustness of image classifiers for continuous and realistic generative\nnuisance shifts. CNS-Bench allows generating a wide range of individual\nnuisance shifts in continuous severities by applying LoRA adapters to diffusion\nmodels. To address failure cases, we propose a filtering mechanism that\noutperforms previous methods, thereby enabling reliable benchmarking with\ngenerative models. With the proposed benchmark, we perform a large-scale study\nto evaluate the robustness of more than 40 classifiers under various nuisance\nshifts. Through carefully designed comparisons and analyses, we find that model\nrankings can change for varying shifts and shift scales, which cannot be\ncaptured when applying common binary shifts. Additionally, we show that\nevaluating the model performance on a continuous scale allows the\nidentification of model failure points, providing a more nuanced understanding\nof model robustness. Project page including code and data:\nhttps://genintel.github.io/CNS."
    },
    {
        "date": "2025-07",
        "title": "Boosting Ray Search Procedure of Hard-label Attacks with Transfer-based Priors",
        "author": "Chen Ma, Xinjie Xu, Shuyu Cheng, and Qi Xuan",
        "link": "http://arxiv.org/abs/2507.17577v1",
        "abstract": "One of the most practical and challenging types of black-box adversarial\nattacks is the hard-label attack, where only the top-1 predicted label is\navailable. One effective approach is to search for the optimal ray direction\nfrom the benign image that minimizes the $\\ell_p$-norm distance to the\nadversarial region. The unique advantage of this approach is that it transforms\nthe hard-label attack into a continuous optimization problem. The objective\nfunction value is the ray's radius, which can be obtained via binary search at\na high query cost. Existing methods use a \"sign trick\" in gradient estimation\nto reduce the number of queries. In this paper, we theoretically analyze the\nquality of this gradient estimation and propose a novel prior-guided approach\nto improve ray search efficiency both theoretically and empirically.\nSpecifically, we utilize the transfer-based priors from surrogate models, and\nour gradient estimators appropriately integrate them by approximating the\nprojection of the true gradient onto the subspace spanned by these priors and\nrandom directions, in a query-efficient manner. We theoretically derive the\nexpected cosine similarities between the obtained gradient estimators and the\ntrue gradient, and demonstrate the improvement achieved by incorporating\npriors. Extensive experiments on the ImageNet and CIFAR-10 datasets show that\nour approach significantly outperforms 11 state-of-the-art methods in terms of\nquery efficiency."
    },
    {
        "date": "2025-07",
        "title": "An h-space Based Adversarial Attack for Protection Against Few-shot Personalization",
        "author": "Xide Xu, Sandesh Kamath, Muhammad Atif Butt, and Bogdan Raducanu",
        "link": "http://arxiv.org/abs/2507.17554v1",
        "abstract": "The versatility of diffusion models in generating customized images from few\nsamples raises significant privacy concerns, particularly regarding\nunauthorized modifications of private content. This concerning issue has\nrenewed the efforts in developing protection mechanisms based on adversarial\nattacks, which generate effective perturbations to poison diffusion models. Our\nwork is motivated by the observation that these models exhibit a high degree of\nabstraction within their semantic latent space (`h-space'), which encodes\ncritical high-level features for generating coherent and meaningful content. In\nthis paper, we propose a novel anti-customization approach, called HAAD\n(h-space based Adversarial Attack for Diffusion models), that leverages\nadversarial attacks to craft perturbations based on the h-space that can\nefficiently degrade the image generation process. Building upon HAAD, we\nfurther introduce a more efficient variant, HAAD-KV, that constructs\nperturbations solely based on the KV parameters of the h-space. This strategy\noffers a stronger protection, that is computationally less expensive. Despite\ntheir simplicity, our methods outperform state-of-the-art adversarial attacks,\nhighlighting their effectiveness."
    },
    {
        "date": "2025-07",
        "title": "Enabling Cyber Security Education through Digital Twins and Generative AI",
        "author": "Vita Santa Barletta, Vito Bavaro, Miriana Calvano, Antonio Curci, Antonio Piccinno, and Davide Pio Posa",
        "link": "http://arxiv.org/abs/2507.17518v1",
        "abstract": "Digital Twins (DTs) are gaining prominence in cybersecurity for their ability\nto replicate complex IT (Information Technology), OT (Operational Technology),\nand IoT (Internet of Things) infrastructures, allowing for real time\nmonitoring, threat analysis, and system simulation. This study investigates how\nintegrating DTs with penetration testing tools and Large Language Models (LLMs)\ncan enhance cybersecurity education and operational readiness. By simulating\nrealistic cyber environments, this approach offers a practical, interactive\nframework for exploring vulnerabilities and defensive strategies. At the core\nof this research is the Red Team Knife (RTK), a custom penetration testing\ntoolkit aligned with the Cyber Kill Chain model. RTK is designed to guide\nlearners through key phases of cyberattacks, including reconnaissance,\nexploitation, and response within a DT powered ecosystem. The incorporation of\nLarge Language Models (LLMs) further enriches the experience by providing\nintelligent, real-time feedback, natural language threat explanations, and\nadaptive learning support during training exercises. This combined DT LLM\nframework is currently being piloted in academic settings to develop hands on\nskills in vulnerability assessment, threat detection, and security operations.\nInitial findings suggest that the integration significantly improves the\neffectiveness and relevance of cybersecurity training, bridging the gap between\ntheoretical knowledge and real-world application. Ultimately, the research\ndemonstrates how DTs and LLMs together can transform cybersecurity education to\nmeet evolving industry demands."
    },
    {
        "date": "2025-07",
        "title": "Active Attack Resilience in 5G: A New Take on Authentication and Key Agreement",
        "author": "Nazatul H. Sultan, Xinlong Guan, Josef Pieprzyk, Wei Ni, Sharif Abuadbba, and Hajime Suzuki",
        "link": "http://arxiv.org/abs/2507.17491v2",
        "abstract": "As 5G networks expand into critical infrastructure, secure and efficient user\nauthentication is more important than ever. The 5G-AKA protocol, standardized\nby 3GPP in TS 33.501, is central to authentication in current 5G deployments.\nIt provides mutual authentication, user privacy, and key secrecy. However,\ndespite its adoption, 5G-AKA has known limitations in both security and\nperformance. While it focuses on protecting privacy against passive attackers,\nrecent studies show its vulnerabilities to active attacks. It also relies on a\nsequence number mechanism to prevent replay attacks, requiring perfect\nsynchronization between the device and the core network. This stateful design\nadds complexity, causes desynchronization, and incurs extra communication\noverhead. More critically, 5G-AKA lacks Perfect Forward Secrecy (PFS), exposing\npast communications if long-term keys are compromised-an increasing concern\namid sophisticated threats. This paper proposes an enhanced authentication\nprotocol that builds on 5G-AKA's design while addressing its shortcomings.\nFirst, we introduce a stateless version that removes sequence number reliance,\nreducing complexity while staying compatible with existing SIM cards and\ninfrastructure. We then extend this design to add PFS with minimal\ncryptographic overhead. Both protocols are rigorously analyzed using ProVerif,\nconfirming their compliance with all major security requirements, including\nresistance to passive and active attacks, as well as those defined by 3GPP and\nacademic studies. We also prototype both protocols and evaluate their\nperformance against 5G-AKA and 5G-AKA' (USENIX'21). Our results show the\nproposed protocols offer stronger security with only minor computational\noverhead, making them practical, future-ready solutions for 5G and beyond."
    },
    {
        "date": "2025-07",
        "title": "Doubly robust outlier resistant inference on causal treatment effect",
        "author": "Joonsung Kang",
        "link": "http://arxiv.org/abs/2507.17439v1",
        "abstract": "Outliers can severely distort causal effect estimation in observational\nstudies, yet this issue has received limited attention in the literature. Their\ninfluence is especially pronounced in small sample sizes, where detecting and\nremoving outliers becomes increasingly difficult. Therefore, it is essential to\nestimate treatment effects robustly without excluding these influential data\npoints. To address this, we propose a doubly robust point estimator for the\naverage treatment effect under a contaminated model that includes outliers.\nRobustness in outcome regression is achieved through a robust estimating\nequation, while covariate balancing propensity scores (CBPS) ensure resilience\nin propensity score modeling.\n  To prevent model overfitting due to the inclusion of numerous parameters, we\nincorporate variable selection. All these components are unified under a\npenalized empirical likelihood framework. For confidence interval estimation,\nmost existing approaches rely on asymptotic properties, which may be unreliable\nin finite samples. We derive an optimal finite-sample confidence interval for\nthe average treatment effect using our proposed estimating equation, ensuring\nthat the interval bounds remain unaffected by outliers. Through simulations and\na real-world application involving hypertension data with outliers, we\ndemonstrate that our method consistently outperforms existing approaches in\nboth accuracy and robustness."
    },
    {
        "date": "2025-07",
        "title": "Ctx2TrajGen: Traffic Context-Aware Microscale Vehicle Trajectories using Generative Adversarial Imitation Learning",
        "author": "Joobin Jin, Seokjun Hong, Gyeongseon Baek, Yeeun Kim, and Byeongjoon Noh",
        "link": "http://arxiv.org/abs/2507.17418v1",
        "abstract": "Precise modeling of microscopic vehicle trajectories is critical for traffic\nbehavior analysis and autonomous driving systems. We propose Ctx2TrajGen, a\ncontext-aware trajectory generation framework that synthesizes realistic urban\ndriving behaviors using GAIL. Leveraging PPO and WGAN-GP, our model addresses\nnonlinear interdependencies and training instability inherent in microscopic\nsettings. By explicitly conditioning on surrounding vehicles and road geometry,\nCtx2TrajGen generates interaction-aware trajectories aligned with real-world\ncontext. Experiments on the drone-captured DRIFT dataset demonstrate superior\nperformance over existing methods in terms of realism, behavioral diversity,\nand contextual fidelity, offering a robust solution to data scarcity and domain\nshift without simulation."
    },
    {
        "date": "2025-07",
        "title": "A Zero-overhead Flow for Security Closure",
        "author": "Mohammad Eslami, Ashira Johara, Kyungbin Park, and Samuel Pagliarini",
        "link": "http://arxiv.org/abs/2507.17385v1",
        "abstract": "In the traditional Application-Specific Integrated Circuit (ASIC) design\nflow, the concept of timing closure implies to reach convergence during\nphysical synthesis such that, under a given area and power budget, the design\nworks at the targeted frequency. However, security has been largely neglected\nwhen evaluating the Quality of Results (QoR) from physical synthesis. In\ngeneral, commercial place & route tools do not understand security goals. In\nthis work, we propose a modified ASIC design flow that is security-aware and,\ndifferently from prior research, does not degrade QoR for the sake of security\nimprovement. Therefore, we propose a first-of-its-kind zero-overhead flow for\nsecurity closure. Our flow is concerned with two distinct threat models: (i)\ninsertion of Hardware Trojans (HTs) and (ii) physical probing/fault injection.\nImportantly, the flow is entirely executed within a commercial place & route\nengine and is scalable. In several metrics, our security-aware flow achieves\nthe best-known results for the ISPD`22 set of benchmark circuits while\nincurring negligible design overheads due to security-related strategies.\nFinally, we open source the entire methodology (as a set of scripts) and also\nshare the protected circuits (as design databases) for the benefit of the\nhardware security community."
    },
    {
        "date": "2025-07",
        "title": "An Empirical Study on Virtual Reality Software Security Weaknesses",
        "author": "Yifan Xu, Jinfu Chen, Zhenyu Qi, Huashan Chen, Junyi Wang, Pengfei Hu, Feng Liu, and Sen He",
        "link": "http://arxiv.org/abs/2507.17324v2",
        "abstract": "Virtual Reality (VR) has emerged as a transformative technology across\nindustries, yet its security weaknesses, including vulnerabilities, are\nunderinvestigated. This study investigates 334 VR projects hosted on GitHub,\nexamining 1,681 software security weaknesses to understand: what types of\nweaknesses are prevalent in VR software; when and how weaknesses are\nintroduced; how long they have survived; and how they have been removed. Due to\nthe limited availability of VR software security weaknesses in public databases\n(e.g., the National Vulnerability Database or NVD), we prepare the first\nsystematic dataset of VR software security weaknesses by introducing a novel\nframework to collect such weaknesses from GitHub commit data. Our empirical\nstudy on the dataset leads to useful insights, including: (i) VR weaknesses are\nheavily skewed toward user interface weaknesses, followed by resource-related\nweaknesses; (ii) VR development tools pose higher security risks than VR\napplications; (iii) VR security weaknesses are often introduced at the VR\nsoftware birth time."
    },
    {
        "date": "2025-07",
        "title": "Tab-MIA: A Benchmark Dataset for Membership Inference Attacks on Tabular Data in LLMs",
        "author": "Eyal German, Sagiv Antebi, Daniel Samira, Asaf Shabtai, and Yuval Elovici",
        "link": "http://arxiv.org/abs/2507.17259v1",
        "abstract": "Large language models (LLMs) are increasingly trained on tabular data, which,\nunlike unstructured text, often contains personally identifiable information\n(PII) in a highly structured and explicit format. As a result, privacy risks\narise, since sensitive records can be inadvertently retained by the model and\nexposed through data extraction or membership inference attacks (MIAs). While\nexisting MIA methods primarily target textual content, their efficacy and\nthreat implications may differ when applied to structured data, due to its\nlimited content, diverse data types, unique value distributions, and\ncolumn-level semantics. In this paper, we present Tab-MIA, a benchmark dataset\nfor evaluating MIAs on tabular data in LLMs and demonstrate how it can be used.\nTab-MIA comprises five data collections, each represented in six different\nencoding formats. Using our Tab-MIA benchmark, we conduct the first evaluation\nof state-of-the-art MIA methods on LLMs finetuned with tabular data across\nmultiple encoding formats. In the evaluation, we analyze the memorization\nbehavior of pretrained LLMs on structured data derived from Wikipedia tables.\nOur findings show that LLMs memorize tabular data in ways that vary across\nencoding formats, making them susceptible to extraction via MIAs. Even when\nfine-tuned for as few as three epochs, models exhibit high vulnerability, with\nAUROC scores approaching 90% in most cases. Tab-MIA enables systematic\nevaluation of these risks and provides a foundation for developing\nprivacy-preserving methods for tabular data in LLMs."
    },
    {
        "date": "2025-07",
        "title": "HuiduRep: A Robust Self-Supervised Framework for Learning Neural Representations from Extracellular Spikes",
        "author": "Feng Cao, and Zishuo Feng",
        "link": "http://arxiv.org/abs/2507.17224v1",
        "abstract": "Extracellular recordings are brief voltage fluctuations recorded near\nneurons, widely used in neuroscience as the basis for decoding brain activity\nat single-neuron resolution. Spike sorting, which assigns each spike to its\nsource neuron, is a critical step in brain sensing pipelines. However, it\nremains challenging under low signal-to-noise ratio (SNR), electrode drift, and\ncross-session variability. In this paper, we propose HuiduRep, a robust\nself-supervised representation learning framework that extracts discriminative\nand generalizable features from extracellular spike waveforms. By combining\ncontrastive learning with a denoising autoencoder, HuiduRep learns latent\nrepresentations that are robust to noise and drift. Built on HuiduRep, we\ndevelop a spike sorting pipeline that clusters spike representations without\nsupervision. Experiments on hybrid and real-world datasets demonstrate that\nHuiduRep achieves strong robustness and the pipeline matches or outperforms\nstate-of-the-art tools such as KiloSort4 and MountainSort5. These findings\ndemonstrate the potential of self-supervised spike representation learning as a\nfoundational tool for robust and generalizable processing of extracellular\nrecordings."
    },
    {
        "date": "2025-07",
        "title": "LLM Meets the Sky: Heuristic Multi-Agent Reinforcement Learning for Secure Heterogeneous UAV Networks",
        "author": "Lijie Zheng, Ji He, Shih Yu Chang, Yulong Shen, and Dusit Niyato",
        "link": "http://arxiv.org/abs/2507.17188v1",
        "abstract": "This work tackles the physical layer security (PLS) problem of maximizing the\nsecrecy rate in heterogeneous UAV networks (HetUAVNs) under propulsion energy\nconstraints. Unlike prior studies that assume uniform UAV capabilities or\noverlook energy-security trade-offs, we consider a realistic scenario where\nUAVs with diverse payloads and computation resources collaborate to serve\nground terminals in the presence of eavesdroppers. To manage the complex\ncoupling between UAV motion and communication, we propose a hierarchical\noptimization framework. The inner layer uses a semidefinite relaxation\n(SDR)-based S2DC algorithm combining penalty functions and difference-of-convex\n(d.c.) programming to solve the secrecy precoding problem with fixed UAV\npositions. The outer layer introduces a Large Language Model (LLM)-guided\nheuristic multi-agent reinforcement learning approach (LLM-HeMARL) for\ntrajectory optimization. LLM-HeMARL efficiently incorporates expert heuristics\npolicy generated by the LLM, enabling UAVs to learn energy-aware,\nsecurity-driven trajectories without the inference overhead of real-time LLM\ncalls. The simulation results show that our method outperforms existing\nbaselines in secrecy rate and energy efficiency, with consistent robustness\nacross varying UAV swarm sizes and random seeds."
    },
    {
        "date": "2025-07",
        "title": "VGS-ATD: Robust Distributed Learning for Multi-Label Medical Image Classification Under Heterogeneous and Imbalanced Conditions",
        "author": "Zehui Zhao, Laith Alzubaidi, Haider A. Alwzwazy, Jinglan Zhang, and Yuantong Gu",
        "link": "http://arxiv.org/abs/2507.18657v2",
        "abstract": "In recent years, advanced deep learning architectures have shown strong\nperformance in medical imaging tasks. However, the traditional centralized\nlearning paradigm poses serious privacy risks as all data is collected and\ntrained on a single server. To mitigate this challenge, decentralized\napproaches such as federated learning and swarm learning have emerged, allowing\nmodel training on local nodes while sharing only model weights. While these\nmethods enhance privacy, they struggle with heterogeneous and imbalanced data\nand suffer from inefficiencies due to frequent communication and the\naggregation of weights. More critically, the dynamic and complex nature of\nclinical environments demands scalable AI systems capable of continuously\nlearning from diverse modalities and multilabels. Yet, both centralized and\ndecentralized models are prone to catastrophic forgetting during system\nexpansion, often requiring full model retraining to incorporate new data. To\naddress these limitations, we propose VGS-ATD, a novel distributed learning\nframework. To validate VGS-ATD, we evaluate it in experiments spanning 30\ndatasets and 80 independent labels across distributed nodes, VGS-ATD achieved\nan overall accuracy of 92.7%, outperforming centralized learning (84.9%) and\nswarm learning (72.99%), while federated learning failed under these conditions\ndue to high requirements on computational resources. VGS-ATD also demonstrated\nstrong scalability, with only a 1% drop in accuracy on existing nodes after\nexpansion, compared to a 20% drop in centralized learning, highlighting its\nresilience to catastrophic forgetting. Additionally, it reduced computational\ncosts by up to 50% relative to both centralized and swarm learning, confirming\nits superior efficiency and scalability."
    },
    {
        "date": "2025-07",
        "title": "Addressing High Class Imbalance in Multi-Class Diabetic Retinopathy Severity Grading with Augmentation and Transfer Learning",
        "author": "Faisal Ahmed",
        "link": "http://arxiv.org/abs/2507.17121v2",
        "abstract": "Diabetic retinopathy (DR) is a leading cause of vision loss worldwide, and\nearly diagnosis through automated retinal image analysis can significantly\nreduce the risk of blindness. This paper presents a robust deep learning\nframework for both binary and five-class DR classification, leveraging transfer\nlearning and extensive data augmentation to address the challenges of class\nimbalance and limited training data. We evaluate a range of pretrained\nconvolutional neural network architectures, including variants of ResNet and\nEfficientNet, on the APTOS 2019 dataset.\n  For binary classification, our proposed model achieves a state-of-the-art\naccuracy of 98.9%, with a precision of 98.6%, recall of 99.3%, F1-score of\n98.9%, and an AUC of 99.4%. In the more challenging five-class severity\nclassification task, our model obtains a competitive accuracy of 84.6% and an\nAUC of 94.1%, outperforming several existing approaches. Our findings also\ndemonstrate that EfficientNet-B0 and ResNet34 offer optimal trade-offs between\naccuracy and computational efficiency across both tasks.\n  These results underscore the effectiveness of combining class-balanced\naugmentation with transfer learning for high-performance DR diagnosis. The\nproposed framework provides a scalable and accurate solution for DR screening,\nwith potential for deployment in real-world clinical environments."
    },
    {
        "date": "2025-07",
        "title": "Advancing Robustness in Deep Reinforcement Learning with an Ensemble Defense Approach",
        "author": "Adithya Mohan, Dominik R\u00f6\u00dfle, Daniel Cremers, and Torsten Sch\u00f6n",
        "link": "http://arxiv.org/abs/2507.17070v1",
        "abstract": "Recent advancements in Deep Reinforcement Learning (DRL) have demonstrated\nits applicability across various domains, including robotics, healthcare,\nenergy optimization, and autonomous driving. However, a critical question\nremains: How robust are DRL models when exposed to adversarial attacks? While\nexisting defense mechanisms such as adversarial training and distillation\nenhance the resilience of DRL models, there remains a significant research gap\nregarding the integration of multiple defenses in autonomous driving scenarios\nspecifically. This paper addresses this gap by proposing a novel ensemble-based\ndefense architecture to mitigate adversarial attacks in autonomous driving. Our\nevaluation demonstrates that the proposed architecture significantly enhances\nthe robustness of DRL models. Compared to the baseline under FGSM attacks, our\nensemble method improves the mean reward from 5.87 to 18.38 (over 213%\nincrease) and reduces the mean collision rate from 0.50 to 0.09 (an 82%\ndecrease) in the highway scenario and merge scenario, outperforming all\nstandalone defense strategies."
    },
    {
        "date": "2025-07",
        "title": "SoK: Securing the Final Frontier for Cybersecurity in Space-Based Infrastructure",
        "author": "Nafisa Anjum, and Tasnuva Farheen",
        "link": "http://arxiv.org/abs/2507.17064v1",
        "abstract": "With the advent of modern technology, critical infrastructure,\ncommunications, and national security depend increasingly on space-based\nassets. These assets, along with associated assets like data relay systems and\nground stations, are, therefore, in serious danger of cyberattacks. Strong\nsecurity defenses are essential to ensure data integrity, maintain secure\noperations, and protect assets in space and on the ground against various\nthreats. Previous research has found discrete vulnerabilities in space systems\nand suggested specific solutions to address them. Such research has yielded\nvaluable insights, but lacks a thorough examination of space cyberattack\nvectors and a rigorous assessment of the efficacy of mitigation techniques.\nThis study tackles this issue by taking a comprehensive approach to analyze the\nrange of possible space cyber-attack vectors, which include ground, space,\nsatellite, and satellite constellations. In order to address the particular\nthreats, the study also assesses the efficacy of mitigation measures that are\nlinked with space infrastructures and proposes a Risk Scoring Framework. Based\non the analysis, this paper identifies potential research challenges for\ndeveloping and testing cutting-edge technology solutions, encouraging robust\ncybersecurity measures needed in space."
    },
    {
        "date": "2025-07",
        "title": "GATEBLEED: Exploiting On-Core Accelerator Power Gating for High Performance & Stealthy Attacks on AI",
        "author": "Joshua Kalyanapu, Farshad Dizani, Darsh Asher, Azam Ghanbari, Rosario Cammarota, Aydin Aysu, and Samira Mirbagher Ajorpaz",
        "link": "http://arxiv.org/abs/2507.17033v1",
        "abstract": "As power consumption from AI training and inference continues to increase, AI\naccelerators are being integrated directly into the CPU. Intel's Advanced\nMatrix Extensions (AMX) is one such example, debuting on the 4th generation\nIntel Xeon Scalable CPU. We discover a timing side and covert channel,\nGATEBLEED, caused by the aggressive power gating utilized to keep the CPU\nwithin operating limits. We show that the GATEBLEED side channel is a threat to\nAI privacy as many ML models such as transformers and CNNs make critical\ncomputationally-heavy decisions based on private values like confidence\nthresholds and routing logits. Timing delays from selective powering down of\nAMX components mean that each matrix multiplication is a potential leakage\npoint when executed on the AMX accelerator. Our research identifies over a\ndozen potential gadgets across popular ML libraries (HuggingFace, PyTorch,\nTensorFlow, etc.), revealing that they can leak sensitive and private\ninformation. GATEBLEED poses a risk for local and remote timing inference, even\nunder previous protective measures. GATEBLEED can be used as a high\nperformance, stealthy remote covert channel and a generic magnifier for timing\ntransmission channels, capable of bypassing traditional cache defenses to leak\narbitrary memory addresses and evading state of the art microarchitectural\nattack detectors under realistic network conditions and system configurations\nin which previous attacks fail. We implement an end-to-end microarchitectural\ninference attack on a transformer model optimized with Intel AMX, achieving a\nmembership inference accuracy of 81% and a precision of 0.89. In a CNN-based or\ntransformer-based mixture-of-experts model optimized with Intel AMX, we leak\nexpert choice with 100% accuracy."
    },
    {
        "date": "2025-07",
        "title": "Towards Trustworthy AI: Secure Deepfake Detection using CNNs and Zero-Knowledge Proofs",
        "author": "H M Mohaimanul Islam, Huynh Q. N. Vo, and Aditya Rane",
        "link": "http://arxiv.org/abs/2507.17010v1",
        "abstract": "In the era of synthetic media, deepfake manipulations pose a significant\nthreat to information integrity. To address this challenge, we propose\nTrustDefender, a two-stage framework comprising (i) a lightweight convolutional\nneural network (CNN) that detects deepfake imagery in real-time extended\nreality (XR) streams, and (ii) an integrated succinct zero-knowledge proof\n(ZKP) protocol that validates detection results without disclosing raw user\ndata. Our design addresses both the computational constraints of XR platforms\nwhile adhering to the stringent privacy requirements in sensitive settings.\nExperimental evaluations on multiple benchmark deepfake datasets demonstrate\nthat TrustDefender achieves 95.3% detection accuracy, coupled with efficient\nproof generation underpinned by rigorous cryptography, ensuring seamless\nintegration with high-performance artificial intelligence (AI) systems. By\nfusing advanced computer vision models with provable security mechanisms, our\nwork establishes a foundation for reliable AI in immersive and\nprivacy-sensitive applications."
    },
    {
        "date": "2025-07",
        "title": "ShrinkBox: Backdoor Attack on Object Detection to Disrupt Collision Avoidance in Machine Learning-based Advanced Driver Assistance Systems",
        "author": "Muhammad Zaeem Shahzad, Muhammad Abdullah Hanif, Bassem Ouni, and Muhammad Shafique",
        "link": "http://arxiv.org/abs/2507.18656v1",
        "abstract": "Advanced Driver Assistance Systems (ADAS) significantly enhance road safety\nby detecting potential collisions and alerting drivers. However, their reliance\non expensive sensor technologies such as LiDAR and radar limits accessibility,\nparticularly in low- and middle-income countries. Machine learning-based ADAS\n(ML-ADAS), leveraging deep neural networks (DNNs) with only standard camera\ninput, offers a cost-effective alternative. Critical to ML-ADAS is the\ncollision avoidance feature, which requires the ability to detect objects and\nestimate their distances accurately. This is achieved with specialized DNNs\nlike YOLO, which provides real-time object detection, and a lightweight,\ndetection-wise distance estimation approach that relies on key features\nextracted from the detections like bounding box dimensions and size. However,\nthe robustness of these systems is undermined by security vulnerabilities in\nobject detectors. In this paper, we introduce ShrinkBox, a novel backdoor\nattack targeting object detection in collision avoidance ML-ADAS. Unlike\nexisting attacks that manipulate object class labels or presence, ShrinkBox\nsubtly shrinks ground truth bounding boxes. This attack remains undetected in\ndataset inspections and standard benchmarks while severely disrupting\ndownstream distance estimation. We demonstrate that ShrinkBox can be realized\nin the YOLOv9m object detector at an Attack Success Rate (ASR) of 96%, with\nonly a 4% poisoning ratio in the training instances of the KITTI dataset.\nFurthermore, given the low error targets introduced in our relaxed poisoning\nstrategy, we find that ShrinkBox increases the Mean Absolute Error (MAE) in\ndownstream distance estimation by more than 3x on poisoned samples, potentially\nresulting in delays or prevention of collision warnings altogether."
    },
    {
        "date": "2025-07",
        "title": "A Hybrid CNN-VSSM model for Multi-View, Multi-Task Mammography Analysis: Robust Diagnosis with Attention-Based Fusion",
        "author": "Yalda Zafari, Roaa Elalfy, Mohamed Mabrok, Somaya Al-Maadeed, Tamer Khattab, and Essam A. Rashed",
        "link": "http://arxiv.org/abs/2507.16955v1",
        "abstract": "Early and accurate interpretation of screening mammograms is essential for\neffective breast cancer detection, yet it remains a complex challenge due to\nsubtle imaging findings and diagnostic ambiguity. Many existing AI approaches\nfall short by focusing on single view inputs or single-task outputs, limiting\ntheir clinical utility. To address these limitations, we propose a novel\nmulti-view, multitask hybrid deep learning framework that processes all four\nstandard mammography views and jointly predicts diagnostic labels and BI-RADS\nscores for each breast. Our architecture integrates a hybrid CNN VSSM backbone,\ncombining convolutional encoders for rich local feature extraction with Visual\nState Space Models (VSSMs) to capture global contextual dependencies. To\nimprove robustness and interpretability, we incorporate a gated attention-based\nfusion module that dynamically weights information across views, effectively\nhandling cases with missing data. We conduct extensive experiments across\ndiagnostic tasks of varying complexity, benchmarking our proposed hybrid models\nagainst baseline CNN architectures and VSSM models in both single task and\nmulti task learning settings. Across all tasks, the hybrid models consistently\noutperform the baselines. In the binary BI-RADS 1 vs. 5 classification task,\nthe shared hybrid model achieves an AUC of 0.9967 and an F1 score of 0.9830.\nFor the more challenging ternary classification, it attains an F1 score of\n0.7790, while in the five-class BI-RADS task, the best F1 score reaches 0.4904.\nThese results highlight the effectiveness of the proposed hybrid framework and\nunderscore both the potential and limitations of multitask learning for\nimproving diagnostic performance and enabling clinically meaningful mammography\nanalysis."
    },
    {
        "date": "2025-07",
        "title": "When LLMs Copy to Think: Uncovering Copy-Guided Attacks in Reasoning LLMs",
        "author": "Yue Li, Xiao Li, Hao Wu, Yue Zhang, Fengyuan Xu, Xiuzhen Cheng, and Sheng Zhong",
        "link": "http://arxiv.org/abs/2507.16773v1",
        "abstract": "Large Language Models (LLMs) have become integral to automated code analysis,\nenabling tasks such as vulnerability detection and code comprehension. However,\ntheir integration introduces novel attack surfaces. In this paper, we identify\nand investigate a new class of prompt-based attacks, termed Copy-Guided Attacks\n(CGA), which exploit the inherent copying tendencies of reasoning-capable LLMs.\nBy injecting carefully crafted triggers into external code snippets,\nadversaries can induce the model to replicate malicious content during\ninference. This behavior enables two classes of vulnerabilities: inference\nlength manipulation, where the model generates abnormally short or excessively\nlong reasoning traces; and inference result manipulation, where the model\nproduces misleading or incorrect conclusions. We formalize CGA as an\noptimization problem and propose a gradient-based approach to synthesize\neffective triggers. Empirical evaluation on state-of-the-art reasoning LLMs\nshows that CGA reliably induces infinite loops, premature termination, false\nrefusals, and semantic distortions in code analysis tasks. While highly\neffective in targeted settings, we observe challenges in generalizing CGA\nacross diverse prompts due to computational constraints, posing an open\nquestion for future research. Our findings expose a critical yet underexplored\nvulnerability in LLM-powered development pipelines and call for urgent advances\nin prompt-level defense mechanisms."
    },
    {
        "date": "2025-07",
        "title": "Towards Robust Foundation Models for Digital Pathology",
        "author": "Jonah K\u00f6men, Edwin D. de Jong, Julius Hense, Hannah Marienwald, Jonas Dippel, Philip Naumann, Eric Marcus, Lukas Ruff, Maximilian Alber, Jonas Teuwen, Frederick Klauschen, and Klaus-Robert M\u00fcller",
        "link": "http://arxiv.org/abs/2507.17845v1",
        "abstract": "Biomedical Foundation Models (FMs) are rapidly transforming AI-enabled\nhealthcare research and entering clinical validation. However, their\nsusceptibility to learning non-biological technical features -- including\nvariations in surgical/endoscopic techniques, laboratory procedures, and\nscanner hardware -- poses risks for clinical deployment. We present the first\nsystematic investigation of pathology FM robustness to non-biological features.\nOur work (i) introduces measures to quantify FM robustness, (ii) demonstrates\nthe consequences of limited robustness, and (iii) proposes a framework for FM\nrobustification to mitigate these issues. Specifically, we developed PathoROB,\na robustness benchmark with three novel metrics, including the robustness\nindex, and four datasets covering 28 biological classes from 34 medical\ncenters. Our experiments reveal robustness deficits across all 20 evaluated\nFMs, and substantial robustness differences between them. We found that\nnon-robust FM representations can cause major diagnostic downstream errors and\nclinical blunders that prevent safe clinical adoption. Using more robust FMs\nand post-hoc robustification considerably reduced (but did not yet eliminate)\nthe risk of such errors. This work establishes that robustness evaluation is\nessential for validating pathology FMs before clinical adoption and\ndemonstrates that future FM development must integrate robustness as a core\ndesign principle. PathoROB provides a blueprint for assessing robustness across\nbiomedical domains, guiding FM improvement efforts towards more robust,\nrepresentative, and clinically deployable AI systems that prioritize biological\ninformation over technical artifacts."
    },
    {
        "date": "2025-07",
        "title": "Denoising-While-Completing Network (DWCNet): Robust Point Cloud Completion Under Corruption",
        "author": "Keneni W. Tesema, Lyndon Hill, Mark W. Jones, and Gary K. L. Tam",
        "link": "http://arxiv.org/abs/2507.16743v1",
        "abstract": "Point cloud completion is crucial for 3D computer vision tasks in autonomous\ndriving, augmented reality, and robotics. However, obtaining clean and complete\npoint clouds from real-world environments is challenging due to noise and\nocclusions. Consequently, most existing completion networks -- trained on\nsynthetic data -- struggle with real-world degradations. In this work, we\ntackle the problem of completing and denoising highly corrupted partial point\nclouds affected by multiple simultaneous degradations. To benchmark robustness,\nwe introduce the Corrupted Point Cloud Completion Dataset (CPCCD), which\nhighlights the limitations of current methods under diverse corruptions.\nBuilding on these insights, we propose DWCNet (Denoising-While-Completing\nNetwork), a completion framework enhanced with a Noise Management Module (NMM)\nthat leverages contrastive learning and self-attention to suppress noise and\nmodel structural relationships. DWCNet achieves state-of-the-art performance on\nboth clean and corrupted, synthetic and real-world datasets. The dataset and\ncode will be publicly available at\nhttps://github.com/keneniwt/DWCNET-Robust-Point-Cloud-Completion-against-Corruptions"
    },
    {
        "date": "2025-07",
        "title": "Swift-Sarsa: Fast and Robust Linear Control",
        "author": "Khurram Javed, and Richard S. Sutton",
        "link": "http://arxiv.org/abs/2507.19539v1",
        "abstract": "Javed, Sharifnassab, and Sutton (2024) introduced a new algorithm for TD\nlearning -- SwiftTD -- that augments True Online TD($\\lambda$) with step-size\noptimization, a bound on the effective learning rate, and step-size decay. In\ntheir experiments SwiftTD outperformed True Online TD($\\lambda$) and\nTD($\\lambda$) on a variety of prediction tasks derived from Atari games, and\nits performance was robust to the choice of hyper-parameters. In this extended\nabstract we extend SwiftTD to work for control problems. We combine the key\nideas behind SwiftTD with True Online Sarsa($\\lambda$) to develop an on-policy\nreinforcement learning algorithm called $\\textit{Swift-Sarsa}$.\n  We propose a simple benchmark for linear on-policy control called the\n$\\textit{operant conditioning benchmark}$. The key challenge in the operant\nconditioning benchmark is that a very small subset of input signals are\nrelevant for decision making. The majority of the signals are noise sampled\nfrom a non-stationary distribution. To learn effectively, the agent must learn\nto differentiate between the relevant signals and the noisy signals, and\nminimize prediction errors by assigning credit to the weight parameters\nassociated with the relevant signals.\n  Swift-Sarsa, when applied to the operant conditioning benchmark, learned to\nassign credit to the relevant signals without any prior knowledge of the\nstructure of the problem. It opens the door for solution methods that learn\nrepresentations by searching over hundreds of millions of features in parallel\nwithout performance degradation due to noisy or bad features."
    },
    {
        "date": "2025-07",
        "title": "Robust Noisy Pseudo-label Learning for Semi-supervised Medical Image Segmentation Using Diffusion Model",
        "author": "Lin Xi, Yingliang Ma, Cheng Wang, Sandra Howell, Aldo Rinaldi, and Kawal S. Rhode",
        "link": "http://arxiv.org/abs/2507.16429v1",
        "abstract": "Obtaining pixel-level annotations in the medical domain is both expensive and\ntime-consuming, often requiring close collaboration between clinical experts\nand developers. Semi-supervised medical image segmentation aims to leverage\nlimited annotated data alongside abundant unlabeled data to achieve accurate\nsegmentation. However, existing semi-supervised methods often struggle to\nstructure semantic distributions in the latent space due to noise introduced by\npseudo-labels. In this paper, we propose a novel diffusion-based framework for\nsemi-supervised medical image segmentation. Our method introduces a constraint\ninto the latent structure of semantic labels during the denoising diffusion\nprocess by enforcing prototype-based contrastive consistency. Rather than\nexplicitly delineating semantic boundaries, the model leverages class\nprototypes centralized semantic representations in the latent space as anchors.\nThis strategy improves the robustness of dense predictions, particularly in the\npresence of noisy pseudo-labels. We also introduce a new publicly available\nbenchmark: Multi-Object Segmentation in X-ray Angiography Videos (MOSXAV),\nwhich provides detailed, manually annotated segmentation ground truth for\nmultiple anatomical structures in X-ray angiography videos. Extensive\nexperiments on the EndoScapes2023 and MOSXAV datasets demonstrate that our\nmethod outperforms state-of-the-art medical image segmentation approaches under\nthe semi-supervised learning setting. This work presents a robust and\ndata-efficient diffusion model that offers enhanced flexibility and strong\npotential for a wide range of clinical applications."
    },
    {
        "date": "2025-07",
        "title": "ADCD-Net: Robust Document Image Forgery Localization via Adaptive DCT Feature and Hierarchical Content Disentanglement",
        "author": "Kahim Wong, Jicheng Zhou, Haiwei Wu, Yain-Whar Si, and Jiantao Zhou",
        "link": "http://arxiv.org/abs/2507.16397v1",
        "abstract": "The advancement of image editing tools has enabled malicious manipulation of\nsensitive document images, underscoring the need for robust document image\nforgery detection.Though forgery detectors for natural images have been\nextensively studied, they struggle with document images, as the tampered\nregions can be seamlessly blended into the uniform document background (BG) and\nstructured text. On the other hand, existing document-specific methods lack\nsufficient robustness against various degradations, which limits their\npractical deployment. This paper presents ADCD-Net, a robust document forgery\nlocalization model that adaptively leverages the RGB/DCT forensic traces and\nintegrates key characteristics of document images. Specifically, to address the\nDCT traces' sensitivity to block misalignment, we adaptively modulate the DCT\nfeature contribution based on a predicted alignment score, resulting in much\nimproved resilience to various distortions, including resizing and cropping.\nAlso, a hierarchical content disentanglement approach is proposed to boost the\nlocalization performance via mitigating the text-BG disparities. Furthermore,\nnoticing the predominantly pristine nature of BG regions, we construct a\npristine prototype capturing traces of untampered regions, and eventually\nenhance both the localization accuracy and robustness. Our proposed ADCD-Net\ndemonstrates superior forgery localization performance, consistently\noutperforming state-of-the-art methods by 20.79\\% averaged over 5 types of\ndistortions. The code is available at https://github.com/KAHIMWONG/ACDC-Net."
    },
    {
        "date": "2025-07",
        "title": "Are Foundation Models All You Need for Zero-shot Face Presentation Attack Detection?",
        "author": "Lazaro Janier Gonzalez-Sole, Juan E. Tapia, and Christoph Busch",
        "link": "http://arxiv.org/abs/2507.16393v1",
        "abstract": "Although face recognition systems have undergone an impressive evolution in\nthe last decade, these technologies are vulnerable to attack presentations\n(AP). These attacks are mostly easy to create and, by executing them against\nthe system's capture device, the malicious actor can impersonate an authorised\nsubject and thus gain access to the latter's information (e.g., financial\ntransactions). To protect facial recognition schemes against presentation\nattacks, state-of-the-art deep learning presentation attack detection (PAD)\napproaches require a large amount of data to produce reliable detection\nperformances and even then, they decrease their performance for unknown\npresentation attack instruments (PAI) or database (information not seen during\ntraining), i.e. they lack generalisability. To mitigate the above problems,\nthis paper focuses on zero-shot PAD. To do so, we first assess the\neffectiveness and generalisability of foundation models in established and\nchallenging experimental scenarios and then propose a simple but effective\nframework for zero-shot PAD. Experimental results show that these models are\nable to achieve performance in difficult scenarios with minimal effort of the\nmore advanced PAD mechanisms, whose weights were optimised mainly with training\nsets that included APs and bona fide presentations. The top-performing\nfoundation model outperforms by a margin the best from the state of the art\nobserved with the leaving-one-out protocol on the SiW-Mv2 database, which\ncontains challenging unknown 2D and 3D attacks"
    },
    {
        "date": "2025-07",
        "title": "The Cost of Compression: Tight Quadratic Black-Box Attacks on Sketches for $\\ell_2$ Norm Estimation",
        "author": "Sara Ahmadian, Edith Cohen, and Uri Stemmer",
        "link": "http://arxiv.org/abs/2507.16345v1",
        "abstract": "Dimensionality reduction via linear sketching is a powerful and widely used\ntechnique, but it is known to be vulnerable to adversarial inputs. We study the\nblack-box adversarial setting, where a fixed, hidden sketching matrix A in\n$R^{k X n}$ maps high-dimensional vectors v $\\in R^n$ to lower-dimensional\nsketches A v in $R^k$, and an adversary can query the system to obtain\napproximate ell2-norm estimates that are computed from the sketch.\n  We present a universal, nonadaptive attack that, using tilde(O)($k^2$)\nqueries, either causes a failure in norm estimation or constructs an\nadversarial input on which the optimal estimator for the query distribution\n(used by the attack) fails. The attack is completely agnostic to the sketching\nmatrix and to the estimator: It applies to any linear sketch and any query\nresponder, including those that are randomized, adaptive, or tailored to the\nquery distribution.\n  Our lower bound construction tightly matches the known upper bounds of\ntilde(Omega)($k^2$), achieved by specialized estimators for Johnson\nLindenstrauss transforms and AMS sketches. Beyond sketching, our results\nuncover structural parallels to adversarial attacks in image classification,\nhighlighting fundamental vulnerabilities of compressed representations."
    },
    {
        "date": "2025-07",
        "title": "Talking Like a Phisher: LLM-Based Attacks on Voice Phishing Classifiers",
        "author": "Wenhao Li, Selvakumar Manickam, Yung-wey Chong, and Shankar Karuppayah",
        "link": "http://arxiv.org/abs/2507.16291v1",
        "abstract": "Voice phishing (vishing) remains a persistent threat in cybersecurity,\nexploiting human trust through persuasive speech. While machine learning\n(ML)-based classifiers have shown promise in detecting malicious call\ntranscripts, they remain vulnerable to adversarial manipulations that preserve\nsemantic content. In this study, we explore a novel attack vector where large\nlanguage models (LLMs) are leveraged to generate adversarial vishing\ntranscripts that evade detection while maintaining deceptive intent. We\nconstruct a systematic attack pipeline that employs prompt engineering and\nsemantic obfuscation to transform real-world vishing scripts using four\ncommercial LLMs. The generated transcripts are evaluated against multiple ML\nclassifiers trained on a real-world Korean vishing dataset (KorCCViD) with\nstatistical testing. Our experiments reveal that LLM-generated transcripts are\nboth practically and statistically effective against ML-based classifiers. In\nparticular, transcripts crafted by GPT-4o significantly reduce classifier\naccuracy (by up to 30.96%) while maintaining high semantic similarity, as\nmeasured by BERTScore. Moreover, these attacks are both time-efficient and\ncost-effective, with average generation times under 9 seconds and negligible\nfinancial cost per query. The results underscore the pressing need for more\nresilient vishing detection frameworks and highlight the imperative for LLM\nproviders to enforce stronger safeguards against prompt misuse in adversarial\nsocial engineering contexts."
    },
    {
        "date": "2025-07",
        "title": "Understanding Generalization, Robustness, and Interpretability in Low-Capacity Neural Networks",
        "author": "Yash Kumar",
        "link": "http://arxiv.org/abs/2507.16278v1",
        "abstract": "Although modern deep learning often relies on massive over-parameterized\nmodels, the fundamental interplay between capacity, sparsity, and robustness in\nlow-capacity networks remains a vital area of study. We introduce a controlled\nframework to investigate these properties by creating a suite of binary\nclassification tasks from the MNIST dataset with increasing visual difficulty\n(e.g., 0 and 1 vs. 4 and 9). Our experiments reveal three core findings. First,\nthe minimum model capacity required for successful generalization scales\ndirectly with task complexity. Second, these trained networks are robust to\nextreme magnitude pruning (up to 95% sparsity), revealing the existence of\nsparse, high-performing subnetworks. Third, we show that over-parameterization\nprovides a significant advantage in robustness against input corruption.\nInterpretability analysis via saliency maps further confirms that these\nidentified sparse subnetworks preserve the core reasoning process of the\noriginal dense models. This work provides a clear, empirical demonstration of\nthe foundational trade-offs governing simple neural networks."
    },
    {
        "date": "2025-07",
        "title": "Building a robust OAuth token based API Security: A High level Overview",
        "author": "Senthilkumar Gopal",
        "link": "http://arxiv.org/abs/2507.16870v1",
        "abstract": "APIs (Application Programming Interfaces) or Web Services are the\nfoundational building blocks that enable interconnected systems. However this\nproliferation of APIs has also introduced security challenges that require\nsystematic and scalable solutions for secure authentication and authorization.\nThis paper presents the fundamentals necessary for building a such a\ntoken-based API security system. It discusses the components necessary, the\nintegration of OAuth 2.0, extensibility of the token architectures, necessary\ncryptographic foundations, and persistence strategies to ensure secure and\nresilient operations. In addition to architectural concerns, the paper explores\nbest practices for token lifecycle management, scope definition, expiration\npolicies, and revocation mechanisms, all framed within a real-world scenario.\nBy adhering to these principles, developers can establish a robust baseline\nwhile maintaining the flexibility to customize their domain-specific\nrequirements. The approach does not claim to cover all variations necessary for\ndiverse architectures but instead focuses on key principles essential for any\nstandard API token authentication system. Throughout, the paper emphasizes\nbalancing practical considerations with security imperatives and uses key\nconcepts such as the CIA triad, OAuth standards, secure token life cycle, and\npractices for protecting sensitive user and application data. The intent is to\nequip developers with the foundational knowledge necessary to build secure,\nscalable token-based API security systems ready to handle the evolving threat\nlandscape."
    },
    {
        "date": "2025-07",
        "title": "Quality Text, Robust Vision: The Role of Language in Enhancing Visual Robustness of Vision-Language Models",
        "author": "Futa Waseda, Saku Sugawara, and Isao Echizen",
        "link": "http://arxiv.org/abs/2507.16257v1",
        "abstract": "Defending pre-trained vision-language models (VLMs), such as CLIP, against\nadversarial attacks is crucial, as these models are widely used in diverse\nzero-shot tasks, including image classification. However, existing adversarial\ntraining (AT) methods for robust fine-tuning largely overlook the role of\nlanguage in enhancing visual robustness. Specifically, (1) supervised AT\nmethods rely on short texts (e.g., class labels) to generate adversarial\nperturbations, leading to overfitting to object classes in the training data,\nand (2) unsupervised AT avoids this overfitting but remains suboptimal against\npractical text-guided adversarial attacks due to its lack of semantic guidance.\nTo address these limitations, we propose Quality Text-guided Adversarial\nFine-Tuning (QT-AFT), which leverages high-quality captions during training to\nguide adversarial examples away from diverse semantics present in images. This\nenables the visual encoder to robustly recognize a broader range of image\nfeatures even under adversarial noise, thereby enhancing robustness across\ndiverse downstream tasks. QT-AFT overcomes the key weaknesses of prior methods\n-- overfitting in supervised AT and lack of semantic awareness in unsupervised\nAT -- achieving state-of-the-art zero-shot adversarial robustness and clean\naccuracy, evaluated across 16 zero-shot datasets. Furthermore, our\ncomprehensive study uncovers several key insights into the role of language in\nenhancing vision robustness; for example, describing object properties in\naddition to object names further enhances zero-shot robustness. Our findings\npoint to an urgent direction for future work -- centering high-quality\nlinguistic supervision in robust visual representation learning."
    },
    {
        "date": "2025-07",
        "title": "Toward a Lightweight and Robust Design for Caching",
        "author": "Peng Chen, Hailiang Zhao, Jiaji Zhang, Xueyan Tang, Yixuan Wang, and Shuiguang Deng",
        "link": "http://arxiv.org/abs/2507.16242v2",
        "abstract": "The online caching problem aims to minimize cache misses when serving a\nsequence of requests under a limited cache size. While naive learning-augmented\ncaching algorithms achieve ideal $1$-consistency, they lack robustness\nguarantees. Existing robustification methods either sacrifice $1$-consistency\nor introduce significant computational overhead. In this paper, we introduce\nGuard, a lightweight robustification framework that enhances the robustness of\na broad class of learning-augmented caching algorithms to $2H_k + 2$, while\npreserving their $1$-consistency. Guard achieves the current best-known\ntrade-off between consistency and robustness, with only $O(1)$ additional\nper-request overhead, thereby maintaining the original time complexity of the\nbase algorithm. Extensive experiments across multiple real-world datasets and\nprediction models validate the effectiveness of Guard in practice."
    },
    {
        "date": "2025-07",
        "title": "SVAgent: AI Agent for Hardware Security Verification Assertion",
        "author": "Rui Guo, Avinash Ayalasomayajula, Henian Li, Jingbo Zhou, Sujan Kumar Saha, and Farimah Farahmandi",
        "link": "http://arxiv.org/abs/2507.16203v1",
        "abstract": "Verification using SystemVerilog assertions (SVA) is one of the most popular\nmethods for detecting circuit design vulnerabilities. However, with the\nglobalization of integrated circuit design and the continuous upgrading of\nsecurity requirements, the SVA development model has exposed major limitations.\nIt is not only inefficient in development, but also unable to effectively deal\nwith the increasing number of security vulnerabilities in modern complex\nintegrated circuits. In response to these challenges, this paper proposes an\ninnovative SVA automatic generation framework SVAgent. SVAgent introduces a\nrequirement decomposition mechanism to transform the original complex\nrequirements into a structured, gradually solvable fine-grained problem-solving\nchain. Experiments have shown that SVAgent can effectively suppress the\ninfluence of hallucinations and random answers, and the key evaluation\nindicators such as the accuracy and consistency of the SVA are significantly\nbetter than existing frameworks. More importantly, we successfully integrated\nSVAgent into the most mainstream integrated circuit vulnerability assessment\nframework and verified its practicality and reliability in a real engineering\ndesign environment."
    },
    {
        "date": "2025-07",
        "title": "Pulse-Level Simulation of Crosstalk Attacks on Superconducting Quantum Hardware",
        "author": "Syed Emad Uddin Shubha, and Tasnuva Farheen",
        "link": "http://arxiv.org/abs/2507.16181v2",
        "abstract": "Hardware crosstalk in multi-tenant superconducting quantum computers poses a\nsevere security threat, allowing adversaries to induce targeted errors across\ntenant boundaries by injecting carefully engineered pulses. We present a\nsimulation-based study of active crosstalk attacks at the pulse level,\nanalyzing how adversarial control of pulse timing, shape, amplitude, and\ncoupling can disrupt a victim's computation. Our framework models the\ntime-dependent dynamics of a three-qubit system in the rotating frame,\ncapturing both always-on couplings and injected drive pulses. We examine two\nattack strategies: attacker-first (pulse before victim operation) and\nvictim-first (pulse after), and systematically identify the pulse and coupling\nconfigurations that cause the largest logical errors. Protocol-level\nexperiments on quantum coin flip and XOR classification circuits show that some\nprotocols are highly vulnerable to these attacks, while others remain robust.\nBased on these findings, we discuss practical methods for detection and\nmitigation to improve security in quantum cloud platforms."
    },
    {
        "date": "2025-07",
        "title": "Attacking interpretable NLP systems",
        "author": "Eldor Abdukhamidov, Tamer Abuhmed, Joanna C. S. Santos, and Mohammed Abuhamad",
        "link": "http://arxiv.org/abs/2507.16164v1",
        "abstract": "Studies have shown that machine learning systems are vulnerable to\nadversarial examples in theory and practice. Where previous attacks have\nfocused mainly on visual models that exploit the difference between human and\nmachine perception, text-based models have also fallen victim to these attacks.\nHowever, these attacks often fail to maintain the semantic meaning of the text\nand similarity. This paper introduces AdvChar, a black-box attack on\nInterpretable Natural Language Processing Systems, designed to mislead the\nclassifier while keeping the interpretation similar to benign inputs, thus\nexploiting trust in system transparency. AdvChar achieves this by making less\nnoticeable modifications to text input, forcing the deep learning classifier to\nmake incorrect predictions and preserve the original interpretation. We use an\ninterpretation-focused scoring approach to determine the most critical tokens\nthat, when changed, can cause the classifier to misclassify the input. We apply\nsimple character-level modifications to measure the importance of tokens,\nminimizing the difference between the original and new text while generating\nadversarial interpretations similar to benign ones. We thoroughly evaluated\nAdvChar by testing it against seven NLP models and three interpretation models\nusing benchmark datasets for the classification task. Our experiments show that\nAdvChar can significantly reduce the prediction accuracy of current deep\nlearning models by altering just two characters on average in input samples."
    },
    {
        "date": "2025-07",
        "title": "DP2Guard: A Lightweight and Byzantine-Robust Privacy-Preserving Federated Learning Scheme for Industrial IoT",
        "author": "Baofu Han, Bing Li, Yining Qi, Raja Jurdak, Kaibin Huang, and Chau Yuen",
        "link": "http://arxiv.org/abs/2507.16134v1",
        "abstract": "Privacy-Preserving Federated Learning (PPFL) has emerged as a secure\ndistributed Machine Learning (ML) paradigm that aggregates locally trained\ngradients without exposing raw data. To defend against model poisoning threats,\nseveral robustness-enhanced PPFL schemes have been proposed by integrating\nanomaly detection. Nevertheless, they still face two major challenges: (1) the\nreliance on heavyweight encryption techniques results in substantial\ncommunication and computation overhead; and (2) single-strategy defense\nmechanisms often fail to provide sufficient robustness against adaptive\nadversaries. To overcome these challenges, we propose DP2Guard, a lightweight\nPPFL framework that enhances both privacy and robustness. DP2Guard leverages a\nlightweight gradient masking mechanism to replace costly cryptographic\noperations while ensuring the privacy of local gradients. A hybrid defense\nstrategy is proposed, which extracts gradient features using singular value\ndecomposition and cosine similarity, and applies a clustering algorithm to\neffectively identify malicious gradients. Additionally, DP2Guard adopts a trust\nscore-based adaptive aggregation scheme that adjusts client weights according\nto historical behavior, while blockchain records aggregated results and trust\nscores to ensure tamper-proof and auditable training. Extensive experiments\nconducted on two public datasets demonstrate that DP2Guard effectively defends\nagainst four advanced poisoning attacks while ensuring privacy with reduced\ncommunication and computation costs."
    },
    {
        "date": "2025-07",
        "title": "Disrupting Semantic and Abstract Features for Better Adversarial Transferability",
        "author": "Yuyang Luo, Xiaosen Wang, Zhijin Ge, and Yingzhe He",
        "link": "http://arxiv.org/abs/2507.16052v1",
        "abstract": "Adversarial examples pose significant threats to deep neural networks (DNNs),\nand their property of transferability in the black-box setting has led to the\nemergence of transfer-based attacks, making it feasible to target real-world\napplications employing DNNs. Among them, feature-level attacks, where\nintermediate features are perturbed based on feature importance weight matrix\ncomputed from transformed images, have gained popularity. In this work, we find\nthat existing feature-level attacks primarily manipulate the semantic\ninformation to derive the weight matrix. Inspired by several works that find\nCNNs tend to focus more on high-frequency components (a.k.a. abstract features,\ne.g., texture, edge, etc.), we validate that transforming images in the\nhigh-frequency space also improves transferability. Based on this finding, we\npropose a balanced approach called Semantic and Abstract FEatures disRuption\n(SAFER). Specifically, SAFER conducts BLOCKMIX on the input image and SELF-MIX\non the frequency spectrum when computing the weight matrix to highlight crucial\nfeatures. By using such a weight matrix, we can direct the attacker to disrupt\nboth semantic and abstract features, leading to improved transferability.\nExtensive experiments on the ImageNet dataset also demonstrate the\neffectiveness of our method in boosting adversarial transferability."
    },
    {
        "date": "2025-07",
        "title": "Look Before You Fuse: 2D-Guided Cross-Modal Alignment for Robust 3D Detection",
        "author": "Xiang Li",
        "link": "http://arxiv.org/abs/2507.16861v1",
        "abstract": "Integrating LiDAR and camera inputs into a unified Bird's-Eye-View (BEV)\nrepresentation is crucial for enhancing 3D perception capabilities of\nautonomous vehicles. However, current methods are often affected by\nmisalignment between camera and LiDAR features. This misalignment leads to\ninaccurate depth supervision in camera branch and erroneous fusion during\ncross-modal feature aggregation. The root cause of this misalignment lies in\nprojection errors, stemming from minor extrinsic calibration inaccuracies and\nrolling shutter effect of LiDAR during vehicle motion. In this work, our key\ninsight is that these projection errors are predominantly concentrated at\nobject-background boundaries, which are readily identified by 2D detectors.\nBased on this, our main motivation is to utilize 2D object priors to pre-align\ncross-modal features before fusion. To address local misalignment, we propose\nPrior Guided Depth Calibration (PGDC), which leverages 2D priors to correct\nlocal misalignment and preserve correct cross-modal feature pairs. To resolve\nglobal misalignment, we introduce Discontinuity Aware Geometric Fusion (DAGF)\nto process calibrated results from PGDC, suppressing noise and explicitly\nenhancing sharp transitions at object-background boundaries. To effectively\nutilize these transition-aware depth representations, we incorporate Structural\nGuidance Depth Modulator (SGDM), using a gated attention mechanism to\nefficiently fuse aligned depth and image features. Our proposed method achieves\nstate-of-the-art performance on nuScenes validation dataset, with its mAP and\nNDS reaching 71.5% and 73.6% respectively."
    },
    {
        "date": "2025-07",
        "title": "Does More Inference-Time Compute Really Help Robustness?",
        "author": "Tong Wu, Chong Xiang, Jiachen T. Wang, Weichen Yu, Chawin Sitawarin, Vikash Sehwag, and Prateek Mittal",
        "link": "http://arxiv.org/abs/2507.15974v1",
        "abstract": "Recently, Zaremba et al. demonstrated that increasing inference-time\ncomputation improves robustness in large proprietary reasoning LLMs. In this\npaper, we first show that smaller-scale, open-source models (e.g., DeepSeek R1,\nQwen3, Phi-reasoning) can also benefit from inference-time scaling using a\nsimple budget forcing strategy. More importantly, we reveal and critically\nexamine an implicit assumption in prior work: intermediate reasoning steps are\nhidden from adversaries. By relaxing this assumption, we identify an important\nsecurity risk, intuitively motivated and empirically verified as an inverse\nscaling law: if intermediate reasoning steps become explicitly accessible,\nincreased inference-time computation consistently reduces model robustness.\nFinally, we discuss practical scenarios where models with hidden reasoning\nchains are still vulnerable to attacks, such as models with tool-integrated\nreasoning and advanced reasoning extraction attacks. Our findings collectively\ndemonstrate that the robustness benefits of inference-time scaling depend\nheavily on the adversarial setting and deployment context. We urge\npractitioners to carefully weigh these subtle trade-offs before applying\ninference-time scaling in security-sensitive, real-world applications."
    },
    {
        "date": "2025-07",
        "title": "Look, Focus, Act: Efficient and Robust Robot Learning via Human Gaze and Foveated Vision Transformers",
        "author": "Ian Chuang, Andrew Lee, Dechen Gao, Jinyu Zou, and Iman Soltani",
        "link": "http://arxiv.org/abs/2507.15833v1",
        "abstract": "Human vision is a highly active process driven by gaze, which directs\nattention and fixation to task-relevant regions and dramatically reduces visual\nprocessing. In contrast, robot learning systems typically rely on passive,\nuniform processing of raw camera images. In this work, we explore how\nincorporating human-like active gaze into robotic policies can enhance both\nefficiency and performance. We build on recent advances in foveated image\nprocessing and apply them to an Active Vision robot system that emulates both\nhuman head movement and eye tracking. Extending prior work on the AV-ALOHA\nrobot simulation platform, we introduce a framework for simultaneously\ncollecting eye-tracking data and robot demonstrations from a human operator as\nwell as a simulation benchmark and dataset for training robot policies that\nincorporate human gaze. Given the widespread use of Vision Transformers (ViTs)\nin robot learning, we integrate gaze information into ViTs using a foveated\npatch tokenization scheme inspired by recent work in image segmentation.\nCompared to uniform patch tokenization, this significantly reduces the number\nof tokens-and thus computation-without sacrificing visual fidelity near regions\nof interest. We also explore two approaches to gaze imitation and prediction\nfrom human data. The first is a two-stage model that predicts gaze to guide\nfoveation and action; the second integrates gaze into the action space,\nallowing the policy to jointly predict gaze and actions end-to-end. Our results\nshow that our method for foveated robot vision not only drastically reduces\ncomputational overhead, but also improves performance for high precision tasks\nand robustness to unseen distractors. Together, these findings suggest that\nhuman-inspired visual processing offers a useful inductive bias for robotic\nvision systems. https://ian-chuang.github.io/gaze-av-aloha/"
    },
    {
        "date": "2025-07",
        "title": "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization",
        "author": "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang",
        "link": "http://arxiv.org/abs/2507.15765v2",
        "abstract": "Dynamic Facial Expression Recognition (DFER) plays a critical role in\naffective computing and human-computer interaction. Although existing methods\nachieve comparable performance, they inevitably suffer from performance\ndegradation under sample heterogeneity caused by multi-source data and\nindividual expression variability. To address these challenges, we propose a\nnovel framework, called Heterogeneity-aware Distributional Framework (HDF), and\ndesign two plug-and-play modules to enhance time-frequency modeling and\nmitigate optimization imbalance caused by hard samples. Specifically, the\nTime-Frequency Distributional Attention Module (DAM) captures both temporal\nconsistency and frequency robustness through a dual-branch attention design,\nimproving tolerance to sequence inconsistency and visual style shifts. Then,\nbased on gradient sensitivity and information bottleneck principles, an\nadaptive optimization module Distribution-aware Scaling Module (DSM) is\nintroduced to dynamically balance classification and contrastive losses,\nenabling more stable and discriminative representation learning. Extensive\nexperiments on two widely used datasets, DFEW and FERV39k, demonstrate that HDF\nsignificantly improves both recognition accuracy and robustness. Our method\nachieves superior weighted average recall (WAR) and unweighted average recall\n(UAR) while maintaining strong generalization across diverse and imbalanced\nscenarios. Codes are released at https://github.com/QIcita/HDF_DFER."
    },
    {
        "date": "2025-07",
        "title": "Missing value imputation with adversarial random forests -- MissARF",
        "author": "Pegah Golchian, Jan Kapar, David S. Watson, and Marvin N. Wright",
        "link": "http://arxiv.org/abs/2507.15681v1",
        "abstract": "Handling missing values is a common challenge in biostatistical analyses,\ntypically addressed by imputation methods. We propose a novel, fast, and\neasy-to-use imputation method called missing value imputation with adversarial\nrandom forests (MissARF), based on generative machine learning, that provides\nboth single and multiple imputation. MissARF employs adversarial random forest\n(ARF) for density estimation and data synthesis. To impute a missing value of\nan observation, we condition on the non-missing values and sample from the\nestimated conditional distribution generated by ARF. Our experiments\ndemonstrate that MissARF performs comparably to state-of-the-art single and\nmultiple imputation methods in terms of imputation quality and fast runtime\nwith no additional costs for multiple imputation."
    },
    {
        "date": "2025-07",
        "title": "Cyber security of Mega Events: A Case Study of Securing the Digital Infrastructure for MahaKumbh 2025 -- A 45 days Mega Event of 600 Million Footfalls",
        "author": "Rohit Negi, Amit Negi, Manish Sharma, S. Venkatesan, Prem Kumar, and Sandeep K. Shukla",
        "link": "http://arxiv.org/abs/2507.15660v1",
        "abstract": "Mega events such as the Olympics, World Cup tournaments, G-20 Summit,\nreligious events such as MahaKumbh are increasingly digitalized. From event\nticketing, vendor booth or lodging reservations, sanitation, event scheduling,\ncustomer service, crime reporting, media streaming and messaging on digital\ndisplay boards, surveillance, crowd control, traffic control and many other\nservices are based on mobile and web applications, wired and wireless\nnetworking, network of Closed-Circuit Television (CCTV) cameras, specialized\ncontrol room with network and video-feed monitoring. Consequently, cyber\nthreats directed at such digital infrastructure are common. Starting from hobby\nhackers, hacktivists, cyber crime gangs, to the nation state actors, all target\nsuch infrastructure to unleash chaos on an otherwise smooth operation, and\noften the cyber threat actors attempt to embarrass the organizing country or\nthe organizers. Unlike long-standing organizations such as a corporate or a\ngovernment department, the infrastructure of mega-events is temporary,\nconstructed over a short time span in expediency, and often shortcuts are taken\nto make the deadline for the event. As a result, securing such an elaborate yet\ntemporary infrastructure requires a different approach than securing a standard\norganizational digital infrastructure. In this paper, we describe our approach\nto securing MahaKumbh 2025, a 600 million footfall event for 45 days in\nPrayagraj, India, as a cyber security assessment and risk management oversight\nteam. We chronicle the scope, process, methodology, and outcome of our team's\neffort to secure this mega event. It should be noted that none of the cyber\nattacks during the 45-day event was successful. Our goal is to put on record\nthe methodology and discuss what we would do differently in case we work on\nsimilar future mega event."
    },
    {
        "date": "2025-07",
        "title": "Multi-Stage Prompt Inference Attacks on Enterprise LLM Systems",
        "author": "Andrii Balashov, Olena Ponomarova, and Xiaohua Zhai",
        "link": "http://arxiv.org/abs/2507.15613v1",
        "abstract": "Large Language Models (LLMs) deployed in enterprise settings (e.g., as\nMicrosoft 365 Copilot) face novel security challenges. One critical threat is\nprompt inference attacks: adversaries chain together seemingly benign prompts\nto gradually extract confidential data. In this paper, we present a\ncomprehensive study of multi-stage prompt inference attacks in an enterprise\nLLM context. We simulate realistic attack scenarios where an attacker uses\nmild-mannered queries and indirect prompt injections to exploit an LLM\nintegrated with private corporate data. We develop a formal threat model for\nthese multi-turn inference attacks and analyze them using probability theory,\noptimization frameworks, and information-theoretic leakage bounds. The attacks\nare shown to reliably exfiltrate sensitive information from the LLM's context\n(e.g., internal SharePoint documents or emails), even when standard safety\nmeasures are in place.\n  We propose and evaluate defenses to counter such attacks, including\nstatistical anomaly detection, fine-grained access control, prompt sanitization\ntechniques, and architectural modifications to LLM deployment. Each defense is\nsupported by mathematical analysis or experimental simulation. For example, we\nderive bounds on information leakage under differential privacy-based training\nand demonstrate an anomaly detection method that flags multi-turn attacks with\nhigh AUC. We also introduce an approach called \"spotlighting\" that uses input\ntransformations to isolate untrusted prompt content, reducing attack success by\nan order of magnitude. Finally, we provide a formal proof of concept and\nempirical validation for a combined defense-in-depth strategy. Our work\nhighlights that securing LLMs in enterprise settings requires moving beyond\nsingle-turn prompt filtering toward a holistic, multi-stage perspective on both\nattacks and defenses."
    },
    {
        "date": "2025-07",
        "title": "Coarse-to-fine crack cue for robust crack detection",
        "author": "Zelong Liu, Yuliang Gu, Zhichao Sun, Huachao Zhu, Xin Xiao, Bo Du, Laurent Najman, and Yongchao Xu",
        "link": "http://arxiv.org/abs/2507.16851v1",
        "abstract": "Crack detection is an important task in computer vision. Despite impressive\nin-dataset performance, deep learning-based methods still struggle in\ngeneralizing to unseen domains. The thin structure property of cracks is\nusually overlooked by previous methods. In this work, we introduce CrackCue, a\nnovel method for robust crack detection based on coarse-to-fine crack cue\ngeneration. The core concept lies on leveraging the thin structure property to\ngenerate a robust crack cue, guiding the crack detection. Specifically, we\nfirst employ a simple max-pooling and upsampling operation on the crack image.\nThis results in a coarse crack-free background, based on which a fine\ncrack-free background can be obtained via a reconstruction network. The\ndifference between the original image and fine crack-free background provides a\nfine crack cue. This fine cue embeds robust crack prior information which is\nunaffected by complex backgrounds, shadow, and varied lighting. As a\nplug-and-play method, we incorporate the proposed CrackCue into three advanced\ncrack detection networks. Extensive experimental results demonstrate that the\nproposed CrackCue significantly improves the generalization ability and\nrobustness of the baseline methods. The source code will be publicly available."
    },
    {
        "date": "2025-07",
        "title": "EndoControlMag: Robust Endoscopic Vascular Motion Magnification with Periodic Reference Resetting and Hierarchical Tissue-aware Dual-Mask Control",
        "author": "An Wang, Rulin Zhou, Mengya Xu, Yiru Ye, Longfei Gou, Yiting Chang, Hao Chen, Chwee Ming Lim, Jiankun Wang, and Hongliang Ren",
        "link": "http://arxiv.org/abs/2507.15292v4",
        "abstract": "Visualizing subtle vascular motions in endoscopic surgery is crucial for\nsurgical precision and decision-making, yet remains challenging due to the\ncomplex and dynamic nature of surgical scenes. To address this, we introduce\nEndoControlMag, a training-free, Lagrangian-based framework with\nmask-conditioned vascular motion magnification tailored to endoscopic\nenvironments. Our approach features two key modules: a Periodic Reference\nResetting (PRR) scheme that divides videos into short overlapping clips with\ndynamically updated reference frames to prevent error accumulation while\nmaintaining temporal coherence, and a Hierarchical Tissue-aware Magnification\n(HTM) framework with dual-mode mask dilation. HTM first tracks vessel cores\nusing a pretrained visual tracking model to maintain accurate localization\ndespite occlusions and view changes. It then applies one of two adaptive\nsoftening strategies to surrounding tissues: motion-based softening that\nmodulates magnification strength proportional to observed tissue displacement,\nor distance-based exponential decay that simulates biomechanical force\nattenuation. This dual-mode approach accommodates diverse surgical\nscenarios-motion-based softening excels with complex tissue deformations while\ndistance-based softening provides stability during unreliable optical flow\nconditions. We evaluate EndoControlMag on our EndoVMM24 dataset spanning four\ndifferent surgery types and various challenging scenarios, including\nocclusions, instrument disturbance, view changes, and vessel deformations.\nQuantitative metrics, visual assessments, and expert surgeon evaluations\ndemonstrate that EndoControlMag significantly outperforms existing methods in\nboth magnification accuracy and visual quality while maintaining robustness\nacross challenging surgical conditions. The code, dataset, and video results\nare available at https://szupc.github.io/EndoControlMag/."
    },
    {
        "date": "2025-07",
        "title": "In-context Learning of Vision Language Models for Detection of Physical and Digital Attacks against Face Recognition Systems",
        "author": "Lazaro Janier Gonzalez-Soler, Maciej Salwowski, and Christoph Busch",
        "link": "http://arxiv.org/abs/2507.15285v1",
        "abstract": "Recent advances in biometric systems have significantly improved the\ndetection and prevention of fraudulent activities. However, as detection\nmethods improve, attack techniques become increasingly sophisticated. Attacks\non face recognition systems can be broadly divided into physical and digital\napproaches. Traditionally, deep learning models have been the primary defence\nagainst such attacks. While these models perform exceptionally well in\nscenarios for which they have been trained, they often struggle to adapt to\ndifferent types of attacks or varying environmental conditions. These\nsubsystems require substantial amounts of training data to achieve reliable\nperformance, yet biometric data collection faces significant challenges,\nincluding privacy concerns and the logistical difficulties of capturing diverse\nattack scenarios under controlled conditions. This work investigates the\napplication of Vision Language Models (VLM) and proposes an in-context learning\nframework for detecting physical presentation attacks and digital morphing\nattacks in biometric systems. Focusing on open-source models, the first\nsystematic framework for the quantitative evaluation of VLMs in\nsecurity-critical scenarios through in-context learning techniques is\nestablished. The experimental evaluation conducted on freely available\ndatabases demonstrates that the proposed subsystem achieves competitive\nperformance for physical and digital attack detection, outperforming some of\nthe traditional CNNs without resource-intensive training. The experimental\nresults validate the proposed framework as a promising tool for improving\ngeneralisation in attack detection."
    },
    {
        "date": "2025-07",
        "title": "Robust and Differentially Private PCA for non-Gaussian data",
        "author": "Minwoo Kim, and Sungkyu Jung",
        "link": "http://arxiv.org/abs/2507.15232v1",
        "abstract": "Recent advances have sparked significant interest in the development of\nprivacy-preserving Principal Component Analysis (PCA). However, many existing\napproaches rely on restrictive assumptions, such as assuming sub-Gaussian data\nor being vulnerable to data contamination. Additionally, some methods are\ncomputationally expensive or depend on unknown model parameters that must be\nestimated, limiting their accessibility for data analysts seeking\nprivacy-preserving PCA. In this paper, we propose a differentially private PCA\nmethod applicable to heavy-tailed and potentially contaminated data. Our\napproach leverages the property that the covariance matrix of properly rescaled\ndata preserves eigenvectors and their order under elliptical distributions,\nwhich include Gaussian and heavy-tailed distributions. By applying a bounded\ntransformation, we enable straightforward computation of principal components\nin a differentially private manner. Additionally, boundedness guarantees\nrobustness against data contamination. We conduct both theoretical analysis and\nempirical evaluations of the proposed method, focusing on its ability to\nrecover the subspace spanned by the leading principal components. Extensive\nnumerical experiments demonstrate that our method consistently outperforms\nexisting approaches in terms of statistical utility, particularly in\nnon-Gaussian or contaminated data settings."
    },
    {
        "date": "2025-07",
        "title": "PromptArmor: Simple yet Effective Prompt Injection Defenses",
        "author": "Tianneng Shi, Kaijie Zhu, Zhun Wang, Yuqi Jia, Will Cai, Weida Liang, Haonan Wang, Hend Alzahrani, Joshua Lu, Kenji Kawaguchi, Basel Alomair, Xuandong Zhao, William Yang Wang, Neil Gong, Wenbo Guo, and Dawn Song",
        "link": "http://arxiv.org/abs/2507.15219v1",
        "abstract": "Despite their potential, recent research has demonstrated that LLM agents are\nvulnerable to prompt injection attacks, where malicious prompts are injected\ninto the agent's input, causing it to perform an attacker-specified task rather\nthan the intended task provided by the user. In this paper, we present\nPromptArmor, a simple yet effective defense against prompt injection attacks.\nSpecifically, PromptArmor prompts an off-the-shelf LLM to detect and remove\npotential injected prompts from the input before the agent processes it. Our\nresults show that PromptArmor can accurately identify and remove injected\nprompts. For example, using GPT-4o, GPT-4.1, or o4-mini, PromptArmor achieves\nboth a false positive rate and a false negative rate below 1% on the AgentDojo\nbenchmark. Moreover, after removing injected prompts with PromptArmor, the\nattack success rate drops to below 1%. We also demonstrate PromptArmor's\neffectiveness against adaptive attacks and explore different strategies for\nprompting an LLM. We recommend that PromptArmor be adopted as a standard\nbaseline for evaluating new defenses against prompt injection attacks."
    },
    {
        "date": "2025-07",
        "title": "Exploiting Context-dependent Duration Features for Voice Anonymization Attack Systems",
        "author": "Natalia Tomashenko, Emmanuel Vincent, and Marc Tommasi",
        "link": "http://arxiv.org/abs/2507.15214v1",
        "abstract": "The temporal dynamics of speech, encompassing variations in rhythm,\nintonation, and speaking rate, contain important and unique information about\nspeaker identity. This paper proposes a new method for representing speaker\ncharacteristics by extracting context-dependent duration embeddings from speech\ntemporal dynamics. We develop novel attack models using these representations\nand analyze the potential vulnerabilities in speaker verification and voice\nanonymization systems.The experimental results show that the developed attack\nmodels provide a significant improvement in speaker verification performance\nfor both original and anonymized data in comparison with simpler\nrepresentations of speech temporal dynamics reported in the literature."
    },
    {
        "date": "2025-07",
        "title": "Adaptive Network Security Policies via Belief Aggregation and Rollout",
        "author": "Kim Hammar, Yuchao Li, Tansu Alpcan, Emil C. Lupu, and Dimitri Bertsekas",
        "link": "http://arxiv.org/abs/2507.15163v1",
        "abstract": "Evolving security vulnerabilities and shifting operational conditions require\nfrequent updates to network security policies. These updates include\nadjustments to incident response procedures and modifications to access\ncontrols, among others. Reinforcement learning methods have been proposed for\nautomating such policy adaptations, but most of the methods in the research\nliterature lack performance guarantees and adapt slowly to changes. In this\npaper, we address these limitations and present a method for computing security\npolicies that is scalable, offers theoretical guarantees, and adapts quickly to\nchanges. It assumes a model or simulator of the system and comprises three\ncomponents: belief estimation through particle filtering, offline policy\ncomputation through aggregation, and online policy adaptation through rollout.\nCentral to our method is a new feature-based aggregation technique, which\nimproves scalability and flexibility. We analyze the approximation error of\naggregation and show that rollout efficiently adapts policies to changes under\ncertain conditions. Simulations and testbed results demonstrate that our method\noutperforms state-of-the-art methods on several benchmarks, including CAGE-2."
    },
    {
        "date": "2025-07",
        "title": "Quantum Machine Learning for Secure Cooperative Multi-Layer Edge AI with Proportional Fairness",
        "author": "Thai T. Vu, and John Le",
        "link": "http://arxiv.org/abs/2507.15145v1",
        "abstract": "This paper proposes a communication-efficient, event-triggered inference\nframework for cooperative edge AI systems comprising multiple user devices and\nedge servers. Building upon dual-threshold early-exit strategies for rare-event\ndetection, the proposed approach extends classical single-device inference to a\ndistributed, multi-device setting while incorporating proportional fairness\nconstraints across users. A joint optimization framework is formulated to\nmaximize classification utility under communication, energy, and fairness\nconstraints. To solve the resulting problem efficiently, we exploit the\nmonotonicity of the utility function with respect to the confidence thresholds\nand apply alternating optimization with Benders decomposition. Experimental\nresults show that the proposed framework significantly enhances system-wide\nperformance and fairness in resource allocation compared to single-device\nbaselines."
    },
    {
        "date": "2025-07",
        "title": "From Kicking to Causality: Simulating Infant Agency Detection with a Robust Intrinsic Reward",
        "author": "Xia Xu, and Jochen Triesch",
        "link": "http://arxiv.org/abs/2507.15106v1",
        "abstract": "While human infants robustly discover their own causal efficacy, standard\nreinforcement learning agents remain brittle, as their reliance on\ncorrelation-based rewards fails in noisy, ecologically valid scenarios. To\naddress this, we introduce the Causal Action Influence Score (CAIS), a novel\nintrinsic reward rooted in causal inference. CAIS quantifies an action's\ninfluence by measuring the 1-Wasserstein distance between the learned\ndistribution of sensory outcomes conditional on that action, $p(h|a)$, and the\nbaseline outcome distribution, $p(h)$. This divergence provides a robust reward\nthat isolates the agent's causal impact from confounding environmental noise.\nWe test our approach in a simulated infant-mobile environment where\ncorrelation-based perceptual rewards fail completely when the mobile is\nsubjected to external forces. In stark contrast, CAIS enables the agent to\nfilter this noise, identify its influence, and learn the correct policy.\nFurthermore, the high-quality predictive model learned for CAIS allows our\nagent, when augmented with a surprise signal, to successfully reproduce the\n\"extinction burst\" phenomenon. We conclude that explicitly inferring causality\nis a crucial mechanism for developing a robust sense of agency, offering a\npsychologically plausible framework for more adaptive autonomous systems."
    },
    {
        "date": "2025-07",
        "title": "Robust Control with Gradient Uncertainty",
        "author": "Qian Qi",
        "link": "http://arxiv.org/abs/2507.15082v1",
        "abstract": "We introduce a novel extension to robust control theory that explicitly\naddresses uncertainty in the value function's gradient, a form of uncertainty\nendemic to applications like reinforcement learning where value functions are\napproximated. We formulate a zero-sum dynamic game where an adversary perturbs\nboth system dynamics and the value function gradient, leading to a new, highly\nnonlinear partial differential equation: the Hamilton-Jacobi-Bellman-Isaacs\nEquation with Gradient Uncertainty (GU-HJBI). We establish its well-posedness\nby proving a comparison principle for its viscosity solutions under a uniform\nellipticity condition. Our analysis of the linear-quadratic (LQ) case yields a\nkey insight: we prove that the classical quadratic value function assumption\nfails for any non-zero gradient uncertainty, fundamentally altering the problem\nstructure. A formal perturbation analysis characterizes the non-polynomial\ncorrection to the value function and the resulting nonlinearity of the optimal\ncontrol law, which we validate with numerical studies. Finally, we bridge\ntheory to practice by proposing a novel Gradient-Uncertainty-Robust\nActor-Critic (GURAC) algorithm, accompanied by an empirical study demonstrating\nits effectiveness in stabilizing training. This work provides a new direction\nfor robust control, holding significant implications for fields where function\napproximation is common, including reinforcement learning and computational\nfinance."
    },
    {
        "date": "2025-07",
        "title": "ROBAD: Robust Adversary-aware Local-Global Attended Bad Actor Detection Sequential Model",
        "author": "Bing He, Mustaque Ahamad, and Srijan Kumar",
        "link": "http://arxiv.org/abs/2507.15067v1",
        "abstract": "Detecting bad actors is critical to ensure the safety and integrity of\ninternet platforms. Several deep learning-based models have been developed to\nidentify such users. These models should not only accurately detect bad actors,\nbut also be robust against adversarial attacks that aim to evade detection.\nHowever, past deep learning-based detection models do not meet the robustness\nrequirement because they are sensitive to even minor changes in the input\nsequence. To address this issue, we focus on (1) improving the model\nunderstanding capability and (2) enhancing the model knowledge such that the\nmodel can recognize potential input modifications when making predictions. To\nachieve these goals, we create a novel transformer-based classification model,\ncalled ROBAD (RObust adversary-aware local-global attended Bad Actor Detection\nmodel), which uses the sequence of user posts to generate user embedding to\ndetect bad actors. Particularly, ROBAD first leverages the transformer encoder\nblock to encode each post bidirectionally, thus building a post embedding to\ncapture the local information at the post level. Next, it adopts the\ntransformer decoder block to model the sequential pattern in the post\nembeddings by using the attention mechanism, which generates the sequence\nembedding to obtain the global information at the sequence level. Finally, to\nenrich the knowledge of the model, embeddings of modified sequences by mimicked\nattackers are fed into a contrastive-learning-enhanced classification layer for\nsequence prediction. In essence, by capturing the local and global information\n(i.e., the post and sequence information) and leveraging the mimicked behaviors\nof bad actors in training, ROBAD can be robust to adversarial attacks.\nExtensive experiments on Yelp and Wikipedia datasets show that ROBAD can\neffectively detect bad actors when under state-of-the-art adversarial attacks."
    },
    {
        "date": "2025-07",
        "title": "DeRAG: Black-box Adversarial Attacks on Multiple Retrieval-Augmented Generation Applications via Prompt Injection",
        "author": "Jerry Wang, and Fang Yu",
        "link": "http://arxiv.org/abs/2507.15042v1",
        "abstract": "Adversarial prompt attacks can significantly alter the reliability of\nRetrieval-Augmented Generation (RAG) systems by re-ranking them to produce\nincorrect outputs. In this paper, we present a novel method that applies\nDifferential Evolution (DE) to optimize adversarial prompt suffixes for\nRAG-based question answering. Our approach is gradient-free, treating the RAG\npipeline as a black box and evolving a population of candidate suffixes to\nmaximize the retrieval rank of a targeted incorrect document to be closer to\nreal world scenarios. We conducted experiments on the BEIR QA datasets to\nevaluate attack success at certain retrieval rank thresholds under multiple\nretrieving applications. Our results demonstrate that DE-based prompt\noptimization attains competitive (and in some cases higher) success rates\ncompared to GGPP to dense retrievers and PRADA to sparse retrievers, while\nusing only a small number of tokens (<=5 tokens) in the adversarial suffix.\nFurthermore, we introduce a readability-aware suffix construction strategy,\nvalidated by a statistically significant reduction in MLM negative\nlog-likelihood with Welch's t-test. Through evaluations with a BERT-based\nadversarial suffix detector, we show that DE-generated suffixes evade\ndetection, yielding near-chance detection accuracy."
    },
    {
        "date": "2025-07",
        "title": "Metaverse Security and Privacy Research: A Systematic Review",
        "author": "Argianto Rahartomo, Leonel Merino, and Mohammad Ghafari",
        "link": "http://arxiv.org/abs/2507.14985v1",
        "abstract": "The rapid growth of metaverse technologies, including virtual worlds,\naugmented reality, and lifelogging, has accelerated their adoption across\ndiverse domains. This rise exposes users to significant new security and\nprivacy challenges due to sociotechnical complexity, pervasive connectivity,\nand extensive user data collection in immersive environments. We present a\nsystematic review of the literature published between 2013 and 2024, offering a\ncomprehensive analysis of how the research community has addressed\nmetaverse-related security and privacy issues over the past decade. We organize\nthe studies by method, examined the security and privacy properties, immersive\ncomponents, and evaluation strategies. Our investigation reveals a sharp\nincrease in research activity in the last five years, a strong focus on\npractical and user-centered approaches, and a predominant use of benchmarking,\nhuman experimentation, and qualitative methods. Authentication and\nunobservability are the most frequently studied properties. However, critical\ngaps remain in areas such as policy compliance, accessibility,\ninteroperability, and back-end infrastructure security. We emphasize the\nintertwined technical complexity and human factors of the metaverse and call\nfor integrated, interdisciplinary approaches to securing inclusive and\ntrustworthy immersive environments."
    },
    {
        "date": "2025-07",
        "title": "Byzantine-Robust Decentralized Coordination of LLM Agents",
        "author": "Yongrae Jo, and Chanik Park",
        "link": "http://arxiv.org/abs/2507.14928v1",
        "abstract": "Collaboration among multiple large language model (LLM) agents is a promising\napproach to overcome inherent limitations of single-agent systems, such as\nhallucinations and single points of failure. As LLM agents are increasingly\ndeployed on open blockchain platforms, multi-agent systems capable of\ntolerating malicious (Byzantine) agents have become essential.\n  Recent Byzantine-robust multi-agent systems typically rely on leader-driven\ncoordination, which suffers from two major drawbacks. First, they are\ninherently vulnerable to targeted attacks against the leader. If consecutive\nleaders behave maliciously, the system repeatedly fails to achieve consensus,\nforcing new consensus rounds, which is particularly costly given the high\nlatency of LLM invocations. Second, an underperforming proposal from the leader\ncan be accepted as the final answer even when higher-quality alternatives are\navailable, as existing methods finalize the leader's proposal once it receives\na quorum of votes.\n  To address these issues, we propose DecentLLMs, a novel decentralized\nconsensus approach for multi-agent LLM systems, where worker agents generate\nanswers concurrently and evaluator agents independently score and rank these\nanswers to select the best available one. This decentralized architecture\nenables faster consensus despite the presence of Byzantine agents and\nconsistently selects higher-quality answers through Byzantine-robust\naggregation techniques.\n  Experimental results demonstrate that DecentLLMs effectively tolerates\nByzantine agents and significantly improves the quality of selected answers."
    },
    {
        "date": "2025-07",
        "title": "BeatFormer: Efficient motion-robust remote heart rate estimation through unsupervised spectral zoomed attention filters",
        "author": "Joaquim Comas, and Federico Sukno",
        "link": "http://arxiv.org/abs/2507.14885v1",
        "abstract": "Remote photoplethysmography (rPPG) captures cardiac signals from facial\nvideos and is gaining attention for its diverse applications. While deep\nlearning has advanced rPPG estimation, it relies on large, diverse datasets for\neffective generalization. In contrast, handcrafted methods utilize\nphysiological priors for better generalization in unseen scenarios like motion\nwhile maintaining computational efficiency. However, their linear assumptions\nlimit performance in complex conditions, where deep learning provides superior\npulsatile information extraction. This highlights the need for hybrid\napproaches that combine the strengths of both methods. To address this, we\npresent BeatFormer, a lightweight spectral attention model for rPPG estimation,\nwhich integrates zoomed orthonormal complex attention and frequency-domain\nenergy measurement, enabling a highly efficient model. Additionally, we\nintroduce Spectral Contrastive Learning (SCL), which allows BeatFormer to be\ntrained without any PPG or HR labels. We validate BeatFormer on the PURE,\nUBFC-rPPG, and MMPD datasets, demonstrating its robustness and performance,\nparticularly in cross-dataset evaluations under motion scenarios."
    },
    {
        "date": "2025-07",
        "title": "A Privacy-Centric Approach: Scalable and Secure Federated Learning Enabled by Hybrid Homomorphic Encryption",
        "author": "Khoa Nguyen, Tanveer Khan, and Antonis Michalas",
        "link": "http://arxiv.org/abs/2507.14853v1",
        "abstract": "Federated Learning (FL) enables collaborative model training without sharing\nraw data, making it a promising approach for privacy-sensitive domains. Despite\nits potential, FL faces significant challenges, particularly in terms of\ncommunication overhead and data privacy. Privacy-preserving Techniques (PPTs)\nsuch as Homomorphic Encryption (HE) have been used to mitigate these concerns.\nHowever, these techniques introduce substantial computational and communication\ncosts, limiting their practical deployment. In this work, we explore how Hybrid\nHomomorphic Encryption (HHE), a cryptographic protocol that combines symmetric\nencryption with HE, can be effectively integrated with FL to address both\ncommunication and privacy challenges, paving the way for scalable and secure\ndecentralized learning system."
    },
    {
        "date": "2025-07",
        "title": "Manipulating LLM Web Agents with Indirect Prompt Injection Attack via HTML Accessibility Tree",
        "author": "Sam Johnson, Viet Pham, and Thai Le",
        "link": "http://arxiv.org/abs/2507.14799v1",
        "abstract": "This work demonstrates that LLM-based web navigation agents offer powerful\nautomation capabilities but are vulnerable to Indirect Prompt Injection (IPI)\nattacks. We show that adversaries can embed universal adversarial triggers in\nwebpage HTML to hijack agent behavior that utilizes the accessibility tree to\nparse HTML, causing unintended or malicious actions. Using the Greedy\nCoordinate Gradient (GCG) algorithm and a Browser Gym agent powered by\nLlama-3.1, our system demonstrates high success rates across real websites in\nboth targeted and general attacks, including login credential exfiltration and\nforced ad clicks. Our empirical results highlight critical security risks and\nthe need for stronger defenses as LLM-driven autonomous web agents become more\nwidely adopted. The system software\n(https://github.com/sej2020/manipulating-web-agents) is released under the MIT\nLicense, with an accompanying publicly available demo website\n(http://lethaiq.github.io/attack-web-llm-agent)."
    },
    {
        "date": "2025-07",
        "title": "Collusion-Resilient Hierarchical Secure Aggregation with Heterogeneous Security Constraints",
        "author": "Zhou Li, Xiang Zhang, Jiawen Lv, Jihao Fan, Haiqiang Chen, and Giuseppe Caire",
        "link": "http://arxiv.org/abs/2507.14768v1",
        "abstract": "Motivated by federated learning (FL), secure aggregation (SA) aims to\nsecurely compute, as efficiently as possible, the sum of a set of inputs\ndistributed across many users. To understand the impact of network topology,\nhierarchical secure aggregation (HSA) investigated the communication and secret\nkey generation efficiency in a 3-layer relay network, where clusters of users\nare connected to the aggregation server through an intermediate layer of\nrelays. Due to the pre-aggregation of the messages at the relays, HSA reduces\nthe communication burden on the relay-to-server links and is able to support a\nlarge number of users. However, as the number of users increases, a practical\nchallenge arises from heterogeneous security requirements--for example, users\nin different clusters may require varying levels of input protection. Motivated\nby this, we study weakly-secure HSA (WS-HSA) with collusion resilience, where\ninstead of protecting all the inputs from any set of colluding users, only the\ninputs belonging to a predefined collection of user groups (referred to as\nsecurity input sets) need to be protected against another predefined collection\nof user groups (referred to as collusion sets). Since the security input sets\nand collusion sets can be arbitrarily defined, our formulation offers a\nflexible framework for addressing heterogeneous security requirements in HSA.\nWe characterize the optimal total key rate, i.e., the total number of\nindependent key symbols required to ensure both server and relay security, for\na broad range of parameter configurations. For the remaining cases, we\nestablish lower and upper bounds on the optimal key rate, providing\nconstant-factor gap optimality guarantees."
    },
    {
        "date": "2025-07",
        "title": "Analyzing Internal Activity and Robustness of SNNs Across Neuron Parameter Space",
        "author": "Szymon Mazurek, Jakub Caputa, and Maciej Wielgosz",
        "link": "http://arxiv.org/abs/2507.14757v1",
        "abstract": "Spiking Neural Networks (SNNs) offer energy-efficient and biologically\nplausible alternatives to traditional artificial neural networks, but their\nperformance depends critically on the tuning of neuron model parameters. In\nthis work, we identify and characterize an operational space - a constrained\nregion in the neuron hyperparameter domain (specifically membrane time constant\ntau and voltage threshold vth) - within which the network exhibits meaningful\nactivity and functional behavior. Operating inside this manifold yields optimal\ntrade-offs between classification accuracy and spiking activity, while stepping\noutside leads to degeneration: either excessive energy use or complete network\nsilence.\n  Through systematic exploration across datasets and architectures, we\nvisualize and quantify this manifold and identify efficient operating points.\nWe further assess robustness to adversarial noise, showing that SNNs exhibit\nincreased spike correlation and internal synchrony when operating outside their\noptimal region. These findings highlight the importance of principled\nhyperparameter tuning to ensure both task performance and energy efficiency.\nOur results offer practical guidelines for deploying robust and efficient SNNs,\nparticularly in neuromorphic computing scenarios."
    },
    {
        "date": "2025-07",
        "title": "CANDoSA: A Hardware Performance Counter-Based Intrusion Detection System for DoS Attacks on Automotive CAN bus",
        "author": "Franco Oberti, Stefano Di Carlo, and Alessandro Savino",
        "link": "http://arxiv.org/abs/2507.14739v1",
        "abstract": "The Controller Area Network (CAN) protocol, essential for automotive embedded\nsystems, lacks inherent security features, making it vulnerable to cyber\nthreats, especially with the rise of autonomous vehicles. Traditional security\nmeasures offer limited protection, such as payload encryption and message\nauthentication. This paper presents a novel Intrusion Detection System (IDS)\ndesigned for the CAN environment, utilizing Hardware Performance Counters\n(HPCs) to detect anomalies indicative of cyber attacks. A RISC-V-based CAN\nreceiver is simulated using the gem5 simulator, processing CAN frame payloads\nwith AES-128 encryption as FreeRTOS tasks, which trigger distinct HPC\nresponses. Key HPC features are optimized through data extraction and\ncorrelation analysis to enhance classification efficiency. Results indicate\nthat this approach could significantly improve CAN security and address\nemerging challenges in automotive cybersecurity."
    },
    {
        "date": "2025-07",
        "title": "Balancing Expressivity and Robustness: Constrained Rational Activations for Reinforcement Learning",
        "author": "Rafa\u0142 Surdej, Micha\u0142 Bortkiewicz, Alex Lewandowski, Mateusz Ostaszewski, and Clare Lyle",
        "link": "http://arxiv.org/abs/2507.14736v1",
        "abstract": "Trainable activation functions, whose parameters are optimized alongside\nnetwork weights, offer increased expressivity compared to fixed activation\nfunctions. Specifically, trainable activation functions defined as ratios of\npolynomials (rational functions) have been proposed to enhance plasticity in\nreinforcement learning. However, their impact on training stability remains\nunclear. In this work, we study trainable rational activations in both\nreinforcement and continual learning settings. We find that while their\nflexibility enhances adaptability, it can also introduce instability, leading\nto overestimation in RL and feature collapse in longer continual learning\nscenarios. Our main result is demonstrating a trade-off between expressivity\nand plasticity in rational activations. To address this, we propose a\nconstrained variant that structurally limits excessive output scaling while\npreserving adaptability. Experiments across MetaWorld and DeepMind Control\nSuite (DMC) environments show that our approach improves training stability and\nperformance. In continual learning benchmarks, including MNIST with reshuffled\nlabels and Split CIFAR-100, we reveal how different constraints affect the\nbalance between expressivity and long-term retention. While preliminary\nexperiments in discrete action domains (e.g., Atari) did not show similar\ninstability, this suggests that the trade-off is particularly relevant for\ncontinuous control. Together, our findings provide actionable design principles\nfor robust and adaptable trainable activations in dynamic, non-stationary\nenvironments. Code available at:\nhttps://github.com/special114/rl_rational_plasticity."
    },
    {
        "date": "2025-07",
        "title": "VTarbel: Targeted Label Attack with Minimal Knowledge on Detector-enhanced Vertical Federated Learning",
        "author": "Juntao Tan, Anran Li, Quanchao Liu, Peng Ran, and Lan Zhang",
        "link": "http://arxiv.org/abs/2507.14625v1",
        "abstract": "Vertical federated learning (VFL) enables multiple parties with disjoint\nfeatures to collaboratively train models without sharing raw data. While\nprivacy vulnerabilities of VFL are extensively-studied, its security\nthreats-particularly targeted label attacks-remain underexplored. In such\nattacks, a passive party perturbs inputs at inference to force\nmisclassification into adversary-chosen labels. Existing methods rely on\nunrealistic assumptions (e.g., accessing VFL-model's outputs) and ignore\nanomaly detectors deployed in real-world systems. To bridge this gap, we\nintroduce VTarbel, a two-stage, minimal-knowledge attack framework explicitly\ndesigned to evade detector-enhanced VFL inference. During the preparation\nstage, the attacker selects a minimal set of high-expressiveness samples (via\nmaximum mean discrepancy), submits them through VFL protocol to collect\npredicted labels, and uses these pseudo-labels to train estimated detector and\nsurrogate model on local features. In attack stage, these models guide\ngradient-based perturbations of remaining samples, crafting adversarial\ninstances that induce targeted misclassifications and evade detection. We\nimplement VTarbel and evaluate it against four model architectures, seven\nmultimodal datasets, and two anomaly detectors. Across all settings, VTarbel\noutperforms four state-of-the-art baselines, evades detection, and retains\neffective against three representative privacy-preserving defenses. These\nresults reveal critical security blind spots in current VFL deployments and\nunderscore urgent need for robust, attack-aware defenses."
    },
    {
        "date": "2025-07",
        "title": "A Hybrid Classical-Quantum Rainbow Table Attack on Human Passwords",
        "author": "MA. Khajeian",
        "link": "http://arxiv.org/abs/2507.14600v2",
        "abstract": "Long, human-generated passwords pose significant challenges to both classical\nand quantum attacks due to their irregular structure and large search space. In\nthis work, we propose an enhanced classical-quantum hybrid attack specifically\ndesigned for this scenario. Our approach constructs rainbow tables using\ndictionary-based password generation augmented with transformation rules that\nbetter capture real-world user behavior. These tables are organized into\nbuckets, enabling faster lookup and reduced space complexity. For the search\nwithin each bucket, we employ a distributed exact variant of Grover's\nalgorithm. This method provides deterministic success and significantly lower\ncircuit depth, enhancing robustness against noise-particularly depolarizing\nerrors common in near-term quantum devices. Overall, our hybrid framework\nimproves the efficiency and practicality of password recovery for long,\nhuman-readable passwords in realistic adversarial settings."
    },
    {
        "date": "2025-07",
        "title": "VisGuard: Securing Visualization Dissemination through Tamper-Resistant Data Retrieval",
        "author": "Huayuan Ye, Juntong Chen, Shenzhuo Zhang, Yipeng Zhang, Changbo Wang, and Chenhui Li",
        "link": "http://arxiv.org/abs/2507.14459v1",
        "abstract": "The dissemination of visualizations is primarily in the form of raster\nimages, which often results in the loss of critical information such as source\ncode, interactive features, and metadata. While previous methods have proposed\nembedding metadata into images to facilitate Visualization Image Data Retrieval\n(VIDR), most existing methods lack practicability since they are fragile to\ncommon image tampering during online distribution such as cropping and editing.\nTo address this issue, we propose VisGuard, a tamper-resistant VIDR framework\nthat reliably embeds metadata link into visualization images. The embedded data\nlink remains recoverable even after substantial tampering upon images. We\npropose several techniques to enhance robustness, including repetitive data\ntiling, invertible information broadcasting, and an anchor-based scheme for\ncrop localization. VisGuard enables various applications, including interactive\nchart reconstruction, tampering detection, and copyright protection. We conduct\ncomprehensive experiments on VisGuard's superior performance in data retrieval\naccuracy, embedding capacity, and security against tampering and steganalysis,\ndemonstrating VisGuard's competence in facilitating and safeguarding\nvisualization dissemination and information conveyance."
    },
    {
        "date": "2025-07",
        "title": "GPI-Net: Gestalt-Guided Parallel Interaction Network via Orthogonal Geometric Consistency for Robust Point Cloud Registration",
        "author": "Weikang Gu, Mingyue Han, Li Xue, Heng Dong, Changcai Yang, Riqing Chen, and Lifang Wei",
        "link": "http://arxiv.org/abs/2507.14452v1",
        "abstract": "The accurate identification of high-quality correspondences is a prerequisite\ntask in feature-based point cloud registration. However, it is extremely\nchallenging to handle the fusion of local and global features due to feature\nredundancy and complex spatial relationships. Given that Gestalt principles\nprovide key advantages in analyzing local and global relationships, we propose\na novel Gestalt-guided Parallel Interaction Network via orthogonal geometric\nconsistency (GPI-Net) in this paper. It utilizes Gestalt principles to\nfacilitate complementary communication between local and global information.\nSpecifically, we introduce an orthogonal integration strategy to optimally\nreduce redundant information and generate a more compact global structure for\nhigh-quality correspondences. To capture geometric features in correspondences,\nwe leverage a Gestalt Feature Attention (GFA) block through a hybrid\nutilization of self-attention and cross-attention mechanisms. Furthermore, to\nfacilitate the integration of local detail information into the global\nstructure, we design an innovative Dual-path Multi-Granularity parallel\ninteraction aggregation (DMG) block to promote information exchange across\ndifferent granularities. Extensive experiments on various challenging tasks\ndemonstrate the superior performance of our proposed GPI-Net in comparison to\nexisting methods. The code will be released at https://github.com/gwk/GPI-Net."
    },
    {
        "date": "2025-07",
        "title": "Topological Social Choice: Designing a Noise-Robust Polar Distance for Persistence Diagrams",
        "author": "Athanasios Andrikopoulos, and Nikolaos Sampanis",
        "link": "http://arxiv.org/abs/2507.14340v1",
        "abstract": "Topological Data Analysis (TDA) has emerged as a powerful framework for\nextracting robust and interpretable features from noisy high-dimensional data.\nIn the context of Social Choice Theory, where preference profiles and\ncollective decisions are geometrically rich yet sensitive to perturbations, TDA\nremains largely unexplored. This work introduces a novel conceptual bridge\nbetween these domains by proposing a new metric framework for persistence\ndiagrams tailored to noisy preference data.We define a polar coordinate-based\ndistance that captures both the magnitude and orientation of topological\nfeatures in a smooth and differentiable manner. Our metric addresses key\nlimitations of classical distances, such as bottleneck and Wasserstein,\nincluding instability under perturbation, lack of continuity, and\nincompatibility with gradient-based learning. The resulting formulation offers\nimproved behavior in both theoretical and applied settings.To the best of our\nknowledge, this is the first study to systematically apply persistent homology\nto social choice systems, providing a mathematically grounded method for\ncomparing topological summaries of voting structures and preference dynamics.\nWe demonstrate the superiority of our approach through extensive experiments,\nincluding robustness tests and supervised learning tasks, and we propose a\nmodular pipeline for building predictive models from online preference data.\nThis work contributes a conceptually novel and computationally effective tool\nto the emerging interface of topology and decision theory, opening new\ndirections in interpretable machine learning for political and economic\nsystems."
    },
    {
        "date": "2025-07",
        "title": "FedStrategist: A Meta-Learning Framework for Adaptive and Robust Aggregation in Federated Learning",
        "author": "Md Rafid Haque, Abu Raihan Mostofa Kamal, and Md. Azam Hossain",
        "link": "http://arxiv.org/abs/2507.14322v2",
        "abstract": "Federated Learning (FL) offers a paradigm for privacy-preserving\ncollaborative AI, but its decentralized nature creates significant\nvulnerabilities to model poisoning attacks. While numerous static defenses\nexist, their effectiveness is highly context-dependent, often failing against\nadaptive adversaries or in heterogeneous data environments. This paper\nintroduces FedStrategist, a novel meta-learning framework that reframes robust\naggregation as a real-time, cost-aware control problem. We design a lightweight\ncontextual bandit agent that dynamically selects the optimal aggregation rule\nfrom an arsenal of defenses based on real-time diagnostic metrics. Through\ncomprehensive experiments, we demonstrate that no single static rule is\nuniversally optimal. We show that our adaptive agent successfully learns\nsuperior policies across diverse scenarios, including a ``Krum-favorable\"\nenvironment and against a sophisticated \"stealth\" adversary designed to\nneutralize specific diagnostic signals. Critically, we analyze the paradoxical\nscenario where a non-robust baseline achieves high but compromised accuracy,\nand demonstrate that our agent learns a conservative policy to prioritize model\nintegrity. Furthermore, we prove the agent's policy is controllable via a\nsingle \"risk tolerance\" parameter, allowing practitioners to explicitly manage\nthe trade-off between performance and security. Our work provides a new,\npractical, and analyzable approach to creating resilient and intelligent\ndecentralized AI systems."
    },
    {
        "date": "2025-07",
        "title": "CLIPTTA: Robust Contrastive Vision-Language Test-Time Adaptation",
        "author": "Marc Lafon, Gustavo Adolfo Vargas Hakim, Cl\u00e9ment Rambour, Christian Desrosier, and Nicolas Thome",
        "link": "http://arxiv.org/abs/2507.14312v1",
        "abstract": "Vision-language models (VLMs) like CLIP exhibit strong zero-shot capabilities\nbut often fail to generalize under distribution shifts. Test-time adaptation\n(TTA) allows models to update at inference time without labeled data, typically\nvia entropy minimization. However, this objective is fundamentally misaligned\nwith the contrastive image-text training of VLMs, limiting adaptation\nperformance and introducing failure modes such as pseudo-label drift and class\ncollapse. We propose CLIPTTA, a new gradient-based TTA method for\nvision-language models that leverages a soft contrastive loss aligned with\nCLIP's pre-training objective. We provide a theoretical analysis of CLIPTTA's\ngradients, showing how its batch-aware design mitigates the risk of collapse.\nWe further extend CLIPTTA to the open-set setting, where both in-distribution\n(ID) and out-of-distribution (OOD) samples are encountered, using an Outlier\nContrastive Exposure (OCE) loss to improve OOD detection. Evaluated on 75\ndatasets spanning diverse distribution shifts, CLIPTTA consistently outperforms\nentropy-based objectives and is highly competitive with state-of-the-art TTA\nmethods, outperforming them on a large number of datasets and exhibiting more\nstable performance across diverse shifts."
    },
    {
        "date": "2025-07",
        "title": "An Adversarial-Driven Experimental Study on Deep Learning for RF Fingerprinting",
        "author": "Xinyu Cao, Bimal Adhikari, Shangqing Zhao, Jingxian Wu, and Yanjun Pan",
        "link": "http://arxiv.org/abs/2507.14109v1",
        "abstract": "Radio frequency (RF) fingerprinting, which extracts unique hardware\nimperfections of radio devices, has emerged as a promising physical-layer\ndevice identification mechanism in zero trust architectures and beyond 5G\nnetworks. In particular, deep learning (DL) methods have demonstrated\nstate-of-the-art performance in this domain. However, existing approaches have\nprimarily focused on enhancing system robustness against temporal and spatial\nvariations in wireless environments, while the security vulnerabilities of\nthese DL-based approaches have often been overlooked. In this work, we\nsystematically investigate the security risks of DL-based RF fingerprinting\nsystems through an adversarial-driven experimental analysis. We observe a\nconsistent misclassification behavior for DL models under domain shifts, where\na device is frequently misclassified as another specific one. Our analysis\nbased on extensive real-world experiments demonstrates that this behavior can\nbe exploited as an effective backdoor to enable external attackers to intrude\ninto the system. Furthermore, we show that training DL models on raw received\nsignals causes the models to entangle RF fingerprints with environmental and\nsignal-pattern features, creating additional attack vectors that cannot be\nmitigated solely through post-processing security methods such as confidence\nthresholds."
    },
    {
        "date": "2025-07",
        "title": "Glucose-ML: A collection of longitudinal diabetes datasets for development of robust AI solutions",
        "author": "Temiloluwa Prioleau, Baiying Lu, and Yanjun Cui",
        "link": "http://arxiv.org/abs/2507.14077v1",
        "abstract": "Artificial intelligence (AI) algorithms are a critical part of\nstate-of-the-art digital health technology for diabetes management. Yet, access\nto large high-quality datasets is creating barriers that impede development of\nrobust AI solutions. To accelerate development of transparent, reproducible,\nand robust AI solutions, we present Glucose-ML, a collection of 10 publicly\navailable diabetes datasets, released within the last 7 years (i.e., 2018 -\n2025). The Glucose-ML collection comprises over 300,000 days of continuous\nglucose monitor (CGM) data with a total of 38 million glucose samples collected\nfrom 2500+ people across 4 countries. Participants include persons living with\ntype 1 diabetes, type 2 diabetes, prediabetes, and no diabetes. To support\nresearchers and innovators with using this rich collection of diabetes\ndatasets, we present a comparative analysis to guide algorithm developers with\ndata selection. Additionally, we conduct a case study for the task of blood\nglucose prediction - one of the most common AI tasks within the field. Through\nthis case study, we provide a benchmark for short-term blood glucose prediction\nacross all 10 publicly available diabetes datasets within the Glucose-ML\ncollection. We show that the same algorithm can have significantly different\nprediction results when developed/evaluated with different datasets. Findings\nfrom this study are then used to inform recommendations for developing robust\nAI solutions within the diabetes or broader health domain. We provide direct\nlinks to each longitudinal diabetes dataset in the Glucose-ML collection and\nopenly provide our code."
    },
    {
        "date": "2025-07",
        "title": "The CryptoNeo Threat Modelling Framework (CNTMF): Securing Neobanks and Fintech in Integrated Blockchain Ecosystems",
        "author": "Serhan W. Bahar",
        "link": "http://arxiv.org/abs/2507.14007v1",
        "abstract": "The rapid integration of blockchain, cryptocurrency, and Web3 technologies\ninto digital banks and fintech operations has created an integrated environment\nblending traditional financial systems with decentralised elements. This paper\nintroduces the CryptoNeo Threat Modelling Framework (CNTMF), a proposed\nframework designed to address the risks in these ecosystems, such as oracle\nmanipulation and cross-chain exploits. CNTMF represents a proposed extension of\nestablished methodologies like STRIDE, OWASP Top 10, NIST frameworks, LINDDUN,\nand PASTA, while incorporating tailored components including Hybrid Layer\nAnalysis, the CRYPTOQ mnemonic for cryptocurrency-specific risks, and an\nAI-Augmented Feedback Loop. Drawing on real-world data from 2025 incidents,\nCNTMF supports data-driven mitigation to reduce losses, which totalled\napproximately $2.47 billion in the first half of 2025 across 344 security\nevents (CertiK via GlobeNewswire, 2025; Infosecurity Magazine, 2025). Its\nphases guide asset mapping, risk profiling, prioritisation, mitigation, and\niterative feedback. This supports security against evolving risks like\nstate-sponsored attacks."
    },
    {
        "date": "2025-07",
        "title": "Robust Anomaly Detection with Graph Neural Networks using Controllability",
        "author": "Yifan Wei, Anwar Said, Waseem Abbas, and Xenofon Koutsoukos",
        "link": "http://arxiv.org/abs/2507.13954v1",
        "abstract": "Anomaly detection in complex domains poses significant challenges due to the\nneed for extensive labeled data and the inherently imbalanced nature of\nanomalous versus benign samples. Graph-based machine learning models have\nemerged as a promising solution that combines attribute and relational data to\nuncover intricate patterns. However, the scarcity of anomalous data exacerbates\nthe challenge, which requires innovative strategies to enhance model learning\nwith limited information. In this paper, we hypothesize that the incorporation\nof the influence of the nodes, quantified through average controllability, can\nsignificantly improve the performance of anomaly detection. We propose two\nnovel approaches to integrate average controllability into graph-based\nframeworks: (1) using average controllability as an edge weight and (2)\nencoding it as a one-hot edge attribute vector. Through rigorous evaluation on\nreal-world and synthetic networks with six state-of-the-art baselines, our\nproposed methods demonstrate improved performance in identifying anomalies,\nhighlighting the critical role of controllability measures in enhancing the\nperformance of graph machine learning models. This work underscores the\npotential of integrating average controllability as additional metrics to\naddress the challenges of anomaly detection in sparse and imbalanced datasets."
    },
    {
        "date": "2025-07",
        "title": "Developers Insight On Manifest v3 Privacy and Security Webextensions",
        "author": "Libor Pol\u010d\u00e1k, Giorgio Maone, Michael McMahon, and Martin Bedn\u00e1\u0159",
        "link": "http://arxiv.org/abs/2507.13926v1",
        "abstract": "Webextensions can improve web browser privacy, security, and user experience.\nThe APIs offered by the browser to webextensions affect possible functionality.\nCurrently, Chrome transitions to a modified set of APIs called Manifest v3.\nThis paper studies the challenges and opportunities of Manifest v3 with an\nin-depth structured qualitative research. Even though some projects observed\npositive effects, a majority expresses concerns over limited benefits to users,\nremoval of crucial APIs, or the need to find workarounds. Our findings indicate\nthat the transition affects different types of webextensions differently; some\ncan migrate without losing functionality, while other projects remove\nfunctionality or decline to update. The respondents identified several critical\nmissing APIs, including reliable APIs to inject content scripts, APIs for\nstoring confidential content, and others."
    },
    {
        "date": "2025-07",
        "title": "Adversarial Training Improves Generalization Under Distribution Shifts in Bioacoustics",
        "author": "Ren\u00e9 Heinrich, Lukas Rauch, Bernhard Sick, and Christoph Scholz",
        "link": "http://arxiv.org/abs/2507.13727v1",
        "abstract": "Adversarial training is a promising strategy for enhancing model robustness\nagainst adversarial attacks. However, its impact on generalization under\nsubstantial data distribution shifts in audio classification remains largely\nunexplored. To address this gap, this work investigates how different\nadversarial training strategies improve generalization performance and\nadversarial robustness in audio classification. The study focuses on two model\narchitectures: a conventional convolutional neural network (ConvNeXt) and an\ninherently interpretable prototype-based model (AudioProtoPNet). The approach\nis evaluated using a challenging bird sound classification benchmark. This\nbenchmark is characterized by pronounced distribution shifts between training\nand test data due to varying environmental conditions and recording methods, a\ncommon real-world challenge. The investigation explores two adversarial\ntraining strategies: one based on output-space attacks that maximize the\nclassification loss function, and another based on embedding-space attacks\ndesigned to maximize embedding dissimilarity. These attack types are also used\nfor robustness evaluation. Additionally, for AudioProtoPNet, the study assesses\nthe stability of its learned prototypes under targeted embedding-space attacks.\nResults show that adversarial training, particularly using output-space\nattacks, improves clean test data performance by an average of 10.5% relative\nand simultaneously strengthens the adversarial robustness of the models. These\nfindings, although derived from the bird sound domain, suggest that adversarial\ntraining holds potential to enhance robustness against both strong distribution\nshifts and adversarial attacks in challenging audio classification settings."
    },
    {
        "date": "2025-07",
        "title": "TopicAttack: An Indirect Prompt Injection Attack via Topic Transition",
        "author": "Yulin Chen, Haoran Li, Yuexin Li, Yue Liu, Yangqiu Song, and Bryan Hooi",
        "link": "http://arxiv.org/abs/2507.13686v1",
        "abstract": "Large language models (LLMs) have shown remarkable performance across a range\nof NLP tasks. However, their strong instruction-following capabilities and\ninability to distinguish instructions from data content make them vulnerable to\nindirect prompt injection attacks. In such attacks, instructions with malicious\npurposes are injected into external data sources, such as web documents. When\nLLMs retrieve this injected data through tools, such as a search engine and\nexecute the injected instructions, they provide misled responses. Recent attack\nmethods have demonstrated potential, but their abrupt instruction injection\noften undermines their effectiveness. Motivated by the limitations of existing\nattack methods, we propose TopicAttack, which prompts the LLM to generate a\nfabricated conversational transition prompt that gradually shifts the topic\ntoward the injected instruction, making the injection smoother and enhancing\nthe plausibility and success of the attack. Through comprehensive experiments,\nTopicAttack achieves state-of-the-art performance, with an attack success rate\n(ASR) over 90\\% in most cases, even when various defense methods are applied.\nWe further analyze its effectiveness by examining attention scores. We find\nthat a higher injected-to-original attention ratio leads to a greater success\nprobability, and our method achieves a much higher ratio than the baseline\nmethods."
    },
    {
        "date": "2025-07",
        "title": "MaskHOI: Robust 3D Hand-Object Interaction Estimation via Masked Pre-training",
        "author": "Yuechen Xie, Haobo Jiang, Jian Yang, Yigong Zhang, and Jin Xie",
        "link": "http://arxiv.org/abs/2507.13673v1",
        "abstract": "In 3D hand-object interaction (HOI) tasks, estimating precise joint poses of\nhands and objects from monocular RGB input remains highly challenging due to\nthe inherent geometric ambiguity of RGB images and the severe mutual occlusions\nthat occur during interaction.To address these challenges, we propose MaskHOI,\na novel Masked Autoencoder (MAE)-driven pretraining framework for enhanced HOI\npose estimation. Our core idea is to leverage the masking-then-reconstruction\nstrategy of MAE to encourage the feature encoder to infer missing spatial and\nstructural information, thereby facilitating geometric-aware and\nocclusion-robust representation learning. Specifically, based on our\nobservation that human hands exhibit far greater geometric complexity than\nrigid objects, conventional uniform masking fails to effectively guide the\nreconstruction of fine-grained hand structures. To overcome this limitation, we\nintroduce a Region-specific Mask Ratio Allocation, primarily comprising the\nregion-specific masking assignment and the skeleton-driven hand masking\nguidance. The former adaptively assigns lower masking ratios to hand regions\nthan to rigid objects, balancing their feature learning difficulty, while the\nlatter prioritizes masking critical hand parts (e.g., fingertips or entire\nfingers) to realistically simulate occlusion patterns in real-world\ninteractions. Furthermore, to enhance the geometric awareness of the pretrained\nencoder, we introduce a novel Masked Signed Distance Field (SDF)-driven\nmultimodal learning mechanism. Through the self-masking 3D SDF prediction, the\nlearned encoder is able to perceive the global geometric structure of hands and\nobjects beyond the 2D image plane, overcoming the inherent limitations of\nmonocular input and alleviating self-occlusion issues. Extensive experiments\ndemonstrate that our method significantly outperforms existing state-of-the-art\napproaches."
    },
    {
        "date": "2025-07",
        "title": "Breaking the Illusion of Security via Interpretation: Interpretable Vision Transformer Systems under Attack",
        "author": "Eldor Abdukhamidov, Mohammed Abuhamad, Simon S. Woo, Hyoungshick Kim, and Tamer Abuhmed",
        "link": "http://arxiv.org/abs/2507.14248v1",
        "abstract": "Vision transformer (ViT) models, when coupled with interpretation models, are\nregarded as secure and challenging to deceive, making them well-suited for\nsecurity-critical domains such as medical applications, autonomous vehicles,\ndrones, and robotics. However, successful attacks on these systems can lead to\nsevere consequences. Recent research on threats targeting ViT models primarily\nfocuses on generating the smallest adversarial perturbations that can deceive\nthe models with high confidence, without considering their impact on model\ninterpretations. Nevertheless, the use of interpretation models can effectively\nassist in detecting adversarial examples. This study investigates the\nvulnerability of transformer models to adversarial attacks, even when combined\nwith interpretation models. We propose an attack called \"AdViT\" that generates\nadversarial examples capable of misleading both a given transformer model and\nits coupled interpretation model. Through extensive experiments on various\ntransformer models and two transformer-based interpreters, we demonstrate that\nAdViT achieves a 100% attack success rate in both white-box and black-box\nscenarios. In white-box scenarios, it reaches up to 98% misclassification\nconfidence, while in black-box scenarios, it reaches up to 76%\nmisclassification confidence. Remarkably, AdViT consistently generates accurate\ninterpretations in both scenarios, making the adversarial examples more\ndifficult to detect."
    },
    {
        "date": "2025-07",
        "title": "Large Language Models in Cybersecurity: Applications, Vulnerabilities, and Defense Techniques",
        "author": "Niveen O. Jaffal, Mohammed Alkhanafseh, and David Mohaisen",
        "link": "http://arxiv.org/abs/2507.13629v1",
        "abstract": "Large Language Models (LLMs) are transforming cybersecurity by enabling\nintelligent, adaptive, and automated approaches to threat detection,\nvulnerability assessment, and incident response. With their advanced language\nunderstanding and contextual reasoning, LLMs surpass traditional methods in\ntackling challenges across domains such as IoT, blockchain, and hardware\nsecurity. This survey provides a comprehensive overview of LLM applications in\ncybersecurity, focusing on two core areas: (1) the integration of LLMs into key\ncybersecurity domains, and (2) the vulnerabilities of LLMs themselves, along\nwith mitigation strategies. By synthesizing recent advancements and identifying\nkey limitations, this work offers practical insights and strategic\nrecommendations for leveraging LLMs to build secure, scalable, and future-ready\ncyber defense systems."
    },
    {
        "date": "2025-07",
        "title": "FuSeFL: Fully Secure and Scalable Cross-Silo Federated Learning",
        "author": "Sahar Ghoflsaz Ghinani, and Elaheh Sadredini",
        "link": "http://arxiv.org/abs/2507.13591v1",
        "abstract": "Federated Learning (FL) enables collaborative model training without\ncentralizing client data, making it attractive for privacy-sensitive domains.\nWhile existing approaches employ cryptographic techniques such as homomorphic\nencryption, differential privacy, or secure multiparty computation to mitigate\ninference attacks-including model inversion, membership inference, and gradient\nleakage-they often suffer from high computational, communication, or memory\noverheads. Moreover, many methods overlook the confidentiality of the global\nmodel itself, which may be proprietary and sensitive. These challenges limit\nthe practicality of secure FL, especially in cross-silo deployments involving\nlarge datasets and strict compliance requirements.\n  We present FuSeFL, a fully secure and scalable FL scheme designed for\ncross-silo settings. FuSeFL decentralizes training across client pairs using\nlightweight secure multiparty computation (MPC), while confining the server's\nrole to secure aggregation. This design eliminates server bottlenecks, avoids\ndata offloading, and preserves full confidentiality of data, model, and updates\nthroughout training. FuSeFL defends against inference threats, achieves up to\n95% lower communication latency and 50% lower server memory usage, and improves\naccuracy over prior secure FL solutions, demonstrating strong security and\nefficiency at scale."
    },
    {
        "date": "2025-07",
        "title": "SHIELD: A Secure and Highly Enhanced Integrated Learning for Robust Deepfake Detection against Adversarial Attacks",
        "author": "Kutub Uddin, Awais Khan, Muhammad Umar Farooq, and Khalid Malik",
        "link": "http://arxiv.org/abs/2507.13170v1",
        "abstract": "Audio plays a crucial role in applications like speaker verification,\nvoice-enabled smart devices, and audio conferencing. However, audio\nmanipulations, such as deepfakes, pose significant risks by enabling the spread\nof misinformation. Our empirical analysis reveals that existing methods for\ndetecting deepfake audio are often vulnerable to anti-forensic (AF) attacks,\nparticularly those attacked using generative adversarial networks. In this\narticle, we propose a novel collaborative learning method called SHIELD to\ndefend against generative AF attacks. To expose AF signatures, we integrate an\nauxiliary generative model, called the defense (DF) generative model, which\nfacilitates collaborative learning by combining input and output. Furthermore,\nwe design a triplet model to capture correlations for real and AF attacked\naudios with real-generated and attacked-generated audios using auxiliary\ngenerative models. The proposed SHIELD strengthens the defense against\ngenerative AF attacks and achieves robust performance across various generative\nmodels. The proposed AF significantly reduces the average detection accuracy\nfrom 95.49% to 59.77% for ASVspoof2019, from 99.44% to 38.45% for In-the-Wild,\nand from 98.41% to 51.18% for HalfTruth for three different generative models.\nThe proposed SHIELD mechanism is robust against AF attacks and achieves an\naverage accuracy of 98.13%, 98.58%, and 99.57% in match, and 98.78%, 98.62%,\nand 98.85% in mismatch settings for the ASVspoof2019, In-the-Wild, and\nHalfTruth datasets, respectively."
    },
    {
        "date": "2025-07",
        "title": "Backscattering-Based Security in Wireless Power Transfer Applied to Battery-Free BLE Sensors",
        "author": "Taki Eddine Djidjekh, Ga\u00ebl Loubet, and Alexandru Takacs",
        "link": "http://arxiv.org/abs/2507.13042v1",
        "abstract": "The integration of security and energy efficiency in Internet of Things\nsystems remains a critical challenge, particularly for battery-free and\nresource-constrained devices. This paper explores the scalability and\nprotocol-agnostic nature of a backscattering-based security mechanism by\nintegrating it into Bluetooth Low Energy battery-free Wireless Sensor Network.\nThe proposed approach leverages the Wireless Power Transfer link, traditionally\nused for energy harvesting, to generate additional identification signals\nwithout increasing energy consumption or computational demands. Experimental\nvalidation demonstrates the solution's functionality using compact, low-gain\nantenna, ensuring compatibility with size-constrained applications such as\nStructural Health Monitoring and smart transport. Furthermore, this work\naddresses the challenges associated with backscattering dynamic range and\nmulti-node Wireless Sensor Network scenarios, discussing potential collisions\nbetween identification signals and proposing future improvements to enhance\ngeneralizability and scalability. The findings underscore the potential of the\nbackscattering-based security mechanism for creating secure, sustainable, and\nscalable IoT deployments across diverse protocols and applications."
    },
    {
        "date": "2025-07",
        "title": "MAD-Spear: A Conformity-Driven Prompt Injection Attack on Multi-Agent Debate Systems",
        "author": "Yu Cui, and Hongyang Du",
        "link": "http://arxiv.org/abs/2507.13038v1",
        "abstract": "Multi-agent debate (MAD) systems leverage collaborative interactions among\nlarge language models (LLMs) agents to improve reasoning capabilities. While\nrecent studies have focused on increasing the accuracy and scalability of MAD\nsystems, their security vulnerabilities have received limited attention. In\nthis work, we introduce MAD-Spear, a targeted prompt injection attack that\ncompromises a small subset of agents but significantly disrupts the overall MAD\nprocess. Manipulated agents produce multiple plausible yet incorrect responses,\nexploiting LLMs' conformity tendencies to propagate misinformation and degrade\nconsensus quality. Furthermore, the attack can be composed with other\nstrategies, such as communication attacks, to further amplify its impact by\nincreasing the exposure of agents to incorrect responses. To assess MAD's\nresilience under attack, we propose a formal definition of MAD fault-tolerance\nand develop a comprehensive evaluation framework that jointly considers\naccuracy, consensus efficiency, and scalability. Extensive experiments on five\nbenchmark datasets with varying difficulty levels demonstrate that MAD-Spear\nconsistently outperforms the baseline attack in degrading system performance.\nAdditionally, we observe that agent diversity substantially improves MAD\nperformance in mathematical reasoning tasks, which challenges prior work\nsuggesting that agent diversity has minimal impact on performance. These\nfindings highlight the urgent need to improve the security in MAD design."
    },
    {
        "date": "2025-07",
        "title": "Enterprise Security Incident Analysis and Countermeasures Based on the T-Mobile Data Breach",
        "author": "Zhuohan Cui, and Zikun Song",
        "link": "http://arxiv.org/abs/2507.12937v1",
        "abstract": "This paper presents a comprehensive analysis of T-Mobile's critical data\nbreaches in 2021 and 2023, alongside a full-spectrum security audit targeting\nits systems, infrastructure, and publicly exposed endpoints. By combining\ncase-based vulnerability assessments with active ethical hacking\ntechniques--including Shodan reconnaissance, API misuse simulations, VNC\nbrute-forcing, firmware reverse engineering, and web application scans--we\nuncover structural weaknesses persisting beyond the initial breach events.\nBuilding on these findings, we propose a multi-layered defensive strategy\nencompassing Zero Trust Architecture, granular role-based access control,\nnetwork segmentation, firmware encryption using AES with integrity checks, and\nAPI rate limiting and token lifecycle control. Financial modelling demonstrates\nthat a five-year investment yields less than 1.1% of expected breach losses,\nvalidating the cost-effectiveness of proactive security measures. Our work\nbridges post-incident forensic analysis with hands-on security evaluation,\nproviding an actionable blueprint for large-scale telecoms seeking operational\nresilience, regulatory compliance, and cross-domain threat readiness."
    },
    {
        "date": "2025-07",
        "title": "Architectural Backdoors in Deep Learning: A Survey of Vulnerabilities, Detection, and Defense",
        "author": "Victoria Childress, Josh Collyer, and Jodie Knapp",
        "link": "http://arxiv.org/abs/2507.12919v1",
        "abstract": "Architectural backdoors pose an under-examined but critical threat to deep\nneural networks, embedding malicious logic directly into a model's\ncomputational graph. Unlike traditional data poisoning or parameter\nmanipulation, architectural backdoors evade standard mitigation techniques and\npersist even after clean retraining. This survey systematically consolidates\nresearch on architectural backdoors, spanning compiler-level manipulations,\ntainted AutoML pipelines, and supply-chain vulnerabilities. We assess emerging\ndetection and defense strategies, including static graph inspection, dynamic\nfuzzing, and partial formal verification, and highlight their limitations\nagainst distributed or stealth triggers. Despite recent progress, scalable and\npractical defenses remain elusive. We conclude by outlining open challenges and\nproposing directions for strengthening supply-chain security, cryptographic\nmodel attestations, and next-generation benchmarks. This survey aims to guide\nfuture research toward comprehensive defenses against structural backdoor\nthreats in deep learning systems."
    },
    {
        "date": "2025-07",
        "title": "Robust Explanations Through Uncertainty Decomposition: A Path to Trustworthier AI",
        "author": "Chenrui Zhu, Louenas Bounia, Vu Linh Nguyen, S\u00e9bastien Destercke, and Arthur Hoarau",
        "link": "http://arxiv.org/abs/2507.12913v1",
        "abstract": "Recent advancements in machine learning have emphasized the need for\ntransparency in model predictions, particularly as interpretability diminishes\nwhen using increasingly complex architectures. In this paper, we propose\nleveraging prediction uncertainty as a complementary approach to classical\nexplainability methods. Specifically, we distinguish between aleatoric\n(data-related) and epistemic (model-related) uncertainty to guide the selection\nof appropriate explanations. Epistemic uncertainty serves as a rejection\ncriterion for unreliable explanations and, in itself, provides insight into\ninsufficient training (a new form of explanation). Aleatoric uncertainty\ninforms the choice between feature-importance explanations and counterfactual\nexplanations. This leverages a framework of explainability methods driven by\nuncertainty quantification and disentanglement. Our experiments demonstrate the\nimpact of this uncertainty-aware approach on the robustness and attainability\nof explanations in both traditional machine learning and deep learning\nscenarios."
    },
    {
        "date": "2025-07",
        "title": "Manipulation Attacks by Misaligned AI: Risk Analysis and Safety Case Framework",
        "author": "Rishane Dassanayake, Mario Demetroudi, James Walpole, Lindley Lentati, Jason R. Brown, and Edward James Young",
        "link": "http://arxiv.org/abs/2507.12872v1",
        "abstract": "Frontier AI systems are rapidly advancing in their capabilities to persuade,\ndeceive, and influence human behaviour, with current models already\ndemonstrating human-level persuasion and strategic deception in specific\ncontexts. Humans are often the weakest link in cybersecurity systems, and a\nmisaligned AI system deployed internally within a frontier company may seek to\nundermine human oversight by manipulating employees. Despite this growing\nthreat, manipulation attacks have received little attention, and no systematic\nframework exists for assessing and mitigating these risks. To address this, we\nprovide a detailed explanation of why manipulation attacks are a significant\nthreat and could lead to catastrophic outcomes. Additionally, we present a\nsafety case framework for manipulation risk, structured around three core lines\nof argument: inability, control, and trustworthiness. For each argument, we\nspecify evidence requirements, evaluation methodologies, and implementation\nconsiderations for direct application by AI companies. This paper provides the\nfirst systematic methodology for integrating manipulation risk into AI safety\ngovernance, offering AI companies a concrete foundation to assess and mitigate\nthese threats before deployment."
    },
    {
        "date": "2025-07",
        "title": "IConMark: Robust Interpretable Concept-Based Watermark For AI Images",
        "author": "Vinu Sankar Sadasivan, Mehrdad Saberi, and Soheil Feizi",
        "link": "http://arxiv.org/abs/2507.13407v1",
        "abstract": "With the rapid rise of generative AI and synthetic media, distinguishing\nAI-generated images from real ones has become crucial in safeguarding against\nmisinformation and ensuring digital authenticity. Traditional watermarking\ntechniques have shown vulnerabilities to adversarial attacks, undermining their\neffectiveness in the presence of attackers. We propose IConMark, a novel\nin-generation robust semantic watermarking method that embeds interpretable\nconcepts into AI-generated images, as a first step toward interpretable\nwatermarking. Unlike traditional methods, which rely on adding noise or\nperturbations to AI-generated images, IConMark incorporates meaningful semantic\nattributes, making it interpretable to humans and hence, resilient to\nadversarial manipulation. This method is not only robust against various image\naugmentations but also human-readable, enabling manual verification of\nwatermarks. We demonstrate a detailed evaluation of IConMark's effectiveness,\ndemonstrating its superiority in terms of detection accuracy and maintaining\nimage quality. Moreover, IConMark can be combined with existing watermarking\ntechniques to further enhance and complement its robustness. We introduce\nIConMark+SS and IConMark+TM, hybrid approaches combining IConMark with\nStegaStamp and TrustMark, respectively, to further bolster robustness against\nmultiple types of image manipulations. Our base watermarking technique\n(IConMark) and its variants (+TM and +SS) achieve 10.8%, 14.5%, and 15.9%\nhigher mean area under the receiver operating characteristic curve (AUROC)\nscores for watermark detection, respectively, compared to the best baseline on\nvarious datasets."
    },
    {
        "date": "2025-07",
        "title": "Multimodal-Guided Dynamic Dataset Pruning for Robust and Efficient Data-Centric Learning",
        "author": "Suorong Yang, Peijia Li, Yujie Liu, Zhiming Xu, Peng Ye, Wanli Ouyang, Furao Shen, and Dongzhan Zhou",
        "link": "http://arxiv.org/abs/2507.12750v1",
        "abstract": "Modern deep models are trained on large real-world datasets, where data\nquality varies and redundancy is common. Data-centric approaches such as\ndataset pruning have shown promise in improving training efficiency and model\nperformance. However, most existing methods rely on static heuristics or\ntask-specific metrics, limiting their robustness and generalizability across\ndomains. In this work, we introduce a dynamic dataset pruning framework that\nadaptively selects training samples based on both task-driven difficulty and\ncross-modality semantic consistency. By incorporating supervision from\npretrained multimodal foundation models, our approach captures training\ndynamics while effectively filtering out uninformative samples. Our work\nhighlights the potential of integrating cross-modality alignment for robust\nsample selection, advancing data-centric learning toward more efficient and\nrobust practices across application domains."
    },
    {
        "date": "2025-07",
        "title": "A Novel Data Augmentation Strategy for Robust Deep Learning Classification of Biomedical Time-Series Data: Application to ECG and EEG Analysis",
        "author": "Mohammed Guhdar, Ramadhan J. Mstafa, and Abdulhakeem O. Mohammed",
        "link": "http://arxiv.org/abs/2507.12645v1",
        "abstract": "The increasing need for accurate and unified analysis of diverse biological\nsignals, such as ECG and EEG, is paramount for comprehensive patient\nassessment, especially in synchronous monitoring. Despite advances in\nmulti-sensor fusion, a critical gap remains in developing unified architectures\nthat effectively process and extract features from fundamentally different\nphysiological signals. Another challenge is the inherent class imbalance in\nmany biomedical datasets, often causing biased performance in traditional\nmethods. This study addresses these issues by proposing a novel and unified\ndeep learning framework that achieves state-of-the-art performance across\ndifferent signal types. Our method integrates a ResNet-based CNN with an\nattention mechanism, enhanced by a novel data augmentation strategy:\ntime-domain concatenation of multiple augmented variants of each signal to\ngenerate richer representations. Unlike prior work, we scientifically increase\nsignal complexity to achieve future-reaching capabilities, which resulted in\nthe best predictions compared to the state of the art. Preprocessing steps\nincluded wavelet denoising, baseline removal, and standardization. Class\nimbalance was effectively managed through the combined use of this advanced\ndata augmentation and the Focal Loss function. Regularization techniques were\napplied during training to ensure generalization. We rigorously evaluated the\nproposed architecture on three benchmark datasets: UCI Seizure EEG, MIT-BIH\nArrhythmia, and PTB Diagnostic ECG. It achieved accuracies of 99.96%, 99.78%,\nand 100%, respectively, demonstrating robustness across diverse signal types\nand clinical contexts. Finally, the architecture requires ~130 MB of memory and\nprocesses each sample in ~10 ms, suggesting suitability for deployment on\nlow-end or wearable devices."
    },
    {
        "date": "2025-07",
        "title": "Achieving Robust Channel Estimation Neural Networks by Designed Training Data",
        "author": "Dianxin Luan, and John Thompson",
        "link": "http://arxiv.org/abs/2507.12630v2",
        "abstract": "Channel estimation is crucial in wireless communications. However, in many\npapers neural networks are frequently tested by training and testing on one\nexample channel or similar channels. This is because data-driven methods often\ndegrade on new data which they are not trained on, as they cannot extrapolate\ntheir training knowledge. This is despite the fact physical channels are often\nassumed to be time-variant. However, due to the low latency requirements and\nlimited computing resources, neural networks may not have enough time and\ncomputing resources to execute online training to fine-tune the parameters.\nThis motivates us to design offline-trained neural networks that can perform\nrobustly over wireless channels, but without any actual channel information\nbeing known at design time. In this paper, we propose design criteria to\ngenerate synthetic training datasets for neural networks, which guarantee that\nafter training the resulting networks achieve a certain mean squared error\n(MSE) on new and previously unseen channels. Therefore, trained neural networks\nrequire no prior channel information or parameters update for real-world\nimplementations. Based on the proposed design criteria, we further propose a\nbenchmark design which ensures intelligent operation for different channel\nprofiles. To demonstrate general applicability, we use neural networks with\ndifferent levels of complexity to show that the generalization achieved appears\nto be independent of neural network architecture. From simulations, neural\nnetworks achieve robust generalization to wireless channels with both fixed\nchannel profiles and variable delay spreads."
    },
    {
        "date": "2025-07",
        "title": "Targeted Deep Architectures: A TMLE-Based Framework for Robust Causal Inference in Neural Networks",
        "author": "Yi Li, David Mccoy, Nolan Gunter, Kaitlyn Lee, Alejandro Schuler, and Mark van der Laan",
        "link": "http://arxiv.org/abs/2507.12435v1",
        "abstract": "Modern deep neural networks are powerful predictive tools yet often lack\nvalid inference for causal parameters, such as treatment effects or entire\nsurvival curves. While frameworks like Double Machine Learning (DML) and\nTargeted Maximum Likelihood Estimation (TMLE) can debias machine-learning fits,\nexisting neural implementations either rely on \"targeted losses\" that do not\nguarantee solving the efficient influence function equation or computationally\nexpensive post-hoc \"fluctuations\" for multi-parameter settings. We propose\nTargeted Deep Architectures (TDA), a new framework that embeds TMLE directly\ninto the network's parameter space with no restrictions on the backbone\narchitecture. Specifically, TDA partitions model parameters - freezing all but\na small \"targeting\" subset - and iteratively updates them along a targeting\ngradient, derived from projecting the influence functions onto the span of the\ngradients of the loss with respect to weights. This procedure yields plug-in\nestimates that remove first-order bias and produce asymptotically valid\nconfidence intervals. Crucially, TDA easily extends to multi-dimensional causal\nestimands (e.g., entire survival curves) by merging separate targeting\ngradients into a single universal targeting update. Theoretically, TDA inherits\nclassical TMLE properties, including double robustness and semiparametric\nefficiency. Empirically, on the benchmark IHDP dataset (average treatment\neffects) and simulated survival data with informative censoring, TDA reduces\nbias and improves coverage relative to both standard neural-network estimators\nand prior post-hoc approaches. In doing so, TDA establishes a direct, scalable\npathway toward rigorous causal inference within modern deep architectures for\ncomplex multi-parameter targets."
    },
    {
        "date": "2025-07",
        "title": "Thought Purity: Defense Paradigm For Chain-of-Thought Attack",
        "author": "Zihao Xue, Zhen Bi, Long Ma, Zhenlin Hu, Yan Wang, Zhenfang Liu, Qing Sheng, Jie Xiao, and Jungang Lou",
        "link": "http://arxiv.org/abs/2507.12314v1",
        "abstract": "While reinforcement learning-trained Large Reasoning Models (LRMs, e.g.,\nDeepseek-R1) demonstrate advanced reasoning capabilities in the evolving Large\nLanguage Models (LLMs) domain, their susceptibility to security threats remains\na critical vulnerability. This weakness is particularly evident in\nChain-of-Thought (CoT) generation processes, where adversarial methods like\nbackdoor prompt attacks can systematically subvert the model's core reasoning\nmechanisms. The emerging Chain-of-Thought Attack (CoTA) reveals this\nvulnerability through exploiting prompt controllability, simultaneously\ndegrading both CoT safety and task performance with low-cost interventions. To\naddress this compounded security-performance vulnerability, we propose Thought\nPurity (TP): a defense paradigm that systematically strengthens resistance to\nmalicious content while preserving operational efficacy. Our solution achieves\nthis through three synergistic components: (1) a safety-optimized data\nprocessing pipeline (2) reinforcement learning-enhanced rule constraints (3)\nadaptive monitoring metrics. Our approach establishes the first comprehensive\ndefense mechanism against CoTA vulnerabilities in reinforcement\nlearning-aligned reasoning systems, significantly advancing the\nsecurity-functionality equilibrium for next-generation AI architectures."
    },
    {
        "date": "2025-07",
        "title": "FADE: Adversarial Concept Erasure in Flow Models",
        "author": "Zixuan Fu, Yan Ren, Finn Carter, Chenyue Wang, Ze Niu, Dacheng Yu, Emily Davis, and Bo Zhang",
        "link": "http://arxiv.org/abs/2507.12283v1",
        "abstract": "Diffusion models have demonstrated remarkable image generation capabilities,\nbut also pose risks in privacy and fairness by memorizing sensitive concepts or\nperpetuating biases. We propose a novel \\textbf{concept erasure} method for\ntext-to-image diffusion models, designed to remove specified concepts (e.g., a\nprivate individual or a harmful stereotype) from the model's generative\nrepertoire. Our method, termed \\textbf{FADE} (Fair Adversarial Diffusion\nErasure), combines a trajectory-aware fine-tuning strategy with an adversarial\nobjective to ensure the concept is reliably removed while preserving overall\nmodel fidelity. Theoretically, we prove a formal guarantee that our approach\nminimizes the mutual information between the erased concept and the model's\noutputs, ensuring privacy and fairness. Empirically, we evaluate FADE on Stable\nDiffusion and FLUX, using benchmarks from prior work (e.g., object, celebrity,\nexplicit content, and style erasure tasks from MACE). FADE achieves\nstate-of-the-art concept removal performance, surpassing recent baselines like\nESD, UCE, MACE, and ANT in terms of removal efficacy and image quality.\nNotably, FADE improves the harmonic mean of concept removal and fidelity by\n5--10\\% over the best prior method. We also conduct an ablation study to\nvalidate each component of FADE, confirming that our adversarial and\ntrajectory-preserving objectives each contribute to its superior performance.\nOur work sets a new standard for safe and fair generative modeling by\nunlearning specified concepts without retraining from scratch."
    },
    {
        "date": "2025-07",
        "title": "Site-Level Fine-Tuning with Progressive Layer Freezing: Towards Robust Prediction of Bronchopulmonary Dysplasia from Day-1 Chest Radiographs in Extremely Preterm Infants",
        "author": "Sybelle Goedicke-Fritz, Michelle Bous, Annika Engel, Matthias Flotho, Pascal Hirsch, Hannah Wittig, Dino Milanovic, Dominik Mohr, Mathias Kaspar, Sogand Nemat, Dorothea Kerner, Arno B\u00fccker, Andreas Keller, Sascha Meyer, Michael Zemlin, and Philipp Flotho",
        "link": "http://arxiv.org/abs/2507.12269v2",
        "abstract": "Bronchopulmonary dysplasia (BPD) is a chronic lung disease affecting 35% of\nextremely low birth weight infants. Defined by oxygen dependence at 36 weeks\npostmenstrual age, it causes lifelong respiratory complications. However,\npreventive interventions carry severe risks, including neurodevelopmental\nimpairment, ventilator-induced lung injury, and systemic complications.\nTherefore, early BPD prognosis and prediction of BPD outcome is crucial to\navoid unnecessary toxicity in low risk infants. Admission radiographs of\nextremely preterm infants are routinely acquired within 24h of life and could\nserve as a non-invasive prognostic tool. In this work, we developed and\ninvestigated a deep learning approach using chest X-rays from 163 extremely\nlow-birth-weight infants ($\\leq$32 weeks gestation, 401-999g) obtained within\n24 hours of birth. We fine-tuned a ResNet-50 pretrained specifically on adult\nchest radiographs, employing progressive layer freezing with discriminative\nlearning rates to prevent overfitting and evaluated a CutMix augmentation and\nlinear probing. For moderate/severe BPD outcome prediction, our best performing\nmodel with progressive freezing, linear probing and CutMix achieved an AUROC of\n0.78 $\\pm$ 0.10, balanced accuracy of 0.69 $\\pm$ 0.10, and an F1-score of 0.67\n$\\pm$ 0.11. In-domain pre-training significantly outperformed ImageNet\ninitialization (p = 0.031) which confirms domain-specific pretraining to be\nimportant for BPD outcome prediction. Routine IRDS grades showed limited\nprognostic value (AUROC 0.57 $\\pm$ 0.11), confirming the need of learned\nmarkers. Our approach demonstrates that domain-specific pretraining enables\naccurate BPD prediction from routine day-1 radiographs. Through progressive\nfreezing and linear probing, the method remains computationally feasible for\nsite-level implementation and future federated learning deployments."
    },
    {
        "date": "2025-07",
        "title": "Robust Causal Discovery in Real-World Time Series with Power-Laws",
        "author": "Matteo Tusoni, Giuseppe Masi, Andrea Coletta, Aldo Glielmo, Viviana Arrigoni, and Novella Bartolini",
        "link": "http://arxiv.org/abs/2507.12257v1",
        "abstract": "Exploring causal relationships in stochastic time series is a challenging yet\ncrucial task with a vast range of applications, including finance, economics,\nneuroscience, and climate science. Many algorithms for Causal Discovery (CD)\nhave been proposed, but they often exhibit a high sensitivity to noise,\nresulting in misleading causal inferences when applied to real data. In this\npaper, we observe that the frequency spectra of typical real-world time series\nfollow a power-law distribution, notably due to an inherent self-organizing\nbehavior. Leveraging this insight, we build a robust CD method based on the\nextraction of power -law spectral features that amplify genuine causal signals.\nOur method consistently outperforms state-of-the-art alternatives on both\nsynthetic benchmarks and real-world datasets with known causal structures,\ndemonstrating its robustness and practical relevance."
    },
    {
        "date": "2025-07",
        "title": "RODS: Robust Optimization Inspired Diffusion Sampling for Detecting and Reducing Hallucination in Generative Models",
        "author": "Yiqi Tian, Pengfei Jin, Mingze Yuan, Na Li, Bo Zeng, and Quanzheng Li",
        "link": "http://arxiv.org/abs/2507.12201v1",
        "abstract": "Diffusion models have achieved state-of-the-art performance in generative\nmodeling, yet their sampling procedures remain vulnerable to hallucinations,\noften stemming from inaccuracies in score approximation. In this work, we\nreinterpret diffusion sampling through the lens of optimization and introduce\nRODS (Robust Optimization-inspired Diffusion Sampler), a novel method that\ndetects and corrects high-risk sampling steps using geometric cues from the\nloss landscape. RODS enforces smoother sampling trajectories and adaptively\nadjusts perturbations, reducing hallucinations without retraining and at\nminimal additional inference cost. Experiments on AFHQv2, FFHQ, and 11k-hands\ndemonstrate that RODS improves both sampling fidelity and robustness, detecting\nover 70% of hallucinated samples and correcting more than 25%, all while\navoiding the introduction of new artifacts."
    },
    {
        "date": "2025-07",
        "title": "Exploiting Jailbreaking Vulnerabilities in Generative AI to Bypass Ethical Safeguards for Facilitating Phishing Attacks",
        "author": "Rina Mishra, and Gaurav Varshney",
        "link": "http://arxiv.org/abs/2507.12185v1",
        "abstract": "The advent of advanced Generative AI (GenAI) models such as DeepSeek and\nChatGPT has significantly reshaped the cybersecurity landscape, introducing\nboth promising opportunities and critical risks. This study investigates how\nGenAI powered chatbot services can be exploited via jailbreaking techniques to\nbypass ethical safeguards, enabling the generation of phishing content,\nrecommendation of hacking tools, and orchestration of phishing campaigns. In\nethically controlled experiments, we used ChatGPT 4o Mini selected for its\naccessibility and status as the latest publicly available model at the time of\nexperimentation, as a representative GenAI system. Our findings reveal that the\nmodel could successfully guide novice users in executing phishing attacks\nacross various vectors, including web, email, SMS (smishing), and voice\n(vishing). Unlike automated phishing campaigns that typically follow detectable\npatterns, these human-guided, AI assisted attacks are capable of evading\ntraditional anti phishing mechanisms, thereby posing a growing security threat.\nWe focused on DeepSeek and ChatGPT due to their widespread adoption and\ntechnical relevance in 2025. The study further examines common jailbreaking\ntechniques and the specific vulnerabilities exploited in these models. Finally,\nwe evaluate a range of mitigation strategies such as user education, advanced\nauthentication mechanisms, and regulatory policy measures and discuss emerging\ntrends in GenAI facilitated phishing, outlining future research directions to\nstrengthen cybersecurity defenses in the age of artificial intelligence."
    },
    {
        "date": "2025-07",
        "title": "DoRF: Doppler Radiance Fields for Robust Human Activity Recognition Using Wi-Fi",
        "author": "Navid Hasanzadeh, and Shahrokh Valaee",
        "link": "http://arxiv.org/abs/2507.12132v1",
        "abstract": "Wi-Fi Channel State Information (CSI) has gained increasing interest for\nremote sensing applications. Recent studies show that Doppler velocity\nprojections extracted from CSI can enable human activity recognition (HAR) that\nis robust to environmental changes and generalizes to new users. However,\ndespite these advances, generalizability still remains insufficient for\npractical deployment. Inspired by neural radiance fields (NeRF), which learn a\nvolumetric representation of a 3D scene from 2D images, this work proposes a\nnovel approach to reconstruct an informative 3D latent motion representation\nfrom one-dimensional Doppler velocity projections extracted from Wi-Fi CSI. The\nresulting latent representation is then used to construct a uniform Doppler\nradiance field (DoRF) of the motion, providing a comprehensive view of the\nperformed activity and improving the robustness to environmental variability.\nThe results show that the proposed approach noticeably enhances the\ngeneralization accuracy of Wi-Fi-based HAR, highlighting the strong potential\nof DoRFs for practical sensing applications."
    },
    {
        "date": "2025-07",
        "title": "Self-Adaptive and Robust Federated Spectrum Sensing without Benign Majority for Cellular Networks",
        "author": "Ngoc Duy Pham, Thusitha Dayaratne, Viet Vo, Shangqi Lai, Sharif Abuadbba, Hajime Suzuki, Xingliang Yuan, and Carsten Rudolph",
        "link": "http://arxiv.org/abs/2507.12127v1",
        "abstract": "Advancements in wireless and mobile technologies, including 5G advanced and\nthe envisioned 6G, are driving exponential growth in wireless devices. However,\nthis rapid expansion exacerbates spectrum scarcity, posing a critical\nchallenge. Dynamic spectrum allocation (DSA)--which relies on sensing and\ndynamically sharing spectrum--has emerged as an essential solution to address\nthis issue. While machine learning (ML) models hold significant potential for\nimproving spectrum sensing, their adoption in centralized ML-based DSA systems\nis limited by privacy concerns, bandwidth constraints, and regulatory\nchallenges. To overcome these limitations, distributed ML-based approaches such\nas Federated Learning (FL) offer promising alternatives. This work addresses\ntwo key challenges in FL-based spectrum sensing (FLSS). First, the scarcity of\nlabeled data for training FL models in practical spectrum sensing scenarios is\ntackled with a semi-supervised FL approach, combined with energy detection,\nenabling model training on unlabeled datasets. Second, we examine the security\nvulnerabilities of FLSS, focusing on the impact of data poisoning attacks. Our\nanalysis highlights the shortcomings of existing majority-based defenses in\ncountering such attacks. To address these vulnerabilities, we propose a novel\ndefense mechanism inspired by vaccination, which effectively mitigates data\npoisoning attacks without relying on majority-based assumptions. Extensive\nexperiments on both synthetic and real-world datasets validate our solutions,\ndemonstrating that FLSS can achieve near-perfect accuracy on unlabeled datasets\nand maintain Byzantine robustness against both targeted and untargeted data\npoisoning attacks, even when a significant proportion of participants are\nmalicious."
    },
    {
        "date": "2025-07",
        "title": "Non-Adaptive Adversarial Face Generation",
        "author": "Sunpill Kim, Seunghun Paik, Chanwoo Hwang, Minsu Kim, and Jae Hong Seo",
        "link": "http://arxiv.org/abs/2507.12107v1",
        "abstract": "Adversarial attacks on face recognition systems (FRSs) pose serious security\nand privacy threats, especially when these systems are used for identity\nverification. In this paper, we propose a novel method for generating\nadversarial faces-synthetic facial images that are visually distinct yet\nrecognized as a target identity by the FRS. Unlike iterative optimization-based\napproaches (e.g., gradient descent or other iterative solvers), our method\nleverages the structural characteristics of the FRS feature space. We figure\nout that individuals sharing the same attribute (e.g., gender or race) form an\nattributed subsphere. By utilizing such subspheres, our method achieves both\nnon-adaptiveness and a remarkably small number of queries. This eliminates the\nneed for relying on transferability and open-source surrogate models, which\nhave been a typical strategy when repeated adaptive queries to commercial FRSs\nare impossible. Despite requiring only a single non-adaptive query consisting\nof 100 face images, our method achieves a high success rate of over 93% against\nAWS's CompareFaces API at its default threshold. Furthermore, unlike many\nexisting attacks that perturb a given image, our method can deliberately\nproduce adversarial faces that impersonate the target identity while exhibiting\nhigh-level attributes chosen by the adversary."
    },
    {
        "date": "2025-07",
        "title": "BRUM: Robust 3D Vehicle Reconstruction from 360 Sparse Images",
        "author": "Davide Di Nucci, Matteo Tomei, Guido Borghi, Luca Ciuffreda, Roberto Vezzani, and Rita Cucchiara",
        "link": "http://arxiv.org/abs/2507.12095v1",
        "abstract": "Accurate 3D reconstruction of vehicles is vital for applications such as\nvehicle inspection, predictive maintenance, and urban planning. Existing\nmethods like Neural Radiance Fields and Gaussian Splatting have shown\nimpressive results but remain limited by their reliance on dense input views,\nwhich hinders real-world applicability. This paper addresses the challenge of\nreconstructing vehicles from sparse-view inputs, leveraging depth maps and a\nrobust pose estimation architecture to synthesize novel views and augment\ntraining data. Specifically, we enhance Gaussian Splatting by integrating a\nselective photometric loss, applied only to high-confidence pixels, and\nreplacing standard Structure-from-Motion pipelines with the DUSt3R architecture\nto improve camera pose estimation. Furthermore, we present a novel dataset\nfeaturing both synthetic and real-world public transportation vehicles,\nenabling extensive evaluation of our approach. Experimental results demonstrate\nstate-of-the-art performance across multiple benchmarks, showcasing the\nmethod's ability to achieve high-quality reconstructions even under constrained\ninput conditions."
    },
    {
        "date": "2025-07",
        "title": "YOLOv8-SMOT: An Efficient and Robust Framework for Real-Time Small Object Tracking via Slice-Assisted Training and Adaptive Association",
        "author": "Xiang Yu, Xinyao Liu, and Guang Liang",
        "link": "http://arxiv.org/abs/2507.12087v2",
        "abstract": "Tracking small, agile multi-objects (SMOT), such as birds, from an Unmanned\nAerial Vehicle (UAV) perspective is a highly challenging computer vision task.\nThe difficulty stems from three main sources: the extreme scarcity of target\nappearance features, the complex motion entanglement caused by the combined\ndynamics of the camera and the targets themselves, and the frequent occlusions\nand identity ambiguity arising from dense flocking behavior. This paper details\nour championship-winning solution in the MVA 2025 \"Finding Birds\" Small\nMulti-Object Tracking Challenge (SMOT4SB), which adopts the\ntracking-by-detection paradigm with targeted innovations at both the detection\nand association levels. On the detection side, we propose a systematic training\nenhancement framework named \\textbf{SliceTrain}. This framework, through the\nsynergy of 'deterministic full-coverage slicing' and 'slice-level stochastic\naugmentation, effectively addresses the problem of insufficient learning for\nsmall objects in high-resolution image training. On the tracking side, we\ndesigned a robust tracker that is completely independent of appearance\ninformation. By integrating a \\textbf{motion direction maintenance (EMA)}\nmechanism and an \\textbf{adaptive similarity metric} combining \\textbf{bounding\nbox expansion and distance penalty} into the OC-SORT framework, our tracker can\nstably handle irregular motion and maintain target identities. Our method\nachieves state-of-the-art performance on the SMOT4SB public test set, reaching\nan SO-HOTA score of \\textbf{55.205}, which fully validates the effectiveness\nand advancement of our framework in solving complex real-world SMOT problems.\nThe source code will be made available at\nhttps://github.com/Salvatore-Love/YOLOv8-SMOT."
    },
    {
        "date": "2025-07",
        "title": "Toward an Intent-Based and Ontology-Driven Autonomic Security Response in Security Orchestration Automation and Response",
        "author": "Zequan Huang, Jacques Robin, Nicolas Herbaut, Nourh\u00e8ne Ben Rabah, and B\u00e9n\u00e9dicte Le Grand",
        "link": "http://arxiv.org/abs/2507.12061v1",
        "abstract": "Modern Security Orchestration, Automation, and Response (SOAR) platforms must\nrapidly adapt to continuously evolving cyber attacks. Intent-Based Networking\nhas emerged as a promising paradigm for cyber attack mitigation through\nhigh-level declarative intents, which offer greater flexibility and persistency\nthan procedural actions. In this paper, we bridge the gap between two active\nresearch directions: Intent-Based Cyber Defense and Autonomic Cyber Defense, by\nproposing a unified, ontology-driven security intent definition leveraging the\nMITRE-D3FEND cybersecurity ontology. We also propose a general two-tiered\nmethodology for integrating such security intents into decision-theoretic\nAutonomic Cyber Defense systems, enabling hierarchical and context-aware\nautomated response capabilities. The practicality of our approach is\ndemonstrated through a concrete use case, showcasing its integration within\nnext-generation Security Orchestration, Automation, and Response platforms."
    },
    {
        "date": "2025-07",
        "title": "IDFace: Face Template Protection for Efficient and Secure Identification",
        "author": "Sunpill Kim, Seunghun Paik, Chanwoo Hwang, Dongsoo Kim, Junbum Shin, and Jae Hong Seo",
        "link": "http://arxiv.org/abs/2507.12050v1",
        "abstract": "As face recognition systems (FRS) become more widely used, user privacy\nbecomes more important. A key privacy issue in FRS is protecting the user's\nface template, as the characteristics of the user's face image can be recovered\nfrom the template. Although recent advances in cryptographic tools such as\nhomomorphic encryption (HE) have provided opportunities for securing the FRS,\nHE cannot be used directly with FRS in an efficient plug-and-play manner. In\nparticular, although HE is functionally complete for arbitrary programs, it is\nbasically designed for algebraic operations on encrypted data of predetermined\nshape, such as a polynomial ring. Thus, a non-tailored combination of HE and\nthe system can yield very inefficient performance, and many previous HE-based\nface template protection methods are hundreds of times slower than plain\nsystems without protection. In this study, we propose IDFace, a new HE-based\nsecure and efficient face identification method with template protection.\nIDFace is designed on the basis of two novel techniques for efficient searching\non a (homomorphically encrypted) biometric database with an angular metric. The\nfirst technique is a template representation transformation that sharply\nreduces the unit cost for the matching test. The second is a space-efficient\nencoding that reduces wasted space from the encryption algorithm, thus saving\nthe number of operations on encrypted templates. Through experiments, we show\nthat IDFace can identify a face template from among a database of 1M encrypted\ntemplates in 126ms, showing only 2X overhead compared to the identification\nover plaintexts."
    },
    {
        "date": "2025-07",
        "title": "Expanding ML-Documentation Standards For Better Security",
        "author": "Cara Ellen Appel",
        "link": "http://arxiv.org/abs/2507.12003v1",
        "abstract": "This article presents the current state of ML-security and of the\ndocumentation of ML-based systems, models and datasets in research and practice\nbased on an extensive review of the existing literature. It shows a generally\nlow awareness of security aspects among ML-practitioners and organizations and\nan often unstandardized approach to documentation, leading to overall low\nquality of ML-documentation. Existing standards are not regularly adopted in\npractice and IT-security aspects are often not included in documentation. Due\nto these factors, there is a clear need for improved security documentation in\nML, as one step towards addressing the existing gaps in ML-security. To achieve\nthis, we propose expanding existing documentation standards for\nML-documentation to include a security section with specific security relevant\ninformation. Implementing this, a novel expanded method of documenting security\nrequirements in ML-documentation is presented, based on the existing Model\nCards and Datasheets for Datasets standards, but with the recommendation to\nadopt these findings in all ML-documentation."
    },
    {
        "date": "2025-07",
        "title": "Robust Planning for Autonomous Vehicles with Diffusion-Based Failure Samplers",
        "author": "Juanran Wang, Marc R. Schlichting, and Mykel J. Kochenderfer",
        "link": "http://arxiv.org/abs/2507.11991v1",
        "abstract": "High-risk traffic zones such as intersections are a major cause of\ncollisions. This study leverages deep generative models to enhance the safety\nof autonomous vehicles in an intersection context. We train a 1000-step\ndenoising diffusion probabilistic model to generate collision-causing sensor\nnoise sequences for an autonomous vehicle navigating a four-way intersection\nbased on the current relative position and velocity of an intruder. Using the\ngenerative adversarial architecture, the 1000-step model is distilled into a\nsingle-step denoising diffusion model which demonstrates fast inference speed\nwhile maintaining similar sampling quality. We demonstrate one possible\napplication of the single-step model in building a robust planner for the\nautonomous vehicle. The planner uses the single-step model to efficiently\nsample potential failure cases based on the currently measured traffic state to\ninform its decision-making. Through simulation experiments, the robust planner\ndemonstrates significantly lower failure rate and delay rate compared with the\nbaseline Intelligent Driver Model controller."
    },
    {
        "date": "2025-07",
        "title": "Watch, Listen, Understand, Mislead: Tri-modal Adversarial Attacks on Short Videos for Content Appropriateness Evaluation",
        "author": "Sahid Hossain Mustakim, S M Jishanul Islam, Ummay Maria Muna, Montasir Chowdhury, Mohammed Jawwadul Islam, Sadia Ahmmed, Tashfia Sikder, Syed Tasdid Azam Dhrubo, and Swakkhar Shatabda",
        "link": "http://arxiv.org/abs/2507.11968v1",
        "abstract": "Multimodal Large Language Models (MLLMs) are increasingly used for content\nmoderation, yet their robustness in short-form video contexts remains\nunderexplored. Current safety evaluations often rely on unimodal attacks,\nfailing to address combined attack vulnerabilities. In this paper, we introduce\na comprehensive framework for evaluating the tri-modal safety of MLLMs. First,\nwe present the Short-Video Multimodal Adversarial (SVMA) dataset, comprising\ndiverse short-form videos with human-guided synthetic adversarial attacks.\nSecond, we propose ChimeraBreak, a novel tri-modal attack strategy that\nsimultaneously challenges visual, auditory, and semantic reasoning pathways.\nExtensive experiments on state-of-the-art MLLMs reveal significant\nvulnerabilities with high Attack Success Rates (ASR). Our findings uncover\ndistinct failure modes, showing model biases toward misclassifying benign or\npolicy-violating content. We assess results using LLM-as-a-judge, demonstrating\nattack reasoning efficacy. Our dataset and findings provide crucial insights\nfor developing more robust and safe MLLMs."
    },
    {
        "date": "2025-07",
        "title": "How To Mitigate And Defend Against DDoS Attacks In IoT Devices",
        "author": "Ifiyemi Leigha, Basak Comlekcioglu, and Maria Pilar Bezanilla",
        "link": "http://arxiv.org/abs/2507.11772v1",
        "abstract": "Distributed Denial of Service (DDoS) attacks have become increasingly\nprevalent and dangerous in the context of Internet of Things (IoT) networks,\nprimarily due to the low-security configurations of many connected devices.\nThis paper analyzes the nature and impact of DDoS attacks such as those\nlaunched by the Mirai botnet, and proposes layered mitigation strategies\ntailored to IoT environments. Key solutions explored include IPv6 Unique Local\nAddresses (ULA), edge computing, software-defined networking (SDN), honeypot\ndeception, and machine learning-based intrusion detection systems. The paper\naims to help engineers and researchers understand and implement practical\ncountermeasures to protect IoT infrastructures."
    },
    {
        "date": "2025-07",
        "title": "Reinforcement Learning from Adversarial Preferences in Tabular MDPs",
        "author": "Taira Tsuchiya, Shinji Ito, and Haipeng Luo",
        "link": "http://arxiv.org/abs/2507.11706v1",
        "abstract": "We introduce a new framework of episodic tabular Markov decision processes\n(MDPs) with adversarial preferences, which we refer to as preference-based MDPs\n(PbMDPs). Unlike standard episodic MDPs with adversarial losses, where the\nnumerical value of the loss is directly observed, in PbMDPs the learner instead\nobserves preferences between two candidate arms, which represent the choices\nbeing compared. In this work, we focus specifically on the setting where the\nreward functions are determined by Borda scores. We begin by establishing a\nregret lower bound for PbMDPs with Borda scores. As a preliminary step, we\npresent a simple instance to prove a lower bound of $\\Omega(\\sqrt{HSAT})$ for\nepisodic MDPs with adversarial losses, where $H$ is the number of steps per\nepisode, $S$ is the number of states, $A$ is the number of actions, and $T$ is\nthe number of episodes. Leveraging this construction, we then derive a regret\nlower bound of $\\Omega( (H^2 S K)^{1/3} T^{2/3} )$ for PbMDPs with Borda\nscores, where $K$ is the number of arms. Next, we develop algorithms that\nachieve a regret bound of order $T^{2/3}$. We first propose a global\noptimization approach based on online linear optimization over the set of all\noccupancy measures, achieving a regret bound of $\\tilde{O}((H^2 S^2 K)^{1/3}\nT^{2/3} )$ under known transitions. However, this approach suffers from\nsuboptimal dependence on the potentially large number of states $S$ and\ncomputational inefficiency. To address this, we propose a policy optimization\nalgorithm whose regret is roughly bounded by $\\tilde{O}( (H^6 S K^5)^{1/3}\nT^{2/3} )$ under known transitions, and further extend the result to the\nunknown-transition setting."
    },
    {
        "date": "2025-07",
        "title": "The Impact of Coreset Selection on Spurious Correlations and Group Robustness",
        "author": "Amaya Dharmasiri, William Yang, Polina Kirichenko, Lydia Liu, and Olga Russakovsky",
        "link": "http://arxiv.org/abs/2507.11690v1",
        "abstract": "Coreset selection methods have shown promise in reducing the training data\nsize while maintaining model performance for data-efficient machine learning.\nHowever, as many datasets suffer from biases that cause models to learn\nspurious correlations instead of causal features, it is important to understand\nwhether and how dataset reduction methods may perpetuate, amplify, or mitigate\nthese biases. In this work, we conduct the first comprehensive analysis of the\nimplications of data selection on the spurious bias levels of the selected\ncoresets and the robustness of downstream models trained on them. We use an\nextensive experimental setting spanning ten different spurious correlations\nbenchmarks, five score metrics to characterize sample importance/ difficulty,\nand five data selection policies across a broad range of coreset sizes.\nThereby, we unravel a series of nontrivial nuances in interactions between\nsample difficulty and bias alignment, as well as dataset bias and resultant\nmodel robustness. For example, we find that selecting coresets using\nembedding-based sample characterization scores runs a comparatively lower risk\nof inadvertently exacerbating bias than selecting using characterizations based\non learning dynamics. Most importantly, our analysis reveals that although some\ncoreset selection methods could achieve lower bias levels by prioritizing\ndifficult samples, they do not reliably guarantee downstream robustness."
    },
    {
        "date": "2025-07",
        "title": "Exploring the robustness of TractOracle methods in RL-based tractography",
        "author": "Jeremi Levesque, Antoine Th\u00e9berge, Maxime Descoteaux, and Pierre-Marc Jodoin",
        "link": "http://arxiv.org/abs/2507.11486v1",
        "abstract": "Tractography algorithms leverage diffusion MRI to reconstruct the fibrous\narchitecture of the brain's white matter. Among machine learning approaches,\nreinforcement learning (RL) has emerged as a promising framework for\ntractography, outperforming traditional methods in several key aspects.\nTractOracle-RL, a recent RL-based approach, reduces false positives by\nincorporating anatomical priors into the training process via a reward-based\nmechanism. In this paper, we investigate four extensions of the original\nTractOracle-RL framework by integrating recent advances in RL, and we evaluate\ntheir performance across five diverse diffusion MRI datasets. Results\ndemonstrate that combining an oracle with the RL framework consistently leads\nto robust and reliable tractography, regardless of the specific method or\ndataset used. We also introduce a novel RL training scheme called Iterative\nReward Training (IRT), inspired by the Reinforcement Learning from Human\nFeedback (RLHF) paradigm. Instead of relying on human input, IRT leverages\nbundle filtering methods to iteratively refine the oracle's guidance throughout\ntraining. Experimental results show that RL methods trained with oracle\nfeedback significantly outperform widely used tractography techniques in terms\nof accuracy and anatomical validity."
    },
    {
        "date": "2025-07",
        "title": "D3FL: Data Distribution and Detrending for Robust Federated Learning in Non-linear Time-series Data",
        "author": "Harsha Varun Marisetty, Manik Gupta, and Yogesh Simmhan",
        "link": "http://arxiv.org/abs/2507.11471v1",
        "abstract": "With advancements in computing and communication technologies, the Internet\nof Things (IoT) has seen significant growth. IoT devices typically collect data\nfrom various sensors, such as temperature, humidity, and energy meters. Much of\nthis data is temporal in nature. Traditionally, data from IoT devices is\ncentralized for analysis, but this approach introduces delays and increased\ncommunication costs. Federated learning (FL) has emerged as an effective\nalternative, allowing for model training across distributed devices without the\nneed to centralize data. In many applications, such as smart home energy and\nenvironmental monitoring, the data collected by IoT devices across different\nlocations can exhibit significant variation in trends and seasonal patterns.\nAccurately forecasting such non-stationary, non-linear time-series data is\ncrucial for applications like energy consumption estimation and weather\nforecasting. However, these data variations can severely impact prediction\naccuracy. The key contributions of this paper are: (1) Investigating how\nnon-linear, non-stationary time-series data distributions, like generalized\nextreme value (gen-extreme) and log norm distributions, affect FL performance.\n(2) Analyzing how different detrending techniques for non-linear time-series\ndata influence the forecasting model's performance in a FL setup. We generated\nseveral synthetic time-series datasets using non-linear data distributions and\ntrained an LSTM-based forecasting model using both centralized and FL\napproaches. Additionally, we evaluated the impact of detrending on real-world\ndatasets with non-linear time-series data distributions. Our experimental\nresults show that: (1) FL performs worse than centralized approaches when\ndealing with non-linear data distributions. (2) The use of appropriate\ndetrending techniques improves FL performance, reducing loss across different\ndata distributions."
    },
    {
        "date": "2025-07",
        "title": "Magneto-Ionic Hardware Security Primitives: Embedding Data Protection at the Material Level",
        "author": "Irena Spasojevic, Federica Celegato, Alessandro Magni, Paola Tiberto, and Jordi Sort",
        "link": "http://arxiv.org/abs/2507.14213v1",
        "abstract": "The Big Data revolution has heightened the demand for robust,\nenergy-efficient security hardware capable of withstanding increasingly\nsophisticated cyber threats. Conventional encryption schemes, reliant on\ncomplex algorithms, are resource-intensive and remain vulnerable. To fortify\nsensitive information, society needs innovative anti-hacking and\nanti-counterfeiting technologies that exploit new materials and designs. Here,\nwe present a magneto-ionic strategy for hardware-level security based on fully\nselective voltage-controlled N3- ion migration within pre-defined, initially\nparamagnetic FeCoN dots. This process generates ferromagnetic sublayers of\ntuneable thickness, resulting in either deterministic (single-domain or vortex)\nor probabilistic states (with coexisting magnetic configurations and\nvoltage-adjustable probabilities), each exhibiting stochastic orientation and\nchirality, thereby providing a rich platform for magnetic fingerprinting. This\napproach enables self-protected primitives, including true random number\ngenerators, physical unclonable functions, and in-memory probabilistic\ninference. The resulting reconfigurable architecture combines tamper\nresistance, low energy consumption, and scalability, marking a significant leap\ntoward next-generation hardware security rooted in emergent magnetic phenomena."
    },
    {
        "date": "2025-07",
        "title": "Robust-Multi-Task Gradient Boosting",
        "author": "Seyedsaman Emami, Gonzalo Mart\u00ednez-Mu\u00f1oz, and Daniel Hern\u00e1ndez-Lobato",
        "link": "http://arxiv.org/abs/2507.11411v1",
        "abstract": "Multi-task learning (MTL) has shown effectiveness in exploiting shared\ninformation across tasks to improve generalization. MTL assumes tasks share\nsimilarities that can improve performance. In addition, boosting algorithms\nhave demonstrated exceptional performance across diverse learning problems,\nprimarily due to their ability to focus on hard-to-learn instances and\niteratively reduce residual errors. This makes them a promising approach for\nlearning multi-task problems. However, real-world MTL scenarios often involve\ntasks that are not well-aligned (known as outlier or adversarial tasks), which\ndo not share beneficial similarities with others and can, in fact, deteriorate\nthe performance of the overall model. To overcome this challenge, we propose\nRobust-Multi-Task Gradient Boosting (R-MTGB), a novel boosting framework that\nexplicitly models and adapts to task heterogeneity during training. R-MTGB\nstructures the learning process into three sequential blocks: (1) learning\nshared patterns, (2) partitioning tasks into outliers and non-outliers with\nregularized parameters, and (3) fine-tuning task-specific predictors. This\narchitecture enables R-MTGB to automatically detect and penalize outlier tasks\nwhile promoting effective knowledge transfer among related tasks. Our method\nintegrates these mechanisms seamlessly within gradient boosting, allowing\nrobust handling of noisy or adversarial tasks without sacrificing accuracy.\nExtensive experiments on both synthetic benchmarks and real-world datasets\ndemonstrate that our approach successfully isolates outliers, transfers\nknowledge, and consistently reduces prediction errors for each task\nindividually, and achieves overall performance gains across all tasks. These\nresults highlight robustness, adaptability, and reliable convergence of R-MTGB\nin challenging MTL environments."
    },
    {
        "date": "2025-07",
        "title": "LyAm: Robust Non-Convex Optimization for Stable Learning in Noisy Environments",
        "author": "Elmira Mirzabeigi, Sepehr Rezaee, and Kourosh Parand",
        "link": "http://arxiv.org/abs/2507.11262v1",
        "abstract": "Training deep neural networks, particularly in computer vision tasks, often\nsuffers from noisy gradients and unstable convergence, which hinder performance\nand generalization. In this paper, we propose LyAm, a novel optimizer that\nintegrates Adam's adaptive moment estimation with Lyapunov-based stability\nmechanisms. LyAm dynamically adjusts the learning rate using Lyapunov stability\ntheory to enhance convergence robustness and mitigate training noise. We\nprovide a rigorous theoretical framework proving the convergence guarantees of\nLyAm in complex, non-convex settings. Extensive experiments on like as CIFAR-10\nand CIFAR-100 show that LyAm consistently outperforms state-of-the-art\noptimizers in terms of accuracy, convergence speed, and stability, establishing\nit as a strong candidate for robust deep learning optimization."
    },
    {
        "date": "2025-07",
        "title": "A Robust Incomplete Multimodal Low-Rank Adaptation Approach for Emotion Recognition",
        "author": "Xinkui Zhao, Jinsong Shu, Yangyang Wu, Guanjie Cheng, Zihe Liu, Naibo Wang, Shuiguang Deng, Zhongle Xie, and Jianwei Yin",
        "link": "http://arxiv.org/abs/2507.11202v1",
        "abstract": "Multimodal Emotion Recognition (MER) often encounters incomplete\nmultimodality in practical applications due to sensor failures or privacy\nprotection requirements. While existing methods attempt to address various\nincomplete multimodal scenarios by balancing the training of each modality\ncombination through additional gradients, these approaches face a critical\nlimitation: training gradients from different modality combinations conflict\nwith each other, ultimately degrading the performance of the final prediction\nmodel. In this paper, we propose a unimodal decoupled dynamic low-rank\nadaptation method based on modality combinations, named MCULoRA, which is a\nnovel framework for the parameter-efficient training of incomplete multimodal\nlearning models. MCULoRA consists of two key modules, modality combination\naware low-rank adaptation (MCLA) and dynamic parameter fine-tuning (DPFT). The\nMCLA module effectively decouples the shared information from the distinct\ncharacteristics of individual modality combinations. The DPFT module adjusts\nthe training ratio of modality combinations based on the separability of each\nmodality's representation space, optimizing the learning efficiency across\ndifferent modality combinations. Our extensive experimental evaluation in\nmultiple benchmark datasets demonstrates that MCULoRA substantially outperforms\nprevious incomplete multimodal learning approaches in downstream task accuracy."
    },
    {
        "date": "2025-07",
        "title": "Secure Goal-Oriented Communication: Defending against Eavesdropping Timing Attacks",
        "author": "Federico Mason, Federico Chiariotti, Pietro Talli, and Andrea Zanella",
        "link": "http://arxiv.org/abs/2507.14212v1",
        "abstract": "Goal-oriented Communication (GoC) is a new paradigm that plans data\ntransmission to occur only when it is instrumental for the receiver to achieve\na certain goal. This leads to the advantage of reducing the frequency of\ntransmissions significantly while maintaining adherence to the receiver's\nobjectives. However, GoC scheduling also opens a timing-based side channel that\nan eavesdropper can exploit to obtain information about the state of the\nsystem. This type of attack sidesteps even information-theoretic security, as\nit exploits the timing of updates rather than their content. In this work, we\nstudy such an eavesdropping attack against pull-based goal-oriented scheduling\nfor remote monitoring and control of Markov processes. We provide a theoretical\nframework for defining the effectiveness of the attack and propose possible\ncountermeasures, including two practical heuristics that provide a balance\nbetween the performance gains offered by GoC and the amount of leaked\ninformation. Our results show that, while a naive goal-oriented scheduler\nallows the eavesdropper to correctly guess the system state about 60% of the\ntime, our heuristic defenses can halve the leakage with a marginal reduction of\nthe benefits of goal-oriented approaches."
    },
    {
        "date": "2025-07",
        "title": "Hashed Watermark as a Filter: Defeating Forging and Overwriting Attacks in Weight-based Neural Network Watermarking",
        "author": "Yuan Yao, Jin Song, and Jian Jin",
        "link": "http://arxiv.org/abs/2507.11137v1",
        "abstract": "As valuable digital assets, deep neural networks necessitate robust ownership\nprotection, positioning neural network watermarking (NNW) as a promising\nsolution. Among various NNW approaches, weight-based methods are favored for\ntheir simplicity and practicality; however, they remain vulnerable to forging\nand overwriting attacks. To address those challenges, we propose NeuralMark, a\nrobust method built around a hashed watermark filter. Specifically, we utilize\na hash function to generate an irreversible binary watermark from a secret key,\nwhich is then used as a filter to select the model parameters for embedding.\nThis design cleverly intertwines the embedding parameters with the hashed\nwatermark, providing a robust defense against both forging and overwriting\nattacks. An average pooling is also incorporated to resist fine-tuning and\npruning attacks. Furthermore, it can be seamlessly integrated into various\nneural network architectures, ensuring broad applicability. Theoretically, we\nanalyze its security boundary. Empirically, we verify its effectiveness and\nrobustness across 13 distinct Convolutional and Transformer architectures,\ncovering five image classification tasks and one text generation task. The\nsource codes are available at https://github.com/AIResearch-Group/NeuralMark."
    },
    {
        "date": "2025-07",
        "title": "Robust 3D-Masked Part-level Editing in 3D Gaussian Splatting with Regularized Score Distillation Sampling",
        "author": "Hayeon Kim, Ji Ha Jang, and Se Young Chun",
        "link": "http://arxiv.org/abs/2507.11061v2",
        "abstract": "Recent advances in 3D neural representations and instance-level editing\nmodels have enabled the efficient creation of high-quality 3D content. However,\nachieving precise local 3D edits remains challenging, especially for Gaussian\nSplatting, due to inconsistent multi-view 2D part segmentations and inherently\nambiguous nature of Score Distillation Sampling (SDS) loss. To address these\nlimitations, we propose RoMaP, a novel local 3D Gaussian editing framework that\nenables precise and drastic part-level modifications. First, we introduce a\nrobust 3D mask generation module with our 3D-Geometry Aware Label Prediction\n(3D-GALP), which uses spherical harmonics (SH) coefficients to model\nview-dependent label variations and soft-label property, yielding accurate and\nconsistent part segmentations across viewpoints. Second, we propose a\nregularized SDS loss that combines the standard SDS loss with additional\nregularizers. In particular, an L1 anchor loss is introduced via our Scheduled\nLatent Mixing and Part (SLaMP) editing method, which generates high-quality\npart-edited 2D images and confines modifications only to the target region\nwhile preserving contextual coherence. Additional regularizers, such as\nGaussian prior removal, further improve flexibility by allowing changes beyond\nthe existing context, and robust 3D masking prevents unintended edits.\nExperimental results demonstrate that our RoMaP achieves state-of-the-art local\n3D editing on both reconstructed and generated Gaussian scenes and objects\nqualitatively and quantitatively, making it possible for more robust and\nflexible part-level 3D Gaussian editing. Code is available at\nhttps://janeyeon.github.io/romap."
    },
    {
        "date": "2025-07",
        "title": "GATE: Graph Attention Neural Networks with Real-Time Edge Construction for Robust Indoor Localization using Mobile Embedded Devices",
        "author": "Danish Gufran, and Sudeep Pasricha",
        "link": "http://arxiv.org/abs/2507.11053v1",
        "abstract": "Accurate indoor localization is crucial for enabling spatial context in smart\nenvironments and navigation systems. Wi-Fi Received Signal Strength (RSS)\nfingerprinting is a widely used indoor localization approach due to its\ncompatibility with mobile embedded devices. Deep Learning (DL) models improve\naccuracy in localization tasks by learning RSS variations across locations, but\nthey assume fingerprint vectors exist in a Euclidean space, failing to\nincorporate spatial relationships and the non-uniform distribution of\nreal-world RSS noise. This results in poor generalization across heterogeneous\nmobile devices, where variations in hardware and signal processing distort RSS\nreadings. Graph Neural Networks (GNNs) can improve upon conventional DL models\nby encoding indoor locations as nodes and modeling their spatial and signal\nrelationships as edges. However, GNNs struggle with non-Euclidean noise\ndistributions and suffer from the GNN blind spot problem, leading to degraded\naccuracy in environments with dense access points (APs). To address these\nchallenges, we propose GATE, a novel framework that constructs an adaptive\ngraph representation of fingerprint vectors while preserving an indoor\nstate-space topology, modeling the non-Euclidean structure of RSS noise to\nmitigate environmental noise and address device heterogeneity. GATE introduces\n1) a novel Attention Hyperspace Vector (AHV) for enhanced message passing, 2) a\nnovel Multi-Dimensional Hyperspace Vector (MDHV) to mitigate the GNN blind\nspot, and 3) an new Real-Time Edge Construction (RTEC) approach for dynamic\ngraph adaptation. Extensive real-world evaluations across multiple indoor\nspaces with varying path lengths, AP densities, and heterogeneous devices\ndemonstrate that GATE achieves 1.6x to 4.72x lower mean localization errors and\n1.85x to 4.57x lower worst-case errors compared to state-of-the-art indoor\nlocalization frameworks."
    },
    {
        "date": "2025-07",
        "title": "A Multi-View High-Resolution Foot-Ankle Complex Point Cloud Dataset During Gait for Occlusion-Robust 3D Completion",
        "author": "Jie-Wen Li, Zi-Han Ye, Qingyuan Zhou, Jiayi Song, Ying He, Ben Fei, and Wen-Ming Chen",
        "link": "http://arxiv.org/abs/2507.11037v1",
        "abstract": "The kinematics analysis of foot-ankle complex during gait is essential for\nadvancing biomechanical research and clinical assessment. Collecting accurate\nsurface geometry data from the foot and ankle during dynamic gait conditions is\ninherently challenging due to swing foot occlusions and viewing limitations.\nThus, this paper introduces FootGait3D, a novel multi-view dataset of\nhigh-resolution ankle-foot surface point clouds captured during natural gait.\nDifferent from existing gait datasets that typically target whole-body or\nlower-limb motion, FootGait3D focuses specifically on the detailed modeling of\nthe ankle-foot region, offering a finer granularity of motion data. To address\nthis, FootGait3D consists of 8,403 point cloud frames collected from 46\nsubjects using a custom five-camera depth sensing system. Each frame includes a\ncomplete 5-view reconstruction of the foot and ankle (serving as ground truth)\nalong with partial point clouds obtained from only four, three, or two views.\nThis structured variation enables rigorous evaluation of 3D point cloud\ncompletion methods under varying occlusion levels and viewpoints. Our dataset\nis designed for shape completion tasks, facilitating the benchmarking of\nstate-of-the-art single-modal (e.g., PointTr, SnowflakeNet, Anchorformer) and\nmulti-modal (e.g., SVDFormer, PointSea, CSDN) completion networks on the\nchallenge of recovering the full foot geometry from occluded inputs. FootGait3D\nhas significant potential to advance research in biomechanics and multi-segment\nfoot modeling, offering a valuable testbed for clinical gait analysis,\nprosthetic design, and robotics applications requiring detailed 3D models of\nthe foot during motion. The dataset is now available at\nhttps://huggingface.co/datasets/ljw285/FootGait3D."
    },
    {
        "date": "2025-07",
        "title": "Crafting Imperceptible On-Manifold Adversarial Attacks for Tabular Data",
        "author": "Zhipeng He, Alexander Stevens, Chun Ouyang, Johannes De Smedt, Alistair Barros, and Catarina Moreira",
        "link": "http://arxiv.org/abs/2507.10998v1",
        "abstract": "Adversarial attacks on tabular data present fundamental challenges distinct\nfrom image or text domains due to the heterogeneous nature of mixed categorical\nand numerical features. Unlike images where pixel perturbations maintain visual\nsimilarity, tabular data lacks intuitive similarity metrics, making it\ndifficult to define imperceptible modifications. Additionally, traditional\ngradient-based methods prioritise $\\ell_p$-norm constraints, often producing\nadversarial examples that deviate from the original data distributions, making\nthem detectable. We propose a latent space perturbation framework using a\nmixed-input Variational Autoencoder (VAE) to generate imperceptible adversarial\nexamples. The proposed VAE integrates categorical embeddings and numerical\nfeatures into a unified latent manifold, enabling perturbations that preserve\nstatistical consistency. We specify In-Distribution Success Rate (IDSR) to\nmeasure the proportion of adversarial examples that remain statistically\nindistinguishable from the input distribution. Evaluation across six publicly\navailable datasets and three model architectures demonstrates that our method\nachieves substantially lower outlier rates and more consistent performance\ncompared to traditional input-space attacks and other VAE-based methods adapted\nfrom image domain approaches. Our comprehensive analysis includes\nhyperparameter sensitivity, sparsity control mechanisms, and generative\narchitectural comparisons, revealing that VAE-based attacks depend critically\non reconstruction quality but offer superior practical utility when sufficient\ntraining data is available. This work highlights the importance of on-manifold\nperturbations for realistic adversarial attacks on tabular data, offering a\nrobust approach for practical deployment. The source code can be accessed\nthrough https://github.com/ZhipengHe/VAE-TabAttack."
    },
    {
        "date": "2025-07",
        "title": "Security Enclave Architecture for Heterogeneous Security Primitives for Supply-Chain Attacks",
        "author": "Kshitij Raj, Atri Chatterjee, Patanjali SLPSK, Swarup Bhunia, and Sandip Ray",
        "link": "http://arxiv.org/abs/2507.10971v1",
        "abstract": "Designing secure architectures for system-on-chip (SoC) platforms is a highly\nintricate and time-intensive task, often requiring months of development and\nmeticulous verification. Even minor architectural oversights can lead to\ncritical vulnerabilities that undermine the security of the entire chip. In\nresponse to this challenge, we introduce CITADEL, a modular security framework\naimed at streamlining the creation of robust security architectures for SoCs.\nCITADEL offers a configurable, plug-and-play subsystem composed of custom\nintellectual property (IP) blocks, enabling the construction of diverse\nsecurity mechanisms tailored to specific threats. As a concrete demonstration,\nwe instantiate CITADEL to defend against supply-chain threats, illustrating how\nthe framework adapts to one of the most pressing concerns in hardware security.\nThis paper explores the range of obstacles encountered when building a unified\nsecurity architecture capable of addressing multiple attack vectors and\npresents CITADEL's strategies for overcoming them. Through several real-world\ncase studies, we showcase the practical implementation of CITADEL and present a\nthorough evaluation of its impact on silicon area and power consumption across\nvarious ASIC technologies. Results indicate that CITADEL introduces only\nminimal resource overhead, making it a practical solution for enhancing SoC\nsecurity."
    },
    {
        "date": "2025-07",
        "title": "Towards Robust Speech Recognition for Jamaican Patois Music Transcription",
        "author": "Jordan Madden, Matthew Stone, Dimitri Johnson, and Daniel Geddez",
        "link": "http://arxiv.org/abs/2507.16834v1",
        "abstract": "Although Jamaican Patois is a widely spoken language, current speech\nrecognition systems perform poorly on Patois music, producing inaccurate\ncaptions that limit accessibility and hinder downstream applications. In this\nwork, we take a data-centric approach to this problem by curating more than 40\nhours of manually transcribed Patois music. We use this dataset to fine-tune\nstate-of-the-art automatic speech recognition (ASR) models, and use the results\nto develop scaling laws for the performance of Whisper models on Jamaican\nPatois audio. We hope that this work will have a positive impact on the\naccessibility of Jamaican Patois music and the future of Jamaican Patois\nlanguage modeling."
    },
    {
        "date": "2025-07",
        "title": "Small Edits, Big Consequences: Telling Good from Bad Robustness in Large Language Models",
        "author": "Altynbek Ismailov, and Salia Asanova",
        "link": "http://arxiv.org/abs/2507.15868v1",
        "abstract": "Large language models (LLMs) now write code in settings where misreading a\nsingle word can break safety or cost money, yet we still expect them to\noverlook stray typos. To probe where useful robustness ends and harmful\ninsensitivity begins, we compile 50 LeetCode problems and craft three minimal\nprompt perturbations that should vary in importance: (i) progressive\nunderspecification deleting 10 % of words per step; (ii) lexical flip swapping\na pivotal quantifier (\"max\" to \"min\"); and (iii) jargon inflation replacing a\ncommon noun with an obscure technical synonym. Six frontier models, including\nthree \"reasoning-tuned\" versions, solve each mutated prompt, and their Python\noutputs are checked against the original test suites to reveal whether they\nreused the baseline solution or adapted. Among 11 853 generations we observe a\nsharp double asymmetry. Models remain correct in 85 % of cases even after 90 %\nof the prompt is missing, showing over-robustness to underspecification, yet\nonly 54 % react to a single quantifier flip that reverses the task, with\nreasoning-tuned variants even less sensitive than their bases. Jargon edits lie\nin between, passing through 56 %. Current LLMs thus blur the line between\nharmless noise and meaning - changing edits, often treating both as ignorable.\nMasking salient anchors such as function names can force re - evaluation. We\nadvocate evaluation and training protocols that reward differential\nsensitivity: stay steady under benign noise but adapt - or refuse - when\nsemantics truly change."
    },
    {
        "date": "2025-07",
        "title": "Robust ID-Specific Face Restoration via Alignment Learning",
        "author": "Yushun Fang, Lu Liu, Xiang Gao, Qiang Hu, Ning Cao, Jianghe Cui, Gang Chen, and Xiaoyun Zhang",
        "link": "http://arxiv.org/abs/2507.10943v1",
        "abstract": "The latest developments in Face Restoration have yielded significant\nadvancements in visual quality through the utilization of diverse diffusion\npriors. Nevertheless, the uncertainty of face identity introduced by\nidentity-obscure inputs and stochastic generative processes remains unresolved.\nTo address this challenge, we present Robust ID-Specific Face Restoration\n(RIDFR), a novel ID-specific face restoration framework based on diffusion\nmodels. Specifically, RIDFR leverages a pre-trained diffusion model in\nconjunction with two parallel conditioning modules. The Content Injection\nModule inputs the severely degraded image, while the Identity Injection Module\nintegrates the specific identity from a given image. Subsequently, RIDFR\nincorporates Alignment Learning, which aligns the restoration results from\nmultiple references with the same identity in order to suppress the\ninterference of ID-irrelevant face semantics (e.g. pose, expression, make-up,\nhair style). Experiments demonstrate that our framework outperforms the\nstate-of-the-art methods, reconstructing high-quality ID-specific results with\nhigh identity fidelity and demonstrating strong robustness."
    },
    {
        "date": "2025-07",
        "title": "How to Protect Models against Adversarial Unlearning?",
        "author": "Patryk Jasiorski, Marek Klonowski, and Micha\u0142 Wo\u017aniak",
        "link": "http://arxiv.org/abs/2507.10886v1",
        "abstract": "AI models need to be unlearned to fulfill the requirements of legal acts such\nas the AI Act or GDPR, and also because of the need to remove toxic content,\ndebiasing, the impact of malicious instances, or changes in the data\ndistribution structure in which a model works. Unfortunately, removing\nknowledge may cause undesirable side effects, such as a deterioration in model\nperformance. In this paper, we investigate the problem of adversarial\nunlearning, where a malicious party intentionally sends unlearn requests to\ndeteriorate the model's performance maximally. We show that this phenomenon and\nthe adversary's capabilities depend on many factors, primarily on the backbone\nmodel itself and strategy/limitations in selecting data to be unlearned. The\nmain result of this work is a new method of protecting model performance from\nthese side effects, both in the case of unlearned behavior resulting from\nspontaneous processes and adversary actions."
    },
    {
        "date": "2025-07",
        "title": "Learning from Imperfect Data: Robust Inference of Dynamic Systems using Simulation-based Generative Model",
        "author": "Hyunwoo Cho, Hyeontae Jo, and Hyung Ju Hwang",
        "link": "http://arxiv.org/abs/2507.10884v1",
        "abstract": "System inference for nonlinear dynamic models, represented by ordinary\ndifferential equations (ODEs), remains a significant challenge in many fields,\nparticularly when the data are noisy, sparse, or partially observable. In this\npaper, we propose a Simulation-based Generative Model for Imperfect Data\n(SiGMoID) that enables precise and robust inference for dynamic systems. The\nproposed approach integrates two key methods: (1) physics-informed neural\nnetworks with hyper-networks that constructs an ODE solver, and (2) Wasserstein\ngenerative adversarial networks that estimates ODE parameters by effectively\ncapturing noisy data distributions. We demonstrate that SiGMoID quantifies data\nnoise, estimates system parameters, and infers unobserved system components.\nIts effectiveness is validated validated through realistic experimental\nexamples, showcasing its broad applicability in various domains, from\nscientific research to engineered systems, and enabling the discovery of full\nsystem dynamics."
    },
    {
        "date": "2025-07",
        "title": "A Lightweight and Robust Framework for Real-Time Colorectal Polyp Detection Using LOF-Based Preprocessing and YOLO-v11n",
        "author": "Saadat Behzadi, Danial Sharifrazi, Bita Mesbahzadeh, Javad Hassannataj Joloudari, and Roohallah Alizadehsani",
        "link": "http://arxiv.org/abs/2507.10864v2",
        "abstract": "Objectives: Timely and accurate detection of colorectal polyps plays a\ncrucial role in diagnosing and preventing colorectal cancer, a major cause of\nmortality worldwide. This study introduces a new, lightweight, and efficient\nframework for polyp detection that combines the Local Outlier Factor (LOF)\nalgorithm for filtering noisy data with the YOLO-v11n deep learning model.\n  Study design: An experimental study leveraging deep learning and outlier\nremoval techniques across multiple public datasets.\n  Methods: The proposed approach was tested on five diverse and publicly\navailable datasets: CVC-ColonDB, CVC-ClinicDB, Kvasir-SEG, ETIS, and EndoScene.\nSince these datasets originally lacked bounding box annotations, we converted\ntheir segmentation masks into suitable detection labels. To enhance the\nrobustness and generalizability of our model, we apply 5-fold cross-validation\nand remove anomalous samples using the LOF method configured with 30 neighbors\nand a contamination ratio of 5%. Cleaned data are then fed into YOLO-v11n, a\nfast and resource-efficient object detection architecture optimized for\nreal-time applications. We train the model using a combination of modern\naugmentation strategies to improve detection accuracy under diverse conditions.\n  Results: Our approach significantly improves polyp localization performance,\nachieving a precision of 95.83%, recall of 91.85%, F1-score of 93.48%, mAP@0.5\nof 96.48%, and mAP@0.5:0.95 of 77.75%. Compared to previous YOLO-based methods,\nour model demonstrates enhanced accuracy and efficiency.\n  Conclusions: These results suggest that the proposed method is well-suited\nfor real-time colonoscopy support in clinical settings. Overall, the study\nunderscores how crucial data preprocessing and model efficiency are when\ndesigning effective AI systems for medical imaging."
    },
    {
        "date": "2025-07",
        "title": "REAL-IoT: Characterizing GNN Intrusion Detection Robustness under Practical Adversarial Attack",
        "author": "Zhonghao Zhan, Huichi Zhou, and Hamed Haddadi",
        "link": "http://arxiv.org/abs/2507.10836v1",
        "abstract": "Graph Neural Network (GNN)-based network intrusion detection systems (NIDS)\nare often evaluated on single datasets, limiting their ability to generalize\nunder distribution drift. Furthermore, their adversarial robustness is\ntypically assessed using synthetic perturbations that lack realism. This\nmeasurement gap leads to an overestimation of GNN-based NIDS resilience. To\naddress the limitations, we propose \\textbf{REAL-IoT}, a comprehensive\nframework for robustness evaluation of GNN-based NIDS in IoT environments. Our\nframework presents a methodology that creates a unified dataset from canonical\ndatasets to assess generalization under drift. In addition, it features a novel\nintrusion dataset collected from a physical IoT testbed, which captures network\ntraffic and attack scenarios under real-world settings. Furthermore, using\nREAL-IoT, we explore the usage of Large Language Models (LLMs) to analyze\nnetwork data and mitigate the impact of adversarial examples by filtering\nsuspicious flows. Our evaluations using REAL-IoT reveal performance drops in\nGNN models compared to results from standard benchmarks, quantifying their\nsusceptibility to drift and realistic attacks. We also demonstrate the\npotential of LLM-based filtering to enhance robustness. These findings\nemphasize the necessity of realistic threat modeling and rigorous measurement\npractices for developing resilient IoT intrusion detection systems."
    },
    {
        "date": "2025-07",
        "title": "\"Is it always watching? Is it always listening?\" Exploring Contextual Privacy and Security Concerns Toward Domestic Social Robots",
        "author": "Henry Bell, Jabari Kwesi, Hiba Laabadli, and Pardis Emami-Naeini",
        "link": "http://arxiv.org/abs/2507.10786v2",
        "abstract": "Equipped with artificial intelligence (AI) and advanced sensing capabilities,\nsocial robots are gaining interest among consumers in the United States. These\nrobots seem like a natural evolution of traditional smart home devices.\nHowever, their extensive data collection capabilities, anthropomorphic\nfeatures, and capacity to interact with their environment make social robots a\nmore significant security and privacy threat. Increased risks include data\nlinkage, unauthorized data sharing, and the physical safety of users and their\nhomes. It is critical to investigate U.S. users' security and privacy needs and\nconcerns to guide the design of social robots while these devices are still in\nthe early stages of commercialization in the U.S. market. Through 19\nsemi-structured interviews, we identified significant security and privacy\nconcerns, highlighting the need for transparency, usability, and robust privacy\ncontrols to support adoption. For educational applications, participants\nworried most about misinformation, and in medical use cases, they worried about\nthe reliability of these devices. Participants were also concerned with the\ndata inference that social robots could enable. We found that participants\nexpect tangible privacy controls, indicators of data collection, and\ncontext-appropriate functionality."
    },
    {
        "date": "2025-07",
        "title": "Integrating Biological Knowledge for Robust Microscopy Image Profiling on De Novo Cell Lines",
        "author": "Jiayuan Chen, Thai-Hoang Pham, Yuanlong Wang, and Ping Zhang",
        "link": "http://arxiv.org/abs/2507.10737v1",
        "abstract": "High-throughput screening techniques, such as microscopy imaging of cellular\nresponses to genetic and chemical perturbations, play a crucial role in drug\ndiscovery and biomedical research. However, robust perturbation screening for\n\\textit{de novo} cell lines remains challenging due to the significant\nmorphological and biological heterogeneity across cell lines. To address this,\nwe propose a novel framework that integrates external biological knowledge into\nexisting pretraining strategies to enhance microscopy image profiling models.\nOur approach explicitly disentangles perturbation-specific and cell\nline-specific representations using external biological information.\nSpecifically, we construct a knowledge graph leveraging protein interaction\ndata from STRING and Hetionet databases to guide models toward\nperturbation-specific features during pretraining. Additionally, we incorporate\ntranscriptomic features from single-cell foundation models to capture cell\nline-specific representations. By learning these disentangled features, our\nmethod improves the generalization of imaging models to \\textit{de novo} cell\nlines. We evaluate our framework on the RxRx database through one-shot\nfine-tuning on an RxRx1 cell line and few-shot fine-tuning on cell lines from\nthe RxRx19a dataset. Experimental results demonstrate that our method enhances\nmicroscopy image profiling for \\textit{de novo} cell lines, highlighting its\neffectiveness in real-world phenotype-based drug discovery applications."
    },
    {
        "date": "2025-07",
        "title": "3S-Attack: Spatial, Spectral and Semantic Invisible Backdoor Attack Against DNN Models",
        "author": "Jianyao Yin, Luca Arnaboldi, Honglong Chen, and Pascal Berrang",
        "link": "http://arxiv.org/abs/2507.10733v1",
        "abstract": "Backdoor attacks involve either poisoning the training data or directly\nmodifying the model in order to implant a hidden behavior, that causes the\nmodel to misclassify inputs when a specific trigger is present. During\ninference, the model maintains high accuracy on benign samples but\nmisclassifies poisoned samples into an attacker-specified target class.\nExisting research on backdoor attacks has explored developing triggers in the\nspatial, spectral (frequency), and semantic (feature) domains, aiming to make\nthem stealthy. While some approaches have considered designing triggers that\nare imperceptible in both spatial and spectral domains, few have incorporated\nthe semantic domain. In this paper, we propose a novel backdoor attack, termed\n3S-attack, which is stealthy across the spatial, spectral, and semantic\ndomains. The key idea is to exploit the semantic features of benign samples as\ntriggers, using Gradient-weighted Class Activation Mapping (Grad-CAM) and a\npreliminary model for extraction. The trigger is then embedded in the spectral\ndomain, followed by pixel-level restrictions after converting the samples back\nto the spatial domain. This process minimizes the distance between poisoned and\nbenign samples, making the attack harder to detect by existing defenses and\nhuman inspection. Extensive experiments on various datasets, along with\ntheoretical analysis, demonstrate the stealthiness of 3S-attack and highlight\nthe need for stronger defenses to ensure AI security. Our code is available at:\nhttps://anonymous.4open.science/r/anon-project-3776/"
    },
    {
        "date": "2025-07",
        "title": "Access Control for Information-Theoretically Secure Key-Document Stores",
        "author": "Yin Li, Sharad Mehrota, Shantanu Sharma, and Komal Kumari",
        "link": "http://arxiv.org/abs/2507.10730v1",
        "abstract": "This paper presents a novel key-based access control technique for secure\noutsourcing key-value stores where values correspond to documents that are\nindexed and accessed using keys. The proposed approach adopts Shamir's\nsecret-sharing that offers unconditional or information-theoretic security. It\nsupports keyword-based document retrieval while preventing leakage of the data,\naccess rights of users, or the size (\\textit{i}.\\textit{e}., volume of the\noutput that satisfies a query). The proposed approach allows servers to detect\n(and abort) malicious clients from gaining unauthorized access to data, and\nprevents malicious servers from altering data undetected while ensuring\nefficient access -- it takes 231.5ms over 5,000 keywords across 500,000 files."
    },
    {
        "date": "2025-07",
        "title": "Distributionally Robust Optimization with Adversarial Data Contamination",
        "author": "Shuyao Li, Ilias Diakonikolas, and Jelena Diakonikolas",
        "link": "http://arxiv.org/abs/2507.10718v1",
        "abstract": "Distributionally Robust Optimization (DRO) provides a framework for\ndecision-making under distributional uncertainty, yet its effectiveness can be\ncompromised by outliers in the training data. This paper introduces a\nprincipled approach to simultaneously address both challenges. We focus on\noptimizing Wasserstein-1 DRO objectives for generalized linear models with\nconvex Lipschitz loss functions, where an $\\epsilon$-fraction of the training\ndata is adversarially corrupted. Our primary contribution lies in a novel\nmodeling framework that integrates robustness against training data\ncontamination with robustness against distributional shifts, alongside an\nefficient algorithm inspired by robust statistics to solve the resulting\noptimization problem. We prove that our method achieves an estimation error of\n$O(\\sqrt{\\epsilon})$ for the true DRO objective value using only the\ncontaminated data under the bounded covariance assumption. This work\nestablishes the first rigorous guarantees, supported by efficient computation,\nfor learning under the dual challenges of data contamination and distributional\nshifts."
    },
    {
        "date": "2025-07",
        "title": "Robust Multi-Manifold Clustering via Simplex Paths",
        "author": "Haoyu Chen, Anna Little, and Akin Narayan",
        "link": "http://arxiv.org/abs/2507.10710v1",
        "abstract": "This article introduces a novel, geometric approach for multi-manifold\nclustering (MMC), i.e. for clustering a collection of potentially intersecting,\nd-dimensional manifolds into the individual manifold components. We first\ncompute a locality graph on d-simplices, using the dihedral angle in between\nadjacent simplices as the graph weights, and then compute infinity path\ndistances in this simplex graph. This procedure gives a metric on simplices\nwhich we refer to as the largest angle path distance (LAPD). We analyze the\nproperties of LAPD under random sampling, and prove that with an appropriate\ndenoising procedure, this metric separates the manifold components with high\nprobability. We validate the proposed methodology with extensive numerical\nexperiments on both synthetic and real-world data sets. These experiments\ndemonstrate that the method is robust to noise, curvature, and small\nintersection angle, and generally out-performs other MMC algorithms. In\naddition, we provide a highly scalable implementation of the proposed\nalgorithm, which leverages approximation schemes for infinity path distance to\nachieve quasi-linear computational complexity."
    }
]